
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1122 10:30:20.650496 20880 caffe.cpp:219] Using GPUs 0
I1122 10:30:20.824041 20880 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1122 10:30:21.118105 20880 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 10:30:21.133394 20880 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_300k_8L"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1122 10:30:21.133394 20880 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 10:30:21.134394 20880 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 10:30:21.134394 20880 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1122 10:30:21.134394 20880 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1122 10:30:21.134394 20880 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1122 10:30:21.134394 20880 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1122 10:30:21.134394 20880 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1122 10:30:21.134394 20880 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1122 10:30:21.134394 20880 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1122 10:30:21.134394 20880 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1122 10:30:21.134394 20880 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1122 10:30:21.134394 20880 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1122 10:30:21.134394 20880 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1122 10:30:21.134394 20880 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 41
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 43
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 85
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1122 10:30:21.183431 20880 layer_factory.cpp:58] Creating layer cifar
I1122 10:30:21.191392 20880 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1122 10:30:21.191392 20880 net.cpp:84] Creating Layer cifar
I1122 10:30:21.191392 20880 net.cpp:380] cifar -> data
I1122 10:30:21.191392 20880 net.cpp:380] cifar -> label
I1122 10:30:21.192423 20880 data_layer.cpp:45] output data size: 100,3,32,32
I1122 10:30:21.198407 20880 net.cpp:122] Setting up cifar
I1122 10:30:21.198407 20880 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1122 10:30:21.198407 20880 net.cpp:129] Top shape: 100 (100)
I1122 10:30:21.198407 20880 net.cpp:137] Memory required for data: 1229200
I1122 10:30:21.198407 20880 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1122 10:30:21.198407 20880 net.cpp:84] Creating Layer label_cifar_1_split
I1122 10:30:21.198407 20880 net.cpp:406] label_cifar_1_split <- label
I1122 10:30:21.198407 20880 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1122 10:30:21.198407 20880 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1122 10:30:21.198407 20880 net.cpp:122] Setting up label_cifar_1_split
I1122 10:30:21.198407 20880 net.cpp:129] Top shape: 100 (100)
I1122 10:30:21.198407 20880 net.cpp:129] Top shape: 100 (100)
I1122 10:30:21.198407 20880 net.cpp:137] Memory required for data: 1230000
I1122 10:30:21.198407 20880 layer_factory.cpp:58] Creating layer conv1
I1122 10:30:21.198407 20880 net.cpp:84] Creating Layer conv1
I1122 10:30:21.198407 20880 net.cpp:406] conv1 <- data
I1122 10:30:21.198407 20880 net.cpp:380] conv1 -> conv1
I1122 10:30:21.199395 20328 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 10:30:21.443384 20880 net.cpp:122] Setting up conv1
I1122 10:30:21.443384 20880 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 10:30:21.443384 20880 net.cpp:137] Memory required for data: 18023600
I1122 10:30:21.443384 20880 layer_factory.cpp:58] Creating layer bn1
I1122 10:30:21.443384 20880 net.cpp:84] Creating Layer bn1
I1122 10:30:21.443384 20880 net.cpp:406] bn1 <- conv1
I1122 10:30:21.443384 20880 net.cpp:367] bn1 -> conv1 (in-place)
I1122 10:30:21.443384 20880 net.cpp:122] Setting up bn1
I1122 10:30:21.443384 20880 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 10:30:21.443384 20880 net.cpp:137] Memory required for data: 34817200
I1122 10:30:21.443384 20880 layer_factory.cpp:58] Creating layer scale1
I1122 10:30:21.443384 20880 net.cpp:84] Creating Layer scale1
I1122 10:30:21.443384 20880 net.cpp:406] scale1 <- conv1
I1122 10:30:21.443384 20880 net.cpp:367] scale1 -> conv1 (in-place)
I1122 10:30:21.443384 20880 layer_factory.cpp:58] Creating layer scale1
I1122 10:30:21.443384 20880 net.cpp:122] Setting up scale1
I1122 10:30:21.443384 20880 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 10:30:21.443384 20880 net.cpp:137] Memory required for data: 51610800
I1122 10:30:21.443384 20880 layer_factory.cpp:58] Creating layer relu1
I1122 10:30:21.443384 20880 net.cpp:84] Creating Layer relu1
I1122 10:30:21.443384 20880 net.cpp:406] relu1 <- conv1
I1122 10:30:21.443384 20880 net.cpp:367] relu1 -> conv1 (in-place)
I1122 10:30:21.444384 20880 net.cpp:122] Setting up relu1
I1122 10:30:21.444384 20880 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 10:30:21.444384 20880 net.cpp:137] Memory required for data: 68404400
I1122 10:30:21.444384 20880 layer_factory.cpp:58] Creating layer conv2
I1122 10:30:21.444384 20880 net.cpp:84] Creating Layer conv2
I1122 10:30:21.444384 20880 net.cpp:406] conv2 <- conv1
I1122 10:30:21.444384 20880 net.cpp:380] conv2 -> conv2
I1122 10:30:21.445384 20880 net.cpp:122] Setting up conv2
I1122 10:30:21.445384 20880 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 10:30:21.445384 20880 net.cpp:137] Memory required for data: 86017200
I1122 10:30:21.445384 20880 layer_factory.cpp:58] Creating layer bn2
I1122 10:30:21.445384 20880 net.cpp:84] Creating Layer bn2
I1122 10:30:21.445384 20880 net.cpp:406] bn2 <- conv2
I1122 10:30:21.445384 20880 net.cpp:367] bn2 -> conv2 (in-place)
I1122 10:30:21.445384 20880 net.cpp:122] Setting up bn2
I1122 10:30:21.445384 20880 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 10:30:21.445384 20880 net.cpp:137] Memory required for data: 103630000
I1122 10:30:21.445384 20880 layer_factory.cpp:58] Creating layer scale2
I1122 10:30:21.445384 20880 net.cpp:84] Creating Layer scale2
I1122 10:30:21.445384 20880 net.cpp:406] scale2 <- conv2
I1122 10:30:21.445384 20880 net.cpp:367] scale2 -> conv2 (in-place)
I1122 10:30:21.445384 20880 layer_factory.cpp:58] Creating layer scale2
I1122 10:30:21.446383 20880 net.cpp:122] Setting up scale2
I1122 10:30:21.446383 20880 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 10:30:21.446383 20880 net.cpp:137] Memory required for data: 121242800
I1122 10:30:21.446383 20880 layer_factory.cpp:58] Creating layer relu2
I1122 10:30:21.446383 20880 net.cpp:84] Creating Layer relu2
I1122 10:30:21.446383 20880 net.cpp:406] relu2 <- conv2
I1122 10:30:21.446383 20880 net.cpp:367] relu2 -> conv2 (in-place)
I1122 10:30:21.446383 20880 net.cpp:122] Setting up relu2
I1122 10:30:21.446383 20880 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 10:30:21.446383 20880 net.cpp:137] Memory required for data: 138855600
I1122 10:30:21.446383 20880 layer_factory.cpp:58] Creating layer conv2_2
I1122 10:30:21.446383 20880 net.cpp:84] Creating Layer conv2_2
I1122 10:30:21.446383 20880 net.cpp:406] conv2_2 <- conv2
I1122 10:30:21.446383 20880 net.cpp:380] conv2_2 -> conv2_2
I1122 10:30:21.448382 20880 net.cpp:122] Setting up conv2_2
I1122 10:30:21.448382 20880 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 10:30:21.448382 20880 net.cpp:137] Memory required for data: 167527600
I1122 10:30:21.448382 20880 layer_factory.cpp:58] Creating layer bn2_2
I1122 10:30:21.448382 20880 net.cpp:84] Creating Layer bn2_2
I1122 10:30:21.448382 20880 net.cpp:406] bn2_2 <- conv2_2
I1122 10:30:21.448382 20880 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1122 10:30:21.448382 20880 net.cpp:122] Setting up bn2_2
I1122 10:30:21.448382 20880 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 10:30:21.448382 20880 net.cpp:137] Memory required for data: 196199600
I1122 10:30:21.448382 20880 layer_factory.cpp:58] Creating layer scale2_2
I1122 10:30:21.448382 20880 net.cpp:84] Creating Layer scale2_2
I1122 10:30:21.448382 20880 net.cpp:406] scale2_2 <- conv2_2
I1122 10:30:21.448382 20880 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1122 10:30:21.448382 20880 layer_factory.cpp:58] Creating layer scale2_2
I1122 10:30:21.448382 20880 net.cpp:122] Setting up scale2_2
I1122 10:30:21.448382 20880 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 10:30:21.448382 20880 net.cpp:137] Memory required for data: 224871600
I1122 10:30:21.448382 20880 layer_factory.cpp:58] Creating layer relu2_2
I1122 10:30:21.448382 20880 net.cpp:84] Creating Layer relu2_2
I1122 10:30:21.448382 20880 net.cpp:406] relu2_2 <- conv2_2
I1122 10:30:21.448382 20880 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1122 10:30:21.448382 20880 net.cpp:122] Setting up relu2_2
I1122 10:30:21.448382 20880 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 10:30:21.448382 20880 net.cpp:137] Memory required for data: 253543600
I1122 10:30:21.448382 20880 layer_factory.cpp:58] Creating layer pool2_1
I1122 10:30:21.448382 20880 net.cpp:84] Creating Layer pool2_1
I1122 10:30:21.448382 20880 net.cpp:406] pool2_1 <- conv2_2
I1122 10:30:21.448382 20880 net.cpp:380] pool2_1 -> pool2_1
I1122 10:30:21.448382 20880 net.cpp:122] Setting up pool2_1
I1122 10:30:21.448382 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.448382 20880 net.cpp:137] Memory required for data: 260711600
I1122 10:30:21.448382 20880 layer_factory.cpp:58] Creating layer conv3
I1122 10:30:21.448382 20880 net.cpp:84] Creating Layer conv3
I1122 10:30:21.448382 20880 net.cpp:406] conv3 <- pool2_1
I1122 10:30:21.448382 20880 net.cpp:380] conv3 -> conv3
I1122 10:30:21.450392 20880 net.cpp:122] Setting up conv3
I1122 10:30:21.450392 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.450392 20880 net.cpp:137] Memory required for data: 267879600
I1122 10:30:21.450392 20880 layer_factory.cpp:58] Creating layer bn3
I1122 10:30:21.450392 20880 net.cpp:84] Creating Layer bn3
I1122 10:30:21.450392 20880 net.cpp:406] bn3 <- conv3
I1122 10:30:21.450392 20880 net.cpp:367] bn3 -> conv3 (in-place)
I1122 10:30:21.450392 20880 net.cpp:122] Setting up bn3
I1122 10:30:21.450392 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.450392 20880 net.cpp:137] Memory required for data: 275047600
I1122 10:30:21.450392 20880 layer_factory.cpp:58] Creating layer scale3
I1122 10:30:21.450392 20880 net.cpp:84] Creating Layer scale3
I1122 10:30:21.450392 20880 net.cpp:406] scale3 <- conv3
I1122 10:30:21.450392 20880 net.cpp:367] scale3 -> conv3 (in-place)
I1122 10:30:21.450392 20880 layer_factory.cpp:58] Creating layer scale3
I1122 10:30:21.450392 20880 net.cpp:122] Setting up scale3
I1122 10:30:21.450392 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.450392 20880 net.cpp:137] Memory required for data: 282215600
I1122 10:30:21.450392 20880 layer_factory.cpp:58] Creating layer relu3
I1122 10:30:21.450392 20880 net.cpp:84] Creating Layer relu3
I1122 10:30:21.450392 20880 net.cpp:406] relu3 <- conv3
I1122 10:30:21.450392 20880 net.cpp:367] relu3 -> conv3 (in-place)
I1122 10:30:21.451385 20880 net.cpp:122] Setting up relu3
I1122 10:30:21.451385 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.451385 20880 net.cpp:137] Memory required for data: 289383600
I1122 10:30:21.451385 20880 layer_factory.cpp:58] Creating layer conv4
I1122 10:30:21.451385 20880 net.cpp:84] Creating Layer conv4
I1122 10:30:21.451385 20880 net.cpp:406] conv4 <- conv3
I1122 10:30:21.451385 20880 net.cpp:380] conv4 -> conv4
I1122 10:30:21.452383 20880 net.cpp:122] Setting up conv4
I1122 10:30:21.452383 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.452383 20880 net.cpp:137] Memory required for data: 296551600
I1122 10:30:21.452383 20880 layer_factory.cpp:58] Creating layer bn4
I1122 10:30:21.452383 20880 net.cpp:84] Creating Layer bn4
I1122 10:30:21.452383 20880 net.cpp:406] bn4 <- conv4
I1122 10:30:21.452383 20880 net.cpp:367] bn4 -> conv4 (in-place)
I1122 10:30:21.452383 20880 net.cpp:122] Setting up bn4
I1122 10:30:21.452383 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.452383 20880 net.cpp:137] Memory required for data: 303719600
I1122 10:30:21.452383 20880 layer_factory.cpp:58] Creating layer scale4
I1122 10:30:21.452383 20880 net.cpp:84] Creating Layer scale4
I1122 10:30:21.452383 20880 net.cpp:406] scale4 <- conv4
I1122 10:30:21.452383 20880 net.cpp:367] scale4 -> conv4 (in-place)
I1122 10:30:21.452383 20880 layer_factory.cpp:58] Creating layer scale4
I1122 10:30:21.452383 20880 net.cpp:122] Setting up scale4
I1122 10:30:21.452383 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.452383 20880 net.cpp:137] Memory required for data: 310887600
I1122 10:30:21.452383 20880 layer_factory.cpp:58] Creating layer relu4
I1122 10:30:21.453383 20880 net.cpp:84] Creating Layer relu4
I1122 10:30:21.453383 20880 net.cpp:406] relu4 <- conv4
I1122 10:30:21.453383 20880 net.cpp:367] relu4 -> conv4 (in-place)
I1122 10:30:21.453383 20880 net.cpp:122] Setting up relu4
I1122 10:30:21.453383 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.453383 20880 net.cpp:137] Memory required for data: 318055600
I1122 10:30:21.453383 20880 layer_factory.cpp:58] Creating layer conv4_1
I1122 10:30:21.453383 20880 net.cpp:84] Creating Layer conv4_1
I1122 10:30:21.453383 20880 net.cpp:406] conv4_1 <- conv4
I1122 10:30:21.453383 20880 net.cpp:380] conv4_1 -> conv4_1
I1122 10:30:21.454383 20880 net.cpp:122] Setting up conv4_1
I1122 10:30:21.454383 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.454383 20880 net.cpp:137] Memory required for data: 325223600
I1122 10:30:21.454383 20880 layer_factory.cpp:58] Creating layer bn4_1
I1122 10:30:21.454383 20880 net.cpp:84] Creating Layer bn4_1
I1122 10:30:21.454383 20880 net.cpp:406] bn4_1 <- conv4_1
I1122 10:30:21.454383 20880 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1122 10:30:21.454383 20880 net.cpp:122] Setting up bn4_1
I1122 10:30:21.454383 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.454383 20880 net.cpp:137] Memory required for data: 332391600
I1122 10:30:21.454383 20880 layer_factory.cpp:58] Creating layer scale4_1
I1122 10:30:21.454383 20880 net.cpp:84] Creating Layer scale4_1
I1122 10:30:21.454383 20880 net.cpp:406] scale4_1 <- conv4_1
I1122 10:30:21.454383 20880 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1122 10:30:21.455384 20880 layer_factory.cpp:58] Creating layer scale4_1
I1122 10:30:21.455384 20880 net.cpp:122] Setting up scale4_1
I1122 10:30:21.455384 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.455384 20880 net.cpp:137] Memory required for data: 339559600
I1122 10:30:21.455384 20880 layer_factory.cpp:58] Creating layer relu4_1
I1122 10:30:21.455384 20880 net.cpp:84] Creating Layer relu4_1
I1122 10:30:21.455384 20880 net.cpp:406] relu4_1 <- conv4_1
I1122 10:30:21.455384 20880 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1122 10:30:21.455384 20880 net.cpp:122] Setting up relu4_1
I1122 10:30:21.455384 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.455384 20880 net.cpp:137] Memory required for data: 346727600
I1122 10:30:21.455384 20880 layer_factory.cpp:58] Creating layer conv4_2
I1122 10:30:21.455384 20880 net.cpp:84] Creating Layer conv4_2
I1122 10:30:21.455384 20880 net.cpp:406] conv4_2 <- conv4_1
I1122 10:30:21.455384 20880 net.cpp:380] conv4_2 -> conv4_2
I1122 10:30:21.457382 20880 net.cpp:122] Setting up conv4_2
I1122 10:30:21.457382 20880 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 10:30:21.457382 20880 net.cpp:137] Memory required for data: 355431600
I1122 10:30:21.457382 20880 layer_factory.cpp:58] Creating layer bn4_2
I1122 10:30:21.457382 20880 net.cpp:84] Creating Layer bn4_2
I1122 10:30:21.457382 20880 net.cpp:406] bn4_2 <- conv4_2
I1122 10:30:21.457382 20880 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1122 10:30:21.457382 20880 net.cpp:122] Setting up bn4_2
I1122 10:30:21.457382 20880 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 10:30:21.457382 20880 net.cpp:137] Memory required for data: 364135600
I1122 10:30:21.457382 20880 layer_factory.cpp:58] Creating layer scale4_2
I1122 10:30:21.457382 20880 net.cpp:84] Creating Layer scale4_2
I1122 10:30:21.457382 20880 net.cpp:406] scale4_2 <- conv4_2
I1122 10:30:21.457382 20880 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1122 10:30:21.457382 20880 layer_factory.cpp:58] Creating layer scale4_2
I1122 10:30:21.457382 20880 net.cpp:122] Setting up scale4_2
I1122 10:30:21.457382 20880 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 10:30:21.457382 20880 net.cpp:137] Memory required for data: 372839600
I1122 10:30:21.457382 20880 layer_factory.cpp:58] Creating layer relu4_2
I1122 10:30:21.457382 20880 net.cpp:84] Creating Layer relu4_2
I1122 10:30:21.457382 20880 net.cpp:406] relu4_2 <- conv4_2
I1122 10:30:21.457382 20880 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1122 10:30:21.457382 20880 net.cpp:122] Setting up relu4_2
I1122 10:30:21.457382 20880 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 10:30:21.457382 20880 net.cpp:137] Memory required for data: 381543600
I1122 10:30:21.457382 20880 layer_factory.cpp:58] Creating layer pool4_2
I1122 10:30:21.457382 20880 net.cpp:84] Creating Layer pool4_2
I1122 10:30:21.457382 20880 net.cpp:406] pool4_2 <- conv4_2
I1122 10:30:21.457382 20880 net.cpp:380] pool4_2 -> pool4_2
I1122 10:30:21.457382 20880 net.cpp:122] Setting up pool4_2
I1122 10:30:21.457382 20880 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 10:30:21.457382 20880 net.cpp:137] Memory required for data: 383719600
I1122 10:30:21.457382 20880 layer_factory.cpp:58] Creating layer conv12
I1122 10:30:21.457382 20880 net.cpp:84] Creating Layer conv12
I1122 10:30:21.457382 20880 net.cpp:406] conv12 <- pool4_2
I1122 10:30:21.457382 20880 net.cpp:380] conv12 -> conv12
I1122 10:30:21.459384 20880 net.cpp:122] Setting up conv12
I1122 10:30:21.459384 20880 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 10:30:21.459384 20880 net.cpp:137] Memory required for data: 386023600
I1122 10:30:21.459384 20880 layer_factory.cpp:58] Creating layer bn_conv12
I1122 10:30:21.459384 20880 net.cpp:84] Creating Layer bn_conv12
I1122 10:30:21.459384 20880 net.cpp:406] bn_conv12 <- conv12
I1122 10:30:21.459384 20880 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1122 10:30:21.459384 20880 net.cpp:122] Setting up bn_conv12
I1122 10:30:21.459384 20880 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 10:30:21.459384 20880 net.cpp:137] Memory required for data: 388327600
I1122 10:30:21.459384 20880 layer_factory.cpp:58] Creating layer scale_conv12
I1122 10:30:21.459384 20880 net.cpp:84] Creating Layer scale_conv12
I1122 10:30:21.459384 20880 net.cpp:406] scale_conv12 <- conv12
I1122 10:30:21.459384 20880 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1122 10:30:21.459384 20880 layer_factory.cpp:58] Creating layer scale_conv12
I1122 10:30:21.459384 20880 net.cpp:122] Setting up scale_conv12
I1122 10:30:21.459384 20880 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 10:30:21.459384 20880 net.cpp:137] Memory required for data: 390631600
I1122 10:30:21.459384 20880 layer_factory.cpp:58] Creating layer relu_conv12
I1122 10:30:21.459384 20880 net.cpp:84] Creating Layer relu_conv12
I1122 10:30:21.459384 20880 net.cpp:406] relu_conv12 <- conv12
I1122 10:30:21.459384 20880 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1122 10:30:21.460383 20880 net.cpp:122] Setting up relu_conv12
I1122 10:30:21.460383 20880 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 10:30:21.460383 20880 net.cpp:137] Memory required for data: 392935600
I1122 10:30:21.460383 20880 layer_factory.cpp:58] Creating layer poolcp6
I1122 10:30:21.460383 20880 net.cpp:84] Creating Layer poolcp6
I1122 10:30:21.460383 20880 net.cpp:406] poolcp6 <- conv12
I1122 10:30:21.460383 20880 net.cpp:380] poolcp6 -> poolcp6
I1122 10:30:21.460383 20880 net.cpp:122] Setting up poolcp6
I1122 10:30:21.460383 20880 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1122 10:30:21.460383 20880 net.cpp:137] Memory required for data: 392971600
I1122 10:30:21.460383 20880 layer_factory.cpp:58] Creating layer ip1
I1122 10:30:21.460383 20880 net.cpp:84] Creating Layer ip1
I1122 10:30:21.460383 20880 net.cpp:406] ip1 <- poolcp6
I1122 10:30:21.460383 20880 net.cpp:380] ip1 -> ip1
I1122 10:30:21.460383 20880 net.cpp:122] Setting up ip1
I1122 10:30:21.460383 20880 net.cpp:129] Top shape: 100 10 (1000)
I1122 10:30:21.460383 20880 net.cpp:137] Memory required for data: 392975600
I1122 10:30:21.460383 20880 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1122 10:30:21.460383 20880 net.cpp:84] Creating Layer ip1_ip1_0_split
I1122 10:30:21.460383 20880 net.cpp:406] ip1_ip1_0_split <- ip1
I1122 10:30:21.460383 20880 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1122 10:30:21.460383 20880 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1122 10:30:21.460383 20880 net.cpp:122] Setting up ip1_ip1_0_split
I1122 10:30:21.460383 20880 net.cpp:129] Top shape: 100 10 (1000)
I1122 10:30:21.460383 20880 net.cpp:129] Top shape: 100 10 (1000)
I1122 10:30:21.460383 20880 net.cpp:137] Memory required for data: 392983600
I1122 10:30:21.460383 20880 layer_factory.cpp:58] Creating layer accuracy_training
I1122 10:30:21.460383 20880 net.cpp:84] Creating Layer accuracy_training
I1122 10:30:21.460383 20880 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1122 10:30:21.460383 20880 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1122 10:30:21.460383 20880 net.cpp:380] accuracy_training -> accuracy_training
I1122 10:30:21.460383 20880 net.cpp:122] Setting up accuracy_training
I1122 10:30:21.460383 20880 net.cpp:129] Top shape: (1)
I1122 10:30:21.460383 20880 net.cpp:137] Memory required for data: 392983604
I1122 10:30:21.460383 20880 layer_factory.cpp:58] Creating layer loss
I1122 10:30:21.460383 20880 net.cpp:84] Creating Layer loss
I1122 10:30:21.460383 20880 net.cpp:406] loss <- ip1_ip1_0_split_1
I1122 10:30:21.460383 20880 net.cpp:406] loss <- label_cifar_1_split_1
I1122 10:30:21.460383 20880 net.cpp:380] loss -> loss
I1122 10:30:21.460383 20880 layer_factory.cpp:58] Creating layer loss
I1122 10:30:21.460383 20880 net.cpp:122] Setting up loss
I1122 10:30:21.460383 20880 net.cpp:129] Top shape: (1)
I1122 10:30:21.460383 20880 net.cpp:132]     with loss weight 1
I1122 10:30:21.460383 20880 net.cpp:137] Memory required for data: 392983608
I1122 10:30:21.460383 20880 net.cpp:198] loss needs backward computation.
I1122 10:30:21.460383 20880 net.cpp:200] accuracy_training does not need backward computation.
I1122 10:30:21.460383 20880 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1122 10:30:21.460383 20880 net.cpp:198] ip1 needs backward computation.
I1122 10:30:21.460383 20880 net.cpp:198] poolcp6 needs backward computation.
I1122 10:30:21.460383 20880 net.cpp:198] relu_conv12 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] scale_conv12 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] bn_conv12 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] conv12 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] pool4_2 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] relu4_2 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] scale4_2 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] bn4_2 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] conv4_2 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] relu4_1 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] scale4_1 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] bn4_1 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] conv4_1 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] relu4 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] scale4 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] bn4 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] conv4 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] relu3 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] scale3 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] bn3 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] conv3 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] pool2_1 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] relu2_2 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] scale2_2 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] bn2_2 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] conv2_2 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] relu2 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] scale2 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] bn2 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] conv2 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] relu1 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] scale1 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] bn1 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:198] conv1 needs backward computation.
I1122 10:30:21.461383 20880 net.cpp:200] label_cifar_1_split does not need backward computation.
I1122 10:30:21.461383 20880 net.cpp:200] cifar does not need backward computation.
I1122 10:30:21.461383 20880 net.cpp:242] This network produces output accuracy_training
I1122 10:30:21.461383 20880 net.cpp:242] This network produces output loss
I1122 10:30:21.461383 20880 net.cpp:255] Network initialization done.
I1122 10:30:21.461383 20880 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 10:30:21.461383 20880 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1122 10:30:21.461383 20880 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 10:30:21.461383 20880 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1122 10:30:21.461383 20880 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1122 10:30:21.461383 20880 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1122 10:30:21.461383 20880 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1122 10:30:21.461383 20880 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1122 10:30:21.461383 20880 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1122 10:30:21.461383 20880 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1122 10:30:21.461383 20880 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1122 10:30:21.461383 20880 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1122 10:30:21.462383 20880 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1122 10:30:21.462383 20880 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 41
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 43
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 85
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1122 10:30:21.462383 20880 layer_factory.cpp:58] Creating layer cifar
I1122 10:30:21.468387 20880 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1122 10:30:21.468387 20880 net.cpp:84] Creating Layer cifar
I1122 10:30:21.468387 20880 net.cpp:380] cifar -> data
I1122 10:30:21.468387 20880 net.cpp:380] cifar -> label
I1122 10:30:21.468387 20880 data_layer.cpp:45] output data size: 100,3,32,32
I1122 10:30:21.474385 20880 net.cpp:122] Setting up cifar
I1122 10:30:21.474385 20880 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1122 10:30:21.474385 20880 net.cpp:129] Top shape: 100 (100)
I1122 10:30:21.474385 20880 net.cpp:137] Memory required for data: 1229200
I1122 10:30:21.474385 20880 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1122 10:30:21.474385 20880 net.cpp:84] Creating Layer label_cifar_1_split
I1122 10:30:21.474385 20880 net.cpp:406] label_cifar_1_split <- label
I1122 10:30:21.474385 20880 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1122 10:30:21.474385 20880 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1122 10:30:21.474385 20880 net.cpp:122] Setting up label_cifar_1_split
I1122 10:30:21.474385 20880 net.cpp:129] Top shape: 100 (100)
I1122 10:30:21.474385 20880 net.cpp:129] Top shape: 100 (100)
I1122 10:30:21.474385 20880 net.cpp:137] Memory required for data: 1230000
I1122 10:30:21.474385 20880 layer_factory.cpp:58] Creating layer conv1
I1122 10:30:21.474385 20880 net.cpp:84] Creating Layer conv1
I1122 10:30:21.474385 20880 net.cpp:406] conv1 <- data
I1122 10:30:21.474385 20880 net.cpp:380] conv1 -> conv1
I1122 10:30:21.475384 16524 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 10:30:21.476384 20880 net.cpp:122] Setting up conv1
I1122 10:30:21.476384 20880 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 10:30:21.476384 20880 net.cpp:137] Memory required for data: 18023600
I1122 10:30:21.476384 20880 layer_factory.cpp:58] Creating layer bn1
I1122 10:30:21.476384 20880 net.cpp:84] Creating Layer bn1
I1122 10:30:21.476384 20880 net.cpp:406] bn1 <- conv1
I1122 10:30:21.476384 20880 net.cpp:367] bn1 -> conv1 (in-place)
I1122 10:30:21.476384 20880 net.cpp:122] Setting up bn1
I1122 10:30:21.476384 20880 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 10:30:21.476384 20880 net.cpp:137] Memory required for data: 34817200
I1122 10:30:21.476384 20880 layer_factory.cpp:58] Creating layer scale1
I1122 10:30:21.476384 20880 net.cpp:84] Creating Layer scale1
I1122 10:30:21.476384 20880 net.cpp:406] scale1 <- conv1
I1122 10:30:21.476384 20880 net.cpp:367] scale1 -> conv1 (in-place)
I1122 10:30:21.476384 20880 layer_factory.cpp:58] Creating layer scale1
I1122 10:30:21.476384 20880 net.cpp:122] Setting up scale1
I1122 10:30:21.476384 20880 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 10:30:21.476384 20880 net.cpp:137] Memory required for data: 51610800
I1122 10:30:21.476384 20880 layer_factory.cpp:58] Creating layer relu1
I1122 10:30:21.476384 20880 net.cpp:84] Creating Layer relu1
I1122 10:30:21.476384 20880 net.cpp:406] relu1 <- conv1
I1122 10:30:21.476384 20880 net.cpp:367] relu1 -> conv1 (in-place)
I1122 10:30:21.477385 20880 net.cpp:122] Setting up relu1
I1122 10:30:21.477385 20880 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 10:30:21.477385 20880 net.cpp:137] Memory required for data: 68404400
I1122 10:30:21.477385 20880 layer_factory.cpp:58] Creating layer conv2
I1122 10:30:21.477385 20880 net.cpp:84] Creating Layer conv2
I1122 10:30:21.477385 20880 net.cpp:406] conv2 <- conv1
I1122 10:30:21.477385 20880 net.cpp:380] conv2 -> conv2
I1122 10:30:21.478384 20880 net.cpp:122] Setting up conv2
I1122 10:30:21.478384 20880 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 10:30:21.478384 20880 net.cpp:137] Memory required for data: 86017200
I1122 10:30:21.478384 20880 layer_factory.cpp:58] Creating layer bn2
I1122 10:30:21.478384 20880 net.cpp:84] Creating Layer bn2
I1122 10:30:21.478384 20880 net.cpp:406] bn2 <- conv2
I1122 10:30:21.478384 20880 net.cpp:367] bn2 -> conv2 (in-place)
I1122 10:30:21.478384 20880 net.cpp:122] Setting up bn2
I1122 10:30:21.478384 20880 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 10:30:21.478384 20880 net.cpp:137] Memory required for data: 103630000
I1122 10:30:21.478384 20880 layer_factory.cpp:58] Creating layer scale2
I1122 10:30:21.478384 20880 net.cpp:84] Creating Layer scale2
I1122 10:30:21.478384 20880 net.cpp:406] scale2 <- conv2
I1122 10:30:21.478384 20880 net.cpp:367] scale2 -> conv2 (in-place)
I1122 10:30:21.478384 20880 layer_factory.cpp:58] Creating layer scale2
I1122 10:30:21.478384 20880 net.cpp:122] Setting up scale2
I1122 10:30:21.478384 20880 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 10:30:21.478384 20880 net.cpp:137] Memory required for data: 121242800
I1122 10:30:21.478384 20880 layer_factory.cpp:58] Creating layer relu2
I1122 10:30:21.478384 20880 net.cpp:84] Creating Layer relu2
I1122 10:30:21.478384 20880 net.cpp:406] relu2 <- conv2
I1122 10:30:21.478384 20880 net.cpp:367] relu2 -> conv2 (in-place)
I1122 10:30:21.479387 20880 net.cpp:122] Setting up relu2
I1122 10:30:21.479387 20880 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 10:30:21.479387 20880 net.cpp:137] Memory required for data: 138855600
I1122 10:30:21.479387 20880 layer_factory.cpp:58] Creating layer conv2_2
I1122 10:30:21.479387 20880 net.cpp:84] Creating Layer conv2_2
I1122 10:30:21.479387 20880 net.cpp:406] conv2_2 <- conv2
I1122 10:30:21.479387 20880 net.cpp:380] conv2_2 -> conv2_2
I1122 10:30:21.480388 20880 net.cpp:122] Setting up conv2_2
I1122 10:30:21.481385 20880 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 10:30:21.481385 20880 net.cpp:137] Memory required for data: 167527600
I1122 10:30:21.481385 20880 layer_factory.cpp:58] Creating layer bn2_2
I1122 10:30:21.481385 20880 net.cpp:84] Creating Layer bn2_2
I1122 10:30:21.481385 20880 net.cpp:406] bn2_2 <- conv2_2
I1122 10:30:21.481385 20880 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1122 10:30:21.481385 20880 net.cpp:122] Setting up bn2_2
I1122 10:30:21.481385 20880 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 10:30:21.481385 20880 net.cpp:137] Memory required for data: 196199600
I1122 10:30:21.481385 20880 layer_factory.cpp:58] Creating layer scale2_2
I1122 10:30:21.481385 20880 net.cpp:84] Creating Layer scale2_2
I1122 10:30:21.481385 20880 net.cpp:406] scale2_2 <- conv2_2
I1122 10:30:21.481385 20880 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1122 10:30:21.481385 20880 layer_factory.cpp:58] Creating layer scale2_2
I1122 10:30:21.481385 20880 net.cpp:122] Setting up scale2_2
I1122 10:30:21.481385 20880 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 10:30:21.481385 20880 net.cpp:137] Memory required for data: 224871600
I1122 10:30:21.481385 20880 layer_factory.cpp:58] Creating layer relu2_2
I1122 10:30:21.481385 20880 net.cpp:84] Creating Layer relu2_2
I1122 10:30:21.481385 20880 net.cpp:406] relu2_2 <- conv2_2
I1122 10:30:21.481385 20880 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1122 10:30:21.481385 20880 net.cpp:122] Setting up relu2_2
I1122 10:30:21.481385 20880 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 10:30:21.481385 20880 net.cpp:137] Memory required for data: 253543600
I1122 10:30:21.481385 20880 layer_factory.cpp:58] Creating layer pool2_1
I1122 10:30:21.482384 20880 net.cpp:84] Creating Layer pool2_1
I1122 10:30:21.482384 20880 net.cpp:406] pool2_1 <- conv2_2
I1122 10:30:21.482384 20880 net.cpp:380] pool2_1 -> pool2_1
I1122 10:30:21.482384 20880 net.cpp:122] Setting up pool2_1
I1122 10:30:21.482384 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.482384 20880 net.cpp:137] Memory required for data: 260711600
I1122 10:30:21.482384 20880 layer_factory.cpp:58] Creating layer conv3
I1122 10:30:21.482384 20880 net.cpp:84] Creating Layer conv3
I1122 10:30:21.482384 20880 net.cpp:406] conv3 <- pool2_1
I1122 10:30:21.482384 20880 net.cpp:380] conv3 -> conv3
I1122 10:30:21.484385 20880 net.cpp:122] Setting up conv3
I1122 10:30:21.484385 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.484385 20880 net.cpp:137] Memory required for data: 267879600
I1122 10:30:21.484385 20880 layer_factory.cpp:58] Creating layer bn3
I1122 10:30:21.484385 20880 net.cpp:84] Creating Layer bn3
I1122 10:30:21.484385 20880 net.cpp:406] bn3 <- conv3
I1122 10:30:21.484385 20880 net.cpp:367] bn3 -> conv3 (in-place)
I1122 10:30:21.484385 20880 net.cpp:122] Setting up bn3
I1122 10:30:21.484385 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.484385 20880 net.cpp:137] Memory required for data: 275047600
I1122 10:30:21.484385 20880 layer_factory.cpp:58] Creating layer scale3
I1122 10:30:21.484385 20880 net.cpp:84] Creating Layer scale3
I1122 10:30:21.484385 20880 net.cpp:406] scale3 <- conv3
I1122 10:30:21.484385 20880 net.cpp:367] scale3 -> conv3 (in-place)
I1122 10:30:21.484385 20880 layer_factory.cpp:58] Creating layer scale3
I1122 10:30:21.485384 20880 net.cpp:122] Setting up scale3
I1122 10:30:21.485384 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.485384 20880 net.cpp:137] Memory required for data: 282215600
I1122 10:30:21.485384 20880 layer_factory.cpp:58] Creating layer relu3
I1122 10:30:21.485384 20880 net.cpp:84] Creating Layer relu3
I1122 10:30:21.485384 20880 net.cpp:406] relu3 <- conv3
I1122 10:30:21.485384 20880 net.cpp:367] relu3 -> conv3 (in-place)
I1122 10:30:21.485384 20880 net.cpp:122] Setting up relu3
I1122 10:30:21.485384 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.485384 20880 net.cpp:137] Memory required for data: 289383600
I1122 10:30:21.485384 20880 layer_factory.cpp:58] Creating layer conv4
I1122 10:30:21.485384 20880 net.cpp:84] Creating Layer conv4
I1122 10:30:21.485384 20880 net.cpp:406] conv4 <- conv3
I1122 10:30:21.485384 20880 net.cpp:380] conv4 -> conv4
I1122 10:30:21.487385 20880 net.cpp:122] Setting up conv4
I1122 10:30:21.487385 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.487385 20880 net.cpp:137] Memory required for data: 296551600
I1122 10:30:21.487385 20880 layer_factory.cpp:58] Creating layer bn4
I1122 10:30:21.487385 20880 net.cpp:84] Creating Layer bn4
I1122 10:30:21.487385 20880 net.cpp:406] bn4 <- conv4
I1122 10:30:21.487385 20880 net.cpp:367] bn4 -> conv4 (in-place)
I1122 10:30:21.487385 20880 net.cpp:122] Setting up bn4
I1122 10:30:21.487385 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.487385 20880 net.cpp:137] Memory required for data: 303719600
I1122 10:30:21.487385 20880 layer_factory.cpp:58] Creating layer scale4
I1122 10:30:21.487385 20880 net.cpp:84] Creating Layer scale4
I1122 10:30:21.487385 20880 net.cpp:406] scale4 <- conv4
I1122 10:30:21.487385 20880 net.cpp:367] scale4 -> conv4 (in-place)
I1122 10:30:21.487385 20880 layer_factory.cpp:58] Creating layer scale4
I1122 10:30:21.487385 20880 net.cpp:122] Setting up scale4
I1122 10:30:21.487385 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.487385 20880 net.cpp:137] Memory required for data: 310887600
I1122 10:30:21.487385 20880 layer_factory.cpp:58] Creating layer relu4
I1122 10:30:21.487385 20880 net.cpp:84] Creating Layer relu4
I1122 10:30:21.487385 20880 net.cpp:406] relu4 <- conv4
I1122 10:30:21.487385 20880 net.cpp:367] relu4 -> conv4 (in-place)
I1122 10:30:21.487385 20880 net.cpp:122] Setting up relu4
I1122 10:30:21.487385 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.487385 20880 net.cpp:137] Memory required for data: 318055600
I1122 10:30:21.487385 20880 layer_factory.cpp:58] Creating layer conv4_1
I1122 10:30:21.487385 20880 net.cpp:84] Creating Layer conv4_1
I1122 10:30:21.487385 20880 net.cpp:406] conv4_1 <- conv4
I1122 10:30:21.487385 20880 net.cpp:380] conv4_1 -> conv4_1
I1122 10:30:21.489398 20880 net.cpp:122] Setting up conv4_1
I1122 10:30:21.489398 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.489398 20880 net.cpp:137] Memory required for data: 325223600
I1122 10:30:21.489398 20880 layer_factory.cpp:58] Creating layer bn4_1
I1122 10:30:21.489398 20880 net.cpp:84] Creating Layer bn4_1
I1122 10:30:21.489398 20880 net.cpp:406] bn4_1 <- conv4_1
I1122 10:30:21.489398 20880 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1122 10:30:21.489398 20880 net.cpp:122] Setting up bn4_1
I1122 10:30:21.489398 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.489398 20880 net.cpp:137] Memory required for data: 332391600
I1122 10:30:21.490388 20880 layer_factory.cpp:58] Creating layer scale4_1
I1122 10:30:21.490388 20880 net.cpp:84] Creating Layer scale4_1
I1122 10:30:21.490388 20880 net.cpp:406] scale4_1 <- conv4_1
I1122 10:30:21.490388 20880 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1122 10:30:21.490388 20880 layer_factory.cpp:58] Creating layer scale4_1
I1122 10:30:21.490388 20880 net.cpp:122] Setting up scale4_1
I1122 10:30:21.490388 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.490388 20880 net.cpp:137] Memory required for data: 339559600
I1122 10:30:21.490388 20880 layer_factory.cpp:58] Creating layer relu4_1
I1122 10:30:21.490388 20880 net.cpp:84] Creating Layer relu4_1
I1122 10:30:21.490388 20880 net.cpp:406] relu4_1 <- conv4_1
I1122 10:30:21.490388 20880 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1122 10:30:21.490388 20880 net.cpp:122] Setting up relu4_1
I1122 10:30:21.490388 20880 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 10:30:21.490388 20880 net.cpp:137] Memory required for data: 346727600
I1122 10:30:21.490388 20880 layer_factory.cpp:58] Creating layer conv4_2
I1122 10:30:21.490388 20880 net.cpp:84] Creating Layer conv4_2
I1122 10:30:21.490388 20880 net.cpp:406] conv4_2 <- conv4_1
I1122 10:30:21.490388 20880 net.cpp:380] conv4_2 -> conv4_2
I1122 10:30:21.492398 20880 net.cpp:122] Setting up conv4_2
I1122 10:30:21.492398 20880 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 10:30:21.492398 20880 net.cpp:137] Memory required for data: 355431600
I1122 10:30:21.492398 20880 layer_factory.cpp:58] Creating layer bn4_2
I1122 10:30:21.492398 20880 net.cpp:84] Creating Layer bn4_2
I1122 10:30:21.492398 20880 net.cpp:406] bn4_2 <- conv4_2
I1122 10:30:21.492398 20880 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1122 10:30:21.492398 20880 net.cpp:122] Setting up bn4_2
I1122 10:30:21.492398 20880 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 10:30:21.492398 20880 net.cpp:137] Memory required for data: 364135600
I1122 10:30:21.492398 20880 layer_factory.cpp:58] Creating layer scale4_2
I1122 10:30:21.492398 20880 net.cpp:84] Creating Layer scale4_2
I1122 10:30:21.492398 20880 net.cpp:406] scale4_2 <- conv4_2
I1122 10:30:21.492398 20880 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1122 10:30:21.492398 20880 layer_factory.cpp:58] Creating layer scale4_2
I1122 10:30:21.492398 20880 net.cpp:122] Setting up scale4_2
I1122 10:30:21.492398 20880 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 10:30:21.492398 20880 net.cpp:137] Memory required for data: 372839600
I1122 10:30:21.492398 20880 layer_factory.cpp:58] Creating layer relu4_2
I1122 10:30:21.492398 20880 net.cpp:84] Creating Layer relu4_2
I1122 10:30:21.492398 20880 net.cpp:406] relu4_2 <- conv4_2
I1122 10:30:21.492398 20880 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1122 10:30:21.493401 20880 net.cpp:122] Setting up relu4_2
I1122 10:30:21.493401 20880 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 10:30:21.493401 20880 net.cpp:137] Memory required for data: 381543600
I1122 10:30:21.493401 20880 layer_factory.cpp:58] Creating layer pool4_2
I1122 10:30:21.493401 20880 net.cpp:84] Creating Layer pool4_2
I1122 10:30:21.493401 20880 net.cpp:406] pool4_2 <- conv4_2
I1122 10:30:21.493401 20880 net.cpp:380] pool4_2 -> pool4_2
I1122 10:30:21.493401 20880 net.cpp:122] Setting up pool4_2
I1122 10:30:21.493401 20880 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 10:30:21.493401 20880 net.cpp:137] Memory required for data: 383719600
I1122 10:30:21.493401 20880 layer_factory.cpp:58] Creating layer conv12
I1122 10:30:21.493401 20880 net.cpp:84] Creating Layer conv12
I1122 10:30:21.493401 20880 net.cpp:406] conv12 <- pool4_2
I1122 10:30:21.493401 20880 net.cpp:380] conv12 -> conv12
I1122 10:30:21.495398 20880 net.cpp:122] Setting up conv12
I1122 10:30:21.495398 20880 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 10:30:21.495398 20880 net.cpp:137] Memory required for data: 386023600
I1122 10:30:21.495398 20880 layer_factory.cpp:58] Creating layer bn_conv12
I1122 10:30:21.495398 20880 net.cpp:84] Creating Layer bn_conv12
I1122 10:30:21.495398 20880 net.cpp:406] bn_conv12 <- conv12
I1122 10:30:21.495398 20880 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1122 10:30:21.495398 20880 net.cpp:122] Setting up bn_conv12
I1122 10:30:21.495398 20880 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 10:30:21.495398 20880 net.cpp:137] Memory required for data: 388327600
I1122 10:30:21.495398 20880 layer_factory.cpp:58] Creating layer scale_conv12
I1122 10:30:21.495398 20880 net.cpp:84] Creating Layer scale_conv12
I1122 10:30:21.495398 20880 net.cpp:406] scale_conv12 <- conv12
I1122 10:30:21.495398 20880 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1122 10:30:21.495398 20880 layer_factory.cpp:58] Creating layer scale_conv12
I1122 10:30:21.495398 20880 net.cpp:122] Setting up scale_conv12
I1122 10:30:21.495398 20880 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 10:30:21.495398 20880 net.cpp:137] Memory required for data: 390631600
I1122 10:30:21.495398 20880 layer_factory.cpp:58] Creating layer relu_conv12
I1122 10:30:21.495398 20880 net.cpp:84] Creating Layer relu_conv12
I1122 10:30:21.495398 20880 net.cpp:406] relu_conv12 <- conv12
I1122 10:30:21.495398 20880 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1122 10:30:21.496387 20880 net.cpp:122] Setting up relu_conv12
I1122 10:30:21.496387 20880 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 10:30:21.496387 20880 net.cpp:137] Memory required for data: 392935600
I1122 10:30:21.496387 20880 layer_factory.cpp:58] Creating layer poolcp6
I1122 10:30:21.496387 20880 net.cpp:84] Creating Layer poolcp6
I1122 10:30:21.496387 20880 net.cpp:406] poolcp6 <- conv12
I1122 10:30:21.496387 20880 net.cpp:380] poolcp6 -> poolcp6
I1122 10:30:21.496387 20880 net.cpp:122] Setting up poolcp6
I1122 10:30:21.496387 20880 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1122 10:30:21.496387 20880 net.cpp:137] Memory required for data: 392971600
I1122 10:30:21.496387 20880 layer_factory.cpp:58] Creating layer ip1
I1122 10:30:21.496387 20880 net.cpp:84] Creating Layer ip1
I1122 10:30:21.496387 20880 net.cpp:406] ip1 <- poolcp6
I1122 10:30:21.496387 20880 net.cpp:380] ip1 -> ip1
I1122 10:30:21.496387 20880 net.cpp:122] Setting up ip1
I1122 10:30:21.496387 20880 net.cpp:129] Top shape: 100 10 (1000)
I1122 10:30:21.496387 20880 net.cpp:137] Memory required for data: 392975600
I1122 10:30:21.496387 20880 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1122 10:30:21.496387 20880 net.cpp:84] Creating Layer ip1_ip1_0_split
I1122 10:30:21.496387 20880 net.cpp:406] ip1_ip1_0_split <- ip1
I1122 10:30:21.496387 20880 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1122 10:30:21.496387 20880 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1122 10:30:21.496387 20880 net.cpp:122] Setting up ip1_ip1_0_split
I1122 10:30:21.496387 20880 net.cpp:129] Top shape: 100 10 (1000)
I1122 10:30:21.496387 20880 net.cpp:129] Top shape: 100 10 (1000)
I1122 10:30:21.496387 20880 net.cpp:137] Memory required for data: 392983600
I1122 10:30:21.496387 20880 layer_factory.cpp:58] Creating layer accuracy
I1122 10:30:21.496387 20880 net.cpp:84] Creating Layer accuracy
I1122 10:30:21.496387 20880 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1122 10:30:21.496387 20880 net.cpp:406] accuracy <- label_cifar_1_split_0
I1122 10:30:21.496387 20880 net.cpp:380] accuracy -> accuracy
I1122 10:30:21.496387 20880 net.cpp:122] Setting up accuracy
I1122 10:30:21.496387 20880 net.cpp:129] Top shape: (1)
I1122 10:30:21.496387 20880 net.cpp:137] Memory required for data: 392983604
I1122 10:30:21.496387 20880 layer_factory.cpp:58] Creating layer loss
I1122 10:30:21.496387 20880 net.cpp:84] Creating Layer loss
I1122 10:30:21.496387 20880 net.cpp:406] loss <- ip1_ip1_0_split_1
I1122 10:30:21.496387 20880 net.cpp:406] loss <- label_cifar_1_split_1
I1122 10:30:21.496387 20880 net.cpp:380] loss -> loss
I1122 10:30:21.496387 20880 layer_factory.cpp:58] Creating layer loss
I1122 10:30:21.497383 20880 net.cpp:122] Setting up loss
I1122 10:30:21.497383 20880 net.cpp:129] Top shape: (1)
I1122 10:30:21.497383 20880 net.cpp:132]     with loss weight 1
I1122 10:30:21.497383 20880 net.cpp:137] Memory required for data: 392983608
I1122 10:30:21.497383 20880 net.cpp:198] loss needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:200] accuracy does not need backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] ip1 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] poolcp6 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] relu_conv12 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] scale_conv12 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] bn_conv12 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] conv12 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] pool4_2 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] relu4_2 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] scale4_2 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] bn4_2 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] conv4_2 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] relu4_1 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] scale4_1 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] bn4_1 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] conv4_1 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] relu4 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] scale4 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] bn4 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] conv4 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] relu3 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] scale3 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] bn3 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] conv3 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] pool2_1 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] relu2_2 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] scale2_2 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] bn2_2 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] conv2_2 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] relu2 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] scale2 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] bn2 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] conv2 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] relu1 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] scale1 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] bn1 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:198] conv1 needs backward computation.
I1122 10:30:21.497383 20880 net.cpp:200] label_cifar_1_split does not need backward computation.
I1122 10:30:21.497383 20880 net.cpp:200] cifar does not need backward computation.
I1122 10:30:21.497383 20880 net.cpp:242] This network produces output accuracy
I1122 10:30:21.497383 20880 net.cpp:242] This network produces output loss
I1122 10:30:21.497383 20880 net.cpp:255] Network initialization done.
I1122 10:30:21.497383 20880 solver.cpp:56] Solver scaffolding done.
I1122 10:30:21.500394 20880 caffe.cpp:249] Starting Optimization
I1122 10:30:21.500394 20880 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k
I1122 10:30:21.500394 20880 solver.cpp:273] Learning Rate Policy: multistep
I1122 10:30:21.502394 20880 solver.cpp:330] Iteration 0, Testing net (#0)
I1122 10:30:21.503394 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:30:22.607774 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:30:22.650319 20880 solver.cpp:397]     Test net output #0: accuracy = 0.0889
I1122 10:30:22.650319 20880 solver.cpp:397]     Test net output #1: loss = 79.5723 (* 1 = 79.5723 loss)
I1122 10:30:22.725353 20880 solver.cpp:218] Iteration 0 (0 iter/s, 1.2239s/100 iters), loss = 3.655
I1122 10:30:22.725353 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.12
I1122 10:30:22.725353 20880 solver.cpp:237]     Train net output #1: loss = 3.655 (* 1 = 3.655 loss)
I1122 10:30:22.725353 20880 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1122 10:30:26.983510 20880 solver.cpp:218] Iteration 100 (23.4858 iter/s, 4.25788s/100 iters), loss = 1.60775
I1122 10:30:26.983510 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.39
I1122 10:30:26.983510 20880 solver.cpp:237]     Train net output #1: loss = 1.60775 (* 1 = 1.60775 loss)
I1122 10:30:26.983510 20880 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1122 10:30:31.244835 20880 solver.cpp:218] Iteration 200 (23.4677 iter/s, 4.26117s/100 iters), loss = 1.56634
I1122 10:30:31.244835 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.43
I1122 10:30:31.244835 20880 solver.cpp:237]     Train net output #1: loss = 1.56634 (* 1 = 1.56634 loss)
I1122 10:30:31.244835 20880 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1122 10:30:35.507148 20880 solver.cpp:218] Iteration 300 (23.4638 iter/s, 4.26189s/100 iters), loss = 1.31158
I1122 10:30:35.507148 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.54
I1122 10:30:35.507148 20880 solver.cpp:237]     Train net output #1: loss = 1.31158 (* 1 = 1.31158 loss)
I1122 10:30:35.507148 20880 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1122 10:30:39.760812 20880 solver.cpp:218] Iteration 400 (23.5102 iter/s, 4.25347s/100 iters), loss = 1.25184
I1122 10:30:39.760812 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.53
I1122 10:30:39.760812 20880 solver.cpp:237]     Train net output #1: loss = 1.25184 (* 1 = 1.25184 loss)
I1122 10:30:39.760812 20880 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1122 10:30:43.809288 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:30:43.975833 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_500.caffemodel
I1122 10:30:43.990830 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_500.solverstate
I1122 10:30:43.994832 20880 solver.cpp:330] Iteration 500, Testing net (#0)
I1122 10:30:43.994832 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:30:45.051467 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:30:45.092454 20880 solver.cpp:397]     Test net output #0: accuracy = 0.4366
I1122 10:30:45.092454 20880 solver.cpp:397]     Test net output #1: loss = 1.61687 (* 1 = 1.61687 loss)
I1122 10:30:45.133985 20880 solver.cpp:218] Iteration 500 (18.6148 iter/s, 5.37207s/100 iters), loss = 1.17308
I1122 10:30:45.133985 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.56
I1122 10:30:45.133985 20880 solver.cpp:237]     Train net output #1: loss = 1.17308 (* 1 = 1.17308 loss)
I1122 10:30:45.133985 20880 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1122 10:30:49.396782 20880 solver.cpp:218] Iteration 600 (23.4578 iter/s, 4.26297s/100 iters), loss = 1.00964
I1122 10:30:49.396782 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1122 10:30:49.396782 20880 solver.cpp:237]     Train net output #1: loss = 1.00964 (* 1 = 1.00964 loss)
I1122 10:30:49.396782 20880 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1122 10:30:53.653599 20880 solver.cpp:218] Iteration 700 (23.4954 iter/s, 4.25616s/100 iters), loss = 0.993209
I1122 10:30:53.653599 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1122 10:30:53.653599 20880 solver.cpp:237]     Train net output #1: loss = 0.993209 (* 1 = 0.993209 loss)
I1122 10:30:53.653599 20880 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1122 10:30:57.915289 20880 solver.cpp:218] Iteration 800 (23.4667 iter/s, 4.26135s/100 iters), loss = 0.852634
I1122 10:30:57.915289 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1122 10:30:57.915289 20880 solver.cpp:237]     Train net output #1: loss = 0.852634 (* 1 = 0.852634 loss)
I1122 10:30:57.915289 20880 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1122 10:31:02.173527 20880 solver.cpp:218] Iteration 900 (23.4843 iter/s, 4.25816s/100 iters), loss = 0.889197
I1122 10:31:02.173527 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1122 10:31:02.173527 20880 solver.cpp:237]     Train net output #1: loss = 0.889197 (* 1 = 0.889197 loss)
I1122 10:31:02.173527 20880 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1122 10:31:06.222574 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:31:06.390213 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_1000.caffemodel
I1122 10:31:06.400212 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_1000.solverstate
I1122 10:31:06.405210 20880 solver.cpp:330] Iteration 1000, Testing net (#0)
I1122 10:31:06.405210 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:31:07.459574 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:31:07.500574 20880 solver.cpp:397]     Test net output #0: accuracy = 0.6055
I1122 10:31:07.500574 20880 solver.cpp:397]     Test net output #1: loss = 1.1138 (* 1 = 1.1138 loss)
I1122 10:31:07.541584 20880 solver.cpp:218] Iteration 1000 (18.6309 iter/s, 5.36741s/100 iters), loss = 0.864965
I1122 10:31:07.541584 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1122 10:31:07.542094 20880 solver.cpp:237]     Train net output #1: loss = 0.864965 (* 1 = 0.864965 loss)
I1122 10:31:07.542094 20880 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1122 10:31:11.801338 20880 solver.cpp:218] Iteration 1100 (23.4788 iter/s, 4.25915s/100 iters), loss = 0.744942
I1122 10:31:11.801338 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1122 10:31:11.801338 20880 solver.cpp:237]     Train net output #1: loss = 0.744942 (* 1 = 0.744942 loss)
I1122 10:31:11.801338 20880 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1122 10:31:16.060905 20880 solver.cpp:218] Iteration 1200 (23.4799 iter/s, 4.25897s/100 iters), loss = 0.729107
I1122 10:31:16.060905 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1122 10:31:16.060905 20880 solver.cpp:237]     Train net output #1: loss = 0.729107 (* 1 = 0.729107 loss)
I1122 10:31:16.060905 20880 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1122 10:31:20.329603 20880 solver.cpp:218] Iteration 1300 (23.4275 iter/s, 4.26849s/100 iters), loss = 0.719037
I1122 10:31:20.329603 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1122 10:31:20.329603 20880 solver.cpp:237]     Train net output #1: loss = 0.719037 (* 1 = 0.719037 loss)
I1122 10:31:20.329603 20880 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1122 10:31:24.605451 20880 solver.cpp:218] Iteration 1400 (23.388 iter/s, 4.2757s/100 iters), loss = 0.722889
I1122 10:31:24.605451 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1122 10:31:24.605451 20880 solver.cpp:237]     Train net output #1: loss = 0.722889 (* 1 = 0.722889 loss)
I1122 10:31:24.605451 20880 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1122 10:31:28.665185 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:31:28.832701 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_1500.caffemodel
I1122 10:31:28.842698 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_1500.solverstate
I1122 10:31:28.847699 20880 solver.cpp:330] Iteration 1500, Testing net (#0)
I1122 10:31:28.847699 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:31:29.906286 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:31:29.947271 20880 solver.cpp:397]     Test net output #0: accuracy = 0.5667
I1122 10:31:29.947271 20880 solver.cpp:397]     Test net output #1: loss = 1.22533 (* 1 = 1.22533 loss)
I1122 10:31:29.988306 20880 solver.cpp:218] Iteration 1500 (18.5774 iter/s, 5.38289s/100 iters), loss = 0.732652
I1122 10:31:29.988306 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1122 10:31:29.988306 20880 solver.cpp:237]     Train net output #1: loss = 0.732652 (* 1 = 0.732652 loss)
I1122 10:31:29.988306 20880 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1122 10:31:34.249208 20880 solver.cpp:218] Iteration 1600 (23.4769 iter/s, 4.25951s/100 iters), loss = 0.670572
I1122 10:31:34.249208 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 10:31:34.249208 20880 solver.cpp:237]     Train net output #1: loss = 0.670572 (* 1 = 0.670572 loss)
I1122 10:31:34.249208 20880 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1122 10:31:38.505252 20880 solver.cpp:218] Iteration 1700 (23.4971 iter/s, 4.25584s/100 iters), loss = 0.58707
I1122 10:31:38.505252 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 10:31:38.505252 20880 solver.cpp:237]     Train net output #1: loss = 0.58707 (* 1 = 0.58707 loss)
I1122 10:31:38.505252 20880 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1122 10:31:42.766924 20880 solver.cpp:218] Iteration 1800 (23.4671 iter/s, 4.26129s/100 iters), loss = 0.624663
I1122 10:31:42.766924 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1122 10:31:42.766924 20880 solver.cpp:237]     Train net output #1: loss = 0.624663 (* 1 = 0.624663 loss)
I1122 10:31:42.766924 20880 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1122 10:31:47.026813 20880 solver.cpp:218] Iteration 1900 (23.4728 iter/s, 4.26025s/100 iters), loss = 0.567192
I1122 10:31:47.026813 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 10:31:47.026813 20880 solver.cpp:237]     Train net output #1: loss = 0.567192 (* 1 = 0.567192 loss)
I1122 10:31:47.026813 20880 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1122 10:31:51.079738 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:31:51.247099 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_2000.caffemodel
I1122 10:31:51.257098 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_2000.solverstate
I1122 10:31:51.261098 20880 solver.cpp:330] Iteration 2000, Testing net (#0)
I1122 10:31:51.261098 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:31:52.319146 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:31:52.361135 20880 solver.cpp:397]     Test net output #0: accuracy = 0.6466
I1122 10:31:52.361135 20880 solver.cpp:397]     Test net output #1: loss = 1.02338 (* 1 = 1.02338 loss)
I1122 10:31:52.402696 20880 solver.cpp:218] Iteration 2000 (18.6055 iter/s, 5.37474s/100 iters), loss = 0.544667
I1122 10:31:52.402696 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 10:31:52.402696 20880 solver.cpp:237]     Train net output #1: loss = 0.544667 (* 1 = 0.544667 loss)
I1122 10:31:52.402696 20880 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1122 10:31:56.673418 20880 solver.cpp:218] Iteration 2100 (23.418 iter/s, 4.27021s/100 iters), loss = 0.574629
I1122 10:31:56.673418 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 10:31:56.673418 20880 solver.cpp:237]     Train net output #1: loss = 0.574629 (* 1 = 0.574629 loss)
I1122 10:31:56.673418 20880 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1122 10:32:00.939110 20880 solver.cpp:218] Iteration 2200 (23.4401 iter/s, 4.26619s/100 iters), loss = 0.552325
I1122 10:32:00.940105 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 10:32:00.940105 20880 solver.cpp:237]     Train net output #1: loss = 0.552325 (* 1 = 0.552325 loss)
I1122 10:32:00.940105 20880 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1122 10:32:05.210149 20880 solver.cpp:218] Iteration 2300 (23.4167 iter/s, 4.27046s/100 iters), loss = 0.733013
I1122 10:32:05.210149 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1122 10:32:05.210149 20880 solver.cpp:237]     Train net output #1: loss = 0.733013 (* 1 = 0.733013 loss)
I1122 10:32:05.210149 20880 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1122 10:32:09.475903 20880 solver.cpp:218] Iteration 2400 (23.4467 iter/s, 4.265s/100 iters), loss = 0.559179
I1122 10:32:09.475903 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 10:32:09.475903 20880 solver.cpp:237]     Train net output #1: loss = 0.559179 (* 1 = 0.559179 loss)
I1122 10:32:09.476398 20880 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1122 10:32:13.539932 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:32:13.707064 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_2500.caffemodel
I1122 10:32:13.717058 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_2500.solverstate
I1122 10:32:13.722059 20880 solver.cpp:330] Iteration 2500, Testing net (#0)
I1122 10:32:13.722059 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:32:14.782011 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:32:14.822576 20880 solver.cpp:397]     Test net output #0: accuracy = 0.6448
I1122 10:32:14.822576 20880 solver.cpp:397]     Test net output #1: loss = 1.02496 (* 1 = 1.02496 loss)
I1122 10:32:14.863576 20880 solver.cpp:218] Iteration 2500 (18.5603 iter/s, 5.38784s/100 iters), loss = 0.520011
I1122 10:32:14.864580 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 10:32:14.864580 20880 solver.cpp:237]     Train net output #1: loss = 0.520011 (* 1 = 0.520011 loss)
I1122 10:32:14.864580 20880 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1122 10:32:19.135627 20880 solver.cpp:218] Iteration 2600 (23.4122 iter/s, 4.27129s/100 iters), loss = 0.449965
I1122 10:32:19.135627 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1122 10:32:19.135627 20880 solver.cpp:237]     Train net output #1: loss = 0.449965 (* 1 = 0.449965 loss)
I1122 10:32:19.135627 20880 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1122 10:32:23.410549 20880 solver.cpp:218] Iteration 2700 (23.3936 iter/s, 4.27467s/100 iters), loss = 0.512462
I1122 10:32:23.410549 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 10:32:23.410549 20880 solver.cpp:237]     Train net output #1: loss = 0.512462 (* 1 = 0.512462 loss)
I1122 10:32:23.410549 20880 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1122 10:32:27.689311 20880 solver.cpp:218] Iteration 2800 (23.3753 iter/s, 4.27802s/100 iters), loss = 0.552968
I1122 10:32:27.689311 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1122 10:32:27.689311 20880 solver.cpp:237]     Train net output #1: loss = 0.552968 (* 1 = 0.552968 loss)
I1122 10:32:27.689311 20880 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1122 10:32:31.964610 20880 solver.cpp:218] Iteration 2900 (23.3924 iter/s, 4.27489s/100 iters), loss = 0.497606
I1122 10:32:31.964610 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 10:32:31.964610 20880 solver.cpp:237]     Train net output #1: loss = 0.497606 (* 1 = 0.497606 loss)
I1122 10:32:31.964610 20880 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1122 10:32:36.025007 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:32:36.193538 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_3000.caffemodel
I1122 10:32:36.203527 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_3000.solverstate
I1122 10:32:36.208029 20880 solver.cpp:330] Iteration 3000, Testing net (#0)
I1122 10:32:36.208029 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:32:37.268275 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:32:37.309291 20880 solver.cpp:397]     Test net output #0: accuracy = 0.6765
I1122 10:32:37.309291 20880 solver.cpp:397]     Test net output #1: loss = 0.911933 (* 1 = 0.911933 loss)
I1122 10:32:37.350306 20880 solver.cpp:218] Iteration 3000 (18.5681 iter/s, 5.38558s/100 iters), loss = 0.566567
I1122 10:32:37.350306 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 10:32:37.350306 20880 solver.cpp:237]     Train net output #1: loss = 0.566567 (* 1 = 0.566567 loss)
I1122 10:32:37.350306 20880 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1122 10:32:41.620849 20880 solver.cpp:218] Iteration 3100 (23.4207 iter/s, 4.26973s/100 iters), loss = 0.511909
I1122 10:32:41.620849 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 10:32:41.620849 20880 solver.cpp:237]     Train net output #1: loss = 0.511909 (* 1 = 0.511909 loss)
I1122 10:32:41.620849 20880 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1122 10:32:45.892896 20880 solver.cpp:218] Iteration 3200 (23.4096 iter/s, 4.27175s/100 iters), loss = 0.552312
I1122 10:32:45.892896 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 10:32:45.892896 20880 solver.cpp:237]     Train net output #1: loss = 0.552312 (* 1 = 0.552312 loss)
I1122 10:32:45.892896 20880 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1122 10:32:50.166673 20880 solver.cpp:218] Iteration 3300 (23.3997 iter/s, 4.27355s/100 iters), loss = 0.543636
I1122 10:32:50.166673 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 10:32:50.166673 20880 solver.cpp:237]     Train net output #1: loss = 0.543637 (* 1 = 0.543637 loss)
I1122 10:32:50.166673 20880 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1122 10:32:54.438256 20880 solver.cpp:218] Iteration 3400 (23.4109 iter/s, 4.27151s/100 iters), loss = 0.441617
I1122 10:32:54.438256 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 10:32:54.438256 20880 solver.cpp:237]     Train net output #1: loss = 0.441617 (* 1 = 0.441617 loss)
I1122 10:32:54.438256 20880 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1122 10:32:58.501551 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:32:58.669322 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_3500.caffemodel
I1122 10:32:58.678319 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_3500.solverstate
I1122 10:32:58.683320 20880 solver.cpp:330] Iteration 3500, Testing net (#0)
I1122 10:32:58.683320 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:32:59.741787 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:32:59.782786 20880 solver.cpp:397]     Test net output #0: accuracy = 0.6336
I1122 10:32:59.783774 20880 solver.cpp:397]     Test net output #1: loss = 1.06006 (* 1 = 1.06006 loss)
I1122 10:32:59.823807 20880 solver.cpp:218] Iteration 3500 (18.5685 iter/s, 5.38548s/100 iters), loss = 0.516446
I1122 10:32:59.823807 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 10:32:59.823807 20880 solver.cpp:237]     Train net output #1: loss = 0.516447 (* 1 = 0.516447 loss)
I1122 10:32:59.823807 20880 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1122 10:33:04.087270 20880 solver.cpp:218] Iteration 3600 (23.4621 iter/s, 4.2622s/100 iters), loss = 0.509417
I1122 10:33:04.087270 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 10:33:04.087270 20880 solver.cpp:237]     Train net output #1: loss = 0.509417 (* 1 = 0.509417 loss)
I1122 10:33:04.087270 20880 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1122 10:33:08.349222 20880 solver.cpp:218] Iteration 3700 (23.4626 iter/s, 4.2621s/100 iters), loss = 0.510784
I1122 10:33:08.349222 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 10:33:08.349222 20880 solver.cpp:237]     Train net output #1: loss = 0.510784 (* 1 = 0.510784 loss)
I1122 10:33:08.349222 20880 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1122 10:33:12.614997 20880 solver.cpp:218] Iteration 3800 (23.446 iter/s, 4.26512s/100 iters), loss = 0.605213
I1122 10:33:12.614997 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 10:33:12.614997 20880 solver.cpp:237]     Train net output #1: loss = 0.605213 (* 1 = 0.605213 loss)
I1122 10:33:12.614997 20880 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1122 10:33:16.873805 20880 solver.cpp:218] Iteration 3900 (23.4833 iter/s, 4.25834s/100 iters), loss = 0.49556
I1122 10:33:16.873805 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 10:33:16.873805 20880 solver.cpp:237]     Train net output #1: loss = 0.495561 (* 1 = 0.495561 loss)
I1122 10:33:16.873805 20880 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1122 10:33:20.915470 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:33:21.082819 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_4000.caffemodel
I1122 10:33:21.092818 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_4000.solverstate
I1122 10:33:21.096817 20880 solver.cpp:330] Iteration 4000, Testing net (#0)
I1122 10:33:21.096817 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:33:22.155390 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:33:22.196385 20880 solver.cpp:397]     Test net output #0: accuracy = 0.6786
I1122 10:33:22.196385 20880 solver.cpp:397]     Test net output #1: loss = 1.00357 (* 1 = 1.00357 loss)
I1122 10:33:22.237403 20880 solver.cpp:218] Iteration 4000 (18.6441 iter/s, 5.36362s/100 iters), loss = 0.582587
I1122 10:33:22.237403 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 10:33:22.237403 20880 solver.cpp:237]     Train net output #1: loss = 0.582587 (* 1 = 0.582587 loss)
I1122 10:33:22.237403 20880 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1122 10:33:26.514252 20880 solver.cpp:218] Iteration 4100 (23.3827 iter/s, 4.27666s/100 iters), loss = 0.450826
I1122 10:33:26.514252 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 10:33:26.514252 20880 solver.cpp:237]     Train net output #1: loss = 0.450826 (* 1 = 0.450826 loss)
I1122 10:33:26.514252 20880 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1122 10:33:30.795347 20880 solver.cpp:218] Iteration 4200 (23.3629 iter/s, 4.28029s/100 iters), loss = 0.542294
I1122 10:33:30.795347 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 10:33:30.795347 20880 solver.cpp:237]     Train net output #1: loss = 0.542294 (* 1 = 0.542294 loss)
I1122 10:33:30.795347 20880 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1122 10:33:35.071256 20880 solver.cpp:218] Iteration 4300 (23.3866 iter/s, 4.27595s/100 iters), loss = 0.484818
I1122 10:33:35.071256 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 10:33:35.071256 20880 solver.cpp:237]     Train net output #1: loss = 0.484818 (* 1 = 0.484818 loss)
I1122 10:33:35.071256 20880 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1122 10:33:39.352756 20880 solver.cpp:218] Iteration 4400 (23.3598 iter/s, 4.28086s/100 iters), loss = 0.411714
I1122 10:33:39.352756 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 10:33:39.352756 20880 solver.cpp:237]     Train net output #1: loss = 0.411714 (* 1 = 0.411714 loss)
I1122 10:33:39.352756 20880 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1122 10:33:43.419265 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:33:43.586328 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_4500.caffemodel
I1122 10:33:43.596328 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_4500.solverstate
I1122 10:33:43.600328 20880 solver.cpp:330] Iteration 4500, Testing net (#0)
I1122 10:33:43.600328 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:33:44.659255 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:33:44.701239 20880 solver.cpp:397]     Test net output #0: accuracy = 0.7016
I1122 10:33:44.701239 20880 solver.cpp:397]     Test net output #1: loss = 0.905119 (* 1 = 0.905119 loss)
I1122 10:33:44.742274 20880 solver.cpp:218] Iteration 4500 (18.5577 iter/s, 5.38859s/100 iters), loss = 0.437683
I1122 10:33:44.742274 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 10:33:44.742274 20880 solver.cpp:237]     Train net output #1: loss = 0.437683 (* 1 = 0.437683 loss)
I1122 10:33:44.742274 20880 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1122 10:33:49.003880 20880 solver.cpp:218] Iteration 4600 (23.4681 iter/s, 4.26111s/100 iters), loss = 0.431159
I1122 10:33:49.003880 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 10:33:49.003880 20880 solver.cpp:237]     Train net output #1: loss = 0.431159 (* 1 = 0.431159 loss)
I1122 10:33:49.003880 20880 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1122 10:33:53.268031 20880 solver.cpp:218] Iteration 4700 (23.4532 iter/s, 4.26381s/100 iters), loss = 0.483989
I1122 10:33:53.268031 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1122 10:33:53.268031 20880 solver.cpp:237]     Train net output #1: loss = 0.483989 (* 1 = 0.483989 loss)
I1122 10:33:53.268031 20880 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1122 10:33:57.528225 20880 solver.cpp:218] Iteration 4800 (23.4741 iter/s, 4.26002s/100 iters), loss = 0.622992
I1122 10:33:57.528225 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 10:33:57.528225 20880 solver.cpp:237]     Train net output #1: loss = 0.622992 (* 1 = 0.622992 loss)
I1122 10:33:57.528225 20880 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1122 10:34:01.786252 20880 solver.cpp:218] Iteration 4900 (23.4839 iter/s, 4.25823s/100 iters), loss = 0.459883
I1122 10:34:01.786252 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 10:34:01.786252 20880 solver.cpp:237]     Train net output #1: loss = 0.459883 (* 1 = 0.459883 loss)
I1122 10:34:01.786252 20880 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1122 10:34:05.835909 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:34:06.003257 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_5000.caffemodel
I1122 10:34:06.013257 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_5000.solverstate
I1122 10:34:06.018259 20880 solver.cpp:330] Iteration 5000, Testing net (#0)
I1122 10:34:06.018259 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:34:07.078419 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:34:07.119419 20880 solver.cpp:397]     Test net output #0: accuracy = 0.7479
I1122 10:34:07.119419 20880 solver.cpp:397]     Test net output #1: loss = 0.748455 (* 1 = 0.748455 loss)
I1122 10:34:07.160432 20880 solver.cpp:218] Iteration 5000 (18.6103 iter/s, 5.37338s/100 iters), loss = 0.474175
I1122 10:34:07.160432 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 10:34:07.160432 20880 solver.cpp:237]     Train net output #1: loss = 0.474175 (* 1 = 0.474175 loss)
I1122 10:34:07.160432 20880 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1122 10:34:07.160432 20880 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1122 10:34:11.426223 20880 solver.cpp:218] Iteration 5100 (23.4457 iter/s, 4.26517s/100 iters), loss = 0.350204
I1122 10:34:11.426223 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1122 10:34:11.426223 20880 solver.cpp:237]     Train net output #1: loss = 0.350204 (* 1 = 0.350204 loss)
I1122 10:34:11.426223 20880 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1122 10:34:15.692232 20880 solver.cpp:218] Iteration 5200 (23.4395 iter/s, 4.2663s/100 iters), loss = 0.364581
I1122 10:34:15.692232 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1122 10:34:15.692232 20880 solver.cpp:237]     Train net output #1: loss = 0.364581 (* 1 = 0.364581 loss)
I1122 10:34:15.692232 20880 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1122 10:34:19.960806 20880 solver.cpp:218] Iteration 5300 (23.4311 iter/s, 4.26783s/100 iters), loss = 0.309571
I1122 10:34:19.960806 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:34:19.960806 20880 solver.cpp:237]     Train net output #1: loss = 0.309571 (* 1 = 0.309571 loss)
I1122 10:34:19.960806 20880 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1122 10:34:24.229022 20880 solver.cpp:218] Iteration 5400 (23.4318 iter/s, 4.2677s/100 iters), loss = 0.261037
I1122 10:34:24.229022 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 10:34:24.229022 20880 solver.cpp:237]     Train net output #1: loss = 0.261037 (* 1 = 0.261037 loss)
I1122 10:34:24.229022 20880 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1122 10:34:28.289937 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:34:28.458465 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_5500.caffemodel
I1122 10:34:28.467551 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_5500.solverstate
I1122 10:34:28.471551 20880 solver.cpp:330] Iteration 5500, Testing net (#0)
I1122 10:34:28.471551 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:34:29.530980 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:34:29.573004 20880 solver.cpp:397]     Test net output #0: accuracy = 0.8699
I1122 10:34:29.573004 20880 solver.cpp:397]     Test net output #1: loss = 0.383392 (* 1 = 0.383392 loss)
I1122 10:34:29.614012 20880 solver.cpp:218] Iteration 5500 (18.5716 iter/s, 5.38456s/100 iters), loss = 0.236919
I1122 10:34:29.614012 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:34:29.614012 20880 solver.cpp:237]     Train net output #1: loss = 0.236919 (* 1 = 0.236919 loss)
I1122 10:34:29.614012 20880 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1122 10:34:33.893841 20880 solver.cpp:218] Iteration 5600 (23.3685 iter/s, 4.27927s/100 iters), loss = 0.278857
I1122 10:34:33.893841 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 10:34:33.893841 20880 solver.cpp:237]     Train net output #1: loss = 0.278857 (* 1 = 0.278857 loss)
I1122 10:34:33.893841 20880 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1122 10:34:38.170135 20880 solver.cpp:218] Iteration 5700 (23.3834 iter/s, 4.27653s/100 iters), loss = 0.277995
I1122 10:34:38.170135 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 10:34:38.170135 20880 solver.cpp:237]     Train net output #1: loss = 0.277995 (* 1 = 0.277995 loss)
I1122 10:34:38.170135 20880 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1122 10:34:42.448365 20880 solver.cpp:218] Iteration 5800 (23.3778 iter/s, 4.27756s/100 iters), loss = 0.303925
I1122 10:34:42.448365 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:34:42.448365 20880 solver.cpp:237]     Train net output #1: loss = 0.303925 (* 1 = 0.303925 loss)
I1122 10:34:42.448365 20880 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1122 10:34:46.722080 20880 solver.cpp:218] Iteration 5900 (23.3986 iter/s, 4.27377s/100 iters), loss = 0.206152
I1122 10:34:46.722080 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:34:46.722080 20880 solver.cpp:237]     Train net output #1: loss = 0.206152 (* 1 = 0.206152 loss)
I1122 10:34:46.722080 20880 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1122 10:34:50.779146 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:34:50.946156 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_6000.caffemodel
I1122 10:34:50.956156 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_6000.solverstate
I1122 10:34:50.960156 20880 solver.cpp:330] Iteration 6000, Testing net (#0)
I1122 10:34:50.960156 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:34:52.020925 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:34:52.061918 20880 solver.cpp:397]     Test net output #0: accuracy = 0.8737
I1122 10:34:52.061918 20880 solver.cpp:397]     Test net output #1: loss = 0.374709 (* 1 = 0.374709 loss)
I1122 10:34:52.102936 20880 solver.cpp:218] Iteration 6000 (18.5859 iter/s, 5.38044s/100 iters), loss = 0.255089
I1122 10:34:52.102936 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:34:52.102936 20880 solver.cpp:237]     Train net output #1: loss = 0.255089 (* 1 = 0.255089 loss)
I1122 10:34:52.102936 20880 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1122 10:34:56.375207 20880 solver.cpp:218] Iteration 6100 (23.4124 iter/s, 4.27124s/100 iters), loss = 0.304519
I1122 10:34:56.375207 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 10:34:56.375207 20880 solver.cpp:237]     Train net output #1: loss = 0.304519 (* 1 = 0.304519 loss)
I1122 10:34:56.375207 20880 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1122 10:35:00.648509 20880 solver.cpp:218] Iteration 6200 (23.4028 iter/s, 4.27298s/100 iters), loss = 0.279451
I1122 10:35:00.648509 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 10:35:00.648509 20880 solver.cpp:237]     Train net output #1: loss = 0.279452 (* 1 = 0.279452 loss)
I1122 10:35:00.648509 20880 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1122 10:35:04.928489 20880 solver.cpp:218] Iteration 6300 (23.3671 iter/s, 4.27952s/100 iters), loss = 0.313108
I1122 10:35:04.928489 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:35:04.928489 20880 solver.cpp:237]     Train net output #1: loss = 0.313108 (* 1 = 0.313108 loss)
I1122 10:35:04.928489 20880 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1122 10:35:09.203691 20880 solver.cpp:218] Iteration 6400 (23.3908 iter/s, 4.27518s/100 iters), loss = 0.226855
I1122 10:35:09.203691 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:35:09.203691 20880 solver.cpp:237]     Train net output #1: loss = 0.226855 (* 1 = 0.226855 loss)
I1122 10:35:09.203691 20880 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1122 10:35:13.271591 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:35:13.438297 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_6500.caffemodel
I1122 10:35:13.448279 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_6500.solverstate
I1122 10:35:13.453280 20880 solver.cpp:330] Iteration 6500, Testing net (#0)
I1122 10:35:13.453280 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:35:14.511960 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:35:14.552960 20880 solver.cpp:397]     Test net output #0: accuracy = 0.876
I1122 10:35:14.552960 20880 solver.cpp:397]     Test net output #1: loss = 0.369307 (* 1 = 0.369307 loss)
I1122 10:35:14.593487 20880 solver.cpp:218] Iteration 6500 (18.5535 iter/s, 5.38983s/100 iters), loss = 0.233567
I1122 10:35:14.593487 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:35:14.593487 20880 solver.cpp:237]     Train net output #1: loss = 0.233567 (* 1 = 0.233567 loss)
I1122 10:35:14.593487 20880 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1122 10:35:18.855736 20880 solver.cpp:218] Iteration 6600 (23.4653 iter/s, 4.26162s/100 iters), loss = 0.22816
I1122 10:35:18.855736 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:35:18.855736 20880 solver.cpp:237]     Train net output #1: loss = 0.22816 (* 1 = 0.22816 loss)
I1122 10:35:18.855736 20880 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1122 10:35:23.116966 20880 solver.cpp:218] Iteration 6700 (23.4718 iter/s, 4.26044s/100 iters), loss = 0.226819
I1122 10:35:23.116966 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1122 10:35:23.116966 20880 solver.cpp:237]     Train net output #1: loss = 0.226819 (* 1 = 0.226819 loss)
I1122 10:35:23.116966 20880 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1122 10:35:27.375043 20880 solver.cpp:218] Iteration 6800 (23.4833 iter/s, 4.25835s/100 iters), loss = 0.252897
I1122 10:35:27.375043 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 10:35:27.375043 20880 solver.cpp:237]     Train net output #1: loss = 0.252897 (* 1 = 0.252897 loss)
I1122 10:35:27.375043 20880 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1122 10:35:31.638259 20880 solver.cpp:218] Iteration 6900 (23.4634 iter/s, 4.26195s/100 iters), loss = 0.157043
I1122 10:35:31.638259 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:35:31.638259 20880 solver.cpp:237]     Train net output #1: loss = 0.157043 (* 1 = 0.157043 loss)
I1122 10:35:31.638259 20880 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1122 10:35:35.691540 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:35:35.858175 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_7000.caffemodel
I1122 10:35:35.869163 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_7000.solverstate
I1122 10:35:35.873164 20880 solver.cpp:330] Iteration 7000, Testing net (#0)
I1122 10:35:35.873164 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:35:36.931193 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:35:36.973181 20880 solver.cpp:397]     Test net output #0: accuracy = 0.8789
I1122 10:35:36.973181 20880 solver.cpp:397]     Test net output #1: loss = 0.360633 (* 1 = 0.360633 loss)
I1122 10:35:37.013766 20880 solver.cpp:218] Iteration 7000 (18.6012 iter/s, 5.37601s/100 iters), loss = 0.226479
I1122 10:35:37.014765 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:35:37.014765 20880 solver.cpp:237]     Train net output #1: loss = 0.226479 (* 1 = 0.226479 loss)
I1122 10:35:37.014765 20880 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1122 10:35:41.289997 20880 solver.cpp:218] Iteration 7100 (23.3902 iter/s, 4.27529s/100 iters), loss = 0.219134
I1122 10:35:41.289997 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:35:41.289997 20880 solver.cpp:237]     Train net output #1: loss = 0.219134 (* 1 = 0.219134 loss)
I1122 10:35:41.289997 20880 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1122 10:35:45.565573 20880 solver.cpp:218] Iteration 7200 (23.3879 iter/s, 4.27572s/100 iters), loss = 0.203303
I1122 10:35:45.565573 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:35:45.565573 20880 solver.cpp:237]     Train net output #1: loss = 0.203303 (* 1 = 0.203303 loss)
I1122 10:35:45.565573 20880 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1122 10:35:49.836843 20880 solver.cpp:218] Iteration 7300 (23.4183 iter/s, 4.27016s/100 iters), loss = 0.224449
I1122 10:35:49.836843 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:35:49.836843 20880 solver.cpp:237]     Train net output #1: loss = 0.224449 (* 1 = 0.224449 loss)
I1122 10:35:49.836843 20880 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1122 10:35:54.109693 20880 solver.cpp:218] Iteration 7400 (23.4053 iter/s, 4.27253s/100 iters), loss = 0.163208
I1122 10:35:54.109693 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:35:54.109693 20880 solver.cpp:237]     Train net output #1: loss = 0.163208 (* 1 = 0.163208 loss)
I1122 10:35:54.109693 20880 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1122 10:35:58.172370 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:35:58.340601 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_7500.caffemodel
I1122 10:35:58.350591 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_7500.solverstate
I1122 10:35:58.357595 20880 solver.cpp:330] Iteration 7500, Testing net (#0)
I1122 10:35:58.357595 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:35:59.415801 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:35:59.456795 20880 solver.cpp:397]     Test net output #0: accuracy = 0.8774
I1122 10:35:59.456795 20880 solver.cpp:397]     Test net output #1: loss = 0.356701 (* 1 = 0.356701 loss)
I1122 10:35:59.498800 20880 solver.cpp:218] Iteration 7500 (18.5571 iter/s, 5.38879s/100 iters), loss = 0.236391
I1122 10:35:59.498800 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:35:59.498800 20880 solver.cpp:237]     Train net output #1: loss = 0.236391 (* 1 = 0.236391 loss)
I1122 10:35:59.498800 20880 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1122 10:36:03.773490 20880 solver.cpp:218] Iteration 7600 (23.3956 iter/s, 4.27431s/100 iters), loss = 0.246253
I1122 10:36:03.773490 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 10:36:03.773490 20880 solver.cpp:237]     Train net output #1: loss = 0.246253 (* 1 = 0.246253 loss)
I1122 10:36:03.773490 20880 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1122 10:36:08.048733 20880 solver.cpp:218] Iteration 7700 (23.3895 iter/s, 4.27543s/100 iters), loss = 0.217412
I1122 10:36:08.048733 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:36:08.048733 20880 solver.cpp:237]     Train net output #1: loss = 0.217412 (* 1 = 0.217412 loss)
I1122 10:36:08.048733 20880 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1122 10:36:12.323155 20880 solver.cpp:218] Iteration 7800 (23.3962 iter/s, 4.27421s/100 iters), loss = 0.285465
I1122 10:36:12.323155 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:36:12.323155 20880 solver.cpp:237]     Train net output #1: loss = 0.285465 (* 1 = 0.285465 loss)
I1122 10:36:12.323155 20880 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1122 10:36:16.586124 20880 solver.cpp:218] Iteration 7900 (23.4628 iter/s, 4.26207s/100 iters), loss = 0.151291
I1122 10:36:16.586124 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:36:16.586124 20880 solver.cpp:237]     Train net output #1: loss = 0.151291 (* 1 = 0.151291 loss)
I1122 10:36:16.586124 20880 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1122 10:36:20.652560 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:36:20.819577 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_8000.caffemodel
I1122 10:36:20.830585 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_8000.solverstate
I1122 10:36:20.834605 20880 solver.cpp:330] Iteration 8000, Testing net (#0)
I1122 10:36:20.834605 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:36:21.893137 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:36:21.935530 20880 solver.cpp:397]     Test net output #0: accuracy = 0.8801
I1122 10:36:21.935530 20880 solver.cpp:397]     Test net output #1: loss = 0.351867 (* 1 = 0.351867 loss)
I1122 10:36:21.976532 20880 solver.cpp:218] Iteration 8000 (18.5524 iter/s, 5.39015s/100 iters), loss = 0.197571
I1122 10:36:21.976532 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:36:21.976532 20880 solver.cpp:237]     Train net output #1: loss = 0.197571 (* 1 = 0.197571 loss)
I1122 10:36:21.976532 20880 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1122 10:36:26.246398 20880 solver.cpp:218] Iteration 8100 (23.4222 iter/s, 4.26945s/100 iters), loss = 0.232255
I1122 10:36:26.246398 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:36:26.246398 20880 solver.cpp:237]     Train net output #1: loss = 0.232255 (* 1 = 0.232255 loss)
I1122 10:36:26.246398 20880 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1122 10:36:30.517832 20880 solver.cpp:218] Iteration 8200 (23.4135 iter/s, 4.27104s/100 iters), loss = 0.181377
I1122 10:36:30.517832 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:36:30.517832 20880 solver.cpp:237]     Train net output #1: loss = 0.181377 (* 1 = 0.181377 loss)
I1122 10:36:30.517832 20880 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1122 10:36:34.789860 20880 solver.cpp:218] Iteration 8300 (23.4105 iter/s, 4.2716s/100 iters), loss = 0.21103
I1122 10:36:34.789860 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 10:36:34.789860 20880 solver.cpp:237]     Train net output #1: loss = 0.21103 (* 1 = 0.21103 loss)
I1122 10:36:34.789860 20880 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1122 10:36:39.061347 20880 solver.cpp:218] Iteration 8400 (23.4116 iter/s, 4.27138s/100 iters), loss = 0.141329
I1122 10:36:39.061347 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:36:39.061347 20880 solver.cpp:237]     Train net output #1: loss = 0.141329 (* 1 = 0.141329 loss)
I1122 10:36:39.061347 20880 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1122 10:36:43.119837 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:36:43.288003 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_8500.caffemodel
I1122 10:36:43.297988 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_8500.solverstate
I1122 10:36:43.302008 20880 solver.cpp:330] Iteration 8500, Testing net (#0)
I1122 10:36:43.302008 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:36:44.363284 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:36:44.404332 20880 solver.cpp:397]     Test net output #0: accuracy = 0.8812
I1122 10:36:44.404332 20880 solver.cpp:397]     Test net output #1: loss = 0.352796 (* 1 = 0.352796 loss)
I1122 10:36:44.445333 20880 solver.cpp:218] Iteration 8500 (18.577 iter/s, 5.38299s/100 iters), loss = 0.225279
I1122 10:36:44.445333 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:36:44.445333 20880 solver.cpp:237]     Train net output #1: loss = 0.225279 (* 1 = 0.225279 loss)
I1122 10:36:44.445333 20880 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1122 10:36:48.703274 20880 solver.cpp:218] Iteration 8600 (23.4859 iter/s, 4.25786s/100 iters), loss = 0.188383
I1122 10:36:48.703274 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:36:48.703274 20880 solver.cpp:237]     Train net output #1: loss = 0.188383 (* 1 = 0.188383 loss)
I1122 10:36:48.703274 20880 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1122 10:36:52.968220 20880 solver.cpp:218] Iteration 8700 (23.4496 iter/s, 4.26447s/100 iters), loss = 0.195915
I1122 10:36:52.968220 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:36:52.968220 20880 solver.cpp:237]     Train net output #1: loss = 0.195915 (* 1 = 0.195915 loss)
I1122 10:36:52.968220 20880 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1122 10:36:57.230104 20880 solver.cpp:218] Iteration 8800 (23.4622 iter/s, 4.26218s/100 iters), loss = 0.157844
I1122 10:36:57.230104 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:36:57.230104 20880 solver.cpp:237]     Train net output #1: loss = 0.157844 (* 1 = 0.157844 loss)
I1122 10:36:57.230104 20880 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1122 10:37:01.494263 20880 solver.cpp:218] Iteration 8900 (23.4579 iter/s, 4.26295s/100 iters), loss = 0.129296
I1122 10:37:01.494263 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:37:01.494263 20880 solver.cpp:237]     Train net output #1: loss = 0.129296 (* 1 = 0.129296 loss)
I1122 10:37:01.494263 20880 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1122 10:37:05.552563 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:37:05.717211 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_9000.caffemodel
I1122 10:37:05.728209 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_9000.solverstate
I1122 10:37:05.732208 20880 solver.cpp:330] Iteration 9000, Testing net (#0)
I1122 10:37:05.732208 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:37:06.791904 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:37:06.833889 20880 solver.cpp:397]     Test net output #0: accuracy = 0.8835
I1122 10:37:06.833889 20880 solver.cpp:397]     Test net output #1: loss = 0.350599 (* 1 = 0.350599 loss)
I1122 10:37:06.874547 20880 solver.cpp:218] Iteration 9000 (18.5862 iter/s, 5.38033s/100 iters), loss = 0.168769
I1122 10:37:06.874547 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:37:06.874547 20880 solver.cpp:237]     Train net output #1: loss = 0.168769 (* 1 = 0.168769 loss)
I1122 10:37:06.874547 20880 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1122 10:37:11.156955 20880 solver.cpp:218] Iteration 9100 (23.355 iter/s, 4.28175s/100 iters), loss = 0.18922
I1122 10:37:11.156955 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:37:11.156955 20880 solver.cpp:237]     Train net output #1: loss = 0.18922 (* 1 = 0.18922 loss)
I1122 10:37:11.156955 20880 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1122 10:37:15.435400 20880 solver.cpp:218] Iteration 9200 (23.3757 iter/s, 4.27795s/100 iters), loss = 0.166841
I1122 10:37:15.435400 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:37:15.435400 20880 solver.cpp:237]     Train net output #1: loss = 0.166841 (* 1 = 0.166841 loss)
I1122 10:37:15.435400 20880 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1122 10:37:19.709355 20880 solver.cpp:218] Iteration 9300 (23.3964 iter/s, 4.27416s/100 iters), loss = 0.20166
I1122 10:37:19.709355 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:37:19.709355 20880 solver.cpp:237]     Train net output #1: loss = 0.20166 (* 1 = 0.20166 loss)
I1122 10:37:19.709355 20880 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1122 10:37:23.989264 20880 solver.cpp:218] Iteration 9400 (23.3687 iter/s, 4.27924s/100 iters), loss = 0.1366
I1122 10:37:23.989264 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:37:23.989264 20880 solver.cpp:237]     Train net output #1: loss = 0.1366 (* 1 = 0.1366 loss)
I1122 10:37:23.989264 20880 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1122 10:37:28.060516 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:37:28.228349 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_9500.caffemodel
I1122 10:37:28.239325 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_9500.solverstate
I1122 10:37:28.243325 20880 solver.cpp:330] Iteration 9500, Testing net (#0)
I1122 10:37:28.243325 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:37:29.303964 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:37:29.345952 20880 solver.cpp:397]     Test net output #0: accuracy = 0.8808
I1122 10:37:29.345952 20880 solver.cpp:397]     Test net output #1: loss = 0.357372 (* 1 = 0.357372 loss)
I1122 10:37:29.387002 20880 solver.cpp:218] Iteration 9500 (18.528 iter/s, 5.39723s/100 iters), loss = 0.22143
I1122 10:37:29.387002 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 10:37:29.387002 20880 solver.cpp:237]     Train net output #1: loss = 0.22143 (* 1 = 0.22143 loss)
I1122 10:37:29.387002 20880 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1122 10:37:29.387002 20880 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1122 10:37:33.644356 20880 solver.cpp:218] Iteration 9600 (23.4874 iter/s, 4.2576s/100 iters), loss = 0.181917
I1122 10:37:33.644356 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:37:33.644356 20880 solver.cpp:237]     Train net output #1: loss = 0.181917 (* 1 = 0.181917 loss)
I1122 10:37:33.644356 20880 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1122 10:37:37.907624 20880 solver.cpp:218] Iteration 9700 (23.4604 iter/s, 4.26249s/100 iters), loss = 0.169132
I1122 10:37:37.907624 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:37:37.907624 20880 solver.cpp:237]     Train net output #1: loss = 0.169132 (* 1 = 0.169132 loss)
I1122 10:37:37.907624 20880 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1122 10:37:42.168711 20880 solver.cpp:218] Iteration 9800 (23.4717 iter/s, 4.26045s/100 iters), loss = 0.196446
I1122 10:37:42.168711 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:37:42.168711 20880 solver.cpp:237]     Train net output #1: loss = 0.196446 (* 1 = 0.196446 loss)
I1122 10:37:42.168711 20880 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1122 10:37:46.432145 20880 solver.cpp:218] Iteration 9900 (23.4536 iter/s, 4.26374s/100 iters), loss = 0.124257
I1122 10:37:46.432145 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:37:46.432145 20880 solver.cpp:237]     Train net output #1: loss = 0.124257 (* 1 = 0.124257 loss)
I1122 10:37:46.432145 20880 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1122 10:37:50.486583 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:37:50.653618 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_10000.caffemodel
I1122 10:37:50.663616 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_10000.solverstate
I1122 10:37:50.667616 20880 solver.cpp:330] Iteration 10000, Testing net (#0)
I1122 10:37:50.667616 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:37:51.725174 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:37:51.767166 20880 solver.cpp:397]     Test net output #0: accuracy = 0.8994
I1122 10:37:51.767166 20880 solver.cpp:397]     Test net output #1: loss = 0.30801 (* 1 = 0.30801 loss)
I1122 10:37:51.808195 20880 solver.cpp:218] Iteration 10000 (18.6039 iter/s, 5.37521s/100 iters), loss = 0.213941
I1122 10:37:51.808195 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:37:51.808195 20880 solver.cpp:237]     Train net output #1: loss = 0.213941 (* 1 = 0.213941 loss)
I1122 10:37:51.808195 20880 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1122 10:37:56.079026 20880 solver.cpp:218] Iteration 10100 (23.4176 iter/s, 4.2703s/100 iters), loss = 0.176491
I1122 10:37:56.079026 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:37:56.079026 20880 solver.cpp:237]     Train net output #1: loss = 0.176491 (* 1 = 0.176491 loss)
I1122 10:37:56.079026 20880 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1122 10:38:00.350679 20880 solver.cpp:218] Iteration 10200 (23.4107 iter/s, 4.27155s/100 iters), loss = 0.14493
I1122 10:38:00.350679 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:38:00.350679 20880 solver.cpp:237]     Train net output #1: loss = 0.14493 (* 1 = 0.14493 loss)
I1122 10:38:00.350679 20880 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1122 10:38:04.623989 20880 solver.cpp:218] Iteration 10300 (23.4045 iter/s, 4.27268s/100 iters), loss = 0.17563
I1122 10:38:04.623989 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:38:04.623989 20880 solver.cpp:237]     Train net output #1: loss = 0.17563 (* 1 = 0.17563 loss)
I1122 10:38:04.623989 20880 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1122 10:38:08.892098 20880 solver.cpp:218] Iteration 10400 (23.4319 iter/s, 4.26769s/100 iters), loss = 0.098661
I1122 10:38:08.892098 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:38:08.892098 20880 solver.cpp:237]     Train net output #1: loss = 0.098661 (* 1 = 0.098661 loss)
I1122 10:38:08.892098 20880 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1122 10:38:12.955435 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:38:13.122417 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_10500.caffemodel
I1122 10:38:13.132406 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_10500.solverstate
I1122 10:38:13.136407 20880 solver.cpp:330] Iteration 10500, Testing net (#0)
I1122 10:38:13.136407 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:38:14.197937 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:38:14.239454 20880 solver.cpp:397]     Test net output #0: accuracy = 0.8999
I1122 10:38:14.239454 20880 solver.cpp:397]     Test net output #1: loss = 0.30576 (* 1 = 0.30576 loss)
I1122 10:38:14.280465 20880 solver.cpp:218] Iteration 10500 (18.5588 iter/s, 5.38828s/100 iters), loss = 0.205488
I1122 10:38:14.280465 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:38:14.280465 20880 solver.cpp:237]     Train net output #1: loss = 0.205488 (* 1 = 0.205488 loss)
I1122 10:38:14.280465 20880 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1122 10:38:18.553721 20880 solver.cpp:218] Iteration 10600 (23.4058 iter/s, 4.27245s/100 iters), loss = 0.213046
I1122 10:38:18.553721 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:38:18.553721 20880 solver.cpp:237]     Train net output #1: loss = 0.213046 (* 1 = 0.213046 loss)
I1122 10:38:18.553721 20880 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1122 10:38:22.823192 20880 solver.cpp:218] Iteration 10700 (23.4194 iter/s, 4.26997s/100 iters), loss = 0.136567
I1122 10:38:22.823192 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:38:22.823192 20880 solver.cpp:237]     Train net output #1: loss = 0.136567 (* 1 = 0.136567 loss)
I1122 10:38:22.823192 20880 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1122 10:38:27.101068 20880 solver.cpp:218] Iteration 10800 (23.381 iter/s, 4.27698s/100 iters), loss = 0.182104
I1122 10:38:27.101068 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:38:27.101068 20880 solver.cpp:237]     Train net output #1: loss = 0.182104 (* 1 = 0.182104 loss)
I1122 10:38:27.101068 20880 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1122 10:38:31.381912 20880 solver.cpp:218] Iteration 10900 (23.3617 iter/s, 4.28051s/100 iters), loss = 0.110586
I1122 10:38:31.381912 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:38:31.381912 20880 solver.cpp:237]     Train net output #1: loss = 0.110586 (* 1 = 0.110586 loss)
I1122 10:38:31.381912 20880 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1122 10:38:35.444715 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:38:35.611683 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_11000.caffemodel
I1122 10:38:35.621668 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_11000.solverstate
I1122 10:38:35.625669 20880 solver.cpp:330] Iteration 11000, Testing net (#0)
I1122 10:38:35.625669 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:38:36.684733 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:38:36.725762 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9002
I1122 10:38:36.725762 20880 solver.cpp:397]     Test net output #1: loss = 0.304464 (* 1 = 0.304464 loss)
I1122 10:38:36.766772 20880 solver.cpp:218] Iteration 11000 (18.5707 iter/s, 5.38482s/100 iters), loss = 0.173708
I1122 10:38:36.766772 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:38:36.766772 20880 solver.cpp:237]     Train net output #1: loss = 0.173708 (* 1 = 0.173708 loss)
I1122 10:38:36.766772 20880 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1122 10:38:41.040735 20880 solver.cpp:218] Iteration 11100 (23.4 iter/s, 4.2735s/100 iters), loss = 0.192433
I1122 10:38:41.040735 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:38:41.040735 20880 solver.cpp:237]     Train net output #1: loss = 0.192434 (* 1 = 0.192434 loss)
I1122 10:38:41.040735 20880 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1122 10:38:45.315309 20880 solver.cpp:218] Iteration 11200 (23.3976 iter/s, 4.27393s/100 iters), loss = 0.153722
I1122 10:38:45.315309 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:38:45.315309 20880 solver.cpp:237]     Train net output #1: loss = 0.153723 (* 1 = 0.153723 loss)
I1122 10:38:45.315309 20880 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1122 10:38:49.590423 20880 solver.cpp:218] Iteration 11300 (23.3924 iter/s, 4.27489s/100 iters), loss = 0.159408
I1122 10:38:49.590423 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:38:49.590423 20880 solver.cpp:237]     Train net output #1: loss = 0.159408 (* 1 = 0.159408 loss)
I1122 10:38:49.590423 20880 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1122 10:38:53.861300 20880 solver.cpp:218] Iteration 11400 (23.4174 iter/s, 4.27032s/100 iters), loss = 0.106156
I1122 10:38:53.861300 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:38:53.861300 20880 solver.cpp:237]     Train net output #1: loss = 0.106156 (* 1 = 0.106156 loss)
I1122 10:38:53.861300 20880 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1122 10:38:57.929256 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:38:58.096288 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_11500.caffemodel
I1122 10:38:58.107291 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_11500.solverstate
I1122 10:38:58.111291 20880 solver.cpp:330] Iteration 11500, Testing net (#0)
I1122 10:38:58.111291 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:38:59.171030 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:38:59.213019 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9012
I1122 10:38:59.213520 20880 solver.cpp:397]     Test net output #1: loss = 0.303598 (* 1 = 0.303598 loss)
I1122 10:38:59.253103 20880 solver.cpp:218] Iteration 11500 (18.5463 iter/s, 5.39192s/100 iters), loss = 0.18209
I1122 10:38:59.254102 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:38:59.254102 20880 solver.cpp:237]     Train net output #1: loss = 0.182091 (* 1 = 0.182091 loss)
I1122 10:38:59.254102 20880 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1122 10:39:03.514868 20880 solver.cpp:218] Iteration 11600 (23.47 iter/s, 4.26075s/100 iters), loss = 0.18262
I1122 10:39:03.514868 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:39:03.514868 20880 solver.cpp:237]     Train net output #1: loss = 0.18262 (* 1 = 0.18262 loss)
I1122 10:39:03.514868 20880 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1122 10:39:07.775241 20880 solver.cpp:218] Iteration 11700 (23.4738 iter/s, 4.26006s/100 iters), loss = 0.177784
I1122 10:39:07.775241 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:39:07.775241 20880 solver.cpp:237]     Train net output #1: loss = 0.177784 (* 1 = 0.177784 loss)
I1122 10:39:07.775241 20880 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1122 10:39:12.036787 20880 solver.cpp:218] Iteration 11800 (23.4648 iter/s, 4.26171s/100 iters), loss = 0.209871
I1122 10:39:12.036787 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:39:12.036787 20880 solver.cpp:237]     Train net output #1: loss = 0.209871 (* 1 = 0.209871 loss)
I1122 10:39:12.036787 20880 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1122 10:39:16.295635 20880 solver.cpp:218] Iteration 11900 (23.4863 iter/s, 4.2578s/100 iters), loss = 0.114743
I1122 10:39:16.295635 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:39:16.295635 20880 solver.cpp:237]     Train net output #1: loss = 0.114743 (* 1 = 0.114743 loss)
I1122 10:39:16.295635 20880 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1122 10:39:20.345898 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:39:20.512915 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_12000.caffemodel
I1122 10:39:20.524420 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_12000.solverstate
I1122 10:39:20.528921 20880 solver.cpp:330] Iteration 12000, Testing net (#0)
I1122 10:39:20.528921 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:39:21.588425 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:39:21.629429 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9006
I1122 10:39:21.629915 20880 solver.cpp:397]     Test net output #1: loss = 0.301735 (* 1 = 0.301735 loss)
I1122 10:39:21.670442 20880 solver.cpp:218] Iteration 12000 (18.605 iter/s, 5.3749s/100 iters), loss = 0.15075
I1122 10:39:21.670442 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:39:21.670442 20880 solver.cpp:237]     Train net output #1: loss = 0.15075 (* 1 = 0.15075 loss)
I1122 10:39:21.670442 20880 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1122 10:39:25.940780 20880 solver.cpp:218] Iteration 12100 (23.4231 iter/s, 4.26929s/100 iters), loss = 0.21025
I1122 10:39:25.940780 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 10:39:25.940780 20880 solver.cpp:237]     Train net output #1: loss = 0.210251 (* 1 = 0.210251 loss)
I1122 10:39:25.940780 20880 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1122 10:39:30.210278 20880 solver.cpp:218] Iteration 12200 (23.4237 iter/s, 4.26918s/100 iters), loss = 0.130677
I1122 10:39:30.210278 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:39:30.210278 20880 solver.cpp:237]     Train net output #1: loss = 0.130677 (* 1 = 0.130677 loss)
I1122 10:39:30.210278 20880 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1122 10:39:34.486836 20880 solver.cpp:218] Iteration 12300 (23.3816 iter/s, 4.27686s/100 iters), loss = 0.151942
I1122 10:39:34.486836 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:39:34.486836 20880 solver.cpp:237]     Train net output #1: loss = 0.151942 (* 1 = 0.151942 loss)
I1122 10:39:34.486836 20880 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1122 10:39:38.760902 20880 solver.cpp:218] Iteration 12400 (23.4024 iter/s, 4.27306s/100 iters), loss = 0.105527
I1122 10:39:38.760902 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:39:38.760902 20880 solver.cpp:237]     Train net output #1: loss = 0.105528 (* 1 = 0.105528 loss)
I1122 10:39:38.760902 20880 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1122 10:39:42.828744 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:39:42.995236 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_12500.caffemodel
I1122 10:39:43.006234 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_12500.solverstate
I1122 10:39:43.010233 20880 solver.cpp:330] Iteration 12500, Testing net (#0)
I1122 10:39:43.010233 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:39:44.069497 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:39:44.111492 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9002
I1122 10:39:44.111492 20880 solver.cpp:397]     Test net output #1: loss = 0.301233 (* 1 = 0.301233 loss)
I1122 10:39:44.152017 20880 solver.cpp:218] Iteration 12500 (18.5492 iter/s, 5.39106s/100 iters), loss = 0.132122
I1122 10:39:44.152017 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:39:44.152017 20880 solver.cpp:237]     Train net output #1: loss = 0.132122 (* 1 = 0.132122 loss)
I1122 10:39:44.152017 20880 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1122 10:39:48.427377 20880 solver.cpp:218] Iteration 12600 (23.3934 iter/s, 4.27471s/100 iters), loss = 0.178691
I1122 10:39:48.427377 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:39:48.427377 20880 solver.cpp:237]     Train net output #1: loss = 0.178691 (* 1 = 0.178691 loss)
I1122 10:39:48.427377 20880 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1122 10:39:52.693533 20880 solver.cpp:218] Iteration 12700 (23.4396 iter/s, 4.26628s/100 iters), loss = 0.118337
I1122 10:39:52.693533 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:39:52.693533 20880 solver.cpp:237]     Train net output #1: loss = 0.118337 (* 1 = 0.118337 loss)
I1122 10:39:52.693533 20880 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1122 10:39:56.970353 20880 solver.cpp:218] Iteration 12800 (23.3843 iter/s, 4.27638s/100 iters), loss = 0.128841
I1122 10:39:56.970353 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:39:56.970353 20880 solver.cpp:237]     Train net output #1: loss = 0.128841 (* 1 = 0.128841 loss)
I1122 10:39:56.970353 20880 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1122 10:40:01.276566 20880 solver.cpp:218] Iteration 12900 (23.2277 iter/s, 4.30521s/100 iters), loss = 0.127879
I1122 10:40:01.276566 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:40:01.276566 20880 solver.cpp:237]     Train net output #1: loss = 0.127879 (* 1 = 0.127879 loss)
I1122 10:40:01.276566 20880 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1122 10:40:05.341393 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:40:05.508425 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_13000.caffemodel
I1122 10:40:05.518927 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_13000.solverstate
I1122 10:40:05.522430 20880 solver.cpp:330] Iteration 13000, Testing net (#0)
I1122 10:40:05.522430 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:40:06.581972 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:40:06.622992 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9004
I1122 10:40:06.622992 20880 solver.cpp:397]     Test net output #1: loss = 0.301265 (* 1 = 0.301265 loss)
I1122 10:40:06.663990 20880 solver.cpp:218] Iteration 13000 (18.5613 iter/s, 5.38754s/100 iters), loss = 0.18859
I1122 10:40:06.663990 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:40:06.663990 20880 solver.cpp:237]     Train net output #1: loss = 0.18859 (* 1 = 0.18859 loss)
I1122 10:40:06.663990 20880 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1122 10:40:10.934417 20880 solver.cpp:218] Iteration 13100 (23.4207 iter/s, 4.26973s/100 iters), loss = 0.194396
I1122 10:40:10.934417 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:40:10.934417 20880 solver.cpp:237]     Train net output #1: loss = 0.194396 (* 1 = 0.194396 loss)
I1122 10:40:10.934417 20880 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1122 10:40:15.203580 20880 solver.cpp:218] Iteration 13200 (23.4239 iter/s, 4.26915s/100 iters), loss = 0.125675
I1122 10:40:15.203580 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:40:15.203580 20880 solver.cpp:237]     Train net output #1: loss = 0.125675 (* 1 = 0.125675 loss)
I1122 10:40:15.203580 20880 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1122 10:40:19.476197 20880 solver.cpp:218] Iteration 13300 (23.4057 iter/s, 4.27246s/100 iters), loss = 0.169956
I1122 10:40:19.476197 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:40:19.476197 20880 solver.cpp:237]     Train net output #1: loss = 0.169956 (* 1 = 0.169956 loss)
I1122 10:40:19.476197 20880 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1122 10:40:23.745378 20880 solver.cpp:218] Iteration 13400 (23.4289 iter/s, 4.26822s/100 iters), loss = 0.0920355
I1122 10:40:23.745378 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:40:23.745378 20880 solver.cpp:237]     Train net output #1: loss = 0.0920357 (* 1 = 0.0920357 loss)
I1122 10:40:23.745378 20880 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1122 10:40:27.813112 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:40:27.980159 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_13500.caffemodel
I1122 10:40:27.991155 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_13500.solverstate
I1122 10:40:27.995157 20880 solver.cpp:330] Iteration 13500, Testing net (#0)
I1122 10:40:27.995157 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:40:29.054407 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:40:29.095393 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9021
I1122 10:40:29.095393 20880 solver.cpp:397]     Test net output #1: loss = 0.300405 (* 1 = 0.300405 loss)
I1122 10:40:29.135934 20880 solver.cpp:218] Iteration 13500 (18.55 iter/s, 5.39083s/100 iters), loss = 0.145977
I1122 10:40:29.135934 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:40:29.135934 20880 solver.cpp:237]     Train net output #1: loss = 0.145977 (* 1 = 0.145977 loss)
I1122 10:40:29.135934 20880 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1122 10:40:33.394923 20880 solver.cpp:218] Iteration 13600 (23.4822 iter/s, 4.25854s/100 iters), loss = 0.131802
I1122 10:40:33.394923 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:40:33.394923 20880 solver.cpp:237]     Train net output #1: loss = 0.131802 (* 1 = 0.131802 loss)
I1122 10:40:33.394923 20880 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1122 10:40:37.655882 20880 solver.cpp:218] Iteration 13700 (23.4731 iter/s, 4.2602s/100 iters), loss = 0.148842
I1122 10:40:37.655882 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:40:37.655882 20880 solver.cpp:237]     Train net output #1: loss = 0.148842 (* 1 = 0.148842 loss)
I1122 10:40:37.655882 20880 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1122 10:40:41.917510 20880 solver.cpp:218] Iteration 13800 (23.4688 iter/s, 4.26097s/100 iters), loss = 0.143987
I1122 10:40:41.917510 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:40:41.917510 20880 solver.cpp:237]     Train net output #1: loss = 0.143987 (* 1 = 0.143987 loss)
I1122 10:40:41.917510 20880 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1122 10:40:46.181285 20880 solver.cpp:218] Iteration 13900 (23.4557 iter/s, 4.26335s/100 iters), loss = 0.0821302
I1122 10:40:46.181285 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:40:46.181285 20880 solver.cpp:237]     Train net output #1: loss = 0.0821304 (* 1 = 0.0821304 loss)
I1122 10:40:46.181285 20880 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1122 10:40:50.230958 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:40:50.398483 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_14000.caffemodel
I1122 10:40:50.411466 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_14000.solverstate
I1122 10:40:50.415465 20880 solver.cpp:330] Iteration 14000, Testing net (#0)
I1122 10:40:50.415465 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:40:51.473317 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:40:51.514312 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9017
I1122 10:40:51.514312 20880 solver.cpp:397]     Test net output #1: loss = 0.299704 (* 1 = 0.299704 loss)
I1122 10:40:51.555326 20880 solver.cpp:218] Iteration 14000 (18.6067 iter/s, 5.37441s/100 iters), loss = 0.127187
I1122 10:40:51.555326 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:40:51.555326 20880 solver.cpp:237]     Train net output #1: loss = 0.127187 (* 1 = 0.127187 loss)
I1122 10:40:51.555326 20880 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1122 10:40:55.829952 20880 solver.cpp:218] Iteration 14100 (23.3959 iter/s, 4.27425s/100 iters), loss = 0.214078
I1122 10:40:55.829952 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:40:55.829952 20880 solver.cpp:237]     Train net output #1: loss = 0.214079 (* 1 = 0.214079 loss)
I1122 10:40:55.829952 20880 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1122 10:41:00.106402 20880 solver.cpp:218] Iteration 14200 (23.3855 iter/s, 4.27616s/100 iters), loss = 0.134883
I1122 10:41:00.106402 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:41:00.106402 20880 solver.cpp:237]     Train net output #1: loss = 0.134883 (* 1 = 0.134883 loss)
I1122 10:41:00.106402 20880 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1122 10:41:04.387583 20880 solver.cpp:218] Iteration 14300 (23.3597 iter/s, 4.28088s/100 iters), loss = 0.145999
I1122 10:41:04.387583 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:41:04.387583 20880 solver.cpp:237]     Train net output #1: loss = 0.146 (* 1 = 0.146 loss)
I1122 10:41:04.388582 20880 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1122 10:41:08.667026 20880 solver.cpp:218] Iteration 14400 (23.3695 iter/s, 4.27909s/100 iters), loss = 0.0750412
I1122 10:41:08.667026 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:41:08.668027 20880 solver.cpp:237]     Train net output #1: loss = 0.0750414 (* 1 = 0.0750414 loss)
I1122 10:41:08.668027 20880 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1122 10:41:12.736235 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:41:12.904224 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_14500.caffemodel
I1122 10:41:12.914209 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_14500.solverstate
I1122 10:41:12.918225 20880 solver.cpp:330] Iteration 14500, Testing net (#0)
I1122 10:41:12.918225 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:41:13.978958 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:41:14.019953 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9001
I1122 10:41:14.020967 20880 solver.cpp:397]     Test net output #1: loss = 0.300468 (* 1 = 0.300468 loss)
I1122 10:41:14.061517 20880 solver.cpp:218] Iteration 14500 (18.5393 iter/s, 5.39395s/100 iters), loss = 0.138214
I1122 10:41:14.061517 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:41:14.061517 20880 solver.cpp:237]     Train net output #1: loss = 0.138214 (* 1 = 0.138214 loss)
I1122 10:41:14.061517 20880 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1122 10:41:18.320538 20880 solver.cpp:218] Iteration 14600 (23.4838 iter/s, 4.25826s/100 iters), loss = 0.166228
I1122 10:41:18.320538 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:41:18.320538 20880 solver.cpp:237]     Train net output #1: loss = 0.166229 (* 1 = 0.166229 loss)
I1122 10:41:18.320538 20880 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1122 10:41:22.579190 20880 solver.cpp:218] Iteration 14700 (23.4814 iter/s, 4.25868s/100 iters), loss = 0.166023
I1122 10:41:22.579190 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:41:22.579190 20880 solver.cpp:237]     Train net output #1: loss = 0.166023 (* 1 = 0.166023 loss)
I1122 10:41:22.579190 20880 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1122 10:41:26.841591 20880 solver.cpp:218] Iteration 14800 (23.4647 iter/s, 4.26172s/100 iters), loss = 0.169687
I1122 10:41:26.842095 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:41:26.842095 20880 solver.cpp:237]     Train net output #1: loss = 0.169687 (* 1 = 0.169687 loss)
I1122 10:41:26.842095 20880 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1122 10:41:31.100126 20880 solver.cpp:218] Iteration 14900 (23.4829 iter/s, 4.25842s/100 iters), loss = 0.104251
I1122 10:41:31.100126 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:41:31.100126 20880 solver.cpp:237]     Train net output #1: loss = 0.104251 (* 1 = 0.104251 loss)
I1122 10:41:31.100126 20880 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1122 10:41:35.151475 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:41:35.318694 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_15000.caffemodel
I1122 10:41:35.328217 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_15000.solverstate
I1122 10:41:35.332219 20880 solver.cpp:330] Iteration 15000, Testing net (#0)
I1122 10:41:35.332219 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:41:36.393476 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:41:36.435091 20880 solver.cpp:397]     Test net output #0: accuracy = 0.901
I1122 10:41:36.435091 20880 solver.cpp:397]     Test net output #1: loss = 0.300455 (* 1 = 0.300455 loss)
I1122 10:41:36.476092 20880 solver.cpp:218] Iteration 15000 (18.6017 iter/s, 5.37584s/100 iters), loss = 0.140199
I1122 10:41:36.477097 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:41:36.477097 20880 solver.cpp:237]     Train net output #1: loss = 0.140199 (* 1 = 0.140199 loss)
I1122 10:41:36.477097 20880 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1122 10:41:40.739060 20880 solver.cpp:218] Iteration 15100 (23.4597 iter/s, 4.26263s/100 iters), loss = 0.173282
I1122 10:41:40.740074 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:41:40.740074 20880 solver.cpp:237]     Train net output #1: loss = 0.173282 (* 1 = 0.173282 loss)
I1122 10:41:40.740074 20880 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1122 10:41:45.006727 20880 solver.cpp:218] Iteration 15200 (23.4365 iter/s, 4.26685s/100 iters), loss = 0.136929
I1122 10:41:45.006727 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:41:45.006727 20880 solver.cpp:237]     Train net output #1: loss = 0.136929 (* 1 = 0.136929 loss)
I1122 10:41:45.006727 20880 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1122 10:41:49.272222 20880 solver.cpp:218] Iteration 15300 (23.446 iter/s, 4.26511s/100 iters), loss = 0.159748
I1122 10:41:49.272222 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:41:49.272222 20880 solver.cpp:237]     Train net output #1: loss = 0.159748 (* 1 = 0.159748 loss)
I1122 10:41:49.272222 20880 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1122 10:41:49.272222 20880 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1122 10:41:53.537549 20880 solver.cpp:218] Iteration 15400 (23.4475 iter/s, 4.26484s/100 iters), loss = 0.10551
I1122 10:41:53.537549 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:41:53.537549 20880 solver.cpp:237]     Train net output #1: loss = 0.10551 (* 1 = 0.10551 loss)
I1122 10:41:53.537549 20880 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1122 10:41:57.601405 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:41:57.769065 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_15500.caffemodel
I1122 10:41:57.779058 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_15500.solverstate
I1122 10:41:57.783057 20880 solver.cpp:330] Iteration 15500, Testing net (#0)
I1122 10:41:57.783057 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:41:58.844091 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:41:58.886078 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9018
I1122 10:41:58.886078 20880 solver.cpp:397]     Test net output #1: loss = 0.29917 (* 1 = 0.29917 loss)
I1122 10:41:58.927613 20880 solver.cpp:218] Iteration 15500 (18.5558 iter/s, 5.38916s/100 iters), loss = 0.114945
I1122 10:41:58.927613 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:41:58.927613 20880 solver.cpp:237]     Train net output #1: loss = 0.114945 (* 1 = 0.114945 loss)
I1122 10:41:58.927613 20880 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1122 10:42:03.199976 20880 solver.cpp:218] Iteration 15600 (23.4054 iter/s, 4.27252s/100 iters), loss = 0.237319
I1122 10:42:03.199976 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:42:03.199976 20880 solver.cpp:237]     Train net output #1: loss = 0.237319 (* 1 = 0.237319 loss)
I1122 10:42:03.199976 20880 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1122 10:42:07.477094 20880 solver.cpp:218] Iteration 15700 (23.3812 iter/s, 4.27693s/100 iters), loss = 0.124175
I1122 10:42:07.477094 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:42:07.477094 20880 solver.cpp:237]     Train net output #1: loss = 0.124176 (* 1 = 0.124176 loss)
I1122 10:42:07.477094 20880 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1122 10:42:11.752727 20880 solver.cpp:218] Iteration 15800 (23.3929 iter/s, 4.2748s/100 iters), loss = 0.163834
I1122 10:42:11.752727 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:42:11.752727 20880 solver.cpp:237]     Train net output #1: loss = 0.163834 (* 1 = 0.163834 loss)
I1122 10:42:11.752727 20880 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1122 10:42:16.030580 20880 solver.cpp:218] Iteration 15900 (23.3792 iter/s, 4.27731s/100 iters), loss = 0.12356
I1122 10:42:16.030580 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:42:16.030580 20880 solver.cpp:237]     Train net output #1: loss = 0.12356 (* 1 = 0.12356 loss)
I1122 10:42:16.030580 20880 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1122 10:42:20.092655 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:42:20.260645 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_16000.caffemodel
I1122 10:42:20.270630 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_16000.solverstate
I1122 10:42:20.275629 20880 solver.cpp:330] Iteration 16000, Testing net (#0)
I1122 10:42:20.275629 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:42:21.335641 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:42:21.377063 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9021
I1122 10:42:21.377063 20880 solver.cpp:397]     Test net output #1: loss = 0.298775 (* 1 = 0.298775 loss)
I1122 10:42:21.418081 20880 solver.cpp:218] Iteration 16000 (18.5613 iter/s, 5.38756s/100 iters), loss = 0.137534
I1122 10:42:21.418081 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:42:21.418081 20880 solver.cpp:237]     Train net output #1: loss = 0.137534 (* 1 = 0.137534 loss)
I1122 10:42:21.418081 20880 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1122 10:42:25.690868 20880 solver.cpp:218] Iteration 16100 (23.4063 iter/s, 4.27235s/100 iters), loss = 0.218622
I1122 10:42:25.690868 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:42:25.690868 20880 solver.cpp:237]     Train net output #1: loss = 0.218622 (* 1 = 0.218622 loss)
I1122 10:42:25.690868 20880 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1122 10:42:29.964869 20880 solver.cpp:218] Iteration 16200 (23.3984 iter/s, 4.2738s/100 iters), loss = 0.133285
I1122 10:42:29.964869 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:42:29.964869 20880 solver.cpp:237]     Train net output #1: loss = 0.133285 (* 1 = 0.133285 loss)
I1122 10:42:29.964869 20880 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1122 10:42:34.239706 20880 solver.cpp:218] Iteration 16300 (23.3979 iter/s, 4.27388s/100 iters), loss = 0.181914
I1122 10:42:34.239706 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:42:34.239706 20880 solver.cpp:237]     Train net output #1: loss = 0.181914 (* 1 = 0.181914 loss)
I1122 10:42:34.239706 20880 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1122 10:42:38.509816 20880 solver.cpp:218] Iteration 16400 (23.4176 iter/s, 4.27029s/100 iters), loss = 0.0704353
I1122 10:42:38.509816 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:42:38.509816 20880 solver.cpp:237]     Train net output #1: loss = 0.0704354 (* 1 = 0.0704354 loss)
I1122 10:42:38.509816 20880 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1122 10:42:42.567436 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:42:42.735014 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_16500.caffemodel
I1122 10:42:42.744994 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_16500.solverstate
I1122 10:42:42.748993 20880 solver.cpp:330] Iteration 16500, Testing net (#0)
I1122 10:42:42.748993 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:42:43.807803 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:42:43.849794 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9019
I1122 10:42:43.849794 20880 solver.cpp:397]     Test net output #1: loss = 0.298355 (* 1 = 0.298355 loss)
I1122 10:42:43.890823 20880 solver.cpp:218] Iteration 16500 (18.5857 iter/s, 5.38047s/100 iters), loss = 0.130334
I1122 10:42:43.891329 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:42:43.891329 20880 solver.cpp:237]     Train net output #1: loss = 0.130334 (* 1 = 0.130334 loss)
I1122 10:42:43.891329 20880 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1122 10:42:48.157333 20880 solver.cpp:218] Iteration 16600 (23.4415 iter/s, 4.26593s/100 iters), loss = 0.227886
I1122 10:42:48.157333 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:42:48.157333 20880 solver.cpp:237]     Train net output #1: loss = 0.227886 (* 1 = 0.227886 loss)
I1122 10:42:48.157333 20880 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1122 10:42:52.415591 20880 solver.cpp:218] Iteration 16700 (23.4846 iter/s, 4.25812s/100 iters), loss = 0.145552
I1122 10:42:52.415591 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:42:52.415591 20880 solver.cpp:237]     Train net output #1: loss = 0.145552 (* 1 = 0.145552 loss)
I1122 10:42:52.415591 20880 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1122 10:42:56.677871 20880 solver.cpp:218] Iteration 16800 (23.4642 iter/s, 4.26181s/100 iters), loss = 0.165253
I1122 10:42:56.677871 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:42:56.677871 20880 solver.cpp:237]     Train net output #1: loss = 0.165253 (* 1 = 0.165253 loss)
I1122 10:42:56.677871 20880 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1122 10:43:00.945212 20880 solver.cpp:218] Iteration 16900 (23.4327 iter/s, 4.26755s/100 iters), loss = 0.0808481
I1122 10:43:00.945212 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:43:00.945212 20880 solver.cpp:237]     Train net output #1: loss = 0.0808482 (* 1 = 0.0808482 loss)
I1122 10:43:00.946213 20880 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1122 10:43:05.002214 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:43:05.168781 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_17000.caffemodel
I1122 10:43:05.178772 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_17000.solverstate
I1122 10:43:05.182775 20880 solver.cpp:330] Iteration 17000, Testing net (#0)
I1122 10:43:05.182775 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:43:06.243242 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:43:06.285243 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9019
I1122 10:43:06.285243 20880 solver.cpp:397]     Test net output #1: loss = 0.29835 (* 1 = 0.29835 loss)
I1122 10:43:06.326278 20880 solver.cpp:218] Iteration 17000 (18.5868 iter/s, 5.38015s/100 iters), loss = 0.114114
I1122 10:43:06.326278 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:43:06.326278 20880 solver.cpp:237]     Train net output #1: loss = 0.114114 (* 1 = 0.114114 loss)
I1122 10:43:06.326278 20880 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1122 10:43:10.603555 20880 solver.cpp:218] Iteration 17100 (23.3826 iter/s, 4.27669s/100 iters), loss = 0.193517
I1122 10:43:10.603555 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:43:10.603555 20880 solver.cpp:237]     Train net output #1: loss = 0.193517 (* 1 = 0.193517 loss)
I1122 10:43:10.603555 20880 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1122 10:43:14.871489 20880 solver.cpp:218] Iteration 17200 (23.4334 iter/s, 4.26741s/100 iters), loss = 0.176684
I1122 10:43:14.871489 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:43:14.871489 20880 solver.cpp:237]     Train net output #1: loss = 0.176684 (* 1 = 0.176684 loss)
I1122 10:43:14.871489 20880 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1122 10:43:19.142036 20880 solver.cpp:218] Iteration 17300 (23.4179 iter/s, 4.27024s/100 iters), loss = 0.15278
I1122 10:43:19.142036 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:43:19.142036 20880 solver.cpp:237]     Train net output #1: loss = 0.152781 (* 1 = 0.152781 loss)
I1122 10:43:19.142036 20880 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1122 10:43:23.407197 20880 solver.cpp:218] Iteration 17400 (23.4443 iter/s, 4.26543s/100 iters), loss = 0.0850772
I1122 10:43:23.407197 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:43:23.407197 20880 solver.cpp:237]     Train net output #1: loss = 0.0850773 (* 1 = 0.0850773 loss)
I1122 10:43:23.407197 20880 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1122 10:43:27.467892 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:43:27.636241 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_17500.caffemodel
I1122 10:43:27.645241 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_17500.solverstate
I1122 10:43:27.649747 20880 solver.cpp:330] Iteration 17500, Testing net (#0)
I1122 10:43:27.649747 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:43:28.709861 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:43:28.751864 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9012
I1122 10:43:28.751864 20880 solver.cpp:397]     Test net output #1: loss = 0.298484 (* 1 = 0.298484 loss)
I1122 10:43:28.792472 20880 solver.cpp:218] Iteration 17500 (18.5719 iter/s, 5.38448s/100 iters), loss = 0.118694
I1122 10:43:28.792472 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:43:28.792472 20880 solver.cpp:237]     Train net output #1: loss = 0.118694 (* 1 = 0.118694 loss)
I1122 10:43:28.792472 20880 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1122 10:43:33.065487 20880 solver.cpp:218] Iteration 17600 (23.4029 iter/s, 4.27297s/100 iters), loss = 0.199353
I1122 10:43:33.065487 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:43:33.065487 20880 solver.cpp:237]     Train net output #1: loss = 0.199353 (* 1 = 0.199353 loss)
I1122 10:43:33.065487 20880 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1122 10:43:37.343850 20880 solver.cpp:218] Iteration 17700 (23.3761 iter/s, 4.27788s/100 iters), loss = 0.165122
I1122 10:43:37.343850 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:43:37.343850 20880 solver.cpp:237]     Train net output #1: loss = 0.165122 (* 1 = 0.165122 loss)
I1122 10:43:37.343850 20880 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1122 10:43:41.616891 20880 solver.cpp:218] Iteration 17800 (23.4064 iter/s, 4.27233s/100 iters), loss = 0.148213
I1122 10:43:41.616891 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:43:41.616891 20880 solver.cpp:237]     Train net output #1: loss = 0.148213 (* 1 = 0.148213 loss)
I1122 10:43:41.616891 20880 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1122 10:43:45.891510 20880 solver.cpp:218] Iteration 17900 (23.3952 iter/s, 4.27439s/100 iters), loss = 0.0692609
I1122 10:43:45.891510 20880 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 10:43:45.891510 20880 solver.cpp:237]     Train net output #1: loss = 0.069261 (* 1 = 0.069261 loss)
I1122 10:43:45.891510 20880 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1122 10:43:49.960006 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:43:50.126065 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_18000.caffemodel
I1122 10:43:50.137063 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_18000.solverstate
I1122 10:43:50.141062 20880 solver.cpp:330] Iteration 18000, Testing net (#0)
I1122 10:43:50.141062 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:43:51.201185 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:43:51.242185 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9019
I1122 10:43:51.242185 20880 solver.cpp:397]     Test net output #1: loss = 0.298514 (* 1 = 0.298514 loss)
I1122 10:43:51.283205 20880 solver.cpp:218] Iteration 18000 (18.5477 iter/s, 5.39149s/100 iters), loss = 0.124832
I1122 10:43:51.283205 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:43:51.283205 20880 solver.cpp:237]     Train net output #1: loss = 0.124832 (* 1 = 0.124832 loss)
I1122 10:43:51.283205 20880 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1122 10:43:55.551071 20880 solver.cpp:218] Iteration 18100 (23.4347 iter/s, 4.26718s/100 iters), loss = 0.181561
I1122 10:43:55.551071 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:43:55.551071 20880 solver.cpp:237]     Train net output #1: loss = 0.181561 (* 1 = 0.181561 loss)
I1122 10:43:55.551071 20880 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1122 10:43:59.820588 20880 solver.cpp:218] Iteration 18200 (23.4233 iter/s, 4.26925s/100 iters), loss = 0.121074
I1122 10:43:59.820588 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:43:59.820588 20880 solver.cpp:237]     Train net output #1: loss = 0.121075 (* 1 = 0.121075 loss)
I1122 10:43:59.820588 20880 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1122 10:44:04.094404 20880 solver.cpp:218] Iteration 18300 (23.3964 iter/s, 4.27415s/100 iters), loss = 0.180406
I1122 10:44:04.095407 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:44:04.095407 20880 solver.cpp:237]     Train net output #1: loss = 0.180406 (* 1 = 0.180406 loss)
I1122 10:44:04.095407 20880 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1122 10:44:08.371371 20880 solver.cpp:218] Iteration 18400 (23.3874 iter/s, 4.2758s/100 iters), loss = 0.0987381
I1122 10:44:08.371371 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:44:08.371371 20880 solver.cpp:237]     Train net output #1: loss = 0.0987382 (* 1 = 0.0987382 loss)
I1122 10:44:08.371371 20880 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1122 10:44:12.430255 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:44:12.598294 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_18500.caffemodel
I1122 10:44:12.608288 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_18500.solverstate
I1122 10:44:12.612288 20880 solver.cpp:330] Iteration 18500, Testing net (#0)
I1122 10:44:12.612288 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:44:13.672595 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:44:13.714092 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9016
I1122 10:44:13.714092 20880 solver.cpp:397]     Test net output #1: loss = 0.298168 (* 1 = 0.298168 loss)
I1122 10:44:13.754106 20880 solver.cpp:218] Iteration 18500 (18.577 iter/s, 5.38299s/100 iters), loss = 0.132072
I1122 10:44:13.754106 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:44:13.754106 20880 solver.cpp:237]     Train net output #1: loss = 0.132072 (* 1 = 0.132072 loss)
I1122 10:44:13.754106 20880 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1122 10:44:18.016444 20880 solver.cpp:218] Iteration 18600 (23.4663 iter/s, 4.26144s/100 iters), loss = 0.226233
I1122 10:44:18.016444 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:44:18.016444 20880 solver.cpp:237]     Train net output #1: loss = 0.226233 (* 1 = 0.226233 loss)
I1122 10:44:18.016444 20880 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1122 10:44:22.277614 20880 solver.cpp:218] Iteration 18700 (23.4698 iter/s, 4.26079s/100 iters), loss = 0.103621
I1122 10:44:22.277614 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:44:22.277614 20880 solver.cpp:237]     Train net output #1: loss = 0.103621 (* 1 = 0.103621 loss)
I1122 10:44:22.277614 20880 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1122 10:44:26.541466 20880 solver.cpp:218] Iteration 18800 (23.4536 iter/s, 4.26374s/100 iters), loss = 0.141253
I1122 10:44:26.541466 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:44:26.541466 20880 solver.cpp:237]     Train net output #1: loss = 0.141253 (* 1 = 0.141253 loss)
I1122 10:44:26.541466 20880 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1122 10:44:30.802458 20880 solver.cpp:218] Iteration 18900 (23.4685 iter/s, 4.26104s/100 iters), loss = 0.081116
I1122 10:44:30.803460 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:44:30.803460 20880 solver.cpp:237]     Train net output #1: loss = 0.0811161 (* 1 = 0.0811161 loss)
I1122 10:44:30.803460 20880 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1122 10:44:34.853384 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:44:35.021922 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_19000.caffemodel
I1122 10:44:35.030921 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_19000.solverstate
I1122 10:44:35.034921 20880 solver.cpp:330] Iteration 19000, Testing net (#0)
I1122 10:44:35.034921 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:44:36.095556 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:44:36.136564 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9015
I1122 10:44:36.136564 20880 solver.cpp:397]     Test net output #1: loss = 0.298232 (* 1 = 0.298232 loss)
I1122 10:44:36.178568 20880 solver.cpp:218] Iteration 19000 (18.6051 iter/s, 5.37487s/100 iters), loss = 0.181034
I1122 10:44:36.178568 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 10:44:36.178568 20880 solver.cpp:237]     Train net output #1: loss = 0.181034 (* 1 = 0.181034 loss)
I1122 10:44:36.178568 20880 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1122 10:44:40.454637 20880 solver.cpp:218] Iteration 19100 (23.3888 iter/s, 4.27555s/100 iters), loss = 0.191469
I1122 10:44:40.454637 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:44:40.454637 20880 solver.cpp:237]     Train net output #1: loss = 0.191469 (* 1 = 0.191469 loss)
I1122 10:44:40.454637 20880 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1122 10:44:44.727676 20880 solver.cpp:218] Iteration 19200 (23.4043 iter/s, 4.27271s/100 iters), loss = 0.12722
I1122 10:44:44.727676 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:44:44.727676 20880 solver.cpp:237]     Train net output #1: loss = 0.12722 (* 1 = 0.12722 loss)
I1122 10:44:44.727676 20880 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1122 10:44:49.001246 20880 solver.cpp:218] Iteration 19300 (23.4003 iter/s, 4.27345s/100 iters), loss = 0.118251
I1122 10:44:49.001246 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:44:49.001246 20880 solver.cpp:237]     Train net output #1: loss = 0.118251 (* 1 = 0.118251 loss)
I1122 10:44:49.001246 20880 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1122 10:44:53.275092 20880 solver.cpp:218] Iteration 19400 (23.4007 iter/s, 4.27337s/100 iters), loss = 0.0793041
I1122 10:44:53.275092 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:44:53.275092 20880 solver.cpp:237]     Train net output #1: loss = 0.0793042 (* 1 = 0.0793042 loss)
I1122 10:44:53.275092 20880 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1122 10:44:57.346612 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:44:57.515175 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_19500.caffemodel
I1122 10:44:57.524160 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_19500.solverstate
I1122 10:44:57.529160 20880 solver.cpp:330] Iteration 19500, Testing net (#0)
I1122 10:44:57.529160 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:44:58.588018 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:44:58.630043 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9013
I1122 10:44:58.630043 20880 solver.cpp:397]     Test net output #1: loss = 0.298053 (* 1 = 0.298053 loss)
I1122 10:44:58.671052 20880 solver.cpp:218] Iteration 19500 (18.5329 iter/s, 5.3958s/100 iters), loss = 0.0991523
I1122 10:44:58.671052 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:44:58.671052 20880 solver.cpp:237]     Train net output #1: loss = 0.0991523 (* 1 = 0.0991523 loss)
I1122 10:44:58.671052 20880 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1122 10:44:58.671052 20880 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1122 10:45:02.931121 20880 solver.cpp:218] Iteration 19600 (23.4755 iter/s, 4.25976s/100 iters), loss = 0.142267
I1122 10:45:02.931121 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:45:02.931121 20880 solver.cpp:237]     Train net output #1: loss = 0.142267 (* 1 = 0.142267 loss)
I1122 10:45:02.931121 20880 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1122 10:45:07.194083 20880 solver.cpp:218] Iteration 19700 (23.4593 iter/s, 4.26271s/100 iters), loss = 0.12261
I1122 10:45:07.194083 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:45:07.194083 20880 solver.cpp:237]     Train net output #1: loss = 0.12261 (* 1 = 0.12261 loss)
I1122 10:45:07.194083 20880 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1122 10:45:11.453455 20880 solver.cpp:218] Iteration 19800 (23.4806 iter/s, 4.25883s/100 iters), loss = 0.122548
I1122 10:45:11.453455 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:45:11.453455 20880 solver.cpp:237]     Train net output #1: loss = 0.122548 (* 1 = 0.122548 loss)
I1122 10:45:11.453455 20880 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1122 10:45:15.715606 20880 solver.cpp:218] Iteration 19900 (23.4663 iter/s, 4.26143s/100 iters), loss = 0.0753199
I1122 10:45:15.715606 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:45:15.715606 20880 solver.cpp:237]     Train net output #1: loss = 0.07532 (* 1 = 0.07532 loss)
I1122 10:45:15.715606 20880 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1122 10:45:19.764627 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:45:19.931166 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_20000.caffemodel
I1122 10:45:19.941157 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_20000.solverstate
I1122 10:45:19.945158 20880 solver.cpp:330] Iteration 20000, Testing net (#0)
I1122 10:45:19.945158 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:45:21.004423 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:45:21.045450 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9011
I1122 10:45:21.045450 20880 solver.cpp:397]     Test net output #1: loss = 0.298108 (* 1 = 0.298108 loss)
I1122 10:45:21.086449 20880 solver.cpp:218] Iteration 20000 (18.6176 iter/s, 5.37126s/100 iters), loss = 0.144923
I1122 10:45:21.087455 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:45:21.087455 20880 solver.cpp:237]     Train net output #1: loss = 0.144923 (* 1 = 0.144923 loss)
I1122 10:45:21.087455 20880 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1122 10:45:25.358510 20880 solver.cpp:218] Iteration 20100 (23.4151 iter/s, 4.27074s/100 iters), loss = 0.14434
I1122 10:45:25.358510 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:45:25.358510 20880 solver.cpp:237]     Train net output #1: loss = 0.14434 (* 1 = 0.14434 loss)
I1122 10:45:25.358510 20880 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1122 10:45:29.628213 20880 solver.cpp:218] Iteration 20200 (23.4207 iter/s, 4.26972s/100 iters), loss = 0.13946
I1122 10:45:29.628213 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:45:29.628213 20880 solver.cpp:237]     Train net output #1: loss = 0.139461 (* 1 = 0.139461 loss)
I1122 10:45:29.628213 20880 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1122 10:45:33.898797 20880 solver.cpp:218] Iteration 20300 (23.4194 iter/s, 4.26996s/100 iters), loss = 0.159215
I1122 10:45:33.898797 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:45:33.898797 20880 solver.cpp:237]     Train net output #1: loss = 0.159215 (* 1 = 0.159215 loss)
I1122 10:45:33.898797 20880 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1122 10:45:38.167845 20880 solver.cpp:218] Iteration 20400 (23.4234 iter/s, 4.26924s/100 iters), loss = 0.0865735
I1122 10:45:38.167845 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:45:38.167845 20880 solver.cpp:237]     Train net output #1: loss = 0.0865735 (* 1 = 0.0865735 loss)
I1122 10:45:38.167845 20880 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1122 10:45:42.230809 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:45:42.397815 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_20500.caffemodel
I1122 10:45:42.409318 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_20500.solverstate
I1122 10:45:42.413317 20880 solver.cpp:330] Iteration 20500, Testing net (#0)
I1122 10:45:42.413317 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:45:43.472050 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:45:43.514050 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9012
I1122 10:45:43.514050 20880 solver.cpp:397]     Test net output #1: loss = 0.298112 (* 1 = 0.298112 loss)
I1122 10:45:43.554566 20880 solver.cpp:218] Iteration 20500 (18.5658 iter/s, 5.38625s/100 iters), loss = 0.147521
I1122 10:45:43.554566 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:45:43.554566 20880 solver.cpp:237]     Train net output #1: loss = 0.147521 (* 1 = 0.147521 loss)
I1122 10:45:43.554566 20880 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1122 10:45:47.824522 20880 solver.cpp:218] Iteration 20600 (23.4232 iter/s, 4.26927s/100 iters), loss = 0.157397
I1122 10:45:47.824522 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:45:47.824522 20880 solver.cpp:237]     Train net output #1: loss = 0.157397 (* 1 = 0.157397 loss)
I1122 10:45:47.824522 20880 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1122 10:45:52.096127 20880 solver.cpp:218] Iteration 20700 (23.4094 iter/s, 4.27179s/100 iters), loss = 0.119301
I1122 10:45:52.097143 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:45:52.097143 20880 solver.cpp:237]     Train net output #1: loss = 0.119301 (* 1 = 0.119301 loss)
I1122 10:45:52.097143 20880 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1122 10:45:56.372026 20880 solver.cpp:218] Iteration 20800 (23.3936 iter/s, 4.27468s/100 iters), loss = 0.139604
I1122 10:45:56.372026 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:45:56.372026 20880 solver.cpp:237]     Train net output #1: loss = 0.139604 (* 1 = 0.139604 loss)
I1122 10:45:56.372026 20880 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1122 10:46:00.644680 20880 solver.cpp:218] Iteration 20900 (23.4041 iter/s, 4.27276s/100 iters), loss = 0.0828012
I1122 10:46:00.644680 20880 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 10:46:00.644680 20880 solver.cpp:237]     Train net output #1: loss = 0.0828013 (* 1 = 0.0828013 loss)
I1122 10:46:00.644680 20880 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1122 10:46:04.712879 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:46:04.879925 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_21000.caffemodel
I1122 10:46:04.889926 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_21000.solverstate
I1122 10:46:04.893929 20880 solver.cpp:330] Iteration 21000, Testing net (#0)
I1122 10:46:04.893929 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:46:05.953603 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:46:05.995604 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9013
I1122 10:46:05.995604 20880 solver.cpp:397]     Test net output #1: loss = 0.298198 (* 1 = 0.298198 loss)
I1122 10:46:06.037622 20880 solver.cpp:218] Iteration 21000 (18.5459 iter/s, 5.39203s/100 iters), loss = 0.140605
I1122 10:46:06.037622 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:46:06.037622 20880 solver.cpp:237]     Train net output #1: loss = 0.140605 (* 1 = 0.140605 loss)
I1122 10:46:06.037622 20880 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1122 10:46:10.303845 20880 solver.cpp:218] Iteration 21100 (23.4385 iter/s, 4.26648s/100 iters), loss = 0.196942
I1122 10:46:10.303845 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:46:10.303845 20880 solver.cpp:237]     Train net output #1: loss = 0.196942 (* 1 = 0.196942 loss)
I1122 10:46:10.303845 20880 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1122 10:46:14.575839 20880 solver.cpp:218] Iteration 21200 (23.4111 iter/s, 4.27147s/100 iters), loss = 0.111353
I1122 10:46:14.575839 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:46:14.575839 20880 solver.cpp:237]     Train net output #1: loss = 0.111353 (* 1 = 0.111353 loss)
I1122 10:46:14.575839 20880 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1122 10:46:18.850807 20880 solver.cpp:218] Iteration 21300 (23.3956 iter/s, 4.27431s/100 iters), loss = 0.187804
I1122 10:46:18.850807 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:46:18.850807 20880 solver.cpp:237]     Train net output #1: loss = 0.187804 (* 1 = 0.187804 loss)
I1122 10:46:18.850807 20880 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1122 10:46:23.117766 20880 solver.cpp:218] Iteration 21400 (23.4362 iter/s, 4.2669s/100 iters), loss = 0.0584946
I1122 10:46:23.117766 20880 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 10:46:23.117766 20880 solver.cpp:237]     Train net output #1: loss = 0.0584947 (* 1 = 0.0584947 loss)
I1122 10:46:23.117766 20880 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1122 10:46:27.182806 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:46:27.350821 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_21500.caffemodel
I1122 10:46:27.365835 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_21500.solverstate
I1122 10:46:27.369834 20880 solver.cpp:330] Iteration 21500, Testing net (#0)
I1122 10:46:27.370333 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:46:28.428859 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:46:28.470367 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9013
I1122 10:46:28.470367 20880 solver.cpp:397]     Test net output #1: loss = 0.298223 (* 1 = 0.298223 loss)
I1122 10:46:28.510874 20880 solver.cpp:218] Iteration 21500 (18.5442 iter/s, 5.39251s/100 iters), loss = 0.131004
I1122 10:46:28.510874 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:46:28.510874 20880 solver.cpp:237]     Train net output #1: loss = 0.131004 (* 1 = 0.131004 loss)
I1122 10:46:28.510874 20880 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1122 10:46:32.768352 20880 solver.cpp:218] Iteration 21600 (23.4916 iter/s, 4.25684s/100 iters), loss = 0.223572
I1122 10:46:32.768352 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:46:32.768352 20880 solver.cpp:237]     Train net output #1: loss = 0.223572 (* 1 = 0.223572 loss)
I1122 10:46:32.768352 20880 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1122 10:46:37.032168 20880 solver.cpp:218] Iteration 21700 (23.4536 iter/s, 4.26374s/100 iters), loss = 0.141829
I1122 10:46:37.032168 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:46:37.032168 20880 solver.cpp:237]     Train net output #1: loss = 0.141829 (* 1 = 0.141829 loss)
I1122 10:46:37.032168 20880 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1122 10:46:41.292565 20880 solver.cpp:218] Iteration 21800 (23.4757 iter/s, 4.25972s/100 iters), loss = 0.200034
I1122 10:46:41.292565 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 10:46:41.292565 20880 solver.cpp:237]     Train net output #1: loss = 0.200034 (* 1 = 0.200034 loss)
I1122 10:46:41.292565 20880 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1122 10:46:45.551852 20880 solver.cpp:218] Iteration 21900 (23.4778 iter/s, 4.25934s/100 iters), loss = 0.0833719
I1122 10:46:45.551852 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:46:45.551852 20880 solver.cpp:237]     Train net output #1: loss = 0.083372 (* 1 = 0.083372 loss)
I1122 10:46:45.551852 20880 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1122 10:46:49.603845 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:46:49.770898 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_22000.caffemodel
I1122 10:46:49.782399 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_22000.solverstate
I1122 10:46:49.786535 20880 solver.cpp:330] Iteration 22000, Testing net (#0)
I1122 10:46:49.786535 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:46:50.847335 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:46:50.888360 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9011
I1122 10:46:50.889346 20880 solver.cpp:397]     Test net output #1: loss = 0.298179 (* 1 = 0.298179 loss)
I1122 10:46:50.930354 20880 solver.cpp:218] Iteration 22000 (18.5957 iter/s, 5.37758s/100 iters), loss = 0.129401
I1122 10:46:50.930354 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:46:50.930354 20880 solver.cpp:237]     Train net output #1: loss = 0.129401 (* 1 = 0.129401 loss)
I1122 10:46:50.930354 20880 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1122 10:46:50.930354 20880 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1122 10:46:55.204485 20880 solver.cpp:218] Iteration 22100 (23.397 iter/s, 4.27404s/100 iters), loss = 0.173939
I1122 10:46:55.204485 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:46:55.204485 20880 solver.cpp:237]     Train net output #1: loss = 0.173939 (* 1 = 0.173939 loss)
I1122 10:46:55.204485 20880 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1122 10:46:59.476617 20880 solver.cpp:218] Iteration 22200 (23.4104 iter/s, 4.27161s/100 iters), loss = 0.139416
I1122 10:46:59.476617 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:46:59.476617 20880 solver.cpp:237]     Train net output #1: loss = 0.139416 (* 1 = 0.139416 loss)
I1122 10:46:59.476617 20880 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1122 10:47:03.751546 20880 solver.cpp:218] Iteration 22300 (23.3931 iter/s, 4.27477s/100 iters), loss = 0.130377
I1122 10:47:03.751546 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:47:03.751546 20880 solver.cpp:237]     Train net output #1: loss = 0.130377 (* 1 = 0.130377 loss)
I1122 10:47:03.751546 20880 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1122 10:47:08.023995 20880 solver.cpp:218] Iteration 22400 (23.4111 iter/s, 4.27148s/100 iters), loss = 0.0616136
I1122 10:47:08.023995 20880 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 10:47:08.023995 20880 solver.cpp:237]     Train net output #1: loss = 0.0616136 (* 1 = 0.0616136 loss)
I1122 10:47:08.023995 20880 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1122 10:47:12.085587 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:47:12.253603 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_22500.caffemodel
I1122 10:47:12.264609 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_22500.solverstate
I1122 10:47:12.268607 20880 solver.cpp:330] Iteration 22500, Testing net (#0)
I1122 10:47:12.268607 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:47:13.328382 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:47:13.370390 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9011
I1122 10:47:13.370390 20880 solver.cpp:397]     Test net output #1: loss = 0.298205 (* 1 = 0.298205 loss)
I1122 10:47:13.410408 20880 solver.cpp:218] Iteration 22500 (18.564 iter/s, 5.38676s/100 iters), loss = 0.163397
I1122 10:47:13.410408 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:47:13.410408 20880 solver.cpp:237]     Train net output #1: loss = 0.163397 (* 1 = 0.163397 loss)
I1122 10:47:13.410408 20880 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1122 10:47:17.681424 20880 solver.cpp:218] Iteration 22600 (23.4172 iter/s, 4.27036s/100 iters), loss = 0.185149
I1122 10:47:17.681424 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:47:17.681424 20880 solver.cpp:237]     Train net output #1: loss = 0.185149 (* 1 = 0.185149 loss)
I1122 10:47:17.681424 20880 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1122 10:47:21.949084 20880 solver.cpp:218] Iteration 22700 (23.4321 iter/s, 4.26764s/100 iters), loss = 0.117104
I1122 10:47:21.949084 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:47:21.950084 20880 solver.cpp:237]     Train net output #1: loss = 0.117104 (* 1 = 0.117104 loss)
I1122 10:47:21.950084 20880 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1122 10:47:26.225565 20880 solver.cpp:218] Iteration 22800 (23.3868 iter/s, 4.27592s/100 iters), loss = 0.150099
I1122 10:47:26.225565 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:47:26.225565 20880 solver.cpp:237]     Train net output #1: loss = 0.1501 (* 1 = 0.1501 loss)
I1122 10:47:26.225565 20880 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1122 10:47:30.500450 20880 solver.cpp:218] Iteration 22900 (23.3951 iter/s, 4.2744s/100 iters), loss = 0.0851827
I1122 10:47:30.500450 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:47:30.500450 20880 solver.cpp:237]     Train net output #1: loss = 0.0851829 (* 1 = 0.0851829 loss)
I1122 10:47:30.500450 20880 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1122 10:47:34.565215 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:47:34.730362 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_23000.caffemodel
I1122 10:47:34.741348 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_23000.solverstate
I1122 10:47:34.745348 20880 solver.cpp:330] Iteration 23000, Testing net (#0)
I1122 10:47:34.745348 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:47:35.804361 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:47:35.846374 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9012
I1122 10:47:35.846374 20880 solver.cpp:397]     Test net output #1: loss = 0.298173 (* 1 = 0.298173 loss)
I1122 10:47:35.887398 20880 solver.cpp:218] Iteration 23000 (18.5662 iter/s, 5.38613s/100 iters), loss = 0.14613
I1122 10:47:35.887398 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:47:35.887398 20880 solver.cpp:237]     Train net output #1: loss = 0.14613 (* 1 = 0.14613 loss)
I1122 10:47:35.887398 20880 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1122 10:47:40.160318 20880 solver.cpp:218] Iteration 23100 (23.4017 iter/s, 4.2732s/100 iters), loss = 0.15877
I1122 10:47:40.161319 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:47:40.161319 20880 solver.cpp:237]     Train net output #1: loss = 0.15877 (* 1 = 0.15877 loss)
I1122 10:47:40.161319 20880 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1122 10:47:44.431819 20880 solver.cpp:218] Iteration 23200 (23.4138 iter/s, 4.27099s/100 iters), loss = 0.114833
I1122 10:47:44.431819 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:47:44.431819 20880 solver.cpp:237]     Train net output #1: loss = 0.114834 (* 1 = 0.114834 loss)
I1122 10:47:44.431819 20880 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1122 10:47:48.698437 20880 solver.cpp:218] Iteration 23300 (23.4413 iter/s, 4.26598s/100 iters), loss = 0.156644
I1122 10:47:48.698437 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:47:48.698437 20880 solver.cpp:237]     Train net output #1: loss = 0.156644 (* 1 = 0.156644 loss)
I1122 10:47:48.698437 20880 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1122 10:47:52.968920 20880 solver.cpp:218] Iteration 23400 (23.4168 iter/s, 4.27043s/100 iters), loss = 0.113664
I1122 10:47:52.968920 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:47:52.968920 20880 solver.cpp:237]     Train net output #1: loss = 0.113664 (* 1 = 0.113664 loss)
I1122 10:47:52.968920 20880 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1122 10:47:57.032627 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:47:57.200812 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_23500.caffemodel
I1122 10:47:57.210803 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_23500.solverstate
I1122 10:47:57.214802 20880 solver.cpp:330] Iteration 23500, Testing net (#0)
I1122 10:47:57.214802 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:47:58.274361 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:47:58.316372 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9009
I1122 10:47:58.316372 20880 solver.cpp:397]     Test net output #1: loss = 0.298134 (* 1 = 0.298134 loss)
I1122 10:47:58.357378 20880 solver.cpp:218] Iteration 23500 (18.5601 iter/s, 5.38791s/100 iters), loss = 0.110959
I1122 10:47:58.357378 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:47:58.357378 20880 solver.cpp:237]     Train net output #1: loss = 0.11096 (* 1 = 0.11096 loss)
I1122 10:47:58.357378 20880 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1122 10:48:02.621647 20880 solver.cpp:218] Iteration 23600 (23.4562 iter/s, 4.26326s/100 iters), loss = 0.155556
I1122 10:48:02.621647 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:48:02.621647 20880 solver.cpp:237]     Train net output #1: loss = 0.155556 (* 1 = 0.155556 loss)
I1122 10:48:02.621647 20880 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1122 10:48:06.881794 20880 solver.cpp:218] Iteration 23700 (23.4773 iter/s, 4.25943s/100 iters), loss = 0.136592
I1122 10:48:06.881794 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:48:06.881794 20880 solver.cpp:237]     Train net output #1: loss = 0.136592 (* 1 = 0.136592 loss)
I1122 10:48:06.881794 20880 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1122 10:48:11.145864 20880 solver.cpp:218] Iteration 23800 (23.4541 iter/s, 4.26364s/100 iters), loss = 0.122716
I1122 10:48:11.145864 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:48:11.145864 20880 solver.cpp:237]     Train net output #1: loss = 0.122716 (* 1 = 0.122716 loss)
I1122 10:48:11.145864 20880 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1122 10:48:15.406445 20880 solver.cpp:218] Iteration 23900 (23.4731 iter/s, 4.2602s/100 iters), loss = 0.113762
I1122 10:48:15.406445 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:48:15.406445 20880 solver.cpp:237]     Train net output #1: loss = 0.113762 (* 1 = 0.113762 loss)
I1122 10:48:15.406445 20880 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1122 10:48:19.458523 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:48:19.623559 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_24000.caffemodel
I1122 10:48:19.633559 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_24000.solverstate
I1122 10:48:19.638557 20880 solver.cpp:330] Iteration 24000, Testing net (#0)
I1122 10:48:19.638557 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:48:20.697252 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:48:20.738752 20880 solver.cpp:397]     Test net output #0: accuracy = 0.901
I1122 10:48:20.738752 20880 solver.cpp:397]     Test net output #1: loss = 0.298201 (* 1 = 0.298201 loss)
I1122 10:48:20.779764 20880 solver.cpp:218] Iteration 24000 (18.6105 iter/s, 5.37332s/100 iters), loss = 0.151268
I1122 10:48:20.779764 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:48:20.779764 20880 solver.cpp:237]     Train net output #1: loss = 0.151269 (* 1 = 0.151269 loss)
I1122 10:48:20.779764 20880 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1122 10:48:25.055980 20880 solver.cpp:218] Iteration 24100 (23.3887 iter/s, 4.27557s/100 iters), loss = 0.205186
I1122 10:48:25.055980 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:48:25.055980 20880 solver.cpp:237]     Train net output #1: loss = 0.205186 (* 1 = 0.205186 loss)
I1122 10:48:25.055980 20880 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1122 10:48:29.333071 20880 solver.cpp:218] Iteration 24200 (23.3781 iter/s, 4.27751s/100 iters), loss = 0.1563
I1122 10:48:29.333071 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:48:29.333071 20880 solver.cpp:237]     Train net output #1: loss = 0.1563 (* 1 = 0.1563 loss)
I1122 10:48:29.333071 20880 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1122 10:48:33.610544 20880 solver.cpp:218] Iteration 24300 (23.3827 iter/s, 4.27666s/100 iters), loss = 0.201397
I1122 10:48:33.610544 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:48:33.610544 20880 solver.cpp:237]     Train net output #1: loss = 0.201397 (* 1 = 0.201397 loss)
I1122 10:48:33.610544 20880 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1122 10:48:37.888633 20880 solver.cpp:218] Iteration 24400 (23.3744 iter/s, 4.27818s/100 iters), loss = 0.0895655
I1122 10:48:37.888633 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:48:37.888633 20880 solver.cpp:237]     Train net output #1: loss = 0.0895656 (* 1 = 0.0895656 loss)
I1122 10:48:37.888633 20880 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1122 10:48:41.952594 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:48:42.121626 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_24500.caffemodel
I1122 10:48:42.131630 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_24500.solverstate
I1122 10:48:42.135632 20880 solver.cpp:330] Iteration 24500, Testing net (#0)
I1122 10:48:42.135632 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:48:43.195703 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:48:43.237094 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9011
I1122 10:48:43.237094 20880 solver.cpp:397]     Test net output #1: loss = 0.298171 (* 1 = 0.298171 loss)
I1122 10:48:43.278108 20880 solver.cpp:218] Iteration 24500 (18.5578 iter/s, 5.38857s/100 iters), loss = 0.11769
I1122 10:48:43.278108 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:48:43.278108 20880 solver.cpp:237]     Train net output #1: loss = 0.11769 (* 1 = 0.11769 loss)
I1122 10:48:43.278108 20880 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1122 10:48:47.540920 20880 solver.cpp:218] Iteration 24600 (23.4597 iter/s, 4.26264s/100 iters), loss = 0.172287
I1122 10:48:47.540920 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:48:47.540920 20880 solver.cpp:237]     Train net output #1: loss = 0.172287 (* 1 = 0.172287 loss)
I1122 10:48:47.540920 20880 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1122 10:48:51.802052 20880 solver.cpp:218] Iteration 24700 (23.4693 iter/s, 4.26089s/100 iters), loss = 0.129702
I1122 10:48:51.802052 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:48:51.802052 20880 solver.cpp:237]     Train net output #1: loss = 0.129702 (* 1 = 0.129702 loss)
I1122 10:48:51.802052 20880 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1122 10:48:56.061462 20880 solver.cpp:218] Iteration 24800 (23.4808 iter/s, 4.2588s/100 iters), loss = 0.128965
I1122 10:48:56.061462 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:48:56.061462 20880 solver.cpp:237]     Train net output #1: loss = 0.128965 (* 1 = 0.128965 loss)
I1122 10:48:56.061462 20880 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1122 10:49:00.320225 20880 solver.cpp:218] Iteration 24900 (23.4839 iter/s, 4.25824s/100 iters), loss = 0.107725
I1122 10:49:00.320225 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:49:00.320225 20880 solver.cpp:237]     Train net output #1: loss = 0.107725 (* 1 = 0.107725 loss)
I1122 10:49:00.320225 20880 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1122 10:49:04.371060 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:49:04.539604 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_25000.caffemodel
I1122 10:49:04.550603 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_25000.solverstate
I1122 10:49:04.554603 20880 solver.cpp:330] Iteration 25000, Testing net (#0)
I1122 10:49:04.554603 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:49:05.612660 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:49:05.654655 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9011
I1122 10:49:05.654655 20880 solver.cpp:397]     Test net output #1: loss = 0.29822 (* 1 = 0.29822 loss)
I1122 10:49:05.696178 20880 solver.cpp:218] Iteration 25000 (18.6037 iter/s, 5.37527s/100 iters), loss = 0.133243
I1122 10:49:05.696178 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:49:05.696178 20880 solver.cpp:237]     Train net output #1: loss = 0.133243 (* 1 = 0.133243 loss)
I1122 10:49:05.696178 20880 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1122 10:49:09.963629 20880 solver.cpp:218] Iteration 25100 (23.4334 iter/s, 4.26742s/100 iters), loss = 0.167782
I1122 10:49:09.963629 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:49:09.963629 20880 solver.cpp:237]     Train net output #1: loss = 0.167782 (* 1 = 0.167782 loss)
I1122 10:49:09.963629 20880 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1122 10:49:14.234423 20880 solver.cpp:218] Iteration 25200 (23.4157 iter/s, 4.27065s/100 iters), loss = 0.121009
I1122 10:49:14.234423 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:49:14.234423 20880 solver.cpp:237]     Train net output #1: loss = 0.121009 (* 1 = 0.121009 loss)
I1122 10:49:14.234423 20880 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1122 10:49:18.505548 20880 solver.cpp:218] Iteration 25300 (23.4167 iter/s, 4.27045s/100 iters), loss = 0.145197
I1122 10:49:18.505548 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:49:18.505548 20880 solver.cpp:237]     Train net output #1: loss = 0.145197 (* 1 = 0.145197 loss)
I1122 10:49:18.505548 20880 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1122 10:49:22.774365 20880 solver.cpp:218] Iteration 25400 (23.4254 iter/s, 4.26888s/100 iters), loss = 0.105523
I1122 10:49:22.774365 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:49:22.774365 20880 solver.cpp:237]     Train net output #1: loss = 0.105523 (* 1 = 0.105523 loss)
I1122 10:49:22.774365 20880 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1122 10:49:26.833482 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:49:27.001570 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_25500.caffemodel
I1122 10:49:27.011554 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_25500.solverstate
I1122 10:49:27.015553 20880 solver.cpp:330] Iteration 25500, Testing net (#0)
I1122 10:49:27.015553 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:49:28.073665 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:49:28.114676 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9011
I1122 10:49:28.114676 20880 solver.cpp:397]     Test net output #1: loss = 0.298219 (* 1 = 0.298219 loss)
I1122 10:49:28.156191 20880 solver.cpp:218] Iteration 25500 (18.5836 iter/s, 5.3811s/100 iters), loss = 0.107077
I1122 10:49:28.156191 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:49:28.156191 20880 solver.cpp:237]     Train net output #1: loss = 0.107077 (* 1 = 0.107077 loss)
I1122 10:49:28.156191 20880 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1122 10:49:32.430814 20880 solver.cpp:218] Iteration 25600 (23.3937 iter/s, 4.27466s/100 iters), loss = 0.213473
I1122 10:49:32.430814 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:49:32.430814 20880 solver.cpp:237]     Train net output #1: loss = 0.213473 (* 1 = 0.213473 loss)
I1122 10:49:32.430814 20880 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1122 10:49:36.697769 20880 solver.cpp:218] Iteration 25700 (23.4389 iter/s, 4.26641s/100 iters), loss = 0.104427
I1122 10:49:36.697769 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:49:36.697769 20880 solver.cpp:237]     Train net output #1: loss = 0.104428 (* 1 = 0.104428 loss)
I1122 10:49:36.697769 20880 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1122 10:49:40.977017 20880 solver.cpp:218] Iteration 25800 (23.3703 iter/s, 4.27894s/100 iters), loss = 0.130334
I1122 10:49:40.977017 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:49:40.977017 20880 solver.cpp:237]     Train net output #1: loss = 0.130334 (* 1 = 0.130334 loss)
I1122 10:49:40.977017 20880 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1122 10:49:45.247023 20880 solver.cpp:218] Iteration 25900 (23.4204 iter/s, 4.26979s/100 iters), loss = 0.0881898
I1122 10:49:45.247023 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:49:45.247023 20880 solver.cpp:237]     Train net output #1: loss = 0.0881899 (* 1 = 0.0881899 loss)
I1122 10:49:45.247023 20880 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1122 10:49:49.306774 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:49:49.475287 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_26000.caffemodel
I1122 10:49:49.485323 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_26000.solverstate
I1122 10:49:49.489326 20880 solver.cpp:330] Iteration 26000, Testing net (#0)
I1122 10:49:49.489326 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:49:50.553079 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:49:50.595086 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9009
I1122 10:49:50.595086 20880 solver.cpp:397]     Test net output #1: loss = 0.298197 (* 1 = 0.298197 loss)
I1122 10:49:50.636080 20880 solver.cpp:218] Iteration 26000 (18.5605 iter/s, 5.38779s/100 iters), loss = 0.125009
I1122 10:49:50.636080 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:49:50.636080 20880 solver.cpp:237]     Train net output #1: loss = 0.125009 (* 1 = 0.125009 loss)
I1122 10:49:50.636080 20880 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1122 10:49:54.907371 20880 solver.cpp:218] Iteration 26100 (23.4103 iter/s, 4.27163s/100 iters), loss = 0.140032
I1122 10:49:54.907371 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:49:54.907371 20880 solver.cpp:237]     Train net output #1: loss = 0.140032 (* 1 = 0.140032 loss)
I1122 10:49:54.907371 20880 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1122 10:49:59.183526 20880 solver.cpp:218] Iteration 26200 (23.3908 iter/s, 4.27519s/100 iters), loss = 0.109839
I1122 10:49:59.183526 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:49:59.183526 20880 solver.cpp:237]     Train net output #1: loss = 0.109839 (* 1 = 0.109839 loss)
I1122 10:49:59.183526 20880 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1122 10:50:03.477108 20880 solver.cpp:218] Iteration 26300 (23.2936 iter/s, 4.29303s/100 iters), loss = 0.16332
I1122 10:50:03.477108 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:50:03.477108 20880 solver.cpp:237]     Train net output #1: loss = 0.163321 (* 1 = 0.163321 loss)
I1122 10:50:03.477108 20880 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1122 10:50:07.747532 20880 solver.cpp:218] Iteration 26400 (23.4169 iter/s, 4.27043s/100 iters), loss = 0.0774203
I1122 10:50:07.747532 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:50:07.747532 20880 solver.cpp:237]     Train net output #1: loss = 0.0774205 (* 1 = 0.0774205 loss)
I1122 10:50:07.747532 20880 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1122 10:50:11.813047 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:50:11.980607 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_26500.caffemodel
I1122 10:50:11.990607 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_26500.solverstate
I1122 10:50:11.994607 20880 solver.cpp:330] Iteration 26500, Testing net (#0)
I1122 10:50:11.994607 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:50:13.052939 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:50:13.093935 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9014
I1122 10:50:13.093935 20880 solver.cpp:397]     Test net output #1: loss = 0.298136 (* 1 = 0.298136 loss)
I1122 10:50:13.135949 20880 solver.cpp:218] Iteration 26500 (18.5607 iter/s, 5.38773s/100 iters), loss = 0.147351
I1122 10:50:13.135949 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:50:13.135949 20880 solver.cpp:237]     Train net output #1: loss = 0.147352 (* 1 = 0.147352 loss)
I1122 10:50:13.135949 20880 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1122 10:50:17.398250 20880 solver.cpp:218] Iteration 26600 (23.46 iter/s, 4.26258s/100 iters), loss = 0.1883
I1122 10:50:17.398250 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:50:17.398250 20880 solver.cpp:237]     Train net output #1: loss = 0.1883 (* 1 = 0.1883 loss)
I1122 10:50:17.398250 20880 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1122 10:50:21.660120 20880 solver.cpp:218] Iteration 26700 (23.469 iter/s, 4.26094s/100 iters), loss = 0.148227
I1122 10:50:21.660120 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:50:21.660120 20880 solver.cpp:237]     Train net output #1: loss = 0.148227 (* 1 = 0.148227 loss)
I1122 10:50:21.660120 20880 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1122 10:50:25.923167 20880 solver.cpp:218] Iteration 26800 (23.4585 iter/s, 4.26284s/100 iters), loss = 0.149708
I1122 10:50:25.923167 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:50:25.923167 20880 solver.cpp:237]     Train net output #1: loss = 0.149708 (* 1 = 0.149708 loss)
I1122 10:50:25.923167 20880 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1122 10:50:30.186791 20880 solver.cpp:218] Iteration 26900 (23.4544 iter/s, 4.2636s/100 iters), loss = 0.0681638
I1122 10:50:30.186791 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:50:30.186791 20880 solver.cpp:237]     Train net output #1: loss = 0.068164 (* 1 = 0.068164 loss)
I1122 10:50:30.186791 20880 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1122 10:50:34.243841 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:50:34.411923 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_27000.caffemodel
I1122 10:50:34.421921 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_27000.solverstate
I1122 10:50:34.425920 20880 solver.cpp:330] Iteration 27000, Testing net (#0)
I1122 10:50:34.425920 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:50:35.485616 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:50:35.526626 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9009
I1122 10:50:35.526626 20880 solver.cpp:397]     Test net output #1: loss = 0.298158 (* 1 = 0.298158 loss)
I1122 10:50:35.568655 20880 solver.cpp:218] Iteration 27000 (18.5844 iter/s, 5.38086s/100 iters), loss = 0.133019
I1122 10:50:35.568655 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:50:35.568655 20880 solver.cpp:237]     Train net output #1: loss = 0.13302 (* 1 = 0.13302 loss)
I1122 10:50:35.568655 20880 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1122 10:50:35.568655 20880 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1122 10:50:39.837117 20880 solver.cpp:218] Iteration 27100 (23.4288 iter/s, 4.26825s/100 iters), loss = 0.218693
I1122 10:50:39.837117 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 10:50:39.837117 20880 solver.cpp:237]     Train net output #1: loss = 0.218693 (* 1 = 0.218693 loss)
I1122 10:50:39.837117 20880 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1122 10:50:44.108744 20880 solver.cpp:218] Iteration 27200 (23.4085 iter/s, 4.27195s/100 iters), loss = 0.132359
I1122 10:50:44.109748 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:50:44.109748 20880 solver.cpp:237]     Train net output #1: loss = 0.132359 (* 1 = 0.132359 loss)
I1122 10:50:44.109748 20880 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1122 10:50:48.379431 20880 solver.cpp:218] Iteration 27300 (23.4181 iter/s, 4.2702s/100 iters), loss = 0.170338
I1122 10:50:48.379431 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:50:48.379431 20880 solver.cpp:237]     Train net output #1: loss = 0.170338 (* 1 = 0.170338 loss)
I1122 10:50:48.380431 20880 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1122 10:50:52.654078 20880 solver.cpp:218] Iteration 27400 (23.3993 iter/s, 4.27362s/100 iters), loss = 0.077205
I1122 10:50:52.654078 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:50:52.654078 20880 solver.cpp:237]     Train net output #1: loss = 0.0772052 (* 1 = 0.0772052 loss)
I1122 10:50:52.654078 20880 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1122 10:50:56.719617 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:50:56.887195 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_27500.caffemodel
I1122 10:50:56.897181 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_27500.solverstate
I1122 10:50:56.901181 20880 solver.cpp:330] Iteration 27500, Testing net (#0)
I1122 10:50:56.901181 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:50:57.957700 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:50:57.998714 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9009
I1122 10:50:57.998714 20880 solver.cpp:397]     Test net output #1: loss = 0.298092 (* 1 = 0.298092 loss)
I1122 10:50:58.040730 20880 solver.cpp:218] Iteration 27500 (18.5658 iter/s, 5.38626s/100 iters), loss = 0.147754
I1122 10:50:58.040730 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:50:58.040730 20880 solver.cpp:237]     Train net output #1: loss = 0.147754 (* 1 = 0.147754 loss)
I1122 10:50:58.040730 20880 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1122 10:51:02.312655 20880 solver.cpp:218] Iteration 27600 (23.4082 iter/s, 4.272s/100 iters), loss = 0.15877
I1122 10:51:02.312655 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:51:02.312655 20880 solver.cpp:237]     Train net output #1: loss = 0.15877 (* 1 = 0.15877 loss)
I1122 10:51:02.312655 20880 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1122 10:51:06.586508 20880 solver.cpp:218] Iteration 27700 (23.3993 iter/s, 4.27362s/100 iters), loss = 0.121546
I1122 10:51:06.586508 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:51:06.586508 20880 solver.cpp:237]     Train net output #1: loss = 0.121546 (* 1 = 0.121546 loss)
I1122 10:51:06.586508 20880 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1122 10:51:10.866943 20880 solver.cpp:218] Iteration 27800 (23.3678 iter/s, 4.2794s/100 iters), loss = 0.172329
I1122 10:51:10.866943 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:51:10.866943 20880 solver.cpp:237]     Train net output #1: loss = 0.172329 (* 1 = 0.172329 loss)
I1122 10:51:10.866943 20880 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1122 10:51:15.130549 20880 solver.cpp:218] Iteration 27900 (23.4525 iter/s, 4.26394s/100 iters), loss = 0.0773607
I1122 10:51:15.130549 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:51:15.130549 20880 solver.cpp:237]     Train net output #1: loss = 0.0773609 (* 1 = 0.0773609 loss)
I1122 10:51:15.130549 20880 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1122 10:51:19.193809 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:51:19.360776 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_28000.caffemodel
I1122 10:51:19.370775 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_28000.solverstate
I1122 10:51:19.374778 20880 solver.cpp:330] Iteration 28000, Testing net (#0)
I1122 10:51:19.374778 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:51:20.433543 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:51:20.475284 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9008
I1122 10:51:20.475284 20880 solver.cpp:397]     Test net output #1: loss = 0.2982 (* 1 = 0.2982 loss)
I1122 10:51:20.516299 20880 solver.cpp:218] Iteration 28000 (18.5687 iter/s, 5.38541s/100 iters), loss = 0.126121
I1122 10:51:20.516299 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:51:20.517303 20880 solver.cpp:237]     Train net output #1: loss = 0.126121 (* 1 = 0.126121 loss)
I1122 10:51:20.517303 20880 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1122 10:51:24.788086 20880 solver.cpp:218] Iteration 28100 (23.4155 iter/s, 4.27068s/100 iters), loss = 0.1919
I1122 10:51:24.788086 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:51:24.788086 20880 solver.cpp:237]     Train net output #1: loss = 0.1919 (* 1 = 0.1919 loss)
I1122 10:51:24.788086 20880 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1122 10:51:29.056568 20880 solver.cpp:218] Iteration 28200 (23.429 iter/s, 4.26822s/100 iters), loss = 0.15501
I1122 10:51:29.056568 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:51:29.056568 20880 solver.cpp:237]     Train net output #1: loss = 0.15501 (* 1 = 0.15501 loss)
I1122 10:51:29.056568 20880 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1122 10:51:33.327683 20880 solver.cpp:218] Iteration 28300 (23.412 iter/s, 4.27131s/100 iters), loss = 0.119494
I1122 10:51:33.327683 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:51:33.327683 20880 solver.cpp:237]     Train net output #1: loss = 0.119495 (* 1 = 0.119495 loss)
I1122 10:51:33.327683 20880 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1122 10:51:37.601109 20880 solver.cpp:218] Iteration 28400 (23.4033 iter/s, 4.27291s/100 iters), loss = 0.0882528
I1122 10:51:37.601109 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:51:37.601109 20880 solver.cpp:237]     Train net output #1: loss = 0.088253 (* 1 = 0.088253 loss)
I1122 10:51:37.601109 20880 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1122 10:51:41.664482 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:51:41.831557 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_28500.caffemodel
I1122 10:51:41.841533 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_28500.solverstate
I1122 10:51:41.845535 20880 solver.cpp:330] Iteration 28500, Testing net (#0)
I1122 10:51:41.845535 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:51:42.906299 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:51:42.947295 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9008
I1122 10:51:42.947295 20880 solver.cpp:397]     Test net output #1: loss = 0.298273 (* 1 = 0.298273 loss)
I1122 10:51:42.988816 20880 solver.cpp:218] Iteration 28500 (18.5637 iter/s, 5.38685s/100 iters), loss = 0.120047
I1122 10:51:42.988816 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:51:42.988816 20880 solver.cpp:237]     Train net output #1: loss = 0.120047 (* 1 = 0.120047 loss)
I1122 10:51:42.988816 20880 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1122 10:51:47.254379 20880 solver.cpp:218] Iteration 28600 (23.4461 iter/s, 4.26509s/100 iters), loss = 0.199749
I1122 10:51:47.254379 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:51:47.254379 20880 solver.cpp:237]     Train net output #1: loss = 0.199749 (* 1 = 0.199749 loss)
I1122 10:51:47.254379 20880 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1122 10:51:51.517287 20880 solver.cpp:218] Iteration 28700 (23.4574 iter/s, 4.26306s/100 iters), loss = 0.155946
I1122 10:51:51.517287 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:51:51.517287 20880 solver.cpp:237]     Train net output #1: loss = 0.155946 (* 1 = 0.155946 loss)
I1122 10:51:51.517287 20880 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1122 10:51:55.782773 20880 solver.cpp:218] Iteration 28800 (23.448 iter/s, 4.26475s/100 iters), loss = 0.146005
I1122 10:51:55.782773 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:51:55.782773 20880 solver.cpp:237]     Train net output #1: loss = 0.146005 (* 1 = 0.146005 loss)
I1122 10:51:55.782773 20880 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1122 10:52:00.045074 20880 solver.cpp:218] Iteration 28900 (23.463 iter/s, 4.26203s/100 iters), loss = 0.0916206
I1122 10:52:00.045074 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:52:00.045074 20880 solver.cpp:237]     Train net output #1: loss = 0.0916208 (* 1 = 0.0916208 loss)
I1122 10:52:00.045074 20880 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1122 10:52:04.096398 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:52:04.262446 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_29000.caffemodel
I1122 10:52:04.272431 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_29000.solverstate
I1122 10:52:04.276432 20880 solver.cpp:330] Iteration 29000, Testing net (#0)
I1122 10:52:04.276432 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:52:05.336220 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:52:05.378209 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9011
I1122 10:52:05.378209 20880 solver.cpp:397]     Test net output #1: loss = 0.298211 (* 1 = 0.298211 loss)
I1122 10:52:05.419242 20880 solver.cpp:218] Iteration 29000 (18.6094 iter/s, 5.37362s/100 iters), loss = 0.11492
I1122 10:52:05.419242 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:52:05.419242 20880 solver.cpp:237]     Train net output #1: loss = 0.114921 (* 1 = 0.114921 loss)
I1122 10:52:05.419242 20880 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1122 10:52:09.692780 20880 solver.cpp:218] Iteration 29100 (23.4032 iter/s, 4.27292s/100 iters), loss = 0.159168
I1122 10:52:09.692780 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:52:09.692780 20880 solver.cpp:237]     Train net output #1: loss = 0.159168 (* 1 = 0.159168 loss)
I1122 10:52:09.692780 20880 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1122 10:52:13.971997 20880 solver.cpp:218] Iteration 29200 (23.3692 iter/s, 4.27914s/100 iters), loss = 0.137555
I1122 10:52:13.971997 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:52:13.971997 20880 solver.cpp:237]     Train net output #1: loss = 0.137555 (* 1 = 0.137555 loss)
I1122 10:52:13.971997 20880 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1122 10:52:18.251564 20880 solver.cpp:218] Iteration 29300 (23.3707 iter/s, 4.27886s/100 iters), loss = 0.138459
I1122 10:52:18.251564 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:52:18.251564 20880 solver.cpp:237]     Train net output #1: loss = 0.138459 (* 1 = 0.138459 loss)
I1122 10:52:18.251564 20880 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1122 10:52:22.527104 20880 solver.cpp:218] Iteration 29400 (23.3902 iter/s, 4.27529s/100 iters), loss = 0.0704648
I1122 10:52:22.527104 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:52:22.527104 20880 solver.cpp:237]     Train net output #1: loss = 0.0704651 (* 1 = 0.0704651 loss)
I1122 10:52:22.527104 20880 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1122 10:52:26.593354 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:52:26.761514 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_29500.caffemodel
I1122 10:52:26.771512 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_29500.solverstate
I1122 10:52:26.775513 20880 solver.cpp:330] Iteration 29500, Testing net (#0)
I1122 10:52:26.775513 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:52:27.835700 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:52:27.877199 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9011
I1122 10:52:27.877199 20880 solver.cpp:397]     Test net output #1: loss = 0.298155 (* 1 = 0.298155 loss)
I1122 10:52:27.918212 20880 solver.cpp:218] Iteration 29500 (18.5507 iter/s, 5.39064s/100 iters), loss = 0.154145
I1122 10:52:27.918212 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:52:27.918212 20880 solver.cpp:237]     Train net output #1: loss = 0.154145 (* 1 = 0.154145 loss)
I1122 10:52:27.918212 20880 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1122 10:52:32.179020 20880 solver.cpp:218] Iteration 29600 (23.4687 iter/s, 4.26099s/100 iters), loss = 0.197758
I1122 10:52:32.179020 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:52:32.179020 20880 solver.cpp:237]     Train net output #1: loss = 0.197758 (* 1 = 0.197758 loss)
I1122 10:52:32.179020 20880 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1122 10:52:36.440680 20880 solver.cpp:218] Iteration 29700 (23.4679 iter/s, 4.26113s/100 iters), loss = 0.107641
I1122 10:52:36.441179 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:52:36.441179 20880 solver.cpp:237]     Train net output #1: loss = 0.107641 (* 1 = 0.107641 loss)
I1122 10:52:36.441179 20880 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1122 10:52:40.705970 20880 solver.cpp:218] Iteration 29800 (23.4498 iter/s, 4.26442s/100 iters), loss = 0.136798
I1122 10:52:40.705970 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:52:40.705970 20880 solver.cpp:237]     Train net output #1: loss = 0.136798 (* 1 = 0.136798 loss)
I1122 10:52:40.705970 20880 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1122 10:52:44.965114 20880 solver.cpp:218] Iteration 29900 (23.4771 iter/s, 4.25947s/100 iters), loss = 0.0806522
I1122 10:52:44.965114 20880 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:52:44.965114 20880 solver.cpp:237]     Train net output #1: loss = 0.0806525 (* 1 = 0.0806525 loss)
I1122 10:52:44.965114 20880 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1122 10:52:49.017367 20328 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:52:49.185761 20880 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_30000.caffemodel
I1122 10:52:49.194751 20880 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_30000.solverstate
I1122 10:52:49.211756 20880 solver.cpp:310] Iteration 30000, loss = 0.160006
I1122 10:52:49.211756 20880 solver.cpp:330] Iteration 30000, Testing net (#0)
I1122 10:52:49.211756 20880 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:52:50.270102 16524 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:52:50.311096 20880 solver.cpp:397]     Test net output #0: accuracy = 0.9013
I1122 10:52:50.311096 20880 solver.cpp:397]     Test net output #1: loss = 0.298161 (* 1 = 0.298161 loss)
I1122 10:52:50.311096 20880 solver.cpp:315] Optimization Done.
I1122 10:52:50.311096 20880 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 
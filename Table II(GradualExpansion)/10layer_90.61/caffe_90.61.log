
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1122 08:19:52.088945 18896 caffe.cpp:219] Using GPUs 0
I1122 08:19:52.271459 18896 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1122 08:19:52.571440 18896 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 08:19:52.586442 18896 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_300k_10L"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1122 08:19:52.587442 18896 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 08:19:52.588443 18896 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 08:19:52.588443 18896 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1122 08:19:52.588443 18896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1122 08:19:52.588443 18896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1122 08:19:52.588443 18896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1122 08:19:52.588443 18896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1122 08:19:52.588443 18896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1122 08:19:52.588443 18896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1122 08:19:52.588443 18896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1122 08:19:52.588443 18896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1122 08:19:52.588443 18896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1122 08:19:52.588443 18896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1122 08:19:52.588443 18896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1122 08:19:52.588443 18896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1122 08:19:52.589443 18896 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_10L_Simple_NoGrpCon_NoDrp_300k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 75
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 85
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 95
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1122 08:19:52.702491 18896 layer_factory.cpp:58] Creating layer cifar
I1122 08:19:52.726492 18896 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1122 08:19:52.727505 18896 net.cpp:84] Creating Layer cifar
I1122 08:19:52.727505 18896 net.cpp:380] cifar -> data
I1122 08:19:52.727505 18896 net.cpp:380] cifar -> label
I1122 08:19:52.728025 18896 data_layer.cpp:45] output data size: 100,3,32,32
I1122 08:19:52.733510 18896 net.cpp:122] Setting up cifar
I1122 08:19:52.733510 18896 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1122 08:19:52.733510 18896 net.cpp:129] Top shape: 100 (100)
I1122 08:19:52.733510 18896 net.cpp:137] Memory required for data: 1229200
I1122 08:19:52.733510 18896 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1122 08:19:52.733510 18896 net.cpp:84] Creating Layer label_cifar_1_split
I1122 08:19:52.733510 18896 net.cpp:406] label_cifar_1_split <- label
I1122 08:19:52.733510 18896 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1122 08:19:52.733510 18896 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1122 08:19:52.733510 18896 net.cpp:122] Setting up label_cifar_1_split
I1122 08:19:52.733510 18896 net.cpp:129] Top shape: 100 (100)
I1122 08:19:52.733510 18896 net.cpp:129] Top shape: 100 (100)
I1122 08:19:52.733510 18896 net.cpp:137] Memory required for data: 1230000
I1122 08:19:52.733510 18896 layer_factory.cpp:58] Creating layer conv1
I1122 08:19:52.733510 18896 net.cpp:84] Creating Layer conv1
I1122 08:19:52.733510 18896 net.cpp:406] conv1 <- data
I1122 08:19:52.733510 18896 net.cpp:380] conv1 -> conv1
I1122 08:19:52.734521 12432 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 08:19:52.990556 18896 net.cpp:122] Setting up conv1
I1122 08:19:52.990556 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:52.990556 18896 net.cpp:137] Memory required for data: 17614000
I1122 08:19:52.990556 18896 layer_factory.cpp:58] Creating layer bn1
I1122 08:19:52.990556 18896 net.cpp:84] Creating Layer bn1
I1122 08:19:52.990556 18896 net.cpp:406] bn1 <- conv1
I1122 08:19:52.990556 18896 net.cpp:367] bn1 -> conv1 (in-place)
I1122 08:19:52.991554 18896 net.cpp:122] Setting up bn1
I1122 08:19:52.991554 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:52.991554 18896 net.cpp:137] Memory required for data: 33998000
I1122 08:19:52.991554 18896 layer_factory.cpp:58] Creating layer scale1
I1122 08:19:52.991554 18896 net.cpp:84] Creating Layer scale1
I1122 08:19:52.991554 18896 net.cpp:406] scale1 <- conv1
I1122 08:19:52.991554 18896 net.cpp:367] scale1 -> conv1 (in-place)
I1122 08:19:52.991554 18896 layer_factory.cpp:58] Creating layer scale1
I1122 08:19:52.991554 18896 net.cpp:122] Setting up scale1
I1122 08:19:52.991554 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:52.991554 18896 net.cpp:137] Memory required for data: 50382000
I1122 08:19:52.991554 18896 layer_factory.cpp:58] Creating layer relu1
I1122 08:19:52.991554 18896 net.cpp:84] Creating Layer relu1
I1122 08:19:52.991554 18896 net.cpp:406] relu1 <- conv1
I1122 08:19:52.991554 18896 net.cpp:367] relu1 -> conv1 (in-place)
I1122 08:19:52.991554 18896 net.cpp:122] Setting up relu1
I1122 08:19:52.991554 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:52.991554 18896 net.cpp:137] Memory required for data: 66766000
I1122 08:19:52.991554 18896 layer_factory.cpp:58] Creating layer conv2
I1122 08:19:52.991554 18896 net.cpp:84] Creating Layer conv2
I1122 08:19:52.991554 18896 net.cpp:406] conv2 <- conv1
I1122 08:19:52.991554 18896 net.cpp:380] conv2 -> conv2
I1122 08:19:52.993553 18896 net.cpp:122] Setting up conv2
I1122 08:19:52.993553 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:52.993553 18896 net.cpp:137] Memory required for data: 83150000
I1122 08:19:52.993553 18896 layer_factory.cpp:58] Creating layer bn2
I1122 08:19:52.993553 18896 net.cpp:84] Creating Layer bn2
I1122 08:19:52.993553 18896 net.cpp:406] bn2 <- conv2
I1122 08:19:52.993553 18896 net.cpp:367] bn2 -> conv2 (in-place)
I1122 08:19:52.993553 18896 net.cpp:122] Setting up bn2
I1122 08:19:52.993553 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:52.993553 18896 net.cpp:137] Memory required for data: 99534000
I1122 08:19:52.993553 18896 layer_factory.cpp:58] Creating layer scale2
I1122 08:19:52.993553 18896 net.cpp:84] Creating Layer scale2
I1122 08:19:52.993553 18896 net.cpp:406] scale2 <- conv2
I1122 08:19:52.993553 18896 net.cpp:367] scale2 -> conv2 (in-place)
I1122 08:19:52.993553 18896 layer_factory.cpp:58] Creating layer scale2
I1122 08:19:52.993553 18896 net.cpp:122] Setting up scale2
I1122 08:19:52.993553 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:52.993553 18896 net.cpp:137] Memory required for data: 115918000
I1122 08:19:52.993553 18896 layer_factory.cpp:58] Creating layer relu2
I1122 08:19:52.993553 18896 net.cpp:84] Creating Layer relu2
I1122 08:19:52.993553 18896 net.cpp:406] relu2 <- conv2
I1122 08:19:52.993553 18896 net.cpp:367] relu2 -> conv2 (in-place)
I1122 08:19:52.993553 18896 net.cpp:122] Setting up relu2
I1122 08:19:52.993553 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:52.993553 18896 net.cpp:137] Memory required for data: 132302000
I1122 08:19:52.993553 18896 layer_factory.cpp:58] Creating layer conv2_1
I1122 08:19:52.993553 18896 net.cpp:84] Creating Layer conv2_1
I1122 08:19:52.993553 18896 net.cpp:406] conv2_1 <- conv2
I1122 08:19:52.993553 18896 net.cpp:380] conv2_1 -> conv2_1
I1122 08:19:52.994554 18896 net.cpp:122] Setting up conv2_1
I1122 08:19:52.994554 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:52.994554 18896 net.cpp:137] Memory required for data: 148686000
I1122 08:19:52.995553 18896 layer_factory.cpp:58] Creating layer bn2_1
I1122 08:19:52.995553 18896 net.cpp:84] Creating Layer bn2_1
I1122 08:19:52.995553 18896 net.cpp:406] bn2_1 <- conv2_1
I1122 08:19:52.995553 18896 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1122 08:19:52.995553 18896 net.cpp:122] Setting up bn2_1
I1122 08:19:52.995553 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:52.995553 18896 net.cpp:137] Memory required for data: 165070000
I1122 08:19:52.995553 18896 layer_factory.cpp:58] Creating layer scale2_1
I1122 08:19:52.995553 18896 net.cpp:84] Creating Layer scale2_1
I1122 08:19:52.995553 18896 net.cpp:406] scale2_1 <- conv2_1
I1122 08:19:52.995553 18896 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1122 08:19:52.995553 18896 layer_factory.cpp:58] Creating layer scale2_1
I1122 08:19:52.995553 18896 net.cpp:122] Setting up scale2_1
I1122 08:19:52.995553 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:52.995553 18896 net.cpp:137] Memory required for data: 181454000
I1122 08:19:52.995553 18896 layer_factory.cpp:58] Creating layer relu2_1
I1122 08:19:52.995553 18896 net.cpp:84] Creating Layer relu2_1
I1122 08:19:52.995553 18896 net.cpp:406] relu2_1 <- conv2_1
I1122 08:19:52.995553 18896 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1122 08:19:52.995553 18896 net.cpp:122] Setting up relu2_1
I1122 08:19:52.995553 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:52.995553 18896 net.cpp:137] Memory required for data: 197838000
I1122 08:19:52.995553 18896 layer_factory.cpp:58] Creating layer conv2_2
I1122 08:19:52.995553 18896 net.cpp:84] Creating Layer conv2_2
I1122 08:19:52.995553 18896 net.cpp:406] conv2_2 <- conv2_1
I1122 08:19:52.995553 18896 net.cpp:380] conv2_2 -> conv2_2
I1122 08:19:52.997560 18896 net.cpp:122] Setting up conv2_2
I1122 08:19:52.997560 18896 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1122 08:19:52.997560 18896 net.cpp:137] Memory required for data: 220366000
I1122 08:19:52.997560 18896 layer_factory.cpp:58] Creating layer bn2_2
I1122 08:19:52.997560 18896 net.cpp:84] Creating Layer bn2_2
I1122 08:19:52.997560 18896 net.cpp:406] bn2_2 <- conv2_2
I1122 08:19:52.997560 18896 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1122 08:19:52.997560 18896 net.cpp:122] Setting up bn2_2
I1122 08:19:52.997560 18896 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1122 08:19:52.997560 18896 net.cpp:137] Memory required for data: 242894000
I1122 08:19:52.997560 18896 layer_factory.cpp:58] Creating layer scale2_2
I1122 08:19:52.997560 18896 net.cpp:84] Creating Layer scale2_2
I1122 08:19:52.997560 18896 net.cpp:406] scale2_2 <- conv2_2
I1122 08:19:52.997560 18896 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1122 08:19:52.997560 18896 layer_factory.cpp:58] Creating layer scale2_2
I1122 08:19:52.997560 18896 net.cpp:122] Setting up scale2_2
I1122 08:19:52.997560 18896 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1122 08:19:52.997560 18896 net.cpp:137] Memory required for data: 265422000
I1122 08:19:52.997560 18896 layer_factory.cpp:58] Creating layer relu2_2
I1122 08:19:52.997560 18896 net.cpp:84] Creating Layer relu2_2
I1122 08:19:52.997560 18896 net.cpp:406] relu2_2 <- conv2_2
I1122 08:19:52.997560 18896 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1122 08:19:52.998553 18896 net.cpp:122] Setting up relu2_2
I1122 08:19:52.998553 18896 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1122 08:19:52.998553 18896 net.cpp:137] Memory required for data: 287950000
I1122 08:19:52.998553 18896 layer_factory.cpp:58] Creating layer pool2_1
I1122 08:19:52.998553 18896 net.cpp:84] Creating Layer pool2_1
I1122 08:19:52.998553 18896 net.cpp:406] pool2_1 <- conv2_2
I1122 08:19:52.998553 18896 net.cpp:380] pool2_1 -> pool2_1
I1122 08:19:52.998553 18896 net.cpp:122] Setting up pool2_1
I1122 08:19:52.998553 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:52.998553 18896 net.cpp:137] Memory required for data: 293582000
I1122 08:19:52.998553 18896 layer_factory.cpp:58] Creating layer conv3
I1122 08:19:52.998553 18896 net.cpp:84] Creating Layer conv3
I1122 08:19:52.998553 18896 net.cpp:406] conv3 <- pool2_1
I1122 08:19:52.998553 18896 net.cpp:380] conv3 -> conv3
I1122 08:19:52.999553 18896 net.cpp:122] Setting up conv3
I1122 08:19:52.999553 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:52.999553 18896 net.cpp:137] Memory required for data: 299214000
I1122 08:19:52.999553 18896 layer_factory.cpp:58] Creating layer bn3
I1122 08:19:52.999553 18896 net.cpp:84] Creating Layer bn3
I1122 08:19:52.999553 18896 net.cpp:406] bn3 <- conv3
I1122 08:19:52.999553 18896 net.cpp:367] bn3 -> conv3 (in-place)
I1122 08:19:52.999553 18896 net.cpp:122] Setting up bn3
I1122 08:19:52.999553 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:52.999553 18896 net.cpp:137] Memory required for data: 304846000
I1122 08:19:52.999553 18896 layer_factory.cpp:58] Creating layer scale3
I1122 08:19:52.999553 18896 net.cpp:84] Creating Layer scale3
I1122 08:19:52.999553 18896 net.cpp:406] scale3 <- conv3
I1122 08:19:52.999553 18896 net.cpp:367] scale3 -> conv3 (in-place)
I1122 08:19:52.999553 18896 layer_factory.cpp:58] Creating layer scale3
I1122 08:19:52.999553 18896 net.cpp:122] Setting up scale3
I1122 08:19:52.999553 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:52.999553 18896 net.cpp:137] Memory required for data: 310478000
I1122 08:19:52.999553 18896 layer_factory.cpp:58] Creating layer relu3
I1122 08:19:52.999553 18896 net.cpp:84] Creating Layer relu3
I1122 08:19:52.999553 18896 net.cpp:406] relu3 <- conv3
I1122 08:19:52.999553 18896 net.cpp:367] relu3 -> conv3 (in-place)
I1122 08:19:53.000555 18896 net.cpp:122] Setting up relu3
I1122 08:19:53.000555 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.000555 18896 net.cpp:137] Memory required for data: 316110000
I1122 08:19:53.000555 18896 layer_factory.cpp:58] Creating layer conv4
I1122 08:19:53.000555 18896 net.cpp:84] Creating Layer conv4
I1122 08:19:53.000555 18896 net.cpp:406] conv4 <- conv3
I1122 08:19:53.000555 18896 net.cpp:380] conv4 -> conv4
I1122 08:19:53.001554 18896 net.cpp:122] Setting up conv4
I1122 08:19:53.001554 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.001554 18896 net.cpp:137] Memory required for data: 321742000
I1122 08:19:53.001554 18896 layer_factory.cpp:58] Creating layer bn4
I1122 08:19:53.001554 18896 net.cpp:84] Creating Layer bn4
I1122 08:19:53.001554 18896 net.cpp:406] bn4 <- conv4
I1122 08:19:53.001554 18896 net.cpp:367] bn4 -> conv4 (in-place)
I1122 08:19:53.001554 18896 net.cpp:122] Setting up bn4
I1122 08:19:53.001554 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.001554 18896 net.cpp:137] Memory required for data: 327374000
I1122 08:19:53.001554 18896 layer_factory.cpp:58] Creating layer scale4
I1122 08:19:53.001554 18896 net.cpp:84] Creating Layer scale4
I1122 08:19:53.001554 18896 net.cpp:406] scale4 <- conv4
I1122 08:19:53.001554 18896 net.cpp:367] scale4 -> conv4 (in-place)
I1122 08:19:53.001554 18896 layer_factory.cpp:58] Creating layer scale4
I1122 08:19:53.002553 18896 net.cpp:122] Setting up scale4
I1122 08:19:53.002553 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.002553 18896 net.cpp:137] Memory required for data: 333006000
I1122 08:19:53.002553 18896 layer_factory.cpp:58] Creating layer relu4
I1122 08:19:53.002553 18896 net.cpp:84] Creating Layer relu4
I1122 08:19:53.002553 18896 net.cpp:406] relu4 <- conv4
I1122 08:19:53.002553 18896 net.cpp:367] relu4 -> conv4 (in-place)
I1122 08:19:53.002553 18896 net.cpp:122] Setting up relu4
I1122 08:19:53.002553 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.002553 18896 net.cpp:137] Memory required for data: 338638000
I1122 08:19:53.002553 18896 layer_factory.cpp:58] Creating layer conv4_1
I1122 08:19:53.002553 18896 net.cpp:84] Creating Layer conv4_1
I1122 08:19:53.002553 18896 net.cpp:406] conv4_1 <- conv4
I1122 08:19:53.002553 18896 net.cpp:380] conv4_1 -> conv4_1
I1122 08:19:53.004554 18896 net.cpp:122] Setting up conv4_1
I1122 08:19:53.004554 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.004554 18896 net.cpp:137] Memory required for data: 344270000
I1122 08:19:53.004554 18896 layer_factory.cpp:58] Creating layer bn4_1
I1122 08:19:53.004554 18896 net.cpp:84] Creating Layer bn4_1
I1122 08:19:53.004554 18896 net.cpp:406] bn4_1 <- conv4_1
I1122 08:19:53.004554 18896 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1122 08:19:53.004554 18896 net.cpp:122] Setting up bn4_1
I1122 08:19:53.004554 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.004554 18896 net.cpp:137] Memory required for data: 349902000
I1122 08:19:53.004554 18896 layer_factory.cpp:58] Creating layer scale4_1
I1122 08:19:53.004554 18896 net.cpp:84] Creating Layer scale4_1
I1122 08:19:53.004554 18896 net.cpp:406] scale4_1 <- conv4_1
I1122 08:19:53.004554 18896 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1122 08:19:53.004554 18896 layer_factory.cpp:58] Creating layer scale4_1
I1122 08:19:53.004554 18896 net.cpp:122] Setting up scale4_1
I1122 08:19:53.004554 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.004554 18896 net.cpp:137] Memory required for data: 355534000
I1122 08:19:53.004554 18896 layer_factory.cpp:58] Creating layer relu4_1
I1122 08:19:53.004554 18896 net.cpp:84] Creating Layer relu4_1
I1122 08:19:53.004554 18896 net.cpp:406] relu4_1 <- conv4_1
I1122 08:19:53.004554 18896 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1122 08:19:53.004554 18896 net.cpp:122] Setting up relu4_1
I1122 08:19:53.004554 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.004554 18896 net.cpp:137] Memory required for data: 361166000
I1122 08:19:53.004554 18896 layer_factory.cpp:58] Creating layer conv4_2
I1122 08:19:53.004554 18896 net.cpp:84] Creating Layer conv4_2
I1122 08:19:53.004554 18896 net.cpp:406] conv4_2 <- conv4_1
I1122 08:19:53.004554 18896 net.cpp:380] conv4_2 -> conv4_2
I1122 08:19:53.006553 18896 net.cpp:122] Setting up conv4_2
I1122 08:19:53.006553 18896 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1122 08:19:53.006553 18896 net.cpp:137] Memory required for data: 368846000
I1122 08:19:53.006553 18896 layer_factory.cpp:58] Creating layer bn4_2
I1122 08:19:53.006553 18896 net.cpp:84] Creating Layer bn4_2
I1122 08:19:53.006553 18896 net.cpp:406] bn4_2 <- conv4_2
I1122 08:19:53.006553 18896 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1122 08:19:53.006553 18896 net.cpp:122] Setting up bn4_2
I1122 08:19:53.006553 18896 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1122 08:19:53.006553 18896 net.cpp:137] Memory required for data: 376526000
I1122 08:19:53.006553 18896 layer_factory.cpp:58] Creating layer scale4_2
I1122 08:19:53.006553 18896 net.cpp:84] Creating Layer scale4_2
I1122 08:19:53.006553 18896 net.cpp:406] scale4_2 <- conv4_2
I1122 08:19:53.006553 18896 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1122 08:19:53.006553 18896 layer_factory.cpp:58] Creating layer scale4_2
I1122 08:19:53.006553 18896 net.cpp:122] Setting up scale4_2
I1122 08:19:53.006553 18896 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1122 08:19:53.006553 18896 net.cpp:137] Memory required for data: 384206000
I1122 08:19:53.006553 18896 layer_factory.cpp:58] Creating layer relu4_2
I1122 08:19:53.006553 18896 net.cpp:84] Creating Layer relu4_2
I1122 08:19:53.006553 18896 net.cpp:406] relu4_2 <- conv4_2
I1122 08:19:53.006553 18896 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1122 08:19:53.007560 18896 net.cpp:122] Setting up relu4_2
I1122 08:19:53.007560 18896 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1122 08:19:53.007560 18896 net.cpp:137] Memory required for data: 391886000
I1122 08:19:53.007560 18896 layer_factory.cpp:58] Creating layer pool4_2
I1122 08:19:53.007560 18896 net.cpp:84] Creating Layer pool4_2
I1122 08:19:53.007560 18896 net.cpp:406] pool4_2 <- conv4_2
I1122 08:19:53.007560 18896 net.cpp:380] pool4_2 -> pool4_2
I1122 08:19:53.007560 18896 net.cpp:122] Setting up pool4_2
I1122 08:19:53.007560 18896 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1122 08:19:53.007560 18896 net.cpp:137] Memory required for data: 393806000
I1122 08:19:53.007560 18896 layer_factory.cpp:58] Creating layer conv11
I1122 08:19:53.007560 18896 net.cpp:84] Creating Layer conv11
I1122 08:19:53.007560 18896 net.cpp:406] conv11 <- pool4_2
I1122 08:19:53.007560 18896 net.cpp:380] conv11 -> conv11
I1122 08:19:53.009554 18896 net.cpp:122] Setting up conv11
I1122 08:19:53.009554 18896 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 08:19:53.009554 18896 net.cpp:137] Memory required for data: 395982000
I1122 08:19:53.009554 18896 layer_factory.cpp:58] Creating layer bn_conv11
I1122 08:19:53.009554 18896 net.cpp:84] Creating Layer bn_conv11
I1122 08:19:53.009554 18896 net.cpp:406] bn_conv11 <- conv11
I1122 08:19:53.009554 18896 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1122 08:19:53.009554 18896 net.cpp:122] Setting up bn_conv11
I1122 08:19:53.009554 18896 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 08:19:53.009554 18896 net.cpp:137] Memory required for data: 398158000
I1122 08:19:53.009554 18896 layer_factory.cpp:58] Creating layer scale_conv11
I1122 08:19:53.009554 18896 net.cpp:84] Creating Layer scale_conv11
I1122 08:19:53.009554 18896 net.cpp:406] scale_conv11 <- conv11
I1122 08:19:53.009554 18896 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1122 08:19:53.009554 18896 layer_factory.cpp:58] Creating layer scale_conv11
I1122 08:19:53.009554 18896 net.cpp:122] Setting up scale_conv11
I1122 08:19:53.009554 18896 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 08:19:53.009554 18896 net.cpp:137] Memory required for data: 400334000
I1122 08:19:53.009554 18896 layer_factory.cpp:58] Creating layer relu_conv11
I1122 08:19:53.009554 18896 net.cpp:84] Creating Layer relu_conv11
I1122 08:19:53.009554 18896 net.cpp:406] relu_conv11 <- conv11
I1122 08:19:53.009554 18896 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1122 08:19:53.009554 18896 net.cpp:122] Setting up relu_conv11
I1122 08:19:53.009554 18896 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 08:19:53.009554 18896 net.cpp:137] Memory required for data: 402510000
I1122 08:19:53.009554 18896 layer_factory.cpp:58] Creating layer conv12
I1122 08:19:53.009554 18896 net.cpp:84] Creating Layer conv12
I1122 08:19:53.009554 18896 net.cpp:406] conv12 <- conv11
I1122 08:19:53.009554 18896 net.cpp:380] conv12 -> conv12
I1122 08:19:53.011554 18896 net.cpp:122] Setting up conv12
I1122 08:19:53.012554 18896 net.cpp:129] Top shape: 100 95 8 8 (608000)
I1122 08:19:53.012554 18896 net.cpp:137] Memory required for data: 404942000
I1122 08:19:53.012554 18896 layer_factory.cpp:58] Creating layer bn_conv12
I1122 08:19:53.012554 18896 net.cpp:84] Creating Layer bn_conv12
I1122 08:19:53.012554 18896 net.cpp:406] bn_conv12 <- conv12
I1122 08:19:53.012554 18896 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1122 08:19:53.012554 18896 net.cpp:122] Setting up bn_conv12
I1122 08:19:53.012554 18896 net.cpp:129] Top shape: 100 95 8 8 (608000)
I1122 08:19:53.012554 18896 net.cpp:137] Memory required for data: 407374000
I1122 08:19:53.012554 18896 layer_factory.cpp:58] Creating layer scale_conv12
I1122 08:19:53.012554 18896 net.cpp:84] Creating Layer scale_conv12
I1122 08:19:53.012554 18896 net.cpp:406] scale_conv12 <- conv12
I1122 08:19:53.012554 18896 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1122 08:19:53.012554 18896 layer_factory.cpp:58] Creating layer scale_conv12
I1122 08:19:53.012554 18896 net.cpp:122] Setting up scale_conv12
I1122 08:19:53.012554 18896 net.cpp:129] Top shape: 100 95 8 8 (608000)
I1122 08:19:53.012554 18896 net.cpp:137] Memory required for data: 409806000
I1122 08:19:53.012554 18896 layer_factory.cpp:58] Creating layer relu_conv12
I1122 08:19:53.012554 18896 net.cpp:84] Creating Layer relu_conv12
I1122 08:19:53.012554 18896 net.cpp:406] relu_conv12 <- conv12
I1122 08:19:53.012554 18896 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1122 08:19:53.012554 18896 net.cpp:122] Setting up relu_conv12
I1122 08:19:53.012554 18896 net.cpp:129] Top shape: 100 95 8 8 (608000)
I1122 08:19:53.012554 18896 net.cpp:137] Memory required for data: 412238000
I1122 08:19:53.012554 18896 layer_factory.cpp:58] Creating layer poolcp6
I1122 08:19:53.012554 18896 net.cpp:84] Creating Layer poolcp6
I1122 08:19:53.012554 18896 net.cpp:406] poolcp6 <- conv12
I1122 08:19:53.012554 18896 net.cpp:380] poolcp6 -> poolcp6
I1122 08:19:53.012554 18896 net.cpp:122] Setting up poolcp6
I1122 08:19:53.012554 18896 net.cpp:129] Top shape: 100 95 1 1 (9500)
I1122 08:19:53.012554 18896 net.cpp:137] Memory required for data: 412276000
I1122 08:19:53.012554 18896 layer_factory.cpp:58] Creating layer ip1
I1122 08:19:53.012554 18896 net.cpp:84] Creating Layer ip1
I1122 08:19:53.012554 18896 net.cpp:406] ip1 <- poolcp6
I1122 08:19:53.013555 18896 net.cpp:380] ip1 -> ip1
I1122 08:19:53.013555 18896 net.cpp:122] Setting up ip1
I1122 08:19:53.013555 18896 net.cpp:129] Top shape: 100 10 (1000)
I1122 08:19:53.013555 18896 net.cpp:137] Memory required for data: 412280000
I1122 08:19:53.013555 18896 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1122 08:19:53.013555 18896 net.cpp:84] Creating Layer ip1_ip1_0_split
I1122 08:19:53.013555 18896 net.cpp:406] ip1_ip1_0_split <- ip1
I1122 08:19:53.013555 18896 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1122 08:19:53.013555 18896 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1122 08:19:53.013555 18896 net.cpp:122] Setting up ip1_ip1_0_split
I1122 08:19:53.013555 18896 net.cpp:129] Top shape: 100 10 (1000)
I1122 08:19:53.013555 18896 net.cpp:129] Top shape: 100 10 (1000)
I1122 08:19:53.013555 18896 net.cpp:137] Memory required for data: 412288000
I1122 08:19:53.013555 18896 layer_factory.cpp:58] Creating layer accuracy_training
I1122 08:19:53.013555 18896 net.cpp:84] Creating Layer accuracy_training
I1122 08:19:53.013555 18896 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1122 08:19:53.013555 18896 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1122 08:19:53.013555 18896 net.cpp:380] accuracy_training -> accuracy_training
I1122 08:19:53.013555 18896 net.cpp:122] Setting up accuracy_training
I1122 08:19:53.013555 18896 net.cpp:129] Top shape: (1)
I1122 08:19:53.013555 18896 net.cpp:137] Memory required for data: 412288004
I1122 08:19:53.013555 18896 layer_factory.cpp:58] Creating layer loss
I1122 08:19:53.013555 18896 net.cpp:84] Creating Layer loss
I1122 08:19:53.013555 18896 net.cpp:406] loss <- ip1_ip1_0_split_1
I1122 08:19:53.013555 18896 net.cpp:406] loss <- label_cifar_1_split_1
I1122 08:19:53.013555 18896 net.cpp:380] loss -> loss
I1122 08:19:53.013555 18896 layer_factory.cpp:58] Creating layer loss
I1122 08:19:53.014557 18896 net.cpp:122] Setting up loss
I1122 08:19:53.014557 18896 net.cpp:129] Top shape: (1)
I1122 08:19:53.014557 18896 net.cpp:132]     with loss weight 1
I1122 08:19:53.014557 18896 net.cpp:137] Memory required for data: 412288008
I1122 08:19:53.014557 18896 net.cpp:198] loss needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:200] accuracy_training does not need backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] ip1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] poolcp6 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] relu_conv12 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] scale_conv12 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] bn_conv12 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] conv12 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] relu_conv11 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] scale_conv11 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] bn_conv11 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] conv11 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] pool4_2 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] relu4_2 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] scale4_2 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] bn4_2 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] conv4_2 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] relu4_1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] scale4_1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] bn4_1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] conv4_1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] relu4 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] scale4 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] bn4 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] conv4 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] relu3 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] scale3 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] bn3 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] conv3 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] pool2_1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] relu2_2 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] scale2_2 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] bn2_2 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] conv2_2 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] relu2_1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] scale2_1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] bn2_1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] conv2_1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] relu2 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] scale2 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] bn2 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] conv2 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] relu1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] scale1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] bn1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:198] conv1 needs backward computation.
I1122 08:19:53.014557 18896 net.cpp:200] label_cifar_1_split does not need backward computation.
I1122 08:19:53.014557 18896 net.cpp:200] cifar does not need backward computation.
I1122 08:19:53.014557 18896 net.cpp:242] This network produces output accuracy_training
I1122 08:19:53.014557 18896 net.cpp:242] This network produces output loss
I1122 08:19:53.014557 18896 net.cpp:255] Network initialization done.
I1122 08:19:53.015553 18896 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 08:19:53.015553 18896 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1122 08:19:53.015553 18896 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 08:19:53.015553 18896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1122 08:19:53.015553 18896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1122 08:19:53.015553 18896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1122 08:19:53.015553 18896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1122 08:19:53.015553 18896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1122 08:19:53.015553 18896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1122 08:19:53.015553 18896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1122 08:19:53.015553 18896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1122 08:19:53.015553 18896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1122 08:19:53.015553 18896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1122 08:19:53.015553 18896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1122 08:19:53.015553 18896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1122 08:19:53.015553 18896 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_10L_Simple_NoGrpCon_NoDrp_300k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 75
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 85
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 95
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1122 08:19:53.015553 18896 layer_factory.cpp:58] Creating layer cifar
I1122 08:19:53.071563 18896 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1122 08:19:53.071563 18896 net.cpp:84] Creating Layer cifar
I1122 08:19:53.071563 18896 net.cpp:380] cifar -> data
I1122 08:19:53.071563 18896 net.cpp:380] cifar -> label
I1122 08:19:53.071563 18896 data_layer.cpp:45] output data size: 100,3,32,32
I1122 08:19:53.077563 18896 net.cpp:122] Setting up cifar
I1122 08:19:53.077563 18896 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1122 08:19:53.077563 18896 net.cpp:129] Top shape: 100 (100)
I1122 08:19:53.077563 18896 net.cpp:137] Memory required for data: 1229200
I1122 08:19:53.077563 18896 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1122 08:19:53.077563 18896 net.cpp:84] Creating Layer label_cifar_1_split
I1122 08:19:53.077563 18896 net.cpp:406] label_cifar_1_split <- label
I1122 08:19:53.077563 18896 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1122 08:19:53.077563 18896 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1122 08:19:53.077563 18896 net.cpp:122] Setting up label_cifar_1_split
I1122 08:19:53.077563 18896 net.cpp:129] Top shape: 100 (100)
I1122 08:19:53.077563 18896 net.cpp:129] Top shape: 100 (100)
I1122 08:19:53.077563 18896 net.cpp:137] Memory required for data: 1230000
I1122 08:19:53.077563 18896 layer_factory.cpp:58] Creating layer conv1
I1122 08:19:53.077563 18896 net.cpp:84] Creating Layer conv1
I1122 08:19:53.077563 18896 net.cpp:406] conv1 <- data
I1122 08:19:53.077563 18896 net.cpp:380] conv1 -> conv1
I1122 08:19:53.079563 18896 net.cpp:122] Setting up conv1
I1122 08:19:53.079563 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:53.079563 18896 net.cpp:137] Memory required for data: 17614000
I1122 08:19:53.079563 18896 layer_factory.cpp:58] Creating layer bn1
I1122 08:19:53.079563 18896 net.cpp:84] Creating Layer bn1
I1122 08:19:53.079563 18896 net.cpp:406] bn1 <- conv1
I1122 08:19:53.079563 18896 net.cpp:367] bn1 -> conv1 (in-place)
I1122 08:19:53.079563 20276 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 08:19:53.079563 18896 net.cpp:122] Setting up bn1
I1122 08:19:53.079563 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:53.079563 18896 net.cpp:137] Memory required for data: 33998000
I1122 08:19:53.079563 18896 layer_factory.cpp:58] Creating layer scale1
I1122 08:19:53.079563 18896 net.cpp:84] Creating Layer scale1
I1122 08:19:53.079563 18896 net.cpp:406] scale1 <- conv1
I1122 08:19:53.079563 18896 net.cpp:367] scale1 -> conv1 (in-place)
I1122 08:19:53.079563 18896 layer_factory.cpp:58] Creating layer scale1
I1122 08:19:53.079563 18896 net.cpp:122] Setting up scale1
I1122 08:19:53.079563 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:53.080564 18896 net.cpp:137] Memory required for data: 50382000
I1122 08:19:53.080564 18896 layer_factory.cpp:58] Creating layer relu1
I1122 08:19:53.080564 18896 net.cpp:84] Creating Layer relu1
I1122 08:19:53.080564 18896 net.cpp:406] relu1 <- conv1
I1122 08:19:53.080564 18896 net.cpp:367] relu1 -> conv1 (in-place)
I1122 08:19:53.080564 18896 net.cpp:122] Setting up relu1
I1122 08:19:53.080564 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:53.080564 18896 net.cpp:137] Memory required for data: 66766000
I1122 08:19:53.080564 18896 layer_factory.cpp:58] Creating layer conv2
I1122 08:19:53.080564 18896 net.cpp:84] Creating Layer conv2
I1122 08:19:53.080564 18896 net.cpp:406] conv2 <- conv1
I1122 08:19:53.080564 18896 net.cpp:380] conv2 -> conv2
I1122 08:19:53.081563 18896 net.cpp:122] Setting up conv2
I1122 08:19:53.081563 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:53.081563 18896 net.cpp:137] Memory required for data: 83150000
I1122 08:19:53.081563 18896 layer_factory.cpp:58] Creating layer bn2
I1122 08:19:53.081563 18896 net.cpp:84] Creating Layer bn2
I1122 08:19:53.081563 18896 net.cpp:406] bn2 <- conv2
I1122 08:19:53.081563 18896 net.cpp:367] bn2 -> conv2 (in-place)
I1122 08:19:53.081563 18896 net.cpp:122] Setting up bn2
I1122 08:19:53.081563 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:53.081563 18896 net.cpp:137] Memory required for data: 99534000
I1122 08:19:53.081563 18896 layer_factory.cpp:58] Creating layer scale2
I1122 08:19:53.081563 18896 net.cpp:84] Creating Layer scale2
I1122 08:19:53.081563 18896 net.cpp:406] scale2 <- conv2
I1122 08:19:53.081563 18896 net.cpp:367] scale2 -> conv2 (in-place)
I1122 08:19:53.081563 18896 layer_factory.cpp:58] Creating layer scale2
I1122 08:19:53.082563 18896 net.cpp:122] Setting up scale2
I1122 08:19:53.082563 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:53.082563 18896 net.cpp:137] Memory required for data: 115918000
I1122 08:19:53.082563 18896 layer_factory.cpp:58] Creating layer relu2
I1122 08:19:53.082563 18896 net.cpp:84] Creating Layer relu2
I1122 08:19:53.082563 18896 net.cpp:406] relu2 <- conv2
I1122 08:19:53.082563 18896 net.cpp:367] relu2 -> conv2 (in-place)
I1122 08:19:53.082563 18896 net.cpp:122] Setting up relu2
I1122 08:19:53.082563 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:53.082563 18896 net.cpp:137] Memory required for data: 132302000
I1122 08:19:53.082563 18896 layer_factory.cpp:58] Creating layer conv2_1
I1122 08:19:53.082563 18896 net.cpp:84] Creating Layer conv2_1
I1122 08:19:53.082563 18896 net.cpp:406] conv2_1 <- conv2
I1122 08:19:53.082563 18896 net.cpp:380] conv2_1 -> conv2_1
I1122 08:19:53.083562 18896 net.cpp:122] Setting up conv2_1
I1122 08:19:53.083562 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:53.083562 18896 net.cpp:137] Memory required for data: 148686000
I1122 08:19:53.083562 18896 layer_factory.cpp:58] Creating layer bn2_1
I1122 08:19:53.083562 18896 net.cpp:84] Creating Layer bn2_1
I1122 08:19:53.083562 18896 net.cpp:406] bn2_1 <- conv2_1
I1122 08:19:53.083562 18896 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1122 08:19:53.084563 18896 net.cpp:122] Setting up bn2_1
I1122 08:19:53.084563 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:53.084563 18896 net.cpp:137] Memory required for data: 165070000
I1122 08:19:53.084563 18896 layer_factory.cpp:58] Creating layer scale2_1
I1122 08:19:53.084563 18896 net.cpp:84] Creating Layer scale2_1
I1122 08:19:53.084563 18896 net.cpp:406] scale2_1 <- conv2_1
I1122 08:19:53.084563 18896 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1122 08:19:53.084563 18896 layer_factory.cpp:58] Creating layer scale2_1
I1122 08:19:53.084563 18896 net.cpp:122] Setting up scale2_1
I1122 08:19:53.084563 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:53.084563 18896 net.cpp:137] Memory required for data: 181454000
I1122 08:19:53.084563 18896 layer_factory.cpp:58] Creating layer relu2_1
I1122 08:19:53.084563 18896 net.cpp:84] Creating Layer relu2_1
I1122 08:19:53.084563 18896 net.cpp:406] relu2_1 <- conv2_1
I1122 08:19:53.084563 18896 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1122 08:19:53.084563 18896 net.cpp:122] Setting up relu2_1
I1122 08:19:53.084563 18896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 08:19:53.084563 18896 net.cpp:137] Memory required for data: 197838000
I1122 08:19:53.084563 18896 layer_factory.cpp:58] Creating layer conv2_2
I1122 08:19:53.085563 18896 net.cpp:84] Creating Layer conv2_2
I1122 08:19:53.085563 18896 net.cpp:406] conv2_2 <- conv2_1
I1122 08:19:53.085563 18896 net.cpp:380] conv2_2 -> conv2_2
I1122 08:19:53.086565 18896 net.cpp:122] Setting up conv2_2
I1122 08:19:53.086565 18896 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1122 08:19:53.086565 18896 net.cpp:137] Memory required for data: 220366000
I1122 08:19:53.086565 18896 layer_factory.cpp:58] Creating layer bn2_2
I1122 08:19:53.086565 18896 net.cpp:84] Creating Layer bn2_2
I1122 08:19:53.086565 18896 net.cpp:406] bn2_2 <- conv2_2
I1122 08:19:53.086565 18896 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1122 08:19:53.086565 18896 net.cpp:122] Setting up bn2_2
I1122 08:19:53.086565 18896 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1122 08:19:53.086565 18896 net.cpp:137] Memory required for data: 242894000
I1122 08:19:53.086565 18896 layer_factory.cpp:58] Creating layer scale2_2
I1122 08:19:53.086565 18896 net.cpp:84] Creating Layer scale2_2
I1122 08:19:53.086565 18896 net.cpp:406] scale2_2 <- conv2_2
I1122 08:19:53.086565 18896 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1122 08:19:53.086565 18896 layer_factory.cpp:58] Creating layer scale2_2
I1122 08:19:53.086565 18896 net.cpp:122] Setting up scale2_2
I1122 08:19:53.086565 18896 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1122 08:19:53.087563 18896 net.cpp:137] Memory required for data: 265422000
I1122 08:19:53.087563 18896 layer_factory.cpp:58] Creating layer relu2_2
I1122 08:19:53.087563 18896 net.cpp:84] Creating Layer relu2_2
I1122 08:19:53.087563 18896 net.cpp:406] relu2_2 <- conv2_2
I1122 08:19:53.087563 18896 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1122 08:19:53.087563 18896 net.cpp:122] Setting up relu2_2
I1122 08:19:53.087563 18896 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1122 08:19:53.087563 18896 net.cpp:137] Memory required for data: 287950000
I1122 08:19:53.087563 18896 layer_factory.cpp:58] Creating layer pool2_1
I1122 08:19:53.087563 18896 net.cpp:84] Creating Layer pool2_1
I1122 08:19:53.087563 18896 net.cpp:406] pool2_1 <- conv2_2
I1122 08:19:53.087563 18896 net.cpp:380] pool2_1 -> pool2_1
I1122 08:19:53.087563 18896 net.cpp:122] Setting up pool2_1
I1122 08:19:53.087563 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.087563 18896 net.cpp:137] Memory required for data: 293582000
I1122 08:19:53.087563 18896 layer_factory.cpp:58] Creating layer conv3
I1122 08:19:53.087563 18896 net.cpp:84] Creating Layer conv3
I1122 08:19:53.087563 18896 net.cpp:406] conv3 <- pool2_1
I1122 08:19:53.087563 18896 net.cpp:380] conv3 -> conv3
I1122 08:19:53.089563 18896 net.cpp:122] Setting up conv3
I1122 08:19:53.089563 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.089563 18896 net.cpp:137] Memory required for data: 299214000
I1122 08:19:53.089563 18896 layer_factory.cpp:58] Creating layer bn3
I1122 08:19:53.089563 18896 net.cpp:84] Creating Layer bn3
I1122 08:19:53.089563 18896 net.cpp:406] bn3 <- conv3
I1122 08:19:53.089563 18896 net.cpp:367] bn3 -> conv3 (in-place)
I1122 08:19:53.089563 18896 net.cpp:122] Setting up bn3
I1122 08:19:53.089563 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.089563 18896 net.cpp:137] Memory required for data: 304846000
I1122 08:19:53.089563 18896 layer_factory.cpp:58] Creating layer scale3
I1122 08:19:53.089563 18896 net.cpp:84] Creating Layer scale3
I1122 08:19:53.089563 18896 net.cpp:406] scale3 <- conv3
I1122 08:19:53.089563 18896 net.cpp:367] scale3 -> conv3 (in-place)
I1122 08:19:53.089563 18896 layer_factory.cpp:58] Creating layer scale3
I1122 08:19:53.089563 18896 net.cpp:122] Setting up scale3
I1122 08:19:53.089563 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.089563 18896 net.cpp:137] Memory required for data: 310478000
I1122 08:19:53.089563 18896 layer_factory.cpp:58] Creating layer relu3
I1122 08:19:53.089563 18896 net.cpp:84] Creating Layer relu3
I1122 08:19:53.089563 18896 net.cpp:406] relu3 <- conv3
I1122 08:19:53.089563 18896 net.cpp:367] relu3 -> conv3 (in-place)
I1122 08:19:53.090564 18896 net.cpp:122] Setting up relu3
I1122 08:19:53.090564 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.090564 18896 net.cpp:137] Memory required for data: 316110000
I1122 08:19:53.090564 18896 layer_factory.cpp:58] Creating layer conv4
I1122 08:19:53.090564 18896 net.cpp:84] Creating Layer conv4
I1122 08:19:53.090564 18896 net.cpp:406] conv4 <- conv3
I1122 08:19:53.090564 18896 net.cpp:380] conv4 -> conv4
I1122 08:19:53.091563 18896 net.cpp:122] Setting up conv4
I1122 08:19:53.091563 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.091563 18896 net.cpp:137] Memory required for data: 321742000
I1122 08:19:53.091563 18896 layer_factory.cpp:58] Creating layer bn4
I1122 08:19:53.091563 18896 net.cpp:84] Creating Layer bn4
I1122 08:19:53.091563 18896 net.cpp:406] bn4 <- conv4
I1122 08:19:53.091563 18896 net.cpp:367] bn4 -> conv4 (in-place)
I1122 08:19:53.091563 18896 net.cpp:122] Setting up bn4
I1122 08:19:53.091563 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.091563 18896 net.cpp:137] Memory required for data: 327374000
I1122 08:19:53.092566 18896 layer_factory.cpp:58] Creating layer scale4
I1122 08:19:53.092566 18896 net.cpp:84] Creating Layer scale4
I1122 08:19:53.092566 18896 net.cpp:406] scale4 <- conv4
I1122 08:19:53.092566 18896 net.cpp:367] scale4 -> conv4 (in-place)
I1122 08:19:53.092566 18896 layer_factory.cpp:58] Creating layer scale4
I1122 08:19:53.092566 18896 net.cpp:122] Setting up scale4
I1122 08:19:53.092566 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.092566 18896 net.cpp:137] Memory required for data: 333006000
I1122 08:19:53.092566 18896 layer_factory.cpp:58] Creating layer relu4
I1122 08:19:53.092566 18896 net.cpp:84] Creating Layer relu4
I1122 08:19:53.092566 18896 net.cpp:406] relu4 <- conv4
I1122 08:19:53.092566 18896 net.cpp:367] relu4 -> conv4 (in-place)
I1122 08:19:53.092566 18896 net.cpp:122] Setting up relu4
I1122 08:19:53.093564 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.093564 18896 net.cpp:137] Memory required for data: 338638000
I1122 08:19:53.093564 18896 layer_factory.cpp:58] Creating layer conv4_1
I1122 08:19:53.093564 18896 net.cpp:84] Creating Layer conv4_1
I1122 08:19:53.093564 18896 net.cpp:406] conv4_1 <- conv4
I1122 08:19:53.093564 18896 net.cpp:380] conv4_1 -> conv4_1
I1122 08:19:53.094563 18896 net.cpp:122] Setting up conv4_1
I1122 08:19:53.094563 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.094563 18896 net.cpp:137] Memory required for data: 344270000
I1122 08:19:53.094563 18896 layer_factory.cpp:58] Creating layer bn4_1
I1122 08:19:53.094563 18896 net.cpp:84] Creating Layer bn4_1
I1122 08:19:53.094563 18896 net.cpp:406] bn4_1 <- conv4_1
I1122 08:19:53.094563 18896 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1122 08:19:53.094563 18896 net.cpp:122] Setting up bn4_1
I1122 08:19:53.094563 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.094563 18896 net.cpp:137] Memory required for data: 349902000
I1122 08:19:53.094563 18896 layer_factory.cpp:58] Creating layer scale4_1
I1122 08:19:53.094563 18896 net.cpp:84] Creating Layer scale4_1
I1122 08:19:53.094563 18896 net.cpp:406] scale4_1 <- conv4_1
I1122 08:19:53.094563 18896 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1122 08:19:53.094563 18896 layer_factory.cpp:58] Creating layer scale4_1
I1122 08:19:53.094563 18896 net.cpp:122] Setting up scale4_1
I1122 08:19:53.094563 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.094563 18896 net.cpp:137] Memory required for data: 355534000
I1122 08:19:53.094563 18896 layer_factory.cpp:58] Creating layer relu4_1
I1122 08:19:53.094563 18896 net.cpp:84] Creating Layer relu4_1
I1122 08:19:53.094563 18896 net.cpp:406] relu4_1 <- conv4_1
I1122 08:19:53.094563 18896 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1122 08:19:53.095562 18896 net.cpp:122] Setting up relu4_1
I1122 08:19:53.095562 18896 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1122 08:19:53.095562 18896 net.cpp:137] Memory required for data: 361166000
I1122 08:19:53.095562 18896 layer_factory.cpp:58] Creating layer conv4_2
I1122 08:19:53.095562 18896 net.cpp:84] Creating Layer conv4_2
I1122 08:19:53.095562 18896 net.cpp:406] conv4_2 <- conv4_1
I1122 08:19:53.095562 18896 net.cpp:380] conv4_2 -> conv4_2
I1122 08:19:53.097563 18896 net.cpp:122] Setting up conv4_2
I1122 08:19:53.097563 18896 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1122 08:19:53.097563 18896 net.cpp:137] Memory required for data: 368846000
I1122 08:19:53.097563 18896 layer_factory.cpp:58] Creating layer bn4_2
I1122 08:19:53.097563 18896 net.cpp:84] Creating Layer bn4_2
I1122 08:19:53.097563 18896 net.cpp:406] bn4_2 <- conv4_2
I1122 08:19:53.097563 18896 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1122 08:19:53.097563 18896 net.cpp:122] Setting up bn4_2
I1122 08:19:53.097563 18896 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1122 08:19:53.097563 18896 net.cpp:137] Memory required for data: 376526000
I1122 08:19:53.097563 18896 layer_factory.cpp:58] Creating layer scale4_2
I1122 08:19:53.097563 18896 net.cpp:84] Creating Layer scale4_2
I1122 08:19:53.097563 18896 net.cpp:406] scale4_2 <- conv4_2
I1122 08:19:53.097563 18896 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1122 08:19:53.098563 18896 layer_factory.cpp:58] Creating layer scale4_2
I1122 08:19:53.098563 18896 net.cpp:122] Setting up scale4_2
I1122 08:19:53.098563 18896 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1122 08:19:53.098563 18896 net.cpp:137] Memory required for data: 384206000
I1122 08:19:53.098563 18896 layer_factory.cpp:58] Creating layer relu4_2
I1122 08:19:53.098563 18896 net.cpp:84] Creating Layer relu4_2
I1122 08:19:53.098563 18896 net.cpp:406] relu4_2 <- conv4_2
I1122 08:19:53.098563 18896 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1122 08:19:53.098563 18896 net.cpp:122] Setting up relu4_2
I1122 08:19:53.098563 18896 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1122 08:19:53.098563 18896 net.cpp:137] Memory required for data: 391886000
I1122 08:19:53.098563 18896 layer_factory.cpp:58] Creating layer pool4_2
I1122 08:19:53.098563 18896 net.cpp:84] Creating Layer pool4_2
I1122 08:19:53.098563 18896 net.cpp:406] pool4_2 <- conv4_2
I1122 08:19:53.098563 18896 net.cpp:380] pool4_2 -> pool4_2
I1122 08:19:53.098563 18896 net.cpp:122] Setting up pool4_2
I1122 08:19:53.098563 18896 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1122 08:19:53.098563 18896 net.cpp:137] Memory required for data: 393806000
I1122 08:19:53.098563 18896 layer_factory.cpp:58] Creating layer conv11
I1122 08:19:53.098563 18896 net.cpp:84] Creating Layer conv11
I1122 08:19:53.098563 18896 net.cpp:406] conv11 <- pool4_2
I1122 08:19:53.098563 18896 net.cpp:380] conv11 -> conv11
I1122 08:19:53.100564 18896 net.cpp:122] Setting up conv11
I1122 08:19:53.100564 18896 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 08:19:53.100564 18896 net.cpp:137] Memory required for data: 395982000
I1122 08:19:53.100564 18896 layer_factory.cpp:58] Creating layer bn_conv11
I1122 08:19:53.100564 18896 net.cpp:84] Creating Layer bn_conv11
I1122 08:19:53.100564 18896 net.cpp:406] bn_conv11 <- conv11
I1122 08:19:53.100564 18896 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1122 08:19:53.100564 18896 net.cpp:122] Setting up bn_conv11
I1122 08:19:53.100564 18896 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 08:19:53.100564 18896 net.cpp:137] Memory required for data: 398158000
I1122 08:19:53.100564 18896 layer_factory.cpp:58] Creating layer scale_conv11
I1122 08:19:53.100564 18896 net.cpp:84] Creating Layer scale_conv11
I1122 08:19:53.100564 18896 net.cpp:406] scale_conv11 <- conv11
I1122 08:19:53.100564 18896 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1122 08:19:53.100564 18896 layer_factory.cpp:58] Creating layer scale_conv11
I1122 08:19:53.100564 18896 net.cpp:122] Setting up scale_conv11
I1122 08:19:53.101563 18896 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 08:19:53.101563 18896 net.cpp:137] Memory required for data: 400334000
I1122 08:19:53.101563 18896 layer_factory.cpp:58] Creating layer relu_conv11
I1122 08:19:53.101563 18896 net.cpp:84] Creating Layer relu_conv11
I1122 08:19:53.101563 18896 net.cpp:406] relu_conv11 <- conv11
I1122 08:19:53.101563 18896 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1122 08:19:53.101563 18896 net.cpp:122] Setting up relu_conv11
I1122 08:19:53.101563 18896 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 08:19:53.101563 18896 net.cpp:137] Memory required for data: 402510000
I1122 08:19:53.101563 18896 layer_factory.cpp:58] Creating layer conv12
I1122 08:19:53.101563 18896 net.cpp:84] Creating Layer conv12
I1122 08:19:53.101563 18896 net.cpp:406] conv12 <- conv11
I1122 08:19:53.101563 18896 net.cpp:380] conv12 -> conv12
I1122 08:19:53.103564 18896 net.cpp:122] Setting up conv12
I1122 08:19:53.103564 18896 net.cpp:129] Top shape: 100 95 8 8 (608000)
I1122 08:19:53.103564 18896 net.cpp:137] Memory required for data: 404942000
I1122 08:19:53.103564 18896 layer_factory.cpp:58] Creating layer bn_conv12
I1122 08:19:53.103564 18896 net.cpp:84] Creating Layer bn_conv12
I1122 08:19:53.103564 18896 net.cpp:406] bn_conv12 <- conv12
I1122 08:19:53.103564 18896 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1122 08:19:53.103564 18896 net.cpp:122] Setting up bn_conv12
I1122 08:19:53.103564 18896 net.cpp:129] Top shape: 100 95 8 8 (608000)
I1122 08:19:53.103564 18896 net.cpp:137] Memory required for data: 407374000
I1122 08:19:53.103564 18896 layer_factory.cpp:58] Creating layer scale_conv12
I1122 08:19:53.103564 18896 net.cpp:84] Creating Layer scale_conv12
I1122 08:19:53.103564 18896 net.cpp:406] scale_conv12 <- conv12
I1122 08:19:53.103564 18896 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1122 08:19:53.103564 18896 layer_factory.cpp:58] Creating layer scale_conv12
I1122 08:19:53.103564 18896 net.cpp:122] Setting up scale_conv12
I1122 08:19:53.103564 18896 net.cpp:129] Top shape: 100 95 8 8 (608000)
I1122 08:19:53.103564 18896 net.cpp:137] Memory required for data: 409806000
I1122 08:19:53.103564 18896 layer_factory.cpp:58] Creating layer relu_conv12
I1122 08:19:53.103564 18896 net.cpp:84] Creating Layer relu_conv12
I1122 08:19:53.103564 18896 net.cpp:406] relu_conv12 <- conv12
I1122 08:19:53.103564 18896 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1122 08:19:53.104563 18896 net.cpp:122] Setting up relu_conv12
I1122 08:19:53.104563 18896 net.cpp:129] Top shape: 100 95 8 8 (608000)
I1122 08:19:53.104563 18896 net.cpp:137] Memory required for data: 412238000
I1122 08:19:53.104563 18896 layer_factory.cpp:58] Creating layer poolcp6
I1122 08:19:53.104563 18896 net.cpp:84] Creating Layer poolcp6
I1122 08:19:53.104563 18896 net.cpp:406] poolcp6 <- conv12
I1122 08:19:53.104563 18896 net.cpp:380] poolcp6 -> poolcp6
I1122 08:19:53.104563 18896 net.cpp:122] Setting up poolcp6
I1122 08:19:53.104563 18896 net.cpp:129] Top shape: 100 95 1 1 (9500)
I1122 08:19:53.104563 18896 net.cpp:137] Memory required for data: 412276000
I1122 08:19:53.104563 18896 layer_factory.cpp:58] Creating layer ip1
I1122 08:19:53.104563 18896 net.cpp:84] Creating Layer ip1
I1122 08:19:53.104563 18896 net.cpp:406] ip1 <- poolcp6
I1122 08:19:53.104563 18896 net.cpp:380] ip1 -> ip1
I1122 08:19:53.104563 18896 net.cpp:122] Setting up ip1
I1122 08:19:53.104563 18896 net.cpp:129] Top shape: 100 10 (1000)
I1122 08:19:53.104563 18896 net.cpp:137] Memory required for data: 412280000
I1122 08:19:53.104563 18896 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1122 08:19:53.104563 18896 net.cpp:84] Creating Layer ip1_ip1_0_split
I1122 08:19:53.104563 18896 net.cpp:406] ip1_ip1_0_split <- ip1
I1122 08:19:53.104563 18896 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1122 08:19:53.104563 18896 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1122 08:19:53.104563 18896 net.cpp:122] Setting up ip1_ip1_0_split
I1122 08:19:53.104563 18896 net.cpp:129] Top shape: 100 10 (1000)
I1122 08:19:53.104563 18896 net.cpp:129] Top shape: 100 10 (1000)
I1122 08:19:53.104563 18896 net.cpp:137] Memory required for data: 412288000
I1122 08:19:53.104563 18896 layer_factory.cpp:58] Creating layer accuracy
I1122 08:19:53.104563 18896 net.cpp:84] Creating Layer accuracy
I1122 08:19:53.104563 18896 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1122 08:19:53.104563 18896 net.cpp:406] accuracy <- label_cifar_1_split_0
I1122 08:19:53.104563 18896 net.cpp:380] accuracy -> accuracy
I1122 08:19:53.104563 18896 net.cpp:122] Setting up accuracy
I1122 08:19:53.104563 18896 net.cpp:129] Top shape: (1)
I1122 08:19:53.104563 18896 net.cpp:137] Memory required for data: 412288004
I1122 08:19:53.104563 18896 layer_factory.cpp:58] Creating layer loss
I1122 08:19:53.104563 18896 net.cpp:84] Creating Layer loss
I1122 08:19:53.104563 18896 net.cpp:406] loss <- ip1_ip1_0_split_1
I1122 08:19:53.104563 18896 net.cpp:406] loss <- label_cifar_1_split_1
I1122 08:19:53.104563 18896 net.cpp:380] loss -> loss
I1122 08:19:53.104563 18896 layer_factory.cpp:58] Creating layer loss
I1122 08:19:53.105563 18896 net.cpp:122] Setting up loss
I1122 08:19:53.105563 18896 net.cpp:129] Top shape: (1)
I1122 08:19:53.105563 18896 net.cpp:132]     with loss weight 1
I1122 08:19:53.105563 18896 net.cpp:137] Memory required for data: 412288008
I1122 08:19:53.105563 18896 net.cpp:198] loss needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:200] accuracy does not need backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] ip1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] poolcp6 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] relu_conv12 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] scale_conv12 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] bn_conv12 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] conv12 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] relu_conv11 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] scale_conv11 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] bn_conv11 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] conv11 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] pool4_2 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] relu4_2 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] scale4_2 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] bn4_2 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] conv4_2 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] relu4_1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] scale4_1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] bn4_1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] conv4_1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] relu4 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] scale4 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] bn4 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] conv4 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] relu3 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] scale3 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] bn3 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] conv3 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] pool2_1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] relu2_2 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] scale2_2 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] bn2_2 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] conv2_2 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] relu2_1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] scale2_1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] bn2_1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] conv2_1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] relu2 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] scale2 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] bn2 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] conv2 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] relu1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] scale1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] bn1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:198] conv1 needs backward computation.
I1122 08:19:53.105563 18896 net.cpp:200] label_cifar_1_split does not need backward computation.
I1122 08:19:53.105563 18896 net.cpp:200] cifar does not need backward computation.
I1122 08:19:53.105563 18896 net.cpp:242] This network produces output accuracy
I1122 08:19:53.105563 18896 net.cpp:242] This network produces output loss
I1122 08:19:53.105563 18896 net.cpp:255] Network initialization done.
I1122 08:19:53.105563 18896 solver.cpp:56] Solver scaffolding done.
I1122 08:19:53.109562 18896 caffe.cpp:249] Starting Optimization
I1122 08:19:53.109562 18896 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_10L_Simple_NoGrpCon_NoDrp_300k
I1122 08:19:53.109562 18896 solver.cpp:273] Learning Rate Policy: multistep
I1122 08:19:53.112563 18896 solver.cpp:330] Iteration 0, Testing net (#0)
I1122 08:19:53.113562 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:19:54.269299 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:19:54.313307 18896 solver.cpp:397]     Test net output #0: accuracy = 0.0939
I1122 08:19:54.313307 18896 solver.cpp:397]     Test net output #1: loss = 79.1356 (* 1 = 79.1356 loss)
I1122 08:19:54.396831 18896 solver.cpp:218] Iteration 0 (1.30412e+06 iter/s, 1.28622s/100 iters), loss = 3.75462
I1122 08:19:54.396831 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.07
I1122 08:19:54.396831 18896 solver.cpp:237]     Train net output #1: loss = 3.75462 (* 1 = 3.75462 loss)
I1122 08:19:54.396831 18896 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1122 08:19:58.966857 18896 solver.cpp:218] Iteration 100 (21.8841 iter/s, 4.56953s/100 iters), loss = 1.55404
I1122 08:19:58.966857 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.45
I1122 08:19:58.966857 18896 solver.cpp:237]     Train net output #1: loss = 1.55404 (* 1 = 1.55404 loss)
I1122 08:19:58.966857 18896 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1122 08:20:03.484169 18896 solver.cpp:218] Iteration 200 (22.1379 iter/s, 4.51714s/100 iters), loss = 1.59776
I1122 08:20:03.484169 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.47
I1122 08:20:03.484169 18896 solver.cpp:237]     Train net output #1: loss = 1.59776 (* 1 = 1.59776 loss)
I1122 08:20:03.484169 18896 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1122 08:20:08.018687 18896 solver.cpp:218] Iteration 300 (22.0558 iter/s, 4.53396s/100 iters), loss = 1.24885
I1122 08:20:08.018687 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.56
I1122 08:20:08.018687 18896 solver.cpp:237]     Train net output #1: loss = 1.24885 (* 1 = 1.24885 loss)
I1122 08:20:08.018687 18896 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1122 08:20:12.515615 18896 solver.cpp:218] Iteration 400 (22.2413 iter/s, 4.49613s/100 iters), loss = 1.17758
I1122 08:20:12.515615 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.56
I1122 08:20:12.515615 18896 solver.cpp:237]     Train net output #1: loss = 1.17758 (* 1 = 1.17758 loss)
I1122 08:20:12.515615 18896 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1122 08:20:16.800107 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:20:16.974143 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_500.caffemodel
I1122 08:20:16.991143 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_500.solverstate
I1122 08:20:16.995144 18896 solver.cpp:330] Iteration 500, Testing net (#0)
I1122 08:20:16.995144 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:20:18.096346 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:20:18.139834 18896 solver.cpp:397]     Test net output #0: accuracy = 0.4833
I1122 08:20:18.139834 18896 solver.cpp:397]     Test net output #1: loss = 1.43574 (* 1 = 1.43574 loss)
I1122 08:20:18.182344 18896 solver.cpp:218] Iteration 500 (17.648 iter/s, 5.66638s/100 iters), loss = 1.05208
I1122 08:20:18.182344 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1122 08:20:18.182344 18896 solver.cpp:237]     Train net output #1: loss = 1.05208 (* 1 = 1.05208 loss)
I1122 08:20:18.182344 18896 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1122 08:20:22.741664 18896 solver.cpp:218] Iteration 600 (21.9354 iter/s, 4.55884s/100 iters), loss = 0.942329
I1122 08:20:22.741664 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1122 08:20:22.741664 18896 solver.cpp:237]     Train net output #1: loss = 0.942329 (* 1 = 0.942329 loss)
I1122 08:20:22.741664 18896 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1122 08:20:27.291731 18896 solver.cpp:218] Iteration 700 (21.9798 iter/s, 4.54963s/100 iters), loss = 1.07512
I1122 08:20:27.291731 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1122 08:20:27.291731 18896 solver.cpp:237]     Train net output #1: loss = 1.07512 (* 1 = 1.07512 loss)
I1122 08:20:27.292230 18896 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1122 08:20:31.811686 18896 solver.cpp:218] Iteration 800 (22.1264 iter/s, 4.51949s/100 iters), loss = 0.827718
I1122 08:20:31.811686 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1122 08:20:31.811686 18896 solver.cpp:237]     Train net output #1: loss = 0.827718 (* 1 = 0.827718 loss)
I1122 08:20:31.811686 18896 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1122 08:20:36.320595 18896 solver.cpp:218] Iteration 900 (22.1814 iter/s, 4.50828s/100 iters), loss = 0.854501
I1122 08:20:36.320595 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1122 08:20:36.320595 18896 solver.cpp:237]     Train net output #1: loss = 0.854501 (* 1 = 0.854501 loss)
I1122 08:20:36.320595 18896 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1122 08:20:40.620242 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:20:40.794862 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_1000.caffemodel
I1122 08:20:40.806849 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_1000.solverstate
I1122 08:20:40.810863 18896 solver.cpp:330] Iteration 1000, Testing net (#0)
I1122 08:20:40.810863 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:20:41.912953 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:20:41.955940 18896 solver.cpp:397]     Test net output #0: accuracy = 0.5004
I1122 08:20:41.955940 18896 solver.cpp:397]     Test net output #1: loss = 1.42256 (* 1 = 1.42256 loss)
I1122 08:20:41.999747 18896 solver.cpp:218] Iteration 1000 (17.6088 iter/s, 5.67899s/100 iters), loss = 0.827789
I1122 08:20:41.999747 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1122 08:20:41.999747 18896 solver.cpp:237]     Train net output #1: loss = 0.827789 (* 1 = 0.827789 loss)
I1122 08:20:41.999747 18896 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1122 08:20:46.502820 18896 solver.cpp:218] Iteration 1100 (22.2067 iter/s, 4.50314s/100 iters), loss = 0.671615
I1122 08:20:46.503819 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 08:20:46.503819 18896 solver.cpp:237]     Train net output #1: loss = 0.671615 (* 1 = 0.671615 loss)
I1122 08:20:46.503819 18896 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1122 08:20:50.997735 18896 solver.cpp:218] Iteration 1200 (22.2531 iter/s, 4.49375s/100 iters), loss = 0.738039
I1122 08:20:50.997735 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1122 08:20:50.997735 18896 solver.cpp:237]     Train net output #1: loss = 0.738039 (* 1 = 0.738039 loss)
I1122 08:20:50.997735 18896 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1122 08:20:55.493597 18896 solver.cpp:218] Iteration 1300 (22.2442 iter/s, 4.49554s/100 iters), loss = 0.760363
I1122 08:20:55.493597 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1122 08:20:55.493597 18896 solver.cpp:237]     Train net output #1: loss = 0.760363 (* 1 = 0.760363 loss)
I1122 08:20:55.493597 18896 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1122 08:21:00.004300 18896 solver.cpp:218] Iteration 1400 (22.1699 iter/s, 4.51062s/100 iters), loss = 0.730835
I1122 08:21:00.004300 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1122 08:21:00.004300 18896 solver.cpp:237]     Train net output #1: loss = 0.730835 (* 1 = 0.730835 loss)
I1122 08:21:00.004300 18896 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1122 08:21:04.277619 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:21:04.453676 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_1500.caffemodel
I1122 08:21:04.463661 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_1500.solverstate
I1122 08:21:04.468662 18896 solver.cpp:330] Iteration 1500, Testing net (#0)
I1122 08:21:04.468662 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:21:05.573467 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:21:05.617341 18896 solver.cpp:397]     Test net output #0: accuracy = 0.6301
I1122 08:21:05.617341 18896 solver.cpp:397]     Test net output #1: loss = 1.0403 (* 1 = 1.0403 loss)
I1122 08:21:05.660322 18896 solver.cpp:218] Iteration 1500 (17.683 iter/s, 5.65516s/100 iters), loss = 0.706005
I1122 08:21:05.660322 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1122 08:21:05.660322 18896 solver.cpp:237]     Train net output #1: loss = 0.706005 (* 1 = 0.706005 loss)
I1122 08:21:05.660322 18896 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1122 08:21:10.178701 18896 solver.cpp:218] Iteration 1600 (22.1337 iter/s, 4.51801s/100 iters), loss = 0.518638
I1122 08:21:10.178701 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 08:21:10.178701 18896 solver.cpp:237]     Train net output #1: loss = 0.518638 (* 1 = 0.518638 loss)
I1122 08:21:10.178701 18896 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1122 08:21:14.685132 18896 solver.cpp:218] Iteration 1700 (22.1897 iter/s, 4.5066s/100 iters), loss = 0.593038
I1122 08:21:14.685132 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 08:21:14.685132 18896 solver.cpp:237]     Train net output #1: loss = 0.593038 (* 1 = 0.593038 loss)
I1122 08:21:14.685132 18896 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1122 08:21:19.190708 18896 solver.cpp:218] Iteration 1800 (22.1975 iter/s, 4.50502s/100 iters), loss = 0.731893
I1122 08:21:19.190708 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1122 08:21:19.190708 18896 solver.cpp:237]     Train net output #1: loss = 0.731893 (* 1 = 0.731893 loss)
I1122 08:21:19.190708 18896 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1122 08:21:23.700098 18896 solver.cpp:218] Iteration 1900 (22.1795 iter/s, 4.50867s/100 iters), loss = 0.556249
I1122 08:21:23.700098 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 08:21:23.700098 18896 solver.cpp:237]     Train net output #1: loss = 0.556249 (* 1 = 0.556249 loss)
I1122 08:21:23.700098 18896 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1122 08:21:27.993556 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:21:28.170202 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_2000.caffemodel
I1122 08:21:28.181201 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_2000.solverstate
I1122 08:21:28.185201 18896 solver.cpp:330] Iteration 2000, Testing net (#0)
I1122 08:21:28.185201 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:21:29.288702 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:21:29.331931 18896 solver.cpp:397]     Test net output #0: accuracy = 0.5416
I1122 08:21:29.331931 18896 solver.cpp:397]     Test net output #1: loss = 1.26861 (* 1 = 1.26861 loss)
I1122 08:21:29.374938 18896 solver.cpp:218] Iteration 2000 (17.6212 iter/s, 5.67499s/100 iters), loss = 0.595322
I1122 08:21:29.375943 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1122 08:21:29.375943 18896 solver.cpp:237]     Train net output #1: loss = 0.595322 (* 1 = 0.595322 loss)
I1122 08:21:29.375943 18896 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1122 08:21:33.888317 18896 solver.cpp:218] Iteration 2100 (22.1617 iter/s, 4.51229s/100 iters), loss = 0.492236
I1122 08:21:33.888317 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 08:21:33.888317 18896 solver.cpp:237]     Train net output #1: loss = 0.492236 (* 1 = 0.492236 loss)
I1122 08:21:33.888317 18896 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1122 08:21:38.431274 18896 solver.cpp:218] Iteration 2200 (22.0116 iter/s, 4.54306s/100 iters), loss = 0.605628
I1122 08:21:38.431274 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 08:21:38.431274 18896 solver.cpp:237]     Train net output #1: loss = 0.605628 (* 1 = 0.605628 loss)
I1122 08:21:38.431274 18896 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1122 08:21:42.989114 18896 solver.cpp:218] Iteration 2300 (21.9424 iter/s, 4.55739s/100 iters), loss = 0.639502
I1122 08:21:42.989114 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 08:21:42.989114 18896 solver.cpp:237]     Train net output #1: loss = 0.639502 (* 1 = 0.639502 loss)
I1122 08:21:42.989114 18896 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1122 08:21:47.511101 18896 solver.cpp:218] Iteration 2400 (22.1157 iter/s, 4.52168s/100 iters), loss = 0.517611
I1122 08:21:47.512102 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 08:21:47.512102 18896 solver.cpp:237]     Train net output #1: loss = 0.517611 (* 1 = 0.517611 loss)
I1122 08:21:47.512102 18896 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1122 08:21:51.789947 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:21:51.966820 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_2500.caffemodel
I1122 08:21:51.978801 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_2500.solverstate
I1122 08:21:51.982800 18896 solver.cpp:330] Iteration 2500, Testing net (#0)
I1122 08:21:51.982800 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:21:53.085587 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:21:53.129608 18896 solver.cpp:397]     Test net output #0: accuracy = 0.5687
I1122 08:21:53.129608 18896 solver.cpp:397]     Test net output #1: loss = 1.2018 (* 1 = 1.2018 loss)
I1122 08:21:53.172596 18896 solver.cpp:218] Iteration 2500 (17.6646 iter/s, 5.66104s/100 iters), loss = 0.530681
I1122 08:21:53.172596 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 08:21:53.172596 18896 solver.cpp:237]     Train net output #1: loss = 0.530681 (* 1 = 0.530681 loss)
I1122 08:21:53.172596 18896 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1122 08:21:57.751763 18896 solver.cpp:218] Iteration 2600 (21.8415 iter/s, 4.57845s/100 iters), loss = 0.416008
I1122 08:21:57.751763 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 08:21:57.751763 18896 solver.cpp:237]     Train net output #1: loss = 0.416008 (* 1 = 0.416008 loss)
I1122 08:21:57.751763 18896 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1122 08:22:02.310549 18896 solver.cpp:218] Iteration 2700 (21.9383 iter/s, 4.55824s/100 iters), loss = 0.55938
I1122 08:22:02.310549 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1122 08:22:02.310549 18896 solver.cpp:237]     Train net output #1: loss = 0.55938 (* 1 = 0.55938 loss)
I1122 08:22:02.310549 18896 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1122 08:22:06.812075 18896 solver.cpp:218] Iteration 2800 (22.2156 iter/s, 4.50135s/100 iters), loss = 0.561457
I1122 08:22:06.812075 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 08:22:06.812075 18896 solver.cpp:237]     Train net output #1: loss = 0.561457 (* 1 = 0.561457 loss)
I1122 08:22:06.812075 18896 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1122 08:22:11.315251 18896 solver.cpp:218] Iteration 2900 (22.2097 iter/s, 4.50253s/100 iters), loss = 0.593434
I1122 08:22:11.315251 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 08:22:11.315251 18896 solver.cpp:237]     Train net output #1: loss = 0.593434 (* 1 = 0.593434 loss)
I1122 08:22:11.315251 18896 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1122 08:22:15.598762 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:22:15.781904 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_3000.caffemodel
I1122 08:22:15.792899 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_3000.solverstate
I1122 08:22:15.797902 18896 solver.cpp:330] Iteration 3000, Testing net (#0)
I1122 08:22:15.797902 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:22:16.900707 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:22:16.944479 18896 solver.cpp:397]     Test net output #0: accuracy = 0.5972
I1122 08:22:16.944479 18896 solver.cpp:397]     Test net output #1: loss = 1.19939 (* 1 = 1.19939 loss)
I1122 08:22:16.987494 18896 solver.cpp:218] Iteration 3000 (17.6329 iter/s, 5.67121s/100 iters), loss = 0.483843
I1122 08:22:16.987494 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 08:22:16.987494 18896 solver.cpp:237]     Train net output #1: loss = 0.483843 (* 1 = 0.483843 loss)
I1122 08:22:16.987494 18896 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1122 08:22:21.559448 18896 solver.cpp:218] Iteration 3100 (21.8735 iter/s, 4.57174s/100 iters), loss = 0.391875
I1122 08:22:21.559448 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1122 08:22:21.559448 18896 solver.cpp:237]     Train net output #1: loss = 0.391875 (* 1 = 0.391875 loss)
I1122 08:22:21.559448 18896 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1122 08:22:26.129081 18896 solver.cpp:218] Iteration 3200 (21.887 iter/s, 4.56892s/100 iters), loss = 0.554941
I1122 08:22:26.129081 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 08:22:26.129081 18896 solver.cpp:237]     Train net output #1: loss = 0.554941 (* 1 = 0.554941 loss)
I1122 08:22:26.129081 18896 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1122 08:22:30.686755 18896 solver.cpp:218] Iteration 3300 (21.9422 iter/s, 4.55742s/100 iters), loss = 0.593721
I1122 08:22:30.686755 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 08:22:30.686755 18896 solver.cpp:237]     Train net output #1: loss = 0.593721 (* 1 = 0.593721 loss)
I1122 08:22:30.686755 18896 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1122 08:22:35.260427 18896 solver.cpp:218] Iteration 3400 (21.867 iter/s, 4.57311s/100 iters), loss = 0.48553
I1122 08:22:35.260427 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 08:22:35.260427 18896 solver.cpp:237]     Train net output #1: loss = 0.48553 (* 1 = 0.48553 loss)
I1122 08:22:35.260427 18896 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1122 08:22:39.558092 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:22:39.733978 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_3500.caffemodel
I1122 08:22:39.745465 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_3500.solverstate
I1122 08:22:39.749464 18896 solver.cpp:330] Iteration 3500, Testing net (#0)
I1122 08:22:39.749464 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:22:40.855154 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:22:40.898175 18896 solver.cpp:397]     Test net output #0: accuracy = 0.5825
I1122 08:22:40.898175 18896 solver.cpp:397]     Test net output #1: loss = 1.25478 (* 1 = 1.25478 loss)
I1122 08:22:40.941704 18896 solver.cpp:218] Iteration 3500 (17.6019 iter/s, 5.68121s/100 iters), loss = 0.515809
I1122 08:22:40.942220 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1122 08:22:40.942220 18896 solver.cpp:237]     Train net output #1: loss = 0.515809 (* 1 = 0.515809 loss)
I1122 08:22:40.942220 18896 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1122 08:22:45.499408 18896 solver.cpp:218] Iteration 3600 (21.9428 iter/s, 4.5573s/100 iters), loss = 0.448471
I1122 08:22:45.499408 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 08:22:45.499408 18896 solver.cpp:237]     Train net output #1: loss = 0.448471 (* 1 = 0.448471 loss)
I1122 08:22:45.499408 18896 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1122 08:22:50.057615 18896 solver.cpp:218] Iteration 3700 (21.9412 iter/s, 4.55764s/100 iters), loss = 0.483417
I1122 08:22:50.058115 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 08:22:50.058115 18896 solver.cpp:237]     Train net output #1: loss = 0.483417 (* 1 = 0.483417 loss)
I1122 08:22:50.058115 18896 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1122 08:22:54.603138 18896 solver.cpp:218] Iteration 3800 (22.0019 iter/s, 4.54506s/100 iters), loss = 0.518895
I1122 08:22:54.603138 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 08:22:54.603138 18896 solver.cpp:237]     Train net output #1: loss = 0.518895 (* 1 = 0.518895 loss)
I1122 08:22:54.603138 18896 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1122 08:22:59.176908 18896 solver.cpp:218] Iteration 3900 (21.8629 iter/s, 4.57395s/100 iters), loss = 0.60114
I1122 08:22:59.176908 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 08:22:59.177904 18896 solver.cpp:237]     Train net output #1: loss = 0.60114 (* 1 = 0.60114 loss)
I1122 08:22:59.177904 18896 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1122 08:23:03.527333 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:23:03.705421 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_4000.caffemodel
I1122 08:23:03.717422 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_4000.solverstate
I1122 08:23:03.721422 18896 solver.cpp:330] Iteration 4000, Testing net (#0)
I1122 08:23:03.721422 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:23:04.826867 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:23:04.870386 18896 solver.cpp:397]     Test net output #0: accuracy = 0.6845
I1122 08:23:04.870386 18896 solver.cpp:397]     Test net output #1: loss = 0.920827 (* 1 = 0.920827 loss)
I1122 08:23:04.913486 18896 solver.cpp:218] Iteration 4000 (17.4362 iter/s, 5.73519s/100 iters), loss = 0.549132
I1122 08:23:04.913486 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 08:23:04.913486 18896 solver.cpp:237]     Train net output #1: loss = 0.549132 (* 1 = 0.549132 loss)
I1122 08:23:04.913486 18896 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1122 08:23:09.461683 18896 solver.cpp:218] Iteration 4100 (21.9881 iter/s, 4.54791s/100 iters), loss = 0.403591
I1122 08:23:09.461683 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 08:23:09.461683 18896 solver.cpp:237]     Train net output #1: loss = 0.403591 (* 1 = 0.403591 loss)
I1122 08:23:09.461683 18896 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1122 08:23:14.038441 18896 solver.cpp:218] Iteration 4200 (21.8503 iter/s, 4.57659s/100 iters), loss = 0.448792
I1122 08:23:14.038441 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 08:23:14.038441 18896 solver.cpp:237]     Train net output #1: loss = 0.448792 (* 1 = 0.448792 loss)
I1122 08:23:14.038441 18896 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1122 08:23:18.558912 18896 solver.cpp:218] Iteration 4300 (22.1246 iter/s, 4.51985s/100 iters), loss = 0.450028
I1122 08:23:18.558912 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 08:23:18.558912 18896 solver.cpp:237]     Train net output #1: loss = 0.450028 (* 1 = 0.450028 loss)
I1122 08:23:18.558912 18896 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1122 08:23:23.089905 18896 solver.cpp:218] Iteration 4400 (22.0716 iter/s, 4.53072s/100 iters), loss = 0.462271
I1122 08:23:23.089905 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1122 08:23:23.089905 18896 solver.cpp:237]     Train net output #1: loss = 0.462271 (* 1 = 0.462271 loss)
I1122 08:23:23.089905 18896 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1122 08:23:27.396447 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:23:27.572475 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_4500.caffemodel
I1122 08:23:27.583992 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_4500.solverstate
I1122 08:23:27.587992 18896 solver.cpp:330] Iteration 4500, Testing net (#0)
I1122 08:23:27.587992 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:23:28.710647 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:23:28.755647 18896 solver.cpp:397]     Test net output #0: accuracy = 0.599
I1122 08:23:28.755647 18896 solver.cpp:397]     Test net output #1: loss = 1.3716 (* 1 = 1.3716 loss)
I1122 08:23:28.800673 18896 solver.cpp:218] Iteration 4500 (17.5133 iter/s, 5.70993s/100 iters), loss = 0.520772
I1122 08:23:28.800673 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1122 08:23:28.800673 18896 solver.cpp:237]     Train net output #1: loss = 0.520772 (* 1 = 0.520772 loss)
I1122 08:23:28.800673 18896 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1122 08:23:33.333540 18896 solver.cpp:218] Iteration 4600 (22.0632 iter/s, 4.53244s/100 iters), loss = 0.463594
I1122 08:23:33.333540 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 08:23:33.333540 18896 solver.cpp:237]     Train net output #1: loss = 0.463594 (* 1 = 0.463594 loss)
I1122 08:23:33.333540 18896 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1122 08:23:37.930670 18896 solver.cpp:218] Iteration 4700 (21.7545 iter/s, 4.59675s/100 iters), loss = 0.442437
I1122 08:23:37.930670 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1122 08:23:37.930670 18896 solver.cpp:237]     Train net output #1: loss = 0.442437 (* 1 = 0.442437 loss)
I1122 08:23:37.930670 18896 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1122 08:23:42.562808 18896 solver.cpp:218] Iteration 4800 (21.5886 iter/s, 4.63208s/100 iters), loss = 0.554464
I1122 08:23:42.562808 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 08:23:42.562808 18896 solver.cpp:237]     Train net output #1: loss = 0.554464 (* 1 = 0.554464 loss)
I1122 08:23:42.562808 18896 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1122 08:23:47.154738 18896 solver.cpp:218] Iteration 4900 (21.7788 iter/s, 4.59162s/100 iters), loss = 0.431467
I1122 08:23:47.154738 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1122 08:23:47.154738 18896 solver.cpp:237]     Train net output #1: loss = 0.431467 (* 1 = 0.431467 loss)
I1122 08:23:47.154738 18896 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1122 08:23:51.544559 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:23:51.724381 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_5000.caffemodel
I1122 08:23:51.735381 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_5000.solverstate
I1122 08:23:51.739403 18896 solver.cpp:330] Iteration 5000, Testing net (#0)
I1122 08:23:51.739403 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:23:52.847616 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:23:52.890648 18896 solver.cpp:397]     Test net output #0: accuracy = 0.6879
I1122 08:23:52.890648 18896 solver.cpp:397]     Test net output #1: loss = 0.888146 (* 1 = 0.888146 loss)
I1122 08:23:52.933401 18896 solver.cpp:218] Iteration 5000 (17.3083 iter/s, 5.77758s/100 iters), loss = 0.449545
I1122 08:23:52.933401 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 08:23:52.933401 18896 solver.cpp:237]     Train net output #1: loss = 0.449545 (* 1 = 0.449545 loss)
I1122 08:23:52.933401 18896 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1122 08:23:52.933401 18896 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1122 08:23:57.544030 18896 solver.cpp:218] Iteration 5100 (21.6876 iter/s, 4.61093s/100 iters), loss = 0.275652
I1122 08:23:57.545013 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:23:57.545013 18896 solver.cpp:237]     Train net output #1: loss = 0.275652 (* 1 = 0.275652 loss)
I1122 08:23:57.545013 18896 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1122 08:24:02.110366 18896 solver.cpp:218] Iteration 5200 (21.9051 iter/s, 4.56514s/100 iters), loss = 0.353581
I1122 08:24:02.110366 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1122 08:24:02.110366 18896 solver.cpp:237]     Train net output #1: loss = 0.353581 (* 1 = 0.353581 loss)
I1122 08:24:02.110366 18896 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1122 08:24:06.640143 18896 solver.cpp:218] Iteration 5300 (22.0788 iter/s, 4.52923s/100 iters), loss = 0.349606
I1122 08:24:06.640143 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1122 08:24:06.640143 18896 solver.cpp:237]     Train net output #1: loss = 0.349606 (* 1 = 0.349606 loss)
I1122 08:24:06.640143 18896 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1122 08:24:11.170277 18896 solver.cpp:218] Iteration 5400 (22.0739 iter/s, 4.53023s/100 iters), loss = 0.234407
I1122 08:24:11.170277 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:24:11.170277 18896 solver.cpp:237]     Train net output #1: loss = 0.234407 (* 1 = 0.234407 loss)
I1122 08:24:11.170277 18896 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1122 08:24:15.461360 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:24:15.638473 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_5500.caffemodel
I1122 08:24:15.649473 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_5500.solverstate
I1122 08:24:15.654474 18896 solver.cpp:330] Iteration 5500, Testing net (#0)
I1122 08:24:15.654474 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:24:16.768148 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:24:16.812168 18896 solver.cpp:397]     Test net output #0: accuracy = 0.8788
I1122 08:24:16.812168 18896 solver.cpp:397]     Test net output #1: loss = 0.368936 (* 1 = 0.368936 loss)
I1122 08:24:16.857180 18896 solver.cpp:218] Iteration 5500 (17.5875 iter/s, 5.68585s/100 iters), loss = 0.281825
I1122 08:24:16.857180 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 08:24:16.857180 18896 solver.cpp:237]     Train net output #1: loss = 0.281825 (* 1 = 0.281825 loss)
I1122 08:24:16.857180 18896 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1122 08:24:21.395998 18896 solver.cpp:218] Iteration 5600 (22.0345 iter/s, 4.53834s/100 iters), loss = 0.226528
I1122 08:24:21.395998 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:24:21.395998 18896 solver.cpp:237]     Train net output #1: loss = 0.226528 (* 1 = 0.226528 loss)
I1122 08:24:21.395998 18896 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1122 08:24:25.902829 18896 solver.cpp:218] Iteration 5700 (22.1899 iter/s, 4.50656s/100 iters), loss = 0.28989
I1122 08:24:25.902829 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 08:24:25.902829 18896 solver.cpp:237]     Train net output #1: loss = 0.28989 (* 1 = 0.28989 loss)
I1122 08:24:25.902829 18896 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1122 08:24:30.435390 18896 solver.cpp:218] Iteration 5800 (22.0638 iter/s, 4.53231s/100 iters), loss = 0.279931
I1122 08:24:30.435390 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:24:30.435390 18896 solver.cpp:237]     Train net output #1: loss = 0.279931 (* 1 = 0.279931 loss)
I1122 08:24:30.435390 18896 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1122 08:24:34.995576 18896 solver.cpp:218] Iteration 5900 (21.9325 iter/s, 4.55944s/100 iters), loss = 0.238902
I1122 08:24:34.995576 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:24:34.995576 18896 solver.cpp:237]     Train net output #1: loss = 0.238902 (* 1 = 0.238902 loss)
I1122 08:24:34.995576 18896 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1122 08:24:39.356442 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:24:39.539502 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_6000.caffemodel
I1122 08:24:39.550055 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_6000.solverstate
I1122 08:24:39.554075 18896 solver.cpp:330] Iteration 6000, Testing net (#0)
I1122 08:24:39.554075 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:24:40.663094 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:24:40.707079 18896 solver.cpp:397]     Test net output #0: accuracy = 0.8748
I1122 08:24:40.707079 18896 solver.cpp:397]     Test net output #1: loss = 0.378708 (* 1 = 0.378708 loss)
I1122 08:24:40.750126 18896 solver.cpp:218] Iteration 6000 (17.3784 iter/s, 5.75426s/100 iters), loss = 0.305066
I1122 08:24:40.750126 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1122 08:24:40.750126 18896 solver.cpp:237]     Train net output #1: loss = 0.305066 (* 1 = 0.305066 loss)
I1122 08:24:40.750126 18896 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1122 08:24:45.283713 18896 solver.cpp:218] Iteration 6100 (22.0583 iter/s, 4.53343s/100 iters), loss = 0.235191
I1122 08:24:45.283713 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:24:45.283713 18896 solver.cpp:237]     Train net output #1: loss = 0.235191 (* 1 = 0.235191 loss)
I1122 08:24:45.283713 18896 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1122 08:24:49.817726 18896 solver.cpp:218] Iteration 6200 (22.0581 iter/s, 4.53348s/100 iters), loss = 0.26162
I1122 08:24:49.817726 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 08:24:49.817726 18896 solver.cpp:237]     Train net output #1: loss = 0.26162 (* 1 = 0.26162 loss)
I1122 08:24:49.817726 18896 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1122 08:24:54.358099 18896 solver.cpp:218] Iteration 6300 (22.0273 iter/s, 4.53982s/100 iters), loss = 0.321669
I1122 08:24:54.358099 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 08:24:54.358099 18896 solver.cpp:237]     Train net output #1: loss = 0.321669 (* 1 = 0.321669 loss)
I1122 08:24:54.358099 18896 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1122 08:24:58.896553 18896 solver.cpp:218] Iteration 6400 (22.0374 iter/s, 4.53774s/100 iters), loss = 0.177398
I1122 08:24:58.896553 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:24:58.896553 18896 solver.cpp:237]     Train net output #1: loss = 0.177398 (* 1 = 0.177398 loss)
I1122 08:24:58.896553 18896 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1122 08:25:03.210584 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:25:03.386620 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_6500.caffemodel
I1122 08:25:03.398622 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_6500.solverstate
I1122 08:25:03.402621 18896 solver.cpp:330] Iteration 6500, Testing net (#0)
I1122 08:25:03.402621 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:25:04.509248 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:25:04.553253 18896 solver.cpp:397]     Test net output #0: accuracy = 0.8806
I1122 08:25:04.553253 18896 solver.cpp:397]     Test net output #1: loss = 0.359903 (* 1 = 0.359903 loss)
I1122 08:25:04.596266 18896 solver.cpp:218] Iteration 6500 (17.5456 iter/s, 5.69942s/100 iters), loss = 0.215501
I1122 08:25:04.596266 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:25:04.596266 18896 solver.cpp:237]     Train net output #1: loss = 0.215501 (* 1 = 0.215501 loss)
I1122 08:25:04.596266 18896 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1122 08:25:09.126042 18896 solver.cpp:218] Iteration 6600 (22.0766 iter/s, 4.52968s/100 iters), loss = 0.204547
I1122 08:25:09.126042 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:25:09.126042 18896 solver.cpp:237]     Train net output #1: loss = 0.204547 (* 1 = 0.204547 loss)
I1122 08:25:09.126042 18896 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1122 08:25:13.702841 18896 solver.cpp:218] Iteration 6700 (21.85 iter/s, 4.57666s/100 iters), loss = 0.238023
I1122 08:25:13.702841 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 08:25:13.702841 18896 solver.cpp:237]     Train net output #1: loss = 0.238023 (* 1 = 0.238023 loss)
I1122 08:25:13.702841 18896 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1122 08:25:18.307962 18896 solver.cpp:218] Iteration 6800 (21.7208 iter/s, 4.60388s/100 iters), loss = 0.28448
I1122 08:25:18.307962 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 08:25:18.307962 18896 solver.cpp:237]     Train net output #1: loss = 0.28448 (* 1 = 0.28448 loss)
I1122 08:25:18.307962 18896 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1122 08:25:22.891547 18896 solver.cpp:218] Iteration 6900 (21.8149 iter/s, 4.58402s/100 iters), loss = 0.170323
I1122 08:25:22.892547 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:25:22.892547 18896 solver.cpp:237]     Train net output #1: loss = 0.170322 (* 1 = 0.170322 loss)
I1122 08:25:22.892547 18896 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1122 08:25:27.237584 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:25:27.415644 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_7000.caffemodel
I1122 08:25:27.426626 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_7000.solverstate
I1122 08:25:27.431627 18896 solver.cpp:330] Iteration 7000, Testing net (#0)
I1122 08:25:27.431627 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:25:28.558356 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:25:28.602871 18896 solver.cpp:397]     Test net output #0: accuracy = 0.8854
I1122 08:25:28.602871 18896 solver.cpp:397]     Test net output #1: loss = 0.342775 (* 1 = 0.342775 loss)
I1122 08:25:28.646886 18896 solver.cpp:218] Iteration 7000 (17.3774 iter/s, 5.7546s/100 iters), loss = 0.216353
I1122 08:25:28.646886 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:25:28.646886 18896 solver.cpp:237]     Train net output #1: loss = 0.216353 (* 1 = 0.216353 loss)
I1122 08:25:28.646886 18896 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1122 08:25:33.262137 18896 solver.cpp:218] Iteration 7100 (21.6702 iter/s, 4.61462s/100 iters), loss = 0.179304
I1122 08:25:33.262637 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:25:33.262637 18896 solver.cpp:237]     Train net output #1: loss = 0.179304 (* 1 = 0.179304 loss)
I1122 08:25:33.262637 18896 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1122 08:25:37.853966 18896 solver.cpp:218] Iteration 7200 (21.779 iter/s, 4.59158s/100 iters), loss = 0.206421
I1122 08:25:37.853966 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:25:37.853966 18896 solver.cpp:237]     Train net output #1: loss = 0.206421 (* 1 = 0.206421 loss)
I1122 08:25:37.853966 18896 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1122 08:25:42.436445 18896 solver.cpp:218] Iteration 7300 (21.8238 iter/s, 4.58216s/100 iters), loss = 0.209147
I1122 08:25:42.436445 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:25:42.436445 18896 solver.cpp:237]     Train net output #1: loss = 0.209147 (* 1 = 0.209147 loss)
I1122 08:25:42.436445 18896 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1122 08:25:47.008308 18896 solver.cpp:218] Iteration 7400 (21.8773 iter/s, 4.57096s/100 iters), loss = 0.173223
I1122 08:25:47.008308 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:25:47.008308 18896 solver.cpp:237]     Train net output #1: loss = 0.173223 (* 1 = 0.173223 loss)
I1122 08:25:47.008308 18896 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1122 08:25:51.348285 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:25:51.530812 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_7500.caffemodel
I1122 08:25:51.542810 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_7500.solverstate
I1122 08:25:51.547809 18896 solver.cpp:330] Iteration 7500, Testing net (#0)
I1122 08:25:51.547809 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:25:52.675344 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:25:52.719856 18896 solver.cpp:397]     Test net output #0: accuracy = 0.8841
I1122 08:25:52.719856 18896 solver.cpp:397]     Test net output #1: loss = 0.34121 (* 1 = 0.34121 loss)
I1122 08:25:52.764390 18896 solver.cpp:218] Iteration 7500 (17.3744 iter/s, 5.75561s/100 iters), loss = 0.262083
I1122 08:25:52.764390 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:25:52.764390 18896 solver.cpp:237]     Train net output #1: loss = 0.262083 (* 1 = 0.262083 loss)
I1122 08:25:52.764390 18896 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1122 08:25:57.347147 18896 solver.cpp:218] Iteration 7600 (21.8222 iter/s, 4.5825s/100 iters), loss = 0.235247
I1122 08:25:57.347147 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:25:57.347147 18896 solver.cpp:237]     Train net output #1: loss = 0.235247 (* 1 = 0.235247 loss)
I1122 08:25:57.347147 18896 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1122 08:26:01.973548 18896 solver.cpp:218] Iteration 7700 (21.6185 iter/s, 4.62567s/100 iters), loss = 0.217506
I1122 08:26:01.973548 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:26:01.973548 18896 solver.cpp:237]     Train net output #1: loss = 0.217506 (* 1 = 0.217506 loss)
I1122 08:26:01.973548 18896 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1122 08:26:06.579841 18896 solver.cpp:218] Iteration 7800 (21.7106 iter/s, 4.60605s/100 iters), loss = 0.272808
I1122 08:26:06.579841 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:26:06.579841 18896 solver.cpp:237]     Train net output #1: loss = 0.272808 (* 1 = 0.272808 loss)
I1122 08:26:06.579841 18896 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1122 08:26:11.145422 18896 solver.cpp:218] Iteration 7900 (21.9034 iter/s, 4.56551s/100 iters), loss = 0.143878
I1122 08:26:11.145422 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:26:11.145422 18896 solver.cpp:237]     Train net output #1: loss = 0.143878 (* 1 = 0.143878 loss)
I1122 08:26:11.145422 18896 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1122 08:26:15.528378 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:26:15.705826 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_8000.caffemodel
I1122 08:26:15.716825 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_8000.solverstate
I1122 08:26:15.720825 18896 solver.cpp:330] Iteration 8000, Testing net (#0)
I1122 08:26:15.720825 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:26:16.836186 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:26:16.879214 18896 solver.cpp:397]     Test net output #0: accuracy = 0.888
I1122 08:26:16.879709 18896 solver.cpp:397]     Test net output #1: loss = 0.334598 (* 1 = 0.334598 loss)
I1122 08:26:16.922806 18896 solver.cpp:218] Iteration 8000 (17.3102 iter/s, 5.77695s/100 iters), loss = 0.169326
I1122 08:26:16.922806 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:26:16.922806 18896 solver.cpp:237]     Train net output #1: loss = 0.169326 (* 1 = 0.169326 loss)
I1122 08:26:16.922806 18896 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1122 08:26:21.527426 18896 solver.cpp:218] Iteration 8100 (21.7222 iter/s, 4.60359s/100 iters), loss = 0.16904
I1122 08:26:21.527426 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:26:21.527426 18896 solver.cpp:237]     Train net output #1: loss = 0.16904 (* 1 = 0.16904 loss)
I1122 08:26:21.527426 18896 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1122 08:26:26.108268 18896 solver.cpp:218] Iteration 8200 (21.8321 iter/s, 4.58042s/100 iters), loss = 0.21294
I1122 08:26:26.108268 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 08:26:26.108268 18896 solver.cpp:237]     Train net output #1: loss = 0.21294 (* 1 = 0.21294 loss)
I1122 08:26:26.108268 18896 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1122 08:26:30.670873 18896 solver.cpp:218] Iteration 8300 (21.9158 iter/s, 4.56293s/100 iters), loss = 0.20501
I1122 08:26:30.670873 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:26:30.670873 18896 solver.cpp:237]     Train net output #1: loss = 0.20501 (* 1 = 0.20501 loss)
I1122 08:26:30.670873 18896 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1122 08:26:35.270359 18896 solver.cpp:218] Iteration 8400 (21.7443 iter/s, 4.59891s/100 iters), loss = 0.161449
I1122 08:26:35.270359 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:26:35.270359 18896 solver.cpp:237]     Train net output #1: loss = 0.161449 (* 1 = 0.161449 loss)
I1122 08:26:35.270359 18896 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1122 08:26:39.615099 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:26:39.798108 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_8500.caffemodel
I1122 08:26:39.811112 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_8500.solverstate
I1122 08:26:39.815114 18896 solver.cpp:330] Iteration 8500, Testing net (#0)
I1122 08:26:39.815114 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:26:40.931586 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:26:40.975592 18896 solver.cpp:397]     Test net output #0: accuracy = 0.8884
I1122 08:26:40.975592 18896 solver.cpp:397]     Test net output #1: loss = 0.33395 (* 1 = 0.33395 loss)
I1122 08:26:41.018105 18896 solver.cpp:218] Iteration 8500 (17.4004 iter/s, 5.747s/100 iters), loss = 0.197994
I1122 08:26:41.018105 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:26:41.018105 18896 solver.cpp:237]     Train net output #1: loss = 0.197994 (* 1 = 0.197994 loss)
I1122 08:26:41.018105 18896 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1122 08:26:45.603873 18896 solver.cpp:218] Iteration 8600 (21.8096 iter/s, 4.58514s/100 iters), loss = 0.157053
I1122 08:26:45.603873 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:26:45.603873 18896 solver.cpp:237]     Train net output #1: loss = 0.157053 (* 1 = 0.157053 loss)
I1122 08:26:45.603873 18896 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1122 08:26:50.183009 18896 solver.cpp:218] Iteration 8700 (21.8396 iter/s, 4.57884s/100 iters), loss = 0.205894
I1122 08:26:50.183009 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:26:50.183009 18896 solver.cpp:237]     Train net output #1: loss = 0.205894 (* 1 = 0.205894 loss)
I1122 08:26:50.183009 18896 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1122 08:26:54.757737 18896 solver.cpp:218] Iteration 8800 (21.8601 iter/s, 4.57455s/100 iters), loss = 0.18303
I1122 08:26:54.757737 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:26:54.757737 18896 solver.cpp:237]     Train net output #1: loss = 0.18303 (* 1 = 0.18303 loss)
I1122 08:26:54.757737 18896 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1122 08:26:59.343952 18896 solver.cpp:218] Iteration 8900 (21.8051 iter/s, 4.58608s/100 iters), loss = 0.160939
I1122 08:26:59.343952 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:26:59.343952 18896 solver.cpp:237]     Train net output #1: loss = 0.160939 (* 1 = 0.160939 loss)
I1122 08:26:59.343952 18896 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1122 08:27:03.709817 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:27:03.891747 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_9000.caffemodel
I1122 08:27:03.904251 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_9000.solverstate
I1122 08:27:03.908752 18896 solver.cpp:330] Iteration 9000, Testing net (#0)
I1122 08:27:03.908752 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:27:05.018903 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:27:05.061900 18896 solver.cpp:397]     Test net output #0: accuracy = 0.8914
I1122 08:27:05.061900 18896 solver.cpp:397]     Test net output #1: loss = 0.323732 (* 1 = 0.323732 loss)
I1122 08:27:05.105927 18896 solver.cpp:218] Iteration 9000 (17.3583 iter/s, 5.76094s/100 iters), loss = 0.220126
I1122 08:27:05.105927 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:27:05.105927 18896 solver.cpp:237]     Train net output #1: loss = 0.220125 (* 1 = 0.220125 loss)
I1122 08:27:05.105927 18896 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1122 08:27:09.706514 18896 solver.cpp:218] Iteration 9100 (21.7392 iter/s, 4.59999s/100 iters), loss = 0.189751
I1122 08:27:09.706514 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:27:09.706514 18896 solver.cpp:237]     Train net output #1: loss = 0.189751 (* 1 = 0.189751 loss)
I1122 08:27:09.706514 18896 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1122 08:27:14.285282 18896 solver.cpp:218] Iteration 9200 (21.8402 iter/s, 4.57872s/100 iters), loss = 0.189262
I1122 08:27:14.285282 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:27:14.285282 18896 solver.cpp:237]     Train net output #1: loss = 0.189262 (* 1 = 0.189262 loss)
I1122 08:27:14.285282 18896 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1122 08:27:18.879892 18896 solver.cpp:218] Iteration 9300 (21.7665 iter/s, 4.59422s/100 iters), loss = 0.183067
I1122 08:27:18.879892 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:27:18.879892 18896 solver.cpp:237]     Train net output #1: loss = 0.183067 (* 1 = 0.183067 loss)
I1122 08:27:18.879892 18896 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1122 08:27:23.485843 18896 solver.cpp:218] Iteration 9400 (21.7147 iter/s, 4.60518s/100 iters), loss = 0.153719
I1122 08:27:23.485843 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:27:23.485843 18896 solver.cpp:237]     Train net output #1: loss = 0.153719 (* 1 = 0.153719 loss)
I1122 08:27:23.485843 18896 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1122 08:27:27.833027 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:27:28.015732 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_9500.caffemodel
I1122 08:27:28.026732 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_9500.solverstate
I1122 08:27:28.031734 18896 solver.cpp:330] Iteration 9500, Testing net (#0)
I1122 08:27:28.031734 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:27:29.158363 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:27:29.202875 18896 solver.cpp:397]     Test net output #0: accuracy = 0.8783
I1122 08:27:29.202875 18896 solver.cpp:397]     Test net output #1: loss = 0.357922 (* 1 = 0.357922 loss)
I1122 08:27:29.247390 18896 solver.cpp:218] Iteration 9500 (17.3583 iter/s, 5.76092s/100 iters), loss = 0.19776
I1122 08:27:29.247390 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 08:27:29.247390 18896 solver.cpp:237]     Train net output #1: loss = 0.19776 (* 1 = 0.19776 loss)
I1122 08:27:29.247390 18896 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1122 08:27:29.247390 18896 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1122 08:27:33.816834 18896 solver.cpp:218] Iteration 9600 (21.8823 iter/s, 4.5699s/100 iters), loss = 0.197213
I1122 08:27:33.817839 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:27:33.817839 18896 solver.cpp:237]     Train net output #1: loss = 0.197213 (* 1 = 0.197213 loss)
I1122 08:27:33.817839 18896 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1122 08:27:38.432063 18896 solver.cpp:218] Iteration 9700 (21.674 iter/s, 4.61382s/100 iters), loss = 0.181058
I1122 08:27:38.432063 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:27:38.432063 18896 solver.cpp:237]     Train net output #1: loss = 0.181058 (* 1 = 0.181058 loss)
I1122 08:27:38.432063 18896 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1122 08:27:42.986454 18896 solver.cpp:218] Iteration 9800 (21.9555 iter/s, 4.55468s/100 iters), loss = 0.175074
I1122 08:27:42.986454 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:27:42.986454 18896 solver.cpp:237]     Train net output #1: loss = 0.175074 (* 1 = 0.175074 loss)
I1122 08:27:42.986454 18896 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1122 08:27:47.552469 18896 solver.cpp:218] Iteration 9900 (21.9035 iter/s, 4.56548s/100 iters), loss = 0.164207
I1122 08:27:47.552469 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:27:47.552469 18896 solver.cpp:237]     Train net output #1: loss = 0.164207 (* 1 = 0.164207 loss)
I1122 08:27:47.552469 18896 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1122 08:27:51.889097 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:27:52.069645 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_10000.caffemodel
I1122 08:27:52.081645 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_10000.solverstate
I1122 08:27:52.086645 18896 solver.cpp:330] Iteration 10000, Testing net (#0)
I1122 08:27:52.086645 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:27:53.213416 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:27:53.257030 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9019
I1122 08:27:53.257030 18896 solver.cpp:397]     Test net output #1: loss = 0.295792 (* 1 = 0.295792 loss)
I1122 08:27:53.301033 18896 solver.cpp:218] Iteration 10000 (17.3982 iter/s, 5.74772s/100 iters), loss = 0.188513
I1122 08:27:53.301033 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:27:53.301033 18896 solver.cpp:237]     Train net output #1: loss = 0.188513 (* 1 = 0.188513 loss)
I1122 08:27:53.301033 18896 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1122 08:27:57.861682 18896 solver.cpp:218] Iteration 10100 (21.9287 iter/s, 4.56023s/100 iters), loss = 0.100482
I1122 08:27:57.861682 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:27:57.861682 18896 solver.cpp:237]     Train net output #1: loss = 0.100482 (* 1 = 0.100482 loss)
I1122 08:27:57.861682 18896 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1122 08:28:02.464709 18896 solver.cpp:218] Iteration 10200 (21.7276 iter/s, 4.60245s/100 iters), loss = 0.178633
I1122 08:28:02.464709 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:28:02.464709 18896 solver.cpp:237]     Train net output #1: loss = 0.178633 (* 1 = 0.178633 loss)
I1122 08:28:02.464709 18896 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1122 08:28:07.053333 18896 solver.cpp:218] Iteration 10300 (21.7959 iter/s, 4.58802s/100 iters), loss = 0.135439
I1122 08:28:07.053333 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:28:07.053333 18896 solver.cpp:237]     Train net output #1: loss = 0.135439 (* 1 = 0.135439 loss)
I1122 08:28:07.053333 18896 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1122 08:28:11.622346 18896 solver.cpp:218] Iteration 10400 (21.889 iter/s, 4.5685s/100 iters), loss = 0.127085
I1122 08:28:11.622346 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:28:11.622346 18896 solver.cpp:237]     Train net output #1: loss = 0.127085 (* 1 = 0.127085 loss)
I1122 08:28:11.622346 18896 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1122 08:28:15.983831 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:28:16.161399 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_10500.caffemodel
I1122 08:28:16.172391 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_10500.solverstate
I1122 08:28:16.176393 18896 solver.cpp:330] Iteration 10500, Testing net (#0)
I1122 08:28:16.176393 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:28:17.296905 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:28:17.342218 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9007
I1122 08:28:17.342218 18896 solver.cpp:397]     Test net output #1: loss = 0.293422 (* 1 = 0.293422 loss)
I1122 08:28:17.387223 18896 solver.cpp:218] Iteration 10500 (17.3488 iter/s, 5.76409s/100 iters), loss = 0.163945
I1122 08:28:17.387223 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:28:17.387223 18896 solver.cpp:237]     Train net output #1: loss = 0.163945 (* 1 = 0.163945 loss)
I1122 08:28:17.387223 18896 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1122 08:28:22.002508 18896 solver.cpp:218] Iteration 10600 (21.6665 iter/s, 4.61543s/100 iters), loss = 0.186325
I1122 08:28:22.002508 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:28:22.002508 18896 solver.cpp:237]     Train net output #1: loss = 0.186325 (* 1 = 0.186325 loss)
I1122 08:28:22.002508 18896 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1122 08:28:26.599054 18896 solver.cpp:218] Iteration 10700 (21.7576 iter/s, 4.5961s/100 iters), loss = 0.146057
I1122 08:28:26.599054 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:28:26.599054 18896 solver.cpp:237]     Train net output #1: loss = 0.146057 (* 1 = 0.146057 loss)
I1122 08:28:26.599054 18896 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1122 08:28:31.163414 18896 solver.cpp:218] Iteration 10800 (21.9127 iter/s, 4.56356s/100 iters), loss = 0.193063
I1122 08:28:31.163414 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:28:31.163414 18896 solver.cpp:237]     Train net output #1: loss = 0.193063 (* 1 = 0.193063 loss)
I1122 08:28:31.163414 18896 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1122 08:28:35.770453 18896 solver.cpp:218] Iteration 10900 (21.7089 iter/s, 4.6064s/100 iters), loss = 0.135463
I1122 08:28:35.770453 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:28:35.770453 18896 solver.cpp:237]     Train net output #1: loss = 0.135464 (* 1 = 0.135464 loss)
I1122 08:28:35.770453 18896 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1122 08:28:40.143124 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:28:40.326140 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_11000.caffemodel
I1122 08:28:40.338145 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_11000.solverstate
I1122 08:28:40.342645 18896 solver.cpp:330] Iteration 11000, Testing net (#0)
I1122 08:28:40.342645 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:28:41.469584 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:28:41.513568 18896 solver.cpp:397]     Test net output #0: accuracy = 0.903
I1122 08:28:41.513568 18896 solver.cpp:397]     Test net output #1: loss = 0.292736 (* 1 = 0.292736 loss)
I1122 08:28:41.558614 18896 solver.cpp:218] Iteration 11000 (17.2761 iter/s, 5.78833s/100 iters), loss = 0.152773
I1122 08:28:41.558614 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:28:41.558614 18896 solver.cpp:237]     Train net output #1: loss = 0.152773 (* 1 = 0.152773 loss)
I1122 08:28:41.558614 18896 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1122 08:28:46.122309 18896 solver.cpp:218] Iteration 11100 (21.9161 iter/s, 4.56286s/100 iters), loss = 0.133295
I1122 08:28:46.122309 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:28:46.122309 18896 solver.cpp:237]     Train net output #1: loss = 0.133295 (* 1 = 0.133295 loss)
I1122 08:28:46.122309 18896 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1122 08:28:50.681331 18896 solver.cpp:218] Iteration 11200 (21.935 iter/s, 4.55892s/100 iters), loss = 0.158268
I1122 08:28:50.681331 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:28:50.681331 18896 solver.cpp:237]     Train net output #1: loss = 0.158268 (* 1 = 0.158268 loss)
I1122 08:28:50.681331 18896 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1122 08:28:55.236794 18896 solver.cpp:218] Iteration 11300 (21.9563 iter/s, 4.5545s/100 iters), loss = 0.169048
I1122 08:28:55.236794 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:28:55.236794 18896 solver.cpp:237]     Train net output #1: loss = 0.169048 (* 1 = 0.169048 loss)
I1122 08:28:55.236794 18896 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1122 08:28:59.816902 18896 solver.cpp:218] Iteration 11400 (21.833 iter/s, 4.58023s/100 iters), loss = 0.130626
I1122 08:28:59.816902 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:28:59.816902 18896 solver.cpp:237]     Train net output #1: loss = 0.130626 (* 1 = 0.130626 loss)
I1122 08:28:59.816902 18896 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1122 08:29:04.201369 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:29:04.384399 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_11500.caffemodel
I1122 08:29:04.395398 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_11500.solverstate
I1122 08:29:04.400399 18896 solver.cpp:330] Iteration 11500, Testing net (#0)
I1122 08:29:04.400399 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:29:05.506033 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:29:05.550038 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9045
I1122 08:29:05.550038 18896 solver.cpp:397]     Test net output #1: loss = 0.294022 (* 1 = 0.294022 loss)
I1122 08:29:05.592864 18896 solver.cpp:218] Iteration 11500 (17.3161 iter/s, 5.77497s/100 iters), loss = 0.215638
I1122 08:29:05.592864 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:29:05.592864 18896 solver.cpp:237]     Train net output #1: loss = 0.215638 (* 1 = 0.215638 loss)
I1122 08:29:05.592864 18896 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1122 08:29:10.177273 18896 solver.cpp:218] Iteration 11600 (21.8135 iter/s, 4.58432s/100 iters), loss = 0.151052
I1122 08:29:10.177273 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:29:10.177273 18896 solver.cpp:237]     Train net output #1: loss = 0.151052 (* 1 = 0.151052 loss)
I1122 08:29:10.177273 18896 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1122 08:29:14.733003 18896 solver.cpp:218] Iteration 11700 (21.9524 iter/s, 4.55531s/100 iters), loss = 0.15945
I1122 08:29:14.733003 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:29:14.733003 18896 solver.cpp:237]     Train net output #1: loss = 0.15945 (* 1 = 0.15945 loss)
I1122 08:29:14.733003 18896 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1122 08:29:19.302320 18896 solver.cpp:218] Iteration 11800 (21.8856 iter/s, 4.56922s/100 iters), loss = 0.174213
I1122 08:29:19.302320 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:29:19.302320 18896 solver.cpp:237]     Train net output #1: loss = 0.174213 (* 1 = 0.174213 loss)
I1122 08:29:19.302320 18896 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1122 08:29:23.904430 18896 solver.cpp:218] Iteration 11900 (21.7307 iter/s, 4.60179s/100 iters), loss = 0.108597
I1122 08:29:23.905427 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:29:23.905427 18896 solver.cpp:237]     Train net output #1: loss = 0.108597 (* 1 = 0.108597 loss)
I1122 08:29:23.905427 18896 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1122 08:29:28.251397 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:29:28.435396 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_12000.caffemodel
I1122 08:29:28.448397 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_12000.solverstate
I1122 08:29:28.452898 18896 solver.cpp:330] Iteration 12000, Testing net (#0)
I1122 08:29:28.452898 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:29:29.582657 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:29:29.627658 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9042
I1122 08:29:29.627658 18896 solver.cpp:397]     Test net output #1: loss = 0.291203 (* 1 = 0.291203 loss)
I1122 08:29:29.671666 18896 solver.cpp:218] Iteration 12000 (17.3428 iter/s, 5.76608s/100 iters), loss = 0.212994
I1122 08:29:29.671666 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:29:29.671666 18896 solver.cpp:237]     Train net output #1: loss = 0.212994 (* 1 = 0.212994 loss)
I1122 08:29:29.671666 18896 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1122 08:29:34.243971 18896 solver.cpp:218] Iteration 12100 (21.872 iter/s, 4.57205s/100 iters), loss = 0.186354
I1122 08:29:34.243971 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:29:34.243971 18896 solver.cpp:237]     Train net output #1: loss = 0.186354 (* 1 = 0.186354 loss)
I1122 08:29:34.243971 18896 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1122 08:29:38.859158 18896 solver.cpp:218] Iteration 12200 (21.6718 iter/s, 4.61429s/100 iters), loss = 0.11863
I1122 08:29:38.859158 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:29:38.859158 18896 solver.cpp:237]     Train net output #1: loss = 0.11863 (* 1 = 0.11863 loss)
I1122 08:29:38.859158 18896 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1122 08:29:43.449720 18896 solver.cpp:218] Iteration 12300 (21.782 iter/s, 4.59094s/100 iters), loss = 0.150323
I1122 08:29:43.450722 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:29:43.450722 18896 solver.cpp:237]     Train net output #1: loss = 0.150323 (* 1 = 0.150323 loss)
I1122 08:29:43.450722 18896 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1122 08:29:48.026527 18896 solver.cpp:218] Iteration 12400 (21.8523 iter/s, 4.57617s/100 iters), loss = 0.140349
I1122 08:29:48.026527 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:29:48.026527 18896 solver.cpp:237]     Train net output #1: loss = 0.140349 (* 1 = 0.140349 loss)
I1122 08:29:48.026527 18896 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1122 08:29:52.402603 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:29:52.581337 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_12500.caffemodel
I1122 08:29:52.592320 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_12500.solverstate
I1122 08:29:52.596340 18896 solver.cpp:330] Iteration 12500, Testing net (#0)
I1122 08:29:52.596340 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:29:53.705795 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:29:53.750314 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9023
I1122 08:29:53.750314 18896 solver.cpp:397]     Test net output #1: loss = 0.291799 (* 1 = 0.291799 loss)
I1122 08:29:53.795331 18896 solver.cpp:218] Iteration 12500 (17.3381 iter/s, 5.76765s/100 iters), loss = 0.180001
I1122 08:29:53.795331 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:29:53.795331 18896 solver.cpp:237]     Train net output #1: loss = 0.180001 (* 1 = 0.180001 loss)
I1122 08:29:53.795331 18896 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1122 08:29:58.359280 18896 solver.cpp:218] Iteration 12600 (21.9133 iter/s, 4.56345s/100 iters), loss = 0.195496
I1122 08:29:58.359280 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:29:58.359280 18896 solver.cpp:237]     Train net output #1: loss = 0.195496 (* 1 = 0.195496 loss)
I1122 08:29:58.359280 18896 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1122 08:30:02.950142 18896 solver.cpp:218] Iteration 12700 (21.7854 iter/s, 4.59023s/100 iters), loss = 0.184325
I1122 08:30:02.950142 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:30:02.950142 18896 solver.cpp:237]     Train net output #1: loss = 0.184325 (* 1 = 0.184325 loss)
I1122 08:30:02.950142 18896 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1122 08:30:07.546615 18896 solver.cpp:218] Iteration 12800 (21.7548 iter/s, 4.5967s/100 iters), loss = 0.141871
I1122 08:30:07.546615 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:30:07.546615 18896 solver.cpp:237]     Train net output #1: loss = 0.141871 (* 1 = 0.141871 loss)
I1122 08:30:07.546615 18896 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1122 08:30:12.101732 18896 solver.cpp:218] Iteration 12900 (21.9544 iter/s, 4.55489s/100 iters), loss = 0.0859288
I1122 08:30:12.101732 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:30:12.101732 18896 solver.cpp:237]     Train net output #1: loss = 0.0859288 (* 1 = 0.0859288 loss)
I1122 08:30:12.101732 18896 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1122 08:30:16.448791 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:30:16.631326 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_13000.caffemodel
I1122 08:30:16.645328 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_13000.solverstate
I1122 08:30:16.649329 18896 solver.cpp:330] Iteration 13000, Testing net (#0)
I1122 08:30:16.649329 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:30:17.767601 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:30:17.812247 18896 solver.cpp:397]     Test net output #0: accuracy = 0.903
I1122 08:30:17.812247 18896 solver.cpp:397]     Test net output #1: loss = 0.292828 (* 1 = 0.292828 loss)
I1122 08:30:17.857240 18896 solver.cpp:218] Iteration 13000 (17.3782 iter/s, 5.75435s/100 iters), loss = 0.171258
I1122 08:30:17.857240 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:30:17.857240 18896 solver.cpp:237]     Train net output #1: loss = 0.171258 (* 1 = 0.171258 loss)
I1122 08:30:17.857240 18896 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1122 08:30:22.463588 18896 solver.cpp:218] Iteration 13100 (21.71 iter/s, 4.60618s/100 iters), loss = 0.143762
I1122 08:30:22.463588 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:30:22.463588 18896 solver.cpp:237]     Train net output #1: loss = 0.143762 (* 1 = 0.143762 loss)
I1122 08:30:22.463588 18896 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1122 08:30:27.069994 18896 solver.cpp:218] Iteration 13200 (21.7124 iter/s, 4.60566s/100 iters), loss = 0.128465
I1122 08:30:27.069994 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:30:27.069994 18896 solver.cpp:237]     Train net output #1: loss = 0.128465 (* 1 = 0.128465 loss)
I1122 08:30:27.069994 18896 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1122 08:30:31.634294 18896 solver.cpp:218] Iteration 13300 (21.9112 iter/s, 4.56387s/100 iters), loss = 0.12834
I1122 08:30:31.634294 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:30:31.634294 18896 solver.cpp:237]     Train net output #1: loss = 0.12834 (* 1 = 0.12834 loss)
I1122 08:30:31.634294 18896 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1122 08:30:36.224831 18896 solver.cpp:218] Iteration 13400 (21.7864 iter/s, 4.59003s/100 iters), loss = 0.109419
I1122 08:30:36.224831 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:30:36.224831 18896 solver.cpp:237]     Train net output #1: loss = 0.109419 (* 1 = 0.109419 loss)
I1122 08:30:36.224831 18896 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1122 08:30:40.619611 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:30:40.801614 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_13500.caffemodel
I1122 08:30:40.813613 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_13500.solverstate
I1122 08:30:40.817615 18896 solver.cpp:330] Iteration 13500, Testing net (#0)
I1122 08:30:40.817615 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:30:41.939388 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:30:41.983376 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9034
I1122 08:30:41.983376 18896 solver.cpp:397]     Test net output #1: loss = 0.291767 (* 1 = 0.291767 loss)
I1122 08:30:42.025413 18896 solver.cpp:218] Iteration 13500 (17.2385 iter/s, 5.80098s/100 iters), loss = 0.141851
I1122 08:30:42.025413 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:30:42.025413 18896 solver.cpp:237]     Train net output #1: loss = 0.141851 (* 1 = 0.141851 loss)
I1122 08:30:42.025413 18896 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1122 08:30:46.594413 18896 solver.cpp:218] Iteration 13600 (21.8912 iter/s, 4.56804s/100 iters), loss = 0.148263
I1122 08:30:46.594413 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:30:46.594413 18896 solver.cpp:237]     Train net output #1: loss = 0.148263 (* 1 = 0.148263 loss)
I1122 08:30:46.594413 18896 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1122 08:30:51.152837 18896 solver.cpp:218] Iteration 13700 (21.9368 iter/s, 4.55854s/100 iters), loss = 0.178743
I1122 08:30:51.153841 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:30:51.153841 18896 solver.cpp:237]     Train net output #1: loss = 0.178743 (* 1 = 0.178743 loss)
I1122 08:30:51.153841 18896 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1122 08:30:55.732409 18896 solver.cpp:218] Iteration 13800 (21.8419 iter/s, 4.57835s/100 iters), loss = 0.135981
I1122 08:30:55.732409 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 08:30:55.732409 18896 solver.cpp:237]     Train net output #1: loss = 0.135981 (* 1 = 0.135981 loss)
I1122 08:30:55.732409 18896 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1122 08:31:00.301465 18896 solver.cpp:218] Iteration 13900 (21.887 iter/s, 4.56893s/100 iters), loss = 0.09807
I1122 08:31:00.301465 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:31:00.301964 18896 solver.cpp:237]     Train net output #1: loss = 0.09807 (* 1 = 0.09807 loss)
I1122 08:31:00.301964 18896 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1122 08:31:04.660218 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:31:04.836836 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_14000.caffemodel
I1122 08:31:04.848836 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_14000.solverstate
I1122 08:31:04.852836 18896 solver.cpp:330] Iteration 14000, Testing net (#0)
I1122 08:31:04.852836 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:31:05.960810 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:31:06.004325 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9033
I1122 08:31:06.004842 18896 solver.cpp:397]     Test net output #1: loss = 0.291509 (* 1 = 0.291509 loss)
I1122 08:31:06.048352 18896 solver.cpp:218] Iteration 14000 (17.4026 iter/s, 5.74627s/100 iters), loss = 0.167767
I1122 08:31:06.048352 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:31:06.048352 18896 solver.cpp:237]     Train net output #1: loss = 0.167766 (* 1 = 0.167766 loss)
I1122 08:31:06.048352 18896 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1122 08:31:10.632284 18896 solver.cpp:218] Iteration 14100 (21.8147 iter/s, 4.58406s/100 iters), loss = 0.147216
I1122 08:31:10.632284 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:31:10.632284 18896 solver.cpp:237]     Train net output #1: loss = 0.147216 (* 1 = 0.147216 loss)
I1122 08:31:10.632284 18896 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1122 08:31:15.186342 18896 solver.cpp:218] Iteration 14200 (21.9636 iter/s, 4.55298s/100 iters), loss = 0.142615
I1122 08:31:15.186342 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:31:15.186342 18896 solver.cpp:237]     Train net output #1: loss = 0.142615 (* 1 = 0.142615 loss)
I1122 08:31:15.186342 18896 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1122 08:31:19.754464 18896 solver.cpp:218] Iteration 14300 (21.8907 iter/s, 4.56815s/100 iters), loss = 0.142277
I1122 08:31:19.754464 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:31:19.754464 18896 solver.cpp:237]     Train net output #1: loss = 0.142277 (* 1 = 0.142277 loss)
I1122 08:31:19.754464 18896 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1122 08:31:24.344450 18896 solver.cpp:218] Iteration 14400 (21.7881 iter/s, 4.58965s/100 iters), loss = 0.101139
I1122 08:31:24.344450 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:31:24.344450 18896 solver.cpp:237]     Train net output #1: loss = 0.101139 (* 1 = 0.101139 loss)
I1122 08:31:24.344450 18896 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1122 08:31:28.682119 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:31:28.862967 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_14500.caffemodel
I1122 08:31:28.874963 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_14500.solverstate
I1122 08:31:28.879962 18896 solver.cpp:330] Iteration 14500, Testing net (#0)
I1122 08:31:28.879962 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:31:30.004819 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:31:30.049863 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9036
I1122 08:31:30.049863 18896 solver.cpp:397]     Test net output #1: loss = 0.290981 (* 1 = 0.290981 loss)
I1122 08:31:30.094383 18896 solver.cpp:218] Iteration 14500 (17.3944 iter/s, 5.74899s/100 iters), loss = 0.188706
I1122 08:31:30.094383 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 08:31:30.094383 18896 solver.cpp:237]     Train net output #1: loss = 0.188706 (* 1 = 0.188706 loss)
I1122 08:31:30.094383 18896 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1122 08:31:34.665612 18896 solver.cpp:218] Iteration 14600 (21.8785 iter/s, 4.5707s/100 iters), loss = 0.12329
I1122 08:31:34.665612 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:31:34.665612 18896 solver.cpp:237]     Train net output #1: loss = 0.12329 (* 1 = 0.12329 loss)
I1122 08:31:34.665612 18896 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1122 08:31:39.237490 18896 solver.cpp:218] Iteration 14700 (21.874 iter/s, 4.57164s/100 iters), loss = 0.166129
I1122 08:31:39.237490 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:31:39.237490 18896 solver.cpp:237]     Train net output #1: loss = 0.166129 (* 1 = 0.166129 loss)
I1122 08:31:39.237490 18896 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1122 08:31:43.847692 18896 solver.cpp:218] Iteration 14800 (21.6937 iter/s, 4.60963s/100 iters), loss = 0.161029
I1122 08:31:43.847692 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:31:43.847692 18896 solver.cpp:237]     Train net output #1: loss = 0.161029 (* 1 = 0.161029 loss)
I1122 08:31:43.847692 18896 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1122 08:31:48.416226 18896 solver.cpp:218] Iteration 14900 (21.891 iter/s, 4.56808s/100 iters), loss = 0.097237
I1122 08:31:48.416226 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:31:48.416226 18896 solver.cpp:237]     Train net output #1: loss = 0.0972369 (* 1 = 0.0972369 loss)
I1122 08:31:48.416226 18896 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1122 08:31:52.788794 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:31:52.966411 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_15000.caffemodel
I1122 08:31:52.978397 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_15000.solverstate
I1122 08:31:52.982897 18896 solver.cpp:330] Iteration 15000, Testing net (#0)
I1122 08:31:52.982897 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:31:54.091243 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:31:54.135232 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9061
I1122 08:31:54.135232 18896 solver.cpp:397]     Test net output #1: loss = 0.290473 (* 1 = 0.290473 loss)
I1122 08:31:54.178256 18896 solver.cpp:218] Iteration 15000 (17.3561 iter/s, 5.76165s/100 iters), loss = 0.163995
I1122 08:31:54.178256 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 08:31:54.178256 18896 solver.cpp:237]     Train net output #1: loss = 0.163995 (* 1 = 0.163995 loss)
I1122 08:31:54.178256 18896 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1122 08:31:58.759734 18896 solver.cpp:218] Iteration 15100 (21.8252 iter/s, 4.58187s/100 iters), loss = 0.114967
I1122 08:31:58.759734 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:31:58.759734 18896 solver.cpp:237]     Train net output #1: loss = 0.114967 (* 1 = 0.114967 loss)
I1122 08:31:58.759734 18896 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1122 08:32:03.330410 18896 solver.cpp:218] Iteration 15200 (21.8827 iter/s, 4.56983s/100 iters), loss = 0.122219
I1122 08:32:03.330410 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:32:03.330410 18896 solver.cpp:237]     Train net output #1: loss = 0.122219 (* 1 = 0.122219 loss)
I1122 08:32:03.330410 18896 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1122 08:32:07.905477 18896 solver.cpp:218] Iteration 15300 (21.8615 iter/s, 4.57426s/100 iters), loss = 0.126141
I1122 08:32:07.905477 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:32:07.905477 18896 solver.cpp:237]     Train net output #1: loss = 0.126141 (* 1 = 0.126141 loss)
I1122 08:32:07.905477 18896 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1122 08:32:07.905477 18896 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1122 08:32:12.472707 18896 solver.cpp:218] Iteration 15400 (21.8939 iter/s, 4.56748s/100 iters), loss = 0.136356
I1122 08:32:12.472707 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:32:12.472707 18896 solver.cpp:237]     Train net output #1: loss = 0.136356 (* 1 = 0.136356 loss)
I1122 08:32:12.472707 18896 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1122 08:32:16.832057 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:32:17.007671 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_15500.caffemodel
I1122 08:32:17.017664 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_15500.solverstate
I1122 08:32:17.021666 18896 solver.cpp:330] Iteration 15500, Testing net (#0)
I1122 08:32:17.021666 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:32:18.140655 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:32:18.186177 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9052
I1122 08:32:18.186177 18896 solver.cpp:397]     Test net output #1: loss = 0.289646 (* 1 = 0.289646 loss)
I1122 08:32:18.230190 18896 solver.cpp:218] Iteration 15500 (17.3707 iter/s, 5.75683s/100 iters), loss = 0.128931
I1122 08:32:18.230190 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:32:18.230190 18896 solver.cpp:237]     Train net output #1: loss = 0.128931 (* 1 = 0.128931 loss)
I1122 08:32:18.230190 18896 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1122 08:32:22.815059 18896 solver.cpp:218] Iteration 15600 (21.8112 iter/s, 4.58481s/100 iters), loss = 0.172138
I1122 08:32:22.816058 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:32:22.816058 18896 solver.cpp:237]     Train net output #1: loss = 0.172138 (* 1 = 0.172138 loss)
I1122 08:32:22.816058 18896 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1122 08:32:27.414666 18896 solver.cpp:218] Iteration 15700 (21.7459 iter/s, 4.59856s/100 iters), loss = 0.112615
I1122 08:32:27.414666 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:32:27.414666 18896 solver.cpp:237]     Train net output #1: loss = 0.112615 (* 1 = 0.112615 loss)
I1122 08:32:27.414666 18896 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1122 08:32:31.981201 18896 solver.cpp:218] Iteration 15800 (21.9008 iter/s, 4.56604s/100 iters), loss = 0.134689
I1122 08:32:31.981201 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:32:31.981201 18896 solver.cpp:237]     Train net output #1: loss = 0.134689 (* 1 = 0.134689 loss)
I1122 08:32:31.981201 18896 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1122 08:32:36.553577 18896 solver.cpp:218] Iteration 15900 (21.8723 iter/s, 4.57199s/100 iters), loss = 0.148379
I1122 08:32:36.553577 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:32:36.553577 18896 solver.cpp:237]     Train net output #1: loss = 0.148379 (* 1 = 0.148379 loss)
I1122 08:32:36.553577 18896 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1122 08:32:40.952034 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:32:41.130856 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_16000.caffemodel
I1122 08:32:41.140854 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_16000.solverstate
I1122 08:32:41.144855 18896 solver.cpp:330] Iteration 16000, Testing net (#0)
I1122 08:32:41.144855 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:32:42.259219 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:32:42.303740 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9049
I1122 08:32:42.303740 18896 solver.cpp:397]     Test net output #1: loss = 0.289446 (* 1 = 0.289446 loss)
I1122 08:32:42.346694 18896 solver.cpp:218] Iteration 16000 (17.2622 iter/s, 5.793s/100 iters), loss = 0.129549
I1122 08:32:42.346694 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:32:42.346694 18896 solver.cpp:237]     Train net output #1: loss = 0.129549 (* 1 = 0.129549 loss)
I1122 08:32:42.346694 18896 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1122 08:32:46.951907 18896 solver.cpp:218] Iteration 16100 (21.7194 iter/s, 4.60418s/100 iters), loss = 0.170441
I1122 08:32:46.951907 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:32:46.951907 18896 solver.cpp:237]     Train net output #1: loss = 0.170441 (* 1 = 0.170441 loss)
I1122 08:32:46.951907 18896 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1122 08:32:51.503685 18896 solver.cpp:218] Iteration 16200 (21.9703 iter/s, 4.5516s/100 iters), loss = 0.123936
I1122 08:32:51.503685 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:32:51.503685 18896 solver.cpp:237]     Train net output #1: loss = 0.123936 (* 1 = 0.123936 loss)
I1122 08:32:51.503685 18896 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1122 08:32:56.107141 18896 solver.cpp:218] Iteration 16300 (21.726 iter/s, 4.60277s/100 iters), loss = 0.157765
I1122 08:32:56.107141 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:32:56.107141 18896 solver.cpp:237]     Train net output #1: loss = 0.157765 (* 1 = 0.157765 loss)
I1122 08:32:56.107141 18896 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1122 08:33:00.706488 18896 solver.cpp:218] Iteration 16400 (21.744 iter/s, 4.59896s/100 iters), loss = 0.0966483
I1122 08:33:00.706488 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:33:00.706488 18896 solver.cpp:237]     Train net output #1: loss = 0.0966481 (* 1 = 0.0966481 loss)
I1122 08:33:00.706488 18896 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1122 08:33:05.047025 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:33:05.228775 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_16500.caffemodel
I1122 08:33:05.241770 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_16500.solverstate
I1122 08:33:05.245771 18896 solver.cpp:330] Iteration 16500, Testing net (#0)
I1122 08:33:05.245771 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:33:06.361057 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:33:06.406558 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9054
I1122 08:33:06.406558 18896 solver.cpp:397]     Test net output #1: loss = 0.289085 (* 1 = 0.289085 loss)
I1122 08:33:06.450078 18896 solver.cpp:218] Iteration 16500 (17.4093 iter/s, 5.74405s/100 iters), loss = 0.189799
I1122 08:33:06.451079 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:33:06.451079 18896 solver.cpp:237]     Train net output #1: loss = 0.189798 (* 1 = 0.189798 loss)
I1122 08:33:06.451079 18896 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1122 08:33:11.021214 18896 solver.cpp:218] Iteration 16600 (21.8827 iter/s, 4.56982s/100 iters), loss = 0.147362
I1122 08:33:11.021214 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:33:11.021214 18896 solver.cpp:237]     Train net output #1: loss = 0.147362 (* 1 = 0.147362 loss)
I1122 08:33:11.021214 18896 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1122 08:33:15.573698 18896 solver.cpp:218] Iteration 16700 (21.9681 iter/s, 4.55206s/100 iters), loss = 0.113304
I1122 08:33:15.573698 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:33:15.573698 18896 solver.cpp:237]     Train net output #1: loss = 0.113303 (* 1 = 0.113303 loss)
I1122 08:33:15.573698 18896 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1122 08:33:20.138909 18896 solver.cpp:218] Iteration 16800 (21.9046 iter/s, 4.56525s/100 iters), loss = 0.148517
I1122 08:33:20.138909 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:33:20.138909 18896 solver.cpp:237]     Train net output #1: loss = 0.148517 (* 1 = 0.148517 loss)
I1122 08:33:20.138909 18896 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1122 08:33:24.728703 18896 solver.cpp:218] Iteration 16900 (21.7906 iter/s, 4.58914s/100 iters), loss = 0.0831826
I1122 08:33:24.728703 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:33:24.728703 18896 solver.cpp:237]     Train net output #1: loss = 0.0831825 (* 1 = 0.0831825 loss)
I1122 08:33:24.728703 18896 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1122 08:33:29.071568 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:33:29.254340 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_17000.caffemodel
I1122 08:33:29.266340 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_17000.solverstate
I1122 08:33:29.270339 18896 solver.cpp:330] Iteration 17000, Testing net (#0)
I1122 08:33:29.270339 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:33:30.400324 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:33:30.444139 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9059
I1122 08:33:30.444139 18896 solver.cpp:397]     Test net output #1: loss = 0.288989 (* 1 = 0.288989 loss)
I1122 08:33:30.489140 18896 solver.cpp:218] Iteration 17000 (17.3597 iter/s, 5.76048s/100 iters), loss = 0.153502
I1122 08:33:30.489140 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:33:30.489140 18896 solver.cpp:237]     Train net output #1: loss = 0.153502 (* 1 = 0.153502 loss)
I1122 08:33:30.489140 18896 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1122 08:33:35.046423 18896 solver.cpp:218] Iteration 17100 (21.9476 iter/s, 4.55632s/100 iters), loss = 0.182615
I1122 08:33:35.046423 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:33:35.046423 18896 solver.cpp:237]     Train net output #1: loss = 0.182614 (* 1 = 0.182614 loss)
I1122 08:33:35.046423 18896 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1122 08:33:39.617835 18896 solver.cpp:218] Iteration 17200 (21.8747 iter/s, 4.57149s/100 iters), loss = 0.141149
I1122 08:33:39.617835 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:33:39.617835 18896 solver.cpp:237]     Train net output #1: loss = 0.141149 (* 1 = 0.141149 loss)
I1122 08:33:39.617835 18896 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1122 08:33:44.210430 18896 solver.cpp:218] Iteration 17300 (21.7761 iter/s, 4.59218s/100 iters), loss = 0.146736
I1122 08:33:44.210430 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:33:44.210430 18896 solver.cpp:237]     Train net output #1: loss = 0.146736 (* 1 = 0.146736 loss)
I1122 08:33:44.210430 18896 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1122 08:33:48.791574 18896 solver.cpp:218] Iteration 17400 (21.8305 iter/s, 4.58074s/100 iters), loss = 0.125508
I1122 08:33:48.791574 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:33:48.791574 18896 solver.cpp:237]     Train net output #1: loss = 0.125508 (* 1 = 0.125508 loss)
I1122 08:33:48.791574 18896 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1122 08:33:53.163868 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:33:53.342905 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_17500.caffemodel
I1122 08:33:53.355535 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_17500.solverstate
I1122 08:33:53.360530 18896 solver.cpp:330] Iteration 17500, Testing net (#0)
I1122 08:33:53.360530 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:33:54.474757 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:33:54.518757 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9054
I1122 08:33:54.518757 18896 solver.cpp:397]     Test net output #1: loss = 0.289161 (* 1 = 0.289161 loss)
I1122 08:33:54.562799 18896 solver.cpp:218] Iteration 17500 (17.3287 iter/s, 5.77078s/100 iters), loss = 0.176478
I1122 08:33:54.562799 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:33:54.562799 18896 solver.cpp:237]     Train net output #1: loss = 0.176478 (* 1 = 0.176478 loss)
I1122 08:33:54.562799 18896 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1122 08:33:59.157375 18896 solver.cpp:218] Iteration 17600 (21.7687 iter/s, 4.59375s/100 iters), loss = 0.143602
I1122 08:33:59.157375 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:33:59.157375 18896 solver.cpp:237]     Train net output #1: loss = 0.143602 (* 1 = 0.143602 loss)
I1122 08:33:59.157375 18896 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1122 08:34:03.733958 18896 solver.cpp:218] Iteration 17700 (21.8523 iter/s, 4.57619s/100 iters), loss = 0.140874
I1122 08:34:03.733958 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:34:03.733958 18896 solver.cpp:237]     Train net output #1: loss = 0.140873 (* 1 = 0.140873 loss)
I1122 08:34:03.733958 18896 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1122 08:34:08.290575 18896 solver.cpp:218] Iteration 17800 (21.9485 iter/s, 4.55613s/100 iters), loss = 0.155212
I1122 08:34:08.290575 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:34:08.290575 18896 solver.cpp:237]     Train net output #1: loss = 0.155212 (* 1 = 0.155212 loss)
I1122 08:34:08.290575 18896 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1122 08:34:12.842612 18896 solver.cpp:218] Iteration 17900 (21.9708 iter/s, 4.55149s/100 iters), loss = 0.102418
I1122 08:34:12.842612 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:34:12.842612 18896 solver.cpp:237]     Train net output #1: loss = 0.102418 (* 1 = 0.102418 loss)
I1122 08:34:12.842612 18896 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1122 08:34:17.171468 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:34:17.350015 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_18000.caffemodel
I1122 08:34:17.364528 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_18000.solverstate
I1122 08:34:17.369532 18896 solver.cpp:330] Iteration 18000, Testing net (#0)
I1122 08:34:17.369532 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:34:18.495566 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:34:18.539563 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9055
I1122 08:34:18.539563 18896 solver.cpp:397]     Test net output #1: loss = 0.289289 (* 1 = 0.289289 loss)
I1122 08:34:18.583067 18896 solver.cpp:218] Iteration 18000 (17.4195 iter/s, 5.74068s/100 iters), loss = 0.142528
I1122 08:34:18.583067 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:34:18.583067 18896 solver.cpp:237]     Train net output #1: loss = 0.142528 (* 1 = 0.142528 loss)
I1122 08:34:18.583067 18896 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1122 08:34:23.173142 18896 solver.cpp:218] Iteration 18100 (21.7891 iter/s, 4.58944s/100 iters), loss = 0.10624
I1122 08:34:23.173142 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:34:23.173142 18896 solver.cpp:237]     Train net output #1: loss = 0.10624 (* 1 = 0.10624 loss)
I1122 08:34:23.173142 18896 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1122 08:34:27.771720 18896 solver.cpp:218] Iteration 18200 (21.7508 iter/s, 4.59754s/100 iters), loss = 0.13146
I1122 08:34:27.771720 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:34:27.771720 18896 solver.cpp:237]     Train net output #1: loss = 0.131459 (* 1 = 0.131459 loss)
I1122 08:34:27.771720 18896 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1122 08:34:32.332108 18896 solver.cpp:218] Iteration 18300 (21.9254 iter/s, 4.56093s/100 iters), loss = 0.147941
I1122 08:34:32.333108 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:34:32.333108 18896 solver.cpp:237]     Train net output #1: loss = 0.147941 (* 1 = 0.147941 loss)
I1122 08:34:32.333108 18896 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1122 08:34:36.904846 18896 solver.cpp:218] Iteration 18400 (21.8761 iter/s, 4.5712s/100 iters), loss = 0.149883
I1122 08:34:36.904846 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:34:36.904846 18896 solver.cpp:237]     Train net output #1: loss = 0.149883 (* 1 = 0.149883 loss)
I1122 08:34:36.904846 18896 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1122 08:34:41.295620 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:34:41.475113 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_18500.caffemodel
I1122 08:34:41.486613 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_18500.solverstate
I1122 08:34:41.490612 18896 solver.cpp:330] Iteration 18500, Testing net (#0)
I1122 08:34:41.491112 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:34:42.602113 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:34:42.645113 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9055
I1122 08:34:42.645629 18896 solver.cpp:397]     Test net output #1: loss = 0.289304 (* 1 = 0.289304 loss)
I1122 08:34:42.688112 18896 solver.cpp:218] Iteration 18500 (17.2925 iter/s, 5.78286s/100 iters), loss = 0.171582
I1122 08:34:42.688112 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:34:42.688112 18896 solver.cpp:237]     Train net output #1: loss = 0.171582 (* 1 = 0.171582 loss)
I1122 08:34:42.688112 18896 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1122 08:34:47.287291 18896 solver.cpp:218] Iteration 18600 (21.7448 iter/s, 4.5988s/100 iters), loss = 0.139501
I1122 08:34:47.287291 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:34:47.287291 18896 solver.cpp:237]     Train net output #1: loss = 0.139501 (* 1 = 0.139501 loss)
I1122 08:34:47.287291 18896 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1122 08:34:51.836510 18896 solver.cpp:218] Iteration 18700 (21.9825 iter/s, 4.54908s/100 iters), loss = 0.122023
I1122 08:34:51.836510 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:34:51.836510 18896 solver.cpp:237]     Train net output #1: loss = 0.122023 (* 1 = 0.122023 loss)
I1122 08:34:51.836510 18896 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1122 08:34:56.411298 18896 solver.cpp:218] Iteration 18800 (21.8622 iter/s, 4.57411s/100 iters), loss = 0.117854
I1122 08:34:56.411298 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:34:56.411298 18896 solver.cpp:237]     Train net output #1: loss = 0.117854 (* 1 = 0.117854 loss)
I1122 08:34:56.411298 18896 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1122 08:35:01.003446 18896 solver.cpp:218] Iteration 18900 (21.7748 iter/s, 4.59247s/100 iters), loss = 0.118752
I1122 08:35:01.003446 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:35:01.003446 18896 solver.cpp:237]     Train net output #1: loss = 0.118752 (* 1 = 0.118752 loss)
I1122 08:35:01.003446 18896 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1122 08:35:05.321982 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:35:05.503999 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_19000.caffemodel
I1122 08:35:05.516000 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_19000.solverstate
I1122 08:35:05.521000 18896 solver.cpp:330] Iteration 19000, Testing net (#0)
I1122 08:35:05.521000 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:35:06.648334 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:35:06.693346 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9058
I1122 08:35:06.693346 18896 solver.cpp:397]     Test net output #1: loss = 0.289345 (* 1 = 0.289345 loss)
I1122 08:35:06.738361 18896 solver.cpp:218] Iteration 19000 (17.4408 iter/s, 5.73369s/100 iters), loss = 0.177652
I1122 08:35:06.738361 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:35:06.738361 18896 solver.cpp:237]     Train net output #1: loss = 0.177652 (* 1 = 0.177652 loss)
I1122 08:35:06.738361 18896 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1122 08:35:11.284435 18896 solver.cpp:218] Iteration 19100 (21.9993 iter/s, 4.5456s/100 iters), loss = 0.184461
I1122 08:35:11.284435 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:35:11.284435 18896 solver.cpp:237]     Train net output #1: loss = 0.18446 (* 1 = 0.18446 loss)
I1122 08:35:11.284435 18896 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1122 08:35:15.832162 18896 solver.cpp:218] Iteration 19200 (21.9913 iter/s, 4.54726s/100 iters), loss = 0.116614
I1122 08:35:15.832162 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:35:15.832162 18896 solver.cpp:237]     Train net output #1: loss = 0.116614 (* 1 = 0.116614 loss)
I1122 08:35:15.832162 18896 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1122 08:35:20.399807 18896 solver.cpp:218] Iteration 19300 (21.8949 iter/s, 4.56727s/100 iters), loss = 0.132568
I1122 08:35:20.399807 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:35:20.399807 18896 solver.cpp:237]     Train net output #1: loss = 0.132568 (* 1 = 0.132568 loss)
I1122 08:35:20.399807 18896 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1122 08:35:25.009956 18896 solver.cpp:218] Iteration 19400 (21.6915 iter/s, 4.61009s/100 iters), loss = 0.114122
I1122 08:35:25.009956 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:35:25.009956 18896 solver.cpp:237]     Train net output #1: loss = 0.114122 (* 1 = 0.114122 loss)
I1122 08:35:25.009956 18896 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1122 08:35:29.365684 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:35:29.545603 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_19500.caffemodel
I1122 08:35:29.558604 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_19500.solverstate
I1122 08:35:29.562604 18896 solver.cpp:330] Iteration 19500, Testing net (#0)
I1122 08:35:29.562604 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:35:30.690690 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:35:30.735203 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9057
I1122 08:35:30.735203 18896 solver.cpp:397]     Test net output #1: loss = 0.289058 (* 1 = 0.289058 loss)
I1122 08:35:30.779217 18896 solver.cpp:218] Iteration 19500 (17.3354 iter/s, 5.76855s/100 iters), loss = 0.159791
I1122 08:35:30.779217 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:35:30.779217 18896 solver.cpp:237]     Train net output #1: loss = 0.15979 (* 1 = 0.15979 loss)
I1122 08:35:30.779217 18896 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1122 08:35:30.779217 18896 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1122 08:35:35.345593 18896 solver.cpp:218] Iteration 19600 (21.8977 iter/s, 4.56669s/100 iters), loss = 0.0996029
I1122 08:35:35.346595 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:35:35.346595 18896 solver.cpp:237]     Train net output #1: loss = 0.0996027 (* 1 = 0.0996027 loss)
I1122 08:35:35.346595 18896 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1122 08:35:39.917781 18896 solver.cpp:218] Iteration 19700 (21.8747 iter/s, 4.57149s/100 iters), loss = 0.155181
I1122 08:35:39.917781 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:35:39.917781 18896 solver.cpp:237]     Train net output #1: loss = 0.155181 (* 1 = 0.155181 loss)
I1122 08:35:39.917781 18896 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1122 08:35:44.535818 18896 solver.cpp:218] Iteration 19800 (21.659 iter/s, 4.61702s/100 iters), loss = 0.14375
I1122 08:35:44.535818 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:35:44.535818 18896 solver.cpp:237]     Train net output #1: loss = 0.14375 (* 1 = 0.14375 loss)
I1122 08:35:44.535818 18896 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1122 08:35:49.121335 18896 solver.cpp:218] Iteration 19900 (21.8068 iter/s, 4.58573s/100 iters), loss = 0.118073
I1122 08:35:49.121335 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:35:49.121335 18896 solver.cpp:237]     Train net output #1: loss = 0.118073 (* 1 = 0.118073 loss)
I1122 08:35:49.121335 18896 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1122 08:35:53.481525 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:35:53.660061 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_20000.caffemodel
I1122 08:35:53.672106 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_20000.solverstate
I1122 08:35:53.676105 18896 solver.cpp:330] Iteration 20000, Testing net (#0)
I1122 08:35:53.676105 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:35:54.804631 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:35:54.848634 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9055
I1122 08:35:54.848634 18896 solver.cpp:397]     Test net output #1: loss = 0.289247 (* 1 = 0.289247 loss)
I1122 08:35:54.892647 18896 solver.cpp:218] Iteration 20000 (17.3304 iter/s, 5.7702s/100 iters), loss = 0.178149
I1122 08:35:54.892647 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:35:54.892647 18896 solver.cpp:237]     Train net output #1: loss = 0.178149 (* 1 = 0.178149 loss)
I1122 08:35:54.892647 18896 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1122 08:35:59.499037 18896 solver.cpp:218] Iteration 20100 (21.7104 iter/s, 4.60609s/100 iters), loss = 0.157621
I1122 08:35:59.499037 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:35:59.499037 18896 solver.cpp:237]     Train net output #1: loss = 0.157621 (* 1 = 0.157621 loss)
I1122 08:35:59.499037 18896 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1122 08:36:04.085219 18896 solver.cpp:218] Iteration 20200 (21.8089 iter/s, 4.58528s/100 iters), loss = 0.148492
I1122 08:36:04.085219 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 08:36:04.085219 18896 solver.cpp:237]     Train net output #1: loss = 0.148492 (* 1 = 0.148492 loss)
I1122 08:36:04.085219 18896 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1122 08:36:08.653834 18896 solver.cpp:218] Iteration 20300 (21.8878 iter/s, 4.56875s/100 iters), loss = 0.1739
I1122 08:36:08.653834 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:36:08.653834 18896 solver.cpp:237]     Train net output #1: loss = 0.1739 (* 1 = 0.1739 loss)
I1122 08:36:08.653834 18896 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1122 08:36:13.171502 18896 solver.cpp:218] Iteration 20400 (22.1412 iter/s, 4.51647s/100 iters), loss = 0.119489
I1122 08:36:13.171502 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:36:13.171502 18896 solver.cpp:237]     Train net output #1: loss = 0.119489 (* 1 = 0.119489 loss)
I1122 08:36:13.171502 18896 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1122 08:36:17.467906 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:36:17.645278 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_20500.caffemodel
I1122 08:36:17.657260 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_20500.solverstate
I1122 08:36:17.661262 18896 solver.cpp:330] Iteration 20500, Testing net (#0)
I1122 08:36:17.661262 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:36:18.775956 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:36:18.819547 18896 solver.cpp:397]     Test net output #0: accuracy = 0.905
I1122 08:36:18.819547 18896 solver.cpp:397]     Test net output #1: loss = 0.289037 (* 1 = 0.289037 loss)
I1122 08:36:18.862557 18896 solver.cpp:218] Iteration 20500 (17.5703 iter/s, 5.69143s/100 iters), loss = 0.154828
I1122 08:36:18.862557 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:36:18.862557 18896 solver.cpp:237]     Train net output #1: loss = 0.154828 (* 1 = 0.154828 loss)
I1122 08:36:18.862557 18896 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1122 08:36:23.380290 18896 solver.cpp:218] Iteration 20600 (22.1396 iter/s, 4.51679s/100 iters), loss = 0.162257
I1122 08:36:23.380290 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:36:23.380290 18896 solver.cpp:237]     Train net output #1: loss = 0.162257 (* 1 = 0.162257 loss)
I1122 08:36:23.380290 18896 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1122 08:36:27.901124 18896 solver.cpp:218] Iteration 20700 (22.1206 iter/s, 4.52067s/100 iters), loss = 0.129016
I1122 08:36:27.901124 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:36:27.901124 18896 solver.cpp:237]     Train net output #1: loss = 0.129016 (* 1 = 0.129016 loss)
I1122 08:36:27.901124 18896 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1122 08:36:32.413859 18896 solver.cpp:218] Iteration 20800 (22.1631 iter/s, 4.512s/100 iters), loss = 0.147546
I1122 08:36:32.413859 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:36:32.413859 18896 solver.cpp:237]     Train net output #1: loss = 0.147546 (* 1 = 0.147546 loss)
I1122 08:36:32.413859 18896 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1122 08:36:36.944514 18896 solver.cpp:218] Iteration 20900 (22.0705 iter/s, 4.53094s/100 iters), loss = 0.0761247
I1122 08:36:36.945513 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 08:36:36.945513 18896 solver.cpp:237]     Train net output #1: loss = 0.0761246 (* 1 = 0.0761246 loss)
I1122 08:36:36.945513 18896 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1122 08:36:41.248764 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:36:41.426841 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_21000.caffemodel
I1122 08:36:41.441838 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_21000.solverstate
I1122 08:36:41.445839 18896 solver.cpp:330] Iteration 21000, Testing net (#0)
I1122 08:36:41.445839 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:36:42.551468 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:36:42.595485 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9048
I1122 08:36:42.595485 18896 solver.cpp:397]     Test net output #1: loss = 0.28919 (* 1 = 0.28919 loss)
I1122 08:36:42.637500 18896 solver.cpp:218] Iteration 21000 (17.568 iter/s, 5.69216s/100 iters), loss = 0.141871
I1122 08:36:42.637500 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:36:42.637500 18896 solver.cpp:237]     Train net output #1: loss = 0.141871 (* 1 = 0.141871 loss)
I1122 08:36:42.637500 18896 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1122 08:36:47.166519 18896 solver.cpp:218] Iteration 21100 (22.0825 iter/s, 4.52847s/100 iters), loss = 0.116196
I1122 08:36:47.166519 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:36:47.166519 18896 solver.cpp:237]     Train net output #1: loss = 0.116196 (* 1 = 0.116196 loss)
I1122 08:36:47.166519 18896 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1122 08:36:51.695282 18896 solver.cpp:218] Iteration 21200 (22.085 iter/s, 4.52796s/100 iters), loss = 0.146076
I1122 08:36:51.695282 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:36:51.695282 18896 solver.cpp:237]     Train net output #1: loss = 0.146076 (* 1 = 0.146076 loss)
I1122 08:36:51.695282 18896 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1122 08:36:56.224201 18896 solver.cpp:218] Iteration 21300 (22.0778 iter/s, 4.52943s/100 iters), loss = 0.192631
I1122 08:36:56.225204 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:36:56.225204 18896 solver.cpp:237]     Train net output #1: loss = 0.192631 (* 1 = 0.192631 loss)
I1122 08:36:56.225204 18896 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1122 08:37:00.753816 18896 solver.cpp:218] Iteration 21400 (22.0799 iter/s, 4.52901s/100 iters), loss = 0.107196
I1122 08:37:00.753816 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:37:00.753816 18896 solver.cpp:237]     Train net output #1: loss = 0.107196 (* 1 = 0.107196 loss)
I1122 08:37:00.753816 18896 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1122 08:37:05.058677 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:37:05.234700 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_21500.caffemodel
I1122 08:37:05.245700 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_21500.solverstate
I1122 08:37:05.249699 18896 solver.cpp:330] Iteration 21500, Testing net (#0)
I1122 08:37:05.249699 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:37:06.357211 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:37:06.400274 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9047
I1122 08:37:06.400274 18896 solver.cpp:397]     Test net output #1: loss = 0.289207 (* 1 = 0.289207 loss)
I1122 08:37:06.443256 18896 solver.cpp:218] Iteration 21500 (17.5772 iter/s, 5.68919s/100 iters), loss = 0.133781
I1122 08:37:06.444255 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:37:06.444255 18896 solver.cpp:237]     Train net output #1: loss = 0.133781 (* 1 = 0.133781 loss)
I1122 08:37:06.444255 18896 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1122 08:37:11.026320 18896 solver.cpp:218] Iteration 21600 (21.8243 iter/s, 4.58205s/100 iters), loss = 0.166502
I1122 08:37:11.026320 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:37:11.026320 18896 solver.cpp:237]     Train net output #1: loss = 0.166502 (* 1 = 0.166502 loss)
I1122 08:37:11.026320 18896 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1122 08:37:15.613672 18896 solver.cpp:218] Iteration 21700 (21.8014 iter/s, 4.58687s/100 iters), loss = 0.160127
I1122 08:37:15.613672 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:37:15.613672 18896 solver.cpp:237]     Train net output #1: loss = 0.160126 (* 1 = 0.160126 loss)
I1122 08:37:15.613672 18896 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1122 08:37:20.183547 18896 solver.cpp:218] Iteration 21800 (21.882 iter/s, 4.56997s/100 iters), loss = 0.196838
I1122 08:37:20.183547 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:37:20.183547 18896 solver.cpp:237]     Train net output #1: loss = 0.196838 (* 1 = 0.196838 loss)
I1122 08:37:20.183547 18896 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1122 08:37:24.781772 18896 solver.cpp:218] Iteration 21900 (21.7512 iter/s, 4.59744s/100 iters), loss = 0.0767196
I1122 08:37:24.781772 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 08:37:24.781772 18896 solver.cpp:237]     Train net output #1: loss = 0.0767195 (* 1 = 0.0767195 loss)
I1122 08:37:24.781772 18896 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1122 08:37:29.142279 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:37:29.322821 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_22000.caffemodel
I1122 08:37:29.333811 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_22000.solverstate
I1122 08:37:29.337810 18896 solver.cpp:330] Iteration 22000, Testing net (#0)
I1122 08:37:29.337810 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:37:30.457013 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:37:30.500841 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9044
I1122 08:37:30.500841 18896 solver.cpp:397]     Test net output #1: loss = 0.289256 (* 1 = 0.289256 loss)
I1122 08:37:30.544854 18896 solver.cpp:218] Iteration 22000 (17.3543 iter/s, 5.76228s/100 iters), loss = 0.122223
I1122 08:37:30.544854 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:37:30.544854 18896 solver.cpp:237]     Train net output #1: loss = 0.122223 (* 1 = 0.122223 loss)
I1122 08:37:30.544854 18896 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1122 08:37:30.544854 18896 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1122 08:37:35.113996 18896 solver.cpp:218] Iteration 22100 (21.8875 iter/s, 4.56882s/100 iters), loss = 0.152957
I1122 08:37:35.113996 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:37:35.113996 18896 solver.cpp:237]     Train net output #1: loss = 0.152957 (* 1 = 0.152957 loss)
I1122 08:37:35.113996 18896 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1122 08:37:39.686058 18896 solver.cpp:218] Iteration 22200 (21.8744 iter/s, 4.57156s/100 iters), loss = 0.11485
I1122 08:37:39.686058 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:37:39.686058 18896 solver.cpp:237]     Train net output #1: loss = 0.11485 (* 1 = 0.11485 loss)
I1122 08:37:39.686058 18896 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1122 08:37:44.265208 18896 solver.cpp:218] Iteration 22300 (21.8382 iter/s, 4.57913s/100 iters), loss = 0.106167
I1122 08:37:44.265208 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:37:44.265208 18896 solver.cpp:237]     Train net output #1: loss = 0.106167 (* 1 = 0.106167 loss)
I1122 08:37:44.265208 18896 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1122 08:37:48.825131 18896 solver.cpp:218] Iteration 22400 (21.9312 iter/s, 4.55971s/100 iters), loss = 0.0869598
I1122 08:37:48.825131 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:37:48.825131 18896 solver.cpp:237]     Train net output #1: loss = 0.0869597 (* 1 = 0.0869597 loss)
I1122 08:37:48.825131 18896 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1122 08:37:53.212098 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:37:53.394127 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_22500.caffemodel
I1122 08:37:53.405117 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_22500.solverstate
I1122 08:37:53.409134 18896 solver.cpp:330] Iteration 22500, Testing net (#0)
I1122 08:37:53.409134 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:37:54.532965 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:37:54.576488 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9047
I1122 08:37:54.576488 18896 solver.cpp:397]     Test net output #1: loss = 0.289127 (* 1 = 0.289127 loss)
I1122 08:37:54.620597 18896 solver.cpp:218] Iteration 22500 (17.2585 iter/s, 5.79424s/100 iters), loss = 0.152018
I1122 08:37:54.620597 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:37:54.620597 18896 solver.cpp:237]     Train net output #1: loss = 0.152018 (* 1 = 0.152018 loss)
I1122 08:37:54.620597 18896 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1122 08:37:59.240259 18896 solver.cpp:218] Iteration 22600 (21.6465 iter/s, 4.61968s/100 iters), loss = 0.155107
I1122 08:37:59.240259 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:37:59.240259 18896 solver.cpp:237]     Train net output #1: loss = 0.155107 (* 1 = 0.155107 loss)
I1122 08:37:59.240259 18896 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1122 08:38:03.783540 18896 solver.cpp:218] Iteration 22700 (22.0141 iter/s, 4.54254s/100 iters), loss = 0.170221
I1122 08:38:03.783540 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:38:03.783540 18896 solver.cpp:237]     Train net output #1: loss = 0.170221 (* 1 = 0.170221 loss)
I1122 08:38:03.783540 18896 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1122 08:38:08.407869 18896 solver.cpp:218] Iteration 22800 (21.6272 iter/s, 4.62381s/100 iters), loss = 0.152867
I1122 08:38:08.407869 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:38:08.407869 18896 solver.cpp:237]     Train net output #1: loss = 0.152867 (* 1 = 0.152867 loss)
I1122 08:38:08.407869 18896 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1122 08:38:12.993193 18896 solver.cpp:218] Iteration 22900 (21.8104 iter/s, 4.58496s/100 iters), loss = 0.157465
I1122 08:38:12.993193 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:38:12.993193 18896 solver.cpp:237]     Train net output #1: loss = 0.157465 (* 1 = 0.157465 loss)
I1122 08:38:12.993193 18896 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1122 08:38:17.303373 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:38:17.487404 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_23000.caffemodel
I1122 08:38:17.499902 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_23000.solverstate
I1122 08:38:17.504094 18896 solver.cpp:330] Iteration 23000, Testing net (#0)
I1122 08:38:17.504094 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:38:18.614264 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:38:18.658263 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9049
I1122 08:38:18.659262 18896 solver.cpp:397]     Test net output #1: loss = 0.28923 (* 1 = 0.28923 loss)
I1122 08:38:18.701751 18896 solver.cpp:218] Iteration 23000 (17.5191 iter/s, 5.70805s/100 iters), loss = 0.15677
I1122 08:38:18.701751 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:38:18.701751 18896 solver.cpp:237]     Train net output #1: loss = 0.15677 (* 1 = 0.15677 loss)
I1122 08:38:18.701751 18896 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1122 08:38:23.270414 18896 solver.cpp:218] Iteration 23100 (21.8865 iter/s, 4.56903s/100 iters), loss = 0.159704
I1122 08:38:23.271414 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:38:23.271414 18896 solver.cpp:237]     Train net output #1: loss = 0.159704 (* 1 = 0.159704 loss)
I1122 08:38:23.271414 18896 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1122 08:38:27.884001 18896 solver.cpp:218] Iteration 23200 (21.6784 iter/s, 4.61289s/100 iters), loss = 0.115839
I1122 08:38:27.884001 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:38:27.884001 18896 solver.cpp:237]     Train net output #1: loss = 0.115839 (* 1 = 0.115839 loss)
I1122 08:38:27.884001 18896 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1122 08:38:32.466130 18896 solver.cpp:218] Iteration 23300 (21.8278 iter/s, 4.58131s/100 iters), loss = 0.137118
I1122 08:38:32.466130 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:38:32.466130 18896 solver.cpp:237]     Train net output #1: loss = 0.137118 (* 1 = 0.137118 loss)
I1122 08:38:32.466130 18896 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1122 08:38:37.007051 18896 solver.cpp:218] Iteration 23400 (22.0243 iter/s, 4.54044s/100 iters), loss = 0.149895
I1122 08:38:37.007051 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:38:37.007051 18896 solver.cpp:237]     Train net output #1: loss = 0.149894 (* 1 = 0.149894 loss)
I1122 08:38:37.007051 18896 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1122 08:38:41.312371 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:38:41.490887 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_23500.caffemodel
I1122 08:38:41.503891 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_23500.solverstate
I1122 08:38:41.507892 18896 solver.cpp:330] Iteration 23500, Testing net (#0)
I1122 08:38:41.507892 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:38:42.613296 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:38:42.656324 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9045
I1122 08:38:42.656324 18896 solver.cpp:397]     Test net output #1: loss = 0.289238 (* 1 = 0.289238 loss)
I1122 08:38:42.699324 18896 solver.cpp:218] Iteration 23500 (17.5688 iter/s, 5.6919s/100 iters), loss = 0.13364
I1122 08:38:42.699324 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:38:42.699324 18896 solver.cpp:237]     Train net output #1: loss = 0.13364 (* 1 = 0.13364 loss)
I1122 08:38:42.699324 18896 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1122 08:38:47.223538 18896 solver.cpp:218] Iteration 23600 (22.1061 iter/s, 4.52363s/100 iters), loss = 0.116023
I1122 08:38:47.223538 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:38:47.223538 18896 solver.cpp:237]     Train net output #1: loss = 0.116023 (* 1 = 0.116023 loss)
I1122 08:38:47.223538 18896 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1122 08:38:51.744794 18896 solver.cpp:218] Iteration 23700 (22.1196 iter/s, 4.52087s/100 iters), loss = 0.131393
I1122 08:38:51.744794 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:38:51.744794 18896 solver.cpp:237]     Train net output #1: loss = 0.131393 (* 1 = 0.131393 loss)
I1122 08:38:51.744794 18896 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1122 08:38:56.262138 18896 solver.cpp:218] Iteration 23800 (22.136 iter/s, 4.51754s/100 iters), loss = 0.165619
I1122 08:38:56.262138 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:38:56.262138 18896 solver.cpp:237]     Train net output #1: loss = 0.165619 (* 1 = 0.165619 loss)
I1122 08:38:56.262138 18896 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1122 08:39:00.783766 18896 solver.cpp:218] Iteration 23900 (22.1173 iter/s, 4.52135s/100 iters), loss = 0.135088
I1122 08:39:00.783766 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:39:00.783766 18896 solver.cpp:237]     Train net output #1: loss = 0.135088 (* 1 = 0.135088 loss)
I1122 08:39:00.783766 18896 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1122 08:39:05.095237 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:39:05.272363 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_24000.caffemodel
I1122 08:39:05.283350 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_24000.solverstate
I1122 08:39:05.287369 18896 solver.cpp:330] Iteration 24000, Testing net (#0)
I1122 08:39:05.287369 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:39:06.395640 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:39:06.439215 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9048
I1122 08:39:06.439215 18896 solver.cpp:397]     Test net output #1: loss = 0.289249 (* 1 = 0.289249 loss)
I1122 08:39:06.482215 18896 solver.cpp:218] Iteration 24000 (17.5499 iter/s, 5.69803s/100 iters), loss = 0.15423
I1122 08:39:06.482215 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:39:06.482215 18896 solver.cpp:237]     Train net output #1: loss = 0.154229 (* 1 = 0.154229 loss)
I1122 08:39:06.482215 18896 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1122 08:39:11.004947 18896 solver.cpp:218] Iteration 24100 (22.1159 iter/s, 4.52163s/100 iters), loss = 0.122089
I1122 08:39:11.004947 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:39:11.004947 18896 solver.cpp:237]     Train net output #1: loss = 0.122088 (* 1 = 0.122088 loss)
I1122 08:39:11.004947 18896 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1122 08:39:15.535979 18896 solver.cpp:218] Iteration 24200 (22.0698 iter/s, 4.53108s/100 iters), loss = 0.149361
I1122 08:39:15.535979 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:39:15.535979 18896 solver.cpp:237]     Train net output #1: loss = 0.149361 (* 1 = 0.149361 loss)
I1122 08:39:15.535979 18896 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1122 08:39:20.065589 18896 solver.cpp:218] Iteration 24300 (22.078 iter/s, 4.52939s/100 iters), loss = 0.19231
I1122 08:39:20.065589 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:39:20.065589 18896 solver.cpp:237]     Train net output #1: loss = 0.19231 (* 1 = 0.19231 loss)
I1122 08:39:20.065589 18896 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1122 08:39:24.599864 18896 solver.cpp:218] Iteration 24400 (22.0566 iter/s, 4.5338s/100 iters), loss = 0.115931
I1122 08:39:24.599864 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:39:24.599864 18896 solver.cpp:237]     Train net output #1: loss = 0.115931 (* 1 = 0.115931 loss)
I1122 08:39:24.599864 18896 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1122 08:39:28.913522 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:39:29.092058 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_24500.caffemodel
I1122 08:39:29.103039 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_24500.solverstate
I1122 08:39:29.108041 18896 solver.cpp:330] Iteration 24500, Testing net (#0)
I1122 08:39:29.108041 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:39:30.213505 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:39:30.257004 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9049
I1122 08:39:30.257004 18896 solver.cpp:397]     Test net output #1: loss = 0.289212 (* 1 = 0.289212 loss)
I1122 08:39:30.300500 18896 solver.cpp:218] Iteration 24500 (17.5451 iter/s, 5.6996s/100 iters), loss = 0.134441
I1122 08:39:30.300500 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:39:30.300500 18896 solver.cpp:237]     Train net output #1: loss = 0.134441 (* 1 = 0.134441 loss)
I1122 08:39:30.300500 18896 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1122 08:39:34.814841 18896 solver.cpp:218] Iteration 24600 (22.1539 iter/s, 4.51387s/100 iters), loss = 0.146207
I1122 08:39:34.814841 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:39:34.814841 18896 solver.cpp:237]     Train net output #1: loss = 0.146207 (* 1 = 0.146207 loss)
I1122 08:39:34.814841 18896 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1122 08:39:39.331189 18896 solver.cpp:218] Iteration 24700 (22.1437 iter/s, 4.51596s/100 iters), loss = 0.125949
I1122 08:39:39.331189 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:39:39.331189 18896 solver.cpp:237]     Train net output #1: loss = 0.125949 (* 1 = 0.125949 loss)
I1122 08:39:39.331189 18896 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1122 08:39:43.854686 18896 solver.cpp:218] Iteration 24800 (22.1093 iter/s, 4.52298s/100 iters), loss = 0.141117
I1122 08:39:43.854686 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:39:43.854686 18896 solver.cpp:237]     Train net output #1: loss = 0.141117 (* 1 = 0.141117 loss)
I1122 08:39:43.854686 18896 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1122 08:39:48.381003 18896 solver.cpp:218] Iteration 24900 (22.095 iter/s, 4.52592s/100 iters), loss = 0.110739
I1122 08:39:48.381003 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:39:48.381003 18896 solver.cpp:237]     Train net output #1: loss = 0.110739 (* 1 = 0.110739 loss)
I1122 08:39:48.381003 18896 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1122 08:39:52.669917 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:39:52.848024 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_25000.caffemodel
I1122 08:39:52.863632 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_25000.solverstate
I1122 08:39:52.867633 18896 solver.cpp:330] Iteration 25000, Testing net (#0)
I1122 08:39:52.867633 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:39:53.970458 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:39:54.014452 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9048
I1122 08:39:54.014452 18896 solver.cpp:397]     Test net output #1: loss = 0.289184 (* 1 = 0.289184 loss)
I1122 08:39:54.057484 18896 solver.cpp:218] Iteration 25000 (17.6162 iter/s, 5.67659s/100 iters), loss = 0.193074
I1122 08:39:54.057484 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 08:39:54.057484 18896 solver.cpp:237]     Train net output #1: loss = 0.193073 (* 1 = 0.193073 loss)
I1122 08:39:54.057484 18896 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1122 08:39:58.582705 18896 solver.cpp:218] Iteration 25100 (22.1024 iter/s, 4.52439s/100 iters), loss = 0.160069
I1122 08:39:58.582705 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:39:58.582705 18896 solver.cpp:237]     Train net output #1: loss = 0.160069 (* 1 = 0.160069 loss)
I1122 08:39:58.582705 18896 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1122 08:40:03.137599 18896 solver.cpp:218] Iteration 25200 (21.9545 iter/s, 4.55487s/100 iters), loss = 0.113624
I1122 08:40:03.137599 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:40:03.137599 18896 solver.cpp:237]     Train net output #1: loss = 0.113624 (* 1 = 0.113624 loss)
I1122 08:40:03.137599 18896 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1122 08:40:07.657681 18896 solver.cpp:218] Iteration 25300 (22.129 iter/s, 4.51896s/100 iters), loss = 0.187217
I1122 08:40:07.657681 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:40:07.657681 18896 solver.cpp:237]     Train net output #1: loss = 0.187217 (* 1 = 0.187217 loss)
I1122 08:40:07.657681 18896 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1122 08:40:12.183571 18896 solver.cpp:218] Iteration 25400 (22.0959 iter/s, 4.52572s/100 iters), loss = 0.115647
I1122 08:40:12.183571 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:40:12.183571 18896 solver.cpp:237]     Train net output #1: loss = 0.115647 (* 1 = 0.115647 loss)
I1122 08:40:12.183571 18896 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1122 08:40:16.500061 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:40:16.677206 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_25500.caffemodel
I1122 08:40:16.688191 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_25500.solverstate
I1122 08:40:16.692191 18896 solver.cpp:330] Iteration 25500, Testing net (#0)
I1122 08:40:16.692191 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:40:17.799779 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:40:17.843765 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9046
I1122 08:40:17.843765 18896 solver.cpp:397]     Test net output #1: loss = 0.289264 (* 1 = 0.289264 loss)
I1122 08:40:17.887864 18896 solver.cpp:218] Iteration 25500 (17.5321 iter/s, 5.70382s/100 iters), loss = 0.171357
I1122 08:40:17.887864 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:40:17.887864 18896 solver.cpp:237]     Train net output #1: loss = 0.171356 (* 1 = 0.171356 loss)
I1122 08:40:17.887864 18896 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1122 08:40:22.417985 18896 solver.cpp:218] Iteration 25600 (22.0767 iter/s, 4.52966s/100 iters), loss = 0.150047
I1122 08:40:22.417985 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:40:22.417985 18896 solver.cpp:237]     Train net output #1: loss = 0.150047 (* 1 = 0.150047 loss)
I1122 08:40:22.417985 18896 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1122 08:40:26.945349 18896 solver.cpp:218] Iteration 25700 (22.0878 iter/s, 4.52738s/100 iters), loss = 0.125652
I1122 08:40:26.945349 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:40:26.945349 18896 solver.cpp:237]     Train net output #1: loss = 0.125652 (* 1 = 0.125652 loss)
I1122 08:40:26.945349 18896 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1122 08:40:31.469058 18896 solver.cpp:218] Iteration 25800 (22.1087 iter/s, 4.52312s/100 iters), loss = 0.144682
I1122 08:40:31.469058 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:40:31.469058 18896 solver.cpp:237]     Train net output #1: loss = 0.144681 (* 1 = 0.144681 loss)
I1122 08:40:31.469058 18896 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1122 08:40:36.004166 18896 solver.cpp:218] Iteration 25900 (22.0513 iter/s, 4.53488s/100 iters), loss = 0.096298
I1122 08:40:36.004166 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:40:36.004166 18896 solver.cpp:237]     Train net output #1: loss = 0.0962979 (* 1 = 0.0962979 loss)
I1122 08:40:36.004166 18896 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1122 08:40:40.311520 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:40:40.490075 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_26000.caffemodel
I1122 08:40:40.501060 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_26000.solverstate
I1122 08:40:40.505061 18896 solver.cpp:330] Iteration 26000, Testing net (#0)
I1122 08:40:40.505061 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:40:41.611889 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:40:41.654875 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9051
I1122 08:40:41.654875 18896 solver.cpp:397]     Test net output #1: loss = 0.289215 (* 1 = 0.289215 loss)
I1122 08:40:41.699014 18896 solver.cpp:218] Iteration 26000 (17.5616 iter/s, 5.69425s/100 iters), loss = 0.156666
I1122 08:40:41.699014 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:40:41.699014 18896 solver.cpp:237]     Train net output #1: loss = 0.156666 (* 1 = 0.156666 loss)
I1122 08:40:41.699014 18896 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1122 08:40:46.228896 18896 solver.cpp:218] Iteration 26100 (22.0789 iter/s, 4.52922s/100 iters), loss = 0.208307
I1122 08:40:46.228896 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 08:40:46.228896 18896 solver.cpp:237]     Train net output #1: loss = 0.208307 (* 1 = 0.208307 loss)
I1122 08:40:46.228896 18896 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1122 08:40:50.759490 18896 solver.cpp:218] Iteration 26200 (22.0711 iter/s, 4.53081s/100 iters), loss = 0.147163
I1122 08:40:50.759490 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:40:50.759490 18896 solver.cpp:237]     Train net output #1: loss = 0.147163 (* 1 = 0.147163 loss)
I1122 08:40:50.759490 18896 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1122 08:40:55.293028 18896 solver.cpp:218] Iteration 26300 (22.0608 iter/s, 4.53292s/100 iters), loss = 0.124651
I1122 08:40:55.293028 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:40:55.293028 18896 solver.cpp:237]     Train net output #1: loss = 0.124651 (* 1 = 0.124651 loss)
I1122 08:40:55.293028 18896 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1122 08:40:59.845002 18896 solver.cpp:218] Iteration 26400 (21.9717 iter/s, 4.55131s/100 iters), loss = 0.119686
I1122 08:40:59.845002 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:40:59.845002 18896 solver.cpp:237]     Train net output #1: loss = 0.119686 (* 1 = 0.119686 loss)
I1122 08:40:59.845002 18896 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1122 08:41:04.186401 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:41:04.365416 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_26500.caffemodel
I1122 08:41:04.377424 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_26500.solverstate
I1122 08:41:04.381924 18896 solver.cpp:330] Iteration 26500, Testing net (#0)
I1122 08:41:04.381924 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:41:05.485626 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:41:05.529386 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9048
I1122 08:41:05.529386 18896 solver.cpp:397]     Test net output #1: loss = 0.289254 (* 1 = 0.289254 loss)
I1122 08:41:05.572383 18896 solver.cpp:218] Iteration 26500 (17.46 iter/s, 5.72739s/100 iters), loss = 0.11551
I1122 08:41:05.572383 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:41:05.572383 18896 solver.cpp:237]     Train net output #1: loss = 0.11551 (* 1 = 0.11551 loss)
I1122 08:41:05.572383 18896 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1122 08:41:10.120337 18896 solver.cpp:218] Iteration 26600 (21.989 iter/s, 4.54773s/100 iters), loss = 0.150062
I1122 08:41:10.120337 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:41:10.120337 18896 solver.cpp:237]     Train net output #1: loss = 0.150062 (* 1 = 0.150062 loss)
I1122 08:41:10.120337 18896 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1122 08:41:14.715232 18896 solver.cpp:218] Iteration 26700 (21.765 iter/s, 4.59454s/100 iters), loss = 0.106622
I1122 08:41:14.715232 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:41:14.715232 18896 solver.cpp:237]     Train net output #1: loss = 0.106621 (* 1 = 0.106621 loss)
I1122 08:41:14.715232 18896 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1122 08:41:19.242110 18896 solver.cpp:218] Iteration 26800 (22.0937 iter/s, 4.52618s/100 iters), loss = 0.147203
I1122 08:41:19.242110 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:41:19.242110 18896 solver.cpp:237]     Train net output #1: loss = 0.147203 (* 1 = 0.147203 loss)
I1122 08:41:19.242110 18896 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1122 08:41:23.834403 18896 solver.cpp:218] Iteration 26900 (21.7777 iter/s, 4.59185s/100 iters), loss = 0.0917125
I1122 08:41:23.834403 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:41:23.834403 18896 solver.cpp:237]     Train net output #1: loss = 0.0917124 (* 1 = 0.0917124 loss)
I1122 08:41:23.834403 18896 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1122 08:41:28.194654 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:41:28.376276 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_27000.caffemodel
I1122 08:41:28.390276 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_27000.solverstate
I1122 08:41:28.394280 18896 solver.cpp:330] Iteration 27000, Testing net (#0)
I1122 08:41:28.394280 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:41:29.502568 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:41:29.545357 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9051
I1122 08:41:29.545357 18896 solver.cpp:397]     Test net output #1: loss = 0.289224 (* 1 = 0.289224 loss)
I1122 08:41:29.588361 18896 solver.cpp:218] Iteration 27000 (17.3803 iter/s, 5.75364s/100 iters), loss = 0.158124
I1122 08:41:29.588361 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:41:29.588361 18896 solver.cpp:237]     Train net output #1: loss = 0.158124 (* 1 = 0.158124 loss)
I1122 08:41:29.588361 18896 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1122 08:41:29.588361 18896 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1122 08:41:34.110574 18896 solver.cpp:218] Iteration 27100 (22.1134 iter/s, 4.52214s/100 iters), loss = 0.158536
I1122 08:41:34.111573 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:41:34.111573 18896 solver.cpp:237]     Train net output #1: loss = 0.158536 (* 1 = 0.158536 loss)
I1122 08:41:34.111573 18896 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1122 08:41:38.641057 18896 solver.cpp:218] Iteration 27200 (22.0754 iter/s, 4.52994s/100 iters), loss = 0.153972
I1122 08:41:38.641057 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:41:38.641057 18896 solver.cpp:237]     Train net output #1: loss = 0.153972 (* 1 = 0.153972 loss)
I1122 08:41:38.641057 18896 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1122 08:41:43.187121 18896 solver.cpp:218] Iteration 27300 (22.0016 iter/s, 4.54512s/100 iters), loss = 0.163795
I1122 08:41:43.187623 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:41:43.187623 18896 solver.cpp:237]     Train net output #1: loss = 0.163795 (* 1 = 0.163795 loss)
I1122 08:41:43.187623 18896 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1122 08:41:47.738123 18896 solver.cpp:218] Iteration 27400 (21.9763 iter/s, 4.55035s/100 iters), loss = 0.122058
I1122 08:41:47.738123 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:41:47.738123 18896 solver.cpp:237]     Train net output #1: loss = 0.122058 (* 1 = 0.122058 loss)
I1122 08:41:47.738123 18896 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1122 08:41:52.042623 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:41:52.222122 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_27500.caffemodel
I1122 08:41:52.233623 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_27500.solverstate
I1122 08:41:52.238123 18896 solver.cpp:330] Iteration 27500, Testing net (#0)
I1122 08:41:52.238123 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:41:53.343647 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:41:53.387681 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9049
I1122 08:41:53.387681 18896 solver.cpp:397]     Test net output #1: loss = 0.289163 (* 1 = 0.289163 loss)
I1122 08:41:53.430683 18896 solver.cpp:218] Iteration 27500 (17.5671 iter/s, 5.69246s/100 iters), loss = 0.175089
I1122 08:41:53.430683 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:41:53.430683 18896 solver.cpp:237]     Train net output #1: loss = 0.175089 (* 1 = 0.175089 loss)
I1122 08:41:53.430683 18896 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1122 08:41:57.962736 18896 solver.cpp:218] Iteration 27600 (22.0664 iter/s, 4.53177s/100 iters), loss = 0.17803
I1122 08:41:57.962736 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:41:57.962736 18896 solver.cpp:237]     Train net output #1: loss = 0.17803 (* 1 = 0.17803 loss)
I1122 08:41:57.962736 18896 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1122 08:42:02.522637 18896 solver.cpp:218] Iteration 27700 (21.9331 iter/s, 4.55932s/100 iters), loss = 0.150395
I1122 08:42:02.523141 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:42:02.523141 18896 solver.cpp:237]     Train net output #1: loss = 0.150395 (* 1 = 0.150395 loss)
I1122 08:42:02.523141 18896 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1122 08:42:07.076511 18896 solver.cpp:218] Iteration 27800 (21.9636 iter/s, 4.55298s/100 iters), loss = 0.154326
I1122 08:42:07.076511 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:42:07.076511 18896 solver.cpp:237]     Train net output #1: loss = 0.154326 (* 1 = 0.154326 loss)
I1122 08:42:07.076511 18896 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1122 08:42:11.584761 18896 solver.cpp:218] Iteration 27900 (22.1798 iter/s, 4.50861s/100 iters), loss = 0.0772201
I1122 08:42:11.584761 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:42:11.584761 18896 solver.cpp:237]     Train net output #1: loss = 0.07722 (* 1 = 0.07722 loss)
I1122 08:42:11.584761 18896 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1122 08:42:15.884361 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:42:16.061908 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_28000.caffemodel
I1122 08:42:16.072118 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_28000.solverstate
I1122 08:42:16.076118 18896 solver.cpp:330] Iteration 28000, Testing net (#0)
I1122 08:42:16.076118 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:42:17.183913 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:42:17.227900 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9046
I1122 08:42:17.227900 18896 solver.cpp:397]     Test net output #1: loss = 0.289258 (* 1 = 0.289258 loss)
I1122 08:42:17.270514 18896 solver.cpp:218] Iteration 28000 (17.5887 iter/s, 5.68546s/100 iters), loss = 0.152885
I1122 08:42:17.270514 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:42:17.270514 18896 solver.cpp:237]     Train net output #1: loss = 0.152885 (* 1 = 0.152885 loss)
I1122 08:42:17.270514 18896 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1122 08:42:21.779321 18896 solver.cpp:218] Iteration 28100 (22.1819 iter/s, 4.50818s/100 iters), loss = 0.132433
I1122 08:42:21.779321 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:42:21.779321 18896 solver.cpp:237]     Train net output #1: loss = 0.132433 (* 1 = 0.132433 loss)
I1122 08:42:21.779321 18896 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1122 08:42:26.295771 18896 solver.cpp:218] Iteration 28200 (22.1454 iter/s, 4.5156s/100 iters), loss = 0.163075
I1122 08:42:26.295771 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:42:26.295771 18896 solver.cpp:237]     Train net output #1: loss = 0.163075 (* 1 = 0.163075 loss)
I1122 08:42:26.295771 18896 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1122 08:42:30.807806 18896 solver.cpp:218] Iteration 28300 (22.1618 iter/s, 4.51227s/100 iters), loss = 0.107614
I1122 08:42:30.807806 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 08:42:30.807806 18896 solver.cpp:237]     Train net output #1: loss = 0.107614 (* 1 = 0.107614 loss)
I1122 08:42:30.807806 18896 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1122 08:42:35.327008 18896 solver.cpp:218] Iteration 28400 (22.1323 iter/s, 4.51828s/100 iters), loss = 0.0792574
I1122 08:42:35.327008 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 08:42:35.327008 18896 solver.cpp:237]     Train net output #1: loss = 0.0792573 (* 1 = 0.0792573 loss)
I1122 08:42:35.327008 18896 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1122 08:42:39.626996 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:42:39.805320 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_28500.caffemodel
I1122 08:42:39.816320 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_28500.solverstate
I1122 08:42:39.820322 18896 solver.cpp:330] Iteration 28500, Testing net (#0)
I1122 08:42:39.820322 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:42:40.926594 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:42:40.970093 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9048
I1122 08:42:40.970093 18896 solver.cpp:397]     Test net output #1: loss = 0.289089 (* 1 = 0.289089 loss)
I1122 08:42:41.013121 18896 solver.cpp:218] Iteration 28500 (17.5867 iter/s, 5.68611s/100 iters), loss = 0.16814
I1122 08:42:41.013121 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:42:41.013121 18896 solver.cpp:237]     Train net output #1: loss = 0.16814 (* 1 = 0.16814 loss)
I1122 08:42:41.013121 18896 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1122 08:42:45.525955 18896 solver.cpp:218] Iteration 28600 (22.1634 iter/s, 4.51194s/100 iters), loss = 0.176022
I1122 08:42:45.525955 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:42:45.525955 18896 solver.cpp:237]     Train net output #1: loss = 0.176022 (* 1 = 0.176022 loss)
I1122 08:42:45.525955 18896 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1122 08:42:50.056200 18896 solver.cpp:218] Iteration 28700 (22.0732 iter/s, 4.53038s/100 iters), loss = 0.148421
I1122 08:42:50.056200 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:42:50.056200 18896 solver.cpp:237]     Train net output #1: loss = 0.148421 (* 1 = 0.148421 loss)
I1122 08:42:50.056200 18896 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1122 08:42:54.586597 18896 solver.cpp:218] Iteration 28800 (22.0748 iter/s, 4.53005s/100 iters), loss = 0.15967
I1122 08:42:54.586597 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:42:54.586597 18896 solver.cpp:237]     Train net output #1: loss = 0.159669 (* 1 = 0.159669 loss)
I1122 08:42:54.586597 18896 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1122 08:42:59.161913 18896 solver.cpp:218] Iteration 28900 (21.8598 iter/s, 4.57461s/100 iters), loss = 0.119841
I1122 08:42:59.161913 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 08:42:59.161913 18896 solver.cpp:237]     Train net output #1: loss = 0.119841 (* 1 = 0.119841 loss)
I1122 08:42:59.161913 18896 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1122 08:43:03.470548 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:43:03.645812 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_29000.caffemodel
I1122 08:43:03.656802 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_29000.solverstate
I1122 08:43:03.661803 18896 solver.cpp:330] Iteration 29000, Testing net (#0)
I1122 08:43:03.661803 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:43:04.768466 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:43:04.812454 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9046
I1122 08:43:04.812454 18896 solver.cpp:397]     Test net output #1: loss = 0.289264 (* 1 = 0.289264 loss)
I1122 08:43:04.855978 18896 solver.cpp:218] Iteration 29000 (17.5647 iter/s, 5.69325s/100 iters), loss = 0.149536
I1122 08:43:04.855978 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:43:04.855978 18896 solver.cpp:237]     Train net output #1: loss = 0.149536 (* 1 = 0.149536 loss)
I1122 08:43:04.855978 18896 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1122 08:43:09.432268 18896 solver.cpp:218] Iteration 29100 (21.8517 iter/s, 4.57631s/100 iters), loss = 0.207275
I1122 08:43:09.432776 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:43:09.432776 18896 solver.cpp:237]     Train net output #1: loss = 0.207275 (* 1 = 0.207275 loss)
I1122 08:43:09.432776 18896 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1122 08:43:13.967041 18896 solver.cpp:218] Iteration 29200 (22.0548 iter/s, 4.53417s/100 iters), loss = 0.124446
I1122 08:43:13.967041 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:43:13.967041 18896 solver.cpp:237]     Train net output #1: loss = 0.124446 (* 1 = 0.124446 loss)
I1122 08:43:13.967041 18896 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1122 08:43:18.494004 18896 solver.cpp:218] Iteration 29300 (22.0913 iter/s, 4.52667s/100 iters), loss = 0.143863
I1122 08:43:18.494004 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:43:18.494004 18896 solver.cpp:237]     Train net output #1: loss = 0.143863 (* 1 = 0.143863 loss)
I1122 08:43:18.494004 18896 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1122 08:43:23.018240 18896 solver.cpp:218] Iteration 29400 (22.104 iter/s, 4.52407s/100 iters), loss = 0.131142
I1122 08:43:23.018240 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:43:23.018240 18896 solver.cpp:237]     Train net output #1: loss = 0.131142 (* 1 = 0.131142 loss)
I1122 08:43:23.018240 18896 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1122 08:43:27.319941 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:43:27.497648 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_29500.caffemodel
I1122 08:43:27.508635 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_29500.solverstate
I1122 08:43:27.513635 18896 solver.cpp:330] Iteration 29500, Testing net (#0)
I1122 08:43:27.513635 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:43:28.621543 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:43:28.664549 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9049
I1122 08:43:28.664549 18896 solver.cpp:397]     Test net output #1: loss = 0.289085 (* 1 = 0.289085 loss)
I1122 08:43:28.708560 18896 solver.cpp:218] Iteration 29500 (17.5752 iter/s, 5.68982s/100 iters), loss = 0.129045
I1122 08:43:28.708560 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 08:43:28.708560 18896 solver.cpp:237]     Train net output #1: loss = 0.129045 (* 1 = 0.129045 loss)
I1122 08:43:28.708560 18896 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1122 08:43:33.240800 18896 solver.cpp:218] Iteration 29600 (22.0642 iter/s, 4.53223s/100 iters), loss = 0.152165
I1122 08:43:33.240800 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:43:33.240800 18896 solver.cpp:237]     Train net output #1: loss = 0.152164 (* 1 = 0.152164 loss)
I1122 08:43:33.240800 18896 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1122 08:43:37.759743 18896 solver.cpp:218] Iteration 29700 (22.1339 iter/s, 4.51796s/100 iters), loss = 0.108241
I1122 08:43:37.759743 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 08:43:37.759743 18896 solver.cpp:237]     Train net output #1: loss = 0.108241 (* 1 = 0.108241 loss)
I1122 08:43:37.759743 18896 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1122 08:43:42.304113 18896 solver.cpp:218] Iteration 29800 (22.0059 iter/s, 4.54423s/100 iters), loss = 0.130931
I1122 08:43:42.304113 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 08:43:42.304113 18896 solver.cpp:237]     Train net output #1: loss = 0.130931 (* 1 = 0.130931 loss)
I1122 08:43:42.304113 18896 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1122 08:43:46.924398 18896 solver.cpp:218] Iteration 29900 (21.644 iter/s, 4.62022s/100 iters), loss = 0.122855
I1122 08:43:46.924398 18896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 08:43:46.924398 18896 solver.cpp:237]     Train net output #1: loss = 0.122854 (* 1 = 0.122854 loss)
I1122 08:43:46.924398 18896 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1122 08:43:51.279873 12432 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:43:51.458917 18896 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_30000.caffemodel
I1122 08:43:51.470595 18896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_10L_iter_30000.solverstate
I1122 08:43:51.487596 18896 solver.cpp:310] Iteration 30000, loss = 0.193431
I1122 08:43:51.487596 18896 solver.cpp:330] Iteration 30000, Testing net (#0)
I1122 08:43:51.487596 18896 net.cpp:676] Ignoring source layer accuracy_training
I1122 08:43:52.593775 20276 data_layer.cpp:73] Restarting data prefetching from start.
I1122 08:43:52.636790 18896 solver.cpp:397]     Test net output #0: accuracy = 0.9047
I1122 08:43:52.636790 18896 solver.cpp:397]     Test net output #1: loss = 0.289063 (* 1 = 0.289063 loss)
I1122 08:43:52.637789 18896 solver.cpp:315] Optimization Done.
I1122 08:43:52.637789 18896 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 
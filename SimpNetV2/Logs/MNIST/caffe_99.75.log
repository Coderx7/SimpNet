
G:\Caffe\examples\mnist>REM going to the caffe root 

G:\Caffe\examples\mnist>CD ../../ 

G:\Caffe>SET TOOLS=Build/x64/Release 

I1024 16:23:20.389667  1960 solver.cpp:87] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1024 16:23:20.389667  1960 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/mnist/lenet_train_test.prototxt
I1024 16:23:20.389667  1960 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1024 16:23:20.390666  1960 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1024 16:23:20.390666  1960 net.cpp:51] Initializing net from parameters: 
name: "MNIST_SimpleNet_GP_13L_drpall_5Mil_66_maxdrp"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb_norm2"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 66
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "conv1"
  top: "conv1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "conv1_0"
  top: "conv1_0"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "conv2"
  top: "conv2"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "conv2_1"
  top: "conv2_1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop5"
  type: "Dropout"
  bottom: "pool2_1"
  top: "pool2_1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "conv3"
  top: "conv3"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "drop6_1"
  type: "Dropout"
  bottom: "conv3_1"
  top: "conv3_1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "conv4"
  top: "conv4"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "drop8"
  type: "Dropout"
  bottom: "conv4_1"
  top: "conv4_1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 144
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop9"
  type: "Dropout"
  bottom: "pool4_2"
  top: "pool4_2"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 144
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "drop10"
  type: "Dropout"
  bottom: "conv4_0"
  top: "conv4_0"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 178
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "drop11"
  type: "Dropout"
  bottom: "conv11"
  top: "conv11"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 216
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "drop12"
  type: "Dropout"
  bottom: "poolcp6"
  top: "poolcp6"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1024 16:23:20.390666  1960 layer_factory.cpp:58] Creating layer mnist
I1024 16:23:20.396677  1960 db_lmdb.cpp:40] Opened lmdb examples/mnist/mnist_train_lmdb_norm2
I1024 16:23:20.396677  1960 net.cpp:84] Creating Layer mnist
I1024 16:23:20.396677  1960 net.cpp:380] mnist -> data
I1024 16:23:20.396677  1960 net.cpp:380] mnist -> label
I1024 16:23:20.397666  1960 data_layer.cpp:45] output data size: 100,1,28,28
I1024 16:23:20.399675  1960 net.cpp:122] Setting up mnist
I1024 16:23:20.399675  1960 net.cpp:129] Top shape: 100 1 28 28 (78400)
I1024 16:23:20.399675  1960 net.cpp:129] Top shape: 100 (100)
I1024 16:23:20.399675  1960 net.cpp:137] Memory required for data: 314000
I1024 16:23:20.399675  1960 layer_factory.cpp:58] Creating layer label_mnist_1_split
I1024 16:23:20.399675  1960 net.cpp:84] Creating Layer label_mnist_1_split
I1024 16:23:20.399675  1960 net.cpp:406] label_mnist_1_split <- label
I1024 16:23:20.399675  1960 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_0
I1024 16:23:20.399675  1960 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_1
I1024 16:23:20.399675  1960 net.cpp:122] Setting up label_mnist_1_split
I1024 16:23:20.399675  1960 net.cpp:129] Top shape: 100 (100)
I1024 16:23:20.399675  1960 net.cpp:129] Top shape: 100 (100)
I1024 16:23:20.399675  1960 net.cpp:137] Memory required for data: 314800
I1024 16:23:20.399675  1960 layer_factory.cpp:58] Creating layer conv1
I1024 16:23:20.399675  1960 net.cpp:84] Creating Layer conv1
I1024 16:23:20.399675  1960 net.cpp:406] conv1 <- data
I1024 16:23:20.399675  1960 net.cpp:380] conv1 -> conv1
I1024 16:23:20.401674 11712 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1024 16:23:20.654667  1960 net.cpp:122] Setting up conv1
I1024 16:23:20.654667  1960 net.cpp:129] Top shape: 100 66 28 28 (5174400)
I1024 16:23:20.654667  1960 net.cpp:137] Memory required for data: 21012400
I1024 16:23:20.654667  1960 layer_factory.cpp:58] Creating layer bn1
I1024 16:23:20.654667  1960 net.cpp:84] Creating Layer bn1
I1024 16:23:20.654667  1960 net.cpp:406] bn1 <- conv1
I1024 16:23:20.654667  1960 net.cpp:367] bn1 -> conv1 (in-place)
I1024 16:23:20.655666  1960 net.cpp:122] Setting up bn1
I1024 16:23:20.655666  1960 net.cpp:129] Top shape: 100 66 28 28 (5174400)
I1024 16:23:20.655666  1960 net.cpp:137] Memory required for data: 41710000
I1024 16:23:20.655666  1960 layer_factory.cpp:58] Creating layer scale1
I1024 16:23:20.655666  1960 net.cpp:84] Creating Layer scale1
I1024 16:23:20.655666  1960 net.cpp:406] scale1 <- conv1
I1024 16:23:20.655666  1960 net.cpp:367] scale1 -> conv1 (in-place)
I1024 16:23:20.655666  1960 layer_factory.cpp:58] Creating layer scale1
I1024 16:23:20.655666  1960 net.cpp:122] Setting up scale1
I1024 16:23:20.655666  1960 net.cpp:129] Top shape: 100 66 28 28 (5174400)
I1024 16:23:20.655666  1960 net.cpp:137] Memory required for data: 62407600
I1024 16:23:20.655666  1960 layer_factory.cpp:58] Creating layer relu1
I1024 16:23:20.655666  1960 net.cpp:84] Creating Layer relu1
I1024 16:23:20.655666  1960 net.cpp:406] relu1 <- conv1
I1024 16:23:20.655666  1960 net.cpp:367] relu1 -> conv1 (in-place)
I1024 16:23:20.655666  1960 net.cpp:122] Setting up relu1
I1024 16:23:20.655666  1960 net.cpp:129] Top shape: 100 66 28 28 (5174400)
I1024 16:23:20.655666  1960 net.cpp:137] Memory required for data: 83105200
I1024 16:23:20.655666  1960 layer_factory.cpp:58] Creating layer drop1
I1024 16:23:20.655666  1960 net.cpp:84] Creating Layer drop1
I1024 16:23:20.655666  1960 net.cpp:406] drop1 <- conv1
I1024 16:23:20.655666  1960 net.cpp:367] drop1 -> conv1 (in-place)
I1024 16:23:20.655666  1960 net.cpp:122] Setting up drop1
I1024 16:23:20.655666  1960 net.cpp:129] Top shape: 100 66 28 28 (5174400)
I1024 16:23:20.655666  1960 net.cpp:137] Memory required for data: 103802800
I1024 16:23:20.655666  1960 layer_factory.cpp:58] Creating layer conv1_0
I1024 16:23:20.655666  1960 net.cpp:84] Creating Layer conv1_0
I1024 16:23:20.655666  1960 net.cpp:406] conv1_0 <- conv1
I1024 16:23:20.655666  1960 net.cpp:380] conv1_0 -> conv1_0
I1024 16:23:20.657666  1960 net.cpp:122] Setting up conv1_0
I1024 16:23:20.657666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.657666  1960 net.cpp:137] Memory required for data: 123873200
I1024 16:23:20.657666  1960 layer_factory.cpp:58] Creating layer bn1_0
I1024 16:23:20.657666  1960 net.cpp:84] Creating Layer bn1_0
I1024 16:23:20.657666  1960 net.cpp:406] bn1_0 <- conv1_0
I1024 16:23:20.657666  1960 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1024 16:23:20.657666  1960 net.cpp:122] Setting up bn1_0
I1024 16:23:20.657666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.657666  1960 net.cpp:137] Memory required for data: 143943600
I1024 16:23:20.657666  1960 layer_factory.cpp:58] Creating layer scale1_0
I1024 16:23:20.657666  1960 net.cpp:84] Creating Layer scale1_0
I1024 16:23:20.657666  1960 net.cpp:406] scale1_0 <- conv1_0
I1024 16:23:20.657666  1960 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1024 16:23:20.657666  1960 layer_factory.cpp:58] Creating layer scale1_0
I1024 16:23:20.658666  1960 net.cpp:122] Setting up scale1_0
I1024 16:23:20.658666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.658666  1960 net.cpp:137] Memory required for data: 164014000
I1024 16:23:20.658666  1960 layer_factory.cpp:58] Creating layer relu1_0
I1024 16:23:20.658666  1960 net.cpp:84] Creating Layer relu1_0
I1024 16:23:20.658666  1960 net.cpp:406] relu1_0 <- conv1_0
I1024 16:23:20.658666  1960 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1024 16:23:20.658666  1960 net.cpp:122] Setting up relu1_0
I1024 16:23:20.658666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.658666  1960 net.cpp:137] Memory required for data: 184084400
I1024 16:23:20.658666  1960 layer_factory.cpp:58] Creating layer drop2
I1024 16:23:20.658666  1960 net.cpp:84] Creating Layer drop2
I1024 16:23:20.658666  1960 net.cpp:406] drop2 <- conv1_0
I1024 16:23:20.658666  1960 net.cpp:367] drop2 -> conv1_0 (in-place)
I1024 16:23:20.658666  1960 net.cpp:122] Setting up drop2
I1024 16:23:20.658666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.658666  1960 net.cpp:137] Memory required for data: 204154800
I1024 16:23:20.658666  1960 layer_factory.cpp:58] Creating layer conv2
I1024 16:23:20.658666  1960 net.cpp:84] Creating Layer conv2
I1024 16:23:20.658666  1960 net.cpp:406] conv2 <- conv1_0
I1024 16:23:20.658666  1960 net.cpp:380] conv2 -> conv2
I1024 16:23:20.659667  1960 net.cpp:122] Setting up conv2
I1024 16:23:20.659667  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.659667  1960 net.cpp:137] Memory required for data: 224225200
I1024 16:23:20.659667  1960 layer_factory.cpp:58] Creating layer bn2
I1024 16:23:20.659667  1960 net.cpp:84] Creating Layer bn2
I1024 16:23:20.659667  1960 net.cpp:406] bn2 <- conv2
I1024 16:23:20.659667  1960 net.cpp:367] bn2 -> conv2 (in-place)
I1024 16:23:20.659667  1960 net.cpp:122] Setting up bn2
I1024 16:23:20.659667  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.659667  1960 net.cpp:137] Memory required for data: 244295600
I1024 16:23:20.659667  1960 layer_factory.cpp:58] Creating layer scale2
I1024 16:23:20.659667  1960 net.cpp:84] Creating Layer scale2
I1024 16:23:20.659667  1960 net.cpp:406] scale2 <- conv2
I1024 16:23:20.660667  1960 net.cpp:367] scale2 -> conv2 (in-place)
I1024 16:23:20.660667  1960 layer_factory.cpp:58] Creating layer scale2
I1024 16:23:20.660667  1960 net.cpp:122] Setting up scale2
I1024 16:23:20.660667  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.660667  1960 net.cpp:137] Memory required for data: 264366000
I1024 16:23:20.660667  1960 layer_factory.cpp:58] Creating layer relu2
I1024 16:23:20.660667  1960 net.cpp:84] Creating Layer relu2
I1024 16:23:20.660667  1960 net.cpp:406] relu2 <- conv2
I1024 16:23:20.660667  1960 net.cpp:367] relu2 -> conv2 (in-place)
I1024 16:23:20.660667  1960 net.cpp:122] Setting up relu2
I1024 16:23:20.660667  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.660667  1960 net.cpp:137] Memory required for data: 284436400
I1024 16:23:20.660667  1960 layer_factory.cpp:58] Creating layer drop3
I1024 16:23:20.660667  1960 net.cpp:84] Creating Layer drop3
I1024 16:23:20.660667  1960 net.cpp:406] drop3 <- conv2
I1024 16:23:20.660667  1960 net.cpp:367] drop3 -> conv2 (in-place)
I1024 16:23:20.660667  1960 net.cpp:122] Setting up drop3
I1024 16:23:20.660667  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.660667  1960 net.cpp:137] Memory required for data: 304506800
I1024 16:23:20.660667  1960 layer_factory.cpp:58] Creating layer conv2_1
I1024 16:23:20.660667  1960 net.cpp:84] Creating Layer conv2_1
I1024 16:23:20.660667  1960 net.cpp:406] conv2_1 <- conv2
I1024 16:23:20.660667  1960 net.cpp:380] conv2_1 -> conv2_1
I1024 16:23:20.662667  1960 net.cpp:122] Setting up conv2_1
I1024 16:23:20.662667  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.662667  1960 net.cpp:137] Memory required for data: 324577200
I1024 16:23:20.662667  1960 layer_factory.cpp:58] Creating layer bn2_1
I1024 16:23:20.662667  1960 net.cpp:84] Creating Layer bn2_1
I1024 16:23:20.662667  1960 net.cpp:406] bn2_1 <- conv2_1
I1024 16:23:20.662667  1960 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1024 16:23:20.662667  1960 net.cpp:122] Setting up bn2_1
I1024 16:23:20.662667  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.662667  1960 net.cpp:137] Memory required for data: 344647600
I1024 16:23:20.662667  1960 layer_factory.cpp:58] Creating layer scale2_1
I1024 16:23:20.662667  1960 net.cpp:84] Creating Layer scale2_1
I1024 16:23:20.662667  1960 net.cpp:406] scale2_1 <- conv2_1
I1024 16:23:20.662667  1960 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1024 16:23:20.662667  1960 layer_factory.cpp:58] Creating layer scale2_1
I1024 16:23:20.662667  1960 net.cpp:122] Setting up scale2_1
I1024 16:23:20.662667  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.662667  1960 net.cpp:137] Memory required for data: 364718000
I1024 16:23:20.662667  1960 layer_factory.cpp:58] Creating layer relu2_1
I1024 16:23:20.662667  1960 net.cpp:84] Creating Layer relu2_1
I1024 16:23:20.662667  1960 net.cpp:406] relu2_1 <- conv2_1
I1024 16:23:20.662667  1960 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1024 16:23:20.662667  1960 net.cpp:122] Setting up relu2_1
I1024 16:23:20.662667  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.662667  1960 net.cpp:137] Memory required for data: 384788400
I1024 16:23:20.663666  1960 layer_factory.cpp:58] Creating layer drop4
I1024 16:23:20.663666  1960 net.cpp:84] Creating Layer drop4
I1024 16:23:20.663666  1960 net.cpp:406] drop4 <- conv2_1
I1024 16:23:20.663666  1960 net.cpp:367] drop4 -> conv2_1 (in-place)
I1024 16:23:20.663666  1960 net.cpp:122] Setting up drop4
I1024 16:23:20.663666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.663666  1960 net.cpp:137] Memory required for data: 404858800
I1024 16:23:20.663666  1960 layer_factory.cpp:58] Creating layer conv2_2
I1024 16:23:20.663666  1960 net.cpp:84] Creating Layer conv2_2
I1024 16:23:20.663666  1960 net.cpp:406] conv2_2 <- conv2_1
I1024 16:23:20.663666  1960 net.cpp:380] conv2_2 -> conv2_2
I1024 16:23:20.664666  1960 net.cpp:122] Setting up conv2_2
I1024 16:23:20.664666  1960 net.cpp:129] Top shape: 100 96 28 28 (7526400)
I1024 16:23:20.664666  1960 net.cpp:137] Memory required for data: 434964400
I1024 16:23:20.664666  1960 layer_factory.cpp:58] Creating layer bn2_2
I1024 16:23:20.664666  1960 net.cpp:84] Creating Layer bn2_2
I1024 16:23:20.664666  1960 net.cpp:406] bn2_2 <- conv2_2
I1024 16:23:20.664666  1960 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1024 16:23:20.664666  1960 net.cpp:122] Setting up bn2_2
I1024 16:23:20.664666  1960 net.cpp:129] Top shape: 100 96 28 28 (7526400)
I1024 16:23:20.664666  1960 net.cpp:137] Memory required for data: 465070000
I1024 16:23:20.664666  1960 layer_factory.cpp:58] Creating layer scale2_2
I1024 16:23:20.664666  1960 net.cpp:84] Creating Layer scale2_2
I1024 16:23:20.664666  1960 net.cpp:406] scale2_2 <- conv2_2
I1024 16:23:20.664666  1960 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1024 16:23:20.664666  1960 layer_factory.cpp:58] Creating layer scale2_2
I1024 16:23:20.665666  1960 net.cpp:122] Setting up scale2_2
I1024 16:23:20.665666  1960 net.cpp:129] Top shape: 100 96 28 28 (7526400)
I1024 16:23:20.665666  1960 net.cpp:137] Memory required for data: 495175600
I1024 16:23:20.665666  1960 layer_factory.cpp:58] Creating layer relu2_2
I1024 16:23:20.665666  1960 net.cpp:84] Creating Layer relu2_2
I1024 16:23:20.665666  1960 net.cpp:406] relu2_2 <- conv2_2
I1024 16:23:20.665666  1960 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1024 16:23:20.665666  1960 net.cpp:122] Setting up relu2_2
I1024 16:23:20.665666  1960 net.cpp:129] Top shape: 100 96 28 28 (7526400)
I1024 16:23:20.665666  1960 net.cpp:137] Memory required for data: 525281200
I1024 16:23:20.665666  1960 layer_factory.cpp:58] Creating layer pool2_1
I1024 16:23:20.665666  1960 net.cpp:84] Creating Layer pool2_1
I1024 16:23:20.665666  1960 net.cpp:406] pool2_1 <- conv2_2
I1024 16:23:20.665666  1960 net.cpp:380] pool2_1 -> pool2_1
I1024 16:23:20.665666  1960 net.cpp:122] Setting up pool2_1
I1024 16:23:20.665666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.665666  1960 net.cpp:137] Memory required for data: 532807600
I1024 16:23:20.665666  1960 layer_factory.cpp:58] Creating layer drop5
I1024 16:23:20.665666  1960 net.cpp:84] Creating Layer drop5
I1024 16:23:20.665666  1960 net.cpp:406] drop5 <- pool2_1
I1024 16:23:20.665666  1960 net.cpp:367] drop5 -> pool2_1 (in-place)
I1024 16:23:20.665666  1960 net.cpp:122] Setting up drop5
I1024 16:23:20.665666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.665666  1960 net.cpp:137] Memory required for data: 540334000
I1024 16:23:20.665666  1960 layer_factory.cpp:58] Creating layer conv3
I1024 16:23:20.665666  1960 net.cpp:84] Creating Layer conv3
I1024 16:23:20.665666  1960 net.cpp:406] conv3 <- pool2_1
I1024 16:23:20.665666  1960 net.cpp:380] conv3 -> conv3
I1024 16:23:20.667667  1960 net.cpp:122] Setting up conv3
I1024 16:23:20.667667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.667667  1960 net.cpp:137] Memory required for data: 547860400
I1024 16:23:20.667667  1960 layer_factory.cpp:58] Creating layer bn3
I1024 16:23:20.667667  1960 net.cpp:84] Creating Layer bn3
I1024 16:23:20.667667  1960 net.cpp:406] bn3 <- conv3
I1024 16:23:20.667667  1960 net.cpp:367] bn3 -> conv3 (in-place)
I1024 16:23:20.667667  1960 net.cpp:122] Setting up bn3
I1024 16:23:20.667667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.667667  1960 net.cpp:137] Memory required for data: 555386800
I1024 16:23:20.667667  1960 layer_factory.cpp:58] Creating layer scale3
I1024 16:23:20.667667  1960 net.cpp:84] Creating Layer scale3
I1024 16:23:20.667667  1960 net.cpp:406] scale3 <- conv3
I1024 16:23:20.667667  1960 net.cpp:367] scale3 -> conv3 (in-place)
I1024 16:23:20.667667  1960 layer_factory.cpp:58] Creating layer scale3
I1024 16:23:20.667667  1960 net.cpp:122] Setting up scale3
I1024 16:23:20.667667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.667667  1960 net.cpp:137] Memory required for data: 562913200
I1024 16:23:20.667667  1960 layer_factory.cpp:58] Creating layer relu3
I1024 16:23:20.667667  1960 net.cpp:84] Creating Layer relu3
I1024 16:23:20.667667  1960 net.cpp:406] relu3 <- conv3
I1024 16:23:20.667667  1960 net.cpp:367] relu3 -> conv3 (in-place)
I1024 16:23:20.667667  1960 net.cpp:122] Setting up relu3
I1024 16:23:20.667667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.667667  1960 net.cpp:137] Memory required for data: 570439600
I1024 16:23:20.667667  1960 layer_factory.cpp:58] Creating layer drop6
I1024 16:23:20.667667  1960 net.cpp:84] Creating Layer drop6
I1024 16:23:20.668666  1960 net.cpp:406] drop6 <- conv3
I1024 16:23:20.668666  1960 net.cpp:367] drop6 -> conv3 (in-place)
I1024 16:23:20.668666  1960 net.cpp:122] Setting up drop6
I1024 16:23:20.668666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.668666  1960 net.cpp:137] Memory required for data: 577966000
I1024 16:23:20.668666  1960 layer_factory.cpp:58] Creating layer conv3_1
I1024 16:23:20.668666  1960 net.cpp:84] Creating Layer conv3_1
I1024 16:23:20.668666  1960 net.cpp:406] conv3_1 <- conv3
I1024 16:23:20.668666  1960 net.cpp:380] conv3_1 -> conv3_1
I1024 16:23:20.669667  1960 net.cpp:122] Setting up conv3_1
I1024 16:23:20.669667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.669667  1960 net.cpp:137] Memory required for data: 585492400
I1024 16:23:20.669667  1960 layer_factory.cpp:58] Creating layer bn3_1
I1024 16:23:20.669667  1960 net.cpp:84] Creating Layer bn3_1
I1024 16:23:20.669667  1960 net.cpp:406] bn3_1 <- conv3_1
I1024 16:23:20.669667  1960 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1024 16:23:20.669667  1960 net.cpp:122] Setting up bn3_1
I1024 16:23:20.669667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.669667  1960 net.cpp:137] Memory required for data: 593018800
I1024 16:23:20.669667  1960 layer_factory.cpp:58] Creating layer scale3_1
I1024 16:23:20.669667  1960 net.cpp:84] Creating Layer scale3_1
I1024 16:23:20.670666  1960 net.cpp:406] scale3_1 <- conv3_1
I1024 16:23:20.670666  1960 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1024 16:23:20.670666  1960 layer_factory.cpp:58] Creating layer scale3_1
I1024 16:23:20.670666  1960 net.cpp:122] Setting up scale3_1
I1024 16:23:20.670666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.670666  1960 net.cpp:137] Memory required for data: 600545200
I1024 16:23:20.670666  1960 layer_factory.cpp:58] Creating layer relu3_1
I1024 16:23:20.670666  1960 net.cpp:84] Creating Layer relu3_1
I1024 16:23:20.670666  1960 net.cpp:406] relu3_1 <- conv3_1
I1024 16:23:20.670666  1960 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1024 16:23:20.670666  1960 net.cpp:122] Setting up relu3_1
I1024 16:23:20.670666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.670666  1960 net.cpp:137] Memory required for data: 608071600
I1024 16:23:20.670666  1960 layer_factory.cpp:58] Creating layer drop6_1
I1024 16:23:20.670666  1960 net.cpp:84] Creating Layer drop6_1
I1024 16:23:20.670666  1960 net.cpp:406] drop6_1 <- conv3_1
I1024 16:23:20.670666  1960 net.cpp:367] drop6_1 -> conv3_1 (in-place)
I1024 16:23:20.670666  1960 net.cpp:122] Setting up drop6_1
I1024 16:23:20.670666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.670666  1960 net.cpp:137] Memory required for data: 615598000
I1024 16:23:20.670666  1960 layer_factory.cpp:58] Creating layer conv4
I1024 16:23:20.670666  1960 net.cpp:84] Creating Layer conv4
I1024 16:23:20.670666  1960 net.cpp:406] conv4 <- conv3_1
I1024 16:23:20.670666  1960 net.cpp:380] conv4 -> conv4
I1024 16:23:20.672667  1960 net.cpp:122] Setting up conv4
I1024 16:23:20.672667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.672667  1960 net.cpp:137] Memory required for data: 623124400
I1024 16:23:20.672667  1960 layer_factory.cpp:58] Creating layer bn4
I1024 16:23:20.672667  1960 net.cpp:84] Creating Layer bn4
I1024 16:23:20.672667  1960 net.cpp:406] bn4 <- conv4
I1024 16:23:20.672667  1960 net.cpp:367] bn4 -> conv4 (in-place)
I1024 16:23:20.673666  1960 net.cpp:122] Setting up bn4
I1024 16:23:20.673666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.673666  1960 net.cpp:137] Memory required for data: 630650800
I1024 16:23:20.673666  1960 layer_factory.cpp:58] Creating layer scale4
I1024 16:23:20.673666  1960 net.cpp:84] Creating Layer scale4
I1024 16:23:20.673666  1960 net.cpp:406] scale4 <- conv4
I1024 16:23:20.673666  1960 net.cpp:367] scale4 -> conv4 (in-place)
I1024 16:23:20.673666  1960 layer_factory.cpp:58] Creating layer scale4
I1024 16:23:20.673666  1960 net.cpp:122] Setting up scale4
I1024 16:23:20.673666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.673666  1960 net.cpp:137] Memory required for data: 638177200
I1024 16:23:20.673666  1960 layer_factory.cpp:58] Creating layer relu4
I1024 16:23:20.673666  1960 net.cpp:84] Creating Layer relu4
I1024 16:23:20.673666  1960 net.cpp:406] relu4 <- conv4
I1024 16:23:20.673666  1960 net.cpp:367] relu4 -> conv4 (in-place)
I1024 16:23:20.673666  1960 net.cpp:122] Setting up relu4
I1024 16:23:20.673666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.673666  1960 net.cpp:137] Memory required for data: 645703600
I1024 16:23:20.673666  1960 layer_factory.cpp:58] Creating layer drop7
I1024 16:23:20.673666  1960 net.cpp:84] Creating Layer drop7
I1024 16:23:20.673666  1960 net.cpp:406] drop7 <- conv4
I1024 16:23:20.673666  1960 net.cpp:367] drop7 -> conv4 (in-place)
I1024 16:23:20.674666  1960 net.cpp:122] Setting up drop7
I1024 16:23:20.674666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.674666  1960 net.cpp:137] Memory required for data: 653230000
I1024 16:23:20.674666  1960 layer_factory.cpp:58] Creating layer conv4_1
I1024 16:23:20.674666  1960 net.cpp:84] Creating Layer conv4_1
I1024 16:23:20.674666  1960 net.cpp:406] conv4_1 <- conv4
I1024 16:23:20.674666  1960 net.cpp:380] conv4_1 -> conv4_1
I1024 16:23:20.675667  1960 net.cpp:122] Setting up conv4_1
I1024 16:23:20.675667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.675667  1960 net.cpp:137] Memory required for data: 660756400
I1024 16:23:20.675667  1960 layer_factory.cpp:58] Creating layer bn4_1
I1024 16:23:20.675667  1960 net.cpp:84] Creating Layer bn4_1
I1024 16:23:20.675667  1960 net.cpp:406] bn4_1 <- conv4_1
I1024 16:23:20.675667  1960 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1024 16:23:20.675667  1960 net.cpp:122] Setting up bn4_1
I1024 16:23:20.675667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.675667  1960 net.cpp:137] Memory required for data: 668282800
I1024 16:23:20.675667  1960 layer_factory.cpp:58] Creating layer scale4_1
I1024 16:23:20.675667  1960 net.cpp:84] Creating Layer scale4_1
I1024 16:23:20.675667  1960 net.cpp:406] scale4_1 <- conv4_1
I1024 16:23:20.675667  1960 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1024 16:23:20.675667  1960 layer_factory.cpp:58] Creating layer scale4_1
I1024 16:23:20.675667  1960 net.cpp:122] Setting up scale4_1
I1024 16:23:20.675667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.675667  1960 net.cpp:137] Memory required for data: 675809200
I1024 16:23:20.675667  1960 layer_factory.cpp:58] Creating layer relu4_1
I1024 16:23:20.675667  1960 net.cpp:84] Creating Layer relu4_1
I1024 16:23:20.675667  1960 net.cpp:406] relu4_1 <- conv4_1
I1024 16:23:20.675667  1960 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1024 16:23:20.676666  1960 net.cpp:122] Setting up relu4_1
I1024 16:23:20.676666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.676666  1960 net.cpp:137] Memory required for data: 683335600
I1024 16:23:20.676666  1960 layer_factory.cpp:58] Creating layer drop8
I1024 16:23:20.676666  1960 net.cpp:84] Creating Layer drop8
I1024 16:23:20.676666  1960 net.cpp:406] drop8 <- conv4_1
I1024 16:23:20.676666  1960 net.cpp:367] drop8 -> conv4_1 (in-place)
I1024 16:23:20.676666  1960 net.cpp:122] Setting up drop8
I1024 16:23:20.676666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.676666  1960 net.cpp:137] Memory required for data: 690862000
I1024 16:23:20.676666  1960 layer_factory.cpp:58] Creating layer conv4_2
I1024 16:23:20.676666  1960 net.cpp:84] Creating Layer conv4_2
I1024 16:23:20.676666  1960 net.cpp:406] conv4_2 <- conv4_1
I1024 16:23:20.676666  1960 net.cpp:380] conv4_2 -> conv4_2
I1024 16:23:20.678668  1960 net.cpp:122] Setting up conv4_2
I1024 16:23:20.678668  1960 net.cpp:129] Top shape: 100 144 14 14 (2822400)
I1024 16:23:20.678668  1960 net.cpp:137] Memory required for data: 702151600
I1024 16:23:20.678668  1960 layer_factory.cpp:58] Creating layer bn4_2
I1024 16:23:20.678668  1960 net.cpp:84] Creating Layer bn4_2
I1024 16:23:20.678668  1960 net.cpp:406] bn4_2 <- conv4_2
I1024 16:23:20.678668  1960 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1024 16:23:20.679667  1960 net.cpp:122] Setting up bn4_2
I1024 16:23:20.679667  1960 net.cpp:129] Top shape: 100 144 14 14 (2822400)
I1024 16:23:20.679667  1960 net.cpp:137] Memory required for data: 713441200
I1024 16:23:20.679667  1960 layer_factory.cpp:58] Creating layer scale4_2
I1024 16:23:20.679667  1960 net.cpp:84] Creating Layer scale4_2
I1024 16:23:20.679667  1960 net.cpp:406] scale4_2 <- conv4_2
I1024 16:23:20.679667  1960 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1024 16:23:20.679667  1960 layer_factory.cpp:58] Creating layer scale4_2
I1024 16:23:20.679667  1960 net.cpp:122] Setting up scale4_2
I1024 16:23:20.679667  1960 net.cpp:129] Top shape: 100 144 14 14 (2822400)
I1024 16:23:20.679667  1960 net.cpp:137] Memory required for data: 724730800
I1024 16:23:20.679667  1960 layer_factory.cpp:58] Creating layer relu4_2
I1024 16:23:20.679667  1960 net.cpp:84] Creating Layer relu4_2
I1024 16:23:20.679667  1960 net.cpp:406] relu4_2 <- conv4_2
I1024 16:23:20.679667  1960 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1024 16:23:20.679667  1960 net.cpp:122] Setting up relu4_2
I1024 16:23:20.679667  1960 net.cpp:129] Top shape: 100 144 14 14 (2822400)
I1024 16:23:20.679667  1960 net.cpp:137] Memory required for data: 736020400
I1024 16:23:20.679667  1960 layer_factory.cpp:58] Creating layer pool4_2
I1024 16:23:20.679667  1960 net.cpp:84] Creating Layer pool4_2
I1024 16:23:20.679667  1960 net.cpp:406] pool4_2 <- conv4_2
I1024 16:23:20.679667  1960 net.cpp:380] pool4_2 -> pool4_2
I1024 16:23:20.679667  1960 net.cpp:122] Setting up pool4_2
I1024 16:23:20.679667  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.679667  1960 net.cpp:137] Memory required for data: 738842800
I1024 16:23:20.679667  1960 layer_factory.cpp:58] Creating layer drop9
I1024 16:23:20.679667  1960 net.cpp:84] Creating Layer drop9
I1024 16:23:20.679667  1960 net.cpp:406] drop9 <- pool4_2
I1024 16:23:20.679667  1960 net.cpp:367] drop9 -> pool4_2 (in-place)
I1024 16:23:20.679667  1960 net.cpp:122] Setting up drop9
I1024 16:23:20.679667  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.679667  1960 net.cpp:137] Memory required for data: 741665200
I1024 16:23:20.679667  1960 layer_factory.cpp:58] Creating layer conv4_0
I1024 16:23:20.679667  1960 net.cpp:84] Creating Layer conv4_0
I1024 16:23:20.679667  1960 net.cpp:406] conv4_0 <- pool4_2
I1024 16:23:20.679667  1960 net.cpp:380] conv4_0 -> conv4_0
I1024 16:23:20.682667  1960 net.cpp:122] Setting up conv4_0
I1024 16:23:20.682667  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.682667  1960 net.cpp:137] Memory required for data: 744487600
I1024 16:23:20.682667  1960 layer_factory.cpp:58] Creating layer bn4_0
I1024 16:23:20.682667  1960 net.cpp:84] Creating Layer bn4_0
I1024 16:23:20.682667  1960 net.cpp:406] bn4_0 <- conv4_0
I1024 16:23:20.682667  1960 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1024 16:23:20.682667  1960 net.cpp:122] Setting up bn4_0
I1024 16:23:20.682667  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.682667  1960 net.cpp:137] Memory required for data: 747310000
I1024 16:23:20.682667  1960 layer_factory.cpp:58] Creating layer scale4_0
I1024 16:23:20.682667  1960 net.cpp:84] Creating Layer scale4_0
I1024 16:23:20.682667  1960 net.cpp:406] scale4_0 <- conv4_0
I1024 16:23:20.682667  1960 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1024 16:23:20.682667  1960 layer_factory.cpp:58] Creating layer scale4_0
I1024 16:23:20.682667  1960 net.cpp:122] Setting up scale4_0
I1024 16:23:20.682667  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.682667  1960 net.cpp:137] Memory required for data: 750132400
I1024 16:23:20.682667  1960 layer_factory.cpp:58] Creating layer relu4_0
I1024 16:23:20.682667  1960 net.cpp:84] Creating Layer relu4_0
I1024 16:23:20.682667  1960 net.cpp:406] relu4_0 <- conv4_0
I1024 16:23:20.682667  1960 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1024 16:23:20.682667  1960 net.cpp:122] Setting up relu4_0
I1024 16:23:20.682667  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.682667  1960 net.cpp:137] Memory required for data: 752954800
I1024 16:23:20.682667  1960 layer_factory.cpp:58] Creating layer drop10
I1024 16:23:20.682667  1960 net.cpp:84] Creating Layer drop10
I1024 16:23:20.682667  1960 net.cpp:406] drop10 <- conv4_0
I1024 16:23:20.682667  1960 net.cpp:367] drop10 -> conv4_0 (in-place)
I1024 16:23:20.682667  1960 net.cpp:122] Setting up drop10
I1024 16:23:20.683666  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.683666  1960 net.cpp:137] Memory required for data: 755777200
I1024 16:23:20.683666  1960 layer_factory.cpp:58] Creating layer conv11
I1024 16:23:20.683666  1960 net.cpp:84] Creating Layer conv11
I1024 16:23:20.683666  1960 net.cpp:406] conv11 <- conv4_0
I1024 16:23:20.683666  1960 net.cpp:380] conv11 -> conv11
I1024 16:23:20.686666  1960 net.cpp:122] Setting up conv11
I1024 16:23:20.686666  1960 net.cpp:129] Top shape: 100 178 7 7 (872200)
I1024 16:23:20.686666  1960 net.cpp:137] Memory required for data: 759266000
I1024 16:23:20.686666  1960 layer_factory.cpp:58] Creating layer bn_conv11
I1024 16:23:20.686666  1960 net.cpp:84] Creating Layer bn_conv11
I1024 16:23:20.686666  1960 net.cpp:406] bn_conv11 <- conv11
I1024 16:23:20.686666  1960 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1024 16:23:20.686666  1960 net.cpp:122] Setting up bn_conv11
I1024 16:23:20.686666  1960 net.cpp:129] Top shape: 100 178 7 7 (872200)
I1024 16:23:20.686666  1960 net.cpp:137] Memory required for data: 762754800
I1024 16:23:20.686666  1960 layer_factory.cpp:58] Creating layer scale_conv11
I1024 16:23:20.686666  1960 net.cpp:84] Creating Layer scale_conv11
I1024 16:23:20.686666  1960 net.cpp:406] scale_conv11 <- conv11
I1024 16:23:20.686666  1960 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1024 16:23:20.686666  1960 layer_factory.cpp:58] Creating layer scale_conv11
I1024 16:23:20.686666  1960 net.cpp:122] Setting up scale_conv11
I1024 16:23:20.686666  1960 net.cpp:129] Top shape: 100 178 7 7 (872200)
I1024 16:23:20.686666  1960 net.cpp:137] Memory required for data: 766243600
I1024 16:23:20.686666  1960 layer_factory.cpp:58] Creating layer relu_conv11
I1024 16:23:20.686666  1960 net.cpp:84] Creating Layer relu_conv11
I1024 16:23:20.686666  1960 net.cpp:406] relu_conv11 <- conv11
I1024 16:23:20.686666  1960 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1024 16:23:20.687666  1960 net.cpp:122] Setting up relu_conv11
I1024 16:23:20.687666  1960 net.cpp:129] Top shape: 100 178 7 7 (872200)
I1024 16:23:20.687666  1960 net.cpp:137] Memory required for data: 769732400
I1024 16:23:20.687666  1960 layer_factory.cpp:58] Creating layer drop11
I1024 16:23:20.687666  1960 net.cpp:84] Creating Layer drop11
I1024 16:23:20.687666  1960 net.cpp:406] drop11 <- conv11
I1024 16:23:20.687666  1960 net.cpp:367] drop11 -> conv11 (in-place)
I1024 16:23:20.687666  1960 net.cpp:122] Setting up drop11
I1024 16:23:20.687666  1960 net.cpp:129] Top shape: 100 178 7 7 (872200)
I1024 16:23:20.687666  1960 net.cpp:137] Memory required for data: 773221200
I1024 16:23:20.687666  1960 layer_factory.cpp:58] Creating layer conv12
I1024 16:23:20.687666  1960 net.cpp:84] Creating Layer conv12
I1024 16:23:20.687666  1960 net.cpp:406] conv12 <- conv11
I1024 16:23:20.687666  1960 net.cpp:380] conv12 -> conv12
I1024 16:23:20.691666  1960 net.cpp:122] Setting up conv12
I1024 16:23:20.691666  1960 net.cpp:129] Top shape: 100 216 7 7 (1058400)
I1024 16:23:20.691666  1960 net.cpp:137] Memory required for data: 777454800
I1024 16:23:20.691666  1960 layer_factory.cpp:58] Creating layer bn_conv12
I1024 16:23:20.691666  1960 net.cpp:84] Creating Layer bn_conv12
I1024 16:23:20.691666  1960 net.cpp:406] bn_conv12 <- conv12
I1024 16:23:20.691666  1960 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1024 16:23:20.691666  1960 net.cpp:122] Setting up bn_conv12
I1024 16:23:20.691666  1960 net.cpp:129] Top shape: 100 216 7 7 (1058400)
I1024 16:23:20.691666  1960 net.cpp:137] Memory required for data: 781688400
I1024 16:23:20.691666  1960 layer_factory.cpp:58] Creating layer scale_conv12
I1024 16:23:20.691666  1960 net.cpp:84] Creating Layer scale_conv12
I1024 16:23:20.691666  1960 net.cpp:406] scale_conv12 <- conv12
I1024 16:23:20.691666  1960 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1024 16:23:20.691666  1960 layer_factory.cpp:58] Creating layer scale_conv12
I1024 16:23:20.692667  1960 net.cpp:122] Setting up scale_conv12
I1024 16:23:20.692667  1960 net.cpp:129] Top shape: 100 216 7 7 (1058400)
I1024 16:23:20.692667  1960 net.cpp:137] Memory required for data: 785922000
I1024 16:23:20.692667  1960 layer_factory.cpp:58] Creating layer relu_conv12
I1024 16:23:20.692667  1960 net.cpp:84] Creating Layer relu_conv12
I1024 16:23:20.692667  1960 net.cpp:406] relu_conv12 <- conv12
I1024 16:23:20.692667  1960 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1024 16:23:20.692667  1960 net.cpp:122] Setting up relu_conv12
I1024 16:23:20.692667  1960 net.cpp:129] Top shape: 100 216 7 7 (1058400)
I1024 16:23:20.692667  1960 net.cpp:137] Memory required for data: 790155600
I1024 16:23:20.692667  1960 layer_factory.cpp:58] Creating layer poolcp6
I1024 16:23:20.692667  1960 net.cpp:84] Creating Layer poolcp6
I1024 16:23:20.692667  1960 net.cpp:406] poolcp6 <- conv12
I1024 16:23:20.692667  1960 net.cpp:380] poolcp6 -> poolcp6
I1024 16:23:20.692667  1960 net.cpp:122] Setting up poolcp6
I1024 16:23:20.692667  1960 net.cpp:129] Top shape: 100 216 1 1 (21600)
I1024 16:23:20.692667  1960 net.cpp:137] Memory required for data: 790242000
I1024 16:23:20.692667  1960 layer_factory.cpp:58] Creating layer drop12
I1024 16:23:20.692667  1960 net.cpp:84] Creating Layer drop12
I1024 16:23:20.692667  1960 net.cpp:406] drop12 <- poolcp6
I1024 16:23:20.692667  1960 net.cpp:367] drop12 -> poolcp6 (in-place)
I1024 16:23:20.692667  1960 net.cpp:122] Setting up drop12
I1024 16:23:20.692667  1960 net.cpp:129] Top shape: 100 216 1 1 (21600)
I1024 16:23:20.692667  1960 net.cpp:137] Memory required for data: 790328400
I1024 16:23:20.692667  1960 layer_factory.cpp:58] Creating layer ip1
I1024 16:23:20.692667  1960 net.cpp:84] Creating Layer ip1
I1024 16:23:20.692667  1960 net.cpp:406] ip1 <- poolcp6
I1024 16:23:20.692667  1960 net.cpp:380] ip1 -> ip1
I1024 16:23:20.693667  1960 net.cpp:122] Setting up ip1
I1024 16:23:20.693667  1960 net.cpp:129] Top shape: 100 10 (1000)
I1024 16:23:20.693667  1960 net.cpp:137] Memory required for data: 790332400
I1024 16:23:20.693667  1960 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1024 16:23:20.693667  1960 net.cpp:84] Creating Layer ip1_ip1_0_split
I1024 16:23:20.693667  1960 net.cpp:406] ip1_ip1_0_split <- ip1
I1024 16:23:20.693667  1960 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1024 16:23:20.693667  1960 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1024 16:23:20.693667  1960 net.cpp:122] Setting up ip1_ip1_0_split
I1024 16:23:20.693667  1960 net.cpp:129] Top shape: 100 10 (1000)
I1024 16:23:20.693667  1960 net.cpp:129] Top shape: 100 10 (1000)
I1024 16:23:20.693667  1960 net.cpp:137] Memory required for data: 790340400
I1024 16:23:20.693667  1960 layer_factory.cpp:58] Creating layer accuracy_training
I1024 16:23:20.693667  1960 net.cpp:84] Creating Layer accuracy_training
I1024 16:23:20.693667  1960 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1024 16:23:20.693667  1960 net.cpp:406] accuracy_training <- label_mnist_1_split_0
I1024 16:23:20.693667  1960 net.cpp:380] accuracy_training -> accuracy_training
I1024 16:23:20.693667  1960 net.cpp:122] Setting up accuracy_training
I1024 16:23:20.693667  1960 net.cpp:129] Top shape: (1)
I1024 16:23:20.693667  1960 net.cpp:137] Memory required for data: 790340404
I1024 16:23:20.693667  1960 layer_factory.cpp:58] Creating layer loss
I1024 16:23:20.693667  1960 net.cpp:84] Creating Layer loss
I1024 16:23:20.693667  1960 net.cpp:406] loss <- ip1_ip1_0_split_1
I1024 16:23:20.693667  1960 net.cpp:406] loss <- label_mnist_1_split_1
I1024 16:23:20.693667  1960 net.cpp:380] loss -> loss
I1024 16:23:20.693667  1960 layer_factory.cpp:58] Creating layer loss
I1024 16:23:20.694666  1960 net.cpp:122] Setting up loss
I1024 16:23:20.694666  1960 net.cpp:129] Top shape: (1)
I1024 16:23:20.694666  1960 net.cpp:132]     with loss weight 1
I1024 16:23:20.694666  1960 net.cpp:137] Memory required for data: 790340408
I1024 16:23:20.694666  1960 net.cpp:198] loss needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:200] accuracy_training does not need backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] ip1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] drop12 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] poolcp6 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] relu_conv12 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] scale_conv12 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] bn_conv12 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] conv12 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] drop11 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] relu_conv11 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] scale_conv11 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] bn_conv11 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] conv11 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] drop10 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] relu4_0 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] scale4_0 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] bn4_0 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] conv4_0 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] drop9 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] pool4_2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] relu4_2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] scale4_2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] bn4_2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] conv4_2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] drop8 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] relu4_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] scale4_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] bn4_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] conv4_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] drop7 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] relu4 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] scale4 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] bn4 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] conv4 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] drop6_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] relu3_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] scale3_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] bn3_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] conv3_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] drop6 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] relu3 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] scale3 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] bn3 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] conv3 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] drop5 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] pool2_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] relu2_2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] scale2_2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] bn2_2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] conv2_2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] drop4 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] relu2_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] scale2_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] bn2_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] conv2_1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] drop3 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] relu2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] scale2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] bn2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] conv2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] drop2 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] relu1_0 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] scale1_0 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] bn1_0 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] conv1_0 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] drop1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] relu1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] scale1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] bn1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:198] conv1 needs backward computation.
I1024 16:23:20.694666  1960 net.cpp:200] label_mnist_1_split does not need backward computation.
I1024 16:23:20.694666  1960 net.cpp:200] mnist does not need backward computation.
I1024 16:23:20.694666  1960 net.cpp:242] This network produces output accuracy_training
I1024 16:23:20.694666  1960 net.cpp:242] This network produces output loss
I1024 16:23:20.694666  1960 net.cpp:255] Network initialization done.
I1024 16:23:20.695667  1960 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/mnist/lenet_train_test.prototxt
I1024 16:23:20.695667  1960 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1024 16:23:20.695667  1960 solver.cpp:172] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1024 16:23:20.695667  1960 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1024 16:23:20.695667  1960 net.cpp:51] Initializing net from parameters: 
name: "MNIST_SimpleNet_GP_13L_drpall_5Mil_66_maxdrp"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb_norm2"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 66
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "conv1"
  top: "conv1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "conv1_0"
  top: "conv1_0"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "conv2"
  top: "conv2"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "conv2_1"
  top: "conv2_1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop5"
  type: "Dropout"
  bottom: "pool2_1"
  top: "pool2_1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "conv3"
  top: "conv3"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "drop6_1"
  type: "Dropout"
  bottom: "conv3_1"
  top: "conv3_1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "conv4"
  top: "conv4"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "drop8"
  type: "Dropout"
  bottom: "conv4_1"
  top: "conv4_1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 144
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop9"
  type: "Dropout"
  bottom: "pool4_2"
  top: "pool4_2"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 144
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "drop10"
  type: "Dropout"
  bottom: "conv4_0"
  top: "conv4_0"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 178
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "drop11"
  type: "Dropout"
  bottom: "conv11"
  top: "conv11"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 216
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "drop12"
  type: "Dropout"
  bottom: "poolcp6"
  top: "poolcp6"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1024 16:23:20.696666  1960 layer_factory.cpp:58] Creating layer mnist
I1024 16:23:20.703670  1960 db_lmdb.cpp:40] Opened lmdb examples/mnist/mnist_test_lmdb_norm2
I1024 16:23:20.703670  1960 net.cpp:84] Creating Layer mnist
I1024 16:23:20.703670  1960 net.cpp:380] mnist -> data
I1024 16:23:20.703670  1960 net.cpp:380] mnist -> label
I1024 16:23:20.703670  1960 data_layer.cpp:45] output data size: 100,1,28,28
I1024 16:23:20.704672  1960 net.cpp:122] Setting up mnist
I1024 16:23:20.704672  1960 net.cpp:129] Top shape: 100 1 28 28 (78400)
I1024 16:23:20.705667  1960 net.cpp:129] Top shape: 100 (100)
I1024 16:23:20.705667  1960 net.cpp:137] Memory required for data: 314000
I1024 16:23:20.705667  1960 layer_factory.cpp:58] Creating layer label_mnist_1_split
I1024 16:23:20.705667  1960 net.cpp:84] Creating Layer label_mnist_1_split
I1024 16:23:20.705667  1960 net.cpp:406] label_mnist_1_split <- label
I1024 16:23:20.705667  1960 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_0
I1024 16:23:20.705667  1960 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_1
I1024 16:23:20.705667  1960 net.cpp:122] Setting up label_mnist_1_split
I1024 16:23:20.705667  1960 net.cpp:129] Top shape: 100 (100)
I1024 16:23:20.705667  1960 net.cpp:129] Top shape: 100 (100)
I1024 16:23:20.705667  1960 net.cpp:137] Memory required for data: 314800
I1024 16:23:20.705667  1960 layer_factory.cpp:58] Creating layer conv1
I1024 16:23:20.705667  1960 net.cpp:84] Creating Layer conv1
I1024 16:23:20.705667  1960 net.cpp:406] conv1 <- data
I1024 16:23:20.705667  1960 net.cpp:380] conv1 -> conv1
I1024 16:23:20.706666  5208 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1024 16:23:20.707666  1960 net.cpp:122] Setting up conv1
I1024 16:23:20.707666  1960 net.cpp:129] Top shape: 100 66 28 28 (5174400)
I1024 16:23:20.707666  1960 net.cpp:137] Memory required for data: 21012400
I1024 16:23:20.707666  1960 layer_factory.cpp:58] Creating layer bn1
I1024 16:23:20.707666  1960 net.cpp:84] Creating Layer bn1
I1024 16:23:20.707666  1960 net.cpp:406] bn1 <- conv1
I1024 16:23:20.707666  1960 net.cpp:367] bn1 -> conv1 (in-place)
I1024 16:23:20.707666  1960 net.cpp:122] Setting up bn1
I1024 16:23:20.707666  1960 net.cpp:129] Top shape: 100 66 28 28 (5174400)
I1024 16:23:20.707666  1960 net.cpp:137] Memory required for data: 41710000
I1024 16:23:20.707666  1960 layer_factory.cpp:58] Creating layer scale1
I1024 16:23:20.707666  1960 net.cpp:84] Creating Layer scale1
I1024 16:23:20.707666  1960 net.cpp:406] scale1 <- conv1
I1024 16:23:20.707666  1960 net.cpp:367] scale1 -> conv1 (in-place)
I1024 16:23:20.707666  1960 layer_factory.cpp:58] Creating layer scale1
I1024 16:23:20.707666  1960 net.cpp:122] Setting up scale1
I1024 16:23:20.707666  1960 net.cpp:129] Top shape: 100 66 28 28 (5174400)
I1024 16:23:20.707666  1960 net.cpp:137] Memory required for data: 62407600
I1024 16:23:20.707666  1960 layer_factory.cpp:58] Creating layer relu1
I1024 16:23:20.707666  1960 net.cpp:84] Creating Layer relu1
I1024 16:23:20.707666  1960 net.cpp:406] relu1 <- conv1
I1024 16:23:20.707666  1960 net.cpp:367] relu1 -> conv1 (in-place)
I1024 16:23:20.708667  1960 net.cpp:122] Setting up relu1
I1024 16:23:20.708667  1960 net.cpp:129] Top shape: 100 66 28 28 (5174400)
I1024 16:23:20.708667  1960 net.cpp:137] Memory required for data: 83105200
I1024 16:23:20.708667  1960 layer_factory.cpp:58] Creating layer drop1
I1024 16:23:20.708667  1960 net.cpp:84] Creating Layer drop1
I1024 16:23:20.708667  1960 net.cpp:406] drop1 <- conv1
I1024 16:23:20.708667  1960 net.cpp:367] drop1 -> conv1 (in-place)
I1024 16:23:20.708667  1960 net.cpp:122] Setting up drop1
I1024 16:23:20.708667  1960 net.cpp:129] Top shape: 100 66 28 28 (5174400)
I1024 16:23:20.708667  1960 net.cpp:137] Memory required for data: 103802800
I1024 16:23:20.708667  1960 layer_factory.cpp:58] Creating layer conv1_0
I1024 16:23:20.708667  1960 net.cpp:84] Creating Layer conv1_0
I1024 16:23:20.708667  1960 net.cpp:406] conv1_0 <- conv1
I1024 16:23:20.708667  1960 net.cpp:380] conv1_0 -> conv1_0
I1024 16:23:20.709666  1960 net.cpp:122] Setting up conv1_0
I1024 16:23:20.709666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.709666  1960 net.cpp:137] Memory required for data: 123873200
I1024 16:23:20.709666  1960 layer_factory.cpp:58] Creating layer bn1_0
I1024 16:23:20.709666  1960 net.cpp:84] Creating Layer bn1_0
I1024 16:23:20.709666  1960 net.cpp:406] bn1_0 <- conv1_0
I1024 16:23:20.709666  1960 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1024 16:23:20.709666  1960 net.cpp:122] Setting up bn1_0
I1024 16:23:20.709666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.709666  1960 net.cpp:137] Memory required for data: 143943600
I1024 16:23:20.709666  1960 layer_factory.cpp:58] Creating layer scale1_0
I1024 16:23:20.709666  1960 net.cpp:84] Creating Layer scale1_0
I1024 16:23:20.709666  1960 net.cpp:406] scale1_0 <- conv1_0
I1024 16:23:20.709666  1960 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1024 16:23:20.709666  1960 layer_factory.cpp:58] Creating layer scale1_0
I1024 16:23:20.710666  1960 net.cpp:122] Setting up scale1_0
I1024 16:23:20.710666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.710666  1960 net.cpp:137] Memory required for data: 164014000
I1024 16:23:20.710666  1960 layer_factory.cpp:58] Creating layer relu1_0
I1024 16:23:20.710666  1960 net.cpp:84] Creating Layer relu1_0
I1024 16:23:20.710666  1960 net.cpp:406] relu1_0 <- conv1_0
I1024 16:23:20.710666  1960 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1024 16:23:20.710666  1960 net.cpp:122] Setting up relu1_0
I1024 16:23:20.710666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.710666  1960 net.cpp:137] Memory required for data: 184084400
I1024 16:23:20.710666  1960 layer_factory.cpp:58] Creating layer drop2
I1024 16:23:20.710666  1960 net.cpp:84] Creating Layer drop2
I1024 16:23:20.710666  1960 net.cpp:406] drop2 <- conv1_0
I1024 16:23:20.710666  1960 net.cpp:367] drop2 -> conv1_0 (in-place)
I1024 16:23:20.710666  1960 net.cpp:122] Setting up drop2
I1024 16:23:20.710666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.710666  1960 net.cpp:137] Memory required for data: 204154800
I1024 16:23:20.710666  1960 layer_factory.cpp:58] Creating layer conv2
I1024 16:23:20.710666  1960 net.cpp:84] Creating Layer conv2
I1024 16:23:20.710666  1960 net.cpp:406] conv2 <- conv1_0
I1024 16:23:20.710666  1960 net.cpp:380] conv2 -> conv2
I1024 16:23:20.712666  1960 net.cpp:122] Setting up conv2
I1024 16:23:20.712666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.712666  1960 net.cpp:137] Memory required for data: 224225200
I1024 16:23:20.712666  1960 layer_factory.cpp:58] Creating layer bn2
I1024 16:23:20.712666  1960 net.cpp:84] Creating Layer bn2
I1024 16:23:20.712666  1960 net.cpp:406] bn2 <- conv2
I1024 16:23:20.712666  1960 net.cpp:367] bn2 -> conv2 (in-place)
I1024 16:23:20.712666  1960 net.cpp:122] Setting up bn2
I1024 16:23:20.712666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.712666  1960 net.cpp:137] Memory required for data: 244295600
I1024 16:23:20.712666  1960 layer_factory.cpp:58] Creating layer scale2
I1024 16:23:20.712666  1960 net.cpp:84] Creating Layer scale2
I1024 16:23:20.712666  1960 net.cpp:406] scale2 <- conv2
I1024 16:23:20.712666  1960 net.cpp:367] scale2 -> conv2 (in-place)
I1024 16:23:20.712666  1960 layer_factory.cpp:58] Creating layer scale2
I1024 16:23:20.712666  1960 net.cpp:122] Setting up scale2
I1024 16:23:20.712666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.712666  1960 net.cpp:137] Memory required for data: 264366000
I1024 16:23:20.712666  1960 layer_factory.cpp:58] Creating layer relu2
I1024 16:23:20.712666  1960 net.cpp:84] Creating Layer relu2
I1024 16:23:20.712666  1960 net.cpp:406] relu2 <- conv2
I1024 16:23:20.712666  1960 net.cpp:367] relu2 -> conv2 (in-place)
I1024 16:23:20.713666  1960 net.cpp:122] Setting up relu2
I1024 16:23:20.713666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.713666  1960 net.cpp:137] Memory required for data: 284436400
I1024 16:23:20.713666  1960 layer_factory.cpp:58] Creating layer drop3
I1024 16:23:20.713666  1960 net.cpp:84] Creating Layer drop3
I1024 16:23:20.713666  1960 net.cpp:406] drop3 <- conv2
I1024 16:23:20.713666  1960 net.cpp:367] drop3 -> conv2 (in-place)
I1024 16:23:20.713666  1960 net.cpp:122] Setting up drop3
I1024 16:23:20.713666  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.713666  1960 net.cpp:137] Memory required for data: 304506800
I1024 16:23:20.713666  1960 layer_factory.cpp:58] Creating layer conv2_1
I1024 16:23:20.713666  1960 net.cpp:84] Creating Layer conv2_1
I1024 16:23:20.713666  1960 net.cpp:406] conv2_1 <- conv2
I1024 16:23:20.713666  1960 net.cpp:380] conv2_1 -> conv2_1
I1024 16:23:20.714668  1960 net.cpp:122] Setting up conv2_1
I1024 16:23:20.714668  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.714668  1960 net.cpp:137] Memory required for data: 324577200
I1024 16:23:20.714668  1960 layer_factory.cpp:58] Creating layer bn2_1
I1024 16:23:20.714668  1960 net.cpp:84] Creating Layer bn2_1
I1024 16:23:20.714668  1960 net.cpp:406] bn2_1 <- conv2_1
I1024 16:23:20.714668  1960 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1024 16:23:20.714668  1960 net.cpp:122] Setting up bn2_1
I1024 16:23:20.714668  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.714668  1960 net.cpp:137] Memory required for data: 344647600
I1024 16:23:20.714668  1960 layer_factory.cpp:58] Creating layer scale2_1
I1024 16:23:20.714668  1960 net.cpp:84] Creating Layer scale2_1
I1024 16:23:20.714668  1960 net.cpp:406] scale2_1 <- conv2_1
I1024 16:23:20.714668  1960 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1024 16:23:20.714668  1960 layer_factory.cpp:58] Creating layer scale2_1
I1024 16:23:20.714668  1960 net.cpp:122] Setting up scale2_1
I1024 16:23:20.714668  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.714668  1960 net.cpp:137] Memory required for data: 364718000
I1024 16:23:20.714668  1960 layer_factory.cpp:58] Creating layer relu2_1
I1024 16:23:20.714668  1960 net.cpp:84] Creating Layer relu2_1
I1024 16:23:20.714668  1960 net.cpp:406] relu2_1 <- conv2_1
I1024 16:23:20.714668  1960 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1024 16:23:20.715667  1960 net.cpp:122] Setting up relu2_1
I1024 16:23:20.715667  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.715667  1960 net.cpp:137] Memory required for data: 384788400
I1024 16:23:20.715667  1960 layer_factory.cpp:58] Creating layer drop4
I1024 16:23:20.715667  1960 net.cpp:84] Creating Layer drop4
I1024 16:23:20.715667  1960 net.cpp:406] drop4 <- conv2_1
I1024 16:23:20.715667  1960 net.cpp:367] drop4 -> conv2_1 (in-place)
I1024 16:23:20.715667  1960 net.cpp:122] Setting up drop4
I1024 16:23:20.715667  1960 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1024 16:23:20.715667  1960 net.cpp:137] Memory required for data: 404858800
I1024 16:23:20.715667  1960 layer_factory.cpp:58] Creating layer conv2_2
I1024 16:23:20.715667  1960 net.cpp:84] Creating Layer conv2_2
I1024 16:23:20.715667  1960 net.cpp:406] conv2_2 <- conv2_1
I1024 16:23:20.715667  1960 net.cpp:380] conv2_2 -> conv2_2
I1024 16:23:20.717666  1960 net.cpp:122] Setting up conv2_2
I1024 16:23:20.717666  1960 net.cpp:129] Top shape: 100 96 28 28 (7526400)
I1024 16:23:20.717666  1960 net.cpp:137] Memory required for data: 434964400
I1024 16:23:20.717666  1960 layer_factory.cpp:58] Creating layer bn2_2
I1024 16:23:20.717666  1960 net.cpp:84] Creating Layer bn2_2
I1024 16:23:20.717666  1960 net.cpp:406] bn2_2 <- conv2_2
I1024 16:23:20.717666  1960 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1024 16:23:20.717666  1960 net.cpp:122] Setting up bn2_2
I1024 16:23:20.717666  1960 net.cpp:129] Top shape: 100 96 28 28 (7526400)
I1024 16:23:20.717666  1960 net.cpp:137] Memory required for data: 465070000
I1024 16:23:20.717666  1960 layer_factory.cpp:58] Creating layer scale2_2
I1024 16:23:20.717666  1960 net.cpp:84] Creating Layer scale2_2
I1024 16:23:20.717666  1960 net.cpp:406] scale2_2 <- conv2_2
I1024 16:23:20.717666  1960 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1024 16:23:20.717666  1960 layer_factory.cpp:58] Creating layer scale2_2
I1024 16:23:20.717666  1960 net.cpp:122] Setting up scale2_2
I1024 16:23:20.717666  1960 net.cpp:129] Top shape: 100 96 28 28 (7526400)
I1024 16:23:20.717666  1960 net.cpp:137] Memory required for data: 495175600
I1024 16:23:20.717666  1960 layer_factory.cpp:58] Creating layer relu2_2
I1024 16:23:20.717666  1960 net.cpp:84] Creating Layer relu2_2
I1024 16:23:20.717666  1960 net.cpp:406] relu2_2 <- conv2_2
I1024 16:23:20.717666  1960 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1024 16:23:20.718667  1960 net.cpp:122] Setting up relu2_2
I1024 16:23:20.718667  1960 net.cpp:129] Top shape: 100 96 28 28 (7526400)
I1024 16:23:20.718667  1960 net.cpp:137] Memory required for data: 525281200
I1024 16:23:20.718667  1960 layer_factory.cpp:58] Creating layer pool2_1
I1024 16:23:20.718667  1960 net.cpp:84] Creating Layer pool2_1
I1024 16:23:20.718667  1960 net.cpp:406] pool2_1 <- conv2_2
I1024 16:23:20.718667  1960 net.cpp:380] pool2_1 -> pool2_1
I1024 16:23:20.718667  1960 net.cpp:122] Setting up pool2_1
I1024 16:23:20.718667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.718667  1960 net.cpp:137] Memory required for data: 532807600
I1024 16:23:20.718667  1960 layer_factory.cpp:58] Creating layer drop5
I1024 16:23:20.718667  1960 net.cpp:84] Creating Layer drop5
I1024 16:23:20.718667  1960 net.cpp:406] drop5 <- pool2_1
I1024 16:23:20.718667  1960 net.cpp:367] drop5 -> pool2_1 (in-place)
I1024 16:23:20.718667  1960 net.cpp:122] Setting up drop5
I1024 16:23:20.718667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.718667  1960 net.cpp:137] Memory required for data: 540334000
I1024 16:23:20.718667  1960 layer_factory.cpp:58] Creating layer conv3
I1024 16:23:20.718667  1960 net.cpp:84] Creating Layer conv3
I1024 16:23:20.718667  1960 net.cpp:406] conv3 <- pool2_1
I1024 16:23:20.718667  1960 net.cpp:380] conv3 -> conv3
I1024 16:23:20.720669  1960 net.cpp:122] Setting up conv3
I1024 16:23:20.720669  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.720669  1960 net.cpp:137] Memory required for data: 547860400
I1024 16:23:20.720669  1960 layer_factory.cpp:58] Creating layer bn3
I1024 16:23:20.720669  1960 net.cpp:84] Creating Layer bn3
I1024 16:23:20.720669  1960 net.cpp:406] bn3 <- conv3
I1024 16:23:20.720669  1960 net.cpp:367] bn3 -> conv3 (in-place)
I1024 16:23:20.720669  1960 net.cpp:122] Setting up bn3
I1024 16:23:20.720669  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.720669  1960 net.cpp:137] Memory required for data: 555386800
I1024 16:23:20.720669  1960 layer_factory.cpp:58] Creating layer scale3
I1024 16:23:20.720669  1960 net.cpp:84] Creating Layer scale3
I1024 16:23:20.720669  1960 net.cpp:406] scale3 <- conv3
I1024 16:23:20.720669  1960 net.cpp:367] scale3 -> conv3 (in-place)
I1024 16:23:20.720669  1960 layer_factory.cpp:58] Creating layer scale3
I1024 16:23:20.720669  1960 net.cpp:122] Setting up scale3
I1024 16:23:20.720669  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.720669  1960 net.cpp:137] Memory required for data: 562913200
I1024 16:23:20.720669  1960 layer_factory.cpp:58] Creating layer relu3
I1024 16:23:20.720669  1960 net.cpp:84] Creating Layer relu3
I1024 16:23:20.720669  1960 net.cpp:406] relu3 <- conv3
I1024 16:23:20.720669  1960 net.cpp:367] relu3 -> conv3 (in-place)
I1024 16:23:20.721666  1960 net.cpp:122] Setting up relu3
I1024 16:23:20.721666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.721666  1960 net.cpp:137] Memory required for data: 570439600
I1024 16:23:20.721666  1960 layer_factory.cpp:58] Creating layer drop6
I1024 16:23:20.721666  1960 net.cpp:84] Creating Layer drop6
I1024 16:23:20.721666  1960 net.cpp:406] drop6 <- conv3
I1024 16:23:20.721666  1960 net.cpp:367] drop6 -> conv3 (in-place)
I1024 16:23:20.721666  1960 net.cpp:122] Setting up drop6
I1024 16:23:20.721666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.721666  1960 net.cpp:137] Memory required for data: 577966000
I1024 16:23:20.721666  1960 layer_factory.cpp:58] Creating layer conv3_1
I1024 16:23:20.721666  1960 net.cpp:84] Creating Layer conv3_1
I1024 16:23:20.721666  1960 net.cpp:406] conv3_1 <- conv3
I1024 16:23:20.721666  1960 net.cpp:380] conv3_1 -> conv3_1
I1024 16:23:20.723666  1960 net.cpp:122] Setting up conv3_1
I1024 16:23:20.723666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.723666  1960 net.cpp:137] Memory required for data: 585492400
I1024 16:23:20.723666  1960 layer_factory.cpp:58] Creating layer bn3_1
I1024 16:23:20.723666  1960 net.cpp:84] Creating Layer bn3_1
I1024 16:23:20.723666  1960 net.cpp:406] bn3_1 <- conv3_1
I1024 16:23:20.723666  1960 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1024 16:23:20.724666  1960 net.cpp:122] Setting up bn3_1
I1024 16:23:20.724666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.724666  1960 net.cpp:137] Memory required for data: 593018800
I1024 16:23:20.724666  1960 layer_factory.cpp:58] Creating layer scale3_1
I1024 16:23:20.724666  1960 net.cpp:84] Creating Layer scale3_1
I1024 16:23:20.724666  1960 net.cpp:406] scale3_1 <- conv3_1
I1024 16:23:20.724666  1960 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1024 16:23:20.724666  1960 layer_factory.cpp:58] Creating layer scale3_1
I1024 16:23:20.724666  1960 net.cpp:122] Setting up scale3_1
I1024 16:23:20.724666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.724666  1960 net.cpp:137] Memory required for data: 600545200
I1024 16:23:20.724666  1960 layer_factory.cpp:58] Creating layer relu3_1
I1024 16:23:20.724666  1960 net.cpp:84] Creating Layer relu3_1
I1024 16:23:20.724666  1960 net.cpp:406] relu3_1 <- conv3_1
I1024 16:23:20.724666  1960 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1024 16:23:20.724666  1960 net.cpp:122] Setting up relu3_1
I1024 16:23:20.724666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.724666  1960 net.cpp:137] Memory required for data: 608071600
I1024 16:23:20.724666  1960 layer_factory.cpp:58] Creating layer drop6_1
I1024 16:23:20.724666  1960 net.cpp:84] Creating Layer drop6_1
I1024 16:23:20.724666  1960 net.cpp:406] drop6_1 <- conv3_1
I1024 16:23:20.724666  1960 net.cpp:367] drop6_1 -> conv3_1 (in-place)
I1024 16:23:20.724666  1960 net.cpp:122] Setting up drop6_1
I1024 16:23:20.724666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.724666  1960 net.cpp:137] Memory required for data: 615598000
I1024 16:23:20.724666  1960 layer_factory.cpp:58] Creating layer conv4
I1024 16:23:20.724666  1960 net.cpp:84] Creating Layer conv4
I1024 16:23:20.724666  1960 net.cpp:406] conv4 <- conv3_1
I1024 16:23:20.724666  1960 net.cpp:380] conv4 -> conv4
I1024 16:23:20.726667  1960 net.cpp:122] Setting up conv4
I1024 16:23:20.726667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.726667  1960 net.cpp:137] Memory required for data: 623124400
I1024 16:23:20.726667  1960 layer_factory.cpp:58] Creating layer bn4
I1024 16:23:20.726667  1960 net.cpp:84] Creating Layer bn4
I1024 16:23:20.726667  1960 net.cpp:406] bn4 <- conv4
I1024 16:23:20.726667  1960 net.cpp:367] bn4 -> conv4 (in-place)
I1024 16:23:20.726667  1960 net.cpp:122] Setting up bn4
I1024 16:23:20.726667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.726667  1960 net.cpp:137] Memory required for data: 630650800
I1024 16:23:20.726667  1960 layer_factory.cpp:58] Creating layer scale4
I1024 16:23:20.726667  1960 net.cpp:84] Creating Layer scale4
I1024 16:23:20.726667  1960 net.cpp:406] scale4 <- conv4
I1024 16:23:20.726667  1960 net.cpp:367] scale4 -> conv4 (in-place)
I1024 16:23:20.726667  1960 layer_factory.cpp:58] Creating layer scale4
I1024 16:23:20.727666  1960 net.cpp:122] Setting up scale4
I1024 16:23:20.727666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.727666  1960 net.cpp:137] Memory required for data: 638177200
I1024 16:23:20.727666  1960 layer_factory.cpp:58] Creating layer relu4
I1024 16:23:20.727666  1960 net.cpp:84] Creating Layer relu4
I1024 16:23:20.727666  1960 net.cpp:406] relu4 <- conv4
I1024 16:23:20.727666  1960 net.cpp:367] relu4 -> conv4 (in-place)
I1024 16:23:20.727666  1960 net.cpp:122] Setting up relu4
I1024 16:23:20.727666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.727666  1960 net.cpp:137] Memory required for data: 645703600
I1024 16:23:20.727666  1960 layer_factory.cpp:58] Creating layer drop7
I1024 16:23:20.727666  1960 net.cpp:84] Creating Layer drop7
I1024 16:23:20.727666  1960 net.cpp:406] drop7 <- conv4
I1024 16:23:20.727666  1960 net.cpp:367] drop7 -> conv4 (in-place)
I1024 16:23:20.727666  1960 net.cpp:122] Setting up drop7
I1024 16:23:20.727666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.727666  1960 net.cpp:137] Memory required for data: 653230000
I1024 16:23:20.727666  1960 layer_factory.cpp:58] Creating layer conv4_1
I1024 16:23:20.727666  1960 net.cpp:84] Creating Layer conv4_1
I1024 16:23:20.727666  1960 net.cpp:406] conv4_1 <- conv4
I1024 16:23:20.727666  1960 net.cpp:380] conv4_1 -> conv4_1
I1024 16:23:20.730666  1960 net.cpp:122] Setting up conv4_1
I1024 16:23:20.730666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.730666  1960 net.cpp:137] Memory required for data: 660756400
I1024 16:23:20.730666  1960 layer_factory.cpp:58] Creating layer bn4_1
I1024 16:23:20.730666  1960 net.cpp:84] Creating Layer bn4_1
I1024 16:23:20.730666  1960 net.cpp:406] bn4_1 <- conv4_1
I1024 16:23:20.730666  1960 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1024 16:23:20.730666  1960 net.cpp:122] Setting up bn4_1
I1024 16:23:20.730666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.730666  1960 net.cpp:137] Memory required for data: 668282800
I1024 16:23:20.730666  1960 layer_factory.cpp:58] Creating layer scale4_1
I1024 16:23:20.730666  1960 net.cpp:84] Creating Layer scale4_1
I1024 16:23:20.730666  1960 net.cpp:406] scale4_1 <- conv4_1
I1024 16:23:20.730666  1960 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1024 16:23:20.730666  1960 layer_factory.cpp:58] Creating layer scale4_1
I1024 16:23:20.730666  1960 net.cpp:122] Setting up scale4_1
I1024 16:23:20.730666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.730666  1960 net.cpp:137] Memory required for data: 675809200
I1024 16:23:20.730666  1960 layer_factory.cpp:58] Creating layer relu4_1
I1024 16:23:20.730666  1960 net.cpp:84] Creating Layer relu4_1
I1024 16:23:20.730666  1960 net.cpp:406] relu4_1 <- conv4_1
I1024 16:23:20.730666  1960 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1024 16:23:20.730666  1960 net.cpp:122] Setting up relu4_1
I1024 16:23:20.730666  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.730666  1960 net.cpp:137] Memory required for data: 683335600
I1024 16:23:20.730666  1960 layer_factory.cpp:58] Creating layer drop8
I1024 16:23:20.730666  1960 net.cpp:84] Creating Layer drop8
I1024 16:23:20.730666  1960 net.cpp:406] drop8 <- conv4_1
I1024 16:23:20.730666  1960 net.cpp:367] drop8 -> conv4_1 (in-place)
I1024 16:23:20.731667  1960 net.cpp:122] Setting up drop8
I1024 16:23:20.731667  1960 net.cpp:129] Top shape: 100 96 14 14 (1881600)
I1024 16:23:20.731667  1960 net.cpp:137] Memory required for data: 690862000
I1024 16:23:20.731667  1960 layer_factory.cpp:58] Creating layer conv4_2
I1024 16:23:20.731667  1960 net.cpp:84] Creating Layer conv4_2
I1024 16:23:20.731667  1960 net.cpp:406] conv4_2 <- conv4_1
I1024 16:23:20.731667  1960 net.cpp:380] conv4_2 -> conv4_2
I1024 16:23:20.733665  1960 net.cpp:122] Setting up conv4_2
I1024 16:23:20.733665  1960 net.cpp:129] Top shape: 100 144 14 14 (2822400)
I1024 16:23:20.733665  1960 net.cpp:137] Memory required for data: 702151600
I1024 16:23:20.733665  1960 layer_factory.cpp:58] Creating layer bn4_2
I1024 16:23:20.733665  1960 net.cpp:84] Creating Layer bn4_2
I1024 16:23:20.733665  1960 net.cpp:406] bn4_2 <- conv4_2
I1024 16:23:20.733665  1960 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1024 16:23:20.733665  1960 net.cpp:122] Setting up bn4_2
I1024 16:23:20.733665  1960 net.cpp:129] Top shape: 100 144 14 14 (2822400)
I1024 16:23:20.733665  1960 net.cpp:137] Memory required for data: 713441200
I1024 16:23:20.733665  1960 layer_factory.cpp:58] Creating layer scale4_2
I1024 16:23:20.733665  1960 net.cpp:84] Creating Layer scale4_2
I1024 16:23:20.733665  1960 net.cpp:406] scale4_2 <- conv4_2
I1024 16:23:20.733665  1960 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1024 16:23:20.733665  1960 layer_factory.cpp:58] Creating layer scale4_2
I1024 16:23:20.733665  1960 net.cpp:122] Setting up scale4_2
I1024 16:23:20.733665  1960 net.cpp:129] Top shape: 100 144 14 14 (2822400)
I1024 16:23:20.733665  1960 net.cpp:137] Memory required for data: 724730800
I1024 16:23:20.733665  1960 layer_factory.cpp:58] Creating layer relu4_2
I1024 16:23:20.733665  1960 net.cpp:84] Creating Layer relu4_2
I1024 16:23:20.733665  1960 net.cpp:406] relu4_2 <- conv4_2
I1024 16:23:20.733665  1960 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1024 16:23:20.733665  1960 net.cpp:122] Setting up relu4_2
I1024 16:23:20.733665  1960 net.cpp:129] Top shape: 100 144 14 14 (2822400)
I1024 16:23:20.733665  1960 net.cpp:137] Memory required for data: 736020400
I1024 16:23:20.733665  1960 layer_factory.cpp:58] Creating layer pool4_2
I1024 16:23:20.733665  1960 net.cpp:84] Creating Layer pool4_2
I1024 16:23:20.733665  1960 net.cpp:406] pool4_2 <- conv4_2
I1024 16:23:20.733665  1960 net.cpp:380] pool4_2 -> pool4_2
I1024 16:23:20.733665  1960 net.cpp:122] Setting up pool4_2
I1024 16:23:20.733665  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.733665  1960 net.cpp:137] Memory required for data: 738842800
I1024 16:23:20.733665  1960 layer_factory.cpp:58] Creating layer drop9
I1024 16:23:20.733665  1960 net.cpp:84] Creating Layer drop9
I1024 16:23:20.733665  1960 net.cpp:406] drop9 <- pool4_2
I1024 16:23:20.733665  1960 net.cpp:367] drop9 -> pool4_2 (in-place)
I1024 16:23:20.733665  1960 net.cpp:122] Setting up drop9
I1024 16:23:20.734666  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.734666  1960 net.cpp:137] Memory required for data: 741665200
I1024 16:23:20.734666  1960 layer_factory.cpp:58] Creating layer conv4_0
I1024 16:23:20.734666  1960 net.cpp:84] Creating Layer conv4_0
I1024 16:23:20.734666  1960 net.cpp:406] conv4_0 <- pool4_2
I1024 16:23:20.734666  1960 net.cpp:380] conv4_0 -> conv4_0
I1024 16:23:20.736665  1960 net.cpp:122] Setting up conv4_0
I1024 16:23:20.736665  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.736665  1960 net.cpp:137] Memory required for data: 744487600
I1024 16:23:20.736665  1960 layer_factory.cpp:58] Creating layer bn4_0
I1024 16:23:20.736665  1960 net.cpp:84] Creating Layer bn4_0
I1024 16:23:20.736665  1960 net.cpp:406] bn4_0 <- conv4_0
I1024 16:23:20.736665  1960 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1024 16:23:20.737665  1960 net.cpp:122] Setting up bn4_0
I1024 16:23:20.737665  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.737665  1960 net.cpp:137] Memory required for data: 747310000
I1024 16:23:20.737665  1960 layer_factory.cpp:58] Creating layer scale4_0
I1024 16:23:20.737665  1960 net.cpp:84] Creating Layer scale4_0
I1024 16:23:20.737665  1960 net.cpp:406] scale4_0 <- conv4_0
I1024 16:23:20.737665  1960 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1024 16:23:20.737665  1960 layer_factory.cpp:58] Creating layer scale4_0
I1024 16:23:20.737665  1960 net.cpp:122] Setting up scale4_0
I1024 16:23:20.737665  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.737665  1960 net.cpp:137] Memory required for data: 750132400
I1024 16:23:20.737665  1960 layer_factory.cpp:58] Creating layer relu4_0
I1024 16:23:20.737665  1960 net.cpp:84] Creating Layer relu4_0
I1024 16:23:20.737665  1960 net.cpp:406] relu4_0 <- conv4_0
I1024 16:23:20.737665  1960 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1024 16:23:20.737665  1960 net.cpp:122] Setting up relu4_0
I1024 16:23:20.737665  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.737665  1960 net.cpp:137] Memory required for data: 752954800
I1024 16:23:20.737665  1960 layer_factory.cpp:58] Creating layer drop10
I1024 16:23:20.737665  1960 net.cpp:84] Creating Layer drop10
I1024 16:23:20.737665  1960 net.cpp:406] drop10 <- conv4_0
I1024 16:23:20.737665  1960 net.cpp:367] drop10 -> conv4_0 (in-place)
I1024 16:23:20.737665  1960 net.cpp:122] Setting up drop10
I1024 16:23:20.737665  1960 net.cpp:129] Top shape: 100 144 7 7 (705600)
I1024 16:23:20.737665  1960 net.cpp:137] Memory required for data: 755777200
I1024 16:23:20.738665  1960 layer_factory.cpp:58] Creating layer conv11
I1024 16:23:20.738665  1960 net.cpp:84] Creating Layer conv11
I1024 16:23:20.738665  1960 net.cpp:406] conv11 <- conv4_0
I1024 16:23:20.738665  1960 net.cpp:380] conv11 -> conv11
I1024 16:23:20.740666  1960 net.cpp:122] Setting up conv11
I1024 16:23:20.740666  1960 net.cpp:129] Top shape: 100 178 7 7 (872200)
I1024 16:23:20.740666  1960 net.cpp:137] Memory required for data: 759266000
I1024 16:23:20.740666  1960 layer_factory.cpp:58] Creating layer bn_conv11
I1024 16:23:20.740666  1960 net.cpp:84] Creating Layer bn_conv11
I1024 16:23:20.740666  1960 net.cpp:406] bn_conv11 <- conv11
I1024 16:23:20.740666  1960 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1024 16:23:20.740666  1960 net.cpp:122] Setting up bn_conv11
I1024 16:23:20.740666  1960 net.cpp:129] Top shape: 100 178 7 7 (872200)
I1024 16:23:20.740666  1960 net.cpp:137] Memory required for data: 762754800
I1024 16:23:20.740666  1960 layer_factory.cpp:58] Creating layer scale_conv11
I1024 16:23:20.740666  1960 net.cpp:84] Creating Layer scale_conv11
I1024 16:23:20.740666  1960 net.cpp:406] scale_conv11 <- conv11
I1024 16:23:20.740666  1960 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1024 16:23:20.740666  1960 layer_factory.cpp:58] Creating layer scale_conv11
I1024 16:23:20.740666  1960 net.cpp:122] Setting up scale_conv11
I1024 16:23:20.740666  1960 net.cpp:129] Top shape: 100 178 7 7 (872200)
I1024 16:23:20.740666  1960 net.cpp:137] Memory required for data: 766243600
I1024 16:23:20.740666  1960 layer_factory.cpp:58] Creating layer relu_conv11
I1024 16:23:20.740666  1960 net.cpp:84] Creating Layer relu_conv11
I1024 16:23:20.740666  1960 net.cpp:406] relu_conv11 <- conv11
I1024 16:23:20.740666  1960 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1024 16:23:20.741667  1960 net.cpp:122] Setting up relu_conv11
I1024 16:23:20.741667  1960 net.cpp:129] Top shape: 100 178 7 7 (872200)
I1024 16:23:20.741667  1960 net.cpp:137] Memory required for data: 769732400
I1024 16:23:20.741667  1960 layer_factory.cpp:58] Creating layer drop11
I1024 16:23:20.741667  1960 net.cpp:84] Creating Layer drop11
I1024 16:23:20.741667  1960 net.cpp:406] drop11 <- conv11
I1024 16:23:20.741667  1960 net.cpp:367] drop11 -> conv11 (in-place)
I1024 16:23:20.741667  1960 net.cpp:122] Setting up drop11
I1024 16:23:20.741667  1960 net.cpp:129] Top shape: 100 178 7 7 (872200)
I1024 16:23:20.741667  1960 net.cpp:137] Memory required for data: 773221200
I1024 16:23:20.741667  1960 layer_factory.cpp:58] Creating layer conv12
I1024 16:23:20.741667  1960 net.cpp:84] Creating Layer conv12
I1024 16:23:20.741667  1960 net.cpp:406] conv12 <- conv11
I1024 16:23:20.741667  1960 net.cpp:380] conv12 -> conv12
I1024 16:23:20.745666  1960 net.cpp:122] Setting up conv12
I1024 16:23:20.745666  1960 net.cpp:129] Top shape: 100 216 7 7 (1058400)
I1024 16:23:20.745666  1960 net.cpp:137] Memory required for data: 777454800
I1024 16:23:20.745666  1960 layer_factory.cpp:58] Creating layer bn_conv12
I1024 16:23:20.745666  1960 net.cpp:84] Creating Layer bn_conv12
I1024 16:23:20.745666  1960 net.cpp:406] bn_conv12 <- conv12
I1024 16:23:20.745666  1960 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1024 16:23:20.745666  1960 net.cpp:122] Setting up bn_conv12
I1024 16:23:20.745666  1960 net.cpp:129] Top shape: 100 216 7 7 (1058400)
I1024 16:23:20.745666  1960 net.cpp:137] Memory required for data: 781688400
I1024 16:23:20.745666  1960 layer_factory.cpp:58] Creating layer scale_conv12
I1024 16:23:20.745666  1960 net.cpp:84] Creating Layer scale_conv12
I1024 16:23:20.745666  1960 net.cpp:406] scale_conv12 <- conv12
I1024 16:23:20.745666  1960 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1024 16:23:20.745666  1960 layer_factory.cpp:58] Creating layer scale_conv12
I1024 16:23:20.745666  1960 net.cpp:122] Setting up scale_conv12
I1024 16:23:20.745666  1960 net.cpp:129] Top shape: 100 216 7 7 (1058400)
I1024 16:23:20.745666  1960 net.cpp:137] Memory required for data: 785922000
I1024 16:23:20.745666  1960 layer_factory.cpp:58] Creating layer relu_conv12
I1024 16:23:20.745666  1960 net.cpp:84] Creating Layer relu_conv12
I1024 16:23:20.745666  1960 net.cpp:406] relu_conv12 <- conv12
I1024 16:23:20.745666  1960 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1024 16:23:20.746666  1960 net.cpp:122] Setting up relu_conv12
I1024 16:23:20.746666  1960 net.cpp:129] Top shape: 100 216 7 7 (1058400)
I1024 16:23:20.746666  1960 net.cpp:137] Memory required for data: 790155600
I1024 16:23:20.746666  1960 layer_factory.cpp:58] Creating layer poolcp6
I1024 16:23:20.746666  1960 net.cpp:84] Creating Layer poolcp6
I1024 16:23:20.746666  1960 net.cpp:406] poolcp6 <- conv12
I1024 16:23:20.746666  1960 net.cpp:380] poolcp6 -> poolcp6
I1024 16:23:20.746666  1960 net.cpp:122] Setting up poolcp6
I1024 16:23:20.746666  1960 net.cpp:129] Top shape: 100 216 1 1 (21600)
I1024 16:23:20.746666  1960 net.cpp:137] Memory required for data: 790242000
I1024 16:23:20.746666  1960 layer_factory.cpp:58] Creating layer drop12
I1024 16:23:20.746666  1960 net.cpp:84] Creating Layer drop12
I1024 16:23:20.746666  1960 net.cpp:406] drop12 <- poolcp6
I1024 16:23:20.746666  1960 net.cpp:367] drop12 -> poolcp6 (in-place)
I1024 16:23:20.746666  1960 net.cpp:122] Setting up drop12
I1024 16:23:20.746666  1960 net.cpp:129] Top shape: 100 216 1 1 (21600)
I1024 16:23:20.746666  1960 net.cpp:137] Memory required for data: 790328400
I1024 16:23:20.746666  1960 layer_factory.cpp:58] Creating layer ip1
I1024 16:23:20.746666  1960 net.cpp:84] Creating Layer ip1
I1024 16:23:20.746666  1960 net.cpp:406] ip1 <- poolcp6
I1024 16:23:20.746666  1960 net.cpp:380] ip1 -> ip1
I1024 16:23:20.746666  1960 net.cpp:122] Setting up ip1
I1024 16:23:20.746666  1960 net.cpp:129] Top shape: 100 10 (1000)
I1024 16:23:20.746666  1960 net.cpp:137] Memory required for data: 790332400
I1024 16:23:20.746666  1960 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1024 16:23:20.746666  1960 net.cpp:84] Creating Layer ip1_ip1_0_split
I1024 16:23:20.746666  1960 net.cpp:406] ip1_ip1_0_split <- ip1
I1024 16:23:20.746666  1960 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1024 16:23:20.746666  1960 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1024 16:23:20.746666  1960 net.cpp:122] Setting up ip1_ip1_0_split
I1024 16:23:20.746666  1960 net.cpp:129] Top shape: 100 10 (1000)
I1024 16:23:20.746666  1960 net.cpp:129] Top shape: 100 10 (1000)
I1024 16:23:20.746666  1960 net.cpp:137] Memory required for data: 790340400
I1024 16:23:20.746666  1960 layer_factory.cpp:58] Creating layer accuracy
I1024 16:23:20.746666  1960 net.cpp:84] Creating Layer accuracy
I1024 16:23:20.746666  1960 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1024 16:23:20.746666  1960 net.cpp:406] accuracy <- label_mnist_1_split_0
I1024 16:23:20.746666  1960 net.cpp:380] accuracy -> accuracy
I1024 16:23:20.746666  1960 net.cpp:122] Setting up accuracy
I1024 16:23:20.746666  1960 net.cpp:129] Top shape: (1)
I1024 16:23:20.746666  1960 net.cpp:137] Memory required for data: 790340404
I1024 16:23:20.746666  1960 layer_factory.cpp:58] Creating layer loss
I1024 16:23:20.746666  1960 net.cpp:84] Creating Layer loss
I1024 16:23:20.746666  1960 net.cpp:406] loss <- ip1_ip1_0_split_1
I1024 16:23:20.746666  1960 net.cpp:406] loss <- label_mnist_1_split_1
I1024 16:23:20.746666  1960 net.cpp:380] loss -> loss
I1024 16:23:20.746666  1960 layer_factory.cpp:58] Creating layer loss
I1024 16:23:20.747670  1960 net.cpp:122] Setting up loss
I1024 16:23:20.747670  1960 net.cpp:129] Top shape: (1)
I1024 16:23:20.747670  1960 net.cpp:132]     with loss weight 1
I1024 16:23:20.747670  1960 net.cpp:137] Memory required for data: 790340408
I1024 16:23:20.747670  1960 net.cpp:198] loss needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:200] accuracy does not need backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] ip1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] drop12 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] poolcp6 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] relu_conv12 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] scale_conv12 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] bn_conv12 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] conv12 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] drop11 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] relu_conv11 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] scale_conv11 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] bn_conv11 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] conv11 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] drop10 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] relu4_0 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] scale4_0 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] bn4_0 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] conv4_0 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] drop9 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] pool4_2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] relu4_2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] scale4_2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] bn4_2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] conv4_2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] drop8 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] relu4_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] scale4_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] bn4_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] conv4_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] drop7 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] relu4 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] scale4 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] bn4 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] conv4 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] drop6_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] relu3_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] scale3_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] bn3_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] conv3_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] drop6 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] relu3 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] scale3 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] bn3 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] conv3 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] drop5 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] pool2_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] relu2_2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] scale2_2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] bn2_2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] conv2_2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] drop4 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] relu2_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] scale2_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] bn2_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] conv2_1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] drop3 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] relu2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] scale2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] bn2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] conv2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] drop2 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] relu1_0 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] scale1_0 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] bn1_0 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] conv1_0 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] drop1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] relu1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] scale1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] bn1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:198] conv1 needs backward computation.
I1024 16:23:20.747670  1960 net.cpp:200] label_mnist_1_split does not need backward computation.
I1024 16:23:20.747670  1960 net.cpp:200] mnist does not need backward computation.
I1024 16:23:20.747670  1960 net.cpp:242] This network produces output accuracy
I1024 16:23:20.747670  1960 net.cpp:242] This network produces output loss
I1024 16:23:20.747670  1960 net.cpp:255] Network initialization done.
I1024 16:23:20.747670  1960 solver.cpp:56] Solver scaffolding done.
I1024 16:23:20.751665  1960 caffe.cpp:249] Starting Optimization
I1024 16:23:20.751665  1960 solver.cpp:272] Solving MNIST_SimpleNet_GP_13L_drpall_5Mil_66_maxdrp
I1024 16:23:20.751665  1960 solver.cpp:273] Learning Rate Policy: multistep
I1024 16:23:20.755667  1960 solver.cpp:330] Iteration 0, Testing net (#0)
I1024 16:23:20.757666  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:23:22.712666  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:23:22.790665  1960 solver.cpp:397]     Test net output #0: accuracy = 0.025
I1024 16:23:22.790665  1960 solver.cpp:397]     Test net output #1: loss = 85.1531 (* 1 = 85.1531 loss)
I1024 16:23:22.939666  1960 solver.cpp:218] Iteration 0 (0 iter/s, 2.18639s/100 iters), loss = 4.10802
I1024 16:23:22.939666  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.11
I1024 16:23:22.939666  1960 solver.cpp:237]     Train net output #1: loss = 4.10802 (* 1 = 4.10802 loss)
I1024 16:23:22.939666  1960 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1024 16:23:30.865664  1960 solver.cpp:218] Iteration 100 (12.6161 iter/s, 7.92637s/100 iters), loss = 1.67653
I1024 16:23:30.865664  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.44
I1024 16:23:30.865664  1960 solver.cpp:237]     Train net output #1: loss = 1.67653 (* 1 = 1.67653 loss)
I1024 16:23:30.865664  1960 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1024 16:23:38.788664  1960 solver.cpp:218] Iteration 200 (12.6234 iter/s, 7.92183s/100 iters), loss = 0.356374
I1024 16:23:38.788664  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1024 16:23:38.788664  1960 solver.cpp:237]     Train net output #1: loss = 0.356374 (* 1 = 0.356374 loss)
I1024 16:23:38.788664  1960 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1024 16:23:46.710664  1960 solver.cpp:218] Iteration 300 (12.6234 iter/s, 7.92182s/100 iters), loss = 0.612706
I1024 16:23:46.710664  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1024 16:23:46.710664  1960 solver.cpp:237]     Train net output #1: loss = 0.612706 (* 1 = 0.612706 loss)
I1024 16:23:46.710664  1960 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1024 16:23:54.633664  1960 solver.cpp:218] Iteration 400 (12.622 iter/s, 7.9227s/100 iters), loss = 0.231387
I1024 16:23:54.633664  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1024 16:23:54.633664  1960 solver.cpp:237]     Train net output #1: loss = 0.231387 (* 1 = 0.231387 loss)
I1024 16:23:54.633664  1960 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1024 16:24:02.559664  1960 solver.cpp:218] Iteration 500 (12.6178 iter/s, 7.92533s/100 iters), loss = 0.0973177
I1024 16:24:02.559664  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1024 16:24:02.559664  1960 solver.cpp:237]     Train net output #1: loss = 0.0973178 (* 1 = 0.0973178 loss)
I1024 16:24:02.559664  1960 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1024 16:24:10.122666 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:24:10.438665  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_600.caffemodel
I1024 16:24:10.482666  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_600.solverstate
I1024 16:24:10.500665  1960 solver.cpp:330] Iteration 600, Testing net (#0)
I1024 16:24:10.500665  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:24:12.402667  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:24:12.481665  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9757
I1024 16:24:12.481665  1960 solver.cpp:397]     Test net output #1: loss = 0.0767516 (* 1 = 0.0767516 loss)
I1024 16:24:12.558676  1960 solver.cpp:218] Iteration 600 (10.0011 iter/s, 9.99895s/100 iters), loss = 0.0921648
I1024 16:24:12.558676  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1024 16:24:12.558676  1960 solver.cpp:237]     Train net output #1: loss = 0.0921649 (* 1 = 0.0921649 loss)
I1024 16:24:12.558676  1960 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1024 16:24:20.478669  1960 solver.cpp:218] Iteration 700 (12.627 iter/s, 7.91953s/100 iters), loss = 0.1956
I1024 16:24:20.478669  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1024 16:24:20.478669  1960 solver.cpp:237]     Train net output #1: loss = 0.195601 (* 1 = 0.195601 loss)
I1024 16:24:20.478669  1960 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1024 16:24:28.407665  1960 solver.cpp:218] Iteration 800 (12.6134 iter/s, 7.92805s/100 iters), loss = 0.107474
I1024 16:24:28.407665  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1024 16:24:28.407665  1960 solver.cpp:237]     Train net output #1: loss = 0.107474 (* 1 = 0.107474 loss)
I1024 16:24:28.407665  1960 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1024 16:24:36.330720  1960 solver.cpp:218] Iteration 900 (12.622 iter/s, 7.9227s/100 iters), loss = 0.119105
I1024 16:24:36.330720  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1024 16:24:36.330720  1960 solver.cpp:237]     Train net output #1: loss = 0.119105 (* 1 = 0.119105 loss)
I1024 16:24:36.330720  1960 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1024 16:24:44.260720  1960 solver.cpp:218] Iteration 1000 (12.6114 iter/s, 7.92933s/100 iters), loss = 0.025747
I1024 16:24:44.260720  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:24:44.260720  1960 solver.cpp:237]     Train net output #1: loss = 0.0257472 (* 1 = 0.0257472 loss)
I1024 16:24:44.260720  1960 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1024 16:24:52.181718  1960 solver.cpp:218] Iteration 1100 (12.6258 iter/s, 7.92029s/100 iters), loss = 0.0426857
I1024 16:24:52.181718  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:24:52.181718  1960 solver.cpp:237]     Train net output #1: loss = 0.0426858 (* 1 = 0.0426858 loss)
I1024 16:24:52.181718  1960 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1024 16:24:59.707720 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:25:00.023721  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_1200.caffemodel
I1024 16:25:00.058722  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_1200.solverstate
I1024 16:25:00.075733  1960 solver.cpp:330] Iteration 1200, Testing net (#0)
I1024 16:25:00.075733  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:25:01.976719  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:25:02.054720  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9894
I1024 16:25:02.054720  1960 solver.cpp:397]     Test net output #1: loss = 0.0327606 (* 1 = 0.0327606 loss)
I1024 16:25:02.130718  1960 solver.cpp:218] Iteration 1200 (10.0512 iter/s, 9.94907s/100 iters), loss = 0.0327529
I1024 16:25:02.130718  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1024 16:25:02.130718  1960 solver.cpp:237]     Train net output #1: loss = 0.032753 (* 1 = 0.032753 loss)
I1024 16:25:02.130718  1960 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1024 16:25:10.057720  1960 solver.cpp:218] Iteration 1300 (12.6155 iter/s, 7.92673s/100 iters), loss = 0.0939754
I1024 16:25:10.058720  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1024 16:25:10.058720  1960 solver.cpp:237]     Train net output #1: loss = 0.0939756 (* 1 = 0.0939756 loss)
I1024 16:25:10.058720  1960 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1024 16:25:17.984719  1960 solver.cpp:218] Iteration 1400 (12.6166 iter/s, 7.92608s/100 iters), loss = 0.0639446
I1024 16:25:17.984719  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1024 16:25:17.984719  1960 solver.cpp:237]     Train net output #1: loss = 0.0639447 (* 1 = 0.0639447 loss)
I1024 16:25:17.984719  1960 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1024 16:25:25.907719  1960 solver.cpp:218] Iteration 1500 (12.6222 iter/s, 7.92253s/100 iters), loss = 0.0590384
I1024 16:25:25.907719  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1024 16:25:25.907719  1960 solver.cpp:237]     Train net output #1: loss = 0.0590386 (* 1 = 0.0590386 loss)
I1024 16:25:25.907719  1960 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1024 16:25:33.830719  1960 solver.cpp:218] Iteration 1600 (12.6227 iter/s, 7.92221s/100 iters), loss = 0.0232487
I1024 16:25:33.830719  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:25:33.830719  1960 solver.cpp:237]     Train net output #1: loss = 0.0232489 (* 1 = 0.0232489 loss)
I1024 16:25:33.830719  1960 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1024 16:25:41.755719  1960 solver.cpp:218] Iteration 1700 (12.6189 iter/s, 7.92462s/100 iters), loss = 0.0263617
I1024 16:25:41.755719  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:25:41.755719  1960 solver.cpp:237]     Train net output #1: loss = 0.0263619 (* 1 = 0.0263619 loss)
I1024 16:25:41.755719  1960 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1024 16:25:49.287719 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:25:49.602722  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_1800.caffemodel
I1024 16:25:49.641739  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_1800.solverstate
I1024 16:25:49.658722  1960 solver.cpp:330] Iteration 1800, Testing net (#0)
I1024 16:25:49.658722  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:25:51.561720  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:25:51.639720  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9832
I1024 16:25:51.639720  1960 solver.cpp:397]     Test net output #1: loss = 0.0547165 (* 1 = 0.0547165 loss)
I1024 16:25:51.716719  1960 solver.cpp:218] Iteration 1800 (10.0401 iter/s, 9.96004s/100 iters), loss = 0.117216
I1024 16:25:51.716719  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1024 16:25:51.716719  1960 solver.cpp:237]     Train net output #1: loss = 0.117216 (* 1 = 0.117216 loss)
I1024 16:25:51.716719  1960 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1024 16:25:59.642719  1960 solver.cpp:218] Iteration 1900 (12.6169 iter/s, 7.92589s/100 iters), loss = 0.0866105
I1024 16:25:59.642719  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1024 16:25:59.642719  1960 solver.cpp:237]     Train net output #1: loss = 0.0866106 (* 1 = 0.0866106 loss)
I1024 16:25:59.642719  1960 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1024 16:26:07.567719  1960 solver.cpp:218] Iteration 2000 (12.6192 iter/s, 7.92442s/100 iters), loss = 0.0833833
I1024 16:26:07.567719  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1024 16:26:07.567719  1960 solver.cpp:237]     Train net output #1: loss = 0.0833834 (* 1 = 0.0833834 loss)
I1024 16:26:07.567719  1960 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1024 16:26:15.491719  1960 solver.cpp:218] Iteration 2100 (12.6198 iter/s, 7.92406s/100 iters), loss = 0.0128599
I1024 16:26:15.491719  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:26:15.491719  1960 solver.cpp:237]     Train net output #1: loss = 0.0128601 (* 1 = 0.0128601 loss)
I1024 16:26:15.491719  1960 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1024 16:26:23.416719  1960 solver.cpp:218] Iteration 2200 (12.6196 iter/s, 7.92416s/100 iters), loss = 0.040829
I1024 16:26:23.416719  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1024 16:26:23.416719  1960 solver.cpp:237]     Train net output #1: loss = 0.0408292 (* 1 = 0.0408292 loss)
I1024 16:26:23.416719  1960 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1024 16:26:31.343717  1960 solver.cpp:218] Iteration 2300 (12.6159 iter/s, 7.9265s/100 iters), loss = 0.0122282
I1024 16:26:31.343717  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:26:31.343717  1960 solver.cpp:237]     Train net output #1: loss = 0.0122284 (* 1 = 0.0122284 loss)
I1024 16:26:31.343717  1960 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1024 16:26:38.872720 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:26:39.189718  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_2400.caffemodel
I1024 16:26:39.225723  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_2400.solverstate
I1024 16:26:39.243721  1960 solver.cpp:330] Iteration 2400, Testing net (#0)
I1024 16:26:39.243721  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:26:41.144721  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:26:41.222728  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9724
I1024 16:26:41.222728  1960 solver.cpp:397]     Test net output #1: loss = 0.0929214 (* 1 = 0.0929214 loss)
I1024 16:26:41.298718  1960 solver.cpp:218] Iteration 2400 (10.0454 iter/s, 9.95481s/100 iters), loss = 0.0238661
I1024 16:26:41.298718  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:26:41.298718  1960 solver.cpp:237]     Train net output #1: loss = 0.0238662 (* 1 = 0.0238662 loss)
I1024 16:26:41.298718  1960 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1024 16:26:49.224720  1960 solver.cpp:218] Iteration 2500 (12.618 iter/s, 7.92517s/100 iters), loss = 0.177166
I1024 16:26:49.224720  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1024 16:26:49.224720  1960 solver.cpp:237]     Train net output #1: loss = 0.177166 (* 1 = 0.177166 loss)
I1024 16:26:49.224720  1960 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1024 16:26:57.150719  1960 solver.cpp:218] Iteration 2600 (12.6169 iter/s, 7.92585s/100 iters), loss = 0.0722745
I1024 16:26:57.150719  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1024 16:26:57.150719  1960 solver.cpp:237]     Train net output #1: loss = 0.0722747 (* 1 = 0.0722747 loss)
I1024 16:26:57.150719  1960 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1024 16:27:05.072720  1960 solver.cpp:218] Iteration 2700 (12.6235 iter/s, 7.92173s/100 iters), loss = 0.0258624
I1024 16:27:05.073719  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:27:05.073719  1960 solver.cpp:237]     Train net output #1: loss = 0.0258626 (* 1 = 0.0258626 loss)
I1024 16:27:05.073719  1960 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1024 16:27:12.997720  1960 solver.cpp:218] Iteration 2800 (12.6191 iter/s, 7.92448s/100 iters), loss = 0.0300304
I1024 16:27:12.998719  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:27:12.998719  1960 solver.cpp:237]     Train net output #1: loss = 0.0300306 (* 1 = 0.0300306 loss)
I1024 16:27:12.998719  1960 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1024 16:27:20.921717  1960 solver.cpp:218] Iteration 2900 (12.6216 iter/s, 7.92293s/100 iters), loss = 0.08416
I1024 16:27:20.921717  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:27:20.921717  1960 solver.cpp:237]     Train net output #1: loss = 0.0841601 (* 1 = 0.0841601 loss)
I1024 16:27:20.921717  1960 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1024 16:27:28.475231 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:27:28.790231  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_3000.caffemodel
I1024 16:27:28.825232  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_3000.solverstate
I1024 16:27:28.841240  1960 solver.cpp:330] Iteration 3000, Testing net (#0)
I1024 16:27:28.841240  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:27:30.744254  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:27:30.823246  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9888
I1024 16:27:30.823246  1960 solver.cpp:397]     Test net output #1: loss = 0.0395993 (* 1 = 0.0395993 loss)
I1024 16:27:30.900249  1960 solver.cpp:218] Iteration 3000 (10.0224 iter/s, 9.9776s/100 iters), loss = 0.0556905
I1024 16:27:30.900249  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1024 16:27:30.900249  1960 solver.cpp:237]     Train net output #1: loss = 0.0556906 (* 1 = 0.0556906 loss)
I1024 16:27:30.900249  1960 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1024 16:27:38.832244  1960 solver.cpp:218] Iteration 3100 (12.6074 iter/s, 7.93188s/100 iters), loss = 0.0541733
I1024 16:27:38.832244  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1024 16:27:38.832244  1960 solver.cpp:237]     Train net output #1: loss = 0.0541733 (* 1 = 0.0541733 loss)
I1024 16:27:38.832244  1960 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1024 16:27:46.802245  1960 solver.cpp:218] Iteration 3200 (12.5485 iter/s, 7.9691s/100 iters), loss = 0.0695694
I1024 16:27:46.802245  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1024 16:27:46.802245  1960 solver.cpp:237]     Train net output #1: loss = 0.0695695 (* 1 = 0.0695695 loss)
I1024 16:27:46.802245  1960 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1024 16:27:54.737243  1960 solver.cpp:218] Iteration 3300 (12.6031 iter/s, 7.93453s/100 iters), loss = 0.0157202
I1024 16:27:54.737243  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:27:54.737243  1960 solver.cpp:237]     Train net output #1: loss = 0.0157202 (* 1 = 0.0157202 loss)
I1024 16:27:54.737243  1960 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1024 16:28:02.666244  1960 solver.cpp:218] Iteration 3400 (12.612 iter/s, 7.92895s/100 iters), loss = 0.0242585
I1024 16:28:02.666244  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:28:02.666244  1960 solver.cpp:237]     Train net output #1: loss = 0.0242586 (* 1 = 0.0242586 loss)
I1024 16:28:02.666244  1960 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1024 16:28:10.596245  1960 solver.cpp:218] Iteration 3500 (12.6112 iter/s, 7.92948s/100 iters), loss = 0.0360468
I1024 16:28:10.596245  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:28:10.596245  1960 solver.cpp:237]     Train net output #1: loss = 0.0360469 (* 1 = 0.0360469 loss)
I1024 16:28:10.596245  1960 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1024 16:28:18.138247 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:28:18.454246  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_3600.caffemodel
I1024 16:28:18.490247  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_3600.solverstate
I1024 16:28:18.507249  1960 solver.cpp:330] Iteration 3600, Testing net (#0)
I1024 16:28:18.507249  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:28:20.407245  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:28:20.486244  1960 solver.cpp:397]     Test net output #0: accuracy = 0.8003
I1024 16:28:20.486244  1960 solver.cpp:397]     Test net output #1: loss = 0.630375 (* 1 = 0.630375 loss)
I1024 16:28:20.563244  1960 solver.cpp:218] Iteration 3600 (10.034 iter/s, 9.96609s/100 iters), loss = 0.0663004
I1024 16:28:20.563244  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:28:20.563244  1960 solver.cpp:237]     Train net output #1: loss = 0.0663005 (* 1 = 0.0663005 loss)
I1024 16:28:20.563244  1960 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1024 16:28:28.490244  1960 solver.cpp:218] Iteration 3700 (12.616 iter/s, 7.92645s/100 iters), loss = 0.0603268
I1024 16:28:28.490244  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1024 16:28:28.490244  1960 solver.cpp:237]     Train net output #1: loss = 0.0603269 (* 1 = 0.0603269 loss)
I1024 16:28:28.490244  1960 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1024 16:28:36.414242  1960 solver.cpp:218] Iteration 3800 (12.6198 iter/s, 7.92404s/100 iters), loss = 0.0663199
I1024 16:28:36.414242  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1024 16:28:36.414242  1960 solver.cpp:237]     Train net output #1: loss = 0.06632 (* 1 = 0.06632 loss)
I1024 16:28:36.414242  1960 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1024 16:28:44.339244  1960 solver.cpp:218] Iteration 3900 (12.6199 iter/s, 7.92401s/100 iters), loss = 0.511657
I1024 16:28:44.339244  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1024 16:28:44.339244  1960 solver.cpp:237]     Train net output #1: loss = 0.511657 (* 1 = 0.511657 loss)
I1024 16:28:44.339244  1960 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1024 16:28:52.265244  1960 solver.cpp:218] Iteration 4000 (12.6175 iter/s, 7.92549s/100 iters), loss = 0.0316243
I1024 16:28:52.265244  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:28:52.265244  1960 solver.cpp:237]     Train net output #1: loss = 0.0316244 (* 1 = 0.0316244 loss)
I1024 16:28:52.265244  1960 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1024 16:29:00.195302  1960 solver.cpp:218] Iteration 4100 (12.6111 iter/s, 7.92955s/100 iters), loss = 0.026008
I1024 16:29:00.195302  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:29:00.195302  1960 solver.cpp:237]     Train net output #1: loss = 0.0260081 (* 1 = 0.0260081 loss)
I1024 16:29:00.195302  1960 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1024 16:29:07.728394 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:29:08.044399  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_4200.caffemodel
I1024 16:29:08.077394  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_4200.solverstate
I1024 16:29:08.094393  1960 solver.cpp:330] Iteration 4200, Testing net (#0)
I1024 16:29:08.094393  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:29:09.996393  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:29:10.074394  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9874
I1024 16:29:10.074394  1960 solver.cpp:397]     Test net output #1: loss = 0.040721 (* 1 = 0.040721 loss)
I1024 16:29:10.150393  1960 solver.cpp:218] Iteration 4200 (10.0449 iter/s, 9.95532s/100 iters), loss = 0.0460612
I1024 16:29:10.150393  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1024 16:29:10.150393  1960 solver.cpp:237]     Train net output #1: loss = 0.0460613 (* 1 = 0.0460613 loss)
I1024 16:29:10.150393  1960 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1024 16:29:18.079392  1960 solver.cpp:218] Iteration 4300 (12.6139 iter/s, 7.92775s/100 iters), loss = 0.0739815
I1024 16:29:18.079392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1024 16:29:18.079392  1960 solver.cpp:237]     Train net output #1: loss = 0.0739816 (* 1 = 0.0739816 loss)
I1024 16:29:18.079392  1960 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1024 16:29:26.002394  1960 solver.cpp:218] Iteration 4400 (12.6215 iter/s, 7.92301s/100 iters), loss = 0.0347265
I1024 16:29:26.002394  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:29:26.002394  1960 solver.cpp:237]     Train net output #1: loss = 0.0347266 (* 1 = 0.0347266 loss)
I1024 16:29:26.002394  1960 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1024 16:29:33.925393  1960 solver.cpp:218] Iteration 4500 (12.6221 iter/s, 7.92259s/100 iters), loss = 0.00746776
I1024 16:29:33.925393  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:29:33.925393  1960 solver.cpp:237]     Train net output #1: loss = 0.00746784 (* 1 = 0.00746784 loss)
I1024 16:29:33.925393  1960 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1024 16:29:41.855392  1960 solver.cpp:218] Iteration 4600 (12.6109 iter/s, 7.92966s/100 iters), loss = 0.0119667
I1024 16:29:41.856393  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:29:41.856393  1960 solver.cpp:237]     Train net output #1: loss = 0.0119668 (* 1 = 0.0119668 loss)
I1024 16:29:41.856393  1960 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1024 16:29:49.777392  1960 solver.cpp:218] Iteration 4700 (12.6245 iter/s, 7.92113s/100 iters), loss = 0.0213409
I1024 16:29:49.777392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:29:49.777392  1960 solver.cpp:237]     Train net output #1: loss = 0.0213409 (* 1 = 0.0213409 loss)
I1024 16:29:49.777392  1960 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1024 16:29:57.307394 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:29:57.622392  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_4800.caffemodel
I1024 16:29:57.659394  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_4800.solverstate
I1024 16:29:57.675393  1960 solver.cpp:330] Iteration 4800, Testing net (#0)
I1024 16:29:57.675393  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:29:59.578395  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:29:59.656392  1960 solver.cpp:397]     Test net output #0: accuracy = 0.8299
I1024 16:29:59.657392  1960 solver.cpp:397]     Test net output #1: loss = 0.581722 (* 1 = 0.581722 loss)
I1024 16:29:59.733392  1960 solver.cpp:218] Iteration 4800 (10.0448 iter/s, 9.95538s/100 iters), loss = 0.0442222
I1024 16:29:59.733392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1024 16:29:59.733392  1960 solver.cpp:237]     Train net output #1: loss = 0.0442222 (* 1 = 0.0442222 loss)
I1024 16:29:59.733392  1960 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1024 16:30:07.688390  1960 solver.cpp:218] Iteration 4900 (12.5712 iter/s, 7.95469s/100 iters), loss = 0.0599755
I1024 16:30:07.689391  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:30:07.689391  1960 solver.cpp:237]     Train net output #1: loss = 0.0599755 (* 1 = 0.0599755 loss)
I1024 16:30:07.689391  1960 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1024 16:30:15.612392  1960 solver.cpp:218] Iteration 5000 (12.6208 iter/s, 7.9234s/100 iters), loss = 0.0828429
I1024 16:30:15.612392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1024 16:30:15.613394  1960 solver.cpp:237]     Train net output #1: loss = 0.0828429 (* 1 = 0.0828429 loss)
I1024 16:30:15.613394  1960 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1024 16:30:15.613394  1960 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1024 16:30:23.544392  1960 solver.cpp:218] Iteration 5100 (12.6079 iter/s, 7.93155s/100 iters), loss = 0.0271462
I1024 16:30:23.545392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:30:23.545392  1960 solver.cpp:237]     Train net output #1: loss = 0.0271462 (* 1 = 0.0271462 loss)
I1024 16:30:23.545392  1960 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1024 16:30:31.470392  1960 solver.cpp:218] Iteration 5200 (12.6181 iter/s, 7.92514s/100 iters), loss = 0.00586701
I1024 16:30:31.470392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:30:31.470392  1960 solver.cpp:237]     Train net output #1: loss = 0.00586703 (* 1 = 0.00586703 loss)
I1024 16:30:31.470392  1960 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1024 16:30:39.398392  1960 solver.cpp:218] Iteration 5300 (12.6149 iter/s, 7.92713s/100 iters), loss = 0.00585257
I1024 16:30:39.398392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:30:39.398392  1960 solver.cpp:237]     Train net output #1: loss = 0.00585261 (* 1 = 0.00585261 loss)
I1024 16:30:39.398392  1960 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1024 16:30:46.931393 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:30:47.247393  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_5400.caffemodel
I1024 16:30:47.280393  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_5400.solverstate
I1024 16:30:47.297392  1960 solver.cpp:330] Iteration 5400, Testing net (#0)
I1024 16:30:47.297392  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:30:49.201393  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:30:49.279393  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9955
I1024 16:30:49.279393  1960 solver.cpp:397]     Test net output #1: loss = 0.0138151 (* 1 = 0.0138151 loss)
I1024 16:30:49.355392  1960 solver.cpp:218] Iteration 5400 (10.0435 iter/s, 9.95671s/100 iters), loss = 0.00628877
I1024 16:30:49.355392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:30:49.355392  1960 solver.cpp:237]     Train net output #1: loss = 0.0062888 (* 1 = 0.0062888 loss)
I1024 16:30:49.355392  1960 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1024 16:30:57.279392  1960 solver.cpp:218] Iteration 5500 (12.6214 iter/s, 7.92303s/100 iters), loss = 0.0092703
I1024 16:30:57.279392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:30:57.279392  1960 solver.cpp:237]     Train net output #1: loss = 0.00927035 (* 1 = 0.00927035 loss)
I1024 16:30:57.279392  1960 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1024 16:31:05.206393  1960 solver.cpp:218] Iteration 5600 (12.616 iter/s, 7.92647s/100 iters), loss = 0.029506
I1024 16:31:05.206393  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:31:05.206393  1960 solver.cpp:237]     Train net output #1: loss = 0.029506 (* 1 = 0.029506 loss)
I1024 16:31:05.206393  1960 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1024 16:31:13.131392  1960 solver.cpp:218] Iteration 5700 (12.6185 iter/s, 7.9249s/100 iters), loss = 0.0429558
I1024 16:31:13.131392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:31:13.131392  1960 solver.cpp:237]     Train net output #1: loss = 0.0429559 (* 1 = 0.0429559 loss)
I1024 16:31:13.131392  1960 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1024 16:31:21.052392  1960 solver.cpp:218] Iteration 5800 (12.6245 iter/s, 7.92111s/100 iters), loss = 0.010167
I1024 16:31:21.053392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:31:21.053392  1960 solver.cpp:237]     Train net output #1: loss = 0.0101671 (* 1 = 0.0101671 loss)
I1024 16:31:21.053392  1960 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1024 16:31:28.975392  1960 solver.cpp:218] Iteration 5900 (12.623 iter/s, 7.92206s/100 iters), loss = 0.00996909
I1024 16:31:28.975392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:31:28.975392  1960 solver.cpp:237]     Train net output #1: loss = 0.00996912 (* 1 = 0.00996912 loss)
I1024 16:31:28.975392  1960 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1024 16:31:36.508397 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:31:36.824393  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_6000.caffemodel
I1024 16:31:36.860394  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_6000.solverstate
I1024 16:31:36.877394  1960 solver.cpp:330] Iteration 6000, Testing net (#0)
I1024 16:31:36.877394  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:31:38.779402  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:31:38.857393  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9959
I1024 16:31:38.857393  1960 solver.cpp:397]     Test net output #1: loss = 0.0125056 (* 1 = 0.0125056 loss)
I1024 16:31:38.934392  1960 solver.cpp:218] Iteration 6000 (10.0421 iter/s, 9.95811s/100 iters), loss = 0.00346984
I1024 16:31:38.934392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:31:38.934392  1960 solver.cpp:237]     Train net output #1: loss = 0.00346988 (* 1 = 0.00346988 loss)
I1024 16:31:38.934392  1960 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1024 16:31:46.862392  1960 solver.cpp:218] Iteration 6100 (12.6136 iter/s, 7.92795s/100 iters), loss = 0.0170289
I1024 16:31:46.862392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:31:46.862392  1960 solver.cpp:237]     Train net output #1: loss = 0.0170289 (* 1 = 0.0170289 loss)
I1024 16:31:46.862392  1960 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1024 16:31:54.782393  1960 solver.cpp:218] Iteration 6200 (12.6265 iter/s, 7.91987s/100 iters), loss = 0.0175542
I1024 16:31:54.783393  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:31:54.783393  1960 solver.cpp:237]     Train net output #1: loss = 0.0175543 (* 1 = 0.0175543 loss)
I1024 16:31:54.783393  1960 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1024 16:32:02.710392  1960 solver.cpp:218] Iteration 6300 (12.6149 iter/s, 7.92712s/100 iters), loss = 0.028703
I1024 16:32:02.710392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:32:02.710392  1960 solver.cpp:237]     Train net output #1: loss = 0.028703 (* 1 = 0.028703 loss)
I1024 16:32:02.710392  1960 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1024 16:32:10.632395  1960 solver.cpp:218] Iteration 6400 (12.6241 iter/s, 7.92135s/100 iters), loss = 0.0118076
I1024 16:32:10.632395  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:32:10.632395  1960 solver.cpp:237]     Train net output #1: loss = 0.0118076 (* 1 = 0.0118076 loss)
I1024 16:32:10.632395  1960 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1024 16:32:18.552392  1960 solver.cpp:218] Iteration 6500 (12.6266 iter/s, 7.91977s/100 iters), loss = 0.0504983
I1024 16:32:18.552392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:32:18.552392  1960 solver.cpp:237]     Train net output #1: loss = 0.0504984 (* 1 = 0.0504984 loss)
I1024 16:32:18.552392  1960 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1024 16:32:26.083395 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:32:26.398393  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_6600.caffemodel
I1024 16:32:26.433394  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_6600.solverstate
I1024 16:32:26.450395  1960 solver.cpp:330] Iteration 6600, Testing net (#0)
I1024 16:32:26.450395  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:32:28.352393  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:32:28.429394  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9963
I1024 16:32:28.430395  1960 solver.cpp:397]     Test net output #1: loss = 0.0118401 (* 1 = 0.0118401 loss)
I1024 16:32:28.506398  1960 solver.cpp:218] Iteration 6600 (10.047 iter/s, 9.95318s/100 iters), loss = 0.00814038
I1024 16:32:28.506398  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:32:28.506398  1960 solver.cpp:237]     Train net output #1: loss = 0.00814042 (* 1 = 0.00814042 loss)
I1024 16:32:28.506398  1960 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1024 16:32:36.429392  1960 solver.cpp:218] Iteration 6700 (12.623 iter/s, 7.92203s/100 iters), loss = 0.0100519
I1024 16:32:36.429392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:32:36.429392  1960 solver.cpp:237]     Train net output #1: loss = 0.0100519 (* 1 = 0.0100519 loss)
I1024 16:32:36.429392  1960 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1024 16:32:44.351392  1960 solver.cpp:218] Iteration 6800 (12.6234 iter/s, 7.92183s/100 iters), loss = 0.0109111
I1024 16:32:44.351392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:32:44.351392  1960 solver.cpp:237]     Train net output #1: loss = 0.0109111 (* 1 = 0.0109111 loss)
I1024 16:32:44.351392  1960 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1024 16:32:52.268393  1960 solver.cpp:218] Iteration 6900 (12.632 iter/s, 7.9164s/100 iters), loss = 0.0107858
I1024 16:32:52.268393  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:32:52.268393  1960 solver.cpp:237]     Train net output #1: loss = 0.0107858 (* 1 = 0.0107858 loss)
I1024 16:32:52.268393  1960 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1024 16:33:00.193392  1960 solver.cpp:218] Iteration 7000 (12.6194 iter/s, 7.9243s/100 iters), loss = 0.0100861
I1024 16:33:00.193392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:33:00.193392  1960 solver.cpp:237]     Train net output #1: loss = 0.0100861 (* 1 = 0.0100861 loss)
I1024 16:33:00.193392  1960 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1024 16:33:08.122392  1960 solver.cpp:218] Iteration 7100 (12.6114 iter/s, 7.92934s/100 iters), loss = 0.00555021
I1024 16:33:08.122392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:33:08.122392  1960 solver.cpp:237]     Train net output #1: loss = 0.00555025 (* 1 = 0.00555025 loss)
I1024 16:33:08.122392  1960 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1024 16:33:15.653394 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:33:15.969393  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_7200.caffemodel
I1024 16:33:16.005403  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_7200.solverstate
I1024 16:33:16.021394  1960 solver.cpp:330] Iteration 7200, Testing net (#0)
I1024 16:33:16.022394  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:33:17.923408  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:33:18.002394  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9966
I1024 16:33:18.002394  1960 solver.cpp:397]     Test net output #1: loss = 0.0115044 (* 1 = 0.0115044 loss)
I1024 16:33:18.079392  1960 solver.cpp:218] Iteration 7200 (10.0447 iter/s, 9.95552s/100 iters), loss = 0.00413926
I1024 16:33:18.079392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:33:18.079392  1960 solver.cpp:237]     Train net output #1: loss = 0.00413929 (* 1 = 0.00413929 loss)
I1024 16:33:18.079392  1960 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1024 16:33:25.998392  1960 solver.cpp:218] Iteration 7300 (12.6275 iter/s, 7.91923s/100 iters), loss = 0.0247865
I1024 16:33:25.998392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:33:25.998392  1960 solver.cpp:237]     Train net output #1: loss = 0.0247865 (* 1 = 0.0247865 loss)
I1024 16:33:25.998392  1960 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1024 16:33:33.925392  1960 solver.cpp:218] Iteration 7400 (12.6165 iter/s, 7.92615s/100 iters), loss = 0.0081312
I1024 16:33:33.925392  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:33:33.925392  1960 solver.cpp:237]     Train net output #1: loss = 0.00813121 (* 1 = 0.00813121 loss)
I1024 16:33:33.925392  1960 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1024 16:33:41.849540  1960 solver.cpp:218] Iteration 7500 (12.6204 iter/s, 7.92368s/100 iters), loss = 0.0121502
I1024 16:33:41.849540  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:33:41.849540  1960 solver.cpp:237]     Train net output #1: loss = 0.0121502 (* 1 = 0.0121502 loss)
I1024 16:33:41.849540  1960 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1024 16:33:49.772539  1960 solver.cpp:218] Iteration 7600 (12.622 iter/s, 7.92265s/100 iters), loss = 0.00659864
I1024 16:33:49.772539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:33:49.772539  1960 solver.cpp:237]     Train net output #1: loss = 0.00659865 (* 1 = 0.00659865 loss)
I1024 16:33:49.772539  1960 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1024 16:33:57.690539  1960 solver.cpp:218] Iteration 7700 (12.6303 iter/s, 7.91746s/100 iters), loss = 0.0197305
I1024 16:33:57.690539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:33:57.690539  1960 solver.cpp:237]     Train net output #1: loss = 0.0197306 (* 1 = 0.0197306 loss)
I1024 16:33:57.690539  1960 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1024 16:34:05.217559 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:34:05.533545  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_7800.caffemodel
I1024 16:34:05.566540  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_7800.solverstate
I1024 16:34:05.582540  1960 solver.cpp:330] Iteration 7800, Testing net (#0)
I1024 16:34:05.583547  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:34:07.482540  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:34:07.560539  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9965
I1024 16:34:07.560539  1960 solver.cpp:397]     Test net output #1: loss = 0.0117479 (* 1 = 0.0117479 loss)
I1024 16:34:07.637542  1960 solver.cpp:218] Iteration 7800 (10.0541 iter/s, 9.9462s/100 iters), loss = 0.00362185
I1024 16:34:07.637542  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:34:07.637542  1960 solver.cpp:237]     Train net output #1: loss = 0.00362187 (* 1 = 0.00362187 loss)
I1024 16:34:07.637542  1960 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1024 16:34:15.563539  1960 solver.cpp:218] Iteration 7900 (12.6169 iter/s, 7.92588s/100 iters), loss = 0.0118656
I1024 16:34:15.563539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:34:15.563539  1960 solver.cpp:237]     Train net output #1: loss = 0.0118656 (* 1 = 0.0118656 loss)
I1024 16:34:15.563539  1960 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1024 16:34:23.486538  1960 solver.cpp:218] Iteration 8000 (12.6221 iter/s, 7.92264s/100 iters), loss = 0.0197206
I1024 16:34:23.486538  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:34:23.486538  1960 solver.cpp:237]     Train net output #1: loss = 0.0197206 (* 1 = 0.0197206 loss)
I1024 16:34:23.486538  1960 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1024 16:34:31.409538  1960 solver.cpp:218] Iteration 8100 (12.6222 iter/s, 7.92252s/100 iters), loss = 0.00744965
I1024 16:34:31.409538  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:34:31.409538  1960 solver.cpp:237]     Train net output #1: loss = 0.00744967 (* 1 = 0.00744967 loss)
I1024 16:34:31.409538  1960 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1024 16:34:39.333539  1960 solver.cpp:218] Iteration 8200 (12.6218 iter/s, 7.92279s/100 iters), loss = 0.00450312
I1024 16:34:39.333539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:34:39.333539  1960 solver.cpp:237]     Train net output #1: loss = 0.00450312 (* 1 = 0.00450312 loss)
I1024 16:34:39.333539  1960 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1024 16:34:47.257539  1960 solver.cpp:218] Iteration 8300 (12.6207 iter/s, 7.92351s/100 iters), loss = 0.00475756
I1024 16:34:47.257539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:34:47.257539  1960 solver.cpp:237]     Train net output #1: loss = 0.00475755 (* 1 = 0.00475755 loss)
I1024 16:34:47.257539  1960 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1024 16:34:54.793541 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:34:55.109547  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_8400.caffemodel
I1024 16:34:55.144539  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_8400.solverstate
I1024 16:34:55.160539  1960 solver.cpp:330] Iteration 8400, Testing net (#0)
I1024 16:34:55.160539  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:34:57.062541  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:34:57.140538  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9965
I1024 16:34:57.140538  1960 solver.cpp:397]     Test net output #1: loss = 0.0114185 (* 1 = 0.0114185 loss)
I1024 16:34:57.217538  1960 solver.cpp:218] Iteration 8400 (10.0405 iter/s, 9.95967s/100 iters), loss = 0.00468346
I1024 16:34:57.217538  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:34:57.217538  1960 solver.cpp:237]     Train net output #1: loss = 0.00468347 (* 1 = 0.00468347 loss)
I1024 16:34:57.217538  1960 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1024 16:35:05.142537  1960 solver.cpp:218] Iteration 8500 (12.6186 iter/s, 7.92482s/100 iters), loss = 0.0113704
I1024 16:35:05.142537  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:35:05.142537  1960 solver.cpp:237]     Train net output #1: loss = 0.0113704 (* 1 = 0.0113704 loss)
I1024 16:35:05.142537  1960 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1024 16:35:13.069538  1960 solver.cpp:218] Iteration 8600 (12.6161 iter/s, 7.9264s/100 iters), loss = 0.0135679
I1024 16:35:13.069538  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:35:13.069538  1960 solver.cpp:237]     Train net output #1: loss = 0.0135679 (* 1 = 0.0135679 loss)
I1024 16:35:13.069538  1960 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1024 16:35:20.990553  1960 solver.cpp:218] Iteration 8700 (12.6245 iter/s, 7.92108s/100 iters), loss = 0.00816775
I1024 16:35:20.990553  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:35:20.991554  1960 solver.cpp:237]     Train net output #1: loss = 0.00816777 (* 1 = 0.00816777 loss)
I1024 16:35:20.991554  1960 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1024 16:35:28.917538  1960 solver.cpp:218] Iteration 8800 (12.6159 iter/s, 7.9265s/100 iters), loss = 0.0020705
I1024 16:35:28.918540  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:35:28.918540  1960 solver.cpp:237]     Train net output #1: loss = 0.00207052 (* 1 = 0.00207052 loss)
I1024 16:35:28.918540  1960 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1024 16:35:36.845540  1960 solver.cpp:218] Iteration 8900 (12.6152 iter/s, 7.92697s/100 iters), loss = 0.00374172
I1024 16:35:36.845540  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:35:36.845540  1960 solver.cpp:237]     Train net output #1: loss = 0.00374174 (* 1 = 0.00374174 loss)
I1024 16:35:36.845540  1960 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1024 16:35:44.380540 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:35:44.696539  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_9000.caffemodel
I1024 16:35:44.731540  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_9000.solverstate
I1024 16:35:44.748539  1960 solver.cpp:330] Iteration 9000, Testing net (#0)
I1024 16:35:44.748539  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:35:46.649540  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:35:46.727552  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9963
I1024 16:35:46.727552  1960 solver.cpp:397]     Test net output #1: loss = 0.0114366 (* 1 = 0.0114366 loss)
I1024 16:35:46.803539  1960 solver.cpp:218] Iteration 9000 (10.0425 iter/s, 9.95764s/100 iters), loss = 0.00297929
I1024 16:35:46.803539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:35:46.803539  1960 solver.cpp:237]     Train net output #1: loss = 0.00297931 (* 1 = 0.00297931 loss)
I1024 16:35:46.803539  1960 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1024 16:35:54.725538  1960 solver.cpp:218] Iteration 9100 (12.6241 iter/s, 7.92137s/100 iters), loss = 0.0132527
I1024 16:35:54.725538  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:35:54.725538  1960 solver.cpp:237]     Train net output #1: loss = 0.0132527 (* 1 = 0.0132527 loss)
I1024 16:35:54.725538  1960 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1024 16:36:02.652539  1960 solver.cpp:218] Iteration 9200 (12.6162 iter/s, 7.9263s/100 iters), loss = 0.0185936
I1024 16:36:02.652539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:36:02.652539  1960 solver.cpp:237]     Train net output #1: loss = 0.0185936 (* 1 = 0.0185936 loss)
I1024 16:36:02.652539  1960 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1024 16:36:10.576539  1960 solver.cpp:218] Iteration 9300 (12.6206 iter/s, 7.92358s/100 iters), loss = 0.0125793
I1024 16:36:10.576539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:36:10.576539  1960 solver.cpp:237]     Train net output #1: loss = 0.0125793 (* 1 = 0.0125793 loss)
I1024 16:36:10.576539  1960 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1024 16:36:18.503538  1960 solver.cpp:218] Iteration 9400 (12.6154 iter/s, 7.92679s/100 iters), loss = 0.00400623
I1024 16:36:18.503538  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:36:18.503538  1960 solver.cpp:237]     Train net output #1: loss = 0.00400624 (* 1 = 0.00400624 loss)
I1024 16:36:18.503538  1960 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1024 16:36:26.429538  1960 solver.cpp:218] Iteration 9500 (12.6188 iter/s, 7.92469s/100 iters), loss = 0.0112159
I1024 16:36:26.429538  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:36:26.429538  1960 solver.cpp:237]     Train net output #1: loss = 0.011216 (* 1 = 0.011216 loss)
I1024 16:36:26.429538  1960 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1024 16:36:26.429538  1960 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1024 16:36:33.964540 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:36:34.279541  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_9600.caffemodel
I1024 16:36:34.313539  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_9600.solverstate
I1024 16:36:34.330545  1960 solver.cpp:330] Iteration 9600, Testing net (#0)
I1024 16:36:34.330545  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:36:36.232542  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:36:36.311549  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9968
I1024 16:36:36.311549  1960 solver.cpp:397]     Test net output #1: loss = 0.0104334 (* 1 = 0.0104334 loss)
I1024 16:36:36.387539  1960 solver.cpp:218] Iteration 9600 (10.0423 iter/s, 9.95784s/100 iters), loss = 0.00497945
I1024 16:36:36.387539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:36:36.387539  1960 solver.cpp:237]     Train net output #1: loss = 0.00497947 (* 1 = 0.00497947 loss)
I1024 16:36:36.387539  1960 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1024 16:36:44.318539  1960 solver.cpp:218] Iteration 9700 (12.6101 iter/s, 7.93016s/100 iters), loss = 0.0184201
I1024 16:36:44.318539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:36:44.318539  1960 solver.cpp:237]     Train net output #1: loss = 0.0184201 (* 1 = 0.0184201 loss)
I1024 16:36:44.318539  1960 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1024 16:36:52.243538  1960 solver.cpp:218] Iteration 9800 (12.6182 iter/s, 7.92506s/100 iters), loss = 0.016109
I1024 16:36:52.243538  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:36:52.243538  1960 solver.cpp:237]     Train net output #1: loss = 0.016109 (* 1 = 0.016109 loss)
I1024 16:36:52.243538  1960 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1024 16:37:00.165539  1960 solver.cpp:218] Iteration 9900 (12.6243 iter/s, 7.92122s/100 iters), loss = 0.00649115
I1024 16:37:00.165539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:37:00.165539  1960 solver.cpp:237]     Train net output #1: loss = 0.00649118 (* 1 = 0.00649118 loss)
I1024 16:37:00.165539  1960 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1024 16:37:08.096539  1960 solver.cpp:218] Iteration 10000 (12.6099 iter/s, 7.93026s/100 iters), loss = 0.00504308
I1024 16:37:08.096539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:37:08.096539  1960 solver.cpp:237]     Train net output #1: loss = 0.00504313 (* 1 = 0.00504313 loss)
I1024 16:37:08.096539  1960 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1024 16:37:16.020539  1960 solver.cpp:218] Iteration 10100 (12.6201 iter/s, 7.92385s/100 iters), loss = 0.00546233
I1024 16:37:16.020539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:37:16.020539  1960 solver.cpp:237]     Train net output #1: loss = 0.00546238 (* 1 = 0.00546238 loss)
I1024 16:37:16.020539  1960 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1024 16:37:23.562541 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:37:23.877538  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_10200.caffemodel
I1024 16:37:23.912540  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_10200.solverstate
I1024 16:37:23.928542  1960 solver.cpp:330] Iteration 10200, Testing net (#0)
I1024 16:37:23.928542  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:37:25.829540  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:37:25.907539  1960 solver.cpp:397]     Test net output #0: accuracy = 0.997
I1024 16:37:25.907539  1960 solver.cpp:397]     Test net output #1: loss = 0.0100636 (* 1 = 0.0100636 loss)
I1024 16:37:25.984539  1960 solver.cpp:218] Iteration 10200 (10.0363 iter/s, 9.96384s/100 iters), loss = 0.00502373
I1024 16:37:25.984539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:37:25.984539  1960 solver.cpp:237]     Train net output #1: loss = 0.00502379 (* 1 = 0.00502379 loss)
I1024 16:37:25.984539  1960 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1024 16:37:33.911538  1960 solver.cpp:218] Iteration 10300 (12.6164 iter/s, 7.92616s/100 iters), loss = 0.0153644
I1024 16:37:33.911538  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:37:33.911538  1960 solver.cpp:237]     Train net output #1: loss = 0.0153645 (* 1 = 0.0153645 loss)
I1024 16:37:33.911538  1960 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1024 16:37:41.833539  1960 solver.cpp:218] Iteration 10400 (12.6236 iter/s, 7.9217s/100 iters), loss = 0.00568839
I1024 16:37:41.833539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:37:41.833539  1960 solver.cpp:237]     Train net output #1: loss = 0.00568845 (* 1 = 0.00568845 loss)
I1024 16:37:41.833539  1960 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1024 16:37:49.760540  1960 solver.cpp:218] Iteration 10500 (12.6161 iter/s, 7.92641s/100 iters), loss = 0.0184338
I1024 16:37:49.760540  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:37:49.760540  1960 solver.cpp:237]     Train net output #1: loss = 0.0184339 (* 1 = 0.0184339 loss)
I1024 16:37:49.760540  1960 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1024 16:37:57.685539  1960 solver.cpp:218] Iteration 10600 (12.6197 iter/s, 7.92415s/100 iters), loss = 0.00397467
I1024 16:37:57.685539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:37:57.685539  1960 solver.cpp:237]     Train net output #1: loss = 0.00397476 (* 1 = 0.00397476 loss)
I1024 16:37:57.685539  1960 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1024 16:38:05.609539  1960 solver.cpp:218] Iteration 10700 (12.6202 iter/s, 7.92377s/100 iters), loss = 0.0375629
I1024 16:38:05.609539  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:38:05.609539  1960 solver.cpp:237]     Train net output #1: loss = 0.037563 (* 1 = 0.037563 loss)
I1024 16:38:05.609539  1960 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1024 16:38:13.144601 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:38:13.459600  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_10800.caffemodel
I1024 16:38:13.498601  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_10800.solverstate
I1024 16:38:13.518618  1960 solver.cpp:330] Iteration 10800, Testing net (#0)
I1024 16:38:13.518618  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:38:15.420600  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:38:15.498600  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 16:38:15.498600  1960 solver.cpp:397]     Test net output #1: loss = 0.00984548 (* 1 = 0.00984548 loss)
I1024 16:38:15.575600  1960 solver.cpp:218] Iteration 10800 (10.0347 iter/s, 9.96545s/100 iters), loss = 0.00966474
I1024 16:38:15.575600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:38:15.575600  1960 solver.cpp:237]     Train net output #1: loss = 0.00966481 (* 1 = 0.00966481 loss)
I1024 16:38:15.575600  1960 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1024 16:38:23.536598  1960 solver.cpp:218] Iteration 10900 (12.5615 iter/s, 7.96081s/100 iters), loss = 0.0173009
I1024 16:38:23.536598  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:38:23.536598  1960 solver.cpp:237]     Train net output #1: loss = 0.0173009 (* 1 = 0.0173009 loss)
I1024 16:38:23.536598  1960 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1024 16:38:31.435600  1960 solver.cpp:218] Iteration 11000 (12.6609 iter/s, 7.89833s/100 iters), loss = 0.00882512
I1024 16:38:31.435600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:38:31.435600  1960 solver.cpp:237]     Train net output #1: loss = 0.00882518 (* 1 = 0.00882518 loss)
I1024 16:38:31.435600  1960 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1024 16:38:39.340600  1960 solver.cpp:218] Iteration 11100 (12.6507 iter/s, 7.90472s/100 iters), loss = 0.0224271
I1024 16:38:39.340600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:38:39.340600  1960 solver.cpp:237]     Train net output #1: loss = 0.0224272 (* 1 = 0.0224272 loss)
I1024 16:38:39.340600  1960 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1024 16:38:47.239598  1960 solver.cpp:218] Iteration 11200 (12.66 iter/s, 7.89891s/100 iters), loss = 0.0045605
I1024 16:38:47.239598  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:38:47.239598  1960 solver.cpp:237]     Train net output #1: loss = 0.00456056 (* 1 = 0.00456056 loss)
I1024 16:38:47.239598  1960 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1024 16:38:55.138599  1960 solver.cpp:218] Iteration 11300 (12.6608 iter/s, 7.89839s/100 iters), loss = 0.00543108
I1024 16:38:55.138599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:38:55.138599  1960 solver.cpp:237]     Train net output #1: loss = 0.00543114 (* 1 = 0.00543114 loss)
I1024 16:38:55.138599  1960 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1024 16:39:02.643600 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:39:02.958600  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_11400.caffemodel
I1024 16:39:02.993621  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_11400.solverstate
I1024 16:39:03.009615  1960 solver.cpp:330] Iteration 11400, Testing net (#0)
I1024 16:39:03.009615  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:39:04.904600  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:39:04.982600  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9975
I1024 16:39:04.982600  1960 solver.cpp:397]     Test net output #1: loss = 0.00983913 (* 1 = 0.00983913 loss)
I1024 16:39:05.058600  1960 solver.cpp:218] Iteration 11400 (10.0813 iter/s, 9.91933s/100 iters), loss = 0.0059823
I1024 16:39:05.058600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:39:05.058600  1960 solver.cpp:237]     Train net output #1: loss = 0.00598236 (* 1 = 0.00598236 loss)
I1024 16:39:05.058600  1960 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1024 16:39:12.963599  1960 solver.cpp:218] Iteration 11500 (12.6513 iter/s, 7.90434s/100 iters), loss = 0.00988831
I1024 16:39:12.963599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:39:12.963599  1960 solver.cpp:237]     Train net output #1: loss = 0.00988837 (* 1 = 0.00988837 loss)
I1024 16:39:12.963599  1960 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1024 16:39:20.867599  1960 solver.cpp:218] Iteration 11600 (12.6528 iter/s, 7.90336s/100 iters), loss = 0.0125185
I1024 16:39:20.867599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:39:20.867599  1960 solver.cpp:237]     Train net output #1: loss = 0.0125186 (* 1 = 0.0125186 loss)
I1024 16:39:20.867599  1960 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1024 16:39:28.775599  1960 solver.cpp:218] Iteration 11700 (12.6465 iter/s, 7.90735s/100 iters), loss = 0.00581543
I1024 16:39:28.775599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:39:28.775599  1960 solver.cpp:237]     Train net output #1: loss = 0.00581549 (* 1 = 0.00581549 loss)
I1024 16:39:28.775599  1960 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1024 16:39:36.676599  1960 solver.cpp:218] Iteration 11800 (12.6568 iter/s, 7.90091s/100 iters), loss = 0.00210868
I1024 16:39:36.676599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:39:36.676599  1960 solver.cpp:237]     Train net output #1: loss = 0.00210874 (* 1 = 0.00210874 loss)
I1024 16:39:36.676599  1960 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1024 16:39:44.575600  1960 solver.cpp:218] Iteration 11900 (12.6602 iter/s, 7.89878s/100 iters), loss = 0.00920726
I1024 16:39:44.575600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:39:44.575600  1960 solver.cpp:237]     Train net output #1: loss = 0.0092073 (* 1 = 0.0092073 loss)
I1024 16:39:44.575600  1960 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1024 16:39:52.091600 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:39:52.406605  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_12000.caffemodel
I1024 16:39:52.442605  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_12000.solverstate
I1024 16:39:52.458613  1960 solver.cpp:330] Iteration 12000, Testing net (#0)
I1024 16:39:52.458613  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:39:54.354600  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:39:54.431601  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 16:39:54.431601  1960 solver.cpp:397]     Test net output #1: loss = 0.00978817 (* 1 = 0.00978817 loss)
I1024 16:39:54.507604  1960 solver.cpp:218] Iteration 12000 (10.0691 iter/s, 9.93137s/100 iters), loss = 0.0120546
I1024 16:39:54.507604  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:39:54.507604  1960 solver.cpp:237]     Train net output #1: loss = 0.0120546 (* 1 = 0.0120546 loss)
I1024 16:39:54.507604  1960 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1024 16:40:02.407599  1960 solver.cpp:218] Iteration 12100 (12.659 iter/s, 7.89952s/100 iters), loss = 0.00786718
I1024 16:40:02.407599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:40:02.407599  1960 solver.cpp:237]     Train net output #1: loss = 0.00786722 (* 1 = 0.00786722 loss)
I1024 16:40:02.408601  1960 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1024 16:40:10.310598  1960 solver.cpp:218] Iteration 12200 (12.6552 iter/s, 7.90188s/100 iters), loss = 0.00699891
I1024 16:40:10.310598  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:40:10.310598  1960 solver.cpp:237]     Train net output #1: loss = 0.00699896 (* 1 = 0.00699896 loss)
I1024 16:40:10.310598  1960 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1024 16:40:18.207598  1960 solver.cpp:218] Iteration 12300 (12.6632 iter/s, 7.89692s/100 iters), loss = 0.0148892
I1024 16:40:18.207598  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:40:18.207598  1960 solver.cpp:237]     Train net output #1: loss = 0.0148892 (* 1 = 0.0148892 loss)
I1024 16:40:18.207598  1960 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1024 16:40:26.110599  1960 solver.cpp:218] Iteration 12400 (12.6542 iter/s, 7.90252s/100 iters), loss = 0.00271467
I1024 16:40:26.110599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:40:26.110599  1960 solver.cpp:237]     Train net output #1: loss = 0.00271471 (* 1 = 0.00271471 loss)
I1024 16:40:26.110599  1960 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1024 16:40:34.010601  1960 solver.cpp:218] Iteration 12500 (12.6596 iter/s, 7.89914s/100 iters), loss = 0.00434448
I1024 16:40:34.010601  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:40:34.010601  1960 solver.cpp:237]     Train net output #1: loss = 0.00434451 (* 1 = 0.00434451 loss)
I1024 16:40:34.010601  1960 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1024 16:40:41.517603 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:40:41.832600  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_12600.caffemodel
I1024 16:40:41.866600  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_12600.solverstate
I1024 16:40:41.882601  1960 solver.cpp:330] Iteration 12600, Testing net (#0)
I1024 16:40:41.882601  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:40:43.776599  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:40:43.854599  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9972
I1024 16:40:43.854599  1960 solver.cpp:397]     Test net output #1: loss = 0.00957157 (* 1 = 0.00957157 loss)
I1024 16:40:43.931599  1960 solver.cpp:218] Iteration 12600 (10.0801 iter/s, 9.9205s/100 iters), loss = 0.00390499
I1024 16:40:43.931599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:40:43.931599  1960 solver.cpp:237]     Train net output #1: loss = 0.00390504 (* 1 = 0.00390504 loss)
I1024 16:40:43.931599  1960 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1024 16:40:51.830600  1960 solver.cpp:218] Iteration 12700 (12.6602 iter/s, 7.89878s/100 iters), loss = 0.0107928
I1024 16:40:51.830600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:40:51.830600  1960 solver.cpp:237]     Train net output #1: loss = 0.0107928 (* 1 = 0.0107928 loss)
I1024 16:40:51.830600  1960 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1024 16:40:59.728600  1960 solver.cpp:218] Iteration 12800 (12.6621 iter/s, 7.89758s/100 iters), loss = 0.00610424
I1024 16:40:59.728600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:40:59.728600  1960 solver.cpp:237]     Train net output #1: loss = 0.0061043 (* 1 = 0.0061043 loss)
I1024 16:40:59.728600  1960 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1024 16:41:07.632598  1960 solver.cpp:218] Iteration 12900 (12.6527 iter/s, 7.90346s/100 iters), loss = 0.0117741
I1024 16:41:07.632598  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:41:07.632598  1960 solver.cpp:237]     Train net output #1: loss = 0.0117742 (* 1 = 0.0117742 loss)
I1024 16:41:07.632598  1960 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1024 16:41:15.536599  1960 solver.cpp:218] Iteration 13000 (12.6524 iter/s, 7.90363s/100 iters), loss = 0.00277507
I1024 16:41:15.536599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:41:15.536599  1960 solver.cpp:237]     Train net output #1: loss = 0.00277511 (* 1 = 0.00277511 loss)
I1024 16:41:15.536599  1960 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1024 16:41:23.441599  1960 solver.cpp:218] Iteration 13100 (12.6513 iter/s, 7.90434s/100 iters), loss = 0.00413702
I1024 16:41:23.441599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:41:23.441599  1960 solver.cpp:237]     Train net output #1: loss = 0.00413707 (* 1 = 0.00413707 loss)
I1024 16:41:23.441599  1960 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1024 16:41:30.960599 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:41:31.272600  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_13200.caffemodel
I1024 16:41:31.308604  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_13200.solverstate
I1024 16:41:31.324604  1960 solver.cpp:330] Iteration 13200, Testing net (#0)
I1024 16:41:31.324604  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:41:33.220600  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:41:33.298600  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9971
I1024 16:41:33.298600  1960 solver.cpp:397]     Test net output #1: loss = 0.00954865 (* 1 = 0.00954865 loss)
I1024 16:41:33.375599  1960 solver.cpp:218] Iteration 13200 (10.0675 iter/s, 9.933s/100 iters), loss = 0.00899575
I1024 16:41:33.375599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:41:33.375599  1960 solver.cpp:237]     Train net output #1: loss = 0.0089958 (* 1 = 0.0089958 loss)
I1024 16:41:33.375599  1960 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1024 16:41:41.282599  1960 solver.cpp:218] Iteration 13300 (12.6477 iter/s, 7.90658s/100 iters), loss = 0.0150173
I1024 16:41:41.282599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:41:41.282599  1960 solver.cpp:237]     Train net output #1: loss = 0.0150173 (* 1 = 0.0150173 loss)
I1024 16:41:41.282599  1960 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1024 16:41:49.184599  1960 solver.cpp:218] Iteration 13400 (12.6552 iter/s, 7.90192s/100 iters), loss = 0.0111017
I1024 16:41:49.184599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:41:49.184599  1960 solver.cpp:237]     Train net output #1: loss = 0.0111017 (* 1 = 0.0111017 loss)
I1024 16:41:49.184599  1960 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1024 16:41:57.091599  1960 solver.cpp:218] Iteration 13500 (12.6487 iter/s, 7.90597s/100 iters), loss = 0.0108739
I1024 16:41:57.091599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:41:57.091599  1960 solver.cpp:237]     Train net output #1: loss = 0.0108739 (* 1 = 0.0108739 loss)
I1024 16:41:57.091599  1960 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1024 16:42:04.998600  1960 solver.cpp:218] Iteration 13600 (12.6465 iter/s, 7.9073s/100 iters), loss = 0.00298613
I1024 16:42:04.998600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:42:04.998600  1960 solver.cpp:237]     Train net output #1: loss = 0.00298618 (* 1 = 0.00298618 loss)
I1024 16:42:04.998600  1960 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1024 16:42:12.907598  1960 solver.cpp:218] Iteration 13700 (12.6457 iter/s, 7.90783s/100 iters), loss = 0.00554146
I1024 16:42:12.907598  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:42:12.907598  1960 solver.cpp:237]     Train net output #1: loss = 0.0055415 (* 1 = 0.0055415 loss)
I1024 16:42:12.907598  1960 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1024 16:42:20.430609 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:42:20.745600  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_13800.caffemodel
I1024 16:42:20.779603  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_13800.solverstate
I1024 16:42:20.796602  1960 solver.cpp:330] Iteration 13800, Testing net (#0)
I1024 16:42:20.796602  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:42:22.694600  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:42:22.772600  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 16:42:22.772600  1960 solver.cpp:397]     Test net output #1: loss = 0.00954715 (* 1 = 0.00954715 loss)
I1024 16:42:22.848599  1960 solver.cpp:218] Iteration 13800 (10.0595 iter/s, 9.94089s/100 iters), loss = 0.00301781
I1024 16:42:22.848599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:42:22.848599  1960 solver.cpp:237]     Train net output #1: loss = 0.00301785 (* 1 = 0.00301785 loss)
I1024 16:42:22.848599  1960 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1024 16:42:30.764600  1960 solver.cpp:218] Iteration 13900 (12.6338 iter/s, 7.91527s/100 iters), loss = 0.0102371
I1024 16:42:30.764600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:42:30.764600  1960 solver.cpp:237]     Train net output #1: loss = 0.0102371 (* 1 = 0.0102371 loss)
I1024 16:42:30.764600  1960 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1024 16:42:38.676599  1960 solver.cpp:218] Iteration 14000 (12.6391 iter/s, 7.91194s/100 iters), loss = 0.0143881
I1024 16:42:38.676599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:42:38.676599  1960 solver.cpp:237]     Train net output #1: loss = 0.0143881 (* 1 = 0.0143881 loss)
I1024 16:42:38.676599  1960 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1024 16:42:46.583600  1960 solver.cpp:218] Iteration 14100 (12.6486 iter/s, 7.90599s/100 iters), loss = 0.00405082
I1024 16:42:46.583600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:42:46.583600  1960 solver.cpp:237]     Train net output #1: loss = 0.00405085 (* 1 = 0.00405085 loss)
I1024 16:42:46.583600  1960 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1024 16:42:54.494601  1960 solver.cpp:218] Iteration 14200 (12.6409 iter/s, 7.91085s/100 iters), loss = 0.0032122
I1024 16:42:54.494601  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:42:54.494601  1960 solver.cpp:237]     Train net output #1: loss = 0.00321224 (* 1 = 0.00321224 loss)
I1024 16:42:54.494601  1960 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1024 16:43:02.400604  1960 solver.cpp:218] Iteration 14300 (12.6488 iter/s, 7.90588s/100 iters), loss = 0.00764059
I1024 16:43:02.401599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:43:02.401599  1960 solver.cpp:237]     Train net output #1: loss = 0.00764063 (* 1 = 0.00764063 loss)
I1024 16:43:02.401599  1960 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1024 16:43:09.910603 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:43:10.225599  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_14400.caffemodel
I1024 16:43:10.260601  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_14400.solverstate
I1024 16:43:10.277601  1960 solver.cpp:330] Iteration 14400, Testing net (#0)
I1024 16:43:10.277601  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:43:12.171602  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:43:12.249603  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9969
I1024 16:43:12.249603  1960 solver.cpp:397]     Test net output #1: loss = 0.00961816 (* 1 = 0.00961816 loss)
I1024 16:43:12.325615  1960 solver.cpp:218] Iteration 14400 (10.0761 iter/s, 9.92444s/100 iters), loss = 0.00500367
I1024 16:43:12.325615  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:43:12.326616  1960 solver.cpp:237]     Train net output #1: loss = 0.0050037 (* 1 = 0.0050037 loss)
I1024 16:43:12.326616  1960 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1024 16:43:20.230599  1960 solver.cpp:218] Iteration 14500 (12.6514 iter/s, 7.90429s/100 iters), loss = 0.00498082
I1024 16:43:20.230599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:43:20.230599  1960 solver.cpp:237]     Train net output #1: loss = 0.00498085 (* 1 = 0.00498085 loss)
I1024 16:43:20.230599  1960 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1024 16:43:28.137599  1960 solver.cpp:218] Iteration 14600 (12.6488 iter/s, 7.9059s/100 iters), loss = 0.0096376
I1024 16:43:28.137599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:43:28.137599  1960 solver.cpp:237]     Train net output #1: loss = 0.00963764 (* 1 = 0.00963764 loss)
I1024 16:43:28.137599  1960 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1024 16:43:36.038599  1960 solver.cpp:218] Iteration 14700 (12.6565 iter/s, 7.90105s/100 iters), loss = 0.00943262
I1024 16:43:36.038599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:43:36.038599  1960 solver.cpp:237]     Train net output #1: loss = 0.00943266 (* 1 = 0.00943266 loss)
I1024 16:43:36.038599  1960 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1024 16:43:43.940599  1960 solver.cpp:218] Iteration 14800 (12.6554 iter/s, 7.90175s/100 iters), loss = 0.00564391
I1024 16:43:43.940599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:43:43.940599  1960 solver.cpp:237]     Train net output #1: loss = 0.00564395 (* 1 = 0.00564395 loss)
I1024 16:43:43.940599  1960 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1024 16:43:51.842599  1960 solver.cpp:218] Iteration 14900 (12.6565 iter/s, 7.90107s/100 iters), loss = 0.003841
I1024 16:43:51.842599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:43:51.842599  1960 solver.cpp:237]     Train net output #1: loss = 0.00384103 (* 1 = 0.00384103 loss)
I1024 16:43:51.842599  1960 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1024 16:43:59.353601 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:43:59.668601  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_15000.caffemodel
I1024 16:43:59.701601  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_15000.solverstate
I1024 16:43:59.717600  1960 solver.cpp:330] Iteration 15000, Testing net (#0)
I1024 16:43:59.718600  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:44:01.612607  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:44:01.690600  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9972
I1024 16:44:01.690600  1960 solver.cpp:397]     Test net output #1: loss = 0.00953829 (* 1 = 0.00953829 loss)
I1024 16:44:01.766599  1960 solver.cpp:218] Iteration 15000 (10.0767 iter/s, 9.92384s/100 iters), loss = 0.00427112
I1024 16:44:01.766599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:44:01.766599  1960 solver.cpp:237]     Train net output #1: loss = 0.00427115 (* 1 = 0.00427115 loss)
I1024 16:44:01.766599  1960 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1024 16:44:09.668599  1960 solver.cpp:218] Iteration 15100 (12.6555 iter/s, 7.90172s/100 iters), loss = 0.0127841
I1024 16:44:09.668599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:44:09.668599  1960 solver.cpp:237]     Train net output #1: loss = 0.0127842 (* 1 = 0.0127842 loss)
I1024 16:44:09.668599  1960 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1024 16:44:17.569599  1960 solver.cpp:218] Iteration 15200 (12.6589 iter/s, 7.89958s/100 iters), loss = 0.00650043
I1024 16:44:17.569599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:44:17.569599  1960 solver.cpp:237]     Train net output #1: loss = 0.00650046 (* 1 = 0.00650046 loss)
I1024 16:44:17.569599  1960 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1024 16:44:25.470599  1960 solver.cpp:218] Iteration 15300 (12.6562 iter/s, 7.90125s/100 iters), loss = 0.00378249
I1024 16:44:25.470599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:44:25.470599  1960 solver.cpp:237]     Train net output #1: loss = 0.00378252 (* 1 = 0.00378252 loss)
I1024 16:44:25.470599  1960 sgd_solver.cpp:105] Iteration 15300, lr = 0.001
I1024 16:44:33.379600  1960 solver.cpp:218] Iteration 15400 (12.6457 iter/s, 7.90785s/100 iters), loss = 0.00293986
I1024 16:44:33.379600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:44:33.379600  1960 solver.cpp:237]     Train net output #1: loss = 0.0029399 (* 1 = 0.0029399 loss)
I1024 16:44:33.379600  1960 sgd_solver.cpp:105] Iteration 15400, lr = 0.001
I1024 16:44:41.282599  1960 solver.cpp:218] Iteration 15500 (12.6542 iter/s, 7.90248s/100 iters), loss = 0.00720058
I1024 16:44:41.282599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:44:41.282599  1960 solver.cpp:237]     Train net output #1: loss = 0.00720062 (* 1 = 0.00720062 loss)
I1024 16:44:41.282599  1960 sgd_solver.cpp:105] Iteration 15500, lr = 0.001
I1024 16:44:48.795600 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:44:49.109601  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_15600.caffemodel
I1024 16:44:49.145601  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_15600.solverstate
I1024 16:44:49.161600  1960 solver.cpp:330] Iteration 15600, Testing net (#0)
I1024 16:44:49.161600  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:44:51.056601  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:44:51.133599  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9974
I1024 16:44:51.134600  1960 solver.cpp:397]     Test net output #1: loss = 0.00946709 (* 1 = 0.00946709 loss)
I1024 16:44:51.210599  1960 solver.cpp:218] Iteration 15600 (10.0724 iter/s, 9.92816s/100 iters), loss = 0.0104184
I1024 16:44:51.210599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:44:51.210599  1960 solver.cpp:237]     Train net output #1: loss = 0.0104185 (* 1 = 0.0104185 loss)
I1024 16:44:51.210599  1960 sgd_solver.cpp:105] Iteration 15600, lr = 0.001
I1024 16:44:59.113600  1960 solver.cpp:218] Iteration 15700 (12.654 iter/s, 7.90263s/100 iters), loss = 0.00435785
I1024 16:44:59.113600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:44:59.113600  1960 solver.cpp:237]     Train net output #1: loss = 0.0043579 (* 1 = 0.0043579 loss)
I1024 16:44:59.113600  1960 sgd_solver.cpp:105] Iteration 15700, lr = 0.001
I1024 16:45:07.014600  1960 solver.cpp:218] Iteration 15800 (12.6579 iter/s, 7.90022s/100 iters), loss = 0.00985145
I1024 16:45:07.014600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:45:07.014600  1960 solver.cpp:237]     Train net output #1: loss = 0.00985151 (* 1 = 0.00985151 loss)
I1024 16:45:07.014600  1960 sgd_solver.cpp:105] Iteration 15800, lr = 0.001
I1024 16:45:14.917599  1960 solver.cpp:218] Iteration 15900 (12.6542 iter/s, 7.9025s/100 iters), loss = 0.00426138
I1024 16:45:14.917599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:45:14.917599  1960 solver.cpp:237]     Train net output #1: loss = 0.00426144 (* 1 = 0.00426144 loss)
I1024 16:45:14.917599  1960 sgd_solver.cpp:105] Iteration 15900, lr = 0.001
I1024 16:45:22.817600  1960 solver.cpp:218] Iteration 16000 (12.6596 iter/s, 7.89912s/100 iters), loss = 0.00423236
I1024 16:45:22.817600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:45:22.817600  1960 solver.cpp:237]     Train net output #1: loss = 0.00423242 (* 1 = 0.00423242 loss)
I1024 16:45:22.817600  1960 sgd_solver.cpp:105] Iteration 16000, lr = 0.001
I1024 16:45:30.723599  1960 solver.cpp:218] Iteration 16100 (12.6481 iter/s, 7.90632s/100 iters), loss = 0.00600218
I1024 16:45:30.723599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:45:30.723599  1960 solver.cpp:237]     Train net output #1: loss = 0.00600223 (* 1 = 0.00600223 loss)
I1024 16:45:30.723599  1960 sgd_solver.cpp:105] Iteration 16100, lr = 0.001
I1024 16:45:38.234601 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:45:38.550601  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_16200.caffemodel
I1024 16:45:38.583600  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_16200.solverstate
I1024 16:45:38.600600  1960 solver.cpp:330] Iteration 16200, Testing net (#0)
I1024 16:45:38.600600  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:45:40.495604  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:45:40.573616  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9974
I1024 16:45:40.573616  1960 solver.cpp:397]     Test net output #1: loss = 0.00961578 (* 1 = 0.00961578 loss)
I1024 16:45:40.649600  1960 solver.cpp:218] Iteration 16200 (10.0751 iter/s, 9.92546s/100 iters), loss = 0.00649214
I1024 16:45:40.649600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:45:40.649600  1960 solver.cpp:237]     Train net output #1: loss = 0.0064922 (* 1 = 0.0064922 loss)
I1024 16:45:40.649600  1960 sgd_solver.cpp:105] Iteration 16200, lr = 0.001
I1024 16:45:48.546602  1960 solver.cpp:218] Iteration 16300 (12.6638 iter/s, 7.89653s/100 iters), loss = 0.00678167
I1024 16:45:48.546602  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:45:48.546602  1960 solver.cpp:237]     Train net output #1: loss = 0.00678171 (* 1 = 0.00678171 loss)
I1024 16:45:48.547601  1960 sgd_solver.cpp:105] Iteration 16300, lr = 0.001
I1024 16:45:56.442600  1960 solver.cpp:218] Iteration 16400 (12.6654 iter/s, 7.89555s/100 iters), loss = 0.0190179
I1024 16:45:56.443599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:45:56.443599  1960 solver.cpp:237]     Train net output #1: loss = 0.019018 (* 1 = 0.019018 loss)
I1024 16:45:56.443599  1960 sgd_solver.cpp:105] Iteration 16400, lr = 0.001
I1024 16:46:04.340600  1960 solver.cpp:218] Iteration 16500 (12.6631 iter/s, 7.89697s/100 iters), loss = 0.012752
I1024 16:46:04.340600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:46:04.340600  1960 solver.cpp:237]     Train net output #1: loss = 0.0127521 (* 1 = 0.0127521 loss)
I1024 16:46:04.340600  1960 sgd_solver.cpp:105] Iteration 16500, lr = 0.001
I1024 16:46:12.235599  1960 solver.cpp:218] Iteration 16600 (12.6668 iter/s, 7.89466s/100 iters), loss = 0.00662399
I1024 16:46:12.235599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:46:12.235599  1960 solver.cpp:237]     Train net output #1: loss = 0.00662404 (* 1 = 0.00662404 loss)
I1024 16:46:12.235599  1960 sgd_solver.cpp:105] Iteration 16600, lr = 0.001
I1024 16:46:20.135599  1960 solver.cpp:218] Iteration 16700 (12.6585 iter/s, 7.8998s/100 iters), loss = 0.00328779
I1024 16:46:20.135599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:46:20.135599  1960 solver.cpp:237]     Train net output #1: loss = 0.00328784 (* 1 = 0.00328784 loss)
I1024 16:46:20.135599  1960 sgd_solver.cpp:105] Iteration 16700, lr = 0.001
I1024 16:46:27.651598 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:46:27.965607  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_16800.caffemodel
I1024 16:46:28.001603  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_16800.solverstate
I1024 16:46:28.017616  1960 solver.cpp:330] Iteration 16800, Testing net (#0)
I1024 16:46:28.017616  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:46:29.913601  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:46:29.991600  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 16:46:29.991600  1960 solver.cpp:397]     Test net output #1: loss = 0.00958446 (* 1 = 0.00958446 loss)
I1024 16:46:30.067600  1960 solver.cpp:218] Iteration 16800 (10.0692 iter/s, 9.93131s/100 iters), loss = 0.00249355
I1024 16:46:30.067600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:46:30.067600  1960 solver.cpp:237]     Train net output #1: loss = 0.0024936 (* 1 = 0.0024936 loss)
I1024 16:46:30.067600  1960 sgd_solver.cpp:105] Iteration 16800, lr = 0.001
I1024 16:46:37.969599  1960 solver.cpp:218] Iteration 16900 (12.656 iter/s, 7.90136s/100 iters), loss = 0.0125725
I1024 16:46:37.969599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:46:37.969599  1960 solver.cpp:237]     Train net output #1: loss = 0.0125725 (* 1 = 0.0125725 loss)
I1024 16:46:37.969599  1960 sgd_solver.cpp:105] Iteration 16900, lr = 0.001
I1024 16:46:45.873600  1960 solver.cpp:218] Iteration 17000 (12.6533 iter/s, 7.90305s/100 iters), loss = 0.00689466
I1024 16:46:45.873600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:46:45.873600  1960 solver.cpp:237]     Train net output #1: loss = 0.00689471 (* 1 = 0.00689471 loss)
I1024 16:46:45.873600  1960 sgd_solver.cpp:105] Iteration 17000, lr = 0.001
I1024 16:46:53.771600  1960 solver.cpp:218] Iteration 17100 (12.6612 iter/s, 7.89813s/100 iters), loss = 0.00314175
I1024 16:46:53.771600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:46:53.771600  1960 solver.cpp:237]     Train net output #1: loss = 0.0031418 (* 1 = 0.0031418 loss)
I1024 16:46:53.771600  1960 sgd_solver.cpp:105] Iteration 17100, lr = 0.001
I1024 16:47:01.675599  1960 solver.cpp:218] Iteration 17200 (12.6532 iter/s, 7.90311s/100 iters), loss = 0.00638278
I1024 16:47:01.675599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:47:01.675599  1960 solver.cpp:237]     Train net output #1: loss = 0.00638283 (* 1 = 0.00638283 loss)
I1024 16:47:01.675599  1960 sgd_solver.cpp:105] Iteration 17200, lr = 0.001
I1024 16:47:09.580600  1960 solver.cpp:218] Iteration 17300 (12.6517 iter/s, 7.90408s/100 iters), loss = 0.0109344
I1024 16:47:09.580600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:47:09.580600  1960 solver.cpp:237]     Train net output #1: loss = 0.0109344 (* 1 = 0.0109344 loss)
I1024 16:47:09.580600  1960 sgd_solver.cpp:105] Iteration 17300, lr = 0.001
I1024 16:47:17.096601 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:47:17.409600  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_17400.caffemodel
I1024 16:47:17.443600  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_17400.solverstate
I1024 16:47:17.459600  1960 solver.cpp:330] Iteration 17400, Testing net (#0)
I1024 16:47:17.459600  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:47:19.353601  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:47:19.431601  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 16:47:19.431601  1960 solver.cpp:397]     Test net output #1: loss = 0.00937219 (* 1 = 0.00937219 loss)
I1024 16:47:19.507606  1960 solver.cpp:218] Iteration 17400 (10.0736 iter/s, 9.92698s/100 iters), loss = 0.002303
I1024 16:47:19.507606  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:47:19.507606  1960 solver.cpp:237]     Train net output #1: loss = 0.00230307 (* 1 = 0.00230307 loss)
I1024 16:47:19.507606  1960 sgd_solver.cpp:105] Iteration 17400, lr = 0.001
I1024 16:47:27.414599  1960 solver.cpp:218] Iteration 17500 (12.648 iter/s, 7.90636s/100 iters), loss = 0.0113609
I1024 16:47:27.414599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:47:27.414599  1960 solver.cpp:237]     Train net output #1: loss = 0.011361 (* 1 = 0.011361 loss)
I1024 16:47:27.414599  1960 sgd_solver.cpp:105] Iteration 17500, lr = 0.001
I1024 16:47:35.318608  1960 solver.cpp:218] Iteration 17600 (12.6523 iter/s, 7.9037s/100 iters), loss = 0.007245
I1024 16:47:35.318608  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:47:35.318608  1960 solver.cpp:237]     Train net output #1: loss = 0.00724507 (* 1 = 0.00724507 loss)
I1024 16:47:35.318608  1960 sgd_solver.cpp:105] Iteration 17600, lr = 0.001
I1024 16:47:43.222601  1960 solver.cpp:218] Iteration 17700 (12.6524 iter/s, 7.90365s/100 iters), loss = 0.0143008
I1024 16:47:43.222601  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:47:43.222601  1960 solver.cpp:237]     Train net output #1: loss = 0.0143008 (* 1 = 0.0143008 loss)
I1024 16:47:43.222601  1960 sgd_solver.cpp:105] Iteration 17700, lr = 0.001
I1024 16:47:51.124599  1960 solver.cpp:218] Iteration 17800 (12.6561 iter/s, 7.90132s/100 iters), loss = 0.00398581
I1024 16:47:51.124599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:47:51.124599  1960 solver.cpp:237]     Train net output #1: loss = 0.00398588 (* 1 = 0.00398588 loss)
I1024 16:47:51.124599  1960 sgd_solver.cpp:105] Iteration 17800, lr = 0.001
I1024 16:47:59.027600  1960 solver.cpp:218] Iteration 17900 (12.6545 iter/s, 7.90231s/100 iters), loss = 0.00354607
I1024 16:47:59.027600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:47:59.027600  1960 solver.cpp:237]     Train net output #1: loss = 0.00354613 (* 1 = 0.00354613 loss)
I1024 16:47:59.027600  1960 sgd_solver.cpp:105] Iteration 17900, lr = 0.001
I1024 16:48:06.544600 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:48:06.858602  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_18000.caffemodel
I1024 16:48:06.892603  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_18000.solverstate
I1024 16:48:06.910604  1960 solver.cpp:330] Iteration 18000, Testing net (#0)
I1024 16:48:06.910604  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:48:08.806604  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:48:08.884619  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 16:48:08.884619  1960 solver.cpp:397]     Test net output #1: loss = 0.00933104 (* 1 = 0.00933104 loss)
I1024 16:48:08.960615  1960 solver.cpp:218] Iteration 18000 (10.0675 iter/s, 9.93293s/100 iters), loss = 0.00530419
I1024 16:48:08.960615  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:48:08.961621  1960 solver.cpp:237]     Train net output #1: loss = 0.00530427 (* 1 = 0.00530427 loss)
I1024 16:48:08.961621  1960 sgd_solver.cpp:105] Iteration 18000, lr = 0.001
I1024 16:48:16.859599  1960 solver.cpp:218] Iteration 18100 (12.6615 iter/s, 7.89793s/100 iters), loss = 0.0156028
I1024 16:48:16.859599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:48:16.859599  1960 solver.cpp:237]     Train net output #1: loss = 0.0156028 (* 1 = 0.0156028 loss)
I1024 16:48:16.859599  1960 sgd_solver.cpp:105] Iteration 18100, lr = 0.001
I1024 16:48:24.765599  1960 solver.cpp:218] Iteration 18200 (12.6484 iter/s, 7.90612s/100 iters), loss = 0.00688922
I1024 16:48:24.766599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:48:24.766599  1960 solver.cpp:237]     Train net output #1: loss = 0.00688928 (* 1 = 0.00688928 loss)
I1024 16:48:24.766599  1960 sgd_solver.cpp:105] Iteration 18200, lr = 0.001
I1024 16:48:32.674599  1960 solver.cpp:218] Iteration 18300 (12.6453 iter/s, 7.9081s/100 iters), loss = 0.0205813
I1024 16:48:32.674599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:48:32.674599  1960 solver.cpp:237]     Train net output #1: loss = 0.0205814 (* 1 = 0.0205814 loss)
I1024 16:48:32.674599  1960 sgd_solver.cpp:105] Iteration 18300, lr = 0.001
I1024 16:48:40.573598  1960 solver.cpp:218] Iteration 18400 (12.6606 iter/s, 7.89852s/100 iters), loss = 0.00284497
I1024 16:48:40.573598  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:48:40.573598  1960 solver.cpp:237]     Train net output #1: loss = 0.00284503 (* 1 = 0.00284503 loss)
I1024 16:48:40.573598  1960 sgd_solver.cpp:105] Iteration 18400, lr = 0.001
I1024 16:48:48.472600  1960 solver.cpp:218] Iteration 18500 (12.6605 iter/s, 7.89856s/100 iters), loss = 0.0115925
I1024 16:48:48.472600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:48:48.472600  1960 solver.cpp:237]     Train net output #1: loss = 0.0115925 (* 1 = 0.0115925 loss)
I1024 16:48:48.472600  1960 sgd_solver.cpp:105] Iteration 18500, lr = 0.001
I1024 16:48:55.978601 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:48:56.292600  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_18600.caffemodel
I1024 16:48:56.324600  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_18600.solverstate
I1024 16:48:56.340600  1960 solver.cpp:330] Iteration 18600, Testing net (#0)
I1024 16:48:56.340600  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:48:58.234601  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:48:58.312611  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9972
I1024 16:48:58.312611  1960 solver.cpp:397]     Test net output #1: loss = 0.00942162 (* 1 = 0.00942162 loss)
I1024 16:48:58.388599  1960 solver.cpp:218] Iteration 18600 (10.0852 iter/s, 9.91551s/100 iters), loss = 0.00981773
I1024 16:48:58.388599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:48:58.388599  1960 solver.cpp:237]     Train net output #1: loss = 0.00981778 (* 1 = 0.00981778 loss)
I1024 16:48:58.388599  1960 sgd_solver.cpp:105] Iteration 18600, lr = 0.001
I1024 16:49:06.289599  1960 solver.cpp:218] Iteration 18700 (12.6567 iter/s, 7.90094s/100 iters), loss = 0.0104095
I1024 16:49:06.290601  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:49:06.290601  1960 solver.cpp:237]     Train net output #1: loss = 0.0104096 (* 1 = 0.0104096 loss)
I1024 16:49:06.290601  1960 sgd_solver.cpp:105] Iteration 18700, lr = 0.001
I1024 16:49:14.199599  1960 solver.cpp:218] Iteration 18800 (12.643 iter/s, 7.90953s/100 iters), loss = 0.00843861
I1024 16:49:14.200599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:49:14.200599  1960 solver.cpp:237]     Train net output #1: loss = 0.00843866 (* 1 = 0.00843866 loss)
I1024 16:49:14.200599  1960 sgd_solver.cpp:105] Iteration 18800, lr = 0.001
I1024 16:49:22.098599  1960 solver.cpp:218] Iteration 18900 (12.6606 iter/s, 7.89849s/100 iters), loss = 0.0205027
I1024 16:49:22.099599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:49:22.099599  1960 solver.cpp:237]     Train net output #1: loss = 0.0205028 (* 1 = 0.0205028 loss)
I1024 16:49:22.099599  1960 sgd_solver.cpp:105] Iteration 18900, lr = 0.001
I1024 16:49:30.005599  1960 solver.cpp:218] Iteration 19000 (12.6486 iter/s, 7.90604s/100 iters), loss = 0.00565243
I1024 16:49:30.005599  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:49:30.005599  1960 solver.cpp:237]     Train net output #1: loss = 0.00565247 (* 1 = 0.00565247 loss)
I1024 16:49:30.005599  1960 sgd_solver.cpp:105] Iteration 19000, lr = 0.001
I1024 16:49:37.912600  1960 solver.cpp:218] Iteration 19100 (12.6485 iter/s, 7.90607s/100 iters), loss = 0.0062621
I1024 16:49:37.912600  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:49:37.912600  1960 solver.cpp:237]     Train net output #1: loss = 0.00626215 (* 1 = 0.00626215 loss)
I1024 16:49:37.912600  1960 sgd_solver.cpp:105] Iteration 19100, lr = 0.001
I1024 16:49:45.422652 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:49:45.737651  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_19200.caffemodel
I1024 16:49:45.772653  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_19200.solverstate
I1024 16:49:45.789652  1960 solver.cpp:330] Iteration 19200, Testing net (#0)
I1024 16:49:45.789652  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:49:47.685665  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:49:47.763669  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 16:49:47.763669  1960 solver.cpp:397]     Test net output #1: loss = 0.00927603 (* 1 = 0.00927603 loss)
I1024 16:49:47.839651  1960 solver.cpp:218] Iteration 19200 (10.0734 iter/s, 9.9271s/100 iters), loss = 0.00738548
I1024 16:49:47.839651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:49:47.839651  1960 solver.cpp:237]     Train net output #1: loss = 0.00738552 (* 1 = 0.00738552 loss)
I1024 16:49:47.839651  1960 sgd_solver.cpp:105] Iteration 19200, lr = 0.001
I1024 16:49:55.746649  1960 solver.cpp:218] Iteration 19300 (12.6472 iter/s, 7.90686s/100 iters), loss = 0.00962559
I1024 16:49:55.746649  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:49:55.746649  1960 solver.cpp:237]     Train net output #1: loss = 0.00962564 (* 1 = 0.00962564 loss)
I1024 16:49:55.746649  1960 sgd_solver.cpp:105] Iteration 19300, lr = 0.001
I1024 16:50:03.647651  1960 solver.cpp:218] Iteration 19400 (12.6574 iter/s, 7.90054s/100 iters), loss = 0.0230925
I1024 16:50:03.647651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:50:03.647651  1960 solver.cpp:237]     Train net output #1: loss = 0.0230926 (* 1 = 0.0230926 loss)
I1024 16:50:03.648651  1960 sgd_solver.cpp:105] Iteration 19400, lr = 0.001
I1024 16:50:11.547652  1960 solver.cpp:218] Iteration 19500 (12.6592 iter/s, 7.89942s/100 iters), loss = 0.0106172
I1024 16:50:11.547652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:50:11.547652  1960 solver.cpp:237]     Train net output #1: loss = 0.0106172 (* 1 = 0.0106172 loss)
I1024 16:50:11.547652  1960 sgd_solver.cpp:105] Iteration 19500, lr = 0.001
I1024 16:50:19.451650  1960 solver.cpp:218] Iteration 19600 (12.6527 iter/s, 7.90345s/100 iters), loss = 0.00334993
I1024 16:50:19.451650  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:50:19.451650  1960 solver.cpp:237]     Train net output #1: loss = 0.00334998 (* 1 = 0.00334998 loss)
I1024 16:50:19.451650  1960 sgd_solver.cpp:105] Iteration 19600, lr = 0.001
I1024 16:50:27.352651  1960 solver.cpp:218] Iteration 19700 (12.6572 iter/s, 7.90065s/100 iters), loss = 0.00497687
I1024 16:50:27.352651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:50:27.352651  1960 solver.cpp:237]     Train net output #1: loss = 0.00497692 (* 1 = 0.00497692 loss)
I1024 16:50:27.352651  1960 sgd_solver.cpp:105] Iteration 19700, lr = 0.001
I1024 16:50:34.863652 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:50:35.178653  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_19800.caffemodel
I1024 16:50:35.211654  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_19800.solverstate
I1024 16:50:35.227665  1960 solver.cpp:330] Iteration 19800, Testing net (#0)
I1024 16:50:35.227665  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:50:37.124653  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:50:37.202652  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 16:50:37.202652  1960 solver.cpp:397]     Test net output #1: loss = 0.00921491 (* 1 = 0.00921491 loss)
I1024 16:50:37.278651  1960 solver.cpp:218] Iteration 19800 (10.0752 iter/s, 9.92534s/100 iters), loss = 0.015934
I1024 16:50:37.278651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:50:37.278651  1960 solver.cpp:237]     Train net output #1: loss = 0.0159341 (* 1 = 0.0159341 loss)
I1024 16:50:37.278651  1960 sgd_solver.cpp:105] Iteration 19800, lr = 0.001
I1024 16:50:45.182652  1960 solver.cpp:218] Iteration 19900 (12.6538 iter/s, 7.90279s/100 iters), loss = 0.0116278
I1024 16:50:45.182652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:50:45.182652  1960 solver.cpp:237]     Train net output #1: loss = 0.0116279 (* 1 = 0.0116279 loss)
I1024 16:50:45.182652  1960 sgd_solver.cpp:105] Iteration 19900, lr = 0.001
I1024 16:50:53.078658  1960 solver.cpp:218] Iteration 20000 (12.6641 iter/s, 7.89632s/100 iters), loss = 0.0115055
I1024 16:50:53.079651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:50:53.079651  1960 solver.cpp:237]     Train net output #1: loss = 0.0115056 (* 1 = 0.0115056 loss)
I1024 16:50:53.079651  1960 sgd_solver.cpp:105] Iteration 20000, lr = 0.001
I1024 16:51:00.974651  1960 solver.cpp:218] Iteration 20100 (12.6664 iter/s, 7.89489s/100 iters), loss = 0.00448059
I1024 16:51:00.974651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:51:00.974651  1960 solver.cpp:237]     Train net output #1: loss = 0.00448065 (* 1 = 0.00448065 loss)
I1024 16:51:00.974651  1960 sgd_solver.cpp:105] Iteration 20100, lr = 0.001
I1024 16:51:08.881651  1960 solver.cpp:218] Iteration 20200 (12.648 iter/s, 7.90639s/100 iters), loss = 0.00290376
I1024 16:51:08.881651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:51:08.881651  1960 solver.cpp:237]     Train net output #1: loss = 0.00290382 (* 1 = 0.00290382 loss)
I1024 16:51:08.881651  1960 sgd_solver.cpp:105] Iteration 20200, lr = 0.001
I1024 16:51:16.783651  1960 solver.cpp:218] Iteration 20300 (12.6553 iter/s, 7.90185s/100 iters), loss = 0.00710374
I1024 16:51:16.783651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:51:16.783651  1960 solver.cpp:237]     Train net output #1: loss = 0.00710379 (* 1 = 0.00710379 loss)
I1024 16:51:16.783651  1960 sgd_solver.cpp:105] Iteration 20300, lr = 0.001
I1024 16:51:24.292652 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:51:24.607668  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_20400.caffemodel
I1024 16:51:24.647670  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_20400.solverstate
I1024 16:51:24.663669  1960 solver.cpp:330] Iteration 20400, Testing net (#0)
I1024 16:51:24.663669  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:51:26.559653  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:51:26.637653  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 16:51:26.637653  1960 solver.cpp:397]     Test net output #1: loss = 0.00921933 (* 1 = 0.00921933 loss)
I1024 16:51:26.713651  1960 solver.cpp:218] Iteration 20400 (10.0708 iter/s, 9.92972s/100 iters), loss = 0.00915662
I1024 16:51:26.713651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:51:26.713651  1960 solver.cpp:237]     Train net output #1: loss = 0.00915669 (* 1 = 0.00915669 loss)
I1024 16:51:26.713651  1960 sgd_solver.cpp:105] Iteration 20400, lr = 0.001
I1024 16:51:34.614650  1960 solver.cpp:218] Iteration 20500 (12.6588 iter/s, 7.89964s/100 iters), loss = 0.00823907
I1024 16:51:34.614650  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:51:34.614650  1960 solver.cpp:237]     Train net output #1: loss = 0.00823913 (* 1 = 0.00823913 loss)
I1024 16:51:34.614650  1960 sgd_solver.cpp:105] Iteration 20500, lr = 0.001
I1024 16:51:42.513655  1960 solver.cpp:218] Iteration 20600 (12.6593 iter/s, 7.89936s/100 iters), loss = 0.0079264
I1024 16:51:42.513655  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:51:42.513655  1960 solver.cpp:237]     Train net output #1: loss = 0.00792646 (* 1 = 0.00792646 loss)
I1024 16:51:42.513655  1960 sgd_solver.cpp:105] Iteration 20600, lr = 0.001
I1024 16:51:50.413651  1960 solver.cpp:218] Iteration 20700 (12.6602 iter/s, 7.89879s/100 iters), loss = 0.0188202
I1024 16:51:50.413651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:51:50.413651  1960 solver.cpp:237]     Train net output #1: loss = 0.0188203 (* 1 = 0.0188203 loss)
I1024 16:51:50.413651  1960 sgd_solver.cpp:105] Iteration 20700, lr = 0.001
I1024 16:51:58.313652  1960 solver.cpp:218] Iteration 20800 (12.6593 iter/s, 7.8993s/100 iters), loss = 0.00296918
I1024 16:51:58.313652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:51:58.313652  1960 solver.cpp:237]     Train net output #1: loss = 0.00296924 (* 1 = 0.00296924 loss)
I1024 16:51:58.313652  1960 sgd_solver.cpp:105] Iteration 20800, lr = 0.001
I1024 16:52:06.212651  1960 solver.cpp:218] Iteration 20900 (12.6597 iter/s, 7.89905s/100 iters), loss = 0.0052068
I1024 16:52:06.212651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:52:06.212651  1960 solver.cpp:237]     Train net output #1: loss = 0.00520686 (* 1 = 0.00520686 loss)
I1024 16:52:06.212651  1960 sgd_solver.cpp:105] Iteration 20900, lr = 0.001
I1024 16:52:13.721652 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:52:14.035651  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_21000.caffemodel
I1024 16:52:14.067652  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_21000.solverstate
I1024 16:52:14.084652  1960 solver.cpp:330] Iteration 21000, Testing net (#0)
I1024 16:52:14.084652  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:52:15.979660  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:52:16.057667  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9974
I1024 16:52:16.057667  1960 solver.cpp:397]     Test net output #1: loss = 0.00921638 (* 1 = 0.00921638 loss)
I1024 16:52:16.134654  1960 solver.cpp:218] Iteration 21000 (10.0797 iter/s, 9.92091s/100 iters), loss = 0.00367577
I1024 16:52:16.134654  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:52:16.134654  1960 solver.cpp:237]     Train net output #1: loss = 0.00367582 (* 1 = 0.00367582 loss)
I1024 16:52:16.134654  1960 sgd_solver.cpp:105] Iteration 21000, lr = 0.001
I1024 16:52:24.037665  1960 solver.cpp:218] Iteration 21100 (12.6527 iter/s, 7.90344s/100 iters), loss = 0.00746612
I1024 16:52:24.037665  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:52:24.037665  1960 solver.cpp:237]     Train net output #1: loss = 0.00746618 (* 1 = 0.00746618 loss)
I1024 16:52:24.037665  1960 sgd_solver.cpp:105] Iteration 21100, lr = 0.001
I1024 16:52:31.944651  1960 solver.cpp:218] Iteration 21200 (12.6483 iter/s, 7.90623s/100 iters), loss = 0.0216912
I1024 16:52:31.944651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:52:31.944651  1960 solver.cpp:237]     Train net output #1: loss = 0.0216913 (* 1 = 0.0216913 loss)
I1024 16:52:31.944651  1960 sgd_solver.cpp:105] Iteration 21200, lr = 0.001
I1024 16:52:39.841651  1960 solver.cpp:218] Iteration 21300 (12.6644 iter/s, 7.89617s/100 iters), loss = 0.00820652
I1024 16:52:39.841651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:52:39.841651  1960 solver.cpp:237]     Train net output #1: loss = 0.00820659 (* 1 = 0.00820659 loss)
I1024 16:52:39.841651  1960 sgd_solver.cpp:105] Iteration 21300, lr = 0.001
I1024 16:52:47.740653  1960 solver.cpp:218] Iteration 21400 (12.6596 iter/s, 7.89912s/100 iters), loss = 0.00343563
I1024 16:52:47.740653  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:52:47.740653  1960 solver.cpp:237]     Train net output #1: loss = 0.0034357 (* 1 = 0.0034357 loss)
I1024 16:52:47.740653  1960 sgd_solver.cpp:105] Iteration 21400, lr = 0.001
I1024 16:52:55.640650  1960 solver.cpp:218] Iteration 21500 (12.6593 iter/s, 7.89936s/100 iters), loss = 0.00380104
I1024 16:52:55.640650  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:52:55.640650  1960 solver.cpp:237]     Train net output #1: loss = 0.0038011 (* 1 = 0.0038011 loss)
I1024 16:52:55.640650  1960 sgd_solver.cpp:105] Iteration 21500, lr = 0.001
I1024 16:53:03.149655 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:53:03.463651  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_21600.caffemodel
I1024 16:53:03.498652  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_21600.solverstate
I1024 16:53:03.515652  1960 solver.cpp:330] Iteration 21600, Testing net (#0)
I1024 16:53:03.515652  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:53:05.409652  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:53:05.487651  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9974
I1024 16:53:05.487651  1960 solver.cpp:397]     Test net output #1: loss = 0.00927383 (* 1 = 0.00927383 loss)
I1024 16:53:05.563652  1960 solver.cpp:218] Iteration 21600 (10.0781 iter/s, 9.92253s/100 iters), loss = 0.00938638
I1024 16:53:05.563652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:53:05.563652  1960 solver.cpp:237]     Train net output #1: loss = 0.00938645 (* 1 = 0.00938645 loss)
I1024 16:53:05.563652  1960 sgd_solver.cpp:105] Iteration 21600, lr = 0.001
I1024 16:53:13.471652  1960 solver.cpp:218] Iteration 21700 (12.647 iter/s, 7.90701s/100 iters), loss = 0.00676268
I1024 16:53:13.471652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:53:13.471652  1960 solver.cpp:237]     Train net output #1: loss = 0.00676275 (* 1 = 0.00676275 loss)
I1024 16:53:13.471652  1960 sgd_solver.cpp:105] Iteration 21700, lr = 0.001
I1024 16:53:21.372651  1960 solver.cpp:218] Iteration 21800 (12.6572 iter/s, 7.90063s/100 iters), loss = 0.0108033
I1024 16:53:21.372651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:53:21.372651  1960 solver.cpp:237]     Train net output #1: loss = 0.0108034 (* 1 = 0.0108034 loss)
I1024 16:53:21.372651  1960 sgd_solver.cpp:105] Iteration 21800, lr = 0.001
I1024 16:53:29.277631  1960 solver.cpp:218] Iteration 21900 (12.6512 iter/s, 7.90437s/100 iters), loss = 0.00828428
I1024 16:53:29.277631  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:53:29.277631  1960 solver.cpp:237]     Train net output #1: loss = 0.00828435 (* 1 = 0.00828435 loss)
I1024 16:53:29.277631  1960 sgd_solver.cpp:105] Iteration 21900, lr = 0.001
I1024 16:53:37.176651  1960 solver.cpp:218] Iteration 22000 (12.6597 iter/s, 7.8991s/100 iters), loss = 0.00686159
I1024 16:53:37.176651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:53:37.176651  1960 solver.cpp:237]     Train net output #1: loss = 0.00686167 (* 1 = 0.00686167 loss)
I1024 16:53:37.176651  1960 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 3
I1024 16:53:37.176651  1960 sgd_solver.cpp:105] Iteration 22000, lr = 0.0001
I1024 16:53:45.080651  1960 solver.cpp:218] Iteration 22100 (12.6524 iter/s, 7.90364s/100 iters), loss = 0.00684213
I1024 16:53:45.080651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:53:45.080651  1960 solver.cpp:237]     Train net output #1: loss = 0.00684221 (* 1 = 0.00684221 loss)
I1024 16:53:45.081652  1960 sgd_solver.cpp:105] Iteration 22100, lr = 0.0001
I1024 16:53:52.597652 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:53:52.912652  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_22200.caffemodel
I1024 16:53:52.945652  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_22200.solverstate
I1024 16:53:52.961652  1960 solver.cpp:330] Iteration 22200, Testing net (#0)
I1024 16:53:52.961652  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:53:54.856653  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:53:54.934650  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 16:53:54.934650  1960 solver.cpp:397]     Test net output #1: loss = 0.00929782 (* 1 = 0.00929782 loss)
I1024 16:53:55.011651  1960 solver.cpp:218] Iteration 22200 (10.0708 iter/s, 9.92969s/100 iters), loss = 0.0055673
I1024 16:53:55.011651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:53:55.011651  1960 solver.cpp:237]     Train net output #1: loss = 0.00556737 (* 1 = 0.00556737 loss)
I1024 16:53:55.011651  1960 sgd_solver.cpp:105] Iteration 22200, lr = 0.0001
I1024 16:54:02.911651  1960 solver.cpp:218] Iteration 22300 (12.658 iter/s, 7.90011s/100 iters), loss = 0.0056242
I1024 16:54:02.911651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:54:02.911651  1960 solver.cpp:237]     Train net output #1: loss = 0.00562427 (* 1 = 0.00562427 loss)
I1024 16:54:02.911651  1960 sgd_solver.cpp:105] Iteration 22300, lr = 0.0001
I1024 16:54:10.816651  1960 solver.cpp:218] Iteration 22400 (12.6505 iter/s, 7.9048s/100 iters), loss = 0.011574
I1024 16:54:10.817651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:54:10.817651  1960 solver.cpp:237]     Train net output #1: loss = 0.0115741 (* 1 = 0.0115741 loss)
I1024 16:54:10.817651  1960 sgd_solver.cpp:105] Iteration 22400, lr = 0.0001
I1024 16:54:18.721649  1960 solver.cpp:218] Iteration 22500 (12.651 iter/s, 7.9045s/100 iters), loss = 0.0028053
I1024 16:54:18.722651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:54:18.722651  1960 solver.cpp:237]     Train net output #1: loss = 0.00280537 (* 1 = 0.00280537 loss)
I1024 16:54:18.722651  1960 sgd_solver.cpp:105] Iteration 22500, lr = 0.0001
I1024 16:54:26.621650  1960 solver.cpp:218] Iteration 22600 (12.6594 iter/s, 7.89926s/100 iters), loss = 0.00356358
I1024 16:54:26.621650  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:54:26.621650  1960 solver.cpp:237]     Train net output #1: loss = 0.00356364 (* 1 = 0.00356364 loss)
I1024 16:54:26.621650  1960 sgd_solver.cpp:105] Iteration 22600, lr = 0.0001
I1024 16:54:34.524652  1960 solver.cpp:218] Iteration 22700 (12.6551 iter/s, 7.90195s/100 iters), loss = 0.00166451
I1024 16:54:34.524652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:54:34.524652  1960 solver.cpp:237]     Train net output #1: loss = 0.00166457 (* 1 = 0.00166457 loss)
I1024 16:54:34.524652  1960 sgd_solver.cpp:105] Iteration 22700, lr = 0.0001
I1024 16:54:42.031652 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:54:42.346652  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_22800.caffemodel
I1024 16:54:42.379652  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_22800.solverstate
I1024 16:54:42.396652  1960 solver.cpp:330] Iteration 22800, Testing net (#0)
I1024 16:54:42.396652  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:54:44.291652  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:54:44.369652  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9974
I1024 16:54:44.369652  1960 solver.cpp:397]     Test net output #1: loss = 0.00926946 (* 1 = 0.00926946 loss)
I1024 16:54:44.446651  1960 solver.cpp:218] Iteration 22800 (10.0794 iter/s, 9.92121s/100 iters), loss = 0.00715876
I1024 16:54:44.446651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:54:44.446651  1960 solver.cpp:237]     Train net output #1: loss = 0.00715881 (* 1 = 0.00715881 loss)
I1024 16:54:44.446651  1960 sgd_solver.cpp:105] Iteration 22800, lr = 0.0001
I1024 16:54:52.349651  1960 solver.cpp:218] Iteration 22900 (12.6528 iter/s, 7.90339s/100 iters), loss = 0.0102212
I1024 16:54:52.349651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:54:52.349651  1960 solver.cpp:237]     Train net output #1: loss = 0.0102213 (* 1 = 0.0102213 loss)
I1024 16:54:52.349651  1960 sgd_solver.cpp:105] Iteration 22900, lr = 0.0001
I1024 16:55:00.254652  1960 solver.cpp:218] Iteration 23000 (12.6519 iter/s, 7.90395s/100 iters), loss = 0.00913108
I1024 16:55:00.254652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:55:00.254652  1960 solver.cpp:237]     Train net output #1: loss = 0.00913113 (* 1 = 0.00913113 loss)
I1024 16:55:00.254652  1960 sgd_solver.cpp:105] Iteration 23000, lr = 0.0001
I1024 16:55:08.157650  1960 solver.cpp:218] Iteration 23100 (12.6539 iter/s, 7.90269s/100 iters), loss = 0.0426982
I1024 16:55:08.157650  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:55:08.157650  1960 solver.cpp:237]     Train net output #1: loss = 0.0426983 (* 1 = 0.0426983 loss)
I1024 16:55:08.157650  1960 sgd_solver.cpp:105] Iteration 23100, lr = 0.0001
I1024 16:55:16.059650  1960 solver.cpp:218] Iteration 23200 (12.6557 iter/s, 7.90158s/100 iters), loss = 0.00266093
I1024 16:55:16.059650  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:55:16.059650  1960 solver.cpp:237]     Train net output #1: loss = 0.00266098 (* 1 = 0.00266098 loss)
I1024 16:55:16.059650  1960 sgd_solver.cpp:105] Iteration 23200, lr = 0.0001
I1024 16:55:23.961652  1960 solver.cpp:218] Iteration 23300 (12.6554 iter/s, 7.90174s/100 iters), loss = 0.00356609
I1024 16:55:23.961652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:55:23.961652  1960 solver.cpp:237]     Train net output #1: loss = 0.00356614 (* 1 = 0.00356614 loss)
I1024 16:55:23.961652  1960 sgd_solver.cpp:105] Iteration 23300, lr = 0.0001
I1024 16:55:31.470651 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:55:31.785651  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_23400.caffemodel
I1024 16:55:31.819654  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_23400.solverstate
I1024 16:55:31.835654  1960 solver.cpp:330] Iteration 23400, Testing net (#0)
I1024 16:55:31.835654  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:55:33.730654  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:55:33.808651  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9975
I1024 16:55:33.808651  1960 solver.cpp:397]     Test net output #1: loss = 0.00923458 (* 1 = 0.00923458 loss)
I1024 16:55:33.884651  1960 solver.cpp:218] Iteration 23400 (10.0781 iter/s, 9.92252s/100 iters), loss = 0.00524013
I1024 16:55:33.884651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:55:33.884651  1960 solver.cpp:237]     Train net output #1: loss = 0.00524017 (* 1 = 0.00524017 loss)
I1024 16:55:33.884651  1960 sgd_solver.cpp:105] Iteration 23400, lr = 0.0001
I1024 16:55:41.789649  1960 solver.cpp:218] Iteration 23500 (12.6517 iter/s, 7.90407s/100 iters), loss = 0.0115626
I1024 16:55:41.789649  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:55:41.789649  1960 solver.cpp:237]     Train net output #1: loss = 0.0115626 (* 1 = 0.0115626 loss)
I1024 16:55:41.789649  1960 sgd_solver.cpp:105] Iteration 23500, lr = 0.0001
I1024 16:55:49.690651  1960 solver.cpp:218] Iteration 23600 (12.6562 iter/s, 7.90125s/100 iters), loss = 0.00922753
I1024 16:55:49.691651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:55:49.691651  1960 solver.cpp:237]     Train net output #1: loss = 0.00922756 (* 1 = 0.00922756 loss)
I1024 16:55:49.691651  1960 sgd_solver.cpp:105] Iteration 23600, lr = 0.0001
I1024 16:55:57.598650  1960 solver.cpp:218] Iteration 23700 (12.6463 iter/s, 7.90745s/100 iters), loss = 0.00532967
I1024 16:55:57.599651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:55:57.599651  1960 solver.cpp:237]     Train net output #1: loss = 0.0053297 (* 1 = 0.0053297 loss)
I1024 16:55:57.599651  1960 sgd_solver.cpp:105] Iteration 23700, lr = 0.0001
I1024 16:56:05.506654  1960 solver.cpp:218] Iteration 23800 (12.647 iter/s, 7.90702s/100 iters), loss = 0.00455211
I1024 16:56:05.506654  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:56:05.506654  1960 solver.cpp:237]     Train net output #1: loss = 0.00455214 (* 1 = 0.00455214 loss)
I1024 16:56:05.506654  1960 sgd_solver.cpp:105] Iteration 23800, lr = 0.0001
I1024 16:56:13.406652  1960 solver.cpp:218] Iteration 23900 (12.6596 iter/s, 7.89913s/100 iters), loss = 0.00543351
I1024 16:56:13.406652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:56:13.406652  1960 solver.cpp:237]     Train net output #1: loss = 0.00543354 (* 1 = 0.00543354 loss)
I1024 16:56:13.406652  1960 sgd_solver.cpp:105] Iteration 23900, lr = 0.0001
I1024 16:56:20.917651 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:56:21.231652  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_24000.caffemodel
I1024 16:56:21.267653  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_24000.solverstate
I1024 16:56:21.283653  1960 solver.cpp:330] Iteration 24000, Testing net (#0)
I1024 16:56:21.283653  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:56:23.177654  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:56:23.256660  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9975
I1024 16:56:23.256660  1960 solver.cpp:397]     Test net output #1: loss = 0.00931387 (* 1 = 0.00931387 loss)
I1024 16:56:23.332651  1960 solver.cpp:218] Iteration 24000 (10.0752 iter/s, 9.92534s/100 iters), loss = 0.00265137
I1024 16:56:23.332651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:56:23.332651  1960 solver.cpp:237]     Train net output #1: loss = 0.0026514 (* 1 = 0.0026514 loss)
I1024 16:56:23.332651  1960 sgd_solver.cpp:105] Iteration 24000, lr = 0.0001
I1024 16:56:31.236651  1960 solver.cpp:218] Iteration 24100 (12.6528 iter/s, 7.90337s/100 iters), loss = 0.0151477
I1024 16:56:31.236651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:56:31.236651  1960 solver.cpp:237]     Train net output #1: loss = 0.0151478 (* 1 = 0.0151478 loss)
I1024 16:56:31.236651  1960 sgd_solver.cpp:105] Iteration 24100, lr = 0.0001
I1024 16:56:39.135651  1960 solver.cpp:218] Iteration 24200 (12.66 iter/s, 7.89891s/100 iters), loss = 0.0108788
I1024 16:56:39.135651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:56:39.135651  1960 solver.cpp:237]     Train net output #1: loss = 0.0108788 (* 1 = 0.0108788 loss)
I1024 16:56:39.135651  1960 sgd_solver.cpp:105] Iteration 24200, lr = 0.0001
I1024 16:56:47.040652  1960 solver.cpp:218] Iteration 24300 (12.6511 iter/s, 7.90446s/100 iters), loss = 0.00295885
I1024 16:56:47.040652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:56:47.040652  1960 solver.cpp:237]     Train net output #1: loss = 0.00295889 (* 1 = 0.00295889 loss)
I1024 16:56:47.040652  1960 sgd_solver.cpp:105] Iteration 24300, lr = 0.0001
I1024 16:56:54.941651  1960 solver.cpp:218] Iteration 24400 (12.657 iter/s, 7.90079s/100 iters), loss = 0.00635995
I1024 16:56:54.941651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:56:54.941651  1960 solver.cpp:237]     Train net output #1: loss = 0.00635999 (* 1 = 0.00635999 loss)
I1024 16:56:54.941651  1960 sgd_solver.cpp:105] Iteration 24400, lr = 0.0001
I1024 16:57:02.847657  1960 solver.cpp:218] Iteration 24500 (12.6502 iter/s, 7.905s/100 iters), loss = 0.00488599
I1024 16:57:02.847657  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:57:02.847657  1960 solver.cpp:237]     Train net output #1: loss = 0.00488603 (* 1 = 0.00488603 loss)
I1024 16:57:02.847657  1960 sgd_solver.cpp:105] Iteration 24500, lr = 0.0001
I1024 16:57:10.356652 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:57:10.671674  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_24600.caffemodel
I1024 16:57:10.707656  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_24600.solverstate
I1024 16:57:10.724673  1960 solver.cpp:330] Iteration 24600, Testing net (#0)
I1024 16:57:10.724673  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:57:12.620656  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:57:12.698688  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9975
I1024 16:57:12.698688  1960 solver.cpp:397]     Test net output #1: loss = 0.00929269 (* 1 = 0.00929269 loss)
I1024 16:57:12.775651  1960 solver.cpp:218] Iteration 24600 (10.0733 iter/s, 9.92719s/100 iters), loss = 0.00265966
I1024 16:57:12.775651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:57:12.775651  1960 solver.cpp:237]     Train net output #1: loss = 0.00265971 (* 1 = 0.00265971 loss)
I1024 16:57:12.775651  1960 sgd_solver.cpp:105] Iteration 24600, lr = 0.0001
I1024 16:57:20.676651  1960 solver.cpp:218] Iteration 24700 (12.6572 iter/s, 7.90064s/100 iters), loss = 0.00611002
I1024 16:57:20.676651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:57:20.676651  1960 solver.cpp:237]     Train net output #1: loss = 0.00611007 (* 1 = 0.00611007 loss)
I1024 16:57:20.676651  1960 sgd_solver.cpp:105] Iteration 24700, lr = 0.0001
I1024 16:57:28.574651  1960 solver.cpp:218] Iteration 24800 (12.6614 iter/s, 7.89802s/100 iters), loss = 0.0085738
I1024 16:57:28.574651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:57:28.574651  1960 solver.cpp:237]     Train net output #1: loss = 0.00857383 (* 1 = 0.00857383 loss)
I1024 16:57:28.574651  1960 sgd_solver.cpp:105] Iteration 24800, lr = 0.0001
I1024 16:57:36.478652  1960 solver.cpp:218] Iteration 24900 (12.6536 iter/s, 7.90287s/100 iters), loss = 0.0080965
I1024 16:57:36.478652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:57:36.478652  1960 solver.cpp:237]     Train net output #1: loss = 0.00809654 (* 1 = 0.00809654 loss)
I1024 16:57:36.478652  1960 sgd_solver.cpp:105] Iteration 24900, lr = 0.0001
I1024 16:57:44.379652  1960 solver.cpp:218] Iteration 25000 (12.6567 iter/s, 7.90095s/100 iters), loss = 0.00392736
I1024 16:57:44.379652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:57:44.379652  1960 solver.cpp:237]     Train net output #1: loss = 0.0039274 (* 1 = 0.0039274 loss)
I1024 16:57:44.379652  1960 sgd_solver.cpp:105] Iteration 25000, lr = 0.0001
I1024 16:57:52.283651  1960 solver.cpp:218] Iteration 25100 (12.6522 iter/s, 7.90379s/100 iters), loss = 0.00379616
I1024 16:57:52.283651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:57:52.283651  1960 solver.cpp:237]     Train net output #1: loss = 0.0037962 (* 1 = 0.0037962 loss)
I1024 16:57:52.283651  1960 sgd_solver.cpp:105] Iteration 25100, lr = 0.0001
I1024 16:57:59.797652 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:58:00.111652  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_25200.caffemodel
I1024 16:58:00.145656  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_25200.solverstate
I1024 16:58:00.161651  1960 solver.cpp:330] Iteration 25200, Testing net (#0)
I1024 16:58:00.161651  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:58:02.055652  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:58:02.132652  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 16:58:02.132652  1960 solver.cpp:397]     Test net output #1: loss = 0.0092742 (* 1 = 0.0092742 loss)
I1024 16:58:02.208652  1960 solver.cpp:218] Iteration 25200 (10.0759 iter/s, 9.92465s/100 iters), loss = 0.00953656
I1024 16:58:02.208652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:58:02.208652  1960 solver.cpp:237]     Train net output #1: loss = 0.00953661 (* 1 = 0.00953661 loss)
I1024 16:58:02.208652  1960 sgd_solver.cpp:105] Iteration 25200, lr = 0.0001
I1024 16:58:10.112651  1960 solver.cpp:218] Iteration 25300 (12.6538 iter/s, 7.90274s/100 iters), loss = 0.0036515
I1024 16:58:10.112651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:58:10.112651  1960 solver.cpp:237]     Train net output #1: loss = 0.00365155 (* 1 = 0.00365155 loss)
I1024 16:58:10.112651  1960 sgd_solver.cpp:105] Iteration 25300, lr = 0.0001
I1024 16:58:18.018651  1960 solver.cpp:218] Iteration 25400 (12.6482 iter/s, 7.90629s/100 iters), loss = 0.00550571
I1024 16:58:18.018651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:58:18.018651  1960 solver.cpp:237]     Train net output #1: loss = 0.00550576 (* 1 = 0.00550576 loss)
I1024 16:58:18.018651  1960 sgd_solver.cpp:105] Iteration 25400, lr = 0.0001
I1024 16:58:25.924650  1960 solver.cpp:218] Iteration 25500 (12.6493 iter/s, 7.9056s/100 iters), loss = 0.00649683
I1024 16:58:25.924650  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:58:25.924650  1960 solver.cpp:237]     Train net output #1: loss = 0.00649688 (* 1 = 0.00649688 loss)
I1024 16:58:25.924650  1960 sgd_solver.cpp:105] Iteration 25500, lr = 0.0001
I1024 16:58:33.825651  1960 solver.cpp:218] Iteration 25600 (12.6577 iter/s, 7.90034s/100 iters), loss = 0.00182111
I1024 16:58:33.825651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:58:33.825651  1960 solver.cpp:237]     Train net output #1: loss = 0.00182117 (* 1 = 0.00182117 loss)
I1024 16:58:33.825651  1960 sgd_solver.cpp:105] Iteration 25600, lr = 0.0001
I1024 16:58:41.728652  1960 solver.cpp:218] Iteration 25700 (12.6539 iter/s, 7.90273s/100 iters), loss = 0.00495069
I1024 16:58:41.728652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:58:41.728652  1960 solver.cpp:237]     Train net output #1: loss = 0.00495075 (* 1 = 0.00495075 loss)
I1024 16:58:41.728652  1960 sgd_solver.cpp:105] Iteration 25700, lr = 0.0001
I1024 16:58:49.240653 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:58:49.556651  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_25800.caffemodel
I1024 16:58:49.591652  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_25800.solverstate
I1024 16:58:49.607652  1960 solver.cpp:330] Iteration 25800, Testing net (#0)
I1024 16:58:49.607652  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:58:51.502660  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:58:51.580651  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 16:58:51.580651  1960 solver.cpp:397]     Test net output #1: loss = 0.00928861 (* 1 = 0.00928861 loss)
I1024 16:58:51.657650  1960 solver.cpp:218] Iteration 25800 (10.0726 iter/s, 9.92793s/100 iters), loss = 0.00331104
I1024 16:58:51.657650  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:58:51.657650  1960 solver.cpp:237]     Train net output #1: loss = 0.0033111 (* 1 = 0.0033111 loss)
I1024 16:58:51.657650  1960 sgd_solver.cpp:105] Iteration 25800, lr = 0.0001
I1024 16:58:59.558651  1960 solver.cpp:218] Iteration 25900 (12.6572 iter/s, 7.90061s/100 iters), loss = 0.0129133
I1024 16:58:59.558651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:58:59.558651  1960 solver.cpp:237]     Train net output #1: loss = 0.0129134 (* 1 = 0.0129134 loss)
I1024 16:58:59.558651  1960 sgd_solver.cpp:105] Iteration 25900, lr = 0.0001
I1024 16:59:07.463651  1960 solver.cpp:218] Iteration 26000 (12.6509 iter/s, 7.90459s/100 iters), loss = 0.00806469
I1024 16:59:07.463651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:59:07.463651  1960 solver.cpp:237]     Train net output #1: loss = 0.00806475 (* 1 = 0.00806475 loss)
I1024 16:59:07.463651  1960 sgd_solver.cpp:105] Iteration 26000, lr = 0.0001
I1024 16:59:15.369652  1960 solver.cpp:218] Iteration 26100 (12.6497 iter/s, 7.90531s/100 iters), loss = 0.00437621
I1024 16:59:15.369652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:59:15.369652  1960 solver.cpp:237]     Train net output #1: loss = 0.00437626 (* 1 = 0.00437626 loss)
I1024 16:59:15.369652  1960 sgd_solver.cpp:105] Iteration 26100, lr = 0.0001
I1024 16:59:23.274652  1960 solver.cpp:218] Iteration 26200 (12.6509 iter/s, 7.90457s/100 iters), loss = 0.00251347
I1024 16:59:23.274652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:59:23.274652  1960 solver.cpp:237]     Train net output #1: loss = 0.00251353 (* 1 = 0.00251353 loss)
I1024 16:59:23.274652  1960 sgd_solver.cpp:105] Iteration 26200, lr = 0.0001
I1024 16:59:31.178652  1960 solver.cpp:218] Iteration 26300 (12.6521 iter/s, 7.90384s/100 iters), loss = 0.011928
I1024 16:59:31.178652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 16:59:31.178652  1960 solver.cpp:237]     Train net output #1: loss = 0.011928 (* 1 = 0.011928 loss)
I1024 16:59:31.178652  1960 sgd_solver.cpp:105] Iteration 26300, lr = 0.0001
I1024 16:59:38.690651 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:59:39.006651  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_26400.caffemodel
I1024 16:59:39.039654  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_26400.solverstate
I1024 16:59:39.055652  1960 solver.cpp:330] Iteration 26400, Testing net (#0)
I1024 16:59:39.055652  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 16:59:40.953654  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 16:59:41.031651  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 16:59:41.031651  1960 solver.cpp:397]     Test net output #1: loss = 0.00928352 (* 1 = 0.00928352 loss)
I1024 16:59:41.108651  1960 solver.cpp:218] Iteration 26400 (10.0715 iter/s, 9.92896s/100 iters), loss = 0.00761294
I1024 16:59:41.108651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:59:41.108651  1960 solver.cpp:237]     Train net output #1: loss = 0.00761299 (* 1 = 0.00761299 loss)
I1024 16:59:41.108651  1960 sgd_solver.cpp:105] Iteration 26400, lr = 0.0001
I1024 16:59:49.006652  1960 solver.cpp:218] Iteration 26500 (12.6612 iter/s, 7.89817s/100 iters), loss = 0.0063699
I1024 16:59:49.006652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:59:49.006652  1960 solver.cpp:237]     Train net output #1: loss = 0.00636995 (* 1 = 0.00636995 loss)
I1024 16:59:49.006652  1960 sgd_solver.cpp:105] Iteration 26500, lr = 0.0001
I1024 16:59:56.908658  1960 solver.cpp:218] Iteration 26600 (12.6569 iter/s, 7.90084s/100 iters), loss = 0.0140046
I1024 16:59:56.908658  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 16:59:56.908658  1960 solver.cpp:237]     Train net output #1: loss = 0.0140047 (* 1 = 0.0140047 loss)
I1024 16:59:56.908658  1960 sgd_solver.cpp:105] Iteration 26600, lr = 0.0001
I1024 17:00:04.809651  1960 solver.cpp:218] Iteration 26700 (12.6568 iter/s, 7.90088s/100 iters), loss = 0.003888
I1024 17:00:04.809651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:00:04.809651  1960 solver.cpp:237]     Train net output #1: loss = 0.00388805 (* 1 = 0.00388805 loss)
I1024 17:00:04.809651  1960 sgd_solver.cpp:105] Iteration 26700, lr = 0.0001
I1024 17:00:12.715652  1960 solver.cpp:218] Iteration 26800 (12.6486 iter/s, 7.90601s/100 iters), loss = 0.00556947
I1024 17:00:12.716652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:00:12.716652  1960 solver.cpp:237]     Train net output #1: loss = 0.00556953 (* 1 = 0.00556953 loss)
I1024 17:00:12.716652  1960 sgd_solver.cpp:105] Iteration 26800, lr = 0.0001
I1024 17:00:20.619652  1960 solver.cpp:218] Iteration 26900 (12.6528 iter/s, 7.90338s/100 iters), loss = 0.00243887
I1024 17:00:20.619652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:00:20.619652  1960 solver.cpp:237]     Train net output #1: loss = 0.00243893 (* 1 = 0.00243893 loss)
I1024 17:00:20.619652  1960 sgd_solver.cpp:105] Iteration 26900, lr = 0.0001
I1024 17:00:28.130652 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 17:00:28.444653  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_27000.caffemodel
I1024 17:00:28.479655  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_27000.solverstate
I1024 17:00:28.494654  1960 solver.cpp:330] Iteration 27000, Testing net (#0)
I1024 17:00:28.494654  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 17:00:30.390652  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 17:00:30.468652  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 17:00:30.468652  1960 solver.cpp:397]     Test net output #1: loss = 0.00927238 (* 1 = 0.00927238 loss)
I1024 17:00:30.544651  1960 solver.cpp:218] Iteration 27000 (10.076 iter/s, 9.92458s/100 iters), loss = 0.00306978
I1024 17:00:30.545651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:00:30.545651  1960 solver.cpp:237]     Train net output #1: loss = 0.00306984 (* 1 = 0.00306984 loss)
I1024 17:00:30.545651  1960 sgd_solver.cpp:105] Iteration 27000, lr = 0.0001
I1024 17:00:38.450651  1960 solver.cpp:218] Iteration 27100 (12.6504 iter/s, 7.90486s/100 iters), loss = 0.0169076
I1024 17:00:38.450651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 17:00:38.450651  1960 solver.cpp:237]     Train net output #1: loss = 0.0169077 (* 1 = 0.0169077 loss)
I1024 17:00:38.450651  1960 sgd_solver.cpp:105] Iteration 27100, lr = 0.0001
I1024 17:00:46.358651  1960 solver.cpp:218] Iteration 27200 (12.6466 iter/s, 7.90727s/100 iters), loss = 0.0142641
I1024 17:00:46.358651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 17:00:46.358651  1960 solver.cpp:237]     Train net output #1: loss = 0.0142642 (* 1 = 0.0142642 loss)
I1024 17:00:46.358651  1960 sgd_solver.cpp:105] Iteration 27200, lr = 0.0001
I1024 17:00:54.258651  1960 solver.cpp:218] Iteration 27300 (12.6591 iter/s, 7.89949s/100 iters), loss = 0.0147929
I1024 17:00:54.258651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:00:54.258651  1960 solver.cpp:237]     Train net output #1: loss = 0.014793 (* 1 = 0.014793 loss)
I1024 17:00:54.258651  1960 sgd_solver.cpp:105] Iteration 27300, lr = 0.0001
I1024 17:01:02.157652  1960 solver.cpp:218] Iteration 27400 (12.6598 iter/s, 7.89904s/100 iters), loss = 0.00253187
I1024 17:01:02.157652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:01:02.157652  1960 solver.cpp:237]     Train net output #1: loss = 0.00253194 (* 1 = 0.00253194 loss)
I1024 17:01:02.157652  1960 sgd_solver.cpp:105] Iteration 27400, lr = 0.0001
I1024 17:01:10.066651  1960 solver.cpp:218] Iteration 27500 (12.645 iter/s, 7.90825s/100 iters), loss = 0.00538743
I1024 17:01:10.066651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:01:10.066651  1960 solver.cpp:237]     Train net output #1: loss = 0.00538752 (* 1 = 0.00538752 loss)
I1024 17:01:10.066651  1960 sgd_solver.cpp:105] Iteration 27500, lr = 0.0001
I1024 17:01:17.579651 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 17:01:17.893652  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_27600.caffemodel
I1024 17:01:17.926652  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_27600.solverstate
I1024 17:01:17.943660  1960 solver.cpp:330] Iteration 27600, Testing net (#0)
I1024 17:01:17.943660  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 17:01:19.837652  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 17:01:19.915652  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9973
I1024 17:01:19.915652  1960 solver.cpp:397]     Test net output #1: loss = 0.00928455 (* 1 = 0.00928455 loss)
I1024 17:01:19.992650  1960 solver.cpp:218] Iteration 27600 (10.0752 iter/s, 9.92532s/100 iters), loss = 0.00287543
I1024 17:01:19.992650  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:01:19.992650  1960 solver.cpp:237]     Train net output #1: loss = 0.00287552 (* 1 = 0.00287552 loss)
I1024 17:01:19.992650  1960 sgd_solver.cpp:105] Iteration 27600, lr = 0.0001
I1024 17:01:27.895650  1960 solver.cpp:218] Iteration 27700 (12.6541 iter/s, 7.90255s/100 iters), loss = 0.00486837
I1024 17:01:27.895650  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:01:27.895650  1960 solver.cpp:237]     Train net output #1: loss = 0.00486846 (* 1 = 0.00486846 loss)
I1024 17:01:27.895650  1960 sgd_solver.cpp:105] Iteration 27700, lr = 0.0001
I1024 17:01:35.796650  1960 solver.cpp:218] Iteration 27800 (12.6565 iter/s, 7.90108s/100 iters), loss = 0.00988281
I1024 17:01:35.796650  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:01:35.796650  1960 solver.cpp:237]     Train net output #1: loss = 0.00988289 (* 1 = 0.00988289 loss)
I1024 17:01:35.796650  1960 sgd_solver.cpp:105] Iteration 27800, lr = 0.0001
I1024 17:01:43.695652  1960 solver.cpp:218] Iteration 27900 (12.6614 iter/s, 7.89804s/100 iters), loss = 0.00302284
I1024 17:01:43.695652  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:01:43.695652  1960 solver.cpp:237]     Train net output #1: loss = 0.00302292 (* 1 = 0.00302292 loss)
I1024 17:01:43.695652  1960 sgd_solver.cpp:105] Iteration 27900, lr = 0.0001
I1024 17:01:51.596650  1960 solver.cpp:218] Iteration 28000 (12.657 iter/s, 7.90074s/100 iters), loss = 0.00332955
I1024 17:01:51.596650  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:01:51.596650  1960 solver.cpp:237]     Train net output #1: loss = 0.00332963 (* 1 = 0.00332963 loss)
I1024 17:01:51.596650  1960 sgd_solver.cpp:105] Iteration 28000, lr = 0.0001
I1024 17:01:59.497651  1960 solver.cpp:218] Iteration 28100 (12.6578 iter/s, 7.90027s/100 iters), loss = 0.00247385
I1024 17:01:59.497651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:01:59.497651  1960 solver.cpp:237]     Train net output #1: loss = 0.00247393 (* 1 = 0.00247393 loss)
I1024 17:01:59.497651  1960 sgd_solver.cpp:105] Iteration 28100, lr = 0.0001
I1024 17:02:07.012652 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 17:02:07.325651  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_28200.caffemodel
I1024 17:02:07.360651  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_28200.solverstate
I1024 17:02:07.376652  1960 solver.cpp:330] Iteration 28200, Testing net (#0)
I1024 17:02:07.376652  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 17:02:09.272652  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 17:02:09.350652  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9974
I1024 17:02:09.350652  1960 solver.cpp:397]     Test net output #1: loss = 0.00928386 (* 1 = 0.00928386 loss)
I1024 17:02:09.426651  1960 solver.cpp:218] Iteration 28200 (10.0712 iter/s, 9.92931s/100 iters), loss = 0.0136921
I1024 17:02:09.426651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 17:02:09.427651  1960 solver.cpp:237]     Train net output #1: loss = 0.0136922 (* 1 = 0.0136922 loss)
I1024 17:02:09.427651  1960 sgd_solver.cpp:105] Iteration 28200, lr = 0.0001
I1024 17:02:17.331651  1960 solver.cpp:218] Iteration 28300 (12.6519 iter/s, 7.90398s/100 iters), loss = 0.0141372
I1024 17:02:17.331651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 17:02:17.331651  1960 solver.cpp:237]     Train net output #1: loss = 0.0141373 (* 1 = 0.0141373 loss)
I1024 17:02:17.331651  1960 sgd_solver.cpp:105] Iteration 28300, lr = 0.0001
I1024 17:02:25.236651  1960 solver.cpp:218] Iteration 28400 (12.6507 iter/s, 7.90471s/100 iters), loss = 0.0123571
I1024 17:02:25.236651  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:02:25.236651  1960 solver.cpp:237]     Train net output #1: loss = 0.0123571 (* 1 = 0.0123571 loss)
I1024 17:02:25.236651  1960 sgd_solver.cpp:105] Iteration 28400, lr = 0.0001
I1024 17:02:33.137717  1960 solver.cpp:218] Iteration 28500 (12.658 iter/s, 7.90011s/100 iters), loss = 0.00531763
I1024 17:02:33.137717  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:02:33.137717  1960 solver.cpp:237]     Train net output #1: loss = 0.00531769 (* 1 = 0.00531769 loss)
I1024 17:02:33.137717  1960 sgd_solver.cpp:105] Iteration 28500, lr = 0.0001
I1024 17:02:41.041714  1960 solver.cpp:218] Iteration 28600 (12.6521 iter/s, 7.90384s/100 iters), loss = 0.00323111
I1024 17:02:41.041714  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:02:41.041714  1960 solver.cpp:237]     Train net output #1: loss = 0.00323117 (* 1 = 0.00323117 loss)
I1024 17:02:41.041714  1960 sgd_solver.cpp:105] Iteration 28600, lr = 0.0001
I1024 17:02:48.942716  1960 solver.cpp:218] Iteration 28700 (12.6575 iter/s, 7.90044s/100 iters), loss = 0.00351159
I1024 17:02:48.942716  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:02:48.942716  1960 solver.cpp:237]     Train net output #1: loss = 0.00351165 (* 1 = 0.00351165 loss)
I1024 17:02:48.942716  1960 sgd_solver.cpp:105] Iteration 28700, lr = 0.0001
I1024 17:02:56.451716 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 17:02:56.767717  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_28800.caffemodel
I1024 17:02:56.800717  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_28800.solverstate
I1024 17:02:56.816717  1960 solver.cpp:330] Iteration 28800, Testing net (#0)
I1024 17:02:56.817718  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 17:02:58.710716  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 17:02:58.788717  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9972
I1024 17:02:58.788717  1960 solver.cpp:397]     Test net output #1: loss = 0.00929135 (* 1 = 0.00929135 loss)
I1024 17:02:58.864717  1960 solver.cpp:218] Iteration 28800 (10.0792 iter/s, 9.92138s/100 iters), loss = 0.00503063
I1024 17:02:58.864717  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:02:58.864717  1960 solver.cpp:237]     Train net output #1: loss = 0.00503069 (* 1 = 0.00503069 loss)
I1024 17:02:58.864717  1960 sgd_solver.cpp:105] Iteration 28800, lr = 0.0001
I1024 17:03:06.763715  1960 solver.cpp:218] Iteration 28900 (12.6606 iter/s, 7.89853s/100 iters), loss = 0.0182222
I1024 17:03:06.763715  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 17:03:06.763715  1960 solver.cpp:237]     Train net output #1: loss = 0.0182223 (* 1 = 0.0182223 loss)
I1024 17:03:06.763715  1960 sgd_solver.cpp:105] Iteration 28900, lr = 0.0001
I1024 17:03:14.674715  1960 solver.cpp:218] Iteration 29000 (12.6414 iter/s, 7.91054s/100 iters), loss = 0.0102954
I1024 17:03:14.674715  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:03:14.674715  1960 solver.cpp:237]     Train net output #1: loss = 0.0102955 (* 1 = 0.0102955 loss)
I1024 17:03:14.674715  1960 sgd_solver.cpp:105] Iteration 29000, lr = 0.0001
I1024 17:03:22.575716  1960 solver.cpp:218] Iteration 29100 (12.6577 iter/s, 7.90031s/100 iters), loss = 0.00379758
I1024 17:03:22.575716  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:03:22.575716  1960 solver.cpp:237]     Train net output #1: loss = 0.00379765 (* 1 = 0.00379765 loss)
I1024 17:03:22.575716  1960 sgd_solver.cpp:105] Iteration 29100, lr = 0.0001
I1024 17:03:30.709715  1960 solver.cpp:218] Iteration 29200 (12.2952 iter/s, 8.13328s/100 iters), loss = 0.00336889
I1024 17:03:30.709715  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:03:30.709715  1960 solver.cpp:237]     Train net output #1: loss = 0.00336896 (* 1 = 0.00336896 loss)
I1024 17:03:30.709715  1960 sgd_solver.cpp:105] Iteration 29200, lr = 0.0001
I1024 17:03:38.664716  1960 solver.cpp:218] Iteration 29300 (12.5703 iter/s, 7.95529s/100 iters), loss = 0.00880176
I1024 17:03:38.664716  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:03:38.664716  1960 solver.cpp:237]     Train net output #1: loss = 0.00880182 (* 1 = 0.00880182 loss)
I1024 17:03:38.664716  1960 sgd_solver.cpp:105] Iteration 29300, lr = 0.0001
I1024 17:03:46.225718 11712 data_layer.cpp:73] Restarting data prefetching from start.
I1024 17:03:46.541716  1960 solver.cpp:447] Snapshotting to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_29400.caffemodel
I1024 17:03:46.576716  1960 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/mnist_simplenet66_13LGP_5M_maxdrp_iter_29400.solverstate
I1024 17:03:46.592716  1960 solver.cpp:330] Iteration 29400, Testing net (#0)
I1024 17:03:46.592716  1960 net.cpp:676] Ignoring source layer accuracy_training
I1024 17:03:48.494717  5208 data_layer.cpp:73] Restarting data prefetching from start.
I1024 17:03:48.573716  1960 solver.cpp:397]     Test net output #0: accuracy = 0.9974
I1024 17:03:48.573716  1960 solver.cpp:397]     Test net output #1: loss = 0.00925289 (* 1 = 0.00925289 loss)
I1024 17:03:48.649716  1960 solver.cpp:218] Iteration 29400 (10.016 iter/s, 9.98398s/100 iters), loss = 0.00668553
I1024 17:03:48.649716  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:03:48.649716  1960 solver.cpp:237]     Train net output #1: loss = 0.0066856 (* 1 = 0.0066856 loss)
I1024 17:03:48.649716  1960 sgd_solver.cpp:105] Iteration 29400, lr = 0.0001
I1024 17:03:56.575716  1960 solver.cpp:218] Iteration 29500 (12.6182 iter/s, 7.92509s/100 iters), loss = 0.0212031
I1024 17:03:56.575716  1960 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1024 17:03:56.575716  1960 solver.cpp:237]     Train net output #1: loss = 0.0212032 (* 1 = 0.0212032 loss)
I1024 17:03:56.575716  1960 sgd_solver.cpp:105] Iteration 29500, lr = 0.0001
I1024 17:04:04.502715  1960 solver.cpp:218] Iteration 29600 (12.6145 iter/s, 7.92738s/100 iters), loss = 0.00543542
I1024 17:04:04.503717  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:04:04.503717  1960 solver.cpp:237]     Train net output #1: loss = 0.00543549 (* 1 = 0.00543549 loss)
I1024 17:04:04.503717  1960 sgd_solver.cpp:46] MultiStep Status: Iteration 29600, step = 4
I1024 17:04:04.503717  1960 sgd_solver.cpp:105] Iteration 29600, lr = 1e-05
I1024 17:04:12.423715  1960 solver.cpp:218] Iteration 29700 (12.6255 iter/s, 7.92048s/100 iters), loss = 0.00813683
I1024 17:04:12.424721  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:04:12.424721  1960 solver.cpp:237]     Train net output #1: loss = 0.00813691 (* 1 = 0.00813691 loss)
I1024 17:04:12.424721  1960 sgd_solver.cpp:105] Iteration 29700, lr = 1e-05
I1024 17:04:20.357715  1960 solver.cpp:218] Iteration 29800 (12.606 iter/s, 7.93274s/100 iters), loss = 0.0016134
I1024 17:04:20.357715  1960 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1024 17:04:20.357715  1960 solver.cpp:237]     Train net output #1: loss = 0.00161347 (* 1 = 0.00161347 loss)
I1024 17:04:20.357715  1960 sgd_solver
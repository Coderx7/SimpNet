
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt --snapshot=examples/cifar10/snaps/squeezenet_batchnorm_iter_90000.solverstate 
I1107 14:44:44.367962 15588 caffe.cpp:219] Using GPUs 0
I1107 14:44:44.546278 15588 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1107 14:44:44.838050 15588 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1107 14:44:44.856556 15588 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/squeezenet_batchnorm"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 195000
stepvalue: 220000
stepvalue: 270000
type: "AdaDelta"
I1107 14:44:44.857058 15588 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1107 14:44:44.859557 15588 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1107 14:44:44.859557 15588 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_3
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_4
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_5
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_6
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_7
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_8
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_9
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_10
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_11
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_12
I1107 14:44:44.859557 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_13
I1107 14:44:44.860057 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_14
I1107 14:44:44.860057 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_15
I1107 14:44:44.860057 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_16
I1107 14:44:44.860057 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_17
I1107 14:44:44.860057 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_18
I1107 14:44:44.860057 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_19
I1107 14:44:44.860057 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_20
I1107 14:44:44.860057 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_21
I1107 14:44:44.860057 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_22
I1107 14:44:44.889073 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_23
I1107 14:44:44.889073 15588 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1107 14:44:44.890061 15588 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_Squeezenet_1.1_Batchnorm"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "fire2/squeeze1x1"
  type: "Convolution"
  bottom: "pool1"
  top: "fire2/squeeze1x1"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire2/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire2/squeeze1x1"
  top: "fire2/squeeze1x1"
}
layer {
  name: "fire2/expand1x1"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "conv2"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "fire2/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire2/relu_expand1x1"
  type: "ReLU"
  bottom: "fire2/expand1x1"
  top: "fire2/expand1x1"
}
layer {
  name: "fire2/expand3x3"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "conv_3"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_3"
  type: "BatchNorm"
  bottom: "conv_3"
  top: "conv_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_3"
  type: "Scale"
  bottom: "conv_3"
  top: "fire2/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire2/relu_expand3x3"
  type: "ReLU"
  bottom: "fire2/expand3x3"
  top: "fire2/expand3x3"
}
layer {
  name: "fire2/concat"
  type: "Concat"
  bottom: "fire2/expand1x1"
  bottom: "fire2/expand3x3"
  top: "fire2/concat"
}
layer {
  name: "fire3/squeeze1x1"
  type: "Convolution"
  bottom: "fire2/concat"
  top: "conv_4"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_4"
  type: "BatchNorm"
  bottom: "conv_4"
  top: "conv_4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_4"
  type: "Scale"
  bottom: "conv_4"
  top: "fire3/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire3/squeeze1x1"
  top: "fire3/squeeze1x1"
}
layer {
  name: "fire3/expand1x1"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "conv_5"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_5"
  type: "BatchNorm"
  bottom: "conv_5"
  top: "conv_5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_5"
  type: "Scale"
  bottom: "conv_5"
  top: "fire3/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_expand1x1"
  type: "ReLU"
  bottom: "fire3/expand1x1"
  top: "fire3/expand1x1"
}
layer {
  name: "fire3/expand3x3"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "conv_6"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_6"
  type: "BatchNorm"
  bottom: "conv_6"
  top: "conv_6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_6"
  type: "Scale"
  bottom: "conv_6"
  top: "fire3/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_expand3x3"
  type: "ReLU"
  bottom: "fire3/expand3x3"
  top: "fire3/expand3x3"
}
layer {
  name: "fire3/concat"
  type: "Concat"
  bottom: "fire3/expand1x1"
  bottom: "fire3/expand3x3"
  top: "fire3/concat"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "fire3/concat"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire4/squeeze1x1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv_7"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_7"
  type: "BatchNorm"
  bottom: "conv_7"
  top: "conv_7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_7"
  type: "Scale"
  bottom: "conv_7"
  top: "fire4/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire4/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire4/squeeze1x1"
  top: "fire4/squeeze1x1"
}
layer {
  name: "fire4/expand1x1"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "fire4/expand1x1"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire4/relu_expand1x1"
  type: "ReLU"
  bottom: "fire4/expand1x1"
  top: "fire4/expand1x1"
}
layer {
  name: "fire4/expand3x3"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "conv_8"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_8"
  type: "BatchNorm"
  bottom: "conv_8"
  top: "conv_8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_8"
  type: "Scale"
  bottom: "conv_8"
  top: "fire4/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire4/relu_expand3x3"
  type: "ReLU"
  bottom: "fire4/expand3x3"
  top: "fire4/expand3x3"
}
layer {
  name: "fire4/concat"
  type: "Concat"
  bottom: "fire4/expand1x1"
  bottom: "fire4/expand3x3"
  top: "fire4/concat"
}
layer {
  name: "fire5/squeeze1x1"
  type: "Convolution"
  bottom: "fire4/concat"
  top: "conv_9"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_9"
  type: "BatchNorm"
  bottom: "conv_9"
  top: "conv_9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_9"
  type: "Scale"
  bottom: "conv_9"
  top: "fire5/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire5/squeeze1x1"
  top: "fire5/squeeze1x1"
}
layer {
  name: "fire5/expand1x1"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "conv_10"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_10"
  type: "BatchNorm"
  bottom: "conv_10"
  top: "conv_10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_10"
  type: "Scale"
  bottom: "conv_10"
  top: "fire5/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_expand1x1"
  type: "ReLU"
  bottom: "fire5/expand1x1"
  top: "fire5/expand1x1"
}
layer {
  name: "fire5/expand3x3"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "conv_11"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_11"
  type: "BatchNorm"
  bottom: "conv_11"
  top: "conv_11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_11"
  type: "Scale"
  bottom: "conv_11"
  top: "fire5/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_expand3x3"
  type: "ReLU"
  bottom: "fire5/expand3x3"
  top: "fire5/expand3x3"
}
layer {
  name: "fire5/concat"
  type: "Concat"
  bottom: "fire5/expand1x1"
  bottom: "fire5/expand3x3"
  top: "fire5/concat"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "fire5/concat"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire6/squeeze1x1"
  type: "Convolution"
  bottom: "pool5"
  top: "conv_12"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_12"
  type: "BatchNorm"
  bottom: "conv_12"
  top: "conv_12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_12"
  type: "Scale"
  bottom: "conv_12"
  top: "fire6/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire6/squeeze1x1"
  top: "fire6/squeeze1x1"
}
layer {
  name: "fire6/expand1x1"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "conv_13"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_13"
  type: "BatchNorm"
  bottom: "conv_13"
  top: "conv_13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_13"
  type: "Scale"
  bottom: "conv_13"
  top: "fire6/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_expand1x1"
  type: "ReLU"
  bottom: "fire6/expand1x1"
  top: "fire6/expand1x1"
}
layer {
  name: "fire6/expand3x3"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "conv_14"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_14"
  type: "BatchNorm"
  bottom: "conv_14"
  top: "conv_14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_14"
  type: "Scale"
  bottom: "conv_14"
  top: "fire6/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_expand3x3"
  type: "ReLU"
  bottom: "fire6/expand3x3"
  top: "fire6/expand3x3"
}
layer {
  name: "fire6/concat"
  type: "Concat"
  bottom: "fire6/expand1x1"
  bottom: "fire6/expand3x3"
  top: "fire6/concat"
}
layer {
  name: "fire7/squeeze1x1"
  type: "Convolution"
  bottom: "fire6/concat"
  top: "conv_15"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_15"
  type: "BatchNorm"
  bottom: "conv_15"
  top: "conv_15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_15"
  type: "Scale"
  bottom: "conv_15"
  top: "fire7/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire7/squeeze1x1"
  top: "fire7/squeeze1x1"
}
layer {
  name: "fire7/expand1x1"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "conv_16"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_16"
  type: "BatchNorm"
  bottom: "conv_16"
  top: "conv_16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_16"
  type: "Scale"
  bottom: "conv_16"
  top: "fire7/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_expand1x1"
  type: "ReLU"
  bottom: "fire7/expand1x1"
  top: "fire7/expand1x1"
}
layer {
  name: "fire7/expand3x3"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "conv_17"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_17"
  type: "BatchNorm"
  bottom: "conv_17"
  top: "conv_17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_17"
  type: "Scale"
  bottom: "conv_17"
  top: "fire7/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_expand3x3"
  type: "ReLU"
  bottom: "fire7/expand3x3"
  top: "fire7/expand3x3"
}
layer {
  name: "fire7/concat"
  type: "Concat"
  bottom: "fire7/expand1x1"
  bottom: "fire7/expand3x3"
  top: "fire7/concat"
}
layer {
  name: "fire8/squeeze1x1"
  type: "Convolution"
  bottom: "fire7/concat"
  top: "conv_18"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_18"
  type: "BatchNorm"
  bottom: "conv_18"
  top: "conv_18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_18"
  type: "Scale"
  bottom: "conv_18"
  top: "fire8/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire8/squeeze1x1"
  top: "fire8/squeeze1x1"
}
layer {
  name: "fire8/expand1x1"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "conv_19"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_19"
  type: "BatchNorm"
  bottom: "conv_19"
  top: "conv_19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_19"
  type: "Scale"
  bottom: "conv_19"
  top: "fire8/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_expand1x1"
  type: "ReLU"
  bottom: "fire8/expand1x1"
  top: "fire8/expand1x1"
}
layer {
  name: "fire8/expand3x3"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "conv_20"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_20"
  type: "BatchNorm"
  bottom: "conv_20"
  top: "conv_20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_20"
  type: "Scale"
  bottom: "conv_20"
  top: "fire8/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_expand3x3"
  type: "ReLU"
  bottom: "fire8/expand3x3"
  top: "fire8/expand3x3"
}
layer {
  name: "fire8/concat"
  type: "Concat"
  bottom: "fire8/expand1x1"
  bottom: "fire8/expand3x3"
  top: "fire8/concat"
}
layer {
  name: "fire9/squeeze1x1"
  type: "Convolution"
  bottom: "fire8/concat"
  top: "conv_21"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_21"
  type: "BatchNorm"
  bottom: "conv_21"
  top: "conv_21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_21"
  type: "Scale"
  bottom: "conv_21"
  top: "fire9/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire9/squeeze1x1"
  top: "fire9/squeeze1x1"
}
layer {
  name: "fire9/expand1x1"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "conv_22"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_22"
  type: "BatchNorm"
  bottom: "conv_22"
  top: "conv_22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_22"
  type: "Scale"
  bottom: "conv_22"
  top: "fire9/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_expand1x1"
  type: "ReLU"
  bottom: "fire9/expand1x1"
  top: "fire9/expand1x1"
}
layer {
  name: "fire9/expand3x3"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "conv_23"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_23"
  type: "BatchNorm"
  bottom: "conv_23"
  top: "conv_23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_23"
  type: "Scale"
  bottom: "conv_23"
  top: "fire9/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_expand3x3"
  type: "ReLU"
  bottom: "fire9/expand3x3"
  top: "fire9/expand3x3"
}
layer {
  name: "fire9/concat"
  type: "Concat"
  bottom: "fire9/expand1x1"
  bottom: "fire9/expand3x3"
  top: "fire9/concat"
}
layer {
  name: "drop9"
  type: "Dropout"
  bottom: "fire9/concat"
  top: "fire9/concat"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv10"
  type: "Convolution"
  bottom: "fire9/concat"
  top: "conv10"
  convolution_param {
    num_output: 10
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
  }
}
layer {
  name: "relu_conv10"
  type: "ReLU"
  bottom: "conv10"
  top: "conv10"
}
layer {
  name: "pool10"
  type: "Pooling"
  bottom: "conv10"
  top: "pool10"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "pool10"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "pool10"
  bottom: "label"
  top: "loss"
}
I1107 14:44:44.890061 15588 layer_factory.cpp:58] Creating layer cifar
I1107 14:44:44.897068 15588 db_leveldb.cpp:18] Opened leveldb examples/cifar10/cifar10_train_leveldb_padding
I1107 14:44:44.898061 15588 net.cpp:84] Creating Layer cifar
I1107 14:44:44.898061 15588 net.cpp:380] cifar -> data
I1107 14:44:44.898061 15588 net.cpp:380] cifar -> label
I1107 14:44:44.898061 15588 data_layer.cpp:45] output data size: 100,3,32,32
I1107 14:44:44.906060 15588 net.cpp:122] Setting up cifar
I1107 14:44:44.906060 15588 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1107 14:44:44.906060 15588 net.cpp:129] Top shape: 100 (100)
I1107 14:44:44.906060 15588 net.cpp:137] Memory required for data: 1229200
I1107 14:44:44.906060 15588 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1107 14:44:44.906060 15588 net.cpp:84] Creating Layer label_cifar_1_split
I1107 14:44:44.906060 15588 net.cpp:406] label_cifar_1_split <- label
I1107 14:44:44.906060 15588 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1107 14:44:44.906060 15588 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1107 14:44:44.906060 15588 net.cpp:122] Setting up label_cifar_1_split
I1107 14:44:44.906060 15588 net.cpp:129] Top shape: 100 (100)
I1107 14:44:44.906060 15588 net.cpp:129] Top shape: 100 (100)
I1107 14:44:44.906060 15588 net.cpp:137] Memory required for data: 1230000
I1107 14:44:44.906060 15588 layer_factory.cpp:58] Creating layer conv1
I1107 14:44:44.906060 15588 net.cpp:84] Creating Layer conv1
I1107 14:44:44.906060 15588 net.cpp:406] conv1 <- data
I1107 14:44:44.906060 15588 net.cpp:380] conv1 -> conv1
I1107 14:44:44.908061 10064 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1107 14:44:45.152578 15588 net.cpp:122] Setting up conv1
I1107 14:44:45.152578 15588 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 14:44:45.152578 15588 net.cpp:137] Memory required for data: 24270000
I1107 14:44:45.153079 15588 layer_factory.cpp:58] Creating layer bn1
I1107 14:44:45.153079 15588 net.cpp:84] Creating Layer bn1
I1107 14:44:45.153079 15588 net.cpp:406] bn1 <- conv1
I1107 14:44:45.153079 15588 net.cpp:367] bn1 -> conv1 (in-place)
I1107 14:44:45.153079 15588 net.cpp:122] Setting up bn1
I1107 14:44:45.153079 15588 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 14:44:45.153079 15588 net.cpp:137] Memory required for data: 47310000
I1107 14:44:45.153079 15588 layer_factory.cpp:58] Creating layer scale1
I1107 14:44:45.153079 15588 net.cpp:84] Creating Layer scale1
I1107 14:44:45.153079 15588 net.cpp:406] scale1 <- conv1
I1107 14:44:45.153079 15588 net.cpp:367] scale1 -> conv1 (in-place)
I1107 14:44:45.153079 15588 layer_factory.cpp:58] Creating layer scale1
I1107 14:44:45.153079 15588 net.cpp:122] Setting up scale1
I1107 14:44:45.153079 15588 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 14:44:45.153079 15588 net.cpp:137] Memory required for data: 70350000
I1107 14:44:45.153079 15588 layer_factory.cpp:58] Creating layer relu_conv1
I1107 14:44:45.153079 15588 net.cpp:84] Creating Layer relu_conv1
I1107 14:44:45.153079 15588 net.cpp:406] relu_conv1 <- conv1
I1107 14:44:45.153079 15588 net.cpp:367] relu_conv1 -> conv1 (in-place)
I1107 14:44:45.153579 15588 net.cpp:122] Setting up relu_conv1
I1107 14:44:45.153579 15588 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 14:44:45.153579 15588 net.cpp:137] Memory required for data: 93390000
I1107 14:44:45.153579 15588 layer_factory.cpp:58] Creating layer pool1
I1107 14:44:45.153579 15588 net.cpp:84] Creating Layer pool1
I1107 14:44:45.153579 15588 net.cpp:406] pool1 <- conv1
I1107 14:44:45.153579 15588 net.cpp:380] pool1 -> pool1
I1107 14:44:45.153579 15588 net.cpp:122] Setting up pool1
I1107 14:44:45.153579 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.153579 15588 net.cpp:137] Memory required for data: 113460400
I1107 14:44:45.153579 15588 layer_factory.cpp:58] Creating layer fire2/squeeze1x1
I1107 14:44:45.153579 15588 net.cpp:84] Creating Layer fire2/squeeze1x1
I1107 14:44:45.153579 15588 net.cpp:406] fire2/squeeze1x1 <- pool1
I1107 14:44:45.153579 15588 net.cpp:380] fire2/squeeze1x1 -> fire2/squeeze1x1
I1107 14:44:45.154579 15588 net.cpp:122] Setting up fire2/squeeze1x1
I1107 14:44:45.154579 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.154579 15588 net.cpp:137] Memory required for data: 118478000
I1107 14:44:45.154579 15588 layer_factory.cpp:58] Creating layer fire2/relu_squeeze1x1
I1107 14:44:45.154579 15588 net.cpp:84] Creating Layer fire2/relu_squeeze1x1
I1107 14:44:45.154579 15588 net.cpp:406] fire2/relu_squeeze1x1 <- fire2/squeeze1x1
I1107 14:44:45.155079 15588 net.cpp:367] fire2/relu_squeeze1x1 -> fire2/squeeze1x1 (in-place)
I1107 14:44:45.155079 15588 net.cpp:122] Setting up fire2/relu_squeeze1x1
I1107 14:44:45.155580 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.155580 15588 net.cpp:137] Memory required for data: 123495600
I1107 14:44:45.155580 15588 layer_factory.cpp:58] Creating layer fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 14:44:45.155580 15588 net.cpp:84] Creating Layer fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 14:44:45.155580 15588 net.cpp:406] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split <- fire2/squeeze1x1
I1107 14:44:45.155580 15588 net.cpp:380] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split -> fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_0
I1107 14:44:45.155580 15588 net.cpp:380] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split -> fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_1
I1107 14:44:45.155580 15588 net.cpp:122] Setting up fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 14:44:45.155580 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.155580 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.155580 15588 net.cpp:137] Memory required for data: 133530800
I1107 14:44:45.155580 15588 layer_factory.cpp:58] Creating layer fire2/expand1x1
I1107 14:44:45.155580 15588 net.cpp:84] Creating Layer fire2/expand1x1
I1107 14:44:45.155580 15588 net.cpp:406] fire2/expand1x1 <- fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_0
I1107 14:44:45.155580 15588 net.cpp:380] fire2/expand1x1 -> conv2
I1107 14:44:45.156080 15588 net.cpp:122] Setting up fire2/expand1x1
I1107 14:44:45.156080 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.156080 15588 net.cpp:137] Memory required for data: 153601200
I1107 14:44:45.156080 15588 layer_factory.cpp:58] Creating layer bn2
I1107 14:44:45.156579 15588 net.cpp:84] Creating Layer bn2
I1107 14:44:45.156579 15588 net.cpp:406] bn2 <- conv2
I1107 14:44:45.156579 15588 net.cpp:367] bn2 -> conv2 (in-place)
I1107 14:44:45.156579 15588 net.cpp:122] Setting up bn2
I1107 14:44:45.156579 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.156579 15588 net.cpp:137] Memory required for data: 173671600
I1107 14:44:45.156579 15588 layer_factory.cpp:58] Creating layer scale2
I1107 14:44:45.156579 15588 net.cpp:84] Creating Layer scale2
I1107 14:44:45.156579 15588 net.cpp:406] scale2 <- conv2
I1107 14:44:45.156579 15588 net.cpp:380] scale2 -> fire2/expand1x1
I1107 14:44:45.156579 15588 layer_factory.cpp:58] Creating layer scale2
I1107 14:44:45.156579 15588 net.cpp:122] Setting up scale2
I1107 14:44:45.156579 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.156579 15588 net.cpp:137] Memory required for data: 193742000
I1107 14:44:45.156579 15588 layer_factory.cpp:58] Creating layer fire2/relu_expand1x1
I1107 14:44:45.156579 15588 net.cpp:84] Creating Layer fire2/relu_expand1x1
I1107 14:44:45.156579 15588 net.cpp:406] fire2/relu_expand1x1 <- fire2/expand1x1
I1107 14:44:45.157079 15588 net.cpp:367] fire2/relu_expand1x1 -> fire2/expand1x1 (in-place)
I1107 14:44:45.157079 15588 net.cpp:122] Setting up fire2/relu_expand1x1
I1107 14:44:45.157079 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.157579 15588 net.cpp:137] Memory required for data: 213812400
I1107 14:44:45.157579 15588 layer_factory.cpp:58] Creating layer fire2/expand3x3
I1107 14:44:45.157579 15588 net.cpp:84] Creating Layer fire2/expand3x3
I1107 14:44:45.157579 15588 net.cpp:406] fire2/expand3x3 <- fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_1
I1107 14:44:45.157579 15588 net.cpp:380] fire2/expand3x3 -> conv_3
I1107 14:44:45.159078 15588 net.cpp:122] Setting up fire2/expand3x3
I1107 14:44:45.159078 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.159078 15588 net.cpp:137] Memory required for data: 233882800
I1107 14:44:45.159078 15588 layer_factory.cpp:58] Creating layer bn_3
I1107 14:44:45.159078 15588 net.cpp:84] Creating Layer bn_3
I1107 14:44:45.159078 15588 net.cpp:406] bn_3 <- conv_3
I1107 14:44:45.159078 15588 net.cpp:367] bn_3 -> conv_3 (in-place)
I1107 14:44:45.159579 15588 net.cpp:122] Setting up bn_3
I1107 14:44:45.159579 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.159579 15588 net.cpp:137] Memory required for data: 253953200
I1107 14:44:45.159579 15588 layer_factory.cpp:58] Creating layer scale_3
I1107 14:44:45.159579 15588 net.cpp:84] Creating Layer scale_3
I1107 14:44:45.159579 15588 net.cpp:406] scale_3 <- conv_3
I1107 14:44:45.159579 15588 net.cpp:380] scale_3 -> fire2/expand3x3
I1107 14:44:45.159579 15588 layer_factory.cpp:58] Creating layer scale_3
I1107 14:44:45.159579 15588 net.cpp:122] Setting up scale_3
I1107 14:44:45.159579 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.159579 15588 net.cpp:137] Memory required for data: 274023600
I1107 14:44:45.159579 15588 layer_factory.cpp:58] Creating layer fire2/relu_expand3x3
I1107 14:44:45.159579 15588 net.cpp:84] Creating Layer fire2/relu_expand3x3
I1107 14:44:45.159579 15588 net.cpp:406] fire2/relu_expand3x3 <- fire2/expand3x3
I1107 14:44:45.159579 15588 net.cpp:367] fire2/relu_expand3x3 -> fire2/expand3x3 (in-place)
I1107 14:44:45.160079 15588 net.cpp:122] Setting up fire2/relu_expand3x3
I1107 14:44:45.160079 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.160079 15588 net.cpp:137] Memory required for data: 294094000
I1107 14:44:45.160079 15588 layer_factory.cpp:58] Creating layer fire2/concat
I1107 14:44:45.160079 15588 net.cpp:84] Creating Layer fire2/concat
I1107 14:44:45.160079 15588 net.cpp:406] fire2/concat <- fire2/expand1x1
I1107 14:44:45.160079 15588 net.cpp:406] fire2/concat <- fire2/expand3x3
I1107 14:44:45.160079 15588 net.cpp:380] fire2/concat -> fire2/concat
I1107 14:44:45.160079 15588 net.cpp:122] Setting up fire2/concat
I1107 14:44:45.160079 15588 net.cpp:129] Top shape: 100 128 28 28 (10035200)
I1107 14:44:45.160079 15588 net.cpp:137] Memory required for data: 334234800
I1107 14:44:45.160079 15588 layer_factory.cpp:58] Creating layer fire3/squeeze1x1
I1107 14:44:45.160079 15588 net.cpp:84] Creating Layer fire3/squeeze1x1
I1107 14:44:45.160079 15588 net.cpp:406] fire3/squeeze1x1 <- fire2/concat
I1107 14:44:45.160079 15588 net.cpp:380] fire3/squeeze1x1 -> conv_4
I1107 14:44:45.161079 15588 net.cpp:122] Setting up fire3/squeeze1x1
I1107 14:44:45.161579 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.161579 15588 net.cpp:137] Memory required for data: 339252400
I1107 14:44:45.161579 15588 layer_factory.cpp:58] Creating layer bn_4
I1107 14:44:45.161579 15588 net.cpp:84] Creating Layer bn_4
I1107 14:44:45.161579 15588 net.cpp:406] bn_4 <- conv_4
I1107 14:44:45.161579 15588 net.cpp:367] bn_4 -> conv_4 (in-place)
I1107 14:44:45.161579 15588 net.cpp:122] Setting up bn_4
I1107 14:44:45.161579 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.161579 15588 net.cpp:137] Memory required for data: 344270000
I1107 14:44:45.161579 15588 layer_factory.cpp:58] Creating layer scale_4
I1107 14:44:45.161579 15588 net.cpp:84] Creating Layer scale_4
I1107 14:44:45.161579 15588 net.cpp:406] scale_4 <- conv_4
I1107 14:44:45.161579 15588 net.cpp:380] scale_4 -> fire3/squeeze1x1
I1107 14:44:45.161579 15588 layer_factory.cpp:58] Creating layer scale_4
I1107 14:44:45.161579 15588 net.cpp:122] Setting up scale_4
I1107 14:44:45.161579 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.161579 15588 net.cpp:137] Memory required for data: 349287600
I1107 14:44:45.161579 15588 layer_factory.cpp:58] Creating layer fire3/relu_squeeze1x1
I1107 14:44:45.161579 15588 net.cpp:84] Creating Layer fire3/relu_squeeze1x1
I1107 14:44:45.161579 15588 net.cpp:406] fire3/relu_squeeze1x1 <- fire3/squeeze1x1
I1107 14:44:45.161579 15588 net.cpp:367] fire3/relu_squeeze1x1 -> fire3/squeeze1x1 (in-place)
I1107 14:44:45.162080 15588 net.cpp:122] Setting up fire3/relu_squeeze1x1
I1107 14:44:45.162080 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.162080 15588 net.cpp:137] Memory required for data: 354305200
I1107 14:44:45.162080 15588 layer_factory.cpp:58] Creating layer fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 14:44:45.162080 15588 net.cpp:84] Creating Layer fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 14:44:45.162080 15588 net.cpp:406] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split <- fire3/squeeze1x1
I1107 14:44:45.162080 15588 net.cpp:380] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split -> fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_0
I1107 14:44:45.162080 15588 net.cpp:380] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split -> fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_1
I1107 14:44:45.162080 15588 net.cpp:122] Setting up fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 14:44:45.162080 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.162080 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.162080 15588 net.cpp:137] Memory required for data: 364340400
I1107 14:44:45.162080 15588 layer_factory.cpp:58] Creating layer fire3/expand1x1
I1107 14:44:45.162080 15588 net.cpp:84] Creating Layer fire3/expand1x1
I1107 14:44:45.162080 15588 net.cpp:406] fire3/expand1x1 <- fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_0
I1107 14:44:45.162080 15588 net.cpp:380] fire3/expand1x1 -> conv_5
I1107 14:44:45.163079 15588 net.cpp:122] Setting up fire3/expand1x1
I1107 14:44:45.163079 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.163079 15588 net.cpp:137] Memory required for data: 384410800
I1107 14:44:45.163079 15588 layer_factory.cpp:58] Creating layer bn_5
I1107 14:44:45.163079 15588 net.cpp:84] Creating Layer bn_5
I1107 14:44:45.163079 15588 net.cpp:406] bn_5 <- conv_5
I1107 14:44:45.163079 15588 net.cpp:367] bn_5 -> conv_5 (in-place)
I1107 14:44:45.163079 15588 net.cpp:122] Setting up bn_5
I1107 14:44:45.163079 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.163079 15588 net.cpp:137] Memory required for data: 404481200
I1107 14:44:45.163079 15588 layer_factory.cpp:58] Creating layer scale_5
I1107 14:44:45.163079 15588 net.cpp:84] Creating Layer scale_5
I1107 14:44:45.163079 15588 net.cpp:406] scale_5 <- conv_5
I1107 14:44:45.163079 15588 net.cpp:380] scale_5 -> fire3/expand1x1
I1107 14:44:45.163079 15588 layer_factory.cpp:58] Creating layer scale_5
I1107 14:44:45.163079 15588 net.cpp:122] Setting up scale_5
I1107 14:44:45.163079 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.163079 15588 net.cpp:137] Memory required for data: 424551600
I1107 14:44:45.163079 15588 layer_factory.cpp:58] Creating layer fire3/relu_expand1x1
I1107 14:44:45.163079 15588 net.cpp:84] Creating Layer fire3/relu_expand1x1
I1107 14:44:45.163079 15588 net.cpp:406] fire3/relu_expand1x1 <- fire3/expand1x1
I1107 14:44:45.163079 15588 net.cpp:367] fire3/relu_expand1x1 -> fire3/expand1x1 (in-place)
I1107 14:44:45.164084 15588 net.cpp:122] Setting up fire3/relu_expand1x1
I1107 14:44:45.164084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.164084 15588 net.cpp:137] Memory required for data: 444622000
I1107 14:44:45.164084 15588 layer_factory.cpp:58] Creating layer fire3/expand3x3
I1107 14:44:45.164084 15588 net.cpp:84] Creating Layer fire3/expand3x3
I1107 14:44:45.164084 15588 net.cpp:406] fire3/expand3x3 <- fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_1
I1107 14:44:45.164084 15588 net.cpp:380] fire3/expand3x3 -> conv_6
I1107 14:44:45.165083 15588 net.cpp:122] Setting up fire3/expand3x3
I1107 14:44:45.165083 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.165083 15588 net.cpp:137] Memory required for data: 464692400
I1107 14:44:45.165083 15588 layer_factory.cpp:58] Creating layer bn_6
I1107 14:44:45.165083 15588 net.cpp:84] Creating Layer bn_6
I1107 14:44:45.165083 15588 net.cpp:406] bn_6 <- conv_6
I1107 14:44:45.165083 15588 net.cpp:367] bn_6 -> conv_6 (in-place)
I1107 14:44:45.165083 15588 net.cpp:122] Setting up bn_6
I1107 14:44:45.165083 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.165083 15588 net.cpp:137] Memory required for data: 484762800
I1107 14:44:45.165083 15588 layer_factory.cpp:58] Creating layer scale_6
I1107 14:44:45.165083 15588 net.cpp:84] Creating Layer scale_6
I1107 14:44:45.165083 15588 net.cpp:406] scale_6 <- conv_6
I1107 14:44:45.165083 15588 net.cpp:380] scale_6 -> fire3/expand3x3
I1107 14:44:45.165083 15588 layer_factory.cpp:58] Creating layer scale_6
I1107 14:44:45.165083 15588 net.cpp:122] Setting up scale_6
I1107 14:44:45.165083 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.165083 15588 net.cpp:137] Memory required for data: 504833200
I1107 14:44:45.165083 15588 layer_factory.cpp:58] Creating layer fire3/relu_expand3x3
I1107 14:44:45.165083 15588 net.cpp:84] Creating Layer fire3/relu_expand3x3
I1107 14:44:45.165083 15588 net.cpp:406] fire3/relu_expand3x3 <- fire3/expand3x3
I1107 14:44:45.165083 15588 net.cpp:367] fire3/relu_expand3x3 -> fire3/expand3x3 (in-place)
I1107 14:44:45.165083 15588 net.cpp:122] Setting up fire3/relu_expand3x3
I1107 14:44:45.166083 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.166083 15588 net.cpp:137] Memory required for data: 524903600
I1107 14:44:45.166083 15588 layer_factory.cpp:58] Creating layer fire3/concat
I1107 14:44:45.166083 15588 net.cpp:84] Creating Layer fire3/concat
I1107 14:44:45.166083 15588 net.cpp:406] fire3/concat <- fire3/expand1x1
I1107 14:44:45.166083 15588 net.cpp:406] fire3/concat <- fire3/expand3x3
I1107 14:44:45.166083 15588 net.cpp:380] fire3/concat -> fire3/concat
I1107 14:44:45.166083 15588 net.cpp:122] Setting up fire3/concat
I1107 14:44:45.166083 15588 net.cpp:129] Top shape: 100 128 28 28 (10035200)
I1107 14:44:45.166083 15588 net.cpp:137] Memory required for data: 565044400
I1107 14:44:45.166083 15588 layer_factory.cpp:58] Creating layer pool3
I1107 14:44:45.166083 15588 net.cpp:84] Creating Layer pool3
I1107 14:44:45.166083 15588 net.cpp:406] pool3 <- fire3/concat
I1107 14:44:45.166083 15588 net.cpp:380] pool3 -> pool3
I1107 14:44:45.166083 15588 net.cpp:122] Setting up pool3
I1107 14:44:45.166083 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.166083 15588 net.cpp:137] Memory required for data: 575079600
I1107 14:44:45.166083 15588 layer_factory.cpp:58] Creating layer fire4/squeeze1x1
I1107 14:44:45.166083 15588 net.cpp:84] Creating Layer fire4/squeeze1x1
I1107 14:44:45.166083 15588 net.cpp:406] fire4/squeeze1x1 <- pool3
I1107 14:44:45.166083 15588 net.cpp:380] fire4/squeeze1x1 -> conv_7
I1107 14:44:45.167083 15588 net.cpp:122] Setting up fire4/squeeze1x1
I1107 14:44:45.167083 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.167083 15588 net.cpp:137] Memory required for data: 577588400
I1107 14:44:45.167083 15588 layer_factory.cpp:58] Creating layer bn_7
I1107 14:44:45.167083 15588 net.cpp:84] Creating Layer bn_7
I1107 14:44:45.167083 15588 net.cpp:406] bn_7 <- conv_7
I1107 14:44:45.167083 15588 net.cpp:367] bn_7 -> conv_7 (in-place)
I1107 14:44:45.168083 15588 net.cpp:122] Setting up bn_7
I1107 14:44:45.168083 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.168083 15588 net.cpp:137] Memory required for data: 580097200
I1107 14:44:45.168083 15588 layer_factory.cpp:58] Creating layer scale_7
I1107 14:44:45.168083 15588 net.cpp:84] Creating Layer scale_7
I1107 14:44:45.168083 15588 net.cpp:406] scale_7 <- conv_7
I1107 14:44:45.168083 15588 net.cpp:380] scale_7 -> fire4/squeeze1x1
I1107 14:44:45.168083 15588 layer_factory.cpp:58] Creating layer scale_7
I1107 14:44:45.168083 15588 net.cpp:122] Setting up scale_7
I1107 14:44:45.168083 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.168083 15588 net.cpp:137] Memory required for data: 582606000
I1107 14:44:45.168083 15588 layer_factory.cpp:58] Creating layer fire4/relu_squeeze1x1
I1107 14:44:45.168083 15588 net.cpp:84] Creating Layer fire4/relu_squeeze1x1
I1107 14:44:45.168083 15588 net.cpp:406] fire4/relu_squeeze1x1 <- fire4/squeeze1x1
I1107 14:44:45.168083 15588 net.cpp:367] fire4/relu_squeeze1x1 -> fire4/squeeze1x1 (in-place)
I1107 14:44:45.168083 15588 net.cpp:122] Setting up fire4/relu_squeeze1x1
I1107 14:44:45.168083 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.168083 15588 net.cpp:137] Memory required for data: 585114800
I1107 14:44:45.168083 15588 layer_factory.cpp:58] Creating layer fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 14:44:45.168083 15588 net.cpp:84] Creating Layer fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 14:44:45.168083 15588 net.cpp:406] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split <- fire4/squeeze1x1
I1107 14:44:45.168083 15588 net.cpp:380] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split -> fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_0
I1107 14:44:45.168083 15588 net.cpp:380] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split -> fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_1
I1107 14:44:45.168083 15588 net.cpp:122] Setting up fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 14:44:45.168083 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.168083 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.168083 15588 net.cpp:137] Memory required for data: 590132400
I1107 14:44:45.168083 15588 layer_factory.cpp:58] Creating layer fire4/expand1x1
I1107 14:44:45.168083 15588 net.cpp:84] Creating Layer fire4/expand1x1
I1107 14:44:45.168083 15588 net.cpp:406] fire4/expand1x1 <- fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_0
I1107 14:44:45.168083 15588 net.cpp:380] fire4/expand1x1 -> fire4/expand1x1
I1107 14:44:45.169083 15588 net.cpp:122] Setting up fire4/expand1x1
I1107 14:44:45.169083 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.169083 15588 net.cpp:137] Memory required for data: 600167600
I1107 14:44:45.169083 15588 layer_factory.cpp:58] Creating layer fire4/relu_expand1x1
I1107 14:44:45.169083 15588 net.cpp:84] Creating Layer fire4/relu_expand1x1
I1107 14:44:45.169083 15588 net.cpp:406] fire4/relu_expand1x1 <- fire4/expand1x1
I1107 14:44:45.169083 15588 net.cpp:367] fire4/relu_expand1x1 -> fire4/expand1x1 (in-place)
I1107 14:44:45.170084 15588 net.cpp:122] Setting up fire4/relu_expand1x1
I1107 14:44:45.170084 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.170084 15588 net.cpp:137] Memory required for data: 610202800
I1107 14:44:45.170084 15588 layer_factory.cpp:58] Creating layer fire4/expand3x3
I1107 14:44:45.170084 15588 net.cpp:84] Creating Layer fire4/expand3x3
I1107 14:44:45.170084 15588 net.cpp:406] fire4/expand3x3 <- fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_1
I1107 14:44:45.170084 15588 net.cpp:380] fire4/expand3x3 -> conv_8
I1107 14:44:45.172085 15588 net.cpp:122] Setting up fire4/expand3x3
I1107 14:44:45.172085 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.172085 15588 net.cpp:137] Memory required for data: 620238000
I1107 14:44:45.172085 15588 layer_factory.cpp:58] Creating layer bn_8
I1107 14:44:45.172085 15588 net.cpp:84] Creating Layer bn_8
I1107 14:44:45.172085 15588 net.cpp:406] bn_8 <- conv_8
I1107 14:44:45.172085 15588 net.cpp:367] bn_8 -> conv_8 (in-place)
I1107 14:44:45.172085 15588 net.cpp:122] Setting up bn_8
I1107 14:44:45.172085 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.172085 15588 net.cpp:137] Memory required for data: 630273200
I1107 14:44:45.172085 15588 layer_factory.cpp:58] Creating layer scale_8
I1107 14:44:45.172085 15588 net.cpp:84] Creating Layer scale_8
I1107 14:44:45.172085 15588 net.cpp:406] scale_8 <- conv_8
I1107 14:44:45.172085 15588 net.cpp:380] scale_8 -> fire4/expand3x3
I1107 14:44:45.172085 15588 layer_factory.cpp:58] Creating layer scale_8
I1107 14:44:45.172085 15588 net.cpp:122] Setting up scale_8
I1107 14:44:45.172085 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.172085 15588 net.cpp:137] Memory required for data: 640308400
I1107 14:44:45.172085 15588 layer_factory.cpp:58] Creating layer fire4/relu_expand3x3
I1107 14:44:45.172085 15588 net.cpp:84] Creating Layer fire4/relu_expand3x3
I1107 14:44:45.172085 15588 net.cpp:406] fire4/relu_expand3x3 <- fire4/expand3x3
I1107 14:44:45.172085 15588 net.cpp:367] fire4/relu_expand3x3 -> fire4/expand3x3 (in-place)
I1107 14:44:45.172085 15588 net.cpp:122] Setting up fire4/relu_expand3x3
I1107 14:44:45.172085 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.172085 15588 net.cpp:137] Memory required for data: 650343600
I1107 14:44:45.172085 15588 layer_factory.cpp:58] Creating layer fire4/concat
I1107 14:44:45.172085 15588 net.cpp:84] Creating Layer fire4/concat
I1107 14:44:45.172085 15588 net.cpp:406] fire4/concat <- fire4/expand1x1
I1107 14:44:45.172085 15588 net.cpp:406] fire4/concat <- fire4/expand3x3
I1107 14:44:45.172085 15588 net.cpp:380] fire4/concat -> fire4/concat
I1107 14:44:45.172085 15588 net.cpp:122] Setting up fire4/concat
I1107 14:44:45.173084 15588 net.cpp:129] Top shape: 100 256 14 14 (5017600)
I1107 14:44:45.173084 15588 net.cpp:137] Memory required for data: 670414000
I1107 14:44:45.173084 15588 layer_factory.cpp:58] Creating layer fire5/squeeze1x1
I1107 14:44:45.173084 15588 net.cpp:84] Creating Layer fire5/squeeze1x1
I1107 14:44:45.173084 15588 net.cpp:406] fire5/squeeze1x1 <- fire4/concat
I1107 14:44:45.173084 15588 net.cpp:380] fire5/squeeze1x1 -> conv_9
I1107 14:44:45.174084 15588 net.cpp:122] Setting up fire5/squeeze1x1
I1107 14:44:45.174084 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.174084 15588 net.cpp:137] Memory required for data: 672922800
I1107 14:44:45.174084 15588 layer_factory.cpp:58] Creating layer bn_9
I1107 14:44:45.174084 15588 net.cpp:84] Creating Layer bn_9
I1107 14:44:45.174084 15588 net.cpp:406] bn_9 <- conv_9
I1107 14:44:45.174084 15588 net.cpp:367] bn_9 -> conv_9 (in-place)
I1107 14:44:45.174084 15588 net.cpp:122] Setting up bn_9
I1107 14:44:45.174084 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.174084 15588 net.cpp:137] Memory required for data: 675431600
I1107 14:44:45.174084 15588 layer_factory.cpp:58] Creating layer scale_9
I1107 14:44:45.174084 15588 net.cpp:84] Creating Layer scale_9
I1107 14:44:45.174084 15588 net.cpp:406] scale_9 <- conv_9
I1107 14:44:45.174084 15588 net.cpp:380] scale_9 -> fire5/squeeze1x1
I1107 14:44:45.174084 15588 layer_factory.cpp:58] Creating layer scale_9
I1107 14:44:45.174084 15588 net.cpp:122] Setting up scale_9
I1107 14:44:45.174084 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.174084 15588 net.cpp:137] Memory required for data: 677940400
I1107 14:44:45.174084 15588 layer_factory.cpp:58] Creating layer fire5/relu_squeeze1x1
I1107 14:44:45.174084 15588 net.cpp:84] Creating Layer fire5/relu_squeeze1x1
I1107 14:44:45.174084 15588 net.cpp:406] fire5/relu_squeeze1x1 <- fire5/squeeze1x1
I1107 14:44:45.174084 15588 net.cpp:367] fire5/relu_squeeze1x1 -> fire5/squeeze1x1 (in-place)
I1107 14:44:45.175083 15588 net.cpp:122] Setting up fire5/relu_squeeze1x1
I1107 14:44:45.175083 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.175083 15588 net.cpp:137] Memory required for data: 680449200
I1107 14:44:45.175083 15588 layer_factory.cpp:58] Creating layer fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 14:44:45.175083 15588 net.cpp:84] Creating Layer fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 14:44:45.175083 15588 net.cpp:406] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split <- fire5/squeeze1x1
I1107 14:44:45.175083 15588 net.cpp:380] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split -> fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_0
I1107 14:44:45.175083 15588 net.cpp:380] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split -> fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_1
I1107 14:44:45.175083 15588 net.cpp:122] Setting up fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 14:44:45.175083 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.175083 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.175083 15588 net.cpp:137] Memory required for data: 685466800
I1107 14:44:45.175083 15588 layer_factory.cpp:58] Creating layer fire5/expand1x1
I1107 14:44:45.175083 15588 net.cpp:84] Creating Layer fire5/expand1x1
I1107 14:44:45.175083 15588 net.cpp:406] fire5/expand1x1 <- fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_0
I1107 14:44:45.175083 15588 net.cpp:380] fire5/expand1x1 -> conv_10
I1107 14:44:45.176084 15588 net.cpp:122] Setting up fire5/expand1x1
I1107 14:44:45.176084 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.176084 15588 net.cpp:137] Memory required for data: 695502000
I1107 14:44:45.176084 15588 layer_factory.cpp:58] Creating layer bn_10
I1107 14:44:45.176084 15588 net.cpp:84] Creating Layer bn_10
I1107 14:44:45.176084 15588 net.cpp:406] bn_10 <- conv_10
I1107 14:44:45.176084 15588 net.cpp:367] bn_10 -> conv_10 (in-place)
I1107 14:44:45.176084 15588 net.cpp:122] Setting up bn_10
I1107 14:44:45.176084 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.176084 15588 net.cpp:137] Memory required for data: 705537200
I1107 14:44:45.177084 15588 layer_factory.cpp:58] Creating layer scale_10
I1107 14:44:45.177084 15588 net.cpp:84] Creating Layer scale_10
I1107 14:44:45.177084 15588 net.cpp:406] scale_10 <- conv_10
I1107 14:44:45.177084 15588 net.cpp:380] scale_10 -> fire5/expand1x1
I1107 14:44:45.177084 15588 layer_factory.cpp:58] Creating layer scale_10
I1107 14:44:45.177084 15588 net.cpp:122] Setting up scale_10
I1107 14:44:45.177084 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.177084 15588 net.cpp:137] Memory required for data: 715572400
I1107 14:44:45.177084 15588 layer_factory.cpp:58] Creating layer fire5/relu_expand1x1
I1107 14:44:45.177084 15588 net.cpp:84] Creating Layer fire5/relu_expand1x1
I1107 14:44:45.177084 15588 net.cpp:406] fire5/relu_expand1x1 <- fire5/expand1x1
I1107 14:44:45.177084 15588 net.cpp:367] fire5/relu_expand1x1 -> fire5/expand1x1 (in-place)
I1107 14:44:45.177084 15588 net.cpp:122] Setting up fire5/relu_expand1x1
I1107 14:44:45.177084 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.177084 15588 net.cpp:137] Memory required for data: 725607600
I1107 14:44:45.177084 15588 layer_factory.cpp:58] Creating layer fire5/expand3x3
I1107 14:44:45.177084 15588 net.cpp:84] Creating Layer fire5/expand3x3
I1107 14:44:45.177084 15588 net.cpp:406] fire5/expand3x3 <- fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_1
I1107 14:44:45.177084 15588 net.cpp:380] fire5/expand3x3 -> conv_11
I1107 14:44:45.178084 15588 net.cpp:122] Setting up fire5/expand3x3
I1107 14:44:45.178084 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.178084 15588 net.cpp:137] Memory required for data: 735642800
I1107 14:44:45.178084 15588 layer_factory.cpp:58] Creating layer bn_11
I1107 14:44:45.178084 15588 net.cpp:84] Creating Layer bn_11
I1107 14:44:45.178084 15588 net.cpp:406] bn_11 <- conv_11
I1107 14:44:45.178084 15588 net.cpp:367] bn_11 -> conv_11 (in-place)
I1107 14:44:45.178084 15588 net.cpp:122] Setting up bn_11
I1107 14:44:45.178084 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.178084 15588 net.cpp:137] Memory required for data: 745678000
I1107 14:44:45.178084 15588 layer_factory.cpp:58] Creating layer scale_11
I1107 14:44:45.179085 15588 net.cpp:84] Creating Layer scale_11
I1107 14:44:45.179085 15588 net.cpp:406] scale_11 <- conv_11
I1107 14:44:45.179085 15588 net.cpp:380] scale_11 -> fire5/expand3x3
I1107 14:44:45.179085 15588 layer_factory.cpp:58] Creating layer scale_11
I1107 14:44:45.179085 15588 net.cpp:122] Setting up scale_11
I1107 14:44:45.179085 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.179085 15588 net.cpp:137] Memory required for data: 755713200
I1107 14:44:45.179085 15588 layer_factory.cpp:58] Creating layer fire5/relu_expand3x3
I1107 14:44:45.179085 15588 net.cpp:84] Creating Layer fire5/relu_expand3x3
I1107 14:44:45.179085 15588 net.cpp:406] fire5/relu_expand3x3 <- fire5/expand3x3
I1107 14:44:45.179085 15588 net.cpp:367] fire5/relu_expand3x3 -> fire5/expand3x3 (in-place)
I1107 14:44:45.179085 15588 net.cpp:122] Setting up fire5/relu_expand3x3
I1107 14:44:45.179085 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.179085 15588 net.cpp:137] Memory required for data: 765748400
I1107 14:44:45.179085 15588 layer_factory.cpp:58] Creating layer fire5/concat
I1107 14:44:45.179085 15588 net.cpp:84] Creating Layer fire5/concat
I1107 14:44:45.179085 15588 net.cpp:406] fire5/concat <- fire5/expand1x1
I1107 14:44:45.179085 15588 net.cpp:406] fire5/concat <- fire5/expand3x3
I1107 14:44:45.179085 15588 net.cpp:380] fire5/concat -> fire5/concat
I1107 14:44:45.179085 15588 net.cpp:122] Setting up fire5/concat
I1107 14:44:45.179085 15588 net.cpp:129] Top shape: 100 256 14 14 (5017600)
I1107 14:44:45.179085 15588 net.cpp:137] Memory required for data: 785818800
I1107 14:44:45.179085 15588 layer_factory.cpp:58] Creating layer pool5
I1107 14:44:45.179085 15588 net.cpp:84] Creating Layer pool5
I1107 14:44:45.179085 15588 net.cpp:406] pool5 <- fire5/concat
I1107 14:44:45.179085 15588 net.cpp:380] pool5 -> pool5
I1107 14:44:45.179085 15588 net.cpp:122] Setting up pool5
I1107 14:44:45.179085 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.179085 15588 net.cpp:137] Memory required for data: 790836400
I1107 14:44:45.179085 15588 layer_factory.cpp:58] Creating layer fire6/squeeze1x1
I1107 14:44:45.179085 15588 net.cpp:84] Creating Layer fire6/squeeze1x1
I1107 14:44:45.179085 15588 net.cpp:406] fire6/squeeze1x1 <- pool5
I1107 14:44:45.179085 15588 net.cpp:380] fire6/squeeze1x1 -> conv_12
I1107 14:44:45.180085 15588 net.cpp:122] Setting up fire6/squeeze1x1
I1107 14:44:45.180085 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.180085 15588 net.cpp:137] Memory required for data: 791777200
I1107 14:44:45.180085 15588 layer_factory.cpp:58] Creating layer bn_12
I1107 14:44:45.180085 15588 net.cpp:84] Creating Layer bn_12
I1107 14:44:45.180085 15588 net.cpp:406] bn_12 <- conv_12
I1107 14:44:45.180085 15588 net.cpp:367] bn_12 -> conv_12 (in-place)
I1107 14:44:45.181084 15588 net.cpp:122] Setting up bn_12
I1107 14:44:45.181084 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.181084 15588 net.cpp:137] Memory required for data: 792718000
I1107 14:44:45.181084 15588 layer_factory.cpp:58] Creating layer scale_12
I1107 14:44:45.181084 15588 net.cpp:84] Creating Layer scale_12
I1107 14:44:45.181084 15588 net.cpp:406] scale_12 <- conv_12
I1107 14:44:45.181084 15588 net.cpp:380] scale_12 -> fire6/squeeze1x1
I1107 14:44:45.181084 15588 layer_factory.cpp:58] Creating layer scale_12
I1107 14:44:45.181084 15588 net.cpp:122] Setting up scale_12
I1107 14:44:45.181084 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.181084 15588 net.cpp:137] Memory required for data: 793658800
I1107 14:44:45.181084 15588 layer_factory.cpp:58] Creating layer fire6/relu_squeeze1x1
I1107 14:44:45.181084 15588 net.cpp:84] Creating Layer fire6/relu_squeeze1x1
I1107 14:44:45.181084 15588 net.cpp:406] fire6/relu_squeeze1x1 <- fire6/squeeze1x1
I1107 14:44:45.181084 15588 net.cpp:367] fire6/relu_squeeze1x1 -> fire6/squeeze1x1 (in-place)
I1107 14:44:45.181084 15588 net.cpp:122] Setting up fire6/relu_squeeze1x1
I1107 14:44:45.181084 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.181084 15588 net.cpp:137] Memory required for data: 794599600
I1107 14:44:45.181084 15588 layer_factory.cpp:58] Creating layer fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 14:44:45.181084 15588 net.cpp:84] Creating Layer fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 14:44:45.181084 15588 net.cpp:406] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split <- fire6/squeeze1x1
I1107 14:44:45.181084 15588 net.cpp:380] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split -> fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_0
I1107 14:44:45.181084 15588 net.cpp:380] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split -> fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_1
I1107 14:44:45.181084 15588 net.cpp:122] Setting up fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 14:44:45.181084 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.181084 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.182085 15588 net.cpp:137] Memory required for data: 796481200
I1107 14:44:45.182085 15588 layer_factory.cpp:58] Creating layer fire6/expand1x1
I1107 14:44:45.182085 15588 net.cpp:84] Creating Layer fire6/expand1x1
I1107 14:44:45.182085 15588 net.cpp:406] fire6/expand1x1 <- fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_0
I1107 14:44:45.182085 15588 net.cpp:380] fire6/expand1x1 -> conv_13
I1107 14:44:45.183084 15588 net.cpp:122] Setting up fire6/expand1x1
I1107 14:44:45.183084 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.183084 15588 net.cpp:137] Memory required for data: 800244400
I1107 14:44:45.183084 15588 layer_factory.cpp:58] Creating layer bn_13
I1107 14:44:45.183084 15588 net.cpp:84] Creating Layer bn_13
I1107 14:44:45.183084 15588 net.cpp:406] bn_13 <- conv_13
I1107 14:44:45.183084 15588 net.cpp:367] bn_13 -> conv_13 (in-place)
I1107 14:44:45.183084 15588 net.cpp:122] Setting up bn_13
I1107 14:44:45.183084 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.183084 15588 net.cpp:137] Memory required for data: 804007600
I1107 14:44:45.183084 15588 layer_factory.cpp:58] Creating layer scale_13
I1107 14:44:45.183084 15588 net.cpp:84] Creating Layer scale_13
I1107 14:44:45.183084 15588 net.cpp:406] scale_13 <- conv_13
I1107 14:44:45.183084 15588 net.cpp:380] scale_13 -> fire6/expand1x1
I1107 14:44:45.183084 15588 layer_factory.cpp:58] Creating layer scale_13
I1107 14:44:45.183084 15588 net.cpp:122] Setting up scale_13
I1107 14:44:45.183084 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.183084 15588 net.cpp:137] Memory required for data: 807770800
I1107 14:44:45.183084 15588 layer_factory.cpp:58] Creating layer fire6/relu_expand1x1
I1107 14:44:45.183084 15588 net.cpp:84] Creating Layer fire6/relu_expand1x1
I1107 14:44:45.183084 15588 net.cpp:406] fire6/relu_expand1x1 <- fire6/expand1x1
I1107 14:44:45.183084 15588 net.cpp:367] fire6/relu_expand1x1 -> fire6/expand1x1 (in-place)
I1107 14:44:45.184084 15588 net.cpp:122] Setting up fire6/relu_expand1x1
I1107 14:44:45.184084 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.184084 15588 net.cpp:137] Memory required for data: 811534000
I1107 14:44:45.184084 15588 layer_factory.cpp:58] Creating layer fire6/expand3x3
I1107 14:44:45.184084 15588 net.cpp:84] Creating Layer fire6/expand3x3
I1107 14:44:45.184084 15588 net.cpp:406] fire6/expand3x3 <- fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_1
I1107 14:44:45.184084 15588 net.cpp:380] fire6/expand3x3 -> conv_14
I1107 14:44:45.185084 15588 net.cpp:122] Setting up fire6/expand3x3
I1107 14:44:45.185084 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.185084 15588 net.cpp:137] Memory required for data: 815297200
I1107 14:44:45.185084 15588 layer_factory.cpp:58] Creating layer bn_14
I1107 14:44:45.185084 15588 net.cpp:84] Creating Layer bn_14
I1107 14:44:45.185084 15588 net.cpp:406] bn_14 <- conv_14
I1107 14:44:45.185084 15588 net.cpp:367] bn_14 -> conv_14 (in-place)
I1107 14:44:45.186084 15588 net.cpp:122] Setting up bn_14
I1107 14:44:45.186084 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.186084 15588 net.cpp:137] Memory required for data: 819060400
I1107 14:44:45.186084 15588 layer_factory.cpp:58] Creating layer scale_14
I1107 14:44:45.186084 15588 net.cpp:84] Creating Layer scale_14
I1107 14:44:45.186084 15588 net.cpp:406] scale_14 <- conv_14
I1107 14:44:45.186084 15588 net.cpp:380] scale_14 -> fire6/expand3x3
I1107 14:44:45.186084 15588 layer_factory.cpp:58] Creating layer scale_14
I1107 14:44:45.186084 15588 net.cpp:122] Setting up scale_14
I1107 14:44:45.186084 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.186084 15588 net.cpp:137] Memory required for data: 822823600
I1107 14:44:45.186084 15588 layer_factory.cpp:58] Creating layer fire6/relu_expand3x3
I1107 14:44:45.186084 15588 net.cpp:84] Creating Layer fire6/relu_expand3x3
I1107 14:44:45.186084 15588 net.cpp:406] fire6/relu_expand3x3 <- fire6/expand3x3
I1107 14:44:45.186084 15588 net.cpp:367] fire6/relu_expand3x3 -> fire6/expand3x3 (in-place)
I1107 14:44:45.186084 15588 net.cpp:122] Setting up fire6/relu_expand3x3
I1107 14:44:45.186084 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.186084 15588 net.cpp:137] Memory required for data: 826586800
I1107 14:44:45.186084 15588 layer_factory.cpp:58] Creating layer fire6/concat
I1107 14:44:45.186084 15588 net.cpp:84] Creating Layer fire6/concat
I1107 14:44:45.186084 15588 net.cpp:406] fire6/concat <- fire6/expand1x1
I1107 14:44:45.186084 15588 net.cpp:406] fire6/concat <- fire6/expand3x3
I1107 14:44:45.186084 15588 net.cpp:380] fire6/concat -> fire6/concat
I1107 14:44:45.186084 15588 net.cpp:122] Setting up fire6/concat
I1107 14:44:45.186084 15588 net.cpp:129] Top shape: 100 384 7 7 (1881600)
I1107 14:44:45.186084 15588 net.cpp:137] Memory required for data: 834113200
I1107 14:44:45.186084 15588 layer_factory.cpp:58] Creating layer fire7/squeeze1x1
I1107 14:44:45.186084 15588 net.cpp:84] Creating Layer fire7/squeeze1x1
I1107 14:44:45.186084 15588 net.cpp:406] fire7/squeeze1x1 <- fire6/concat
I1107 14:44:45.186084 15588 net.cpp:380] fire7/squeeze1x1 -> conv_15
I1107 14:44:45.187084 15588 net.cpp:122] Setting up fire7/squeeze1x1
I1107 14:44:45.187084 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.187084 15588 net.cpp:137] Memory required for data: 835054000
I1107 14:44:45.187084 15588 layer_factory.cpp:58] Creating layer bn_15
I1107 14:44:45.187084 15588 net.cpp:84] Creating Layer bn_15
I1107 14:44:45.187084 15588 net.cpp:406] bn_15 <- conv_15
I1107 14:44:45.187084 15588 net.cpp:367] bn_15 -> conv_15 (in-place)
I1107 14:44:45.188084 15588 net.cpp:122] Setting up bn_15
I1107 14:44:45.188084 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.188084 15588 net.cpp:137] Memory required for data: 835994800
I1107 14:44:45.188084 15588 layer_factory.cpp:58] Creating layer scale_15
I1107 14:44:45.188084 15588 net.cpp:84] Creating Layer scale_15
I1107 14:44:45.188084 15588 net.cpp:406] scale_15 <- conv_15
I1107 14:44:45.188084 15588 net.cpp:380] scale_15 -> fire7/squeeze1x1
I1107 14:44:45.188084 15588 layer_factory.cpp:58] Creating layer scale_15
I1107 14:44:45.188084 15588 net.cpp:122] Setting up scale_15
I1107 14:44:45.188084 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.188084 15588 net.cpp:137] Memory required for data: 836935600
I1107 14:44:45.188084 15588 layer_factory.cpp:58] Creating layer fire7/relu_squeeze1x1
I1107 14:44:45.188084 15588 net.cpp:84] Creating Layer fire7/relu_squeeze1x1
I1107 14:44:45.188084 15588 net.cpp:406] fire7/relu_squeeze1x1 <- fire7/squeeze1x1
I1107 14:44:45.188084 15588 net.cpp:367] fire7/relu_squeeze1x1 -> fire7/squeeze1x1 (in-place)
I1107 14:44:45.188084 15588 net.cpp:122] Setting up fire7/relu_squeeze1x1
I1107 14:44:45.188084 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.188084 15588 net.cpp:137] Memory required for data: 837876400
I1107 14:44:45.188084 15588 layer_factory.cpp:58] Creating layer fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 14:44:45.188084 15588 net.cpp:84] Creating Layer fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 14:44:45.188084 15588 net.cpp:406] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split <- fire7/squeeze1x1
I1107 14:44:45.188084 15588 net.cpp:380] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split -> fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_0
I1107 14:44:45.188084 15588 net.cpp:380] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split -> fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_1
I1107 14:44:45.188084 15588 net.cpp:122] Setting up fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 14:44:45.188084 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.188084 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.188084 15588 net.cpp:137] Memory required for data: 839758000
I1107 14:44:45.188084 15588 layer_factory.cpp:58] Creating layer fire7/expand1x1
I1107 14:44:45.188084 15588 net.cpp:84] Creating Layer fire7/expand1x1
I1107 14:44:45.188084 15588 net.cpp:406] fire7/expand1x1 <- fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_0
I1107 14:44:45.188084 15588 net.cpp:380] fire7/expand1x1 -> conv_16
I1107 14:44:45.189085 15588 net.cpp:122] Setting up fire7/expand1x1
I1107 14:44:45.189085 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.189085 15588 net.cpp:137] Memory required for data: 843521200
I1107 14:44:45.189085 15588 layer_factory.cpp:58] Creating layer bn_16
I1107 14:44:45.189085 15588 net.cpp:84] Creating Layer bn_16
I1107 14:44:45.189085 15588 net.cpp:406] bn_16 <- conv_16
I1107 14:44:45.189085 15588 net.cpp:367] bn_16 -> conv_16 (in-place)
I1107 14:44:45.190084 15588 net.cpp:122] Setting up bn_16
I1107 14:44:45.190084 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.190084 15588 net.cpp:137] Memory required for data: 847284400
I1107 14:44:45.190084 15588 layer_factory.cpp:58] Creating layer scale_16
I1107 14:44:45.190084 15588 net.cpp:84] Creating Layer scale_16
I1107 14:44:45.190084 15588 net.cpp:406] scale_16 <- conv_16
I1107 14:44:45.190084 15588 net.cpp:380] scale_16 -> fire7/expand1x1
I1107 14:44:45.190084 15588 layer_factory.cpp:58] Creating layer scale_16
I1107 14:44:45.190084 15588 net.cpp:122] Setting up scale_16
I1107 14:44:45.190084 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.190084 15588 net.cpp:137] Memory required for data: 851047600
I1107 14:44:45.190084 15588 layer_factory.cpp:58] Creating layer fire7/relu_expand1x1
I1107 14:44:45.190084 15588 net.cpp:84] Creating Layer fire7/relu_expand1x1
I1107 14:44:45.190084 15588 net.cpp:406] fire7/relu_expand1x1 <- fire7/expand1x1
I1107 14:44:45.190084 15588 net.cpp:367] fire7/relu_expand1x1 -> fire7/expand1x1 (in-place)
I1107 14:44:45.190084 15588 net.cpp:122] Setting up fire7/relu_expand1x1
I1107 14:44:45.190084 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.190084 15588 net.cpp:137] Memory required for data: 854810800
I1107 14:44:45.190084 15588 layer_factory.cpp:58] Creating layer fire7/expand3x3
I1107 14:44:45.190084 15588 net.cpp:84] Creating Layer fire7/expand3x3
I1107 14:44:45.190084 15588 net.cpp:406] fire7/expand3x3 <- fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_1
I1107 14:44:45.190084 15588 net.cpp:380] fire7/expand3x3 -> conv_17
I1107 14:44:45.192085 15588 net.cpp:122] Setting up fire7/expand3x3
I1107 14:44:45.192085 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.192085 15588 net.cpp:137] Memory required for data: 858574000
I1107 14:44:45.192085 15588 layer_factory.cpp:58] Creating layer bn_17
I1107 14:44:45.192085 15588 net.cpp:84] Creating Layer bn_17
I1107 14:44:45.192085 15588 net.cpp:406] bn_17 <- conv_17
I1107 14:44:45.192085 15588 net.cpp:367] bn_17 -> conv_17 (in-place)
I1107 14:44:45.192085 15588 net.cpp:122] Setting up bn_17
I1107 14:44:45.192085 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.192085 15588 net.cpp:137] Memory required for data: 862337200
I1107 14:44:45.192085 15588 layer_factory.cpp:58] Creating layer scale_17
I1107 14:44:45.192085 15588 net.cpp:84] Creating Layer scale_17
I1107 14:44:45.192085 15588 net.cpp:406] scale_17 <- conv_17
I1107 14:44:45.192085 15588 net.cpp:380] scale_17 -> fire7/expand3x3
I1107 14:44:45.192085 15588 layer_factory.cpp:58] Creating layer scale_17
I1107 14:44:45.192085 15588 net.cpp:122] Setting up scale_17
I1107 14:44:45.192085 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.192085 15588 net.cpp:137] Memory required for data: 866100400
I1107 14:44:45.192085 15588 layer_factory.cpp:58] Creating layer fire7/relu_expand3x3
I1107 14:44:45.192085 15588 net.cpp:84] Creating Layer fire7/relu_expand3x3
I1107 14:44:45.192085 15588 net.cpp:406] fire7/relu_expand3x3 <- fire7/expand3x3
I1107 14:44:45.192085 15588 net.cpp:367] fire7/relu_expand3x3 -> fire7/expand3x3 (in-place)
I1107 14:44:45.193084 15588 net.cpp:122] Setting up fire7/relu_expand3x3
I1107 14:44:45.193084 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.193084 15588 net.cpp:137] Memory required for data: 869863600
I1107 14:44:45.193084 15588 layer_factory.cpp:58] Creating layer fire7/concat
I1107 14:44:45.193084 15588 net.cpp:84] Creating Layer fire7/concat
I1107 14:44:45.193084 15588 net.cpp:406] fire7/concat <- fire7/expand1x1
I1107 14:44:45.193084 15588 net.cpp:406] fire7/concat <- fire7/expand3x3
I1107 14:44:45.193084 15588 net.cpp:380] fire7/concat -> fire7/concat
I1107 14:44:45.193084 15588 net.cpp:122] Setting up fire7/concat
I1107 14:44:45.193084 15588 net.cpp:129] Top shape: 100 384 7 7 (1881600)
I1107 14:44:45.193084 15588 net.cpp:137] Memory required for data: 877390000
I1107 14:44:45.193084 15588 layer_factory.cpp:58] Creating layer fire8/squeeze1x1
I1107 14:44:45.193084 15588 net.cpp:84] Creating Layer fire8/squeeze1x1
I1107 14:44:45.193084 15588 net.cpp:406] fire8/squeeze1x1 <- fire7/concat
I1107 14:44:45.193084 15588 net.cpp:380] fire8/squeeze1x1 -> conv_18
I1107 14:44:45.194084 15588 net.cpp:122] Setting up fire8/squeeze1x1
I1107 14:44:45.194084 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.194084 15588 net.cpp:137] Memory required for data: 878644400
I1107 14:44:45.194084 15588 layer_factory.cpp:58] Creating layer bn_18
I1107 14:44:45.194084 15588 net.cpp:84] Creating Layer bn_18
I1107 14:44:45.194084 15588 net.cpp:406] bn_18 <- conv_18
I1107 14:44:45.194084 15588 net.cpp:367] bn_18 -> conv_18 (in-place)
I1107 14:44:45.194084 15588 net.cpp:122] Setting up bn_18
I1107 14:44:45.194084 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.194084 15588 net.cpp:137] Memory required for data: 879898800
I1107 14:44:45.194084 15588 layer_factory.cpp:58] Creating layer scale_18
I1107 14:44:45.195085 15588 net.cpp:84] Creating Layer scale_18
I1107 14:44:45.195085 15588 net.cpp:406] scale_18 <- conv_18
I1107 14:44:45.195085 15588 net.cpp:380] scale_18 -> fire8/squeeze1x1
I1107 14:44:45.195085 15588 layer_factory.cpp:58] Creating layer scale_18
I1107 14:44:45.195085 15588 net.cpp:122] Setting up scale_18
I1107 14:44:45.195085 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.195085 15588 net.cpp:137] Memory required for data: 881153200
I1107 14:44:45.195085 15588 layer_factory.cpp:58] Creating layer fire8/relu_squeeze1x1
I1107 14:44:45.195085 15588 net.cpp:84] Creating Layer fire8/relu_squeeze1x1
I1107 14:44:45.195085 15588 net.cpp:406] fire8/relu_squeeze1x1 <- fire8/squeeze1x1
I1107 14:44:45.195085 15588 net.cpp:367] fire8/relu_squeeze1x1 -> fire8/squeeze1x1 (in-place)
I1107 14:44:45.195085 15588 net.cpp:122] Setting up fire8/relu_squeeze1x1
I1107 14:44:45.195085 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.195085 15588 net.cpp:137] Memory required for data: 882407600
I1107 14:44:45.195085 15588 layer_factory.cpp:58] Creating layer fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 14:44:45.195085 15588 net.cpp:84] Creating Layer fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 14:44:45.195085 15588 net.cpp:406] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split <- fire8/squeeze1x1
I1107 14:44:45.195085 15588 net.cpp:380] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split -> fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_0
I1107 14:44:45.195085 15588 net.cpp:380] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split -> fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_1
I1107 14:44:45.195085 15588 net.cpp:122] Setting up fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 14:44:45.195085 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.195085 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.195085 15588 net.cpp:137] Memory required for data: 884916400
I1107 14:44:45.195085 15588 layer_factory.cpp:58] Creating layer fire8/expand1x1
I1107 14:44:45.195085 15588 net.cpp:84] Creating Layer fire8/expand1x1
I1107 14:44:45.195085 15588 net.cpp:406] fire8/expand1x1 <- fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_0
I1107 14:44:45.195085 15588 net.cpp:380] fire8/expand1x1 -> conv_19
I1107 14:44:45.196084 15588 net.cpp:122] Setting up fire8/expand1x1
I1107 14:44:45.196084 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.196084 15588 net.cpp:137] Memory required for data: 889934000
I1107 14:44:45.196084 15588 layer_factory.cpp:58] Creating layer bn_19
I1107 14:44:45.196084 15588 net.cpp:84] Creating Layer bn_19
I1107 14:44:45.196084 15588 net.cpp:406] bn_19 <- conv_19
I1107 14:44:45.196084 15588 net.cpp:367] bn_19 -> conv_19 (in-place)
I1107 14:44:45.197084 15588 net.cpp:122] Setting up bn_19
I1107 14:44:45.197084 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.197084 15588 net.cpp:137] Memory required for data: 894951600
I1107 14:44:45.197084 15588 layer_factory.cpp:58] Creating layer scale_19
I1107 14:44:45.197084 15588 net.cpp:84] Creating Layer scale_19
I1107 14:44:45.197084 15588 net.cpp:406] scale_19 <- conv_19
I1107 14:44:45.197084 15588 net.cpp:380] scale_19 -> fire8/expand1x1
I1107 14:44:45.197084 15588 layer_factory.cpp:58] Creating layer scale_19
I1107 14:44:45.197084 15588 net.cpp:122] Setting up scale_19
I1107 14:44:45.197084 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.197084 15588 net.cpp:137] Memory required for data: 899969200
I1107 14:44:45.197084 15588 layer_factory.cpp:58] Creating layer fire8/relu_expand1x1
I1107 14:44:45.197084 15588 net.cpp:84] Creating Layer fire8/relu_expand1x1
I1107 14:44:45.197084 15588 net.cpp:406] fire8/relu_expand1x1 <- fire8/expand1x1
I1107 14:44:45.197084 15588 net.cpp:367] fire8/relu_expand1x1 -> fire8/expand1x1 (in-place)
I1107 14:44:45.197084 15588 net.cpp:122] Setting up fire8/relu_expand1x1
I1107 14:44:45.197084 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.197084 15588 net.cpp:137] Memory required for data: 904986800
I1107 14:44:45.197084 15588 layer_factory.cpp:58] Creating layer fire8/expand3x3
I1107 14:44:45.197084 15588 net.cpp:84] Creating Layer fire8/expand3x3
I1107 14:44:45.197084 15588 net.cpp:406] fire8/expand3x3 <- fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_1
I1107 14:44:45.197084 15588 net.cpp:380] fire8/expand3x3 -> conv_20
I1107 14:44:45.199084 15588 net.cpp:122] Setting up fire8/expand3x3
I1107 14:44:45.199084 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.199084 15588 net.cpp:137] Memory required for data: 910004400
I1107 14:44:45.199084 15588 layer_factory.cpp:58] Creating layer bn_20
I1107 14:44:45.199084 15588 net.cpp:84] Creating Layer bn_20
I1107 14:44:45.199084 15588 net.cpp:406] bn_20 <- conv_20
I1107 14:44:45.199084 15588 net.cpp:367] bn_20 -> conv_20 (in-place)
I1107 14:44:45.200084 15588 net.cpp:122] Setting up bn_20
I1107 14:44:45.200084 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.200084 15588 net.cpp:137] Memory required for data: 915022000
I1107 14:44:45.200084 15588 layer_factory.cpp:58] Creating layer scale_20
I1107 14:44:45.200084 15588 net.cpp:84] Creating Layer scale_20
I1107 14:44:45.200084 15588 net.cpp:406] scale_20 <- conv_20
I1107 14:44:45.200084 15588 net.cpp:380] scale_20 -> fire8/expand3x3
I1107 14:44:45.200084 15588 layer_factory.cpp:58] Creating layer scale_20
I1107 14:44:45.200084 15588 net.cpp:122] Setting up scale_20
I1107 14:44:45.200084 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.200084 15588 net.cpp:137] Memory required for data: 920039600
I1107 14:44:45.200084 15588 layer_factory.cpp:58] Creating layer fire8/relu_expand3x3
I1107 14:44:45.200084 15588 net.cpp:84] Creating Layer fire8/relu_expand3x3
I1107 14:44:45.200084 15588 net.cpp:406] fire8/relu_expand3x3 <- fire8/expand3x3
I1107 14:44:45.200084 15588 net.cpp:367] fire8/relu_expand3x3 -> fire8/expand3x3 (in-place)
I1107 14:44:45.200084 15588 net.cpp:122] Setting up fire8/relu_expand3x3
I1107 14:44:45.200084 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.200084 15588 net.cpp:137] Memory required for data: 925057200
I1107 14:44:45.200084 15588 layer_factory.cpp:58] Creating layer fire8/concat
I1107 14:44:45.200084 15588 net.cpp:84] Creating Layer fire8/concat
I1107 14:44:45.200084 15588 net.cpp:406] fire8/concat <- fire8/expand1x1
I1107 14:44:45.200084 15588 net.cpp:406] fire8/concat <- fire8/expand3x3
I1107 14:44:45.200084 15588 net.cpp:380] fire8/concat -> fire8/concat
I1107 14:44:45.200084 15588 net.cpp:122] Setting up fire8/concat
I1107 14:44:45.200084 15588 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 14:44:45.200084 15588 net.cpp:137] Memory required for data: 935092400
I1107 14:44:45.200084 15588 layer_factory.cpp:58] Creating layer fire9/squeeze1x1
I1107 14:44:45.200084 15588 net.cpp:84] Creating Layer fire9/squeeze1x1
I1107 14:44:45.200084 15588 net.cpp:406] fire9/squeeze1x1 <- fire8/concat
I1107 14:44:45.200084 15588 net.cpp:380] fire9/squeeze1x1 -> conv_21
I1107 14:44:45.202085 15588 net.cpp:122] Setting up fire9/squeeze1x1
I1107 14:44:45.202085 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.202085 15588 net.cpp:137] Memory required for data: 936346800
I1107 14:44:45.202085 15588 layer_factory.cpp:58] Creating layer bn_21
I1107 14:44:45.202085 15588 net.cpp:84] Creating Layer bn_21
I1107 14:44:45.202085 15588 net.cpp:406] bn_21 <- conv_21
I1107 14:44:45.202085 15588 net.cpp:367] bn_21 -> conv_21 (in-place)
I1107 14:44:45.202085 15588 net.cpp:122] Setting up bn_21
I1107 14:44:45.202085 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.202085 15588 net.cpp:137] Memory required for data: 937601200
I1107 14:44:45.202085 15588 layer_factory.cpp:58] Creating layer scale_21
I1107 14:44:45.202085 15588 net.cpp:84] Creating Layer scale_21
I1107 14:44:45.202085 15588 net.cpp:406] scale_21 <- conv_21
I1107 14:44:45.202085 15588 net.cpp:380] scale_21 -> fire9/squeeze1x1
I1107 14:44:45.202085 15588 layer_factory.cpp:58] Creating layer scale_21
I1107 14:44:45.202085 15588 net.cpp:122] Setting up scale_21
I1107 14:44:45.202085 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.202085 15588 net.cpp:137] Memory required for data: 938855600
I1107 14:44:45.202085 15588 layer_factory.cpp:58] Creating layer fire9/relu_squeeze1x1
I1107 14:44:45.202085 15588 net.cpp:84] Creating Layer fire9/relu_squeeze1x1
I1107 14:44:45.202085 15588 net.cpp:406] fire9/relu_squeeze1x1 <- fire9/squeeze1x1
I1107 14:44:45.202085 15588 net.cpp:367] fire9/relu_squeeze1x1 -> fire9/squeeze1x1 (in-place)
I1107 14:44:45.202085 15588 net.cpp:122] Setting up fire9/relu_squeeze1x1
I1107 14:44:45.202085 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.202085 15588 net.cpp:137] Memory required for data: 940110000
I1107 14:44:45.202085 15588 layer_factory.cpp:58] Creating layer fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 14:44:45.202085 15588 net.cpp:84] Creating Layer fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 14:44:45.202085 15588 net.cpp:406] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split <- fire9/squeeze1x1
I1107 14:44:45.202085 15588 net.cpp:380] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split -> fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_0
I1107 14:44:45.202085 15588 net.cpp:380] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split -> fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_1
I1107 14:44:45.202085 15588 net.cpp:122] Setting up fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 14:44:45.202085 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.202085 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.202085 15588 net.cpp:137] Memory required for data: 942618800
I1107 14:44:45.202085 15588 layer_factory.cpp:58] Creating layer fire9/expand1x1
I1107 14:44:45.202085 15588 net.cpp:84] Creating Layer fire9/expand1x1
I1107 14:44:45.202085 15588 net.cpp:406] fire9/expand1x1 <- fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_0
I1107 14:44:45.202085 15588 net.cpp:380] fire9/expand1x1 -> conv_22
I1107 14:44:45.204084 15588 net.cpp:122] Setting up fire9/expand1x1
I1107 14:44:45.204084 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.204084 15588 net.cpp:137] Memory required for data: 947636400
I1107 14:44:45.204084 15588 layer_factory.cpp:58] Creating layer bn_22
I1107 14:44:45.204084 15588 net.cpp:84] Creating Layer bn_22
I1107 14:44:45.204084 15588 net.cpp:406] bn_22 <- conv_22
I1107 14:44:45.204084 15588 net.cpp:367] bn_22 -> conv_22 (in-place)
I1107 14:44:45.204084 15588 net.cpp:122] Setting up bn_22
I1107 14:44:45.204084 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.204084 15588 net.cpp:137] Memory required for data: 952654000
I1107 14:44:45.204084 15588 layer_factory.cpp:58] Creating layer scale_22
I1107 14:44:45.204084 15588 net.cpp:84] Creating Layer scale_22
I1107 14:44:45.204084 15588 net.cpp:406] scale_22 <- conv_22
I1107 14:44:45.204084 15588 net.cpp:380] scale_22 -> fire9/expand1x1
I1107 14:44:45.204084 15588 layer_factory.cpp:58] Creating layer scale_22
I1107 14:44:45.204084 15588 net.cpp:122] Setting up scale_22
I1107 14:44:45.204084 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.204084 15588 net.cpp:137] Memory required for data: 957671600
I1107 14:44:45.204084 15588 layer_factory.cpp:58] Creating layer fire9/relu_expand1x1
I1107 14:44:45.204084 15588 net.cpp:84] Creating Layer fire9/relu_expand1x1
I1107 14:44:45.204084 15588 net.cpp:406] fire9/relu_expand1x1 <- fire9/expand1x1
I1107 14:44:45.204084 15588 net.cpp:367] fire9/relu_expand1x1 -> fire9/expand1x1 (in-place)
I1107 14:44:45.204084 15588 net.cpp:122] Setting up fire9/relu_expand1x1
I1107 14:44:45.204084 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.204084 15588 net.cpp:137] Memory required for data: 962689200
I1107 14:44:45.204084 15588 layer_factory.cpp:58] Creating layer fire9/expand3x3
I1107 14:44:45.204084 15588 net.cpp:84] Creating Layer fire9/expand3x3
I1107 14:44:45.204084 15588 net.cpp:406] fire9/expand3x3 <- fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_1
I1107 14:44:45.204084 15588 net.cpp:380] fire9/expand3x3 -> conv_23
I1107 14:44:45.207085 15588 net.cpp:122] Setting up fire9/expand3x3
I1107 14:44:45.207085 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.207085 15588 net.cpp:137] Memory required for data: 967706800
I1107 14:44:45.207085 15588 layer_factory.cpp:58] Creating layer bn_23
I1107 14:44:45.207085 15588 net.cpp:84] Creating Layer bn_23
I1107 14:44:45.207085 15588 net.cpp:406] bn_23 <- conv_23
I1107 14:44:45.207085 15588 net.cpp:367] bn_23 -> conv_23 (in-place)
I1107 14:44:45.207085 15588 net.cpp:122] Setting up bn_23
I1107 14:44:45.207085 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.207085 15588 net.cpp:137] Memory required for data: 972724400
I1107 14:44:45.207085 15588 layer_factory.cpp:58] Creating layer scale_23
I1107 14:44:45.207085 15588 net.cpp:84] Creating Layer scale_23
I1107 14:44:45.207085 15588 net.cpp:406] scale_23 <- conv_23
I1107 14:44:45.207085 15588 net.cpp:380] scale_23 -> fire9/expand3x3
I1107 14:44:45.207085 15588 layer_factory.cpp:58] Creating layer scale_23
I1107 14:44:45.207085 15588 net.cpp:122] Setting up scale_23
I1107 14:44:45.207085 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.207085 15588 net.cpp:137] Memory required for data: 977742000
I1107 14:44:45.207085 15588 layer_factory.cpp:58] Creating layer fire9/relu_expand3x3
I1107 14:44:45.207085 15588 net.cpp:84] Creating Layer fire9/relu_expand3x3
I1107 14:44:45.207085 15588 net.cpp:406] fire9/relu_expand3x3 <- fire9/expand3x3
I1107 14:44:45.207085 15588 net.cpp:367] fire9/relu_expand3x3 -> fire9/expand3x3 (in-place)
I1107 14:44:45.207085 15588 net.cpp:122] Setting up fire9/relu_expand3x3
I1107 14:44:45.208084 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.208084 15588 net.cpp:137] Memory required for data: 982759600
I1107 14:44:45.208084 15588 layer_factory.cpp:58] Creating layer fire9/concat
I1107 14:44:45.208084 15588 net.cpp:84] Creating Layer fire9/concat
I1107 14:44:45.208084 15588 net.cpp:406] fire9/concat <- fire9/expand1x1
I1107 14:44:45.208084 15588 net.cpp:406] fire9/concat <- fire9/expand3x3
I1107 14:44:45.208084 15588 net.cpp:380] fire9/concat -> fire9/concat
I1107 14:44:45.208084 15588 net.cpp:122] Setting up fire9/concat
I1107 14:44:45.208084 15588 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 14:44:45.208084 15588 net.cpp:137] Memory required for data: 992794800
I1107 14:44:45.208084 15588 layer_factory.cpp:58] Creating layer drop9
I1107 14:44:45.208084 15588 net.cpp:84] Creating Layer drop9
I1107 14:44:45.208084 15588 net.cpp:406] drop9 <- fire9/concat
I1107 14:44:45.208084 15588 net.cpp:367] drop9 -> fire9/concat (in-place)
I1107 14:44:45.208084 15588 net.cpp:122] Setting up drop9
I1107 14:44:45.208084 15588 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 14:44:45.208084 15588 net.cpp:137] Memory required for data: 1002830000
I1107 14:44:45.208084 15588 layer_factory.cpp:58] Creating layer conv10
I1107 14:44:45.208084 15588 net.cpp:84] Creating Layer conv10
I1107 14:44:45.208084 15588 net.cpp:406] conv10 <- fire9/concat
I1107 14:44:45.208084 15588 net.cpp:380] conv10 -> conv10
I1107 14:44:45.209084 15588 net.cpp:122] Setting up conv10
I1107 14:44:45.209084 15588 net.cpp:129] Top shape: 100 10 7 7 (49000)
I1107 14:44:45.209084 15588 net.cpp:137] Memory required for data: 1003026000
I1107 14:44:45.209084 15588 layer_factory.cpp:58] Creating layer relu_conv10
I1107 14:44:45.209084 15588 net.cpp:84] Creating Layer relu_conv10
I1107 14:44:45.209084 15588 net.cpp:406] relu_conv10 <- conv10
I1107 14:44:45.209084 15588 net.cpp:367] relu_conv10 -> conv10 (in-place)
I1107 14:44:45.209084 15588 net.cpp:122] Setting up relu_conv10
I1107 14:44:45.209084 15588 net.cpp:129] Top shape: 100 10 7 7 (49000)
I1107 14:44:45.209084 15588 net.cpp:137] Memory required for data: 1003222000
I1107 14:44:45.209084 15588 layer_factory.cpp:58] Creating layer pool10
I1107 14:44:45.209084 15588 net.cpp:84] Creating Layer pool10
I1107 14:44:45.209084 15588 net.cpp:406] pool10 <- conv10
I1107 14:44:45.209084 15588 net.cpp:380] pool10 -> pool10
I1107 14:44:45.210084 15588 net.cpp:122] Setting up pool10
I1107 14:44:45.210084 15588 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 14:44:45.210084 15588 net.cpp:137] Memory required for data: 1003226000
I1107 14:44:45.210084 15588 layer_factory.cpp:58] Creating layer pool10_pool10_0_split
I1107 14:44:45.210084 15588 net.cpp:84] Creating Layer pool10_pool10_0_split
I1107 14:44:45.210084 15588 net.cpp:406] pool10_pool10_0_split <- pool10
I1107 14:44:45.210084 15588 net.cpp:380] pool10_pool10_0_split -> pool10_pool10_0_split_0
I1107 14:44:45.210084 15588 net.cpp:380] pool10_pool10_0_split -> pool10_pool10_0_split_1
I1107 14:44:45.210084 15588 net.cpp:122] Setting up pool10_pool10_0_split
I1107 14:44:45.210084 15588 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 14:44:45.210084 15588 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 14:44:45.210084 15588 net.cpp:137] Memory required for data: 1003234000
I1107 14:44:45.210084 15588 layer_factory.cpp:58] Creating layer accuracy_training
I1107 14:44:45.210084 15588 net.cpp:84] Creating Layer accuracy_training
I1107 14:44:45.210084 15588 net.cpp:406] accuracy_training <- pool10_pool10_0_split_0
I1107 14:44:45.210084 15588 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1107 14:44:45.210084 15588 net.cpp:380] accuracy_training -> accuracy_training
I1107 14:44:45.210084 15588 net.cpp:122] Setting up accuracy_training
I1107 14:44:45.210084 15588 net.cpp:129] Top shape: (1)
I1107 14:44:45.210084 15588 net.cpp:137] Memory required for data: 1003234004
I1107 14:44:45.210084 15588 layer_factory.cpp:58] Creating layer loss
I1107 14:44:45.210084 15588 net.cpp:84] Creating Layer loss
I1107 14:44:45.210084 15588 net.cpp:406] loss <- pool10_pool10_0_split_1
I1107 14:44:45.210084 15588 net.cpp:406] loss <- label_cifar_1_split_1
I1107 14:44:45.210084 15588 net.cpp:380] loss -> loss
I1107 14:44:45.210084 15588 layer_factory.cpp:58] Creating layer loss
I1107 14:44:45.210084 15588 net.cpp:122] Setting up loss
I1107 14:44:45.210084 15588 net.cpp:129] Top shape: (1)
I1107 14:44:45.210084 15588 net.cpp:132]     with loss weight 1
I1107 14:44:45.210084 15588 net.cpp:137] Memory required for data: 1003234008
I1107 14:44:45.210084 15588 net.cpp:198] loss needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:200] accuracy_training does not need backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] pool10_pool10_0_split needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] pool10 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] relu_conv10 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] conv10 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] drop9 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire9/concat needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire9/relu_expand3x3 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] scale_23 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] bn_23 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire9/expand3x3 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire9/relu_expand1x1 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] scale_22 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] bn_22 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire9/expand1x1 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire9/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] scale_21 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] bn_21 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire9/squeeze1x1 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire8/concat needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire8/relu_expand3x3 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] scale_20 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] bn_20 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire8/expand3x3 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire8/relu_expand1x1 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] scale_19 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] bn_19 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire8/expand1x1 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire8/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] scale_18 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] bn_18 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire8/squeeze1x1 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire7/concat needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire7/relu_expand3x3 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] scale_17 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] bn_17 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire7/expand3x3 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire7/relu_expand1x1 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] scale_16 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] bn_16 needs backward computation.
I1107 14:44:45.210084 15588 net.cpp:198] fire7/expand1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire7/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale_15 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn_15 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire7/squeeze1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire6/concat needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire6/relu_expand3x3 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale_14 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn_14 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire6/expand3x3 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire6/relu_expand1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale_13 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn_13 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire6/expand1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire6/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale_12 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn_12 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire6/squeeze1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] pool5 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire5/concat needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire5/relu_expand3x3 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale_11 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn_11 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire5/expand3x3 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire5/relu_expand1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale_10 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn_10 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire5/expand1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire5/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale_9 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn_9 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire5/squeeze1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire4/concat needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire4/relu_expand3x3 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale_8 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn_8 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire4/expand3x3 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire4/relu_expand1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire4/expand1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire4/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale_7 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn_7 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire4/squeeze1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] pool3 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire3/concat needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire3/relu_expand3x3 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale_6 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn_6 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire3/expand3x3 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire3/relu_expand1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale_5 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn_5 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire3/expand1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire3/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale_4 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn_4 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire3/squeeze1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire2/concat needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire2/relu_expand3x3 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale_3 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn_3 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire2/expand3x3 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire2/relu_expand1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale2 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn2 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire2/expand1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire2/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] fire2/squeeze1x1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] pool1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] relu_conv1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] scale1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] bn1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:198] conv1 needs backward computation.
I1107 14:44:45.211084 15588 net.cpp:200] label_cifar_1_split does not need backward computation.
I1107 14:44:45.211084 15588 net.cpp:200] cifar does not need backward computation.
I1107 14:44:45.211084 15588 net.cpp:242] This network produces output accuracy_training
I1107 14:44:45.211084 15588 net.cpp:242] This network produces output loss
I1107 14:44:45.211084 15588 net.cpp:255] Network initialization done.
I1107 14:44:45.212085 15588 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1107 14:44:45.212085 15588 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1107 14:44:45.212085 15588 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_3
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_4
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_5
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_6
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_7
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_8
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_9
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_10
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_11
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_12
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_13
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_14
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_15
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_16
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_17
I1107 14:44:45.212085 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_18
I1107 14:44:45.213084 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_19
I1107 14:44:45.213084 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_20
I1107 14:44:45.213084 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_21
I1107 14:44:45.213084 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_22
I1107 14:44:45.213084 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_23
I1107 14:44:45.213084 15588 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1107 14:44:45.213084 15588 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_Squeezenet_1.1_Batchnorm"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "fire2/squeeze1x1"
  type: "Convolution"
  bottom: "pool1"
  top: "fire2/squeeze1x1"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire2/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire2/squeeze1x1"
  top: "fire2/squeeze1x1"
}
layer {
  name: "fire2/expand1x1"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "conv2"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "fire2/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire2/relu_expand1x1"
  type: "ReLU"
  bottom: "fire2/expand1x1"
  top: "fire2/expand1x1"
}
layer {
  name: "fire2/expand3x3"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "conv_3"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_3"
  type: "BatchNorm"
  bottom: "conv_3"
  top: "conv_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_3"
  type: "Scale"
  bottom: "conv_3"
  top: "fire2/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire2/relu_expand3x3"
  type: "ReLU"
  bottom: "fire2/expand3x3"
  top: "fire2/expand3x3"
}
layer {
  name: "fire2/concat"
  type: "Concat"
  bottom: "fire2/expand1x1"
  bottom: "fire2/expand3x3"
  top: "fire2/concat"
}
layer {
  name: "fire3/squeeze1x1"
  type: "Convolution"
  bottom: "fire2/concat"
  top: "conv_4"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_4"
  type: "BatchNorm"
  bottom: "conv_4"
  top: "conv_4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_4"
  type: "Scale"
  bottom: "conv_4"
  top: "fire3/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire3/squeeze1x1"
  top: "fire3/squeeze1x1"
}
layer {
  name: "fire3/expand1x1"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "conv_5"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_5"
  type: "BatchNorm"
  bottom: "conv_5"
  top: "conv_5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_5"
  type: "Scale"
  bottom: "conv_5"
  top: "fire3/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_expand1x1"
  type: "ReLU"
  bottom: "fire3/expand1x1"
  top: "fire3/expand1x1"
}
layer {
  name: "fire3/expand3x3"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "conv_6"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_6"
  type: "BatchNorm"
  bottom: "conv_6"
  top: "conv_6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_6"
  type: "Scale"
  bottom: "conv_6"
  top: "fire3/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_expand3x3"
  type: "ReLU"
  bottom: "fire3/expand3x3"
  top: "fire3/expand3x3"
}
layer {
  name: "fire3/concat"
  type: "Concat"
  bottom: "fire3/expand1x1"
  bottom: "fire3/expand3x3"
  top: "fire3/concat"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "fire3/concat"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire4/squeeze1x1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv_7"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_7"
  type: "BatchNorm"
  bottom: "conv_7"
  top: "conv_7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_7"
  type: "Scale"
  bottom: "conv_7"
  top: "fire4/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire4/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire4/squeeze1x1"
  top: "fire4/squeeze1x1"
}
layer {
  name: "fire4/expand1x1"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "fire4/expand1x1"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire4/relu_expand1x1"
  type: "ReLU"
  bottom: "fire4/expand1x1"
  top: "fire4/expand1x1"
}
layer {
  name: "fire4/expand3x3"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "conv_8"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_8"
  type: "BatchNorm"
  bottom: "conv_8"
  top: "conv_8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_8"
  type: "Scale"
  bottom: "conv_8"
  top: "fire4/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire4/relu_expand3x3"
  type: "ReLU"
  bottom: "fire4/expand3x3"
  top: "fire4/expand3x3"
}
layer {
  name: "fire4/concat"
  type: "Concat"
  bottom: "fire4/expand1x1"
  bottom: "fire4/expand3x3"
  top: "fire4/concat"
}
layer {
  name: "fire5/squeeze1x1"
  type: "Convolution"
  bottom: "fire4/concat"
  top: "conv_9"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_9"
  type: "BatchNorm"
  bottom: "conv_9"
  top: "conv_9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_9"
  type: "Scale"
  bottom: "conv_9"
  top: "fire5/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire5/squeeze1x1"
  top: "fire5/squeeze1x1"
}
layer {
  name: "fire5/expand1x1"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "conv_10"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_10"
  type: "BatchNorm"
  bottom: "conv_10"
  top: "conv_10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_10"
  type: "Scale"
  bottom: "conv_10"
  top: "fire5/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_expand1x1"
  type: "ReLU"
  bottom: "fire5/expand1x1"
  top: "fire5/expand1x1"
}
layer {
  name: "fire5/expand3x3"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "conv_11"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_11"
  type: "BatchNorm"
  bottom: "conv_11"
  top: "conv_11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_11"
  type: "Scale"
  bottom: "conv_11"
  top: "fire5/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_expand3x3"
  type: "ReLU"
  bottom: "fire5/expand3x3"
  top: "fire5/expand3x3"
}
layer {
  name: "fire5/concat"
  type: "Concat"
  bottom: "fire5/expand1x1"
  bottom: "fire5/expand3x3"
  top: "fire5/concat"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "fire5/concat"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire6/squeeze1x1"
  type: "Convolution"
  bottom: "pool5"
  top: "conv_12"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_12"
  type: "BatchNorm"
  bottom: "conv_12"
  top: "conv_12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_12"
  type: "Scale"
  bottom: "conv_12"
  top: "fire6/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire6/squeeze1x1"
  top: "fire6/squeeze1x1"
}
layer {
  name: "fire6/expand1x1"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "conv_13"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_13"
  type: "BatchNorm"
  bottom: "conv_13"
  top: "conv_13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_13"
  type: "Scale"
  bottom: "conv_13"
  top: "fire6/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_expand1x1"
  type: "ReLU"
  bottom: "fire6/expand1x1"
  top: "fire6/expand1x1"
}
layer {
  name: "fire6/expand3x3"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "conv_14"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_14"
  type: "BatchNorm"
  bottom: "conv_14"
  top: "conv_14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_14"
  type: "Scale"
  bottom: "conv_14"
  top: "fire6/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_expand3x3"
  type: "ReLU"
  bottom: "fire6/expand3x3"
  top: "fire6/expand3x3"
}
layer {
  name: "fire6/concat"
  type: "Concat"
  bottom: "fire6/expand1x1"
  bottom: "fire6/expand3x3"
  top: "fire6/concat"
}
layer {
  name: "fire7/squeeze1x1"
  type: "Convolution"
  bottom: "fire6/concat"
  top: "conv_15"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_15"
  type: "BatchNorm"
  bottom: "conv_15"
  top: "conv_15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_15"
  type: "Scale"
  bottom: "conv_15"
  top: "fire7/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire7/squeeze1x1"
  top: "fire7/squeeze1x1"
}
layer {
  name: "fire7/expand1x1"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "conv_16"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_16"
  type: "BatchNorm"
  bottom: "conv_16"
  top: "conv_16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_16"
  type: "Scale"
  bottom: "conv_16"
  top: "fire7/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_expand1x1"
  type: "ReLU"
  bottom: "fire7/expand1x1"
  top: "fire7/expand1x1"
}
layer {
  name: "fire7/expand3x3"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "conv_17"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_17"
  type: "BatchNorm"
  bottom: "conv_17"
  top: "conv_17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_17"
  type: "Scale"
  bottom: "conv_17"
  top: "fire7/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_expand3x3"
  type: "ReLU"
  bottom: "fire7/expand3x3"
  top: "fire7/expand3x3"
}
layer {
  name: "fire7/concat"
  type: "Concat"
  bottom: "fire7/expand1x1"
  bottom: "fire7/expand3x3"
  top: "fire7/concat"
}
layer {
  name: "fire8/squeeze1x1"
  type: "Convolution"
  bottom: "fire7/concat"
  top: "conv_18"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_18"
  type: "BatchNorm"
  bottom: "conv_18"
  top: "conv_18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_18"
  type: "Scale"
  bottom: "conv_18"
  top: "fire8/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire8/squeeze1x1"
  top: "fire8/squeeze1x1"
}
layer {
  name: "fire8/expand1x1"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "conv_19"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_19"
  type: "BatchNorm"
  bottom: "conv_19"
  top: "conv_19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_19"
  type: "Scale"
  bottom: "conv_19"
  top: "fire8/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_expand1x1"
  type: "ReLU"
  bottom: "fire8/expand1x1"
  top: "fire8/expand1x1"
}
layer {
  name: "fire8/expand3x3"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "conv_20"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_20"
  type: "BatchNorm"
  bottom: "conv_20"
  top: "conv_20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_20"
  type: "Scale"
  bottom: "conv_20"
  top: "fire8/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_expand3x3"
  type: "ReLU"
  bottom: "fire8/expand3x3"
  top: "fire8/expand3x3"
}
layer {
  name: "fire8/concat"
  type: "Concat"
  bottom: "fire8/expand1x1"
  bottom: "fire8/expand3x3"
  top: "fire8/concat"
}
layer {
  name: "fire9/squeeze1x1"
  type: "Convolution"
  bottom: "fire8/concat"
  top: "conv_21"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_21"
  type: "BatchNorm"
  bottom: "conv_21"
  top: "conv_21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_21"
  type: "Scale"
  bottom: "conv_21"
  top: "fire9/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire9/squeeze1x1"
  top: "fire9/squeeze1x1"
}
layer {
  name: "fire9/expand1x1"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "conv_22"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_22"
  type: "BatchNorm"
  bottom: "conv_22"
  top: "conv_22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_22"
  type: "Scale"
  bottom: "conv_22"
  top: "fire9/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_expand1x1"
  type: "ReLU"
  bottom: "fire9/expand1x1"
  top: "fire9/expand1x1"
}
layer {
  name: "fire9/expand3x3"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "conv_23"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_23"
  type: "BatchNorm"
  bottom: "conv_23"
  top: "conv_23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_23"
  type: "Scale"
  bottom: "conv_23"
  top: "fire9/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_expand3x3"
  type: "ReLU"
  bottom: "fire9/expand3x3"
  top: "fire9/expand3x3"
}
layer {
  name: "fire9/concat"
  type: "Concat"
  bottom: "fire9/expand1x1"
  bottom: "fire9/expand3x3"
  top: "fire9/concat"
}
layer {
  name: "drop9"
  type: "Dropout"
  bottom: "fire9/concat"
  top: "fire9/concat"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv10"
  type: "Convolution"
  bottom: "fire9/concat"
  top: "conv10"
  convolution_param {
    num_output: 10
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
  }
}
layer {
  name: "relu_conv10"
  type: "ReLU"
  bottom: "conv10"
  top: "conv10"
}
layer {
  name: "pool10"
  type: "Pooling"
  bottom: "conv10"
  top: "pool10"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "pool10"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "pool10"
  bottom: "label"
  top: "loss"
}
I1107 14:44:45.213084 15588 layer_factory.cpp:58] Creating layer cifar
I1107 14:44:45.218085 15588 db_leveldb.cpp:18] Opened leveldb examples/cifar10/cifar10_test_leveldb_padding
I1107 14:44:45.219084 15588 net.cpp:84] Creating Layer cifar
I1107 14:44:45.219084 15588 net.cpp:380] cifar -> data
I1107 14:44:45.219084 15588 net.cpp:380] cifar -> label
I1107 14:44:45.219084 15588 data_layer.cpp:45] output data size: 100,3,32,32
I1107 14:44:45.226083 15588 net.cpp:122] Setting up cifar
I1107 14:44:45.226083 15588 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1107 14:44:45.226083 15588 net.cpp:129] Top shape: 100 (100)
I1107 14:44:45.226083 15588 net.cpp:137] Memory required for data: 1229200
I1107 14:44:45.226083 15588 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1107 14:44:45.226083 15588 net.cpp:84] Creating Layer label_cifar_1_split
I1107 14:44:45.226083 15588 net.cpp:406] label_cifar_1_split <- label
I1107 14:44:45.226083 15588 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1107 14:44:45.227083 15588 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1107 14:44:45.227083 15588 net.cpp:122] Setting up label_cifar_1_split
I1107 14:44:45.227083 15588 net.cpp:129] Top shape: 100 (100)
I1107 14:44:45.227083 15588 net.cpp:129] Top shape: 100 (100)
I1107 14:44:45.227083 15588 net.cpp:137] Memory required for data: 1230000
I1107 14:44:45.227083 15588 layer_factory.cpp:58] Creating layer conv1
I1107 14:44:45.227083 15588 net.cpp:84] Creating Layer conv1
I1107 14:44:45.227083 15588 net.cpp:406] conv1 <- data
I1107 14:44:45.227083 15588 net.cpp:380] conv1 -> conv1
I1107 14:44:45.228085  3600 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1107 14:44:45.228085 15588 net.cpp:122] Setting up conv1
I1107 14:44:45.228085 15588 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 14:44:45.228085 15588 net.cpp:137] Memory required for data: 24270000
I1107 14:44:45.228085 15588 layer_factory.cpp:58] Creating layer bn1
I1107 14:44:45.228085 15588 net.cpp:84] Creating Layer bn1
I1107 14:44:45.228085 15588 net.cpp:406] bn1 <- conv1
I1107 14:44:45.228085 15588 net.cpp:367] bn1 -> conv1 (in-place)
I1107 14:44:45.229084 15588 net.cpp:122] Setting up bn1
I1107 14:44:45.229084 15588 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 14:44:45.229084 15588 net.cpp:137] Memory required for data: 47310000
I1107 14:44:45.229084 15588 layer_factory.cpp:58] Creating layer scale1
I1107 14:44:45.229084 15588 net.cpp:84] Creating Layer scale1
I1107 14:44:45.229084 15588 net.cpp:406] scale1 <- conv1
I1107 14:44:45.229084 15588 net.cpp:367] scale1 -> conv1 (in-place)
I1107 14:44:45.229084 15588 layer_factory.cpp:58] Creating layer scale1
I1107 14:44:45.229084 15588 net.cpp:122] Setting up scale1
I1107 14:44:45.229084 15588 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 14:44:45.229084 15588 net.cpp:137] Memory required for data: 70350000
I1107 14:44:45.229084 15588 layer_factory.cpp:58] Creating layer relu_conv1
I1107 14:44:45.229084 15588 net.cpp:84] Creating Layer relu_conv1
I1107 14:44:45.229084 15588 net.cpp:406] relu_conv1 <- conv1
I1107 14:44:45.229084 15588 net.cpp:367] relu_conv1 -> conv1 (in-place)
I1107 14:44:45.229084 15588 net.cpp:122] Setting up relu_conv1
I1107 14:44:45.229084 15588 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 14:44:45.229084 15588 net.cpp:137] Memory required for data: 93390000
I1107 14:44:45.229084 15588 layer_factory.cpp:58] Creating layer pool1
I1107 14:44:45.229084 15588 net.cpp:84] Creating Layer pool1
I1107 14:44:45.229084 15588 net.cpp:406] pool1 <- conv1
I1107 14:44:45.229084 15588 net.cpp:380] pool1 -> pool1
I1107 14:44:45.229084 15588 net.cpp:122] Setting up pool1
I1107 14:44:45.229084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.229084 15588 net.cpp:137] Memory required for data: 113460400
I1107 14:44:45.229084 15588 layer_factory.cpp:58] Creating layer fire2/squeeze1x1
I1107 14:44:45.229084 15588 net.cpp:84] Creating Layer fire2/squeeze1x1
I1107 14:44:45.229084 15588 net.cpp:406] fire2/squeeze1x1 <- pool1
I1107 14:44:45.229084 15588 net.cpp:380] fire2/squeeze1x1 -> fire2/squeeze1x1
I1107 14:44:45.231084 15588 net.cpp:122] Setting up fire2/squeeze1x1
I1107 14:44:45.231084 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.231084 15588 net.cpp:137] Memory required for data: 118478000
I1107 14:44:45.231084 15588 layer_factory.cpp:58] Creating layer fire2/relu_squeeze1x1
I1107 14:44:45.231084 15588 net.cpp:84] Creating Layer fire2/relu_squeeze1x1
I1107 14:44:45.231084 15588 net.cpp:406] fire2/relu_squeeze1x1 <- fire2/squeeze1x1
I1107 14:44:45.231084 15588 net.cpp:367] fire2/relu_squeeze1x1 -> fire2/squeeze1x1 (in-place)
I1107 14:44:45.231084 15588 net.cpp:122] Setting up fire2/relu_squeeze1x1
I1107 14:44:45.231084 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.231084 15588 net.cpp:137] Memory required for data: 123495600
I1107 14:44:45.231084 15588 layer_factory.cpp:58] Creating layer fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 14:44:45.231084 15588 net.cpp:84] Creating Layer fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 14:44:45.231084 15588 net.cpp:406] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split <- fire2/squeeze1x1
I1107 14:44:45.231084 15588 net.cpp:380] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split -> fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_0
I1107 14:44:45.231084 15588 net.cpp:380] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split -> fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_1
I1107 14:44:45.231084 15588 net.cpp:122] Setting up fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 14:44:45.231084 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.231084 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.231084 15588 net.cpp:137] Memory required for data: 133530800
I1107 14:44:45.231084 15588 layer_factory.cpp:58] Creating layer fire2/expand1x1
I1107 14:44:45.231084 15588 net.cpp:84] Creating Layer fire2/expand1x1
I1107 14:44:45.231084 15588 net.cpp:406] fire2/expand1x1 <- fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_0
I1107 14:44:45.231084 15588 net.cpp:380] fire2/expand1x1 -> conv2
I1107 14:44:45.233084 15588 net.cpp:122] Setting up fire2/expand1x1
I1107 14:44:45.233084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.233084 15588 net.cpp:137] Memory required for data: 153601200
I1107 14:44:45.233084 15588 layer_factory.cpp:58] Creating layer bn2
I1107 14:44:45.233084 15588 net.cpp:84] Creating Layer bn2
I1107 14:44:45.233084 15588 net.cpp:406] bn2 <- conv2
I1107 14:44:45.233084 15588 net.cpp:367] bn2 -> conv2 (in-place)
I1107 14:44:45.233084 15588 net.cpp:122] Setting up bn2
I1107 14:44:45.233084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.233084 15588 net.cpp:137] Memory required for data: 173671600
I1107 14:44:45.233084 15588 layer_factory.cpp:58] Creating layer scale2
I1107 14:44:45.233084 15588 net.cpp:84] Creating Layer scale2
I1107 14:44:45.233084 15588 net.cpp:406] scale2 <- conv2
I1107 14:44:45.233084 15588 net.cpp:380] scale2 -> fire2/expand1x1
I1107 14:44:45.233084 15588 layer_factory.cpp:58] Creating layer scale2
I1107 14:44:45.233084 15588 net.cpp:122] Setting up scale2
I1107 14:44:45.233084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.233084 15588 net.cpp:137] Memory required for data: 193742000
I1107 14:44:45.233084 15588 layer_factory.cpp:58] Creating layer fire2/relu_expand1x1
I1107 14:44:45.233084 15588 net.cpp:84] Creating Layer fire2/relu_expand1x1
I1107 14:44:45.233084 15588 net.cpp:406] fire2/relu_expand1x1 <- fire2/expand1x1
I1107 14:44:45.233084 15588 net.cpp:367] fire2/relu_expand1x1 -> fire2/expand1x1 (in-place)
I1107 14:44:45.234084 15588 net.cpp:122] Setting up fire2/relu_expand1x1
I1107 14:44:45.234084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.234084 15588 net.cpp:137] Memory required for data: 213812400
I1107 14:44:45.234084 15588 layer_factory.cpp:58] Creating layer fire2/expand3x3
I1107 14:44:45.234084 15588 net.cpp:84] Creating Layer fire2/expand3x3
I1107 14:44:45.234084 15588 net.cpp:406] fire2/expand3x3 <- fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_1
I1107 14:44:45.234084 15588 net.cpp:380] fire2/expand3x3 -> conv_3
I1107 14:44:45.236084 15588 net.cpp:122] Setting up fire2/expand3x3
I1107 14:44:45.236084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.236084 15588 net.cpp:137] Memory required for data: 233882800
I1107 14:44:45.236084 15588 layer_factory.cpp:58] Creating layer bn_3
I1107 14:44:45.236084 15588 net.cpp:84] Creating Layer bn_3
I1107 14:44:45.236084 15588 net.cpp:406] bn_3 <- conv_3
I1107 14:44:45.236084 15588 net.cpp:367] bn_3 -> conv_3 (in-place)
I1107 14:44:45.236084 15588 net.cpp:122] Setting up bn_3
I1107 14:44:45.236084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.236084 15588 net.cpp:137] Memory required for data: 253953200
I1107 14:44:45.236084 15588 layer_factory.cpp:58] Creating layer scale_3
I1107 14:44:45.236084 15588 net.cpp:84] Creating Layer scale_3
I1107 14:44:45.236084 15588 net.cpp:406] scale_3 <- conv_3
I1107 14:44:45.236084 15588 net.cpp:380] scale_3 -> fire2/expand3x3
I1107 14:44:45.236084 15588 layer_factory.cpp:58] Creating layer scale_3
I1107 14:44:45.236084 15588 net.cpp:122] Setting up scale_3
I1107 14:44:45.236084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.236084 15588 net.cpp:137] Memory required for data: 274023600
I1107 14:44:45.236084 15588 layer_factory.cpp:58] Creating layer fire2/relu_expand3x3
I1107 14:44:45.236084 15588 net.cpp:84] Creating Layer fire2/relu_expand3x3
I1107 14:44:45.236084 15588 net.cpp:406] fire2/relu_expand3x3 <- fire2/expand3x3
I1107 14:44:45.236084 15588 net.cpp:367] fire2/relu_expand3x3 -> fire2/expand3x3 (in-place)
I1107 14:44:45.237084 15588 net.cpp:122] Setting up fire2/relu_expand3x3
I1107 14:44:45.237084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.237084 15588 net.cpp:137] Memory required for data: 294094000
I1107 14:44:45.237084 15588 layer_factory.cpp:58] Creating layer fire2/concat
I1107 14:44:45.237084 15588 net.cpp:84] Creating Layer fire2/concat
I1107 14:44:45.237084 15588 net.cpp:406] fire2/concat <- fire2/expand1x1
I1107 14:44:45.237084 15588 net.cpp:406] fire2/concat <- fire2/expand3x3
I1107 14:44:45.237084 15588 net.cpp:380] fire2/concat -> fire2/concat
I1107 14:44:45.237084 15588 net.cpp:122] Setting up fire2/concat
I1107 14:44:45.237084 15588 net.cpp:129] Top shape: 100 128 28 28 (10035200)
I1107 14:44:45.237084 15588 net.cpp:137] Memory required for data: 334234800
I1107 14:44:45.237084 15588 layer_factory.cpp:58] Creating layer fire3/squeeze1x1
I1107 14:44:45.237084 15588 net.cpp:84] Creating Layer fire3/squeeze1x1
I1107 14:44:45.237084 15588 net.cpp:406] fire3/squeeze1x1 <- fire2/concat
I1107 14:44:45.237084 15588 net.cpp:380] fire3/squeeze1x1 -> conv_4
I1107 14:44:45.238085 15588 net.cpp:122] Setting up fire3/squeeze1x1
I1107 14:44:45.238085 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.238085 15588 net.cpp:137] Memory required for data: 339252400
I1107 14:44:45.238085 15588 layer_factory.cpp:58] Creating layer bn_4
I1107 14:44:45.238085 15588 net.cpp:84] Creating Layer bn_4
I1107 14:44:45.238085 15588 net.cpp:406] bn_4 <- conv_4
I1107 14:44:45.238085 15588 net.cpp:367] bn_4 -> conv_4 (in-place)
I1107 14:44:45.238085 15588 net.cpp:122] Setting up bn_4
I1107 14:44:45.238085 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.238085 15588 net.cpp:137] Memory required for data: 344270000
I1107 14:44:45.238085 15588 layer_factory.cpp:58] Creating layer scale_4
I1107 14:44:45.238085 15588 net.cpp:84] Creating Layer scale_4
I1107 14:44:45.238085 15588 net.cpp:406] scale_4 <- conv_4
I1107 14:44:45.239084 15588 net.cpp:380] scale_4 -> fire3/squeeze1x1
I1107 14:44:45.239084 15588 layer_factory.cpp:58] Creating layer scale_4
I1107 14:44:45.239084 15588 net.cpp:122] Setting up scale_4
I1107 14:44:45.239084 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.239084 15588 net.cpp:137] Memory required for data: 349287600
I1107 14:44:45.239084 15588 layer_factory.cpp:58] Creating layer fire3/relu_squeeze1x1
I1107 14:44:45.239084 15588 net.cpp:84] Creating Layer fire3/relu_squeeze1x1
I1107 14:44:45.239084 15588 net.cpp:406] fire3/relu_squeeze1x1 <- fire3/squeeze1x1
I1107 14:44:45.239084 15588 net.cpp:367] fire3/relu_squeeze1x1 -> fire3/squeeze1x1 (in-place)
I1107 14:44:45.239084 15588 net.cpp:122] Setting up fire3/relu_squeeze1x1
I1107 14:44:45.239084 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.239084 15588 net.cpp:137] Memory required for data: 354305200
I1107 14:44:45.239084 15588 layer_factory.cpp:58] Creating layer fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 14:44:45.239084 15588 net.cpp:84] Creating Layer fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 14:44:45.239084 15588 net.cpp:406] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split <- fire3/squeeze1x1
I1107 14:44:45.239084 15588 net.cpp:380] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split -> fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_0
I1107 14:44:45.239084 15588 net.cpp:380] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split -> fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_1
I1107 14:44:45.239084 15588 net.cpp:122] Setting up fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 14:44:45.239084 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.239084 15588 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 14:44:45.239084 15588 net.cpp:137] Memory required for data: 364340400
I1107 14:44:45.239084 15588 layer_factory.cpp:58] Creating layer fire3/expand1x1
I1107 14:44:45.239084 15588 net.cpp:84] Creating Layer fire3/expand1x1
I1107 14:44:45.239084 15588 net.cpp:406] fire3/expand1x1 <- fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_0
I1107 14:44:45.239084 15588 net.cpp:380] fire3/expand1x1 -> conv_5
I1107 14:44:45.241084 15588 net.cpp:122] Setting up fire3/expand1x1
I1107 14:44:45.241084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.241084 15588 net.cpp:137] Memory required for data: 384410800
I1107 14:44:45.241084 15588 layer_factory.cpp:58] Creating layer bn_5
I1107 14:44:45.241084 15588 net.cpp:84] Creating Layer bn_5
I1107 14:44:45.241084 15588 net.cpp:406] bn_5 <- conv_5
I1107 14:44:45.241084 15588 net.cpp:367] bn_5 -> conv_5 (in-place)
I1107 14:44:45.241084 15588 net.cpp:122] Setting up bn_5
I1107 14:44:45.241084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.241084 15588 net.cpp:137] Memory required for data: 404481200
I1107 14:44:45.241084 15588 layer_factory.cpp:58] Creating layer scale_5
I1107 14:44:45.241084 15588 net.cpp:84] Creating Layer scale_5
I1107 14:44:45.241084 15588 net.cpp:406] scale_5 <- conv_5
I1107 14:44:45.241084 15588 net.cpp:380] scale_5 -> fire3/expand1x1
I1107 14:44:45.241084 15588 layer_factory.cpp:58] Creating layer scale_5
I1107 14:44:45.241084 15588 net.cpp:122] Setting up scale_5
I1107 14:44:45.241084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.241084 15588 net.cpp:137] Memory required for data: 424551600
I1107 14:44:45.241084 15588 layer_factory.cpp:58] Creating layer fire3/relu_expand1x1
I1107 14:44:45.241084 15588 net.cpp:84] Creating Layer fire3/relu_expand1x1
I1107 14:44:45.241084 15588 net.cpp:406] fire3/relu_expand1x1 <- fire3/expand1x1
I1107 14:44:45.241084 15588 net.cpp:367] fire3/relu_expand1x1 -> fire3/expand1x1 (in-place)
I1107 14:44:45.241084 15588 net.cpp:122] Setting up fire3/relu_expand1x1
I1107 14:44:45.242084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.242084 15588 net.cpp:137] Memory required for data: 444622000
I1107 14:44:45.242084 15588 layer_factory.cpp:58] Creating layer fire3/expand3x3
I1107 14:44:45.242084 15588 net.cpp:84] Creating Layer fire3/expand3x3
I1107 14:44:45.242084 15588 net.cpp:406] fire3/expand3x3 <- fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_1
I1107 14:44:45.242084 15588 net.cpp:380] fire3/expand3x3 -> conv_6
I1107 14:44:45.243083 15588 net.cpp:122] Setting up fire3/expand3x3
I1107 14:44:45.243083 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.243083 15588 net.cpp:137] Memory required for data: 464692400
I1107 14:44:45.243083 15588 layer_factory.cpp:58] Creating layer bn_6
I1107 14:44:45.243083 15588 net.cpp:84] Creating Layer bn_6
I1107 14:44:45.243083 15588 net.cpp:406] bn_6 <- conv_6
I1107 14:44:45.243083 15588 net.cpp:367] bn_6 -> conv_6 (in-place)
I1107 14:44:45.244084 15588 net.cpp:122] Setting up bn_6
I1107 14:44:45.244084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.244084 15588 net.cpp:137] Memory required for data: 484762800
I1107 14:44:45.244084 15588 layer_factory.cpp:58] Creating layer scale_6
I1107 14:44:45.244084 15588 net.cpp:84] Creating Layer scale_6
I1107 14:44:45.244084 15588 net.cpp:406] scale_6 <- conv_6
I1107 14:44:45.244084 15588 net.cpp:380] scale_6 -> fire3/expand3x3
I1107 14:44:45.244084 15588 layer_factory.cpp:58] Creating layer scale_6
I1107 14:44:45.244084 15588 net.cpp:122] Setting up scale_6
I1107 14:44:45.244084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.244084 15588 net.cpp:137] Memory required for data: 504833200
I1107 14:44:45.244084 15588 layer_factory.cpp:58] Creating layer fire3/relu_expand3x3
I1107 14:44:45.244084 15588 net.cpp:84] Creating Layer fire3/relu_expand3x3
I1107 14:44:45.244084 15588 net.cpp:406] fire3/relu_expand3x3 <- fire3/expand3x3
I1107 14:44:45.244084 15588 net.cpp:367] fire3/relu_expand3x3 -> fire3/expand3x3 (in-place)
I1107 14:44:45.244084 15588 net.cpp:122] Setting up fire3/relu_expand3x3
I1107 14:44:45.244084 15588 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 14:44:45.244084 15588 net.cpp:137] Memory required for data: 524903600
I1107 14:44:45.244084 15588 layer_factory.cpp:58] Creating layer fire3/concat
I1107 14:44:45.244084 15588 net.cpp:84] Creating Layer fire3/concat
I1107 14:44:45.244084 15588 net.cpp:406] fire3/concat <- fire3/expand1x1
I1107 14:44:45.244084 15588 net.cpp:406] fire3/concat <- fire3/expand3x3
I1107 14:44:45.244084 15588 net.cpp:380] fire3/concat -> fire3/concat
I1107 14:44:45.244084 15588 net.cpp:122] Setting up fire3/concat
I1107 14:44:45.244084 15588 net.cpp:129] Top shape: 100 128 28 28 (10035200)
I1107 14:44:45.244084 15588 net.cpp:137] Memory required for data: 565044400
I1107 14:44:45.244084 15588 layer_factory.cpp:58] Creating layer pool3
I1107 14:44:45.244084 15588 net.cpp:84] Creating Layer pool3
I1107 14:44:45.244084 15588 net.cpp:406] pool3 <- fire3/concat
I1107 14:44:45.244084 15588 net.cpp:380] pool3 -> pool3
I1107 14:44:45.244084 15588 net.cpp:122] Setting up pool3
I1107 14:44:45.244084 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.244084 15588 net.cpp:137] Memory required for data: 575079600
I1107 14:44:45.244084 15588 layer_factory.cpp:58] Creating layer fire4/squeeze1x1
I1107 14:44:45.244084 15588 net.cpp:84] Creating Layer fire4/squeeze1x1
I1107 14:44:45.244084 15588 net.cpp:406] fire4/squeeze1x1 <- pool3
I1107 14:44:45.244084 15588 net.cpp:380] fire4/squeeze1x1 -> conv_7
I1107 14:44:45.246084 15588 net.cpp:122] Setting up fire4/squeeze1x1
I1107 14:44:45.246084 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.246084 15588 net.cpp:137] Memory required for data: 577588400
I1107 14:44:45.246084 15588 layer_factory.cpp:58] Creating layer bn_7
I1107 14:44:45.246084 15588 net.cpp:84] Creating Layer bn_7
I1107 14:44:45.246084 15588 net.cpp:406] bn_7 <- conv_7
I1107 14:44:45.246084 15588 net.cpp:367] bn_7 -> conv_7 (in-place)
I1107 14:44:45.246084 15588 net.cpp:122] Setting up bn_7
I1107 14:44:45.246084 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.246084 15588 net.cpp:137] Memory required for data: 580097200
I1107 14:44:45.246084 15588 layer_factory.cpp:58] Creating layer scale_7
I1107 14:44:45.246084 15588 net.cpp:84] Creating Layer scale_7
I1107 14:44:45.246084 15588 net.cpp:406] scale_7 <- conv_7
I1107 14:44:45.246084 15588 net.cpp:380] scale_7 -> fire4/squeeze1x1
I1107 14:44:45.246084 15588 layer_factory.cpp:58] Creating layer scale_7
I1107 14:44:45.246084 15588 net.cpp:122] Setting up scale_7
I1107 14:44:45.246084 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.246084 15588 net.cpp:137] Memory required for data: 582606000
I1107 14:44:45.246084 15588 layer_factory.cpp:58] Creating layer fire4/relu_squeeze1x1
I1107 14:44:45.246084 15588 net.cpp:84] Creating Layer fire4/relu_squeeze1x1
I1107 14:44:45.246084 15588 net.cpp:406] fire4/relu_squeeze1x1 <- fire4/squeeze1x1
I1107 14:44:45.246084 15588 net.cpp:367] fire4/relu_squeeze1x1 -> fire4/squeeze1x1 (in-place)
I1107 14:44:45.246084 15588 net.cpp:122] Setting up fire4/relu_squeeze1x1
I1107 14:44:45.246084 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.246084 15588 net.cpp:137] Memory required for data: 585114800
I1107 14:44:45.246084 15588 layer_factory.cpp:58] Creating layer fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 14:44:45.246084 15588 net.cpp:84] Creating Layer fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 14:44:45.246084 15588 net.cpp:406] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split <- fire4/squeeze1x1
I1107 14:44:45.246084 15588 net.cpp:380] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split -> fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_0
I1107 14:44:45.246084 15588 net.cpp:380] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split -> fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_1
I1107 14:44:45.246084 15588 net.cpp:122] Setting up fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 14:44:45.246084 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.246084 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.246084 15588 net.cpp:137] Memory required for data: 590132400
I1107 14:44:45.246084 15588 layer_factory.cpp:58] Creating layer fire4/expand1x1
I1107 14:44:45.246084 15588 net.cpp:84] Creating Layer fire4/expand1x1
I1107 14:44:45.246084 15588 net.cpp:406] fire4/expand1x1 <- fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_0
I1107 14:44:45.246084 15588 net.cpp:380] fire4/expand1x1 -> fire4/expand1x1
I1107 14:44:45.248087 15588 net.cpp:122] Setting up fire4/expand1x1
I1107 14:44:45.248087 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.248087 15588 net.cpp:137] Memory required for data: 600167600
I1107 14:44:45.248087 15588 layer_factory.cpp:58] Creating layer fire4/relu_expand1x1
I1107 14:44:45.248087 15588 net.cpp:84] Creating Layer fire4/relu_expand1x1
I1107 14:44:45.248087 15588 net.cpp:406] fire4/relu_expand1x1 <- fire4/expand1x1
I1107 14:44:45.248087 15588 net.cpp:367] fire4/relu_expand1x1 -> fire4/expand1x1 (in-place)
I1107 14:44:45.248587 15588 net.cpp:122] Setting up fire4/relu_expand1x1
I1107 14:44:45.248587 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.248587 15588 net.cpp:137] Memory required for data: 610202800
I1107 14:44:45.248587 15588 layer_factory.cpp:58] Creating layer fire4/expand3x3
I1107 14:44:45.248587 15588 net.cpp:84] Creating Layer fire4/expand3x3
I1107 14:44:45.248587 15588 net.cpp:406] fire4/expand3x3 <- fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_1
I1107 14:44:45.248587 15588 net.cpp:380] fire4/expand3x3 -> conv_8
I1107 14:44:45.250088 15588 net.cpp:122] Setting up fire4/expand3x3
I1107 14:44:45.250088 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.250088 15588 net.cpp:137] Memory required for data: 620238000
I1107 14:44:45.250088 15588 layer_factory.cpp:58] Creating layer bn_8
I1107 14:44:45.250088 15588 net.cpp:84] Creating Layer bn_8
I1107 14:44:45.250088 15588 net.cpp:406] bn_8 <- conv_8
I1107 14:44:45.250088 15588 net.cpp:367] bn_8 -> conv_8 (in-place)
I1107 14:44:45.250088 15588 net.cpp:122] Setting up bn_8
I1107 14:44:45.250088 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.250088 15588 net.cpp:137] Memory required for data: 630273200
I1107 14:44:45.250088 15588 layer_factory.cpp:58] Creating layer scale_8
I1107 14:44:45.250088 15588 net.cpp:84] Creating Layer scale_8
I1107 14:44:45.250088 15588 net.cpp:406] scale_8 <- conv_8
I1107 14:44:45.250088 15588 net.cpp:380] scale_8 -> fire4/expand3x3
I1107 14:44:45.250088 15588 layer_factory.cpp:58] Creating layer scale_8
I1107 14:44:45.250588 15588 net.cpp:122] Setting up scale_8
I1107 14:44:45.250588 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.250588 15588 net.cpp:137] Memory required for data: 640308400
I1107 14:44:45.250588 15588 layer_factory.cpp:58] Creating layer fire4/relu_expand3x3
I1107 14:44:45.250588 15588 net.cpp:84] Creating Layer fire4/relu_expand3x3
I1107 14:44:45.250588 15588 net.cpp:406] fire4/relu_expand3x3 <- fire4/expand3x3
I1107 14:44:45.250588 15588 net.cpp:367] fire4/relu_expand3x3 -> fire4/expand3x3 (in-place)
I1107 14:44:45.250588 15588 net.cpp:122] Setting up fire4/relu_expand3x3
I1107 14:44:45.250588 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.250588 15588 net.cpp:137] Memory required for data: 650343600
I1107 14:44:45.250588 15588 layer_factory.cpp:58] Creating layer fire4/concat
I1107 14:44:45.250588 15588 net.cpp:84] Creating Layer fire4/concat
I1107 14:44:45.250588 15588 net.cpp:406] fire4/concat <- fire4/expand1x1
I1107 14:44:45.250588 15588 net.cpp:406] fire4/concat <- fire4/expand3x3
I1107 14:44:45.250588 15588 net.cpp:380] fire4/concat -> fire4/concat
I1107 14:44:45.250588 15588 net.cpp:122] Setting up fire4/concat
I1107 14:44:45.250588 15588 net.cpp:129] Top shape: 100 256 14 14 (5017600)
I1107 14:44:45.250588 15588 net.cpp:137] Memory required for data: 670414000
I1107 14:44:45.250588 15588 layer_factory.cpp:58] Creating layer fire5/squeeze1x1
I1107 14:44:45.250588 15588 net.cpp:84] Creating Layer fire5/squeeze1x1
I1107 14:44:45.250588 15588 net.cpp:406] fire5/squeeze1x1 <- fire4/concat
I1107 14:44:45.250588 15588 net.cpp:380] fire5/squeeze1x1 -> conv_9
I1107 14:44:45.252089 15588 net.cpp:122] Setting up fire5/squeeze1x1
I1107 14:44:45.252089 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.252089 15588 net.cpp:137] Memory required for data: 672922800
I1107 14:44:45.252089 15588 layer_factory.cpp:58] Creating layer bn_9
I1107 14:44:45.252089 15588 net.cpp:84] Creating Layer bn_9
I1107 14:44:45.252089 15588 net.cpp:406] bn_9 <- conv_9
I1107 14:44:45.252089 15588 net.cpp:367] bn_9 -> conv_9 (in-place)
I1107 14:44:45.252089 15588 net.cpp:122] Setting up bn_9
I1107 14:44:45.252089 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.252089 15588 net.cpp:137] Memory required for data: 675431600
I1107 14:44:45.252089 15588 layer_factory.cpp:58] Creating layer scale_9
I1107 14:44:45.252089 15588 net.cpp:84] Creating Layer scale_9
I1107 14:44:45.252089 15588 net.cpp:406] scale_9 <- conv_9
I1107 14:44:45.252089 15588 net.cpp:380] scale_9 -> fire5/squeeze1x1
I1107 14:44:45.252588 15588 layer_factory.cpp:58] Creating layer scale_9
I1107 14:44:45.252588 15588 net.cpp:122] Setting up scale_9
I1107 14:44:45.252588 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.252588 15588 net.cpp:137] Memory required for data: 677940400
I1107 14:44:45.252588 15588 layer_factory.cpp:58] Creating layer fire5/relu_squeeze1x1
I1107 14:44:45.252588 15588 net.cpp:84] Creating Layer fire5/relu_squeeze1x1
I1107 14:44:45.252588 15588 net.cpp:406] fire5/relu_squeeze1x1 <- fire5/squeeze1x1
I1107 14:44:45.252588 15588 net.cpp:367] fire5/relu_squeeze1x1 -> fire5/squeeze1x1 (in-place)
I1107 14:44:45.252588 15588 net.cpp:122] Setting up fire5/relu_squeeze1x1
I1107 14:44:45.252588 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.252588 15588 net.cpp:137] Memory required for data: 680449200
I1107 14:44:45.252588 15588 layer_factory.cpp:58] Creating layer fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 14:44:45.252588 15588 net.cpp:84] Creating Layer fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 14:44:45.252588 15588 net.cpp:406] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split <- fire5/squeeze1x1
I1107 14:44:45.252588 15588 net.cpp:380] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split -> fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_0
I1107 14:44:45.252588 15588 net.cpp:380] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split -> fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_1
I1107 14:44:45.253088 15588 net.cpp:122] Setting up fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 14:44:45.253088 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.253088 15588 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 14:44:45.253088 15588 net.cpp:137] Memory required for data: 685466800
I1107 14:44:45.253088 15588 layer_factory.cpp:58] Creating layer fire5/expand1x1
I1107 14:44:45.253088 15588 net.cpp:84] Creating Layer fire5/expand1x1
I1107 14:44:45.253088 15588 net.cpp:406] fire5/expand1x1 <- fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_0
I1107 14:44:45.253088 15588 net.cpp:380] fire5/expand1x1 -> conv_10
I1107 14:44:45.254087 15588 net.cpp:122] Setting up fire5/expand1x1
I1107 14:44:45.254087 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.254087 15588 net.cpp:137] Memory required for data: 695502000
I1107 14:44:45.254087 15588 layer_factory.cpp:58] Creating layer bn_10
I1107 14:44:45.254087 15588 net.cpp:84] Creating Layer bn_10
I1107 14:44:45.254087 15588 net.cpp:406] bn_10 <- conv_10
I1107 14:44:45.254087 15588 net.cpp:367] bn_10 -> conv_10 (in-place)
I1107 14:44:45.254588 15588 net.cpp:122] Setting up bn_10
I1107 14:44:45.254588 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.254588 15588 net.cpp:137] Memory required for data: 705537200
I1107 14:44:45.254588 15588 layer_factory.cpp:58] Creating layer scale_10
I1107 14:44:45.254588 15588 net.cpp:84] Creating Layer scale_10
I1107 14:44:45.254588 15588 net.cpp:406] scale_10 <- conv_10
I1107 14:44:45.254588 15588 net.cpp:380] scale_10 -> fire5/expand1x1
I1107 14:44:45.254588 15588 layer_factory.cpp:58] Creating layer scale_10
I1107 14:44:45.254588 15588 net.cpp:122] Setting up scale_10
I1107 14:44:45.254588 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.254588 15588 net.cpp:137] Memory required for data: 715572400
I1107 14:44:45.254588 15588 layer_factory.cpp:58] Creating layer fire5/relu_expand1x1
I1107 14:44:45.254588 15588 net.cpp:84] Creating Layer fire5/relu_expand1x1
I1107 14:44:45.254588 15588 net.cpp:406] fire5/relu_expand1x1 <- fire5/expand1x1
I1107 14:44:45.254588 15588 net.cpp:367] fire5/relu_expand1x1 -> fire5/expand1x1 (in-place)
I1107 14:44:45.255089 15588 net.cpp:122] Setting up fire5/relu_expand1x1
I1107 14:44:45.255089 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.255089 15588 net.cpp:137] Memory required for data: 725607600
I1107 14:44:45.255089 15588 layer_factory.cpp:58] Creating layer fire5/expand3x3
I1107 14:44:45.255089 15588 net.cpp:84] Creating Layer fire5/expand3x3
I1107 14:44:45.255089 15588 net.cpp:406] fire5/expand3x3 <- fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_1
I1107 14:44:45.255089 15588 net.cpp:380] fire5/expand3x3 -> conv_11
I1107 14:44:45.256088 15588 net.cpp:122] Setting up fire5/expand3x3
I1107 14:44:45.256088 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.256088 15588 net.cpp:137] Memory required for data: 735642800
I1107 14:44:45.256588 15588 layer_factory.cpp:58] Creating layer bn_11
I1107 14:44:45.256588 15588 net.cpp:84] Creating Layer bn_11
I1107 14:44:45.256588 15588 net.cpp:406] bn_11 <- conv_11
I1107 14:44:45.256588 15588 net.cpp:367] bn_11 -> conv_11 (in-place)
I1107 14:44:45.256588 15588 net.cpp:122] Setting up bn_11
I1107 14:44:45.256588 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.256588 15588 net.cpp:137] Memory required for data: 745678000
I1107 14:44:45.256588 15588 layer_factory.cpp:58] Creating layer scale_11
I1107 14:44:45.256588 15588 net.cpp:84] Creating Layer scale_11
I1107 14:44:45.256588 15588 net.cpp:406] scale_11 <- conv_11
I1107 14:44:45.256588 15588 net.cpp:380] scale_11 -> fire5/expand3x3
I1107 14:44:45.256588 15588 layer_factory.cpp:58] Creating layer scale_11
I1107 14:44:45.256588 15588 net.cpp:122] Setting up scale_11
I1107 14:44:45.256588 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.256588 15588 net.cpp:137] Memory required for data: 755713200
I1107 14:44:45.256588 15588 layer_factory.cpp:58] Creating layer fire5/relu_expand3x3
I1107 14:44:45.256588 15588 net.cpp:84] Creating Layer fire5/relu_expand3x3
I1107 14:44:45.256588 15588 net.cpp:406] fire5/relu_expand3x3 <- fire5/expand3x3
I1107 14:44:45.256588 15588 net.cpp:367] fire5/relu_expand3x3 -> fire5/expand3x3 (in-place)
I1107 14:44:45.257088 15588 net.cpp:122] Setting up fire5/relu_expand3x3
I1107 14:44:45.257088 15588 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 14:44:45.257088 15588 net.cpp:137] Memory required for data: 765748400
I1107 14:44:45.257088 15588 layer_factory.cpp:58] Creating layer fire5/concat
I1107 14:44:45.257088 15588 net.cpp:84] Creating Layer fire5/concat
I1107 14:44:45.257088 15588 net.cpp:406] fire5/concat <- fire5/expand1x1
I1107 14:44:45.257088 15588 net.cpp:406] fire5/concat <- fire5/expand3x3
I1107 14:44:45.257088 15588 net.cpp:380] fire5/concat -> fire5/concat
I1107 14:44:45.257088 15588 net.cpp:122] Setting up fire5/concat
I1107 14:44:45.257088 15588 net.cpp:129] Top shape: 100 256 14 14 (5017600)
I1107 14:44:45.257088 15588 net.cpp:137] Memory required for data: 785818800
I1107 14:44:45.257088 15588 layer_factory.cpp:58] Creating layer pool5
I1107 14:44:45.257088 15588 net.cpp:84] Creating Layer pool5
I1107 14:44:45.257088 15588 net.cpp:406] pool5 <- fire5/concat
I1107 14:44:45.257088 15588 net.cpp:380] pool5 -> pool5
I1107 14:44:45.257088 15588 net.cpp:122] Setting up pool5
I1107 14:44:45.257088 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.257088 15588 net.cpp:137] Memory required for data: 790836400
I1107 14:44:45.257088 15588 layer_factory.cpp:58] Creating layer fire6/squeeze1x1
I1107 14:44:45.257088 15588 net.cpp:84] Creating Layer fire6/squeeze1x1
I1107 14:44:45.257088 15588 net.cpp:406] fire6/squeeze1x1 <- pool5
I1107 14:44:45.257088 15588 net.cpp:380] fire6/squeeze1x1 -> conv_12
I1107 14:44:45.258587 15588 net.cpp:122] Setting up fire6/squeeze1x1
I1107 14:44:45.258587 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.258587 15588 net.cpp:137] Memory required for data: 791777200
I1107 14:44:45.258587 15588 layer_factory.cpp:58] Creating layer bn_12
I1107 14:44:45.258587 15588 net.cpp:84] Creating Layer bn_12
I1107 14:44:45.258587 15588 net.cpp:406] bn_12 <- conv_12
I1107 14:44:45.258587 15588 net.cpp:367] bn_12 -> conv_12 (in-place)
I1107 14:44:45.258587 15588 net.cpp:122] Setting up bn_12
I1107 14:44:45.258587 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.258587 15588 net.cpp:137] Memory required for data: 792718000
I1107 14:44:45.258587 15588 layer_factory.cpp:58] Creating layer scale_12
I1107 14:44:45.258587 15588 net.cpp:84] Creating Layer scale_12
I1107 14:44:45.258587 15588 net.cpp:406] scale_12 <- conv_12
I1107 14:44:45.258587 15588 net.cpp:380] scale_12 -> fire6/squeeze1x1
I1107 14:44:45.258587 15588 layer_factory.cpp:58] Creating layer scale_12
I1107 14:44:45.259088 15588 net.cpp:122] Setting up scale_12
I1107 14:44:45.259088 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.259088 15588 net.cpp:137] Memory required for data: 793658800
I1107 14:44:45.259088 15588 layer_factory.cpp:58] Creating layer fire6/relu_squeeze1x1
I1107 14:44:45.259088 15588 net.cpp:84] Creating Layer fire6/relu_squeeze1x1
I1107 14:44:45.259088 15588 net.cpp:406] fire6/relu_squeeze1x1 <- fire6/squeeze1x1
I1107 14:44:45.259088 15588 net.cpp:367] fire6/relu_squeeze1x1 -> fire6/squeeze1x1 (in-place)
I1107 14:44:45.259088 15588 net.cpp:122] Setting up fire6/relu_squeeze1x1
I1107 14:44:45.259088 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.259088 15588 net.cpp:137] Memory required for data: 794599600
I1107 14:44:45.259088 15588 layer_factory.cpp:58] Creating layer fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 14:44:45.259088 15588 net.cpp:84] Creating Layer fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 14:44:45.259088 15588 net.cpp:406] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split <- fire6/squeeze1x1
I1107 14:44:45.259088 15588 net.cpp:380] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split -> fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_0
I1107 14:44:45.259088 15588 net.cpp:380] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split -> fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_1
I1107 14:44:45.259088 15588 net.cpp:122] Setting up fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 14:44:45.259088 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.259088 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.259088 15588 net.cpp:137] Memory required for data: 796481200
I1107 14:44:45.259588 15588 layer_factory.cpp:58] Creating layer fire6/expand1x1
I1107 14:44:45.259588 15588 net.cpp:84] Creating Layer fire6/expand1x1
I1107 14:44:45.259588 15588 net.cpp:406] fire6/expand1x1 <- fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_0
I1107 14:44:45.259588 15588 net.cpp:380] fire6/expand1x1 -> conv_13
I1107 14:44:45.260588 15588 net.cpp:122] Setting up fire6/expand1x1
I1107 14:44:45.260588 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.260588 15588 net.cpp:137] Memory required for data: 800244400
I1107 14:44:45.260588 15588 layer_factory.cpp:58] Creating layer bn_13
I1107 14:44:45.260588 15588 net.cpp:84] Creating Layer bn_13
I1107 14:44:45.260588 15588 net.cpp:406] bn_13 <- conv_13
I1107 14:44:45.260588 15588 net.cpp:367] bn_13 -> conv_13 (in-place)
I1107 14:44:45.260588 15588 net.cpp:122] Setting up bn_13
I1107 14:44:45.260588 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.260588 15588 net.cpp:137] Memory required for data: 804007600
I1107 14:44:45.260588 15588 layer_factory.cpp:58] Creating layer scale_13
I1107 14:44:45.260588 15588 net.cpp:84] Creating Layer scale_13
I1107 14:44:45.260588 15588 net.cpp:406] scale_13 <- conv_13
I1107 14:44:45.260588 15588 net.cpp:380] scale_13 -> fire6/expand1x1
I1107 14:44:45.260588 15588 layer_factory.cpp:58] Creating layer scale_13
I1107 14:44:45.261088 15588 net.cpp:122] Setting up scale_13
I1107 14:44:45.261088 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.261088 15588 net.cpp:137] Memory required for data: 807770800
I1107 14:44:45.261088 15588 layer_factory.cpp:58] Creating layer fire6/relu_expand1x1
I1107 14:44:45.261088 15588 net.cpp:84] Creating Layer fire6/relu_expand1x1
I1107 14:44:45.261088 15588 net.cpp:406] fire6/relu_expand1x1 <- fire6/expand1x1
I1107 14:44:45.261088 15588 net.cpp:367] fire6/relu_expand1x1 -> fire6/expand1x1 (in-place)
I1107 14:44:45.261589 15588 net.cpp:122] Setting up fire6/relu_expand1x1
I1107 14:44:45.261589 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.261589 15588 net.cpp:137] Memory required for data: 811534000
I1107 14:44:45.261589 15588 layer_factory.cpp:58] Creating layer fire6/expand3x3
I1107 14:44:45.261589 15588 net.cpp:84] Creating Layer fire6/expand3x3
I1107 14:44:45.261589 15588 net.cpp:406] fire6/expand3x3 <- fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_1
I1107 14:44:45.261589 15588 net.cpp:380] fire6/expand3x3 -> conv_14
I1107 14:44:45.263088 15588 net.cpp:122] Setting up fire6/expand3x3
I1107 14:44:45.263088 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.263088 15588 net.cpp:137] Memory required for data: 815297200
I1107 14:44:45.263088 15588 layer_factory.cpp:58] Creating layer bn_14
I1107 14:44:45.263088 15588 net.cpp:84] Creating Layer bn_14
I1107 14:44:45.263088 15588 net.cpp:406] bn_14 <- conv_14
I1107 14:44:45.263088 15588 net.cpp:367] bn_14 -> conv_14 (in-place)
I1107 14:44:45.263088 15588 net.cpp:122] Setting up bn_14
I1107 14:44:45.263088 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.263088 15588 net.cpp:137] Memory required for data: 819060400
I1107 14:44:45.263088 15588 layer_factory.cpp:58] Creating layer scale_14
I1107 14:44:45.263088 15588 net.cpp:84] Creating Layer scale_14
I1107 14:44:45.263088 15588 net.cpp:406] scale_14 <- conv_14
I1107 14:44:45.263088 15588 net.cpp:380] scale_14 -> fire6/expand3x3
I1107 14:44:45.263088 15588 layer_factory.cpp:58] Creating layer scale_14
I1107 14:44:45.264092 15588 net.cpp:122] Setting up scale_14
I1107 14:44:45.264092 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.264092 15588 net.cpp:137] Memory required for data: 822823600
I1107 14:44:45.264092 15588 layer_factory.cpp:58] Creating layer fire6/relu_expand3x3
I1107 14:44:45.264092 15588 net.cpp:84] Creating Layer fire6/relu_expand3x3
I1107 14:44:45.264092 15588 net.cpp:406] fire6/relu_expand3x3 <- fire6/expand3x3
I1107 14:44:45.264092 15588 net.cpp:367] fire6/relu_expand3x3 -> fire6/expand3x3 (in-place)
I1107 14:44:45.264092 15588 net.cpp:122] Setting up fire6/relu_expand3x3
I1107 14:44:45.264092 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.264092 15588 net.cpp:137] Memory required for data: 826586800
I1107 14:44:45.264092 15588 layer_factory.cpp:58] Creating layer fire6/concat
I1107 14:44:45.264092 15588 net.cpp:84] Creating Layer fire6/concat
I1107 14:44:45.264092 15588 net.cpp:406] fire6/concat <- fire6/expand1x1
I1107 14:44:45.264092 15588 net.cpp:406] fire6/concat <- fire6/expand3x3
I1107 14:44:45.264092 15588 net.cpp:380] fire6/concat -> fire6/concat
I1107 14:44:45.264092 15588 net.cpp:122] Setting up fire6/concat
I1107 14:44:45.264092 15588 net.cpp:129] Top shape: 100 384 7 7 (1881600)
I1107 14:44:45.264092 15588 net.cpp:137] Memory required for data: 834113200
I1107 14:44:45.264092 15588 layer_factory.cpp:58] Creating layer fire7/squeeze1x1
I1107 14:44:45.264092 15588 net.cpp:84] Creating Layer fire7/squeeze1x1
I1107 14:44:45.264092 15588 net.cpp:406] fire7/squeeze1x1 <- fire6/concat
I1107 14:44:45.264092 15588 net.cpp:380] fire7/squeeze1x1 -> conv_15
I1107 14:44:45.265092 15588 net.cpp:122] Setting up fire7/squeeze1x1
I1107 14:44:45.265092 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.265092 15588 net.cpp:137] Memory required for data: 835054000
I1107 14:44:45.265092 15588 layer_factory.cpp:58] Creating layer bn_15
I1107 14:44:45.265092 15588 net.cpp:84] Creating Layer bn_15
I1107 14:44:45.265092 15588 net.cpp:406] bn_15 <- conv_15
I1107 14:44:45.265092 15588 net.cpp:367] bn_15 -> conv_15 (in-place)
I1107 14:44:45.265092 15588 net.cpp:122] Setting up bn_15
I1107 14:44:45.266093 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.266093 15588 net.cpp:137] Memory required for data: 835994800
I1107 14:44:45.266093 15588 layer_factory.cpp:58] Creating layer scale_15
I1107 14:44:45.266093 15588 net.cpp:84] Creating Layer scale_15
I1107 14:44:45.266093 15588 net.cpp:406] scale_15 <- conv_15
I1107 14:44:45.266093 15588 net.cpp:380] scale_15 -> fire7/squeeze1x1
I1107 14:44:45.266093 15588 layer_factory.cpp:58] Creating layer scale_15
I1107 14:44:45.266093 15588 net.cpp:122] Setting up scale_15
I1107 14:44:45.266093 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.266093 15588 net.cpp:137] Memory required for data: 836935600
I1107 14:44:45.266093 15588 layer_factory.cpp:58] Creating layer fire7/relu_squeeze1x1
I1107 14:44:45.266093 15588 net.cpp:84] Creating Layer fire7/relu_squeeze1x1
I1107 14:44:45.266093 15588 net.cpp:406] fire7/relu_squeeze1x1 <- fire7/squeeze1x1
I1107 14:44:45.266093 15588 net.cpp:367] fire7/relu_squeeze1x1 -> fire7/squeeze1x1 (in-place)
I1107 14:44:45.266093 15588 net.cpp:122] Setting up fire7/relu_squeeze1x1
I1107 14:44:45.266093 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.266093 15588 net.cpp:137] Memory required for data: 837876400
I1107 14:44:45.266093 15588 layer_factory.cpp:58] Creating layer fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 14:44:45.266093 15588 net.cpp:84] Creating Layer fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 14:44:45.266093 15588 net.cpp:406] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split <- fire7/squeeze1x1
I1107 14:44:45.266093 15588 net.cpp:380] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split -> fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_0
I1107 14:44:45.266093 15588 net.cpp:380] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split -> fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_1
I1107 14:44:45.266093 15588 net.cpp:122] Setting up fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 14:44:45.266093 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.266093 15588 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 14:44:45.266093 15588 net.cpp:137] Memory required for data: 839758000
I1107 14:44:45.266093 15588 layer_factory.cpp:58] Creating layer fire7/expand1x1
I1107 14:44:45.266093 15588 net.cpp:84] Creating Layer fire7/expand1x1
I1107 14:44:45.266093 15588 net.cpp:406] fire7/expand1x1 <- fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_0
I1107 14:44:45.266093 15588 net.cpp:380] fire7/expand1x1 -> conv_16
I1107 14:44:45.267091 15588 net.cpp:122] Setting up fire7/expand1x1
I1107 14:44:45.267091 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.267091 15588 net.cpp:137] Memory required for data: 843521200
I1107 14:44:45.267091 15588 layer_factory.cpp:58] Creating layer bn_16
I1107 14:44:45.267091 15588 net.cpp:84] Creating Layer bn_16
I1107 14:44:45.267091 15588 net.cpp:406] bn_16 <- conv_16
I1107 14:44:45.267091 15588 net.cpp:367] bn_16 -> conv_16 (in-place)
I1107 14:44:45.268092 15588 net.cpp:122] Setting up bn_16
I1107 14:44:45.268092 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.268092 15588 net.cpp:137] Memory required for data: 847284400
I1107 14:44:45.268092 15588 layer_factory.cpp:58] Creating layer scale_16
I1107 14:44:45.268092 15588 net.cpp:84] Creating Layer scale_16
I1107 14:44:45.268092 15588 net.cpp:406] scale_16 <- conv_16
I1107 14:44:45.268092 15588 net.cpp:380] scale_16 -> fire7/expand1x1
I1107 14:44:45.268092 15588 layer_factory.cpp:58] Creating layer scale_16
I1107 14:44:45.268092 15588 net.cpp:122] Setting up scale_16
I1107 14:44:45.268092 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.268092 15588 net.cpp:137] Memory required for data: 851047600
I1107 14:44:45.268092 15588 layer_factory.cpp:58] Creating layer fire7/relu_expand1x1
I1107 14:44:45.268092 15588 net.cpp:84] Creating Layer fire7/relu_expand1x1
I1107 14:44:45.268092 15588 net.cpp:406] fire7/relu_expand1x1 <- fire7/expand1x1
I1107 14:44:45.268092 15588 net.cpp:367] fire7/relu_expand1x1 -> fire7/expand1x1 (in-place)
I1107 14:44:45.268092 15588 net.cpp:122] Setting up fire7/relu_expand1x1
I1107 14:44:45.268092 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.268092 15588 net.cpp:137] Memory required for data: 854810800
I1107 14:44:45.268092 15588 layer_factory.cpp:58] Creating layer fire7/expand3x3
I1107 14:44:45.268092 15588 net.cpp:84] Creating Layer fire7/expand3x3
I1107 14:44:45.268092 15588 net.cpp:406] fire7/expand3x3 <- fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_1
I1107 14:44:45.268092 15588 net.cpp:380] fire7/expand3x3 -> conv_17
I1107 14:44:45.270092 15588 net.cpp:122] Setting up fire7/expand3x3
I1107 14:44:45.270092 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.270092 15588 net.cpp:137] Memory required for data: 858574000
I1107 14:44:45.270092 15588 layer_factory.cpp:58] Creating layer bn_17
I1107 14:44:45.270092 15588 net.cpp:84] Creating Layer bn_17
I1107 14:44:45.270092 15588 net.cpp:406] bn_17 <- conv_17
I1107 14:44:45.270092 15588 net.cpp:367] bn_17 -> conv_17 (in-place)
I1107 14:44:45.271092 15588 net.cpp:122] Setting up bn_17
I1107 14:44:45.271092 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.271092 15588 net.cpp:137] Memory required for data: 862337200
I1107 14:44:45.271092 15588 layer_factory.cpp:58] Creating layer scale_17
I1107 14:44:45.271092 15588 net.cpp:84] Creating Layer scale_17
I1107 14:44:45.271092 15588 net.cpp:406] scale_17 <- conv_17
I1107 14:44:45.271092 15588 net.cpp:380] scale_17 -> fire7/expand3x3
I1107 14:44:45.271092 15588 layer_factory.cpp:58] Creating layer scale_17
I1107 14:44:45.271092 15588 net.cpp:122] Setting up scale_17
I1107 14:44:45.271092 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.271092 15588 net.cpp:137] Memory required for data: 866100400
I1107 14:44:45.271092 15588 layer_factory.cpp:58] Creating layer fire7/relu_expand3x3
I1107 14:44:45.271092 15588 net.cpp:84] Creating Layer fire7/relu_expand3x3
I1107 14:44:45.271092 15588 net.cpp:406] fire7/relu_expand3x3 <- fire7/expand3x3
I1107 14:44:45.271092 15588 net.cpp:367] fire7/relu_expand3x3 -> fire7/expand3x3 (in-place)
I1107 14:44:45.272092 15588 net.cpp:122] Setting up fire7/relu_expand3x3
I1107 14:44:45.272092 15588 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 14:44:45.272092 15588 net.cpp:137] Memory required for data: 869863600
I1107 14:44:45.272092 15588 layer_factory.cpp:58] Creating layer fire7/concat
I1107 14:44:45.272092 15588 net.cpp:84] Creating Layer fire7/concat
I1107 14:44:45.272092 15588 net.cpp:406] fire7/concat <- fire7/expand1x1
I1107 14:44:45.272092 15588 net.cpp:406] fire7/concat <- fire7/expand3x3
I1107 14:44:45.272092 15588 net.cpp:380] fire7/concat -> fire7/concat
I1107 14:44:45.272092 15588 net.cpp:122] Setting up fire7/concat
I1107 14:44:45.272092 15588 net.cpp:129] Top shape: 100 384 7 7 (1881600)
I1107 14:44:45.272092 15588 net.cpp:137] Memory required for data: 877390000
I1107 14:44:45.272092 15588 layer_factory.cpp:58] Creating layer fire8/squeeze1x1
I1107 14:44:45.272092 15588 net.cpp:84] Creating Layer fire8/squeeze1x1
I1107 14:44:45.272092 15588 net.cpp:406] fire8/squeeze1x1 <- fire7/concat
I1107 14:44:45.272092 15588 net.cpp:380] fire8/squeeze1x1 -> conv_18
I1107 14:44:45.273092 15588 net.cpp:122] Setting up fire8/squeeze1x1
I1107 14:44:45.273092 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.273092 15588 net.cpp:137] Memory required for data: 878644400
I1107 14:44:45.273092 15588 layer_factory.cpp:58] Creating layer bn_18
I1107 14:44:45.273092 15588 net.cpp:84] Creating Layer bn_18
I1107 14:44:45.273092 15588 net.cpp:406] bn_18 <- conv_18
I1107 14:44:45.273092 15588 net.cpp:367] bn_18 -> conv_18 (in-place)
I1107 14:44:45.273092 15588 net.cpp:122] Setting up bn_18
I1107 14:44:45.273092 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.273092 15588 net.cpp:137] Memory required for data: 879898800
I1107 14:44:45.273092 15588 layer_factory.cpp:58] Creating layer scale_18
I1107 14:44:45.273092 15588 net.cpp:84] Creating Layer scale_18
I1107 14:44:45.273092 15588 net.cpp:406] scale_18 <- conv_18
I1107 14:44:45.273092 15588 net.cpp:380] scale_18 -> fire8/squeeze1x1
I1107 14:44:45.273092 15588 layer_factory.cpp:58] Creating layer scale_18
I1107 14:44:45.274091 15588 net.cpp:122] Setting up scale_18
I1107 14:44:45.274091 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.274091 15588 net.cpp:137] Memory required for data: 881153200
I1107 14:44:45.274091 15588 layer_factory.cpp:58] Creating layer fire8/relu_squeeze1x1
I1107 14:44:45.274091 15588 net.cpp:84] Creating Layer fire8/relu_squeeze1x1
I1107 14:44:45.274091 15588 net.cpp:406] fire8/relu_squeeze1x1 <- fire8/squeeze1x1
I1107 14:44:45.274091 15588 net.cpp:367] fire8/relu_squeeze1x1 -> fire8/squeeze1x1 (in-place)
I1107 14:44:45.274091 15588 net.cpp:122] Setting up fire8/relu_squeeze1x1
I1107 14:44:45.274091 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.274091 15588 net.cpp:137] Memory required for data: 882407600
I1107 14:44:45.274091 15588 layer_factory.cpp:58] Creating layer fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 14:44:45.274091 15588 net.cpp:84] Creating Layer fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 14:44:45.274091 15588 net.cpp:406] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split <- fire8/squeeze1x1
I1107 14:44:45.274091 15588 net.cpp:380] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split -> fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_0
I1107 14:44:45.274091 15588 net.cpp:380] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split -> fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_1
I1107 14:44:45.274091 15588 net.cpp:122] Setting up fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 14:44:45.274091 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.274091 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.274091 15588 net.cpp:137] Memory required for data: 884916400
I1107 14:44:45.274091 15588 layer_factory.cpp:58] Creating layer fire8/expand1x1
I1107 14:44:45.274091 15588 net.cpp:84] Creating Layer fire8/expand1x1
I1107 14:44:45.274091 15588 net.cpp:406] fire8/expand1x1 <- fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_0
I1107 14:44:45.274091 15588 net.cpp:380] fire8/expand1x1 -> conv_19
I1107 14:44:45.276093 15588 net.cpp:122] Setting up fire8/expand1x1
I1107 14:44:45.276093 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.276093 15588 net.cpp:137] Memory required for data: 889934000
I1107 14:44:45.276093 15588 layer_factory.cpp:58] Creating layer bn_19
I1107 14:44:45.276093 15588 net.cpp:84] Creating Layer bn_19
I1107 14:44:45.276093 15588 net.cpp:406] bn_19 <- conv_19
I1107 14:44:45.276093 15588 net.cpp:367] bn_19 -> conv_19 (in-place)
I1107 14:44:45.276093 15588 net.cpp:122] Setting up bn_19
I1107 14:44:45.276093 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.276093 15588 net.cpp:137] Memory required for data: 894951600
I1107 14:44:45.276093 15588 layer_factory.cpp:58] Creating layer scale_19
I1107 14:44:45.276093 15588 net.cpp:84] Creating Layer scale_19
I1107 14:44:45.276093 15588 net.cpp:406] scale_19 <- conv_19
I1107 14:44:45.276093 15588 net.cpp:380] scale_19 -> fire8/expand1x1
I1107 14:44:45.276093 15588 layer_factory.cpp:58] Creating layer scale_19
I1107 14:44:45.276093 15588 net.cpp:122] Setting up scale_19
I1107 14:44:45.276093 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.276093 15588 net.cpp:137] Memory required for data: 899969200
I1107 14:44:45.276093 15588 layer_factory.cpp:58] Creating layer fire8/relu_expand1x1
I1107 14:44:45.276093 15588 net.cpp:84] Creating Layer fire8/relu_expand1x1
I1107 14:44:45.276093 15588 net.cpp:406] fire8/relu_expand1x1 <- fire8/expand1x1
I1107 14:44:45.276093 15588 net.cpp:367] fire8/relu_expand1x1 -> fire8/expand1x1 (in-place)
I1107 14:44:45.276093 15588 net.cpp:122] Setting up fire8/relu_expand1x1
I1107 14:44:45.276093 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.276093 15588 net.cpp:137] Memory required for data: 904986800
I1107 14:44:45.276093 15588 layer_factory.cpp:58] Creating layer fire8/expand3x3
I1107 14:44:45.276093 15588 net.cpp:84] Creating Layer fire8/expand3x3
I1107 14:44:45.277092 15588 net.cpp:406] fire8/expand3x3 <- fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_1
I1107 14:44:45.277092 15588 net.cpp:380] fire8/expand3x3 -> conv_20
I1107 14:44:45.279093 15588 net.cpp:122] Setting up fire8/expand3x3
I1107 14:44:45.279093 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.279093 15588 net.cpp:137] Memory required for data: 910004400
I1107 14:44:45.279093 15588 layer_factory.cpp:58] Creating layer bn_20
I1107 14:44:45.279093 15588 net.cpp:84] Creating Layer bn_20
I1107 14:44:45.279093 15588 net.cpp:406] bn_20 <- conv_20
I1107 14:44:45.279093 15588 net.cpp:367] bn_20 -> conv_20 (in-place)
I1107 14:44:45.279093 15588 net.cpp:122] Setting up bn_20
I1107 14:44:45.279093 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.279093 15588 net.cpp:137] Memory required for data: 915022000
I1107 14:44:45.279093 15588 layer_factory.cpp:58] Creating layer scale_20
I1107 14:44:45.279093 15588 net.cpp:84] Creating Layer scale_20
I1107 14:44:45.279093 15588 net.cpp:406] scale_20 <- conv_20
I1107 14:44:45.279093 15588 net.cpp:380] scale_20 -> fire8/expand3x3
I1107 14:44:45.279093 15588 layer_factory.cpp:58] Creating layer scale_20
I1107 14:44:45.279093 15588 net.cpp:122] Setting up scale_20
I1107 14:44:45.279093 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.279093 15588 net.cpp:137] Memory required for data: 920039600
I1107 14:44:45.279093 15588 layer_factory.cpp:58] Creating layer fire8/relu_expand3x3
I1107 14:44:45.279093 15588 net.cpp:84] Creating Layer fire8/relu_expand3x3
I1107 14:44:45.279093 15588 net.cpp:406] fire8/relu_expand3x3 <- fire8/expand3x3
I1107 14:44:45.279093 15588 net.cpp:367] fire8/relu_expand3x3 -> fire8/expand3x3 (in-place)
I1107 14:44:45.280092 15588 net.cpp:122] Setting up fire8/relu_expand3x3
I1107 14:44:45.280092 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.280092 15588 net.cpp:137] Memory required for data: 925057200
I1107 14:44:45.280092 15588 layer_factory.cpp:58] Creating layer fire8/concat
I1107 14:44:45.280092 15588 net.cpp:84] Creating Layer fire8/concat
I1107 14:44:45.280092 15588 net.cpp:406] fire8/concat <- fire8/expand1x1
I1107 14:44:45.280092 15588 net.cpp:406] fire8/concat <- fire8/expand3x3
I1107 14:44:45.280092 15588 net.cpp:380] fire8/concat -> fire8/concat
I1107 14:44:45.280092 15588 net.cpp:122] Setting up fire8/concat
I1107 14:44:45.280092 15588 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 14:44:45.280092 15588 net.cpp:137] Memory required for data: 935092400
I1107 14:44:45.280092 15588 layer_factory.cpp:58] Creating layer fire9/squeeze1x1
I1107 14:44:45.280092 15588 net.cpp:84] Creating Layer fire9/squeeze1x1
I1107 14:44:45.280092 15588 net.cpp:406] fire9/squeeze1x1 <- fire8/concat
I1107 14:44:45.280092 15588 net.cpp:380] fire9/squeeze1x1 -> conv_21
I1107 14:44:45.281091 15588 net.cpp:122] Setting up fire9/squeeze1x1
I1107 14:44:45.281091 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.281091 15588 net.cpp:137] Memory required for data: 936346800
I1107 14:44:45.281091 15588 layer_factory.cpp:58] Creating layer bn_21
I1107 14:44:45.281091 15588 net.cpp:84] Creating Layer bn_21
I1107 14:44:45.281091 15588 net.cpp:406] bn_21 <- conv_21
I1107 14:44:45.281091 15588 net.cpp:367] bn_21 -> conv_21 (in-place)
I1107 14:44:45.281091 15588 net.cpp:122] Setting up bn_21
I1107 14:44:45.281091 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.281091 15588 net.cpp:137] Memory required for data: 937601200
I1107 14:44:45.281091 15588 layer_factory.cpp:58] Creating layer scale_21
I1107 14:44:45.281091 15588 net.cpp:84] Creating Layer scale_21
I1107 14:44:45.281091 15588 net.cpp:406] scale_21 <- conv_21
I1107 14:44:45.281091 15588 net.cpp:380] scale_21 -> fire9/squeeze1x1
I1107 14:44:45.281091 15588 layer_factory.cpp:58] Creating layer scale_21
I1107 14:44:45.282093 15588 net.cpp:122] Setting up scale_21
I1107 14:44:45.282093 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.282093 15588 net.cpp:137] Memory required for data: 938855600
I1107 14:44:45.282093 15588 layer_factory.cpp:58] Creating layer fire9/relu_squeeze1x1
I1107 14:44:45.282093 15588 net.cpp:84] Creating Layer fire9/relu_squeeze1x1
I1107 14:44:45.282093 15588 net.cpp:406] fire9/relu_squeeze1x1 <- fire9/squeeze1x1
I1107 14:44:45.282093 15588 net.cpp:367] fire9/relu_squeeze1x1 -> fire9/squeeze1x1 (in-place)
I1107 14:44:45.282093 15588 net.cpp:122] Setting up fire9/relu_squeeze1x1
I1107 14:44:45.282093 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.282093 15588 net.cpp:137] Memory required for data: 940110000
I1107 14:44:45.282093 15588 layer_factory.cpp:58] Creating layer fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 14:44:45.282093 15588 net.cpp:84] Creating Layer fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 14:44:45.282093 15588 net.cpp:406] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split <- fire9/squeeze1x1
I1107 14:44:45.282093 15588 net.cpp:380] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split -> fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_0
I1107 14:44:45.282093 15588 net.cpp:380] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split -> fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_1
I1107 14:44:45.282093 15588 net.cpp:122] Setting up fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 14:44:45.282093 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.282093 15588 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 14:44:45.282093 15588 net.cpp:137] Memory required for data: 942618800
I1107 14:44:45.282093 15588 layer_factory.cpp:58] Creating layer fire9/expand1x1
I1107 14:44:45.282093 15588 net.cpp:84] Creating Layer fire9/expand1x1
I1107 14:44:45.282093 15588 net.cpp:406] fire9/expand1x1 <- fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_0
I1107 14:44:45.282093 15588 net.cpp:380] fire9/expand1x1 -> conv_22
I1107 14:44:45.283092 15588 net.cpp:122] Setting up fire9/expand1x1
I1107 14:44:45.283092 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.283092 15588 net.cpp:137] Memory required for data: 947636400
I1107 14:44:45.283092 15588 layer_factory.cpp:58] Creating layer bn_22
I1107 14:44:45.283092 15588 net.cpp:84] Creating Layer bn_22
I1107 14:44:45.283092 15588 net.cpp:406] bn_22 <- conv_22
I1107 14:44:45.283092 15588 net.cpp:367] bn_22 -> conv_22 (in-place)
I1107 14:44:45.284092 15588 net.cpp:122] Setting up bn_22
I1107 14:44:45.284092 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.284092 15588 net.cpp:137] Memory required for data: 952654000
I1107 14:44:45.284092 15588 layer_factory.cpp:58] Creating layer scale_22
I1107 14:44:45.284092 15588 net.cpp:84] Creating Layer scale_22
I1107 14:44:45.284092 15588 net.cpp:406] scale_22 <- conv_22
I1107 14:44:45.284092 15588 net.cpp:380] scale_22 -> fire9/expand1x1
I1107 14:44:45.284092 15588 layer_factory.cpp:58] Creating layer scale_22
I1107 14:44:45.284092 15588 net.cpp:122] Setting up scale_22
I1107 14:44:45.284092 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.284092 15588 net.cpp:137] Memory required for data: 957671600
I1107 14:44:45.284092 15588 layer_factory.cpp:58] Creating layer fire9/relu_expand1x1
I1107 14:44:45.284092 15588 net.cpp:84] Creating Layer fire9/relu_expand1x1
I1107 14:44:45.284092 15588 net.cpp:406] fire9/relu_expand1x1 <- fire9/expand1x1
I1107 14:44:45.284092 15588 net.cpp:367] fire9/relu_expand1x1 -> fire9/expand1x1 (in-place)
I1107 14:44:45.284092 15588 net.cpp:122] Setting up fire9/relu_expand1x1
I1107 14:44:45.284092 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.284092 15588 net.cpp:137] Memory required for data: 962689200
I1107 14:44:45.284092 15588 layer_factory.cpp:58] Creating layer fire9/expand3x3
I1107 14:44:45.284092 15588 net.cpp:84] Creating Layer fire9/expand3x3
I1107 14:44:45.284092 15588 net.cpp:406] fire9/expand3x3 <- fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_1
I1107 14:44:45.284092 15588 net.cpp:380] fire9/expand3x3 -> conv_23
I1107 14:44:45.286092 15588 net.cpp:122] Setting up fire9/expand3x3
I1107 14:44:45.286092 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.286092 15588 net.cpp:137] Memory required for data: 967706800
I1107 14:44:45.286092 15588 layer_factory.cpp:58] Creating layer bn_23
I1107 14:44:45.286092 15588 net.cpp:84] Creating Layer bn_23
I1107 14:44:45.286092 15588 net.cpp:406] bn_23 <- conv_23
I1107 14:44:45.286092 15588 net.cpp:367] bn_23 -> conv_23 (in-place)
I1107 14:44:45.287092 15588 net.cpp:122] Setting up bn_23
I1107 14:44:45.287092 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.287092 15588 net.cpp:137] Memory required for data: 972724400
I1107 14:44:45.287092 15588 layer_factory.cpp:58] Creating layer scale_23
I1107 14:44:45.287092 15588 net.cpp:84] Creating Layer scale_23
I1107 14:44:45.287092 15588 net.cpp:406] scale_23 <- conv_23
I1107 14:44:45.287092 15588 net.cpp:380] scale_23 -> fire9/expand3x3
I1107 14:44:45.287092 15588 layer_factory.cpp:58] Creating layer scale_23
I1107 14:44:45.287092 15588 net.cpp:122] Setting up scale_23
I1107 14:44:45.287092 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.287092 15588 net.cpp:137] Memory required for data: 977742000
I1107 14:44:45.287092 15588 layer_factory.cpp:58] Creating layer fire9/relu_expand3x3
I1107 14:44:45.287092 15588 net.cpp:84] Creating Layer fire9/relu_expand3x3
I1107 14:44:45.287092 15588 net.cpp:406] fire9/relu_expand3x3 <- fire9/expand3x3
I1107 14:44:45.287092 15588 net.cpp:367] fire9/relu_expand3x3 -> fire9/expand3x3 (in-place)
I1107 14:44:45.287092 15588 net.cpp:122] Setting up fire9/relu_expand3x3
I1107 14:44:45.287092 15588 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 14:44:45.287092 15588 net.cpp:137] Memory required for data: 982759600
I1107 14:44:45.287092 15588 layer_factory.cpp:58] Creating layer fire9/concat
I1107 14:44:45.287092 15588 net.cpp:84] Creating Layer fire9/concat
I1107 14:44:45.287092 15588 net.cpp:406] fire9/concat <- fire9/expand1x1
I1107 14:44:45.287092 15588 net.cpp:406] fire9/concat <- fire9/expand3x3
I1107 14:44:45.287092 15588 net.cpp:380] fire9/concat -> fire9/concat
I1107 14:44:45.287092 15588 net.cpp:122] Setting up fire9/concat
I1107 14:44:45.287092 15588 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 14:44:45.287092 15588 net.cpp:137] Memory required for data: 992794800
I1107 14:44:45.287092 15588 layer_factory.cpp:58] Creating layer drop9
I1107 14:44:45.287092 15588 net.cpp:84] Creating Layer drop9
I1107 14:44:45.287092 15588 net.cpp:406] drop9 <- fire9/concat
I1107 14:44:45.287092 15588 net.cpp:367] drop9 -> fire9/concat (in-place)
I1107 14:44:45.287092 15588 net.cpp:122] Setting up drop9
I1107 14:44:45.288092 15588 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 14:44:45.288092 15588 net.cpp:137] Memory required for data: 1002830000
I1107 14:44:45.288092 15588 layer_factory.cpp:58] Creating layer conv10
I1107 14:44:45.288092 15588 net.cpp:84] Creating Layer conv10
I1107 14:44:45.288092 15588 net.cpp:406] conv10 <- fire9/concat
I1107 14:44:45.288092 15588 net.cpp:380] conv10 -> conv10
I1107 14:44:45.289093 15588 net.cpp:122] Setting up conv10
I1107 14:44:45.289093 15588 net.cpp:129] Top shape: 100 10 7 7 (49000)
I1107 14:44:45.289093 15588 net.cpp:137] Memory required for data: 1003026000
I1107 14:44:45.289093 15588 layer_factory.cpp:58] Creating layer relu_conv10
I1107 14:44:45.289093 15588 net.cpp:84] Creating Layer relu_conv10
I1107 14:44:45.289093 15588 net.cpp:406] relu_conv10 <- conv10
I1107 14:44:45.289093 15588 net.cpp:367] relu_conv10 -> conv10 (in-place)
I1107 14:44:45.289093 15588 net.cpp:122] Setting up relu_conv10
I1107 14:44:45.289093 15588 net.cpp:129] Top shape: 100 10 7 7 (49000)
I1107 14:44:45.289093 15588 net.cpp:137] Memory required for data: 1003222000
I1107 14:44:45.289093 15588 layer_factory.cpp:58] Creating layer pool10
I1107 14:44:45.289093 15588 net.cpp:84] Creating Layer pool10
I1107 14:44:45.289093 15588 net.cpp:406] pool10 <- conv10
I1107 14:44:45.289093 15588 net.cpp:380] pool10 -> pool10
I1107 14:44:45.290091 15588 net.cpp:122] Setting up pool10
I1107 14:44:45.290091 15588 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 14:44:45.290091 15588 net.cpp:137] Memory required for data: 1003226000
I1107 14:44:45.290091 15588 layer_factory.cpp:58] Creating layer pool10_pool10_0_split
I1107 14:44:45.290091 15588 net.cpp:84] Creating Layer pool10_pool10_0_split
I1107 14:44:45.290091 15588 net.cpp:406] pool10_pool10_0_split <- pool10
I1107 14:44:45.290091 15588 net.cpp:380] pool10_pool10_0_split -> pool10_pool10_0_split_0
I1107 14:44:45.290091 15588 net.cpp:380] pool10_pool10_0_split -> pool10_pool10_0_split_1
I1107 14:44:45.290091 15588 net.cpp:122] Setting up pool10_pool10_0_split
I1107 14:44:45.290091 15588 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 14:44:45.290091 15588 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 14:44:45.290091 15588 net.cpp:137] Memory required for data: 1003234000
I1107 14:44:45.290091 15588 layer_factory.cpp:58] Creating layer accuracy
I1107 14:44:45.290091 15588 net.cpp:84] Creating Layer accuracy
I1107 14:44:45.290091 15588 net.cpp:406] accuracy <- pool10_pool10_0_split_0
I1107 14:44:45.290091 15588 net.cpp:406] accuracy <- label_cifar_1_split_0
I1107 14:44:45.290091 15588 net.cpp:380] accuracy -> accuracy
I1107 14:44:45.290091 15588 net.cpp:122] Setting up accuracy
I1107 14:44:45.290091 15588 net.cpp:129] Top shape: (1)
I1107 14:44:45.290091 15588 net.cpp:137] Memory required for data: 1003234004
I1107 14:44:45.290091 15588 layer_factory.cpp:58] Creating layer loss
I1107 14:44:45.290091 15588 net.cpp:84] Creating Layer loss
I1107 14:44:45.290091 15588 net.cpp:406] loss <- pool10_pool10_0_split_1
I1107 14:44:45.290091 15588 net.cpp:406] loss <- label_cifar_1_split_1
I1107 14:44:45.290091 15588 net.cpp:380] loss -> loss
I1107 14:44:45.290091 15588 layer_factory.cpp:58] Creating layer loss
I1107 14:44:45.290091 15588 net.cpp:122] Setting up loss
I1107 14:44:45.290091 15588 net.cpp:129] Top shape: (1)
I1107 14:44:45.290091 15588 net.cpp:132]     with loss weight 1
I1107 14:44:45.290091 15588 net.cpp:137] Memory required for data: 1003234008
I1107 14:44:45.290091 15588 net.cpp:198] loss needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:200] accuracy does not need backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] pool10_pool10_0_split needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] pool10 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] relu_conv10 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] conv10 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] drop9 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire9/concat needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire9/relu_expand3x3 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] scale_23 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] bn_23 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire9/expand3x3 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire9/relu_expand1x1 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] scale_22 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] bn_22 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire9/expand1x1 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire9/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] scale_21 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] bn_21 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire9/squeeze1x1 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire8/concat needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire8/relu_expand3x3 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] scale_20 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] bn_20 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire8/expand3x3 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire8/relu_expand1x1 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] scale_19 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] bn_19 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire8/expand1x1 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire8/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] scale_18 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] bn_18 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire8/squeeze1x1 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire7/concat needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire7/relu_expand3x3 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] scale_17 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] bn_17 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire7/expand3x3 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire7/relu_expand1x1 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] scale_16 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] bn_16 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire7/expand1x1 needs backward computation.
I1107 14:44:45.290091 15588 net.cpp:198] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire7/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale_15 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn_15 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire7/squeeze1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire6/concat needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire6/relu_expand3x3 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale_14 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn_14 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire6/expand3x3 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire6/relu_expand1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale_13 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn_13 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire6/expand1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire6/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale_12 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn_12 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire6/squeeze1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] pool5 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire5/concat needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire5/relu_expand3x3 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale_11 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn_11 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire5/expand3x3 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire5/relu_expand1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale_10 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn_10 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire5/expand1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire5/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale_9 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn_9 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire5/squeeze1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire4/concat needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire4/relu_expand3x3 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale_8 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn_8 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire4/expand3x3 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire4/relu_expand1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire4/expand1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire4/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale_7 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn_7 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire4/squeeze1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] pool3 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire3/concat needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire3/relu_expand3x3 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale_6 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn_6 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire3/expand3x3 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire3/relu_expand1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale_5 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn_5 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire3/expand1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire3/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale_4 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn_4 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire3/squeeze1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire2/concat needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire2/relu_expand3x3 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale_3 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn_3 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire2/expand3x3 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire2/relu_expand1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale2 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn2 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire2/expand1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire2/relu_squeeze1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] fire2/squeeze1x1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] pool1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] relu_conv1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] scale1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] bn1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:198] conv1 needs backward computation.
I1107 14:44:45.291091 15588 net.cpp:200] label_cifar_1_split does not need backward computation.
I1107 14:44:45.291091 15588 net.cpp:200] cifar does not need backward computation.
I1107 14:44:45.291091 15588 net.cpp:242] This network produces output accuracy
I1107 14:44:45.291091 15588 net.cpp:242] This network produces output loss
I1107 14:44:45.291091 15588 net.cpp:255] Network initialization done.
I1107 14:44:45.291091 15588 solver.cpp:56] Solver scaffolding done.
I1107 14:44:45.299090 15588 caffe.cpp:243] Resuming from examples/cifar10/snaps/squeezenet_batchnorm_iter_90000.solverstate
I1107 14:44:45.410071 15588 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/snaps/squeezenet_batchnorm_iter_90000.caffemodel
I1107 14:44:45.410071 15588 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1107 14:44:45.410071 15588 sgd_solver.cpp:318] SGDSolver: restoring history
I1107 14:44:45.417073 15588 caffe.cpp:249] Starting Optimization
I1107 14:44:45.417073 15588 solver.cpp:272] Solving CIFAR10_Squeezenet_1.1_Batchnorm
I1107 14:44:45.417073 15588 solver.cpp:273] Learning Rate Policy: multistep
I1107 14:44:45.421072 15588 solver.cpp:330] Iteration 90000, Testing net (#0)
I1107 14:44:45.424072 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:44:47.515234  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:44:47.597239 15588 solver.cpp:397]     Test net output #0: accuracy = 0.8211
I1107 14:44:47.597239 15588 solver.cpp:397]     Test net output #1: loss = 0.614796 (* 1 = 0.614796 loss)
I1107 14:44:47.796263 15588 solver.cpp:218] Iteration 90000 (37850.4 iter/s, 2.37778s/100 iters), loss = 0.206618
I1107 14:44:47.796263 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 14:44:47.796263 15588 solver.cpp:237]     Train net output #1: loss = 0.206618 (* 1 = 0.206618 loss)
I1107 14:44:47.796263 15588 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1107 14:44:56.332866 15588 solver.cpp:218] Iteration 90100 (11.7161 iter/s, 8.53527s/100 iters), loss = 0.16157
I1107 14:44:56.332866 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:44:56.332866 15588 solver.cpp:237]     Train net output #1: loss = 0.16157 (* 1 = 0.16157 loss)
I1107 14:44:56.332866 15588 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1107 14:45:04.805405 15588 solver.cpp:218] Iteration 90200 (11.8027 iter/s, 8.47262s/100 iters), loss = 0.197222
I1107 14:45:04.805405 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:45:04.805405 15588 solver.cpp:237]     Train net output #1: loss = 0.197222 (* 1 = 0.197222 loss)
I1107 14:45:04.805405 15588 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1107 14:45:13.352351 15588 solver.cpp:218] Iteration 90300 (11.7009 iter/s, 8.54637s/100 iters), loss = 0.140948
I1107 14:45:13.352351 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 14:45:13.352351 15588 solver.cpp:237]     Train net output #1: loss = 0.140949 (* 1 = 0.140949 loss)
I1107 14:45:13.352351 15588 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1107 14:45:21.861207 15588 solver.cpp:218] Iteration 90400 (11.7532 iter/s, 8.50829s/100 iters), loss = 0.206096
I1107 14:45:21.861207 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 14:45:21.861207 15588 solver.cpp:237]     Train net output #1: loss = 0.206096 (* 1 = 0.206096 loss)
I1107 14:45:21.861207 15588 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1107 14:45:29.965271 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:45:30.304306 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_90500.caffemodel
I1107 14:45:30.341308 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_90500.solverstate
I1107 14:45:30.351310 15588 solver.cpp:330] Iteration 90500, Testing net (#0)
I1107 14:45:30.351310 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:45:32.349496  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:45:32.429502 15588 solver.cpp:397]     Test net output #0: accuracy = 0.8591
I1107 14:45:32.429502 15588 solver.cpp:397]     Test net output #1: loss = 0.456758 (* 1 = 0.456758 loss)
I1107 14:45:32.510506 15588 solver.cpp:218] Iteration 90500 (9.3909 iter/s, 10.6486s/100 iters), loss = 0.15958
I1107 14:45:32.510506 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:45:32.510506 15588 solver.cpp:237]     Train net output #1: loss = 0.15958 (* 1 = 0.15958 loss)
I1107 14:45:32.510506 15588 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1107 14:45:41.039216 15588 solver.cpp:218] Iteration 90600 (11.7254 iter/s, 8.52846s/100 iters), loss = 0.214194
I1107 14:45:41.039216 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 14:45:41.039216 15588 solver.cpp:237]     Train net output #1: loss = 0.214194 (* 1 = 0.214194 loss)
I1107 14:45:41.039216 15588 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1107 14:45:49.589076 15588 solver.cpp:218] Iteration 90700 (11.6974 iter/s, 8.54888s/100 iters), loss = 0.141788
I1107 14:45:49.589076 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:45:49.589076 15588 solver.cpp:237]     Train net output #1: loss = 0.141788 (* 1 = 0.141788 loss)
I1107 14:45:49.589076 15588 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1107 14:45:58.180938 15588 solver.cpp:218] Iteration 90800 (11.6388 iter/s, 8.59193s/100 iters), loss = 0.175235
I1107 14:45:58.181939 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 14:45:58.181939 15588 solver.cpp:237]     Train net output #1: loss = 0.175235 (* 1 = 0.175235 loss)
I1107 14:45:58.181939 15588 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1107 14:46:06.745575 15588 solver.cpp:218] Iteration 90900 (11.6775 iter/s, 8.5635s/100 iters), loss = 0.196726
I1107 14:46:06.745575 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 14:46:06.745575 15588 solver.cpp:237]     Train net output #1: loss = 0.196726 (* 1 = 0.196726 loss)
I1107 14:46:06.745575 15588 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1107 14:46:14.867280 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:46:15.206315 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_91000.caffemodel
I1107 14:46:15.235321 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_91000.solverstate
I1107 14:46:15.244320 15588 solver.cpp:330] Iteration 91000, Testing net (#0)
I1107 14:46:15.244320 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:46:17.239382  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:46:17.318395 15588 solver.cpp:397]     Test net output #0: accuracy = 0.8115
I1107 14:46:17.318395 15588 solver.cpp:397]     Test net output #1: loss = 0.663357 (* 1 = 0.663357 loss)
I1107 14:46:17.399896 15588 solver.cpp:218] Iteration 91000 (9.38645 iter/s, 10.6537s/100 iters), loss = 0.202843
I1107 14:46:17.399896 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 14:46:17.399896 15588 solver.cpp:237]     Train net output #1: loss = 0.202843 (* 1 = 0.202843 loss)
I1107 14:46:17.399896 15588 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1107 14:46:25.938741 15588 solver.cpp:218] Iteration 91100 (11.7116 iter/s, 8.53851s/100 iters), loss = 0.242047
I1107 14:46:25.939242 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 14:46:25.939242 15588 solver.cpp:237]     Train net output #1: loss = 0.242047 (* 1 = 0.242047 loss)
I1107 14:46:25.939242 15588 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1107 14:46:34.643118 15588 solver.cpp:218] Iteration 91200 (11.4897 iter/s, 8.70344s/100 iters), loss = 0.206762
I1107 14:46:34.643118 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 14:46:34.643118 15588 solver.cpp:237]     Train net output #1: loss = 0.206762 (* 1 = 0.206762 loss)
I1107 14:46:34.643118 15588 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1107 14:46:43.471097 15588 solver.cpp:218] Iteration 91300 (11.3284 iter/s, 8.8274s/100 iters), loss = 0.189523
I1107 14:46:43.471097 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 14:46:43.471097 15588 solver.cpp:237]     Train net output #1: loss = 0.189523 (* 1 = 0.189523 loss)
I1107 14:46:43.471097 15588 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1107 14:46:52.359030 15588 solver.cpp:218] Iteration 91400 (11.2515 iter/s, 8.88768s/100 iters), loss = 0.219426
I1107 14:46:52.359030 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 14:46:52.359030 15588 solver.cpp:237]     Train net output #1: loss = 0.219426 (* 1 = 0.219426 loss)
I1107 14:46:52.359030 15588 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1107 14:47:00.552608 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:47:00.896656 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_91500.caffemodel
I1107 14:47:00.928660 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_91500.solverstate
I1107 14:47:00.937659 15588 solver.cpp:330] Iteration 91500, Testing net (#0)
I1107 14:47:00.937659 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:47:02.937904  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:47:03.017411 15588 solver.cpp:397]     Test net output #0: accuracy = 0.841
I1107 14:47:03.018409 15588 solver.cpp:397]     Test net output #1: loss = 0.532742 (* 1 = 0.532742 loss)
I1107 14:47:03.097929 15588 solver.cpp:218] Iteration 91500 (9.31256 iter/s, 10.7382s/100 iters), loss = 0.128565
I1107 14:47:03.097929 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 14:47:03.097929 15588 solver.cpp:237]     Train net output #1: loss = 0.128565 (* 1 = 0.128565 loss)
I1107 14:47:03.097929 15588 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1107 14:47:11.637735 15588 solver.cpp:218] Iteration 91600 (11.7113 iter/s, 8.53879s/100 iters), loss = 0.188982
I1107 14:47:11.637735 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 14:47:11.637735 15588 solver.cpp:237]     Train net output #1: loss = 0.188982 (* 1 = 0.188982 loss)
I1107 14:47:11.637735 15588 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1107 14:47:20.213481 15588 solver.cpp:218] Iteration 91700 (11.6606 iter/s, 8.57587s/100 iters), loss = 0.198532
I1107 14:47:20.213481 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 14:47:20.213481 15588 solver.cpp:237]     Train net output #1: loss = 0.198532 (* 1 = 0.198532 loss)
I1107 14:47:20.213481 15588 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1107 14:47:28.722199 15588 solver.cpp:218] Iteration 91800 (11.7533 iter/s, 8.50827s/100 iters), loss = 0.221526
I1107 14:47:28.722199 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 14:47:28.722199 15588 solver.cpp:237]     Train net output #1: loss = 0.221526 (* 1 = 0.221526 loss)
I1107 14:47:28.722199 15588 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1107 14:47:37.244277 15588 solver.cpp:218] Iteration 91900 (11.7353 iter/s, 8.52128s/100 iters), loss = 0.1094
I1107 14:47:37.244277 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 14:47:37.244277 15588 solver.cpp:237]     Train net output #1: loss = 0.1094 (* 1 = 0.1094 loss)
I1107 14:47:37.244277 15588 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1107 14:47:45.347776 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:47:45.685775 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_92000.caffemodel
I1107 14:47:45.718276 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_92000.solverstate
I1107 14:47:45.727776 15588 solver.cpp:330] Iteration 92000, Testing net (#0)
I1107 14:47:45.728276 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:47:47.718565  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:47:47.798566 15588 solver.cpp:397]     Test net output #0: accuracy = 0.8621
I1107 14:47:47.798566 15588 solver.cpp:397]     Test net output #1: loss = 0.461363 (* 1 = 0.461363 loss)
I1107 14:47:47.879572 15588 solver.cpp:218] Iteration 92000 (9.40328 iter/s, 10.6346s/100 iters), loss = 0.177979
I1107 14:47:47.879572 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 14:47:47.879572 15588 solver.cpp:237]     Train net output #1: loss = 0.177979 (* 1 = 0.177979 loss)
I1107 14:47:47.879572 15588 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1107 14:47:56.402657 15588 solver.cpp:218] Iteration 92100 (11.7328 iter/s, 8.52315s/100 iters), loss = 0.177041
I1107 14:47:56.402657 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 14:47:56.402657 15588 solver.cpp:237]     Train net output #1: loss = 0.177041 (* 1 = 0.177041 loss)
I1107 14:47:56.403657 15588 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1107 14:48:04.906392 15588 solver.cpp:218] Iteration 92200 (11.7614 iter/s, 8.50237s/100 iters), loss = 0.186824
I1107 14:48:04.906392 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 14:48:04.906392 15588 solver.cpp:237]     Train net output #1: loss = 0.186824 (* 1 = 0.186824 loss)
I1107 14:48:04.906392 15588 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1107 14:48:13.413107 15588 solver.cpp:218] Iteration 92300 (11.7558 iter/s, 8.50645s/100 iters), loss = 0.169251
I1107 14:48:13.413107 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 14:48:13.413107 15588 solver.cpp:237]     Train net output #1: loss = 0.169251 (* 1 = 0.169251 loss)
I1107 14:48:13.413107 15588 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1107 14:48:21.993981 15588 solver.cpp:218] Iteration 92400 (11.6542 iter/s, 8.5806s/100 iters), loss = 0.216433
I1107 14:48:21.993981 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 14:48:21.993981 15588 solver.cpp:237]     Train net output #1: loss = 0.216433 (* 1 = 0.216433 loss)
I1107 14:48:21.993981 15588 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1107 14:48:30.410861 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:48:30.759897 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_92500.caffemodel
I1107 14:48:30.799906 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_92500.solverstate
I1107 14:48:30.808907 15588 solver.cpp:330] Iteration 92500, Testing net (#0)
I1107 14:48:30.808907 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:48:32.866442  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:48:32.948457 15588 solver.cpp:397]     Test net output #0: accuracy = 0.8295
I1107 14:48:32.949445 15588 solver.cpp:397]     Test net output #1: loss = 0.568452 (* 1 = 0.568452 loss)
I1107 14:48:33.033454 15588 solver.cpp:218] Iteration 92500 (9.059 iter/s, 11.0387s/100 iters), loss = 0.142885
I1107 14:48:33.033454 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 14:48:33.033454 15588 solver.cpp:237]     Train net output #1: loss = 0.142885 (* 1 = 0.142885 loss)
I1107 14:48:33.033454 15588 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1107 14:48:41.637138 15588 solver.cpp:218] Iteration 92600 (11.6235 iter/s, 8.60326s/100 iters), loss = 0.235063
I1107 14:48:41.637138 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 14:48:41.637138 15588 solver.cpp:237]     Train net output #1: loss = 0.235063 (* 1 = 0.235063 loss)
I1107 14:48:41.637138 15588 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1107 14:48:50.163993 15588 solver.cpp:218] Iteration 92700 (11.7289 iter/s, 8.52598s/100 iters), loss = 0.173313
I1107 14:48:50.163993 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 14:48:50.163993 15588 solver.cpp:237]     Train net output #1: loss = 0.173313 (* 1 = 0.173313 loss)
I1107 14:48:50.163993 15588 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1107 14:48:58.686316 15588 solver.cpp:218] Iteration 92800 (11.7348 iter/s, 8.52169s/100 iters), loss = 0.252356
I1107 14:48:58.686316 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 14:48:58.686316 15588 solver.cpp:237]     Train net output #1: loss = 0.252356 (* 1 = 0.252356 loss)
I1107 14:48:58.686316 15588 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1107 14:49:07.206025 15588 solver.cpp:218] Iteration 92900 (11.7375 iter/s, 8.51971s/100 iters), loss = 0.162516
I1107 14:49:07.206025 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:49:07.206025 15588 solver.cpp:237]     Train net output #1: loss = 0.162516 (* 1 = 0.162516 loss)
I1107 14:49:07.206025 15588 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1107 14:49:15.294201 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:49:15.630234 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_93000.caffemodel
I1107 14:49:15.665232 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_93000.solverstate
I1107 14:49:15.675236 15588 solver.cpp:330] Iteration 93000, Testing net (#0)
I1107 14:49:15.675236 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:49:17.696750  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:49:17.778751 15588 solver.cpp:397]     Test net output #0: accuracy = 0.8599
I1107 14:49:17.778751 15588 solver.cpp:397]     Test net output #1: loss = 0.466266 (* 1 = 0.466266 loss)
I1107 14:49:17.861754 15588 solver.cpp:218] Iteration 93000 (9.38532 iter/s, 10.6549s/100 iters), loss = 0.135631
I1107 14:49:17.861754 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 14:49:17.861754 15588 solver.cpp:237]     Train net output #1: loss = 0.135631 (* 1 = 0.135631 loss)
I1107 14:49:17.861754 15588 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1107 14:49:26.698858 15588 solver.cpp:218] Iteration 93100 (11.3177 iter/s, 8.8357s/100 iters), loss = 0.226522
I1107 14:49:26.698858 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 14:49:26.698858 15588 solver.cpp:237]     Train net output #1: loss = 0.226522 (* 1 = 0.226522 loss)
I1107 14:49:26.698858 15588 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1107 14:49:35.541826 15588 solver.cpp:218] Iteration 93200 (11.3091 iter/s, 8.84243s/100 iters), loss = 0.14603
I1107 14:49:35.541826 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 14:49:35.541826 15588 solver.cpp:237]     Train net output #1: loss = 0.14603 (* 1 = 0.14603 loss)
I1107 14:49:35.541826 15588 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1107 14:49:44.390867 15588 solver.cpp:218] Iteration 93300 (11.3011 iter/s, 8.84873s/100 iters), loss = 0.237176
I1107 14:49:44.390867 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 14:49:44.390867 15588 solver.cpp:237]     Train net output #1: loss = 0.237176 (* 1 = 0.237176 loss)
I1107 14:49:44.390867 15588 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1107 14:49:52.969341 15588 solver.cpp:218] Iteration 93400 (11.658 iter/s, 8.57782s/100 iters), loss = 0.149791
I1107 14:49:52.969341 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 14:49:52.969341 15588 solver.cpp:237]     Train net output #1: loss = 0.149791 (* 1 = 0.149791 loss)
I1107 14:49:52.969341 15588 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1107 14:50:01.084672 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:50:01.429378 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_93500.caffemodel
I1107 14:50:01.458377 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_93500.solverstate
I1107 14:50:01.467396 15588 solver.cpp:330] Iteration 93500, Testing net (#0)
I1107 14:50:01.467396 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:50:03.460816  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:50:03.539829 15588 solver.cpp:397]     Test net output #0: accuracy = 0.8623
I1107 14:50:03.539829 15588 solver.cpp:397]     Test net output #1: loss = 0.442913 (* 1 = 0.442913 loss)
I1107 14:50:03.620848 15588 solver.cpp:218] Iteration 93500 (9.38835 iter/s, 10.6515s/100 iters), loss = 0.195554
I1107 14:50:03.621850 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:50:03.621850 15588 solver.cpp:237]     Train net output #1: loss = 0.195554 (* 1 = 0.195554 loss)
I1107 14:50:03.621850 15588 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1107 14:50:12.147727 15588 solver.cpp:218] Iteration 93600 (11.7295 iter/s, 8.52554s/100 iters), loss = 0.288463
I1107 14:50:12.147727 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 14:50:12.147727 15588 solver.cpp:237]     Train net output #1: loss = 0.288463 (* 1 = 0.288463 loss)
I1107 14:50:12.147727 15588 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1107 14:50:20.673635 15588 solver.cpp:218] Iteration 93700 (11.7289 iter/s, 8.52597s/100 iters), loss = 0.133376
I1107 14:50:20.673635 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 14:50:20.673635 15588 solver.cpp:237]     Train net output #1: loss = 0.133376 (* 1 = 0.133376 loss)
I1107 14:50:20.673635 15588 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1107 14:50:29.191675 15588 solver.cpp:218] Iteration 93800 (11.7409 iter/s, 8.5172s/100 iters), loss = 0.234428
I1107 14:50:29.191675 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:50:29.191675 15588 solver.cpp:237]     Train net output #1: loss = 0.234428 (* 1 = 0.234428 loss)
I1107 14:50:29.191675 15588 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1107 14:50:37.718456 15588 solver.cpp:218] Iteration 93900 (11.7277 iter/s, 8.52684s/100 iters), loss = 0.175032
I1107 14:50:37.718456 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 14:50:37.719456 15588 solver.cpp:237]     Train net output #1: loss = 0.175032 (* 1 = 0.175032 loss)
I1107 14:50:37.719456 15588 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1107 14:50:45.795452 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:50:46.130973 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_94000.caffemodel
I1107 14:50:46.166973 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_94000.solverstate
I1107 14:50:46.175973 15588 solver.cpp:330] Iteration 94000, Testing net (#0)
I1107 14:50:46.175973 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:50:48.162137  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:50:48.242144 15588 solver.cpp:397]     Test net output #0: accuracy = 0.8528
I1107 14:50:48.242144 15588 solver.cpp:397]     Test net output #1: loss = 0.502715 (* 1 = 0.502715 loss)
I1107 14:50:48.323164 15588 solver.cpp:218] Iteration 94000 (9.4303 iter/s, 10.6041s/100 iters), loss = 0.160787
I1107 14:50:48.323164 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:50:48.323164 15588 solver.cpp:237]     Train net output #1: loss = 0.160787 (* 1 = 0.160787 loss)
I1107 14:50:48.323164 15588 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1107 14:50:56.866614 15588 solver.cpp:218] Iteration 94100 (11.7059 iter/s, 8.54273s/100 iters), loss = 0.208854
I1107 14:50:56.866614 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 14:50:56.866614 15588 solver.cpp:237]     Train net output #1: loss = 0.208854 (* 1 = 0.208854 loss)
I1107 14:50:56.866614 15588 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1107 14:51:05.617620 15588 solver.cpp:218] Iteration 94200 (11.4282 iter/s, 8.75026s/100 iters), loss = 0.172269
I1107 14:51:05.617620 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:51:05.617620 15588 solver.cpp:237]     Train net output #1: loss = 0.172269 (* 1 = 0.172269 loss)
I1107 14:51:05.617620 15588 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1107 14:51:14.425684 15588 solver.cpp:218] Iteration 94300 (11.3536 iter/s, 8.80781s/100 iters), loss = 0.142207
I1107 14:51:14.425684 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:51:14.425684 15588 solver.cpp:237]     Train net output #1: loss = 0.142207 (* 1 = 0.142207 loss)
I1107 14:51:14.425684 15588 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1107 14:51:23.243782 15588 solver.cpp:218] Iteration 94400 (11.3418 iter/s, 8.81693s/100 iters), loss = 0.167534
I1107 14:51:23.243782 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 14:51:23.243782 15588 solver.cpp:237]     Train net output #1: loss = 0.167535 (* 1 = 0.167535 loss)
I1107 14:51:23.243782 15588 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1107 14:51:31.661723 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:51:32.019875 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_94500.caffemodel
I1107 14:51:32.053865 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_94500.solverstate
I1107 14:51:32.062865 15588 solver.cpp:330] Iteration 94500, Testing net (#0)
I1107 14:51:32.062865 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:51:34.122022  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:51:34.204524 15588 solver.cpp:397]     Test net output #0: accuracy = 0.8518
I1107 14:51:34.204524 15588 solver.cpp:397]     Test net output #1: loss = 0.48637 (* 1 = 0.48637 loss)
I1107 14:51:34.288027 15588 solver.cpp:218] Iteration 94500 (9.05432 iter/s, 11.0444s/100 iters), loss = 0.183572
I1107 14:51:34.289028 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 14:51:34.289028 15588 solver.cpp:237]     Train net output #1: loss = 0.183572 (* 1 = 0.183572 loss)
I1107 14:51:34.289028 15588 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1107 14:51:43.221354 15588 solver.cpp:218] Iteration 94600 (11.1948 iter/s, 8.93274s/100 iters), loss = 0.258702
I1107 14:51:43.222353 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 14:51:43.222353 15588 solver.cpp:237]     Train net output #1: loss = 0.258702 (* 1 = 0.258702 loss)
I1107 14:51:43.222353 15588 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1107 14:51:52.096612 15588 solver.cpp:218] Iteration 94700 (11.2688 iter/s, 8.87403s/100 iters), loss = 0.225951
I1107 14:51:52.097103 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 14:51:52.097103 15588 solver.cpp:237]     Train net output #1: loss = 0.225951 (* 1 = 0.225951 loss)
I1107 14:51:52.097103 15588 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1107 14:52:00.804416 15588 solver.cpp:218] Iteration 94800 (11.4853 iter/s, 8.70675s/100 iters), loss = 0.168635
I1107 14:52:00.804416 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:52:00.804416 15588 solver.cpp:237]     Train net output #1: loss = 0.168636 (* 1 = 0.168636 loss)
I1107 14:52:00.804416 15588 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1107 14:52:09.398733 15588 solver.cpp:218] Iteration 94900 (11.6362 iter/s, 8.59389s/100 iters), loss = 0.154347
I1107 14:52:09.398733 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 14:52:09.398733 15588 solver.cpp:237]     Train net output #1: loss = 0.154347 (* 1 = 0.154347 loss)
I1107 14:52:09.398733 15588 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1107 14:52:17.574769 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:52:17.920805 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_95000.caffemodel
I1107 14:52:17.948804 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_95000.solverstate
I1107 14:52:17.958817 15588 solver.cpp:330] Iteration 95000, Testing net (#0)
I1107 14:52:17.958817 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:52:19.998235  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:52:20.079246 15588 solver.cpp:397]     Test net output #0: accuracy = 0.8568
I1107 14:52:20.079246 15588 solver.cpp:397]     Test net output #1: loss = 0.471371 (* 1 = 0.471371 loss)
I1107 14:52:20.161250 15588 solver.cpp:218] Iteration 95000 (9.29194 iter/s, 10.762s/100 iters), loss = 0.193554
I1107 14:52:20.161250 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:52:20.161250 15588 solver.cpp:237]     Train net output #1: loss = 0.193554 (* 1 = 0.193554 loss)
I1107 14:52:20.161250 15588 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1107 14:52:20.161250 15588 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1107 14:52:28.797650 15588 solver.cpp:218] Iteration 95100 (11.5802 iter/s, 8.63547s/100 iters), loss = 0.125834
I1107 14:52:28.797650 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 14:52:28.797650 15588 solver.cpp:237]     Train net output #1: loss = 0.125834 (* 1 = 0.125834 loss)
I1107 14:52:28.797650 15588 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1107 14:52:37.486670 15588 solver.cpp:218] Iteration 95200 (11.5092 iter/s, 8.68872s/100 iters), loss = 0.0624654
I1107 14:52:37.486670 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:52:37.486670 15588 solver.cpp:237]     Train net output #1: loss = 0.0624656 (* 1 = 0.0624656 loss)
I1107 14:52:37.486670 15588 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1107 14:52:46.033949 15588 solver.cpp:218] Iteration 95300 (11.7008 iter/s, 8.54644s/100 iters), loss = 0.0645384
I1107 14:52:46.033949 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:52:46.033949 15588 solver.cpp:237]     Train net output #1: loss = 0.0645386 (* 1 = 0.0645386 loss)
I1107 14:52:46.033949 15588 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1107 14:52:54.607820 15588 solver.cpp:218] Iteration 95400 (11.6641 iter/s, 8.57333s/100 iters), loss = 0.123865
I1107 14:52:54.607820 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:52:54.607820 15588 solver.cpp:237]     Train net output #1: loss = 0.123865 (* 1 = 0.123865 loss)
I1107 14:52:54.607820 15588 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1107 14:53:02.867425 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:53:03.207486 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_95500.caffemodel
I1107 14:53:03.243489 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_95500.solverstate
I1107 14:53:03.252488 15588 solver.cpp:330] Iteration 95500, Testing net (#0)
I1107 14:53:03.252488 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:53:05.251660  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:53:05.332690 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9174
I1107 14:53:05.332690 15588 solver.cpp:397]     Test net output #1: loss = 0.275177 (* 1 = 0.275177 loss)
I1107 14:53:05.414189 15588 solver.cpp:218] Iteration 95500 (9.2544 iter/s, 10.8057s/100 iters), loss = 0.0737544
I1107 14:53:05.414189 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:53:05.414189 15588 solver.cpp:237]     Train net output #1: loss = 0.0737546 (* 1 = 0.0737546 loss)
I1107 14:53:05.414189 15588 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1107 14:53:14.040073 15588 solver.cpp:218] Iteration 95600 (11.5937 iter/s, 8.62541s/100 iters), loss = 0.120352
I1107 14:53:14.040073 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 14:53:14.040073 15588 solver.cpp:237]     Train net output #1: loss = 0.120352 (* 1 = 0.120352 loss)
I1107 14:53:14.040073 15588 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1107 14:53:22.612872 15588 solver.cpp:218] Iteration 95700 (11.6659 iter/s, 8.572s/100 iters), loss = 0.0843903
I1107 14:53:22.612872 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 14:53:22.612872 15588 solver.cpp:237]     Train net output #1: loss = 0.0843905 (* 1 = 0.0843905 loss)
I1107 14:53:22.612872 15588 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1107 14:53:31.110023 15588 solver.cpp:218] Iteration 95800 (11.7689 iter/s, 8.49701s/100 iters), loss = 0.0786155
I1107 14:53:31.110023 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 14:53:31.110023 15588 solver.cpp:237]     Train net output #1: loss = 0.0786157 (* 1 = 0.0786157 loss)
I1107 14:53:31.110023 15588 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1107 14:53:39.600225 15588 solver.cpp:218] Iteration 95900 (11.7794 iter/s, 8.48941s/100 iters), loss = 0.0747357
I1107 14:53:39.600225 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:53:39.600225 15588 solver.cpp:237]     Train net output #1: loss = 0.0747359 (* 1 = 0.0747359 loss)
I1107 14:53:39.600225 15588 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1107 14:53:47.692955 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:53:48.029990 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_96000.caffemodel
I1107 14:53:48.064990 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_96000.solverstate
I1107 14:53:48.074990 15588 solver.cpp:330] Iteration 96000, Testing net (#0)
I1107 14:53:48.074990 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:53:50.066140  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:53:50.147150 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9201
I1107 14:53:50.147150 15588 solver.cpp:397]     Test net output #1: loss = 0.268411 (* 1 = 0.268411 loss)
I1107 14:53:50.228154 15588 solver.cpp:218] Iteration 96000 (9.40968 iter/s, 10.6274s/100 iters), loss = 0.0814044
I1107 14:53:50.228154 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 14:53:50.228154 15588 solver.cpp:237]     Train net output #1: loss = 0.0814046 (* 1 = 0.0814046 loss)
I1107 14:53:50.228154 15588 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1107 14:53:58.830677 15588 solver.cpp:218] Iteration 96100 (11.6246 iter/s, 8.60245s/100 iters), loss = 0.08509
I1107 14:53:58.830677 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 14:53:58.830677 15588 solver.cpp:237]     Train net output #1: loss = 0.0850902 (* 1 = 0.0850902 loss)
I1107 14:53:58.830677 15588 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1107 14:54:07.359438 15588 solver.cpp:218] Iteration 96200 (11.7262 iter/s, 8.5279s/100 iters), loss = 0.0640491
I1107 14:54:07.359438 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:54:07.359438 15588 solver.cpp:237]     Train net output #1: loss = 0.0640493 (* 1 = 0.0640493 loss)
I1107 14:54:07.359438 15588 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1107 14:54:16.003631 15588 solver.cpp:218] Iteration 96300 (11.5681 iter/s, 8.64449s/100 iters), loss = 0.0503714
I1107 14:54:16.004632 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:54:16.004632 15588 solver.cpp:237]     Train net output #1: loss = 0.0503716 (* 1 = 0.0503716 loss)
I1107 14:54:16.004632 15588 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1107 14:54:24.631052 15588 solver.cpp:218] Iteration 96400 (11.5929 iter/s, 8.62599s/100 iters), loss = 0.078353
I1107 14:54:24.631052 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 14:54:24.631052 15588 solver.cpp:237]     Train net output #1: loss = 0.0783531 (* 1 = 0.0783531 loss)
I1107 14:54:24.631052 15588 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1107 14:54:32.867854 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:54:33.201886 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_96500.caffemodel
I1107 14:54:33.237893 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_96500.solverstate
I1107 14:54:33.246892 15588 solver.cpp:330] Iteration 96500, Testing net (#0)
I1107 14:54:33.246892 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:54:35.236043  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:54:35.315047 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 14:54:35.316048 15588 solver.cpp:397]     Test net output #1: loss = 0.27078 (* 1 = 0.27078 loss)
I1107 14:54:35.397308 15588 solver.cpp:218] Iteration 96500 (9.28855 iter/s, 10.7659s/100 iters), loss = 0.0801367
I1107 14:54:35.397308 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:54:35.397308 15588 solver.cpp:237]     Train net output #1: loss = 0.0801369 (* 1 = 0.0801369 loss)
I1107 14:54:35.397308 15588 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1107 14:54:44.024371 15588 solver.cpp:218] Iteration 96600 (11.5924 iter/s, 8.62635s/100 iters), loss = 0.126442
I1107 14:54:44.024371 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:54:44.024371 15588 solver.cpp:237]     Train net output #1: loss = 0.126442 (* 1 = 0.126442 loss)
I1107 14:54:44.024371 15588 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1107 14:54:52.719280 15588 solver.cpp:218] Iteration 96700 (11.5009 iter/s, 8.69499s/100 iters), loss = 0.0940326
I1107 14:54:52.720281 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 14:54:52.720281 15588 solver.cpp:237]     Train net output #1: loss = 0.0940328 (* 1 = 0.0940328 loss)
I1107 14:54:52.720281 15588 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1107 14:55:01.415194 15588 solver.cpp:218] Iteration 96800 (11.5015 iter/s, 8.6945s/100 iters), loss = 0.0648331
I1107 14:55:01.415194 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:55:01.415194 15588 solver.cpp:237]     Train net output #1: loss = 0.0648333 (* 1 = 0.0648333 loss)
I1107 14:55:01.415194 15588 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1107 14:55:10.014118 15588 solver.cpp:218] Iteration 96900 (11.6295 iter/s, 8.5988s/100 iters), loss = 0.0818728
I1107 14:55:10.014118 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 14:55:10.014118 15588 solver.cpp:237]     Train net output #1: loss = 0.081873 (* 1 = 0.081873 loss)
I1107 14:55:10.014118 15588 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1107 14:55:18.143530 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:55:18.481690 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_97000.caffemodel
I1107 14:55:18.518690 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_97000.solverstate
I1107 14:55:18.527690 15588 solver.cpp:330] Iteration 97000, Testing net (#0)
I1107 14:55:18.527690 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:55:20.523828  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:55:20.603835 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9175
I1107 14:55:20.603835 15588 solver.cpp:397]     Test net output #1: loss = 0.27325 (* 1 = 0.27325 loss)
I1107 14:55:20.683840 15588 solver.cpp:218] Iteration 97000 (9.37338 iter/s, 10.6685s/100 iters), loss = 0.0558712
I1107 14:55:20.683840 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:55:20.683840 15588 solver.cpp:237]     Train net output #1: loss = 0.0558714 (* 1 = 0.0558714 loss)
I1107 14:55:20.683840 15588 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1107 14:55:29.249910 15588 solver.cpp:218] Iteration 97100 (11.6748 iter/s, 8.56549s/100 iters), loss = 0.0974123
I1107 14:55:29.249910 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 14:55:29.249910 15588 solver.cpp:237]     Train net output #1: loss = 0.0974125 (* 1 = 0.0974125 loss)
I1107 14:55:29.249910 15588 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1107 14:55:37.784545 15588 solver.cpp:218] Iteration 97200 (11.7172 iter/s, 8.53444s/100 iters), loss = 0.110818
I1107 14:55:37.784545 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 14:55:37.784545 15588 solver.cpp:237]     Train net output #1: loss = 0.110818 (* 1 = 0.110818 loss)
I1107 14:55:37.784545 15588 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1107 14:55:46.400867 15588 solver.cpp:218] Iteration 97300 (11.6072 iter/s, 8.61533s/100 iters), loss = 0.045472
I1107 14:55:46.400867 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:55:46.400867 15588 solver.cpp:237]     Train net output #1: loss = 0.0454722 (* 1 = 0.0454722 loss)
I1107 14:55:46.400867 15588 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1107 14:55:54.985426 15588 solver.cpp:218] Iteration 97400 (11.6498 iter/s, 8.58384s/100 iters), loss = 0.0955834
I1107 14:55:54.985426 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 14:55:54.985426 15588 solver.cpp:237]     Train net output #1: loss = 0.0955836 (* 1 = 0.0955836 loss)
I1107 14:55:54.985426 15588 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1107 14:56:03.059209 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:56:03.393751 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_97500.caffemodel
I1107 14:56:03.423894 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_97500.solverstate
I1107 14:56:03.432888 15588 solver.cpp:330] Iteration 97500, Testing net (#0)
I1107 14:56:03.432888 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:56:05.415544  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:56:05.495066 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9196
I1107 14:56:05.495066 15588 solver.cpp:397]     Test net output #1: loss = 0.272172 (* 1 = 0.272172 loss)
I1107 14:56:05.576565 15588 solver.cpp:218] Iteration 97500 (9.44239 iter/s, 10.5905s/100 iters), loss = 0.0838194
I1107 14:56:05.576565 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:56:05.576565 15588 solver.cpp:237]     Train net output #1: loss = 0.0838195 (* 1 = 0.0838195 loss)
I1107 14:56:05.576565 15588 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1107 14:56:14.111353 15588 solver.cpp:218] Iteration 97600 (11.7174 iter/s, 8.53434s/100 iters), loss = 0.0663245
I1107 14:56:14.111353 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 14:56:14.111353 15588 solver.cpp:237]     Train net output #1: loss = 0.0663247 (* 1 = 0.0663247 loss)
I1107 14:56:14.111353 15588 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1107 14:56:22.741221 15588 solver.cpp:218] Iteration 97700 (11.588 iter/s, 8.6296s/100 iters), loss = 0.0379172
I1107 14:56:22.741721 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:56:22.741721 15588 solver.cpp:237]     Train net output #1: loss = 0.0379173 (* 1 = 0.0379173 loss)
I1107 14:56:22.741721 15588 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1107 14:56:31.350847 15588 solver.cpp:218] Iteration 97800 (11.6158 iter/s, 8.60894s/100 iters), loss = 0.0654021
I1107 14:56:31.350847 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:56:31.351347 15588 solver.cpp:237]     Train net output #1: loss = 0.0654022 (* 1 = 0.0654022 loss)
I1107 14:56:31.351347 15588 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1107 14:56:39.954020 15588 solver.cpp:218] Iteration 97900 (11.6243 iter/s, 8.60266s/100 iters), loss = 0.0540914
I1107 14:56:39.954020 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:56:39.954020 15588 solver.cpp:237]     Train net output #1: loss = 0.0540915 (* 1 = 0.0540915 loss)
I1107 14:56:39.954520 15588 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1107 14:56:48.024374 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:56:48.360321 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_98000.caffemodel
I1107 14:56:48.392340 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_98000.solverstate
I1107 14:56:48.401338 15588 solver.cpp:330] Iteration 98000, Testing net (#0)
I1107 14:56:48.401338 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:56:50.391901  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:56:50.471422 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1107 14:56:50.471422 15588 solver.cpp:397]     Test net output #1: loss = 0.277717 (* 1 = 0.277717 loss)
I1107 14:56:50.551928 15588 solver.cpp:218] Iteration 98000 (9.43644 iter/s, 10.5972s/100 iters), loss = 0.0667162
I1107 14:56:50.551928 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:56:50.551928 15588 solver.cpp:237]     Train net output #1: loss = 0.0667163 (* 1 = 0.0667163 loss)
I1107 14:56:50.551928 15588 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1107 14:56:59.164155 15588 solver.cpp:218] Iteration 98100 (11.6115 iter/s, 8.61215s/100 iters), loss = 0.119825
I1107 14:56:59.164155 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 14:56:59.164155 15588 solver.cpp:237]     Train net output #1: loss = 0.119825 (* 1 = 0.119825 loss)
I1107 14:56:59.164155 15588 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1107 14:57:07.790563 15588 solver.cpp:218] Iteration 98200 (11.5933 iter/s, 8.62567s/100 iters), loss = 0.0444647
I1107 14:57:07.790563 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:57:07.790563 15588 solver.cpp:237]     Train net output #1: loss = 0.0444649 (* 1 = 0.0444649 loss)
I1107 14:57:07.790563 15588 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1107 14:57:16.501189 15588 solver.cpp:218] Iteration 98300 (11.4811 iter/s, 8.70995s/100 iters), loss = 0.0736377
I1107 14:57:16.501189 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 14:57:16.501189 15588 solver.cpp:237]     Train net output #1: loss = 0.0736378 (* 1 = 0.0736378 loss)
I1107 14:57:16.501189 15588 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1107 14:57:25.166208 15588 solver.cpp:218] Iteration 98400 (11.5414 iter/s, 8.66449s/100 iters), loss = 0.0642893
I1107 14:57:25.166709 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:57:25.166709 15588 solver.cpp:237]     Train net output #1: loss = 0.0642895 (* 1 = 0.0642895 loss)
I1107 14:57:25.166709 15588 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1107 14:57:33.289433 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:57:33.625447 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_98500.caffemodel
I1107 14:57:33.662451 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_98500.solverstate
I1107 14:57:33.671454 15588 solver.cpp:330] Iteration 98500, Testing net (#0)
I1107 14:57:33.671454 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:57:35.660984  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:57:35.740484 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1107 14:57:35.740484 15588 solver.cpp:397]     Test net output #1: loss = 0.275361 (* 1 = 0.275361 loss)
I1107 14:57:35.820489 15588 solver.cpp:218] Iteration 98500 (9.38617 iter/s, 10.654s/100 iters), loss = 0.0659206
I1107 14:57:35.820489 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:57:35.820489 15588 solver.cpp:237]     Train net output #1: loss = 0.0659208 (* 1 = 0.0659208 loss)
I1107 14:57:35.820489 15588 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1107 14:57:44.336509 15588 solver.cpp:218] Iteration 98600 (11.7436 iter/s, 8.51528s/100 iters), loss = 0.09835
I1107 14:57:44.336509 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 14:57:44.336509 15588 solver.cpp:237]     Train net output #1: loss = 0.0983501 (* 1 = 0.0983501 loss)
I1107 14:57:44.336509 15588 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1107 14:57:52.866772 15588 solver.cpp:218] Iteration 98700 (11.7238 iter/s, 8.52969s/100 iters), loss = 0.0486888
I1107 14:57:52.867274 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:57:52.867274 15588 solver.cpp:237]     Train net output #1: loss = 0.048689 (* 1 = 0.048689 loss)
I1107 14:57:52.867274 15588 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1107 14:58:01.395959 15588 solver.cpp:218] Iteration 98800 (11.7251 iter/s, 8.52872s/100 iters), loss = 0.0606407
I1107 14:58:01.395959 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:58:01.395959 15588 solver.cpp:237]     Train net output #1: loss = 0.0606408 (* 1 = 0.0606408 loss)
I1107 14:58:01.395959 15588 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1107 14:58:09.917850 15588 solver.cpp:218] Iteration 98900 (11.7358 iter/s, 8.52095s/100 iters), loss = 0.050418
I1107 14:58:09.917850 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:58:09.917850 15588 solver.cpp:237]     Train net output #1: loss = 0.0504182 (* 1 = 0.0504182 loss)
I1107 14:58:09.917850 15588 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1107 14:58:18.086922 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:58:18.434953 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_99000.caffemodel
I1107 14:58:18.472457 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_99000.solverstate
I1107 14:58:18.481961 15588 solver.cpp:330] Iteration 99000, Testing net (#0)
I1107 14:58:18.481961 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:58:20.534056  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:58:20.616062 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 14:58:20.616062 15588 solver.cpp:397]     Test net output #1: loss = 0.277191 (* 1 = 0.277191 loss)
I1107 14:58:20.699084 15588 solver.cpp:218] Iteration 99000 (9.27559 iter/s, 10.781s/100 iters), loss = 0.0693209
I1107 14:58:20.699084 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:58:20.699084 15588 solver.cpp:237]     Train net output #1: loss = 0.069321 (* 1 = 0.069321 loss)
I1107 14:58:20.699084 15588 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1107 14:58:29.537127 15588 solver.cpp:218] Iteration 99100 (11.3162 iter/s, 8.8369s/100 iters), loss = 0.0749678
I1107 14:58:29.537127 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 14:58:29.537127 15588 solver.cpp:237]     Train net output #1: loss = 0.0749679 (* 1 = 0.0749679 loss)
I1107 14:58:29.537127 15588 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1107 14:58:38.062085 15588 solver.cpp:218] Iteration 99200 (11.7308 iter/s, 8.52456s/100 iters), loss = 0.0698005
I1107 14:58:38.062085 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:58:38.062085 15588 solver.cpp:237]     Train net output #1: loss = 0.0698006 (* 1 = 0.0698006 loss)
I1107 14:58:38.062085 15588 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1107 14:58:46.569628 15588 solver.cpp:218] Iteration 99300 (11.7551 iter/s, 8.50697s/100 iters), loss = 0.0329319
I1107 14:58:46.569628 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:58:46.569628 15588 solver.cpp:237]     Train net output #1: loss = 0.032932 (* 1 = 0.032932 loss)
I1107 14:58:46.569628 15588 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1107 14:58:55.164204 15588 solver.cpp:218] Iteration 99400 (11.6349 iter/s, 8.59484s/100 iters), loss = 0.0397711
I1107 14:58:55.164204 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:58:55.164204 15588 solver.cpp:237]     Train net output #1: loss = 0.0397713 (* 1 = 0.0397713 loss)
I1107 14:58:55.164204 15588 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1107 14:59:03.536332 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:59:03.883375 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_99500.caffemodel
I1107 14:59:03.912376 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_99500.solverstate
I1107 14:59:03.922376 15588 solver.cpp:330] Iteration 99500, Testing net (#0)
I1107 14:59:03.922376 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:59:05.951535  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:59:06.031540 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9198
I1107 14:59:06.031540 15588 solver.cpp:397]     Test net output #1: loss = 0.276579 (* 1 = 0.276579 loss)
I1107 14:59:06.111589 15588 solver.cpp:218] Iteration 99500 (9.13509 iter/s, 10.9468s/100 iters), loss = 0.0528909
I1107 14:59:06.111589 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:59:06.111589 15588 solver.cpp:237]     Train net output #1: loss = 0.052891 (* 1 = 0.052891 loss)
I1107 14:59:06.111589 15588 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1107 14:59:14.633735 15588 solver.cpp:218] Iteration 99600 (11.7349 iter/s, 8.52159s/100 iters), loss = 0.081907
I1107 14:59:14.633735 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 14:59:14.633735 15588 solver.cpp:237]     Train net output #1: loss = 0.0819071 (* 1 = 0.0819071 loss)
I1107 14:59:14.633735 15588 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1107 14:59:23.152272 15588 solver.cpp:218] Iteration 99700 (11.7404 iter/s, 8.51757s/100 iters), loss = 0.0448395
I1107 14:59:23.152272 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:59:23.152272 15588 solver.cpp:237]     Train net output #1: loss = 0.0448397 (* 1 = 0.0448397 loss)
I1107 14:59:23.152272 15588 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1107 14:59:31.683255 15588 solver.cpp:218] Iteration 99800 (11.7231 iter/s, 8.53018s/100 iters), loss = 0.0601878
I1107 14:59:31.683255 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:59:31.683255 15588 solver.cpp:237]     Train net output #1: loss = 0.060188 (* 1 = 0.060188 loss)
I1107 14:59:31.683255 15588 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1107 14:59:40.204401 15588 solver.cpp:218] Iteration 99900 (11.7358 iter/s, 8.52093s/100 iters), loss = 0.0523802
I1107 14:59:40.204401 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:59:40.204401 15588 solver.cpp:237]     Train net output #1: loss = 0.0523804 (* 1 = 0.0523804 loss)
I1107 14:59:40.204401 15588 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1107 14:59:48.318647 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:59:48.652655 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_100000.caffemodel
I1107 14:59:48.682162 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_100000.solverstate
I1107 14:59:48.691161 15588 solver.cpp:330] Iteration 100000, Testing net (#0)
I1107 14:59:48.691161 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:59:50.677398  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:59:50.756834 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9214
I1107 14:59:50.756834 15588 solver.cpp:397]     Test net output #1: loss = 0.275189 (* 1 = 0.275189 loss)
I1107 14:59:50.837841 15588 solver.cpp:218] Iteration 100000 (9.40472 iter/s, 10.633s/100 iters), loss = 0.0498
I1107 14:59:50.837841 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:59:50.837841 15588 solver.cpp:237]     Train net output #1: loss = 0.0498002 (* 1 = 0.0498002 loss)
I1107 14:59:50.837841 15588 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1107 14:59:59.347326 15588 solver.cpp:218] Iteration 100100 (11.7523 iter/s, 8.50901s/100 iters), loss = 0.0680624
I1107 14:59:59.347326 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 14:59:59.347326 15588 solver.cpp:237]     Train net output #1: loss = 0.0680626 (* 1 = 0.0680626 loss)
I1107 14:59:59.347326 15588 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1107 15:00:07.928840 15588 solver.cpp:218] Iteration 100200 (11.6535 iter/s, 8.58113s/100 iters), loss = 0.0414744
I1107 15:00:07.928840 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:00:07.928840 15588 solver.cpp:237]     Train net output #1: loss = 0.0414745 (* 1 = 0.0414745 loss)
I1107 15:00:07.928840 15588 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1107 15:00:16.496660 15588 solver.cpp:218] Iteration 100300 (11.673 iter/s, 8.56681s/100 iters), loss = 0.0594431
I1107 15:00:16.496660 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:00:16.496660 15588 solver.cpp:237]     Train net output #1: loss = 0.0594432 (* 1 = 0.0594432 loss)
I1107 15:00:16.496660 15588 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1107 15:00:25.012511 15588 solver.cpp:218] Iteration 100400 (11.7434 iter/s, 8.51541s/100 iters), loss = 0.0584852
I1107 15:00:25.012511 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:00:25.012511 15588 solver.cpp:237]     Train net output #1: loss = 0.0584854 (* 1 = 0.0584854 loss)
I1107 15:00:25.012511 15588 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1107 15:00:33.126101 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:00:33.470661 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_100500.caffemodel
I1107 15:00:33.502660 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_100500.solverstate
I1107 15:00:33.511662 15588 solver.cpp:330] Iteration 100500, Testing net (#0)
I1107 15:00:33.511662 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:00:35.502050  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:00:35.582053 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 15:00:35.582053 15588 solver.cpp:397]     Test net output #1: loss = 0.281027 (* 1 = 0.281027 loss)
I1107 15:00:35.663058 15588 solver.cpp:218] Iteration 100500 (9.38966 iter/s, 10.65s/100 iters), loss = 0.070328
I1107 15:00:35.663058 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:00:35.663058 15588 solver.cpp:237]     Train net output #1: loss = 0.0703281 (* 1 = 0.0703281 loss)
I1107 15:00:35.663058 15588 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1107 15:00:44.186801 15588 solver.cpp:218] Iteration 100600 (11.732 iter/s, 8.52372s/100 iters), loss = 0.085255
I1107 15:00:44.187803 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:00:44.187803 15588 solver.cpp:237]     Train net output #1: loss = 0.0852551 (* 1 = 0.0852551 loss)
I1107 15:00:44.187803 15588 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1107 15:00:52.718148 15588 solver.cpp:218] Iteration 100700 (11.723 iter/s, 8.53024s/100 iters), loss = 0.040355
I1107 15:00:52.718148 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:00:52.718148 15588 solver.cpp:237]     Train net output #1: loss = 0.0403552 (* 1 = 0.0403552 loss)
I1107 15:00:52.718148 15588 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1107 15:01:01.232727 15588 solver.cpp:218] Iteration 100800 (11.7457 iter/s, 8.51378s/100 iters), loss = 0.0735818
I1107 15:01:01.232727 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:01:01.232727 15588 solver.cpp:237]     Train net output #1: loss = 0.0735819 (* 1 = 0.0735819 loss)
I1107 15:01:01.232727 15588 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1107 15:01:09.753666 15588 solver.cpp:218] Iteration 100900 (11.7358 iter/s, 8.52095s/100 iters), loss = 0.0492756
I1107 15:01:09.753666 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:01:09.753666 15588 solver.cpp:237]     Train net output #1: loss = 0.0492757 (* 1 = 0.0492757 loss)
I1107 15:01:09.753666 15588 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1107 15:01:17.889475 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:01:18.228011 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_101000.caffemodel
I1107 15:01:18.255515 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_101000.solverstate
I1107 15:01:18.265517 15588 solver.cpp:330] Iteration 101000, Testing net (#0)
I1107 15:01:18.265517 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:01:20.255722  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:01:20.335739 15588 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 15:01:20.335739 15588 solver.cpp:397]     Test net output #1: loss = 0.283837 (* 1 = 0.283837 loss)
I1107 15:01:20.417750 15588 solver.cpp:218] Iteration 101000 (9.37829 iter/s, 10.6629s/100 iters), loss = 0.051236
I1107 15:01:20.417750 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:01:20.417750 15588 solver.cpp:237]     Train net output #1: loss = 0.0512361 (* 1 = 0.0512361 loss)
I1107 15:01:20.417750 15588 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1107 15:01:28.938838 15588 solver.cpp:218] Iteration 101100 (11.7352 iter/s, 8.52138s/100 iters), loss = 0.078996
I1107 15:01:28.938838 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:01:28.938838 15588 solver.cpp:237]     Train net output #1: loss = 0.0789961 (* 1 = 0.0789961 loss)
I1107 15:01:28.938838 15588 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1107 15:01:37.476753 15588 solver.cpp:218] Iteration 101200 (11.7141 iter/s, 8.53673s/100 iters), loss = 0.0602451
I1107 15:01:37.476753 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:01:37.476753 15588 solver.cpp:237]     Train net output #1: loss = 0.0602452 (* 1 = 0.0602452 loss)
I1107 15:01:37.476753 15588 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1107 15:01:46.022933 15588 solver.cpp:218] Iteration 101300 (11.7021 iter/s, 8.5455s/100 iters), loss = 0.0292473
I1107 15:01:46.022933 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:01:46.022933 15588 solver.cpp:237]     Train net output #1: loss = 0.0292474 (* 1 = 0.0292474 loss)
I1107 15:01:46.022933 15588 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1107 15:01:54.558353 15588 solver.cpp:218] Iteration 101400 (11.7159 iter/s, 8.53541s/100 iters), loss = 0.0711982
I1107 15:01:54.558353 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:01:54.558353 15588 solver.cpp:237]     Train net output #1: loss = 0.0711983 (* 1 = 0.0711983 loss)
I1107 15:01:54.558353 15588 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1107 15:02:02.823668 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:02:03.174818 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_101500.caffemodel
I1107 15:02:03.208817 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_101500.solverstate
I1107 15:02:03.217818 15588 solver.cpp:330] Iteration 101500, Testing net (#0)
I1107 15:02:03.217818 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:02:05.273077  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:02:05.356577 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 15:02:05.356577 15588 solver.cpp:397]     Test net output #1: loss = 0.281899 (* 1 = 0.281899 loss)
I1107 15:02:05.440080 15588 solver.cpp:218] Iteration 101500 (9.19006 iter/s, 10.8813s/100 iters), loss = 0.0622247
I1107 15:02:05.440080 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:02:05.440080 15588 solver.cpp:237]     Train net output #1: loss = 0.0622248 (* 1 = 0.0622248 loss)
I1107 15:02:05.440080 15588 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1107 15:02:14.352025 15588 solver.cpp:218] Iteration 101600 (11.2225 iter/s, 8.91069s/100 iters), loss = 0.101219
I1107 15:02:14.352025 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 15:02:14.352025 15588 solver.cpp:237]     Train net output #1: loss = 0.101219 (* 1 = 0.101219 loss)
I1107 15:02:14.352025 15588 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1107 15:02:22.983646 15588 solver.cpp:218] Iteration 101700 (11.5857 iter/s, 8.63135s/100 iters), loss = 0.0602226
I1107 15:02:22.983646 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:02:22.983646 15588 solver.cpp:237]     Train net output #1: loss = 0.0602227 (* 1 = 0.0602227 loss)
I1107 15:02:22.983646 15588 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1107 15:02:31.572762 15588 solver.cpp:218] Iteration 101800 (11.6439 iter/s, 8.58817s/100 iters), loss = 0.0422465
I1107 15:02:31.572762 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:02:31.572762 15588 solver.cpp:237]     Train net output #1: loss = 0.0422466 (* 1 = 0.0422466 loss)
I1107 15:02:31.572762 15588 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1107 15:02:40.091207 15588 solver.cpp:218] Iteration 101900 (11.7392 iter/s, 8.51846s/100 iters), loss = 0.0489243
I1107 15:02:40.091207 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:02:40.091207 15588 solver.cpp:237]     Train net output #1: loss = 0.0489244 (* 1 = 0.0489244 loss)
I1107 15:02:40.091207 15588 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1107 15:02:48.277673 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:02:48.624246 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_102000.caffemodel
I1107 15:02:48.661247 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_102000.solverstate
I1107 15:02:48.671753 15588 solver.cpp:330] Iteration 102000, Testing net (#0)
I1107 15:02:48.671753 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:02:50.694536  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:02:50.774538 15588 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 15:02:50.774538 15588 solver.cpp:397]     Test net output #1: loss = 0.282346 (* 1 = 0.282346 loss)
I1107 15:02:50.854540 15588 solver.cpp:218] Iteration 102000 (9.29146 iter/s, 10.7626s/100 iters), loss = 0.0524985
I1107 15:02:50.854540 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:02:50.854540 15588 solver.cpp:237]     Train net output #1: loss = 0.0524986 (* 1 = 0.0524986 loss)
I1107 15:02:50.854540 15588 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1107 15:02:59.454394 15588 solver.cpp:218] Iteration 102100 (11.629 iter/s, 8.59919s/100 iters), loss = 0.0919142
I1107 15:02:59.454394 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:02:59.454394 15588 solver.cpp:237]     Train net output #1: loss = 0.0919143 (* 1 = 0.0919143 loss)
I1107 15:02:59.454394 15588 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1107 15:03:08.027194 15588 solver.cpp:218] Iteration 102200 (11.6653 iter/s, 8.57241s/100 iters), loss = 0.0669329
I1107 15:03:08.027194 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:03:08.027194 15588 solver.cpp:237]     Train net output #1: loss = 0.066933 (* 1 = 0.066933 loss)
I1107 15:03:08.027194 15588 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1107 15:03:16.723217 15588 solver.cpp:218] Iteration 102300 (11.5009 iter/s, 8.69499s/100 iters), loss = 0.0361836
I1107 15:03:16.723217 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:03:16.723217 15588 solver.cpp:237]     Train net output #1: loss = 0.0361837 (* 1 = 0.0361837 loss)
I1107 15:03:16.723217 15588 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1107 15:03:25.280284 15588 solver.cpp:218] Iteration 102400 (11.6866 iter/s, 8.55684s/100 iters), loss = 0.0523772
I1107 15:03:25.280284 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:03:25.280284 15588 solver.cpp:237]     Train net output #1: loss = 0.0523773 (* 1 = 0.0523773 loss)
I1107 15:03:25.280284 15588 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1107 15:03:33.349508 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:03:33.685595 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_102500.caffemodel
I1107 15:03:33.719596 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_102500.solverstate
I1107 15:03:33.728597 15588 solver.cpp:330] Iteration 102500, Testing net (#0)
I1107 15:03:33.728597 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:03:35.719883  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:03:35.799887 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9188
I1107 15:03:35.799887 15588 solver.cpp:397]     Test net output #1: loss = 0.281362 (* 1 = 0.281362 loss)
I1107 15:03:35.880888 15588 solver.cpp:218] Iteration 102500 (9.43424 iter/s, 10.5997s/100 iters), loss = 0.0404135
I1107 15:03:35.880888 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:03:35.880888 15588 solver.cpp:237]     Train net output #1: loss = 0.0404136 (* 1 = 0.0404136 loss)
I1107 15:03:35.880888 15588 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1107 15:03:44.355607 15588 solver.cpp:218] Iteration 102600 (11.7993 iter/s, 8.47507s/100 iters), loss = 0.0653981
I1107 15:03:44.355607 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:03:44.355607 15588 solver.cpp:237]     Train net output #1: loss = 0.0653983 (* 1 = 0.0653983 loss)
I1107 15:03:44.355607 15588 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1107 15:03:52.843108 15588 solver.cpp:218] Iteration 102700 (11.7838 iter/s, 8.48621s/100 iters), loss = 0.0454491
I1107 15:03:52.843108 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:03:52.843108 15588 solver.cpp:237]     Train net output #1: loss = 0.0454492 (* 1 = 0.0454492 loss)
I1107 15:03:52.843108 15588 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1107 15:04:01.340283 15588 solver.cpp:218] Iteration 102800 (11.7683 iter/s, 8.4974s/100 iters), loss = 0.0413842
I1107 15:04:01.340283 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:04:01.340283 15588 solver.cpp:237]     Train net output #1: loss = 0.0413843 (* 1 = 0.0413843 loss)
I1107 15:04:01.340283 15588 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1107 15:04:09.857875 15588 solver.cpp:218] Iteration 102900 (11.7421 iter/s, 8.51636s/100 iters), loss = 0.0519822
I1107 15:04:09.857875 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:04:09.857875 15588 solver.cpp:237]     Train net output #1: loss = 0.0519823 (* 1 = 0.0519823 loss)
I1107 15:04:09.857875 15588 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1107 15:04:17.990120 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:04:18.326160 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_103000.caffemodel
I1107 15:04:18.355159 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_103000.solverstate
I1107 15:04:18.363158 15588 solver.cpp:330] Iteration 103000, Testing net (#0)
I1107 15:04:18.364158 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:04:20.367295  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:04:20.447300 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9216
I1107 15:04:20.447300 15588 solver.cpp:397]     Test net output #1: loss = 0.279447 (* 1 = 0.279447 loss)
I1107 15:04:20.527307 15588 solver.cpp:218] Iteration 103000 (9.37247 iter/s, 10.6695s/100 iters), loss = 0.072452
I1107 15:04:20.528306 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:04:20.528306 15588 solver.cpp:237]     Train net output #1: loss = 0.0724521 (* 1 = 0.0724521 loss)
I1107 15:04:20.528306 15588 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1107 15:04:29.143381 15588 solver.cpp:218] Iteration 103100 (11.6076 iter/s, 8.61504s/100 iters), loss = 0.0476123
I1107 15:04:29.143381 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:04:29.143381 15588 solver.cpp:237]     Train net output #1: loss = 0.0476124 (* 1 = 0.0476124 loss)
I1107 15:04:29.143381 15588 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1107 15:04:37.646857 15588 solver.cpp:218] Iteration 103200 (11.7603 iter/s, 8.50322s/100 iters), loss = 0.0662805
I1107 15:04:37.646857 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:04:37.646857 15588 solver.cpp:237]     Train net output #1: loss = 0.0662806 (* 1 = 0.0662806 loss)
I1107 15:04:37.646857 15588 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1107 15:04:46.147925 15588 solver.cpp:218] Iteration 103300 (11.7646 iter/s, 8.50005s/100 iters), loss = 0.0510444
I1107 15:04:46.147925 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:04:46.147925 15588 solver.cpp:237]     Train net output #1: loss = 0.0510445 (* 1 = 0.0510445 loss)
I1107 15:04:46.147925 15588 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1107 15:04:54.631491 15588 solver.cpp:218] Iteration 103400 (11.7877 iter/s, 8.48341s/100 iters), loss = 0.0449082
I1107 15:04:54.631491 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:04:54.631491 15588 solver.cpp:237]     Train net output #1: loss = 0.0449084 (* 1 = 0.0449084 loss)
I1107 15:04:54.631491 15588 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1107 15:05:02.720049 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:05:03.055598 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_103500.caffemodel
I1107 15:05:03.085606 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_103500.solverstate
I1107 15:05:03.094606 15588 solver.cpp:330] Iteration 103500, Testing net (#0)
I1107 15:05:03.094606 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:05:05.080044  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:05:05.159049 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9196
I1107 15:05:05.159049 15588 solver.cpp:397]     Test net output #1: loss = 0.283044 (* 1 = 0.283044 loss)
I1107 15:05:05.239295 15588 solver.cpp:218] Iteration 103500 (9.4272 iter/s, 10.6076s/100 iters), loss = 0.0476931
I1107 15:05:05.240295 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:05:05.240295 15588 solver.cpp:237]     Train net output #1: loss = 0.0476933 (* 1 = 0.0476933 loss)
I1107 15:05:05.240295 15588 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1107 15:05:13.740409 15588 solver.cpp:218] Iteration 103600 (11.7641 iter/s, 8.50043s/100 iters), loss = 0.0686652
I1107 15:05:13.740409 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:05:13.740409 15588 solver.cpp:237]     Train net output #1: loss = 0.0686653 (* 1 = 0.0686653 loss)
I1107 15:05:13.740409 15588 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1107 15:05:22.263365 15588 solver.cpp:218] Iteration 103700 (11.7348 iter/s, 8.52169s/100 iters), loss = 0.0440258
I1107 15:05:22.263365 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:05:22.263365 15588 solver.cpp:237]     Train net output #1: loss = 0.0440259 (* 1 = 0.0440259 loss)
I1107 15:05:22.263365 15588 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1107 15:05:30.828238 15588 solver.cpp:218] Iteration 103800 (11.6758 iter/s, 8.56475s/100 iters), loss = 0.0394063
I1107 15:05:30.828238 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:05:30.828238 15588 solver.cpp:237]     Train net output #1: loss = 0.0394064 (* 1 = 0.0394064 loss)
I1107 15:05:30.828238 15588 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1107 15:05:39.447948 15588 solver.cpp:218] Iteration 103900 (11.6018 iter/s, 8.61934s/100 iters), loss = 0.0377216
I1107 15:05:39.447948 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:05:39.447948 15588 solver.cpp:237]     Train net output #1: loss = 0.0377217 (* 1 = 0.0377217 loss)
I1107 15:05:39.447948 15588 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1107 15:05:47.540388 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:05:47.879019 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_104000.caffemodel
I1107 15:05:47.914525 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_104000.solverstate
I1107 15:05:47.923789 15588 solver.cpp:330] Iteration 104000, Testing net (#0)
I1107 15:05:47.923789 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:05:49.919625  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:05:49.999137 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9172
I1107 15:05:49.999137 15588 solver.cpp:397]     Test net output #1: loss = 0.284935 (* 1 = 0.284935 loss)
I1107 15:05:50.080709 15588 solver.cpp:218] Iteration 104000 (9.40579 iter/s, 10.6317s/100 iters), loss = 0.0660543
I1107 15:05:50.080709 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 15:05:50.080709 15588 solver.cpp:237]     Train net output #1: loss = 0.0660544 (* 1 = 0.0660544 loss)
I1107 15:05:50.080709 15588 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1107 15:05:58.617152 15588 solver.cpp:218] Iteration 104100 (11.7154 iter/s, 8.53581s/100 iters), loss = 0.11274
I1107 15:05:58.617152 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 15:05:58.617152 15588 solver.cpp:237]     Train net output #1: loss = 0.11274 (* 1 = 0.11274 loss)
I1107 15:05:58.617152 15588 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1107 15:06:07.135277 15588 solver.cpp:218] Iteration 104200 (11.7401 iter/s, 8.51778s/100 iters), loss = 0.0785537
I1107 15:06:07.135277 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:06:07.135277 15588 solver.cpp:237]     Train net output #1: loss = 0.0785538 (* 1 = 0.0785538 loss)
I1107 15:06:07.135277 15588 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1107 15:06:15.676026 15588 solver.cpp:218] Iteration 104300 (11.7094 iter/s, 8.54013s/100 iters), loss = 0.0392793
I1107 15:06:15.676026 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:06:15.676026 15588 solver.cpp:237]     Train net output #1: loss = 0.0392794 (* 1 = 0.0392794 loss)
I1107 15:06:15.676026 15588 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1107 15:06:24.208101 15588 solver.cpp:218] Iteration 104400 (11.7209 iter/s, 8.53174s/100 iters), loss = 0.0418232
I1107 15:06:24.208101 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:06:24.208101 15588 solver.cpp:237]     Train net output #1: loss = 0.0418233 (* 1 = 0.0418233 loss)
I1107 15:06:24.208101 15588 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1107 15:06:32.304862 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:06:32.642861 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_104500.caffemodel
I1107 15:06:32.673362 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_104500.solverstate
I1107 15:06:32.682361 15588 solver.cpp:330] Iteration 104500, Testing net (#0)
I1107 15:06:32.682361 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:06:34.672361  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:06:34.751862 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 15:06:34.751862 15588 solver.cpp:397]     Test net output #1: loss = 0.283369 (* 1 = 0.283369 loss)
I1107 15:06:34.832860 15588 solver.cpp:218] Iteration 104500 (9.41227 iter/s, 10.6244s/100 iters), loss = 0.054046
I1107 15:06:34.833361 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:06:34.833361 15588 solver.cpp:237]     Train net output #1: loss = 0.0540462 (* 1 = 0.0540462 loss)
I1107 15:06:34.833361 15588 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1107 15:06:43.349918 15588 solver.cpp:218] Iteration 104600 (11.7419 iter/s, 8.51648s/100 iters), loss = 0.0708548
I1107 15:06:43.349918 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:06:43.349918 15588 solver.cpp:237]     Train net output #1: loss = 0.0708549 (* 1 = 0.0708549 loss)
I1107 15:06:43.349918 15588 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1107 15:06:51.864672 15588 solver.cpp:218] Iteration 104700 (11.7455 iter/s, 8.51392s/100 iters), loss = 0.0436031
I1107 15:06:51.864672 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:06:51.864672 15588 solver.cpp:237]     Train net output #1: loss = 0.0436033 (* 1 = 0.0436033 loss)
I1107 15:06:51.864672 15588 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1107 15:07:00.376262 15588 solver.cpp:218] Iteration 104800 (11.7496 iter/s, 8.51094s/100 iters), loss = 0.0602655
I1107 15:07:00.376262 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:07:00.376262 15588 solver.cpp:237]     Train net output #1: loss = 0.0602656 (* 1 = 0.0602656 loss)
I1107 15:07:00.376262 15588 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1107 15:07:08.897387 15588 solver.cpp:218] Iteration 104900 (11.7358 iter/s, 8.52096s/100 iters), loss = 0.0320475
I1107 15:07:08.897387 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:07:08.897887 15588 solver.cpp:237]     Train net output #1: loss = 0.0320477 (* 1 = 0.0320477 loss)
I1107 15:07:08.897887 15588 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1107 15:07:16.996424 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:07:17.331923 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_105000.caffemodel
I1107 15:07:17.359922 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_105000.solverstate
I1107 15:07:17.368923 15588 solver.cpp:330] Iteration 105000, Testing net (#0)
I1107 15:07:17.368923 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:07:19.355927  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:07:19.436430 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 15:07:19.436430 15588 solver.cpp:397]     Test net output #1: loss = 0.285208 (* 1 = 0.285208 loss)
I1107 15:07:19.517932 15588 solver.cpp:218] Iteration 105000 (9.41656 iter/s, 10.6196s/100 iters), loss = 0.0536198
I1107 15:07:19.517932 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:07:19.517932 15588 solver.cpp:237]     Train net output #1: loss = 0.05362 (* 1 = 0.05362 loss)
I1107 15:07:19.517932 15588 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1107 15:07:28.025291 15588 solver.cpp:218] Iteration 105100 (11.7549 iter/s, 8.50712s/100 iters), loss = 0.050531
I1107 15:07:28.025291 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:07:28.025291 15588 solver.cpp:237]     Train net output #1: loss = 0.0505312 (* 1 = 0.0505312 loss)
I1107 15:07:28.025291 15588 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1107 15:07:36.538430 15588 solver.cpp:218] Iteration 105200 (11.747 iter/s, 8.51278s/100 iters), loss = 0.0355902
I1107 15:07:36.538930 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:07:36.538930 15588 solver.cpp:237]     Train net output #1: loss = 0.0355903 (* 1 = 0.0355903 loss)
I1107 15:07:36.538930 15588 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1107 15:07:45.057826 15588 solver.cpp:218] Iteration 105300 (11.7392 iter/s, 8.51849s/100 iters), loss = 0.0269651
I1107 15:07:45.057826 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:07:45.057826 15588 solver.cpp:237]     Train net output #1: loss = 0.0269653 (* 1 = 0.0269653 loss)
I1107 15:07:45.057826 15588 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1107 15:07:53.578845 15588 solver.cpp:218] Iteration 105400 (11.7358 iter/s, 8.5209s/100 iters), loss = 0.0572486
I1107 15:07:53.579346 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:07:53.579346 15588 solver.cpp:237]     Train net output #1: loss = 0.0572488 (* 1 = 0.0572488 loss)
I1107 15:07:53.579346 15588 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1107 15:08:01.702303 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:08:02.040303 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_105500.caffemodel
I1107 15:08:02.077302 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_105500.solverstate
I1107 15:08:02.087302 15588 solver.cpp:330] Iteration 105500, Testing net (#0)
I1107 15:08:02.087302 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:08:04.080803  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:08:04.160305 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 15:08:04.160305 15588 solver.cpp:397]     Test net output #1: loss = 0.289189 (* 1 = 0.289189 loss)
I1107 15:08:04.240803 15588 solver.cpp:218] Iteration 105500 (9.37969 iter/s, 10.6613s/100 iters), loss = 0.0528705
I1107 15:08:04.240803 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:08:04.240803 15588 solver.cpp:237]     Train net output #1: loss = 0.0528707 (* 1 = 0.0528707 loss)
I1107 15:08:04.241303 15588 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1107 15:08:12.858476 15588 solver.cpp:218] Iteration 105600 (11.6044 iter/s, 8.61745s/100 iters), loss = 0.081507
I1107 15:08:12.858476 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 15:08:12.858476 15588 solver.cpp:237]     Train net output #1: loss = 0.0815071 (* 1 = 0.0815071 loss)
I1107 15:08:12.858476 15588 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1107 15:08:21.375165 15588 solver.cpp:218] Iteration 105700 (11.7429 iter/s, 8.51578s/100 iters), loss = 0.0320166
I1107 15:08:21.375165 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:08:21.375165 15588 solver.cpp:237]     Train net output #1: loss = 0.0320168 (* 1 = 0.0320168 loss)
I1107 15:08:21.375165 15588 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1107 15:08:29.907531 15588 solver.cpp:218] Iteration 105800 (11.7203 iter/s, 8.53218s/100 iters), loss = 0.0372203
I1107 15:08:29.907531 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:08:29.907531 15588 solver.cpp:237]     Train net output #1: loss = 0.0372205 (* 1 = 0.0372205 loss)
I1107 15:08:29.907531 15588 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1107 15:08:38.404376 15588 solver.cpp:218] Iteration 105900 (11.7696 iter/s, 8.49649s/100 iters), loss = 0.049053
I1107 15:08:38.404376 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:08:38.404376 15588 solver.cpp:237]     Train net output #1: loss = 0.0490531 (* 1 = 0.0490531 loss)
I1107 15:08:38.404376 15588 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1107 15:08:46.477078 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:08:46.813117 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_106000.caffemodel
I1107 15:08:46.846115 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_106000.solverstate
I1107 15:08:46.855115 15588 solver.cpp:330] Iteration 106000, Testing net (#0)
I1107 15:08:46.855115 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:08:48.867316  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:08:48.947322 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 15:08:48.947322 15588 solver.cpp:397]     Test net output #1: loss = 0.289425 (* 1 = 0.289425 loss)
I1107 15:08:49.028342 15588 solver.cpp:218] Iteration 106000 (9.41319 iter/s, 10.6234s/100 iters), loss = 0.0476217
I1107 15:08:49.028342 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:08:49.028342 15588 solver.cpp:237]     Train net output #1: loss = 0.0476218 (* 1 = 0.0476218 loss)
I1107 15:08:49.028342 15588 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1107 15:08:57.621803 15588 solver.cpp:218] Iteration 106100 (11.6379 iter/s, 8.59258s/100 iters), loss = 0.0837137
I1107 15:08:57.621803 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:08:57.621803 15588 solver.cpp:237]     Train net output #1: loss = 0.0837138 (* 1 = 0.0837138 loss)
I1107 15:08:57.621803 15588 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1107 15:09:06.206889 15588 solver.cpp:218] Iteration 106200 (11.6484 iter/s, 8.58486s/100 iters), loss = 0.0486556
I1107 15:09:06.206889 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:09:06.206889 15588 solver.cpp:237]     Train net output #1: loss = 0.0486558 (* 1 = 0.0486558 loss)
I1107 15:09:06.206889 15588 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1107 15:09:14.750748 15588 solver.cpp:218] Iteration 106300 (11.7059 iter/s, 8.54271s/100 iters), loss = 0.0303022
I1107 15:09:14.750748 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:09:14.750748 15588 solver.cpp:237]     Train net output #1: loss = 0.0303024 (* 1 = 0.0303024 loss)
I1107 15:09:14.750748 15588 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1107 15:09:23.284574 15588 solver.cpp:218] Iteration 106400 (11.7187 iter/s, 8.53336s/100 iters), loss = 0.038121
I1107 15:09:23.284574 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:09:23.284574 15588 solver.cpp:237]     Train net output #1: loss = 0.0381212 (* 1 = 0.0381212 loss)
I1107 15:09:23.284574 15588 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1107 15:09:31.512681 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:09:31.850708 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_106500.caffemodel
I1107 15:09:31.880209 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_106500.solverstate
I1107 15:09:31.894709 15588 solver.cpp:330] Iteration 106500, Testing net (#0)
I1107 15:09:31.895217 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:09:33.917923  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:09:33.999932 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9185
I1107 15:09:33.999932 15588 solver.cpp:397]     Test net output #1: loss = 0.28735 (* 1 = 0.28735 loss)
I1107 15:09:34.084969 15588 solver.cpp:218] Iteration 106500 (9.25955 iter/s, 10.7997s/100 iters), loss = 0.0737009
I1107 15:09:34.084969 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:09:34.084969 15588 solver.cpp:237]     Train net output #1: loss = 0.0737011 (* 1 = 0.0737011 loss)
I1107 15:09:34.084969 15588 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1107 15:09:42.654284 15588 solver.cpp:218] Iteration 106600 (11.6695 iter/s, 8.56934s/100 iters), loss = 0.042914
I1107 15:09:42.654284 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:09:42.654284 15588 solver.cpp:237]     Train net output #1: loss = 0.0429142 (* 1 = 0.0429142 loss)
I1107 15:09:42.654284 15588 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1107 15:09:51.227144 15588 solver.cpp:218] Iteration 106700 (11.6665 iter/s, 8.57159s/100 iters), loss = 0.0268158
I1107 15:09:51.227144 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:09:51.227144 15588 solver.cpp:237]     Train net output #1: loss = 0.026816 (* 1 = 0.026816 loss)
I1107 15:09:51.227144 15588 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1107 15:09:59.838407 15588 solver.cpp:218] Iteration 106800 (11.6132 iter/s, 8.61092s/100 iters), loss = 0.0291826
I1107 15:09:59.838407 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:09:59.838407 15588 solver.cpp:237]     Train net output #1: loss = 0.0291827 (* 1 = 0.0291827 loss)
I1107 15:09:59.838407 15588 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1107 15:10:08.439272 15588 solver.cpp:218] Iteration 106900 (11.6273 iter/s, 8.60046s/100 iters), loss = 0.0358202
I1107 15:10:08.439272 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:10:08.439272 15588 solver.cpp:237]     Train net output #1: loss = 0.0358204 (* 1 = 0.0358204 loss)
I1107 15:10:08.439272 15588 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1107 15:10:16.640488 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:10:16.981516 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_107000.caffemodel
I1107 15:10:17.019524 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_107000.solverstate
I1107 15:10:17.028525 15588 solver.cpp:330] Iteration 107000, Testing net (#0)
I1107 15:10:17.028525 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:10:19.031667  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:10:19.111671 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9173
I1107 15:10:19.111671 15588 solver.cpp:397]     Test net output #1: loss = 0.291568 (* 1 = 0.291568 loss)
I1107 15:10:19.191673 15588 solver.cpp:218] Iteration 107000 (9.30068 iter/s, 10.7519s/100 iters), loss = 0.039582
I1107 15:10:19.191673 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:10:19.191673 15588 solver.cpp:237]     Train net output #1: loss = 0.0395822 (* 1 = 0.0395822 loss)
I1107 15:10:19.191673 15588 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1107 15:10:27.748601 15588 solver.cpp:218] Iteration 107100 (11.6874 iter/s, 8.55623s/100 iters), loss = 0.083582
I1107 15:10:27.748601 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 15:10:27.748601 15588 solver.cpp:237]     Train net output #1: loss = 0.0835822 (* 1 = 0.0835822 loss)
I1107 15:10:27.748601 15588 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1107 15:10:36.261742 15588 solver.cpp:218] Iteration 107200 (11.7468 iter/s, 8.51292s/100 iters), loss = 0.0583966
I1107 15:10:36.261742 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:10:36.261742 15588 solver.cpp:237]     Train net output #1: loss = 0.0583967 (* 1 = 0.0583967 loss)
I1107 15:10:36.261742 15588 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1107 15:10:44.783438 15588 solver.cpp:218] Iteration 107300 (11.7351 iter/s, 8.52146s/100 iters), loss = 0.025822
I1107 15:10:44.783438 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:10:44.783438 15588 solver.cpp:237]     Train net output #1: loss = 0.0258221 (* 1 = 0.0258221 loss)
I1107 15:10:44.783438 15588 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1107 15:10:53.286389 15588 solver.cpp:218] Iteration 107400 (11.7623 iter/s, 8.50175s/100 iters), loss = 0.0517184
I1107 15:10:53.286389 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:10:53.286389 15588 solver.cpp:237]     Train net output #1: loss = 0.0517185 (* 1 = 0.0517185 loss)
I1107 15:10:53.286389 15588 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1107 15:11:01.407565 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:11:01.744601 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_107500.caffemodel
I1107 15:11:01.780607 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_107500.solverstate
I1107 15:11:01.789608 15588 solver.cpp:330] Iteration 107500, Testing net (#0)
I1107 15:11:01.789608 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:11:03.779907  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:11:03.860911 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9171
I1107 15:11:03.860911 15588 solver.cpp:397]     Test net output #1: loss = 0.289024 (* 1 = 0.289024 loss)
I1107 15:11:03.942919 15588 solver.cpp:218] Iteration 107500 (9.38447 iter/s, 10.6559s/100 iters), loss = 0.0730023
I1107 15:11:03.942919 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:11:03.942919 15588 solver.cpp:237]     Train net output #1: loss = 0.0730024 (* 1 = 0.0730024 loss)
I1107 15:11:03.942919 15588 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1107 15:11:12.484812 15588 solver.cpp:218] Iteration 107600 (11.7067 iter/s, 8.54213s/100 iters), loss = 0.037193
I1107 15:11:12.484812 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:11:12.484812 15588 solver.cpp:237]     Train net output #1: loss = 0.0371931 (* 1 = 0.0371931 loss)
I1107 15:11:12.484812 15588 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1107 15:11:21.105463 15588 solver.cpp:218] Iteration 107700 (11.6017 iter/s, 8.61941s/100 iters), loss = 0.0334815
I1107 15:11:21.105463 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:11:21.105463 15588 solver.cpp:237]     Train net output #1: loss = 0.0334817 (* 1 = 0.0334817 loss)
I1107 15:11:21.105463 15588 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1107 15:11:29.781193 15588 solver.cpp:218] Iteration 107800 (11.5271 iter/s, 8.67518s/100 iters), loss = 0.0290477
I1107 15:11:29.781193 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:11:29.781193 15588 solver.cpp:237]     Train net output #1: loss = 0.0290479 (* 1 = 0.0290479 loss)
I1107 15:11:29.781193 15588 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1107 15:11:38.326282 15588 solver.cpp:218] Iteration 107900 (11.7029 iter/s, 8.54491s/100 iters), loss = 0.0527145
I1107 15:11:38.326282 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:11:38.326282 15588 solver.cpp:237]     Train net output #1: loss = 0.0527147 (* 1 = 0.0527147 loss)
I1107 15:11:38.326282 15588 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1107 15:11:46.468406 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:11:46.808444 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_108000.caffemodel
I1107 15:11:46.843443 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_108000.solverstate
I1107 15:11:46.853443 15588 solver.cpp:330] Iteration 108000, Testing net (#0)
I1107 15:11:46.853443 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:11:48.845751  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:11:48.925925 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9175
I1107 15:11:48.925925 15588 solver.cpp:397]     Test net output #1: loss = 0.291803 (* 1 = 0.291803 loss)
I1107 15:11:49.006052 15588 solver.cpp:218] Iteration 108000 (9.3639 iter/s, 10.6793s/100 iters), loss = 0.0437547
I1107 15:11:49.006052 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:11:49.006052 15588 solver.cpp:237]     Train net output #1: loss = 0.0437549 (* 1 = 0.0437549 loss)
I1107 15:11:49.006052 15588 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1107 15:11:57.621716 15588 solver.cpp:218] Iteration 108100 (11.6075 iter/s, 8.61511s/100 iters), loss = 0.0668377
I1107 15:11:57.621716 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:11:57.621716 15588 solver.cpp:237]     Train net output #1: loss = 0.0668379 (* 1 = 0.0668379 loss)
I1107 15:11:57.621716 15588 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1107 15:12:06.138001 15588 solver.cpp:218] Iteration 108200 (11.7442 iter/s, 8.51484s/100 iters), loss = 0.0541384
I1107 15:12:06.138001 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:12:06.138001 15588 solver.cpp:237]     Train net output #1: loss = 0.0541386 (* 1 = 0.0541386 loss)
I1107 15:12:06.138001 15588 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1107 15:12:14.689785 15588 solver.cpp:218] Iteration 108300 (11.6936 iter/s, 8.55166s/100 iters), loss = 0.0262114
I1107 15:12:14.689785 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:12:14.689785 15588 solver.cpp:237]     Train net output #1: loss = 0.0262116 (* 1 = 0.0262116 loss)
I1107 15:12:14.689785 15588 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1107 15:12:23.254680 15588 solver.cpp:218] Iteration 108400 (11.6761 iter/s, 8.56453s/100 iters), loss = 0.0540086
I1107 15:12:23.254680 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:12:23.254680 15588 solver.cpp:237]     Train net output #1: loss = 0.0540088 (* 1 = 0.0540088 loss)
I1107 15:12:23.254680 15588 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1107 15:12:31.400037 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:12:31.747057 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_108500.caffemodel
I1107 15:12:31.779563 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_108500.solverstate
I1107 15:12:31.789072 15588 solver.cpp:330] Iteration 108500, Testing net (#0)
I1107 15:12:31.789072 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:12:33.816275  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:12:33.896281 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 15:12:33.896281 15588 solver.cpp:397]     Test net output #1: loss = 0.289099 (* 1 = 0.289099 loss)
I1107 15:12:33.978282 15588 solver.cpp:218] Iteration 108500 (9.32629 iter/s, 10.7224s/100 iters), loss = 0.070981
I1107 15:12:33.978282 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:12:33.978282 15588 solver.cpp:237]     Train net output #1: loss = 0.0709811 (* 1 = 0.0709811 loss)
I1107 15:12:33.978282 15588 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1107 15:12:42.475916 15588 solver.cpp:218] Iteration 108600 (11.7688 iter/s, 8.49705s/100 iters), loss = 0.0440383
I1107 15:12:42.475916 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:12:42.475916 15588 solver.cpp:237]     Train net output #1: loss = 0.0440385 (* 1 = 0.0440385 loss)
I1107 15:12:42.475916 15588 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1107 15:12:50.974834 15588 solver.cpp:218] Iteration 108700 (11.7669 iter/s, 8.49844s/100 iters), loss = 0.0391013
I1107 15:12:50.974834 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:12:50.974834 15588 solver.cpp:237]     Train net output #1: loss = 0.0391015 (* 1 = 0.0391015 loss)
I1107 15:12:50.974834 15588 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1107 15:12:59.501579 15588 solver.cpp:218] Iteration 108800 (11.728 iter/s, 8.52662s/100 iters), loss = 0.0406576
I1107 15:12:59.501579 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:12:59.501579 15588 solver.cpp:237]     Train net output #1: loss = 0.0406578 (* 1 = 0.0406578 loss)
I1107 15:12:59.501579 15588 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1107 15:13:08.078516 15588 solver.cpp:218] Iteration 108900 (11.6603 iter/s, 8.57614s/100 iters), loss = 0.0295881
I1107 15:13:08.078516 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:13:08.078516 15588 solver.cpp:237]     Train net output #1: loss = 0.0295883 (* 1 = 0.0295883 loss)
I1107 15:13:08.078516 15588 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1107 15:13:16.154242 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:13:16.491277 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_109000.caffemodel
I1107 15:13:16.526280 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_109000.solverstate
I1107 15:13:16.535280 15588 solver.cpp:330] Iteration 109000, Testing net (#0)
I1107 15:13:16.535280 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:13:18.521546  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:13:18.600551 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9185
I1107 15:13:18.600551 15588 solver.cpp:397]     Test net output #1: loss = 0.291177 (* 1 = 0.291177 loss)
I1107 15:13:18.681054 15588 solver.cpp:218] Iteration 109000 (9.43239 iter/s, 10.6018s/100 iters), loss = 0.0620763
I1107 15:13:18.681054 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:13:18.681054 15588 solver.cpp:237]     Train net output #1: loss = 0.0620765 (* 1 = 0.0620765 loss)
I1107 15:13:18.681054 15588 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1107 15:13:27.182977 15588 solver.cpp:218] Iteration 109100 (11.7626 iter/s, 8.50156s/100 iters), loss = 0.0963713
I1107 15:13:27.182977 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 15:13:27.182977 15588 solver.cpp:237]     Train net output #1: loss = 0.0963715 (* 1 = 0.0963715 loss)
I1107 15:13:27.182977 15588 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1107 15:13:35.672114 15588 solver.cpp:218] Iteration 109200 (11.7794 iter/s, 8.48938s/100 iters), loss = 0.035689
I1107 15:13:35.673115 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:13:35.673115 15588 solver.cpp:237]     Train net output #1: loss = 0.0356892 (* 1 = 0.0356892 loss)
I1107 15:13:35.673115 15588 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1107 15:13:44.165786 15588 solver.cpp:218] Iteration 109300 (11.7743 iter/s, 8.4931s/100 iters), loss = 0.0340273
I1107 15:13:44.165786 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:13:44.165786 15588 solver.cpp:237]     Train net output #1: loss = 0.0340275 (* 1 = 0.0340275 loss)
I1107 15:13:44.166786 15588 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1107 15:13:52.660143 15588 solver.cpp:218] Iteration 109400 (11.7739 iter/s, 8.49334s/100 iters), loss = 0.0360369
I1107 15:13:52.660143 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:13:52.660143 15588 solver.cpp:237]     Train net output #1: loss = 0.0360371 (* 1 = 0.0360371 loss)
I1107 15:13:52.660143 15588 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1107 15:14:00.766968 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:14:01.102998 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_109500.caffemodel
I1107 15:14:01.140003 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_109500.solverstate
I1107 15:14:01.150002 15588 solver.cpp:330] Iteration 109500, Testing net (#0)
I1107 15:14:01.150002 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:14:03.136224  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:14:03.215229 15588 solver.cpp:397]     Test net output #0: accuracy = 0.917
I1107 15:14:03.215229 15588 solver.cpp:397]     Test net output #1: loss = 0.294201 (* 1 = 0.294201 loss)
I1107 15:14:03.296730 15588 solver.cpp:218] Iteration 109500 (9.40235 iter/s, 10.6356s/100 iters), loss = 0.0533199
I1107 15:14:03.296730 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:14:03.296730 15588 solver.cpp:237]     Train net output #1: loss = 0.0533201 (* 1 = 0.0533201 loss)
I1107 15:14:03.296730 15588 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1107 15:14:11.777117 15588 solver.cpp:218] Iteration 109600 (11.7921 iter/s, 8.48023s/100 iters), loss = 0.0536643
I1107 15:14:11.777117 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:14:11.777117 15588 solver.cpp:237]     Train net output #1: loss = 0.0536644 (* 1 = 0.0536644 loss)
I1107 15:14:11.777117 15588 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1107 15:14:20.261833 15588 solver.cpp:218] Iteration 109700 (11.7866 iter/s, 8.48425s/100 iters), loss = 0.0298571
I1107 15:14:20.261833 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:14:20.261833 15588 solver.cpp:237]     Train net output #1: loss = 0.0298573 (* 1 = 0.0298573 loss)
I1107 15:14:20.261833 15588 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1107 15:14:28.745355 15588 solver.cpp:218] Iteration 109800 (11.788 iter/s, 8.48323s/100 iters), loss = 0.0587152
I1107 15:14:28.745355 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:14:28.745355 15588 solver.cpp:237]     Train net output #1: loss = 0.0587154 (* 1 = 0.0587154 loss)
I1107 15:14:28.745355 15588 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1107 15:14:37.245769 15588 solver.cpp:218] Iteration 109900 (11.765 iter/s, 8.49975s/100 iters), loss = 0.0693002
I1107 15:14:37.245769 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:14:37.245769 15588 solver.cpp:237]     Train net output #1: loss = 0.0693004 (* 1 = 0.0693004 loss)
I1107 15:14:37.245769 15588 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1107 15:14:45.338135 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:14:45.674154 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_110000.caffemodel
I1107 15:14:45.709159 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_110000.solverstate
I1107 15:14:45.718159 15588 solver.cpp:330] Iteration 110000, Testing net (#0)
I1107 15:14:45.719158 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:14:47.707788  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:14:47.788293 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1107 15:14:47.788293 15588 solver.cpp:397]     Test net output #1: loss = 0.290727 (* 1 = 0.290727 loss)
I1107 15:14:47.869299 15588 solver.cpp:218] Iteration 110000 (9.41321 iter/s, 10.6234s/100 iters), loss = 0.0388963
I1107 15:14:47.870299 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:14:47.870299 15588 solver.cpp:237]     Train net output #1: loss = 0.0388965 (* 1 = 0.0388965 loss)
I1107 15:14:47.870299 15588 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1107 15:14:56.374210 15588 solver.cpp:218] Iteration 110100 (11.7591 iter/s, 8.50406s/100 iters), loss = 0.0293001
I1107 15:14:56.374210 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:14:56.374210 15588 solver.cpp:237]     Train net output #1: loss = 0.0293003 (* 1 = 0.0293003 loss)
I1107 15:14:56.374210 15588 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1107 15:15:04.865877 15588 solver.cpp:218] Iteration 110200 (11.7777 iter/s, 8.49059s/100 iters), loss = 0.0450091
I1107 15:15:04.865877 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:15:04.865877 15588 solver.cpp:237]     Train net output #1: loss = 0.0450093 (* 1 = 0.0450093 loss)
I1107 15:15:04.865877 15588 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1107 15:15:13.351588 15588 solver.cpp:218] Iteration 110300 (11.7842 iter/s, 8.48591s/100 iters), loss = 0.0673023
I1107 15:15:13.351588 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:15:13.352588 15588 solver.cpp:237]     Train net output #1: loss = 0.0673025 (* 1 = 0.0673025 loss)
I1107 15:15:13.352588 15588 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1107 15:15:21.862128 15588 solver.cpp:218] Iteration 110400 (11.7518 iter/s, 8.50934s/100 iters), loss = 0.0684857
I1107 15:15:21.862128 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 15:15:21.862128 15588 solver.cpp:237]     Train net output #1: loss = 0.0684859 (* 1 = 0.0684859 loss)
I1107 15:15:21.862128 15588 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1107 15:15:29.942869 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:15:30.277892 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_110500.caffemodel
I1107 15:15:30.308894 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_110500.solverstate
I1107 15:15:30.320894 15588 solver.cpp:330] Iteration 110500, Testing net (#0)
I1107 15:15:30.320894 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:15:32.306200  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:15:32.385206 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 15:15:32.385206 15588 solver.cpp:397]     Test net output #1: loss = 0.29106 (* 1 = 0.29106 loss)
I1107 15:15:32.466212 15588 solver.cpp:218] Iteration 110500 (9.43054 iter/s, 10.6038s/100 iters), loss = 0.0442027
I1107 15:15:32.466212 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:15:32.466212 15588 solver.cpp:237]     Train net output #1: loss = 0.0442029 (* 1 = 0.0442029 loss)
I1107 15:15:32.466212 15588 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1107 15:15:40.969848 15588 solver.cpp:218] Iteration 110600 (11.7608 iter/s, 8.50283s/100 iters), loss = 0.0520164
I1107 15:15:40.969848 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:15:40.969848 15588 solver.cpp:237]     Train net output #1: loss = 0.0520166 (* 1 = 0.0520166 loss)
I1107 15:15:40.969848 15588 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1107 15:15:49.469694 15588 solver.cpp:218] Iteration 110700 (11.7647 iter/s, 8.5s/100 iters), loss = 0.0268317
I1107 15:15:49.470695 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:15:49.470695 15588 solver.cpp:237]     Train net output #1: loss = 0.0268319 (* 1 = 0.0268319 loss)
I1107 15:15:49.470695 15588 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1107 15:15:57.975826 15588 solver.cpp:218] Iteration 110800 (11.7573 iter/s, 8.50538s/100 iters), loss = 0.0478782
I1107 15:15:57.975826 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:15:57.975826 15588 solver.cpp:237]     Train net output #1: loss = 0.0478783 (* 1 = 0.0478783 loss)
I1107 15:15:57.975826 15588 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1107 15:16:06.506887 15588 solver.cpp:218] Iteration 110900 (11.7235 iter/s, 8.52985s/100 iters), loss = 0.0305861
I1107 15:16:06.506887 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:16:06.506887 15588 solver.cpp:237]     Train net output #1: loss = 0.0305863 (* 1 = 0.0305863 loss)
I1107 15:16:06.506887 15588 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1107 15:16:14.584250 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:16:14.920797 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_111000.caffemodel
I1107 15:16:14.956796 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_111000.solverstate
I1107 15:16:14.965797 15588 solver.cpp:330] Iteration 111000, Testing net (#0)
I1107 15:16:14.966797 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:16:16.951997  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:16:17.030999 15588 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1107 15:16:17.030999 15588 solver.cpp:397]     Test net output #1: loss = 0.295843 (* 1 = 0.295843 loss)
I1107 15:16:17.112004 15588 solver.cpp:218] Iteration 111000 (9.42926 iter/s, 10.6053s/100 iters), loss = 0.0562458
I1107 15:16:17.112004 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:16:17.112004 15588 solver.cpp:237]     Train net output #1: loss = 0.056246 (* 1 = 0.056246 loss)
I1107 15:16:17.112004 15588 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1107 15:16:25.599747 15588 solver.cpp:218] Iteration 111100 (11.7821 iter/s, 8.48743s/100 iters), loss = 0.0498469
I1107 15:16:25.599747 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:16:25.599747 15588 solver.cpp:237]     Train net output #1: loss = 0.049847 (* 1 = 0.049847 loss)
I1107 15:16:25.599747 15588 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1107 15:16:34.126690 15588 solver.cpp:218] Iteration 111200 (11.7285 iter/s, 8.52627s/100 iters), loss = 0.0275115
I1107 15:16:34.126690 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:16:34.126690 15588 solver.cpp:237]     Train net output #1: loss = 0.0275117 (* 1 = 0.0275117 loss)
I1107 15:16:34.126690 15588 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1107 15:16:42.695175 15588 solver.cpp:218] Iteration 111300 (11.672 iter/s, 8.5675s/100 iters), loss = 0.0360615
I1107 15:16:42.695175 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:16:42.695175 15588 solver.cpp:237]     Train net output #1: loss = 0.0360616 (* 1 = 0.0360616 loss)
I1107 15:16:42.695175 15588 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1107 15:16:51.322165 15588 solver.cpp:218] Iteration 111400 (11.5922 iter/s, 8.62648s/100 iters), loss = 0.0376344
I1107 15:16:51.322165 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:16:51.322165 15588 solver.cpp:237]     Train net output #1: loss = 0.0376346 (* 1 = 0.0376346 loss)
I1107 15:16:51.322165 15588 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1107 15:16:59.529053 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:16:59.878090 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_111500.caffemodel
I1107 15:16:59.912102 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_111500.solverstate
I1107 15:16:59.921103 15588 solver.cpp:330] Iteration 111500, Testing net (#0)
I1107 15:16:59.921103 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:17:01.949178  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:17:02.030189 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1107 15:17:02.030189 15588 solver.cpp:397]     Test net output #1: loss = 0.290275 (* 1 = 0.290275 loss)
I1107 15:17:02.111192 15588 solver.cpp:218] Iteration 111500 (9.26945 iter/s, 10.7881s/100 iters), loss = 0.0449616
I1107 15:17:02.111192 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:17:02.111192 15588 solver.cpp:237]     Train net output #1: loss = 0.0449617 (* 1 = 0.0449617 loss)
I1107 15:17:02.111192 15588 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1107 15:17:10.745786 15588 solver.cpp:218] Iteration 111600 (11.5809 iter/s, 8.63491s/100 iters), loss = 0.0495717
I1107 15:17:10.745786 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:17:10.745786 15588 solver.cpp:237]     Train net output #1: loss = 0.0495719 (* 1 = 0.0495719 loss)
I1107 15:17:10.745786 15588 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1107 15:17:19.405285 15588 solver.cpp:218] Iteration 111700 (11.5493 iter/s, 8.65853s/100 iters), loss = 0.031745
I1107 15:17:19.405786 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:17:19.405786 15588 solver.cpp:237]     Train net output #1: loss = 0.0317451 (* 1 = 0.0317451 loss)
I1107 15:17:19.405786 15588 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1107 15:17:27.983103 15588 solver.cpp:218] Iteration 111800 (11.659 iter/s, 8.57703s/100 iters), loss = 0.0583209
I1107 15:17:27.983103 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:17:27.983103 15588 solver.cpp:237]     Train net output #1: loss = 0.058321 (* 1 = 0.058321 loss)
I1107 15:17:27.983103 15588 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1107 15:17:36.555155 15588 solver.cpp:218] Iteration 111900 (11.6661 iter/s, 8.57187s/100 iters), loss = 0.0341472
I1107 15:17:36.555155 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:17:36.555155 15588 solver.cpp:237]     Train net output #1: loss = 0.0341474 (* 1 = 0.0341474 loss)
I1107 15:17:36.555155 15588 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1107 15:17:44.703837 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:17:45.039372 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_112000.caffemodel
I1107 15:17:45.073374 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_112000.solverstate
I1107 15:17:45.082371 15588 solver.cpp:330] Iteration 112000, Testing net (#0)
I1107 15:17:45.082371 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:17:47.086510  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:17:47.165514 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 15:17:47.165514 15588 solver.cpp:397]     Test net output #1: loss = 0.291386 (* 1 = 0.291386 loss)
I1107 15:17:47.247534 15588 solver.cpp:218] Iteration 112000 (9.3529 iter/s, 10.6919s/100 iters), loss = 0.0294396
I1107 15:17:47.247534 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:17:47.247534 15588 solver.cpp:237]     Train net output #1: loss = 0.0294398 (* 1 = 0.0294398 loss)
I1107 15:17:47.247534 15588 sgd_solver.cpp:105] Iteration 112000, lr = 0.001
I1107 15:17:55.818523 15588 solver.cpp:218] Iteration 112100 (11.668 iter/s, 8.57046s/100 iters), loss = 0.0540695
I1107 15:17:55.818523 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:17:55.818523 15588 solver.cpp:237]     Train net output #1: loss = 0.0540697 (* 1 = 0.0540697 loss)
I1107 15:17:55.818523 15588 sgd_solver.cpp:105] Iteration 112100, lr = 0.001
I1107 15:18:04.353283 15588 solver.cpp:218] Iteration 112200 (11.7184 iter/s, 8.53356s/100 iters), loss = 0.0316996
I1107 15:18:04.353283 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:18:04.353283 15588 solver.cpp:237]     Train net output #1: loss = 0.0316998 (* 1 = 0.0316998 loss)
I1107 15:18:04.353283 15588 sgd_solver.cpp:105] Iteration 112200, lr = 0.001
I1107 15:18:12.877171 15588 solver.cpp:218] Iteration 112300 (11.7313 iter/s, 8.52423s/100 iters), loss = 0.0245487
I1107 15:18:12.877171 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:18:12.877171 15588 solver.cpp:237]     Train net output #1: loss = 0.0245488 (* 1 = 0.0245488 loss)
I1107 15:18:12.877171 15588 sgd_solver.cpp:105] Iteration 112300, lr = 0.001
I1107 15:18:21.400074 15588 solver.cpp:218] Iteration 112400 (11.735 iter/s, 8.52154s/100 iters), loss = 0.0304986
I1107 15:18:21.400074 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:18:21.400074 15588 solver.cpp:237]     Train net output #1: loss = 0.0304987 (* 1 = 0.0304987 loss)
I1107 15:18:21.400074 15588 sgd_solver.cpp:105] Iteration 112400, lr = 0.001
I1107 15:18:29.500877 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:18:29.836053 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_112500.caffemodel
I1107 15:18:29.872035 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_112500.solverstate
I1107 15:18:29.882040 15588 solver.cpp:330] Iteration 112500, Testing net (#0)
I1107 15:18:29.882040 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:18:31.880465  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:18:31.959487 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9189
I1107 15:18:31.959487 15588 solver.cpp:397]     Test net output #1: loss = 0.29649 (* 1 = 0.29649 loss)
I1107 15:18:32.042544 15588 solver.cpp:218] Iteration 112500 (9.39646 iter/s, 10.6423s/100 iters), loss = 0.0448432
I1107 15:18:32.042544 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:18:32.042544 15588 solver.cpp:237]     Train net output #1: loss = 0.0448433 (* 1 = 0.0448433 loss)
I1107 15:18:32.042544 15588 sgd_solver.cpp:105] Iteration 112500, lr = 0.001
I1107 15:18:40.564821 15588 solver.cpp:218] Iteration 112600 (11.7359 iter/s, 8.52084s/100 iters), loss = 0.0354757
I1107 15:18:40.564821 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:18:40.564821 15588 solver.cpp:237]     Train net output #1: loss = 0.0354758 (* 1 = 0.0354758 loss)
I1107 15:18:40.564821 15588 sgd_solver.cpp:105] Iteration 112600, lr = 0.001
I1107 15:18:49.107933 15588 solver.cpp:218] Iteration 112700 (11.7054 iter/s, 8.54306s/100 iters), loss = 0.0378779
I1107 15:18:49.108433 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:18:49.108433 15588 solver.cpp:237]     Train net output #1: loss = 0.0378781 (* 1 = 0.0378781 loss)
I1107 15:18:49.108433 15588 sgd_solver.cpp:105] Iteration 112700, lr = 0.001
I1107 15:18:57.653852 15588 solver.cpp:218] Iteration 112800 (11.7017 iter/s, 8.54578s/100 iters), loss = 0.0393399
I1107 15:18:57.653852 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:18:57.653852 15588 solver.cpp:237]     Train net output #1: loss = 0.0393401 (* 1 = 0.0393401 loss)
I1107 15:18:57.653852 15588 sgd_solver.cpp:105] Iteration 112800, lr = 0.001
I1107 15:19:06.227090 15588 solver.cpp:218] Iteration 112900 (11.6657 iter/s, 8.57217s/100 iters), loss = 0.0381698
I1107 15:19:06.227090 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:19:06.227090 15588 solver.cpp:237]     Train net output #1: loss = 0.03817 (* 1 = 0.03817 loss)
I1107 15:19:06.227090 15588 sgd_solver.cpp:105] Iteration 112900, lr = 0.001
I1107 15:19:14.446969 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:19:14.796994 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_113000.caffemodel
I1107 15:19:14.829826 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_113000.solverstate
I1107 15:19:14.838830 15588 solver.cpp:330] Iteration 113000, Testing net (#0)
I1107 15:19:14.838830 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:19:16.858914  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:19:16.940932 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9172
I1107 15:19:16.941929 15588 solver.cpp:397]     Test net output #1: loss = 0.298842 (* 1 = 0.298842 loss)
I1107 15:19:17.024935 15588 solver.cpp:218] Iteration 113000 (9.26189 iter/s, 10.7969s/100 iters), loss = 0.0655049
I1107 15:19:17.024935 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:19:17.024935 15588 solver.cpp:237]     Train net output #1: loss = 0.0655051 (* 1 = 0.0655051 loss)
I1107 15:19:17.024935 15588 sgd_solver.cpp:105] Iteration 113000, lr = 0.001
I1107 15:19:25.751765 15588 solver.cpp:218] Iteration 113100 (11.4593 iter/s, 8.72657s/100 iters), loss = 0.0394437
I1107 15:19:25.751765 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:19:25.751765 15588 solver.cpp:237]     Train net output #1: loss = 0.0394438 (* 1 = 0.0394438 loss)
I1107 15:19:25.751765 15588 sgd_solver.cpp:105] Iteration 113100, lr = 0.001
I1107 15:19:34.404443 15588 solver.cpp:218] Iteration 113200 (11.5577 iter/s, 8.65226s/100 iters), loss = 0.0494779
I1107 15:19:34.404443 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:19:34.404443 15588 solver.cpp:237]     Train net output #1: loss = 0.0494781 (* 1 = 0.0494781 loss)
I1107 15:19:34.404443 15588 sgd_solver.cpp:105] Iteration 113200, lr = 0.001
I1107 15:19:43.063784 15588 solver.cpp:218] Iteration 113300 (11.5493 iter/s, 8.65855s/100 iters), loss = 0.0316621
I1107 15:19:43.063784 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:19:43.063784 15588 solver.cpp:237]     Train net output #1: loss = 0.0316622 (* 1 = 0.0316622 loss)
I1107 15:19:43.063784 15588 sgd_solver.cpp:105] Iteration 113300, lr = 0.001
I1107 15:19:51.639770 15588 solver.cpp:218] Iteration 113400 (11.6608 iter/s, 8.57574s/100 iters), loss = 0.0472592
I1107 15:19:51.639770 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:19:51.639770 15588 solver.cpp:237]     Train net output #1: loss = 0.0472594 (* 1 = 0.0472594 loss)
I1107 15:19:51.639770 15588 sgd_solver.cpp:105] Iteration 113400, lr = 0.001
I1107 15:19:59.720132 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:20:00.057685 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_113500.caffemodel
I1107 15:20:00.093685 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_113500.solverstate
I1107 15:20:00.103685 15588 solver.cpp:330] Iteration 113500, Testing net (#0)
I1107 15:20:00.103685 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:20:02.126480  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:20:02.205982 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 15:20:02.205982 15588 solver.cpp:397]     Test net output #1: loss = 0.297258 (* 1 = 0.297258 loss)
I1107 15:20:02.286998 15588 solver.cpp:218] Iteration 113500 (9.39256 iter/s, 10.6467s/100 iters), loss = 0.0427797
I1107 15:20:02.286998 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:20:02.286998 15588 solver.cpp:237]     Train net output #1: loss = 0.0427798 (* 1 = 0.0427798 loss)
I1107 15:20:02.286998 15588 sgd_solver.cpp:105] Iteration 113500, lr = 0.001
I1107 15:20:10.795605 15588 solver.cpp:218] Iteration 113600 (11.7536 iter/s, 8.50804s/100 iters), loss = 0.0372119
I1107 15:20:10.795605 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:20:10.795605 15588 solver.cpp:237]     Train net output #1: loss = 0.0372121 (* 1 = 0.0372121 loss)
I1107 15:20:10.795605 15588 sgd_solver.cpp:105] Iteration 113600, lr = 0.001
I1107 15:20:19.295413 15588 solver.cpp:218] Iteration 113700 (11.7657 iter/s, 8.4993s/100 iters), loss = 0.0249919
I1107 15:20:19.295413 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:20:19.295413 15588 solver.cpp:237]     Train net output #1: loss = 0.0249921 (* 1 = 0.0249921 loss)
I1107 15:20:19.295413 15588 sgd_solver.cpp:105] Iteration 113700, lr = 0.001
I1107 15:20:27.793129 15588 solver.cpp:218] Iteration 113800 (11.7687 iter/s, 8.49711s/100 iters), loss = 0.0240546
I1107 15:20:27.793129 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:20:27.793129 15588 solver.cpp:237]     Train net output #1: loss = 0.0240548 (* 1 = 0.0240548 loss)
I1107 15:20:27.793129 15588 sgd_solver.cpp:105] Iteration 113800, lr = 0.001
I1107 15:20:36.295567 15588 solver.cpp:218] Iteration 113900 (11.7618 iter/s, 8.50211s/100 iters), loss = 0.034333
I1107 15:20:36.295567 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:20:36.295567 15588 solver.cpp:237]     Train net output #1: loss = 0.0343332 (* 1 = 0.0343332 loss)
I1107 15:20:36.295567 15588 sgd_solver.cpp:105] Iteration 113900, lr = 0.001
I1107 15:20:44.370482 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:20:44.704589 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_114000.caffemodel
I1107 15:20:44.736119 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_114000.solverstate
I1107 15:20:44.744619 15588 solver.cpp:330] Iteration 114000, Testing net (#0)
I1107 15:20:44.744619 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:20:46.744040  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:20:46.823572 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9168
I1107 15:20:46.823572 15588 solver.cpp:397]     Test net output #1: loss = 0.298572 (* 1 = 0.298572 loss)
I1107 15:20:46.904991 15588 solver.cpp:218] Iteration 114000 (9.42657 iter/s, 10.6083s/100 iters), loss = 0.0352376
I1107 15:20:46.904991 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:20:46.904991 15588 solver.cpp:237]     Train net output #1: loss = 0.0352377 (* 1 = 0.0352377 loss)
I1107 15:20:46.904991 15588 sgd_solver.cpp:105] Iteration 114000, lr = 0.001
I1107 15:20:55.433794 15588 solver.cpp:218] Iteration 114100 (11.7252 iter/s, 8.52861s/100 iters), loss = 0.0583981
I1107 15:20:55.434293 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:20:55.434293 15588 solver.cpp:237]     Train net output #1: loss = 0.0583983 (* 1 = 0.0583983 loss)
I1107 15:20:55.434293 15588 sgd_solver.cpp:105] Iteration 114100, lr = 0.001
I1107 15:21:03.928664 15588 solver.cpp:218] Iteration 114200 (11.7727 iter/s, 8.49421s/100 iters), loss = 0.0328726
I1107 15:21:03.928664 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:21:03.928664 15588 solver.cpp:237]     Train net output #1: loss = 0.0328728 (* 1 = 0.0328728 loss)
I1107 15:21:03.928664 15588 sgd_solver.cpp:105] Iteration 114200, lr = 0.001
I1107 15:21:12.457073 15588 solver.cpp:218] Iteration 114300 (11.7259 iter/s, 8.52814s/100 iters), loss = 0.0402874
I1107 15:21:12.457073 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:21:12.457073 15588 solver.cpp:237]     Train net output #1: loss = 0.0402876 (* 1 = 0.0402876 loss)
I1107 15:21:12.457073 15588 sgd_solver.cpp:105] Iteration 114300, lr = 0.001
I1107 15:21:20.985791 15588 solver.cpp:218] Iteration 114400 (11.7261 iter/s, 8.528s/100 iters), loss = 0.0258778
I1107 15:21:20.985791 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:21:20.985791 15588 solver.cpp:237]     Train net output #1: loss = 0.025878 (* 1 = 0.025878 loss)
I1107 15:21:20.985791 15588 sgd_solver.cpp:105] Iteration 114400, lr = 0.001
I1107 15:21:29.137893 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:21:29.474920 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_114500.caffemodel
I1107 15:21:29.511920 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_114500.solverstate
I1107 15:21:29.520920 15588 solver.cpp:330] Iteration 114500, Testing net (#0)
I1107 15:21:29.520920 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:21:31.543957  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:21:31.622459 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 15:21:31.623458 15588 solver.cpp:397]     Test net output #1: loss = 0.29646 (* 1 = 0.29646 loss)
I1107 15:21:31.703464 15588 solver.cpp:218] Iteration 114500 (9.33059 iter/s, 10.7174s/100 iters), loss = 0.0596901
I1107 15:21:31.703464 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:21:31.703464 15588 solver.cpp:237]     Train net output #1: loss = 0.0596903 (* 1 = 0.0596903 loss)
I1107 15:21:31.703464 15588 sgd_solver.cpp:105] Iteration 114500, lr = 0.001
I1107 15:21:40.211414 15588 solver.cpp:218] Iteration 114600 (11.7552 iter/s, 8.50686s/100 iters), loss = 0.068408
I1107 15:21:40.211414 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:21:40.211414 15588 solver.cpp:237]     Train net output #1: loss = 0.0684082 (* 1 = 0.0684082 loss)
I1107 15:21:40.211414 15588 sgd_solver.cpp:105] Iteration 114600, lr = 0.001
I1107 15:21:48.733726 15588 solver.cpp:218] Iteration 114700 (11.7349 iter/s, 8.52157s/100 iters), loss = 0.0306665
I1107 15:21:48.733726 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:21:48.733726 15588 solver.cpp:237]     Train net output #1: loss = 0.0306667 (* 1 = 0.0306667 loss)
I1107 15:21:48.733726 15588 sgd_solver.cpp:105] Iteration 114700, lr = 0.001
I1107 15:21:57.237090 15588 solver.cpp:218] Iteration 114800 (11.7607 iter/s, 8.50288s/100 iters), loss = 0.049875
I1107 15:21:57.237090 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:21:57.237090 15588 solver.cpp:237]     Train net output #1: loss = 0.0498752 (* 1 = 0.0498752 loss)
I1107 15:21:57.237090 15588 sgd_solver.cpp:105] Iteration 114800, lr = 0.001
I1107 15:22:05.728289 15588 solver.cpp:218] Iteration 114900 (11.7774 iter/s, 8.49081s/100 iters), loss = 0.0480558
I1107 15:22:05.728289 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:22:05.728289 15588 solver.cpp:237]     Train net output #1: loss = 0.048056 (* 1 = 0.048056 loss)
I1107 15:22:05.728289 15588 sgd_solver.cpp:105] Iteration 114900, lr = 0.001
I1107 15:22:13.912011 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:22:14.257046 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_115000.caffemodel
I1107 15:22:14.288053 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_115000.solverstate
I1107 15:22:14.297055 15588 solver.cpp:330] Iteration 115000, Testing net (#0)
I1107 15:22:14.297055 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:22:16.290366  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:22:16.370872 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9173
I1107 15:22:16.370872 15588 solver.cpp:397]     Test net output #1: loss = 0.298436 (* 1 = 0.298436 loss)
I1107 15:22:16.451375 15588 solver.cpp:218] Iteration 115000 (9.32594 iter/s, 10.7228s/100 iters), loss = 0.0580378
I1107 15:22:16.451375 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:22:16.451375 15588 solver.cpp:237]     Train net output #1: loss = 0.058038 (* 1 = 0.058038 loss)
I1107 15:22:16.451375 15588 sgd_solver.cpp:105] Iteration 115000, lr = 0.001
I1107 15:22:25.090658 15588 solver.cpp:218] Iteration 115100 (11.576 iter/s, 8.63854s/100 iters), loss = 0.056266
I1107 15:22:25.090658 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:22:25.090658 15588 solver.cpp:237]     Train net output #1: loss = 0.0562662 (* 1 = 0.0562662 loss)
I1107 15:22:25.090658 15588 sgd_solver.cpp:105] Iteration 115100, lr = 0.001
I1107 15:22:33.717854 15588 solver.cpp:218] Iteration 115200 (11.5915 iter/s, 8.62703s/100 iters), loss = 0.0283923
I1107 15:22:33.717854 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:22:33.717854 15588 solver.cpp:237]     Train net output #1: loss = 0.0283925 (* 1 = 0.0283925 loss)
I1107 15:22:33.717854 15588 sgd_solver.cpp:105] Iteration 115200, lr = 0.001
I1107 15:22:42.299288 15588 solver.cpp:218] Iteration 115300 (11.6543 iter/s, 8.58053s/100 iters), loss = 0.0331573
I1107 15:22:42.299288 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:22:42.299288 15588 solver.cpp:237]     Train net output #1: loss = 0.0331575 (* 1 = 0.0331575 loss)
I1107 15:22:42.299288 15588 sgd_solver.cpp:105] Iteration 115300, lr = 0.001
I1107 15:22:50.859769 15588 solver.cpp:218] Iteration 115400 (11.6816 iter/s, 8.56047s/100 iters), loss = 0.0512372
I1107 15:22:50.859769 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:22:50.859769 15588 solver.cpp:237]     Train net output #1: loss = 0.0512374 (* 1 = 0.0512374 loss)
I1107 15:22:50.859769 15588 sgd_solver.cpp:105] Iteration 115400, lr = 0.001
I1107 15:22:59.108161 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:22:59.442832 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_115500.caffemodel
I1107 15:22:59.477836 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_115500.solverstate
I1107 15:22:59.486836 15588 solver.cpp:330] Iteration 115500, Testing net (#0)
I1107 15:22:59.486836 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:23:01.475167  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:23:01.554174 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 15:23:01.555176 15588 solver.cpp:397]     Test net output #1: loss = 0.293918 (* 1 = 0.293918 loss)
I1107 15:23:01.635679 15588 solver.cpp:218] Iteration 115500 (9.28082 iter/s, 10.7749s/100 iters), loss = 0.0334624
I1107 15:23:01.635679 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:23:01.635679 15588 solver.cpp:237]     Train net output #1: loss = 0.0334626 (* 1 = 0.0334626 loss)
I1107 15:23:01.635679 15588 sgd_solver.cpp:105] Iteration 115500, lr = 0.001
I1107 15:23:10.293341 15588 solver.cpp:218] Iteration 115600 (11.5506 iter/s, 8.65756s/100 iters), loss = 0.0372765
I1107 15:23:10.293341 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:23:10.293341 15588 solver.cpp:237]     Train net output #1: loss = 0.0372767 (* 1 = 0.0372767 loss)
I1107 15:23:10.293341 15588 sgd_solver.cpp:105] Iteration 115600, lr = 0.001
I1107 15:23:18.932837 15588 solver.cpp:218] Iteration 115700 (11.5751 iter/s, 8.63921s/100 iters), loss = 0.0243673
I1107 15:23:18.933837 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:23:18.933837 15588 solver.cpp:237]     Train net output #1: loss = 0.0243675 (* 1 = 0.0243675 loss)
I1107 15:23:18.933837 15588 sgd_solver.cpp:105] Iteration 115700, lr = 0.001
I1107 15:23:27.445225 15588 solver.cpp:218] Iteration 115800 (11.749 iter/s, 8.51134s/100 iters), loss = 0.0338092
I1107 15:23:27.445225 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:23:27.445225 15588 solver.cpp:237]     Train net output #1: loss = 0.0338093 (* 1 = 0.0338093 loss)
I1107 15:23:27.445225 15588 sgd_solver.cpp:105] Iteration 115800, lr = 0.001
I1107 15:23:35.942461 15588 solver.cpp:218] Iteration 115900 (11.7692 iter/s, 8.49677s/100 iters), loss = 0.0607708
I1107 15:23:35.942461 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:23:35.942461 15588 solver.cpp:237]     Train net output #1: loss = 0.060771 (* 1 = 0.060771 loss)
I1107 15:23:35.942461 15588 sgd_solver.cpp:105] Iteration 115900, lr = 0.001
I1107 15:23:44.032234 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:23:44.369258 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_116000.caffemodel
I1107 15:23:44.400264 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_116000.solverstate
I1107 15:23:44.410264 15588 solver.cpp:330] Iteration 116000, Testing net (#0)
I1107 15:23:44.410264 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:23:46.398413  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:23:46.477421 15588 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 15:23:46.477421 15588 solver.cpp:397]     Test net output #1: loss = 0.29882 (* 1 = 0.29882 loss)
I1107 15:23:46.557426 15588 solver.cpp:218] Iteration 116000 (9.42113 iter/s, 10.6144s/100 iters), loss = 0.0581351
I1107 15:23:46.557426 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:23:46.557426 15588 solver.cpp:237]     Train net output #1: loss = 0.0581353 (* 1 = 0.0581353 loss)
I1107 15:23:46.557426 15588 sgd_solver.cpp:105] Iteration 116000, lr = 0.001
I1107 15:23:55.048218 15588 solver.cpp:218] Iteration 116100 (11.7782 iter/s, 8.49023s/100 iters), loss = 0.0371757
I1107 15:23:55.048218 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:23:55.048218 15588 solver.cpp:237]     Train net output #1: loss = 0.0371759 (* 1 = 0.0371759 loss)
I1107 15:23:55.048218 15588 sgd_solver.cpp:105] Iteration 116100, lr = 0.001
I1107 15:24:03.655463 15588 solver.cpp:218] Iteration 116200 (11.6193 iter/s, 8.60635s/100 iters), loss = 0.0274475
I1107 15:24:03.655463 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:24:03.655463 15588 solver.cpp:237]     Train net output #1: loss = 0.0274477 (* 1 = 0.0274477 loss)
I1107 15:24:03.655463 15588 sgd_solver.cpp:105] Iteration 116200, lr = 0.001
I1107 15:24:12.529726 15588 solver.cpp:218] Iteration 116300 (11.2689 iter/s, 8.87398s/100 iters), loss = 0.0294191
I1107 15:24:12.529726 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:24:12.529726 15588 solver.cpp:237]     Train net output #1: loss = 0.0294193 (* 1 = 0.0294193 loss)
I1107 15:24:12.529726 15588 sgd_solver.cpp:105] Iteration 116300, lr = 0.001
I1107 15:24:21.549389 15588 solver.cpp:218] Iteration 116400 (11.0882 iter/s, 9.01862s/100 iters), loss = 0.0306631
I1107 15:24:21.549389 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:24:21.549389 15588 solver.cpp:237]     Train net output #1: loss = 0.0306634 (* 1 = 0.0306634 loss)
I1107 15:24:21.549389 15588 sgd_solver.cpp:105] Iteration 116400, lr = 0.001
I1107 15:24:29.917537 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:24:30.262048 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_116500.caffemodel
I1107 15:24:30.294047 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_116500.solverstate
I1107 15:24:30.303066 15588 solver.cpp:330] Iteration 116500, Testing net (#0)
I1107 15:24:30.304047 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:24:32.341717  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:24:32.423223 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 15:24:32.423223 15588 solver.cpp:397]     Test net output #1: loss = 0.303649 (* 1 = 0.303649 loss)
I1107 15:24:32.504227 15588 solver.cpp:218] Iteration 116500 (9.12866 iter/s, 10.9545s/100 iters), loss = 0.0409783
I1107 15:24:32.504227 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:24:32.504227 15588 solver.cpp:237]     Train net output #1: loss = 0.0409785 (* 1 = 0.0409785 loss)
I1107 15:24:32.504227 15588 sgd_solver.cpp:105] Iteration 116500, lr = 0.001
I1107 15:24:41.354568 15588 solver.cpp:218] Iteration 116600 (11.3 iter/s, 8.84954s/100 iters), loss = 0.0458378
I1107 15:24:41.354568 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:24:41.355069 15588 solver.cpp:237]     Train net output #1: loss = 0.045838 (* 1 = 0.045838 loss)
I1107 15:24:41.355069 15588 sgd_solver.cpp:105] Iteration 116600, lr = 0.001
I1107 15:24:50.093209 15588 solver.cpp:218] Iteration 116700 (11.4445 iter/s, 8.73779s/100 iters), loss = 0.0232615
I1107 15:24:50.093209 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:24:50.093209 15588 solver.cpp:237]     Train net output #1: loss = 0.0232617 (* 1 = 0.0232617 loss)
I1107 15:24:50.093209 15588 sgd_solver.cpp:105] Iteration 116700, lr = 0.001
I1107 15:24:58.829174 15588 solver.cpp:218] Iteration 116800 (11.4477 iter/s, 8.73535s/100 iters), loss = 0.037286
I1107 15:24:58.829174 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:24:58.829174 15588 solver.cpp:237]     Train net output #1: loss = 0.0372862 (* 1 = 0.0372862 loss)
I1107 15:24:58.829174 15588 sgd_solver.cpp:105] Iteration 116800, lr = 0.001
I1107 15:25:07.561322 15588 solver.cpp:218] Iteration 116900 (11.4526 iter/s, 8.73165s/100 iters), loss = 0.0269051
I1107 15:25:07.561322 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:25:07.561322 15588 solver.cpp:237]     Train net output #1: loss = 0.0269053 (* 1 = 0.0269053 loss)
I1107 15:25:07.561322 15588 sgd_solver.cpp:105] Iteration 116900, lr = 0.001
I1107 15:25:15.863262 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:25:16.209261 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_117000.caffemodel
I1107 15:25:16.246261 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_117000.solverstate
I1107 15:25:16.255767 15588 solver.cpp:330] Iteration 117000, Testing net (#0)
I1107 15:25:16.255767 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:25:18.282953  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:25:18.363956 15588 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 15:25:18.363956 15588 solver.cpp:397]     Test net output #1: loss = 0.293733 (* 1 = 0.293733 loss)
I1107 15:25:18.444963 15588 solver.cpp:218] Iteration 117000 (9.18846 iter/s, 10.8832s/100 iters), loss = 0.0612581
I1107 15:25:18.444963 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:25:18.444963 15588 solver.cpp:237]     Train net output #1: loss = 0.0612582 (* 1 = 0.0612582 loss)
I1107 15:25:18.444963 15588 sgd_solver.cpp:105] Iteration 117000, lr = 0.001
I1107 15:25:27.056385 15588 solver.cpp:218] Iteration 117100 (11.6136 iter/s, 8.6106s/100 iters), loss = 0.0550408
I1107 15:25:27.056385 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:25:27.056385 15588 solver.cpp:237]     Train net output #1: loss = 0.0550409 (* 1 = 0.0550409 loss)
I1107 15:25:27.056385 15588 sgd_solver.cpp:105] Iteration 117100, lr = 0.001
I1107 15:25:35.612257 15588 solver.cpp:218] Iteration 117200 (11.688 iter/s, 8.55579s/100 iters), loss = 0.0414229
I1107 15:25:35.612257 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:25:35.612257 15588 solver.cpp:237]     Train net output #1: loss = 0.0414231 (* 1 = 0.0414231 loss)
I1107 15:25:35.612257 15588 sgd_solver.cpp:105] Iteration 117200, lr = 0.001
I1107 15:25:44.114348 15588 solver.cpp:218] Iteration 117300 (11.7631 iter/s, 8.50117s/100 iters), loss = 0.0327946
I1107 15:25:44.114348 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:25:44.114348 15588 solver.cpp:237]     Train net output #1: loss = 0.0327947 (* 1 = 0.0327947 loss)
I1107 15:25:44.114348 15588 sgd_solver.cpp:105] Iteration 117300, lr = 0.001
I1107 15:25:52.803334 15588 solver.cpp:218] Iteration 117400 (11.5095 iter/s, 8.68851s/100 iters), loss = 0.0265831
I1107 15:25:52.803334 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:25:52.803334 15588 solver.cpp:237]     Train net output #1: loss = 0.0265833 (* 1 = 0.0265833 loss)
I1107 15:25:52.803334 15588 sgd_solver.cpp:105] Iteration 117400, lr = 0.001
I1107 15:26:01.107475 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:26:01.442488 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_117500.caffemodel
I1107 15:26:01.479497 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_117500.solverstate
I1107 15:26:01.489498 15588 solver.cpp:330] Iteration 117500, Testing net (#0)
I1107 15:26:01.489498 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:26:03.536829  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:26:03.622831 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9167
I1107 15:26:03.622831 15588 solver.cpp:397]     Test net output #1: loss = 0.305232 (* 1 = 0.305232 loss)
I1107 15:26:03.710835 15588 solver.cpp:218] Iteration 117500 (9.16875 iter/s, 10.9066s/100 iters), loss = 0.0511669
I1107 15:26:03.710835 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:26:03.710835 15588 solver.cpp:237]     Train net output #1: loss = 0.0511671 (* 1 = 0.0511671 loss)
I1107 15:26:03.710835 15588 sgd_solver.cpp:105] Iteration 117500, lr = 0.001
I1107 15:26:12.319665 15588 solver.cpp:218] Iteration 117600 (11.6157 iter/s, 8.60902s/100 iters), loss = 0.0355141
I1107 15:26:12.319665 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:26:12.319665 15588 solver.cpp:237]     Train net output #1: loss = 0.0355143 (* 1 = 0.0355143 loss)
I1107 15:26:12.319665 15588 sgd_solver.cpp:105] Iteration 117600, lr = 0.001
I1107 15:26:20.957202 15588 solver.cpp:218] Iteration 117700 (11.5794 iter/s, 8.636s/100 iters), loss = 0.0258186
I1107 15:26:20.957202 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:26:20.957202 15588 solver.cpp:237]     Train net output #1: loss = 0.0258188 (* 1 = 0.0258188 loss)
I1107 15:26:20.957202 15588 sgd_solver.cpp:105] Iteration 117700, lr = 0.001
I1107 15:26:29.511842 15588 solver.cpp:218] Iteration 117800 (11.6889 iter/s, 8.55513s/100 iters), loss = 0.0300608
I1107 15:26:29.512843 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:26:29.512843 15588 solver.cpp:237]     Train net output #1: loss = 0.030061 (* 1 = 0.030061 loss)
I1107 15:26:29.512843 15588 sgd_solver.cpp:105] Iteration 117800, lr = 0.001
I1107 15:26:38.107363 15588 solver.cpp:218] Iteration 117900 (11.6357 iter/s, 8.59422s/100 iters), loss = 0.0360323
I1107 15:26:38.107363 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:26:38.107363 15588 solver.cpp:237]     Train net output #1: loss = 0.0360325 (* 1 = 0.0360325 loss)
I1107 15:26:38.107363 15588 sgd_solver.cpp:105] Iteration 117900, lr = 0.001
I1107 15:26:46.301133 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:26:46.640153 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_118000.caffemodel
I1107 15:26:46.672158 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_118000.solverstate
I1107 15:26:46.680161 15588 solver.cpp:330] Iteration 118000, Testing net (#0)
I1107 15:26:46.681162 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:26:48.679514  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:26:48.759516 15588 solver.cpp:397]     Test net output #0: accuracy = 0.916
I1107 15:26:48.759516 15588 solver.cpp:397]     Test net output #1: loss = 0.302311 (* 1 = 0.302311 loss)
I1107 15:26:48.841517 15588 solver.cpp:218] Iteration 118000 (9.31657 iter/s, 10.7336s/100 iters), loss = 0.0328812
I1107 15:26:48.841517 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:26:48.841517 15588 solver.cpp:237]     Train net output #1: loss = 0.0328813 (* 1 = 0.0328813 loss)
I1107 15:26:48.841517 15588 sgd_solver.cpp:105] Iteration 118000, lr = 0.001
I1107 15:26:57.410326 15588 solver.cpp:218] Iteration 118100 (11.6713 iter/s, 8.568s/100 iters), loss = 0.0281225
I1107 15:26:57.410326 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:26:57.410326 15588 solver.cpp:237]     Train net output #1: loss = 0.0281227 (* 1 = 0.0281227 loss)
I1107 15:26:57.410326 15588 sgd_solver.cpp:105] Iteration 118100, lr = 0.001
I1107 15:27:05.957904 15588 solver.cpp:218] Iteration 118200 (11.6998 iter/s, 8.54719s/100 iters), loss = 0.0346631
I1107 15:27:05.957904 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:27:05.957904 15588 solver.cpp:237]     Train net output #1: loss = 0.0346633 (* 1 = 0.0346633 loss)
I1107 15:27:05.957904 15588 sgd_solver.cpp:105] Iteration 118200, lr = 0.001
I1107 15:27:14.509670 15588 solver.cpp:218] Iteration 118300 (11.6935 iter/s, 8.55173s/100 iters), loss = 0.028642
I1107 15:27:14.509670 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:27:14.509670 15588 solver.cpp:237]     Train net output #1: loss = 0.0286422 (* 1 = 0.0286422 loss)
I1107 15:27:14.509670 15588 sgd_solver.cpp:105] Iteration 118300, lr = 0.001
I1107 15:27:23.157706 15588 solver.cpp:218] Iteration 118400 (11.5649 iter/s, 8.64687s/100 iters), loss = 0.0273927
I1107 15:27:23.157706 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:27:23.157706 15588 solver.cpp:237]     Train net output #1: loss = 0.0273929 (* 1 = 0.0273929 loss)
I1107 15:27:23.157706 15588 sgd_solver.cpp:105] Iteration 118400, lr = 0.001
I1107 15:27:31.390555 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:27:31.732578 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_118500.caffemodel
I1107 15:27:31.763082 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_118500.solverstate
I1107 15:27:31.773083 15588 solver.cpp:330] Iteration 118500, Testing net (#0)
I1107 15:27:31.773083 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:27:33.780735  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:27:33.861735 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 15:27:33.861735 15588 solver.cpp:397]     Test net output #1: loss = 0.301909 (* 1 = 0.301909 loss)
I1107 15:27:33.943742 15588 solver.cpp:218] Iteration 118500 (9.27194 iter/s, 10.7852s/100 iters), loss = 0.0396274
I1107 15:27:33.943742 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:27:33.943742 15588 solver.cpp:237]     Train net output #1: loss = 0.0396276 (* 1 = 0.0396276 loss)
I1107 15:27:33.943742 15588 sgd_solver.cpp:105] Iteration 118500, lr = 0.001
I1107 15:27:42.544148 15588 solver.cpp:218] Iteration 118600 (11.6275 iter/s, 8.60028s/100 iters), loss = 0.0593908
I1107 15:27:42.544148 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:27:42.544148 15588 solver.cpp:237]     Train net output #1: loss = 0.059391 (* 1 = 0.059391 loss)
I1107 15:27:42.544148 15588 sgd_solver.cpp:105] Iteration 118600, lr = 0.001
I1107 15:27:51.165351 15588 solver.cpp:218] Iteration 118700 (11.6001 iter/s, 8.62063s/100 iters), loss = 0.0315105
I1107 15:27:51.165351 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:27:51.165351 15588 solver.cpp:237]     Train net output #1: loss = 0.0315107 (* 1 = 0.0315107 loss)
I1107 15:27:51.165351 15588 sgd_solver.cpp:105] Iteration 118700, lr = 0.001
I1107 15:27:59.776026 15588 solver.cpp:218] Iteration 118800 (11.6151 iter/s, 8.60949s/100 iters), loss = 0.0229843
I1107 15:27:59.776026 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:27:59.776026 15588 solver.cpp:237]     Train net output #1: loss = 0.0229845 (* 1 = 0.0229845 loss)
I1107 15:27:59.776026 15588 sgd_solver.cpp:105] Iteration 118800, lr = 0.001
I1107 15:28:08.361734 15588 solver.cpp:218] Iteration 118900 (11.6477 iter/s, 8.58542s/100 iters), loss = 0.026086
I1107 15:28:08.361734 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:28:08.361734 15588 solver.cpp:237]     Train net output #1: loss = 0.0260862 (* 1 = 0.0260862 loss)
I1107 15:28:08.361734 15588 sgd_solver.cpp:105] Iteration 118900, lr = 0.001
I1107 15:28:16.634552 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:28:16.979082 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_119000.caffemodel
I1107 15:28:17.015589 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_119000.solverstate
I1107 15:28:17.025590 15588 solver.cpp:330] Iteration 119000, Testing net (#0)
I1107 15:28:17.025590 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:28:19.056733  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:28:19.139739 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9185
I1107 15:28:19.139739 15588 solver.cpp:397]     Test net output #1: loss = 0.300377 (* 1 = 0.300377 loss)
I1107 15:28:19.221743 15588 solver.cpp:218] Iteration 119000 (9.20857 iter/s, 10.8595s/100 iters), loss = 0.0526033
I1107 15:28:19.221743 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:28:19.221743 15588 solver.cpp:237]     Train net output #1: loss = 0.0526035 (* 1 = 0.0526035 loss)
I1107 15:28:19.221743 15588 sgd_solver.cpp:105] Iteration 119000, lr = 0.001
I1107 15:28:28.053634 15588 solver.cpp:218] Iteration 119100 (11.3234 iter/s, 8.83127s/100 iters), loss = 0.0439355
I1107 15:28:28.053634 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:28:28.053634 15588 solver.cpp:237]     Train net output #1: loss = 0.0439357 (* 1 = 0.0439357 loss)
I1107 15:28:28.053634 15588 sgd_solver.cpp:105] Iteration 119100, lr = 0.001
I1107 15:28:36.827390 15588 solver.cpp:218] Iteration 119200 (11.3986 iter/s, 8.77299s/100 iters), loss = 0.0390019
I1107 15:28:36.827390 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:28:36.827390 15588 solver.cpp:237]     Train net output #1: loss = 0.0390021 (* 1 = 0.0390021 loss)
I1107 15:28:36.827390 15588 sgd_solver.cpp:105] Iteration 119200, lr = 0.001
I1107 15:28:45.407248 15588 solver.cpp:218] Iteration 119300 (11.6558 iter/s, 8.57942s/100 iters), loss = 0.0265258
I1107 15:28:45.407248 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:28:45.407248 15588 solver.cpp:237]     Train net output #1: loss = 0.026526 (* 1 = 0.026526 loss)
I1107 15:28:45.407248 15588 sgd_solver.cpp:105] Iteration 119300, lr = 0.001
I1107 15:28:53.974009 15588 solver.cpp:218] Iteration 119400 (11.6742 iter/s, 8.56593s/100 iters), loss = 0.0523957
I1107 15:28:53.974009 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:28:53.974009 15588 solver.cpp:237]     Train net output #1: loss = 0.0523959 (* 1 = 0.0523959 loss)
I1107 15:28:53.974009 15588 sgd_solver.cpp:105] Iteration 119400, lr = 0.001
I1107 15:29:02.091167 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:29:02.428191 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_119500.caffemodel
I1107 15:29:02.466696 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_119500.solverstate
I1107 15:29:02.476197 15588 solver.cpp:330] Iteration 119500, Testing net (#0)
I1107 15:29:02.476697 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:29:04.473886  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:29:04.553385 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9175
I1107 15:29:04.553385 15588 solver.cpp:397]     Test net output #1: loss = 0.305776 (* 1 = 0.305776 loss)
I1107 15:29:04.634390 15588 solver.cpp:218] Iteration 119500 (9.3809 iter/s, 10.66s/100 iters), loss = 0.0480142
I1107 15:29:04.634390 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:29:04.634390 15588 solver.cpp:237]     Train net output #1: loss = 0.0480144 (* 1 = 0.0480144 loss)
I1107 15:29:04.634390 15588 sgd_solver.cpp:105] Iteration 119500, lr = 0.001
I1107 15:29:13.161077 15588 solver.cpp:218] Iteration 119600 (11.7291 iter/s, 8.52577s/100 iters), loss = 0.0505915
I1107 15:29:13.161077 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:29:13.161077 15588 solver.cpp:237]     Train net output #1: loss = 0.0505918 (* 1 = 0.0505918 loss)
I1107 15:29:13.161077 15588 sgd_solver.cpp:105] Iteration 119600, lr = 0.001
I1107 15:29:21.810389 15588 solver.cpp:218] Iteration 119700 (11.5622 iter/s, 8.64885s/100 iters), loss = 0.0455216
I1107 15:29:21.810389 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:29:21.810389 15588 solver.cpp:237]     Train net output #1: loss = 0.0455218 (* 1 = 0.0455218 loss)
I1107 15:29:21.810389 15588 sgd_solver.cpp:105] Iteration 119700, lr = 0.001
I1107 15:29:30.400660 15588 solver.cpp:218] Iteration 119800 (11.6424 iter/s, 8.58926s/100 iters), loss = 0.020743
I1107 15:29:30.400660 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:29:30.400660 15588 solver.cpp:237]     Train net output #1: loss = 0.0207432 (* 1 = 0.0207432 loss)
I1107 15:29:30.400660 15588 sgd_solver.cpp:105] Iteration 119800, lr = 0.001
I1107 15:29:38.943941 15588 solver.cpp:218] Iteration 119900 (11.7057 iter/s, 8.54286s/100 iters), loss = 0.0291089
I1107 15:29:38.943941 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:29:38.943941 15588 solver.cpp:237]     Train net output #1: loss = 0.0291091 (* 1 = 0.0291091 loss)
I1107 15:29:38.943941 15588 sgd_solver.cpp:105] Iteration 119900, lr = 0.001
I1107 15:29:47.081264 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:29:47.417763 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_120000.caffemodel
I1107 15:29:47.456264 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_120000.solverstate
I1107 15:29:47.465764 15588 solver.cpp:330] Iteration 120000, Testing net (#0)
I1107 15:29:47.466264 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:29:49.466264  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:29:49.545763 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1107 15:29:49.545763 15588 solver.cpp:397]     Test net output #1: loss = 0.304612 (* 1 = 0.304612 loss)
I1107 15:29:49.627264 15588 solver.cpp:218] Iteration 120000 (9.3608 iter/s, 10.6828s/100 iters), loss = 0.0423045
I1107 15:29:49.627264 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:29:49.627264 15588 solver.cpp:237]     Train net output #1: loss = 0.0423047 (* 1 = 0.0423047 loss)
I1107 15:29:49.627264 15588 sgd_solver.cpp:105] Iteration 120000, lr = 0.001
I1107 15:29:58.172530 15588 solver.cpp:218] Iteration 120100 (11.7033 iter/s, 8.54457s/100 iters), loss = 0.0422948
I1107 15:29:58.172530 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:29:58.172530 15588 solver.cpp:237]     Train net output #1: loss = 0.042295 (* 1 = 0.042295 loss)
I1107 15:29:58.172530 15588 sgd_solver.cpp:105] Iteration 120100, lr = 0.001
I1107 15:30:06.747809 15588 solver.cpp:218] Iteration 120200 (11.6622 iter/s, 8.57473s/100 iters), loss = 0.0434613
I1107 15:30:06.747809 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:30:06.747809 15588 solver.cpp:237]     Train net output #1: loss = 0.0434616 (* 1 = 0.0434616 loss)
I1107 15:30:06.747809 15588 sgd_solver.cpp:105] Iteration 120200, lr = 0.001
I1107 15:30:15.296566 15588 solver.cpp:218] Iteration 120300 (11.6977 iter/s, 8.5487s/100 iters), loss = 0.022661
I1107 15:30:15.296566 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:30:15.296566 15588 solver.cpp:237]     Train net output #1: loss = 0.0226611 (* 1 = 0.0226611 loss)
I1107 15:30:15.296566 15588 sgd_solver.cpp:105] Iteration 120300, lr = 0.001
I1107 15:30:23.840288 15588 solver.cpp:218] Iteration 120400 (11.7055 iter/s, 8.54301s/100 iters), loss = 0.0370414
I1107 15:30:23.840288 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:30:23.840288 15588 solver.cpp:237]     Train net output #1: loss = 0.0370416 (* 1 = 0.0370416 loss)
I1107 15:30:23.840288 15588 sgd_solver.cpp:105] Iteration 120400, lr = 0.001
I1107 15:30:31.955518 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:30:32.293536 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_120500.caffemodel
I1107 15:30:32.329046 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_120500.solverstate
I1107 15:30:32.342550 15588 solver.cpp:330] Iteration 120500, Testing net (#0)
I1107 15:30:32.342550 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:30:34.340735  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:30:34.420737 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9159
I1107 15:30:34.420737 15588 solver.cpp:397]     Test net output #1: loss = 0.301655 (* 1 = 0.301655 loss)
I1107 15:30:34.501739 15588 solver.cpp:218] Iteration 120500 (9.38009 iter/s, 10.6609s/100 iters), loss = 0.0345607
I1107 15:30:34.501739 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:30:34.501739 15588 solver.cpp:237]     Train net output #1: loss = 0.0345609 (* 1 = 0.0345609 loss)
I1107 15:30:34.501739 15588 sgd_solver.cpp:105] Iteration 120500, lr = 0.001
I1107 15:30:43.024091 15588 solver.cpp:218] Iteration 120600 (11.7347 iter/s, 8.52176s/100 iters), loss = 0.0288292
I1107 15:30:43.024091 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:30:43.024091 15588 solver.cpp:237]     Train net output #1: loss = 0.0288294 (* 1 = 0.0288294 loss)
I1107 15:30:43.024091 15588 sgd_solver.cpp:105] Iteration 120600, lr = 0.001
I1107 15:30:51.550550 15588 solver.cpp:218] Iteration 120700 (11.7293 iter/s, 8.52563s/100 iters), loss = 0.031532
I1107 15:30:51.550550 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:30:51.550550 15588 solver.cpp:237]     Train net output #1: loss = 0.0315322 (* 1 = 0.0315322 loss)
I1107 15:30:51.550550 15588 sgd_solver.cpp:105] Iteration 120700, lr = 0.001
I1107 15:31:00.085952 15588 solver.cpp:218] Iteration 120800 (11.716 iter/s, 8.53533s/100 iters), loss = 0.0353484
I1107 15:31:00.085952 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:31:00.085952 15588 solver.cpp:237]     Train net output #1: loss = 0.0353486 (* 1 = 0.0353486 loss)
I1107 15:31:00.085952 15588 sgd_solver.cpp:105] Iteration 120800, lr = 0.001
I1107 15:31:08.672565 15588 solver.cpp:218] Iteration 120900 (11.6465 iter/s, 8.58628s/100 iters), loss = 0.0278632
I1107 15:31:08.672565 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:31:08.672565 15588 solver.cpp:237]     Train net output #1: loss = 0.0278634 (* 1 = 0.0278634 loss)
I1107 15:31:08.672565 15588 sgd_solver.cpp:105] Iteration 120900, lr = 0.001
I1107 15:31:16.808419 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:31:17.146489 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_121000.caffemodel
I1107 15:31:17.177057 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_121000.solverstate
I1107 15:31:17.186058 15588 solver.cpp:330] Iteration 121000, Testing net (#0)
I1107 15:31:17.186058 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:31:19.184725  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:31:19.264726 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1107 15:31:19.264726 15588 solver.cpp:397]     Test net output #1: loss = 0.301123 (* 1 = 0.301123 loss)
I1107 15:31:19.345729 15588 solver.cpp:218] Iteration 121000 (9.37017 iter/s, 10.6722s/100 iters), loss = 0.0390687
I1107 15:31:19.345729 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:31:19.345729 15588 solver.cpp:237]     Train net output #1: loss = 0.0390689 (* 1 = 0.0390689 loss)
I1107 15:31:19.345729 15588 sgd_solver.cpp:105] Iteration 121000, lr = 0.001
I1107 15:31:27.867956 15588 solver.cpp:218] Iteration 121100 (11.735 iter/s, 8.5215s/100 iters), loss = 0.0526096
I1107 15:31:27.867956 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:31:27.867956 15588 solver.cpp:237]     Train net output #1: loss = 0.0526097 (* 1 = 0.0526097 loss)
I1107 15:31:27.867956 15588 sgd_solver.cpp:105] Iteration 121100, lr = 0.001
I1107 15:31:36.387893 15588 solver.cpp:218] Iteration 121200 (11.7374 iter/s, 8.51981s/100 iters), loss = 0.0350126
I1107 15:31:36.387893 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:31:36.387893 15588 solver.cpp:237]     Train net output #1: loss = 0.0350128 (* 1 = 0.0350128 loss)
I1107 15:31:36.387893 15588 sgd_solver.cpp:105] Iteration 121200, lr = 0.001
I1107 15:31:44.934566 15588 solver.cpp:218] Iteration 121300 (11.7009 iter/s, 8.54639s/100 iters), loss = 0.0216587
I1107 15:31:44.934566 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:31:44.934566 15588 solver.cpp:237]     Train net output #1: loss = 0.0216589 (* 1 = 0.0216589 loss)
I1107 15:31:44.934566 15588 sgd_solver.cpp:105] Iteration 121300, lr = 0.001
I1107 15:31:53.552608 15588 solver.cpp:218] Iteration 121400 (11.6052 iter/s, 8.61681s/100 iters), loss = 0.030334
I1107 15:31:53.552608 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:31:53.552608 15588 solver.cpp:237]     Train net output #1: loss = 0.0303342 (* 1 = 0.0303342 loss)
I1107 15:31:53.552608 15588 sgd_solver.cpp:105] Iteration 121400, lr = 0.001
I1107 15:32:01.752341 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:32:02.088362 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_121500.caffemodel
I1107 15:32:02.118362 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_121500.solverstate
I1107 15:32:02.127364 15588 solver.cpp:330] Iteration 121500, Testing net (#0)
I1107 15:32:02.127364 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:32:04.152709  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:32:04.232713 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 15:32:04.232713 15588 solver.cpp:397]     Test net output #1: loss = 0.296899 (* 1 = 0.296899 loss)
I1107 15:32:04.313725 15588 solver.cpp:218] Iteration 121500 (9.29308 iter/s, 10.7607s/100 iters), loss = 0.0431163
I1107 15:32:04.313725 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:32:04.313725 15588 solver.cpp:237]     Train net output #1: loss = 0.0431165 (* 1 = 0.0431165 loss)
I1107 15:32:04.313725 15588 sgd_solver.cpp:105] Iteration 121500, lr = 0.001
I1107 15:32:13.022792 15588 solver.cpp:218] Iteration 121600 (11.4829 iter/s, 8.70862s/100 iters), loss = 0.0562056
I1107 15:32:13.022792 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:32:13.022792 15588 solver.cpp:237]     Train net output #1: loss = 0.0562057 (* 1 = 0.0562057 loss)
I1107 15:32:13.022792 15588 sgd_solver.cpp:105] Iteration 121600, lr = 0.001
I1107 15:32:21.651264 15588 solver.cpp:218] Iteration 121700 (11.5907 iter/s, 8.62759s/100 iters), loss = 0.0436342
I1107 15:32:21.651264 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:32:21.651264 15588 solver.cpp:237]     Train net output #1: loss = 0.0436343 (* 1 = 0.0436343 loss)
I1107 15:32:21.651264 15588 sgd_solver.cpp:105] Iteration 121700, lr = 0.001
I1107 15:32:30.422619 15588 solver.cpp:218] Iteration 121800 (11.4015 iter/s, 8.77078s/100 iters), loss = 0.0245668
I1107 15:32:30.422619 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:32:30.422619 15588 solver.cpp:237]     Train net output #1: loss = 0.024567 (* 1 = 0.024567 loss)
I1107 15:32:30.422619 15588 sgd_solver.cpp:105] Iteration 121800, lr = 0.001
I1107 15:32:39.219367 15588 solver.cpp:218] Iteration 121900 (11.3684 iter/s, 8.79628s/100 iters), loss = 0.0279105
I1107 15:32:39.219367 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:32:39.219367 15588 solver.cpp:237]     Train net output #1: loss = 0.0279106 (* 1 = 0.0279106 loss)
I1107 15:32:39.219367 15588 sgd_solver.cpp:105] Iteration 121900, lr = 0.001
I1107 15:32:47.485143 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:32:47.830654 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_122000.caffemodel
I1107 15:32:47.869141 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_122000.solverstate
I1107 15:32:47.878142 15588 solver.cpp:330] Iteration 122000, Testing net (#0)
I1107 15:32:47.878641 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:32:49.915640  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:32:49.997141 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 15:32:49.997141 15588 solver.cpp:397]     Test net output #1: loss = 0.300796 (* 1 = 0.300796 loss)
I1107 15:32:50.079640 15588 solver.cpp:218] Iteration 122000 (9.20838 iter/s, 10.8597s/100 iters), loss = 0.0549706
I1107 15:32:50.079640 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:32:50.079640 15588 solver.cpp:237]     Train net output #1: loss = 0.0549708 (* 1 = 0.0549708 loss)
I1107 15:32:50.079640 15588 sgd_solver.cpp:105] Iteration 122000, lr = 0.001
I1107 15:32:58.921886 15588 solver.cpp:218] Iteration 122100 (11.3103 iter/s, 8.84148s/100 iters), loss = 0.0406174
I1107 15:32:58.921886 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:32:58.921886 15588 solver.cpp:237]     Train net output #1: loss = 0.0406176 (* 1 = 0.0406176 loss)
I1107 15:32:58.921886 15588 sgd_solver.cpp:105] Iteration 122100, lr = 0.001
I1107 15:33:07.641942 15588 solver.cpp:218] Iteration 122200 (11.4684 iter/s, 8.71964s/100 iters), loss = 0.0346133
I1107 15:33:07.641942 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:33:07.641942 15588 solver.cpp:237]     Train net output #1: loss = 0.0346134 (* 1 = 0.0346134 loss)
I1107 15:33:07.641942 15588 sgd_solver.cpp:105] Iteration 122200, lr = 0.001
I1107 15:33:16.417023 15588 solver.cpp:218] Iteration 122300 (11.3969 iter/s, 8.77432s/100 iters), loss = 0.0250178
I1107 15:33:16.417023 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:33:16.417023 15588 solver.cpp:237]     Train net output #1: loss = 0.025018 (* 1 = 0.025018 loss)
I1107 15:33:16.417023 15588 sgd_solver.cpp:105] Iteration 122300, lr = 0.001
I1107 15:33:25.231108 15588 solver.cpp:218] Iteration 122400 (11.346 iter/s, 8.81369s/100 iters), loss = 0.0413789
I1107 15:33:25.231108 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:33:25.231108 15588 solver.cpp:237]     Train net output #1: loss = 0.0413791 (* 1 = 0.0413791 loss)
I1107 15:33:25.231108 15588 sgd_solver.cpp:105] Iteration 122400, lr = 0.001
I1107 15:33:33.478302 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:33:33.814339 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_122500.caffemodel
I1107 15:33:33.843348 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_122500.solverstate
I1107 15:33:33.852349 15588 solver.cpp:330] Iteration 122500, Testing net (#0)
I1107 15:33:33.852349 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:33:35.848481  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:33:35.928486 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9188
I1107 15:33:35.928486 15588 solver.cpp:397]     Test net output #1: loss = 0.303222 (* 1 = 0.303222 loss)
I1107 15:33:36.009506 15588 solver.cpp:218] Iteration 122500 (9.27891 iter/s, 10.7771s/100 iters), loss = 0.0369161
I1107 15:33:36.009506 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:33:36.009506 15588 solver.cpp:237]     Train net output #1: loss = 0.0369163 (* 1 = 0.0369163 loss)
I1107 15:33:36.009506 15588 sgd_solver.cpp:105] Iteration 122500, lr = 0.001
I1107 15:33:44.545677 15588 solver.cpp:218] Iteration 122600 (11.7149 iter/s, 8.53614s/100 iters), loss = 0.0560393
I1107 15:33:44.545677 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:33:44.545677 15588 solver.cpp:237]     Train net output #1: loss = 0.0560395 (* 1 = 0.0560395 loss)
I1107 15:33:44.545677 15588 sgd_solver.cpp:105] Iteration 122600, lr = 0.001
I1107 15:33:53.086460 15588 solver.cpp:218] Iteration 122700 (11.7099 iter/s, 8.53977s/100 iters), loss = 0.0417344
I1107 15:33:53.086460 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:33:53.086460 15588 solver.cpp:237]     Train net output #1: loss = 0.0417346 (* 1 = 0.0417346 loss)
I1107 15:33:53.086460 15588 sgd_solver.cpp:105] Iteration 122700, lr = 0.001
I1107 15:34:01.619197 15588 solver.cpp:218] Iteration 122800 (11.72 iter/s, 8.53243s/100 iters), loss = 0.0370114
I1107 15:34:01.619197 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:34:01.619197 15588 solver.cpp:237]     Train net output #1: loss = 0.0370116 (* 1 = 0.0370116 loss)
I1107 15:34:01.619197 15588 sgd_solver.cpp:105] Iteration 122800, lr = 0.001
I1107 15:34:10.217613 15588 solver.cpp:218] Iteration 122900 (11.6313 iter/s, 8.59746s/100 iters), loss = 0.0325154
I1107 15:34:10.217613 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:34:10.217613 15588 solver.cpp:237]     Train net output #1: loss = 0.0325156 (* 1 = 0.0325156 loss)
I1107 15:34:10.217613 15588 sgd_solver.cpp:105] Iteration 122900, lr = 0.001
I1107 15:34:18.494807 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:34:18.841842 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_123000.caffemodel
I1107 15:34:18.874848 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_123000.solverstate
I1107 15:34:18.883857 15588 solver.cpp:330] Iteration 123000, Testing net (#0)
I1107 15:34:18.884855 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:34:20.916064  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:34:20.997069 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 15:34:20.997069 15588 solver.cpp:397]     Test net output #1: loss = 0.30423 (* 1 = 0.30423 loss)
I1107 15:34:21.081574 15588 solver.cpp:218] Iteration 123000 (9.20505 iter/s, 10.8636s/100 iters), loss = 0.0388594
I1107 15:34:21.081574 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:34:21.081574 15588 solver.cpp:237]     Train net output #1: loss = 0.0388596 (* 1 = 0.0388596 loss)
I1107 15:34:21.081574 15588 sgd_solver.cpp:105] Iteration 123000, lr = 0.001
I1107 15:34:29.822971 15588 solver.cpp:218] Iteration 123100 (11.4409 iter/s, 8.74059s/100 iters), loss = 0.0525852
I1107 15:34:29.822971 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:34:29.822971 15588 solver.cpp:237]     Train net output #1: loss = 0.0525854 (* 1 = 0.0525854 loss)
I1107 15:34:29.822971 15588 sgd_solver.cpp:105] Iteration 123100, lr = 0.001
I1107 15:34:38.554368 15588 solver.cpp:218] Iteration 123200 (11.4535 iter/s, 8.73099s/100 iters), loss = 0.0354589
I1107 15:34:38.554368 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:34:38.554368 15588 solver.cpp:237]     Train net output #1: loss = 0.035459 (* 1 = 0.035459 loss)
I1107 15:34:38.554368 15588 sgd_solver.cpp:105] Iteration 123200, lr = 0.001
I1107 15:34:47.082957 15588 solver.cpp:218] Iteration 123300 (11.7254 iter/s, 8.52849s/100 iters), loss = 0.0269828
I1107 15:34:47.082957 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:34:47.082957 15588 solver.cpp:237]     Train net output #1: loss = 0.0269829 (* 1 = 0.0269829 loss)
I1107 15:34:47.082957 15588 sgd_solver.cpp:105] Iteration 123300, lr = 0.001
I1107 15:34:55.659399 15588 solver.cpp:218] Iteration 123400 (11.6609 iter/s, 8.57568s/100 iters), loss = 0.0472324
I1107 15:34:55.659399 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:34:55.659399 15588 solver.cpp:237]     Train net output #1: loss = 0.0472326 (* 1 = 0.0472326 loss)
I1107 15:34:55.659399 15588 sgd_solver.cpp:105] Iteration 123400, lr = 0.001
I1107 15:35:03.820276 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:35:04.166275 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_123500.caffemodel
I1107 15:35:04.203285 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_123500.solverstate
I1107 15:35:04.213284 15588 solver.cpp:330] Iteration 123500, Testing net (#0)
I1107 15:35:04.213284 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:35:06.242604  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:35:06.324614 15588 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 15:35:06.324614 15588 solver.cpp:397]     Test net output #1: loss = 0.304328 (* 1 = 0.304328 loss)
I1107 15:35:06.408617 15588 solver.cpp:218] Iteration 123500 (9.30355 iter/s, 10.7486s/100 iters), loss = 0.0284752
I1107 15:35:06.408617 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:35:06.408617 15588 solver.cpp:237]     Train net output #1: loss = 0.0284754 (* 1 = 0.0284754 loss)
I1107 15:35:06.408617 15588 sgd_solver.cpp:105] Iteration 123500, lr = 0.001
I1107 15:35:15.159503 15588 solver.cpp:218] Iteration 123600 (11.428 iter/s, 8.75047s/100 iters), loss = 0.0370714
I1107 15:35:15.159503 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:35:15.159503 15588 solver.cpp:237]     Train net output #1: loss = 0.0370716 (* 1 = 0.0370716 loss)
I1107 15:35:15.159503 15588 sgd_solver.cpp:105] Iteration 123600, lr = 0.001
I1107 15:35:23.915513 15588 solver.cpp:218] Iteration 123700 (11.4223 iter/s, 8.7548s/100 iters), loss = 0.0429069
I1107 15:35:23.915513 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:35:23.915513 15588 solver.cpp:237]     Train net output #1: loss = 0.0429071 (* 1 = 0.0429071 loss)
I1107 15:35:23.915513 15588 sgd_solver.cpp:105] Iteration 123700, lr = 0.001
I1107 15:35:32.649421 15588 solver.cpp:218] Iteration 123800 (11.4498 iter/s, 8.73374s/100 iters), loss = 0.0241527
I1107 15:35:32.649421 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:35:32.649421 15588 solver.cpp:237]     Train net output #1: loss = 0.0241529 (* 1 = 0.0241529 loss)
I1107 15:35:32.649421 15588 sgd_solver.cpp:105] Iteration 123800, lr = 0.001
I1107 15:35:41.252822 15588 solver.cpp:218] Iteration 123900 (11.6247 iter/s, 8.6024s/100 iters), loss = 0.0547359
I1107 15:35:41.252822 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:35:41.252822 15588 solver.cpp:237]     Train net output #1: loss = 0.0547361 (* 1 = 0.0547361 loss)
I1107 15:35:41.252822 15588 sgd_solver.cpp:105] Iteration 123900, lr = 0.001
I1107 15:35:49.392225 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:35:49.729245 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_124000.caffemodel
I1107 15:35:49.759251 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_124000.solverstate
I1107 15:35:49.768252 15588 solver.cpp:330] Iteration 124000, Testing net (#0)
I1107 15:35:49.768252 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:35:51.772542  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:35:51.852548 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 15:35:51.852548 15588 solver.cpp:397]     Test net output #1: loss = 0.307977 (* 1 = 0.307977 loss)
I1107 15:35:51.933548 15588 solver.cpp:218] Iteration 124000 (9.36293 iter/s, 10.6804s/100 iters), loss = 0.0390557
I1107 15:35:51.933548 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:35:51.933548 15588 solver.cpp:237]     Train net output #1: loss = 0.0390559 (* 1 = 0.0390559 loss)
I1107 15:35:51.933548 15588 sgd_solver.cpp:105] Iteration 124000, lr = 0.001
I1107 15:36:00.472538 15588 solver.cpp:218] Iteration 124100 (11.7122 iter/s, 8.53809s/100 iters), loss = 0.0333948
I1107 15:36:00.472538 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:36:00.472538 15588 solver.cpp:237]     Train net output #1: loss = 0.033395 (* 1 = 0.033395 loss)
I1107 15:36:00.472538 15588 sgd_solver.cpp:105] Iteration 124100, lr = 0.001
I1107 15:36:09.060001 15588 solver.cpp:218] Iteration 124200 (11.6454 iter/s, 8.58706s/100 iters), loss = 0.0236691
I1107 15:36:09.060001 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:36:09.060001 15588 solver.cpp:237]     Train net output #1: loss = 0.0236693 (* 1 = 0.0236693 loss)
I1107 15:36:09.060001 15588 sgd_solver.cpp:105] Iteration 124200, lr = 0.001
I1107 15:36:17.687494 15588 solver.cpp:218] Iteration 124300 (11.5918 iter/s, 8.62681s/100 iters), loss = 0.0233388
I1107 15:36:17.687494 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:36:17.687494 15588 solver.cpp:237]     Train net output #1: loss = 0.023339 (* 1 = 0.023339 loss)
I1107 15:36:17.687494 15588 sgd_solver.cpp:105] Iteration 124300, lr = 0.001
I1107 15:36:26.257634 15588 solver.cpp:218] Iteration 124400 (11.6688 iter/s, 8.56989s/100 iters), loss = 0.0335075
I1107 15:36:26.257634 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:36:26.257634 15588 solver.cpp:237]     Train net output #1: loss = 0.0335077 (* 1 = 0.0335077 loss)
I1107 15:36:26.257634 15588 sgd_solver.cpp:105] Iteration 124400, lr = 0.001
I1107 15:36:34.456980 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:36:34.794998 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_124500.caffemodel
I1107 15:36:34.822002 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_124500.solverstate
I1107 15:36:34.831002 15588 solver.cpp:330] Iteration 124500, Testing net (#0)
I1107 15:36:34.831002 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:36:36.822165  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:36:36.901170 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1107 15:36:36.902171 15588 solver.cpp:397]     Test net output #1: loss = 0.304017 (* 1 = 0.304017 loss)
I1107 15:36:36.982169 15588 solver.cpp:218] Iteration 124500 (9.3246 iter/s, 10.7243s/100 iters), loss = 0.0318758
I1107 15:36:36.982169 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:36:36.982169 15588 solver.cpp:237]     Train net output #1: loss = 0.031876 (* 1 = 0.031876 loss)
I1107 15:36:36.982169 15588 sgd_solver.cpp:105] Iteration 124500, lr = 0.001
I1107 15:36:45.605258 15588 solver.cpp:218] Iteration 124600 (11.5976 iter/s, 8.62245s/100 iters), loss = 0.0256924
I1107 15:36:45.605258 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:36:45.605258 15588 solver.cpp:237]     Train net output #1: loss = 0.0256926 (* 1 = 0.0256926 loss)
I1107 15:36:45.605258 15588 sgd_solver.cpp:105] Iteration 124600, lr = 0.001
I1107 15:36:54.134088 15588 solver.cpp:218] Iteration 124700 (11.7266 iter/s, 8.52763s/100 iters), loss = 0.0246679
I1107 15:36:54.134088 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:36:54.134088 15588 solver.cpp:237]     Train net output #1: loss = 0.0246681 (* 1 = 0.0246681 loss)
I1107 15:36:54.134088 15588 sgd_solver.cpp:105] Iteration 124700, lr = 0.001
I1107 15:37:02.663107 15588 solver.cpp:218] Iteration 124800 (11.7241 iter/s, 8.52941s/100 iters), loss = 0.0290581
I1107 15:37:02.664108 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:37:02.664108 15588 solver.cpp:237]     Train net output #1: loss = 0.0290583 (* 1 = 0.0290583 loss)
I1107 15:37:02.664108 15588 sgd_solver.cpp:105] Iteration 124800, lr = 0.001
I1107 15:37:11.186900 15588 solver.cpp:218] Iteration 124900 (11.7331 iter/s, 8.52291s/100 iters), loss = 0.0320576
I1107 15:37:11.186900 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:37:11.186900 15588 solver.cpp:237]     Train net output #1: loss = 0.0320578 (* 1 = 0.0320578 loss)
I1107 15:37:11.186900 15588 sgd_solver.cpp:105] Iteration 124900, lr = 0.001
I1107 15:37:19.277565 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:37:19.613615 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_125000.caffemodel
I1107 15:37:19.644614 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_125000.solverstate
I1107 15:37:19.653615 15588 solver.cpp:330] Iteration 125000, Testing net (#0)
I1107 15:37:19.653615 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:37:21.638792  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:37:21.717797 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9195
I1107 15:37:21.717797 15588 solver.cpp:397]     Test net output #1: loss = 0.307228 (* 1 = 0.307228 loss)
I1107 15:37:21.798797 15588 solver.cpp:218] Iteration 125000 (9.42411 iter/s, 10.6111s/100 iters), loss = 0.0460687
I1107 15:37:21.798797 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:37:21.798797 15588 solver.cpp:237]     Train net output #1: loss = 0.0460689 (* 1 = 0.0460689 loss)
I1107 15:37:21.798797 15588 sgd_solver.cpp:105] Iteration 125000, lr = 0.001
I1107 15:37:30.370122 15588 solver.cpp:218] Iteration 125100 (11.6669 iter/s, 8.57128s/100 iters), loss = 0.0282166
I1107 15:37:30.370122 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:37:30.370122 15588 solver.cpp:237]     Train net output #1: loss = 0.0282168 (* 1 = 0.0282168 loss)
I1107 15:37:30.370122 15588 sgd_solver.cpp:105] Iteration 125100, lr = 0.001
I1107 15:37:39.005596 15588 solver.cpp:218] Iteration 125200 (11.5814 iter/s, 8.63456s/100 iters), loss = 0.0232788
I1107 15:37:39.006096 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:37:39.006096 15588 solver.cpp:237]     Train net output #1: loss = 0.023279 (* 1 = 0.023279 loss)
I1107 15:37:39.006096 15588 sgd_solver.cpp:105] Iteration 125200, lr = 0.001
I1107 15:37:47.505336 15588 solver.cpp:218] Iteration 125300 (11.7662 iter/s, 8.49891s/100 iters), loss = 0.0260752
I1107 15:37:47.505336 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:37:47.505336 15588 solver.cpp:237]     Train net output #1: loss = 0.0260754 (* 1 = 0.0260754 loss)
I1107 15:37:47.505336 15588 sgd_solver.cpp:105] Iteration 125300, lr = 0.001
I1107 15:37:56.012397 15588 solver.cpp:218] Iteration 125400 (11.756 iter/s, 8.50631s/100 iters), loss = 0.0382976
I1107 15:37:56.012397 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:37:56.012397 15588 solver.cpp:237]     Train net output #1: loss = 0.0382978 (* 1 = 0.0382978 loss)
I1107 15:37:56.012397 15588 sgd_solver.cpp:105] Iteration 125400, lr = 0.001
I1107 15:38:04.094192 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:38:04.429246 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_125500.caffemodel
I1107 15:38:04.464248 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_125500.solverstate
I1107 15:38:04.473248 15588 solver.cpp:330] Iteration 125500, Testing net (#0)
I1107 15:38:04.473248 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:38:06.458382  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:38:06.537386 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9178
I1107 15:38:06.537386 15588 solver.cpp:397]     Test net output #1: loss = 0.308969 (* 1 = 0.308969 loss)
I1107 15:38:06.618392 15588 solver.cpp:218] Iteration 125500 (9.429 iter/s, 10.6056s/100 iters), loss = 0.0299796
I1107 15:38:06.618392 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:38:06.618392 15588 solver.cpp:237]     Train net output #1: loss = 0.0299798 (* 1 = 0.0299798 loss)
I1107 15:38:06.618392 15588 sgd_solver.cpp:105] Iteration 125500, lr = 0.001
I1107 15:38:15.132081 15588 solver.cpp:218] Iteration 125600 (11.7466 iter/s, 8.51313s/100 iters), loss = 0.0746091
I1107 15:38:15.132081 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 15:38:15.132081 15588 solver.cpp:237]     Train net output #1: loss = 0.0746093 (* 1 = 0.0746093 loss)
I1107 15:38:15.132081 15588 sgd_solver.cpp:105] Iteration 125600, lr = 0.001
I1107 15:38:23.638999 15588 solver.cpp:218] Iteration 125700 (11.7561 iter/s, 8.50619s/100 iters), loss = 0.0318835
I1107 15:38:23.638999 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:38:23.638999 15588 solver.cpp:237]     Train net output #1: loss = 0.0318837 (* 1 = 0.0318837 loss)
I1107 15:38:23.638999 15588 sgd_solver.cpp:105] Iteration 125700, lr = 0.001
I1107 15:38:32.259677 15588 solver.cpp:218] Iteration 125800 (11.6005 iter/s, 8.62033s/100 iters), loss = 0.0230047
I1107 15:38:32.259677 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:38:32.259677 15588 solver.cpp:237]     Train net output #1: loss = 0.0230049 (* 1 = 0.0230049 loss)
I1107 15:38:32.259677 15588 sgd_solver.cpp:105] Iteration 125800, lr = 0.001
I1107 15:38:40.754739 15588 solver.cpp:218] Iteration 125900 (11.7715 iter/s, 8.49512s/100 iters), loss = 0.0318763
I1107 15:38:40.754739 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:38:40.754739 15588 solver.cpp:237]     Train net output #1: loss = 0.0318765 (* 1 = 0.0318765 loss)
I1107 15:38:40.754739 15588 sgd_solver.cpp:105] Iteration 125900, lr = 0.001
I1107 15:38:48.847486 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:38:49.181505 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_126000.caffemodel
I1107 15:38:49.211010 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_126000.solverstate
I1107 15:38:49.219513 15588 solver.cpp:330] Iteration 126000, Testing net (#0)
I1107 15:38:49.219513 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:38:51.232686  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:38:51.312207 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9197
I1107 15:38:51.312207 15588 solver.cpp:397]     Test net output #1: loss = 0.30503 (* 1 = 0.30503 loss)
I1107 15:38:51.394711 15588 solver.cpp:218] Iteration 126000 (9.39977 iter/s, 10.6386s/100 iters), loss = 0.0575771
I1107 15:38:51.394711 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:38:51.394711 15588 solver.cpp:237]     Train net output #1: loss = 0.0575773 (* 1 = 0.0575773 loss)
I1107 15:38:51.394711 15588 sgd_solver.cpp:105] Iteration 126000, lr = 0.001
I1107 15:38:59.917430 15588 solver.cpp:218] Iteration 126100 (11.7336 iter/s, 8.52255s/100 iters), loss = 0.0453389
I1107 15:38:59.917430 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:38:59.917430 15588 solver.cpp:237]     Train net output #1: loss = 0.0453391 (* 1 = 0.0453391 loss)
I1107 15:38:59.917430 15588 sgd_solver.cpp:105] Iteration 126100, lr = 0.001
I1107 15:39:08.548086 15588 solver.cpp:218] Iteration 126200 (11.5877 iter/s, 8.62984s/100 iters), loss = 0.0241133
I1107 15:39:08.548086 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:39:08.548086 15588 solver.cpp:237]     Train net output #1: loss = 0.0241135 (* 1 = 0.0241135 loss)
I1107 15:39:08.548086 15588 sgd_solver.cpp:105] Iteration 126200, lr = 0.001
I1107 15:39:17.143820 15588 solver.cpp:218] Iteration 126300 (11.6338 iter/s, 8.59566s/100 iters), loss = 0.0373199
I1107 15:39:17.143820 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:39:17.143820 15588 solver.cpp:237]     Train net output #1: loss = 0.0373201 (* 1 = 0.0373201 loss)
I1107 15:39:17.143820 15588 sgd_solver.cpp:105] Iteration 126300, lr = 0.001
I1107 15:39:25.689076 15588 solver.cpp:218] Iteration 126400 (11.703 iter/s, 8.54479s/100 iters), loss = 0.0283096
I1107 15:39:25.689076 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:39:25.689076 15588 solver.cpp:237]     Train net output #1: loss = 0.0283098 (* 1 = 0.0283098 loss)
I1107 15:39:25.689076 15588 sgd_solver.cpp:105] Iteration 126400, lr = 0.001
I1107 15:39:33.819592 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:39:34.155618 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_126500.caffemodel
I1107 15:39:34.185618 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_126500.solverstate
I1107 15:39:34.194619 15588 solver.cpp:330] Iteration 126500, Testing net (#0)
I1107 15:39:34.194619 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:39:36.197778  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:39:36.278785 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9176
I1107 15:39:36.278785 15588 solver.cpp:397]     Test net output #1: loss = 0.310531 (* 1 = 0.310531 loss)
I1107 15:39:36.362798 15588 solver.cpp:218] Iteration 126500 (9.36942 iter/s, 10.673s/100 iters), loss = 0.0280128
I1107 15:39:36.362798 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:39:36.362798 15588 solver.cpp:237]     Train net output #1: loss = 0.028013 (* 1 = 0.028013 loss)
I1107 15:39:36.362798 15588 sgd_solver.cpp:105] Iteration 126500, lr = 0.001
I1107 15:39:44.894513 15588 solver.cpp:218] Iteration 126600 (11.7221 iter/s, 8.53088s/100 iters), loss = 0.0266463
I1107 15:39:44.894513 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:39:44.894513 15588 solver.cpp:237]     Train net output #1: loss = 0.0266465 (* 1 = 0.0266465 loss)
I1107 15:39:44.894513 15588 sgd_solver.cpp:105] Iteration 126600, lr = 0.001
I1107 15:39:53.431382 15588 solver.cpp:218] Iteration 126700 (11.7147 iter/s, 8.53629s/100 iters), loss = 0.0191247
I1107 15:39:53.431382 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:39:53.431382 15588 solver.cpp:237]     Train net output #1: loss = 0.0191249 (* 1 = 0.0191249 loss)
I1107 15:39:53.431382 15588 sgd_solver.cpp:105] Iteration 126700, lr = 0.001
I1107 15:40:02.009541 15588 solver.cpp:218] Iteration 126800 (11.6581 iter/s, 8.57774s/100 iters), loss = 0.045327
I1107 15:40:02.009541 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:40:02.009541 15588 solver.cpp:237]     Train net output #1: loss = 0.0453273 (* 1 = 0.0453273 loss)
I1107 15:40:02.009541 15588 sgd_solver.cpp:105] Iteration 126800, lr = 0.001
I1107 15:40:10.538446 15588 solver.cpp:218] Iteration 126900 (11.7253 iter/s, 8.52858s/100 iters), loss = 0.0253462
I1107 15:40:10.538446 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:40:10.538446 15588 solver.cpp:237]     Train net output #1: loss = 0.0253465 (* 1 = 0.0253465 loss)
I1107 15:40:10.538446 15588 sgd_solver.cpp:105] Iteration 126900, lr = 0.001
I1107 15:40:18.677184 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:40:19.012218 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_127000.caffemodel
I1107 15:40:19.048223 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_127000.solverstate
I1107 15:40:19.057222 15588 solver.cpp:330] Iteration 127000, Testing net (#0)
I1107 15:40:19.057222 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:40:21.057379  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:40:21.139385 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9174
I1107 15:40:21.139385 15588 solver.cpp:397]     Test net output #1: loss = 0.311099 (* 1 = 0.311099 loss)
I1107 15:40:21.223393 15588 solver.cpp:218] Iteration 127000 (9.35929 iter/s, 10.6846s/100 iters), loss = 0.034883
I1107 15:40:21.223393 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:40:21.223393 15588 solver.cpp:237]     Train net output #1: loss = 0.0348832 (* 1 = 0.0348832 loss)
I1107 15:40:21.223393 15588 sgd_solver.cpp:105] Iteration 127000, lr = 0.001
I1107 15:40:29.761106 15588 solver.cpp:218] Iteration 127100 (11.7136 iter/s, 8.53707s/100 iters), loss = 0.0449192
I1107 15:40:29.761106 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:40:29.761106 15588 solver.cpp:237]     Train net output #1: loss = 0.0449194 (* 1 = 0.0449194 loss)
I1107 15:40:29.761106 15588 sgd_solver.cpp:105] Iteration 127100, lr = 0.001
I1107 15:40:38.292343 15588 solver.cpp:218] Iteration 127200 (11.7225 iter/s, 8.53058s/100 iters), loss = 0.0289229
I1107 15:40:38.292343 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:40:38.292343 15588 solver.cpp:237]     Train net output #1: loss = 0.0289231 (* 1 = 0.0289231 loss)
I1107 15:40:38.292343 15588 sgd_solver.cpp:105] Iteration 127200, lr = 0.001
I1107 15:40:46.807185 15588 solver.cpp:218] Iteration 127300 (11.7455 iter/s, 8.51389s/100 iters), loss = 0.0250743
I1107 15:40:46.807185 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:40:46.807185 15588 solver.cpp:237]     Train net output #1: loss = 0.0250745 (* 1 = 0.0250745 loss)
I1107 15:40:46.807185 15588 sgd_solver.cpp:105] Iteration 127300, lr = 0.001
I1107 15:40:55.314782 15588 solver.cpp:218] Iteration 127400 (11.7544 iter/s, 8.50742s/100 iters), loss = 0.0256914
I1107 15:40:55.315284 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:40:55.315284 15588 solver.cpp:237]     Train net output #1: loss = 0.0256916 (* 1 = 0.0256916 loss)
I1107 15:40:55.315284 15588 sgd_solver.cpp:105] Iteration 127400, lr = 0.001
I1107 15:41:03.437386 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:41:03.783888 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_127500.caffemodel
I1107 15:41:03.813886 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_127500.solverstate
I1107 15:41:03.823386 15588 solver.cpp:330] Iteration 127500, Testing net (#0)
I1107 15:41:03.823386 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:41:05.815021  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:41:05.895026 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9172
I1107 15:41:05.895026 15588 solver.cpp:397]     Test net output #1: loss = 0.310497 (* 1 = 0.310497 loss)
I1107 15:41:05.975028 15588 solver.cpp:218] Iteration 127500 (9.38103 iter/s, 10.6598s/100 iters), loss = 0.0327681
I1107 15:41:05.975028 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:41:05.975028 15588 solver.cpp:237]     Train net output #1: loss = 0.0327683 (* 1 = 0.0327683 loss)
I1107 15:41:05.975028 15588 sgd_solver.cpp:105] Iteration 127500, lr = 0.001
I1107 15:41:14.478806 15588 solver.cpp:218] Iteration 127600 (11.7598 iter/s, 8.50352s/100 iters), loss = 0.0303775
I1107 15:41:14.479789 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:41:14.479789 15588 solver.cpp:237]     Train net output #1: loss = 0.0303777 (* 1 = 0.0303777 loss)
I1107 15:41:14.479789 15588 sgd_solver.cpp:105] Iteration 127600, lr = 0.001
I1107 15:41:23.004551 15588 solver.cpp:218] Iteration 127700 (11.7311 iter/s, 8.52437s/100 iters), loss = 0.0245992
I1107 15:41:23.004551 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:41:23.004551 15588 solver.cpp:237]     Train net output #1: loss = 0.0245994 (* 1 = 0.0245994 loss)
I1107 15:41:23.004551 15588 sgd_solver.cpp:105] Iteration 127700, lr = 0.001
I1107 15:41:31.549257 15588 solver.cpp:218] Iteration 127800 (11.7036 iter/s, 8.54436s/100 iters), loss = 0.0178554
I1107 15:41:31.549257 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:41:31.549257 15588 solver.cpp:237]     Train net output #1: loss = 0.0178556 (* 1 = 0.0178556 loss)
I1107 15:41:31.549257 15588 sgd_solver.cpp:105] Iteration 127800, lr = 0.001
I1107 15:41:40.083973 15588 solver.cpp:218] Iteration 127900 (11.7182 iter/s, 8.53375s/100 iters), loss = 0.0299826
I1107 15:41:40.083973 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:41:40.083973 15588 solver.cpp:237]     Train net output #1: loss = 0.0299828 (* 1 = 0.0299828 loss)
I1107 15:41:40.083973 15588 sgd_solver.cpp:105] Iteration 127900, lr = 0.001
I1107 15:41:48.219784 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:41:48.556805 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_128000.caffemodel
I1107 15:41:48.586805 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_128000.solverstate
I1107 15:41:48.595309 15588 solver.cpp:330] Iteration 128000, Testing net (#0)
I1107 15:41:48.595808 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:41:50.596649  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:41:50.676143 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9165
I1107 15:41:50.676143 15588 solver.cpp:397]     Test net output #1: loss = 0.312627 (* 1 = 0.312627 loss)
I1107 15:41:50.757148 15588 solver.cpp:218] Iteration 128000 (9.3694 iter/s, 10.673s/100 iters), loss = 0.030946
I1107 15:41:50.757148 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:41:50.757148 15588 solver.cpp:237]     Train net output #1: loss = 0.0309462 (* 1 = 0.0309462 loss)
I1107 15:41:50.757148 15588 sgd_solver.cpp:105] Iteration 128000, lr = 0.001
I1107 15:41:59.255822 15588 solver.cpp:218] Iteration 128100 (11.767 iter/s, 8.49837s/100 iters), loss = 0.0541074
I1107 15:41:59.255822 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:41:59.255822 15588 solver.cpp:237]     Train net output #1: loss = 0.0541076 (* 1 = 0.0541076 loss)
I1107 15:41:59.255822 15588 sgd_solver.cpp:105] Iteration 128100, lr = 0.001
I1107 15:42:07.842447 15588 solver.cpp:218] Iteration 128200 (11.6473 iter/s, 8.5857s/100 iters), loss = 0.0352613
I1107 15:42:07.842447 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:42:07.842447 15588 solver.cpp:237]     Train net output #1: loss = 0.0352615 (* 1 = 0.0352615 loss)
I1107 15:42:07.842447 15588 sgd_solver.cpp:105] Iteration 128200, lr = 0.001
I1107 15:42:16.429225 15588 solver.cpp:218] Iteration 128300 (11.6461 iter/s, 8.58658s/100 iters), loss = 0.0254149
I1107 15:42:16.429225 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:42:16.429225 15588 solver.cpp:237]     Train net output #1: loss = 0.0254151 (* 1 = 0.0254151 loss)
I1107 15:42:16.429225 15588 sgd_solver.cpp:105] Iteration 128300, lr = 0.001
I1107 15:42:25.056991 15588 solver.cpp:218] Iteration 128400 (11.5918 iter/s, 8.62679s/100 iters), loss = 0.0255817
I1107 15:42:25.056991 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:42:25.056991 15588 solver.cpp:237]     Train net output #1: loss = 0.0255819 (* 1 = 0.0255819 loss)
I1107 15:42:25.056991 15588 sgd_solver.cpp:105] Iteration 128400, lr = 0.001
I1107 15:42:33.189815 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:42:33.526854 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_128500.caffemodel
I1107 15:42:33.563854 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_128500.solverstate
I1107 15:42:33.572854 15588 solver.cpp:330] Iteration 128500, Testing net (#0)
I1107 15:42:33.572854 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:42:35.561995  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:42:35.642000 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9173
I1107 15:42:35.642000 15588 solver.cpp:397]     Test net output #1: loss = 0.312792 (* 1 = 0.312792 loss)
I1107 15:42:35.723004 15588 solver.cpp:218] Iteration 128500 (9.37617 iter/s, 10.6653s/100 iters), loss = 0.0415733
I1107 15:42:35.723004 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:42:35.723004 15588 solver.cpp:237]     Train net output #1: loss = 0.0415735 (* 1 = 0.0415735 loss)
I1107 15:42:35.723004 15588 sgd_solver.cpp:105] Iteration 128500, lr = 0.001
I1107 15:42:44.233770 15588 solver.cpp:218] Iteration 128600 (11.7496 iter/s, 8.51096s/100 iters), loss = 0.0309331
I1107 15:42:44.234771 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:42:44.234771 15588 solver.cpp:237]     Train net output #1: loss = 0.0309333 (* 1 = 0.0309333 loss)
I1107 15:42:44.234771 15588 sgd_solver.cpp:105] Iteration 128600, lr = 0.001
I1107 15:42:52.849637 15588 solver.cpp:218] Iteration 128700 (11.6074 iter/s, 8.61521s/100 iters), loss = 0.0409072
I1107 15:42:52.849637 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:42:52.849637 15588 solver.cpp:237]     Train net output #1: loss = 0.0409074 (* 1 = 0.0409074 loss)
I1107 15:42:52.849637 15588 sgd_solver.cpp:105] Iteration 128700, lr = 0.001
I1107 15:43:01.430115 15588 solver.cpp:218] Iteration 128800 (11.6557 iter/s, 8.57952s/100 iters), loss = 0.0216648
I1107 15:43:01.430115 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:43:01.430115 15588 solver.cpp:237]     Train net output #1: loss = 0.0216651 (* 1 = 0.0216651 loss)
I1107 15:43:01.430115 15588 sgd_solver.cpp:105] Iteration 128800, lr = 0.001
I1107 15:43:10.119125 15588 solver.cpp:218] Iteration 128900 (11.5097 iter/s, 8.68829s/100 iters), loss = 0.0323826
I1107 15:43:10.119125 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:43:10.119125 15588 solver.cpp:237]     Train net output #1: loss = 0.0323828 (* 1 = 0.0323828 loss)
I1107 15:43:10.119125 15588 sgd_solver.cpp:105] Iteration 128900, lr = 0.001
I1107 15:43:18.266983 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:43:18.610019 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_129000.caffemodel
I1107 15:43:18.646018 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_129000.solverstate
I1107 15:43:18.656018 15588 solver.cpp:330] Iteration 129000, Testing net (#0)
I1107 15:43:18.656018 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:43:20.646208  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:43:20.725211 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9166
I1107 15:43:20.725211 15588 solver.cpp:397]     Test net output #1: loss = 0.313035 (* 1 = 0.313035 loss)
I1107 15:43:20.806213 15588 solver.cpp:218] Iteration 129000 (9.35773 iter/s, 10.6864s/100 iters), loss = 0.0397863
I1107 15:43:20.806213 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:43:20.806213 15588 solver.cpp:237]     Train net output #1: loss = 0.0397865 (* 1 = 0.0397865 loss)
I1107 15:43:20.806213 15588 sgd_solver.cpp:105] Iteration 129000, lr = 0.001
I1107 15:43:29.346124 15588 solver.cpp:218] Iteration 129100 (11.7101 iter/s, 8.53961s/100 iters), loss = 0.0356692
I1107 15:43:29.346124 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:43:29.346124 15588 solver.cpp:237]     Train net output #1: loss = 0.0356694 (* 1 = 0.0356694 loss)
I1107 15:43:29.346124 15588 sgd_solver.cpp:105] Iteration 129100, lr = 0.001
I1107 15:43:37.984549 15588 solver.cpp:218] Iteration 129200 (11.5762 iter/s, 8.63839s/100 iters), loss = 0.0241292
I1107 15:43:37.984549 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:43:37.984549 15588 solver.cpp:237]     Train net output #1: loss = 0.0241294 (* 1 = 0.0241294 loss)
I1107 15:43:37.984549 15588 sgd_solver.cpp:105] Iteration 129200, lr = 0.001
I1107 15:43:46.573993 15588 solver.cpp:218] Iteration 129300 (11.644 iter/s, 8.58809s/100 iters), loss = 0.0278321
I1107 15:43:46.573993 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:43:46.573993 15588 solver.cpp:237]     Train net output #1: loss = 0.0278323 (* 1 = 0.0278323 loss)
I1107 15:43:46.573993 15588 sgd_solver.cpp:105] Iteration 129300, lr = 0.001
I1107 15:43:55.175880 15588 solver.cpp:218] Iteration 129400 (11.626 iter/s, 8.6014s/100 iters), loss = 0.055161
I1107 15:43:55.175880 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:43:55.175880 15588 solver.cpp:237]     Train net output #1: loss = 0.0551612 (* 1 = 0.0551612 loss)
I1107 15:43:55.175880 15588 sgd_solver.cpp:105] Iteration 129400, lr = 0.001
I1107 15:44:03.420747 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:44:03.755795 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_129500.caffemodel
I1107 15:44:03.791796 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_129500.solverstate
I1107 15:44:03.800796 15588 solver.cpp:330] Iteration 129500, Testing net (#0)
I1107 15:44:03.801796 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:44:05.811499  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:44:05.892501 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9155
I1107 15:44:05.892501 15588 solver.cpp:397]     Test net output #1: loss = 0.314119 (* 1 = 0.314119 loss)
I1107 15:44:05.976511 15588 solver.cpp:218] Iteration 129500 (9.25925 iter/s, 10.8s/100 iters), loss = 0.0299915
I1107 15:44:05.976511 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:44:05.976511 15588 solver.cpp:237]     Train net output #1: loss = 0.0299917 (* 1 = 0.0299917 loss)
I1107 15:44:05.976511 15588 sgd_solver.cpp:105] Iteration 129500, lr = 0.001
I1107 15:44:14.649945 15588 solver.cpp:218] Iteration 129600 (11.5298 iter/s, 8.67321s/100 iters), loss = 0.0283511
I1107 15:44:14.649945 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:44:14.649945 15588 solver.cpp:237]     Train net output #1: loss = 0.0283513 (* 1 = 0.0283513 loss)
I1107 15:44:14.649945 15588 sgd_solver.cpp:105] Iteration 129600, lr = 0.001
I1107 15:44:23.273901 15588 solver.cpp:218] Iteration 129700 (11.5971 iter/s, 8.62286s/100 iters), loss = 0.0195954
I1107 15:44:23.273901 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:44:23.273901 15588 solver.cpp:237]     Train net output #1: loss = 0.0195956 (* 1 = 0.0195956 loss)
I1107 15:44:23.273901 15588 sgd_solver.cpp:105] Iteration 129700, lr = 0.001
I1107 15:44:31.974351 15588 solver.cpp:218] Iteration 129800 (11.4946 iter/s, 8.69971s/100 iters), loss = 0.0243177
I1107 15:44:31.974351 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:44:31.974351 15588 solver.cpp:237]     Train net output #1: loss = 0.0243179 (* 1 = 0.0243179 loss)
I1107 15:44:31.974351 15588 sgd_solver.cpp:105] Iteration 129800, lr = 0.001
I1107 15:44:40.790679 15588 solver.cpp:218] Iteration 129900 (11.3433 iter/s, 8.81575s/100 iters), loss = 0.044058
I1107 15:44:40.790679 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:44:40.790679 15588 solver.cpp:237]     Train net output #1: loss = 0.0440582 (* 1 = 0.0440582 loss)
I1107 15:44:40.790679 15588 sgd_solver.cpp:105] Iteration 129900, lr = 0.001
I1107 15:44:49.294458 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:44:49.640457 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_130000.caffemodel
I1107 15:44:49.676456 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_130000.solverstate
I1107 15:44:49.685456 15588 solver.cpp:330] Iteration 130000, Testing net (#0)
I1107 15:44:49.685456 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:44:51.738471  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:44:51.819970 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1107 15:44:51.819970 15588 solver.cpp:397]     Test net output #1: loss = 0.306615 (* 1 = 0.306615 loss)
I1107 15:44:51.904969 15588 solver.cpp:218] Iteration 130000 (8.99804 iter/s, 11.1135s/100 iters), loss = 0.0431424
I1107 15:44:51.904969 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:44:51.904969 15588 solver.cpp:237]     Train net output #1: loss = 0.0431426 (* 1 = 0.0431426 loss)
I1107 15:44:51.904969 15588 sgd_solver.cpp:105] Iteration 130000, lr = 0.001
I1107 15:45:00.648516 15588 solver.cpp:218] Iteration 130100 (11.4374 iter/s, 8.74324s/100 iters), loss = 0.0657659
I1107 15:45:00.648516 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 15:45:00.648516 15588 solver.cpp:237]     Train net output #1: loss = 0.0657661 (* 1 = 0.0657661 loss)
I1107 15:45:00.648516 15588 sgd_solver.cpp:105] Iteration 130100, lr = 0.001
I1107 15:45:09.497122 15588 solver.cpp:218] Iteration 130200 (11.302 iter/s, 8.84799s/100 iters), loss = 0.0214735
I1107 15:45:09.497122 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:45:09.497122 15588 solver.cpp:237]     Train net output #1: loss = 0.0214737 (* 1 = 0.0214737 loss)
I1107 15:45:09.497122 15588 sgd_solver.cpp:105] Iteration 130200, lr = 0.001
I1107 15:45:18.229081 15588 solver.cpp:218] Iteration 130300 (11.4531 iter/s, 8.73123s/100 iters), loss = 0.0303545
I1107 15:45:18.229081 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:45:18.229081 15588 solver.cpp:237]     Train net output #1: loss = 0.0303548 (* 1 = 0.0303548 loss)
I1107 15:45:18.229081 15588 sgd_solver.cpp:105] Iteration 130300, lr = 0.001
I1107 15:45:27.012161 15588 solver.cpp:218] Iteration 130400 (11.3861 iter/s, 8.78264s/100 iters), loss = 0.0261452
I1107 15:45:27.012161 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:45:27.012161 15588 solver.cpp:237]     Train net output #1: loss = 0.0261454 (* 1 = 0.0261454 loss)
I1107 15:45:27.012161 15588 sgd_solver.cpp:105] Iteration 130400, lr = 0.001
I1107 15:45:35.284168 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:45:35.626732 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_130500.caffemodel
I1107 15:45:35.661231 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_130500.solverstate
I1107 15:45:35.670231 15588 solver.cpp:330] Iteration 130500, Testing net (#0)
I1107 15:45:35.670732 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:45:37.695251  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:45:37.776232 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1107 15:45:37.776732 15588 solver.cpp:397]     Test net output #1: loss = 0.309748 (* 1 = 0.309748 loss)
I1107 15:45:37.859732 15588 solver.cpp:218] Iteration 130500 (9.21902 iter/s, 10.8471s/100 iters), loss = 0.0371315
I1107 15:45:37.859732 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:45:37.859732 15588 solver.cpp:237]     Train net output #1: loss = 0.0371317 (* 1 = 0.0371317 loss)
I1107 15:45:37.859732 15588 sgd_solver.cpp:105] Iteration 130500, lr = 0.001
I1107 15:45:46.555896 15588 solver.cpp:218] Iteration 130600 (11.5001 iter/s, 8.6956s/100 iters), loss = 0.0527743
I1107 15:45:46.555896 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:45:46.555896 15588 solver.cpp:237]     Train net output #1: loss = 0.0527745 (* 1 = 0.0527745 loss)
I1107 15:45:46.555896 15588 sgd_solver.cpp:105] Iteration 130600, lr = 0.001
I1107 15:45:55.249313 15588 solver.cpp:218] Iteration 130700 (11.5037 iter/s, 8.69284s/100 iters), loss = 0.0217228
I1107 15:45:55.249313 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:45:55.249313 15588 solver.cpp:237]     Train net output #1: loss = 0.021723 (* 1 = 0.021723 loss)
I1107 15:45:55.249313 15588 sgd_solver.cpp:105] Iteration 130700, lr = 0.001
I1107 15:46:03.946367 15588 solver.cpp:218] Iteration 130800 (11.4989 iter/s, 8.69645s/100 iters), loss = 0.0219454
I1107 15:46:03.946367 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:46:03.946367 15588 solver.cpp:237]     Train net output #1: loss = 0.0219456 (* 1 = 0.0219456 loss)
I1107 15:46:03.946367 15588 sgd_solver.cpp:105] Iteration 130800, lr = 0.001
I1107 15:46:12.645390 15588 solver.cpp:218] Iteration 130900 (11.4965 iter/s, 8.69829s/100 iters), loss = 0.0268393
I1107 15:46:12.645390 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:46:12.645390 15588 solver.cpp:237]     Train net output #1: loss = 0.0268395 (* 1 = 0.0268395 loss)
I1107 15:46:12.645390 15588 sgd_solver.cpp:105] Iteration 130900, lr = 0.001
I1107 15:46:20.917449 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:46:21.261448 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_131000.caffemodel
I1107 15:46:21.298449 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_131000.solverstate
I1107 15:46:21.308449 15588 solver.cpp:330] Iteration 131000, Testing net (#0)
I1107 15:46:21.308449 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:46:23.336449  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:46:23.417449 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 15:46:23.417449 15588 solver.cpp:397]     Test net output #1: loss = 0.312467 (* 1 = 0.312467 loss)
I1107 15:46:23.499449 15588 solver.cpp:218] Iteration 131000 (9.21372 iter/s, 10.8534s/100 iters), loss = 0.0261387
I1107 15:46:23.499449 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:46:23.499449 15588 solver.cpp:237]     Train net output #1: loss = 0.0261388 (* 1 = 0.0261388 loss)
I1107 15:46:23.499449 15588 sgd_solver.cpp:105] Iteration 131000, lr = 0.001
I1107 15:46:32.206262 15588 solver.cpp:218] Iteration 131100 (11.4856 iter/s, 8.70655s/100 iters), loss = 0.0256486
I1107 15:46:32.206763 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:46:32.206763 15588 solver.cpp:237]     Train net output #1: loss = 0.0256488 (* 1 = 0.0256488 loss)
I1107 15:46:32.206763 15588 sgd_solver.cpp:105] Iteration 131100, lr = 0.001
I1107 15:46:41.028172 15588 solver.cpp:218] Iteration 131200 (11.3368 iter/s, 8.8208s/100 iters), loss = 0.0229326
I1107 15:46:41.028172 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:46:41.028172 15588 solver.cpp:237]     Train net output #1: loss = 0.0229327 (* 1 = 0.0229327 loss)
I1107 15:46:41.028172 15588 sgd_solver.cpp:105] Iteration 131200, lr = 0.001
I1107 15:46:49.797590 15588 solver.cpp:218] Iteration 131300 (11.4042 iter/s, 8.76872s/100 iters), loss = 0.0223438
I1107 15:46:49.797590 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:46:49.797590 15588 solver.cpp:237]     Train net output #1: loss = 0.022344 (* 1 = 0.022344 loss)
I1107 15:46:49.797590 15588 sgd_solver.cpp:105] Iteration 131300, lr = 0.001
I1107 15:46:58.503422 15588 solver.cpp:218] Iteration 131400 (11.4868 iter/s, 8.70561s/100 iters), loss = 0.0449204
I1107 15:46:58.503923 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:46:58.503923 15588 solver.cpp:237]     Train net output #1: loss = 0.0449206 (* 1 = 0.0449206 loss)
I1107 15:46:58.503923 15588 sgd_solver.cpp:105] Iteration 131400, lr = 0.001
I1107 15:47:06.780225 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:47:07.123941 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_131500.caffemodel
I1107 15:47:07.160442 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_131500.solverstate
I1107 15:47:07.169942 15588 solver.cpp:330] Iteration 131500, Testing net (#0)
I1107 15:47:07.169942 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:47:09.195941  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:47:09.276950 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9189
I1107 15:47:09.276950 15588 solver.cpp:397]     Test net output #1: loss = 0.312811 (* 1 = 0.312811 loss)
I1107 15:47:09.359941 15588 solver.cpp:218] Iteration 131500 (9.21201 iter/s, 10.8554s/100 iters), loss = 0.0575938
I1107 15:47:09.359941 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:47:09.359941 15588 solver.cpp:237]     Train net output #1: loss = 0.057594 (* 1 = 0.057594 loss)
I1107 15:47:09.359941 15588 sgd_solver.cpp:105] Iteration 131500, lr = 0.001
I1107 15:47:18.055377 15588 solver.cpp:218] Iteration 131600 (11.5008 iter/s, 8.69502s/100 iters), loss = 0.0582306
I1107 15:47:18.055377 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 15:47:18.055377 15588 solver.cpp:237]     Train net output #1: loss = 0.0582308 (* 1 = 0.0582308 loss)
I1107 15:47:18.055377 15588 sgd_solver.cpp:105] Iteration 131600, lr = 0.001
I1107 15:47:26.751632 15588 solver.cpp:218] Iteration 131700 (11.5003 iter/s, 8.6954s/100 iters), loss = 0.0255067
I1107 15:47:26.751632 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:47:26.751632 15588 solver.cpp:237]     Train net output #1: loss = 0.0255069 (* 1 = 0.0255069 loss)
I1107 15:47:26.751632 15588 sgd_solver.cpp:105] Iteration 131700, lr = 0.001
I1107 15:47:35.593544 15588 solver.cpp:218] Iteration 131800 (11.3108 iter/s, 8.84115s/100 iters), loss = 0.0199867
I1107 15:47:35.593544 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:47:35.593544 15588 solver.cpp:237]     Train net output #1: loss = 0.0199869 (* 1 = 0.0199869 loss)
I1107 15:47:35.593544 15588 sgd_solver.cpp:105] Iteration 131800, lr = 0.001
I1107 15:47:44.326315 15588 solver.cpp:218] Iteration 131900 (11.4517 iter/s, 8.7323s/100 iters), loss = 0.0275972
I1107 15:47:44.326315 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:47:44.326315 15588 solver.cpp:237]     Train net output #1: loss = 0.0275974 (* 1 = 0.0275974 loss)
I1107 15:47:44.326315 15588 sgd_solver.cpp:105] Iteration 131900, lr = 0.001
I1107 15:47:52.741369 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:47:53.084369 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_132000.caffemodel
I1107 15:47:53.117869 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_132000.solverstate
I1107 15:47:53.127370 15588 solver.cpp:330] Iteration 132000, Testing net (#0)
I1107 15:47:53.127370 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:47:55.153369  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:47:55.234369 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9172
I1107 15:47:55.234869 15588 solver.cpp:397]     Test net output #1: loss = 0.311594 (* 1 = 0.311594 loss)
I1107 15:47:55.317870 15588 solver.cpp:218] Iteration 132000 (9.09863 iter/s, 10.9907s/100 iters), loss = 0.0343056
I1107 15:47:55.317870 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:47:55.317870 15588 solver.cpp:237]     Train net output #1: loss = 0.0343058 (* 1 = 0.0343058 loss)
I1107 15:47:55.317870 15588 sgd_solver.cpp:105] Iteration 132000, lr = 0.001
I1107 15:48:04.021394 15588 solver.cpp:218] Iteration 132100 (11.4905 iter/s, 8.70282s/100 iters), loss = 0.0346725
I1107 15:48:04.021394 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:48:04.021394 15588 solver.cpp:237]     Train net output #1: loss = 0.0346727 (* 1 = 0.0346727 loss)
I1107 15:48:04.021394 15588 sgd_solver.cpp:105] Iteration 132100, lr = 0.001
I1107 15:48:12.872709 15588 solver.cpp:218] Iteration 132200 (11.2987 iter/s, 8.85057s/100 iters), loss = 0.0336233
I1107 15:48:12.872709 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:48:12.872709 15588 solver.cpp:237]     Train net output #1: loss = 0.0336235 (* 1 = 0.0336235 loss)
I1107 15:48:12.872709 15588 sgd_solver.cpp:105] Iteration 132200, lr = 0.001
I1107 15:48:21.576799 15588 solver.cpp:218] Iteration 132300 (11.4894 iter/s, 8.70369s/100 iters), loss = 0.021198
I1107 15:48:21.576799 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:48:21.576799 15588 solver.cpp:237]     Train net output #1: loss = 0.0211982 (* 1 = 0.0211982 loss)
I1107 15:48:21.576799 15588 sgd_solver.cpp:105] Iteration 132300, lr = 0.001
I1107 15:48:30.278931 15588 solver.cpp:218] Iteration 132400 (11.4925 iter/s, 8.70136s/100 iters), loss = 0.0268002
I1107 15:48:30.278931 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:48:30.278931 15588 solver.cpp:237]     Train net output #1: loss = 0.0268003 (* 1 = 0.0268003 loss)
I1107 15:48:30.278931 15588 sgd_solver.cpp:105] Iteration 132400, lr = 0.001
I1107 15:48:38.629225 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:48:38.972725 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_132500.caffemodel
I1107 15:48:39.011725 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_132500.solverstate
I1107 15:48:39.020725 15588 solver.cpp:330] Iteration 132500, Testing net (#0)
I1107 15:48:39.021225 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:48:41.056726  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:48:41.139226 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 15:48:41.139226 15588 solver.cpp:397]     Test net output #1: loss = 0.312224 (* 1 = 0.312224 loss)
I1107 15:48:41.222745 15588 solver.cpp:218] Iteration 132500 (9.13793 iter/s, 10.9434s/100 iters), loss = 0.0313935
I1107 15:48:41.222745 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:48:41.222745 15588 solver.cpp:237]     Train net output #1: loss = 0.0313937 (* 1 = 0.0313937 loss)
I1107 15:48:41.223227 15588 sgd_solver.cpp:105] Iteration 132500, lr = 0.001
I1107 15:48:50.065114 15588 solver.cpp:218] Iteration 132600 (11.3104 iter/s, 8.84141s/100 iters), loss = 0.0325963
I1107 15:48:50.065114 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:48:50.065114 15588 solver.cpp:237]     Train net output #1: loss = 0.0325964 (* 1 = 0.0325964 loss)
I1107 15:48:50.065114 15588 sgd_solver.cpp:105] Iteration 132600, lr = 0.001
I1107 15:48:58.869604 15588 solver.cpp:218] Iteration 132700 (11.3586 iter/s, 8.80388s/100 iters), loss = 0.0262316
I1107 15:48:58.869604 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:48:58.869604 15588 solver.cpp:237]     Train net output #1: loss = 0.0262318 (* 1 = 0.0262318 loss)
I1107 15:48:58.869604 15588 sgd_solver.cpp:105] Iteration 132700, lr = 0.001
I1107 15:49:07.576177 15588 solver.cpp:218] Iteration 132800 (11.4862 iter/s, 8.70608s/100 iters), loss = 0.0183392
I1107 15:49:07.576177 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:49:07.576678 15588 solver.cpp:237]     Train net output #1: loss = 0.0183394 (* 1 = 0.0183394 loss)
I1107 15:49:07.576678 15588 sgd_solver.cpp:105] Iteration 132800, lr = 0.001
I1107 15:49:16.336746 15588 solver.cpp:218] Iteration 132900 (11.4157 iter/s, 8.75987s/100 iters), loss = 0.0260177
I1107 15:49:16.336746 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:49:16.336746 15588 solver.cpp:237]     Train net output #1: loss = 0.0260179 (* 1 = 0.0260179 loss)
I1107 15:49:16.336746 15588 sgd_solver.cpp:105] Iteration 132900, lr = 0.001
I1107 15:49:24.706768 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:49:25.057268 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_133000.caffemodel
I1107 15:49:25.088284 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_133000.solverstate
I1107 15:49:25.098284 15588 solver.cpp:330] Iteration 133000, Testing net (#0)
I1107 15:49:25.098284 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:49:27.147778  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:49:27.229269 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 15:49:27.229269 15588 solver.cpp:397]     Test net output #1: loss = 0.311171 (* 1 = 0.311171 loss)
I1107 15:49:27.310767 15588 solver.cpp:218] Iteration 133000 (9.11303 iter/s, 10.9733s/100 iters), loss = 0.0505566
I1107 15:49:27.311269 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:49:27.311269 15588 solver.cpp:237]     Train net output #1: loss = 0.0505568 (* 1 = 0.0505568 loss)
I1107 15:49:27.311269 15588 sgd_solver.cpp:105] Iteration 133000, lr = 0.001
I1107 15:49:36.083974 15588 solver.cpp:218] Iteration 133100 (11.3996 iter/s, 8.77225s/100 iters), loss = 0.0305934
I1107 15:49:36.083974 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:49:36.083974 15588 solver.cpp:237]     Train net output #1: loss = 0.0305935 (* 1 = 0.0305935 loss)
I1107 15:49:36.083974 15588 sgd_solver.cpp:105] Iteration 133100, lr = 0.001
I1107 15:49:44.848615 15588 solver.cpp:218] Iteration 133200 (11.4101 iter/s, 8.76413s/100 iters), loss = 0.0305243
I1107 15:49:44.848615 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:49:44.848615 15588 solver.cpp:237]     Train net output #1: loss = 0.0305245 (* 1 = 0.0305245 loss)
I1107 15:49:44.848615 15588 sgd_solver.cpp:105] Iteration 133200, lr = 0.001
I1107 15:49:53.418526 15588 solver.cpp:218] Iteration 133300 (11.6685 iter/s, 8.57008s/100 iters), loss = 0.0214126
I1107 15:49:53.419526 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:49:53.419526 15588 solver.cpp:237]     Train net output #1: loss = 0.0214128 (* 1 = 0.0214128 loss)
I1107 15:49:53.419526 15588 sgd_solver.cpp:105] Iteration 133300, lr = 0.001
I1107 15:50:02.051260 15588 solver.cpp:218] Iteration 133400 (11.5859 iter/s, 8.63121s/100 iters), loss = 0.0309557
I1107 15:50:02.051260 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:50:02.051260 15588 solver.cpp:237]     Train net output #1: loss = 0.0309559 (* 1 = 0.0309559 loss)
I1107 15:50:02.051260 15588 sgd_solver.cpp:105] Iteration 133400, lr = 0.001
I1107 15:50:10.143975 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:50:10.479007 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_133500.caffemodel
I1107 15:50:10.516010 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_133500.solverstate
I1107 15:50:10.525009 15588 solver.cpp:330] Iteration 133500, Testing net (#0)
I1107 15:50:10.525009 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:50:12.513130  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:50:12.592134 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9176
I1107 15:50:12.592134 15588 solver.cpp:397]     Test net output #1: loss = 0.314351 (* 1 = 0.314351 loss)
I1107 15:50:12.678635 15588 solver.cpp:218] Iteration 133500 (9.40988 iter/s, 10.6271s/100 iters), loss = 0.0272072
I1107 15:50:12.678635 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:50:12.678635 15588 solver.cpp:237]     Train net output #1: loss = 0.0272073 (* 1 = 0.0272073 loss)
I1107 15:50:12.678635 15588 sgd_solver.cpp:105] Iteration 133500, lr = 0.001
I1107 15:50:21.227624 15588 solver.cpp:218] Iteration 133600 (11.6983 iter/s, 8.54824s/100 iters), loss = 0.0279013
I1107 15:50:21.227624 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:50:21.227624 15588 solver.cpp:237]     Train net output #1: loss = 0.0279015 (* 1 = 0.0279015 loss)
I1107 15:50:21.227624 15588 sgd_solver.cpp:105] Iteration 133600, lr = 0.001
I1107 15:50:29.802597 15588 solver.cpp:218] Iteration 133700 (11.662 iter/s, 8.57489s/100 iters), loss = 0.018965
I1107 15:50:29.802597 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:50:29.802597 15588 solver.cpp:237]     Train net output #1: loss = 0.0189652 (* 1 = 0.0189652 loss)
I1107 15:50:29.802597 15588 sgd_solver.cpp:105] Iteration 133700, lr = 0.001
I1107 15:50:38.400504 15588 solver.cpp:218] Iteration 133800 (11.6313 iter/s, 8.59748s/100 iters), loss = 0.0327348
I1107 15:50:38.400504 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:50:38.400504 15588 solver.cpp:237]     Train net output #1: loss = 0.032735 (* 1 = 0.032735 loss)
I1107 15:50:38.400504 15588 sgd_solver.cpp:105] Iteration 133800, lr = 0.001
I1107 15:50:47.165570 15588 solver.cpp:218] Iteration 133900 (11.4106 iter/s, 8.76381s/100 iters), loss = 0.0312288
I1107 15:50:47.165570 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:50:47.165570 15588 solver.cpp:237]     Train net output #1: loss = 0.031229 (* 1 = 0.031229 loss)
I1107 15:50:47.165570 15588 sgd_solver.cpp:105] Iteration 133900, lr = 0.001
I1107 15:50:55.421514 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:50:55.773530 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_134000.caffemodel
I1107 15:50:55.812533 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_134000.solverstate
I1107 15:50:55.821533 15588 solver.cpp:330] Iteration 134000, Testing net (#0)
I1107 15:50:55.821533 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:50:57.808753  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:50:57.888757 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9166
I1107 15:50:57.888757 15588 solver.cpp:397]     Test net output #1: loss = 0.315168 (* 1 = 0.315168 loss)
I1107 15:50:57.969758 15588 solver.cpp:218] Iteration 134000 (9.25605 iter/s, 10.8037s/100 iters), loss = 0.0389453
I1107 15:50:57.969758 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:50:57.969758 15588 solver.cpp:237]     Train net output #1: loss = 0.0389455 (* 1 = 0.0389455 loss)
I1107 15:50:57.969758 15588 sgd_solver.cpp:105] Iteration 134000, lr = 0.001
I1107 15:51:06.559600 15588 solver.cpp:218] Iteration 134100 (11.6418 iter/s, 8.58973s/100 iters), loss = 0.0354386
I1107 15:51:06.559600 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:51:06.559600 15588 solver.cpp:237]     Train net output #1: loss = 0.0354388 (* 1 = 0.0354388 loss)
I1107 15:51:06.559600 15588 sgd_solver.cpp:105] Iteration 134100, lr = 0.001
I1107 15:51:15.214946 15588 solver.cpp:218] Iteration 134200 (11.555 iter/s, 8.65424s/100 iters), loss = 0.0222064
I1107 15:51:15.214946 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:51:15.214946 15588 solver.cpp:237]     Train net output #1: loss = 0.0222066 (* 1 = 0.0222066 loss)
I1107 15:51:15.214946 15588 sgd_solver.cpp:105] Iteration 134200, lr = 0.001
I1107 15:51:23.713646 15588 solver.cpp:218] Iteration 134300 (11.7661 iter/s, 8.499s/100 iters), loss = 0.0247245
I1107 15:51:23.714648 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:51:23.714648 15588 solver.cpp:237]     Train net output #1: loss = 0.0247246 (* 1 = 0.0247246 loss)
I1107 15:51:23.714648 15588 sgd_solver.cpp:105] Iteration 134300, lr = 0.001
I1107 15:51:32.217070 15588 solver.cpp:218] Iteration 134400 (11.761 iter/s, 8.50266s/100 iters), loss = 0.0374408
I1107 15:51:32.217070 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:51:32.217070 15588 solver.cpp:237]     Train net output #1: loss = 0.037441 (* 1 = 0.037441 loss)
I1107 15:51:32.217070 15588 sgd_solver.cpp:105] Iteration 134400, lr = 0.001
I1107 15:51:40.297317 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:51:40.635408 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_134500.caffemodel
I1107 15:51:40.670405 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_134500.solverstate
I1107 15:51:40.679414 15588 solver.cpp:330] Iteration 134500, Testing net (#0)
I1107 15:51:40.679414 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:51:42.664400  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:51:42.743940 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9188
I1107 15:51:42.743940 15588 solver.cpp:397]     Test net output #1: loss = 0.31592 (* 1 = 0.31592 loss)
I1107 15:51:42.824976 15588 solver.cpp:218] Iteration 134500 (9.42734 iter/s, 10.6074s/100 iters), loss = 0.0254112
I1107 15:51:42.824976 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:51:42.824976 15588 solver.cpp:237]     Train net output #1: loss = 0.0254114 (* 1 = 0.0254114 loss)
I1107 15:51:42.824976 15588 sgd_solver.cpp:105] Iteration 134500, lr = 0.001
I1107 15:51:51.334247 15588 solver.cpp:218] Iteration 134600 (11.7529 iter/s, 8.5085s/100 iters), loss = 0.0630168
I1107 15:51:51.334247 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:51:51.334247 15588 solver.cpp:237]     Train net output #1: loss = 0.063017 (* 1 = 0.063017 loss)
I1107 15:51:51.334247 15588 sgd_solver.cpp:105] Iteration 134600, lr = 0.001
I1107 15:51:59.830930 15588 solver.cpp:218] Iteration 134700 (11.7708 iter/s, 8.49563s/100 iters), loss = 0.0248934
I1107 15:51:59.830930 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:51:59.830930 15588 solver.cpp:237]     Train net output #1: loss = 0.0248936 (* 1 = 0.0248936 loss)
I1107 15:51:59.830930 15588 sgd_solver.cpp:105] Iteration 134700, lr = 0.001
I1107 15:52:08.323758 15588 solver.cpp:218] Iteration 134800 (11.7748 iter/s, 8.49274s/100 iters), loss = 0.0321766
I1107 15:52:08.323758 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:52:08.323758 15588 solver.cpp:237]     Train net output #1: loss = 0.0321768 (* 1 = 0.0321768 loss)
I1107 15:52:08.323758 15588 sgd_solver.cpp:105] Iteration 134800, lr = 0.001
I1107 15:52:16.924506 15588 solver.cpp:218] Iteration 134900 (11.628 iter/s, 8.59991s/100 iters), loss = 0.0312876
I1107 15:52:16.924506 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:52:16.924506 15588 solver.cpp:237]     Train net output #1: loss = 0.0312878 (* 1 = 0.0312878 loss)
I1107 15:52:16.924506 15588 sgd_solver.cpp:105] Iteration 134900, lr = 0.001
I1107 15:52:25.164754 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:52:25.501408 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_135000.caffemodel
I1107 15:52:25.538913 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_135000.solverstate
I1107 15:52:25.548913 15588 solver.cpp:330] Iteration 135000, Testing net (#0)
I1107 15:52:25.548913 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:52:27.546058  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:52:27.625088 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 15:52:27.625088 15588 solver.cpp:397]     Test net output #1: loss = 0.315183 (* 1 = 0.315183 loss)
I1107 15:52:27.707079 15588 solver.cpp:218] Iteration 135000 (9.27467 iter/s, 10.7821s/100 iters), loss = 0.0312934
I1107 15:52:27.707079 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:52:27.707581 15588 solver.cpp:237]     Train net output #1: loss = 0.0312936 (* 1 = 0.0312936 loss)
I1107 15:52:27.707581 15588 sgd_solver.cpp:105] Iteration 135000, lr = 0.001
I1107 15:52:36.210750 15588 solver.cpp:218] Iteration 135100 (11.7605 iter/s, 8.50302s/100 iters), loss = 0.0295119
I1107 15:52:36.210750 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:52:36.210750 15588 solver.cpp:237]     Train net output #1: loss = 0.0295121 (* 1 = 0.0295121 loss)
I1107 15:52:36.210750 15588 sgd_solver.cpp:105] Iteration 135100, lr = 0.001
I1107 15:52:44.708009 15588 solver.cpp:218] Iteration 135200 (11.769 iter/s, 8.49687s/100 iters), loss = 0.0234074
I1107 15:52:44.708511 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:52:44.708511 15588 solver.cpp:237]     Train net output #1: loss = 0.0234075 (* 1 = 0.0234075 loss)
I1107 15:52:44.708511 15588 sgd_solver.cpp:105] Iteration 135200, lr = 0.001
I1107 15:52:53.227252 15588 solver.cpp:218] Iteration 135300 (11.7393 iter/s, 8.51839s/100 iters), loss = 0.019046
I1107 15:52:53.227252 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:52:53.227252 15588 solver.cpp:237]     Train net output #1: loss = 0.0190461 (* 1 = 0.0190461 loss)
I1107 15:52:53.227252 15588 sgd_solver.cpp:105] Iteration 135300, lr = 0.001
I1107 15:53:01.983886 15588 solver.cpp:218] Iteration 135400 (11.4201 iter/s, 8.75647s/100 iters), loss = 0.0274338
I1107 15:53:01.983886 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:53:01.983886 15588 solver.cpp:237]     Train net output #1: loss = 0.027434 (* 1 = 0.027434 loss)
I1107 15:53:01.983886 15588 sgd_solver.cpp:105] Iteration 135400, lr = 0.001
I1107 15:53:10.185333 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:53:10.539394 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_135500.caffemodel
I1107 15:53:10.575397 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_135500.solverstate
I1107 15:53:10.585394 15588 solver.cpp:330] Iteration 135500, Testing net (#0)
I1107 15:53:10.585394 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:53:12.594060  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:53:12.675066 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1107 15:53:12.675066 15588 solver.cpp:397]     Test net output #1: loss = 0.310343 (* 1 = 0.310343 loss)
I1107 15:53:12.758075 15588 solver.cpp:218] Iteration 135500 (9.28185 iter/s, 10.7737s/100 iters), loss = 0.0304416
I1107 15:53:12.758075 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:53:12.758075 15588 solver.cpp:237]     Train net output #1: loss = 0.0304417 (* 1 = 0.0304417 loss)
I1107 15:53:12.758075 15588 sgd_solver.cpp:105] Iteration 135500, lr = 0.001
I1107 15:53:21.303880 15588 solver.cpp:218] Iteration 135600 (11.7029 iter/s, 8.54492s/100 iters), loss = 0.0300989
I1107 15:53:21.303880 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:53:21.303880 15588 solver.cpp:237]     Train net output #1: loss = 0.030099 (* 1 = 0.030099 loss)
I1107 15:53:21.303880 15588 sgd_solver.cpp:105] Iteration 135600, lr = 0.001
I1107 15:53:29.978715 15588 solver.cpp:218] Iteration 135700 (11.5287 iter/s, 8.67401s/100 iters), loss = 0.0277899
I1107 15:53:29.978715 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:53:29.978715 15588 solver.cpp:237]     Train net output #1: loss = 0.0277901 (* 1 = 0.0277901 loss)
I1107 15:53:29.978715 15588 sgd_solver.cpp:105] Iteration 135700, lr = 0.001
I1107 15:53:38.687047 15588 solver.cpp:218] Iteration 135800 (11.4838 iter/s, 8.70789s/100 iters), loss = 0.0222823
I1107 15:53:38.687047 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:53:38.687047 15588 solver.cpp:237]     Train net output #1: loss = 0.0222825 (* 1 = 0.0222825 loss)
I1107 15:53:38.687047 15588 sgd_solver.cpp:105] Iteration 135800, lr = 0.001
I1107 15:53:47.309279 15588 solver.cpp:218] Iteration 135900 (11.5987 iter/s, 8.62167s/100 iters), loss = 0.0262854
I1107 15:53:47.309279 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:53:47.309279 15588 solver.cpp:237]     Train net output #1: loss = 0.0262856 (* 1 = 0.0262856 loss)
I1107 15:53:47.309279 15588 sgd_solver.cpp:105] Iteration 135900, lr = 0.001
I1107 15:53:55.583842 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:53:55.930883 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_136000.caffemodel
I1107 15:53:55.966883 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_136000.solverstate
I1107 15:53:55.976882 15588 solver.cpp:330] Iteration 136000, Testing net (#0)
I1107 15:53:55.976882 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:53:58.019688  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:53:58.101191 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9157
I1107 15:53:58.101191 15588 solver.cpp:397]     Test net output #1: loss = 0.317129 (* 1 = 0.317129 loss)
I1107 15:53:58.185204 15588 solver.cpp:218] Iteration 136000 (9.19475 iter/s, 10.8758s/100 iters), loss = 0.0286372
I1107 15:53:58.185204 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:53:58.185204 15588 solver.cpp:237]     Train net output #1: loss = 0.0286374 (* 1 = 0.0286374 loss)
I1107 15:53:58.185204 15588 sgd_solver.cpp:105] Iteration 136000, lr = 0.001
I1107 15:54:06.871613 15588 solver.cpp:218] Iteration 136100 (11.5131 iter/s, 8.68578s/100 iters), loss = 0.0262685
I1107 15:54:06.871613 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:54:06.871613 15588 solver.cpp:237]     Train net output #1: loss = 0.0262686 (* 1 = 0.0262686 loss)
I1107 15:54:06.871613 15588 sgd_solver.cpp:105] Iteration 136100, lr = 0.001
I1107 15:54:15.575307 15588 solver.cpp:218] Iteration 136200 (11.4895 iter/s, 8.70357s/100 iters), loss = 0.0214369
I1107 15:54:15.575307 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:54:15.575307 15588 solver.cpp:237]     Train net output #1: loss = 0.0214371 (* 1 = 0.0214371 loss)
I1107 15:54:15.575307 15588 sgd_solver.cpp:105] Iteration 136200, lr = 0.001
I1107 15:54:24.233312 15588 solver.cpp:218] Iteration 136300 (11.5511 iter/s, 8.6572s/100 iters), loss = 0.022543
I1107 15:54:24.233312 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:54:24.233312 15588 solver.cpp:237]     Train net output #1: loss = 0.0225432 (* 1 = 0.0225432 loss)
I1107 15:54:24.233312 15588 sgd_solver.cpp:105] Iteration 136300, lr = 0.001
I1107 15:54:32.958220 15588 solver.cpp:218] Iteration 136400 (11.4628 iter/s, 8.72387s/100 iters), loss = 0.0259197
I1107 15:54:32.958220 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:54:32.958220 15588 solver.cpp:237]     Train net output #1: loss = 0.0259198 (* 1 = 0.0259198 loss)
I1107 15:54:32.958220 15588 sgd_solver.cpp:105] Iteration 136400, lr = 0.001
I1107 15:54:41.165987 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:54:41.503088 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_136500.caffemodel
I1107 15:54:41.540097 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_136500.solverstate
I1107 15:54:41.549098 15588 solver.cpp:330] Iteration 136500, Testing net (#0)
I1107 15:54:41.549098 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:54:43.583416  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:54:43.666425 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9171
I1107 15:54:43.666425 15588 solver.cpp:397]     Test net output #1: loss = 0.323833 (* 1 = 0.323833 loss)
I1107 15:54:43.749426 15588 solver.cpp:218] Iteration 136500 (9.2671 iter/s, 10.7909s/100 iters), loss = 0.0322958
I1107 15:54:43.749426 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:54:43.749426 15588 solver.cpp:237]     Train net output #1: loss = 0.032296 (* 1 = 0.032296 loss)
I1107 15:54:43.749426 15588 sgd_solver.cpp:105] Iteration 136500, lr = 0.001
I1107 15:54:52.422868 15588 solver.cpp:218] Iteration 136600 (11.531 iter/s, 8.6723s/100 iters), loss = 0.0305893
I1107 15:54:52.422868 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:54:52.422868 15588 solver.cpp:237]     Train net output #1: loss = 0.0305895 (* 1 = 0.0305895 loss)
I1107 15:54:52.422868 15588 sgd_solver.cpp:105] Iteration 136600, lr = 0.001
I1107 15:55:01.116878 15588 solver.cpp:218] Iteration 136700 (11.5029 iter/s, 8.69343s/100 iters), loss = 0.0319085
I1107 15:55:01.116878 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:55:01.116878 15588 solver.cpp:237]     Train net output #1: loss = 0.0319086 (* 1 = 0.0319086 loss)
I1107 15:55:01.116878 15588 sgd_solver.cpp:105] Iteration 136700, lr = 0.001
I1107 15:55:09.825037 15588 solver.cpp:218] Iteration 136800 (11.4833 iter/s, 8.70827s/100 iters), loss = 0.0239858
I1107 15:55:09.825037 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:55:09.825037 15588 solver.cpp:237]     Train net output #1: loss = 0.023986 (* 1 = 0.023986 loss)
I1107 15:55:09.825037 15588 sgd_solver.cpp:105] Iteration 136800, lr = 0.001
I1107 15:55:18.555968 15588 solver.cpp:218] Iteration 136900 (11.4543 iter/s, 8.73032s/100 iters), loss = 0.0305155
I1107 15:55:18.555968 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:55:18.555968 15588 solver.cpp:237]     Train net output #1: loss = 0.0305156 (* 1 = 0.0305156 loss)
I1107 15:55:18.555968 15588 sgd_solver.cpp:105] Iteration 136900, lr = 0.001
I1107 15:55:26.855559 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:55:27.203598 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_137000.caffemodel
I1107 15:55:27.242586 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_137000.solverstate
I1107 15:55:27.251585 15588 solver.cpp:330] Iteration 137000, Testing net (#0)
I1107 15:55:27.251585 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:55:29.265732  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:55:29.345736 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 15:55:29.345736 15588 solver.cpp:397]     Test net output #1: loss = 0.314205 (* 1 = 0.314205 loss)
I1107 15:55:29.426740 15588 solver.cpp:218] Iteration 137000 (9.19949 iter/s, 10.8702s/100 iters), loss = 0.0318222
I1107 15:55:29.426740 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:55:29.426740 15588 solver.cpp:237]     Train net output #1: loss = 0.0318224 (* 1 = 0.0318224 loss)
I1107 15:55:29.426740 15588 sgd_solver.cpp:105] Iteration 137000, lr = 0.001
I1107 15:55:38.092514 15588 solver.cpp:218] Iteration 137100 (11.5409 iter/s, 8.66481s/100 iters), loss = 0.0312401
I1107 15:55:38.092514 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:55:38.093015 15588 solver.cpp:237]     Train net output #1: loss = 0.0312402 (* 1 = 0.0312402 loss)
I1107 15:55:38.093015 15588 sgd_solver.cpp:105] Iteration 137100, lr = 0.001
I1107 15:55:46.760423 15588 solver.cpp:218] Iteration 137200 (11.5375 iter/s, 8.66737s/100 iters), loss = 0.0237691
I1107 15:55:46.760423 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:55:46.760423 15588 solver.cpp:237]     Train net output #1: loss = 0.0237692 (* 1 = 0.0237692 loss)
I1107 15:55:46.760423 15588 sgd_solver.cpp:105] Iteration 137200, lr = 0.001
I1107 15:55:55.427371 15588 solver.cpp:218] Iteration 137300 (11.5389 iter/s, 8.66637s/100 iters), loss = 0.0175502
I1107 15:55:55.427371 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:55:55.427371 15588 solver.cpp:237]     Train net output #1: loss = 0.0175503 (* 1 = 0.0175503 loss)
I1107 15:55:55.427371 15588 sgd_solver.cpp:105] Iteration 137300, lr = 0.001
I1107 15:56:04.161502 15588 solver.cpp:218] Iteration 137400 (11.4502 iter/s, 8.73345s/100 iters), loss = 0.057
I1107 15:56:04.161502 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:56:04.161502 15588 solver.cpp:237]     Train net output #1: loss = 0.0570002 (* 1 = 0.0570002 loss)
I1107 15:56:04.161502 15588 sgd_solver.cpp:105] Iteration 137400, lr = 0.001
I1107 15:56:12.458890 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:56:12.807500 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_137500.caffemodel
I1107 15:56:12.840500 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_137500.solverstate
I1107 15:56:12.850004 15588 solver.cpp:330] Iteration 137500, Testing net (#0)
I1107 15:56:12.850004 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:56:14.861857  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:56:14.941359 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 15:56:14.941359 15588 solver.cpp:397]     Test net output #1: loss = 0.318273 (* 1 = 0.318273 loss)
I1107 15:56:15.022362 15588 solver.cpp:218] Iteration 137500 (9.20777 iter/s, 10.8604s/100 iters), loss = 0.0256481
I1107 15:56:15.022362 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:56:15.022362 15588 solver.cpp:237]     Train net output #1: loss = 0.0256482 (* 1 = 0.0256482 loss)
I1107 15:56:15.022362 15588 sgd_solver.cpp:105] Iteration 137500, lr = 0.001
I1107 15:56:23.675853 15588 solver.cpp:218] Iteration 137600 (11.5566 iter/s, 8.6531s/100 iters), loss = 0.0256913
I1107 15:56:23.675853 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:56:23.675853 15588 solver.cpp:237]     Train net output #1: loss = 0.0256915 (* 1 = 0.0256915 loss)
I1107 15:56:23.675853 15588 sgd_solver.cpp:105] Iteration 137600, lr = 0.001
I1107 15:56:32.386653 15588 solver.cpp:218] Iteration 137700 (11.4806 iter/s, 8.71033s/100 iters), loss = 0.0248289
I1107 15:56:32.386653 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:56:32.386653 15588 solver.cpp:237]     Train net output #1: loss = 0.024829 (* 1 = 0.024829 loss)
I1107 15:56:32.386653 15588 sgd_solver.cpp:105] Iteration 137700, lr = 0.001
I1107 15:56:41.074517 15588 solver.cpp:218] Iteration 137800 (11.5117 iter/s, 8.68678s/100 iters), loss = 0.0293291
I1107 15:56:41.074517 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:56:41.074517 15588 solver.cpp:237]     Train net output #1: loss = 0.0293292 (* 1 = 0.0293292 loss)
I1107 15:56:41.074517 15588 sgd_solver.cpp:105] Iteration 137800, lr = 0.001
I1107 15:56:49.731932 15588 solver.cpp:218] Iteration 137900 (11.5511 iter/s, 8.65719s/100 iters), loss = 0.0249329
I1107 15:56:49.731932 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:56:49.731932 15588 solver.cpp:237]     Train net output #1: loss = 0.024933 (* 1 = 0.024933 loss)
I1107 15:56:49.731932 15588 sgd_solver.cpp:105] Iteration 137900, lr = 0.001
I1107 15:56:58.025231 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:56:58.375262 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_138000.caffemodel
I1107 15:56:58.407263 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_138000.solverstate
I1107 15:56:58.417263 15588 solver.cpp:330] Iteration 138000, Testing net (#0)
I1107 15:56:58.417263 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:57:00.414417  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:57:00.494423 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9174
I1107 15:57:00.494423 15588 solver.cpp:397]     Test net output #1: loss = 0.312438 (* 1 = 0.312438 loss)
I1107 15:57:00.574426 15588 solver.cpp:218] Iteration 138000 (9.22311 iter/s, 10.8423s/100 iters), loss = 0.0400143
I1107 15:57:00.574426 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:57:00.575428 15588 solver.cpp:237]     Train net output #1: loss = 0.0400144 (* 1 = 0.0400144 loss)
I1107 15:57:00.575428 15588 sgd_solver.cpp:105] Iteration 138000, lr = 0.001
I1107 15:57:09.248565 15588 solver.cpp:218] Iteration 138100 (11.5303 iter/s, 8.6728s/100 iters), loss = 0.0214686
I1107 15:57:09.248565 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:57:09.248565 15588 solver.cpp:237]     Train net output #1: loss = 0.0214687 (* 1 = 0.0214687 loss)
I1107 15:57:09.248565 15588 sgd_solver.cpp:105] Iteration 138100, lr = 0.001
I1107 15:57:17.976073 15588 solver.cpp:218] Iteration 138200 (11.4586 iter/s, 8.72708s/100 iters), loss = 0.037602
I1107 15:57:17.976073 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:57:17.976073 15588 solver.cpp:237]     Train net output #1: loss = 0.0376022 (* 1 = 0.0376022 loss)
I1107 15:57:17.976073 15588 sgd_solver.cpp:105] Iteration 138200, lr = 0.001
I1107 15:57:26.667573 15588 solver.cpp:218] Iteration 138300 (11.506 iter/s, 8.69111s/100 iters), loss = 0.0353019
I1107 15:57:26.667573 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:57:26.668061 15588 solver.cpp:237]     Train net output #1: loss = 0.035302 (* 1 = 0.035302 loss)
I1107 15:57:26.668061 15588 sgd_solver.cpp:105] Iteration 138300, lr = 0.001
I1107 15:57:35.378770 15588 solver.cpp:218] Iteration 138400 (11.4799 iter/s, 8.71086s/100 iters), loss = 0.0578935
I1107 15:57:35.378770 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 15:57:35.378770 15588 solver.cpp:237]     Train net output #1: loss = 0.0578937 (* 1 = 0.0578937 loss)
I1107 15:57:35.378770 15588 sgd_solver.cpp:105] Iteration 138400, lr = 0.001
I1107 15:57:43.606652 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:57:43.942692 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_138500.caffemodel
I1107 15:57:43.970691 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_138500.solverstate
I1107 15:57:43.978691 15588 solver.cpp:330] Iteration 138500, Testing net (#0)
I1107 15:57:43.979691 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:57:46.019690  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:57:46.101200 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1107 15:57:46.101200 15588 solver.cpp:397]     Test net output #1: loss = 0.310499 (* 1 = 0.310499 loss)
I1107 15:57:46.184216 15588 solver.cpp:218] Iteration 138500 (9.25523 iter/s, 10.8047s/100 iters), loss = 0.0291362
I1107 15:57:46.184216 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:57:46.184216 15588 solver.cpp:237]     Train net output #1: loss = 0.0291364 (* 1 = 0.0291364 loss)
I1107 15:57:46.184216 15588 sgd_solver.cpp:105] Iteration 138500, lr = 0.001
I1107 15:57:54.861323 15588 solver.cpp:218] Iteration 138600 (11.5253 iter/s, 8.67659s/100 iters), loss = 0.0464308
I1107 15:57:54.861323 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:57:54.861323 15588 solver.cpp:237]     Train net output #1: loss = 0.0464309 (* 1 = 0.0464309 loss)
I1107 15:57:54.861323 15588 sgd_solver.cpp:105] Iteration 138600, lr = 0.001
I1107 15:58:03.521764 15588 solver.cpp:218] Iteration 138700 (11.5476 iter/s, 8.65982s/100 iters), loss = 0.0248215
I1107 15:58:03.522264 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:58:03.522264 15588 solver.cpp:237]     Train net output #1: loss = 0.0248216 (* 1 = 0.0248216 loss)
I1107 15:58:03.522264 15588 sgd_solver.cpp:105] Iteration 138700, lr = 0.001
I1107 15:58:12.208875 15588 solver.cpp:218] Iteration 138800 (11.5124 iter/s, 8.68625s/100 iters), loss = 0.0217777
I1107 15:58:12.208875 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:58:12.208875 15588 solver.cpp:237]     Train net output #1: loss = 0.0217779 (* 1 = 0.0217779 loss)
I1107 15:58:12.208875 15588 sgd_solver.cpp:105] Iteration 138800, lr = 0.001
I1107 15:58:20.890776 15588 solver.cpp:218] Iteration 138900 (11.5188 iter/s, 8.68143s/100 iters), loss = 0.0217841
I1107 15:58:20.890776 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:58:20.890776 15588 solver.cpp:237]     Train net output #1: loss = 0.0217843 (* 1 = 0.0217843 loss)
I1107 15:58:20.890776 15588 sgd_solver.cpp:105] Iteration 138900, lr = 0.001
I1107 15:58:29.168190 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:58:29.504218 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_139000.caffemodel
I1107 15:58:29.541224 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_139000.solverstate
I1107 15:58:29.550225 15588 solver.cpp:330] Iteration 139000, Testing net (#0)
I1107 15:58:29.550225 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:58:31.581367  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:58:31.663372 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9175
I1107 15:58:31.663372 15588 solver.cpp:397]     Test net output #1: loss = 0.322784 (* 1 = 0.322784 loss)
I1107 15:58:31.748404 15588 solver.cpp:218] Iteration 139000 (9.21082 iter/s, 10.8568s/100 iters), loss = 0.0266496
I1107 15:58:31.748404 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:58:31.748404 15588 solver.cpp:237]     Train net output #1: loss = 0.0266498 (* 1 = 0.0266498 loss)
I1107 15:58:31.748404 15588 sgd_solver.cpp:105] Iteration 139000, lr = 0.001
I1107 15:58:40.467651 15588 solver.cpp:218] Iteration 139100 (11.4698 iter/s, 8.71859s/100 iters), loss = 0.0254083
I1107 15:58:40.467651 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:58:40.467651 15588 solver.cpp:237]     Train net output #1: loss = 0.0254085 (* 1 = 0.0254085 loss)
I1107 15:58:40.467651 15588 sgd_solver.cpp:105] Iteration 139100, lr = 0.001
I1107 15:58:49.147577 15588 solver.cpp:218] Iteration 139200 (11.5208 iter/s, 8.67997s/100 iters), loss = 0.0454433
I1107 15:58:49.147577 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:58:49.147577 15588 solver.cpp:237]     Train net output #1: loss = 0.0454435 (* 1 = 0.0454435 loss)
I1107 15:58:49.147577 15588 sgd_solver.cpp:105] Iteration 139200, lr = 0.001
I1107 15:58:57.813416 15588 solver.cpp:218] Iteration 139300 (11.5403 iter/s, 8.66527s/100 iters), loss = 0.0236142
I1107 15:58:57.813416 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:58:57.813416 15588 solver.cpp:237]     Train net output #1: loss = 0.0236144 (* 1 = 0.0236144 loss)
I1107 15:58:57.813416 15588 sgd_solver.cpp:105] Iteration 139300, lr = 0.001
I1107 15:59:06.509451 15588 solver.cpp:218] Iteration 139400 (11.5006 iter/s, 8.69517s/100 iters), loss = 0.0278988
I1107 15:59:06.509451 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:59:06.509451 15588 solver.cpp:237]     Train net output #1: loss = 0.027899 (* 1 = 0.027899 loss)
I1107 15:59:06.509451 15588 sgd_solver.cpp:105] Iteration 139400, lr = 0.001
I1107 15:59:14.748329 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:59:15.091346 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_139500.caffemodel
I1107 15:59:15.125852 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_139500.solverstate
I1107 15:59:15.135351 15588 solver.cpp:330] Iteration 139500, Testing net (#0)
I1107 15:59:15.135351 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 15:59:17.175843  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 15:59:17.256850 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9173
I1107 15:59:17.256850 15588 solver.cpp:397]     Test net output #1: loss = 0.315488 (* 1 = 0.315488 loss)
I1107 15:59:17.339895 15588 solver.cpp:218] Iteration 139500 (9.23324 iter/s, 10.8304s/100 iters), loss = 0.0294606
I1107 15:59:17.340878 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:59:17.340878 15588 solver.cpp:237]     Train net output #1: loss = 0.0294608 (* 1 = 0.0294608 loss)
I1107 15:59:17.340878 15588 sgd_solver.cpp:105] Iteration 139500, lr = 0.001
I1107 15:59:26.063988 15588 solver.cpp:218] Iteration 139600 (11.4644 iter/s, 8.72262s/100 iters), loss = 0.0282141
I1107 15:59:26.063988 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:59:26.063988 15588 solver.cpp:237]     Train net output #1: loss = 0.0282143 (* 1 = 0.0282143 loss)
I1107 15:59:26.063988 15588 sgd_solver.cpp:105] Iteration 139600, lr = 0.001
I1107 15:59:34.761760 15588 solver.cpp:218] Iteration 139700 (11.498 iter/s, 8.69717s/100 iters), loss = 0.0208795
I1107 15:59:34.761760 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:59:34.761760 15588 solver.cpp:237]     Train net output #1: loss = 0.0208797 (* 1 = 0.0208797 loss)
I1107 15:59:34.761760 15588 sgd_solver.cpp:105] Iteration 139700, lr = 0.001
I1107 15:59:43.406451 15588 solver.cpp:218] Iteration 139800 (11.5686 iter/s, 8.64406s/100 iters), loss = 0.0337149
I1107 15:59:43.406451 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 15:59:43.406451 15588 solver.cpp:237]     Train net output #1: loss = 0.033715 (* 1 = 0.033715 loss)
I1107 15:59:43.406451 15588 sgd_solver.cpp:105] Iteration 139800, lr = 0.001
I1107 15:59:52.129742 15588 solver.cpp:218] Iteration 139900 (11.4639 iter/s, 8.72303s/100 iters), loss = 0.0302416
I1107 15:59:52.129742 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 15:59:52.129742 15588 solver.cpp:237]     Train net output #1: loss = 0.0302418 (* 1 = 0.0302418 loss)
I1107 15:59:52.129742 15588 sgd_solver.cpp:105] Iteration 139900, lr = 0.001
I1107 16:00:00.419905 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:00:00.764940 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_140000.caffemodel
I1107 16:00:00.802944 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_140000.solverstate
I1107 16:00:00.812945 15588 solver.cpp:330] Iteration 140000, Testing net (#0)
I1107 16:00:00.812945 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:00:02.838116  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:00:02.919121 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 16:00:02.919121 15588 solver.cpp:397]     Test net output #1: loss = 0.313724 (* 1 = 0.313724 loss)
I1107 16:00:03.001138 15588 solver.cpp:218] Iteration 140000 (9.19929 iter/s, 10.8704s/100 iters), loss = 0.0264123
I1107 16:00:03.001138 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:00:03.001138 15588 solver.cpp:237]     Train net output #1: loss = 0.0264125 (* 1 = 0.0264125 loss)
I1107 16:00:03.001138 15588 sgd_solver.cpp:105] Iteration 140000, lr = 0.001
I1107 16:00:11.622067 15588 solver.cpp:218] Iteration 140100 (11.5997 iter/s, 8.62092s/100 iters), loss = 0.0286009
I1107 16:00:11.622067 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:00:11.622067 15588 solver.cpp:237]     Train net output #1: loss = 0.0286011 (* 1 = 0.0286011 loss)
I1107 16:00:11.622067 15588 sgd_solver.cpp:105] Iteration 140100, lr = 0.001
I1107 16:00:20.201597 15588 solver.cpp:218] Iteration 140200 (11.6566 iter/s, 8.57882s/100 iters), loss = 0.0196948
I1107 16:00:20.201597 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:00:20.201597 15588 solver.cpp:237]     Train net output #1: loss = 0.019695 (* 1 = 0.019695 loss)
I1107 16:00:20.201597 15588 sgd_solver.cpp:105] Iteration 140200, lr = 0.001
I1107 16:00:28.783015 15588 solver.cpp:218] Iteration 140300 (11.6543 iter/s, 8.58054s/100 iters), loss = 0.0231642
I1107 16:00:28.783015 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:00:28.783015 15588 solver.cpp:237]     Train net output #1: loss = 0.0231644 (* 1 = 0.0231644 loss)
I1107 16:00:28.783015 15588 sgd_solver.cpp:105] Iteration 140300, lr = 0.001
I1107 16:00:37.376278 15588 solver.cpp:218] Iteration 140400 (11.6373 iter/s, 8.59303s/100 iters), loss = 0.0344174
I1107 16:00:37.376780 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:00:37.376780 15588 solver.cpp:237]     Train net output #1: loss = 0.0344176 (* 1 = 0.0344176 loss)
I1107 16:00:37.376780 15588 sgd_solver.cpp:105] Iteration 140400, lr = 0.001
I1107 16:00:45.594928 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:00:45.930960 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_140500.caffemodel
I1107 16:00:45.969964 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_140500.solverstate
I1107 16:00:45.978965 15588 solver.cpp:330] Iteration 140500, Testing net (#0)
I1107 16:00:45.978965 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:00:47.974625  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:00:48.054131 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 16:00:48.054131 15588 solver.cpp:397]     Test net output #1: loss = 0.310548 (* 1 = 0.310548 loss)
I1107 16:00:48.135138 15588 solver.cpp:218] Iteration 140500 (9.29554 iter/s, 10.7579s/100 iters), loss = 0.0300612
I1107 16:00:48.135138 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:00:48.135138 15588 solver.cpp:237]     Train net output #1: loss = 0.0300614 (* 1 = 0.0300614 loss)
I1107 16:00:48.135138 15588 sgd_solver.cpp:105] Iteration 140500, lr = 0.001
I1107 16:00:56.678203 15588 solver.cpp:218] Iteration 140600 (11.7066 iter/s, 8.54222s/100 iters), loss = 0.0260183
I1107 16:00:56.678203 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:00:56.678203 15588 solver.cpp:237]     Train net output #1: loss = 0.0260184 (* 1 = 0.0260184 loss)
I1107 16:00:56.678203 15588 sgd_solver.cpp:105] Iteration 140600, lr = 0.001
I1107 16:01:05.211434 15588 solver.cpp:218] Iteration 140700 (11.7193 iter/s, 8.53295s/100 iters), loss = 0.0315457
I1107 16:01:05.211434 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:01:05.211434 15588 solver.cpp:237]     Train net output #1: loss = 0.0315459 (* 1 = 0.0315459 loss)
I1107 16:01:05.211434 15588 sgd_solver.cpp:105] Iteration 140700, lr = 0.001
I1107 16:01:13.736286 15588 solver.cpp:218] Iteration 140800 (11.7308 iter/s, 8.52454s/100 iters), loss = 0.0244132
I1107 16:01:13.736286 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:01:13.736286 15588 solver.cpp:237]     Train net output #1: loss = 0.0244134 (* 1 = 0.0244134 loss)
I1107 16:01:13.736286 15588 sgd_solver.cpp:105] Iteration 140800, lr = 0.001
I1107 16:01:22.237853 15588 solver.cpp:218] Iteration 140900 (11.7637 iter/s, 8.50073s/100 iters), loss = 0.024409
I1107 16:01:22.237853 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:01:22.237853 15588 solver.cpp:237]     Train net output #1: loss = 0.0244092 (* 1 = 0.0244092 loss)
I1107 16:01:22.237853 15588 sgd_solver.cpp:105] Iteration 140900, lr = 0.001
I1107 16:01:30.330317 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:01:30.664363 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_141000.caffemodel
I1107 16:01:30.701367 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_141000.solverstate
I1107 16:01:30.711369 15588 solver.cpp:330] Iteration 141000, Testing net (#0)
I1107 16:01:30.711369 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:01:32.704579  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:01:32.783586 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1107 16:01:32.783586 15588 solver.cpp:397]     Test net output #1: loss = 0.316428 (* 1 = 0.316428 loss)
I1107 16:01:32.865592 15588 solver.cpp:218] Iteration 141000 (9.40978 iter/s, 10.6272s/100 iters), loss = 0.0303069
I1107 16:01:32.865592 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:01:32.865592 15588 solver.cpp:237]     Train net output #1: loss = 0.030307 (* 1 = 0.030307 loss)
I1107 16:01:32.865592 15588 sgd_solver.cpp:105] Iteration 141000, lr = 0.001
I1107 16:01:41.380220 15588 solver.cpp:218] Iteration 141100 (11.7444 iter/s, 8.51471s/100 iters), loss = 0.0526459
I1107 16:01:41.381214 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:01:41.381214 15588 solver.cpp:237]     Train net output #1: loss = 0.0526461 (* 1 = 0.0526461 loss)
I1107 16:01:41.381214 15588 sgd_solver.cpp:105] Iteration 141100, lr = 0.001
I1107 16:01:49.934620 15588 solver.cpp:218] Iteration 141200 (11.6918 iter/s, 8.55299s/100 iters), loss = 0.0199102
I1107 16:01:49.934620 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:01:49.934620 15588 solver.cpp:237]     Train net output #1: loss = 0.0199104 (* 1 = 0.0199104 loss)
I1107 16:01:49.934620 15588 sgd_solver.cpp:105] Iteration 141200, lr = 0.001
I1107 16:01:58.594828 15588 solver.cpp:218] Iteration 141300 (11.5471 iter/s, 8.66022s/100 iters), loss = 0.0398982
I1107 16:01:58.594828 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:01:58.594828 15588 solver.cpp:237]     Train net output #1: loss = 0.0398984 (* 1 = 0.0398984 loss)
I1107 16:01:58.594828 15588 sgd_solver.cpp:105] Iteration 141300, lr = 0.001
I1107 16:02:07.228545 15588 solver.cpp:218] Iteration 141400 (11.584 iter/s, 8.63259s/100 iters), loss = 0.0374566
I1107 16:02:07.228545 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:02:07.228545 15588 solver.cpp:237]     Train net output #1: loss = 0.0374568 (* 1 = 0.0374568 loss)
I1107 16:02:07.228545 15588 sgd_solver.cpp:105] Iteration 141400, lr = 0.001
I1107 16:02:15.461668 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:02:15.810191 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_141500.caffemodel
I1107 16:02:15.849192 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_141500.solverstate
I1107 16:02:15.859696 15588 solver.cpp:330] Iteration 141500, Testing net (#0)
I1107 16:02:15.860196 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:02:17.898401  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:02:17.979981 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9204
I1107 16:02:17.979981 15588 solver.cpp:397]     Test net output #1: loss = 0.307386 (* 1 = 0.307386 loss)
I1107 16:02:18.063482 15588 solver.cpp:218] Iteration 141500 (9.22983 iter/s, 10.8344s/100 iters), loss = 0.0291176
I1107 16:02:18.063482 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:02:18.063482 15588 solver.cpp:237]     Train net output #1: loss = 0.0291178 (* 1 = 0.0291178 loss)
I1107 16:02:18.063482 15588 sgd_solver.cpp:105] Iteration 141500, lr = 0.001
I1107 16:02:26.666617 15588 solver.cpp:218] Iteration 141600 (11.6244 iter/s, 8.60262s/100 iters), loss = 0.0249647
I1107 16:02:26.666617 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:02:26.666617 15588 solver.cpp:237]     Train net output #1: loss = 0.0249649 (* 1 = 0.0249649 loss)
I1107 16:02:26.666617 15588 sgd_solver.cpp:105] Iteration 141600, lr = 0.001
I1107 16:02:35.205348 15588 solver.cpp:218] Iteration 141700 (11.7122 iter/s, 8.53808s/100 iters), loss = 0.0183473
I1107 16:02:35.205348 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:02:35.205348 15588 solver.cpp:237]     Train net output #1: loss = 0.0183475 (* 1 = 0.0183475 loss)
I1107 16:02:35.205348 15588 sgd_solver.cpp:105] Iteration 141700, lr = 0.001
I1107 16:02:43.947927 15588 solver.cpp:218] Iteration 141800 (11.4391 iter/s, 8.74192s/100 iters), loss = 0.0196698
I1107 16:02:43.947927 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:02:43.947927 15588 solver.cpp:237]     Train net output #1: loss = 0.0196699 (* 1 = 0.0196699 loss)
I1107 16:02:43.947927 15588 sgd_solver.cpp:105] Iteration 141800, lr = 0.001
I1107 16:02:52.491901 15588 solver.cpp:218] Iteration 141900 (11.7045 iter/s, 8.54375s/100 iters), loss = 0.0281708
I1107 16:02:52.491901 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:02:52.491901 15588 solver.cpp:237]     Train net output #1: loss = 0.028171 (* 1 = 0.028171 loss)
I1107 16:02:52.491901 15588 sgd_solver.cpp:105] Iteration 141900, lr = 0.001
I1107 16:03:00.613368 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:03:00.951889 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_142000.caffemodel
I1107 16:03:00.986393 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_142000.solverstate
I1107 16:03:00.995393 15588 solver.cpp:330] Iteration 142000, Testing net (#0)
I1107 16:03:00.996393 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:03:02.981540  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:03:03.061558 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9189
I1107 16:03:03.061558 15588 solver.cpp:397]     Test net output #1: loss = 0.312746 (* 1 = 0.312746 loss)
I1107 16:03:03.142047 15588 solver.cpp:218] Iteration 142000 (9.3902 iter/s, 10.6494s/100 iters), loss = 0.0265263
I1107 16:03:03.142047 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:03:03.142047 15588 solver.cpp:237]     Train net output #1: loss = 0.0265265 (* 1 = 0.0265265 loss)
I1107 16:03:03.142047 15588 sgd_solver.cpp:105] Iteration 142000, lr = 0.001
I1107 16:03:11.667603 15588 solver.cpp:218] Iteration 142100 (11.7298 iter/s, 8.52526s/100 iters), loss = 0.0373154
I1107 16:03:11.667603 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:03:11.667603 15588 solver.cpp:237]     Train net output #1: loss = 0.0373156 (* 1 = 0.0373156 loss)
I1107 16:03:11.667603 15588 sgd_solver.cpp:105] Iteration 142100, lr = 0.001
I1107 16:03:20.370784 15588 solver.cpp:218] Iteration 142200 (11.4913 iter/s, 8.70223s/100 iters), loss = 0.0204782
I1107 16:03:20.370784 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:03:20.370784 15588 solver.cpp:237]     Train net output #1: loss = 0.0204783 (* 1 = 0.0204783 loss)
I1107 16:03:20.370784 15588 sgd_solver.cpp:105] Iteration 142200, lr = 0.001
I1107 16:03:28.985654 15588 solver.cpp:218] Iteration 142300 (11.6086 iter/s, 8.61432s/100 iters), loss = 0.0204947
I1107 16:03:28.985654 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:03:28.985654 15588 solver.cpp:237]     Train net output #1: loss = 0.0204949 (* 1 = 0.0204949 loss)
I1107 16:03:28.985654 15588 sgd_solver.cpp:105] Iteration 142300, lr = 0.001
I1107 16:03:37.549465 15588 solver.cpp:218] Iteration 142400 (11.6778 iter/s, 8.56329s/100 iters), loss = 0.0208629
I1107 16:03:37.549465 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:03:37.549465 15588 solver.cpp:237]     Train net output #1: loss = 0.0208631 (* 1 = 0.0208631 loss)
I1107 16:03:37.549465 15588 sgd_solver.cpp:105] Iteration 142400, lr = 0.001
I1107 16:03:45.850575 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:03:46.190608 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_142500.caffemodel
I1107 16:03:46.228610 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_142500.solverstate
I1107 16:03:46.238611 15588 solver.cpp:330] Iteration 142500, Testing net (#0)
I1107 16:03:46.238611 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:03:48.277833  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:03:48.359835 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 16:03:48.359835 15588 solver.cpp:397]     Test net output #1: loss = 0.312468 (* 1 = 0.312468 loss)
I1107 16:03:48.443857 15588 solver.cpp:218] Iteration 142500 (9.17934 iter/s, 10.894s/100 iters), loss = 0.0258333
I1107 16:03:48.443857 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:03:48.443857 15588 solver.cpp:237]     Train net output #1: loss = 0.0258335 (* 1 = 0.0258335 loss)
I1107 16:03:48.443857 15588 sgd_solver.cpp:105] Iteration 142500, lr = 0.001
I1107 16:03:57.145748 15588 solver.cpp:218] Iteration 142600 (11.493 iter/s, 8.70094s/100 iters), loss = 0.0437687
I1107 16:03:57.145748 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:03:57.145748 15588 solver.cpp:237]     Train net output #1: loss = 0.0437689 (* 1 = 0.0437689 loss)
I1107 16:03:57.145748 15588 sgd_solver.cpp:105] Iteration 142600, lr = 0.001
I1107 16:04:05.844547 15588 solver.cpp:218] Iteration 142700 (11.4965 iter/s, 8.69827s/100 iters), loss = 0.0282757
I1107 16:04:05.844547 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:04:05.844547 15588 solver.cpp:237]     Train net output #1: loss = 0.0282759 (* 1 = 0.0282759 loss)
I1107 16:04:05.844547 15588 sgd_solver.cpp:105] Iteration 142700, lr = 0.001
I1107 16:04:14.586943 15588 solver.cpp:218] Iteration 142800 (11.4383 iter/s, 8.74258s/100 iters), loss = 0.0333687
I1107 16:04:14.586943 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:04:14.586943 15588 solver.cpp:237]     Train net output #1: loss = 0.0333689 (* 1 = 0.0333689 loss)
I1107 16:04:14.586943 15588 sgd_solver.cpp:105] Iteration 142800, lr = 0.001
I1107 16:04:23.162451 15588 solver.cpp:218] Iteration 142900 (11.6627 iter/s, 8.57432s/100 iters), loss = 0.0281474
I1107 16:04:23.162451 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:04:23.162451 15588 solver.cpp:237]     Train net output #1: loss = 0.0281476 (* 1 = 0.0281476 loss)
I1107 16:04:23.162451 15588 sgd_solver.cpp:105] Iteration 142900, lr = 0.001
I1107 16:04:31.316208 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:04:31.654242 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_143000.caffemodel
I1107 16:04:31.690241 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_143000.solverstate
I1107 16:04:31.699241 15588 solver.cpp:330] Iteration 143000, Testing net (#0)
I1107 16:04:31.700242 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:04:33.701725  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:04:33.781730 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 16:04:33.781730 15588 solver.cpp:397]     Test net output #1: loss = 0.312759 (* 1 = 0.312759 loss)
I1107 16:04:33.862735 15588 solver.cpp:218] Iteration 143000 (9.34545 iter/s, 10.7004s/100 iters), loss = 0.0279804
I1107 16:04:33.862735 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:04:33.862735 15588 solver.cpp:237]     Train net output #1: loss = 0.0279806 (* 1 = 0.0279806 loss)
I1107 16:04:33.862735 15588 sgd_solver.cpp:105] Iteration 143000, lr = 0.001
I1107 16:04:42.616417 15588 solver.cpp:218] Iteration 143100 (11.4256 iter/s, 8.75231s/100 iters), loss = 0.0282935
I1107 16:04:42.616417 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:04:42.616417 15588 solver.cpp:237]     Train net output #1: loss = 0.0282937 (* 1 = 0.0282937 loss)
I1107 16:04:42.616417 15588 sgd_solver.cpp:105] Iteration 143100, lr = 0.001
I1107 16:04:51.386011 15588 solver.cpp:218] Iteration 143200 (11.4032 iter/s, 8.76949s/100 iters), loss = 0.0286846
I1107 16:04:51.386011 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:04:51.386011 15588 solver.cpp:237]     Train net output #1: loss = 0.0286848 (* 1 = 0.0286848 loss)
I1107 16:04:51.386011 15588 sgd_solver.cpp:105] Iteration 143200, lr = 0.001
I1107 16:05:00.111842 15588 solver.cpp:218] Iteration 143300 (11.4614 iter/s, 8.72491s/100 iters), loss = 0.032778
I1107 16:05:00.111842 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:05:00.111842 15588 solver.cpp:237]     Train net output #1: loss = 0.0327782 (* 1 = 0.0327782 loss)
I1107 16:05:00.111842 15588 sgd_solver.cpp:105] Iteration 143300, lr = 0.001
I1107 16:05:08.771167 15588 solver.cpp:218] Iteration 143400 (11.549 iter/s, 8.65874s/100 iters), loss = 0.023826
I1107 16:05:08.771167 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:05:08.771167 15588 solver.cpp:237]     Train net output #1: loss = 0.0238262 (* 1 = 0.0238262 loss)
I1107 16:05:08.771167 15588 sgd_solver.cpp:105] Iteration 143400, lr = 0.001
I1107 16:05:17.105232 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:05:17.452265 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_143500.caffemodel
I1107 16:05:17.490264 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_143500.solverstate
I1107 16:05:17.499264 15588 solver.cpp:330] Iteration 143500, Testing net (#0)
I1107 16:05:17.500264 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:05:19.490418  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:05:19.570425 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1107 16:05:19.570425 15588 solver.cpp:397]     Test net output #1: loss = 0.314436 (* 1 = 0.314436 loss)
I1107 16:05:19.651429 15588 solver.cpp:218] Iteration 143500 (9.19141 iter/s, 10.8797s/100 iters), loss = 0.0357529
I1107 16:05:19.651429 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:05:19.651429 15588 solver.cpp:237]     Train net output #1: loss = 0.0357531 (* 1 = 0.0357531 loss)
I1107 16:05:19.651429 15588 sgd_solver.cpp:105] Iteration 143500, lr = 0.001
I1107 16:05:28.305317 15588 solver.cpp:218] Iteration 143600 (11.5563 iter/s, 8.65328s/100 iters), loss = 0.0255837
I1107 16:05:28.305317 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:05:28.305317 15588 solver.cpp:237]     Train net output #1: loss = 0.0255839 (* 1 = 0.0255839 loss)
I1107 16:05:28.305317 15588 sgd_solver.cpp:105] Iteration 143600, lr = 0.001
I1107 16:05:37.079566 15588 solver.cpp:218] Iteration 143700 (11.3979 iter/s, 8.77354s/100 iters), loss = 0.0224998
I1107 16:05:37.079566 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:05:37.079566 15588 solver.cpp:237]     Train net output #1: loss = 0.0225 (* 1 = 0.0225 loss)
I1107 16:05:37.079566 15588 sgd_solver.cpp:105] Iteration 143700, lr = 0.001
I1107 16:05:45.848554 15588 solver.cpp:218] Iteration 143800 (11.4049 iter/s, 8.76819s/100 iters), loss = 0.0390963
I1107 16:05:45.848554 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:05:45.848554 15588 solver.cpp:237]     Train net output #1: loss = 0.0390965 (* 1 = 0.0390965 loss)
I1107 16:05:45.848554 15588 sgd_solver.cpp:105] Iteration 143800, lr = 0.001
I1107 16:05:54.547587 15588 solver.cpp:218] Iteration 143900 (11.4962 iter/s, 8.69856s/100 iters), loss = 0.0252565
I1107 16:05:54.547587 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:05:54.547587 15588 solver.cpp:237]     Train net output #1: loss = 0.0252567 (* 1 = 0.0252567 loss)
I1107 16:05:54.547587 15588 sgd_solver.cpp:105] Iteration 143900, lr = 0.001
I1107 16:06:02.850250 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:06:03.199800 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_144000.caffemodel
I1107 16:06:03.231801 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_144000.solverstate
I1107 16:06:03.240802 15588 solver.cpp:330] Iteration 144000, Testing net (#0)
I1107 16:06:03.241801 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:06:05.284155  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:06:05.367157 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1107 16:06:05.367157 15588 solver.cpp:397]     Test net output #1: loss = 0.313777 (* 1 = 0.313777 loss)
I1107 16:06:05.450160 15588 solver.cpp:218] Iteration 144000 (9.17249 iter/s, 10.9022s/100 iters), loss = 0.024822
I1107 16:06:05.450160 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:06:05.450160 15588 solver.cpp:237]     Train net output #1: loss = 0.0248222 (* 1 = 0.0248222 loss)
I1107 16:06:05.450160 15588 sgd_solver.cpp:105] Iteration 144000, lr = 0.001
I1107 16:06:14.097900 15588 solver.cpp:218] Iteration 144100 (11.565 iter/s, 8.64679s/100 iters), loss = 0.035702
I1107 16:06:14.097900 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:06:14.097900 15588 solver.cpp:237]     Train net output #1: loss = 0.0357023 (* 1 = 0.0357023 loss)
I1107 16:06:14.097900 15588 sgd_solver.cpp:105] Iteration 144100, lr = 0.001
I1107 16:06:22.887233 15588 solver.cpp:218] Iteration 144200 (11.3781 iter/s, 8.78881s/100 iters), loss = 0.0280239
I1107 16:06:22.887233 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:06:22.887233 15588 solver.cpp:237]     Train net output #1: loss = 0.0280241 (* 1 = 0.0280241 loss)
I1107 16:06:22.887233 15588 sgd_solver.cpp:105] Iteration 144200, lr = 0.001
I1107 16:06:31.599709 15588 solver.cpp:218] Iteration 144300 (11.4783 iter/s, 8.71209s/100 iters), loss = 0.0217496
I1107 16:06:31.599709 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:06:31.599709 15588 solver.cpp:237]     Train net output #1: loss = 0.0217498 (* 1 = 0.0217498 loss)
I1107 16:06:31.599709 15588 sgd_solver.cpp:105] Iteration 144300, lr = 0.001
I1107 16:06:40.212132 15588 solver.cpp:218] Iteration 144400 (11.6118 iter/s, 8.61196s/100 iters), loss = 0.0491985
I1107 16:06:40.212132 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:06:40.212132 15588 solver.cpp:237]     Train net output #1: loss = 0.0491987 (* 1 = 0.0491987 loss)
I1107 16:06:40.212132 15588 sgd_solver.cpp:105] Iteration 144400, lr = 0.001
I1107 16:06:48.527084 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:06:48.868113 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_144500.caffemodel
I1107 16:06:48.901124 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_144500.solverstate
I1107 16:06:48.910143 15588 solver.cpp:330] Iteration 144500, Testing net (#0)
I1107 16:06:48.910143 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:06:50.924289  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:06:51.005302 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9175
I1107 16:06:51.005302 15588 solver.cpp:397]     Test net output #1: loss = 0.323047 (* 1 = 0.323047 loss)
I1107 16:06:51.087301 15588 solver.cpp:218] Iteration 144500 (9.19627 iter/s, 10.874s/100 iters), loss = 0.022378
I1107 16:06:51.087301 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:06:51.087301 15588 solver.cpp:237]     Train net output #1: loss = 0.0223782 (* 1 = 0.0223782 loss)
I1107 16:06:51.087301 15588 sgd_solver.cpp:105] Iteration 144500, lr = 0.001
I1107 16:06:59.716207 15588 solver.cpp:218] Iteration 144600 (11.5888 iter/s, 8.62903s/100 iters), loss = 0.0586577
I1107 16:06:59.717206 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:06:59.717206 15588 solver.cpp:237]     Train net output #1: loss = 0.058658 (* 1 = 0.058658 loss)
I1107 16:06:59.717206 15588 sgd_solver.cpp:105] Iteration 144600, lr = 0.001
I1107 16:07:08.441926 15588 solver.cpp:218] Iteration 144700 (11.4616 iter/s, 8.72481s/100 iters), loss = 0.0217197
I1107 16:07:08.441926 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:07:08.441926 15588 solver.cpp:237]     Train net output #1: loss = 0.0217199 (* 1 = 0.0217199 loss)
I1107 16:07:08.441926 15588 sgd_solver.cpp:105] Iteration 144700, lr = 0.001
I1107 16:07:17.172045 15588 solver.cpp:218] Iteration 144800 (11.4554 iter/s, 8.72948s/100 iters), loss = 0.0192028
I1107 16:07:17.172045 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:07:17.172045 15588 solver.cpp:237]     Train net output #1: loss = 0.019203 (* 1 = 0.019203 loss)
I1107 16:07:17.172045 15588 sgd_solver.cpp:105] Iteration 144800, lr = 0.001
I1107 16:07:25.872879 15588 solver.cpp:218] Iteration 144900 (11.4942 iter/s, 8.70002s/100 iters), loss = 0.0244557
I1107 16:07:25.872879 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:07:25.872879 15588 solver.cpp:237]     Train net output #1: loss = 0.0244559 (* 1 = 0.0244559 loss)
I1107 16:07:25.872879 15588 sgd_solver.cpp:105] Iteration 144900, lr = 0.001
I1107 16:07:34.156421 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:07:34.495494 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_145000.caffemodel
I1107 16:07:34.525499 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_145000.solverstate
I1107 16:07:34.535498 15588 solver.cpp:330] Iteration 145000, Testing net (#0)
I1107 16:07:34.535498 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:07:36.529783  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:07:36.609786 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9176
I1107 16:07:36.609786 15588 solver.cpp:397]     Test net output #1: loss = 0.316439 (* 1 = 0.316439 loss)
I1107 16:07:36.691788 15588 solver.cpp:218] Iteration 145000 (9.24372 iter/s, 10.8182s/100 iters), loss = 0.0447321
I1107 16:07:36.691788 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:07:36.691788 15588 solver.cpp:237]     Train net output #1: loss = 0.0447323 (* 1 = 0.0447323 loss)
I1107 16:07:36.691788 15588 sgd_solver.cpp:105] Iteration 145000, lr = 0.001
I1107 16:07:45.405944 15588 solver.cpp:218] Iteration 145100 (11.4755 iter/s, 8.71419s/100 iters), loss = 0.0392716
I1107 16:07:45.405944 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:07:45.405944 15588 solver.cpp:237]     Train net output #1: loss = 0.0392719 (* 1 = 0.0392719 loss)
I1107 16:07:45.405944 15588 sgd_solver.cpp:105] Iteration 145100, lr = 0.001
I1107 16:07:54.032795 15588 solver.cpp:218] Iteration 145200 (11.5927 iter/s, 8.62615s/100 iters), loss = 0.0270819
I1107 16:07:54.032795 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:07:54.032795 15588 solver.cpp:237]     Train net output #1: loss = 0.0270821 (* 1 = 0.0270821 loss)
I1107 16:07:54.032795 15588 sgd_solver.cpp:105] Iteration 145200, lr = 0.001
I1107 16:08:02.556037 15588 solver.cpp:218] Iteration 145300 (11.734 iter/s, 8.52221s/100 iters), loss = 0.0193301
I1107 16:08:02.556037 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:08:02.556037 15588 solver.cpp:237]     Train net output #1: loss = 0.0193303 (* 1 = 0.0193303 loss)
I1107 16:08:02.556037 15588 sgd_solver.cpp:105] Iteration 145300, lr = 0.001
I1107 16:08:11.063827 15588 solver.cpp:218] Iteration 145400 (11.754 iter/s, 8.50773s/100 iters), loss = 0.0262223
I1107 16:08:11.063827 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:08:11.063827 15588 solver.cpp:237]     Train net output #1: loss = 0.0262226 (* 1 = 0.0262226 loss)
I1107 16:08:11.063827 15588 sgd_solver.cpp:105] Iteration 145400, lr = 0.001
I1107 16:08:19.166529 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:08:19.505575 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_145500.caffemodel
I1107 16:08:19.540578 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_145500.solverstate
I1107 16:08:19.549578 15588 solver.cpp:330] Iteration 145500, Testing net (#0)
I1107 16:08:19.549578 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:08:21.543890  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:08:21.624896 15588 solver.cpp:397]     Test net output #0: accuracy = 0.917
I1107 16:08:21.624896 15588 solver.cpp:397]     Test net output #1: loss = 0.320475 (* 1 = 0.320475 loss)
I1107 16:08:21.706398 15588 solver.cpp:218] Iteration 145500 (9.39696 iter/s, 10.6417s/100 iters), loss = 0.024568
I1107 16:08:21.706398 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:08:21.706398 15588 solver.cpp:237]     Train net output #1: loss = 0.0245683 (* 1 = 0.0245683 loss)
I1107 16:08:21.706398 15588 sgd_solver.cpp:105] Iteration 145500, lr = 0.001
I1107 16:08:30.246551 15588 solver.cpp:218] Iteration 145600 (11.7103 iter/s, 8.53946s/100 iters), loss = 0.0300361
I1107 16:08:30.246551 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:08:30.246551 15588 solver.cpp:237]     Train net output #1: loss = 0.0300363 (* 1 = 0.0300363 loss)
I1107 16:08:30.246551 15588 sgd_solver.cpp:105] Iteration 145600, lr = 0.001
I1107 16:08:38.809756 15588 solver.cpp:218] Iteration 145700 (11.6783 iter/s, 8.56288s/100 iters), loss = 0.0209977
I1107 16:08:38.810257 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:08:38.810257 15588 solver.cpp:237]     Train net output #1: loss = 0.0209979 (* 1 = 0.0209979 loss)
I1107 16:08:38.810257 15588 sgd_solver.cpp:105] Iteration 145700, lr = 0.001
I1107 16:08:47.361572 15588 solver.cpp:218] Iteration 145800 (11.6942 iter/s, 8.55122s/100 iters), loss = 0.017489
I1107 16:08:47.361572 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:08:47.361572 15588 solver.cpp:237]     Train net output #1: loss = 0.0174892 (* 1 = 0.0174892 loss)
I1107 16:08:47.361572 15588 sgd_solver.cpp:105] Iteration 145800, lr = 0.001
I1107 16:08:55.944563 15588 solver.cpp:218] Iteration 145900 (11.6514 iter/s, 8.58263s/100 iters), loss = 0.0429853
I1107 16:08:55.944563 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:08:55.944563 15588 solver.cpp:237]     Train net output #1: loss = 0.0429855 (* 1 = 0.0429855 loss)
I1107 16:08:55.944563 15588 sgd_solver.cpp:105] Iteration 145900, lr = 0.001
I1107 16:09:04.094976 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:09:04.436920 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_146000.caffemodel
I1107 16:09:04.473903 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_146000.solverstate
I1107 16:09:04.483916 15588 solver.cpp:330] Iteration 146000, Testing net (#0)
I1107 16:09:04.483916 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:09:06.511405  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:09:06.591922 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 16:09:06.591922 15588 solver.cpp:397]     Test net output #1: loss = 0.313553 (* 1 = 0.313553 loss)
I1107 16:09:06.672920 15588 solver.cpp:218] Iteration 146000 (9.32192 iter/s, 10.7274s/100 iters), loss = 0.0342563
I1107 16:09:06.672920 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:09:06.672920 15588 solver.cpp:237]     Train net output #1: loss = 0.0342565 (* 1 = 0.0342565 loss)
I1107 16:09:06.672920 15588 sgd_solver.cpp:105] Iteration 146000, lr = 0.001
I1107 16:09:15.310912 15588 solver.cpp:218] Iteration 146100 (11.5776 iter/s, 8.63737s/100 iters), loss = 0.021585
I1107 16:09:15.310912 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:09:15.310912 15588 solver.cpp:237]     Train net output #1: loss = 0.0215852 (* 1 = 0.0215852 loss)
I1107 16:09:15.310912 15588 sgd_solver.cpp:105] Iteration 146100, lr = 0.001
I1107 16:09:23.815546 15588 solver.cpp:218] Iteration 146200 (11.7592 iter/s, 8.50401s/100 iters), loss = 0.0321812
I1107 16:09:23.815546 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:09:23.815546 15588 solver.cpp:237]     Train net output #1: loss = 0.0321815 (* 1 = 0.0321815 loss)
I1107 16:09:23.815546 15588 sgd_solver.cpp:105] Iteration 146200, lr = 0.001
I1107 16:09:32.331312 15588 solver.cpp:218] Iteration 146300 (11.7429 iter/s, 8.51581s/100 iters), loss = 0.0183973
I1107 16:09:32.331312 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:09:32.331312 15588 solver.cpp:237]     Train net output #1: loss = 0.0183975 (* 1 = 0.0183975 loss)
I1107 16:09:32.331312 15588 sgd_solver.cpp:105] Iteration 146300, lr = 0.001
I1107 16:09:40.909231 15588 solver.cpp:218] Iteration 146400 (11.6601 iter/s, 8.57625s/100 iters), loss = 0.0268238
I1107 16:09:40.909231 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:09:40.909231 15588 solver.cpp:237]     Train net output #1: loss = 0.026824 (* 1 = 0.026824 loss)
I1107 16:09:40.909231 15588 sgd_solver.cpp:105] Iteration 146400, lr = 0.001
I1107 16:09:49.089859 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:09:49.432139 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_146500.caffemodel
I1107 16:09:49.467139 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_146500.solverstate
I1107 16:09:49.476141 15588 solver.cpp:330] Iteration 146500, Testing net (#0)
I1107 16:09:49.477141 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:09:51.470335  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:09:51.551347 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 16:09:51.551347 15588 solver.cpp:397]     Test net output #1: loss = 0.317724 (* 1 = 0.317724 loss)
I1107 16:09:51.635357 15588 solver.cpp:218] Iteration 146500 (9.32363 iter/s, 10.7254s/100 iters), loss = 0.0293851
I1107 16:09:51.635357 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:09:51.635357 15588 solver.cpp:237]     Train net output #1: loss = 0.0293853 (* 1 = 0.0293853 loss)
I1107 16:09:51.635357 15588 sgd_solver.cpp:105] Iteration 146500, lr = 0.001
I1107 16:10:00.287313 15588 solver.cpp:218] Iteration 146600 (11.5588 iter/s, 8.65145s/100 iters), loss = 0.020779
I1107 16:10:00.287313 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:10:00.287313 15588 solver.cpp:237]     Train net output #1: loss = 0.0207792 (* 1 = 0.0207792 loss)
I1107 16:10:00.287313 15588 sgd_solver.cpp:105] Iteration 146600, lr = 0.001
I1107 16:10:08.909991 15588 solver.cpp:218] Iteration 146700 (11.5979 iter/s, 8.62224s/100 iters), loss = 0.0193279
I1107 16:10:08.909991 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:10:08.909991 15588 solver.cpp:237]     Train net output #1: loss = 0.0193281 (* 1 = 0.0193281 loss)
I1107 16:10:08.910488 15588 sgd_solver.cpp:105] Iteration 146700, lr = 0.001
I1107 16:10:17.541110 15588 solver.cpp:218] Iteration 146800 (11.587 iter/s, 8.63035s/100 iters), loss = 0.0201932
I1107 16:10:17.541110 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:10:17.541110 15588 solver.cpp:237]     Train net output #1: loss = 0.0201934 (* 1 = 0.0201934 loss)
I1107 16:10:17.541110 15588 sgd_solver.cpp:105] Iteration 146800, lr = 0.001
I1107 16:10:26.138612 15588 solver.cpp:218] Iteration 146900 (11.6321 iter/s, 8.59693s/100 iters), loss = 0.0397468
I1107 16:10:26.138612 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:10:26.138612 15588 solver.cpp:237]     Train net output #1: loss = 0.0397471 (* 1 = 0.0397471 loss)
I1107 16:10:26.138612 15588 sgd_solver.cpp:105] Iteration 146900, lr = 0.001
I1107 16:10:34.229653 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:10:34.569188 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_147000.caffemodel
I1107 16:10:34.599691 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_147000.solverstate
I1107 16:10:34.607710 15588 solver.cpp:330] Iteration 147000, Testing net (#0)
I1107 16:10:34.608692 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:10:36.600078  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:10:36.679080 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 16:10:36.679080 15588 solver.cpp:397]     Test net output #1: loss = 0.313575 (* 1 = 0.313575 loss)
I1107 16:10:36.760084 15588 solver.cpp:218] Iteration 147000 (9.41487 iter/s, 10.6215s/100 iters), loss = 0.0243059
I1107 16:10:36.760084 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:10:36.760084 15588 solver.cpp:237]     Train net output #1: loss = 0.0243061 (* 1 = 0.0243061 loss)
I1107 16:10:36.760084 15588 sgd_solver.cpp:105] Iteration 147000, lr = 0.001
I1107 16:10:45.260835 15588 solver.cpp:218] Iteration 147100 (11.7653 iter/s, 8.49954s/100 iters), loss = 0.0351237
I1107 16:10:45.260835 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:10:45.260835 15588 solver.cpp:237]     Train net output #1: loss = 0.035124 (* 1 = 0.035124 loss)
I1107 16:10:45.260835 15588 sgd_solver.cpp:105] Iteration 147100, lr = 0.001
I1107 16:10:53.773970 15588 solver.cpp:218] Iteration 147200 (11.7467 iter/s, 8.51306s/100 iters), loss = 0.020478
I1107 16:10:53.774471 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:10:53.774471 15588 solver.cpp:237]     Train net output #1: loss = 0.0204782 (* 1 = 0.0204782 loss)
I1107 16:10:53.774471 15588 sgd_solver.cpp:105] Iteration 147200, lr = 0.001
I1107 16:11:02.284307 15588 solver.cpp:218] Iteration 147300 (11.7517 iter/s, 8.50943s/100 iters), loss = 0.0225197
I1107 16:11:02.284307 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:11:02.284307 15588 solver.cpp:237]     Train net output #1: loss = 0.0225199 (* 1 = 0.0225199 loss)
I1107 16:11:02.284307 15588 sgd_solver.cpp:105] Iteration 147300, lr = 0.001
I1107 16:11:10.791119 15588 solver.cpp:218] Iteration 147400 (11.7555 iter/s, 8.50667s/100 iters), loss = 0.0240692
I1107 16:11:10.791119 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:11:10.791119 15588 solver.cpp:237]     Train net output #1: loss = 0.0240694 (* 1 = 0.0240694 loss)
I1107 16:11:10.791119 15588 sgd_solver.cpp:105] Iteration 147400, lr = 0.001
I1107 16:11:18.875799 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:11:19.211823 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_147500.caffemodel
I1107 16:11:19.239835 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_147500.solverstate
I1107 16:11:19.248839 15588 solver.cpp:330] Iteration 147500, Testing net (#0)
I1107 16:11:19.248839 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:11:21.235258  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:11:21.314980 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 16:11:21.314980 15588 solver.cpp:397]     Test net output #1: loss = 0.318129 (* 1 = 0.318129 loss)
I1107 16:11:21.397508 15588 solver.cpp:218] Iteration 147500 (9.42898 iter/s, 10.6056s/100 iters), loss = 0.0294648
I1107 16:11:21.397508 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:11:21.397508 15588 solver.cpp:237]     Train net output #1: loss = 0.029465 (* 1 = 0.029465 loss)
I1107 16:11:21.397508 15588 sgd_solver.cpp:105] Iteration 147500, lr = 0.001
I1107 16:11:29.903378 15588 solver.cpp:218] Iteration 147600 (11.7561 iter/s, 8.50622s/100 iters), loss = 0.0335572
I1107 16:11:29.904366 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:11:29.904366 15588 solver.cpp:237]     Train net output #1: loss = 0.0335574 (* 1 = 0.0335574 loss)
I1107 16:11:29.904366 15588 sgd_solver.cpp:105] Iteration 147600, lr = 0.001
I1107 16:11:38.408747 15588 solver.cpp:218] Iteration 147700 (11.7588 iter/s, 8.50428s/100 iters), loss = 0.0220777
I1107 16:11:38.409247 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:11:38.409247 15588 solver.cpp:237]     Train net output #1: loss = 0.0220779 (* 1 = 0.0220779 loss)
I1107 16:11:38.409247 15588 sgd_solver.cpp:105] Iteration 147700, lr = 0.001
I1107 16:11:47.020977 15588 solver.cpp:218] Iteration 147800 (11.6122 iter/s, 8.6116s/100 iters), loss = 0.0242603
I1107 16:11:47.020977 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:11:47.020977 15588 solver.cpp:237]     Train net output #1: loss = 0.0242605 (* 1 = 0.0242605 loss)
I1107 16:11:47.020977 15588 sgd_solver.cpp:105] Iteration 147800, lr = 0.001
I1107 16:11:55.634389 15588 solver.cpp:218] Iteration 147900 (11.6107 iter/s, 8.61277s/100 iters), loss = 0.0378023
I1107 16:11:55.634389 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:11:55.634389 15588 solver.cpp:237]     Train net output #1: loss = 0.0378025 (* 1 = 0.0378025 loss)
I1107 16:11:55.634389 15588 sgd_solver.cpp:105] Iteration 147900, lr = 0.001
I1107 16:12:03.735134 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:12:04.073148 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_148000.caffemodel
I1107 16:12:04.104148 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_148000.solverstate
I1107 16:12:04.113652 15588 solver.cpp:330] Iteration 148000, Testing net (#0)
I1107 16:12:04.113652 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:12:06.104333  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:12:06.183337 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9189
I1107 16:12:06.183337 15588 solver.cpp:397]     Test net output #1: loss = 0.317949 (* 1 = 0.317949 loss)
I1107 16:12:06.263342 15588 solver.cpp:218] Iteration 148000 (9.40835 iter/s, 10.6289s/100 iters), loss = 0.0269535
I1107 16:12:06.263342 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:12:06.263342 15588 solver.cpp:237]     Train net output #1: loss = 0.0269537 (* 1 = 0.0269537 loss)
I1107 16:12:06.264343 15588 sgd_solver.cpp:105] Iteration 148000, lr = 0.001
I1107 16:12:14.788198 15588 solver.cpp:218] Iteration 148100 (11.732 iter/s, 8.52367s/100 iters), loss = 0.0234994
I1107 16:12:14.788198 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:12:14.788198 15588 solver.cpp:237]     Train net output #1: loss = 0.0234996 (* 1 = 0.0234996 loss)
I1107 16:12:14.788198 15588 sgd_solver.cpp:105] Iteration 148100, lr = 0.001
I1107 16:12:23.314301 15588 solver.cpp:218] Iteration 148200 (11.7297 iter/s, 8.5254s/100 iters), loss = 0.0173452
I1107 16:12:23.314301 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:12:23.314301 15588 solver.cpp:237]     Train net output #1: loss = 0.0173454 (* 1 = 0.0173454 loss)
I1107 16:12:23.314301 15588 sgd_solver.cpp:105] Iteration 148200, lr = 0.001
I1107 16:12:31.859192 15588 solver.cpp:218] Iteration 148300 (11.7031 iter/s, 8.54477s/100 iters), loss = 0.0421842
I1107 16:12:31.859192 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:12:31.859192 15588 solver.cpp:237]     Train net output #1: loss = 0.0421844 (* 1 = 0.0421844 loss)
I1107 16:12:31.859192 15588 sgd_solver.cpp:105] Iteration 148300, lr = 0.001
I1107 16:12:40.392133 15588 solver.cpp:218] Iteration 148400 (11.7202 iter/s, 8.53229s/100 iters), loss = 0.0286738
I1107 16:12:40.392133 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:12:40.392133 15588 solver.cpp:237]     Train net output #1: loss = 0.0286741 (* 1 = 0.0286741 loss)
I1107 16:12:40.392133 15588 sgd_solver.cpp:105] Iteration 148400, lr = 0.001
I1107 16:12:48.586694 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:12:48.924223 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_148500.caffemodel
I1107 16:12:48.958726 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_148500.solverstate
I1107 16:12:48.968727 15588 solver.cpp:330] Iteration 148500, Testing net (#0)
I1107 16:12:48.968727 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:12:50.961886  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:12:51.040890 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 16:12:51.040890 15588 solver.cpp:397]     Test net output #1: loss = 0.31908 (* 1 = 0.31908 loss)
I1107 16:12:51.122392 15588 solver.cpp:218] Iteration 148500 (9.32065 iter/s, 10.7289s/100 iters), loss = 0.0374931
I1107 16:12:51.122392 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:12:51.122392 15588 solver.cpp:237]     Train net output #1: loss = 0.0374932 (* 1 = 0.0374932 loss)
I1107 16:12:51.122392 15588 sgd_solver.cpp:105] Iteration 148500, lr = 0.001
I1107 16:12:59.720199 15588 solver.cpp:218] Iteration 148600 (11.6314 iter/s, 8.59741s/100 iters), loss = 0.0295047
I1107 16:12:59.720199 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:12:59.720199 15588 solver.cpp:237]     Train net output #1: loss = 0.0295049 (* 1 = 0.0295049 loss)
I1107 16:12:59.720199 15588 sgd_solver.cpp:105] Iteration 148600, lr = 0.001
I1107 16:13:08.261459 15588 solver.cpp:218] Iteration 148700 (11.7079 iter/s, 8.54124s/100 iters), loss = 0.022916
I1107 16:13:08.261459 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:13:08.261459 15588 solver.cpp:237]     Train net output #1: loss = 0.0229161 (* 1 = 0.0229161 loss)
I1107 16:13:08.261459 15588 sgd_solver.cpp:105] Iteration 148700, lr = 0.001
I1107 16:13:16.803369 15588 solver.cpp:218] Iteration 148800 (11.708 iter/s, 8.54116s/100 iters), loss = 0.0217699
I1107 16:13:16.803369 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:13:16.803369 15588 solver.cpp:237]     Train net output #1: loss = 0.0217701 (* 1 = 0.0217701 loss)
I1107 16:13:16.803369 15588 sgd_solver.cpp:105] Iteration 148800, lr = 0.001
I1107 16:13:25.357090 15588 solver.cpp:218] Iteration 148900 (11.6915 iter/s, 8.55321s/100 iters), loss = 0.0342049
I1107 16:13:25.357090 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:13:25.357090 15588 solver.cpp:237]     Train net output #1: loss = 0.0342051 (* 1 = 0.0342051 loss)
I1107 16:13:25.357090 15588 sgd_solver.cpp:105] Iteration 148900, lr = 0.001
I1107 16:13:33.444833 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:13:33.780860 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_149000.caffemodel
I1107 16:13:33.816860 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_149000.solverstate
I1107 16:13:33.826864 15588 solver.cpp:330] Iteration 149000, Testing net (#0)
I1107 16:13:33.826864 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:13:35.836385  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:13:35.916889 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9185
I1107 16:13:35.916889 15588 solver.cpp:397]     Test net output #1: loss = 0.320428 (* 1 = 0.320428 loss)
I1107 16:13:35.998894 15588 solver.cpp:218] Iteration 149000 (9.39763 iter/s, 10.641s/100 iters), loss = 0.0240219
I1107 16:13:35.998894 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:13:35.998894 15588 solver.cpp:237]     Train net output #1: loss = 0.0240221 (* 1 = 0.0240221 loss)
I1107 16:13:35.998894 15588 sgd_solver.cpp:105] Iteration 149000, lr = 0.001
I1107 16:13:44.547665 15588 solver.cpp:218] Iteration 149100 (11.6976 iter/s, 8.54879s/100 iters), loss = 0.0252865
I1107 16:13:44.547665 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:13:44.547665 15588 solver.cpp:237]     Train net output #1: loss = 0.0252867 (* 1 = 0.0252867 loss)
I1107 16:13:44.547665 15588 sgd_solver.cpp:105] Iteration 149100, lr = 0.001
I1107 16:13:53.071377 15588 solver.cpp:218] Iteration 149200 (11.7329 iter/s, 8.52305s/100 iters), loss = 0.0191195
I1107 16:13:53.071377 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:13:53.071377 15588 solver.cpp:237]     Train net output #1: loss = 0.0191197 (* 1 = 0.0191197 loss)
I1107 16:13:53.071377 15588 sgd_solver.cpp:105] Iteration 149200, lr = 0.001
I1107 16:14:01.585320 15588 solver.cpp:218] Iteration 149300 (11.7459 iter/s, 8.51362s/100 iters), loss = 0.015791
I1107 16:14:01.586321 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:14:01.586321 15588 solver.cpp:237]     Train net output #1: loss = 0.0157912 (* 1 = 0.0157912 loss)
I1107 16:14:01.586321 15588 sgd_solver.cpp:105] Iteration 149300, lr = 0.001
I1107 16:14:10.093082 15588 solver.cpp:218] Iteration 149400 (11.756 iter/s, 8.50631s/100 iters), loss = 0.023668
I1107 16:14:10.093082 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:14:10.093082 15588 solver.cpp:237]     Train net output #1: loss = 0.0236682 (* 1 = 0.0236682 loss)
I1107 16:14:10.093082 15588 sgd_solver.cpp:105] Iteration 149400, lr = 0.001
I1107 16:14:18.203168 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:14:18.540719 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_149500.caffemodel
I1107 16:14:18.575203 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_149500.solverstate
I1107 16:14:18.584203 15588 solver.cpp:330] Iteration 149500, Testing net (#0)
I1107 16:14:18.584203 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:14:20.573338  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:14:20.652374 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9176
I1107 16:14:20.652374 15588 solver.cpp:397]     Test net output #1: loss = 0.314537 (* 1 = 0.314537 loss)
I1107 16:14:20.733896 15588 solver.cpp:218] Iteration 149500 (9.39805 iter/s, 10.6405s/100 iters), loss = 0.0463666
I1107 16:14:20.733896 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:14:20.733896 15588 solver.cpp:237]     Train net output #1: loss = 0.0463668 (* 1 = 0.0463668 loss)
I1107 16:14:20.733896 15588 sgd_solver.cpp:105] Iteration 149500, lr = 0.001
I1107 16:14:29.238824 15588 solver.cpp:218] Iteration 149600 (11.7591 iter/s, 8.50406s/100 iters), loss = 0.022782
I1107 16:14:29.238824 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:14:29.238824 15588 solver.cpp:237]     Train net output #1: loss = 0.0227822 (* 1 = 0.0227822 loss)
I1107 16:14:29.238824 15588 sgd_solver.cpp:105] Iteration 149600, lr = 0.001
I1107 16:14:37.742588 15588 solver.cpp:218] Iteration 149700 (11.7602 iter/s, 8.50329s/100 iters), loss = 0.0154381
I1107 16:14:37.742588 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:14:37.742588 15588 solver.cpp:237]     Train net output #1: loss = 0.0154384 (* 1 = 0.0154384 loss)
I1107 16:14:37.742588 15588 sgd_solver.cpp:105] Iteration 149700, lr = 0.001
I1107 16:14:46.246462 15588 solver.cpp:218] Iteration 149800 (11.7597 iter/s, 8.50363s/100 iters), loss = 0.0378955
I1107 16:14:46.246462 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:14:46.246462 15588 solver.cpp:237]     Train net output #1: loss = 0.0378957 (* 1 = 0.0378957 loss)
I1107 16:14:46.246462 15588 sgd_solver.cpp:105] Iteration 149800, lr = 0.001
I1107 16:14:54.851802 15588 solver.cpp:218] Iteration 149900 (11.6217 iter/s, 8.6046s/100 iters), loss = 0.0412849
I1107 16:14:54.851802 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:14:54.851802 15588 solver.cpp:237]     Train net output #1: loss = 0.0412851 (* 1 = 0.0412851 loss)
I1107 16:14:54.851802 15588 sgd_solver.cpp:105] Iteration 149900, lr = 0.001
I1107 16:15:03.056035 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:15:03.393558 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_150000.caffemodel
I1107 16:15:03.430557 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_150000.solverstate
I1107 16:15:03.439556 15588 solver.cpp:330] Iteration 150000, Testing net (#0)
I1107 16:15:03.439556 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:15:05.450706  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:15:05.530207 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1107 16:15:05.530207 15588 solver.cpp:397]     Test net output #1: loss = 0.319253 (* 1 = 0.319253 loss)
I1107 16:15:05.611212 15588 solver.cpp:218] Iteration 150000 (9.29451 iter/s, 10.759s/100 iters), loss = 0.0255768
I1107 16:15:05.611212 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:15:05.611212 15588 solver.cpp:237]     Train net output #1: loss = 0.025577 (* 1 = 0.025577 loss)
I1107 16:15:05.611212 15588 sgd_solver.cpp:105] Iteration 150000, lr = 0.001
I1107 16:15:14.163151 15588 solver.cpp:218] Iteration 150100 (11.6941 iter/s, 8.55132s/100 iters), loss = 0.0271939
I1107 16:15:14.163151 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:15:14.163151 15588 solver.cpp:237]     Train net output #1: loss = 0.0271941 (* 1 = 0.0271941 loss)
I1107 16:15:14.163151 15588 sgd_solver.cpp:105] Iteration 150100, lr = 0.001
I1107 16:15:22.710903 15588 solver.cpp:218] Iteration 150200 (11.6999 iter/s, 8.54712s/100 iters), loss = 0.0298727
I1107 16:15:22.710903 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:15:22.710903 15588 solver.cpp:237]     Train net output #1: loss = 0.0298729 (* 1 = 0.0298729 loss)
I1107 16:15:22.710903 15588 sgd_solver.cpp:105] Iteration 150200, lr = 0.001
I1107 16:15:31.301519 15588 solver.cpp:218] Iteration 150300 (11.6413 iter/s, 8.59011s/100 iters), loss = 0.0157377
I1107 16:15:31.301519 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:15:31.301519 15588 solver.cpp:237]     Train net output #1: loss = 0.0157379 (* 1 = 0.0157379 loss)
I1107 16:15:31.301519 15588 sgd_solver.cpp:105] Iteration 150300, lr = 0.001
I1107 16:15:39.793048 15588 solver.cpp:218] Iteration 150400 (11.7769 iter/s, 8.49121s/100 iters), loss = 0.0301157
I1107 16:15:39.793048 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:15:39.793048 15588 solver.cpp:237]     Train net output #1: loss = 0.0301159 (* 1 = 0.0301159 loss)
I1107 16:15:39.793048 15588 sgd_solver.cpp:105] Iteration 150400, lr = 0.001
I1107 16:15:47.872002 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:15:48.210026 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_150500.caffemodel
I1107 16:15:48.246035 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_150500.solverstate
I1107 16:15:48.255035 15588 solver.cpp:330] Iteration 150500, Testing net (#0)
I1107 16:15:48.255035 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:15:50.245237  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:15:50.325748 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1107 16:15:50.325748 15588 solver.cpp:397]     Test net output #1: loss = 0.316202 (* 1 = 0.316202 loss)
I1107 16:15:50.406826 15588 solver.cpp:218] Iteration 150500 (9.42207 iter/s, 10.6134s/100 iters), loss = 0.0339932
I1107 16:15:50.406826 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:15:50.406826 15588 solver.cpp:237]     Train net output #1: loss = 0.0339934 (* 1 = 0.0339934 loss)
I1107 16:15:50.406826 15588 sgd_solver.cpp:105] Iteration 150500, lr = 0.001
I1107 16:15:58.891140 15588 solver.cpp:218] Iteration 150600 (11.7874 iter/s, 8.4836s/100 iters), loss = 0.0300536
I1107 16:15:58.891140 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:15:58.891140 15588 solver.cpp:237]     Train net output #1: loss = 0.0300538 (* 1 = 0.0300538 loss)
I1107 16:15:58.891140 15588 sgd_solver.cpp:105] Iteration 150600, lr = 0.001
I1107 16:16:07.673432 15588 solver.cpp:218] Iteration 150700 (11.3872 iter/s, 8.78179s/100 iters), loss = 0.0190424
I1107 16:16:07.673432 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:16:07.673432 15588 solver.cpp:237]     Train net output #1: loss = 0.0190426 (* 1 = 0.0190426 loss)
I1107 16:16:07.673432 15588 sgd_solver.cpp:105] Iteration 150700, lr = 0.001
I1107 16:16:16.313388 15588 solver.cpp:218] Iteration 150800 (11.575 iter/s, 8.63932s/100 iters), loss = 0.0230671
I1107 16:16:16.313388 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:16:16.313388 15588 solver.cpp:237]     Train net output #1: loss = 0.0230673 (* 1 = 0.0230673 loss)
I1107 16:16:16.313388 15588 sgd_solver.cpp:105] Iteration 150800, lr = 0.001
I1107 16:16:24.831647 15588 solver.cpp:218] Iteration 150900 (11.7404 iter/s, 8.51757s/100 iters), loss = 0.0292093
I1107 16:16:24.832157 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:16:24.832157 15588 solver.cpp:237]     Train net output #1: loss = 0.0292095 (* 1 = 0.0292095 loss)
I1107 16:16:24.832157 15588 sgd_solver.cpp:105] Iteration 150900, lr = 0.001
I1107 16:16:32.918921 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:16:33.256973 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_151000.caffemodel
I1107 16:16:33.284971 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_151000.solverstate
I1107 16:16:33.293972 15588 solver.cpp:330] Iteration 151000, Testing net (#0)
I1107 16:16:33.294972 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:16:35.283165  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:16:35.363165 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1107 16:16:35.363165 15588 solver.cpp:397]     Test net output #1: loss = 0.317096 (* 1 = 0.317096 loss)
I1107 16:16:35.443169 15588 solver.cpp:218] Iteration 151000 (9.42387 iter/s, 10.6114s/100 iters), loss = 0.0235081
I1107 16:16:35.443169 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:16:35.443169 15588 solver.cpp:237]     Train net output #1: loss = 0.0235083 (* 1 = 0.0235083 loss)
I1107 16:16:35.443169 15588 sgd_solver.cpp:105] Iteration 151000, lr = 0.001
I1107 16:16:43.935873 15588 solver.cpp:218] Iteration 151100 (11.7766 iter/s, 8.49142s/100 iters), loss = 0.030524
I1107 16:16:43.935873 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:16:43.935873 15588 solver.cpp:237]     Train net output #1: loss = 0.0305242 (* 1 = 0.0305242 loss)
I1107 16:16:43.935873 15588 sgd_solver.cpp:105] Iteration 151100, lr = 0.001
I1107 16:16:52.437108 15588 solver.cpp:218] Iteration 151200 (11.7635 iter/s, 8.50091s/100 iters), loss = 0.0293361
I1107 16:16:52.437108 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:16:52.437108 15588 solver.cpp:237]     Train net output #1: loss = 0.0293363 (* 1 = 0.0293363 loss)
I1107 16:16:52.437108 15588 sgd_solver.cpp:105] Iteration 151200, lr = 0.001
I1107 16:17:00.940395 15588 solver.cpp:218] Iteration 151300 (11.7599 iter/s, 8.50346s/100 iters), loss = 0.0200451
I1107 16:17:00.940395 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:17:00.940395 15588 solver.cpp:237]     Train net output #1: loss = 0.0200453 (* 1 = 0.0200453 loss)
I1107 16:17:00.941396 15588 sgd_solver.cpp:105] Iteration 151300, lr = 0.001
I1107 16:17:09.451448 15588 solver.cpp:218] Iteration 151400 (11.751 iter/s, 8.50991s/100 iters), loss = 0.0296963
I1107 16:17:09.451448 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:17:09.451448 15588 solver.cpp:237]     Train net output #1: loss = 0.0296965 (* 1 = 0.0296965 loss)
I1107 16:17:09.451448 15588 sgd_solver.cpp:105] Iteration 151400, lr = 0.001
I1107 16:17:17.539022 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:17:17.873436 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_151500.caffemodel
I1107 16:17:17.904436 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_151500.solverstate
I1107 16:17:17.914438 15588 solver.cpp:330] Iteration 151500, Testing net (#0)
I1107 16:17:17.914438 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:17:19.910509  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:17:19.990546 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9168
I1107 16:17:19.990546 15588 solver.cpp:397]     Test net output #1: loss = 0.320479 (* 1 = 0.320479 loss)
I1107 16:17:20.072537 15588 solver.cpp:218] Iteration 151500 (9.41574 iter/s, 10.6205s/100 iters), loss = 0.0452837
I1107 16:17:20.072537 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:17:20.072537 15588 solver.cpp:237]     Train net output #1: loss = 0.0452839 (* 1 = 0.0452839 loss)
I1107 16:17:20.072537 15588 sgd_solver.cpp:105] Iteration 151500, lr = 0.001
I1107 16:17:28.572237 15588 solver.cpp:218] Iteration 151600 (11.7655 iter/s, 8.49941s/100 iters), loss = 0.0247103
I1107 16:17:28.572237 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:17:28.572237 15588 solver.cpp:237]     Train net output #1: loss = 0.0247105 (* 1 = 0.0247105 loss)
I1107 16:17:28.572237 15588 sgd_solver.cpp:105] Iteration 151600, lr = 0.001
I1107 16:17:37.076068 15588 solver.cpp:218] Iteration 151700 (11.7597 iter/s, 8.5036s/100 iters), loss = 0.0184738
I1107 16:17:37.076068 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:17:37.076068 15588 solver.cpp:237]     Train net output #1: loss = 0.018474 (* 1 = 0.018474 loss)
I1107 16:17:37.076068 15588 sgd_solver.cpp:105] Iteration 151700, lr = 0.001
I1107 16:17:45.576843 15588 solver.cpp:218] Iteration 151800 (11.7645 iter/s, 8.50016s/100 iters), loss = 0.0349328
I1107 16:17:45.576843 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:17:45.576843 15588 solver.cpp:237]     Train net output #1: loss = 0.034933 (* 1 = 0.034933 loss)
I1107 16:17:45.576843 15588 sgd_solver.cpp:105] Iteration 151800, lr = 0.001
I1107 16:17:54.083752 15588 solver.cpp:218] Iteration 151900 (11.7564 iter/s, 8.50598s/100 iters), loss = 0.0258078
I1107 16:17:54.083752 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:17:54.083752 15588 solver.cpp:237]     Train net output #1: loss = 0.025808 (* 1 = 0.025808 loss)
I1107 16:17:54.083752 15588 sgd_solver.cpp:105] Iteration 151900, lr = 0.001
I1107 16:18:02.171615 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:18:02.507638 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_152000.caffemodel
I1107 16:18:02.540635 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_152000.solverstate
I1107 16:18:02.549638 15588 solver.cpp:330] Iteration 152000, Testing net (#0)
I1107 16:18:02.549638 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:18:04.541815  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:18:04.620817 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9176
I1107 16:18:04.620817 15588 solver.cpp:397]     Test net output #1: loss = 0.323249 (* 1 = 0.323249 loss)
I1107 16:18:04.702824 15588 solver.cpp:218] Iteration 152000 (9.41737 iter/s, 10.6187s/100 iters), loss = 0.0234614
I1107 16:18:04.702824 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:18:04.702824 15588 solver.cpp:237]     Train net output #1: loss = 0.0234616 (* 1 = 0.0234616 loss)
I1107 16:18:04.702824 15588 sgd_solver.cpp:105] Iteration 152000, lr = 0.001
I1107 16:18:13.212129 15588 solver.cpp:218] Iteration 152100 (11.7521 iter/s, 8.50914s/100 iters), loss = 0.0262613
I1107 16:18:13.212129 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:18:13.212129 15588 solver.cpp:237]     Train net output #1: loss = 0.0262615 (* 1 = 0.0262615 loss)
I1107 16:18:13.212129 15588 sgd_solver.cpp:105] Iteration 152100, lr = 0.001
I1107 16:18:21.716060 15588 solver.cpp:218] Iteration 152200 (11.7601 iter/s, 8.50336s/100 iters), loss = 0.0233555
I1107 16:18:21.716060 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:18:21.716060 15588 solver.cpp:237]     Train net output #1: loss = 0.0233558 (* 1 = 0.0233558 loss)
I1107 16:18:21.716060 15588 sgd_solver.cpp:105] Iteration 152200, lr = 0.001
I1107 16:18:30.211932 15588 solver.cpp:218] Iteration 152300 (11.771 iter/s, 8.49542s/100 iters), loss = 0.0168773
I1107 16:18:30.211932 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:18:30.211932 15588 solver.cpp:237]     Train net output #1: loss = 0.0168775 (* 1 = 0.0168775 loss)
I1107 16:18:30.211932 15588 sgd_solver.cpp:105] Iteration 152300, lr = 0.001
I1107 16:18:38.727660 15588 solver.cpp:218] Iteration 152400 (11.7447 iter/s, 8.51447s/100 iters), loss = 0.023588
I1107 16:18:38.727660 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:18:38.727660 15588 solver.cpp:237]     Train net output #1: loss = 0.0235882 (* 1 = 0.0235882 loss)
I1107 16:18:38.727660 15588 sgd_solver.cpp:105] Iteration 152400, lr = 0.001
I1107 16:18:46.815361 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:18:47.153386 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_152500.caffemodel
I1107 16:18:47.185385 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_152500.solverstate
I1107 16:18:47.194386 15588 solver.cpp:330] Iteration 152500, Testing net (#0)
I1107 16:18:47.194386 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:18:49.217692  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:18:49.296699 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 16:18:49.296699 15588 solver.cpp:397]     Test net output #1: loss = 0.316395 (* 1 = 0.316395 loss)
I1107 16:18:49.378717 15588 solver.cpp:218] Iteration 152500 (9.38912 iter/s, 10.6506s/100 iters), loss = 0.049799
I1107 16:18:49.378717 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:18:49.378717 15588 solver.cpp:237]     Train net output #1: loss = 0.0497992 (* 1 = 0.0497992 loss)
I1107 16:18:49.378717 15588 sgd_solver.cpp:105] Iteration 152500, lr = 0.001
I1107 16:18:58.005544 15588 solver.cpp:218] Iteration 152600 (11.5927 iter/s, 8.62614s/100 iters), loss = 0.0277226
I1107 16:18:58.005544 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:18:58.005544 15588 solver.cpp:237]     Train net output #1: loss = 0.0277229 (* 1 = 0.0277229 loss)
I1107 16:18:58.005544 15588 sgd_solver.cpp:105] Iteration 152600, lr = 0.001
I1107 16:19:06.620088 15588 solver.cpp:218] Iteration 152700 (11.6087 iter/s, 8.61419s/100 iters), loss = 0.0172444
I1107 16:19:06.620088 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:19:06.620088 15588 solver.cpp:237]     Train net output #1: loss = 0.0172446 (* 1 = 0.0172446 loss)
I1107 16:19:06.620088 15588 sgd_solver.cpp:105] Iteration 152700, lr = 0.001
I1107 16:19:15.158891 15588 solver.cpp:218] Iteration 152800 (11.711 iter/s, 8.53898s/100 iters), loss = 0.020892
I1107 16:19:15.159893 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:19:15.159893 15588 solver.cpp:237]     Train net output #1: loss = 0.0208922 (* 1 = 0.0208922 loss)
I1107 16:19:15.159893 15588 sgd_solver.cpp:105] Iteration 152800, lr = 0.001
I1107 16:19:23.745110 15588 solver.cpp:218] Iteration 152900 (11.6483 iter/s, 8.58491s/100 iters), loss = 0.0217926
I1107 16:19:23.745110 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:19:23.745110 15588 solver.cpp:237]     Train net output #1: loss = 0.0217929 (* 1 = 0.0217929 loss)
I1107 16:19:23.745611 15588 sgd_solver.cpp:105] Iteration 152900, lr = 0.001
I1107 16:19:31.990262 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:19:32.328025 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_153000.caffemodel
I1107 16:19:32.358070 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_153000.solverstate
I1107 16:19:32.367477 15588 solver.cpp:330] Iteration 153000, Testing net (#0)
I1107 16:19:32.367477 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:19:34.357880  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:19:34.435883 15588 solver.cpp:397]     Test net output #0: accuracy = 0.916
I1107 16:19:34.435883 15588 solver.cpp:397]     Test net output #1: loss = 0.331896 (* 1 = 0.331896 loss)
I1107 16:19:34.517918 15588 solver.cpp:218] Iteration 153000 (9.28295 iter/s, 10.7724s/100 iters), loss = 0.0276587
I1107 16:19:34.517918 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:19:34.517918 15588 solver.cpp:237]     Train net output #1: loss = 0.0276589 (* 1 = 0.0276589 loss)
I1107 16:19:34.517918 15588 sgd_solver.cpp:46] MultiStep Status: Iteration 153000, step = 3
I1107 16:19:34.517918 15588 sgd_solver.cpp:105] Iteration 153000, lr = 0.0001
I1107 16:19:43.144215 15588 solver.cpp:218] Iteration 153100 (11.5941 iter/s, 8.62508s/100 iters), loss = 0.0289988
I1107 16:19:43.144215 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:19:43.144215 15588 solver.cpp:237]     Train net output #1: loss = 0.028999 (* 1 = 0.028999 loss)
I1107 16:19:43.144215 15588 sgd_solver.cpp:105] Iteration 153100, lr = 0.0001
I1107 16:19:51.806772 15588 solver.cpp:218] Iteration 153200 (11.5448 iter/s, 8.66191s/100 iters), loss = 0.0202362
I1107 16:19:51.806772 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:19:51.806772 15588 solver.cpp:237]     Train net output #1: loss = 0.0202364 (* 1 = 0.0202364 loss)
I1107 16:19:51.806772 15588 sgd_solver.cpp:105] Iteration 153200, lr = 0.0001
I1107 16:20:00.353749 15588 solver.cpp:218] Iteration 153300 (11.7003 iter/s, 8.54676s/100 iters), loss = 0.0245892
I1107 16:20:00.353749 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:20:00.353749 15588 solver.cpp:237]     Train net output #1: loss = 0.0245894 (* 1 = 0.0245894 loss)
I1107 16:20:00.353749 15588 sgd_solver.cpp:105] Iteration 153300, lr = 0.0001
I1107 16:20:08.896528 15588 solver.cpp:218] Iteration 153400 (11.707 iter/s, 8.5419s/100 iters), loss = 0.0213951
I1107 16:20:08.896528 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:20:08.896528 15588 solver.cpp:237]     Train net output #1: loss = 0.0213953 (* 1 = 0.0213953 loss)
I1107 16:20:08.896528 15588 sgd_solver.cpp:105] Iteration 153400, lr = 0.0001
I1107 16:20:16.951092 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:20:17.286128 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_153500.caffemodel
I1107 16:20:17.316133 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_153500.solverstate
I1107 16:20:17.329131 15588 solver.cpp:330] Iteration 153500, Testing net (#0)
I1107 16:20:17.329131 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:20:19.314352  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:20:19.393350 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9192
I1107 16:20:19.393350 15588 solver.cpp:397]     Test net output #1: loss = 0.310776 (* 1 = 0.310776 loss)
I1107 16:20:19.474352 15588 solver.cpp:218] Iteration 153500 (9.4541 iter/s, 10.5774s/100 iters), loss = 0.0458214
I1107 16:20:19.474352 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:20:19.474352 15588 solver.cpp:237]     Train net output #1: loss = 0.0458216 (* 1 = 0.0458216 loss)
I1107 16:20:19.474352 15588 sgd_solver.cpp:105] Iteration 153500, lr = 0.0001
I1107 16:20:27.954202 15588 solver.cpp:218] Iteration 153600 (11.793 iter/s, 8.47963s/100 iters), loss = 0.0299615
I1107 16:20:27.954202 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:20:27.954202 15588 solver.cpp:237]     Train net output #1: loss = 0.0299617 (* 1 = 0.0299617 loss)
I1107 16:20:27.954202 15588 sgd_solver.cpp:105] Iteration 153600, lr = 0.0001
I1107 16:20:36.550870 15588 solver.cpp:218] Iteration 153700 (11.6333 iter/s, 8.59602s/100 iters), loss = 0.0184925
I1107 16:20:36.550870 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:20:36.550870 15588 solver.cpp:237]     Train net output #1: loss = 0.0184927 (* 1 = 0.0184927 loss)
I1107 16:20:36.550870 15588 sgd_solver.cpp:105] Iteration 153700, lr = 0.0001
I1107 16:20:45.139338 15588 solver.cpp:218] Iteration 153800 (11.644 iter/s, 8.58813s/100 iters), loss = 0.0179131
I1107 16:20:45.139338 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:20:45.139338 15588 solver.cpp:237]     Train net output #1: loss = 0.0179133 (* 1 = 0.0179133 loss)
I1107 16:20:45.139338 15588 sgd_solver.cpp:105] Iteration 153800, lr = 0.0001
I1107 16:20:53.672972 15588 solver.cpp:218] Iteration 153900 (11.7197 iter/s, 8.53261s/100 iters), loss = 0.030094
I1107 16:20:53.672972 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:20:53.672972 15588 solver.cpp:237]     Train net output #1: loss = 0.0300942 (* 1 = 0.0300942 loss)
I1107 16:20:53.672972 15588 sgd_solver.cpp:105] Iteration 153900, lr = 0.0001
I1107 16:21:01.843571 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:21:02.183605 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_154000.caffemodel
I1107 16:21:02.218607 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_154000.solverstate
I1107 16:21:02.227607 15588 solver.cpp:330] Iteration 154000, Testing net (#0)
I1107 16:21:02.227607 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:21:04.214762  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:21:04.294769 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1107 16:21:04.294769 15588 solver.cpp:397]     Test net output #1: loss = 0.310357 (* 1 = 0.310357 loss)
I1107 16:21:04.376271 15588 solver.cpp:218] Iteration 154000 (9.34371 iter/s, 10.7024s/100 iters), loss = 0.0387566
I1107 16:21:04.376271 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:21:04.376271 15588 solver.cpp:237]     Train net output #1: loss = 0.0387568 (* 1 = 0.0387568 loss)
I1107 16:21:04.376271 15588 sgd_solver.cpp:105] Iteration 154000, lr = 0.0001
I1107 16:21:12.912443 15588 solver.cpp:218] Iteration 154100 (11.7144 iter/s, 8.53653s/100 iters), loss = 0.0242925
I1107 16:21:12.912443 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:21:12.912443 15588 solver.cpp:237]     Train net output #1: loss = 0.0242927 (* 1 = 0.0242927 loss)
I1107 16:21:12.912443 15588 sgd_solver.cpp:105] Iteration 154100, lr = 0.0001
I1107 16:21:21.419028 15588 solver.cpp:218] Iteration 154200 (11.7562 iter/s, 8.50612s/100 iters), loss = 0.0237255
I1107 16:21:21.419028 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:21:21.420029 15588 solver.cpp:237]     Train net output #1: loss = 0.0237257 (* 1 = 0.0237257 loss)
I1107 16:21:21.420029 15588 sgd_solver.cpp:105] Iteration 154200, lr = 0.0001
I1107 16:21:29.900779 15588 solver.cpp:218] Iteration 154300 (11.792 iter/s, 8.48033s/100 iters), loss = 0.0198545
I1107 16:21:29.900779 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:21:29.900779 15588 solver.cpp:237]     Train net output #1: loss = 0.0198547 (* 1 = 0.0198547 loss)
I1107 16:21:29.900779 15588 sgd_solver.cpp:105] Iteration 154300, lr = 0.0001
I1107 16:21:38.425832 15588 solver.cpp:218] Iteration 154400 (11.7304 iter/s, 8.52489s/100 iters), loss = 0.0285727
I1107 16:21:38.425832 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:21:38.425832 15588 solver.cpp:237]     Train net output #1: loss = 0.0285729 (* 1 = 0.0285729 loss)
I1107 16:21:38.425832 15588 sgd_solver.cpp:105] Iteration 154400, lr = 0.0001
I1107 16:21:46.594748 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:21:46.930778 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_154500.caffemodel
I1107 16:21:46.968786 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_154500.solverstate
I1107 16:21:46.978786 15588 solver.cpp:330] Iteration 154500, Testing net (#0)
I1107 16:21:46.978786 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:21:48.963016  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:21:49.043521 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 16:21:49.043521 15588 solver.cpp:397]     Test net output #1: loss = 0.310061 (* 1 = 0.310061 loss)
I1107 16:21:49.125026 15588 solver.cpp:218] Iteration 154500 (9.34733 iter/s, 10.6982s/100 iters), loss = 0.0322937
I1107 16:21:49.125026 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:21:49.125026 15588 solver.cpp:237]     Train net output #1: loss = 0.0322939 (* 1 = 0.0322939 loss)
I1107 16:21:49.125026 15588 sgd_solver.cpp:105] Iteration 154500, lr = 0.0001
I1107 16:21:57.613956 15588 solver.cpp:218] Iteration 154600 (11.7806 iter/s, 8.48851s/100 iters), loss = 0.0272089
I1107 16:21:57.613956 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:21:57.613956 15588 solver.cpp:237]     Train net output #1: loss = 0.0272091 (* 1 = 0.0272091 loss)
I1107 16:21:57.613956 15588 sgd_solver.cpp:105] Iteration 154600, lr = 0.0001
I1107 16:22:06.245779 15588 solver.cpp:218] Iteration 154700 (11.5855 iter/s, 8.63151s/100 iters), loss = 0.0336447
I1107 16:22:06.245779 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:22:06.245779 15588 solver.cpp:237]     Train net output #1: loss = 0.0336449 (* 1 = 0.0336449 loss)
I1107 16:22:06.245779 15588 sgd_solver.cpp:105] Iteration 154700, lr = 0.0001
I1107 16:22:14.831214 15588 solver.cpp:218] Iteration 154800 (11.6484 iter/s, 8.58488s/100 iters), loss = 0.0198201
I1107 16:22:14.831214 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:22:14.831214 15588 solver.cpp:237]     Train net output #1: loss = 0.0198203 (* 1 = 0.0198203 loss)
I1107 16:22:14.831214 15588 sgd_solver.cpp:105] Iteration 154800, lr = 0.0001
I1107 16:22:23.414979 15588 solver.cpp:218] Iteration 154900 (11.6513 iter/s, 8.58271s/100 iters), loss = 0.0213419
I1107 16:22:23.414979 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:22:23.414979 15588 solver.cpp:237]     Train net output #1: loss = 0.0213421 (* 1 = 0.0213421 loss)
I1107 16:22:23.414979 15588 sgd_solver.cpp:105] Iteration 154900, lr = 0.0001
I1107 16:22:31.561043 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:22:31.901592 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_155000.caffemodel
I1107 16:22:31.937095 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_155000.solverstate
I1107 16:22:31.946094 15588 solver.cpp:330] Iteration 155000, Testing net (#0)
I1107 16:22:31.946094 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:22:33.996337  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:22:34.075345 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9197
I1107 16:22:34.075345 15588 solver.cpp:397]     Test net output #1: loss = 0.309254 (* 1 = 0.309254 loss)
I1107 16:22:34.156352 15588 solver.cpp:218] Iteration 155000 (9.31013 iter/s, 10.741s/100 iters), loss = 0.0365695
I1107 16:22:34.156352 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:22:34.156352 15588 solver.cpp:237]     Train net output #1: loss = 0.0365697 (* 1 = 0.0365697 loss)
I1107 16:22:34.156352 15588 sgd_solver.cpp:105] Iteration 155000, lr = 0.0001
I1107 16:22:42.716047 15588 solver.cpp:218] Iteration 155100 (11.6832 iter/s, 8.55929s/100 iters), loss = 0.0311564
I1107 16:22:42.716047 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:22:42.716047 15588 solver.cpp:237]     Train net output #1: loss = 0.0311566 (* 1 = 0.0311566 loss)
I1107 16:22:42.716047 15588 sgd_solver.cpp:105] Iteration 155100, lr = 0.0001
I1107 16:22:51.262137 15588 solver.cpp:218] Iteration 155200 (11.7015 iter/s, 8.54591s/100 iters), loss = 0.0165746
I1107 16:22:51.263137 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:22:51.263137 15588 solver.cpp:237]     Train net output #1: loss = 0.0165748 (* 1 = 0.0165748 loss)
I1107 16:22:51.263137 15588 sgd_solver.cpp:105] Iteration 155200, lr = 0.0001
I1107 16:22:59.751426 15588 solver.cpp:218] Iteration 155300 (11.7814 iter/s, 8.48797s/100 iters), loss = 0.0191247
I1107 16:22:59.751426 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:22:59.751426 15588 solver.cpp:237]     Train net output #1: loss = 0.0191249 (* 1 = 0.0191249 loss)
I1107 16:22:59.751426 15588 sgd_solver.cpp:105] Iteration 155300, lr = 0.0001
I1107 16:23:08.308243 15588 solver.cpp:218] Iteration 155400 (11.6874 iter/s, 8.55624s/100 iters), loss = 0.0240945
I1107 16:23:08.308243 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:23:08.308243 15588 solver.cpp:237]     Train net output #1: loss = 0.0240948 (* 1 = 0.0240948 loss)
I1107 16:23:08.308243 15588 sgd_solver.cpp:105] Iteration 155400, lr = 0.0001
I1107 16:23:16.477936 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:23:16.816440 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_155500.caffemodel
I1107 16:23:16.853940 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_155500.solverstate
I1107 16:23:16.863440 15588 solver.cpp:330] Iteration 155500, Testing net (#0)
I1107 16:23:16.863945 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:23:18.871095  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:23:18.950096 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 16:23:18.950096 15588 solver.cpp:397]     Test net output #1: loss = 0.309119 (* 1 = 0.309119 loss)
I1107 16:23:19.031116 15588 solver.cpp:218] Iteration 155500 (9.32619 iter/s, 10.7225s/100 iters), loss = 0.0347173
I1107 16:23:19.031116 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:23:19.031116 15588 solver.cpp:237]     Train net output #1: loss = 0.0347176 (* 1 = 0.0347176 loss)
I1107 16:23:19.031116 15588 sgd_solver.cpp:105] Iteration 155500, lr = 0.0001
I1107 16:23:27.607918 15588 solver.cpp:218] Iteration 155600 (11.6594 iter/s, 8.57676s/100 iters), loss = 0.0300081
I1107 16:23:27.607918 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:23:27.607918 15588 solver.cpp:237]     Train net output #1: loss = 0.0300083 (* 1 = 0.0300083 loss)
I1107 16:23:27.607918 15588 sgd_solver.cpp:105] Iteration 155600, lr = 0.0001
I1107 16:23:36.178043 15588 solver.cpp:218] Iteration 155700 (11.6702 iter/s, 8.56884s/100 iters), loss = 0.0393659
I1107 16:23:36.178043 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:23:36.178043 15588 solver.cpp:237]     Train net output #1: loss = 0.0393661 (* 1 = 0.0393661 loss)
I1107 16:23:36.178043 15588 sgd_solver.cpp:105] Iteration 155700, lr = 0.0001
I1107 16:23:44.814185 15588 solver.cpp:218] Iteration 155800 (11.5799 iter/s, 8.63563s/100 iters), loss = 0.0198196
I1107 16:23:44.814185 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:23:44.814185 15588 solver.cpp:237]     Train net output #1: loss = 0.0198198 (* 1 = 0.0198198 loss)
I1107 16:23:44.814185 15588 sgd_solver.cpp:105] Iteration 155800, lr = 0.0001
I1107 16:23:53.494678 15588 solver.cpp:218] Iteration 155900 (11.5203 iter/s, 8.68031s/100 iters), loss = 0.0284202
I1107 16:23:53.494678 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:23:53.494678 15588 solver.cpp:237]     Train net output #1: loss = 0.0284204 (* 1 = 0.0284204 loss)
I1107 16:23:53.494678 15588 sgd_solver.cpp:105] Iteration 155900, lr = 0.0001
I1107 16:24:01.698659 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:24:02.057680 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_156000.caffemodel
I1107 16:24:02.090698 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_156000.solverstate
I1107 16:24:02.099704 15588 solver.cpp:330] Iteration 156000, Testing net (#0)
I1107 16:24:02.099704 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:24:04.206949  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:24:04.292472 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 16:24:04.292472 15588 solver.cpp:397]     Test net output #1: loss = 0.309268 (* 1 = 0.309268 loss)
I1107 16:24:04.381558 15588 solver.cpp:218] Iteration 156000 (9.18629 iter/s, 10.8858s/100 iters), loss = 0.0246974
I1107 16:24:04.381558 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:24:04.381558 15588 solver.cpp:237]     Train net output #1: loss = 0.0246976 (* 1 = 0.0246976 loss)
I1107 16:24:04.381558 15588 sgd_solver.cpp:105] Iteration 156000, lr = 0.0001
I1107 16:24:12.884816 15588 solver.cpp:218] Iteration 156100 (11.7602 iter/s, 8.50329s/100 iters), loss = 0.0278692
I1107 16:24:12.884816 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:24:12.884816 15588 solver.cpp:237]     Train net output #1: loss = 0.0278694 (* 1 = 0.0278694 loss)
I1107 16:24:12.884816 15588 sgd_solver.cpp:105] Iteration 156100, lr = 0.0001
I1107 16:24:21.456413 15588 solver.cpp:218] Iteration 156200 (11.6675 iter/s, 8.57083s/100 iters), loss = 0.0208453
I1107 16:24:21.456413 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:24:21.456413 15588 solver.cpp:237]     Train net output #1: loss = 0.0208455 (* 1 = 0.0208455 loss)
I1107 16:24:21.456413 15588 sgd_solver.cpp:105] Iteration 156200, lr = 0.0001
I1107 16:24:30.017910 15588 solver.cpp:218] Iteration 156300 (11.6806 iter/s, 8.5612s/100 iters), loss = 0.0169167
I1107 16:24:30.017910 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:24:30.017910 15588 solver.cpp:237]     Train net output #1: loss = 0.0169169 (* 1 = 0.0169169 loss)
I1107 16:24:30.017910 15588 sgd_solver.cpp:105] Iteration 156300, lr = 0.0001
I1107 16:24:38.503424 15588 solver.cpp:218] Iteration 156400 (11.7857 iter/s, 8.48489s/100 iters), loss = 0.0229625
I1107 16:24:38.503424 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:24:38.503424 15588 solver.cpp:237]     Train net output #1: loss = 0.0229628 (* 1 = 0.0229628 loss)
I1107 16:24:38.503424 15588 sgd_solver.cpp:105] Iteration 156400, lr = 0.0001
I1107 16:24:46.664438 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:24:47.000957 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_156500.caffemodel
I1107 16:24:47.028460 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_156500.solverstate
I1107 16:24:47.037461 15588 solver.cpp:330] Iteration 156500, Testing net (#0)
I1107 16:24:47.037461 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:24:49.033633  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:24:49.113641 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 16:24:49.113641 15588 solver.cpp:397]     Test net output #1: loss = 0.309354 (* 1 = 0.309354 loss)
I1107 16:24:49.194639 15588 solver.cpp:218] Iteration 156500 (9.35356 iter/s, 10.6911s/100 iters), loss = 0.0436397
I1107 16:24:49.194639 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:24:49.194639 15588 solver.cpp:237]     Train net output #1: loss = 0.0436399 (* 1 = 0.0436399 loss)
I1107 16:24:49.194639 15588 sgd_solver.cpp:105] Iteration 156500, lr = 0.0001
I1107 16:24:57.775487 15588 solver.cpp:218] Iteration 156600 (11.6553 iter/s, 8.5798s/100 iters), loss = 0.0217238
I1107 16:24:57.775487 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:24:57.775487 15588 solver.cpp:237]     Train net output #1: loss = 0.021724 (* 1 = 0.021724 loss)
I1107 16:24:57.775487 15588 sgd_solver.cpp:105] Iteration 156600, lr = 0.0001
I1107 16:25:06.383831 15588 solver.cpp:218] Iteration 156700 (11.617 iter/s, 8.60805s/100 iters), loss = 0.0221578
I1107 16:25:06.384831 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:25:06.384831 15588 solver.cpp:237]     Train net output #1: loss = 0.022158 (* 1 = 0.022158 loss)
I1107 16:25:06.384831 15588 sgd_solver.cpp:105] Iteration 156700, lr = 0.0001
I1107 16:25:14.947576 15588 solver.cpp:218] Iteration 156800 (11.6779 iter/s, 8.56318s/100 iters), loss = 0.0182928
I1107 16:25:14.948586 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:25:14.948586 15588 solver.cpp:237]     Train net output #1: loss = 0.018293 (* 1 = 0.018293 loss)
I1107 16:25:14.948586 15588 sgd_solver.cpp:105] Iteration 156800, lr = 0.0001
I1107 16:25:23.634892 15588 solver.cpp:218] Iteration 156900 (11.5127 iter/s, 8.68603s/100 iters), loss = 0.0229197
I1107 16:25:23.635393 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:25:23.635393 15588 solver.cpp:237]     Train net output #1: loss = 0.0229199 (* 1 = 0.0229199 loss)
I1107 16:25:23.635393 15588 sgd_solver.cpp:105] Iteration 156900, lr = 0.0001
I1107 16:25:31.719669 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:25:32.054198 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_157000.caffemodel
I1107 16:25:32.082702 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_157000.solverstate
I1107 16:25:32.091702 15588 solver.cpp:330] Iteration 157000, Testing net (#0)
I1107 16:25:32.091702 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:25:34.075861  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:25:34.155364 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9203
I1107 16:25:34.155364 15588 solver.cpp:397]     Test net output #1: loss = 0.308698 (* 1 = 0.308698 loss)
I1107 16:25:34.235867 15588 solver.cpp:218] Iteration 157000 (9.43377 iter/s, 10.6002s/100 iters), loss = 0.0294969
I1107 16:25:34.235867 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:25:34.235867 15588 solver.cpp:237]     Train net output #1: loss = 0.0294971 (* 1 = 0.0294971 loss)
I1107 16:25:34.235867 15588 sgd_solver.cpp:105] Iteration 157000, lr = 0.0001
I1107 16:25:42.743649 15588 solver.cpp:218] Iteration 157100 (11.7549 iter/s, 8.50711s/100 iters), loss = 0.0366584
I1107 16:25:42.743649 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:25:42.743649 15588 solver.cpp:237]     Train net output #1: loss = 0.0366587 (* 1 = 0.0366587 loss)
I1107 16:25:42.743649 15588 sgd_solver.cpp:105] Iteration 157100, lr = 0.0001
I1107 16:25:51.222612 15588 solver.cpp:218] Iteration 157200 (11.795 iter/s, 8.47818s/100 iters), loss = 0.0215542
I1107 16:25:51.222612 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:25:51.222612 15588 solver.cpp:237]     Train net output #1: loss = 0.0215544 (* 1 = 0.0215544 loss)
I1107 16:25:51.222612 15588 sgd_solver.cpp:105] Iteration 157200, lr = 0.0001
I1107 16:25:59.708185 15588 solver.cpp:218] Iteration 157300 (11.785 iter/s, 8.48534s/100 iters), loss = 0.0187913
I1107 16:25:59.708185 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:25:59.708185 15588 solver.cpp:237]     Train net output #1: loss = 0.0187915 (* 1 = 0.0187915 loss)
I1107 16:25:59.708185 15588 sgd_solver.cpp:105] Iteration 157300, lr = 0.0001
I1107 16:26:08.192247 15588 solver.cpp:218] Iteration 157400 (11.7874 iter/s, 8.48361s/100 iters), loss = 0.0295029
I1107 16:26:08.192247 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:26:08.192247 15588 solver.cpp:237]     Train net output #1: loss = 0.0295031 (* 1 = 0.0295031 loss)
I1107 16:26:08.192247 15588 sgd_solver.cpp:105] Iteration 157400, lr = 0.0001
I1107 16:26:16.256990 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:26:16.590041 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_157500.caffemodel
I1107 16:26:16.625046 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_157500.solverstate
I1107 16:26:16.635044 15588 solver.cpp:330] Iteration 157500, Testing net (#0)
I1107 16:26:16.635044 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:26:18.620235  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:26:18.699244 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 16:26:18.699244 15588 solver.cpp:397]     Test net output #1: loss = 0.308545 (* 1 = 0.308545 loss)
I1107 16:26:18.780246 15588 solver.cpp:218] Iteration 157500 (9.44518 iter/s, 10.5874s/100 iters), loss = 0.0278274
I1107 16:26:18.780246 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:26:18.780246 15588 solver.cpp:237]     Train net output #1: loss = 0.0278276 (* 1 = 0.0278276 loss)
I1107 16:26:18.780246 15588 sgd_solver.cpp:105] Iteration 157500, lr = 0.0001
I1107 16:26:27.252957 15588 solver.cpp:218] Iteration 157600 (11.8028 iter/s, 8.47258s/100 iters), loss = 0.0268639
I1107 16:26:27.252957 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:26:27.252957 15588 solver.cpp:237]     Train net output #1: loss = 0.0268642 (* 1 = 0.0268642 loss)
I1107 16:26:27.252957 15588 sgd_solver.cpp:105] Iteration 157600, lr = 0.0001
I1107 16:26:35.734658 15588 solver.cpp:218] Iteration 157700 (11.7911 iter/s, 8.48096s/100 iters), loss = 0.0211996
I1107 16:26:35.734658 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:26:35.734658 15588 solver.cpp:237]     Train net output #1: loss = 0.0211998 (* 1 = 0.0211998 loss)
I1107 16:26:35.734658 15588 sgd_solver.cpp:105] Iteration 157700, lr = 0.0001
I1107 16:26:44.205309 15588 solver.cpp:218] Iteration 157800 (11.8064 iter/s, 8.47s/100 iters), loss = 0.018213
I1107 16:26:44.205309 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:26:44.205309 15588 solver.cpp:237]     Train net output #1: loss = 0.0182132 (* 1 = 0.0182132 loss)
I1107 16:26:44.205309 15588 sgd_solver.cpp:105] Iteration 157800, lr = 0.0001
I1107 16:26:52.693604 15588 solver.cpp:218] Iteration 157900 (11.7822 iter/s, 8.48739s/100 iters), loss = 0.0207888
I1107 16:26:52.693604 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:26:52.693604 15588 solver.cpp:237]     Train net output #1: loss = 0.020789 (* 1 = 0.020789 loss)
I1107 16:26:52.693604 15588 sgd_solver.cpp:105] Iteration 157900, lr = 0.0001
I1107 16:27:00.754098 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:27:01.088618 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_158000.caffemodel
I1107 16:27:01.126121 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_158000.solverstate
I1107 16:27:01.135120 15588 solver.cpp:330] Iteration 158000, Testing net (#0)
I1107 16:27:01.135120 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:27:03.121237  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:27:03.201246 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1107 16:27:03.201246 15588 solver.cpp:397]     Test net output #1: loss = 0.309594 (* 1 = 0.309594 loss)
I1107 16:27:03.282246 15588 solver.cpp:218] Iteration 158000 (9.44425 iter/s, 10.5885s/100 iters), loss = 0.0237036
I1107 16:27:03.282246 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:27:03.282246 15588 solver.cpp:237]     Train net output #1: loss = 0.0237038 (* 1 = 0.0237038 loss)
I1107 16:27:03.282246 15588 sgd_solver.cpp:105] Iteration 158000, lr = 0.0001
I1107 16:27:11.769047 15588 solver.cpp:218] Iteration 158100 (11.7835 iter/s, 8.48641s/100 iters), loss = 0.0330925
I1107 16:27:11.769047 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:27:11.769047 15588 solver.cpp:237]     Train net output #1: loss = 0.0330927 (* 1 = 0.0330927 loss)
I1107 16:27:11.769047 15588 sgd_solver.cpp:105] Iteration 158100, lr = 0.0001
I1107 16:27:20.252336 15588 solver.cpp:218] Iteration 158200 (11.7889 iter/s, 8.48254s/100 iters), loss = 0.020037
I1107 16:27:20.252336 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:27:20.252336 15588 solver.cpp:237]     Train net output #1: loss = 0.0200372 (* 1 = 0.0200372 loss)
I1107 16:27:20.252336 15588 sgd_solver.cpp:105] Iteration 158200, lr = 0.0001
I1107 16:27:28.733101 15588 solver.cpp:218] Iteration 158300 (11.7923 iter/s, 8.48014s/100 iters), loss = 0.0260714
I1107 16:27:28.733101 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:27:28.733101 15588 solver.cpp:237]     Train net output #1: loss = 0.0260716 (* 1 = 0.0260716 loss)
I1107 16:27:28.733101 15588 sgd_solver.cpp:105] Iteration 158300, lr = 0.0001
I1107 16:27:37.207036 15588 solver.cpp:218] Iteration 158400 (11.8016 iter/s, 8.47342s/100 iters), loss = 0.0264355
I1107 16:27:37.207036 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:27:37.207036 15588 solver.cpp:237]     Train net output #1: loss = 0.0264357 (* 1 = 0.0264357 loss)
I1107 16:27:37.207036 15588 sgd_solver.cpp:105] Iteration 158400, lr = 0.0001
I1107 16:27:45.268786 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:27:45.603803 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_158500.caffemodel
I1107 16:27:45.639809 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_158500.solverstate
I1107 16:27:45.648809 15588 solver.cpp:330] Iteration 158500, Testing net (#0)
I1107 16:27:45.648809 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:27:47.632952  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:27:47.712455 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 16:27:47.712455 15588 solver.cpp:397]     Test net output #1: loss = 0.309101 (* 1 = 0.309101 loss)
I1107 16:27:47.792971 15588 solver.cpp:218] Iteration 158500 (9.44643 iter/s, 10.586s/100 iters), loss = 0.0333649
I1107 16:27:47.792971 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:27:47.792971 15588 solver.cpp:237]     Train net output #1: loss = 0.0333651 (* 1 = 0.0333651 loss)
I1107 16:27:47.792971 15588 sgd_solver.cpp:105] Iteration 158500, lr = 0.0001
I1107 16:27:56.275516 15588 solver.cpp:218] Iteration 158600 (11.7901 iter/s, 8.4817s/100 iters), loss = 0.0269398
I1107 16:27:56.275516 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:27:56.275516 15588 solver.cpp:237]     Train net output #1: loss = 0.02694 (* 1 = 0.02694 loss)
I1107 16:27:56.275516 15588 sgd_solver.cpp:105] Iteration 158600, lr = 0.0001
I1107 16:28:04.765341 15588 solver.cpp:218] Iteration 158700 (11.7787 iter/s, 8.48989s/100 iters), loss = 0.0204837
I1107 16:28:04.766342 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:28:04.766342 15588 solver.cpp:237]     Train net output #1: loss = 0.0204839 (* 1 = 0.0204839 loss)
I1107 16:28:04.766342 15588 sgd_solver.cpp:105] Iteration 158700, lr = 0.0001
I1107 16:28:13.255175 15588 solver.cpp:218] Iteration 158800 (11.7807 iter/s, 8.48848s/100 iters), loss = 0.0185025
I1107 16:28:13.255175 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:28:13.255175 15588 solver.cpp:237]     Train net output #1: loss = 0.0185027 (* 1 = 0.0185027 loss)
I1107 16:28:13.255175 15588 sgd_solver.cpp:105] Iteration 158800, lr = 0.0001
I1107 16:28:21.724861 15588 solver.cpp:218] Iteration 158900 (11.807 iter/s, 8.46953s/100 iters), loss = 0.0347507
I1107 16:28:21.724861 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:28:21.724861 15588 solver.cpp:237]     Train net output #1: loss = 0.0347508 (* 1 = 0.0347508 loss)
I1107 16:28:21.724861 15588 sgd_solver.cpp:105] Iteration 158900, lr = 0.0001
I1107 16:28:29.778659 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:28:30.111721 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_159000.caffemodel
I1107 16:28:30.143721 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_159000.solverstate
I1107 16:28:30.151723 15588 solver.cpp:330] Iteration 159000, Testing net (#0)
I1107 16:28:30.152721 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:28:32.136945  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:28:32.216949 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9197
I1107 16:28:32.216949 15588 solver.cpp:397]     Test net output #1: loss = 0.310003 (* 1 = 0.310003 loss)
I1107 16:28:32.297451 15588 solver.cpp:218] Iteration 159000 (9.45932 iter/s, 10.5716s/100 iters), loss = 0.0336926
I1107 16:28:32.297451 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:28:32.297451 15588 solver.cpp:237]     Train net output #1: loss = 0.0336928 (* 1 = 0.0336928 loss)
I1107 16:28:32.297451 15588 sgd_solver.cpp:105] Iteration 159000, lr = 0.0001
I1107 16:28:40.773835 15588 solver.cpp:218] Iteration 159100 (11.7971 iter/s, 8.47669s/100 iters), loss = 0.0281329
I1107 16:28:40.773835 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:28:40.773835 15588 solver.cpp:237]     Train net output #1: loss = 0.0281331 (* 1 = 0.0281331 loss)
I1107 16:28:40.773835 15588 sgd_solver.cpp:105] Iteration 159100, lr = 0.0001
I1107 16:28:49.265497 15588 solver.cpp:218] Iteration 159200 (11.7777 iter/s, 8.49059s/100 iters), loss = 0.0172306
I1107 16:28:49.265497 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:28:49.265497 15588 solver.cpp:237]     Train net output #1: loss = 0.0172308 (* 1 = 0.0172308 loss)
I1107 16:28:49.265497 15588 sgd_solver.cpp:105] Iteration 159200, lr = 0.0001
I1107 16:28:57.743376 15588 solver.cpp:218] Iteration 159300 (11.7956 iter/s, 8.47775s/100 iters), loss = 0.0187918
I1107 16:28:57.743376 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:28:57.743376 15588 solver.cpp:237]     Train net output #1: loss = 0.018792 (* 1 = 0.018792 loss)
I1107 16:28:57.743376 15588 sgd_solver.cpp:105] Iteration 159300, lr = 0.0001
I1107 16:29:06.217977 15588 solver.cpp:218] Iteration 159400 (11.8015 iter/s, 8.47346s/100 iters), loss = 0.0213086
I1107 16:29:06.217977 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:29:06.217977 15588 solver.cpp:237]     Train net output #1: loss = 0.0213088 (* 1 = 0.0213088 loss)
I1107 16:29:06.217977 15588 sgd_solver.cpp:105] Iteration 159400, lr = 0.0001
I1107 16:29:14.281699 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:29:14.616725 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_159500.caffemodel
I1107 16:29:14.651726 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_159500.solverstate
I1107 16:29:14.660725 15588 solver.cpp:330] Iteration 159500, Testing net (#0)
I1107 16:29:14.660725 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:29:16.646898  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:29:16.725903 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9203
I1107 16:29:16.725903 15588 solver.cpp:397]     Test net output #1: loss = 0.309183 (* 1 = 0.309183 loss)
I1107 16:29:16.806905 15588 solver.cpp:218] Iteration 159500 (9.44433 iter/s, 10.5884s/100 iters), loss = 0.0248278
I1107 16:29:16.806905 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:29:16.806905 15588 solver.cpp:237]     Train net output #1: loss = 0.024828 (* 1 = 0.024828 loss)
I1107 16:29:16.806905 15588 sgd_solver.cpp:105] Iteration 159500, lr = 0.0001
I1107 16:29:25.282655 15588 solver.cpp:218] Iteration 159600 (11.799 iter/s, 8.47526s/100 iters), loss = 0.0291469
I1107 16:29:25.282655 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:29:25.282655 15588 solver.cpp:237]     Train net output #1: loss = 0.0291471 (* 1 = 0.0291471 loss)
I1107 16:29:25.282655 15588 sgd_solver.cpp:105] Iteration 159600, lr = 0.0001
I1107 16:29:33.759497 15588 solver.cpp:218] Iteration 159700 (11.7977 iter/s, 8.47624s/100 iters), loss = 0.0189563
I1107 16:29:33.759497 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:29:33.759497 15588 solver.cpp:237]     Train net output #1: loss = 0.0189565 (* 1 = 0.0189565 loss)
I1107 16:29:33.759497 15588 sgd_solver.cpp:105] Iteration 159700, lr = 0.0001
I1107 16:29:42.251425 15588 solver.cpp:218] Iteration 159800 (11.7758 iter/s, 8.49202s/100 iters), loss = 0.0189511
I1107 16:29:42.251425 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:29:42.251425 15588 solver.cpp:237]     Train net output #1: loss = 0.0189513 (* 1 = 0.0189513 loss)
I1107 16:29:42.251425 15588 sgd_solver.cpp:105] Iteration 159800, lr = 0.0001
I1107 16:29:50.776712 15588 solver.cpp:218] Iteration 159900 (11.7316 iter/s, 8.52401s/100 iters), loss = 0.024869
I1107 16:29:50.776712 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:29:50.776712 15588 solver.cpp:237]     Train net output #1: loss = 0.0248692 (* 1 = 0.0248692 loss)
I1107 16:29:50.776712 15588 sgd_solver.cpp:105] Iteration 159900, lr = 0.0001
I1107 16:29:58.887706 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:29:59.222234 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_160000.caffemodel
I1107 16:29:59.251243 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_160000.solverstate
I1107 16:29:59.259248 15588 solver.cpp:330] Iteration 160000, Testing net (#0)
I1107 16:29:59.259248 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:30:01.279937  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:30:01.360949 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9196
I1107 16:30:01.360949 15588 solver.cpp:397]     Test net output #1: loss = 0.308722 (* 1 = 0.308722 loss)
I1107 16:30:01.440958 15588 solver.cpp:218] Iteration 160000 (9.37692 iter/s, 10.6645s/100 iters), loss = 0.0373586
I1107 16:30:01.440958 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:30:01.440958 15588 solver.cpp:237]     Train net output #1: loss = 0.0373588 (* 1 = 0.0373588 loss)
I1107 16:30:01.440958 15588 sgd_solver.cpp:105] Iteration 160000, lr = 0.0001
I1107 16:30:09.947175 15588 solver.cpp:218] Iteration 160100 (11.7574 iter/s, 8.50528s/100 iters), loss = 0.0403622
I1107 16:30:09.947175 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:30:09.947175 15588 solver.cpp:237]     Train net output #1: loss = 0.0403624 (* 1 = 0.0403624 loss)
I1107 16:30:09.947175 15588 sgd_solver.cpp:105] Iteration 160100, lr = 0.0001
I1107 16:30:18.434998 15588 solver.cpp:218] Iteration 160200 (11.7819 iter/s, 8.48759s/100 iters), loss = 0.0183755
I1107 16:30:18.434998 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:30:18.434998 15588 solver.cpp:237]     Train net output #1: loss = 0.0183757 (* 1 = 0.0183757 loss)
I1107 16:30:18.434998 15588 sgd_solver.cpp:105] Iteration 160200, lr = 0.0001
I1107 16:30:26.912737 15588 solver.cpp:218] Iteration 160300 (11.7959 iter/s, 8.4775s/100 iters), loss = 0.0191067
I1107 16:30:26.913738 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:30:26.913738 15588 solver.cpp:237]     Train net output #1: loss = 0.0191069 (* 1 = 0.0191069 loss)
I1107 16:30:26.913738 15588 sgd_solver.cpp:105] Iteration 160300, lr = 0.0001
I1107 16:30:35.404451 15588 solver.cpp:218] Iteration 160400 (11.7775 iter/s, 8.49074s/100 iters), loss = 0.0246071
I1107 16:30:35.404451 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:30:35.404451 15588 solver.cpp:237]     Train net output #1: loss = 0.0246073 (* 1 = 0.0246073 loss)
I1107 16:30:35.404451 15588 sgd_solver.cpp:105] Iteration 160400, lr = 0.0001
I1107 16:30:43.467085 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:30:43.803119 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_160500.caffemodel
I1107 16:30:43.838119 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_160500.solverstate
I1107 16:30:43.846119 15588 solver.cpp:330] Iteration 160500, Testing net (#0)
I1107 16:30:43.847120 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:30:45.834259  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:30:45.913264 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 16:30:45.914264 15588 solver.cpp:397]     Test net output #1: loss = 0.309424 (* 1 = 0.309424 loss)
I1107 16:30:45.994267 15588 solver.cpp:218] Iteration 160500 (9.44401 iter/s, 10.5887s/100 iters), loss = 0.0361057
I1107 16:30:45.994267 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:30:45.994267 15588 solver.cpp:237]     Train net output #1: loss = 0.0361059 (* 1 = 0.0361059 loss)
I1107 16:30:45.994267 15588 sgd_solver.cpp:105] Iteration 160500, lr = 0.0001
I1107 16:30:54.487306 15588 solver.cpp:218] Iteration 160600 (11.7756 iter/s, 8.49214s/100 iters), loss = 0.0289187
I1107 16:30:54.487306 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:30:54.487306 15588 solver.cpp:237]     Train net output #1: loss = 0.0289189 (* 1 = 0.0289189 loss)
I1107 16:30:54.487306 15588 sgd_solver.cpp:105] Iteration 160600, lr = 0.0001
I1107 16:31:02.987119 15588 solver.cpp:218] Iteration 160700 (11.7655 iter/s, 8.4994s/100 iters), loss = 0.027837
I1107 16:31:02.987119 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:31:02.987119 15588 solver.cpp:237]     Train net output #1: loss = 0.0278372 (* 1 = 0.0278372 loss)
I1107 16:31:02.987119 15588 sgd_solver.cpp:105] Iteration 160700, lr = 0.0001
I1107 16:31:11.494194 15588 solver.cpp:218] Iteration 160800 (11.7555 iter/s, 8.50666s/100 iters), loss = 0.0175974
I1107 16:31:11.494194 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:31:11.494194 15588 solver.cpp:237]     Train net output #1: loss = 0.0175976 (* 1 = 0.0175976 loss)
I1107 16:31:11.494194 15588 sgd_solver.cpp:105] Iteration 160800, lr = 0.0001
I1107 16:31:19.988013 15588 solver.cpp:218] Iteration 160900 (11.7737 iter/s, 8.49349s/100 iters), loss = 0.0215028
I1107 16:31:19.988013 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:31:19.988013 15588 solver.cpp:237]     Train net output #1: loss = 0.021503 (* 1 = 0.021503 loss)
I1107 16:31:19.988013 15588 sgd_solver.cpp:105] Iteration 160900, lr = 0.0001
I1107 16:31:28.062844 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:31:28.398870 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_161000.caffemodel
I1107 16:31:28.426869 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_161000.solverstate
I1107 16:31:28.434870 15588 solver.cpp:330] Iteration 161000, Testing net (#0)
I1107 16:31:28.434870 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:31:30.420130  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:31:30.500133 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9204
I1107 16:31:30.500133 15588 solver.cpp:397]     Test net output #1: loss = 0.309329 (* 1 = 0.309329 loss)
I1107 16:31:30.580636 15588 solver.cpp:218] Iteration 161000 (9.44149 iter/s, 10.5915s/100 iters), loss = 0.0291585
I1107 16:31:30.580636 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:31:30.580636 15588 solver.cpp:237]     Train net output #1: loss = 0.0291587 (* 1 = 0.0291587 loss)
I1107 16:31:30.580636 15588 sgd_solver.cpp:105] Iteration 161000, lr = 0.0001
I1107 16:31:39.058158 15588 solver.cpp:218] Iteration 161100 (11.7965 iter/s, 8.47707s/100 iters), loss = 0.0464975
I1107 16:31:39.058158 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:31:39.058158 15588 solver.cpp:237]     Train net output #1: loss = 0.0464977 (* 1 = 0.0464977 loss)
I1107 16:31:39.058158 15588 sgd_solver.cpp:105] Iteration 161100, lr = 0.0001
I1107 16:31:47.541081 15588 solver.cpp:218] Iteration 161200 (11.7883 iter/s, 8.48297s/100 iters), loss = 0.0179673
I1107 16:31:47.541081 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:31:47.541081 15588 solver.cpp:237]     Train net output #1: loss = 0.0179675 (* 1 = 0.0179675 loss)
I1107 16:31:47.541081 15588 sgd_solver.cpp:105] Iteration 161200, lr = 0.0001
I1107 16:31:56.038872 15588 solver.cpp:218] Iteration 161300 (11.7684 iter/s, 8.49734s/100 iters), loss = 0.0277938
I1107 16:31:56.038872 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:31:56.038872 15588 solver.cpp:237]     Train net output #1: loss = 0.027794 (* 1 = 0.027794 loss)
I1107 16:31:56.038872 15588 sgd_solver.cpp:105] Iteration 161300, lr = 0.0001
I1107 16:32:04.521561 15588 solver.cpp:218] Iteration 161400 (11.7895 iter/s, 8.48211s/100 iters), loss = 0.0323772
I1107 16:32:04.521561 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:32:04.521561 15588 solver.cpp:237]     Train net output #1: loss = 0.0323774 (* 1 = 0.0323774 loss)
I1107 16:32:04.521561 15588 sgd_solver.cpp:105] Iteration 161400, lr = 0.0001
I1107 16:32:12.592793 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:32:12.927819 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_161500.caffemodel
I1107 16:32:12.962818 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_161500.solverstate
I1107 16:32:12.971818 15588 solver.cpp:330] Iteration 161500, Testing net (#0)
I1107 16:32:12.971818 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:32:14.956024  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:32:15.036031 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9196
I1107 16:32:15.036031 15588 solver.cpp:397]     Test net output #1: loss = 0.308563 (* 1 = 0.308563 loss)
I1107 16:32:15.117036 15588 solver.cpp:218] Iteration 161500 (9.43863 iter/s, 10.5948s/100 iters), loss = 0.037372
I1107 16:32:15.117036 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:32:15.117036 15588 solver.cpp:237]     Train net output #1: loss = 0.0373721 (* 1 = 0.0373721 loss)
I1107 16:32:15.117036 15588 sgd_solver.cpp:105] Iteration 161500, lr = 0.0001
I1107 16:32:23.617965 15588 solver.cpp:218] Iteration 161600 (11.7648 iter/s, 8.49997s/100 iters), loss = 0.0420781
I1107 16:32:23.617965 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:32:23.617965 15588 solver.cpp:237]     Train net output #1: loss = 0.0420783 (* 1 = 0.0420783 loss)
I1107 16:32:23.617965 15588 sgd_solver.cpp:105] Iteration 161600, lr = 0.0001
I1107 16:32:32.102782 15588 solver.cpp:218] Iteration 161700 (11.7856 iter/s, 8.48494s/100 iters), loss = 0.0158728
I1107 16:32:32.102782 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:32:32.102782 15588 solver.cpp:237]     Train net output #1: loss = 0.015873 (* 1 = 0.015873 loss)
I1107 16:32:32.102782 15588 sgd_solver.cpp:105] Iteration 161700, lr = 0.0001
I1107 16:32:40.596016 15588 solver.cpp:218] Iteration 161800 (11.7758 iter/s, 8.49199s/100 iters), loss = 0.0165024
I1107 16:32:40.596016 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:32:40.596016 15588 solver.cpp:237]     Train net output #1: loss = 0.0165026 (* 1 = 0.0165026 loss)
I1107 16:32:40.596016 15588 sgd_solver.cpp:105] Iteration 161800, lr = 0.0001
I1107 16:32:49.071008 15588 solver.cpp:218] Iteration 161900 (11.7992 iter/s, 8.47516s/100 iters), loss = 0.0239342
I1107 16:32:49.071008 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:32:49.071008 15588 solver.cpp:237]     Train net output #1: loss = 0.0239343 (* 1 = 0.0239343 loss)
I1107 16:32:49.071008 15588 sgd_solver.cpp:105] Iteration 161900, lr = 0.0001
I1107 16:32:57.137691 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:32:57.472715 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_162000.caffemodel
I1107 16:32:57.503728 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_162000.solverstate
I1107 16:32:57.512228 15588 solver.cpp:330] Iteration 162000, Testing net (#0)
I1107 16:32:57.512228 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:32:59.496876  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:32:59.575880 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9205
I1107 16:32:59.575880 15588 solver.cpp:397]     Test net output #1: loss = 0.308109 (* 1 = 0.308109 loss)
I1107 16:32:59.656883 15588 solver.cpp:218] Iteration 162000 (9.44707 iter/s, 10.5853s/100 iters), loss = 0.0276624
I1107 16:32:59.656883 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:32:59.656883 15588 solver.cpp:237]     Train net output #1: loss = 0.0276626 (* 1 = 0.0276626 loss)
I1107 16:32:59.656883 15588 sgd_solver.cpp:105] Iteration 162000, lr = 0.0001
I1107 16:33:08.189008 15588 solver.cpp:218] Iteration 162100 (11.7221 iter/s, 8.53088s/100 iters), loss = 0.0308676
I1107 16:33:08.189008 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:33:08.189008 15588 solver.cpp:237]     Train net output #1: loss = 0.0308678 (* 1 = 0.0308678 loss)
I1107 16:33:08.189008 15588 sgd_solver.cpp:105] Iteration 162100, lr = 0.0001
I1107 16:33:16.770436 15588 solver.cpp:218] Iteration 162200 (11.6532 iter/s, 8.5813s/100 iters), loss = 0.0198157
I1107 16:33:16.770936 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:33:16.770936 15588 solver.cpp:237]     Train net output #1: loss = 0.0198158 (* 1 = 0.0198158 loss)
I1107 16:33:16.770936 15588 sgd_solver.cpp:105] Iteration 162200, lr = 0.0001
I1107 16:33:25.487638 15588 solver.cpp:218] Iteration 162300 (11.4723 iter/s, 8.71662s/100 iters), loss = 0.0181466
I1107 16:33:25.487638 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:33:25.487638 15588 solver.cpp:237]     Train net output #1: loss = 0.0181468 (* 1 = 0.0181468 loss)
I1107 16:33:25.487638 15588 sgd_solver.cpp:105] Iteration 162300, lr = 0.0001
I1107 16:33:33.989385 15588 solver.cpp:218] Iteration 162400 (11.763 iter/s, 8.50122s/100 iters), loss = 0.0212474
I1107 16:33:33.989385 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:33:33.989385 15588 solver.cpp:237]     Train net output #1: loss = 0.0212476 (* 1 = 0.0212476 loss)
I1107 16:33:33.989385 15588 sgd_solver.cpp:105] Iteration 162400, lr = 0.0001
I1107 16:33:42.090574 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:33:42.426141 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_162500.caffemodel
I1107 16:33:42.457644 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_162500.solverstate
I1107 16:33:42.466646 15588 solver.cpp:330] Iteration 162500, Testing net (#0)
I1107 16:33:42.466646 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:33:44.454826  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:33:44.534831 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9209
I1107 16:33:44.534831 15588 solver.cpp:397]     Test net output #1: loss = 0.308305 (* 1 = 0.308305 loss)
I1107 16:33:44.614831 15588 solver.cpp:218] Iteration 162500 (9.41187 iter/s, 10.6249s/100 iters), loss = 0.0270583
I1107 16:33:44.615332 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:33:44.615332 15588 solver.cpp:237]     Train net output #1: loss = 0.0270585 (* 1 = 0.0270585 loss)
I1107 16:33:44.615332 15588 sgd_solver.cpp:105] Iteration 162500, lr = 0.0001
I1107 16:33:53.168807 15588 solver.cpp:218] Iteration 162600 (11.6905 iter/s, 8.55399s/100 iters), loss = 0.0400414
I1107 16:33:53.169806 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:33:53.169806 15588 solver.cpp:237]     Train net output #1: loss = 0.0400415 (* 1 = 0.0400415 loss)
I1107 16:33:53.169806 15588 sgd_solver.cpp:105] Iteration 162600, lr = 0.0001
I1107 16:34:01.748996 15588 solver.cpp:218] Iteration 162700 (11.6565 iter/s, 8.57888s/100 iters), loss = 0.0273849
I1107 16:34:01.748996 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:34:01.748996 15588 solver.cpp:237]     Train net output #1: loss = 0.0273851 (* 1 = 0.0273851 loss)
I1107 16:34:01.748996 15588 sgd_solver.cpp:105] Iteration 162700, lr = 0.0001
I1107 16:34:10.333880 15588 solver.cpp:218] Iteration 162800 (11.6485 iter/s, 8.5848s/100 iters), loss = 0.0253768
I1107 16:34:10.333880 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:34:10.333880 15588 solver.cpp:237]     Train net output #1: loss = 0.0253769 (* 1 = 0.0253769 loss)
I1107 16:34:10.333880 15588 sgd_solver.cpp:105] Iteration 162800, lr = 0.0001
I1107 16:34:19.017807 15588 solver.cpp:218] Iteration 162900 (11.5164 iter/s, 8.68326s/100 iters), loss = 0.0204008
I1107 16:34:19.018309 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:34:19.018309 15588 solver.cpp:237]     Train net output #1: loss = 0.020401 (* 1 = 0.020401 loss)
I1107 16:34:19.018309 15588 sgd_solver.cpp:105] Iteration 162900, lr = 0.0001
I1107 16:34:27.268077 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:34:27.603106 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_163000.caffemodel
I1107 16:34:27.630115 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_163000.solverstate
I1107 16:34:27.638115 15588 solver.cpp:330] Iteration 163000, Testing net (#0)
I1107 16:34:27.639117 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:34:29.627828  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:34:29.707347 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9213
I1107 16:34:29.707347 15588 solver.cpp:397]     Test net output #1: loss = 0.307606 (* 1 = 0.307606 loss)
I1107 16:34:29.787350 15588 solver.cpp:218] Iteration 163000 (9.28585 iter/s, 10.7691s/100 iters), loss = 0.0274513
I1107 16:34:29.787350 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:34:29.787350 15588 solver.cpp:237]     Train net output #1: loss = 0.0274515 (* 1 = 0.0274515 loss)
I1107 16:34:29.787350 15588 sgd_solver.cpp:105] Iteration 163000, lr = 0.0001
I1107 16:34:38.475172 15588 solver.cpp:218] Iteration 163100 (11.5109 iter/s, 8.68744s/100 iters), loss = 0.0259841
I1107 16:34:38.475172 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:34:38.475172 15588 solver.cpp:237]     Train net output #1: loss = 0.0259843 (* 1 = 0.0259843 loss)
I1107 16:34:38.475172 15588 sgd_solver.cpp:105] Iteration 163100, lr = 0.0001
I1107 16:34:47.106978 15588 solver.cpp:218] Iteration 163200 (11.5854 iter/s, 8.63153s/100 iters), loss = 0.0220386
I1107 16:34:47.107980 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:34:47.107980 15588 solver.cpp:237]     Train net output #1: loss = 0.0220388 (* 1 = 0.0220388 loss)
I1107 16:34:47.107980 15588 sgd_solver.cpp:105] Iteration 163200, lr = 0.0001
I1107 16:34:55.663920 15588 solver.cpp:218] Iteration 163300 (11.6879 iter/s, 8.55586s/100 iters), loss = 0.0217596
I1107 16:34:55.663920 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:34:55.663920 15588 solver.cpp:237]     Train net output #1: loss = 0.0217598 (* 1 = 0.0217598 loss)
I1107 16:34:55.663920 15588 sgd_solver.cpp:105] Iteration 163300, lr = 0.0001
I1107 16:35:04.196620 15588 solver.cpp:218] Iteration 163400 (11.7206 iter/s, 8.532s/100 iters), loss = 0.0229505
I1107 16:35:04.196620 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:35:04.196620 15588 solver.cpp:237]     Train net output #1: loss = 0.0229507 (* 1 = 0.0229507 loss)
I1107 16:35:04.196620 15588 sgd_solver.cpp:105] Iteration 163400, lr = 0.0001
I1107 16:35:12.341524 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:35:12.684051 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_163500.caffemodel
I1107 16:35:12.714555 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_163500.solverstate
I1107 16:35:12.723565 15588 solver.cpp:330] Iteration 163500, Testing net (#0)
I1107 16:35:12.723565 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:35:14.722710  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:35:14.802716 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9203
I1107 16:35:14.802716 15588 solver.cpp:397]     Test net output #1: loss = 0.307893 (* 1 = 0.307893 loss)
I1107 16:35:14.885236 15588 solver.cpp:218] Iteration 163500 (9.35625 iter/s, 10.688s/100 iters), loss = 0.0307955
I1107 16:35:14.885236 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:35:14.885733 15588 solver.cpp:237]     Train net output #1: loss = 0.0307956 (* 1 = 0.0307956 loss)
I1107 16:35:14.885733 15588 sgd_solver.cpp:105] Iteration 163500, lr = 0.0001
I1107 16:35:23.430845 15588 solver.cpp:218] Iteration 163600 (11.7028 iter/s, 8.54497s/100 iters), loss = 0.0240587
I1107 16:35:23.430845 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:35:23.430845 15588 solver.cpp:237]     Train net output #1: loss = 0.0240588 (* 1 = 0.0240588 loss)
I1107 16:35:23.430845 15588 sgd_solver.cpp:105] Iteration 163600, lr = 0.0001
I1107 16:35:31.940608 15588 solver.cpp:218] Iteration 163700 (11.7522 iter/s, 8.50902s/100 iters), loss = 0.0195205
I1107 16:35:31.940608 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:35:31.940608 15588 solver.cpp:237]     Train net output #1: loss = 0.0195206 (* 1 = 0.0195206 loss)
I1107 16:35:31.940608 15588 sgd_solver.cpp:105] Iteration 163700, lr = 0.0001
I1107 16:35:40.553545 15588 solver.cpp:218] Iteration 163800 (11.6106 iter/s, 8.61282s/100 iters), loss = 0.020184
I1107 16:35:40.553545 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:35:40.553545 15588 solver.cpp:237]     Train net output #1: loss = 0.0201841 (* 1 = 0.0201841 loss)
I1107 16:35:40.553545 15588 sgd_solver.cpp:105] Iteration 163800, lr = 0.0001
I1107 16:35:49.127579 15588 solver.cpp:218] Iteration 163900 (11.6635 iter/s, 8.57374s/100 iters), loss = 0.0236267
I1107 16:35:49.127579 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:35:49.127579 15588 solver.cpp:237]     Train net output #1: loss = 0.0236269 (* 1 = 0.0236269 loss)
I1107 16:35:49.127579 15588 sgd_solver.cpp:105] Iteration 163900, lr = 0.0001
I1107 16:35:57.316201 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:35:57.650214 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_164000.caffemodel
I1107 16:35:57.685220 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_164000.solverstate
I1107 16:35:57.694219 15588 solver.cpp:330] Iteration 164000, Testing net (#0)
I1107 16:35:57.694219 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:35:59.682353  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:35:59.761355 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9204
I1107 16:35:59.761355 15588 solver.cpp:397]     Test net output #1: loss = 0.308105 (* 1 = 0.308105 loss)
I1107 16:35:59.841361 15588 solver.cpp:218] Iteration 164000 (9.33447 iter/s, 10.713s/100 iters), loss = 0.0265037
I1107 16:35:59.841361 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:35:59.841361 15588 solver.cpp:237]     Train net output #1: loss = 0.0265039 (* 1 = 0.0265039 loss)
I1107 16:35:59.841361 15588 sgd_solver.cpp:105] Iteration 164000, lr = 0.0001
I1107 16:36:08.420159 15588 solver.cpp:218] Iteration 164100 (11.6569 iter/s, 8.57864s/100 iters), loss = 0.0302308
I1107 16:36:08.421159 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:36:08.421159 15588 solver.cpp:237]     Train net output #1: loss = 0.030231 (* 1 = 0.030231 loss)
I1107 16:36:08.421159 15588 sgd_solver.cpp:105] Iteration 164100, lr = 0.0001
I1107 16:36:16.989105 15588 solver.cpp:218] Iteration 164200 (11.6722 iter/s, 8.5674s/100 iters), loss = 0.0259676
I1107 16:36:16.989105 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:36:16.989105 15588 solver.cpp:237]     Train net output #1: loss = 0.0259678 (* 1 = 0.0259678 loss)
I1107 16:36:16.989105 15588 sgd_solver.cpp:105] Iteration 164200, lr = 0.0001
I1107 16:36:25.584408 15588 solver.cpp:218] Iteration 164300 (11.6347 iter/s, 8.595s/100 iters), loss = 0.0184262
I1107 16:36:25.584408 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:36:25.584408 15588 solver.cpp:237]     Train net output #1: loss = 0.0184263 (* 1 = 0.0184263 loss)
I1107 16:36:25.584408 15588 sgd_solver.cpp:105] Iteration 164300, lr = 0.0001
I1107 16:36:34.165880 15588 solver.cpp:218] Iteration 164400 (11.6537 iter/s, 8.58097s/100 iters), loss = 0.0219937
I1107 16:36:34.165880 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:36:34.165880 15588 solver.cpp:237]     Train net output #1: loss = 0.0219939 (* 1 = 0.0219939 loss)
I1107 16:36:34.165880 15588 sgd_solver.cpp:105] Iteration 164400, lr = 0.0001
I1107 16:36:42.327739 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:36:42.670855 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_164500.caffemodel
I1107 16:36:42.700866 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_164500.solverstate
I1107 16:36:42.709863 15588 solver.cpp:330] Iteration 164500, Testing net (#0)
I1107 16:36:42.709863 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:36:44.727010  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:36:44.807015 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 16:36:44.807015 15588 solver.cpp:397]     Test net output #1: loss = 0.309088 (* 1 = 0.309088 loss)
I1107 16:36:44.888517 15588 solver.cpp:218] Iteration 164500 (9.32654 iter/s, 10.7221s/100 iters), loss = 0.0250379
I1107 16:36:44.888517 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:36:44.888517 15588 solver.cpp:237]     Train net output #1: loss = 0.025038 (* 1 = 0.025038 loss)
I1107 16:36:44.888517 15588 sgd_solver.cpp:105] Iteration 164500, lr = 0.0001
I1107 16:36:53.512017 15588 solver.cpp:218] Iteration 164600 (11.5969 iter/s, 8.62296s/100 iters), loss = 0.0203774
I1107 16:36:53.512017 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:36:53.512017 15588 solver.cpp:237]     Train net output #1: loss = 0.0203776 (* 1 = 0.0203776 loss)
I1107 16:36:53.512017 15588 sgd_solver.cpp:105] Iteration 164600, lr = 0.0001
I1107 16:37:02.092574 15588 solver.cpp:218] Iteration 164700 (11.6541 iter/s, 8.58066s/100 iters), loss = 0.0221365
I1107 16:37:02.092574 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:37:02.092574 15588 solver.cpp:237]     Train net output #1: loss = 0.0221366 (* 1 = 0.0221366 loss)
I1107 16:37:02.092574 15588 sgd_solver.cpp:105] Iteration 164700, lr = 0.0001
I1107 16:37:10.710062 15588 solver.cpp:218] Iteration 164800 (11.606 iter/s, 8.61622s/100 iters), loss = 0.0217391
I1107 16:37:10.710062 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:37:10.710062 15588 solver.cpp:237]     Train net output #1: loss = 0.0217393 (* 1 = 0.0217393 loss)
I1107 16:37:10.710062 15588 sgd_solver.cpp:105] Iteration 164800, lr = 0.0001
I1107 16:37:19.303155 15588 solver.cpp:218] Iteration 164900 (11.6372 iter/s, 8.59312s/100 iters), loss = 0.0221318
I1107 16:37:19.303155 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:37:19.303155 15588 solver.cpp:237]     Train net output #1: loss = 0.0221319 (* 1 = 0.0221319 loss)
I1107 16:37:19.303155 15588 sgd_solver.cpp:105] Iteration 164900, lr = 0.0001
I1107 16:37:27.478894 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:37:27.816295 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_165000.caffemodel
I1107 16:37:27.845294 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_165000.solverstate
I1107 16:37:27.854276 15588 solver.cpp:330] Iteration 165000, Testing net (#0)
I1107 16:37:27.854276 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:37:29.848696  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:37:29.928702 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9197
I1107 16:37:29.928702 15588 solver.cpp:397]     Test net output #1: loss = 0.308914 (* 1 = 0.308914 loss)
I1107 16:37:30.008739 15588 solver.cpp:218] Iteration 165000 (9.34151 iter/s, 10.7049s/100 iters), loss = 0.0249441
I1107 16:37:30.008739 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:37:30.008739 15588 solver.cpp:237]     Train net output #1: loss = 0.0249443 (* 1 = 0.0249443 loss)
I1107 16:37:30.008739 15588 sgd_solver.cpp:105] Iteration 165000, lr = 0.0001
I1107 16:37:38.529968 15588 solver.cpp:218] Iteration 165100 (11.736 iter/s, 8.5208s/100 iters), loss = 0.0272329
I1107 16:37:38.529968 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:37:38.529968 15588 solver.cpp:237]     Train net output #1: loss = 0.027233 (* 1 = 0.027233 loss)
I1107 16:37:38.529968 15588 sgd_solver.cpp:105] Iteration 165100, lr = 0.0001
I1107 16:37:47.103024 15588 solver.cpp:218] Iteration 165200 (11.6651 iter/s, 8.57258s/100 iters), loss = 0.0194913
I1107 16:37:47.103024 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:37:47.103024 15588 solver.cpp:237]     Train net output #1: loss = 0.0194915 (* 1 = 0.0194915 loss)
I1107 16:37:47.103024 15588 sgd_solver.cpp:105] Iteration 165200, lr = 0.0001
I1107 16:37:55.694633 15588 solver.cpp:218] Iteration 165300 (11.6411 iter/s, 8.59029s/100 iters), loss = 0.0191147
I1107 16:37:55.694633 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:37:55.694633 15588 solver.cpp:237]     Train net output #1: loss = 0.0191148 (* 1 = 0.0191148 loss)
I1107 16:37:55.694633 15588 sgd_solver.cpp:105] Iteration 165300, lr = 0.0001
I1107 16:38:04.255228 15588 solver.cpp:218] Iteration 165400 (11.6819 iter/s, 8.56025s/100 iters), loss = 0.0244827
I1107 16:38:04.255228 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:38:04.255228 15588 solver.cpp:237]     Train net output #1: loss = 0.0244829 (* 1 = 0.0244829 loss)
I1107 16:38:04.255228 15588 sgd_solver.cpp:105] Iteration 165400, lr = 0.0001
I1107 16:38:12.339174 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:38:12.675288 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_165500.caffemodel
I1107 16:38:12.710295 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_165500.solverstate
I1107 16:38:12.719297 15588 solver.cpp:330] Iteration 165500, Testing net (#0)
I1107 16:38:12.719297 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:38:14.708675  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:38:14.787679 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 16:38:14.787679 15588 solver.cpp:397]     Test net output #1: loss = 0.308805 (* 1 = 0.308805 loss)
I1107 16:38:14.868686 15588 solver.cpp:218] Iteration 165500 (9.42225 iter/s, 10.6132s/100 iters), loss = 0.034299
I1107 16:38:14.868686 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:38:14.868686 15588 solver.cpp:237]     Train net output #1: loss = 0.0342992 (* 1 = 0.0342992 loss)
I1107 16:38:14.868686 15588 sgd_solver.cpp:105] Iteration 165500, lr = 0.0001
I1107 16:38:23.407599 15588 solver.cpp:218] Iteration 165600 (11.7122 iter/s, 8.5381s/100 iters), loss = 0.0291934
I1107 16:38:23.407599 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:38:23.407599 15588 solver.cpp:237]     Train net output #1: loss = 0.0291935 (* 1 = 0.0291935 loss)
I1107 16:38:23.407599 15588 sgd_solver.cpp:105] Iteration 165600, lr = 0.0001
I1107 16:38:31.944835 15588 solver.cpp:218] Iteration 165700 (11.714 iter/s, 8.53683s/100 iters), loss = 0.0180006
I1107 16:38:31.945338 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:38:31.945338 15588 solver.cpp:237]     Train net output #1: loss = 0.0180008 (* 1 = 0.0180008 loss)
I1107 16:38:31.945338 15588 sgd_solver.cpp:105] Iteration 165700, lr = 0.0001
I1107 16:38:40.543823 15588 solver.cpp:218] Iteration 165800 (11.6304 iter/s, 8.59818s/100 iters), loss = 0.0178548
I1107 16:38:40.543823 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:38:40.543823 15588 solver.cpp:237]     Train net output #1: loss = 0.017855 (* 1 = 0.017855 loss)
I1107 16:38:40.543823 15588 sgd_solver.cpp:105] Iteration 165800, lr = 0.0001
I1107 16:38:49.137848 15588 solver.cpp:218] Iteration 165900 (11.6363 iter/s, 8.59377s/100 iters), loss = 0.0205718
I1107 16:38:49.137848 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:38:49.137848 15588 solver.cpp:237]     Train net output #1: loss = 0.020572 (* 1 = 0.020572 loss)
I1107 16:38:49.137848 15588 sgd_solver.cpp:105] Iteration 165900, lr = 0.0001
I1107 16:38:57.297915 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:38:57.634932 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_166000.caffemodel
I1107 16:38:57.662943 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_166000.solverstate
I1107 16:38:57.671938 15588 solver.cpp:330] Iteration 166000, Testing net (#0)
I1107 16:38:57.671938 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:38:59.675068  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:38:59.755571 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9195
I1107 16:38:59.755571 15588 solver.cpp:397]     Test net output #1: loss = 0.309058 (* 1 = 0.309058 loss)
I1107 16:38:59.837071 15588 solver.cpp:218] Iteration 166000 (9.3472 iter/s, 10.6984s/100 iters), loss = 0.0365534
I1107 16:38:59.837071 15588 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:38:59.837071 15588 solver.cpp:237]     Train net output #1: loss = 0.0365536 (* 1 = 0.0365536 loss)
I1107 16:38:59.837071 15588 sgd_solver.cpp:105] Iteration 166000, lr = 0.0001
I1107 16:39:08.374706 15588 solver.cpp:218] Iteration 166100 (11.7128 iter/s, 8.53764s/100 iters), loss = 0.0224061
I1107 16:39:08.374706 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:39:08.374706 15588 solver.cpp:237]     Train net output #1: loss = 0.0224063 (* 1 = 0.0224063 loss)
I1107 16:39:08.374706 15588 sgd_solver.cpp:105] Iteration 166100, lr = 0.0001
I1107 16:39:16.952049 15588 solver.cpp:218] Iteration 166200 (11.6598 iter/s, 8.5765s/100 iters), loss = 0.022973
I1107 16:39:16.952049 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:39:16.952049 15588 solver.cpp:237]     Train net output #1: loss = 0.0229731 (* 1 = 0.0229731 loss)
I1107 16:39:16.952049 15588 sgd_solver.cpp:105] Iteration 166200, lr = 0.0001
I1107 16:39:25.499212 15588 solver.cpp:218] Iteration 166300 (11.6997 iter/s, 8.54721s/100 iters), loss = 0.0193067
I1107 16:39:25.499212 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:39:25.499212 15588 solver.cpp:237]     Train net output #1: loss = 0.0193069 (* 1 = 0.0193069 loss)
I1107 16:39:25.499212 15588 sgd_solver.cpp:105] Iteration 166300, lr = 0.0001
I1107 16:39:33.978049 15588 solver.cpp:218] Iteration 166400 (11.7947 iter/s, 8.47841s/100 iters), loss = 0.0229798
I1107 16:39:33.979048 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:39:33.979048 15588 solver.cpp:237]     Train net output #1: loss = 0.0229799 (* 1 = 0.0229799 loss)
I1107 16:39:33.979048 15588 sgd_solver.cpp:105] Iteration 166400, lr = 0.0001
I1107 16:39:42.117511 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:39:42.452267 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_166500.caffemodel
I1107 16:39:42.487810 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_166500.solverstate
I1107 16:39:42.496811 15588 solver.cpp:330] Iteration 166500, Testing net (#0)
I1107 16:39:42.497812 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:39:44.495609  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:39:44.575642 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9196
I1107 16:39:44.575642 15588 solver.cpp:397]     Test net output #1: loss = 0.30903 (* 1 = 0.30903 loss)
I1107 16:39:44.656159 15588 solver.cpp:218] Iteration 166500 (9.36592 iter/s, 10.677s/100 iters), loss = 0.0245564
I1107 16:39:44.656159 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:39:44.656159 15588 solver.cpp:237]     Train net output #1: loss = 0.0245566 (* 1 = 0.0245566 loss)
I1107 16:39:44.656159 15588 sgd_solver.cpp:105] Iteration 166500, lr = 0.0001
I1107 16:39:53.264945 15588 solver.cpp:218] Iteration 166600 (11.6172 iter/s, 8.60792s/100 iters), loss = 0.0217249
I1107 16:39:53.264945 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:39:53.264945 15588 solver.cpp:237]     Train net output #1: loss = 0.021725 (* 1 = 0.021725 loss)
I1107 16:39:53.264945 15588 sgd_solver.cpp:105] Iteration 166600, lr = 0.0001
I1107 16:40:01.983388 15588 solver.cpp:218] Iteration 166700 (11.4706 iter/s, 8.71793s/100 iters), loss = 0.0205037
I1107 16:40:01.983388 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:40:01.983388 15588 solver.cpp:237]     Train net output #1: loss = 0.0205039 (* 1 = 0.0205039 loss)
I1107 16:40:01.983388 15588 sgd_solver.cpp:105] Iteration 166700, lr = 0.0001
I1107 16:40:10.505911 15588 solver.cpp:218] Iteration 166800 (11.7342 iter/s, 8.52211s/100 iters), loss = 0.0144217
I1107 16:40:10.505911 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:40:10.505911 15588 solver.cpp:237]     Train net output #1: loss = 0.0144219 (* 1 = 0.0144219 loss)
I1107 16:40:10.505911 15588 sgd_solver.cpp:105] Iteration 166800, lr = 0.0001
I1107 16:40:19.021661 15588 solver.cpp:218] Iteration 166900 (11.7434 iter/s, 8.51543s/100 iters), loss = 0.0238283
I1107 16:40:19.021661 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:40:19.021661 15588 solver.cpp:237]     Train net output #1: loss = 0.0238285 (* 1 = 0.0238285 loss)
I1107 16:40:19.021661 15588 sgd_solver.cpp:105] Iteration 166900, lr = 0.0001
I1107 16:40:27.144695 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:40:27.493677 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_167000.caffemodel
I1107 16:40:27.526679 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_167000.solverstate
I1107 16:40:27.535679 15588 solver.cpp:330] Iteration 167000, Testing net (#0)
I1107 16:40:27.535679 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:40:29.578874  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:40:29.659404 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 16:40:29.659404 15588 solver.cpp:397]     Test net output #1: loss = 0.309329 (* 1 = 0.309329 loss)
I1107 16:40:29.742430 15588 solver.cpp:218] Iteration 167000 (9.32783 iter/s, 10.7206s/100 iters), loss = 0.0256722
I1107 16:40:29.742430 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:40:29.742430 15588 solver.cpp:237]     Train net output #1: loss = 0.0256723 (* 1 = 0.0256723 loss)
I1107 16:40:29.742430 15588 sgd_solver.cpp:105] Iteration 167000, lr = 0.0001
I1107 16:40:38.358249 15588 solver.cpp:218] Iteration 167100 (11.6076 iter/s, 8.61507s/100 iters), loss = 0.0232627
I1107 16:40:38.358249 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:40:38.358249 15588 solver.cpp:237]     Train net output #1: loss = 0.0232628 (* 1 = 0.0232628 loss)
I1107 16:40:38.358249 15588 sgd_solver.cpp:105] Iteration 167100, lr = 0.0001
I1107 16:40:46.884527 15588 solver.cpp:218] Iteration 167200 (11.7294 iter/s, 8.52561s/100 iters), loss = 0.0198312
I1107 16:40:46.884527 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:40:46.885028 15588 solver.cpp:237]     Train net output #1: loss = 0.0198313 (* 1 = 0.0198313 loss)
I1107 16:40:46.885028 15588 sgd_solver.cpp:105] Iteration 167200, lr = 0.0001
I1107 16:40:55.400401 15588 solver.cpp:218] Iteration 167300 (11.7441 iter/s, 8.51494s/100 iters), loss = 0.0170946
I1107 16:40:55.400401 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:40:55.400401 15588 solver.cpp:237]     Train net output #1: loss = 0.0170948 (* 1 = 0.0170948 loss)
I1107 16:40:55.400401 15588 sgd_solver.cpp:105] Iteration 167300, lr = 0.0001
I1107 16:41:03.974321 15588 solver.cpp:218] Iteration 167400 (11.6627 iter/s, 8.57432s/100 iters), loss = 0.0244945
I1107 16:41:03.975323 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:41:03.975323 15588 solver.cpp:237]     Train net output #1: loss = 0.0244947 (* 1 = 0.0244947 loss)
I1107 16:41:03.975323 15588 sgd_solver.cpp:105] Iteration 167400, lr = 0.0001
I1107 16:41:12.096133 10064 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:41:12.433182 15588 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_167500.caffemodel
I1107 16:41:12.467182 15588 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_167500.solverstate
I1107 16:41:12.476182 15588 solver.cpp:330] Iteration 167500, Testing net (#0)
I1107 16:41:12.476182 15588 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:41:14.481343  3600 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:41:14.561357 15588 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1107 16:41:14.561357 15588 solver.cpp:397]     Test net output #1: loss = 0.30865 (* 1 = 0.30865 loss)
I1107 16:41:14.642376 15588 solver.cpp:218] Iteration 167500 (9.37468 iter/s, 10.667s/100 iters), loss = 0.0290345
I1107 16:41:14.642376 15588 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:41:14.642376 15588 solver.cpp:237]     Train net output #1: loss = 0.0290346 (* 1 = 0.0290346 loss)
I1107 16:41:14.642376 15588 sgd_solver.
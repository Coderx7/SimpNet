
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1107 10:17:47.625593 11160 caffe.cpp:219] Using GPUs 0
I1107 10:17:47.800374 11160 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1107 10:17:48.087508 11160 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1107 10:17:48.103528 11160 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/squeezenet_batchnorm"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 195000
stepvalue: 220000
stepvalue: 270000
type: "AdaDelta"
I1107 10:17:48.104529 11160 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1107 10:17:48.105510 11160 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1107 10:17:48.105510 11160 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_3
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_4
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_5
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_6
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_7
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_8
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_9
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_10
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_11
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_12
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_13
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_14
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_15
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_16
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_17
I1107 10:17:48.105510 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_18
I1107 10:17:48.106528 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_19
I1107 10:17:48.106528 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_20
I1107 10:17:48.106528 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_21
I1107 10:17:48.106528 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_22
I1107 10:17:48.129524 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_23
I1107 10:17:48.129524 11160 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1107 10:17:48.129524 11160 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_Squeezenet_1.1_Batchnorm"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "fire2/squeeze1x1"
  type: "Convolution"
  bottom: "pool1"
  top: "fire2/squeeze1x1"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire2/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire2/squeeze1x1"
  top: "fire2/squeeze1x1"
}
layer {
  name: "fire2/expand1x1"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "conv2"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "fire2/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire2/relu_expand1x1"
  type: "ReLU"
  bottom: "fire2/expand1x1"
  top: "fire2/expand1x1"
}
layer {
  name: "fire2/expand3x3"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "conv_3"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_3"
  type: "BatchNorm"
  bottom: "conv_3"
  top: "conv_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_3"
  type: "Scale"
  bottom: "conv_3"
  top: "fire2/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire2/relu_expand3x3"
  type: "ReLU"
  bottom: "fire2/expand3x3"
  top: "fire2/expand3x3"
}
layer {
  name: "fire2/concat"
  type: "Concat"
  bottom: "fire2/expand1x1"
  bottom: "fire2/expand3x3"
  top: "fire2/concat"
}
layer {
  name: "fire3/squeeze1x1"
  type: "Convolution"
  bottom: "fire2/concat"
  top: "conv_4"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_4"
  type: "BatchNorm"
  bottom: "conv_4"
  top: "conv_4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_4"
  type: "Scale"
  bottom: "conv_4"
  top: "fire3/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire3/squeeze1x1"
  top: "fire3/squeeze1x1"
}
layer {
  name: "fire3/expand1x1"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "conv_5"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_5"
  type: "BatchNorm"
  bottom: "conv_5"
  top: "conv_5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_5"
  type: "Scale"
  bottom: "conv_5"
  top: "fire3/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_expand1x1"
  type: "ReLU"
  bottom: "fire3/expand1x1"
  top: "fire3/expand1x1"
}
layer {
  name: "fire3/expand3x3"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "conv_6"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_6"
  type: "BatchNorm"
  bottom: "conv_6"
  top: "conv_6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_6"
  type: "Scale"
  bottom: "conv_6"
  top: "fire3/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_expand3x3"
  type: "ReLU"
  bottom: "fire3/expand3x3"
  top: "fire3/expand3x3"
}
layer {
  name: "fire3/concat"
  type: "Concat"
  bottom: "fire3/expand1x1"
  bottom: "fire3/expand3x3"
  top: "fire3/concat"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "fire3/concat"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire4/squeeze1x1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv_7"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_7"
  type: "BatchNorm"
  bottom: "conv_7"
  top: "conv_7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_7"
  type: "Scale"
  bottom: "conv_7"
  top: "fire4/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire4/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire4/squeeze1x1"
  top: "fire4/squeeze1x1"
}
layer {
  name: "fire4/expand1x1"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "fire4/expand1x1"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire4/relu_expand1x1"
  type: "ReLU"
  bottom: "fire4/expand1x1"
  top: "fire4/expand1x1"
}
layer {
  name: "fire4/expand3x3"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "conv_8"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_8"
  type: "BatchNorm"
  bottom: "conv_8"
  top: "conv_8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_8"
  type: "Scale"
  bottom: "conv_8"
  top: "fire4/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire4/relu_expand3x3"
  type: "ReLU"
  bottom: "fire4/expand3x3"
  top: "fire4/expand3x3"
}
layer {
  name: "fire4/concat"
  type: "Concat"
  bottom: "fire4/expand1x1"
  bottom: "fire4/expand3x3"
  top: "fire4/concat"
}
layer {
  name: "fire5/squeeze1x1"
  type: "Convolution"
  bottom: "fire4/concat"
  top: "conv_9"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_9"
  type: "BatchNorm"
  bottom: "conv_9"
  top: "conv_9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_9"
  type: "Scale"
  bottom: "conv_9"
  top: "fire5/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire5/squeeze1x1"
  top: "fire5/squeeze1x1"
}
layer {
  name: "fire5/expand1x1"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "conv_10"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_10"
  type: "BatchNorm"
  bottom: "conv_10"
  top: "conv_10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_10"
  type: "Scale"
  bottom: "conv_10"
  top: "fire5/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_expand1x1"
  type: "ReLU"
  bottom: "fire5/expand1x1"
  top: "fire5/expand1x1"
}
layer {
  name: "fire5/expand3x3"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "conv_11"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_11"
  type: "BatchNorm"
  bottom: "conv_11"
  top: "conv_11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_11"
  type: "Scale"
  bottom: "conv_11"
  top: "fire5/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_expand3x3"
  type: "ReLU"
  bottom: "fire5/expand3x3"
  top: "fire5/expand3x3"
}
layer {
  name: "fire5/concat"
  type: "Concat"
  bottom: "fire5/expand1x1"
  bottom: "fire5/expand3x3"
  top: "fire5/concat"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "fire5/concat"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire6/squeeze1x1"
  type: "Convolution"
  bottom: "pool5"
  top: "conv_12"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_12"
  type: "BatchNorm"
  bottom: "conv_12"
  top: "conv_12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_12"
  type: "Scale"
  bottom: "conv_12"
  top: "fire6/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire6/squeeze1x1"
  top: "fire6/squeeze1x1"
}
layer {
  name: "fire6/expand1x1"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "conv_13"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_13"
  type: "BatchNorm"
  bottom: "conv_13"
  top: "conv_13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_13"
  type: "Scale"
  bottom: "conv_13"
  top: "fire6/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_expand1x1"
  type: "ReLU"
  bottom: "fire6/expand1x1"
  top: "fire6/expand1x1"
}
layer {
  name: "fire6/expand3x3"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "conv_14"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_14"
  type: "BatchNorm"
  bottom: "conv_14"
  top: "conv_14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_14"
  type: "Scale"
  bottom: "conv_14"
  top: "fire6/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_expand3x3"
  type: "ReLU"
  bottom: "fire6/expand3x3"
  top: "fire6/expand3x3"
}
layer {
  name: "fire6/concat"
  type: "Concat"
  bottom: "fire6/expand1x1"
  bottom: "fire6/expand3x3"
  top: "fire6/concat"
}
layer {
  name: "fire7/squeeze1x1"
  type: "Convolution"
  bottom: "fire6/concat"
  top: "conv_15"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_15"
  type: "BatchNorm"
  bottom: "conv_15"
  top: "conv_15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_15"
  type: "Scale"
  bottom: "conv_15"
  top: "fire7/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire7/squeeze1x1"
  top: "fire7/squeeze1x1"
}
layer {
  name: "fire7/expand1x1"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "conv_16"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_16"
  type: "BatchNorm"
  bottom: "conv_16"
  top: "conv_16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_16"
  type: "Scale"
  bottom: "conv_16"
  top: "fire7/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_expand1x1"
  type: "ReLU"
  bottom: "fire7/expand1x1"
  top: "fire7/expand1x1"
}
layer {
  name: "fire7/expand3x3"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "conv_17"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_17"
  type: "BatchNorm"
  bottom: "conv_17"
  top: "conv_17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_17"
  type: "Scale"
  bottom: "conv_17"
  top: "fire7/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_expand3x3"
  type: "ReLU"
  bottom: "fire7/expand3x3"
  top: "fire7/expand3x3"
}
layer {
  name: "fire7/concat"
  type: "Concat"
  bottom: "fire7/expand1x1"
  bottom: "fire7/expand3x3"
  top: "fire7/concat"
}
layer {
  name: "fire8/squeeze1x1"
  type: "Convolution"
  bottom: "fire7/concat"
  top: "conv_18"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_18"
  type: "BatchNorm"
  bottom: "conv_18"
  top: "conv_18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_18"
  type: "Scale"
  bottom: "conv_18"
  top: "fire8/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire8/squeeze1x1"
  top: "fire8/squeeze1x1"
}
layer {
  name: "fire8/expand1x1"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "conv_19"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_19"
  type: "BatchNorm"
  bottom: "conv_19"
  top: "conv_19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_19"
  type: "Scale"
  bottom: "conv_19"
  top: "fire8/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_expand1x1"
  type: "ReLU"
  bottom: "fire8/expand1x1"
  top: "fire8/expand1x1"
}
layer {
  name: "fire8/expand3x3"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "conv_20"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_20"
  type: "BatchNorm"
  bottom: "conv_20"
  top: "conv_20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_20"
  type: "Scale"
  bottom: "conv_20"
  top: "fire8/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_expand3x3"
  type: "ReLU"
  bottom: "fire8/expand3x3"
  top: "fire8/expand3x3"
}
layer {
  name: "fire8/concat"
  type: "Concat"
  bottom: "fire8/expand1x1"
  bottom: "fire8/expand3x3"
  top: "fire8/concat"
}
layer {
  name: "fire9/squeeze1x1"
  type: "Convolution"
  bottom: "fire8/concat"
  top: "conv_21"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_21"
  type: "BatchNorm"
  bottom: "conv_21"
  top: "conv_21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_21"
  type: "Scale"
  bottom: "conv_21"
  top: "fire9/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire9/squeeze1x1"
  top: "fire9/squeeze1x1"
}
layer {
  name: "fire9/expand1x1"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "conv_22"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_22"
  type: "BatchNorm"
  bottom: "conv_22"
  top: "conv_22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_22"
  type: "Scale"
  bottom: "conv_22"
  top: "fire9/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_expand1x1"
  type: "ReLU"
  bottom: "fire9/expand1x1"
  top: "fire9/expand1x1"
}
layer {
  name: "fire9/expand3x3"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "conv_23"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_23"
  type: "BatchNorm"
  bottom: "conv_23"
  top: "conv_23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_23"
  type: "Scale"
  bottom: "conv_23"
  top: "fire9/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_expand3x3"
  type: "ReLU"
  bottom: "fire9/expand3x3"
  top: "fire9/expand3x3"
}
layer {
  name: "fire9/concat"
  type: "Concat"
  bottom: "fire9/expand1x1"
  bottom: "fire9/expand3x3"
  top: "fire9/concat"
}
layer {
  name: "drop9"
  type: "Dropout"
  bottom: "fire9/concat"
  top: "fire9/concat"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv10"
  type: "Convolution"
  bottom: "fire9/concat"
  top: "conv10"
  convolution_param {
    num_output: 10
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
  }
}
layer {
  name: "relu_conv10"
  type: "ReLU"
  bottom: "conv10"
  top: "conv10"
}
layer {
  name: "pool10"
  type: "Pooling"
  bottom: "conv10"
  top: "pool10"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "pool10"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "pool10"
  bottom: "label"
  top: "loss"
}
I1107 10:17:48.130524 11160 layer_factory.cpp:58] Creating layer cifar
I1107 10:17:48.135521 11160 db_leveldb.cpp:18] Opened leveldb examples/cifar10/cifar10_train_leveldb_padding
I1107 10:17:48.136524 11160 net.cpp:84] Creating Layer cifar
I1107 10:17:48.136524 11160 net.cpp:380] cifar -> data
I1107 10:17:48.136524 11160 net.cpp:380] cifar -> label
I1107 10:17:48.137519 11160 data_layer.cpp:45] output data size: 100,3,32,32
I1107 10:17:48.141530 11160 net.cpp:122] Setting up cifar
I1107 10:17:48.141530 11160 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1107 10:17:48.141530 11160 net.cpp:129] Top shape: 100 (100)
I1107 10:17:48.141530 11160 net.cpp:137] Memory required for data: 1229200
I1107 10:17:48.141530 11160 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1107 10:17:48.142529 11160 net.cpp:84] Creating Layer label_cifar_1_split
I1107 10:17:48.142529 11160 net.cpp:406] label_cifar_1_split <- label
I1107 10:17:48.142529 11160 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1107 10:17:48.142529 11160 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1107 10:17:48.142529 11160 net.cpp:122] Setting up label_cifar_1_split
I1107 10:17:48.142529 11160 net.cpp:129] Top shape: 100 (100)
I1107 10:17:48.142529 11160 net.cpp:129] Top shape: 100 (100)
I1107 10:17:48.142529 11160 net.cpp:137] Memory required for data: 1230000
I1107 10:17:48.142529 11160 layer_factory.cpp:58] Creating layer conv1
I1107 10:17:48.142529 11160 net.cpp:84] Creating Layer conv1
I1107 10:17:48.142529 11160 net.cpp:406] conv1 <- data
I1107 10:17:48.142529 11160 net.cpp:380] conv1 -> conv1
I1107 10:17:48.144529  1048 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1107 10:17:48.390063 11160 net.cpp:122] Setting up conv1
I1107 10:17:48.390063 11160 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 10:17:48.390063 11160 net.cpp:137] Memory required for data: 24270000
I1107 10:17:48.390063 11160 layer_factory.cpp:58] Creating layer bn1
I1107 10:17:48.390063 11160 net.cpp:84] Creating Layer bn1
I1107 10:17:48.390063 11160 net.cpp:406] bn1 <- conv1
I1107 10:17:48.390063 11160 net.cpp:367] bn1 -> conv1 (in-place)
I1107 10:17:48.390063 11160 net.cpp:122] Setting up bn1
I1107 10:17:48.390063 11160 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 10:17:48.390063 11160 net.cpp:137] Memory required for data: 47310000
I1107 10:17:48.390063 11160 layer_factory.cpp:58] Creating layer scale1
I1107 10:17:48.390063 11160 net.cpp:84] Creating Layer scale1
I1107 10:17:48.390063 11160 net.cpp:406] scale1 <- conv1
I1107 10:17:48.390063 11160 net.cpp:367] scale1 -> conv1 (in-place)
I1107 10:17:48.390063 11160 layer_factory.cpp:58] Creating layer scale1
I1107 10:17:48.390063 11160 net.cpp:122] Setting up scale1
I1107 10:17:48.390063 11160 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 10:17:48.390063 11160 net.cpp:137] Memory required for data: 70350000
I1107 10:17:48.390063 11160 layer_factory.cpp:58] Creating layer relu_conv1
I1107 10:17:48.390063 11160 net.cpp:84] Creating Layer relu_conv1
I1107 10:17:48.390063 11160 net.cpp:406] relu_conv1 <- conv1
I1107 10:17:48.390063 11160 net.cpp:367] relu_conv1 -> conv1 (in-place)
I1107 10:17:48.390063 11160 net.cpp:122] Setting up relu_conv1
I1107 10:17:48.390063 11160 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 10:17:48.390063 11160 net.cpp:137] Memory required for data: 93390000
I1107 10:17:48.390063 11160 layer_factory.cpp:58] Creating layer pool1
I1107 10:17:48.390063 11160 net.cpp:84] Creating Layer pool1
I1107 10:17:48.390063 11160 net.cpp:406] pool1 <- conv1
I1107 10:17:48.390063 11160 net.cpp:380] pool1 -> pool1
I1107 10:17:48.390063 11160 net.cpp:122] Setting up pool1
I1107 10:17:48.390063 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.390063 11160 net.cpp:137] Memory required for data: 113460400
I1107 10:17:48.390063 11160 layer_factory.cpp:58] Creating layer fire2/squeeze1x1
I1107 10:17:48.390063 11160 net.cpp:84] Creating Layer fire2/squeeze1x1
I1107 10:17:48.390063 11160 net.cpp:406] fire2/squeeze1x1 <- pool1
I1107 10:17:48.390063 11160 net.cpp:380] fire2/squeeze1x1 -> fire2/squeeze1x1
I1107 10:17:48.391109 11160 net.cpp:122] Setting up fire2/squeeze1x1
I1107 10:17:48.391109 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.391109 11160 net.cpp:137] Memory required for data: 118478000
I1107 10:17:48.391109 11160 layer_factory.cpp:58] Creating layer fire2/relu_squeeze1x1
I1107 10:17:48.391109 11160 net.cpp:84] Creating Layer fire2/relu_squeeze1x1
I1107 10:17:48.391109 11160 net.cpp:406] fire2/relu_squeeze1x1 <- fire2/squeeze1x1
I1107 10:17:48.391109 11160 net.cpp:367] fire2/relu_squeeze1x1 -> fire2/squeeze1x1 (in-place)
I1107 10:17:48.392109 11160 net.cpp:122] Setting up fire2/relu_squeeze1x1
I1107 10:17:48.392109 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.392109 11160 net.cpp:137] Memory required for data: 123495600
I1107 10:17:48.392109 11160 layer_factory.cpp:58] Creating layer fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 10:17:48.392109 11160 net.cpp:84] Creating Layer fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 10:17:48.392109 11160 net.cpp:406] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split <- fire2/squeeze1x1
I1107 10:17:48.392109 11160 net.cpp:380] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split -> fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_0
I1107 10:17:48.392109 11160 net.cpp:380] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split -> fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_1
I1107 10:17:48.392109 11160 net.cpp:122] Setting up fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 10:17:48.392109 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.392109 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.392109 11160 net.cpp:137] Memory required for data: 133530800
I1107 10:17:48.392109 11160 layer_factory.cpp:58] Creating layer fire2/expand1x1
I1107 10:17:48.392109 11160 net.cpp:84] Creating Layer fire2/expand1x1
I1107 10:17:48.392109 11160 net.cpp:406] fire2/expand1x1 <- fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_0
I1107 10:17:48.392109 11160 net.cpp:380] fire2/expand1x1 -> conv2
I1107 10:17:48.393110 11160 net.cpp:122] Setting up fire2/expand1x1
I1107 10:17:48.393110 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.393110 11160 net.cpp:137] Memory required for data: 153601200
I1107 10:17:48.393110 11160 layer_factory.cpp:58] Creating layer bn2
I1107 10:17:48.393110 11160 net.cpp:84] Creating Layer bn2
I1107 10:17:48.393110 11160 net.cpp:406] bn2 <- conv2
I1107 10:17:48.393110 11160 net.cpp:367] bn2 -> conv2 (in-place)
I1107 10:17:48.393110 11160 net.cpp:122] Setting up bn2
I1107 10:17:48.393110 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.393110 11160 net.cpp:137] Memory required for data: 173671600
I1107 10:17:48.393110 11160 layer_factory.cpp:58] Creating layer scale2
I1107 10:17:48.393110 11160 net.cpp:84] Creating Layer scale2
I1107 10:17:48.393110 11160 net.cpp:406] scale2 <- conv2
I1107 10:17:48.393110 11160 net.cpp:380] scale2 -> fire2/expand1x1
I1107 10:17:48.393110 11160 layer_factory.cpp:58] Creating layer scale2
I1107 10:17:48.393110 11160 net.cpp:122] Setting up scale2
I1107 10:17:48.393110 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.393110 11160 net.cpp:137] Memory required for data: 193742000
I1107 10:17:48.393110 11160 layer_factory.cpp:58] Creating layer fire2/relu_expand1x1
I1107 10:17:48.393110 11160 net.cpp:84] Creating Layer fire2/relu_expand1x1
I1107 10:17:48.393110 11160 net.cpp:406] fire2/relu_expand1x1 <- fire2/expand1x1
I1107 10:17:48.393110 11160 net.cpp:367] fire2/relu_expand1x1 -> fire2/expand1x1 (in-place)
I1107 10:17:48.394127 11160 net.cpp:122] Setting up fire2/relu_expand1x1
I1107 10:17:48.394127 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.394127 11160 net.cpp:137] Memory required for data: 213812400
I1107 10:17:48.394127 11160 layer_factory.cpp:58] Creating layer fire2/expand3x3
I1107 10:17:48.394127 11160 net.cpp:84] Creating Layer fire2/expand3x3
I1107 10:17:48.394127 11160 net.cpp:406] fire2/expand3x3 <- fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_1
I1107 10:17:48.394127 11160 net.cpp:380] fire2/expand3x3 -> conv_3
I1107 10:17:48.395110 11160 net.cpp:122] Setting up fire2/expand3x3
I1107 10:17:48.395110 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.395110 11160 net.cpp:137] Memory required for data: 233882800
I1107 10:17:48.395110 11160 layer_factory.cpp:58] Creating layer bn_3
I1107 10:17:48.395110 11160 net.cpp:84] Creating Layer bn_3
I1107 10:17:48.395110 11160 net.cpp:406] bn_3 <- conv_3
I1107 10:17:48.395110 11160 net.cpp:367] bn_3 -> conv_3 (in-place)
I1107 10:17:48.396127 11160 net.cpp:122] Setting up bn_3
I1107 10:17:48.396127 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.396127 11160 net.cpp:137] Memory required for data: 253953200
I1107 10:17:48.396127 11160 layer_factory.cpp:58] Creating layer scale_3
I1107 10:17:48.396127 11160 net.cpp:84] Creating Layer scale_3
I1107 10:17:48.396127 11160 net.cpp:406] scale_3 <- conv_3
I1107 10:17:48.396127 11160 net.cpp:380] scale_3 -> fire2/expand3x3
I1107 10:17:48.396127 11160 layer_factory.cpp:58] Creating layer scale_3
I1107 10:17:48.396127 11160 net.cpp:122] Setting up scale_3
I1107 10:17:48.396127 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.396127 11160 net.cpp:137] Memory required for data: 274023600
I1107 10:17:48.396127 11160 layer_factory.cpp:58] Creating layer fire2/relu_expand3x3
I1107 10:17:48.396127 11160 net.cpp:84] Creating Layer fire2/relu_expand3x3
I1107 10:17:48.396127 11160 net.cpp:406] fire2/relu_expand3x3 <- fire2/expand3x3
I1107 10:17:48.396127 11160 net.cpp:367] fire2/relu_expand3x3 -> fire2/expand3x3 (in-place)
I1107 10:17:48.396127 11160 net.cpp:122] Setting up fire2/relu_expand3x3
I1107 10:17:48.396127 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.396127 11160 net.cpp:137] Memory required for data: 294094000
I1107 10:17:48.396127 11160 layer_factory.cpp:58] Creating layer fire2/concat
I1107 10:17:48.396127 11160 net.cpp:84] Creating Layer fire2/concat
I1107 10:17:48.396127 11160 net.cpp:406] fire2/concat <- fire2/expand1x1
I1107 10:17:48.396127 11160 net.cpp:406] fire2/concat <- fire2/expand3x3
I1107 10:17:48.396127 11160 net.cpp:380] fire2/concat -> fire2/concat
I1107 10:17:48.396127 11160 net.cpp:122] Setting up fire2/concat
I1107 10:17:48.396127 11160 net.cpp:129] Top shape: 100 128 28 28 (10035200)
I1107 10:17:48.396127 11160 net.cpp:137] Memory required for data: 334234800
I1107 10:17:48.396127 11160 layer_factory.cpp:58] Creating layer fire3/squeeze1x1
I1107 10:17:48.396127 11160 net.cpp:84] Creating Layer fire3/squeeze1x1
I1107 10:17:48.396127 11160 net.cpp:406] fire3/squeeze1x1 <- fire2/concat
I1107 10:17:48.396127 11160 net.cpp:380] fire3/squeeze1x1 -> conv_4
I1107 10:17:48.397130 11160 net.cpp:122] Setting up fire3/squeeze1x1
I1107 10:17:48.397130 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.397130 11160 net.cpp:137] Memory required for data: 339252400
I1107 10:17:48.397130 11160 layer_factory.cpp:58] Creating layer bn_4
I1107 10:17:48.397130 11160 net.cpp:84] Creating Layer bn_4
I1107 10:17:48.397130 11160 net.cpp:406] bn_4 <- conv_4
I1107 10:17:48.397130 11160 net.cpp:367] bn_4 -> conv_4 (in-place)
I1107 10:17:48.397130 11160 net.cpp:122] Setting up bn_4
I1107 10:17:48.397130 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.397130 11160 net.cpp:137] Memory required for data: 344270000
I1107 10:17:48.397130 11160 layer_factory.cpp:58] Creating layer scale_4
I1107 10:17:48.397130 11160 net.cpp:84] Creating Layer scale_4
I1107 10:17:48.397130 11160 net.cpp:406] scale_4 <- conv_4
I1107 10:17:48.397130 11160 net.cpp:380] scale_4 -> fire3/squeeze1x1
I1107 10:17:48.397130 11160 layer_factory.cpp:58] Creating layer scale_4
I1107 10:17:48.398128 11160 net.cpp:122] Setting up scale_4
I1107 10:17:48.398128 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.398128 11160 net.cpp:137] Memory required for data: 349287600
I1107 10:17:48.398128 11160 layer_factory.cpp:58] Creating layer fire3/relu_squeeze1x1
I1107 10:17:48.398128 11160 net.cpp:84] Creating Layer fire3/relu_squeeze1x1
I1107 10:17:48.398128 11160 net.cpp:406] fire3/relu_squeeze1x1 <- fire3/squeeze1x1
I1107 10:17:48.398128 11160 net.cpp:367] fire3/relu_squeeze1x1 -> fire3/squeeze1x1 (in-place)
I1107 10:17:48.398128 11160 net.cpp:122] Setting up fire3/relu_squeeze1x1
I1107 10:17:48.398128 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.398128 11160 net.cpp:137] Memory required for data: 354305200
I1107 10:17:48.398128 11160 layer_factory.cpp:58] Creating layer fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 10:17:48.398128 11160 net.cpp:84] Creating Layer fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 10:17:48.398128 11160 net.cpp:406] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split <- fire3/squeeze1x1
I1107 10:17:48.398128 11160 net.cpp:380] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split -> fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_0
I1107 10:17:48.398128 11160 net.cpp:380] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split -> fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_1
I1107 10:17:48.398128 11160 net.cpp:122] Setting up fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 10:17:48.398128 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.398128 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.398128 11160 net.cpp:137] Memory required for data: 364340400
I1107 10:17:48.398128 11160 layer_factory.cpp:58] Creating layer fire3/expand1x1
I1107 10:17:48.398128 11160 net.cpp:84] Creating Layer fire3/expand1x1
I1107 10:17:48.398128 11160 net.cpp:406] fire3/expand1x1 <- fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_0
I1107 10:17:48.398128 11160 net.cpp:380] fire3/expand1x1 -> conv_5
I1107 10:17:48.399128 11160 net.cpp:122] Setting up fire3/expand1x1
I1107 10:17:48.399128 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.399128 11160 net.cpp:137] Memory required for data: 384410800
I1107 10:17:48.399128 11160 layer_factory.cpp:58] Creating layer bn_5
I1107 10:17:48.399128 11160 net.cpp:84] Creating Layer bn_5
I1107 10:17:48.399128 11160 net.cpp:406] bn_5 <- conv_5
I1107 10:17:48.399128 11160 net.cpp:367] bn_5 -> conv_5 (in-place)
I1107 10:17:48.399128 11160 net.cpp:122] Setting up bn_5
I1107 10:17:48.399128 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.399128 11160 net.cpp:137] Memory required for data: 404481200
I1107 10:17:48.399128 11160 layer_factory.cpp:58] Creating layer scale_5
I1107 10:17:48.400108 11160 net.cpp:84] Creating Layer scale_5
I1107 10:17:48.400108 11160 net.cpp:406] scale_5 <- conv_5
I1107 10:17:48.400108 11160 net.cpp:380] scale_5 -> fire3/expand1x1
I1107 10:17:48.400108 11160 layer_factory.cpp:58] Creating layer scale_5
I1107 10:17:48.400108 11160 net.cpp:122] Setting up scale_5
I1107 10:17:48.400108 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.400108 11160 net.cpp:137] Memory required for data: 424551600
I1107 10:17:48.400108 11160 layer_factory.cpp:58] Creating layer fire3/relu_expand1x1
I1107 10:17:48.400108 11160 net.cpp:84] Creating Layer fire3/relu_expand1x1
I1107 10:17:48.400108 11160 net.cpp:406] fire3/relu_expand1x1 <- fire3/expand1x1
I1107 10:17:48.400108 11160 net.cpp:367] fire3/relu_expand1x1 -> fire3/expand1x1 (in-place)
I1107 10:17:48.400108 11160 net.cpp:122] Setting up fire3/relu_expand1x1
I1107 10:17:48.400108 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.400108 11160 net.cpp:137] Memory required for data: 444622000
I1107 10:17:48.400108 11160 layer_factory.cpp:58] Creating layer fire3/expand3x3
I1107 10:17:48.400108 11160 net.cpp:84] Creating Layer fire3/expand3x3
I1107 10:17:48.400108 11160 net.cpp:406] fire3/expand3x3 <- fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_1
I1107 10:17:48.400108 11160 net.cpp:380] fire3/expand3x3 -> conv_6
I1107 10:17:48.401129 11160 net.cpp:122] Setting up fire3/expand3x3
I1107 10:17:48.401129 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.401129 11160 net.cpp:137] Memory required for data: 464692400
I1107 10:17:48.401129 11160 layer_factory.cpp:58] Creating layer bn_6
I1107 10:17:48.401129 11160 net.cpp:84] Creating Layer bn_6
I1107 10:17:48.401129 11160 net.cpp:406] bn_6 <- conv_6
I1107 10:17:48.401129 11160 net.cpp:367] bn_6 -> conv_6 (in-place)
I1107 10:17:48.401129 11160 net.cpp:122] Setting up bn_6
I1107 10:17:48.401129 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.401129 11160 net.cpp:137] Memory required for data: 484762800
I1107 10:17:48.401129 11160 layer_factory.cpp:58] Creating layer scale_6
I1107 10:17:48.401129 11160 net.cpp:84] Creating Layer scale_6
I1107 10:17:48.401129 11160 net.cpp:406] scale_6 <- conv_6
I1107 10:17:48.401129 11160 net.cpp:380] scale_6 -> fire3/expand3x3
I1107 10:17:48.401129 11160 layer_factory.cpp:58] Creating layer scale_6
I1107 10:17:48.401129 11160 net.cpp:122] Setting up scale_6
I1107 10:17:48.401129 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.401129 11160 net.cpp:137] Memory required for data: 504833200
I1107 10:17:48.401129 11160 layer_factory.cpp:58] Creating layer fire3/relu_expand3x3
I1107 10:17:48.401129 11160 net.cpp:84] Creating Layer fire3/relu_expand3x3
I1107 10:17:48.401129 11160 net.cpp:406] fire3/relu_expand3x3 <- fire3/expand3x3
I1107 10:17:48.401129 11160 net.cpp:367] fire3/relu_expand3x3 -> fire3/expand3x3 (in-place)
I1107 10:17:48.402127 11160 net.cpp:122] Setting up fire3/relu_expand3x3
I1107 10:17:48.402127 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.402127 11160 net.cpp:137] Memory required for data: 524903600
I1107 10:17:48.402127 11160 layer_factory.cpp:58] Creating layer fire3/concat
I1107 10:17:48.402127 11160 net.cpp:84] Creating Layer fire3/concat
I1107 10:17:48.402127 11160 net.cpp:406] fire3/concat <- fire3/expand1x1
I1107 10:17:48.402127 11160 net.cpp:406] fire3/concat <- fire3/expand3x3
I1107 10:17:48.402127 11160 net.cpp:380] fire3/concat -> fire3/concat
I1107 10:17:48.402127 11160 net.cpp:122] Setting up fire3/concat
I1107 10:17:48.402127 11160 net.cpp:129] Top shape: 100 128 28 28 (10035200)
I1107 10:17:48.402127 11160 net.cpp:137] Memory required for data: 565044400
I1107 10:17:48.402127 11160 layer_factory.cpp:58] Creating layer pool3
I1107 10:17:48.402127 11160 net.cpp:84] Creating Layer pool3
I1107 10:17:48.402127 11160 net.cpp:406] pool3 <- fire3/concat
I1107 10:17:48.402127 11160 net.cpp:380] pool3 -> pool3
I1107 10:17:48.402127 11160 net.cpp:122] Setting up pool3
I1107 10:17:48.402127 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.402127 11160 net.cpp:137] Memory required for data: 575079600
I1107 10:17:48.402127 11160 layer_factory.cpp:58] Creating layer fire4/squeeze1x1
I1107 10:17:48.402127 11160 net.cpp:84] Creating Layer fire4/squeeze1x1
I1107 10:17:48.402127 11160 net.cpp:406] fire4/squeeze1x1 <- pool3
I1107 10:17:48.402127 11160 net.cpp:380] fire4/squeeze1x1 -> conv_7
I1107 10:17:48.403127 11160 net.cpp:122] Setting up fire4/squeeze1x1
I1107 10:17:48.403127 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.403127 11160 net.cpp:137] Memory required for data: 577588400
I1107 10:17:48.403127 11160 layer_factory.cpp:58] Creating layer bn_7
I1107 10:17:48.403127 11160 net.cpp:84] Creating Layer bn_7
I1107 10:17:48.403127 11160 net.cpp:406] bn_7 <- conv_7
I1107 10:17:48.403127 11160 net.cpp:367] bn_7 -> conv_7 (in-place)
I1107 10:17:48.403127 11160 net.cpp:122] Setting up bn_7
I1107 10:17:48.403127 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.403127 11160 net.cpp:137] Memory required for data: 580097200
I1107 10:17:48.404127 11160 layer_factory.cpp:58] Creating layer scale_7
I1107 10:17:48.404127 11160 net.cpp:84] Creating Layer scale_7
I1107 10:17:48.404127 11160 net.cpp:406] scale_7 <- conv_7
I1107 10:17:48.404127 11160 net.cpp:380] scale_7 -> fire4/squeeze1x1
I1107 10:17:48.404127 11160 layer_factory.cpp:58] Creating layer scale_7
I1107 10:17:48.404127 11160 net.cpp:122] Setting up scale_7
I1107 10:17:48.404127 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.404127 11160 net.cpp:137] Memory required for data: 582606000
I1107 10:17:48.404127 11160 layer_factory.cpp:58] Creating layer fire4/relu_squeeze1x1
I1107 10:17:48.404127 11160 net.cpp:84] Creating Layer fire4/relu_squeeze1x1
I1107 10:17:48.404127 11160 net.cpp:406] fire4/relu_squeeze1x1 <- fire4/squeeze1x1
I1107 10:17:48.404127 11160 net.cpp:367] fire4/relu_squeeze1x1 -> fire4/squeeze1x1 (in-place)
I1107 10:17:48.404127 11160 net.cpp:122] Setting up fire4/relu_squeeze1x1
I1107 10:17:48.404127 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.404127 11160 net.cpp:137] Memory required for data: 585114800
I1107 10:17:48.404127 11160 layer_factory.cpp:58] Creating layer fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 10:17:48.404127 11160 net.cpp:84] Creating Layer fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 10:17:48.404127 11160 net.cpp:406] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split <- fire4/squeeze1x1
I1107 10:17:48.404127 11160 net.cpp:380] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split -> fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_0
I1107 10:17:48.404127 11160 net.cpp:380] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split -> fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_1
I1107 10:17:48.404127 11160 net.cpp:122] Setting up fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 10:17:48.404127 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.404127 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.404127 11160 net.cpp:137] Memory required for data: 590132400
I1107 10:17:48.404127 11160 layer_factory.cpp:58] Creating layer fire4/expand1x1
I1107 10:17:48.404127 11160 net.cpp:84] Creating Layer fire4/expand1x1
I1107 10:17:48.404127 11160 net.cpp:406] fire4/expand1x1 <- fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_0
I1107 10:17:48.404127 11160 net.cpp:380] fire4/expand1x1 -> fire4/expand1x1
I1107 10:17:48.405128 11160 net.cpp:122] Setting up fire4/expand1x1
I1107 10:17:48.405128 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.405128 11160 net.cpp:137] Memory required for data: 600167600
I1107 10:17:48.405128 11160 layer_factory.cpp:58] Creating layer fire4/relu_expand1x1
I1107 10:17:48.405128 11160 net.cpp:84] Creating Layer fire4/relu_expand1x1
I1107 10:17:48.405128 11160 net.cpp:406] fire4/relu_expand1x1 <- fire4/expand1x1
I1107 10:17:48.405128 11160 net.cpp:367] fire4/relu_expand1x1 -> fire4/expand1x1 (in-place)
I1107 10:17:48.405128 11160 net.cpp:122] Setting up fire4/relu_expand1x1
I1107 10:17:48.405128 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.406127 11160 net.cpp:137] Memory required for data: 610202800
I1107 10:17:48.406127 11160 layer_factory.cpp:58] Creating layer fire4/expand3x3
I1107 10:17:48.406127 11160 net.cpp:84] Creating Layer fire4/expand3x3
I1107 10:17:48.406127 11160 net.cpp:406] fire4/expand3x3 <- fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_1
I1107 10:17:48.406127 11160 net.cpp:380] fire4/expand3x3 -> conv_8
I1107 10:17:48.407127 11160 net.cpp:122] Setting up fire4/expand3x3
I1107 10:17:48.407127 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.407127 11160 net.cpp:137] Memory required for data: 620238000
I1107 10:17:48.407127 11160 layer_factory.cpp:58] Creating layer bn_8
I1107 10:17:48.407127 11160 net.cpp:84] Creating Layer bn_8
I1107 10:17:48.407127 11160 net.cpp:406] bn_8 <- conv_8
I1107 10:17:48.407127 11160 net.cpp:367] bn_8 -> conv_8 (in-place)
I1107 10:17:48.408128 11160 net.cpp:122] Setting up bn_8
I1107 10:17:48.408128 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.408128 11160 net.cpp:137] Memory required for data: 630273200
I1107 10:17:48.408128 11160 layer_factory.cpp:58] Creating layer scale_8
I1107 10:17:48.408128 11160 net.cpp:84] Creating Layer scale_8
I1107 10:17:48.408128 11160 net.cpp:406] scale_8 <- conv_8
I1107 10:17:48.408128 11160 net.cpp:380] scale_8 -> fire4/expand3x3
I1107 10:17:48.408128 11160 layer_factory.cpp:58] Creating layer scale_8
I1107 10:17:48.408128 11160 net.cpp:122] Setting up scale_8
I1107 10:17:48.408128 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.408128 11160 net.cpp:137] Memory required for data: 640308400
I1107 10:17:48.408128 11160 layer_factory.cpp:58] Creating layer fire4/relu_expand3x3
I1107 10:17:48.408128 11160 net.cpp:84] Creating Layer fire4/relu_expand3x3
I1107 10:17:48.408128 11160 net.cpp:406] fire4/relu_expand3x3 <- fire4/expand3x3
I1107 10:17:48.408128 11160 net.cpp:367] fire4/relu_expand3x3 -> fire4/expand3x3 (in-place)
I1107 10:17:48.408128 11160 net.cpp:122] Setting up fire4/relu_expand3x3
I1107 10:17:48.408128 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.408128 11160 net.cpp:137] Memory required for data: 650343600
I1107 10:17:48.408128 11160 layer_factory.cpp:58] Creating layer fire4/concat
I1107 10:17:48.408128 11160 net.cpp:84] Creating Layer fire4/concat
I1107 10:17:48.408128 11160 net.cpp:406] fire4/concat <- fire4/expand1x1
I1107 10:17:48.408128 11160 net.cpp:406] fire4/concat <- fire4/expand3x3
I1107 10:17:48.408128 11160 net.cpp:380] fire4/concat -> fire4/concat
I1107 10:17:48.408128 11160 net.cpp:122] Setting up fire4/concat
I1107 10:17:48.408128 11160 net.cpp:129] Top shape: 100 256 14 14 (5017600)
I1107 10:17:48.408128 11160 net.cpp:137] Memory required for data: 670414000
I1107 10:17:48.408128 11160 layer_factory.cpp:58] Creating layer fire5/squeeze1x1
I1107 10:17:48.408128 11160 net.cpp:84] Creating Layer fire5/squeeze1x1
I1107 10:17:48.408128 11160 net.cpp:406] fire5/squeeze1x1 <- fire4/concat
I1107 10:17:48.408128 11160 net.cpp:380] fire5/squeeze1x1 -> conv_9
I1107 10:17:48.409127 11160 net.cpp:122] Setting up fire5/squeeze1x1
I1107 10:17:48.409127 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.409127 11160 net.cpp:137] Memory required for data: 672922800
I1107 10:17:48.409127 11160 layer_factory.cpp:58] Creating layer bn_9
I1107 10:17:48.409127 11160 net.cpp:84] Creating Layer bn_9
I1107 10:17:48.409127 11160 net.cpp:406] bn_9 <- conv_9
I1107 10:17:48.409127 11160 net.cpp:367] bn_9 -> conv_9 (in-place)
I1107 10:17:48.409127 11160 net.cpp:122] Setting up bn_9
I1107 10:17:48.409127 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.409127 11160 net.cpp:137] Memory required for data: 675431600
I1107 10:17:48.409127 11160 layer_factory.cpp:58] Creating layer scale_9
I1107 10:17:48.409127 11160 net.cpp:84] Creating Layer scale_9
I1107 10:17:48.409127 11160 net.cpp:406] scale_9 <- conv_9
I1107 10:17:48.409127 11160 net.cpp:380] scale_9 -> fire5/squeeze1x1
I1107 10:17:48.409127 11160 layer_factory.cpp:58] Creating layer scale_9
I1107 10:17:48.410127 11160 net.cpp:122] Setting up scale_9
I1107 10:17:48.410127 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.410127 11160 net.cpp:137] Memory required for data: 677940400
I1107 10:17:48.410127 11160 layer_factory.cpp:58] Creating layer fire5/relu_squeeze1x1
I1107 10:17:48.410127 11160 net.cpp:84] Creating Layer fire5/relu_squeeze1x1
I1107 10:17:48.410127 11160 net.cpp:406] fire5/relu_squeeze1x1 <- fire5/squeeze1x1
I1107 10:17:48.410127 11160 net.cpp:367] fire5/relu_squeeze1x1 -> fire5/squeeze1x1 (in-place)
I1107 10:17:48.410127 11160 net.cpp:122] Setting up fire5/relu_squeeze1x1
I1107 10:17:48.410127 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.410127 11160 net.cpp:137] Memory required for data: 680449200
I1107 10:17:48.410127 11160 layer_factory.cpp:58] Creating layer fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 10:17:48.410127 11160 net.cpp:84] Creating Layer fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 10:17:48.410127 11160 net.cpp:406] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split <- fire5/squeeze1x1
I1107 10:17:48.410127 11160 net.cpp:380] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split -> fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_0
I1107 10:17:48.410127 11160 net.cpp:380] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split -> fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_1
I1107 10:17:48.410127 11160 net.cpp:122] Setting up fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 10:17:48.410127 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.410127 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.410127 11160 net.cpp:137] Memory required for data: 685466800
I1107 10:17:48.410127 11160 layer_factory.cpp:58] Creating layer fire5/expand1x1
I1107 10:17:48.410127 11160 net.cpp:84] Creating Layer fire5/expand1x1
I1107 10:17:48.410127 11160 net.cpp:406] fire5/expand1x1 <- fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_0
I1107 10:17:48.410127 11160 net.cpp:380] fire5/expand1x1 -> conv_10
I1107 10:17:48.411129 11160 net.cpp:122] Setting up fire5/expand1x1
I1107 10:17:48.411129 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.411129 11160 net.cpp:137] Memory required for data: 695502000
I1107 10:17:48.411129 11160 layer_factory.cpp:58] Creating layer bn_10
I1107 10:17:48.411129 11160 net.cpp:84] Creating Layer bn_10
I1107 10:17:48.411129 11160 net.cpp:406] bn_10 <- conv_10
I1107 10:17:48.411129 11160 net.cpp:367] bn_10 -> conv_10 (in-place)
I1107 10:17:48.411129 11160 net.cpp:122] Setting up bn_10
I1107 10:17:48.411129 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.411129 11160 net.cpp:137] Memory required for data: 705537200
I1107 10:17:48.411129 11160 layer_factory.cpp:58] Creating layer scale_10
I1107 10:17:48.411129 11160 net.cpp:84] Creating Layer scale_10
I1107 10:17:48.411129 11160 net.cpp:406] scale_10 <- conv_10
I1107 10:17:48.411129 11160 net.cpp:380] scale_10 -> fire5/expand1x1
I1107 10:17:48.411129 11160 layer_factory.cpp:58] Creating layer scale_10
I1107 10:17:48.411129 11160 net.cpp:122] Setting up scale_10
I1107 10:17:48.411129 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.411129 11160 net.cpp:137] Memory required for data: 715572400
I1107 10:17:48.411129 11160 layer_factory.cpp:58] Creating layer fire5/relu_expand1x1
I1107 10:17:48.411129 11160 net.cpp:84] Creating Layer fire5/relu_expand1x1
I1107 10:17:48.411129 11160 net.cpp:406] fire5/relu_expand1x1 <- fire5/expand1x1
I1107 10:17:48.411129 11160 net.cpp:367] fire5/relu_expand1x1 -> fire5/expand1x1 (in-place)
I1107 10:17:48.412127 11160 net.cpp:122] Setting up fire5/relu_expand1x1
I1107 10:17:48.412127 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.412127 11160 net.cpp:137] Memory required for data: 725607600
I1107 10:17:48.412127 11160 layer_factory.cpp:58] Creating layer fire5/expand3x3
I1107 10:17:48.412127 11160 net.cpp:84] Creating Layer fire5/expand3x3
I1107 10:17:48.412127 11160 net.cpp:406] fire5/expand3x3 <- fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_1
I1107 10:17:48.412127 11160 net.cpp:380] fire5/expand3x3 -> conv_11
I1107 10:17:48.413128 11160 net.cpp:122] Setting up fire5/expand3x3
I1107 10:17:48.413128 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.413128 11160 net.cpp:137] Memory required for data: 735642800
I1107 10:17:48.413128 11160 layer_factory.cpp:58] Creating layer bn_11
I1107 10:17:48.413128 11160 net.cpp:84] Creating Layer bn_11
I1107 10:17:48.413128 11160 net.cpp:406] bn_11 <- conv_11
I1107 10:17:48.413128 11160 net.cpp:367] bn_11 -> conv_11 (in-place)
I1107 10:17:48.413128 11160 net.cpp:122] Setting up bn_11
I1107 10:17:48.413128 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.413128 11160 net.cpp:137] Memory required for data: 745678000
I1107 10:17:48.413128 11160 layer_factory.cpp:58] Creating layer scale_11
I1107 10:17:48.413128 11160 net.cpp:84] Creating Layer scale_11
I1107 10:17:48.413128 11160 net.cpp:406] scale_11 <- conv_11
I1107 10:17:48.413128 11160 net.cpp:380] scale_11 -> fire5/expand3x3
I1107 10:17:48.413128 11160 layer_factory.cpp:58] Creating layer scale_11
I1107 10:17:48.413128 11160 net.cpp:122] Setting up scale_11
I1107 10:17:48.413128 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.413128 11160 net.cpp:137] Memory required for data: 755713200
I1107 10:17:48.413128 11160 layer_factory.cpp:58] Creating layer fire5/relu_expand3x3
I1107 10:17:48.413128 11160 net.cpp:84] Creating Layer fire5/relu_expand3x3
I1107 10:17:48.413128 11160 net.cpp:406] fire5/relu_expand3x3 <- fire5/expand3x3
I1107 10:17:48.413128 11160 net.cpp:367] fire5/relu_expand3x3 -> fire5/expand3x3 (in-place)
I1107 10:17:48.413128 11160 net.cpp:122] Setting up fire5/relu_expand3x3
I1107 10:17:48.413128 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.413128 11160 net.cpp:137] Memory required for data: 765748400
I1107 10:17:48.413128 11160 layer_factory.cpp:58] Creating layer fire5/concat
I1107 10:17:48.413128 11160 net.cpp:84] Creating Layer fire5/concat
I1107 10:17:48.413128 11160 net.cpp:406] fire5/concat <- fire5/expand1x1
I1107 10:17:48.413128 11160 net.cpp:406] fire5/concat <- fire5/expand3x3
I1107 10:17:48.413128 11160 net.cpp:380] fire5/concat -> fire5/concat
I1107 10:17:48.413128 11160 net.cpp:122] Setting up fire5/concat
I1107 10:17:48.413128 11160 net.cpp:129] Top shape: 100 256 14 14 (5017600)
I1107 10:17:48.413128 11160 net.cpp:137] Memory required for data: 785818800
I1107 10:17:48.413128 11160 layer_factory.cpp:58] Creating layer pool5
I1107 10:17:48.413128 11160 net.cpp:84] Creating Layer pool5
I1107 10:17:48.413128 11160 net.cpp:406] pool5 <- fire5/concat
I1107 10:17:48.414129 11160 net.cpp:380] pool5 -> pool5
I1107 10:17:48.414129 11160 net.cpp:122] Setting up pool5
I1107 10:17:48.414129 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.414129 11160 net.cpp:137] Memory required for data: 790836400
I1107 10:17:48.414129 11160 layer_factory.cpp:58] Creating layer fire6/squeeze1x1
I1107 10:17:48.414129 11160 net.cpp:84] Creating Layer fire6/squeeze1x1
I1107 10:17:48.414129 11160 net.cpp:406] fire6/squeeze1x1 <- pool5
I1107 10:17:48.414129 11160 net.cpp:380] fire6/squeeze1x1 -> conv_12
I1107 10:17:48.415128 11160 net.cpp:122] Setting up fire6/squeeze1x1
I1107 10:17:48.415128 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.415128 11160 net.cpp:137] Memory required for data: 791777200
I1107 10:17:48.415128 11160 layer_factory.cpp:58] Creating layer bn_12
I1107 10:17:48.415128 11160 net.cpp:84] Creating Layer bn_12
I1107 10:17:48.415128 11160 net.cpp:406] bn_12 <- conv_12
I1107 10:17:48.415128 11160 net.cpp:367] bn_12 -> conv_12 (in-place)
I1107 10:17:48.415128 11160 net.cpp:122] Setting up bn_12
I1107 10:17:48.415128 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.415128 11160 net.cpp:137] Memory required for data: 792718000
I1107 10:17:48.415128 11160 layer_factory.cpp:58] Creating layer scale_12
I1107 10:17:48.415128 11160 net.cpp:84] Creating Layer scale_12
I1107 10:17:48.415128 11160 net.cpp:406] scale_12 <- conv_12
I1107 10:17:48.415128 11160 net.cpp:380] scale_12 -> fire6/squeeze1x1
I1107 10:17:48.415128 11160 layer_factory.cpp:58] Creating layer scale_12
I1107 10:17:48.415128 11160 net.cpp:122] Setting up scale_12
I1107 10:17:48.415128 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.415128 11160 net.cpp:137] Memory required for data: 793658800
I1107 10:17:48.415128 11160 layer_factory.cpp:58] Creating layer fire6/relu_squeeze1x1
I1107 10:17:48.415128 11160 net.cpp:84] Creating Layer fire6/relu_squeeze1x1
I1107 10:17:48.415128 11160 net.cpp:406] fire6/relu_squeeze1x1 <- fire6/squeeze1x1
I1107 10:17:48.415128 11160 net.cpp:367] fire6/relu_squeeze1x1 -> fire6/squeeze1x1 (in-place)
I1107 10:17:48.416127 11160 net.cpp:122] Setting up fire6/relu_squeeze1x1
I1107 10:17:48.416127 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.416127 11160 net.cpp:137] Memory required for data: 794599600
I1107 10:17:48.416127 11160 layer_factory.cpp:58] Creating layer fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 10:17:48.416127 11160 net.cpp:84] Creating Layer fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 10:17:48.416127 11160 net.cpp:406] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split <- fire6/squeeze1x1
I1107 10:17:48.416127 11160 net.cpp:380] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split -> fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_0
I1107 10:17:48.416127 11160 net.cpp:380] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split -> fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_1
I1107 10:17:48.416127 11160 net.cpp:122] Setting up fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 10:17:48.416127 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.416127 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.416127 11160 net.cpp:137] Memory required for data: 796481200
I1107 10:17:48.416127 11160 layer_factory.cpp:58] Creating layer fire6/expand1x1
I1107 10:17:48.416127 11160 net.cpp:84] Creating Layer fire6/expand1x1
I1107 10:17:48.416127 11160 net.cpp:406] fire6/expand1x1 <- fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_0
I1107 10:17:48.416127 11160 net.cpp:380] fire6/expand1x1 -> conv_13
I1107 10:17:48.417127 11160 net.cpp:122] Setting up fire6/expand1x1
I1107 10:17:48.417127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.417127 11160 net.cpp:137] Memory required for data: 800244400
I1107 10:17:48.417127 11160 layer_factory.cpp:58] Creating layer bn_13
I1107 10:17:48.417127 11160 net.cpp:84] Creating Layer bn_13
I1107 10:17:48.417127 11160 net.cpp:406] bn_13 <- conv_13
I1107 10:17:48.417127 11160 net.cpp:367] bn_13 -> conv_13 (in-place)
I1107 10:17:48.417127 11160 net.cpp:122] Setting up bn_13
I1107 10:17:48.417127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.417127 11160 net.cpp:137] Memory required for data: 804007600
I1107 10:17:48.417127 11160 layer_factory.cpp:58] Creating layer scale_13
I1107 10:17:48.417127 11160 net.cpp:84] Creating Layer scale_13
I1107 10:17:48.417127 11160 net.cpp:406] scale_13 <- conv_13
I1107 10:17:48.417127 11160 net.cpp:380] scale_13 -> fire6/expand1x1
I1107 10:17:48.417127 11160 layer_factory.cpp:58] Creating layer scale_13
I1107 10:17:48.418128 11160 net.cpp:122] Setting up scale_13
I1107 10:17:48.418128 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.418128 11160 net.cpp:137] Memory required for data: 807770800
I1107 10:17:48.418128 11160 layer_factory.cpp:58] Creating layer fire6/relu_expand1x1
I1107 10:17:48.418128 11160 net.cpp:84] Creating Layer fire6/relu_expand1x1
I1107 10:17:48.418128 11160 net.cpp:406] fire6/relu_expand1x1 <- fire6/expand1x1
I1107 10:17:48.418128 11160 net.cpp:367] fire6/relu_expand1x1 -> fire6/expand1x1 (in-place)
I1107 10:17:48.418128 11160 net.cpp:122] Setting up fire6/relu_expand1x1
I1107 10:17:48.418128 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.418128 11160 net.cpp:137] Memory required for data: 811534000
I1107 10:17:48.418128 11160 layer_factory.cpp:58] Creating layer fire6/expand3x3
I1107 10:17:48.418128 11160 net.cpp:84] Creating Layer fire6/expand3x3
I1107 10:17:48.418128 11160 net.cpp:406] fire6/expand3x3 <- fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_1
I1107 10:17:48.418128 11160 net.cpp:380] fire6/expand3x3 -> conv_14
I1107 10:17:48.420127 11160 net.cpp:122] Setting up fire6/expand3x3
I1107 10:17:48.420127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.420127 11160 net.cpp:137] Memory required for data: 815297200
I1107 10:17:48.420127 11160 layer_factory.cpp:58] Creating layer bn_14
I1107 10:17:48.420127 11160 net.cpp:84] Creating Layer bn_14
I1107 10:17:48.420127 11160 net.cpp:406] bn_14 <- conv_14
I1107 10:17:48.420127 11160 net.cpp:367] bn_14 -> conv_14 (in-place)
I1107 10:17:48.420127 11160 net.cpp:122] Setting up bn_14
I1107 10:17:48.420127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.420127 11160 net.cpp:137] Memory required for data: 819060400
I1107 10:17:48.420127 11160 layer_factory.cpp:58] Creating layer scale_14
I1107 10:17:48.420127 11160 net.cpp:84] Creating Layer scale_14
I1107 10:17:48.420127 11160 net.cpp:406] scale_14 <- conv_14
I1107 10:17:48.420127 11160 net.cpp:380] scale_14 -> fire6/expand3x3
I1107 10:17:48.420127 11160 layer_factory.cpp:58] Creating layer scale_14
I1107 10:17:48.420127 11160 net.cpp:122] Setting up scale_14
I1107 10:17:48.420127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.420127 11160 net.cpp:137] Memory required for data: 822823600
I1107 10:17:48.420127 11160 layer_factory.cpp:58] Creating layer fire6/relu_expand3x3
I1107 10:17:48.420127 11160 net.cpp:84] Creating Layer fire6/relu_expand3x3
I1107 10:17:48.420127 11160 net.cpp:406] fire6/relu_expand3x3 <- fire6/expand3x3
I1107 10:17:48.420127 11160 net.cpp:367] fire6/relu_expand3x3 -> fire6/expand3x3 (in-place)
I1107 10:17:48.420127 11160 net.cpp:122] Setting up fire6/relu_expand3x3
I1107 10:17:48.420127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.420127 11160 net.cpp:137] Memory required for data: 826586800
I1107 10:17:48.420127 11160 layer_factory.cpp:58] Creating layer fire6/concat
I1107 10:17:48.420127 11160 net.cpp:84] Creating Layer fire6/concat
I1107 10:17:48.420127 11160 net.cpp:406] fire6/concat <- fire6/expand1x1
I1107 10:17:48.420127 11160 net.cpp:406] fire6/concat <- fire6/expand3x3
I1107 10:17:48.421128 11160 net.cpp:380] fire6/concat -> fire6/concat
I1107 10:17:48.421128 11160 net.cpp:122] Setting up fire6/concat
I1107 10:17:48.421128 11160 net.cpp:129] Top shape: 100 384 7 7 (1881600)
I1107 10:17:48.421128 11160 net.cpp:137] Memory required for data: 834113200
I1107 10:17:48.421128 11160 layer_factory.cpp:58] Creating layer fire7/squeeze1x1
I1107 10:17:48.421128 11160 net.cpp:84] Creating Layer fire7/squeeze1x1
I1107 10:17:48.421128 11160 net.cpp:406] fire7/squeeze1x1 <- fire6/concat
I1107 10:17:48.421128 11160 net.cpp:380] fire7/squeeze1x1 -> conv_15
I1107 10:17:48.422127 11160 net.cpp:122] Setting up fire7/squeeze1x1
I1107 10:17:48.422127 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.422127 11160 net.cpp:137] Memory required for data: 835054000
I1107 10:17:48.422127 11160 layer_factory.cpp:58] Creating layer bn_15
I1107 10:17:48.422127 11160 net.cpp:84] Creating Layer bn_15
I1107 10:17:48.422127 11160 net.cpp:406] bn_15 <- conv_15
I1107 10:17:48.422127 11160 net.cpp:367] bn_15 -> conv_15 (in-place)
I1107 10:17:48.422127 11160 net.cpp:122] Setting up bn_15
I1107 10:17:48.422127 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.422127 11160 net.cpp:137] Memory required for data: 835994800
I1107 10:17:48.422127 11160 layer_factory.cpp:58] Creating layer scale_15
I1107 10:17:48.422127 11160 net.cpp:84] Creating Layer scale_15
I1107 10:17:48.422127 11160 net.cpp:406] scale_15 <- conv_15
I1107 10:17:48.422127 11160 net.cpp:380] scale_15 -> fire7/squeeze1x1
I1107 10:17:48.422127 11160 layer_factory.cpp:58] Creating layer scale_15
I1107 10:17:48.422127 11160 net.cpp:122] Setting up scale_15
I1107 10:17:48.422127 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.422127 11160 net.cpp:137] Memory required for data: 836935600
I1107 10:17:48.422127 11160 layer_factory.cpp:58] Creating layer fire7/relu_squeeze1x1
I1107 10:17:48.422127 11160 net.cpp:84] Creating Layer fire7/relu_squeeze1x1
I1107 10:17:48.422127 11160 net.cpp:406] fire7/relu_squeeze1x1 <- fire7/squeeze1x1
I1107 10:17:48.422127 11160 net.cpp:367] fire7/relu_squeeze1x1 -> fire7/squeeze1x1 (in-place)
I1107 10:17:48.422127 11160 net.cpp:122] Setting up fire7/relu_squeeze1x1
I1107 10:17:48.422127 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.422127 11160 net.cpp:137] Memory required for data: 837876400
I1107 10:17:48.422127 11160 layer_factory.cpp:58] Creating layer fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 10:17:48.422127 11160 net.cpp:84] Creating Layer fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 10:17:48.422127 11160 net.cpp:406] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split <- fire7/squeeze1x1
I1107 10:17:48.422127 11160 net.cpp:380] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split -> fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_0
I1107 10:17:48.422127 11160 net.cpp:380] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split -> fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_1
I1107 10:17:48.422127 11160 net.cpp:122] Setting up fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 10:17:48.423127 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.423127 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.423127 11160 net.cpp:137] Memory required for data: 839758000
I1107 10:17:48.423127 11160 layer_factory.cpp:58] Creating layer fire7/expand1x1
I1107 10:17:48.423127 11160 net.cpp:84] Creating Layer fire7/expand1x1
I1107 10:17:48.423127 11160 net.cpp:406] fire7/expand1x1 <- fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_0
I1107 10:17:48.423127 11160 net.cpp:380] fire7/expand1x1 -> conv_16
I1107 10:17:48.424127 11160 net.cpp:122] Setting up fire7/expand1x1
I1107 10:17:48.424127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.424127 11160 net.cpp:137] Memory required for data: 843521200
I1107 10:17:48.424127 11160 layer_factory.cpp:58] Creating layer bn_16
I1107 10:17:48.424127 11160 net.cpp:84] Creating Layer bn_16
I1107 10:17:48.424127 11160 net.cpp:406] bn_16 <- conv_16
I1107 10:17:48.424127 11160 net.cpp:367] bn_16 -> conv_16 (in-place)
I1107 10:17:48.424127 11160 net.cpp:122] Setting up bn_16
I1107 10:17:48.424127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.424127 11160 net.cpp:137] Memory required for data: 847284400
I1107 10:17:48.424127 11160 layer_factory.cpp:58] Creating layer scale_16
I1107 10:17:48.424127 11160 net.cpp:84] Creating Layer scale_16
I1107 10:17:48.424127 11160 net.cpp:406] scale_16 <- conv_16
I1107 10:17:48.424127 11160 net.cpp:380] scale_16 -> fire7/expand1x1
I1107 10:17:48.424127 11160 layer_factory.cpp:58] Creating layer scale_16
I1107 10:17:48.424127 11160 net.cpp:122] Setting up scale_16
I1107 10:17:48.424127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.424127 11160 net.cpp:137] Memory required for data: 851047600
I1107 10:17:48.424127 11160 layer_factory.cpp:58] Creating layer fire7/relu_expand1x1
I1107 10:17:48.424127 11160 net.cpp:84] Creating Layer fire7/relu_expand1x1
I1107 10:17:48.424127 11160 net.cpp:406] fire7/relu_expand1x1 <- fire7/expand1x1
I1107 10:17:48.424127 11160 net.cpp:367] fire7/relu_expand1x1 -> fire7/expand1x1 (in-place)
I1107 10:17:48.424127 11160 net.cpp:122] Setting up fire7/relu_expand1x1
I1107 10:17:48.424127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.424127 11160 net.cpp:137] Memory required for data: 854810800
I1107 10:17:48.424127 11160 layer_factory.cpp:58] Creating layer fire7/expand3x3
I1107 10:17:48.424127 11160 net.cpp:84] Creating Layer fire7/expand3x3
I1107 10:17:48.424127 11160 net.cpp:406] fire7/expand3x3 <- fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_1
I1107 10:17:48.424127 11160 net.cpp:380] fire7/expand3x3 -> conv_17
I1107 10:17:48.426127 11160 net.cpp:122] Setting up fire7/expand3x3
I1107 10:17:48.426127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.426127 11160 net.cpp:137] Memory required for data: 858574000
I1107 10:17:48.426127 11160 layer_factory.cpp:58] Creating layer bn_17
I1107 10:17:48.426127 11160 net.cpp:84] Creating Layer bn_17
I1107 10:17:48.426127 11160 net.cpp:406] bn_17 <- conv_17
I1107 10:17:48.426127 11160 net.cpp:367] bn_17 -> conv_17 (in-place)
I1107 10:17:48.427127 11160 net.cpp:122] Setting up bn_17
I1107 10:17:48.427127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.427127 11160 net.cpp:137] Memory required for data: 862337200
I1107 10:17:48.427127 11160 layer_factory.cpp:58] Creating layer scale_17
I1107 10:17:48.427127 11160 net.cpp:84] Creating Layer scale_17
I1107 10:17:48.427127 11160 net.cpp:406] scale_17 <- conv_17
I1107 10:17:48.427127 11160 net.cpp:380] scale_17 -> fire7/expand3x3
I1107 10:17:48.427127 11160 layer_factory.cpp:58] Creating layer scale_17
I1107 10:17:48.427127 11160 net.cpp:122] Setting up scale_17
I1107 10:17:48.427127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.427127 11160 net.cpp:137] Memory required for data: 866100400
I1107 10:17:48.427127 11160 layer_factory.cpp:58] Creating layer fire7/relu_expand3x3
I1107 10:17:48.427127 11160 net.cpp:84] Creating Layer fire7/relu_expand3x3
I1107 10:17:48.427127 11160 net.cpp:406] fire7/relu_expand3x3 <- fire7/expand3x3
I1107 10:17:48.427127 11160 net.cpp:367] fire7/relu_expand3x3 -> fire7/expand3x3 (in-place)
I1107 10:17:48.427127 11160 net.cpp:122] Setting up fire7/relu_expand3x3
I1107 10:17:48.427127 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.427127 11160 net.cpp:137] Memory required for data: 869863600
I1107 10:17:48.427127 11160 layer_factory.cpp:58] Creating layer fire7/concat
I1107 10:17:48.427127 11160 net.cpp:84] Creating Layer fire7/concat
I1107 10:17:48.427127 11160 net.cpp:406] fire7/concat <- fire7/expand1x1
I1107 10:17:48.427127 11160 net.cpp:406] fire7/concat <- fire7/expand3x3
I1107 10:17:48.427127 11160 net.cpp:380] fire7/concat -> fire7/concat
I1107 10:17:48.427127 11160 net.cpp:122] Setting up fire7/concat
I1107 10:17:48.427127 11160 net.cpp:129] Top shape: 100 384 7 7 (1881600)
I1107 10:17:48.427127 11160 net.cpp:137] Memory required for data: 877390000
I1107 10:17:48.427127 11160 layer_factory.cpp:58] Creating layer fire8/squeeze1x1
I1107 10:17:48.427127 11160 net.cpp:84] Creating Layer fire8/squeeze1x1
I1107 10:17:48.427127 11160 net.cpp:406] fire8/squeeze1x1 <- fire7/concat
I1107 10:17:48.427127 11160 net.cpp:380] fire8/squeeze1x1 -> conv_18
I1107 10:17:48.428128 11160 net.cpp:122] Setting up fire8/squeeze1x1
I1107 10:17:48.428128 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.428128 11160 net.cpp:137] Memory required for data: 878644400
I1107 10:17:48.428128 11160 layer_factory.cpp:58] Creating layer bn_18
I1107 10:17:48.428128 11160 net.cpp:84] Creating Layer bn_18
I1107 10:17:48.428128 11160 net.cpp:406] bn_18 <- conv_18
I1107 10:17:48.428128 11160 net.cpp:367] bn_18 -> conv_18 (in-place)
I1107 10:17:48.428128 11160 net.cpp:122] Setting up bn_18
I1107 10:17:48.428128 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.428128 11160 net.cpp:137] Memory required for data: 879898800
I1107 10:17:48.428128 11160 layer_factory.cpp:58] Creating layer scale_18
I1107 10:17:48.428128 11160 net.cpp:84] Creating Layer scale_18
I1107 10:17:48.428128 11160 net.cpp:406] scale_18 <- conv_18
I1107 10:17:48.428128 11160 net.cpp:380] scale_18 -> fire8/squeeze1x1
I1107 10:17:48.429127 11160 layer_factory.cpp:58] Creating layer scale_18
I1107 10:17:48.429127 11160 net.cpp:122] Setting up scale_18
I1107 10:17:48.429127 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.429127 11160 net.cpp:137] Memory required for data: 881153200
I1107 10:17:48.429127 11160 layer_factory.cpp:58] Creating layer fire8/relu_squeeze1x1
I1107 10:17:48.429127 11160 net.cpp:84] Creating Layer fire8/relu_squeeze1x1
I1107 10:17:48.429127 11160 net.cpp:406] fire8/relu_squeeze1x1 <- fire8/squeeze1x1
I1107 10:17:48.429127 11160 net.cpp:367] fire8/relu_squeeze1x1 -> fire8/squeeze1x1 (in-place)
I1107 10:17:48.429127 11160 net.cpp:122] Setting up fire8/relu_squeeze1x1
I1107 10:17:48.429127 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.429127 11160 net.cpp:137] Memory required for data: 882407600
I1107 10:17:48.429127 11160 layer_factory.cpp:58] Creating layer fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 10:17:48.429127 11160 net.cpp:84] Creating Layer fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 10:17:48.429127 11160 net.cpp:406] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split <- fire8/squeeze1x1
I1107 10:17:48.429127 11160 net.cpp:380] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split -> fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_0
I1107 10:17:48.429127 11160 net.cpp:380] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split -> fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_1
I1107 10:17:48.429127 11160 net.cpp:122] Setting up fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 10:17:48.429127 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.429127 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.429127 11160 net.cpp:137] Memory required for data: 884916400
I1107 10:17:48.429127 11160 layer_factory.cpp:58] Creating layer fire8/expand1x1
I1107 10:17:48.429127 11160 net.cpp:84] Creating Layer fire8/expand1x1
I1107 10:17:48.429127 11160 net.cpp:406] fire8/expand1x1 <- fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_0
I1107 10:17:48.429127 11160 net.cpp:380] fire8/expand1x1 -> conv_19
I1107 10:17:48.430128 11160 net.cpp:122] Setting up fire8/expand1x1
I1107 10:17:48.430128 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.430128 11160 net.cpp:137] Memory required for data: 889934000
I1107 10:17:48.430128 11160 layer_factory.cpp:58] Creating layer bn_19
I1107 10:17:48.430128 11160 net.cpp:84] Creating Layer bn_19
I1107 10:17:48.430128 11160 net.cpp:406] bn_19 <- conv_19
I1107 10:17:48.430128 11160 net.cpp:367] bn_19 -> conv_19 (in-place)
I1107 10:17:48.430128 11160 net.cpp:122] Setting up bn_19
I1107 10:17:48.430128 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.430128 11160 net.cpp:137] Memory required for data: 894951600
I1107 10:17:48.430128 11160 layer_factory.cpp:58] Creating layer scale_19
I1107 10:17:48.430128 11160 net.cpp:84] Creating Layer scale_19
I1107 10:17:48.430128 11160 net.cpp:406] scale_19 <- conv_19
I1107 10:17:48.430128 11160 net.cpp:380] scale_19 -> fire8/expand1x1
I1107 10:17:48.430128 11160 layer_factory.cpp:58] Creating layer scale_19
I1107 10:17:48.431128 11160 net.cpp:122] Setting up scale_19
I1107 10:17:48.431128 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.431128 11160 net.cpp:137] Memory required for data: 899969200
I1107 10:17:48.431128 11160 layer_factory.cpp:58] Creating layer fire8/relu_expand1x1
I1107 10:17:48.431128 11160 net.cpp:84] Creating Layer fire8/relu_expand1x1
I1107 10:17:48.431128 11160 net.cpp:406] fire8/relu_expand1x1 <- fire8/expand1x1
I1107 10:17:48.431128 11160 net.cpp:367] fire8/relu_expand1x1 -> fire8/expand1x1 (in-place)
I1107 10:17:48.431128 11160 net.cpp:122] Setting up fire8/relu_expand1x1
I1107 10:17:48.431128 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.431128 11160 net.cpp:137] Memory required for data: 904986800
I1107 10:17:48.431128 11160 layer_factory.cpp:58] Creating layer fire8/expand3x3
I1107 10:17:48.431128 11160 net.cpp:84] Creating Layer fire8/expand3x3
I1107 10:17:48.431128 11160 net.cpp:406] fire8/expand3x3 <- fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_1
I1107 10:17:48.431128 11160 net.cpp:380] fire8/expand3x3 -> conv_20
I1107 10:17:48.433127 11160 net.cpp:122] Setting up fire8/expand3x3
I1107 10:17:48.433127 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.433127 11160 net.cpp:137] Memory required for data: 910004400
I1107 10:17:48.433127 11160 layer_factory.cpp:58] Creating layer bn_20
I1107 10:17:48.433127 11160 net.cpp:84] Creating Layer bn_20
I1107 10:17:48.433127 11160 net.cpp:406] bn_20 <- conv_20
I1107 10:17:48.433127 11160 net.cpp:367] bn_20 -> conv_20 (in-place)
I1107 10:17:48.433127 11160 net.cpp:122] Setting up bn_20
I1107 10:17:48.434128 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.434128 11160 net.cpp:137] Memory required for data: 915022000
I1107 10:17:48.434128 11160 layer_factory.cpp:58] Creating layer scale_20
I1107 10:17:48.434128 11160 net.cpp:84] Creating Layer scale_20
I1107 10:17:48.434128 11160 net.cpp:406] scale_20 <- conv_20
I1107 10:17:48.434128 11160 net.cpp:380] scale_20 -> fire8/expand3x3
I1107 10:17:48.434128 11160 layer_factory.cpp:58] Creating layer scale_20
I1107 10:17:48.434128 11160 net.cpp:122] Setting up scale_20
I1107 10:17:48.434128 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.434128 11160 net.cpp:137] Memory required for data: 920039600
I1107 10:17:48.434128 11160 layer_factory.cpp:58] Creating layer fire8/relu_expand3x3
I1107 10:17:48.434128 11160 net.cpp:84] Creating Layer fire8/relu_expand3x3
I1107 10:17:48.434128 11160 net.cpp:406] fire8/relu_expand3x3 <- fire8/expand3x3
I1107 10:17:48.434128 11160 net.cpp:367] fire8/relu_expand3x3 -> fire8/expand3x3 (in-place)
I1107 10:17:48.434128 11160 net.cpp:122] Setting up fire8/relu_expand3x3
I1107 10:17:48.434128 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.434128 11160 net.cpp:137] Memory required for data: 925057200
I1107 10:17:48.434128 11160 layer_factory.cpp:58] Creating layer fire8/concat
I1107 10:17:48.434128 11160 net.cpp:84] Creating Layer fire8/concat
I1107 10:17:48.434128 11160 net.cpp:406] fire8/concat <- fire8/expand1x1
I1107 10:17:48.434128 11160 net.cpp:406] fire8/concat <- fire8/expand3x3
I1107 10:17:48.434128 11160 net.cpp:380] fire8/concat -> fire8/concat
I1107 10:17:48.434128 11160 net.cpp:122] Setting up fire8/concat
I1107 10:17:48.434128 11160 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 10:17:48.434128 11160 net.cpp:137] Memory required for data: 935092400
I1107 10:17:48.434128 11160 layer_factory.cpp:58] Creating layer fire9/squeeze1x1
I1107 10:17:48.434128 11160 net.cpp:84] Creating Layer fire9/squeeze1x1
I1107 10:17:48.434128 11160 net.cpp:406] fire9/squeeze1x1 <- fire8/concat
I1107 10:17:48.434128 11160 net.cpp:380] fire9/squeeze1x1 -> conv_21
I1107 10:17:48.435127 11160 net.cpp:122] Setting up fire9/squeeze1x1
I1107 10:17:48.435127 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.435127 11160 net.cpp:137] Memory required for data: 936346800
I1107 10:17:48.435127 11160 layer_factory.cpp:58] Creating layer bn_21
I1107 10:17:48.435127 11160 net.cpp:84] Creating Layer bn_21
I1107 10:17:48.435127 11160 net.cpp:406] bn_21 <- conv_21
I1107 10:17:48.435127 11160 net.cpp:367] bn_21 -> conv_21 (in-place)
I1107 10:17:48.435127 11160 net.cpp:122] Setting up bn_21
I1107 10:17:48.435127 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.435127 11160 net.cpp:137] Memory required for data: 937601200
I1107 10:17:48.435127 11160 layer_factory.cpp:58] Creating layer scale_21
I1107 10:17:48.435127 11160 net.cpp:84] Creating Layer scale_21
I1107 10:17:48.435127 11160 net.cpp:406] scale_21 <- conv_21
I1107 10:17:48.435127 11160 net.cpp:380] scale_21 -> fire9/squeeze1x1
I1107 10:17:48.436127 11160 layer_factory.cpp:58] Creating layer scale_21
I1107 10:17:48.436127 11160 net.cpp:122] Setting up scale_21
I1107 10:17:48.436127 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.436127 11160 net.cpp:137] Memory required for data: 938855600
I1107 10:17:48.436127 11160 layer_factory.cpp:58] Creating layer fire9/relu_squeeze1x1
I1107 10:17:48.436127 11160 net.cpp:84] Creating Layer fire9/relu_squeeze1x1
I1107 10:17:48.436127 11160 net.cpp:406] fire9/relu_squeeze1x1 <- fire9/squeeze1x1
I1107 10:17:48.436127 11160 net.cpp:367] fire9/relu_squeeze1x1 -> fire9/squeeze1x1 (in-place)
I1107 10:17:48.436127 11160 net.cpp:122] Setting up fire9/relu_squeeze1x1
I1107 10:17:48.436127 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.436127 11160 net.cpp:137] Memory required for data: 940110000
I1107 10:17:48.436127 11160 layer_factory.cpp:58] Creating layer fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 10:17:48.436127 11160 net.cpp:84] Creating Layer fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 10:17:48.436127 11160 net.cpp:406] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split <- fire9/squeeze1x1
I1107 10:17:48.436127 11160 net.cpp:380] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split -> fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_0
I1107 10:17:48.436127 11160 net.cpp:380] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split -> fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_1
I1107 10:17:48.436127 11160 net.cpp:122] Setting up fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 10:17:48.436127 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.436127 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.436127 11160 net.cpp:137] Memory required for data: 942618800
I1107 10:17:48.436127 11160 layer_factory.cpp:58] Creating layer fire9/expand1x1
I1107 10:17:48.436127 11160 net.cpp:84] Creating Layer fire9/expand1x1
I1107 10:17:48.436127 11160 net.cpp:406] fire9/expand1x1 <- fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_0
I1107 10:17:48.436127 11160 net.cpp:380] fire9/expand1x1 -> conv_22
I1107 10:17:48.437127 11160 net.cpp:122] Setting up fire9/expand1x1
I1107 10:17:48.437127 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.437127 11160 net.cpp:137] Memory required for data: 947636400
I1107 10:17:48.437127 11160 layer_factory.cpp:58] Creating layer bn_22
I1107 10:17:48.437127 11160 net.cpp:84] Creating Layer bn_22
I1107 10:17:48.437127 11160 net.cpp:406] bn_22 <- conv_22
I1107 10:17:48.437127 11160 net.cpp:367] bn_22 -> conv_22 (in-place)
I1107 10:17:48.437127 11160 net.cpp:122] Setting up bn_22
I1107 10:17:48.437127 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.437127 11160 net.cpp:137] Memory required for data: 952654000
I1107 10:17:48.437127 11160 layer_factory.cpp:58] Creating layer scale_22
I1107 10:17:48.437127 11160 net.cpp:84] Creating Layer scale_22
I1107 10:17:48.437127 11160 net.cpp:406] scale_22 <- conv_22
I1107 10:17:48.437127 11160 net.cpp:380] scale_22 -> fire9/expand1x1
I1107 10:17:48.437127 11160 layer_factory.cpp:58] Creating layer scale_22
I1107 10:17:48.438127 11160 net.cpp:122] Setting up scale_22
I1107 10:17:48.438127 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.438127 11160 net.cpp:137] Memory required for data: 957671600
I1107 10:17:48.438127 11160 layer_factory.cpp:58] Creating layer fire9/relu_expand1x1
I1107 10:17:48.438127 11160 net.cpp:84] Creating Layer fire9/relu_expand1x1
I1107 10:17:48.438127 11160 net.cpp:406] fire9/relu_expand1x1 <- fire9/expand1x1
I1107 10:17:48.438127 11160 net.cpp:367] fire9/relu_expand1x1 -> fire9/expand1x1 (in-place)
I1107 10:17:48.438127 11160 net.cpp:122] Setting up fire9/relu_expand1x1
I1107 10:17:48.438127 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.438127 11160 net.cpp:137] Memory required for data: 962689200
I1107 10:17:48.438127 11160 layer_factory.cpp:58] Creating layer fire9/expand3x3
I1107 10:17:48.438127 11160 net.cpp:84] Creating Layer fire9/expand3x3
I1107 10:17:48.438127 11160 net.cpp:406] fire9/expand3x3 <- fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_1
I1107 10:17:48.438127 11160 net.cpp:380] fire9/expand3x3 -> conv_23
I1107 10:17:48.440127 11160 net.cpp:122] Setting up fire9/expand3x3
I1107 10:17:48.440127 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.440127 11160 net.cpp:137] Memory required for data: 967706800
I1107 10:17:48.440127 11160 layer_factory.cpp:58] Creating layer bn_23
I1107 10:17:48.440127 11160 net.cpp:84] Creating Layer bn_23
I1107 10:17:48.440127 11160 net.cpp:406] bn_23 <- conv_23
I1107 10:17:48.440127 11160 net.cpp:367] bn_23 -> conv_23 (in-place)
I1107 10:17:48.440127 11160 net.cpp:122] Setting up bn_23
I1107 10:17:48.440127 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.440127 11160 net.cpp:137] Memory required for data: 972724400
I1107 10:17:48.440127 11160 layer_factory.cpp:58] Creating layer scale_23
I1107 10:17:48.440127 11160 net.cpp:84] Creating Layer scale_23
I1107 10:17:48.440127 11160 net.cpp:406] scale_23 <- conv_23
I1107 10:17:48.440127 11160 net.cpp:380] scale_23 -> fire9/expand3x3
I1107 10:17:48.440127 11160 layer_factory.cpp:58] Creating layer scale_23
I1107 10:17:48.440127 11160 net.cpp:122] Setting up scale_23
I1107 10:17:48.440127 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.440127 11160 net.cpp:137] Memory required for data: 977742000
I1107 10:17:48.440127 11160 layer_factory.cpp:58] Creating layer fire9/relu_expand3x3
I1107 10:17:48.440127 11160 net.cpp:84] Creating Layer fire9/relu_expand3x3
I1107 10:17:48.440127 11160 net.cpp:406] fire9/relu_expand3x3 <- fire9/expand3x3
I1107 10:17:48.440127 11160 net.cpp:367] fire9/relu_expand3x3 -> fire9/expand3x3 (in-place)
I1107 10:17:48.441128 11160 net.cpp:122] Setting up fire9/relu_expand3x3
I1107 10:17:48.441128 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.441128 11160 net.cpp:137] Memory required for data: 982759600
I1107 10:17:48.441128 11160 layer_factory.cpp:58] Creating layer fire9/concat
I1107 10:17:48.441128 11160 net.cpp:84] Creating Layer fire9/concat
I1107 10:17:48.441128 11160 net.cpp:406] fire9/concat <- fire9/expand1x1
I1107 10:17:48.441128 11160 net.cpp:406] fire9/concat <- fire9/expand3x3
I1107 10:17:48.441128 11160 net.cpp:380] fire9/concat -> fire9/concat
I1107 10:17:48.441128 11160 net.cpp:122] Setting up fire9/concat
I1107 10:17:48.441128 11160 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 10:17:48.441128 11160 net.cpp:137] Memory required for data: 992794800
I1107 10:17:48.441128 11160 layer_factory.cpp:58] Creating layer drop9
I1107 10:17:48.441128 11160 net.cpp:84] Creating Layer drop9
I1107 10:17:48.441128 11160 net.cpp:406] drop9 <- fire9/concat
I1107 10:17:48.441128 11160 net.cpp:367] drop9 -> fire9/concat (in-place)
I1107 10:17:48.441128 11160 net.cpp:122] Setting up drop9
I1107 10:17:48.441128 11160 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 10:17:48.441128 11160 net.cpp:137] Memory required for data: 1002830000
I1107 10:17:48.441128 11160 layer_factory.cpp:58] Creating layer conv10
I1107 10:17:48.441128 11160 net.cpp:84] Creating Layer conv10
I1107 10:17:48.441128 11160 net.cpp:406] conv10 <- fire9/concat
I1107 10:17:48.441128 11160 net.cpp:380] conv10 -> conv10
I1107 10:17:48.442127 11160 net.cpp:122] Setting up conv10
I1107 10:17:48.442127 11160 net.cpp:129] Top shape: 100 10 7 7 (49000)
I1107 10:17:48.442127 11160 net.cpp:137] Memory required for data: 1003026000
I1107 10:17:48.442127 11160 layer_factory.cpp:58] Creating layer relu_conv10
I1107 10:17:48.442127 11160 net.cpp:84] Creating Layer relu_conv10
I1107 10:17:48.442127 11160 net.cpp:406] relu_conv10 <- conv10
I1107 10:17:48.442127 11160 net.cpp:367] relu_conv10 -> conv10 (in-place)
I1107 10:17:48.442127 11160 net.cpp:122] Setting up relu_conv10
I1107 10:17:48.442127 11160 net.cpp:129] Top shape: 100 10 7 7 (49000)
I1107 10:17:48.443127 11160 net.cpp:137] Memory required for data: 1003222000
I1107 10:17:48.443127 11160 layer_factory.cpp:58] Creating layer pool10
I1107 10:17:48.443127 11160 net.cpp:84] Creating Layer pool10
I1107 10:17:48.443127 11160 net.cpp:406] pool10 <- conv10
I1107 10:17:48.443127 11160 net.cpp:380] pool10 -> pool10
I1107 10:17:48.443127 11160 net.cpp:122] Setting up pool10
I1107 10:17:48.443127 11160 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 10:17:48.443127 11160 net.cpp:137] Memory required for data: 1003226000
I1107 10:17:48.443127 11160 layer_factory.cpp:58] Creating layer pool10_pool10_0_split
I1107 10:17:48.443127 11160 net.cpp:84] Creating Layer pool10_pool10_0_split
I1107 10:17:48.443127 11160 net.cpp:406] pool10_pool10_0_split <- pool10
I1107 10:17:48.443127 11160 net.cpp:380] pool10_pool10_0_split -> pool10_pool10_0_split_0
I1107 10:17:48.443127 11160 net.cpp:380] pool10_pool10_0_split -> pool10_pool10_0_split_1
I1107 10:17:48.443127 11160 net.cpp:122] Setting up pool10_pool10_0_split
I1107 10:17:48.443127 11160 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 10:17:48.443127 11160 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 10:17:48.443127 11160 net.cpp:137] Memory required for data: 1003234000
I1107 10:17:48.443127 11160 layer_factory.cpp:58] Creating layer accuracy_training
I1107 10:17:48.443127 11160 net.cpp:84] Creating Layer accuracy_training
I1107 10:17:48.443127 11160 net.cpp:406] accuracy_training <- pool10_pool10_0_split_0
I1107 10:17:48.443127 11160 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1107 10:17:48.443127 11160 net.cpp:380] accuracy_training -> accuracy_training
I1107 10:17:48.443127 11160 net.cpp:122] Setting up accuracy_training
I1107 10:17:48.443127 11160 net.cpp:129] Top shape: (1)
I1107 10:17:48.443127 11160 net.cpp:137] Memory required for data: 1003234004
I1107 10:17:48.443127 11160 layer_factory.cpp:58] Creating layer loss
I1107 10:17:48.443127 11160 net.cpp:84] Creating Layer loss
I1107 10:17:48.443127 11160 net.cpp:406] loss <- pool10_pool10_0_split_1
I1107 10:17:48.443127 11160 net.cpp:406] loss <- label_cifar_1_split_1
I1107 10:17:48.443127 11160 net.cpp:380] loss -> loss
I1107 10:17:48.443127 11160 layer_factory.cpp:58] Creating layer loss
I1107 10:17:48.443127 11160 net.cpp:122] Setting up loss
I1107 10:17:48.443127 11160 net.cpp:129] Top shape: (1)
I1107 10:17:48.443127 11160 net.cpp:132]     with loss weight 1
I1107 10:17:48.443127 11160 net.cpp:137] Memory required for data: 1003234008
I1107 10:17:48.443127 11160 net.cpp:198] loss needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:200] accuracy_training does not need backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] pool10_pool10_0_split needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] pool10 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] relu_conv10 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] conv10 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] drop9 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire9/concat needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire9/relu_expand3x3 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] scale_23 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] bn_23 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire9/expand3x3 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire9/relu_expand1x1 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] scale_22 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] bn_22 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire9/expand1x1 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire9/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] scale_21 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] bn_21 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire9/squeeze1x1 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire8/concat needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire8/relu_expand3x3 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] scale_20 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] bn_20 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire8/expand3x3 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire8/relu_expand1x1 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] scale_19 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] bn_19 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire8/expand1x1 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] fire8/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] scale_18 needs backward computation.
I1107 10:17:48.443127 11160 net.cpp:198] bn_18 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire8/squeeze1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire7/concat needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire7/relu_expand3x3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_17 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_17 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire7/expand3x3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire7/relu_expand1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_16 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_16 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire7/expand1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire7/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_15 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_15 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire7/squeeze1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire6/concat needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire6/relu_expand3x3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_14 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_14 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire6/expand3x3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire6/relu_expand1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_13 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_13 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire6/expand1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire6/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_12 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_12 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire6/squeeze1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] pool5 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire5/concat needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire5/relu_expand3x3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_11 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_11 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire5/expand3x3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire5/relu_expand1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_10 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_10 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire5/expand1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire5/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_9 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_9 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire5/squeeze1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire4/concat needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire4/relu_expand3x3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_8 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_8 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire4/expand3x3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire4/relu_expand1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire4/expand1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire4/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_7 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_7 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire4/squeeze1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] pool3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire3/concat needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire3/relu_expand3x3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_6 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_6 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire3/expand3x3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire3/relu_expand1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_5 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_5 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire3/expand1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire3/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_4 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_4 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire3/squeeze1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire2/concat needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire2/relu_expand3x3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale_3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn_3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire2/expand3x3 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire2/relu_expand1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale2 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn2 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire2/expand1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire2/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] fire2/squeeze1x1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] pool1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] relu_conv1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] scale1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] bn1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:198] conv1 needs backward computation.
I1107 10:17:48.444128 11160 net.cpp:200] label_cifar_1_split does not need backward computation.
I1107 10:17:48.444128 11160 net.cpp:200] cifar does not need backward computation.
I1107 10:17:48.444128 11160 net.cpp:242] This network produces output accuracy_training
I1107 10:17:48.444128 11160 net.cpp:242] This network produces output loss
I1107 10:17:48.444128 11160 net.cpp:255] Network initialization done.
I1107 10:17:48.445127 11160 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1107 10:17:48.445127 11160 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1107 10:17:48.445127 11160 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_3
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_4
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_5
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_6
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_7
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_8
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_9
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_10
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_11
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_12
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_13
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_14
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_15
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_16
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_17
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_18
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_19
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_20
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_21
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_22
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_23
I1107 10:17:48.446127 11160 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1107 10:17:48.446127 11160 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_Squeezenet_1.1_Batchnorm"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "fire2/squeeze1x1"
  type: "Convolution"
  bottom: "pool1"
  top: "fire2/squeeze1x1"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire2/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire2/squeeze1x1"
  top: "fire2/squeeze1x1"
}
layer {
  name: "fire2/expand1x1"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "conv2"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "fire2/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire2/relu_expand1x1"
  type: "ReLU"
  bottom: "fire2/expand1x1"
  top: "fire2/expand1x1"
}
layer {
  name: "fire2/expand3x3"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "conv_3"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_3"
  type: "BatchNorm"
  bottom: "conv_3"
  top: "conv_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_3"
  type: "Scale"
  bottom: "conv_3"
  top: "fire2/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire2/relu_expand3x3"
  type: "ReLU"
  bottom: "fire2/expand3x3"
  top: "fire2/expand3x3"
}
layer {
  name: "fire2/concat"
  type: "Concat"
  bottom: "fire2/expand1x1"
  bottom: "fire2/expand3x3"
  top: "fire2/concat"
}
layer {
  name: "fire3/squeeze1x1"
  type: "Convolution"
  bottom: "fire2/concat"
  top: "conv_4"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_4"
  type: "BatchNorm"
  bottom: "conv_4"
  top: "conv_4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_4"
  type: "Scale"
  bottom: "conv_4"
  top: "fire3/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire3/squeeze1x1"
  top: "fire3/squeeze1x1"
}
layer {
  name: "fire3/expand1x1"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "conv_5"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_5"
  type: "BatchNorm"
  bottom: "conv_5"
  top: "conv_5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_5"
  type: "Scale"
  bottom: "conv_5"
  top: "fire3/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_expand1x1"
  type: "ReLU"
  bottom: "fire3/expand1x1"
  top: "fire3/expand1x1"
}
layer {
  name: "fire3/expand3x3"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "conv_6"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_6"
  type: "BatchNorm"
  bottom: "conv_6"
  top: "conv_6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_6"
  type: "Scale"
  bottom: "conv_6"
  top: "fire3/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_expand3x3"
  type: "ReLU"
  bottom: "fire3/expand3x3"
  top: "fire3/expand3x3"
}
layer {
  name: "fire3/concat"
  type: "Concat"
  bottom: "fire3/expand1x1"
  bottom: "fire3/expand3x3"
  top: "fire3/concat"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "fire3/concat"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire4/squeeze1x1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv_7"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_7"
  type: "BatchNorm"
  bottom: "conv_7"
  top: "conv_7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_7"
  type: "Scale"
  bottom: "conv_7"
  top: "fire4/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire4/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire4/squeeze1x1"
  top: "fire4/squeeze1x1"
}
layer {
  name: "fire4/expand1x1"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "fire4/expand1x1"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire4/relu_expand1x1"
  type: "ReLU"
  bottom: "fire4/expand1x1"
  top: "fire4/expand1x1"
}
layer {
  name: "fire4/expand3x3"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "conv_8"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_8"
  type: "BatchNorm"
  bottom: "conv_8"
  top: "conv_8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_8"
  type: "Scale"
  bottom: "conv_8"
  top: "fire4/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire4/relu_expand3x3"
  type: "ReLU"
  bottom: "fire4/expand3x3"
  top: "fire4/expand3x3"
}
layer {
  name: "fire4/concat"
  type: "Concat"
  bottom: "fire4/expand1x1"
  bottom: "fire4/expand3x3"
  top: "fire4/concat"
}
layer {
  name: "fire5/squeeze1x1"
  type: "Convolution"
  bottom: "fire4/concat"
  top: "conv_9"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_9"
  type: "BatchNorm"
  bottom: "conv_9"
  top: "conv_9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_9"
  type: "Scale"
  bottom: "conv_9"
  top: "fire5/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire5/squeeze1x1"
  top: "fire5/squeeze1x1"
}
layer {
  name: "fire5/expand1x1"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "conv_10"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_10"
  type: "BatchNorm"
  bottom: "conv_10"
  top: "conv_10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_10"
  type: "Scale"
  bottom: "conv_10"
  top: "fire5/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_expand1x1"
  type: "ReLU"
  bottom: "fire5/expand1x1"
  top: "fire5/expand1x1"
}
layer {
  name: "fire5/expand3x3"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "conv_11"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_11"
  type: "BatchNorm"
  bottom: "conv_11"
  top: "conv_11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_11"
  type: "Scale"
  bottom: "conv_11"
  top: "fire5/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_expand3x3"
  type: "ReLU"
  bottom: "fire5/expand3x3"
  top: "fire5/expand3x3"
}
layer {
  name: "fire5/concat"
  type: "Concat"
  bottom: "fire5/expand1x1"
  bottom: "fire5/expand3x3"
  top: "fire5/concat"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "fire5/concat"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire6/squeeze1x1"
  type: "Convolution"
  bottom: "pool5"
  top: "conv_12"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_12"
  type: "BatchNorm"
  bottom: "conv_12"
  top: "conv_12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_12"
  type: "Scale"
  bottom: "conv_12"
  top: "fire6/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire6/squeeze1x1"
  top: "fire6/squeeze1x1"
}
layer {
  name: "fire6/expand1x1"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "conv_13"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_13"
  type: "BatchNorm"
  bottom: "conv_13"
  top: "conv_13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_13"
  type: "Scale"
  bottom: "conv_13"
  top: "fire6/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_expand1x1"
  type: "ReLU"
  bottom: "fire6/expand1x1"
  top: "fire6/expand1x1"
}
layer {
  name: "fire6/expand3x3"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "conv_14"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_14"
  type: "BatchNorm"
  bottom: "conv_14"
  top: "conv_14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_14"
  type: "Scale"
  bottom: "conv_14"
  top: "fire6/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_expand3x3"
  type: "ReLU"
  bottom: "fire6/expand3x3"
  top: "fire6/expand3x3"
}
layer {
  name: "fire6/concat"
  type: "Concat"
  bottom: "fire6/expand1x1"
  bottom: "fire6/expand3x3"
  top: "fire6/concat"
}
layer {
  name: "fire7/squeeze1x1"
  type: "Convolution"
  bottom: "fire6/concat"
  top: "conv_15"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_15"
  type: "BatchNorm"
  bottom: "conv_15"
  top: "conv_15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_15"
  type: "Scale"
  bottom: "conv_15"
  top: "fire7/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire7/squeeze1x1"
  top: "fire7/squeeze1x1"
}
layer {
  name: "fire7/expand1x1"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "conv_16"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_16"
  type: "BatchNorm"
  bottom: "conv_16"
  top: "conv_16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_16"
  type: "Scale"
  bottom: "conv_16"
  top: "fire7/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_expand1x1"
  type: "ReLU"
  bottom: "fire7/expand1x1"
  top: "fire7/expand1x1"
}
layer {
  name: "fire7/expand3x3"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "conv_17"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_17"
  type: "BatchNorm"
  bottom: "conv_17"
  top: "conv_17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_17"
  type: "Scale"
  bottom: "conv_17"
  top: "fire7/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_expand3x3"
  type: "ReLU"
  bottom: "fire7/expand3x3"
  top: "fire7/expand3x3"
}
layer {
  name: "fire7/concat"
  type: "Concat"
  bottom: "fire7/expand1x1"
  bottom: "fire7/expand3x3"
  top: "fire7/concat"
}
layer {
  name: "fire8/squeeze1x1"
  type: "Convolution"
  bottom: "fire7/concat"
  top: "conv_18"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_18"
  type: "BatchNorm"
  bottom: "conv_18"
  top: "conv_18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_18"
  type: "Scale"
  bottom: "conv_18"
  top: "fire8/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire8/squeeze1x1"
  top: "fire8/squeeze1x1"
}
layer {
  name: "fire8/expand1x1"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "conv_19"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_19"
  type: "BatchNorm"
  bottom: "conv_19"
  top: "conv_19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_19"
  type: "Scale"
  bottom: "conv_19"
  top: "fire8/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_expand1x1"
  type: "ReLU"
  bottom: "fire8/expand1x1"
  top: "fire8/expand1x1"
}
layer {
  name: "fire8/expand3x3"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "conv_20"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_20"
  type: "BatchNorm"
  bottom: "conv_20"
  top: "conv_20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_20"
  type: "Scale"
  bottom: "conv_20"
  top: "fire8/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_expand3x3"
  type: "ReLU"
  bottom: "fire8/expand3x3"
  top: "fire8/expand3x3"
}
layer {
  name: "fire8/concat"
  type: "Concat"
  bottom: "fire8/expand1x1"
  bottom: "fire8/expand3x3"
  top: "fire8/concat"
}
layer {
  name: "fire9/squeeze1x1"
  type: "Convolution"
  bottom: "fire8/concat"
  top: "conv_21"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_21"
  type: "BatchNorm"
  bottom: "conv_21"
  top: "conv_21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_21"
  type: "Scale"
  bottom: "conv_21"
  top: "fire9/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire9/squeeze1x1"
  top: "fire9/squeeze1x1"
}
layer {
  name: "fire9/expand1x1"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "conv_22"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_22"
  type: "BatchNorm"
  bottom: "conv_22"
  top: "conv_22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_22"
  type: "Scale"
  bottom: "conv_22"
  top: "fire9/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_expand1x1"
  type: "ReLU"
  bottom: "fire9/expand1x1"
  top: "fire9/expand1x1"
}
layer {
  name: "fire9/expand3x3"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "conv_23"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_23"
  type: "BatchNorm"
  bottom: "conv_23"
  top: "conv_23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_23"
  type: "Scale"
  bottom: "conv_23"
  top: "fire9/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_expand3x3"
  type: "ReLU"
  bottom: "fire9/expand3x3"
  top: "fire9/expand3x3"
}
layer {
  name: "fire9/concat"
  type: "Concat"
  bottom: "fire9/expand1x1"
  bottom: "fire9/expand3x3"
  top: "fire9/concat"
}
layer {
  name: "drop9"
  type: "Dropout"
  bottom: "fire9/concat"
  top: "fire9/concat"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv10"
  type: "Convolution"
  bottom: "fire9/concat"
  top: "conv10"
  convolution_param {
    num_output: 10
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
  }
}
layer {
  name: "relu_conv10"
  type: "ReLU"
  bottom: "conv10"
  top: "conv10"
}
layer {
  name: "pool10"
  type: "Pooling"
  bottom: "conv10"
  top: "pool10"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "pool10"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "pool10"
  bottom: "label"
  top: "loss"
}
I1107 10:17:48.447127 11160 layer_factory.cpp:58] Creating layer cifar
I1107 10:17:48.450112 11160 db_leveldb.cpp:18] Opened leveldb examples/cifar10/cifar10_test_leveldb_padding
I1107 10:17:48.450112 11160 net.cpp:84] Creating Layer cifar
I1107 10:17:48.450112 11160 net.cpp:380] cifar -> data
I1107 10:17:48.450112 11160 net.cpp:380] cifar -> label
I1107 10:17:48.450112 11160 data_layer.cpp:45] output data size: 100,3,32,32
I1107 10:17:48.461119 11160 net.cpp:122] Setting up cifar
I1107 10:17:48.461119 11160 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1107 10:17:48.461119 11160 net.cpp:129] Top shape: 100 (100)
I1107 10:17:48.461119 11160 net.cpp:137] Memory required for data: 1229200
I1107 10:17:48.461119 11160 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1107 10:17:48.461119 11160 net.cpp:84] Creating Layer label_cifar_1_split
I1107 10:17:48.461119 11160 net.cpp:406] label_cifar_1_split <- label
I1107 10:17:48.461119 11160 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1107 10:17:48.461119 11160 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1107 10:17:48.461119 11160 net.cpp:122] Setting up label_cifar_1_split
I1107 10:17:48.461119 11160 net.cpp:129] Top shape: 100 (100)
I1107 10:17:48.461119 11160 net.cpp:129] Top shape: 100 (100)
I1107 10:17:48.461119 11160 net.cpp:137] Memory required for data: 1230000
I1107 10:17:48.461119 11160 layer_factory.cpp:58] Creating layer conv1
I1107 10:17:48.461119 11160 net.cpp:84] Creating Layer conv1
I1107 10:17:48.461119 11160 net.cpp:406] conv1 <- data
I1107 10:17:48.461119 11160 net.cpp:380] conv1 -> conv1
I1107 10:17:48.463119 16904 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1107 10:17:48.463119 11160 net.cpp:122] Setting up conv1
I1107 10:17:48.463119 11160 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 10:17:48.463119 11160 net.cpp:137] Memory required for data: 24270000
I1107 10:17:48.463119 11160 layer_factory.cpp:58] Creating layer bn1
I1107 10:17:48.463119 11160 net.cpp:84] Creating Layer bn1
I1107 10:17:48.463119 11160 net.cpp:406] bn1 <- conv1
I1107 10:17:48.463119 11160 net.cpp:367] bn1 -> conv1 (in-place)
I1107 10:17:48.463119 11160 net.cpp:122] Setting up bn1
I1107 10:17:48.463119 11160 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 10:17:48.463119 11160 net.cpp:137] Memory required for data: 47310000
I1107 10:17:48.463119 11160 layer_factory.cpp:58] Creating layer scale1
I1107 10:17:48.463119 11160 net.cpp:84] Creating Layer scale1
I1107 10:17:48.463119 11160 net.cpp:406] scale1 <- conv1
I1107 10:17:48.463119 11160 net.cpp:367] scale1 -> conv1 (in-place)
I1107 10:17:48.464119 11160 layer_factory.cpp:58] Creating layer scale1
I1107 10:17:48.464119 11160 net.cpp:122] Setting up scale1
I1107 10:17:48.464119 11160 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 10:17:48.464119 11160 net.cpp:137] Memory required for data: 70350000
I1107 10:17:48.464119 11160 layer_factory.cpp:58] Creating layer relu_conv1
I1107 10:17:48.464119 11160 net.cpp:84] Creating Layer relu_conv1
I1107 10:17:48.464119 11160 net.cpp:406] relu_conv1 <- conv1
I1107 10:17:48.464119 11160 net.cpp:367] relu_conv1 -> conv1 (in-place)
I1107 10:17:48.464119 11160 net.cpp:122] Setting up relu_conv1
I1107 10:17:48.464119 11160 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 10:17:48.464119 11160 net.cpp:137] Memory required for data: 93390000
I1107 10:17:48.464119 11160 layer_factory.cpp:58] Creating layer pool1
I1107 10:17:48.464119 11160 net.cpp:84] Creating Layer pool1
I1107 10:17:48.464119 11160 net.cpp:406] pool1 <- conv1
I1107 10:17:48.464119 11160 net.cpp:380] pool1 -> pool1
I1107 10:17:48.464119 11160 net.cpp:122] Setting up pool1
I1107 10:17:48.464119 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.464119 11160 net.cpp:137] Memory required for data: 113460400
I1107 10:17:48.464119 11160 layer_factory.cpp:58] Creating layer fire2/squeeze1x1
I1107 10:17:48.464119 11160 net.cpp:84] Creating Layer fire2/squeeze1x1
I1107 10:17:48.464119 11160 net.cpp:406] fire2/squeeze1x1 <- pool1
I1107 10:17:48.464119 11160 net.cpp:380] fire2/squeeze1x1 -> fire2/squeeze1x1
I1107 10:17:48.466110 11160 net.cpp:122] Setting up fire2/squeeze1x1
I1107 10:17:48.466110 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.466110 11160 net.cpp:137] Memory required for data: 118478000
I1107 10:17:48.466110 11160 layer_factory.cpp:58] Creating layer fire2/relu_squeeze1x1
I1107 10:17:48.466110 11160 net.cpp:84] Creating Layer fire2/relu_squeeze1x1
I1107 10:17:48.466110 11160 net.cpp:406] fire2/relu_squeeze1x1 <- fire2/squeeze1x1
I1107 10:17:48.466110 11160 net.cpp:367] fire2/relu_squeeze1x1 -> fire2/squeeze1x1 (in-place)
I1107 10:17:48.466110 11160 net.cpp:122] Setting up fire2/relu_squeeze1x1
I1107 10:17:48.466110 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.466110 11160 net.cpp:137] Memory required for data: 123495600
I1107 10:17:48.466110 11160 layer_factory.cpp:58] Creating layer fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 10:17:48.466110 11160 net.cpp:84] Creating Layer fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 10:17:48.466110 11160 net.cpp:406] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split <- fire2/squeeze1x1
I1107 10:17:48.466110 11160 net.cpp:380] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split -> fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_0
I1107 10:17:48.466110 11160 net.cpp:380] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split -> fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_1
I1107 10:17:48.466110 11160 net.cpp:122] Setting up fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 10:17:48.466110 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.466110 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.466110 11160 net.cpp:137] Memory required for data: 133530800
I1107 10:17:48.466110 11160 layer_factory.cpp:58] Creating layer fire2/expand1x1
I1107 10:17:48.466110 11160 net.cpp:84] Creating Layer fire2/expand1x1
I1107 10:17:48.466110 11160 net.cpp:406] fire2/expand1x1 <- fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_0
I1107 10:17:48.466110 11160 net.cpp:380] fire2/expand1x1 -> conv2
I1107 10:17:48.467120 11160 net.cpp:122] Setting up fire2/expand1x1
I1107 10:17:48.467120 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.467120 11160 net.cpp:137] Memory required for data: 153601200
I1107 10:17:48.467120 11160 layer_factory.cpp:58] Creating layer bn2
I1107 10:17:48.467120 11160 net.cpp:84] Creating Layer bn2
I1107 10:17:48.467120 11160 net.cpp:406] bn2 <- conv2
I1107 10:17:48.467120 11160 net.cpp:367] bn2 -> conv2 (in-place)
I1107 10:17:48.468120 11160 net.cpp:122] Setting up bn2
I1107 10:17:48.468120 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.468120 11160 net.cpp:137] Memory required for data: 173671600
I1107 10:17:48.468120 11160 layer_factory.cpp:58] Creating layer scale2
I1107 10:17:48.468120 11160 net.cpp:84] Creating Layer scale2
I1107 10:17:48.468120 11160 net.cpp:406] scale2 <- conv2
I1107 10:17:48.468120 11160 net.cpp:380] scale2 -> fire2/expand1x1
I1107 10:17:48.468120 11160 layer_factory.cpp:58] Creating layer scale2
I1107 10:17:48.468120 11160 net.cpp:122] Setting up scale2
I1107 10:17:48.468120 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.468120 11160 net.cpp:137] Memory required for data: 193742000
I1107 10:17:48.468120 11160 layer_factory.cpp:58] Creating layer fire2/relu_expand1x1
I1107 10:17:48.468120 11160 net.cpp:84] Creating Layer fire2/relu_expand1x1
I1107 10:17:48.468120 11160 net.cpp:406] fire2/relu_expand1x1 <- fire2/expand1x1
I1107 10:17:48.468120 11160 net.cpp:367] fire2/relu_expand1x1 -> fire2/expand1x1 (in-place)
I1107 10:17:48.469111 11160 net.cpp:122] Setting up fire2/relu_expand1x1
I1107 10:17:48.469111 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.469111 11160 net.cpp:137] Memory required for data: 213812400
I1107 10:17:48.469111 11160 layer_factory.cpp:58] Creating layer fire2/expand3x3
I1107 10:17:48.469111 11160 net.cpp:84] Creating Layer fire2/expand3x3
I1107 10:17:48.469111 11160 net.cpp:406] fire2/expand3x3 <- fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_1
I1107 10:17:48.469111 11160 net.cpp:380] fire2/expand3x3 -> conv_3
I1107 10:17:48.470108 11160 net.cpp:122] Setting up fire2/expand3x3
I1107 10:17:48.470108 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.470108 11160 net.cpp:137] Memory required for data: 233882800
I1107 10:17:48.470108 11160 layer_factory.cpp:58] Creating layer bn_3
I1107 10:17:48.470108 11160 net.cpp:84] Creating Layer bn_3
I1107 10:17:48.470108 11160 net.cpp:406] bn_3 <- conv_3
I1107 10:17:48.470108 11160 net.cpp:367] bn_3 -> conv_3 (in-place)
I1107 10:17:48.470108 11160 net.cpp:122] Setting up bn_3
I1107 10:17:48.470108 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.470108 11160 net.cpp:137] Memory required for data: 253953200
I1107 10:17:48.470108 11160 layer_factory.cpp:58] Creating layer scale_3
I1107 10:17:48.470108 11160 net.cpp:84] Creating Layer scale_3
I1107 10:17:48.470108 11160 net.cpp:406] scale_3 <- conv_3
I1107 10:17:48.470108 11160 net.cpp:380] scale_3 -> fire2/expand3x3
I1107 10:17:48.470108 11160 layer_factory.cpp:58] Creating layer scale_3
I1107 10:17:48.471120 11160 net.cpp:122] Setting up scale_3
I1107 10:17:48.471120 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.471120 11160 net.cpp:137] Memory required for data: 274023600
I1107 10:17:48.471120 11160 layer_factory.cpp:58] Creating layer fire2/relu_expand3x3
I1107 10:17:48.471120 11160 net.cpp:84] Creating Layer fire2/relu_expand3x3
I1107 10:17:48.471120 11160 net.cpp:406] fire2/relu_expand3x3 <- fire2/expand3x3
I1107 10:17:48.471120 11160 net.cpp:367] fire2/relu_expand3x3 -> fire2/expand3x3 (in-place)
I1107 10:17:48.471120 11160 net.cpp:122] Setting up fire2/relu_expand3x3
I1107 10:17:48.471120 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.471120 11160 net.cpp:137] Memory required for data: 294094000
I1107 10:17:48.471120 11160 layer_factory.cpp:58] Creating layer fire2/concat
I1107 10:17:48.471120 11160 net.cpp:84] Creating Layer fire2/concat
I1107 10:17:48.471120 11160 net.cpp:406] fire2/concat <- fire2/expand1x1
I1107 10:17:48.471120 11160 net.cpp:406] fire2/concat <- fire2/expand3x3
I1107 10:17:48.471120 11160 net.cpp:380] fire2/concat -> fire2/concat
I1107 10:17:48.471120 11160 net.cpp:122] Setting up fire2/concat
I1107 10:17:48.471120 11160 net.cpp:129] Top shape: 100 128 28 28 (10035200)
I1107 10:17:48.471120 11160 net.cpp:137] Memory required for data: 334234800
I1107 10:17:48.471120 11160 layer_factory.cpp:58] Creating layer fire3/squeeze1x1
I1107 10:17:48.471120 11160 net.cpp:84] Creating Layer fire3/squeeze1x1
I1107 10:17:48.471120 11160 net.cpp:406] fire3/squeeze1x1 <- fire2/concat
I1107 10:17:48.471120 11160 net.cpp:380] fire3/squeeze1x1 -> conv_4
I1107 10:17:48.472120 11160 net.cpp:122] Setting up fire3/squeeze1x1
I1107 10:17:48.472120 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.472120 11160 net.cpp:137] Memory required for data: 339252400
I1107 10:17:48.472120 11160 layer_factory.cpp:58] Creating layer bn_4
I1107 10:17:48.472120 11160 net.cpp:84] Creating Layer bn_4
I1107 10:17:48.472120 11160 net.cpp:406] bn_4 <- conv_4
I1107 10:17:48.472120 11160 net.cpp:367] bn_4 -> conv_4 (in-place)
I1107 10:17:48.473119 11160 net.cpp:122] Setting up bn_4
I1107 10:17:48.473119 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.473119 11160 net.cpp:137] Memory required for data: 344270000
I1107 10:17:48.473119 11160 layer_factory.cpp:58] Creating layer scale_4
I1107 10:17:48.473119 11160 net.cpp:84] Creating Layer scale_4
I1107 10:17:48.473119 11160 net.cpp:406] scale_4 <- conv_4
I1107 10:17:48.473119 11160 net.cpp:380] scale_4 -> fire3/squeeze1x1
I1107 10:17:48.473119 11160 layer_factory.cpp:58] Creating layer scale_4
I1107 10:17:48.473119 11160 net.cpp:122] Setting up scale_4
I1107 10:17:48.473119 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.473119 11160 net.cpp:137] Memory required for data: 349287600
I1107 10:17:48.473119 11160 layer_factory.cpp:58] Creating layer fire3/relu_squeeze1x1
I1107 10:17:48.473119 11160 net.cpp:84] Creating Layer fire3/relu_squeeze1x1
I1107 10:17:48.473119 11160 net.cpp:406] fire3/relu_squeeze1x1 <- fire3/squeeze1x1
I1107 10:17:48.473119 11160 net.cpp:367] fire3/relu_squeeze1x1 -> fire3/squeeze1x1 (in-place)
I1107 10:17:48.473119 11160 net.cpp:122] Setting up fire3/relu_squeeze1x1
I1107 10:17:48.473119 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.473119 11160 net.cpp:137] Memory required for data: 354305200
I1107 10:17:48.473119 11160 layer_factory.cpp:58] Creating layer fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 10:17:48.473119 11160 net.cpp:84] Creating Layer fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 10:17:48.473119 11160 net.cpp:406] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split <- fire3/squeeze1x1
I1107 10:17:48.473119 11160 net.cpp:380] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split -> fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_0
I1107 10:17:48.473119 11160 net.cpp:380] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split -> fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_1
I1107 10:17:48.473119 11160 net.cpp:122] Setting up fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 10:17:48.473119 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.473119 11160 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 10:17:48.473119 11160 net.cpp:137] Memory required for data: 364340400
I1107 10:17:48.473119 11160 layer_factory.cpp:58] Creating layer fire3/expand1x1
I1107 10:17:48.473119 11160 net.cpp:84] Creating Layer fire3/expand1x1
I1107 10:17:48.473119 11160 net.cpp:406] fire3/expand1x1 <- fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_0
I1107 10:17:48.473119 11160 net.cpp:380] fire3/expand1x1 -> conv_5
I1107 10:17:48.474119 11160 net.cpp:122] Setting up fire3/expand1x1
I1107 10:17:48.474119 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.474119 11160 net.cpp:137] Memory required for data: 384410800
I1107 10:17:48.474119 11160 layer_factory.cpp:58] Creating layer bn_5
I1107 10:17:48.474119 11160 net.cpp:84] Creating Layer bn_5
I1107 10:17:48.474119 11160 net.cpp:406] bn_5 <- conv_5
I1107 10:17:48.475119 11160 net.cpp:367] bn_5 -> conv_5 (in-place)
I1107 10:17:48.475119 11160 net.cpp:122] Setting up bn_5
I1107 10:17:48.475119 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.475119 11160 net.cpp:137] Memory required for data: 404481200
I1107 10:17:48.475119 11160 layer_factory.cpp:58] Creating layer scale_5
I1107 10:17:48.475119 11160 net.cpp:84] Creating Layer scale_5
I1107 10:17:48.475119 11160 net.cpp:406] scale_5 <- conv_5
I1107 10:17:48.475119 11160 net.cpp:380] scale_5 -> fire3/expand1x1
I1107 10:17:48.475633 11160 layer_factory.cpp:58] Creating layer scale_5
I1107 10:17:48.475633 11160 net.cpp:122] Setting up scale_5
I1107 10:17:48.475633 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.475633 11160 net.cpp:137] Memory required for data: 424551600
I1107 10:17:48.475633 11160 layer_factory.cpp:58] Creating layer fire3/relu_expand1x1
I1107 10:17:48.475633 11160 net.cpp:84] Creating Layer fire3/relu_expand1x1
I1107 10:17:48.475633 11160 net.cpp:406] fire3/relu_expand1x1 <- fire3/expand1x1
I1107 10:17:48.475633 11160 net.cpp:367] fire3/relu_expand1x1 -> fire3/expand1x1 (in-place)
I1107 10:17:48.475633 11160 net.cpp:122] Setting up fire3/relu_expand1x1
I1107 10:17:48.475633 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.475633 11160 net.cpp:137] Memory required for data: 444622000
I1107 10:17:48.475633 11160 layer_factory.cpp:58] Creating layer fire3/expand3x3
I1107 10:17:48.475633 11160 net.cpp:84] Creating Layer fire3/expand3x3
I1107 10:17:48.475633 11160 net.cpp:406] fire3/expand3x3 <- fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_1
I1107 10:17:48.475633 11160 net.cpp:380] fire3/expand3x3 -> conv_6
I1107 10:17:48.477133 11160 net.cpp:122] Setting up fire3/expand3x3
I1107 10:17:48.477133 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.477133 11160 net.cpp:137] Memory required for data: 464692400
I1107 10:17:48.477133 11160 layer_factory.cpp:58] Creating layer bn_6
I1107 10:17:48.477133 11160 net.cpp:84] Creating Layer bn_6
I1107 10:17:48.477133 11160 net.cpp:406] bn_6 <- conv_6
I1107 10:17:48.477133 11160 net.cpp:367] bn_6 -> conv_6 (in-place)
I1107 10:17:48.477133 11160 net.cpp:122] Setting up bn_6
I1107 10:17:48.477133 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.477133 11160 net.cpp:137] Memory required for data: 484762800
I1107 10:17:48.477133 11160 layer_factory.cpp:58] Creating layer scale_6
I1107 10:17:48.477133 11160 net.cpp:84] Creating Layer scale_6
I1107 10:17:48.477133 11160 net.cpp:406] scale_6 <- conv_6
I1107 10:17:48.477133 11160 net.cpp:380] scale_6 -> fire3/expand3x3
I1107 10:17:48.477133 11160 layer_factory.cpp:58] Creating layer scale_6
I1107 10:17:48.477633 11160 net.cpp:122] Setting up scale_6
I1107 10:17:48.477633 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.477633 11160 net.cpp:137] Memory required for data: 504833200
I1107 10:17:48.477633 11160 layer_factory.cpp:58] Creating layer fire3/relu_expand3x3
I1107 10:17:48.477633 11160 net.cpp:84] Creating Layer fire3/relu_expand3x3
I1107 10:17:48.477633 11160 net.cpp:406] fire3/relu_expand3x3 <- fire3/expand3x3
I1107 10:17:48.477633 11160 net.cpp:367] fire3/relu_expand3x3 -> fire3/expand3x3 (in-place)
I1107 10:17:48.477633 11160 net.cpp:122] Setting up fire3/relu_expand3x3
I1107 10:17:48.477633 11160 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 10:17:48.477633 11160 net.cpp:137] Memory required for data: 524903600
I1107 10:17:48.477633 11160 layer_factory.cpp:58] Creating layer fire3/concat
I1107 10:17:48.477633 11160 net.cpp:84] Creating Layer fire3/concat
I1107 10:17:48.477633 11160 net.cpp:406] fire3/concat <- fire3/expand1x1
I1107 10:17:48.477633 11160 net.cpp:406] fire3/concat <- fire3/expand3x3
I1107 10:17:48.477633 11160 net.cpp:380] fire3/concat -> fire3/concat
I1107 10:17:48.477633 11160 net.cpp:122] Setting up fire3/concat
I1107 10:17:48.477633 11160 net.cpp:129] Top shape: 100 128 28 28 (10035200)
I1107 10:17:48.477633 11160 net.cpp:137] Memory required for data: 565044400
I1107 10:17:48.477633 11160 layer_factory.cpp:58] Creating layer pool3
I1107 10:17:48.477633 11160 net.cpp:84] Creating Layer pool3
I1107 10:17:48.477633 11160 net.cpp:406] pool3 <- fire3/concat
I1107 10:17:48.477633 11160 net.cpp:380] pool3 -> pool3
I1107 10:17:48.477633 11160 net.cpp:122] Setting up pool3
I1107 10:17:48.477633 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.477633 11160 net.cpp:137] Memory required for data: 575079600
I1107 10:17:48.477633 11160 layer_factory.cpp:58] Creating layer fire4/squeeze1x1
I1107 10:17:48.477633 11160 net.cpp:84] Creating Layer fire4/squeeze1x1
I1107 10:17:48.477633 11160 net.cpp:406] fire4/squeeze1x1 <- pool3
I1107 10:17:48.478133 11160 net.cpp:380] fire4/squeeze1x1 -> conv_7
I1107 10:17:48.479135 11160 net.cpp:122] Setting up fire4/squeeze1x1
I1107 10:17:48.479135 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.479135 11160 net.cpp:137] Memory required for data: 577588400
I1107 10:17:48.479135 11160 layer_factory.cpp:58] Creating layer bn_7
I1107 10:17:48.479135 11160 net.cpp:84] Creating Layer bn_7
I1107 10:17:48.479135 11160 net.cpp:406] bn_7 <- conv_7
I1107 10:17:48.479135 11160 net.cpp:367] bn_7 -> conv_7 (in-place)
I1107 10:17:48.479135 11160 net.cpp:122] Setting up bn_7
I1107 10:17:48.479135 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.479135 11160 net.cpp:137] Memory required for data: 580097200
I1107 10:17:48.479135 11160 layer_factory.cpp:58] Creating layer scale_7
I1107 10:17:48.479635 11160 net.cpp:84] Creating Layer scale_7
I1107 10:17:48.479635 11160 net.cpp:406] scale_7 <- conv_7
I1107 10:17:48.479635 11160 net.cpp:380] scale_7 -> fire4/squeeze1x1
I1107 10:17:48.479635 11160 layer_factory.cpp:58] Creating layer scale_7
I1107 10:17:48.479635 11160 net.cpp:122] Setting up scale_7
I1107 10:17:48.479635 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.479635 11160 net.cpp:137] Memory required for data: 582606000
I1107 10:17:48.479635 11160 layer_factory.cpp:58] Creating layer fire4/relu_squeeze1x1
I1107 10:17:48.479635 11160 net.cpp:84] Creating Layer fire4/relu_squeeze1x1
I1107 10:17:48.479635 11160 net.cpp:406] fire4/relu_squeeze1x1 <- fire4/squeeze1x1
I1107 10:17:48.479635 11160 net.cpp:367] fire4/relu_squeeze1x1 -> fire4/squeeze1x1 (in-place)
I1107 10:17:48.479635 11160 net.cpp:122] Setting up fire4/relu_squeeze1x1
I1107 10:17:48.479635 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.479635 11160 net.cpp:137] Memory required for data: 585114800
I1107 10:17:48.479635 11160 layer_factory.cpp:58] Creating layer fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 10:17:48.479635 11160 net.cpp:84] Creating Layer fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 10:17:48.479635 11160 net.cpp:406] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split <- fire4/squeeze1x1
I1107 10:17:48.479635 11160 net.cpp:380] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split -> fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_0
I1107 10:17:48.479635 11160 net.cpp:380] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split -> fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_1
I1107 10:17:48.480134 11160 net.cpp:122] Setting up fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 10:17:48.480134 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.480134 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.480134 11160 net.cpp:137] Memory required for data: 590132400
I1107 10:17:48.480134 11160 layer_factory.cpp:58] Creating layer fire4/expand1x1
I1107 10:17:48.480134 11160 net.cpp:84] Creating Layer fire4/expand1x1
I1107 10:17:48.480134 11160 net.cpp:406] fire4/expand1x1 <- fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_0
I1107 10:17:48.480134 11160 net.cpp:380] fire4/expand1x1 -> fire4/expand1x1
I1107 10:17:48.481139 11160 net.cpp:122] Setting up fire4/expand1x1
I1107 10:17:48.481139 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.481139 11160 net.cpp:137] Memory required for data: 600167600
I1107 10:17:48.481139 11160 layer_factory.cpp:58] Creating layer fire4/relu_expand1x1
I1107 10:17:48.481139 11160 net.cpp:84] Creating Layer fire4/relu_expand1x1
I1107 10:17:48.481139 11160 net.cpp:406] fire4/relu_expand1x1 <- fire4/expand1x1
I1107 10:17:48.481139 11160 net.cpp:367] fire4/relu_expand1x1 -> fire4/expand1x1 (in-place)
I1107 10:17:48.482139 11160 net.cpp:122] Setting up fire4/relu_expand1x1
I1107 10:17:48.482139 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.482139 11160 net.cpp:137] Memory required for data: 610202800
I1107 10:17:48.482139 11160 layer_factory.cpp:58] Creating layer fire4/expand3x3
I1107 10:17:48.482139 11160 net.cpp:84] Creating Layer fire4/expand3x3
I1107 10:17:48.482139 11160 net.cpp:406] fire4/expand3x3 <- fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_1
I1107 10:17:48.482139 11160 net.cpp:380] fire4/expand3x3 -> conv_8
I1107 10:17:48.483633 11160 net.cpp:122] Setting up fire4/expand3x3
I1107 10:17:48.483633 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.483633 11160 net.cpp:137] Memory required for data: 620238000
I1107 10:17:48.483633 11160 layer_factory.cpp:58] Creating layer bn_8
I1107 10:17:48.483633 11160 net.cpp:84] Creating Layer bn_8
I1107 10:17:48.483633 11160 net.cpp:406] bn_8 <- conv_8
I1107 10:17:48.483633 11160 net.cpp:367] bn_8 -> conv_8 (in-place)
I1107 10:17:48.484133 11160 net.cpp:122] Setting up bn_8
I1107 10:17:48.484133 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.484133 11160 net.cpp:137] Memory required for data: 630273200
I1107 10:17:48.484133 11160 layer_factory.cpp:58] Creating layer scale_8
I1107 10:17:48.484133 11160 net.cpp:84] Creating Layer scale_8
I1107 10:17:48.484133 11160 net.cpp:406] scale_8 <- conv_8
I1107 10:17:48.484133 11160 net.cpp:380] scale_8 -> fire4/expand3x3
I1107 10:17:48.484133 11160 layer_factory.cpp:58] Creating layer scale_8
I1107 10:17:48.484133 11160 net.cpp:122] Setting up scale_8
I1107 10:17:48.484133 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.484133 11160 net.cpp:137] Memory required for data: 640308400
I1107 10:17:48.484133 11160 layer_factory.cpp:58] Creating layer fire4/relu_expand3x3
I1107 10:17:48.484133 11160 net.cpp:84] Creating Layer fire4/relu_expand3x3
I1107 10:17:48.484133 11160 net.cpp:406] fire4/relu_expand3x3 <- fire4/expand3x3
I1107 10:17:48.484133 11160 net.cpp:367] fire4/relu_expand3x3 -> fire4/expand3x3 (in-place)
I1107 10:17:48.484638 11160 net.cpp:122] Setting up fire4/relu_expand3x3
I1107 10:17:48.484638 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.484638 11160 net.cpp:137] Memory required for data: 650343600
I1107 10:17:48.484638 11160 layer_factory.cpp:58] Creating layer fire4/concat
I1107 10:17:48.484638 11160 net.cpp:84] Creating Layer fire4/concat
I1107 10:17:48.484638 11160 net.cpp:406] fire4/concat <- fire4/expand1x1
I1107 10:17:48.484638 11160 net.cpp:406] fire4/concat <- fire4/expand3x3
I1107 10:17:48.484638 11160 net.cpp:380] fire4/concat -> fire4/concat
I1107 10:17:48.484638 11160 net.cpp:122] Setting up fire4/concat
I1107 10:17:48.484638 11160 net.cpp:129] Top shape: 100 256 14 14 (5017600)
I1107 10:17:48.484638 11160 net.cpp:137] Memory required for data: 670414000
I1107 10:17:48.484638 11160 layer_factory.cpp:58] Creating layer fire5/squeeze1x1
I1107 10:17:48.484638 11160 net.cpp:84] Creating Layer fire5/squeeze1x1
I1107 10:17:48.484638 11160 net.cpp:406] fire5/squeeze1x1 <- fire4/concat
I1107 10:17:48.484638 11160 net.cpp:380] fire5/squeeze1x1 -> conv_9
I1107 10:17:48.486135 11160 net.cpp:122] Setting up fire5/squeeze1x1
I1107 10:17:48.486135 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.486135 11160 net.cpp:137] Memory required for data: 672922800
I1107 10:17:48.486135 11160 layer_factory.cpp:58] Creating layer bn_9
I1107 10:17:48.486135 11160 net.cpp:84] Creating Layer bn_9
I1107 10:17:48.486135 11160 net.cpp:406] bn_9 <- conv_9
I1107 10:17:48.486135 11160 net.cpp:367] bn_9 -> conv_9 (in-place)
I1107 10:17:48.486135 11160 net.cpp:122] Setting up bn_9
I1107 10:17:48.486135 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.486135 11160 net.cpp:137] Memory required for data: 675431600
I1107 10:17:48.486135 11160 layer_factory.cpp:58] Creating layer scale_9
I1107 10:17:48.486135 11160 net.cpp:84] Creating Layer scale_9
I1107 10:17:48.486135 11160 net.cpp:406] scale_9 <- conv_9
I1107 10:17:48.486135 11160 net.cpp:380] scale_9 -> fire5/squeeze1x1
I1107 10:17:48.486135 11160 layer_factory.cpp:58] Creating layer scale_9
I1107 10:17:48.486135 11160 net.cpp:122] Setting up scale_9
I1107 10:17:48.486135 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.486634 11160 net.cpp:137] Memory required for data: 677940400
I1107 10:17:48.486634 11160 layer_factory.cpp:58] Creating layer fire5/relu_squeeze1x1
I1107 10:17:48.486634 11160 net.cpp:84] Creating Layer fire5/relu_squeeze1x1
I1107 10:17:48.486634 11160 net.cpp:406] fire5/relu_squeeze1x1 <- fire5/squeeze1x1
I1107 10:17:48.486634 11160 net.cpp:367] fire5/relu_squeeze1x1 -> fire5/squeeze1x1 (in-place)
I1107 10:17:48.486634 11160 net.cpp:122] Setting up fire5/relu_squeeze1x1
I1107 10:17:48.486634 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.486634 11160 net.cpp:137] Memory required for data: 680449200
I1107 10:17:48.486634 11160 layer_factory.cpp:58] Creating layer fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 10:17:48.486634 11160 net.cpp:84] Creating Layer fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 10:17:48.486634 11160 net.cpp:406] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split <- fire5/squeeze1x1
I1107 10:17:48.486634 11160 net.cpp:380] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split -> fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_0
I1107 10:17:48.486634 11160 net.cpp:380] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split -> fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_1
I1107 10:17:48.486634 11160 net.cpp:122] Setting up fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 10:17:48.486634 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.486634 11160 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 10:17:48.486634 11160 net.cpp:137] Memory required for data: 685466800
I1107 10:17:48.486634 11160 layer_factory.cpp:58] Creating layer fire5/expand1x1
I1107 10:17:48.486634 11160 net.cpp:84] Creating Layer fire5/expand1x1
I1107 10:17:48.486634 11160 net.cpp:406] fire5/expand1x1 <- fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_0
I1107 10:17:48.486634 11160 net.cpp:380] fire5/expand1x1 -> conv_10
I1107 10:17:48.488124 11160 net.cpp:122] Setting up fire5/expand1x1
I1107 10:17:48.488124 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.488124 11160 net.cpp:137] Memory required for data: 695502000
I1107 10:17:48.488124 11160 layer_factory.cpp:58] Creating layer bn_10
I1107 10:17:48.488124 11160 net.cpp:84] Creating Layer bn_10
I1107 10:17:48.488124 11160 net.cpp:406] bn_10 <- conv_10
I1107 10:17:48.488124 11160 net.cpp:367] bn_10 -> conv_10 (in-place)
I1107 10:17:48.488633 11160 net.cpp:122] Setting up bn_10
I1107 10:17:48.488633 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.488633 11160 net.cpp:137] Memory required for data: 705537200
I1107 10:17:48.488633 11160 layer_factory.cpp:58] Creating layer scale_10
I1107 10:17:48.488633 11160 net.cpp:84] Creating Layer scale_10
I1107 10:17:48.488633 11160 net.cpp:406] scale_10 <- conv_10
I1107 10:17:48.488633 11160 net.cpp:380] scale_10 -> fire5/expand1x1
I1107 10:17:48.488633 11160 layer_factory.cpp:58] Creating layer scale_10
I1107 10:17:48.488633 11160 net.cpp:122] Setting up scale_10
I1107 10:17:48.488633 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.488633 11160 net.cpp:137] Memory required for data: 715572400
I1107 10:17:48.488633 11160 layer_factory.cpp:58] Creating layer fire5/relu_expand1x1
I1107 10:17:48.488633 11160 net.cpp:84] Creating Layer fire5/relu_expand1x1
I1107 10:17:48.488633 11160 net.cpp:406] fire5/relu_expand1x1 <- fire5/expand1x1
I1107 10:17:48.488633 11160 net.cpp:367] fire5/relu_expand1x1 -> fire5/expand1x1 (in-place)
I1107 10:17:48.489135 11160 net.cpp:122] Setting up fire5/relu_expand1x1
I1107 10:17:48.489135 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.489135 11160 net.cpp:137] Memory required for data: 725607600
I1107 10:17:48.489135 11160 layer_factory.cpp:58] Creating layer fire5/expand3x3
I1107 10:17:48.489135 11160 net.cpp:84] Creating Layer fire5/expand3x3
I1107 10:17:48.489135 11160 net.cpp:406] fire5/expand3x3 <- fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_1
I1107 10:17:48.489135 11160 net.cpp:380] fire5/expand3x3 -> conv_11
I1107 10:17:48.490134 11160 net.cpp:122] Setting up fire5/expand3x3
I1107 10:17:48.490635 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.490635 11160 net.cpp:137] Memory required for data: 735642800
I1107 10:17:48.490635 11160 layer_factory.cpp:58] Creating layer bn_11
I1107 10:17:48.490635 11160 net.cpp:84] Creating Layer bn_11
I1107 10:17:48.490635 11160 net.cpp:406] bn_11 <- conv_11
I1107 10:17:48.490635 11160 net.cpp:367] bn_11 -> conv_11 (in-place)
I1107 10:17:48.490635 11160 net.cpp:122] Setting up bn_11
I1107 10:17:48.490635 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.490635 11160 net.cpp:137] Memory required for data: 745678000
I1107 10:17:48.490635 11160 layer_factory.cpp:58] Creating layer scale_11
I1107 10:17:48.490635 11160 net.cpp:84] Creating Layer scale_11
I1107 10:17:48.490635 11160 net.cpp:406] scale_11 <- conv_11
I1107 10:17:48.490635 11160 net.cpp:380] scale_11 -> fire5/expand3x3
I1107 10:17:48.490635 11160 layer_factory.cpp:58] Creating layer scale_11
I1107 10:17:48.490635 11160 net.cpp:122] Setting up scale_11
I1107 10:17:48.490635 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.490635 11160 net.cpp:137] Memory required for data: 755713200
I1107 10:17:48.490635 11160 layer_factory.cpp:58] Creating layer fire5/relu_expand3x3
I1107 10:17:48.490635 11160 net.cpp:84] Creating Layer fire5/relu_expand3x3
I1107 10:17:48.490635 11160 net.cpp:406] fire5/relu_expand3x3 <- fire5/expand3x3
I1107 10:17:48.490635 11160 net.cpp:367] fire5/relu_expand3x3 -> fire5/expand3x3 (in-place)
I1107 10:17:48.490635 11160 net.cpp:122] Setting up fire5/relu_expand3x3
I1107 10:17:48.490635 11160 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 10:17:48.490635 11160 net.cpp:137] Memory required for data: 765748400
I1107 10:17:48.490635 11160 layer_factory.cpp:58] Creating layer fire5/concat
I1107 10:17:48.490635 11160 net.cpp:84] Creating Layer fire5/concat
I1107 10:17:48.490635 11160 net.cpp:406] fire5/concat <- fire5/expand1x1
I1107 10:17:48.490635 11160 net.cpp:406] fire5/concat <- fire5/expand3x3
I1107 10:17:48.490635 11160 net.cpp:380] fire5/concat -> fire5/concat
I1107 10:17:48.490635 11160 net.cpp:122] Setting up fire5/concat
I1107 10:17:48.490635 11160 net.cpp:129] Top shape: 100 256 14 14 (5017600)
I1107 10:17:48.490635 11160 net.cpp:137] Memory required for data: 785818800
I1107 10:17:48.490635 11160 layer_factory.cpp:58] Creating layer pool5
I1107 10:17:48.490635 11160 net.cpp:84] Creating Layer pool5
I1107 10:17:48.490635 11160 net.cpp:406] pool5 <- fire5/concat
I1107 10:17:48.490635 11160 net.cpp:380] pool5 -> pool5
I1107 10:17:48.490635 11160 net.cpp:122] Setting up pool5
I1107 10:17:48.490635 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.490635 11160 net.cpp:137] Memory required for data: 790836400
I1107 10:17:48.490635 11160 layer_factory.cpp:58] Creating layer fire6/squeeze1x1
I1107 10:17:48.490635 11160 net.cpp:84] Creating Layer fire6/squeeze1x1
I1107 10:17:48.490635 11160 net.cpp:406] fire6/squeeze1x1 <- pool5
I1107 10:17:48.490635 11160 net.cpp:380] fire6/squeeze1x1 -> conv_12
I1107 10:17:48.492647 11160 net.cpp:122] Setting up fire6/squeeze1x1
I1107 10:17:48.492647 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.492647 11160 net.cpp:137] Memory required for data: 791777200
I1107 10:17:48.492647 11160 layer_factory.cpp:58] Creating layer bn_12
I1107 10:17:48.492647 11160 net.cpp:84] Creating Layer bn_12
I1107 10:17:48.492647 11160 net.cpp:406] bn_12 <- conv_12
I1107 10:17:48.492647 11160 net.cpp:367] bn_12 -> conv_12 (in-place)
I1107 10:17:48.492647 11160 net.cpp:122] Setting up bn_12
I1107 10:17:48.492647 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.492647 11160 net.cpp:137] Memory required for data: 792718000
I1107 10:17:48.492647 11160 layer_factory.cpp:58] Creating layer scale_12
I1107 10:17:48.492647 11160 net.cpp:84] Creating Layer scale_12
I1107 10:17:48.492647 11160 net.cpp:406] scale_12 <- conv_12
I1107 10:17:48.492647 11160 net.cpp:380] scale_12 -> fire6/squeeze1x1
I1107 10:17:48.492647 11160 layer_factory.cpp:58] Creating layer scale_12
I1107 10:17:48.492647 11160 net.cpp:122] Setting up scale_12
I1107 10:17:48.492647 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.492647 11160 net.cpp:137] Memory required for data: 793658800
I1107 10:17:48.492647 11160 layer_factory.cpp:58] Creating layer fire6/relu_squeeze1x1
I1107 10:17:48.492647 11160 net.cpp:84] Creating Layer fire6/relu_squeeze1x1
I1107 10:17:48.492647 11160 net.cpp:406] fire6/relu_squeeze1x1 <- fire6/squeeze1x1
I1107 10:17:48.492647 11160 net.cpp:367] fire6/relu_squeeze1x1 -> fire6/squeeze1x1 (in-place)
I1107 10:17:48.492647 11160 net.cpp:122] Setting up fire6/relu_squeeze1x1
I1107 10:17:48.492647 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.492647 11160 net.cpp:137] Memory required for data: 794599600
I1107 10:17:48.492647 11160 layer_factory.cpp:58] Creating layer fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 10:17:48.492647 11160 net.cpp:84] Creating Layer fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 10:17:48.492647 11160 net.cpp:406] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split <- fire6/squeeze1x1
I1107 10:17:48.492647 11160 net.cpp:380] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split -> fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_0
I1107 10:17:48.492647 11160 net.cpp:380] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split -> fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_1
I1107 10:17:48.492647 11160 net.cpp:122] Setting up fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 10:17:48.492647 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.492647 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.492647 11160 net.cpp:137] Memory required for data: 796481200
I1107 10:17:48.493647 11160 layer_factory.cpp:58] Creating layer fire6/expand1x1
I1107 10:17:48.493647 11160 net.cpp:84] Creating Layer fire6/expand1x1
I1107 10:17:48.493647 11160 net.cpp:406] fire6/expand1x1 <- fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_0
I1107 10:17:48.493647 11160 net.cpp:380] fire6/expand1x1 -> conv_13
I1107 10:17:48.494647 11160 net.cpp:122] Setting up fire6/expand1x1
I1107 10:17:48.494647 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.494647 11160 net.cpp:137] Memory required for data: 800244400
I1107 10:17:48.494647 11160 layer_factory.cpp:58] Creating layer bn_13
I1107 10:17:48.494647 11160 net.cpp:84] Creating Layer bn_13
I1107 10:17:48.494647 11160 net.cpp:406] bn_13 <- conv_13
I1107 10:17:48.494647 11160 net.cpp:367] bn_13 -> conv_13 (in-place)
I1107 10:17:48.494647 11160 net.cpp:122] Setting up bn_13
I1107 10:17:48.494647 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.494647 11160 net.cpp:137] Memory required for data: 804007600
I1107 10:17:48.494647 11160 layer_factory.cpp:58] Creating layer scale_13
I1107 10:17:48.494647 11160 net.cpp:84] Creating Layer scale_13
I1107 10:17:48.494647 11160 net.cpp:406] scale_13 <- conv_13
I1107 10:17:48.494647 11160 net.cpp:380] scale_13 -> fire6/expand1x1
I1107 10:17:48.494647 11160 layer_factory.cpp:58] Creating layer scale_13
I1107 10:17:48.494647 11160 net.cpp:122] Setting up scale_13
I1107 10:17:48.494647 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.494647 11160 net.cpp:137] Memory required for data: 807770800
I1107 10:17:48.494647 11160 layer_factory.cpp:58] Creating layer fire6/relu_expand1x1
I1107 10:17:48.494647 11160 net.cpp:84] Creating Layer fire6/relu_expand1x1
I1107 10:17:48.494647 11160 net.cpp:406] fire6/relu_expand1x1 <- fire6/expand1x1
I1107 10:17:48.494647 11160 net.cpp:367] fire6/relu_expand1x1 -> fire6/expand1x1 (in-place)
I1107 10:17:48.495637 11160 net.cpp:122] Setting up fire6/relu_expand1x1
I1107 10:17:48.495637 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.495637 11160 net.cpp:137] Memory required for data: 811534000
I1107 10:17:48.495637 11160 layer_factory.cpp:58] Creating layer fire6/expand3x3
I1107 10:17:48.495637 11160 net.cpp:84] Creating Layer fire6/expand3x3
I1107 10:17:48.495637 11160 net.cpp:406] fire6/expand3x3 <- fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_1
I1107 10:17:48.495637 11160 net.cpp:380] fire6/expand3x3 -> conv_14
I1107 10:17:48.497637 11160 net.cpp:122] Setting up fire6/expand3x3
I1107 10:17:48.497637 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.497637 11160 net.cpp:137] Memory required for data: 815297200
I1107 10:17:48.497637 11160 layer_factory.cpp:58] Creating layer bn_14
I1107 10:17:48.497637 11160 net.cpp:84] Creating Layer bn_14
I1107 10:17:48.497637 11160 net.cpp:406] bn_14 <- conv_14
I1107 10:17:48.497637 11160 net.cpp:367] bn_14 -> conv_14 (in-place)
I1107 10:17:48.497637 11160 net.cpp:122] Setting up bn_14
I1107 10:17:48.497637 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.497637 11160 net.cpp:137] Memory required for data: 819060400
I1107 10:17:48.497637 11160 layer_factory.cpp:58] Creating layer scale_14
I1107 10:17:48.497637 11160 net.cpp:84] Creating Layer scale_14
I1107 10:17:48.497637 11160 net.cpp:406] scale_14 <- conv_14
I1107 10:17:48.497637 11160 net.cpp:380] scale_14 -> fire6/expand3x3
I1107 10:17:48.497637 11160 layer_factory.cpp:58] Creating layer scale_14
I1107 10:17:48.497637 11160 net.cpp:122] Setting up scale_14
I1107 10:17:48.497637 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.497637 11160 net.cpp:137] Memory required for data: 822823600
I1107 10:17:48.497637 11160 layer_factory.cpp:58] Creating layer fire6/relu_expand3x3
I1107 10:17:48.497637 11160 net.cpp:84] Creating Layer fire6/relu_expand3x3
I1107 10:17:48.497637 11160 net.cpp:406] fire6/relu_expand3x3 <- fire6/expand3x3
I1107 10:17:48.497637 11160 net.cpp:367] fire6/relu_expand3x3 -> fire6/expand3x3 (in-place)
I1107 10:17:48.498636 11160 net.cpp:122] Setting up fire6/relu_expand3x3
I1107 10:17:48.498636 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.498636 11160 net.cpp:137] Memory required for data: 826586800
I1107 10:17:48.498636 11160 layer_factory.cpp:58] Creating layer fire6/concat
I1107 10:17:48.498636 11160 net.cpp:84] Creating Layer fire6/concat
I1107 10:17:48.498636 11160 net.cpp:406] fire6/concat <- fire6/expand1x1
I1107 10:17:48.498636 11160 net.cpp:406] fire6/concat <- fire6/expand3x3
I1107 10:17:48.498636 11160 net.cpp:380] fire6/concat -> fire6/concat
I1107 10:17:48.498636 11160 net.cpp:122] Setting up fire6/concat
I1107 10:17:48.498636 11160 net.cpp:129] Top shape: 100 384 7 7 (1881600)
I1107 10:17:48.498636 11160 net.cpp:137] Memory required for data: 834113200
I1107 10:17:48.498636 11160 layer_factory.cpp:58] Creating layer fire7/squeeze1x1
I1107 10:17:48.498636 11160 net.cpp:84] Creating Layer fire7/squeeze1x1
I1107 10:17:48.498636 11160 net.cpp:406] fire7/squeeze1x1 <- fire6/concat
I1107 10:17:48.498636 11160 net.cpp:380] fire7/squeeze1x1 -> conv_15
I1107 10:17:48.499640 11160 net.cpp:122] Setting up fire7/squeeze1x1
I1107 10:17:48.499640 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.499640 11160 net.cpp:137] Memory required for data: 835054000
I1107 10:17:48.499640 11160 layer_factory.cpp:58] Creating layer bn_15
I1107 10:17:48.499640 11160 net.cpp:84] Creating Layer bn_15
I1107 10:17:48.499640 11160 net.cpp:406] bn_15 <- conv_15
I1107 10:17:48.499640 11160 net.cpp:367] bn_15 -> conv_15 (in-place)
I1107 10:17:48.500651 11160 net.cpp:122] Setting up bn_15
I1107 10:17:48.500651 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.500651 11160 net.cpp:137] Memory required for data: 835994800
I1107 10:17:48.500651 11160 layer_factory.cpp:58] Creating layer scale_15
I1107 10:17:48.500651 11160 net.cpp:84] Creating Layer scale_15
I1107 10:17:48.500651 11160 net.cpp:406] scale_15 <- conv_15
I1107 10:17:48.500651 11160 net.cpp:380] scale_15 -> fire7/squeeze1x1
I1107 10:17:48.500651 11160 layer_factory.cpp:58] Creating layer scale_15
I1107 10:17:48.500651 11160 net.cpp:122] Setting up scale_15
I1107 10:17:48.500651 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.500651 11160 net.cpp:137] Memory required for data: 836935600
I1107 10:17:48.500651 11160 layer_factory.cpp:58] Creating layer fire7/relu_squeeze1x1
I1107 10:17:48.500651 11160 net.cpp:84] Creating Layer fire7/relu_squeeze1x1
I1107 10:17:48.500651 11160 net.cpp:406] fire7/relu_squeeze1x1 <- fire7/squeeze1x1
I1107 10:17:48.500651 11160 net.cpp:367] fire7/relu_squeeze1x1 -> fire7/squeeze1x1 (in-place)
I1107 10:17:48.500651 11160 net.cpp:122] Setting up fire7/relu_squeeze1x1
I1107 10:17:48.500651 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.500651 11160 net.cpp:137] Memory required for data: 837876400
I1107 10:17:48.500651 11160 layer_factory.cpp:58] Creating layer fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 10:17:48.500651 11160 net.cpp:84] Creating Layer fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 10:17:48.500651 11160 net.cpp:406] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split <- fire7/squeeze1x1
I1107 10:17:48.500651 11160 net.cpp:380] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split -> fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_0
I1107 10:17:48.500651 11160 net.cpp:380] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split -> fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_1
I1107 10:17:48.500651 11160 net.cpp:122] Setting up fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 10:17:48.500651 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.500651 11160 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 10:17:48.500651 11160 net.cpp:137] Memory required for data: 839758000
I1107 10:17:48.500651 11160 layer_factory.cpp:58] Creating layer fire7/expand1x1
I1107 10:17:48.500651 11160 net.cpp:84] Creating Layer fire7/expand1x1
I1107 10:17:48.500651 11160 net.cpp:406] fire7/expand1x1 <- fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_0
I1107 10:17:48.500651 11160 net.cpp:380] fire7/expand1x1 -> conv_16
I1107 10:17:48.501652 11160 net.cpp:122] Setting up fire7/expand1x1
I1107 10:17:48.501652 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.501652 11160 net.cpp:137] Memory required for data: 843521200
I1107 10:17:48.501652 11160 layer_factory.cpp:58] Creating layer bn_16
I1107 10:17:48.501652 11160 net.cpp:84] Creating Layer bn_16
I1107 10:17:48.502651 11160 net.cpp:406] bn_16 <- conv_16
I1107 10:17:48.502651 11160 net.cpp:367] bn_16 -> conv_16 (in-place)
I1107 10:17:48.502651 11160 net.cpp:122] Setting up bn_16
I1107 10:17:48.502651 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.502651 11160 net.cpp:137] Memory required for data: 847284400
I1107 10:17:48.502651 11160 layer_factory.cpp:58] Creating layer scale_16
I1107 10:17:48.502651 11160 net.cpp:84] Creating Layer scale_16
I1107 10:17:48.502651 11160 net.cpp:406] scale_16 <- conv_16
I1107 10:17:48.502651 11160 net.cpp:380] scale_16 -> fire7/expand1x1
I1107 10:17:48.502651 11160 layer_factory.cpp:58] Creating layer scale_16
I1107 10:17:48.502651 11160 net.cpp:122] Setting up scale_16
I1107 10:17:48.502651 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.502651 11160 net.cpp:137] Memory required for data: 851047600
I1107 10:17:48.502651 11160 layer_factory.cpp:58] Creating layer fire7/relu_expand1x1
I1107 10:17:48.502651 11160 net.cpp:84] Creating Layer fire7/relu_expand1x1
I1107 10:17:48.502651 11160 net.cpp:406] fire7/relu_expand1x1 <- fire7/expand1x1
I1107 10:17:48.502651 11160 net.cpp:367] fire7/relu_expand1x1 -> fire7/expand1x1 (in-place)
I1107 10:17:48.502651 11160 net.cpp:122] Setting up fire7/relu_expand1x1
I1107 10:17:48.502651 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.502651 11160 net.cpp:137] Memory required for data: 854810800
I1107 10:17:48.502651 11160 layer_factory.cpp:58] Creating layer fire7/expand3x3
I1107 10:17:48.502651 11160 net.cpp:84] Creating Layer fire7/expand3x3
I1107 10:17:48.502651 11160 net.cpp:406] fire7/expand3x3 <- fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_1
I1107 10:17:48.502651 11160 net.cpp:380] fire7/expand3x3 -> conv_17
I1107 10:17:48.504665 11160 net.cpp:122] Setting up fire7/expand3x3
I1107 10:17:48.504665 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.504665 11160 net.cpp:137] Memory required for data: 858574000
I1107 10:17:48.504665 11160 layer_factory.cpp:58] Creating layer bn_17
I1107 10:17:48.504665 11160 net.cpp:84] Creating Layer bn_17
I1107 10:17:48.505636 11160 net.cpp:406] bn_17 <- conv_17
I1107 10:17:48.505636 11160 net.cpp:367] bn_17 -> conv_17 (in-place)
I1107 10:17:48.505636 11160 net.cpp:122] Setting up bn_17
I1107 10:17:48.505636 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.505636 11160 net.cpp:137] Memory required for data: 862337200
I1107 10:17:48.505636 11160 layer_factory.cpp:58] Creating layer scale_17
I1107 10:17:48.505636 11160 net.cpp:84] Creating Layer scale_17
I1107 10:17:48.505636 11160 net.cpp:406] scale_17 <- conv_17
I1107 10:17:48.505636 11160 net.cpp:380] scale_17 -> fire7/expand3x3
I1107 10:17:48.505636 11160 layer_factory.cpp:58] Creating layer scale_17
I1107 10:17:48.505636 11160 net.cpp:122] Setting up scale_17
I1107 10:17:48.505636 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.505636 11160 net.cpp:137] Memory required for data: 866100400
I1107 10:17:48.505636 11160 layer_factory.cpp:58] Creating layer fire7/relu_expand3x3
I1107 10:17:48.505636 11160 net.cpp:84] Creating Layer fire7/relu_expand3x3
I1107 10:17:48.505636 11160 net.cpp:406] fire7/relu_expand3x3 <- fire7/expand3x3
I1107 10:17:48.505636 11160 net.cpp:367] fire7/relu_expand3x3 -> fire7/expand3x3 (in-place)
I1107 10:17:48.505636 11160 net.cpp:122] Setting up fire7/relu_expand3x3
I1107 10:17:48.505636 11160 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 10:17:48.505636 11160 net.cpp:137] Memory required for data: 869863600
I1107 10:17:48.505636 11160 layer_factory.cpp:58] Creating layer fire7/concat
I1107 10:17:48.505636 11160 net.cpp:84] Creating Layer fire7/concat
I1107 10:17:48.505636 11160 net.cpp:406] fire7/concat <- fire7/expand1x1
I1107 10:17:48.505636 11160 net.cpp:406] fire7/concat <- fire7/expand3x3
I1107 10:17:48.505636 11160 net.cpp:380] fire7/concat -> fire7/concat
I1107 10:17:48.505636 11160 net.cpp:122] Setting up fire7/concat
I1107 10:17:48.505636 11160 net.cpp:129] Top shape: 100 384 7 7 (1881600)
I1107 10:17:48.505636 11160 net.cpp:137] Memory required for data: 877390000
I1107 10:17:48.505636 11160 layer_factory.cpp:58] Creating layer fire8/squeeze1x1
I1107 10:17:48.505636 11160 net.cpp:84] Creating Layer fire8/squeeze1x1
I1107 10:17:48.505636 11160 net.cpp:406] fire8/squeeze1x1 <- fire7/concat
I1107 10:17:48.505636 11160 net.cpp:380] fire8/squeeze1x1 -> conv_18
I1107 10:17:48.507652 11160 net.cpp:122] Setting up fire8/squeeze1x1
I1107 10:17:48.507652 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.507652 11160 net.cpp:137] Memory required for data: 878644400
I1107 10:17:48.507652 11160 layer_factory.cpp:58] Creating layer bn_18
I1107 10:17:48.507652 11160 net.cpp:84] Creating Layer bn_18
I1107 10:17:48.507652 11160 net.cpp:406] bn_18 <- conv_18
I1107 10:17:48.507652 11160 net.cpp:367] bn_18 -> conv_18 (in-place)
I1107 10:17:48.507652 11160 net.cpp:122] Setting up bn_18
I1107 10:17:48.507652 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.507652 11160 net.cpp:137] Memory required for data: 879898800
I1107 10:17:48.507652 11160 layer_factory.cpp:58] Creating layer scale_18
I1107 10:17:48.507652 11160 net.cpp:84] Creating Layer scale_18
I1107 10:17:48.507652 11160 net.cpp:406] scale_18 <- conv_18
I1107 10:17:48.507652 11160 net.cpp:380] scale_18 -> fire8/squeeze1x1
I1107 10:17:48.507652 11160 layer_factory.cpp:58] Creating layer scale_18
I1107 10:17:48.507652 11160 net.cpp:122] Setting up scale_18
I1107 10:17:48.507652 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.507652 11160 net.cpp:137] Memory required for data: 881153200
I1107 10:17:48.507652 11160 layer_factory.cpp:58] Creating layer fire8/relu_squeeze1x1
I1107 10:17:48.507652 11160 net.cpp:84] Creating Layer fire8/relu_squeeze1x1
I1107 10:17:48.507652 11160 net.cpp:406] fire8/relu_squeeze1x1 <- fire8/squeeze1x1
I1107 10:17:48.507652 11160 net.cpp:367] fire8/relu_squeeze1x1 -> fire8/squeeze1x1 (in-place)
I1107 10:17:48.507652 11160 net.cpp:122] Setting up fire8/relu_squeeze1x1
I1107 10:17:48.507652 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.507652 11160 net.cpp:137] Memory required for data: 882407600
I1107 10:17:48.507652 11160 layer_factory.cpp:58] Creating layer fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 10:17:48.507652 11160 net.cpp:84] Creating Layer fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 10:17:48.507652 11160 net.cpp:406] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split <- fire8/squeeze1x1
I1107 10:17:48.507652 11160 net.cpp:380] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split -> fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_0
I1107 10:17:48.507652 11160 net.cpp:380] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split -> fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_1
I1107 10:17:48.507652 11160 net.cpp:122] Setting up fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 10:17:48.507652 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.507652 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.508651 11160 net.cpp:137] Memory required for data: 884916400
I1107 10:17:48.508651 11160 layer_factory.cpp:58] Creating layer fire8/expand1x1
I1107 10:17:48.508651 11160 net.cpp:84] Creating Layer fire8/expand1x1
I1107 10:17:48.508651 11160 net.cpp:406] fire8/expand1x1 <- fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_0
I1107 10:17:48.508651 11160 net.cpp:380] fire8/expand1x1 -> conv_19
I1107 10:17:48.509662 11160 net.cpp:122] Setting up fire8/expand1x1
I1107 10:17:48.509662 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.509662 11160 net.cpp:137] Memory required for data: 889934000
I1107 10:17:48.509662 11160 layer_factory.cpp:58] Creating layer bn_19
I1107 10:17:48.509662 11160 net.cpp:84] Creating Layer bn_19
I1107 10:17:48.509662 11160 net.cpp:406] bn_19 <- conv_19
I1107 10:17:48.509662 11160 net.cpp:367] bn_19 -> conv_19 (in-place)
I1107 10:17:48.509662 11160 net.cpp:122] Setting up bn_19
I1107 10:17:48.509662 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.509662 11160 net.cpp:137] Memory required for data: 894951600
I1107 10:17:48.509662 11160 layer_factory.cpp:58] Creating layer scale_19
I1107 10:17:48.509662 11160 net.cpp:84] Creating Layer scale_19
I1107 10:17:48.509662 11160 net.cpp:406] scale_19 <- conv_19
I1107 10:17:48.509662 11160 net.cpp:380] scale_19 -> fire8/expand1x1
I1107 10:17:48.509662 11160 layer_factory.cpp:58] Creating layer scale_19
I1107 10:17:48.509662 11160 net.cpp:122] Setting up scale_19
I1107 10:17:48.509662 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.509662 11160 net.cpp:137] Memory required for data: 899969200
I1107 10:17:48.509662 11160 layer_factory.cpp:58] Creating layer fire8/relu_expand1x1
I1107 10:17:48.509662 11160 net.cpp:84] Creating Layer fire8/relu_expand1x1
I1107 10:17:48.509662 11160 net.cpp:406] fire8/relu_expand1x1 <- fire8/expand1x1
I1107 10:17:48.509662 11160 net.cpp:367] fire8/relu_expand1x1 -> fire8/expand1x1 (in-place)
I1107 10:17:48.510651 11160 net.cpp:122] Setting up fire8/relu_expand1x1
I1107 10:17:48.510651 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.510651 11160 net.cpp:137] Memory required for data: 904986800
I1107 10:17:48.510651 11160 layer_factory.cpp:58] Creating layer fire8/expand3x3
I1107 10:17:48.510651 11160 net.cpp:84] Creating Layer fire8/expand3x3
I1107 10:17:48.510651 11160 net.cpp:406] fire8/expand3x3 <- fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_1
I1107 10:17:48.510651 11160 net.cpp:380] fire8/expand3x3 -> conv_20
I1107 10:17:48.512655 11160 net.cpp:122] Setting up fire8/expand3x3
I1107 10:17:48.512655 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.512655 11160 net.cpp:137] Memory required for data: 910004400
I1107 10:17:48.512655 11160 layer_factory.cpp:58] Creating layer bn_20
I1107 10:17:48.512655 11160 net.cpp:84] Creating Layer bn_20
I1107 10:17:48.512655 11160 net.cpp:406] bn_20 <- conv_20
I1107 10:17:48.512655 11160 net.cpp:367] bn_20 -> conv_20 (in-place)
I1107 10:17:48.512655 11160 net.cpp:122] Setting up bn_20
I1107 10:17:48.512655 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.512655 11160 net.cpp:137] Memory required for data: 915022000
I1107 10:17:48.512655 11160 layer_factory.cpp:58] Creating layer scale_20
I1107 10:17:48.512655 11160 net.cpp:84] Creating Layer scale_20
I1107 10:17:48.512655 11160 net.cpp:406] scale_20 <- conv_20
I1107 10:17:48.512655 11160 net.cpp:380] scale_20 -> fire8/expand3x3
I1107 10:17:48.512655 11160 layer_factory.cpp:58] Creating layer scale_20
I1107 10:17:48.513639 11160 net.cpp:122] Setting up scale_20
I1107 10:17:48.513639 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.513639 11160 net.cpp:137] Memory required for data: 920039600
I1107 10:17:48.513639 11160 layer_factory.cpp:58] Creating layer fire8/relu_expand3x3
I1107 10:17:48.513639 11160 net.cpp:84] Creating Layer fire8/relu_expand3x3
I1107 10:17:48.513639 11160 net.cpp:406] fire8/relu_expand3x3 <- fire8/expand3x3
I1107 10:17:48.513639 11160 net.cpp:367] fire8/relu_expand3x3 -> fire8/expand3x3 (in-place)
I1107 10:17:48.513639 11160 net.cpp:122] Setting up fire8/relu_expand3x3
I1107 10:17:48.513639 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.513639 11160 net.cpp:137] Memory required for data: 925057200
I1107 10:17:48.513639 11160 layer_factory.cpp:58] Creating layer fire8/concat
I1107 10:17:48.513639 11160 net.cpp:84] Creating Layer fire8/concat
I1107 10:17:48.513639 11160 net.cpp:406] fire8/concat <- fire8/expand1x1
I1107 10:17:48.513639 11160 net.cpp:406] fire8/concat <- fire8/expand3x3
I1107 10:17:48.513639 11160 net.cpp:380] fire8/concat -> fire8/concat
I1107 10:17:48.513639 11160 net.cpp:122] Setting up fire8/concat
I1107 10:17:48.513639 11160 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 10:17:48.513639 11160 net.cpp:137] Memory required for data: 935092400
I1107 10:17:48.513639 11160 layer_factory.cpp:58] Creating layer fire9/squeeze1x1
I1107 10:17:48.513639 11160 net.cpp:84] Creating Layer fire9/squeeze1x1
I1107 10:17:48.513639 11160 net.cpp:406] fire9/squeeze1x1 <- fire8/concat
I1107 10:17:48.513639 11160 net.cpp:380] fire9/squeeze1x1 -> conv_21
I1107 10:17:48.515638 11160 net.cpp:122] Setting up fire9/squeeze1x1
I1107 10:17:48.515638 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.515638 11160 net.cpp:137] Memory required for data: 936346800
I1107 10:17:48.515638 11160 layer_factory.cpp:58] Creating layer bn_21
I1107 10:17:48.515638 11160 net.cpp:84] Creating Layer bn_21
I1107 10:17:48.515638 11160 net.cpp:406] bn_21 <- conv_21
I1107 10:17:48.515638 11160 net.cpp:367] bn_21 -> conv_21 (in-place)
I1107 10:17:48.515638 11160 net.cpp:122] Setting up bn_21
I1107 10:17:48.515638 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.515638 11160 net.cpp:137] Memory required for data: 937601200
I1107 10:17:48.515638 11160 layer_factory.cpp:58] Creating layer scale_21
I1107 10:17:48.515638 11160 net.cpp:84] Creating Layer scale_21
I1107 10:17:48.515638 11160 net.cpp:406] scale_21 <- conv_21
I1107 10:17:48.515638 11160 net.cpp:380] scale_21 -> fire9/squeeze1x1
I1107 10:17:48.515638 11160 layer_factory.cpp:58] Creating layer scale_21
I1107 10:17:48.515638 11160 net.cpp:122] Setting up scale_21
I1107 10:17:48.515638 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.515638 11160 net.cpp:137] Memory required for data: 938855600
I1107 10:17:48.515638 11160 layer_factory.cpp:58] Creating layer fire9/relu_squeeze1x1
I1107 10:17:48.515638 11160 net.cpp:84] Creating Layer fire9/relu_squeeze1x1
I1107 10:17:48.515638 11160 net.cpp:406] fire9/relu_squeeze1x1 <- fire9/squeeze1x1
I1107 10:17:48.515638 11160 net.cpp:367] fire9/relu_squeeze1x1 -> fire9/squeeze1x1 (in-place)
I1107 10:17:48.515638 11160 net.cpp:122] Setting up fire9/relu_squeeze1x1
I1107 10:17:48.515638 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.515638 11160 net.cpp:137] Memory required for data: 940110000
I1107 10:17:48.515638 11160 layer_factory.cpp:58] Creating layer fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 10:17:48.515638 11160 net.cpp:84] Creating Layer fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 10:17:48.515638 11160 net.cpp:406] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split <- fire9/squeeze1x1
I1107 10:17:48.515638 11160 net.cpp:380] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split -> fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_0
I1107 10:17:48.515638 11160 net.cpp:380] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split -> fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_1
I1107 10:17:48.516655 11160 net.cpp:122] Setting up fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 10:17:48.516655 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.516655 11160 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 10:17:48.516655 11160 net.cpp:137] Memory required for data: 942618800
I1107 10:17:48.516655 11160 layer_factory.cpp:58] Creating layer fire9/expand1x1
I1107 10:17:48.516655 11160 net.cpp:84] Creating Layer fire9/expand1x1
I1107 10:17:48.516655 11160 net.cpp:406] fire9/expand1x1 <- fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_0
I1107 10:17:48.516655 11160 net.cpp:380] fire9/expand1x1 -> conv_22
I1107 10:17:48.517638 11160 net.cpp:122] Setting up fire9/expand1x1
I1107 10:17:48.517638 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.517638 11160 net.cpp:137] Memory required for data: 947636400
I1107 10:17:48.517638 11160 layer_factory.cpp:58] Creating layer bn_22
I1107 10:17:48.517638 11160 net.cpp:84] Creating Layer bn_22
I1107 10:17:48.517638 11160 net.cpp:406] bn_22 <- conv_22
I1107 10:17:48.517638 11160 net.cpp:367] bn_22 -> conv_22 (in-place)
I1107 10:17:48.517638 11160 net.cpp:122] Setting up bn_22
I1107 10:17:48.517638 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.517638 11160 net.cpp:137] Memory required for data: 952654000
I1107 10:17:48.517638 11160 layer_factory.cpp:58] Creating layer scale_22
I1107 10:17:48.517638 11160 net.cpp:84] Creating Layer scale_22
I1107 10:17:48.517638 11160 net.cpp:406] scale_22 <- conv_22
I1107 10:17:48.517638 11160 net.cpp:380] scale_22 -> fire9/expand1x1
I1107 10:17:48.517638 11160 layer_factory.cpp:58] Creating layer scale_22
I1107 10:17:48.518651 11160 net.cpp:122] Setting up scale_22
I1107 10:17:48.518651 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.518651 11160 net.cpp:137] Memory required for data: 957671600
I1107 10:17:48.518651 11160 layer_factory.cpp:58] Creating layer fire9/relu_expand1x1
I1107 10:17:48.518651 11160 net.cpp:84] Creating Layer fire9/relu_expand1x1
I1107 10:17:48.518651 11160 net.cpp:406] fire9/relu_expand1x1 <- fire9/expand1x1
I1107 10:17:48.518651 11160 net.cpp:367] fire9/relu_expand1x1 -> fire9/expand1x1 (in-place)
I1107 10:17:48.518651 11160 net.cpp:122] Setting up fire9/relu_expand1x1
I1107 10:17:48.518651 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.518651 11160 net.cpp:137] Memory required for data: 962689200
I1107 10:17:48.518651 11160 layer_factory.cpp:58] Creating layer fire9/expand3x3
I1107 10:17:48.518651 11160 net.cpp:84] Creating Layer fire9/expand3x3
I1107 10:17:48.518651 11160 net.cpp:406] fire9/expand3x3 <- fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_1
I1107 10:17:48.518651 11160 net.cpp:380] fire9/expand3x3 -> conv_23
I1107 10:17:48.520651 11160 net.cpp:122] Setting up fire9/expand3x3
I1107 10:17:48.520651 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.520651 11160 net.cpp:137] Memory required for data: 967706800
I1107 10:17:48.520651 11160 layer_factory.cpp:58] Creating layer bn_23
I1107 10:17:48.520651 11160 net.cpp:84] Creating Layer bn_23
I1107 10:17:48.520651 11160 net.cpp:406] bn_23 <- conv_23
I1107 10:17:48.520651 11160 net.cpp:367] bn_23 -> conv_23 (in-place)
I1107 10:17:48.520651 11160 net.cpp:122] Setting up bn_23
I1107 10:17:48.520651 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.520651 11160 net.cpp:137] Memory required for data: 972724400
I1107 10:17:48.520651 11160 layer_factory.cpp:58] Creating layer scale_23
I1107 10:17:48.520651 11160 net.cpp:84] Creating Layer scale_23
I1107 10:17:48.520651 11160 net.cpp:406] scale_23 <- conv_23
I1107 10:17:48.520651 11160 net.cpp:380] scale_23 -> fire9/expand3x3
I1107 10:17:48.520651 11160 layer_factory.cpp:58] Creating layer scale_23
I1107 10:17:48.521651 11160 net.cpp:122] Setting up scale_23
I1107 10:17:48.521651 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.521651 11160 net.cpp:137] Memory required for data: 977742000
I1107 10:17:48.521651 11160 layer_factory.cpp:58] Creating layer fire9/relu_expand3x3
I1107 10:17:48.521651 11160 net.cpp:84] Creating Layer fire9/relu_expand3x3
I1107 10:17:48.521651 11160 net.cpp:406] fire9/relu_expand3x3 <- fire9/expand3x3
I1107 10:17:48.521651 11160 net.cpp:367] fire9/relu_expand3x3 -> fire9/expand3x3 (in-place)
I1107 10:17:48.521651 11160 net.cpp:122] Setting up fire9/relu_expand3x3
I1107 10:17:48.521651 11160 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 10:17:48.521651 11160 net.cpp:137] Memory required for data: 982759600
I1107 10:17:48.521651 11160 layer_factory.cpp:58] Creating layer fire9/concat
I1107 10:17:48.521651 11160 net.cpp:84] Creating Layer fire9/concat
I1107 10:17:48.521651 11160 net.cpp:406] fire9/concat <- fire9/expand1x1
I1107 10:17:48.521651 11160 net.cpp:406] fire9/concat <- fire9/expand3x3
I1107 10:17:48.521651 11160 net.cpp:380] fire9/concat -> fire9/concat
I1107 10:17:48.521651 11160 net.cpp:122] Setting up fire9/concat
I1107 10:17:48.521651 11160 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 10:17:48.521651 11160 net.cpp:137] Memory required for data: 992794800
I1107 10:17:48.521651 11160 layer_factory.cpp:58] Creating layer drop9
I1107 10:17:48.521651 11160 net.cpp:84] Creating Layer drop9
I1107 10:17:48.521651 11160 net.cpp:406] drop9 <- fire9/concat
I1107 10:17:48.521651 11160 net.cpp:367] drop9 -> fire9/concat (in-place)
I1107 10:17:48.521651 11160 net.cpp:122] Setting up drop9
I1107 10:17:48.521651 11160 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 10:17:48.521651 11160 net.cpp:137] Memory required for data: 1002830000
I1107 10:17:48.521651 11160 layer_factory.cpp:58] Creating layer conv10
I1107 10:17:48.521651 11160 net.cpp:84] Creating Layer conv10
I1107 10:17:48.521651 11160 net.cpp:406] conv10 <- fire9/concat
I1107 10:17:48.521651 11160 net.cpp:380] conv10 -> conv10
I1107 10:17:48.522636 11160 net.cpp:122] Setting up conv10
I1107 10:17:48.522636 11160 net.cpp:129] Top shape: 100 10 7 7 (49000)
I1107 10:17:48.522636 11160 net.cpp:137] Memory required for data: 1003026000
I1107 10:17:48.522636 11160 layer_factory.cpp:58] Creating layer relu_conv10
I1107 10:17:48.522636 11160 net.cpp:84] Creating Layer relu_conv10
I1107 10:17:48.522636 11160 net.cpp:406] relu_conv10 <- conv10
I1107 10:17:48.522636 11160 net.cpp:367] relu_conv10 -> conv10 (in-place)
I1107 10:17:48.523651 11160 net.cpp:122] Setting up relu_conv10
I1107 10:17:48.523651 11160 net.cpp:129] Top shape: 100 10 7 7 (49000)
I1107 10:17:48.523651 11160 net.cpp:137] Memory required for data: 1003222000
I1107 10:17:48.523651 11160 layer_factory.cpp:58] Creating layer pool10
I1107 10:17:48.523651 11160 net.cpp:84] Creating Layer pool10
I1107 10:17:48.523651 11160 net.cpp:406] pool10 <- conv10
I1107 10:17:48.523651 11160 net.cpp:380] pool10 -> pool10
I1107 10:17:48.523651 11160 net.cpp:122] Setting up pool10
I1107 10:17:48.523651 11160 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 10:17:48.523651 11160 net.cpp:137] Memory required for data: 1003226000
I1107 10:17:48.523651 11160 layer_factory.cpp:58] Creating layer pool10_pool10_0_split
I1107 10:17:48.523651 11160 net.cpp:84] Creating Layer pool10_pool10_0_split
I1107 10:17:48.523651 11160 net.cpp:406] pool10_pool10_0_split <- pool10
I1107 10:17:48.523651 11160 net.cpp:380] pool10_pool10_0_split -> pool10_pool10_0_split_0
I1107 10:17:48.523651 11160 net.cpp:380] pool10_pool10_0_split -> pool10_pool10_0_split_1
I1107 10:17:48.523651 11160 net.cpp:122] Setting up pool10_pool10_0_split
I1107 10:17:48.523651 11160 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 10:17:48.523651 11160 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 10:17:48.523651 11160 net.cpp:137] Memory required for data: 1003234000
I1107 10:17:48.523651 11160 layer_factory.cpp:58] Creating layer accuracy
I1107 10:17:48.523651 11160 net.cpp:84] Creating Layer accuracy
I1107 10:17:48.523651 11160 net.cpp:406] accuracy <- pool10_pool10_0_split_0
I1107 10:17:48.523651 11160 net.cpp:406] accuracy <- label_cifar_1_split_0
I1107 10:17:48.523651 11160 net.cpp:380] accuracy -> accuracy
I1107 10:17:48.523651 11160 net.cpp:122] Setting up accuracy
I1107 10:17:48.523651 11160 net.cpp:129] Top shape: (1)
I1107 10:17:48.523651 11160 net.cpp:137] Memory required for data: 1003234004
I1107 10:17:48.523651 11160 layer_factory.cpp:58] Creating layer loss
I1107 10:17:48.523651 11160 net.cpp:84] Creating Layer loss
I1107 10:17:48.523651 11160 net.cpp:406] loss <- pool10_pool10_0_split_1
I1107 10:17:48.523651 11160 net.cpp:406] loss <- label_cifar_1_split_1
I1107 10:17:48.523651 11160 net.cpp:380] loss -> loss
I1107 10:17:48.523651 11160 layer_factory.cpp:58] Creating layer loss
I1107 10:17:48.524652 11160 net.cpp:122] Setting up loss
I1107 10:17:48.524652 11160 net.cpp:129] Top shape: (1)
I1107 10:17:48.524652 11160 net.cpp:132]     with loss weight 1
I1107 10:17:48.524652 11160 net.cpp:137] Memory required for data: 1003234008
I1107 10:17:48.524652 11160 net.cpp:198] loss needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:200] accuracy does not need backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] pool10_pool10_0_split needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] pool10 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] relu_conv10 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] conv10 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] drop9 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire9/concat needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire9/relu_expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_23 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_23 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire9/expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire9/relu_expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_22 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_22 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire9/expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire9/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_21 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_21 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire9/squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire8/concat needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire8/relu_expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_20 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_20 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire8/expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire8/relu_expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_19 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_19 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire8/expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire8/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_18 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_18 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire8/squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire7/concat needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire7/relu_expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_17 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_17 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire7/expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire7/relu_expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_16 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_16 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire7/expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire7/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_15 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_15 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire7/squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire6/concat needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire6/relu_expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_14 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_14 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire6/expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire6/relu_expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_13 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_13 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire6/expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire6/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_12 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_12 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire6/squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] pool5 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire5/concat needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire5/relu_expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_11 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_11 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire5/expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire5/relu_expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_10 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_10 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire5/expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire5/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_9 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_9 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire5/squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire4/concat needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire4/relu_expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_8 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_8 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire4/expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire4/relu_expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire4/expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire4/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_7 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_7 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire4/squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] pool3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire3/concat needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire3/relu_expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_6 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_6 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire3/expand3x3 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire3/relu_expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_5 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_5 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire3/expand1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire3/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] scale_4 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] bn_4 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire3/squeeze1x1 needs backward computation.
I1107 10:17:48.524652 11160 net.cpp:198] fire2/concat needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] fire2/relu_expand3x3 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] scale_3 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] bn_3 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] fire2/expand3x3 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] fire2/relu_expand1x1 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] scale2 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] bn2 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] fire2/expand1x1 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] fire2/relu_squeeze1x1 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] fire2/squeeze1x1 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] pool1 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] relu_conv1 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] scale1 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] bn1 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:198] conv1 needs backward computation.
I1107 10:17:48.525651 11160 net.cpp:200] label_cifar_1_split does not need backward computation.
I1107 10:17:48.525651 11160 net.cpp:200] cifar does not need backward computation.
I1107 10:17:48.525651 11160 net.cpp:242] This network produces output accuracy
I1107 10:17:48.525651 11160 net.cpp:242] This network produces output loss
I1107 10:17:48.525651 11160 net.cpp:255] Network initialization done.
I1107 10:17:48.525651 11160 solver.cpp:56] Solver scaffolding done.
I1107 10:17:48.534647 11160 caffe.cpp:249] Starting Optimization
I1107 10:17:48.534647 11160 solver.cpp:272] Solving CIFAR10_Squeezenet_1.1_Batchnorm
I1107 10:17:48.534647 11160 solver.cpp:273] Learning Rate Policy: multistep
I1107 10:17:48.537647 11160 solver.cpp:330] Iteration 0, Testing net (#0)
I1107 10:17:48.540652 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:17:50.618017 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:17:50.698030 11160 solver.cpp:397]     Test net output #0: accuracy = 0.1
I1107 10:17:50.698030 11160 solver.cpp:397]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1107 10:17:50.891325 11160 solver.cpp:218] Iteration 0 (0 iter/s, 2.35517s/100 iters), loss = 2.29886
I1107 10:17:50.891325 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.14
I1107 10:17:50.891325 11160 solver.cpp:237]     Train net output #1: loss = 2.29886 (* 1 = 2.29886 loss)
I1107 10:17:50.891325 11160 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1107 10:17:59.454103 11160 solver.cpp:218] Iteration 100 (11.6791 iter/s, 8.56228s/100 iters), loss = 1.69684
I1107 10:17:59.454103 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.33
I1107 10:17:59.454103 11160 solver.cpp:237]     Train net output #1: loss = 1.69684 (* 1 = 1.69684 loss)
I1107 10:17:59.454103 11160 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1107 10:18:05.618465 11160 blocking_queue.cpp:49] Waiting for data
I1107 10:18:11.811563 11160 solver.cpp:218] Iteration 200 (8.09257 iter/s, 12.357s/100 iters), loss = 1.8127
I1107 10:18:11.811563 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.34
I1107 10:18:11.811563 11160 solver.cpp:237]     Train net output #1: loss = 1.8127 (* 1 = 1.8127 loss)
I1107 10:18:11.811563 11160 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1107 10:18:23.842669 11160 solver.cpp:218] Iteration 300 (8.31222 iter/s, 12.0305s/100 iters), loss = 1.57035
I1107 10:18:23.842669 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.47
I1107 10:18:23.842669 11160 solver.cpp:237]     Train net output #1: loss = 1.57035 (* 1 = 1.57035 loss)
I1107 10:18:23.842669 11160 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1107 10:18:32.537508 11160 solver.cpp:218] Iteration 400 (11.5017 iter/s, 8.69436s/100 iters), loss = 1.24157
I1107 10:18:32.537508 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.57
I1107 10:18:32.537508 11160 solver.cpp:237]     Train net output #1: loss = 1.24157 (* 1 = 1.24157 loss)
I1107 10:18:32.537508 11160 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1107 10:18:40.994617  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:18:41.147555 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_500.caffemodel
I1107 10:18:41.190560 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_500.solverstate
I1107 10:18:41.200060 11160 solver.cpp:330] Iteration 500, Testing net (#0)
I1107 10:18:41.200060 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:18:43.225462 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:18:43.304471 11160 solver.cpp:397]     Test net output #0: accuracy = 0.2085
I1107 10:18:43.304471 11160 solver.cpp:397]     Test net output #1: loss = 2.6467 (* 1 = 2.6467 loss)
I1107 10:18:43.386984 11160 solver.cpp:218] Iteration 500 (9.21781 iter/s, 10.8486s/100 iters), loss = 1.34367
I1107 10:18:43.386984 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.55
I1107 10:18:43.386984 11160 solver.cpp:237]     Train net output #1: loss = 1.34367 (* 1 = 1.34367 loss)
I1107 10:18:43.386984 11160 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1107 10:18:52.016366 11160 solver.cpp:218] Iteration 600 (11.5892 iter/s, 8.62871s/100 iters), loss = 1.21218
I1107 10:18:52.016366 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.58
I1107 10:18:52.016366 11160 solver.cpp:237]     Train net output #1: loss = 1.21218 (* 1 = 1.21218 loss)
I1107 10:18:52.016366 11160 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1107 10:19:00.651814 11160 solver.cpp:218] Iteration 700 (11.5804 iter/s, 8.63525s/100 iters), loss = 1.27131
I1107 10:19:00.652313 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.52
I1107 10:19:00.652313 11160 solver.cpp:237]     Train net output #1: loss = 1.27131 (* 1 = 1.27131 loss)
I1107 10:19:00.652313 11160 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1107 10:19:09.314762 11160 solver.cpp:218] Iteration 800 (11.5446 iter/s, 8.66207s/100 iters), loss = 1.13137
I1107 10:19:09.314762 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1107 10:19:09.314762 11160 solver.cpp:237]     Train net output #1: loss = 1.13137 (* 1 = 1.13137 loss)
I1107 10:19:09.314762 11160 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1107 10:19:17.897234 11160 solver.cpp:218] Iteration 900 (11.6527 iter/s, 8.58167s/100 iters), loss = 1.07589
I1107 10:19:17.897234 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.59
I1107 10:19:17.897234 11160 solver.cpp:237]     Train net output #1: loss = 1.07589 (* 1 = 1.07589 loss)
I1107 10:19:17.897234 11160 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1107 10:19:26.167800  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:19:26.517441 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_1000.caffemodel
I1107 10:19:26.547945 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_1000.solverstate
I1107 10:19:26.557943 11160 solver.cpp:330] Iteration 1000, Testing net (#0)
I1107 10:19:26.557943 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:19:28.601133 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:19:28.682154 11160 solver.cpp:397]     Test net output #0: accuracy = 0.2154
I1107 10:19:28.682154 11160 solver.cpp:397]     Test net output #1: loss = 2.55822 (* 1 = 2.55822 loss)
I1107 10:19:28.766149 11160 solver.cpp:218] Iteration 1000 (9.20081 iter/s, 10.8686s/100 iters), loss = 1.08306
I1107 10:19:28.766149 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1107 10:19:28.766149 11160 solver.cpp:237]     Train net output #1: loss = 1.08306 (* 1 = 1.08306 loss)
I1107 10:19:28.766149 11160 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1107 10:19:37.424772 11160 solver.cpp:218] Iteration 1100 (11.5501 iter/s, 8.65796s/100 iters), loss = 1.03164
I1107 10:19:37.425271 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1107 10:19:37.425271 11160 solver.cpp:237]     Train net output #1: loss = 1.03164 (* 1 = 1.03164 loss)
I1107 10:19:37.425271 11160 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1107 10:19:46.110352 11160 solver.cpp:218] Iteration 1200 (11.5147 iter/s, 8.68456s/100 iters), loss = 1.03249
I1107 10:19:46.110352 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.59
I1107 10:19:46.110352 11160 solver.cpp:237]     Train net output #1: loss = 1.03249 (* 1 = 1.03249 loss)
I1107 10:19:46.110352 11160 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1107 10:19:54.802109 11160 solver.cpp:218] Iteration 1300 (11.5046 iter/s, 8.69214s/100 iters), loss = 1.06175
I1107 10:19:54.803108 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1107 10:19:54.803108 11160 solver.cpp:237]     Train net output #1: loss = 1.06175 (* 1 = 1.06175 loss)
I1107 10:19:54.803108 11160 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1107 10:20:03.526597 11160 solver.cpp:218] Iteration 1400 (11.4638 iter/s, 8.72314s/100 iters), loss = 0.996439
I1107 10:20:03.526597 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.6
I1107 10:20:03.526597 11160 solver.cpp:237]     Train net output #1: loss = 0.996439 (* 1 = 0.996439 loss)
I1107 10:20:03.526597 11160 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1107 10:20:11.733718  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:20:12.070514 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_1500.caffemodel
I1107 10:20:12.101508 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_1500.solverstate
I1107 10:20:12.110496 11160 solver.cpp:330] Iteration 1500, Testing net (#0)
I1107 10:20:12.110496 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:20:14.106611 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:20:14.185647 11160 solver.cpp:397]     Test net output #0: accuracy = 0.2688
I1107 10:20:14.185647 11160 solver.cpp:397]     Test net output #1: loss = 2.2717 (* 1 = 2.2717 loss)
I1107 10:20:14.267228 11160 solver.cpp:218] Iteration 1500 (9.31036 iter/s, 10.7407s/100 iters), loss = 1.0207
I1107 10:20:14.267228 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.62
I1107 10:20:14.267228 11160 solver.cpp:237]     Train net output #1: loss = 1.0207 (* 1 = 1.0207 loss)
I1107 10:20:14.267228 11160 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1107 10:20:22.815254 11160 solver.cpp:218] Iteration 1600 (11.7 iter/s, 8.547s/100 iters), loss = 0.933282
I1107 10:20:22.815254 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1107 10:20:22.815254 11160 solver.cpp:237]     Train net output #1: loss = 0.933282 (* 1 = 0.933282 loss)
I1107 10:20:22.815254 11160 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1107 10:20:31.351063 11160 solver.cpp:218] Iteration 1700 (11.7158 iter/s, 8.53548s/100 iters), loss = 0.970657
I1107 10:20:31.351063 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1107 10:20:31.351063 11160 solver.cpp:237]     Train net output #1: loss = 0.970657 (* 1 = 0.970657 loss)
I1107 10:20:31.351063 11160 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1107 10:20:39.892194 11160 solver.cpp:218] Iteration 1800 (11.7091 iter/s, 8.54039s/100 iters), loss = 0.908986
I1107 10:20:39.892194 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1107 10:20:39.892194 11160 solver.cpp:237]     Train net output #1: loss = 0.908986 (* 1 = 0.908986 loss)
I1107 10:20:39.892194 11160 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1107 10:20:48.593519 11160 solver.cpp:218] Iteration 1900 (11.4927 iter/s, 8.70117s/100 iters), loss = 0.984548
I1107 10:20:48.593519 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1107 10:20:48.593519 11160 solver.cpp:237]     Train net output #1: loss = 0.984548 (* 1 = 0.984548 loss)
I1107 10:20:48.593519 11160 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1107 10:20:56.740588  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:20:57.076627 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_2000.caffemodel
I1107 10:20:57.106626 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_2000.solverstate
I1107 10:20:57.114626 11160 solver.cpp:330] Iteration 2000, Testing net (#0)
I1107 10:20:57.115628 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:20:59.126891 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:20:59.206899 11160 solver.cpp:397]     Test net output #0: accuracy = 0.2827
I1107 10:20:59.206899 11160 solver.cpp:397]     Test net output #1: loss = 2.28114 (* 1 = 2.28114 loss)
I1107 10:20:59.287919 11160 solver.cpp:218] Iteration 2000 (9.35164 iter/s, 10.6933s/100 iters), loss = 0.969321
I1107 10:20:59.287919 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1107 10:20:59.287919 11160 solver.cpp:237]     Train net output #1: loss = 0.969321 (* 1 = 0.969321 loss)
I1107 10:20:59.287919 11160 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1107 10:21:07.830001 11160 solver.cpp:218] Iteration 2100 (11.7078 iter/s, 8.54133s/100 iters), loss = 0.882711
I1107 10:21:07.830001 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1107 10:21:07.830001 11160 solver.cpp:237]     Train net output #1: loss = 0.882711 (* 1 = 0.882711 loss)
I1107 10:21:07.830001 11160 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1107 10:21:16.531952 11160 solver.cpp:218] Iteration 2200 (11.4916 iter/s, 8.702s/100 iters), loss = 1.02743
I1107 10:21:16.531952 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1107 10:21:16.531952 11160 solver.cpp:237]     Train net output #1: loss = 1.02743 (* 1 = 1.02743 loss)
I1107 10:21:16.531952 11160 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1107 10:21:25.261168 11160 solver.cpp:218] Iteration 2300 (11.4573 iter/s, 8.72803s/100 iters), loss = 0.733732
I1107 10:21:25.261168 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:21:25.261168 11160 solver.cpp:237]     Train net output #1: loss = 0.733732 (* 1 = 0.733732 loss)
I1107 10:21:25.261168 11160 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1107 10:21:33.998874 11160 solver.cpp:218] Iteration 2400 (11.4451 iter/s, 8.73733s/100 iters), loss = 0.929761
I1107 10:21:33.998874 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1107 10:21:33.998874 11160 solver.cpp:237]     Train net output #1: loss = 0.929761 (* 1 = 0.929761 loss)
I1107 10:21:33.998874 11160 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1107 10:21:42.263417  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:21:42.610327 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_2500.caffemodel
I1107 10:21:42.641341 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_2500.solverstate
I1107 10:21:42.650326 11160 solver.cpp:330] Iteration 2500, Testing net (#0)
I1107 10:21:42.651327 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:21:44.685500 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:21:44.766014 11160 solver.cpp:397]     Test net output #0: accuracy = 0.3726
I1107 10:21:44.766014 11160 solver.cpp:397]     Test net output #1: loss = 1.85862 (* 1 = 1.85862 loss)
I1107 10:21:44.850054 11160 solver.cpp:218] Iteration 2500 (9.21615 iter/s, 10.8505s/100 iters), loss = 0.914315
I1107 10:21:44.850054 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1107 10:21:44.850054 11160 solver.cpp:237]     Train net output #1: loss = 0.914315 (* 1 = 0.914315 loss)
I1107 10:21:44.850054 11160 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1107 10:21:53.580984 11160 solver.cpp:218] Iteration 2600 (11.4536 iter/s, 8.73089s/100 iters), loss = 0.903925
I1107 10:21:53.580984 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1107 10:21:53.580984 11160 solver.cpp:237]     Train net output #1: loss = 0.903925 (* 1 = 0.903925 loss)
I1107 10:21:53.581985 11160 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1107 10:22:02.314271 11160 solver.cpp:218] Iteration 2700 (11.4515 iter/s, 8.7325s/100 iters), loss = 0.928161
I1107 10:22:02.314271 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.61
I1107 10:22:02.314271 11160 solver.cpp:237]     Train net output #1: loss = 0.928161 (* 1 = 0.928161 loss)
I1107 10:22:02.314271 11160 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1107 10:22:11.016017 11160 solver.cpp:218] Iteration 2800 (11.4936 iter/s, 8.70048s/100 iters), loss = 0.78381
I1107 10:22:11.016017 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1107 10:22:11.016017 11160 solver.cpp:237]     Train net output #1: loss = 0.78381 (* 1 = 0.78381 loss)
I1107 10:22:11.016017 11160 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1107 10:22:19.686581 11160 solver.cpp:218] Iteration 2900 (11.5342 iter/s, 8.6699s/100 iters), loss = 0.892131
I1107 10:22:19.686581 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1107 10:22:19.686581 11160 solver.cpp:237]     Train net output #1: loss = 0.892131 (* 1 = 0.892131 loss)
I1107 10:22:19.686581 11160 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1107 10:22:27.995151  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:22:28.344707 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_3000.caffemodel
I1107 10:22:28.375713 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_3000.solverstate
I1107 10:22:28.384714 11160 solver.cpp:330] Iteration 3000, Testing net (#0)
I1107 10:22:28.384714 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:22:30.408881 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:22:30.489881 11160 solver.cpp:397]     Test net output #0: accuracy = 0.4022
I1107 10:22:30.489881 11160 solver.cpp:397]     Test net output #1: loss = 1.75392 (* 1 = 1.75392 loss)
I1107 10:22:30.572886 11160 solver.cpp:218] Iteration 3000 (9.18563 iter/s, 10.8866s/100 iters), loss = 1.00588
I1107 10:22:30.572886 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1107 10:22:30.573889 11160 solver.cpp:237]     Train net output #1: loss = 1.00588 (* 1 = 1.00588 loss)
I1107 10:22:30.573889 11160 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1107 10:22:39.282753 11160 solver.cpp:218] Iteration 3100 (11.483 iter/s, 8.7085s/100 iters), loss = 0.762436
I1107 10:22:39.282753 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:22:39.282753 11160 solver.cpp:237]     Train net output #1: loss = 0.762436 (* 1 = 0.762436 loss)
I1107 10:22:39.282753 11160 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1107 10:22:47.987013 11160 solver.cpp:218] Iteration 3200 (11.4893 iter/s, 8.70374s/100 iters), loss = 0.896575
I1107 10:22:47.987013 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1107 10:22:47.987013 11160 solver.cpp:237]     Train net output #1: loss = 0.896575 (* 1 = 0.896575 loss)
I1107 10:22:47.987013 11160 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1107 10:22:56.725744 11160 solver.cpp:218] Iteration 3300 (11.4438 iter/s, 8.73832s/100 iters), loss = 0.832494
I1107 10:22:56.725744 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1107 10:22:56.725744 11160 solver.cpp:237]     Train net output #1: loss = 0.832494 (* 1 = 0.832494 loss)
I1107 10:22:56.725744 11160 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1107 10:23:05.399341 11160 solver.cpp:218] Iteration 3400 (11.5307 iter/s, 8.67252s/100 iters), loss = 0.917135
I1107 10:23:05.399341 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1107 10:23:05.399341 11160 solver.cpp:237]     Train net output #1: loss = 0.917135 (* 1 = 0.917135 loss)
I1107 10:23:05.399341 11160 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1107 10:23:13.656474  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:23:13.997509 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_3500.caffemodel
I1107 10:23:14.027513 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_3500.solverstate
I1107 10:23:14.036514 11160 solver.cpp:330] Iteration 3500, Testing net (#0)
I1107 10:23:14.036514 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:23:16.029752 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:23:16.109777 11160 solver.cpp:397]     Test net output #0: accuracy = 0.3871
I1107 10:23:16.109777 11160 solver.cpp:397]     Test net output #1: loss = 1.81866 (* 1 = 1.81866 loss)
I1107 10:23:16.192782 11160 solver.cpp:218] Iteration 3500 (9.26553 iter/s, 10.7927s/100 iters), loss = 0.891739
I1107 10:23:16.192782 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1107 10:23:16.192782 11160 solver.cpp:237]     Train net output #1: loss = 0.891739 (* 1 = 0.891739 loss)
I1107 10:23:16.192782 11160 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1107 10:23:24.877069 11160 solver.cpp:218] Iteration 3600 (11.5149 iter/s, 8.6844s/100 iters), loss = 0.677408
I1107 10:23:24.877069 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:23:24.877069 11160 solver.cpp:237]     Train net output #1: loss = 0.677408 (* 1 = 0.677408 loss)
I1107 10:23:24.877069 11160 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1107 10:23:33.496747 11160 solver.cpp:218] Iteration 3700 (11.6025 iter/s, 8.61884s/100 iters), loss = 0.793286
I1107 10:23:33.496747 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1107 10:23:33.496747 11160 solver.cpp:237]     Train net output #1: loss = 0.793286 (* 1 = 0.793286 loss)
I1107 10:23:33.496747 11160 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1107 10:23:42.050669 11160 solver.cpp:218] Iteration 3800 (11.6915 iter/s, 8.55322s/100 iters), loss = 0.731156
I1107 10:23:42.050669 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:23:42.050669 11160 solver.cpp:237]     Train net output #1: loss = 0.731156 (* 1 = 0.731156 loss)
I1107 10:23:42.050669 11160 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1107 10:23:50.682600 11160 solver.cpp:218] Iteration 3900 (11.5849 iter/s, 8.6319s/100 iters), loss = 0.945316
I1107 10:23:50.682600 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1107 10:23:50.682600 11160 solver.cpp:237]     Train net output #1: loss = 0.945316 (* 1 = 0.945316 loss)
I1107 10:23:50.682600 11160 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1107 10:23:58.804606  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:23:59.143635 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_4000.caffemodel
I1107 10:23:59.174634 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_4000.solverstate
I1107 10:23:59.183634 11160 solver.cpp:330] Iteration 4000, Testing net (#0)
I1107 10:23:59.183634 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:24:01.178828 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:24:01.258836 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6393
I1107 10:24:01.258836 11160 solver.cpp:397]     Test net output #1: loss = 1.09458 (* 1 = 1.09458 loss)
I1107 10:24:01.339848 11160 solver.cpp:218] Iteration 4000 (9.38381 iter/s, 10.6567s/100 iters), loss = 0.860621
I1107 10:24:01.339848 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1107 10:24:01.339848 11160 solver.cpp:237]     Train net output #1: loss = 0.860621 (* 1 = 0.860621 loss)
I1107 10:24:01.339848 11160 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1107 10:24:09.880868 11160 solver.cpp:218] Iteration 4100 (11.7093 iter/s, 8.54019s/100 iters), loss = 0.728152
I1107 10:24:09.880868 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:24:09.880868 11160 solver.cpp:237]     Train net output #1: loss = 0.728152 (* 1 = 0.728152 loss)
I1107 10:24:09.880868 11160 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1107 10:24:18.420717 11160 solver.cpp:218] Iteration 4200 (11.7113 iter/s, 8.53877s/100 iters), loss = 0.763008
I1107 10:24:18.420717 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:24:18.420717 11160 solver.cpp:237]     Train net output #1: loss = 0.763008 (* 1 = 0.763008 loss)
I1107 10:24:18.420717 11160 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1107 10:24:26.971782 11160 solver.cpp:218] Iteration 4300 (11.6942 iter/s, 8.55122s/100 iters), loss = 0.685984
I1107 10:24:26.971782 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:24:26.971782 11160 solver.cpp:237]     Train net output #1: loss = 0.685984 (* 1 = 0.685984 loss)
I1107 10:24:26.971782 11160 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1107 10:24:35.528158 11160 solver.cpp:218] Iteration 4400 (11.6887 iter/s, 8.55525s/100 iters), loss = 0.804881
I1107 10:24:35.528158 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1107 10:24:35.528158 11160 solver.cpp:237]     Train net output #1: loss = 0.804881 (* 1 = 0.804881 loss)
I1107 10:24:35.528158 11160 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1107 10:24:43.796340  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:24:44.134096 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_4500.caffemodel
I1107 10:24:44.165333 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_4500.solverstate
I1107 10:24:44.174360 11160 solver.cpp:330] Iteration 4500, Testing net (#0)
I1107 10:24:44.174360 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:24:46.172868 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:24:46.253387 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5382
I1107 10:24:46.253387 11160 solver.cpp:397]     Test net output #1: loss = 1.37699 (* 1 = 1.37699 loss)
I1107 10:24:46.334403 11160 solver.cpp:218] Iteration 4500 (9.25378 iter/s, 10.8064s/100 iters), loss = 0.850475
I1107 10:24:46.334403 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1107 10:24:46.335403 11160 solver.cpp:237]     Train net output #1: loss = 0.850475 (* 1 = 0.850475 loss)
I1107 10:24:46.335403 11160 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1107 10:24:54.923005 11160 solver.cpp:218] Iteration 4600 (11.6453 iter/s, 8.58716s/100 iters), loss = 0.669988
I1107 10:24:54.923005 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:24:54.923005 11160 solver.cpp:237]     Train net output #1: loss = 0.669988 (* 1 = 0.669988 loss)
I1107 10:24:54.923005 11160 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1107 10:25:03.598649 11160 solver.cpp:218] Iteration 4700 (11.5263 iter/s, 8.6758s/100 iters), loss = 0.652484
I1107 10:25:03.598649 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:25:03.598649 11160 solver.cpp:237]     Train net output #1: loss = 0.652484 (* 1 = 0.652484 loss)
I1107 10:25:03.598649 11160 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1107 10:25:12.340433 11160 solver.cpp:218] Iteration 4800 (11.4406 iter/s, 8.7408s/100 iters), loss = 0.67203
I1107 10:25:12.340433 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:25:12.340433 11160 solver.cpp:237]     Train net output #1: loss = 0.67203 (* 1 = 0.67203 loss)
I1107 10:25:12.340433 11160 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1107 10:25:21.027249 11160 solver.cpp:218] Iteration 4900 (11.5118 iter/s, 8.68672s/100 iters), loss = 0.760901
I1107 10:25:21.028232 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1107 10:25:21.028232 11160 solver.cpp:237]     Train net output #1: loss = 0.760901 (* 1 = 0.760901 loss)
I1107 10:25:21.028232 11160 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1107 10:25:29.398926  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:25:29.750520 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_5000.caffemodel
I1107 10:25:29.779027 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_5000.solverstate
I1107 10:25:29.788025 11160 solver.cpp:330] Iteration 5000, Testing net (#0)
I1107 10:25:29.788025 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:25:31.826210 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:25:31.907213 11160 solver.cpp:397]     Test net output #0: accuracy = 0.4922
I1107 10:25:31.907213 11160 solver.cpp:397]     Test net output #1: loss = 1.5577 (* 1 = 1.5577 loss)
I1107 10:25:31.991235 11160 solver.cpp:218] Iteration 5000 (9.12179 iter/s, 10.9628s/100 iters), loss = 0.831464
I1107 10:25:31.991235 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1107 10:25:31.991235 11160 solver.cpp:237]     Train net output #1: loss = 0.831464 (* 1 = 0.831464 loss)
I1107 10:25:31.991235 11160 sgd_solver.cpp:105] Iteration 5000, lr = 0.1
I1107 10:25:40.557126 11160 solver.cpp:218] Iteration 5100 (11.6743 iter/s, 8.5658s/100 iters), loss = 0.625654
I1107 10:25:40.558137 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:25:40.558137 11160 solver.cpp:237]     Train net output #1: loss = 0.625654 (* 1 = 0.625654 loss)
I1107 10:25:40.558137 11160 sgd_solver.cpp:105] Iteration 5100, lr = 0.1
I1107 10:25:49.223618 11160 solver.cpp:218] Iteration 5200 (11.5401 iter/s, 8.66547s/100 iters), loss = 0.90261
I1107 10:25:49.223618 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:25:49.223618 11160 solver.cpp:237]     Train net output #1: loss = 0.90261 (* 1 = 0.90261 loss)
I1107 10:25:49.223618 11160 sgd_solver.cpp:105] Iteration 5200, lr = 0.1
I1107 10:25:57.753933 11160 solver.cpp:218] Iteration 5300 (11.7236 iter/s, 8.52979s/100 iters), loss = 0.680304
I1107 10:25:57.754434 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:25:57.754434 11160 solver.cpp:237]     Train net output #1: loss = 0.680304 (* 1 = 0.680304 loss)
I1107 10:25:57.754434 11160 sgd_solver.cpp:105] Iteration 5300, lr = 0.1
I1107 10:26:06.300690 11160 solver.cpp:218] Iteration 5400 (11.7013 iter/s, 8.54607s/100 iters), loss = 0.695108
I1107 10:26:06.300690 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1107 10:26:06.300690 11160 solver.cpp:237]     Train net output #1: loss = 0.695108 (* 1 = 0.695108 loss)
I1107 10:26:06.300690 11160 sgd_solver.cpp:105] Iteration 5400, lr = 0.1
I1107 10:26:14.430441  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:26:14.766475 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_5500.caffemodel
I1107 10:26:14.797479 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_5500.solverstate
I1107 10:26:14.805480 11160 solver.cpp:330] Iteration 5500, Testing net (#0)
I1107 10:26:14.806480 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:26:16.792629 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:26:16.872632 11160 solver.cpp:397]     Test net output #0: accuracy = 0.4681
I1107 10:26:16.872632 11160 solver.cpp:397]     Test net output #1: loss = 1.8051 (* 1 = 1.8051 loss)
I1107 10:26:16.954133 11160 solver.cpp:218] Iteration 5500 (9.38742 iter/s, 10.6526s/100 iters), loss = 0.808026
I1107 10:26:16.954133 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:26:16.954133 11160 solver.cpp:237]     Train net output #1: loss = 0.808026 (* 1 = 0.808026 loss)
I1107 10:26:16.954133 11160 sgd_solver.cpp:105] Iteration 5500, lr = 0.1
I1107 10:26:25.494089 11160 solver.cpp:218] Iteration 5600 (11.7104 iter/s, 8.53944s/100 iters), loss = 0.614583
I1107 10:26:25.494089 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:26:25.494089 11160 solver.cpp:237]     Train net output #1: loss = 0.614583 (* 1 = 0.614583 loss)
I1107 10:26:25.494089 11160 sgd_solver.cpp:105] Iteration 5600, lr = 0.1
I1107 10:26:34.177660 11160 solver.cpp:218] Iteration 5700 (11.5161 iter/s, 8.6835s/100 iters), loss = 0.665799
I1107 10:26:34.177660 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:26:34.177660 11160 solver.cpp:237]     Train net output #1: loss = 0.665799 (* 1 = 0.665799 loss)
I1107 10:26:34.177660 11160 sgd_solver.cpp:105] Iteration 5700, lr = 0.1
I1107 10:26:42.811561 11160 solver.cpp:218] Iteration 5800 (11.5831 iter/s, 8.63327s/100 iters), loss = 0.643719
I1107 10:26:42.811561 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:26:42.811561 11160 solver.cpp:237]     Train net output #1: loss = 0.643719 (* 1 = 0.643719 loss)
I1107 10:26:42.811561 11160 sgd_solver.cpp:105] Iteration 5800, lr = 0.1
I1107 10:26:51.325594 11160 solver.cpp:218] Iteration 5900 (11.7458 iter/s, 8.51369s/100 iters), loss = 0.742223
I1107 10:26:51.325594 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1107 10:26:51.325594 11160 solver.cpp:237]     Train net output #1: loss = 0.742223 (* 1 = 0.742223 loss)
I1107 10:26:51.325594 11160 sgd_solver.cpp:105] Iteration 5900, lr = 0.1
I1107 10:26:59.435920  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:26:59.772423 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_6000.caffemodel
I1107 10:26:59.800974 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_6000.solverstate
I1107 10:26:59.809978 11160 solver.cpp:330] Iteration 6000, Testing net (#0)
I1107 10:26:59.809978 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:27:01.847137 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:27:01.928730 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5078
I1107 10:27:01.928730 11160 solver.cpp:397]     Test net output #1: loss = 1.67538 (* 1 = 1.67538 loss)
I1107 10:27:02.013774 11160 solver.cpp:218] Iteration 6000 (9.35733 iter/s, 10.6868s/100 iters), loss = 0.717426
I1107 10:27:02.013774 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 10:27:02.013774 11160 solver.cpp:237]     Train net output #1: loss = 0.717426 (* 1 = 0.717426 loss)
I1107 10:27:02.013774 11160 sgd_solver.cpp:105] Iteration 6000, lr = 0.1
I1107 10:27:10.663403 11160 solver.cpp:218] Iteration 6100 (11.5619 iter/s, 8.64911s/100 iters), loss = 0.601712
I1107 10:27:10.663403 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 10:27:10.663403 11160 solver.cpp:237]     Train net output #1: loss = 0.601712 (* 1 = 0.601712 loss)
I1107 10:27:10.663403 11160 sgd_solver.cpp:105] Iteration 6100, lr = 0.1
I1107 10:27:19.239502 11160 solver.cpp:218] Iteration 6200 (11.6609 iter/s, 8.57569s/100 iters), loss = 0.76113
I1107 10:27:19.239502 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1107 10:27:19.239502 11160 solver.cpp:237]     Train net output #1: loss = 0.76113 (* 1 = 0.76113 loss)
I1107 10:27:19.239502 11160 sgd_solver.cpp:105] Iteration 6200, lr = 0.1
I1107 10:27:27.939450 11160 solver.cpp:218] Iteration 6300 (11.4946 iter/s, 8.69976s/100 iters), loss = 0.567847
I1107 10:27:27.939450 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 10:27:27.939450 11160 solver.cpp:237]     Train net output #1: loss = 0.567847 (* 1 = 0.567847 loss)
I1107 10:27:27.939450 11160 sgd_solver.cpp:105] Iteration 6300, lr = 0.1
I1107 10:27:36.496762 11160 solver.cpp:218] Iteration 6400 (11.6872 iter/s, 8.55635s/100 iters), loss = 0.626856
I1107 10:27:36.496762 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:27:36.496762 11160 solver.cpp:237]     Train net output #1: loss = 0.626856 (* 1 = 0.626856 loss)
I1107 10:27:36.496762 11160 sgd_solver.cpp:105] Iteration 6400, lr = 0.1
I1107 10:27:44.752661  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:27:45.097710 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_6500.caffemodel
I1107 10:27:45.127723 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_6500.solverstate
I1107 10:27:45.136725 11160 solver.cpp:330] Iteration 6500, Testing net (#0)
I1107 10:27:45.137230 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:27:47.175968 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:27:47.257966 11160 solver.cpp:397]     Test net output #0: accuracy = 0.7025
I1107 10:27:47.257966 11160 solver.cpp:397]     Test net output #1: loss = 0.898606 (* 1 = 0.898606 loss)
I1107 10:27:47.339980 11160 solver.cpp:218] Iteration 6500 (9.22273 iter/s, 10.8428s/100 iters), loss = 0.842753
I1107 10:27:47.339980 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1107 10:27:47.339980 11160 solver.cpp:237]     Train net output #1: loss = 0.842753 (* 1 = 0.842753 loss)
I1107 10:27:47.339980 11160 sgd_solver.cpp:105] Iteration 6500, lr = 0.1
I1107 10:27:55.960090 11160 solver.cpp:218] Iteration 6600 (11.6016 iter/s, 8.61951s/100 iters), loss = 0.70261
I1107 10:27:55.960090 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1107 10:27:55.960090 11160 solver.cpp:237]     Train net output #1: loss = 0.70261 (* 1 = 0.70261 loss)
I1107 10:27:55.960090 11160 sgd_solver.cpp:105] Iteration 6600, lr = 0.1
I1107 10:28:04.642477 11160 solver.cpp:218] Iteration 6700 (11.5181 iter/s, 8.68197s/100 iters), loss = 0.659273
I1107 10:28:04.642477 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:28:04.642477 11160 solver.cpp:237]     Train net output #1: loss = 0.659273 (* 1 = 0.659273 loss)
I1107 10:28:04.642477 11160 sgd_solver.cpp:105] Iteration 6700, lr = 0.1
I1107 10:28:13.365821 11160 solver.cpp:218] Iteration 6800 (11.4644 iter/s, 8.72266s/100 iters), loss = 0.699735
I1107 10:28:13.365821 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:28:13.366322 11160 solver.cpp:237]     Train net output #1: loss = 0.699735 (* 1 = 0.699735 loss)
I1107 10:28:13.366322 11160 sgd_solver.cpp:105] Iteration 6800, lr = 0.1
I1107 10:28:21.984035 11160 solver.cpp:218] Iteration 6900 (11.604 iter/s, 8.61769s/100 iters), loss = 0.692761
I1107 10:28:21.984035 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:28:21.984035 11160 solver.cpp:237]     Train net output #1: loss = 0.692761 (* 1 = 0.692761 loss)
I1107 10:28:21.984035 11160 sgd_solver.cpp:105] Iteration 6900, lr = 0.1
I1107 10:28:30.240052  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:28:30.589583 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_7000.caffemodel
I1107 10:28:30.624593 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_7000.solverstate
I1107 10:28:30.636595 11160 solver.cpp:330] Iteration 7000, Testing net (#0)
I1107 10:28:30.636595 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:28:32.686617 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:28:32.768117 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6285
I1107 10:28:32.768117 11160 solver.cpp:397]     Test net output #1: loss = 1.16132 (* 1 = 1.16132 loss)
I1107 10:28:32.853116 11160 solver.cpp:218] Iteration 7000 (9.20102 iter/s, 10.8684s/100 iters), loss = 0.631561
I1107 10:28:32.853615 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:28:32.853615 11160 solver.cpp:237]     Train net output #1: loss = 0.631561 (* 1 = 0.631561 loss)
I1107 10:28:32.853615 11160 sgd_solver.cpp:105] Iteration 7000, lr = 0.1
I1107 10:28:41.405495 11160 solver.cpp:218] Iteration 7100 (11.6938 iter/s, 8.55157s/100 iters), loss = 0.561377
I1107 10:28:41.405495 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:28:41.405495 11160 solver.cpp:237]     Train net output #1: loss = 0.561377 (* 1 = 0.561377 loss)
I1107 10:28:41.405495 11160 sgd_solver.cpp:105] Iteration 7100, lr = 0.1
I1107 10:28:50.039755 11160 solver.cpp:218] Iteration 7200 (11.5824 iter/s, 8.6338s/100 iters), loss = 0.616881
I1107 10:28:50.039755 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:28:50.039755 11160 solver.cpp:237]     Train net output #1: loss = 0.616881 (* 1 = 0.616881 loss)
I1107 10:28:50.040256 11160 sgd_solver.cpp:105] Iteration 7200, lr = 0.1
I1107 10:28:58.765527 11160 solver.cpp:218] Iteration 7300 (11.4612 iter/s, 8.72512s/100 iters), loss = 0.522308
I1107 10:28:58.765527 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 10:28:58.765527 11160 solver.cpp:237]     Train net output #1: loss = 0.522308 (* 1 = 0.522308 loss)
I1107 10:28:58.765527 11160 sgd_solver.cpp:105] Iteration 7300, lr = 0.1
I1107 10:29:07.429790 11160 solver.cpp:218] Iteration 7400 (11.5428 iter/s, 8.66339s/100 iters), loss = 0.657852
I1107 10:29:07.429790 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:29:07.429790 11160 solver.cpp:237]     Train net output #1: loss = 0.657852 (* 1 = 0.657852 loss)
I1107 10:29:07.429790 11160 sgd_solver.cpp:105] Iteration 7400, lr = 0.1
I1107 10:29:15.611205  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:29:15.957320 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_7500.caffemodel
I1107 10:29:15.988324 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_7500.solverstate
I1107 10:29:15.997319 11160 solver.cpp:330] Iteration 7500, Testing net (#0)
I1107 10:29:15.997319 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:29:18.006575 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:29:18.085583 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6835
I1107 10:29:18.085583 11160 solver.cpp:397]     Test net output #1: loss = 0.943233 (* 1 = 0.943233 loss)
I1107 10:29:18.166594 11160 solver.cpp:218] Iteration 7500 (9.31378 iter/s, 10.7368s/100 iters), loss = 0.7397
I1107 10:29:18.166594 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:29:18.166594 11160 solver.cpp:237]     Train net output #1: loss = 0.7397 (* 1 = 0.7397 loss)
I1107 10:29:18.166594 11160 sgd_solver.cpp:105] Iteration 7500, lr = 0.1
I1107 10:29:26.721962 11160 solver.cpp:218] Iteration 7600 (11.6889 iter/s, 8.55516s/100 iters), loss = 0.673742
I1107 10:29:26.721962 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:29:26.721962 11160 solver.cpp:237]     Train net output #1: loss = 0.673742 (* 1 = 0.673742 loss)
I1107 10:29:26.721962 11160 sgd_solver.cpp:105] Iteration 7600, lr = 0.1
I1107 10:29:35.325803 11160 solver.cpp:218] Iteration 7700 (11.6235 iter/s, 8.60329s/100 iters), loss = 0.5882
I1107 10:29:35.325803 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:29:35.325803 11160 solver.cpp:237]     Train net output #1: loss = 0.5882 (* 1 = 0.5882 loss)
I1107 10:29:35.325803 11160 sgd_solver.cpp:105] Iteration 7700, lr = 0.1
I1107 10:29:43.934885 11160 solver.cpp:218] Iteration 7800 (11.6171 iter/s, 8.60798s/100 iters), loss = 0.646959
I1107 10:29:43.934885 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:29:43.934885 11160 solver.cpp:237]     Train net output #1: loss = 0.646959 (* 1 = 0.646959 loss)
I1107 10:29:43.934885 11160 sgd_solver.cpp:105] Iteration 7800, lr = 0.1
I1107 10:29:52.583709 11160 solver.cpp:218] Iteration 7900 (11.563 iter/s, 8.64827s/100 iters), loss = 0.657866
I1107 10:29:52.583709 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:29:52.583709 11160 solver.cpp:237]     Train net output #1: loss = 0.657866 (* 1 = 0.657866 loss)
I1107 10:29:52.583709 11160 sgd_solver.cpp:105] Iteration 7900, lr = 0.1
I1107 10:30:00.790824  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:30:01.142371 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_8000.caffemodel
I1107 10:30:01.173882 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_8000.solverstate
I1107 10:30:01.183879 11160 solver.cpp:330] Iteration 8000, Testing net (#0)
I1107 10:30:01.183879 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:30:03.190099 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:30:03.270107 11160 solver.cpp:397]     Test net output #0: accuracy = 0.463
I1107 10:30:03.270107 11160 solver.cpp:397]     Test net output #1: loss = 1.96876 (* 1 = 1.96876 loss)
I1107 10:30:03.351121 11160 solver.cpp:218] Iteration 8000 (9.28809 iter/s, 10.7665s/100 iters), loss = 0.751701
I1107 10:30:03.351121 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:30:03.351121 11160 solver.cpp:237]     Train net output #1: loss = 0.751701 (* 1 = 0.751701 loss)
I1107 10:30:03.351121 11160 sgd_solver.cpp:105] Iteration 8000, lr = 0.1
I1107 10:30:11.986824 11160 solver.cpp:218] Iteration 8100 (11.5801 iter/s, 8.63552s/100 iters), loss = 0.58467
I1107 10:30:11.986824 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:30:11.986824 11160 solver.cpp:237]     Train net output #1: loss = 0.58467 (* 1 = 0.58467 loss)
I1107 10:30:11.986824 11160 sgd_solver.cpp:105] Iteration 8100, lr = 0.1
I1107 10:30:20.705699 11160 solver.cpp:218] Iteration 8200 (11.4695 iter/s, 8.71876s/100 iters), loss = 0.725989
I1107 10:30:20.705699 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1107 10:30:20.706701 11160 solver.cpp:237]     Train net output #1: loss = 0.725989 (* 1 = 0.725989 loss)
I1107 10:30:20.706701 11160 sgd_solver.cpp:105] Iteration 8200, lr = 0.1
I1107 10:30:29.248112 11160 solver.cpp:218] Iteration 8300 (11.7078 iter/s, 8.54132s/100 iters), loss = 0.585844
I1107 10:30:29.248112 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:30:29.248112 11160 solver.cpp:237]     Train net output #1: loss = 0.585844 (* 1 = 0.585844 loss)
I1107 10:30:29.248112 11160 sgd_solver.cpp:105] Iteration 8300, lr = 0.1
I1107 10:30:37.769524 11160 solver.cpp:218] Iteration 8400 (11.7352 iter/s, 8.52136s/100 iters), loss = 0.725085
I1107 10:30:37.770512 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1107 10:30:37.770512 11160 solver.cpp:237]     Train net output #1: loss = 0.725085 (* 1 = 0.725085 loss)
I1107 10:30:37.770512 11160 sgd_solver.cpp:105] Iteration 8400, lr = 0.1
I1107 10:30:45.881147  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:30:46.217985 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_8500.caffemodel
I1107 10:30:46.246515 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_8500.solverstate
I1107 10:30:46.255504 11160 solver.cpp:330] Iteration 8500, Testing net (#0)
I1107 10:30:46.255504 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:30:48.239476 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:30:48.318981 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6063
I1107 10:30:48.318981 11160 solver.cpp:397]     Test net output #1: loss = 1.20149 (* 1 = 1.20149 loss)
I1107 10:30:48.399989 11160 solver.cpp:218] Iteration 8500 (9.40748 iter/s, 10.6298s/100 iters), loss = 0.71599
I1107 10:30:48.399989 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 10:30:48.399989 11160 solver.cpp:237]     Train net output #1: loss = 0.71599 (* 1 = 0.71599 loss)
I1107 10:30:48.400991 11160 sgd_solver.cpp:105] Iteration 8500, lr = 0.1
I1107 10:30:56.909801 11160 solver.cpp:218] Iteration 8600 (11.7526 iter/s, 8.50872s/100 iters), loss = 0.558395
I1107 10:30:56.909801 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:30:56.909801 11160 solver.cpp:237]     Train net output #1: loss = 0.558395 (* 1 = 0.558395 loss)
I1107 10:30:56.909801 11160 sgd_solver.cpp:105] Iteration 8600, lr = 0.1
I1107 10:31:05.423207 11160 solver.cpp:218] Iteration 8700 (11.7474 iter/s, 8.51252s/100 iters), loss = 0.742219
I1107 10:31:05.423207 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:31:05.423207 11160 solver.cpp:237]     Train net output #1: loss = 0.742219 (* 1 = 0.742219 loss)
I1107 10:31:05.423207 11160 sgd_solver.cpp:105] Iteration 8700, lr = 0.1
I1107 10:31:13.959229 11160 solver.cpp:218] Iteration 8800 (11.7148 iter/s, 8.53619s/100 iters), loss = 0.586941
I1107 10:31:13.959229 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:31:13.959229 11160 solver.cpp:237]     Train net output #1: loss = 0.586941 (* 1 = 0.586941 loss)
I1107 10:31:13.959229 11160 sgd_solver.cpp:105] Iteration 8800, lr = 0.1
I1107 10:31:22.579208 11160 solver.cpp:218] Iteration 8900 (11.6016 iter/s, 8.61952s/100 iters), loss = 0.584243
I1107 10:31:22.579208 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:31:22.579208 11160 solver.cpp:237]     Train net output #1: loss = 0.584243 (* 1 = 0.584243 loss)
I1107 10:31:22.579208 11160 sgd_solver.cpp:105] Iteration 8900, lr = 0.1
I1107 10:31:30.714864  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:31:31.050981 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_9000.caffemodel
I1107 10:31:31.081985 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_9000.solverstate
I1107 10:31:31.091009 11160 solver.cpp:330] Iteration 9000, Testing net (#0)
I1107 10:31:31.091509 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:31:33.102771 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:31:33.182770 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5513
I1107 10:31:33.182770 11160 solver.cpp:397]     Test net output #1: loss = 1.48308 (* 1 = 1.48308 loss)
I1107 10:31:33.263797 11160 solver.cpp:218] Iteration 9000 (9.36003 iter/s, 10.6837s/100 iters), loss = 0.589309
I1107 10:31:33.263797 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:31:33.263797 11160 solver.cpp:237]     Train net output #1: loss = 0.589309 (* 1 = 0.589309 loss)
I1107 10:31:33.263797 11160 sgd_solver.cpp:105] Iteration 9000, lr = 0.1
I1107 10:31:41.823346 11160 solver.cpp:218] Iteration 9100 (11.683 iter/s, 8.55942s/100 iters), loss = 0.593889
I1107 10:31:41.823346 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 10:31:41.823346 11160 solver.cpp:237]     Train net output #1: loss = 0.593889 (* 1 = 0.593889 loss)
I1107 10:31:41.823346 11160 sgd_solver.cpp:105] Iteration 9100, lr = 0.1
I1107 10:31:50.389111 11160 solver.cpp:218] Iteration 9200 (11.6758 iter/s, 8.5647s/100 iters), loss = 0.868831
I1107 10:31:50.389111 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1107 10:31:50.389111 11160 solver.cpp:237]     Train net output #1: loss = 0.868831 (* 1 = 0.868831 loss)
I1107 10:31:50.389111 11160 sgd_solver.cpp:105] Iteration 9200, lr = 0.1
I1107 10:31:58.922360 11160 solver.cpp:218] Iteration 9300 (11.7198 iter/s, 8.53255s/100 iters), loss = 0.584031
I1107 10:31:58.922360 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 10:31:58.922360 11160 solver.cpp:237]     Train net output #1: loss = 0.584031 (* 1 = 0.584031 loss)
I1107 10:31:58.922360 11160 sgd_solver.cpp:105] Iteration 9300, lr = 0.1
I1107 10:32:07.478576 11160 solver.cpp:218] Iteration 9400 (11.6872 iter/s, 8.5564s/100 iters), loss = 0.7693
I1107 10:32:07.478576 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1107 10:32:07.478576 11160 solver.cpp:237]     Train net output #1: loss = 0.7693 (* 1 = 0.7693 loss)
I1107 10:32:07.478576 11160 sgd_solver.cpp:105] Iteration 9400, lr = 0.1
I1107 10:32:15.806089  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:32:16.161281 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_9500.caffemodel
I1107 10:32:16.196799 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_9500.solverstate
I1107 10:32:16.205809 11160 solver.cpp:330] Iteration 9500, Testing net (#0)
I1107 10:32:16.205809 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:32:18.239334 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:32:18.321821 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5084
I1107 10:32:18.321821 11160 solver.cpp:397]     Test net output #1: loss = 1.60465 (* 1 = 1.60465 loss)
I1107 10:32:18.406828 11160 solver.cpp:218] Iteration 9500 (9.15145 iter/s, 10.9272s/100 iters), loss = 0.714829
I1107 10:32:18.406828 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:32:18.406828 11160 solver.cpp:237]     Train net output #1: loss = 0.714829 (* 1 = 0.714829 loss)
I1107 10:32:18.406828 11160 sgd_solver.cpp:105] Iteration 9500, lr = 0.1
I1107 10:32:27.160892 11160 solver.cpp:218] Iteration 9600 (11.4237 iter/s, 8.7537s/100 iters), loss = 0.512431
I1107 10:32:27.160892 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 10:32:27.160892 11160 solver.cpp:237]     Train net output #1: loss = 0.512431 (* 1 = 0.512431 loss)
I1107 10:32:27.160892 11160 sgd_solver.cpp:105] Iteration 9600, lr = 0.1
I1107 10:32:35.817613 11160 solver.cpp:218] Iteration 9700 (11.5533 iter/s, 8.65553s/100 iters), loss = 0.720786
I1107 10:32:35.817613 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:32:35.817613 11160 solver.cpp:237]     Train net output #1: loss = 0.720786 (* 1 = 0.720786 loss)
I1107 10:32:35.817613 11160 sgd_solver.cpp:105] Iteration 9700, lr = 0.1
I1107 10:32:44.428376 11160 solver.cpp:218] Iteration 9800 (11.613 iter/s, 8.61104s/100 iters), loss = 0.648442
I1107 10:32:44.428376 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:32:44.428376 11160 solver.cpp:237]     Train net output #1: loss = 0.648442 (* 1 = 0.648442 loss)
I1107 10:32:44.428376 11160 sgd_solver.cpp:105] Iteration 9800, lr = 0.1
I1107 10:32:52.944526 11160 solver.cpp:218] Iteration 9900 (11.7431 iter/s, 8.51564s/100 iters), loss = 0.644456
I1107 10:32:52.944526 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:32:52.944526 11160 solver.cpp:237]     Train net output #1: loss = 0.644456 (* 1 = 0.644456 loss)
I1107 10:32:52.944526 11160 sgd_solver.cpp:105] Iteration 9900, lr = 0.1
I1107 10:33:01.044940  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:33:01.381976 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_10000.caffemodel
I1107 10:33:01.413975 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_10000.solverstate
I1107 10:33:01.422976 11160 solver.cpp:330] Iteration 10000, Testing net (#0)
I1107 10:33:01.422976 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:33:03.414213 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:33:03.494199 11160 solver.cpp:397]     Test net output #0: accuracy = 0.554
I1107 10:33:03.494199 11160 solver.cpp:397]     Test net output #1: loss = 1.47518 (* 1 = 1.47518 loss)
I1107 10:33:03.575202 11160 solver.cpp:218] Iteration 10000 (9.40736 iter/s, 10.63s/100 iters), loss = 0.768146
I1107 10:33:03.575202 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1107 10:33:03.575202 11160 solver.cpp:237]     Train net output #1: loss = 0.768146 (* 1 = 0.768146 loss)
I1107 10:33:03.575202 11160 sgd_solver.cpp:105] Iteration 10000, lr = 0.1
I1107 10:33:12.103978 11160 solver.cpp:218] Iteration 10100 (11.7265 iter/s, 8.52772s/100 iters), loss = 0.671083
I1107 10:33:12.103978 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:33:12.103978 11160 solver.cpp:237]     Train net output #1: loss = 0.671083 (* 1 = 0.671083 loss)
I1107 10:33:12.103978 11160 sgd_solver.cpp:105] Iteration 10100, lr = 0.1
I1107 10:33:20.627192 11160 solver.cpp:218] Iteration 10200 (11.7327 iter/s, 8.52318s/100 iters), loss = 0.647318
I1107 10:33:20.627192 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:33:20.627192 11160 solver.cpp:237]     Train net output #1: loss = 0.647318 (* 1 = 0.647318 loss)
I1107 10:33:20.627192 11160 sgd_solver.cpp:105] Iteration 10200, lr = 0.1
I1107 10:33:29.160456 11160 solver.cpp:218] Iteration 10300 (11.7203 iter/s, 8.53222s/100 iters), loss = 0.663706
I1107 10:33:29.160456 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:33:29.160456 11160 solver.cpp:237]     Train net output #1: loss = 0.663706 (* 1 = 0.663706 loss)
I1107 10:33:29.160456 11160 sgd_solver.cpp:105] Iteration 10300, lr = 0.1
I1107 10:33:37.685302 11160 solver.cpp:218] Iteration 10400 (11.7307 iter/s, 8.52468s/100 iters), loss = 0.59539
I1107 10:33:37.685302 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 10:33:37.685302 11160 solver.cpp:237]     Train net output #1: loss = 0.59539 (* 1 = 0.59539 loss)
I1107 10:33:37.685302 11160 sgd_solver.cpp:105] Iteration 10400, lr = 0.1
I1107 10:33:45.805693  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:33:46.145558 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_10500.caffemodel
I1107 10:33:46.175664 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_10500.solverstate
I1107 10:33:46.183661 11160 solver.cpp:330] Iteration 10500, Testing net (#0)
I1107 10:33:46.183661 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:33:48.177186 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:33:48.257223 11160 solver.cpp:397]     Test net output #0: accuracy = 0.631
I1107 10:33:48.257223 11160 solver.cpp:397]     Test net output #1: loss = 1.14868 (* 1 = 1.14868 loss)
I1107 10:33:48.339210 11160 solver.cpp:218] Iteration 10500 (9.38707 iter/s, 10.653s/100 iters), loss = 0.888088
I1107 10:33:48.339210 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1107 10:33:48.339210 11160 solver.cpp:237]     Train net output #1: loss = 0.888088 (* 1 = 0.888088 loss)
I1107 10:33:48.339210 11160 sgd_solver.cpp:105] Iteration 10500, lr = 0.1
I1107 10:33:56.867600 11160 solver.cpp:218] Iteration 10600 (11.7264 iter/s, 8.52778s/100 iters), loss = 0.728235
I1107 10:33:56.867600 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:33:56.867600 11160 solver.cpp:237]     Train net output #1: loss = 0.728235 (* 1 = 0.728235 loss)
I1107 10:33:56.867600 11160 sgd_solver.cpp:105] Iteration 10600, lr = 0.1
I1107 10:34:05.391733 11160 solver.cpp:218] Iteration 10700 (11.7321 iter/s, 8.52361s/100 iters), loss = 0.634676
I1107 10:34:05.391733 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:34:05.391733 11160 solver.cpp:237]     Train net output #1: loss = 0.634676 (* 1 = 0.634676 loss)
I1107 10:34:05.391733 11160 sgd_solver.cpp:105] Iteration 10700, lr = 0.1
I1107 10:34:13.928470 11160 solver.cpp:218] Iteration 10800 (11.7143 iter/s, 8.53658s/100 iters), loss = 0.563172
I1107 10:34:13.928470 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 10:34:13.928470 11160 solver.cpp:237]     Train net output #1: loss = 0.563172 (* 1 = 0.563172 loss)
I1107 10:34:13.928470 11160 sgd_solver.cpp:105] Iteration 10800, lr = 0.1
I1107 10:34:22.453685 11160 solver.cpp:218] Iteration 10900 (11.7314 iter/s, 8.52415s/100 iters), loss = 0.499614
I1107 10:34:22.453685 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 10:34:22.453685 11160 solver.cpp:237]     Train net output #1: loss = 0.499614 (* 1 = 0.499614 loss)
I1107 10:34:22.453685 11160 sgd_solver.cpp:105] Iteration 10900, lr = 0.1
I1107 10:34:30.561625  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:34:30.898874 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_11000.caffemodel
I1107 10:34:30.927875 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_11000.solverstate
I1107 10:34:30.935884 11160 solver.cpp:330] Iteration 11000, Testing net (#0)
I1107 10:34:30.935884 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:34:32.923830 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:34:33.004377 11160 solver.cpp:397]     Test net output #0: accuracy = 0.4363
I1107 10:34:33.004377 11160 solver.cpp:397]     Test net output #1: loss = 1.95846 (* 1 = 1.95846 loss)
I1107 10:34:33.087069 11160 solver.cpp:218] Iteration 11000 (9.40458 iter/s, 10.6331s/100 iters), loss = 0.779724
I1107 10:34:33.087069 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1107 10:34:33.087069 11160 solver.cpp:237]     Train net output #1: loss = 0.779724 (* 1 = 0.779724 loss)
I1107 10:34:33.087069 11160 sgd_solver.cpp:105] Iteration 11000, lr = 0.1
I1107 10:34:41.622817 11160 solver.cpp:218] Iteration 11100 (11.7163 iter/s, 8.53512s/100 iters), loss = 0.616443
I1107 10:34:41.622817 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:34:41.622817 11160 solver.cpp:237]     Train net output #1: loss = 0.616443 (* 1 = 0.616443 loss)
I1107 10:34:41.622817 11160 sgd_solver.cpp:105] Iteration 11100, lr = 0.1
I1107 10:34:50.168704 11160 solver.cpp:218] Iteration 11200 (11.7018 iter/s, 8.54571s/100 iters), loss = 0.734818
I1107 10:34:50.168704 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:34:50.168704 11160 solver.cpp:237]     Train net output #1: loss = 0.734818 (* 1 = 0.734818 loss)
I1107 10:34:50.168704 11160 sgd_solver.cpp:105] Iteration 11200, lr = 0.1
I1107 10:34:58.707691 11160 solver.cpp:218] Iteration 11300 (11.7123 iter/s, 8.53805s/100 iters), loss = 0.504655
I1107 10:34:58.707691 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 10:34:58.707691 11160 solver.cpp:237]     Train net output #1: loss = 0.504655 (* 1 = 0.504655 loss)
I1107 10:34:58.707691 11160 sgd_solver.cpp:105] Iteration 11300, lr = 0.1
I1107 10:35:07.248598 11160 solver.cpp:218] Iteration 11400 (11.7092 iter/s, 8.54029s/100 iters), loss = 0.672977
I1107 10:35:07.248598 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:35:07.248598 11160 solver.cpp:237]     Train net output #1: loss = 0.672977 (* 1 = 0.672977 loss)
I1107 10:35:07.248598 11160 sgd_solver.cpp:105] Iteration 11400, lr = 0.1
I1107 10:35:15.375782  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:35:15.714834 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_11500.caffemodel
I1107 10:35:15.744843 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_11500.solverstate
I1107 10:35:15.753842 11160 solver.cpp:330] Iteration 11500, Testing net (#0)
I1107 10:35:15.753842 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:35:17.745182 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:35:17.825184 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6514
I1107 10:35:17.825184 11160 solver.cpp:397]     Test net output #1: loss = 1.10559 (* 1 = 1.10559 loss)
I1107 10:35:17.906184 11160 solver.cpp:218] Iteration 11500 (9.38296 iter/s, 10.6576s/100 iters), loss = 0.698899
I1107 10:35:17.906184 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:35:17.906184 11160 solver.cpp:237]     Train net output #1: loss = 0.698899 (* 1 = 0.698899 loss)
I1107 10:35:17.906184 11160 sgd_solver.cpp:105] Iteration 11500, lr = 0.1
I1107 10:35:26.430204 11160 solver.cpp:218] Iteration 11600 (11.7321 iter/s, 8.52365s/100 iters), loss = 0.581573
I1107 10:35:26.430204 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:35:26.430204 11160 solver.cpp:237]     Train net output #1: loss = 0.581573 (* 1 = 0.581573 loss)
I1107 10:35:26.430204 11160 sgd_solver.cpp:105] Iteration 11600, lr = 0.1
I1107 10:35:34.958125 11160 solver.cpp:218] Iteration 11700 (11.7268 iter/s, 8.52746s/100 iters), loss = 0.723935
I1107 10:35:34.958125 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1107 10:35:34.958125 11160 solver.cpp:237]     Train net output #1: loss = 0.723935 (* 1 = 0.723935 loss)
I1107 10:35:34.959125 11160 sgd_solver.cpp:105] Iteration 11700, lr = 0.1
I1107 10:35:43.488739 11160 solver.cpp:218] Iteration 11800 (11.7243 iter/s, 8.52926s/100 iters), loss = 0.543591
I1107 10:35:43.488739 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 10:35:43.488739 11160 solver.cpp:237]     Train net output #1: loss = 0.543591 (* 1 = 0.543591 loss)
I1107 10:35:43.488739 11160 sgd_solver.cpp:105] Iteration 11800, lr = 0.1
I1107 10:35:52.017215 11160 solver.cpp:218] Iteration 11900 (11.7262 iter/s, 8.52793s/100 iters), loss = 0.63997
I1107 10:35:52.017215 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:35:52.017215 11160 solver.cpp:237]     Train net output #1: loss = 0.63997 (* 1 = 0.63997 loss)
I1107 10:35:52.017215 11160 sgd_solver.cpp:105] Iteration 11900, lr = 0.1
I1107 10:36:00.128787  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:36:00.465848 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_12000.caffemodel
I1107 10:36:00.494853 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_12000.solverstate
I1107 10:36:00.502857 11160 solver.cpp:330] Iteration 12000, Testing net (#0)
I1107 10:36:00.503859 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:36:02.492974 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:36:02.571975 11160 solver.cpp:397]     Test net output #0: accuracy = 0.528
I1107 10:36:02.571975 11160 solver.cpp:397]     Test net output #1: loss = 1.56907 (* 1 = 1.56907 loss)
I1107 10:36:02.653980 11160 solver.cpp:218] Iteration 12000 (9.40156 iter/s, 10.6365s/100 iters), loss = 0.797737
I1107 10:36:02.653980 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1107 10:36:02.653980 11160 solver.cpp:237]     Train net output #1: loss = 0.797737 (* 1 = 0.797737 loss)
I1107 10:36:02.653980 11160 sgd_solver.cpp:105] Iteration 12000, lr = 0.1
I1107 10:36:11.187764 11160 solver.cpp:218] Iteration 12100 (11.7182 iter/s, 8.53377s/100 iters), loss = 0.521979
I1107 10:36:11.188765 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 10:36:11.188765 11160 solver.cpp:237]     Train net output #1: loss = 0.521979 (* 1 = 0.521979 loss)
I1107 10:36:11.188765 11160 sgd_solver.cpp:105] Iteration 12100, lr = 0.1
I1107 10:36:19.710295 11160 solver.cpp:218] Iteration 12200 (11.7351 iter/s, 8.52146s/100 iters), loss = 0.779554
I1107 10:36:19.710295 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:36:19.710295 11160 solver.cpp:237]     Train net output #1: loss = 0.779554 (* 1 = 0.779554 loss)
I1107 10:36:19.710295 11160 sgd_solver.cpp:105] Iteration 12200, lr = 0.1
I1107 10:36:28.243870 11160 solver.cpp:218] Iteration 12300 (11.7188 iter/s, 8.53327s/100 iters), loss = 0.724638
I1107 10:36:28.243870 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:36:28.243870 11160 solver.cpp:237]     Train net output #1: loss = 0.724638 (* 1 = 0.724638 loss)
I1107 10:36:28.243870 11160 sgd_solver.cpp:105] Iteration 12300, lr = 0.1
I1107 10:36:36.767601 11160 solver.cpp:218] Iteration 12400 (11.7322 iter/s, 8.52353s/100 iters), loss = 0.607294
I1107 10:36:36.767601 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:36:36.768601 11160 solver.cpp:237]     Train net output #1: loss = 0.607294 (* 1 = 0.607294 loss)
I1107 10:36:36.768601 11160 sgd_solver.cpp:105] Iteration 12400, lr = 0.1
I1107 10:36:44.871096  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:36:45.208122 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_12500.caffemodel
I1107 10:36:45.241127 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_12500.solverstate
I1107 10:36:45.249625 11160 solver.cpp:330] Iteration 12500, Testing net (#0)
I1107 10:36:45.250125 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:36:47.241340 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:36:47.320346 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5683
I1107 10:36:47.320346 11160 solver.cpp:397]     Test net output #1: loss = 1.4533 (* 1 = 1.4533 loss)
I1107 10:36:47.402349 11160 solver.cpp:218] Iteration 12500 (9.40399 iter/s, 10.6338s/100 iters), loss = 0.689584
I1107 10:36:47.402349 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1107 10:36:47.402349 11160 solver.cpp:237]     Train net output #1: loss = 0.689584 (* 1 = 0.689584 loss)
I1107 10:36:47.402349 11160 sgd_solver.cpp:105] Iteration 12500, lr = 0.1
I1107 10:36:55.936102 11160 solver.cpp:218] Iteration 12600 (11.719 iter/s, 8.53317s/100 iters), loss = 0.656075
I1107 10:36:55.936102 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:36:55.936102 11160 solver.cpp:237]     Train net output #1: loss = 0.656075 (* 1 = 0.656075 loss)
I1107 10:36:55.936102 11160 sgd_solver.cpp:105] Iteration 12600, lr = 0.1
I1107 10:37:04.473042 11160 solver.cpp:218] Iteration 12700 (11.7145 iter/s, 8.53642s/100 iters), loss = 0.758732
I1107 10:37:04.473042 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:37:04.473042 11160 solver.cpp:237]     Train net output #1: loss = 0.758732 (* 1 = 0.758732 loss)
I1107 10:37:04.473042 11160 sgd_solver.cpp:105] Iteration 12700, lr = 0.1
I1107 10:37:13.022924 11160 solver.cpp:218] Iteration 12800 (11.6965 iter/s, 8.54954s/100 iters), loss = 0.622638
I1107 10:37:13.022924 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:37:13.022924 11160 solver.cpp:237]     Train net output #1: loss = 0.622638 (* 1 = 0.622638 loss)
I1107 10:37:13.022924 11160 sgd_solver.cpp:105] Iteration 12800, lr = 0.1
I1107 10:37:21.565850 11160 solver.cpp:218] Iteration 12900 (11.7072 iter/s, 8.54175s/100 iters), loss = 0.598817
I1107 10:37:21.565850 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:37:21.565850 11160 solver.cpp:237]     Train net output #1: loss = 0.598817 (* 1 = 0.598817 loss)
I1107 10:37:21.565850 11160 sgd_solver.cpp:105] Iteration 12900, lr = 0.1
I1107 10:37:29.690768  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:37:30.028827 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_13000.caffemodel
I1107 10:37:30.059828 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_13000.solverstate
I1107 10:37:30.067826 11160 solver.cpp:330] Iteration 13000, Testing net (#0)
I1107 10:37:30.068828 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:37:32.058971 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:37:32.137974 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5461
I1107 10:37:32.137974 11160 solver.cpp:397]     Test net output #1: loss = 1.5563 (* 1 = 1.5563 loss)
I1107 10:37:32.219980 11160 solver.cpp:218] Iteration 13000 (9.38609 iter/s, 10.6541s/100 iters), loss = 0.682427
I1107 10:37:32.219980 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:37:32.219980 11160 solver.cpp:237]     Train net output #1: loss = 0.682427 (* 1 = 0.682427 loss)
I1107 10:37:32.219980 11160 sgd_solver.cpp:105] Iteration 13000, lr = 0.1
I1107 10:37:40.751178 11160 solver.cpp:218] Iteration 13100 (11.7219 iter/s, 8.53102s/100 iters), loss = 0.520673
I1107 10:37:40.751178 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:37:40.751178 11160 solver.cpp:237]     Train net output #1: loss = 0.520673 (* 1 = 0.520673 loss)
I1107 10:37:40.751178 11160 sgd_solver.cpp:105] Iteration 13100, lr = 0.1
I1107 10:37:49.283573 11160 solver.cpp:218] Iteration 13200 (11.721 iter/s, 8.53167s/100 iters), loss = 0.611987
I1107 10:37:49.283573 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:37:49.283573 11160 solver.cpp:237]     Train net output #1: loss = 0.611987 (* 1 = 0.611987 loss)
I1107 10:37:49.283573 11160 sgd_solver.cpp:105] Iteration 13200, lr = 0.1
I1107 10:37:57.820479 11160 solver.cpp:218] Iteration 13300 (11.714 iter/s, 8.53676s/100 iters), loss = 0.687665
I1107 10:37:57.820479 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:37:57.820479 11160 solver.cpp:237]     Train net output #1: loss = 0.687665 (* 1 = 0.687665 loss)
I1107 10:37:57.821480 11160 sgd_solver.cpp:105] Iteration 13300, lr = 0.1
I1107 10:38:06.426082 11160 solver.cpp:218] Iteration 13400 (11.622 iter/s, 8.60434s/100 iters), loss = 0.744477
I1107 10:38:06.426082 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:38:06.426082 11160 solver.cpp:237]     Train net output #1: loss = 0.744477 (* 1 = 0.744477 loss)
I1107 10:38:06.426082 11160 sgd_solver.cpp:105] Iteration 13400, lr = 0.1
I1107 10:38:14.581594  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:38:14.924145 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_13500.caffemodel
I1107 10:38:14.955145 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_13500.solverstate
I1107 10:38:15.008162 11160 solver.cpp:330] Iteration 13500, Testing net (#0)
I1107 10:38:15.008162 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:38:17.035488 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:38:17.115473 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6103
I1107 10:38:17.115473 11160 solver.cpp:397]     Test net output #1: loss = 1.22268 (* 1 = 1.22268 loss)
I1107 10:38:17.196476 11160 solver.cpp:218] Iteration 13500 (9.28519 iter/s, 10.7698s/100 iters), loss = 0.659498
I1107 10:38:17.196476 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:38:17.196476 11160 solver.cpp:237]     Train net output #1: loss = 0.659498 (* 1 = 0.659498 loss)
I1107 10:38:17.196476 11160 sgd_solver.cpp:105] Iteration 13500, lr = 0.1
I1107 10:38:25.963158 11160 solver.cpp:218] Iteration 13600 (11.4076 iter/s, 8.76612s/100 iters), loss = 0.712821
I1107 10:38:25.963158 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:38:25.963158 11160 solver.cpp:237]     Train net output #1: loss = 0.712821 (* 1 = 0.712821 loss)
I1107 10:38:25.963158 11160 sgd_solver.cpp:105] Iteration 13600, lr = 0.1
I1107 10:38:34.580725 11160 solver.cpp:218] Iteration 13700 (11.6047 iter/s, 8.61718s/100 iters), loss = 0.567682
I1107 10:38:34.581226 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:38:34.581226 11160 solver.cpp:237]     Train net output #1: loss = 0.567682 (* 1 = 0.567682 loss)
I1107 10:38:34.581226 11160 sgd_solver.cpp:105] Iteration 13700, lr = 0.1
I1107 10:38:43.118782 11160 solver.cpp:218] Iteration 13800 (11.7128 iter/s, 8.53768s/100 iters), loss = 0.588868
I1107 10:38:43.118782 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:38:43.118782 11160 solver.cpp:237]     Train net output #1: loss = 0.588868 (* 1 = 0.588868 loss)
I1107 10:38:43.118782 11160 sgd_solver.cpp:105] Iteration 13800, lr = 0.1
I1107 10:38:51.656339 11160 solver.cpp:218] Iteration 13900 (11.714 iter/s, 8.53683s/100 iters), loss = 0.580339
I1107 10:38:51.656839 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:38:51.656839 11160 solver.cpp:237]     Train net output #1: loss = 0.580339 (* 1 = 0.580339 loss)
I1107 10:38:51.656839 11160 sgd_solver.cpp:105] Iteration 13900, lr = 0.1
I1107 10:38:59.779685  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:39:00.117184 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_14000.caffemodel
I1107 10:39:00.143184 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_14000.solverstate
I1107 10:39:00.151685 11160 solver.cpp:330] Iteration 14000, Testing net (#0)
I1107 10:39:00.152185 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:39:02.144193 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:39:02.223703 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6047
I1107 10:39:02.223703 11160 solver.cpp:397]     Test net output #1: loss = 1.30415 (* 1 = 1.30415 loss)
I1107 10:39:02.304693 11160 solver.cpp:218] Iteration 14000 (9.3917 iter/s, 10.6477s/100 iters), loss = 0.717047
I1107 10:39:02.304693 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1107 10:39:02.304693 11160 solver.cpp:237]     Train net output #1: loss = 0.717047 (* 1 = 0.717047 loss)
I1107 10:39:02.304693 11160 sgd_solver.cpp:105] Iteration 14000, lr = 0.1
I1107 10:39:10.928004 11160 solver.cpp:218] Iteration 14100 (11.597 iter/s, 8.62289s/100 iters), loss = 0.65659
I1107 10:39:10.928004 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:39:10.928505 11160 solver.cpp:237]     Train net output #1: loss = 0.65659 (* 1 = 0.65659 loss)
I1107 10:39:10.928505 11160 sgd_solver.cpp:105] Iteration 14100, lr = 0.1
I1107 10:39:19.509727 11160 solver.cpp:218] Iteration 14200 (11.6534 iter/s, 8.58119s/100 iters), loss = 0.657835
I1107 10:39:19.509727 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1107 10:39:19.510227 11160 solver.cpp:237]     Train net output #1: loss = 0.657835 (* 1 = 0.657835 loss)
I1107 10:39:19.510227 11160 sgd_solver.cpp:105] Iteration 14200, lr = 0.1
I1107 10:39:28.059185 11160 solver.cpp:218] Iteration 14300 (11.698 iter/s, 8.54848s/100 iters), loss = 0.581986
I1107 10:39:28.059185 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:39:28.059185 11160 solver.cpp:237]     Train net output #1: loss = 0.581986 (* 1 = 0.581986 loss)
I1107 10:39:28.059185 11160 sgd_solver.cpp:105] Iteration 14300, lr = 0.1
I1107 10:39:36.723551 11160 solver.cpp:218] Iteration 14400 (11.5414 iter/s, 8.66443s/100 iters), loss = 0.631171
I1107 10:39:36.723551 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:39:36.723551 11160 solver.cpp:237]     Train net output #1: loss = 0.631171 (* 1 = 0.631171 loss)
I1107 10:39:36.723551 11160 sgd_solver.cpp:105] Iteration 14400, lr = 0.1
I1107 10:39:44.965894  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:39:45.305394 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_14500.caffemodel
I1107 10:39:45.338893 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_14500.solverstate
I1107 10:39:45.375396 11160 solver.cpp:330] Iteration 14500, Testing net (#0)
I1107 10:39:45.375396 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:39:47.398396 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:39:47.478395 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5795
I1107 10:39:47.478395 11160 solver.cpp:397]     Test net output #1: loss = 1.30258 (* 1 = 1.30258 loss)
I1107 10:39:47.560396 11160 solver.cpp:218] Iteration 14500 (9.22876 iter/s, 10.8357s/100 iters), loss = 0.67112
I1107 10:39:47.560396 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:39:47.560396 11160 solver.cpp:237]     Train net output #1: loss = 0.67112 (* 1 = 0.67112 loss)
I1107 10:39:47.560396 11160 sgd_solver.cpp:105] Iteration 14500, lr = 0.1
I1107 10:39:56.176071 11160 solver.cpp:218] Iteration 14600 (11.6074 iter/s, 8.61516s/100 iters), loss = 0.559957
I1107 10:39:56.176071 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:39:56.176071 11160 solver.cpp:237]     Train net output #1: loss = 0.559957 (* 1 = 0.559957 loss)
I1107 10:39:56.176071 11160 sgd_solver.cpp:105] Iteration 14600, lr = 0.1
I1107 10:40:04.777194 11160 solver.cpp:218] Iteration 14700 (11.627 iter/s, 8.60068s/100 iters), loss = 0.674841
I1107 10:40:04.777194 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1107 10:40:04.777194 11160 solver.cpp:237]     Train net output #1: loss = 0.674841 (* 1 = 0.674841 loss)
I1107 10:40:04.777194 11160 sgd_solver.cpp:105] Iteration 14700, lr = 0.1
I1107 10:40:13.324705 11160 solver.cpp:218] Iteration 14800 (11.7 iter/s, 8.547s/100 iters), loss = 0.670168
I1107 10:40:13.324705 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:40:13.324705 11160 solver.cpp:237]     Train net output #1: loss = 0.670168 (* 1 = 0.670168 loss)
I1107 10:40:13.324705 11160 sgd_solver.cpp:105] Iteration 14800, lr = 0.1
I1107 10:40:21.909276 11160 solver.cpp:218] Iteration 14900 (11.6495 iter/s, 8.58405s/100 iters), loss = 0.659015
I1107 10:40:21.909276 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:40:21.909276 11160 solver.cpp:237]     Train net output #1: loss = 0.659015 (* 1 = 0.659015 loss)
I1107 10:40:21.909276 11160 sgd_solver.cpp:105] Iteration 14900, lr = 0.1
I1107 10:40:30.061446  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:40:30.398947 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_15000.caffemodel
I1107 10:40:30.429445 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_15000.solverstate
I1107 10:40:30.437947 11160 solver.cpp:330] Iteration 15000, Testing net (#0)
I1107 10:40:30.437947 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:40:32.425489 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:40:32.504989 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6091
I1107 10:40:32.504989 11160 solver.cpp:397]     Test net output #1: loss = 1.17428 (* 1 = 1.17428 loss)
I1107 10:40:32.585989 11160 solver.cpp:218] Iteration 15000 (9.36693 iter/s, 10.6759s/100 iters), loss = 0.828759
I1107 10:40:32.585989 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1107 10:40:32.585989 11160 solver.cpp:237]     Train net output #1: loss = 0.828759 (* 1 = 0.828759 loss)
I1107 10:40:32.585989 11160 sgd_solver.cpp:105] Iteration 15000, lr = 0.1
I1107 10:40:41.135524 11160 solver.cpp:218] Iteration 15100 (11.6972 iter/s, 8.54904s/100 iters), loss = 0.504274
I1107 10:40:41.135524 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1107 10:40:41.135524 11160 solver.cpp:237]     Train net output #1: loss = 0.504274 (* 1 = 0.504274 loss)
I1107 10:40:41.135524 11160 sgd_solver.cpp:105] Iteration 15100, lr = 0.1
I1107 10:40:49.775383 11160 solver.cpp:218] Iteration 15200 (11.575 iter/s, 8.63928s/100 iters), loss = 0.635305
I1107 10:40:49.775383 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:40:49.775383 11160 solver.cpp:237]     Train net output #1: loss = 0.635305 (* 1 = 0.635305 loss)
I1107 10:40:49.775383 11160 sgd_solver.cpp:105] Iteration 15200, lr = 0.1
I1107 10:40:58.397529 11160 solver.cpp:218] Iteration 15300 (11.5976 iter/s, 8.62245s/100 iters), loss = 0.630397
I1107 10:40:58.398530 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:40:58.398530 11160 solver.cpp:237]     Train net output #1: loss = 0.630397 (* 1 = 0.630397 loss)
I1107 10:40:58.398530 11160 sgd_solver.cpp:105] Iteration 15300, lr = 0.1
I1107 10:41:07.336796 11160 solver.cpp:218] Iteration 15400 (11.1876 iter/s, 8.93847s/100 iters), loss = 0.589714
I1107 10:41:07.336796 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:41:07.336796 11160 solver.cpp:237]     Train net output #1: loss = 0.589714 (* 1 = 0.589714 loss)
I1107 10:41:07.336796 11160 sgd_solver.cpp:105] Iteration 15400, lr = 0.1
I1107 10:41:15.798777  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:41:16.148756 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_15500.caffemodel
I1107 10:41:16.180739 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_15500.solverstate
I1107 10:41:16.190747 11160 solver.cpp:330] Iteration 15500, Testing net (#0)
I1107 10:41:16.190747 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:41:18.237944 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:41:18.319943 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5819
I1107 10:41:18.319943 11160 solver.cpp:397]     Test net output #1: loss = 1.39163 (* 1 = 1.39163 loss)
I1107 10:41:18.403944 11160 solver.cpp:218] Iteration 15500 (9.03684 iter/s, 11.0658s/100 iters), loss = 0.663798
I1107 10:41:18.403944 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:41:18.403944 11160 solver.cpp:237]     Train net output #1: loss = 0.663798 (* 1 = 0.663798 loss)
I1107 10:41:18.403944 11160 sgd_solver.cpp:105] Iteration 15500, lr = 0.1
I1107 10:41:27.271526 11160 solver.cpp:218] Iteration 15600 (11.2773 iter/s, 8.8674s/100 iters), loss = 0.656608
I1107 10:41:27.271526 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:41:27.271526 11160 solver.cpp:237]     Train net output #1: loss = 0.656608 (* 1 = 0.656608 loss)
I1107 10:41:27.271526 11160 sgd_solver.cpp:105] Iteration 15600, lr = 0.1
I1107 10:41:36.100046 11160 solver.cpp:218] Iteration 15700 (11.3286 iter/s, 8.82724s/100 iters), loss = 0.728689
I1107 10:41:36.100046 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:41:36.100046 11160 solver.cpp:237]     Train net output #1: loss = 0.728689 (* 1 = 0.728689 loss)
I1107 10:41:36.100046 11160 sgd_solver.cpp:105] Iteration 15700, lr = 0.1
I1107 10:41:44.845793 11160 solver.cpp:218] Iteration 15800 (11.435 iter/s, 8.74508s/100 iters), loss = 0.592387
I1107 10:41:44.845793 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 10:41:44.845793 11160 solver.cpp:237]     Train net output #1: loss = 0.592387 (* 1 = 0.592387 loss)
I1107 10:41:44.845793 11160 sgd_solver.cpp:105] Iteration 15800, lr = 0.1
I1107 10:41:53.446900 11160 solver.cpp:218] Iteration 15900 (11.627 iter/s, 8.60069s/100 iters), loss = 0.516967
I1107 10:41:53.446900 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 10:41:53.446900 11160 solver.cpp:237]     Train net output #1: loss = 0.516967 (* 1 = 0.516967 loss)
I1107 10:41:53.446900 11160 sgd_solver.cpp:105] Iteration 15900, lr = 0.1
I1107 10:42:01.852944  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:42:02.203946 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_16000.caffemodel
I1107 10:42:02.274446 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_16000.solverstate
I1107 10:42:02.300446 11160 solver.cpp:330] Iteration 16000, Testing net (#0)
I1107 10:42:02.300945 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:42:04.343807 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:42:04.423817 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6335
I1107 10:42:04.423817 11160 solver.cpp:397]     Test net output #1: loss = 1.14941 (* 1 = 1.14941 loss)
I1107 10:42:04.508826 11160 solver.cpp:218] Iteration 16000 (9.04071 iter/s, 11.0611s/100 iters), loss = 0.696684
I1107 10:42:04.508826 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:42:04.508826 11160 solver.cpp:237]     Train net output #1: loss = 0.696684 (* 1 = 0.696684 loss)
I1107 10:42:04.508826 11160 sgd_solver.cpp:105] Iteration 16000, lr = 0.1
I1107 10:42:13.345736 11160 solver.cpp:218] Iteration 16100 (11.3165 iter/s, 8.83663s/100 iters), loss = 0.622573
I1107 10:42:13.345736 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:42:13.345736 11160 solver.cpp:237]     Train net output #1: loss = 0.622573 (* 1 = 0.622573 loss)
I1107 10:42:13.345736 11160 sgd_solver.cpp:105] Iteration 16100, lr = 0.1
I1107 10:42:22.162387 11160 solver.cpp:218] Iteration 16200 (11.343 iter/s, 8.81602s/100 iters), loss = 0.602743
I1107 10:42:22.162387 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:42:22.162387 11160 solver.cpp:237]     Train net output #1: loss = 0.602743 (* 1 = 0.602743 loss)
I1107 10:42:22.162387 11160 sgd_solver.cpp:105] Iteration 16200, lr = 0.1
I1107 10:42:30.989389 11160 solver.cpp:218] Iteration 16300 (11.3294 iter/s, 8.82661s/100 iters), loss = 0.530116
I1107 10:42:30.989900 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:42:30.989900 11160 solver.cpp:237]     Train net output #1: loss = 0.530116 (* 1 = 0.530116 loss)
I1107 10:42:30.989900 11160 sgd_solver.cpp:105] Iteration 16300, lr = 0.1
I1107 10:42:39.790670 11160 solver.cpp:218] Iteration 16400 (11.3631 iter/s, 8.80044s/100 iters), loss = 0.624235
I1107 10:42:39.790670 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:42:39.790670 11160 solver.cpp:237]     Train net output #1: loss = 0.624235 (* 1 = 0.624235 loss)
I1107 10:42:39.790670 11160 sgd_solver.cpp:105] Iteration 16400, lr = 0.1
I1107 10:42:48.130446  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:42:48.480446 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_16500.caffemodel
I1107 10:42:48.538949 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_16500.solverstate
I1107 10:42:48.550447 11160 solver.cpp:330] Iteration 16500, Testing net (#0)
I1107 10:42:48.550956 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:42:50.559945 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:42:50.639948 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5401
I1107 10:42:50.639948 11160 solver.cpp:397]     Test net output #1: loss = 1.55583 (* 1 = 1.55583 loss)
I1107 10:42:50.722947 11160 solver.cpp:218] Iteration 16500 (9.14774 iter/s, 10.9317s/100 iters), loss = 0.66382
I1107 10:42:50.722947 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:42:50.723451 11160 solver.cpp:237]     Train net output #1: loss = 0.66382 (* 1 = 0.66382 loss)
I1107 10:42:50.723451 11160 sgd_solver.cpp:105] Iteration 16500, lr = 0.1
I1107 10:42:59.516717 11160 solver.cpp:218] Iteration 16600 (11.3726 iter/s, 8.79303s/100 iters), loss = 0.498182
I1107 10:42:59.516717 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 10:42:59.517217 11160 solver.cpp:237]     Train net output #1: loss = 0.498182 (* 1 = 0.498182 loss)
I1107 10:42:59.517217 11160 sgd_solver.cpp:105] Iteration 16600, lr = 0.1
I1107 10:43:08.295128 11160 solver.cpp:218] Iteration 16700 (11.3926 iter/s, 8.77765s/100 iters), loss = 0.618561
I1107 10:43:08.295128 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:43:08.295128 11160 solver.cpp:237]     Train net output #1: loss = 0.618561 (* 1 = 0.618561 loss)
I1107 10:43:08.295128 11160 sgd_solver.cpp:105] Iteration 16700, lr = 0.1
I1107 10:43:16.967428 11160 solver.cpp:218] Iteration 16800 (11.5314 iter/s, 8.67196s/100 iters), loss = 0.623127
I1107 10:43:16.967929 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:43:16.967929 11160 solver.cpp:237]     Train net output #1: loss = 0.623127 (* 1 = 0.623127 loss)
I1107 10:43:16.967929 11160 sgd_solver.cpp:105] Iteration 16800, lr = 0.1
I1107 10:43:25.584759 11160 solver.cpp:218] Iteration 16900 (11.6054 iter/s, 8.61664s/100 iters), loss = 0.601518
I1107 10:43:25.584759 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:43:25.584759 11160 solver.cpp:237]     Train net output #1: loss = 0.601518 (* 1 = 0.601518 loss)
I1107 10:43:25.584759 11160 sgd_solver.cpp:105] Iteration 16900, lr = 0.1
I1107 10:43:33.695757  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:43:34.031756 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_17000.caffemodel
I1107 10:43:34.067256 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_17000.solverstate
I1107 10:43:34.076257 11160 solver.cpp:330] Iteration 17000, Testing net (#0)
I1107 10:43:34.076757 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:43:36.072805 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:43:36.151830 11160 solver.cpp:397]     Test net output #0: accuracy = 0.7048
I1107 10:43:36.151830 11160 solver.cpp:397]     Test net output #1: loss = 0.88669 (* 1 = 0.88669 loss)
I1107 10:43:36.232832 11160 solver.cpp:218] Iteration 17000 (9.3919 iter/s, 10.6475s/100 iters), loss = 0.633258
I1107 10:43:36.232832 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:43:36.232832 11160 solver.cpp:237]     Train net output #1: loss = 0.633258 (* 1 = 0.633258 loss)
I1107 10:43:36.232832 11160 sgd_solver.cpp:105] Iteration 17000, lr = 0.1
I1107 10:43:44.762295 11160 solver.cpp:218] Iteration 17100 (11.7247 iter/s, 8.52901s/100 iters), loss = 0.560654
I1107 10:43:44.762295 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 10:43:44.762795 11160 solver.cpp:237]     Train net output #1: loss = 0.560654 (* 1 = 0.560654 loss)
I1107 10:43:44.762795 11160 sgd_solver.cpp:105] Iteration 17100, lr = 0.1
I1107 10:43:53.274909 11160 solver.cpp:218] Iteration 17200 (11.7483 iter/s, 8.5119s/100 iters), loss = 0.593443
I1107 10:43:53.274909 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:43:53.274909 11160 solver.cpp:237]     Train net output #1: loss = 0.593443 (* 1 = 0.593443 loss)
I1107 10:43:53.274909 11160 sgd_solver.cpp:105] Iteration 17200, lr = 0.1
I1107 10:44:01.792914 11160 solver.cpp:218] Iteration 17300 (11.7404 iter/s, 8.51759s/100 iters), loss = 0.605048
I1107 10:44:01.792914 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:44:01.792914 11160 solver.cpp:237]     Train net output #1: loss = 0.605048 (* 1 = 0.605048 loss)
I1107 10:44:01.792914 11160 sgd_solver.cpp:105] Iteration 17300, lr = 0.1
I1107 10:44:10.309496 11160 solver.cpp:218] Iteration 17400 (11.7429 iter/s, 8.51576s/100 iters), loss = 0.586633
I1107 10:44:10.309496 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:44:10.309496 11160 solver.cpp:237]     Train net output #1: loss = 0.586633 (* 1 = 0.586633 loss)
I1107 10:44:10.309496 11160 sgd_solver.cpp:105] Iteration 17400, lr = 0.1
I1107 10:44:18.408998  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:44:18.746497 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_17500.caffemodel
I1107 10:44:18.787497 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_17500.solverstate
I1107 10:44:18.796497 11160 solver.cpp:330] Iteration 17500, Testing net (#0)
I1107 10:44:18.796998 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:44:20.789052 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:44:20.868553 11160 solver.cpp:397]     Test net output #0: accuracy = 0.666
I1107 10:44:20.868553 11160 solver.cpp:397]     Test net output #1: loss = 0.981334 (* 1 = 0.981334 loss)
I1107 10:44:20.949053 11160 solver.cpp:218] Iteration 17500 (9.3991 iter/s, 10.6393s/100 iters), loss = 0.681465
I1107 10:44:20.949053 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1107 10:44:20.949053 11160 solver.cpp:237]     Train net output #1: loss = 0.681465 (* 1 = 0.681465 loss)
I1107 10:44:20.949053 11160 sgd_solver.cpp:105] Iteration 17500, lr = 0.1
I1107 10:44:29.460103 11160 solver.cpp:218] Iteration 17600 (11.75 iter/s, 8.51064s/100 iters), loss = 0.442005
I1107 10:44:29.460603 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1107 10:44:29.460603 11160 solver.cpp:237]     Train net output #1: loss = 0.442005 (* 1 = 0.442005 loss)
I1107 10:44:29.460603 11160 sgd_solver.cpp:105] Iteration 17600, lr = 0.1
I1107 10:44:37.965423 11160 solver.cpp:218] Iteration 17700 (11.7581 iter/s, 8.50476s/100 iters), loss = 0.484819
I1107 10:44:37.965423 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 10:44:37.965924 11160 solver.cpp:237]     Train net output #1: loss = 0.484819 (* 1 = 0.484819 loss)
I1107 10:44:37.965924 11160 sgd_solver.cpp:105] Iteration 17700, lr = 0.1
I1107 10:44:46.474520 11160 solver.cpp:218] Iteration 17800 (11.7534 iter/s, 8.50821s/100 iters), loss = 0.499614
I1107 10:44:46.474520 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 10:44:46.474520 11160 solver.cpp:237]     Train net output #1: loss = 0.499614 (* 1 = 0.499614 loss)
I1107 10:44:46.474520 11160 sgd_solver.cpp:105] Iteration 17800, lr = 0.1
I1107 10:44:54.979856 11160 solver.cpp:218] Iteration 17900 (11.7577 iter/s, 8.50506s/100 iters), loss = 0.663898
I1107 10:44:54.979856 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:44:54.979856 11160 solver.cpp:237]     Train net output #1: loss = 0.663898 (* 1 = 0.663898 loss)
I1107 10:44:54.979856 11160 sgd_solver.cpp:105] Iteration 17900, lr = 0.1
I1107 10:45:03.067602  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:45:03.404103 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_18000.caffemodel
I1107 10:45:03.434602 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_18000.solverstate
I1107 10:45:03.466603 11160 solver.cpp:330] Iteration 18000, Testing net (#0)
I1107 10:45:03.467103 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:45:05.455102 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:45:05.535102 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5601
I1107 10:45:05.535102 11160 solver.cpp:397]     Test net output #1: loss = 1.38533 (* 1 = 1.38533 loss)
I1107 10:45:05.616612 11160 solver.cpp:218] Iteration 18000 (9.40183 iter/s, 10.6362s/100 iters), loss = 0.625548
I1107 10:45:05.616612 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:45:05.616612 11160 solver.cpp:237]     Train net output #1: loss = 0.625548 (* 1 = 0.625548 loss)
I1107 10:45:05.616612 11160 sgd_solver.cpp:105] Iteration 18000, lr = 0.1
I1107 10:45:14.125006 11160 solver.cpp:218] Iteration 18100 (11.7538 iter/s, 8.50792s/100 iters), loss = 0.525838
I1107 10:45:14.125006 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 10:45:14.125006 11160 solver.cpp:237]     Train net output #1: loss = 0.525838 (* 1 = 0.525838 loss)
I1107 10:45:14.125006 11160 sgd_solver.cpp:105] Iteration 18100, lr = 0.1
I1107 10:45:22.636183 11160 solver.cpp:218] Iteration 18200 (11.7503 iter/s, 8.51039s/100 iters), loss = 0.678421
I1107 10:45:22.636183 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:45:22.636183 11160 solver.cpp:237]     Train net output #1: loss = 0.678421 (* 1 = 0.678421 loss)
I1107 10:45:22.636183 11160 sgd_solver.cpp:105] Iteration 18200, lr = 0.1
I1107 10:45:31.149905 11160 solver.cpp:218] Iteration 18300 (11.7465 iter/s, 8.5132s/100 iters), loss = 0.594709
I1107 10:45:31.149905 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:45:31.149905 11160 solver.cpp:237]     Train net output #1: loss = 0.594709 (* 1 = 0.594709 loss)
I1107 10:45:31.149905 11160 sgd_solver.cpp:105] Iteration 18300, lr = 0.1
I1107 10:45:39.649778 11160 solver.cpp:218] Iteration 18400 (11.7655 iter/s, 8.49942s/100 iters), loss = 0.674019
I1107 10:45:39.649778 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:45:39.649778 11160 solver.cpp:237]     Train net output #1: loss = 0.674019 (* 1 = 0.674019 loss)
I1107 10:45:39.649778 11160 sgd_solver.cpp:105] Iteration 18400, lr = 0.1
I1107 10:45:47.755280  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:45:48.092779 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_18500.caffemodel
I1107 10:45:48.119778 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_18500.solverstate
I1107 10:45:48.128779 11160 solver.cpp:330] Iteration 18500, Testing net (#0)
I1107 10:45:48.128779 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:45:50.118279 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:45:50.197783 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6694
I1107 10:45:50.197783 11160 solver.cpp:397]     Test net output #1: loss = 0.986942 (* 1 = 0.986942 loss)
I1107 10:45:50.279278 11160 solver.cpp:218] Iteration 18500 (9.40849 iter/s, 10.6287s/100 iters), loss = 0.631558
I1107 10:45:50.279278 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:45:50.279278 11160 solver.cpp:237]     Train net output #1: loss = 0.631558 (* 1 = 0.631558 loss)
I1107 10:45:50.279278 11160 sgd_solver.cpp:105] Iteration 18500, lr = 0.1
I1107 10:45:58.798920 11160 solver.cpp:218] Iteration 18600 (11.7377 iter/s, 8.51955s/100 iters), loss = 0.674491
I1107 10:45:58.799422 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:45:58.799422 11160 solver.cpp:237]     Train net output #1: loss = 0.674491 (* 1 = 0.674491 loss)
I1107 10:45:58.799422 11160 sgd_solver.cpp:105] Iteration 18600, lr = 0.1
I1107 10:46:07.314060 11160 solver.cpp:218] Iteration 18700 (11.7447 iter/s, 8.51449s/100 iters), loss = 0.701244
I1107 10:46:07.314560 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:46:07.314560 11160 solver.cpp:237]     Train net output #1: loss = 0.701244 (* 1 = 0.701244 loss)
I1107 10:46:07.314560 11160 sgd_solver.cpp:105] Iteration 18700, lr = 0.1
I1107 10:46:15.843096 11160 solver.cpp:218] Iteration 18800 (11.7262 iter/s, 8.52792s/100 iters), loss = 0.513024
I1107 10:46:15.843096 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 10:46:15.843096 11160 solver.cpp:237]     Train net output #1: loss = 0.513024 (* 1 = 0.513024 loss)
I1107 10:46:15.843096 11160 sgd_solver.cpp:105] Iteration 18800, lr = 0.1
I1107 10:46:24.358119 11160 solver.cpp:218] Iteration 18900 (11.7441 iter/s, 8.51492s/100 iters), loss = 0.616119
I1107 10:46:24.358619 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:46:24.358619 11160 solver.cpp:237]     Train net output #1: loss = 0.616119 (* 1 = 0.616119 loss)
I1107 10:46:24.358619 11160 sgd_solver.cpp:105] Iteration 18900, lr = 0.1
I1107 10:46:32.458204  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:46:32.794703 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_19000.caffemodel
I1107 10:46:32.824702 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_19000.solverstate
I1107 10:46:32.850203 11160 solver.cpp:330] Iteration 19000, Testing net (#0)
I1107 10:46:32.850203 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:46:34.839202 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:46:34.918701 11160 solver.cpp:397]     Test net output #0: accuracy = 0.41
I1107 10:46:34.918701 11160 solver.cpp:397]     Test net output #1: loss = 2.01565 (* 1 = 2.01565 loss)
I1107 10:46:34.999701 11160 solver.cpp:218] Iteration 19000 (9.39758 iter/s, 10.641s/100 iters), loss = 0.739668
I1107 10:46:35.000202 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1107 10:46:35.000202 11160 solver.cpp:237]     Train net output #1: loss = 0.739668 (* 1 = 0.739668 loss)
I1107 10:46:35.000202 11160 sgd_solver.cpp:105] Iteration 19000, lr = 0.1
I1107 10:46:43.510818 11160 solver.cpp:218] Iteration 19100 (11.7507 iter/s, 8.51011s/100 iters), loss = 0.525045
I1107 10:46:43.510818 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 10:46:43.510818 11160 solver.cpp:237]     Train net output #1: loss = 0.525045 (* 1 = 0.525045 loss)
I1107 10:46:43.510818 11160 sgd_solver.cpp:105] Iteration 19100, lr = 0.1
I1107 10:46:52.023458 11160 solver.cpp:218] Iteration 19200 (11.7479 iter/s, 8.51219s/100 iters), loss = 0.748462
I1107 10:46:52.023458 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:46:52.023458 11160 solver.cpp:237]     Train net output #1: loss = 0.748462 (* 1 = 0.748462 loss)
I1107 10:46:52.023458 11160 sgd_solver.cpp:105] Iteration 19200, lr = 0.1
I1107 10:47:00.531957 11160 solver.cpp:218] Iteration 19300 (11.7539 iter/s, 8.50783s/100 iters), loss = 0.533649
I1107 10:47:00.531957 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:47:00.531957 11160 solver.cpp:237]     Train net output #1: loss = 0.533649 (* 1 = 0.533649 loss)
I1107 10:47:00.531957 11160 sgd_solver.cpp:105] Iteration 19300, lr = 0.1
I1107 10:47:09.032038 11160 solver.cpp:218] Iteration 19400 (11.7651 iter/s, 8.4997s/100 iters), loss = 0.592909
I1107 10:47:09.032038 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:47:09.032038 11160 solver.cpp:237]     Train net output #1: loss = 0.592909 (* 1 = 0.592909 loss)
I1107 10:47:09.032038 11160 sgd_solver.cpp:105] Iteration 19400, lr = 0.1
I1107 10:47:17.120101  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:47:17.456598 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_19500.caffemodel
I1107 10:47:17.485599 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_19500.solverstate
I1107 10:47:17.494101 11160 solver.cpp:330] Iteration 19500, Testing net (#0)
I1107 10:47:17.494599 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:47:19.484598 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:47:19.563598 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5338
I1107 10:47:19.563598 11160 solver.cpp:397]     Test net output #1: loss = 1.52595 (* 1 = 1.52595 loss)
I1107 10:47:19.645099 11160 solver.cpp:218] Iteration 19500 (9.42273 iter/s, 10.6126s/100 iters), loss = 0.621575
I1107 10:47:19.645099 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:47:19.645099 11160 solver.cpp:237]     Train net output #1: loss = 0.621575 (* 1 = 0.621575 loss)
I1107 10:47:19.645099 11160 sgd_solver.cpp:105] Iteration 19500, lr = 0.1
I1107 10:47:28.150734 11160 solver.cpp:218] Iteration 19600 (11.7579 iter/s, 8.50495s/100 iters), loss = 0.561717
I1107 10:47:28.150734 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:47:28.150734 11160 solver.cpp:237]     Train net output #1: loss = 0.561717 (* 1 = 0.561717 loss)
I1107 10:47:28.150734 11160 sgd_solver.cpp:105] Iteration 19600, lr = 0.1
I1107 10:47:36.667325 11160 solver.cpp:218] Iteration 19700 (11.7423 iter/s, 8.51624s/100 iters), loss = 0.710347
I1107 10:47:36.667325 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:47:36.667325 11160 solver.cpp:237]     Train net output #1: loss = 0.710347 (* 1 = 0.710347 loss)
I1107 10:47:36.667325 11160 sgd_solver.cpp:105] Iteration 19700, lr = 0.1
I1107 10:47:45.191510 11160 solver.cpp:218] Iteration 19800 (11.7319 iter/s, 8.52374s/100 iters), loss = 0.628424
I1107 10:47:45.191510 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:47:45.191510 11160 solver.cpp:237]     Train net output #1: loss = 0.628424 (* 1 = 0.628424 loss)
I1107 10:47:45.191510 11160 sgd_solver.cpp:105] Iteration 19800, lr = 0.1
I1107 10:47:53.699424 11160 solver.cpp:218] Iteration 19900 (11.7547 iter/s, 8.5072s/100 iters), loss = 0.580172
I1107 10:47:53.699424 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:47:53.699424 11160 solver.cpp:237]     Train net output #1: loss = 0.580172 (* 1 = 0.580172 loss)
I1107 10:47:53.699424 11160 sgd_solver.cpp:105] Iteration 19900, lr = 0.1
I1107 10:48:01.795939  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:48:02.131439 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_20000.caffemodel
I1107 10:48:02.157939 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_20000.solverstate
I1107 10:48:02.166937 11160 solver.cpp:330] Iteration 20000, Testing net (#0)
I1107 10:48:02.166937 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:48:04.153980 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:48:04.233979 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6792
I1107 10:48:04.233979 11160 solver.cpp:397]     Test net output #1: loss = 0.98829 (* 1 = 0.98829 loss)
I1107 10:48:04.314980 11160 solver.cpp:218] Iteration 20000 (9.42065 iter/s, 10.615s/100 iters), loss = 0.635723
I1107 10:48:04.314980 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:48:04.314980 11160 solver.cpp:237]     Train net output #1: loss = 0.635723 (* 1 = 0.635723 loss)
I1107 10:48:04.314980 11160 sgd_solver.cpp:105] Iteration 20000, lr = 0.1
I1107 10:48:12.824352 11160 solver.cpp:218] Iteration 20100 (11.7526 iter/s, 8.50877s/100 iters), loss = 0.450081
I1107 10:48:12.824352 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 10:48:12.824352 11160 solver.cpp:237]     Train net output #1: loss = 0.450081 (* 1 = 0.450081 loss)
I1107 10:48:12.824352 11160 sgd_solver.cpp:105] Iteration 20100, lr = 0.1
I1107 10:48:21.343390 11160 solver.cpp:218] Iteration 20200 (11.7394 iter/s, 8.51829s/100 iters), loss = 0.647586
I1107 10:48:21.343390 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 10:48:21.343390 11160 solver.cpp:237]     Train net output #1: loss = 0.647586 (* 1 = 0.647586 loss)
I1107 10:48:21.343390 11160 sgd_solver.cpp:105] Iteration 20200, lr = 0.1
I1107 10:48:29.866461 11160 solver.cpp:218] Iteration 20300 (11.7334 iter/s, 8.52265s/100 iters), loss = 0.557214
I1107 10:48:29.866461 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:48:29.866461 11160 solver.cpp:237]     Train net output #1: loss = 0.557214 (* 1 = 0.557214 loss)
I1107 10:48:29.866461 11160 sgd_solver.cpp:105] Iteration 20300, lr = 0.1
I1107 10:48:38.385154 11160 solver.cpp:218] Iteration 20400 (11.7398 iter/s, 8.518s/100 iters), loss = 0.602074
I1107 10:48:38.385154 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:48:38.385154 11160 solver.cpp:237]     Train net output #1: loss = 0.602074 (* 1 = 0.602074 loss)
I1107 10:48:38.385154 11160 sgd_solver.cpp:105] Iteration 20400, lr = 0.1
I1107 10:48:46.492317  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:48:46.829319 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_20500.caffemodel
I1107 10:48:46.859318 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_20500.solverstate
I1107 10:48:46.868319 11160 solver.cpp:330] Iteration 20500, Testing net (#0)
I1107 10:48:46.868319 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:48:48.857317 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:48:48.936818 11160 solver.cpp:397]     Test net output #0: accuracy = 0.7187
I1107 10:48:48.936818 11160 solver.cpp:397]     Test net output #1: loss = 0.838458 (* 1 = 0.838458 loss)
I1107 10:48:49.017817 11160 solver.cpp:218] Iteration 20500 (9.40523 iter/s, 10.6324s/100 iters), loss = 0.589956
I1107 10:48:49.017817 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:48:49.017817 11160 solver.cpp:237]     Train net output #1: loss = 0.589956 (* 1 = 0.589956 loss)
I1107 10:48:49.017817 11160 sgd_solver.cpp:105] Iteration 20500, lr = 0.1
I1107 10:48:57.532361 11160 solver.cpp:218] Iteration 20600 (11.7456 iter/s, 8.51382s/100 iters), loss = 0.561203
I1107 10:48:57.532361 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 10:48:57.532361 11160 solver.cpp:237]     Train net output #1: loss = 0.561203 (* 1 = 0.561203 loss)
I1107 10:48:57.532361 11160 sgd_solver.cpp:105] Iteration 20600, lr = 0.1
I1107 10:49:06.054527 11160 solver.cpp:218] Iteration 20700 (11.735 iter/s, 8.52153s/100 iters), loss = 0.608816
I1107 10:49:06.054527 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:49:06.054527 11160 solver.cpp:237]     Train net output #1: loss = 0.608816 (* 1 = 0.608816 loss)
I1107 10:49:06.054527 11160 sgd_solver.cpp:105] Iteration 20700, lr = 0.1
I1107 10:49:14.567363 11160 solver.cpp:218] Iteration 20800 (11.7476 iter/s, 8.5124s/100 iters), loss = 0.613325
I1107 10:49:14.567363 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:49:14.567363 11160 solver.cpp:237]     Train net output #1: loss = 0.613325 (* 1 = 0.613325 loss)
I1107 10:49:14.567363 11160 sgd_solver.cpp:105] Iteration 20800, lr = 0.1
I1107 10:49:23.082384 11160 solver.cpp:218] Iteration 20900 (11.7445 iter/s, 8.51465s/100 iters), loss = 0.497066
I1107 10:49:23.082384 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 10:49:23.082384 11160 solver.cpp:237]     Train net output #1: loss = 0.497066 (* 1 = 0.497066 loss)
I1107 10:49:23.082384 11160 sgd_solver.cpp:105] Iteration 20900, lr = 0.1
I1107 10:49:31.179428  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:49:31.516927 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_21000.caffemodel
I1107 10:49:31.543927 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_21000.solverstate
I1107 10:49:31.552927 11160 solver.cpp:330] Iteration 21000, Testing net (#0)
I1107 10:49:31.552927 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:49:33.538985 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:49:33.618499 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6083
I1107 10:49:33.618499 11160 solver.cpp:397]     Test net output #1: loss = 1.19295 (* 1 = 1.19295 loss)
I1107 10:49:33.699983 11160 solver.cpp:218] Iteration 21000 (9.41872 iter/s, 10.6171s/100 iters), loss = 0.711059
I1107 10:49:33.700484 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1107 10:49:33.700484 11160 solver.cpp:237]     Train net output #1: loss = 0.711059 (* 1 = 0.711059 loss)
I1107 10:49:33.700484 11160 sgd_solver.cpp:105] Iteration 21000, lr = 0.1
I1107 10:49:42.208510 11160 solver.cpp:218] Iteration 21100 (11.7539 iter/s, 8.50784s/100 iters), loss = 0.663427
I1107 10:49:42.208510 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:49:42.208510 11160 solver.cpp:237]     Train net output #1: loss = 0.663427 (* 1 = 0.663427 loss)
I1107 10:49:42.208510 11160 sgd_solver.cpp:105] Iteration 21100, lr = 0.1
I1107 10:49:50.722512 11160 solver.cpp:218] Iteration 21200 (11.746 iter/s, 8.51355s/100 iters), loss = 0.692331
I1107 10:49:50.722512 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:49:50.722512 11160 solver.cpp:237]     Train net output #1: loss = 0.692331 (* 1 = 0.692331 loss)
I1107 10:49:50.722512 11160 sgd_solver.cpp:105] Iteration 21200, lr = 0.1
I1107 10:49:59.232594 11160 solver.cpp:218] Iteration 21300 (11.7517 iter/s, 8.5094s/100 iters), loss = 0.639834
I1107 10:49:59.232594 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:49:59.232594 11160 solver.cpp:237]     Train net output #1: loss = 0.639834 (* 1 = 0.639834 loss)
I1107 10:49:59.232594 11160 sgd_solver.cpp:105] Iteration 21300, lr = 0.1
I1107 10:50:07.763123 11160 solver.cpp:218] Iteration 21400 (11.7234 iter/s, 8.52993s/100 iters), loss = 0.578491
I1107 10:50:07.763123 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:50:07.763123 11160 solver.cpp:237]     Train net output #1: loss = 0.578491 (* 1 = 0.578491 loss)
I1107 10:50:07.763123 11160 sgd_solver.cpp:105] Iteration 21400, lr = 0.1
I1107 10:50:15.859573  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:50:16.196573 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_21500.caffemodel
I1107 10:50:16.226572 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_21500.solverstate
I1107 10:50:16.235574 11160 solver.cpp:330] Iteration 21500, Testing net (#0)
I1107 10:50:16.235574 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:50:18.225073 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:50:18.305073 11160 solver.cpp:397]     Test net output #0: accuracy = 0.4725
I1107 10:50:18.305073 11160 solver.cpp:397]     Test net output #1: loss = 1.83146 (* 1 = 1.83146 loss)
I1107 10:50:18.386071 11160 solver.cpp:218] Iteration 21500 (9.41415 iter/s, 10.6223s/100 iters), loss = 0.63065
I1107 10:50:18.386071 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:50:18.386071 11160 solver.cpp:237]     Train net output #1: loss = 0.63065 (* 1 = 0.63065 loss)
I1107 10:50:18.386071 11160 sgd_solver.cpp:105] Iteration 21500, lr = 0.1
I1107 10:50:26.894295 11160 solver.cpp:218] Iteration 21600 (11.7537 iter/s, 8.50795s/100 iters), loss = 0.497352
I1107 10:50:26.894295 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:50:26.894295 11160 solver.cpp:237]     Train net output #1: loss = 0.497352 (* 1 = 0.497352 loss)
I1107 10:50:26.894295 11160 sgd_solver.cpp:105] Iteration 21600, lr = 0.1
I1107 10:50:35.395426 11160 solver.cpp:218] Iteration 21700 (11.7642 iter/s, 8.50033s/100 iters), loss = 0.679141
I1107 10:50:35.395426 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:50:35.395426 11160 solver.cpp:237]     Train net output #1: loss = 0.679141 (* 1 = 0.679141 loss)
I1107 10:50:35.395426 11160 sgd_solver.cpp:105] Iteration 21700, lr = 0.1
I1107 10:50:43.913482 11160 solver.cpp:218] Iteration 21800 (11.7403 iter/s, 8.51765s/100 iters), loss = 0.597228
I1107 10:50:43.913482 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:50:43.913482 11160 solver.cpp:237]     Train net output #1: loss = 0.597228 (* 1 = 0.597228 loss)
I1107 10:50:43.913482 11160 sgd_solver.cpp:105] Iteration 21800, lr = 0.1
I1107 10:50:52.422982 11160 solver.cpp:218] Iteration 21900 (11.7526 iter/s, 8.50879s/100 iters), loss = 0.639244
I1107 10:50:52.422982 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:50:52.422982 11160 solver.cpp:237]     Train net output #1: loss = 0.639244 (* 1 = 0.639244 loss)
I1107 10:50:52.422982 11160 sgd_solver.cpp:105] Iteration 21900, lr = 0.1
I1107 10:51:00.511092  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:51:00.849092 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_22000.caffemodel
I1107 10:51:00.878592 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_22000.solverstate
I1107 10:51:00.887591 11160 solver.cpp:330] Iteration 22000, Testing net (#0)
I1107 10:51:00.887591 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:51:02.875092 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:51:02.954591 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6489
I1107 10:51:02.954591 11160 solver.cpp:397]     Test net output #1: loss = 1.05849 (* 1 = 1.05849 loss)
I1107 10:51:03.037091 11160 solver.cpp:218] Iteration 22000 (9.42194 iter/s, 10.6135s/100 iters), loss = 0.564566
I1107 10:51:03.037091 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:51:03.037091 11160 solver.cpp:237]     Train net output #1: loss = 0.564566 (* 1 = 0.564566 loss)
I1107 10:51:03.037091 11160 sgd_solver.cpp:105] Iteration 22000, lr = 0.1
I1107 10:51:11.549197 11160 solver.cpp:218] Iteration 22100 (11.7483 iter/s, 8.51189s/100 iters), loss = 0.726639
I1107 10:51:11.549197 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:51:11.549197 11160 solver.cpp:237]     Train net output #1: loss = 0.726639 (* 1 = 0.726639 loss)
I1107 10:51:11.549197 11160 sgd_solver.cpp:105] Iteration 22100, lr = 0.1
I1107 10:51:20.054729 11160 solver.cpp:218] Iteration 22200 (11.7579 iter/s, 8.50489s/100 iters), loss = 0.625608
I1107 10:51:20.054729 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:51:20.054729 11160 solver.cpp:237]     Train net output #1: loss = 0.625608 (* 1 = 0.625608 loss)
I1107 10:51:20.054729 11160 sgd_solver.cpp:105] Iteration 22200, lr = 0.1
I1107 10:51:28.555830 11160 solver.cpp:218] Iteration 22300 (11.764 iter/s, 8.50051s/100 iters), loss = 0.528986
I1107 10:51:28.555830 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:51:28.555830 11160 solver.cpp:237]     Train net output #1: loss = 0.528986 (* 1 = 0.528986 loss)
I1107 10:51:28.555830 11160 sgd_solver.cpp:105] Iteration 22300, lr = 0.1
I1107 10:51:37.077831 11160 solver.cpp:218] Iteration 22400 (11.7353 iter/s, 8.52132s/100 iters), loss = 0.468006
I1107 10:51:37.077831 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 10:51:37.077831 11160 solver.cpp:237]     Train net output #1: loss = 0.468006 (* 1 = 0.468006 loss)
I1107 10:51:37.077831 11160 sgd_solver.cpp:105] Iteration 22400, lr = 0.1
I1107 10:51:45.179847  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:51:45.516345 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_22500.caffemodel
I1107 10:51:45.572845 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_22500.solverstate
I1107 10:51:45.581845 11160 solver.cpp:330] Iteration 22500, Testing net (#0)
I1107 10:51:45.581845 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:51:47.573348 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:51:47.651346 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5379
I1107 10:51:47.651346 11160 solver.cpp:397]     Test net output #1: loss = 1.6244 (* 1 = 1.6244 loss)
I1107 10:51:47.732344 11160 solver.cpp:218] Iteration 22500 (9.38587 iter/s, 10.6543s/100 iters), loss = 0.740922
I1107 10:51:47.732344 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:51:47.732846 11160 solver.cpp:237]     Train net output #1: loss = 0.740922 (* 1 = 0.740922 loss)
I1107 10:51:47.732846 11160 sgd_solver.cpp:105] Iteration 22500, lr = 0.1
I1107 10:51:56.255522 11160 solver.cpp:218] Iteration 22600 (11.7334 iter/s, 8.52267s/100 iters), loss = 0.426612
I1107 10:51:56.256022 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1107 10:51:56.256022 11160 solver.cpp:237]     Train net output #1: loss = 0.426612 (* 1 = 0.426612 loss)
I1107 10:51:56.256022 11160 sgd_solver.cpp:105] Iteration 22600, lr = 0.1
I1107 10:52:04.774022 11160 solver.cpp:218] Iteration 22700 (11.7399 iter/s, 8.51797s/100 iters), loss = 0.644731
I1107 10:52:04.774523 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:52:04.774523 11160 solver.cpp:237]     Train net output #1: loss = 0.644731 (* 1 = 0.644731 loss)
I1107 10:52:04.774523 11160 sgd_solver.cpp:105] Iteration 22700, lr = 0.1
I1107 10:52:13.289155 11160 solver.cpp:218] Iteration 22800 (11.7447 iter/s, 8.51445s/100 iters), loss = 0.572211
I1107 10:52:13.289155 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:52:13.289155 11160 solver.cpp:237]     Train net output #1: loss = 0.572211 (* 1 = 0.572211 loss)
I1107 10:52:13.289155 11160 sgd_solver.cpp:105] Iteration 22800, lr = 0.1
I1107 10:52:21.799934 11160 solver.cpp:218] Iteration 22900 (11.7505 iter/s, 8.51026s/100 iters), loss = 0.587515
I1107 10:52:21.799934 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:52:21.799934 11160 solver.cpp:237]     Train net output #1: loss = 0.587515 (* 1 = 0.587515 loss)
I1107 10:52:21.799934 11160 sgd_solver.cpp:105] Iteration 22900, lr = 0.1
I1107 10:52:29.900442  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:52:30.235942 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_23000.caffemodel
I1107 10:52:30.269942 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_23000.solverstate
I1107 10:52:30.291942 11160 solver.cpp:330] Iteration 23000, Testing net (#0)
I1107 10:52:30.292443 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:52:32.280942 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:52:32.360942 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6808
I1107 10:52:32.360942 11160 solver.cpp:397]     Test net output #1: loss = 0.954493 (* 1 = 0.954493 loss)
I1107 10:52:32.441442 11160 solver.cpp:218] Iteration 23000 (9.39763 iter/s, 10.641s/100 iters), loss = 0.627348
I1107 10:52:32.441442 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:52:32.441442 11160 solver.cpp:237]     Train net output #1: loss = 0.627348 (* 1 = 0.627348 loss)
I1107 10:52:32.441442 11160 sgd_solver.cpp:105] Iteration 23000, lr = 0.1
I1107 10:52:40.963651 11160 solver.cpp:218] Iteration 23100 (11.7351 iter/s, 8.52141s/100 iters), loss = 0.50548
I1107 10:52:40.963651 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 10:52:40.963651 11160 solver.cpp:237]     Train net output #1: loss = 0.50548 (* 1 = 0.50548 loss)
I1107 10:52:40.963651 11160 sgd_solver.cpp:105] Iteration 23100, lr = 0.1
I1107 10:52:49.478152 11160 solver.cpp:218] Iteration 23200 (11.7448 iter/s, 8.51437s/100 iters), loss = 0.592608
I1107 10:52:49.478652 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:52:49.478652 11160 solver.cpp:237]     Train net output #1: loss = 0.592608 (* 1 = 0.592608 loss)
I1107 10:52:49.478652 11160 sgd_solver.cpp:105] Iteration 23200, lr = 0.1
I1107 10:52:57.998808 11160 solver.cpp:218] Iteration 23300 (11.7369 iter/s, 8.52013s/100 iters), loss = 0.632838
I1107 10:52:57.999307 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:52:57.999307 11160 solver.cpp:237]     Train net output #1: loss = 0.632838 (* 1 = 0.632838 loss)
I1107 10:52:57.999307 11160 sgd_solver.cpp:105] Iteration 23300, lr = 0.1
I1107 10:53:06.523393 11160 solver.cpp:218] Iteration 23400 (11.7316 iter/s, 8.52399s/100 iters), loss = 0.661073
I1107 10:53:06.523893 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:53:06.523893 11160 solver.cpp:237]     Train net output #1: loss = 0.661073 (* 1 = 0.661073 loss)
I1107 10:53:06.523893 11160 sgd_solver.cpp:105] Iteration 23400, lr = 0.1
I1107 10:53:14.626468  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:53:14.964488 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_23500.caffemodel
I1107 10:53:14.995499 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_23500.solverstate
I1107 10:53:15.016492 11160 solver.cpp:330] Iteration 23500, Testing net (#0)
I1107 10:53:15.016492 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:53:17.006093 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:53:17.086093 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6037
I1107 10:53:17.086093 11160 solver.cpp:397]     Test net output #1: loss = 1.18165 (* 1 = 1.18165 loss)
I1107 10:53:17.166592 11160 solver.cpp:218] Iteration 23500 (9.39621 iter/s, 10.6426s/100 iters), loss = 0.675684
I1107 10:53:17.167093 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:53:17.167093 11160 solver.cpp:237]     Train net output #1: loss = 0.675684 (* 1 = 0.675684 loss)
I1107 10:53:17.167093 11160 sgd_solver.cpp:105] Iteration 23500, lr = 0.1
I1107 10:53:25.677464 11160 solver.cpp:218] Iteration 23600 (11.7507 iter/s, 8.51011s/100 iters), loss = 0.679837
I1107 10:53:25.677464 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:53:25.677464 11160 solver.cpp:237]     Train net output #1: loss = 0.679837 (* 1 = 0.679837 loss)
I1107 10:53:25.677464 11160 sgd_solver.cpp:105] Iteration 23600, lr = 0.1
I1107 10:53:34.194963 11160 solver.cpp:218] Iteration 23700 (11.7415 iter/s, 8.51681s/100 iters), loss = 0.656548
I1107 10:53:34.194963 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:53:34.194963 11160 solver.cpp:237]     Train net output #1: loss = 0.656548 (* 1 = 0.656548 loss)
I1107 10:53:34.194963 11160 sgd_solver.cpp:105] Iteration 23700, lr = 0.1
I1107 10:53:42.711525 11160 solver.cpp:218] Iteration 23800 (11.7424 iter/s, 8.51618s/100 iters), loss = 0.550855
I1107 10:53:42.711525 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 10:53:42.711525 11160 solver.cpp:237]     Train net output #1: loss = 0.550855 (* 1 = 0.550855 loss)
I1107 10:53:42.711525 11160 sgd_solver.cpp:105] Iteration 23800, lr = 0.1
I1107 10:53:51.227632 11160 solver.cpp:218] Iteration 23900 (11.7431 iter/s, 8.51564s/100 iters), loss = 0.624852
I1107 10:53:51.227632 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:53:51.227632 11160 solver.cpp:237]     Train net output #1: loss = 0.624852 (* 1 = 0.624852 loss)
I1107 10:53:51.227632 11160 sgd_solver.cpp:105] Iteration 23900, lr = 0.1
I1107 10:53:59.324714  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:53:59.661315 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_24000.caffemodel
I1107 10:53:59.688815 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_24000.solverstate
I1107 10:53:59.697315 11160 solver.cpp:330] Iteration 24000, Testing net (#0)
I1107 10:53:59.697815 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:54:01.689316 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:54:01.768815 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6808
I1107 10:54:01.768815 11160 solver.cpp:397]     Test net output #1: loss = 0.946501 (* 1 = 0.946501 loss)
I1107 10:54:01.849814 11160 solver.cpp:218] Iteration 24000 (9.41489 iter/s, 10.6215s/100 iters), loss = 0.679069
I1107 10:54:01.849814 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 10:54:01.849814 11160 solver.cpp:237]     Train net output #1: loss = 0.679069 (* 1 = 0.679069 loss)
I1107 10:54:01.849814 11160 sgd_solver.cpp:105] Iteration 24000, lr = 0.1
I1107 10:54:10.365059 11160 solver.cpp:218] Iteration 24100 (11.7445 iter/s, 8.51464s/100 iters), loss = 0.531915
I1107 10:54:10.365059 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:54:10.365059 11160 solver.cpp:237]     Train net output #1: loss = 0.531915 (* 1 = 0.531915 loss)
I1107 10:54:10.365059 11160 sgd_solver.cpp:105] Iteration 24100, lr = 0.1
I1107 10:54:18.875691 11160 solver.cpp:218] Iteration 24200 (11.7508 iter/s, 8.51004s/100 iters), loss = 0.773632
I1107 10:54:18.875691 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1107 10:54:18.875691 11160 solver.cpp:237]     Train net output #1: loss = 0.773632 (* 1 = 0.773632 loss)
I1107 10:54:18.875691 11160 sgd_solver.cpp:105] Iteration 24200, lr = 0.1
I1107 10:54:27.385181 11160 solver.cpp:218] Iteration 24300 (11.7521 iter/s, 8.5091s/100 iters), loss = 0.538788
I1107 10:54:27.385181 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 10:54:27.385181 11160 solver.cpp:237]     Train net output #1: loss = 0.538788 (* 1 = 0.538788 loss)
I1107 10:54:27.385181 11160 sgd_solver.cpp:105] Iteration 24300, lr = 0.1
I1107 10:54:35.903592 11160 solver.cpp:218] Iteration 24400 (11.74 iter/s, 8.51792s/100 iters), loss = 0.510747
I1107 10:54:35.903592 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:54:35.903592 11160 solver.cpp:237]     Train net output #1: loss = 0.510747 (* 1 = 0.510747 loss)
I1107 10:54:35.903592 11160 sgd_solver.cpp:105] Iteration 24400, lr = 0.1
I1107 10:54:44.000247  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:54:44.334748 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_24500.caffemodel
I1107 10:54:44.387744 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_24500.solverstate
I1107 10:54:44.396745 11160 solver.cpp:330] Iteration 24500, Testing net (#0)
I1107 10:54:44.396745 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:54:46.385246 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:54:46.465246 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6001
I1107 10:54:46.465246 11160 solver.cpp:397]     Test net output #1: loss = 1.24192 (* 1 = 1.24192 loss)
I1107 10:54:46.546244 11160 solver.cpp:218] Iteration 24500 (9.39664 iter/s, 10.6421s/100 iters), loss = 0.565885
I1107 10:54:46.546244 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:54:46.546244 11160 solver.cpp:237]     Train net output #1: loss = 0.565885 (* 1 = 0.565885 loss)
I1107 10:54:46.546244 11160 sgd_solver.cpp:105] Iteration 24500, lr = 0.1
I1107 10:54:55.066795 11160 solver.cpp:218] Iteration 24600 (11.7367 iter/s, 8.52027s/100 iters), loss = 0.500351
I1107 10:54:55.067296 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 10:54:55.067296 11160 solver.cpp:237]     Train net output #1: loss = 0.500351 (* 1 = 0.500351 loss)
I1107 10:54:55.067296 11160 sgd_solver.cpp:105] Iteration 24600, lr = 0.1
I1107 10:55:03.582423 11160 solver.cpp:218] Iteration 24700 (11.7444 iter/s, 8.51472s/100 iters), loss = 0.54745
I1107 10:55:03.582423 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:55:03.582423 11160 solver.cpp:237]     Train net output #1: loss = 0.54745 (* 1 = 0.54745 loss)
I1107 10:55:03.582423 11160 sgd_solver.cpp:105] Iteration 24700, lr = 0.1
I1107 10:55:12.101550 11160 solver.cpp:218] Iteration 24800 (11.739 iter/s, 8.51863s/100 iters), loss = 0.525007
I1107 10:55:12.101550 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 10:55:12.101550 11160 solver.cpp:237]     Train net output #1: loss = 0.525007 (* 1 = 0.525007 loss)
I1107 10:55:12.101550 11160 sgd_solver.cpp:105] Iteration 24800, lr = 0.1
I1107 10:55:20.612051 11160 solver.cpp:218] Iteration 24900 (11.7508 iter/s, 8.51003s/100 iters), loss = 0.69847
I1107 10:55:20.612051 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:55:20.612051 11160 solver.cpp:237]     Train net output #1: loss = 0.69847 (* 1 = 0.69847 loss)
I1107 10:55:20.612051 11160 sgd_solver.cpp:105] Iteration 24900, lr = 0.1
I1107 10:55:28.699137  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:55:29.035156 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_25000.caffemodel
I1107 10:55:29.065670 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_25000.solverstate
I1107 10:55:29.074683 11160 solver.cpp:330] Iteration 25000, Testing net (#0)
I1107 10:55:29.074683 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:55:31.065163 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:55:31.145162 11160 solver.cpp:397]     Test net output #0: accuracy = 0.4635
I1107 10:55:31.145162 11160 solver.cpp:397]     Test net output #1: loss = 1.81213 (* 1 = 1.81213 loss)
I1107 10:55:31.226662 11160 solver.cpp:218] Iteration 25000 (9.42166 iter/s, 10.6138s/100 iters), loss = 0.860102
I1107 10:55:31.226662 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1107 10:55:31.226662 11160 solver.cpp:237]     Train net output #1: loss = 0.860102 (* 1 = 0.860102 loss)
I1107 10:55:31.226662 11160 sgd_solver.cpp:105] Iteration 25000, lr = 0.1
I1107 10:55:39.736162 11160 solver.cpp:218] Iteration 25100 (11.7522 iter/s, 8.50904s/100 iters), loss = 0.649439
I1107 10:55:39.736162 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:55:39.736162 11160 solver.cpp:237]     Train net output #1: loss = 0.649439 (* 1 = 0.649439 loss)
I1107 10:55:39.736162 11160 sgd_solver.cpp:105] Iteration 25100, lr = 0.1
I1107 10:55:48.249846 11160 solver.cpp:218] Iteration 25200 (11.7463 iter/s, 8.51332s/100 iters), loss = 0.666591
I1107 10:55:48.249846 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:55:48.250347 11160 solver.cpp:237]     Train net output #1: loss = 0.666591 (* 1 = 0.666591 loss)
I1107 10:55:48.250347 11160 sgd_solver.cpp:105] Iteration 25200, lr = 0.1
I1107 10:55:56.752182 11160 solver.cpp:218] Iteration 25300 (11.7627 iter/s, 8.50147s/100 iters), loss = 0.604817
I1107 10:55:56.752182 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:55:56.752182 11160 solver.cpp:237]     Train net output #1: loss = 0.604817 (* 1 = 0.604817 loss)
I1107 10:55:56.752182 11160 sgd_solver.cpp:105] Iteration 25300, lr = 0.1
I1107 10:56:05.264199 11160 solver.cpp:218] Iteration 25400 (11.7488 iter/s, 8.5115s/100 iters), loss = 0.588116
I1107 10:56:05.264199 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:56:05.264199 11160 solver.cpp:237]     Train net output #1: loss = 0.588116 (* 1 = 0.588116 loss)
I1107 10:56:05.264199 11160 sgd_solver.cpp:105] Iteration 25400, lr = 0.1
I1107 10:56:13.350286  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:56:13.686838 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_25500.caffemodel
I1107 10:56:13.716853 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_25500.solverstate
I1107 10:56:13.725858 11160 solver.cpp:330] Iteration 25500, Testing net (#0)
I1107 10:56:13.725858 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:56:15.717836 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:56:15.796838 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5656
I1107 10:56:15.796838 11160 solver.cpp:397]     Test net output #1: loss = 1.36039 (* 1 = 1.36039 loss)
I1107 10:56:15.877836 11160 solver.cpp:218] Iteration 25500 (9.42225 iter/s, 10.6132s/100 iters), loss = 0.630812
I1107 10:56:15.877836 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:56:15.877836 11160 solver.cpp:237]     Train net output #1: loss = 0.630812 (* 1 = 0.630812 loss)
I1107 10:56:15.877836 11160 sgd_solver.cpp:105] Iteration 25500, lr = 0.1
I1107 10:56:24.386921 11160 solver.cpp:218] Iteration 25600 (11.7526 iter/s, 8.50874s/100 iters), loss = 0.621996
I1107 10:56:24.386921 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:56:24.386921 11160 solver.cpp:237]     Train net output #1: loss = 0.621996 (* 1 = 0.621996 loss)
I1107 10:56:24.386921 11160 sgd_solver.cpp:105] Iteration 25600, lr = 0.1
I1107 10:56:32.901759 11160 solver.cpp:218] Iteration 25700 (11.7452 iter/s, 8.51411s/100 iters), loss = 0.538957
I1107 10:56:32.901759 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:56:32.901759 11160 solver.cpp:237]     Train net output #1: loss = 0.538957 (* 1 = 0.538957 loss)
I1107 10:56:32.901759 11160 sgd_solver.cpp:105] Iteration 25700, lr = 0.1
I1107 10:56:41.412107 11160 solver.cpp:218] Iteration 25800 (11.7508 iter/s, 8.51004s/100 iters), loss = 0.497769
I1107 10:56:41.412107 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 10:56:41.412107 11160 solver.cpp:237]     Train net output #1: loss = 0.497769 (* 1 = 0.497769 loss)
I1107 10:56:41.412107 11160 sgd_solver.cpp:105] Iteration 25800, lr = 0.1
I1107 10:56:49.927094 11160 solver.cpp:218] Iteration 25900 (11.7452 iter/s, 8.51415s/100 iters), loss = 0.44457
I1107 10:56:49.927094 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 10:56:49.927094 11160 solver.cpp:237]     Train net output #1: loss = 0.44457 (* 1 = 0.44457 loss)
I1107 10:56:49.927094 11160 sgd_solver.cpp:105] Iteration 25900, lr = 0.1
I1107 10:56:58.014253  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:56:58.350754 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_26000.caffemodel
I1107 10:56:58.380753 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_26000.solverstate
I1107 10:56:58.389253 11160 solver.cpp:330] Iteration 26000, Testing net (#0)
I1107 10:56:58.389753 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:57:00.379256 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:57:00.458760 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6611
I1107 10:57:00.458760 11160 solver.cpp:397]     Test net output #1: loss = 1.02861 (* 1 = 1.02861 loss)
I1107 10:57:00.540252 11160 solver.cpp:218] Iteration 26000 (9.42284 iter/s, 10.6125s/100 iters), loss = 0.620286
I1107 10:57:00.540252 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:57:00.540252 11160 solver.cpp:237]     Train net output #1: loss = 0.620286 (* 1 = 0.620286 loss)
I1107 10:57:00.540252 11160 sgd_solver.cpp:105] Iteration 26000, lr = 0.1
I1107 10:57:09.065435 11160 solver.cpp:218] Iteration 26100 (11.7307 iter/s, 8.52464s/100 iters), loss = 0.608691
I1107 10:57:09.065435 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:57:09.065435 11160 solver.cpp:237]     Train net output #1: loss = 0.608691 (* 1 = 0.608691 loss)
I1107 10:57:09.065435 11160 sgd_solver.cpp:105] Iteration 26100, lr = 0.1
I1107 10:57:17.574935 11160 solver.cpp:218] Iteration 26200 (11.7524 iter/s, 8.50893s/100 iters), loss = 0.566292
I1107 10:57:17.574935 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:57:17.574935 11160 solver.cpp:237]     Train net output #1: loss = 0.566292 (* 1 = 0.566292 loss)
I1107 10:57:17.574935 11160 sgd_solver.cpp:105] Iteration 26200, lr = 0.1
I1107 10:57:26.085700 11160 solver.cpp:218] Iteration 26300 (11.7502 iter/s, 8.51049s/100 iters), loss = 0.542776
I1107 10:57:26.085700 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:57:26.085700 11160 solver.cpp:237]     Train net output #1: loss = 0.542776 (* 1 = 0.542776 loss)
I1107 10:57:26.085700 11160 sgd_solver.cpp:105] Iteration 26300, lr = 0.1
I1107 10:57:34.595667 11160 solver.cpp:218] Iteration 26400 (11.7516 iter/s, 8.50948s/100 iters), loss = 0.608923
I1107 10:57:34.595667 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:57:34.595667 11160 solver.cpp:237]     Train net output #1: loss = 0.608923 (* 1 = 0.608923 loss)
I1107 10:57:34.595667 11160 sgd_solver.cpp:105] Iteration 26400, lr = 0.1
I1107 10:57:42.687182  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:57:43.024682 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_26500.caffemodel
I1107 10:57:43.080181 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_26500.solverstate
I1107 10:57:43.089181 11160 solver.cpp:330] Iteration 26500, Testing net (#0)
I1107 10:57:43.089682 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:57:45.078682 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:57:45.158682 11160 solver.cpp:397]     Test net output #0: accuracy = 0.4669
I1107 10:57:45.158682 11160 solver.cpp:397]     Test net output #1: loss = 1.88398 (* 1 = 1.88398 loss)
I1107 10:57:45.239681 11160 solver.cpp:218] Iteration 26500 (9.39535 iter/s, 10.6436s/100 iters), loss = 0.672516
I1107 10:57:45.240190 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:57:45.240190 11160 solver.cpp:237]     Train net output #1: loss = 0.672516 (* 1 = 0.672516 loss)
I1107 10:57:45.240190 11160 sgd_solver.cpp:105] Iteration 26500, lr = 0.1
I1107 10:57:53.745251 11160 solver.cpp:218] Iteration 26600 (11.7581 iter/s, 8.50478s/100 iters), loss = 0.474149
I1107 10:57:53.745251 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 10:57:53.745251 11160 solver.cpp:237]     Train net output #1: loss = 0.474149 (* 1 = 0.474149 loss)
I1107 10:57:53.745251 11160 sgd_solver.cpp:105] Iteration 26600, lr = 0.1
I1107 10:58:02.264554 11160 solver.cpp:218] Iteration 26700 (11.7384 iter/s, 8.51907s/100 iters), loss = 0.650024
I1107 10:58:02.265054 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 10:58:02.265054 11160 solver.cpp:237]     Train net output #1: loss = 0.650024 (* 1 = 0.650024 loss)
I1107 10:58:02.265054 11160 sgd_solver.cpp:105] Iteration 26700, lr = 0.1
I1107 10:58:10.791592 11160 solver.cpp:218] Iteration 26800 (11.7286 iter/s, 8.52619s/100 iters), loss = 0.577808
I1107 10:58:10.791592 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:58:10.791592 11160 solver.cpp:237]     Train net output #1: loss = 0.577808 (* 1 = 0.577808 loss)
I1107 10:58:10.791592 11160 sgd_solver.cpp:105] Iteration 26800, lr = 0.1
I1107 10:58:19.298166 11160 solver.cpp:218] Iteration 26900 (11.7564 iter/s, 8.50604s/100 iters), loss = 0.59791
I1107 10:58:19.298166 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:58:19.298166 11160 solver.cpp:237]     Train net output #1: loss = 0.59791 (* 1 = 0.59791 loss)
I1107 10:58:19.298166 11160 sgd_solver.cpp:105] Iteration 26900, lr = 0.1
I1107 10:58:27.396698  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:58:27.733696 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_27000.caffemodel
I1107 10:58:27.765204 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_27000.solverstate
I1107 10:58:27.773697 11160 solver.cpp:330] Iteration 27000, Testing net (#0)
I1107 10:58:27.773697 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:58:29.762697 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:58:29.842197 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6611
I1107 10:58:29.842197 11160 solver.cpp:397]     Test net output #1: loss = 1.01195 (* 1 = 1.01195 loss)
I1107 10:58:29.923696 11160 solver.cpp:218] Iteration 27000 (9.41162 iter/s, 10.6252s/100 iters), loss = 0.540983
I1107 10:58:29.923696 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 10:58:29.923696 11160 solver.cpp:237]     Train net output #1: loss = 0.540983 (* 1 = 0.540983 loss)
I1107 10:58:29.923696 11160 sgd_solver.cpp:105] Iteration 27000, lr = 0.1
I1107 10:58:38.492741 11160 solver.cpp:218] Iteration 27100 (11.6705 iter/s, 8.56861s/100 iters), loss = 0.508714
I1107 10:58:38.493242 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:58:38.493242 11160 solver.cpp:237]     Train net output #1: loss = 0.508714 (* 1 = 0.508714 loss)
I1107 10:58:38.493242 11160 sgd_solver.cpp:105] Iteration 27100, lr = 0.1
I1107 10:58:46.992841 11160 solver.cpp:218] Iteration 27200 (11.7653 iter/s, 8.49957s/100 iters), loss = 0.597322
I1107 10:58:46.992841 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 10:58:46.993342 11160 solver.cpp:237]     Train net output #1: loss = 0.597322 (* 1 = 0.597322 loss)
I1107 10:58:46.993342 11160 sgd_solver.cpp:105] Iteration 27200, lr = 0.1
I1107 10:58:55.490988 11160 solver.cpp:218] Iteration 27300 (11.7682 iter/s, 8.49749s/100 iters), loss = 0.610427
I1107 10:58:55.490988 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 10:58:55.490988 11160 solver.cpp:237]     Train net output #1: loss = 0.610427 (* 1 = 0.610427 loss)
I1107 10:58:55.490988 11160 sgd_solver.cpp:105] Iteration 27300, lr = 0.1
I1107 10:59:03.991577 11160 solver.cpp:218] Iteration 27400 (11.765 iter/s, 8.49976s/100 iters), loss = 0.562255
I1107 10:59:03.991577 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 10:59:03.991577 11160 solver.cpp:237]     Train net output #1: loss = 0.562255 (* 1 = 0.562255 loss)
I1107 10:59:03.991577 11160 sgd_solver.cpp:105] Iteration 27400, lr = 0.1
I1107 10:59:12.076177  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:59:12.414695 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_27500.caffemodel
I1107 10:59:12.443691 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_27500.solverstate
I1107 10:59:12.452697 11160 solver.cpp:330] Iteration 27500, Testing net (#0)
I1107 10:59:12.452697 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:59:14.438177 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:59:14.517176 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5281
I1107 10:59:14.517678 11160 solver.cpp:397]     Test net output #1: loss = 1.51706 (* 1 = 1.51706 loss)
I1107 10:59:14.598176 11160 solver.cpp:218] Iteration 27500 (9.42846 iter/s, 10.6062s/100 iters), loss = 0.731685
I1107 10:59:14.598176 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1107 10:59:14.598176 11160 solver.cpp:237]     Train net output #1: loss = 0.731685 (* 1 = 0.731685 loss)
I1107 10:59:14.598176 11160 sgd_solver.cpp:105] Iteration 27500, lr = 0.1
I1107 10:59:23.082319 11160 solver.cpp:218] Iteration 27600 (11.7874 iter/s, 8.48364s/100 iters), loss = 0.426225
I1107 10:59:23.082319 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 10:59:23.082319 11160 solver.cpp:237]     Train net output #1: loss = 0.426225 (* 1 = 0.426225 loss)
I1107 10:59:23.082319 11160 sgd_solver.cpp:105] Iteration 27600, lr = 0.1
I1107 10:59:31.571367 11160 solver.cpp:218] Iteration 27700 (11.7808 iter/s, 8.48838s/100 iters), loss = 0.683161
I1107 10:59:31.571367 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 10:59:31.571367 11160 solver.cpp:237]     Train net output #1: loss = 0.683161 (* 1 = 0.683161 loss)
I1107 10:59:31.571367 11160 sgd_solver.cpp:105] Iteration 27700, lr = 0.1
I1107 10:59:40.064986 11160 solver.cpp:218] Iteration 27800 (11.7741 iter/s, 8.49323s/100 iters), loss = 0.602756
I1107 10:59:40.064986 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 10:59:40.064986 11160 solver.cpp:237]     Train net output #1: loss = 0.602756 (* 1 = 0.602756 loss)
I1107 10:59:40.064986 11160 sgd_solver.cpp:105] Iteration 27800, lr = 0.1
I1107 10:59:48.549124 11160 solver.cpp:218] Iteration 27900 (11.7874 iter/s, 8.48363s/100 iters), loss = 0.698886
I1107 10:59:48.549124 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 10:59:48.549124 11160 solver.cpp:237]     Train net output #1: loss = 0.698886 (* 1 = 0.698886 loss)
I1107 10:59:48.549124 11160 sgd_solver.cpp:105] Iteration 27900, lr = 0.1
I1107 10:59:56.633728  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:59:56.967727 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_28000.caffemodel
I1107 10:59:56.998227 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_28000.solverstate
I1107 10:59:57.006727 11160 solver.cpp:330] Iteration 28000, Testing net (#0)
I1107 10:59:57.006727 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 10:59:58.991284 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 10:59:59.071285 11160 solver.cpp:397]     Test net output #0: accuracy = 0.683
I1107 10:59:59.071285 11160 solver.cpp:397]     Test net output #1: loss = 0.955037 (* 1 = 0.955037 loss)
I1107 10:59:59.151784 11160 solver.cpp:218] Iteration 28000 (9.43207 iter/s, 10.6021s/100 iters), loss = 0.702728
I1107 10:59:59.151784 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1107 10:59:59.151784 11160 solver.cpp:237]     Train net output #1: loss = 0.702728 (* 1 = 0.702728 loss)
I1107 10:59:59.152284 11160 sgd_solver.cpp:105] Iteration 28000, lr = 0.1
I1107 11:00:07.641057 11160 solver.cpp:218] Iteration 28100 (11.7808 iter/s, 8.48841s/100 iters), loss = 0.604524
I1107 11:00:07.641057 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 11:00:07.641057 11160 solver.cpp:237]     Train net output #1: loss = 0.604524 (* 1 = 0.604524 loss)
I1107 11:00:07.641057 11160 sgd_solver.cpp:105] Iteration 28100, lr = 0.1
I1107 11:00:16.136751 11160 solver.cpp:218] Iteration 28200 (11.7714 iter/s, 8.49519s/100 iters), loss = 0.723322
I1107 11:00:16.136751 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:00:16.136751 11160 solver.cpp:237]     Train net output #1: loss = 0.723322 (* 1 = 0.723322 loss)
I1107 11:00:16.136751 11160 sgd_solver.cpp:105] Iteration 28200, lr = 0.1
I1107 11:00:24.632057 11160 solver.cpp:218] Iteration 28300 (11.7717 iter/s, 8.49498s/100 iters), loss = 0.519334
I1107 11:00:24.632057 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:00:24.632057 11160 solver.cpp:237]     Train net output #1: loss = 0.519334 (* 1 = 0.519334 loss)
I1107 11:00:24.632057 11160 sgd_solver.cpp:105] Iteration 28300, lr = 0.1
I1107 11:00:33.136857 11160 solver.cpp:218] Iteration 28400 (11.7592 iter/s, 8.50397s/100 iters), loss = 0.655076
I1107 11:00:33.136857 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 11:00:33.136857 11160 solver.cpp:237]     Train net output #1: loss = 0.655076 (* 1 = 0.655076 loss)
I1107 11:00:33.136857 11160 sgd_solver.cpp:105] Iteration 28400, lr = 0.1
I1107 11:00:41.210360  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:00:41.546357 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_28500.caffemodel
I1107 11:00:41.575358 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_28500.solverstate
I1107 11:00:41.584357 11160 solver.cpp:330] Iteration 28500, Testing net (#0)
I1107 11:00:41.584357 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:00:43.568898 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:00:43.648397 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6375
I1107 11:00:43.648898 11160 solver.cpp:397]     Test net output #1: loss = 1.08479 (* 1 = 1.08479 loss)
I1107 11:00:43.729897 11160 solver.cpp:218] Iteration 28500 (9.4406 iter/s, 10.5926s/100 iters), loss = 0.582145
I1107 11:00:43.729897 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:00:43.729897 11160 solver.cpp:237]     Train net output #1: loss = 0.582145 (* 1 = 0.582145 loss)
I1107 11:00:43.729897 11160 sgd_solver.cpp:105] Iteration 28500, lr = 0.1
I1107 11:00:52.229969 11160 solver.cpp:218] Iteration 28600 (11.765 iter/s, 8.49979s/100 iters), loss = 0.49929
I1107 11:00:52.229969 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:00:52.229969 11160 solver.cpp:237]     Train net output #1: loss = 0.49929 (* 1 = 0.49929 loss)
I1107 11:00:52.229969 11160 sgd_solver.cpp:105] Iteration 28600, lr = 0.1
I1107 11:01:00.733547 11160 solver.cpp:218] Iteration 28700 (11.7605 iter/s, 8.50304s/100 iters), loss = 0.674857
I1107 11:01:00.733547 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:01:00.734048 11160 solver.cpp:237]     Train net output #1: loss = 0.674857 (* 1 = 0.674857 loss)
I1107 11:01:00.734048 11160 sgd_solver.cpp:105] Iteration 28700, lr = 0.1
I1107 11:01:09.220971 11160 solver.cpp:218] Iteration 28800 (11.783 iter/s, 8.48678s/100 iters), loss = 0.535774
I1107 11:01:09.220971 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:01:09.220971 11160 solver.cpp:237]     Train net output #1: loss = 0.535774 (* 1 = 0.535774 loss)
I1107 11:01:09.220971 11160 sgd_solver.cpp:105] Iteration 28800, lr = 0.1
I1107 11:01:17.710048 11160 solver.cpp:218] Iteration 28900 (11.7809 iter/s, 8.48831s/100 iters), loss = 0.535789
I1107 11:01:17.710048 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:01:17.710048 11160 solver.cpp:237]     Train net output #1: loss = 0.535789 (* 1 = 0.535789 loss)
I1107 11:01:17.710048 11160 sgd_solver.cpp:105] Iteration 28900, lr = 0.1
I1107 11:01:25.784049  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:01:26.119050 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_29000.caffemodel
I1107 11:01:26.150049 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_29000.solverstate
I1107 11:01:26.158551 11160 solver.cpp:330] Iteration 29000, Testing net (#0)
I1107 11:01:26.158551 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:01:28.143589 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:01:28.223592 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6812
I1107 11:01:28.223592 11160 solver.cpp:397]     Test net output #1: loss = 0.97701 (* 1 = 0.97701 loss)
I1107 11:01:28.304590 11160 solver.cpp:218] Iteration 29000 (9.43923 iter/s, 10.5941s/100 iters), loss = 0.608987
I1107 11:01:28.304590 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:01:28.304590 11160 solver.cpp:237]     Train net output #1: loss = 0.608987 (* 1 = 0.608987 loss)
I1107 11:01:28.304590 11160 sgd_solver.cpp:105] Iteration 29000, lr = 0.1
I1107 11:01:36.796146 11160 solver.cpp:218] Iteration 29100 (11.7771 iter/s, 8.49106s/100 iters), loss = 0.455904
I1107 11:01:36.796146 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 11:01:36.796146 11160 solver.cpp:237]     Train net output #1: loss = 0.455904 (* 1 = 0.455904 loss)
I1107 11:01:36.796146 11160 sgd_solver.cpp:105] Iteration 29100, lr = 0.1
I1107 11:01:45.286269 11160 solver.cpp:218] Iteration 29200 (11.7789 iter/s, 8.48979s/100 iters), loss = 0.608968
I1107 11:01:45.286269 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:01:45.286770 11160 solver.cpp:237]     Train net output #1: loss = 0.608968 (* 1 = 0.608968 loss)
I1107 11:01:45.286770 11160 sgd_solver.cpp:105] Iteration 29200, lr = 0.1
I1107 11:01:53.792268 11160 solver.cpp:218] Iteration 29300 (11.7573 iter/s, 8.50537s/100 iters), loss = 0.539238
I1107 11:01:53.792268 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:01:53.792268 11160 solver.cpp:237]     Train net output #1: loss = 0.539238 (* 1 = 0.539238 loss)
I1107 11:01:53.792268 11160 sgd_solver.cpp:105] Iteration 29300, lr = 0.1
I1107 11:02:02.283268 11160 solver.cpp:218] Iteration 29400 (11.7778 iter/s, 8.49056s/100 iters), loss = 0.622946
I1107 11:02:02.283268 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:02:02.283778 11160 solver.cpp:237]     Train net output #1: loss = 0.622946 (* 1 = 0.622946 loss)
I1107 11:02:02.283778 11160 sgd_solver.cpp:105] Iteration 29400, lr = 0.1
I1107 11:02:10.355726  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:02:10.692225 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_29500.caffemodel
I1107 11:02:10.719724 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_29500.solverstate
I1107 11:02:10.728224 11160 solver.cpp:330] Iteration 29500, Testing net (#0)
I1107 11:02:10.728730 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:02:12.712725 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:02:12.792225 11160 solver.cpp:397]     Test net output #0: accuracy = 0.7232
I1107 11:02:12.792225 11160 solver.cpp:397]     Test net output #1: loss = 0.834119 (* 1 = 0.834119 loss)
I1107 11:02:12.873724 11160 solver.cpp:218] Iteration 29500 (9.44312 iter/s, 10.5897s/100 iters), loss = 0.575684
I1107 11:02:12.874224 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:02:12.874224 11160 solver.cpp:237]     Train net output #1: loss = 0.575684 (* 1 = 0.575684 loss)
I1107 11:02:12.874224 11160 sgd_solver.cpp:105] Iteration 29500, lr = 0.1
I1107 11:02:21.379395 11160 solver.cpp:218] Iteration 29600 (11.758 iter/s, 8.50487s/100 iters), loss = 0.596029
I1107 11:02:21.379395 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:02:21.379395 11160 solver.cpp:237]     Train net output #1: loss = 0.596029 (* 1 = 0.596029 loss)
I1107 11:02:21.379395 11160 sgd_solver.cpp:105] Iteration 29600, lr = 0.1
I1107 11:02:29.887089 11160 solver.cpp:218] Iteration 29700 (11.755 iter/s, 8.50704s/100 iters), loss = 0.578164
I1107 11:02:29.887089 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:02:29.887089 11160 solver.cpp:237]     Train net output #1: loss = 0.578164 (* 1 = 0.578164 loss)
I1107 11:02:29.887089 11160 sgd_solver.cpp:105] Iteration 29700, lr = 0.1
I1107 11:02:38.375650 11160 solver.cpp:218] Iteration 29800 (11.7808 iter/s, 8.48835s/100 iters), loss = 0.510481
I1107 11:02:38.375650 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:02:38.375650 11160 solver.cpp:237]     Train net output #1: loss = 0.510481 (* 1 = 0.510481 loss)
I1107 11:02:38.375650 11160 sgd_solver.cpp:105] Iteration 29800, lr = 0.1
I1107 11:02:46.869776 11160 solver.cpp:218] Iteration 29900 (11.7736 iter/s, 8.49358s/100 iters), loss = 0.534754
I1107 11:02:46.869776 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:02:46.869776 11160 solver.cpp:237]     Train net output #1: loss = 0.534754 (* 1 = 0.534754 loss)
I1107 11:02:46.869776 11160 sgd_solver.cpp:105] Iteration 29900, lr = 0.1
I1107 11:02:54.947275  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:02:55.284276 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_30000.caffemodel
I1107 11:02:55.315279 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_30000.solverstate
I1107 11:02:55.325276 11160 solver.cpp:330] Iteration 30000, Testing net (#0)
I1107 11:02:55.325276 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:02:57.309775 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:02:57.388775 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5892
I1107 11:02:57.389274 11160 solver.cpp:397]     Test net output #1: loss = 1.35658 (* 1 = 1.35658 loss)
I1107 11:02:57.469274 11160 solver.cpp:218] Iteration 30000 (9.43504 iter/s, 10.5988s/100 iters), loss = 0.691867
I1107 11:02:57.469274 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1107 11:02:57.469274 11160 solver.cpp:237]     Train net output #1: loss = 0.691867 (* 1 = 0.691867 loss)
I1107 11:02:57.469274 11160 sgd_solver.cpp:105] Iteration 30000, lr = 0.1
I1107 11:03:05.955611 11160 solver.cpp:218] Iteration 30100 (11.7843 iter/s, 8.48587s/100 iters), loss = 0.537229
I1107 11:03:05.955611 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:03:05.955611 11160 solver.cpp:237]     Train net output #1: loss = 0.537229 (* 1 = 0.537229 loss)
I1107 11:03:05.955611 11160 sgd_solver.cpp:105] Iteration 30100, lr = 0.1
I1107 11:03:14.455755 11160 solver.cpp:218] Iteration 30200 (11.7652 iter/s, 8.49963s/100 iters), loss = 0.639946
I1107 11:03:14.455755 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:03:14.455755 11160 solver.cpp:237]     Train net output #1: loss = 0.639946 (* 1 = 0.639946 loss)
I1107 11:03:14.455755 11160 sgd_solver.cpp:105] Iteration 30200, lr = 0.1
I1107 11:03:22.948092 11160 solver.cpp:218] Iteration 30300 (11.7761 iter/s, 8.49177s/100 iters), loss = 0.590836
I1107 11:03:22.948092 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:03:22.948092 11160 solver.cpp:237]     Train net output #1: loss = 0.590836 (* 1 = 0.590836 loss)
I1107 11:03:22.948593 11160 sgd_solver.cpp:105] Iteration 30300, lr = 0.1
I1107 11:03:31.439759 11160 solver.cpp:218] Iteration 30400 (11.777 iter/s, 8.49112s/100 iters), loss = 0.487308
I1107 11:03:31.439759 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:03:31.439759 11160 solver.cpp:237]     Train net output #1: loss = 0.487308 (* 1 = 0.487308 loss)
I1107 11:03:31.439759 11160 sgd_solver.cpp:105] Iteration 30400, lr = 0.1
I1107 11:03:39.520128  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:03:39.856626 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_30500.caffemodel
I1107 11:03:39.885628 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_30500.solverstate
I1107 11:03:39.894629 11160 solver.cpp:330] Iteration 30500, Testing net (#0)
I1107 11:03:39.894629 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:03:41.878127 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:03:41.957628 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6384
I1107 11:03:41.957628 11160 solver.cpp:397]     Test net output #1: loss = 1.12025 (* 1 = 1.12025 loss)
I1107 11:03:42.038126 11160 solver.cpp:218] Iteration 30500 (9.4361 iter/s, 10.5976s/100 iters), loss = 0.618512
I1107 11:03:42.038126 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:03:42.038126 11160 solver.cpp:237]     Train net output #1: loss = 0.618512 (* 1 = 0.618512 loss)
I1107 11:03:42.038126 11160 sgd_solver.cpp:105] Iteration 30500, lr = 0.1
I1107 11:03:50.519795 11160 solver.cpp:218] Iteration 30600 (11.791 iter/s, 8.48102s/100 iters), loss = 0.603873
I1107 11:03:50.519795 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:03:50.519795 11160 solver.cpp:237]     Train net output #1: loss = 0.603873 (* 1 = 0.603873 loss)
I1107 11:03:50.519795 11160 sgd_solver.cpp:105] Iteration 30600, lr = 0.1
I1107 11:03:59.007992 11160 solver.cpp:218] Iteration 30700 (11.7814 iter/s, 8.48795s/100 iters), loss = 0.590342
I1107 11:03:59.007992 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:03:59.007992 11160 solver.cpp:237]     Train net output #1: loss = 0.590342 (* 1 = 0.590342 loss)
I1107 11:03:59.007992 11160 sgd_solver.cpp:105] Iteration 30700, lr = 0.1
I1107 11:04:07.494660 11160 solver.cpp:218] Iteration 30800 (11.7841 iter/s, 8.48605s/100 iters), loss = 0.571703
I1107 11:04:07.494660 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 11:04:07.494660 11160 solver.cpp:237]     Train net output #1: loss = 0.571703 (* 1 = 0.571703 loss)
I1107 11:04:07.494660 11160 sgd_solver.cpp:105] Iteration 30800, lr = 0.1
I1107 11:04:15.982360 11160 solver.cpp:218] Iteration 30900 (11.7825 iter/s, 8.48719s/100 iters), loss = 0.49796
I1107 11:04:15.982360 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 11:04:15.982360 11160 solver.cpp:237]     Train net output #1: loss = 0.49796 (* 1 = 0.49796 loss)
I1107 11:04:15.982360 11160 sgd_solver.cpp:105] Iteration 30900, lr = 0.1
I1107 11:04:24.057950  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:04:24.394449 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_31000.caffemodel
I1107 11:04:24.423468 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_31000.solverstate
I1107 11:04:24.432469 11160 solver.cpp:330] Iteration 31000, Testing net (#0)
I1107 11:04:24.432950 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:04:26.419450 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:04:26.498950 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6731
I1107 11:04:26.498950 11160 solver.cpp:397]     Test net output #1: loss = 1.00725 (* 1 = 1.00725 loss)
I1107 11:04:26.579947 11160 solver.cpp:218] Iteration 31000 (9.4369 iter/s, 10.5967s/100 iters), loss = 0.780433
I1107 11:04:26.579947 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 11:04:26.579947 11160 solver.cpp:237]     Train net output #1: loss = 0.780433 (* 1 = 0.780433 loss)
I1107 11:04:26.579947 11160 sgd_solver.cpp:105] Iteration 31000, lr = 0.1
I1107 11:04:35.077051 11160 solver.cpp:218] Iteration 31100 (11.7689 iter/s, 8.497s/100 iters), loss = 0.616958
I1107 11:04:35.077553 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:04:35.077553 11160 solver.cpp:237]     Train net output #1: loss = 0.616958 (* 1 = 0.616958 loss)
I1107 11:04:35.077553 11160 sgd_solver.cpp:105] Iteration 31100, lr = 0.1
I1107 11:04:43.564154 11160 solver.cpp:218] Iteration 31200 (11.7835 iter/s, 8.48647s/100 iters), loss = 0.585484
I1107 11:04:43.564154 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:04:43.564154 11160 solver.cpp:237]     Train net output #1: loss = 0.585484 (* 1 = 0.585484 loss)
I1107 11:04:43.564154 11160 sgd_solver.cpp:105] Iteration 31200, lr = 0.1
I1107 11:04:52.059309 11160 solver.cpp:218] Iteration 31300 (11.7721 iter/s, 8.49468s/100 iters), loss = 0.626754
I1107 11:04:52.059809 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:04:52.059809 11160 solver.cpp:237]     Train net output #1: loss = 0.626754 (* 1 = 0.626754 loss)
I1107 11:04:52.059809 11160 sgd_solver.cpp:105] Iteration 31300, lr = 0.1
I1107 11:05:00.544960 11160 solver.cpp:218] Iteration 31400 (11.7856 iter/s, 8.48492s/100 iters), loss = 0.478888
I1107 11:05:00.544960 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:05:00.544960 11160 solver.cpp:237]     Train net output #1: loss = 0.478888 (* 1 = 0.478888 loss)
I1107 11:05:00.544960 11160 sgd_solver.cpp:105] Iteration 31400, lr = 0.1
I1107 11:05:08.614569  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:05:08.950116 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_31500.caffemodel
I1107 11:05:08.981098 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_31500.solverstate
I1107 11:05:08.990101 11160 solver.cpp:330] Iteration 31500, Testing net (#0)
I1107 11:05:08.990101 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:05:10.973695 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:05:11.052695 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6402
I1107 11:05:11.052695 11160 solver.cpp:397]     Test net output #1: loss = 1.11372 (* 1 = 1.11372 loss)
I1107 11:05:11.134193 11160 solver.cpp:218] Iteration 31500 (9.44406 iter/s, 10.5887s/100 iters), loss = 0.640447
I1107 11:05:11.134193 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 11:05:11.134193 11160 solver.cpp:237]     Train net output #1: loss = 0.640447 (* 1 = 0.640447 loss)
I1107 11:05:11.134193 11160 sgd_solver.cpp:105] Iteration 31500, lr = 0.1
I1107 11:05:19.622212 11160 solver.cpp:218] Iteration 31600 (11.7819 iter/s, 8.48756s/100 iters), loss = 0.591581
I1107 11:05:19.622212 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:05:19.622212 11160 solver.cpp:237]     Train net output #1: loss = 0.591581 (* 1 = 0.591581 loss)
I1107 11:05:19.622212 11160 sgd_solver.cpp:105] Iteration 31600, lr = 0.1
I1107 11:05:28.121507 11160 solver.cpp:218] Iteration 31700 (11.7668 iter/s, 8.49851s/100 iters), loss = 0.629685
I1107 11:05:28.121507 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:05:28.121507 11160 solver.cpp:237]     Train net output #1: loss = 0.629685 (* 1 = 0.629685 loss)
I1107 11:05:28.121507 11160 sgd_solver.cpp:105] Iteration 31700, lr = 0.1
I1107 11:05:36.621546 11160 solver.cpp:218] Iteration 31800 (11.7653 iter/s, 8.49956s/100 iters), loss = 0.434804
I1107 11:05:36.621546 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:05:36.621546 11160 solver.cpp:237]     Train net output #1: loss = 0.434804 (* 1 = 0.434804 loss)
I1107 11:05:36.621546 11160 sgd_solver.cpp:105] Iteration 31800, lr = 0.1
I1107 11:05:45.116868 11160 solver.cpp:218] Iteration 31900 (11.7718 iter/s, 8.49491s/100 iters), loss = 0.503961
I1107 11:05:45.116868 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:05:45.116868 11160 solver.cpp:237]     Train net output #1: loss = 0.503961 (* 1 = 0.503961 loss)
I1107 11:05:45.116868 11160 sgd_solver.cpp:105] Iteration 31900, lr = 0.1
I1107 11:05:53.199594  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:05:53.536593 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_32000.caffemodel
I1107 11:05:53.564592 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_32000.solverstate
I1107 11:05:53.573093 11160 solver.cpp:330] Iteration 32000, Testing net (#0)
I1107 11:05:53.573598 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:05:55.558101 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:05:55.637593 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6168
I1107 11:05:55.637593 11160 solver.cpp:397]     Test net output #1: loss = 1.2412 (* 1 = 1.2412 loss)
I1107 11:05:55.718592 11160 solver.cpp:218] Iteration 32000 (9.43276 iter/s, 10.6014s/100 iters), loss = 0.588454
I1107 11:05:55.718592 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:05:55.718592 11160 solver.cpp:237]     Train net output #1: loss = 0.588454 (* 1 = 0.588454 loss)
I1107 11:05:55.718592 11160 sgd_solver.cpp:105] Iteration 32000, lr = 0.1
I1107 11:06:04.202852 11160 solver.cpp:218] Iteration 32100 (11.7874 iter/s, 8.48363s/100 iters), loss = 0.530897
I1107 11:06:04.202852 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:06:04.202852 11160 solver.cpp:237]     Train net output #1: loss = 0.530897 (* 1 = 0.530897 loss)
I1107 11:06:04.202852 11160 sgd_solver.cpp:105] Iteration 32100, lr = 0.1
I1107 11:06:12.686074 11160 solver.cpp:218] Iteration 32200 (11.7885 iter/s, 8.48283s/100 iters), loss = 0.666998
I1107 11:06:12.686074 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:06:12.686074 11160 solver.cpp:237]     Train net output #1: loss = 0.666998 (* 1 = 0.666998 loss)
I1107 11:06:12.686074 11160 sgd_solver.cpp:105] Iteration 32200, lr = 0.1
I1107 11:06:21.171140 11160 solver.cpp:218] Iteration 32300 (11.7863 iter/s, 8.48442s/100 iters), loss = 0.507354
I1107 11:06:21.171140 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:06:21.171140 11160 solver.cpp:237]     Train net output #1: loss = 0.507354 (* 1 = 0.507354 loss)
I1107 11:06:21.171140 11160 sgd_solver.cpp:105] Iteration 32300, lr = 0.1
I1107 11:06:29.661669 11160 solver.cpp:218] Iteration 32400 (11.7783 iter/s, 8.49021s/100 iters), loss = 0.515385
I1107 11:06:29.661669 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:06:29.661669 11160 solver.cpp:237]     Train net output #1: loss = 0.515385 (* 1 = 0.515385 loss)
I1107 11:06:29.661669 11160 sgd_solver.cpp:105] Iteration 32400, lr = 0.1
I1107 11:06:37.735735  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:06:38.072736 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_32500.caffemodel
I1107 11:06:38.103235 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_32500.solverstate
I1107 11:06:38.111737 11160 solver.cpp:330] Iteration 32500, Testing net (#0)
I1107 11:06:38.112236 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:06:40.096236 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:06:40.175753 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6049
I1107 11:06:40.176254 11160 solver.cpp:397]     Test net output #1: loss = 1.26975 (* 1 = 1.26975 loss)
I1107 11:06:40.257235 11160 solver.cpp:218] Iteration 32500 (9.43869 iter/s, 10.5947s/100 iters), loss = 0.658343
I1107 11:06:40.257235 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:06:40.257235 11160 solver.cpp:237]     Train net output #1: loss = 0.658343 (* 1 = 0.658343 loss)
I1107 11:06:40.257235 11160 sgd_solver.cpp:105] Iteration 32500, lr = 0.1
I1107 11:06:48.760772 11160 solver.cpp:218] Iteration 32600 (11.7605 iter/s, 8.50305s/100 iters), loss = 0.434593
I1107 11:06:48.760772 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1107 11:06:48.760772 11160 solver.cpp:237]     Train net output #1: loss = 0.434593 (* 1 = 0.434593 loss)
I1107 11:06:48.760772 11160 sgd_solver.cpp:105] Iteration 32600, lr = 0.1
I1107 11:06:57.258554 11160 solver.cpp:218] Iteration 32700 (11.7685 iter/s, 8.49725s/100 iters), loss = 0.665126
I1107 11:06:57.258554 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:06:57.258554 11160 solver.cpp:237]     Train net output #1: loss = 0.665126 (* 1 = 0.665126 loss)
I1107 11:06:57.258554 11160 sgd_solver.cpp:105] Iteration 32700, lr = 0.1
I1107 11:07:05.752054 11160 solver.cpp:218] Iteration 32800 (11.7744 iter/s, 8.49297s/100 iters), loss = 0.51481
I1107 11:07:05.752054 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:07:05.752054 11160 solver.cpp:237]     Train net output #1: loss = 0.51481 (* 1 = 0.51481 loss)
I1107 11:07:05.752054 11160 sgd_solver.cpp:105] Iteration 32800, lr = 0.1
I1107 11:07:14.249167 11160 solver.cpp:218] Iteration 32900 (11.7694 iter/s, 8.49662s/100 iters), loss = 0.641762
I1107 11:07:14.249167 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:07:14.249167 11160 solver.cpp:237]     Train net output #1: loss = 0.641762 (* 1 = 0.641762 loss)
I1107 11:07:14.249167 11160 sgd_solver.cpp:105] Iteration 32900, lr = 0.1
I1107 11:07:22.320338  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:07:22.658335 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_33000.caffemodel
I1107 11:07:22.687834 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_33000.solverstate
I1107 11:07:22.724335 11160 solver.cpp:330] Iteration 33000, Testing net (#0)
I1107 11:07:22.724835 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:07:24.709834 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:07:24.788833 11160 solver.cpp:397]     Test net output #0: accuracy = 0.7045
I1107 11:07:24.789335 11160 solver.cpp:397]     Test net output #1: loss = 0.904409 (* 1 = 0.904409 loss)
I1107 11:07:24.869833 11160 solver.cpp:218] Iteration 33000 (9.41576 iter/s, 10.6205s/100 iters), loss = 0.677545
I1107 11:07:24.870343 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 11:07:24.870343 11160 solver.cpp:237]     Train net output #1: loss = 0.677545 (* 1 = 0.677545 loss)
I1107 11:07:24.870343 11160 sgd_solver.cpp:105] Iteration 33000, lr = 0.1
I1107 11:07:33.369459 11160 solver.cpp:218] Iteration 33100 (11.7662 iter/s, 8.49894s/100 iters), loss = 0.442011
I1107 11:07:33.369459 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 11:07:33.369459 11160 solver.cpp:237]     Train net output #1: loss = 0.442011 (* 1 = 0.442011 loss)
I1107 11:07:33.369459 11160 sgd_solver.cpp:105] Iteration 33100, lr = 0.1
I1107 11:07:41.870055 11160 solver.cpp:218] Iteration 33200 (11.765 iter/s, 8.49978s/100 iters), loss = 0.679957
I1107 11:07:41.870055 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:07:41.870055 11160 solver.cpp:237]     Train net output #1: loss = 0.679957 (* 1 = 0.679957 loss)
I1107 11:07:41.870055 11160 sgd_solver.cpp:105] Iteration 33200, lr = 0.1
I1107 11:07:50.355106 11160 solver.cpp:218] Iteration 33300 (11.7855 iter/s, 8.485s/100 iters), loss = 0.559824
I1107 11:07:50.355607 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:07:50.355607 11160 solver.cpp:237]     Train net output #1: loss = 0.559824 (* 1 = 0.559824 loss)
I1107 11:07:50.355607 11160 sgd_solver.cpp:105] Iteration 33300, lr = 0.1
I1107 11:07:58.840889 11160 solver.cpp:218] Iteration 33400 (11.7852 iter/s, 8.48521s/100 iters), loss = 0.520084
I1107 11:07:58.841390 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:07:58.841390 11160 solver.cpp:237]     Train net output #1: loss = 0.520084 (* 1 = 0.520084 loss)
I1107 11:07:58.841390 11160 sgd_solver.cpp:105] Iteration 33400, lr = 0.1
I1107 11:08:06.921509  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:08:07.259511 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_33500.caffemodel
I1107 11:08:07.288508 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_33500.solverstate
I1107 11:08:07.297508 11160 solver.cpp:330] Iteration 33500, Testing net (#0)
I1107 11:08:07.297508 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:08:09.281626 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:08:09.361126 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6766
I1107 11:08:09.361126 11160 solver.cpp:397]     Test net output #1: loss = 0.98422 (* 1 = 0.98422 loss)
I1107 11:08:09.442631 11160 solver.cpp:218] Iteration 33500 (9.43304 iter/s, 10.601s/100 iters), loss = 0.589579
I1107 11:08:09.442631 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:08:09.442631 11160 solver.cpp:237]     Train net output #1: loss = 0.589579 (* 1 = 0.589579 loss)
I1107 11:08:09.442631 11160 sgd_solver.cpp:105] Iteration 33500, lr = 0.1
I1107 11:08:17.946540 11160 solver.cpp:218] Iteration 33600 (11.7601 iter/s, 8.50336s/100 iters), loss = 0.503825
I1107 11:08:17.946540 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:08:17.946540 11160 solver.cpp:237]     Train net output #1: loss = 0.503825 (* 1 = 0.503825 loss)
I1107 11:08:17.946540 11160 sgd_solver.cpp:105] Iteration 33600, lr = 0.1
I1107 11:08:26.455015 11160 solver.cpp:218] Iteration 33700 (11.7536 iter/s, 8.50802s/100 iters), loss = 0.659817
I1107 11:08:26.455015 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:08:26.455015 11160 solver.cpp:237]     Train net output #1: loss = 0.659817 (* 1 = 0.659817 loss)
I1107 11:08:26.455015 11160 sgd_solver.cpp:105] Iteration 33700, lr = 0.1
I1107 11:08:34.950340 11160 solver.cpp:218] Iteration 33800 (11.7721 iter/s, 8.49464s/100 iters), loss = 0.622387
I1107 11:08:34.950340 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:08:34.950340 11160 solver.cpp:237]     Train net output #1: loss = 0.622387 (* 1 = 0.622387 loss)
I1107 11:08:34.950340 11160 sgd_solver.cpp:105] Iteration 33800, lr = 0.1
I1107 11:08:43.450870 11160 solver.cpp:218] Iteration 33900 (11.7644 iter/s, 8.50025s/100 iters), loss = 0.588439
I1107 11:08:43.451371 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:08:43.451371 11160 solver.cpp:237]     Train net output #1: loss = 0.588439 (* 1 = 0.588439 loss)
I1107 11:08:43.451371 11160 sgd_solver.cpp:105] Iteration 33900, lr = 0.1
I1107 11:08:51.536931  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:08:51.872431 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_34000.caffemodel
I1107 11:08:51.901432 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_34000.solverstate
I1107 11:08:51.932932 11160 solver.cpp:330] Iteration 34000, Testing net (#0)
I1107 11:08:51.932932 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:08:53.917482 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:08:53.996982 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6233
I1107 11:08:53.996982 11160 solver.cpp:397]     Test net output #1: loss = 1.13243 (* 1 = 1.13243 loss)
I1107 11:08:54.078481 11160 solver.cpp:218] Iteration 34000 (9.41035 iter/s, 10.6266s/100 iters), loss = 0.590883
I1107 11:08:54.078481 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:08:54.078481 11160 solver.cpp:237]     Train net output #1: loss = 0.590883 (* 1 = 0.590883 loss)
I1107 11:08:54.078481 11160 sgd_solver.cpp:105] Iteration 34000, lr = 0.1
I1107 11:09:02.582119 11160 solver.cpp:218] Iteration 34100 (11.7602 iter/s, 8.50325s/100 iters), loss = 0.49598
I1107 11:09:02.582119 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:09:02.582119 11160 solver.cpp:237]     Train net output #1: loss = 0.49598 (* 1 = 0.49598 loss)
I1107 11:09:02.582119 11160 sgd_solver.cpp:105] Iteration 34100, lr = 0.1
I1107 11:09:11.075742 11160 solver.cpp:218] Iteration 34200 (11.774 iter/s, 8.49328s/100 iters), loss = 0.58732
I1107 11:09:11.075742 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:09:11.075742 11160 solver.cpp:237]     Train net output #1: loss = 0.58732 (* 1 = 0.58732 loss)
I1107 11:09:11.076242 11160 sgd_solver.cpp:105] Iteration 34200, lr = 0.1
I1107 11:09:19.574674 11160 solver.cpp:218] Iteration 34300 (11.767 iter/s, 8.49835s/100 iters), loss = 0.553167
I1107 11:09:19.574674 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:09:19.574674 11160 solver.cpp:237]     Train net output #1: loss = 0.553167 (* 1 = 0.553167 loss)
I1107 11:09:19.574674 11160 sgd_solver.cpp:105] Iteration 34300, lr = 0.1
I1107 11:09:28.073707 11160 solver.cpp:218] Iteration 34400 (11.7672 iter/s, 8.49822s/100 iters), loss = 0.653334
I1107 11:09:28.073707 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:09:28.073707 11160 solver.cpp:237]     Train net output #1: loss = 0.653334 (* 1 = 0.653334 loss)
I1107 11:09:28.073707 11160 sgd_solver.cpp:105] Iteration 34400, lr = 0.1
I1107 11:09:36.157086  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:09:36.492585 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_34500.caffemodel
I1107 11:09:36.522086 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_34500.solverstate
I1107 11:09:36.530586 11160 solver.cpp:330] Iteration 34500, Testing net (#0)
I1107 11:09:36.530586 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:09:38.513772 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:09:38.592270 11160 solver.cpp:397]     Test net output #0: accuracy = 0.7239
I1107 11:09:38.592270 11160 solver.cpp:397]     Test net output #1: loss = 0.82032 (* 1 = 0.82032 loss)
I1107 11:09:38.673770 11160 solver.cpp:218] Iteration 34500 (9.43421 iter/s, 10.5997s/100 iters), loss = 0.695444
I1107 11:09:38.673770 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 11:09:38.673770 11160 solver.cpp:237]     Train net output #1: loss = 0.695444 (* 1 = 0.695444 loss)
I1107 11:09:38.673770 11160 sgd_solver.cpp:105] Iteration 34500, lr = 0.1
I1107 11:09:47.173504 11160 solver.cpp:218] Iteration 34600 (11.7658 iter/s, 8.4992s/100 iters), loss = 0.425569
I1107 11:09:47.173504 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1107 11:09:47.173504 11160 solver.cpp:237]     Train net output #1: loss = 0.425569 (* 1 = 0.425569 loss)
I1107 11:09:47.173504 11160 sgd_solver.cpp:105] Iteration 34600, lr = 0.1
I1107 11:09:55.676595 11160 solver.cpp:218] Iteration 34700 (11.7613 iter/s, 8.50243s/100 iters), loss = 0.602895
I1107 11:09:55.676595 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 11:09:55.676595 11160 solver.cpp:237]     Train net output #1: loss = 0.602895 (* 1 = 0.602895 loss)
I1107 11:09:55.676595 11160 sgd_solver.cpp:105] Iteration 34700, lr = 0.1
I1107 11:10:04.172310 11160 solver.cpp:218] Iteration 34800 (11.7709 iter/s, 8.49549s/100 iters), loss = 0.668107
I1107 11:10:04.172310 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 11:10:04.172811 11160 solver.cpp:237]     Train net output #1: loss = 0.668107 (* 1 = 0.668107 loss)
I1107 11:10:04.172811 11160 sgd_solver.cpp:105] Iteration 34800, lr = 0.1
I1107 11:10:12.668584 11160 solver.cpp:218] Iteration 34900 (11.7711 iter/s, 8.49537s/100 iters), loss = 0.568178
I1107 11:10:12.668584 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:10:12.668584 11160 solver.cpp:237]     Train net output #1: loss = 0.568178 (* 1 = 0.568178 loss)
I1107 11:10:12.668584 11160 sgd_solver.cpp:105] Iteration 34900, lr = 0.1
I1107 11:10:20.740670  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:10:21.075670 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_35000.caffemodel
I1107 11:10:21.106672 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_35000.solverstate
I1107 11:10:21.124670 11160 solver.cpp:330] Iteration 35000, Testing net (#0)
I1107 11:10:21.124670 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:10:23.109670 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:10:23.189175 11160 solver.cpp:397]     Test net output #0: accuracy = 0.715
I1107 11:10:23.189175 11160 solver.cpp:397]     Test net output #1: loss = 0.85792 (* 1 = 0.85792 loss)
I1107 11:10:23.269670 11160 solver.cpp:218] Iteration 35000 (9.43323 iter/s, 10.6008s/100 iters), loss = 0.683435
I1107 11:10:23.269670 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:10:23.269670 11160 solver.cpp:237]     Train net output #1: loss = 0.683435 (* 1 = 0.683435 loss)
I1107 11:10:23.269670 11160 sgd_solver.cpp:105] Iteration 35000, lr = 0.1
I1107 11:10:31.763470 11160 solver.cpp:218] Iteration 35100 (11.7742 iter/s, 8.49313s/100 iters), loss = 0.636425
I1107 11:10:31.763470 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:10:31.763470 11160 solver.cpp:237]     Train net output #1: loss = 0.636425 (* 1 = 0.636425 loss)
I1107 11:10:31.763470 11160 sgd_solver.cpp:105] Iteration 35100, lr = 0.1
I1107 11:10:40.257382 11160 solver.cpp:218] Iteration 35200 (11.7737 iter/s, 8.49354s/100 iters), loss = 0.723348
I1107 11:10:40.257382 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:10:40.257884 11160 solver.cpp:237]     Train net output #1: loss = 0.723348 (* 1 = 0.723348 loss)
I1107 11:10:40.257884 11160 sgd_solver.cpp:105] Iteration 35200, lr = 0.1
I1107 11:10:48.751600 11160 solver.cpp:218] Iteration 35300 (11.7738 iter/s, 8.49346s/100 iters), loss = 0.554119
I1107 11:10:48.751600 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:10:48.751600 11160 solver.cpp:237]     Train net output #1: loss = 0.554119 (* 1 = 0.554119 loss)
I1107 11:10:48.751600 11160 sgd_solver.cpp:105] Iteration 35300, lr = 0.1
I1107 11:10:57.241240 11160 solver.cpp:218] Iteration 35400 (11.7794 iter/s, 8.48939s/100 iters), loss = 0.659691
I1107 11:10:57.241240 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:10:57.241240 11160 solver.cpp:237]     Train net output #1: loss = 0.659691 (* 1 = 0.659691 loss)
I1107 11:10:57.241740 11160 sgd_solver.cpp:105] Iteration 35400, lr = 0.1
I1107 11:11:05.316881  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:11:05.651877 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_35500.caffemodel
I1107 11:11:05.686378 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_35500.solverstate
I1107 11:11:05.695376 11160 solver.cpp:330] Iteration 35500, Testing net (#0)
I1107 11:11:05.695376 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:11:07.680377 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:11:07.760377 11160 solver.cpp:397]     Test net output #0: accuracy = 0.594
I1107 11:11:07.760377 11160 solver.cpp:397]     Test net output #1: loss = 1.25423 (* 1 = 1.25423 loss)
I1107 11:11:07.841876 11160 solver.cpp:218] Iteration 35500 (9.434 iter/s, 10.6s/100 iters), loss = 0.623107
I1107 11:11:07.841876 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:11:07.841876 11160 solver.cpp:237]     Train net output #1: loss = 0.623107 (* 1 = 0.623107 loss)
I1107 11:11:07.841876 11160 sgd_solver.cpp:105] Iteration 35500, lr = 0.1
I1107 11:11:16.344877 11160 solver.cpp:218] Iteration 35600 (11.7611 iter/s, 8.50258s/100 iters), loss = 0.477189
I1107 11:11:16.344877 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 11:11:16.344877 11160 solver.cpp:237]     Train net output #1: loss = 0.477189 (* 1 = 0.477189 loss)
I1107 11:11:16.344877 11160 sgd_solver.cpp:105] Iteration 35600, lr = 0.1
I1107 11:11:24.850457 11160 solver.cpp:218] Iteration 35700 (11.758 iter/s, 8.50485s/100 iters), loss = 0.681622
I1107 11:11:24.850457 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1107 11:11:24.850457 11160 solver.cpp:237]     Train net output #1: loss = 0.681622 (* 1 = 0.681622 loss)
I1107 11:11:24.850457 11160 sgd_solver.cpp:105] Iteration 35700, lr = 0.1
I1107 11:11:33.353632 11160 solver.cpp:218] Iteration 35800 (11.7607 iter/s, 8.50288s/100 iters), loss = 0.529691
I1107 11:11:33.354132 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:11:33.354132 11160 solver.cpp:237]     Train net output #1: loss = 0.529691 (* 1 = 0.529691 loss)
I1107 11:11:33.354132 11160 sgd_solver.cpp:105] Iteration 35800, lr = 0.1
I1107 11:11:41.852675 11160 solver.cpp:218] Iteration 35900 (11.7668 iter/s, 8.4985s/100 iters), loss = 0.53507
I1107 11:11:41.853175 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:11:41.853175 11160 solver.cpp:237]     Train net output #1: loss = 0.53507 (* 1 = 0.53507 loss)
I1107 11:11:41.853175 11160 sgd_solver.cpp:105] Iteration 35900, lr = 0.1
I1107 11:11:49.929267  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:11:50.266273 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_36000.caffemodel
I1107 11:11:50.322273 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_36000.solverstate
I1107 11:11:50.331291 11160 solver.cpp:330] Iteration 36000, Testing net (#0)
I1107 11:11:50.331774 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:11:52.317773 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:11:52.397274 11160 solver.cpp:397]     Test net output #0: accuracy = 0.666
I1107 11:11:52.397274 11160 solver.cpp:397]     Test net output #1: loss = 0.989642 (* 1 = 0.989642 loss)
I1107 11:11:52.478772 11160 solver.cpp:218] Iteration 36000 (9.41171 iter/s, 10.6251s/100 iters), loss = 0.62067
I1107 11:11:52.478772 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:11:52.478772 11160 solver.cpp:237]     Train net output #1: loss = 0.62067 (* 1 = 0.62067 loss)
I1107 11:11:52.478772 11160 sgd_solver.cpp:105] Iteration 36000, lr = 0.1
I1107 11:12:00.977023 11160 solver.cpp:218] Iteration 36100 (11.7677 iter/s, 8.49781s/100 iters), loss = 0.605558
I1107 11:12:00.977023 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:12:00.977023 11160 solver.cpp:237]     Train net output #1: loss = 0.605558 (* 1 = 0.605558 loss)
I1107 11:12:00.977023 11160 sgd_solver.cpp:105] Iteration 36100, lr = 0.1
I1107 11:12:09.480648 11160 solver.cpp:218] Iteration 36200 (11.7604 iter/s, 8.50311s/100 iters), loss = 0.598291
I1107 11:12:09.480648 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:12:09.480648 11160 solver.cpp:237]     Train net output #1: loss = 0.598291 (* 1 = 0.598291 loss)
I1107 11:12:09.480648 11160 sgd_solver.cpp:105] Iteration 36200, lr = 0.1
I1107 11:12:17.984522 11160 solver.cpp:218] Iteration 36300 (11.7601 iter/s, 8.50336s/100 iters), loss = 0.620776
I1107 11:12:17.984522 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:12:17.984522 11160 solver.cpp:237]     Train net output #1: loss = 0.620776 (* 1 = 0.620776 loss)
I1107 11:12:17.984522 11160 sgd_solver.cpp:105] Iteration 36300, lr = 0.1
I1107 11:12:26.491188 11160 solver.cpp:218] Iteration 36400 (11.7563 iter/s, 8.5061s/100 iters), loss = 0.48605
I1107 11:12:26.491188 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:12:26.491188 11160 solver.cpp:237]     Train net output #1: loss = 0.48605 (* 1 = 0.48605 loss)
I1107 11:12:26.491188 11160 sgd_solver.cpp:105] Iteration 36400, lr = 0.1
I1107 11:12:34.579780  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:12:34.917780 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_36500.caffemodel
I1107 11:12:34.946779 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_36500.solverstate
I1107 11:12:34.983279 11160 solver.cpp:330] Iteration 36500, Testing net (#0)
I1107 11:12:34.983279 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:12:36.969280 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:12:37.049279 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5292
I1107 11:12:37.049279 11160 solver.cpp:397]     Test net output #1: loss = 1.46922 (* 1 = 1.46922 loss)
I1107 11:12:37.130285 11160 solver.cpp:218] Iteration 36500 (9.3995 iter/s, 10.6389s/100 iters), loss = 0.625074
I1107 11:12:37.130285 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:12:37.130285 11160 solver.cpp:237]     Train net output #1: loss = 0.625074 (* 1 = 0.625074 loss)
I1107 11:12:37.130285 11160 sgd_solver.cpp:105] Iteration 36500, lr = 0.1
I1107 11:12:45.629319 11160 solver.cpp:218] Iteration 36600 (11.7669 iter/s, 8.4984s/100 iters), loss = 0.588382
I1107 11:12:45.629319 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:12:45.629319 11160 solver.cpp:237]     Train net output #1: loss = 0.588382 (* 1 = 0.588382 loss)
I1107 11:12:45.629319 11160 sgd_solver.cpp:105] Iteration 36600, lr = 0.1
I1107 11:12:54.125411 11160 solver.cpp:218] Iteration 36700 (11.771 iter/s, 8.49548s/100 iters), loss = 0.745861
I1107 11:12:54.125411 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:12:54.125411 11160 solver.cpp:237]     Train net output #1: loss = 0.745861 (* 1 = 0.745861 loss)
I1107 11:12:54.125411 11160 sgd_solver.cpp:105] Iteration 36700, lr = 0.1
I1107 11:13:02.636466 11160 solver.cpp:218] Iteration 36800 (11.7501 iter/s, 8.51059s/100 iters), loss = 0.527974
I1107 11:13:02.636466 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 11:13:02.636466 11160 solver.cpp:237]     Train net output #1: loss = 0.527974 (* 1 = 0.527974 loss)
I1107 11:13:02.636466 11160 sgd_solver.cpp:105] Iteration 36800, lr = 0.1
I1107 11:13:11.136068 11160 solver.cpp:218] Iteration 36900 (11.7661 iter/s, 8.49898s/100 iters), loss = 0.501209
I1107 11:13:11.136068 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 11:13:11.136068 11160 solver.cpp:237]     Train net output #1: loss = 0.501209 (* 1 = 0.501209 loss)
I1107 11:13:11.136068 11160 sgd_solver.cpp:105] Iteration 36900, lr = 0.1
I1107 11:13:19.219225  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:13:19.553725 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_37000.caffemodel
I1107 11:13:19.587226 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_37000.solverstate
I1107 11:13:19.596226 11160 solver.cpp:330] Iteration 37000, Testing net (#0)
I1107 11:13:19.596724 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:13:21.582726 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:13:21.662724 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6662
I1107 11:13:21.662724 11160 solver.cpp:397]     Test net output #1: loss = 1.00598 (* 1 = 1.00598 loss)
I1107 11:13:21.744240 11160 solver.cpp:218] Iteration 37000 (9.42714 iter/s, 10.6077s/100 iters), loss = 0.673784
I1107 11:13:21.744240 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:13:21.744240 11160 solver.cpp:237]     Train net output #1: loss = 0.673784 (* 1 = 0.673784 loss)
I1107 11:13:21.744240 11160 sgd_solver.cpp:105] Iteration 37000, lr = 0.1
I1107 11:13:30.250349 11160 solver.cpp:218] Iteration 37100 (11.7568 iter/s, 8.50573s/100 iters), loss = 0.618281
I1107 11:13:30.250349 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:13:30.250850 11160 solver.cpp:237]     Train net output #1: loss = 0.618281 (* 1 = 0.618281 loss)
I1107 11:13:30.250850 11160 sgd_solver.cpp:105] Iteration 37100, lr = 0.1
I1107 11:13:38.966179 11160 solver.cpp:218] Iteration 37200 (11.474 iter/s, 8.71535s/100 iters), loss = 0.58402
I1107 11:13:38.966680 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:13:38.966680 11160 solver.cpp:237]     Train net output #1: loss = 0.58402 (* 1 = 0.58402 loss)
I1107 11:13:38.966680 11160 sgd_solver.cpp:105] Iteration 37200, lr = 0.1
I1107 11:13:47.581346 11160 solver.cpp:218] Iteration 37300 (11.6085 iter/s, 8.61436s/100 iters), loss = 0.644811
I1107 11:13:47.581346 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:13:47.581346 11160 solver.cpp:237]     Train net output #1: loss = 0.644811 (* 1 = 0.644811 loss)
I1107 11:13:47.581346 11160 sgd_solver.cpp:105] Iteration 37300, lr = 0.1
I1107 11:13:56.196915 11160 solver.cpp:218] Iteration 37400 (11.6077 iter/s, 8.615s/100 iters), loss = 0.510252
I1107 11:13:56.196915 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1107 11:13:56.196915 11160 solver.cpp:237]     Train net output #1: loss = 0.510252 (* 1 = 0.510252 loss)
I1107 11:13:56.196915 11160 sgd_solver.cpp:105] Iteration 37400, lr = 0.1
I1107 11:14:04.583313  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:14:04.924821 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_37500.caffemodel
I1107 11:14:04.954322 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_37500.solverstate
I1107 11:14:04.983829 11160 solver.cpp:330] Iteration 37500, Testing net (#0)
I1107 11:14:04.984321 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:14:06.987820 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:14:07.067342 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6022
I1107 11:14:07.067342 11160 solver.cpp:397]     Test net output #1: loss = 1.22066 (* 1 = 1.22066 loss)
I1107 11:14:07.149322 11160 solver.cpp:218] Iteration 37500 (9.13127 iter/s, 10.9514s/100 iters), loss = 0.596761
I1107 11:14:07.149322 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:14:07.149322 11160 solver.cpp:237]     Train net output #1: loss = 0.596761 (* 1 = 0.596761 loss)
I1107 11:14:07.149322 11160 sgd_solver.cpp:105] Iteration 37500, lr = 0.1
I1107 11:14:15.722015 11160 solver.cpp:218] Iteration 37600 (11.665 iter/s, 8.57266s/100 iters), loss = 0.491215
I1107 11:14:15.722015 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:14:15.722015 11160 solver.cpp:237]     Train net output #1: loss = 0.491215 (* 1 = 0.491215 loss)
I1107 11:14:15.722015 11160 sgd_solver.cpp:105] Iteration 37600, lr = 0.1
I1107 11:14:24.297472 11160 solver.cpp:218] Iteration 37700 (11.6625 iter/s, 8.57453s/100 iters), loss = 0.61858
I1107 11:14:24.297472 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:14:24.297472 11160 solver.cpp:237]     Train net output #1: loss = 0.61858 (* 1 = 0.61858 loss)
I1107 11:14:24.297472 11160 sgd_solver.cpp:105] Iteration 37700, lr = 0.1
I1107 11:14:32.926805 11160 solver.cpp:218] Iteration 37800 (11.5888 iter/s, 8.62903s/100 iters), loss = 0.540962
I1107 11:14:32.926805 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:14:32.926805 11160 solver.cpp:237]     Train net output #1: loss = 0.540962 (* 1 = 0.540962 loss)
I1107 11:14:32.926805 11160 sgd_solver.cpp:105] Iteration 37800, lr = 0.1
I1107 11:14:41.550999 11160 solver.cpp:218] Iteration 37900 (11.5962 iter/s, 8.62354s/100 iters), loss = 0.639617
I1107 11:14:41.550999 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:14:41.550999 11160 solver.cpp:237]     Train net output #1: loss = 0.639617 (* 1 = 0.639617 loss)
I1107 11:14:41.550999 11160 sgd_solver.cpp:105] Iteration 37900, lr = 0.1
I1107 11:14:49.757721  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:14:50.103747 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_38000.caffemodel
I1107 11:14:50.137251 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_38000.solverstate
I1107 11:14:50.145757 11160 solver.cpp:330] Iteration 38000, Testing net (#0)
I1107 11:14:50.145757 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:14:52.157855 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:14:52.238354 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6197
I1107 11:14:52.238354 11160 solver.cpp:397]     Test net output #1: loss = 1.21641 (* 1 = 1.21641 loss)
I1107 11:14:52.319356 11160 solver.cpp:218] Iteration 38000 (9.28712 iter/s, 10.7676s/100 iters), loss = 0.591088
I1107 11:14:52.319356 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:14:52.319356 11160 solver.cpp:237]     Train net output #1: loss = 0.591088 (* 1 = 0.591088 loss)
I1107 11:14:52.319356 11160 sgd_solver.cpp:105] Iteration 38000, lr = 0.1
I1107 11:15:00.852533 11160 solver.cpp:218] Iteration 38100 (11.7191 iter/s, 8.53311s/100 iters), loss = 0.502846
I1107 11:15:00.852533 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 11:15:00.852533 11160 solver.cpp:237]     Train net output #1: loss = 0.502846 (* 1 = 0.502846 loss)
I1107 11:15:00.852533 11160 sgd_solver.cpp:105] Iteration 38100, lr = 0.1
I1107 11:15:09.385107 11160 solver.cpp:218] Iteration 38200 (11.721 iter/s, 8.53166s/100 iters), loss = 0.680418
I1107 11:15:09.385107 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:15:09.385107 11160 solver.cpp:237]     Train net output #1: loss = 0.680418 (* 1 = 0.680418 loss)
I1107 11:15:09.385107 11160 sgd_solver.cpp:105] Iteration 38200, lr = 0.1
I1107 11:15:17.947671 11160 solver.cpp:218] Iteration 38300 (11.6798 iter/s, 8.56182s/100 iters), loss = 0.540628
I1107 11:15:17.947671 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:15:17.947671 11160 solver.cpp:237]     Train net output #1: loss = 0.540628 (* 1 = 0.540628 loss)
I1107 11:15:17.947671 11160 sgd_solver.cpp:105] Iteration 38300, lr = 0.1
I1107 11:15:26.479317 11160 solver.cpp:218] Iteration 38400 (11.7217 iter/s, 8.5312s/100 iters), loss = 0.541137
I1107 11:15:26.479317 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:15:26.479317 11160 solver.cpp:237]     Train net output #1: loss = 0.541137 (* 1 = 0.541137 loss)
I1107 11:15:26.479317 11160 sgd_solver.cpp:105] Iteration 38400, lr = 0.1
I1107 11:15:34.585880  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:15:34.922878 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_38500.caffemodel
I1107 11:15:34.952378 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_38500.solverstate
I1107 11:15:34.982879 11160 solver.cpp:330] Iteration 38500, Testing net (#0)
I1107 11:15:34.983379 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:15:36.974378 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:15:37.054914 11160 solver.cpp:397]     Test net output #0: accuracy = 0.7088
I1107 11:15:37.054914 11160 solver.cpp:397]     Test net output #1: loss = 0.912463 (* 1 = 0.912463 loss)
I1107 11:15:37.136379 11160 solver.cpp:218] Iteration 38500 (9.38408 iter/s, 10.6564s/100 iters), loss = 0.542239
I1107 11:15:37.136379 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 11:15:37.136379 11160 solver.cpp:237]     Train net output #1: loss = 0.542239 (* 1 = 0.542239 loss)
I1107 11:15:37.136379 11160 sgd_solver.cpp:105] Iteration 38500, lr = 0.1
I1107 11:15:45.664460 11160 solver.cpp:218] Iteration 38600 (11.7266 iter/s, 8.52763s/100 iters), loss = 0.607416
I1107 11:15:45.664460 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:15:45.664460 11160 solver.cpp:237]     Train net output #1: loss = 0.607416 (* 1 = 0.607416 loss)
I1107 11:15:45.664460 11160 sgd_solver.cpp:105] Iteration 38600, lr = 0.1
I1107 11:15:54.194736 11160 solver.cpp:218] Iteration 38700 (11.7232 iter/s, 8.53007s/100 iters), loss = 0.638533
I1107 11:15:54.195237 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:15:54.195237 11160 solver.cpp:237]     Train net output #1: loss = 0.638533 (* 1 = 0.638533 loss)
I1107 11:15:54.195237 11160 sgd_solver.cpp:105] Iteration 38700, lr = 0.1
I1107 11:16:02.775301 11160 solver.cpp:218] Iteration 38800 (11.6553 iter/s, 8.57976s/100 iters), loss = 0.621211
I1107 11:16:02.775301 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:16:02.775301 11160 solver.cpp:237]     Train net output #1: loss = 0.621211 (* 1 = 0.621211 loss)
I1107 11:16:02.775301 11160 sgd_solver.cpp:105] Iteration 38800, lr = 0.1
I1107 11:16:11.291394 11160 solver.cpp:218] Iteration 38900 (11.7428 iter/s, 8.51585s/100 iters), loss = 0.640826
I1107 11:16:11.291894 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:16:11.291894 11160 solver.cpp:237]     Train net output #1: loss = 0.640826 (* 1 = 0.640826 loss)
I1107 11:16:11.291894 11160 sgd_solver.cpp:105] Iteration 38900, lr = 0.1
I1107 11:16:19.392524  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:16:19.731523 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_39000.caffemodel
I1107 11:16:19.762022 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_39000.solverstate
I1107 11:16:19.771023 11160 solver.cpp:330] Iteration 39000, Testing net (#0)
I1107 11:16:19.771023 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:16:21.763576 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:16:21.843075 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5673
I1107 11:16:21.843075 11160 solver.cpp:397]     Test net output #1: loss = 1.4448 (* 1 = 1.4448 loss)
I1107 11:16:21.924073 11160 solver.cpp:218] Iteration 39000 (9.40576 iter/s, 10.6318s/100 iters), loss = 0.578479
I1107 11:16:21.924073 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:16:21.924073 11160 solver.cpp:237]     Train net output #1: loss = 0.578479 (* 1 = 0.578479 loss)
I1107 11:16:21.924073 11160 sgd_solver.cpp:105] Iteration 39000, lr = 0.1
I1107 11:16:30.465324 11160 solver.cpp:218] Iteration 39100 (11.7085 iter/s, 8.54078s/100 iters), loss = 0.632438
I1107 11:16:30.465324 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:16:30.465324 11160 solver.cpp:237]     Train net output #1: loss = 0.632438 (* 1 = 0.632438 loss)
I1107 11:16:30.465324 11160 sgd_solver.cpp:105] Iteration 39100, lr = 0.1
I1107 11:16:39.002837 11160 solver.cpp:218] Iteration 39200 (11.7137 iter/s, 8.53703s/100 iters), loss = 0.627724
I1107 11:16:39.002837 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:16:39.002837 11160 solver.cpp:237]     Train net output #1: loss = 0.627724 (* 1 = 0.627724 loss)
I1107 11:16:39.002837 11160 sgd_solver.cpp:105] Iteration 39200, lr = 0.1
I1107 11:16:47.532001 11160 solver.cpp:218] Iteration 39300 (11.7253 iter/s, 8.52855s/100 iters), loss = 0.581704
I1107 11:16:47.532001 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:16:47.532001 11160 solver.cpp:237]     Train net output #1: loss = 0.581704 (* 1 = 0.581704 loss)
I1107 11:16:47.532001 11160 sgd_solver.cpp:105] Iteration 39300, lr = 0.1
I1107 11:16:56.068181 11160 solver.cpp:218] Iteration 39400 (11.7158 iter/s, 8.53551s/100 iters), loss = 0.559561
I1107 11:16:56.068181 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:16:56.068181 11160 solver.cpp:237]     Train net output #1: loss = 0.559561 (* 1 = 0.559561 loss)
I1107 11:16:56.068181 11160 sgd_solver.cpp:105] Iteration 39400, lr = 0.1
I1107 11:17:04.177014  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:17:04.513512 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_39500.caffemodel
I1107 11:17:04.547013 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_39500.solverstate
I1107 11:17:04.585016 11160 solver.cpp:330] Iteration 39500, Testing net (#0)
I1107 11:17:04.585515 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:17:06.588636 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:17:06.668643 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6884
I1107 11:17:06.668643 11160 solver.cpp:397]     Test net output #1: loss = 0.914329 (* 1 = 0.914329 loss)
I1107 11:17:06.749650 11160 solver.cpp:218] Iteration 39500 (9.36236 iter/s, 10.6811s/100 iters), loss = 0.562961
I1107 11:17:06.749650 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:17:06.749650 11160 solver.cpp:237]     Train net output #1: loss = 0.562961 (* 1 = 0.562961 loss)
I1107 11:17:06.749650 11160 sgd_solver.cpp:105] Iteration 39500, lr = 0.1
I1107 11:17:15.353864 11160 solver.cpp:218] Iteration 39600 (11.6229 iter/s, 8.6037s/100 iters), loss = 0.557878
I1107 11:17:15.353864 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 11:17:15.353864 11160 solver.cpp:237]     Train net output #1: loss = 0.557878 (* 1 = 0.557878 loss)
I1107 11:17:15.353864 11160 sgd_solver.cpp:105] Iteration 39600, lr = 0.1
I1107 11:17:23.907272 11160 solver.cpp:218] Iteration 39700 (11.692 iter/s, 8.55284s/100 iters), loss = 0.561244
I1107 11:17:23.907272 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:17:23.907272 11160 solver.cpp:237]     Train net output #1: loss = 0.561244 (* 1 = 0.561244 loss)
I1107 11:17:23.907272 11160 sgd_solver.cpp:105] Iteration 39700, lr = 0.1
I1107 11:17:32.483639 11160 solver.cpp:218] Iteration 39800 (11.6599 iter/s, 8.57642s/100 iters), loss = 0.608574
I1107 11:17:32.483639 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:17:32.483639 11160 solver.cpp:237]     Train net output #1: loss = 0.608574 (* 1 = 0.608574 loss)
I1107 11:17:32.483639 11160 sgd_solver.cpp:105] Iteration 39800, lr = 0.1
I1107 11:17:41.098973 11160 solver.cpp:218] Iteration 39900 (11.6086 iter/s, 8.61429s/100 iters), loss = 0.633337
I1107 11:17:41.098973 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:17:41.098973 11160 solver.cpp:237]     Train net output #1: loss = 0.633337 (* 1 = 0.633337 loss)
I1107 11:17:41.098973 11160 sgd_solver.cpp:105] Iteration 39900, lr = 0.1
I1107 11:17:49.297896  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:17:49.635918 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_40000.caffemodel
I1107 11:17:49.666939 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_40000.solverstate
I1107 11:17:49.676424 11160 solver.cpp:330] Iteration 40000, Testing net (#0)
I1107 11:17:49.676424 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:17:51.691145 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:17:51.771162 11160 solver.cpp:397]     Test net output #0: accuracy = 0.4474
I1107 11:17:51.771162 11160 solver.cpp:397]     Test net output #1: loss = 1.94479 (* 1 = 1.94479 loss)
I1107 11:17:51.853168 11160 solver.cpp:218] Iteration 40000 (9.29871 iter/s, 10.7542s/100 iters), loss = 0.740177
I1107 11:17:51.854168 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:17:51.854168 11160 solver.cpp:237]     Train net output #1: loss = 0.740177 (* 1 = 0.740177 loss)
I1107 11:17:51.854168 11160 sgd_solver.cpp:105] Iteration 40000, lr = 0.1
I1107 11:18:00.424376 11160 solver.cpp:218] Iteration 40100 (11.6685 iter/s, 8.57007s/100 iters), loss = 0.48075
I1107 11:18:00.424376 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 11:18:00.424376 11160 solver.cpp:237]     Train net output #1: loss = 0.48075 (* 1 = 0.48075 loss)
I1107 11:18:00.424376 11160 sgd_solver.cpp:105] Iteration 40100, lr = 0.1
I1107 11:18:09.004537 11160 solver.cpp:218] Iteration 40200 (11.6555 iter/s, 8.57966s/100 iters), loss = 0.601355
I1107 11:18:09.004537 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:18:09.004537 11160 solver.cpp:237]     Train net output #1: loss = 0.601355 (* 1 = 0.601355 loss)
I1107 11:18:09.004537 11160 sgd_solver.cpp:105] Iteration 40200, lr = 0.1
I1107 11:18:17.550539 11160 solver.cpp:218] Iteration 40300 (11.7022 iter/s, 8.54537s/100 iters), loss = 0.635165
I1107 11:18:17.550539 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:18:17.550539 11160 solver.cpp:237]     Train net output #1: loss = 0.635165 (* 1 = 0.635165 loss)
I1107 11:18:17.550539 11160 sgd_solver.cpp:105] Iteration 40300, lr = 0.1
I1107 11:18:26.098130 11160 solver.cpp:218] Iteration 40400 (11.6999 iter/s, 8.54711s/100 iters), loss = 0.607223
I1107 11:18:26.098130 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:18:26.098130 11160 solver.cpp:237]     Train net output #1: loss = 0.607223 (* 1 = 0.607223 loss)
I1107 11:18:26.098130 11160 sgd_solver.cpp:105] Iteration 40400, lr = 0.1
I1107 11:18:34.220067  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:18:34.556115 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_40500.caffemodel
I1107 11:18:34.590119 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_40500.solverstate
I1107 11:18:34.624115 11160 solver.cpp:330] Iteration 40500, Testing net (#0)
I1107 11:18:34.624115 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:18:36.610297 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:18:36.690304 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6931
I1107 11:18:36.690304 11160 solver.cpp:397]     Test net output #1: loss = 0.916755 (* 1 = 0.916755 loss)
I1107 11:18:36.771307 11160 solver.cpp:218] Iteration 40500 (9.36943 iter/s, 10.673s/100 iters), loss = 0.591018
I1107 11:18:36.771307 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:18:36.771307 11160 solver.cpp:237]     Train net output #1: loss = 0.591018 (* 1 = 0.591018 loss)
I1107 11:18:36.771307 11160 sgd_solver.cpp:105] Iteration 40500, lr = 0.1
I1107 11:18:45.298837 11160 solver.cpp:218] Iteration 40600 (11.7283 iter/s, 8.52642s/100 iters), loss = 0.551947
I1107 11:18:45.298837 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 11:18:45.298837 11160 solver.cpp:237]     Train net output #1: loss = 0.551947 (* 1 = 0.551947 loss)
I1107 11:18:45.298837 11160 sgd_solver.cpp:105] Iteration 40600, lr = 0.1
I1107 11:18:53.839679 11160 solver.cpp:218] Iteration 40700 (11.7091 iter/s, 8.54033s/100 iters), loss = 0.545774
I1107 11:18:53.839679 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1107 11:18:53.839679 11160 solver.cpp:237]     Train net output #1: loss = 0.545774 (* 1 = 0.545774 loss)
I1107 11:18:53.839679 11160 sgd_solver.cpp:105] Iteration 40700, lr = 0.1
I1107 11:19:02.386441 11160 solver.cpp:218] Iteration 40800 (11.7006 iter/s, 8.54655s/100 iters), loss = 0.801076
I1107 11:19:02.386441 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 11:19:02.386441 11160 solver.cpp:237]     Train net output #1: loss = 0.801076 (* 1 = 0.801076 loss)
I1107 11:19:02.386441 11160 sgd_solver.cpp:105] Iteration 40800, lr = 0.1
I1107 11:19:10.919322 11160 solver.cpp:218] Iteration 40900 (11.7205 iter/s, 8.53205s/100 iters), loss = 0.494209
I1107 11:19:10.919322 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:19:10.919322 11160 solver.cpp:237]     Train net output #1: loss = 0.494209 (* 1 = 0.494209 loss)
I1107 11:19:10.919322 11160 sgd_solver.cpp:105] Iteration 40900, lr = 0.1
I1107 11:19:19.043738  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:19:19.380761 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_41000.caffemodel
I1107 11:19:19.410780 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_41000.solverstate
I1107 11:19:19.418779 11160 solver.cpp:330] Iteration 41000, Testing net (#0)
I1107 11:19:19.418779 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:19:21.429955 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:19:21.509959 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6698
I1107 11:19:21.509959 11160 solver.cpp:397]     Test net output #1: loss = 0.998111 (* 1 = 0.998111 loss)
I1107 11:19:21.590965 11160 solver.cpp:218] Iteration 41000 (9.37084 iter/s, 10.6714s/100 iters), loss = 0.589741
I1107 11:19:21.590965 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:19:21.590965 11160 solver.cpp:237]     Train net output #1: loss = 0.589741 (* 1 = 0.589741 loss)
I1107 11:19:21.590965 11160 sgd_solver.cpp:105] Iteration 41000, lr = 0.1
I1107 11:19:30.144064 11160 solver.cpp:218] Iteration 41100 (11.6918 iter/s, 8.553s/100 iters), loss = 0.492565
I1107 11:19:30.144064 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 11:19:30.144064 11160 solver.cpp:237]     Train net output #1: loss = 0.492565 (* 1 = 0.492565 loss)
I1107 11:19:30.144064 11160 sgd_solver.cpp:105] Iteration 41100, lr = 0.1
I1107 11:19:38.768491 11160 solver.cpp:218] Iteration 41200 (11.5963 iter/s, 8.62343s/100 iters), loss = 0.642497
I1107 11:19:38.768491 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 11:19:38.768491 11160 solver.cpp:237]     Train net output #1: loss = 0.642497 (* 1 = 0.642497 loss)
I1107 11:19:38.768491 11160 sgd_solver.cpp:105] Iteration 41200, lr = 0.1
I1107 11:19:47.383489 11160 solver.cpp:218] Iteration 41300 (11.6076 iter/s, 8.61501s/100 iters), loss = 0.590623
I1107 11:19:47.384490 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:19:47.384490 11160 solver.cpp:237]     Train net output #1: loss = 0.590623 (* 1 = 0.590623 loss)
I1107 11:19:47.384490 11160 sgd_solver.cpp:105] Iteration 41300, lr = 0.1
I1107 11:19:55.988710 11160 solver.cpp:218] Iteration 41400 (11.6222 iter/s, 8.60419s/100 iters), loss = 0.582104
I1107 11:19:55.988710 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:19:55.988710 11160 solver.cpp:237]     Train net output #1: loss = 0.582104 (* 1 = 0.582104 loss)
I1107 11:19:55.988710 11160 sgd_solver.cpp:105] Iteration 41400, lr = 0.1
I1107 11:20:04.230211  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:20:04.580081 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_41500.caffemodel
I1107 11:20:04.609176 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_41500.solverstate
I1107 11:20:04.650174 11160 solver.cpp:330] Iteration 41500, Testing net (#0)
I1107 11:20:04.650174 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:20:06.643210 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:20:06.723239 11160 solver.cpp:397]     Test net output #0: accuracy = 0.68
I1107 11:20:06.723239 11160 solver.cpp:397]     Test net output #1: loss = 0.956628 (* 1 = 0.956628 loss)
I1107 11:20:06.805250 11160 solver.cpp:218] Iteration 41500 (9.24577 iter/s, 10.8158s/100 iters), loss = 0.562778
I1107 11:20:06.805250 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:20:06.805250 11160 solver.cpp:237]     Train net output #1: loss = 0.562778 (* 1 = 0.562778 loss)
I1107 11:20:06.805250 11160 sgd_solver.cpp:105] Iteration 41500, lr = 0.1
I1107 11:20:15.458134 11160 solver.cpp:218] Iteration 41600 (11.5571 iter/s, 8.65271s/100 iters), loss = 0.525492
I1107 11:20:15.458134 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:20:15.458134 11160 solver.cpp:237]     Train net output #1: loss = 0.525492 (* 1 = 0.525492 loss)
I1107 11:20:15.458134 11160 sgd_solver.cpp:105] Iteration 41600, lr = 0.1
I1107 11:20:23.992697 11160 solver.cpp:218] Iteration 41700 (11.7182 iter/s, 8.5337s/100 iters), loss = 0.605344
I1107 11:20:23.992697 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:20:23.992697 11160 solver.cpp:237]     Train net output #1: loss = 0.605344 (* 1 = 0.605344 loss)
I1107 11:20:23.992697 11160 sgd_solver.cpp:105] Iteration 41700, lr = 0.1
I1107 11:20:32.565424 11160 solver.cpp:218] Iteration 41800 (11.6648 iter/s, 8.5728s/100 iters), loss = 0.605607
I1107 11:20:32.565424 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:20:32.565424 11160 solver.cpp:237]     Train net output #1: loss = 0.605607 (* 1 = 0.605607 loss)
I1107 11:20:32.565424 11160 sgd_solver.cpp:105] Iteration 41800, lr = 0.1
I1107 11:20:41.101531 11160 solver.cpp:218] Iteration 41900 (11.716 iter/s, 8.5353s/100 iters), loss = 0.608082
I1107 11:20:41.102030 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1107 11:20:41.102030 11160 solver.cpp:237]     Train net output #1: loss = 0.608082 (* 1 = 0.608082 loss)
I1107 11:20:41.102030 11160 sgd_solver.cpp:105] Iteration 41900, lr = 0.1
I1107 11:20:49.216876  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:20:49.554872 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_42000.caffemodel
I1107 11:20:49.583871 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_42000.solverstate
I1107 11:20:49.593379 11160 solver.cpp:330] Iteration 42000, Testing net (#0)
I1107 11:20:49.593379 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:20:51.584061 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:20:51.664084 11160 solver.cpp:397]     Test net output #0: accuracy = 0.7257
I1107 11:20:51.664084 11160 solver.cpp:397]     Test net output #1: loss = 0.830882 (* 1 = 0.830882 loss)
I1107 11:20:51.746098 11160 solver.cpp:218] Iteration 42000 (9.39521 iter/s, 10.6437s/100 iters), loss = 0.653284
I1107 11:20:51.746098 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:20:51.746098 11160 solver.cpp:237]     Train net output #1: loss = 0.653284 (* 1 = 0.653284 loss)
I1107 11:20:51.746098 11160 sgd_solver.cpp:105] Iteration 42000, lr = 0.1
I1107 11:21:00.401633 11160 solver.cpp:218] Iteration 42100 (11.5538 iter/s, 8.65517s/100 iters), loss = 0.499921
I1107 11:21:00.401633 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:21:00.401633 11160 solver.cpp:237]     Train net output #1: loss = 0.499921 (* 1 = 0.499921 loss)
I1107 11:21:00.401633 11160 sgd_solver.cpp:105] Iteration 42100, lr = 0.1
I1107 11:21:09.155336 11160 solver.cpp:218] Iteration 42200 (11.4249 iter/s, 8.75281s/100 iters), loss = 0.628263
I1107 11:21:09.155336 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:21:09.155336 11160 solver.cpp:237]     Train net output #1: loss = 0.628263 (* 1 = 0.628263 loss)
I1107 11:21:09.155336 11160 sgd_solver.cpp:105] Iteration 42200, lr = 0.1
I1107 11:21:17.816526 11160 solver.cpp:218] Iteration 42300 (11.5463 iter/s, 8.66076s/100 iters), loss = 0.643269
I1107 11:21:17.816526 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:21:17.816526 11160 solver.cpp:237]     Train net output #1: loss = 0.643269 (* 1 = 0.643269 loss)
I1107 11:21:17.816526 11160 sgd_solver.cpp:105] Iteration 42300, lr = 0.1
I1107 11:21:26.559054 11160 solver.cpp:218] Iteration 42400 (11.4389 iter/s, 8.74212s/100 iters), loss = 0.659922
I1107 11:21:26.559054 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:21:26.559054 11160 solver.cpp:237]     Train net output #1: loss = 0.659922 (* 1 = 0.659922 loss)
I1107 11:21:26.559054 11160 sgd_solver.cpp:105] Iteration 42400, lr = 0.1
I1107 11:21:34.677826  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:21:35.015682 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_42500.caffemodel
I1107 11:21:35.045783 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_42500.solverstate
I1107 11:21:35.098798 11160 solver.cpp:330] Iteration 42500, Testing net (#0)
I1107 11:21:35.098798 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:21:37.130244 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:21:37.210753 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5289
I1107 11:21:37.210753 11160 solver.cpp:397]     Test net output #1: loss = 1.62557 (* 1 = 1.62557 loss)
I1107 11:21:37.293012 11160 solver.cpp:218] Iteration 42500 (9.31702 iter/s, 10.733s/100 iters), loss = 0.631507
I1107 11:21:37.293012 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:21:37.293012 11160 solver.cpp:237]     Train net output #1: loss = 0.631507 (* 1 = 0.631507 loss)
I1107 11:21:37.293012 11160 sgd_solver.cpp:105] Iteration 42500, lr = 0.1
I1107 11:21:45.902432 11160 solver.cpp:218] Iteration 42600 (11.6158 iter/s, 8.60895s/100 iters), loss = 0.618737
I1107 11:21:45.902432 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:21:45.902432 11160 solver.cpp:237]     Train net output #1: loss = 0.618737 (* 1 = 0.618737 loss)
I1107 11:21:45.902432 11160 sgd_solver.cpp:105] Iteration 42600, lr = 0.1
I1107 11:21:54.573076 11160 solver.cpp:218] Iteration 42700 (11.5331 iter/s, 8.67068s/100 iters), loss = 0.592161
I1107 11:21:54.573076 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:21:54.573076 11160 solver.cpp:237]     Train net output #1: loss = 0.592161 (* 1 = 0.592161 loss)
I1107 11:21:54.573076 11160 sgd_solver.cpp:105] Iteration 42700, lr = 0.1
I1107 11:22:03.126816 11160 solver.cpp:218] Iteration 42800 (11.6923 iter/s, 8.55264s/100 iters), loss = 0.562431
I1107 11:22:03.126816 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:22:03.126816 11160 solver.cpp:237]     Train net output #1: loss = 0.562431 (* 1 = 0.562431 loss)
I1107 11:22:03.126816 11160 sgd_solver.cpp:105] Iteration 42800, lr = 0.1
I1107 11:22:11.659756 11160 solver.cpp:218] Iteration 42900 (11.7197 iter/s, 8.53267s/100 iters), loss = 0.579833
I1107 11:22:11.659756 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:22:11.659756 11160 solver.cpp:237]     Train net output #1: loss = 0.579833 (* 1 = 0.579833 loss)
I1107 11:22:11.659756 11160 sgd_solver.cpp:105] Iteration 42900, lr = 0.1
I1107 11:22:19.775589  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:22:20.111605 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_43000.caffemodel
I1107 11:22:20.140612 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_43000.solverstate
I1107 11:22:20.149615 11160 solver.cpp:330] Iteration 43000, Testing net (#0)
I1107 11:22:20.149615 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:22:22.138847 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:22:22.218849 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6109
I1107 11:22:22.218849 11160 solver.cpp:397]     Test net output #1: loss = 1.22725 (* 1 = 1.22725 loss)
I1107 11:22:22.298851 11160 solver.cpp:218] Iteration 43000 (9.39925 iter/s, 10.6391s/100 iters), loss = 0.689047
I1107 11:22:22.298851 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:22:22.298851 11160 solver.cpp:237]     Train net output #1: loss = 0.689047 (* 1 = 0.689047 loss)
I1107 11:22:22.299851 11160 sgd_solver.cpp:105] Iteration 43000, lr = 0.1
I1107 11:22:30.827188 11160 solver.cpp:218] Iteration 43100 (11.7274 iter/s, 8.52706s/100 iters), loss = 0.606105
I1107 11:22:30.827188 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:22:30.827188 11160 solver.cpp:237]     Train net output #1: loss = 0.606105 (* 1 = 0.606105 loss)
I1107 11:22:30.827188 11160 sgd_solver.cpp:105] Iteration 43100, lr = 0.1
I1107 11:22:39.349633 11160 solver.cpp:218] Iteration 43200 (11.7345 iter/s, 8.52191s/100 iters), loss = 0.643872
I1107 11:22:39.349633 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:22:39.349633 11160 solver.cpp:237]     Train net output #1: loss = 0.643872 (* 1 = 0.643872 loss)
I1107 11:22:39.349633 11160 sgd_solver.cpp:105] Iteration 43200, lr = 0.1
I1107 11:22:47.908849 11160 solver.cpp:218] Iteration 43300 (11.684 iter/s, 8.55869s/100 iters), loss = 0.587878
I1107 11:22:47.908849 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:22:47.908849 11160 solver.cpp:237]     Train net output #1: loss = 0.587878 (* 1 = 0.587878 loss)
I1107 11:22:47.908849 11160 sgd_solver.cpp:105] Iteration 43300, lr = 0.1
I1107 11:22:56.490473 11160 solver.cpp:218] Iteration 43400 (11.6533 iter/s, 8.58124s/100 iters), loss = 0.554132
I1107 11:22:56.490473 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:22:56.490473 11160 solver.cpp:237]     Train net output #1: loss = 0.554132 (* 1 = 0.554132 loss)
I1107 11:22:56.490473 11160 sgd_solver.cpp:105] Iteration 43400, lr = 0.1
I1107 11:23:04.591104  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:23:04.927654 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_43500.caffemodel
I1107 11:23:04.963757 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_43500.solverstate
I1107 11:23:04.999233 11160 solver.cpp:330] Iteration 43500, Testing net (#0)
I1107 11:23:04.999233 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:23:06.990978 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:23:07.071033 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6272
I1107 11:23:07.071033 11160 solver.cpp:397]     Test net output #1: loss = 1.14514 (* 1 = 1.14514 loss)
I1107 11:23:07.152578 11160 solver.cpp:218] Iteration 43500 (9.37942 iter/s, 10.6616s/100 iters), loss = 0.598276
I1107 11:23:07.152578 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:23:07.152578 11160 solver.cpp:237]     Train net output #1: loss = 0.598276 (* 1 = 0.598276 loss)
I1107 11:23:07.152578 11160 sgd_solver.cpp:105] Iteration 43500, lr = 0.1
I1107 11:23:15.709957 11160 solver.cpp:218] Iteration 43600 (11.6867 iter/s, 8.55675s/100 iters), loss = 0.553003
I1107 11:23:15.709957 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:23:15.709957 11160 solver.cpp:237]     Train net output #1: loss = 0.553003 (* 1 = 0.553003 loss)
I1107 11:23:15.709957 11160 sgd_solver.cpp:105] Iteration 43600, lr = 0.1
I1107 11:23:24.241653 11160 solver.cpp:218] Iteration 43700 (11.7217 iter/s, 8.53122s/100 iters), loss = 0.72165
I1107 11:23:24.241653 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:23:24.241653 11160 solver.cpp:237]     Train net output #1: loss = 0.72165 (* 1 = 0.72165 loss)
I1107 11:23:24.241653 11160 sgd_solver.cpp:105] Iteration 43700, lr = 0.1
I1107 11:23:32.789830 11160 solver.cpp:218] Iteration 43800 (11.6993 iter/s, 8.54753s/100 iters), loss = 0.453063
I1107 11:23:32.789830 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1107 11:23:32.789830 11160 solver.cpp:237]     Train net output #1: loss = 0.453063 (* 1 = 0.453063 loss)
I1107 11:23:32.789830 11160 sgd_solver.cpp:105] Iteration 43800, lr = 0.1
I1107 11:23:41.397275 11160 solver.cpp:218] Iteration 43900 (11.6182 iter/s, 8.60722s/100 iters), loss = 0.603728
I1107 11:23:41.397275 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:23:41.397275 11160 solver.cpp:237]     Train net output #1: loss = 0.603728 (* 1 = 0.603728 loss)
I1107 11:23:41.397275 11160 sgd_solver.cpp:105] Iteration 43900, lr = 0.1
I1107 11:23:49.503031  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:23:49.841065 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_44000.caffemodel
I1107 11:23:49.871109 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_44000.solverstate
I1107 11:23:49.880102 11160 solver.cpp:330] Iteration 44000, Testing net (#0)
I1107 11:23:49.880102 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:23:51.870893 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:23:51.950893 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6522
I1107 11:23:51.950893 11160 solver.cpp:397]     Test net output #1: loss = 1.07749 (* 1 = 1.07749 loss)
I1107 11:23:52.032933 11160 solver.cpp:218] Iteration 44000 (9.40324 iter/s, 10.6346s/100 iters), loss = 0.599987
I1107 11:23:52.032933 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:23:52.032933 11160 solver.cpp:237]     Train net output #1: loss = 0.599987 (* 1 = 0.599987 loss)
I1107 11:23:52.032933 11160 sgd_solver.cpp:105] Iteration 44000, lr = 0.1
I1107 11:24:00.559562 11160 solver.cpp:218] Iteration 44100 (11.7287 iter/s, 8.52613s/100 iters), loss = 0.522472
I1107 11:24:00.559562 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:24:00.559562 11160 solver.cpp:237]     Train net output #1: loss = 0.522472 (* 1 = 0.522472 loss)
I1107 11:24:00.559562 11160 sgd_solver.cpp:105] Iteration 44100, lr = 0.1
I1107 11:24:09.163276 11160 solver.cpp:218] Iteration 44200 (11.6231 iter/s, 8.60357s/100 iters), loss = 0.667155
I1107 11:24:09.163776 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:24:09.163776 11160 solver.cpp:237]     Train net output #1: loss = 0.667155 (* 1 = 0.667155 loss)
I1107 11:24:09.163776 11160 sgd_solver.cpp:105] Iteration 44200, lr = 0.1
I1107 11:24:17.740648 11160 solver.cpp:218] Iteration 44300 (11.66 iter/s, 8.57632s/100 iters), loss = 0.599123
I1107 11:24:17.740648 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:24:17.740648 11160 solver.cpp:237]     Train net output #1: loss = 0.599123 (* 1 = 0.599123 loss)
I1107 11:24:17.740648 11160 sgd_solver.cpp:105] Iteration 44300, lr = 0.1
I1107 11:24:26.269484 11160 solver.cpp:218] Iteration 44400 (11.7257 iter/s, 8.52829s/100 iters), loss = 0.527797
I1107 11:24:26.269484 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 11:24:26.269484 11160 solver.cpp:237]     Train net output #1: loss = 0.527797 (* 1 = 0.527797 loss)
I1107 11:24:26.269484 11160 sgd_solver.cpp:105] Iteration 44400, lr = 0.1
I1107 11:24:34.413223  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:24:34.749239 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_44500.caffemodel
I1107 11:24:34.782246 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_44500.solverstate
I1107 11:24:34.815246 11160 solver.cpp:330] Iteration 44500, Testing net (#0)
I1107 11:24:34.815246 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:24:36.815618 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:24:36.895620 11160 solver.cpp:397]     Test net output #0: accuracy = 0.7101
I1107 11:24:36.895620 11160 solver.cpp:397]     Test net output #1: loss = 0.870024 (* 1 = 0.870024 loss)
I1107 11:24:36.977638 11160 solver.cpp:218] Iteration 44500 (9.33901 iter/s, 10.7078s/100 iters), loss = 0.633496
I1107 11:24:36.977638 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:24:36.977638 11160 solver.cpp:237]     Train net output #1: loss = 0.633496 (* 1 = 0.633496 loss)
I1107 11:24:36.977638 11160 sgd_solver.cpp:105] Iteration 44500, lr = 0.1
I1107 11:24:45.546643 11160 solver.cpp:218] Iteration 44600 (11.6711 iter/s, 8.5682s/100 iters), loss = 0.612809
I1107 11:24:45.546643 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:24:45.546643 11160 solver.cpp:237]     Train net output #1: loss = 0.612809 (* 1 = 0.612809 loss)
I1107 11:24:45.546643 11160 sgd_solver.cpp:105] Iteration 44600, lr = 0.1
I1107 11:24:54.108757 11160 solver.cpp:218] Iteration 44700 (11.6797 iter/s, 8.56184s/100 iters), loss = 0.5881
I1107 11:24:54.108757 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:24:54.108757 11160 solver.cpp:237]     Train net output #1: loss = 0.5881 (* 1 = 0.5881 loss)
I1107 11:24:54.108757 11160 sgd_solver.cpp:105] Iteration 44700, lr = 0.1
I1107 11:25:02.678771 11160 solver.cpp:218] Iteration 44800 (11.6693 iter/s, 8.56951s/100 iters), loss = 0.624156
I1107 11:25:02.678771 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:25:02.678771 11160 solver.cpp:237]     Train net output #1: loss = 0.624156 (* 1 = 0.624156 loss)
I1107 11:25:02.678771 11160 sgd_solver.cpp:105] Iteration 44800, lr = 0.1
I1107 11:25:11.257021 11160 solver.cpp:218] Iteration 44900 (11.6584 iter/s, 8.57748s/100 iters), loss = 0.635277
I1107 11:25:11.257021 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:25:11.257021 11160 solver.cpp:237]     Train net output #1: loss = 0.635277 (* 1 = 0.635277 loss)
I1107 11:25:11.257021 11160 sgd_solver.cpp:105] Iteration 44900, lr = 0.1
I1107 11:25:19.509371  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:25:19.846411 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_45000.caffemodel
I1107 11:25:19.876394 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_45000.solverstate
I1107 11:25:19.885416 11160 solver.cpp:330] Iteration 45000, Testing net (#0)
I1107 11:25:19.885416 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:25:21.875639 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:25:21.955643 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6265
I1107 11:25:21.955643 11160 solver.cpp:397]     Test net output #1: loss = 1.12304 (* 1 = 1.12304 loss)
I1107 11:25:22.036649 11160 solver.cpp:218] Iteration 45000 (9.27654 iter/s, 10.7799s/100 iters), loss = 0.527113
I1107 11:25:22.036649 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:25:22.036649 11160 solver.cpp:237]     Train net output #1: loss = 0.527113 (* 1 = 0.527113 loss)
I1107 11:25:22.036649 11160 sgd_solver.cpp:105] Iteration 45000, lr = 0.1
I1107 11:25:30.608113 11160 solver.cpp:218] Iteration 45100 (11.6679 iter/s, 8.57053s/100 iters), loss = 0.567884
I1107 11:25:30.608613 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:25:30.608613 11160 solver.cpp:237]     Train net output #1: loss = 0.567884 (* 1 = 0.567884 loss)
I1107 11:25:30.608613 11160 sgd_solver.cpp:105] Iteration 45100, lr = 0.1
I1107 11:25:39.194087 11160 solver.cpp:218] Iteration 45200 (11.6472 iter/s, 8.58575s/100 iters), loss = 0.75094
I1107 11:25:39.194087 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1107 11:25:39.194087 11160 solver.cpp:237]     Train net output #1: loss = 0.75094 (* 1 = 0.75094 loss)
I1107 11:25:39.194087 11160 sgd_solver.cpp:105] Iteration 45200, lr = 0.1
I1107 11:25:47.775533 11160 solver.cpp:218] Iteration 45300 (11.6541 iter/s, 8.58068s/100 iters), loss = 0.511504
I1107 11:25:47.775533 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:25:47.775533 11160 solver.cpp:237]     Train net output #1: loss = 0.511504 (* 1 = 0.511504 loss)
I1107 11:25:47.775533 11160 sgd_solver.cpp:105] Iteration 45300, lr = 0.1
I1107 11:25:56.406666 11160 solver.cpp:218] Iteration 45400 (11.5863 iter/s, 8.63086s/100 iters), loss = 0.58951
I1107 11:25:56.406666 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:25:56.406666 11160 solver.cpp:237]     Train net output #1: loss = 0.58951 (* 1 = 0.58951 loss)
I1107 11:25:56.406666 11160 sgd_solver.cpp:105] Iteration 45400, lr = 0.1
I1107 11:26:04.542120  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:26:04.878398 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_45500.caffemodel
I1107 11:26:04.928407 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_45500.solverstate
I1107 11:26:04.938407 11160 solver.cpp:330] Iteration 45500, Testing net (#0)
I1107 11:26:04.938407 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:26:06.960587 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:26:07.040590 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6203
I1107 11:26:07.040590 11160 solver.cpp:397]     Test net output #1: loss = 1.15693 (* 1 = 1.15693 loss)
I1107 11:26:07.122092 11160 solver.cpp:218] Iteration 45500 (9.33324 iter/s, 10.7144s/100 iters), loss = 0.669417
I1107 11:26:07.122092 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:26:07.122092 11160 solver.cpp:237]     Train net output #1: loss = 0.669417 (* 1 = 0.669417 loss)
I1107 11:26:07.122092 11160 sgd_solver.cpp:105] Iteration 45500, lr = 0.1
I1107 11:26:15.724570 11160 solver.cpp:218] Iteration 45600 (11.6254 iter/s, 8.60182s/100 iters), loss = 0.529219
I1107 11:26:15.724570 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:26:15.724570 11160 solver.cpp:237]     Train net output #1: loss = 0.529219 (* 1 = 0.529219 loss)
I1107 11:26:15.724570 11160 sgd_solver.cpp:105] Iteration 45600, lr = 0.1
I1107 11:26:24.452553 11160 solver.cpp:218] Iteration 45700 (11.4577 iter/s, 8.72775s/100 iters), loss = 0.658022
I1107 11:26:24.452553 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1107 11:26:24.452553 11160 solver.cpp:237]     Train net output #1: loss = 0.658022 (* 1 = 0.658022 loss)
I1107 11:26:24.452553 11160 sgd_solver.cpp:105] Iteration 45700, lr = 0.1
I1107 11:26:33.043980 11160 solver.cpp:218] Iteration 45800 (11.6399 iter/s, 8.59116s/100 iters), loss = 0.459787
I1107 11:26:33.043980 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:26:33.043980 11160 solver.cpp:237]     Train net output #1: loss = 0.459787 (* 1 = 0.459787 loss)
I1107 11:26:33.043980 11160 sgd_solver.cpp:105] Iteration 45800, lr = 0.1
I1107 11:26:41.627789 11160 solver.cpp:218] Iteration 45900 (11.6507 iter/s, 8.58321s/100 iters), loss = 0.592454
I1107 11:26:41.627789 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:26:41.627789 11160 solver.cpp:237]     Train net output #1: loss = 0.592454 (* 1 = 0.592454 loss)
I1107 11:26:41.627789 11160 sgd_solver.cpp:105] Iteration 45900, lr = 0.1
I1107 11:26:49.758383  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:26:50.095512 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_46000.caffemodel
I1107 11:26:50.126513 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_46000.solverstate
I1107 11:26:50.157045 11160 solver.cpp:330] Iteration 46000, Testing net (#0)
I1107 11:26:50.157045 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:26:52.161149 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:26:52.240650 11160 solver.cpp:397]     Test net output #0: accuracy = 0.748
I1107 11:26:52.240650 11160 solver.cpp:397]     Test net output #1: loss = 0.754352 (* 1 = 0.754352 loss)
I1107 11:26:52.322152 11160 solver.cpp:218] Iteration 46000 (9.35142 iter/s, 10.6936s/100 iters), loss = 0.59344
I1107 11:26:52.322152 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:26:52.322152 11160 solver.cpp:237]     Train net output #1: loss = 0.59344 (* 1 = 0.59344 loss)
I1107 11:26:52.322152 11160 sgd_solver.cpp:105] Iteration 46000, lr = 0.1
I1107 11:27:00.886366 11160 solver.cpp:218] Iteration 46100 (11.6766 iter/s, 8.56417s/100 iters), loss = 0.619428
I1107 11:27:00.887367 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:27:00.887367 11160 solver.cpp:237]     Train net output #1: loss = 0.619428 (* 1 = 0.619428 loss)
I1107 11:27:00.887367 11160 sgd_solver.cpp:105] Iteration 46100, lr = 0.1
I1107 11:27:09.438361 11160 solver.cpp:218] Iteration 46200 (11.6947 iter/s, 8.55091s/100 iters), loss = 0.597513
I1107 11:27:09.438361 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:27:09.438361 11160 solver.cpp:237]     Train net output #1: loss = 0.597513 (* 1 = 0.597513 loss)
I1107 11:27:09.438361 11160 sgd_solver.cpp:105] Iteration 46200, lr = 0.1
I1107 11:27:18.115084 11160 solver.cpp:218] Iteration 46300 (11.5262 iter/s, 8.67589s/100 iters), loss = 0.525929
I1107 11:27:18.115084 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1107 11:27:18.115084 11160 solver.cpp:237]     Train net output #1: loss = 0.525929 (* 1 = 0.525929 loss)
I1107 11:27:18.115084 11160 sgd_solver.cpp:105] Iteration 46300, lr = 0.1
I1107 11:27:26.723099 11160 solver.cpp:218] Iteration 46400 (11.6178 iter/s, 8.60746s/100 iters), loss = 0.531872
I1107 11:27:26.723099 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:27:26.723099 11160 solver.cpp:237]     Train net output #1: loss = 0.531872 (* 1 = 0.531872 loss)
I1107 11:27:26.723099 11160 sgd_solver.cpp:105] Iteration 46400, lr = 0.1
I1107 11:27:34.920207  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:27:35.270772 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_46500.caffemodel
I1107 11:27:35.300787 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_46500.solverstate
I1107 11:27:35.309773 11160 solver.cpp:330] Iteration 46500, Testing net (#0)
I1107 11:27:35.309773 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:27:37.306857 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:27:37.387399 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5884
I1107 11:27:37.387399 11160 solver.cpp:397]     Test net output #1: loss = 1.2607 (* 1 = 1.2607 loss)
I1107 11:27:37.467422 11160 solver.cpp:218] Iteration 46500 (9.30702 iter/s, 10.7446s/100 iters), loss = 0.575794
I1107 11:27:37.468427 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:27:37.468427 11160 solver.cpp:237]     Train net output #1: loss = 0.575794 (* 1 = 0.575794 loss)
I1107 11:27:37.468427 11160 sgd_solver.cpp:105] Iteration 46500, lr = 0.1
I1107 11:27:46.026526 11160 solver.cpp:218] Iteration 46600 (11.6843 iter/s, 8.55846s/100 iters), loss = 0.619814
I1107 11:27:46.026526 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:27:46.026526 11160 solver.cpp:237]     Train net output #1: loss = 0.619814 (* 1 = 0.619814 loss)
I1107 11:27:46.026526 11160 sgd_solver.cpp:105] Iteration 46600, lr = 0.1
I1107 11:27:54.556953 11160 solver.cpp:218] Iteration 46700 (11.724 iter/s, 8.52954s/100 iters), loss = 0.557134
I1107 11:27:54.557454 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:27:54.557454 11160 solver.cpp:237]     Train net output #1: loss = 0.557134 (* 1 = 0.557134 loss)
I1107 11:27:54.557454 11160 sgd_solver.cpp:105] Iteration 46700, lr = 0.1
I1107 11:28:03.097851 11160 solver.cpp:218] Iteration 46800 (11.7097 iter/s, 8.53996s/100 iters), loss = 0.628604
I1107 11:28:03.097851 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:28:03.097851 11160 solver.cpp:237]     Train net output #1: loss = 0.628604 (* 1 = 0.628604 loss)
I1107 11:28:03.097851 11160 sgd_solver.cpp:105] Iteration 46800, lr = 0.1
I1107 11:28:11.652107 11160 solver.cpp:218] Iteration 46900 (11.6908 iter/s, 8.55373s/100 iters), loss = 0.561623
I1107 11:28:11.652107 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 11:28:11.652107 11160 solver.cpp:237]     Train net output #1: loss = 0.561623 (* 1 = 0.561623 loss)
I1107 11:28:11.652107 11160 sgd_solver.cpp:105] Iteration 46900, lr = 0.1
I1107 11:28:19.816329  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:28:20.155848 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_47000.caffemodel
I1107 11:28:20.213855 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_47000.solverstate
I1107 11:28:20.221861 11160 solver.cpp:330] Iteration 47000, Testing net (#0)
I1107 11:28:20.221861 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:28:22.233085 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:28:22.313590 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6473
I1107 11:28:22.313590 11160 solver.cpp:397]     Test net output #1: loss = 1.08672 (* 1 = 1.08672 loss)
I1107 11:28:22.395098 11160 solver.cpp:218] Iteration 47000 (9.3083 iter/s, 10.7431s/100 iters), loss = 0.611785
I1107 11:28:22.395098 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:28:22.395098 11160 solver.cpp:237]     Train net output #1: loss = 0.611785 (* 1 = 0.611785 loss)
I1107 11:28:22.395098 11160 sgd_solver.cpp:105] Iteration 47000, lr = 0.1
I1107 11:28:30.944695 11160 solver.cpp:218] Iteration 47100 (11.6969 iter/s, 8.54924s/100 iters), loss = 0.571105
I1107 11:28:30.945696 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:28:30.945696 11160 solver.cpp:237]     Train net output #1: loss = 0.571105 (* 1 = 0.571105 loss)
I1107 11:28:30.945696 11160 sgd_solver.cpp:105] Iteration 47100, lr = 0.1
I1107 11:28:39.501989 11160 solver.cpp:218] Iteration 47200 (11.6867 iter/s, 8.55676s/100 iters), loss = 0.665992
I1107 11:28:39.502990 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:28:39.502990 11160 solver.cpp:237]     Train net output #1: loss = 0.665992 (* 1 = 0.665992 loss)
I1107 11:28:39.502990 11160 sgd_solver.cpp:105] Iteration 47200, lr = 0.1
I1107 11:28:48.052788 11160 solver.cpp:218] Iteration 47300 (11.6962 iter/s, 8.5498s/100 iters), loss = 0.571925
I1107 11:28:48.052788 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1107 11:28:48.052788 11160 solver.cpp:237]     Train net output #1: loss = 0.571925 (* 1 = 0.571925 loss)
I1107 11:28:48.052788 11160 sgd_solver.cpp:105] Iteration 47300, lr = 0.1
I1107 11:28:56.595952 11160 solver.cpp:218] Iteration 47400 (11.7055 iter/s, 8.54296s/100 iters), loss = 0.544634
I1107 11:28:56.595952 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:28:56.595952 11160 solver.cpp:237]     Train net output #1: loss = 0.544634 (* 1 = 0.544634 loss)
I1107 11:28:56.595952 11160 sgd_solver.cpp:105] Iteration 47400, lr = 0.1
I1107 11:29:04.772161  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:29:05.108176 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_47500.caffemodel
I1107 11:29:05.138185 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_47500.solverstate
I1107 11:29:05.147186 11160 solver.cpp:330] Iteration 47500, Testing net (#0)
I1107 11:29:05.147186 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:29:07.135469 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:29:07.214972 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6295
I1107 11:29:07.214972 11160 solver.cpp:397]     Test net output #1: loss = 1.20116 (* 1 = 1.20116 loss)
I1107 11:29:07.295483 11160 solver.cpp:218] Iteration 47500 (9.34682 iter/s, 10.6988s/100 iters), loss = 0.558477
I1107 11:29:07.295483 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:29:07.295483 11160 solver.cpp:237]     Train net output #1: loss = 0.558477 (* 1 = 0.558477 loss)
I1107 11:29:07.295483 11160 sgd_solver.cpp:105] Iteration 47500, lr = 0.1
I1107 11:29:15.864872 11160 solver.cpp:218] Iteration 47600 (11.67 iter/s, 8.56899s/100 iters), loss = 0.547301
I1107 11:29:15.864872 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 11:29:15.864872 11160 solver.cpp:237]     Train net output #1: loss = 0.547301 (* 1 = 0.547301 loss)
I1107 11:29:15.864872 11160 sgd_solver.cpp:105] Iteration 47600, lr = 0.1
I1107 11:29:24.431213 11160 solver.cpp:218] Iteration 47700 (11.6754 iter/s, 8.56504s/100 iters), loss = 0.682233
I1107 11:29:24.431213 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:29:24.431213 11160 solver.cpp:237]     Train net output #1: loss = 0.682233 (* 1 = 0.682233 loss)
I1107 11:29:24.431213 11160 sgd_solver.cpp:105] Iteration 47700, lr = 0.1
I1107 11:29:33.035451 11160 solver.cpp:218] Iteration 47800 (11.622 iter/s, 8.6044s/100 iters), loss = 0.613534
I1107 11:29:33.035451 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:29:33.035451 11160 solver.cpp:237]     Train net output #1: loss = 0.613534 (* 1 = 0.613534 loss)
I1107 11:29:33.035451 11160 sgd_solver.cpp:105] Iteration 47800, lr = 0.1
I1107 11:29:41.636771 11160 solver.cpp:218] Iteration 47900 (11.6274 iter/s, 8.60041s/100 iters), loss = 0.479398
I1107 11:29:41.636771 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1107 11:29:41.636771 11160 solver.cpp:237]     Train net output #1: loss = 0.479398 (* 1 = 0.479398 loss)
I1107 11:29:41.636771 11160 sgd_solver.cpp:105] Iteration 47900, lr = 0.1
I1107 11:29:49.822903  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:29:50.162910 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_48000.caffemodel
I1107 11:29:50.213932 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_48000.solverstate
I1107 11:29:50.221932 11160 solver.cpp:330] Iteration 48000, Testing net (#0)
I1107 11:29:50.221932 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:29:52.222051 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:29:52.301587 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6438
I1107 11:29:52.301587 11160 solver.cpp:397]     Test net output #1: loss = 1.07693 (* 1 = 1.07693 loss)
I1107 11:29:52.384079 11160 solver.cpp:218] Iteration 48000 (9.30542 iter/s, 10.7464s/100 iters), loss = 0.799116
I1107 11:29:52.384079 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1107 11:29:52.384079 11160 solver.cpp:237]     Train net output #1: loss = 0.799116 (* 1 = 0.799116 loss)
I1107 11:29:52.384079 11160 sgd_solver.cpp:105] Iteration 48000, lr = 0.1
I1107 11:30:01.199507 11160 solver.cpp:218] Iteration 48100 (11.3444 iter/s, 8.81493s/100 iters), loss = 0.53252
I1107 11:30:01.199507 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:30:01.199507 11160 solver.cpp:237]     Train net output #1: loss = 0.53252 (* 1 = 0.53252 loss)
I1107 11:30:01.199507 11160 sgd_solver.cpp:105] Iteration 48100, lr = 0.1
I1107 11:30:09.788844 11160 solver.cpp:218] Iteration 48200 (11.6431 iter/s, 8.58875s/100 iters), loss = 0.597102
I1107 11:30:09.788844 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1107 11:30:09.788844 11160 solver.cpp:237]     Train net output #1: loss = 0.597102 (* 1 = 0.597102 loss)
I1107 11:30:09.788844 11160 sgd_solver.cpp:105] Iteration 48200, lr = 0.1
I1107 11:30:18.326074 11160 solver.cpp:218] Iteration 48300 (11.7141 iter/s, 8.53672s/100 iters), loss = 0.483841
I1107 11:30:18.326074 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 11:30:18.326074 11160 solver.cpp:237]     Train net output #1: loss = 0.483841 (* 1 = 0.483841 loss)
I1107 11:30:18.326074 11160 sgd_solver.cpp:105] Iteration 48300, lr = 0.1
I1107 11:30:26.860541 11160 solver.cpp:218] Iteration 48400 (11.7181 iter/s, 8.53382s/100 iters), loss = 0.635177
I1107 11:30:26.860541 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:30:26.860541 11160 solver.cpp:237]     Train net output #1: loss = 0.635177 (* 1 = 0.635177 loss)
I1107 11:30:26.860541 11160 sgd_solver.cpp:105] Iteration 48400, lr = 0.1
I1107 11:30:34.969099  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:30:35.305119 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_48500.caffemodel
I1107 11:30:35.338621 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_48500.solverstate
I1107 11:30:35.348125 11160 solver.cpp:330] Iteration 48500, Testing net (#0)
I1107 11:30:35.348125 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:30:37.335305 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:30:37.415336 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5944
I1107 11:30:37.415336 11160 solver.cpp:397]     Test net output #1: loss = 1.2154 (* 1 = 1.2154 loss)
I1107 11:30:37.496326 11160 solver.cpp:218] Iteration 48500 (9.40237 iter/s, 10.6356s/100 iters), loss = 0.612114
I1107 11:30:37.496326 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:30:37.496326 11160 solver.cpp:237]     Train net output #1: loss = 0.612114 (* 1 = 0.612114 loss)
I1107 11:30:37.496326 11160 sgd_solver.cpp:105] Iteration 48500, lr = 0.1
I1107 11:30:46.034157 11160 solver.cpp:218] Iteration 48600 (11.7127 iter/s, 8.53777s/100 iters), loss = 0.592148
I1107 11:30:46.034157 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:30:46.034157 11160 solver.cpp:237]     Train net output #1: loss = 0.592148 (* 1 = 0.592148 loss)
I1107 11:30:46.034157 11160 sgd_solver.cpp:105] Iteration 48600, lr = 0.1
I1107 11:30:54.558836 11160 solver.cpp:218] Iteration 48700 (11.7317 iter/s, 8.52389s/100 iters), loss = 0.636636
I1107 11:30:54.558836 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1107 11:30:54.558836 11160 solver.cpp:237]     Train net output #1: loss = 0.636636 (* 1 = 0.636636 loss)
I1107 11:30:54.558836 11160 sgd_solver.cpp:105] Iteration 48700, lr = 0.1
I1107 11:31:03.209614 11160 solver.cpp:218] Iteration 48800 (11.5604 iter/s, 8.65023s/100 iters), loss = 0.441617
I1107 11:31:03.209614 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1107 11:31:03.209614 11160 solver.cpp:237]     Train net output #1: loss = 0.441617 (* 1 = 0.441617 loss)
I1107 11:31:03.209614 11160 sgd_solver.cpp:105] Iteration 48800, lr = 0.1
I1107 11:31:11.857894 11160 solver.cpp:218] Iteration 48900 (11.5643 iter/s, 8.64732s/100 iters), loss = 0.490579
I1107 11:31:11.857894 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1107 11:31:11.857894 11160 solver.cpp:237]     Train net output #1: loss = 0.490579 (* 1 = 0.490579 loss)
I1107 11:31:11.857894 11160 sgd_solver.cpp:105] Iteration 48900, lr = 0.1
I1107 11:31:20.008080  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:31:20.347095 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_49000.caffemodel
I1107 11:31:20.405128 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_49000.solverstate
I1107 11:31:20.414129 11160 solver.cpp:330] Iteration 49000, Testing net (#0)
I1107 11:31:20.414129 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:31:22.411226 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:31:22.490231 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6451
I1107 11:31:22.490231 11160 solver.cpp:397]     Test net output #1: loss = 1.09862 (* 1 = 1.09862 loss)
I1107 11:31:22.573734 11160 solver.cpp:218] Iteration 49000 (9.33234 iter/s, 10.7154s/100 iters), loss = 0.507301
I1107 11:31:22.573734 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 11:31:22.573734 11160 solver.cpp:237]     Train net output #1: loss = 0.507301 (* 1 = 0.507301 loss)
I1107 11:31:22.573734 11160 sgd_solver.cpp:105] Iteration 49000, lr = 0.1
I1107 11:31:31.224810 11160 solver.cpp:218] Iteration 49100 (11.5593 iter/s, 8.65106s/100 iters), loss = 0.558382
I1107 11:31:31.224810 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1107 11:31:31.224810 11160 solver.cpp:237]     Train net output #1: loss = 0.558382 (* 1 = 0.558382 loss)
I1107 11:31:31.224810 11160 sgd_solver.cpp:105] Iteration 49100, lr = 0.1
I1107 11:31:40.020723 11160 solver.cpp:218] Iteration 49200 (11.3695 iter/s, 8.79542s/100 iters), loss = 0.611514
I1107 11:31:40.020723 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1107 11:31:40.020723 11160 solver.cpp:237]     Train net output #1: loss = 0.611514 (* 1 = 0.611514 loss)
I1107 11:31:40.020723 11160 sgd_solver.cpp:105] Iteration 49200, lr = 0.1
I1107 11:31:48.596595 11160 solver.cpp:218] Iteration 49300 (11.6623 iter/s, 8.57462s/100 iters), loss = 0.592538
I1107 11:31:48.596595 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:31:48.596595 11160 solver.cpp:237]     Train net output #1: loss = 0.592538 (* 1 = 0.592538 loss)
I1107 11:31:48.596595 11160 sgd_solver.cpp:105] Iteration 49300, lr = 0.1
I1107 11:31:57.140622 11160 solver.cpp:218] Iteration 49400 (11.7046 iter/s, 8.54365s/100 iters), loss = 0.540305
I1107 11:31:57.140622 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:31:57.140622 11160 solver.cpp:237]     Train net output #1: loss = 0.540305 (* 1 = 0.540305 loss)
I1107 11:31:57.140622 11160 sgd_solver.cpp:105] Iteration 49400, lr = 0.1
I1107 11:32:05.260812  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:32:05.597832 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_49500.caffemodel
I1107 11:32:05.626845 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_49500.solverstate
I1107 11:32:05.636849 11160 solver.cpp:330] Iteration 49500, Testing net (#0)
I1107 11:32:05.636849 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:32:07.637046 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:32:07.718053 11160 solver.cpp:397]     Test net output #0: accuracy = 0.6672
I1107 11:32:07.718053 11160 solver.cpp:397]     Test net output #1: loss = 1.0481 (* 1 = 1.0481 loss)
I1107 11:32:07.799055 11160 solver.cpp:218] Iteration 49500 (9.38214 iter/s, 10.6585s/100 iters), loss = 0.522533
I1107 11:32:07.799055 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1107 11:32:07.800056 11160 solver.cpp:237]     Train net output #1: loss = 0.522533 (* 1 = 0.522533 loss)
I1107 11:32:07.800056 11160 sgd_solver.cpp:105] Iteration 49500, lr = 0.1
I1107 11:32:16.415328 11160 solver.cpp:218] Iteration 49600 (11.6079 iter/s, 8.61486s/100 iters), loss = 0.578238
I1107 11:32:16.415328 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1107 11:32:16.415328 11160 solver.cpp:237]     Train net output #1: loss = 0.578238 (* 1 = 0.578238 loss)
I1107 11:32:16.415328 11160 sgd_solver.cpp:105] Iteration 49600, lr = 0.1
I1107 11:32:25.001225 11160 solver.cpp:218] Iteration 49700 (11.6469 iter/s, 8.58598s/100 iters), loss = 0.669935
I1107 11:32:25.001225 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1107 11:32:25.001225 11160 solver.cpp:237]     Train net output #1: loss = 0.669935 (* 1 = 0.669935 loss)
I1107 11:32:25.001225 11160 sgd_solver.cpp:105] Iteration 49700, lr = 0.1
I1107 11:32:33.580622 11160 solver.cpp:218] Iteration 49800 (11.6576 iter/s, 8.5781s/100 iters), loss = 0.542824
I1107 11:32:33.580622 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1107 11:32:33.580622 11160 solver.cpp:237]     Train net output #1: loss = 0.542824 (* 1 = 0.542824 loss)
I1107 11:32:33.580622 11160 sgd_solver.cpp:105] Iteration 49800, lr = 0.1
I1107 11:32:42.206630 11160 solver.cpp:218] Iteration 49900 (11.5937 iter/s, 8.62537s/100 iters), loss = 0.486742
I1107 11:32:42.206630 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 11:32:42.206630 11160 solver.cpp:237]     Train net output #1: loss = 0.486743 (* 1 = 0.486743 loss)
I1107 11:32:42.206630 11160 sgd_solver.cpp:105] Iteration 49900, lr = 0.1
I1107 11:32:50.337163  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:32:50.674201 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_50000.caffemodel
I1107 11:32:50.704207 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_50000.solverstate
I1107 11:32:50.713208 11160 solver.cpp:330] Iteration 50000, Testing net (#0)
I1107 11:32:50.713208 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:32:52.700353 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:32:52.779356 11160 solver.cpp:397]     Test net output #0: accuracy = 0.5749
I1107 11:32:52.779356 11160 solver.cpp:397]     Test net output #1: loss = 1.4129 (* 1 = 1.4129 loss)
I1107 11:32:52.861361 11160 solver.cpp:218] Iteration 50000 (9.38585 iter/s, 10.6543s/100 iters), loss = 0.639086
I1107 11:32:52.861361 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1107 11:32:52.861361 11160 solver.cpp:237]     Train net output #1: loss = 0.639086 (* 1 = 0.639086 loss)
I1107 11:32:52.861361 11160 sgd_solver.cpp:46] MultiStep Status: Iteration 50000, step = 1
I1107 11:32:52.861361 11160 sgd_solver.cpp:105] Iteration 50000, lr = 0.01
I1107 11:33:01.426261 11160 solver.cpp:218] Iteration 50100 (11.6772 iter/s, 8.56368s/100 iters), loss = 0.355321
I1107 11:33:01.426261 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:33:01.426261 11160 solver.cpp:237]     Train net output #1: loss = 0.355321 (* 1 = 0.355321 loss)
I1107 11:33:01.426261 11160 sgd_solver.cpp:105] Iteration 50100, lr = 0.01
I1107 11:33:09.975563 11160 solver.cpp:218] Iteration 50200 (11.697 iter/s, 8.54922s/100 iters), loss = 0.398564
I1107 11:33:09.975563 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1107 11:33:09.975563 11160 solver.cpp:237]     Train net output #1: loss = 0.398564 (* 1 = 0.398564 loss)
I1107 11:33:09.975563 11160 sgd_solver.cpp:105] Iteration 50200, lr = 0.01
I1107 11:33:18.560674 11160 solver.cpp:218] Iteration 50300 (11.6487 iter/s, 8.58462s/100 iters), loss = 0.380934
I1107 11:33:18.560674 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:33:18.560674 11160 solver.cpp:237]     Train net output #1: loss = 0.380934 (* 1 = 0.380934 loss)
I1107 11:33:18.560674 11160 sgd_solver.cpp:105] Iteration 50300, lr = 0.01
I1107 11:33:27.128162 11160 solver.cpp:218] Iteration 50400 (11.6728 iter/s, 8.56693s/100 iters), loss = 0.321501
I1107 11:33:27.128162 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:33:27.128664 11160 solver.cpp:237]     Train net output #1: loss = 0.321501 (* 1 = 0.321501 loss)
I1107 11:33:27.128664 11160 sgd_solver.cpp:105] Iteration 50400, lr = 0.01
I1107 11:33:35.246997  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:33:35.583026 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_50500.caffemodel
I1107 11:33:35.619045 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_50500.solverstate
I1107 11:33:35.628046 11160 solver.cpp:330] Iteration 50500, Testing net (#0)
I1107 11:33:35.628046 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:33:37.616287 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:33:37.696313 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8821
I1107 11:33:37.696313 11160 solver.cpp:397]     Test net output #1: loss = 0.37246 (* 1 = 0.37246 loss)
I1107 11:33:37.778322 11160 solver.cpp:218] Iteration 50500 (9.39024 iter/s, 10.6494s/100 iters), loss = 0.371796
I1107 11:33:37.778322 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:33:37.778322 11160 solver.cpp:237]     Train net output #1: loss = 0.371796 (* 1 = 0.371796 loss)
I1107 11:33:37.778322 11160 sgd_solver.cpp:105] Iteration 50500, lr = 0.01
I1107 11:33:46.338436 11160 solver.cpp:218] Iteration 50600 (11.6831 iter/s, 8.55935s/100 iters), loss = 0.3558
I1107 11:33:46.338436 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1107 11:33:46.338436 11160 solver.cpp:237]     Train net output #1: loss = 0.3558 (* 1 = 0.3558 loss)
I1107 11:33:46.338436 11160 sgd_solver.cpp:105] Iteration 50600, lr = 0.01
I1107 11:33:54.934541 11160 solver.cpp:218] Iteration 50700 (11.6339 iter/s, 8.5956s/100 iters), loss = 0.342985
I1107 11:33:54.934541 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:33:54.934541 11160 solver.cpp:237]     Train net output #1: loss = 0.342985 (* 1 = 0.342985 loss)
I1107 11:33:54.934541 11160 sgd_solver.cpp:105] Iteration 50700, lr = 0.01
I1107 11:34:03.521036 11160 solver.cpp:218] Iteration 50800 (11.647 iter/s, 8.5859s/100 iters), loss = 0.30884
I1107 11:34:03.521036 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:34:03.521036 11160 solver.cpp:237]     Train net output #1: loss = 0.30884 (* 1 = 0.30884 loss)
I1107 11:34:03.521036 11160 sgd_solver.cpp:105] Iteration 50800, lr = 0.01
I1107 11:34:12.168872 11160 solver.cpp:218] Iteration 50900 (11.564 iter/s, 8.64749s/100 iters), loss = 0.28569
I1107 11:34:12.168872 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:34:12.168872 11160 solver.cpp:237]     Train net output #1: loss = 0.28569 (* 1 = 0.28569 loss)
I1107 11:34:12.168872 11160 sgd_solver.cpp:105] Iteration 50900, lr = 0.01
I1107 11:34:20.282232  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:34:20.620270 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_51000.caffemodel
I1107 11:34:20.650782 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_51000.solverstate
I1107 11:34:20.659282 11160 solver.cpp:330] Iteration 51000, Testing net (#0)
I1107 11:34:20.659782 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:34:22.648247 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:34:22.727754 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8855
I1107 11:34:22.727754 11160 solver.cpp:397]     Test net output #1: loss = 0.347046 (* 1 = 0.347046 loss)
I1107 11:34:22.808759 11160 solver.cpp:218] Iteration 51000 (9.39856 iter/s, 10.6399s/100 iters), loss = 0.342557
I1107 11:34:22.808759 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:34:22.808759 11160 solver.cpp:237]     Train net output #1: loss = 0.342557 (* 1 = 0.342557 loss)
I1107 11:34:22.808759 11160 sgd_solver.cpp:105] Iteration 51000, lr = 0.01
I1107 11:34:31.384944 11160 solver.cpp:218] Iteration 51100 (11.6611 iter/s, 8.5755s/100 iters), loss = 0.348686
I1107 11:34:31.384944 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:34:31.384944 11160 solver.cpp:237]     Train net output #1: loss = 0.348686 (* 1 = 0.348686 loss)
I1107 11:34:31.384944 11160 sgd_solver.cpp:105] Iteration 51100, lr = 0.01
I1107 11:34:39.952530 11160 solver.cpp:218] Iteration 51200 (11.6734 iter/s, 8.56649s/100 iters), loss = 0.356024
I1107 11:34:39.952530 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:34:39.952530 11160 solver.cpp:237]     Train net output #1: loss = 0.356024 (* 1 = 0.356024 loss)
I1107 11:34:39.952530 11160 sgd_solver.cpp:105] Iteration 51200, lr = 0.01
I1107 11:34:48.715991 11160 solver.cpp:218] Iteration 51300 (11.4107 iter/s, 8.76369s/100 iters), loss = 0.32945
I1107 11:34:48.716974 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:34:48.716974 11160 solver.cpp:237]     Train net output #1: loss = 0.32945 (* 1 = 0.32945 loss)
I1107 11:34:48.716974 11160 sgd_solver.cpp:105] Iteration 51300, lr = 0.01
I1107 11:34:57.407865 11160 solver.cpp:218] Iteration 51400 (11.5065 iter/s, 8.69071s/100 iters), loss = 0.218045
I1107 11:34:57.407865 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:34:57.407865 11160 solver.cpp:237]     Train net output #1: loss = 0.218045 (* 1 = 0.218045 loss)
I1107 11:34:57.407865 11160 sgd_solver.cpp:105] Iteration 51400, lr = 0.01
I1107 11:35:05.705778  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:35:06.055382 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_51500.caffemodel
I1107 11:35:06.085383 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_51500.solverstate
I1107 11:35:06.094383 11160 solver.cpp:330] Iteration 51500, Testing net (#0)
I1107 11:35:06.094383 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:35:08.139433 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:35:08.220448 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8891
I1107 11:35:08.220938 11160 solver.cpp:397]     Test net output #1: loss = 0.343569 (* 1 = 0.343569 loss)
I1107 11:35:08.302096 11160 solver.cpp:218] Iteration 51500 (9.17935 iter/s, 10.894s/100 iters), loss = 0.287699
I1107 11:35:08.302096 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:35:08.302096 11160 solver.cpp:237]     Train net output #1: loss = 0.287699 (* 1 = 0.287699 loss)
I1107 11:35:08.302096 11160 sgd_solver.cpp:105] Iteration 51500, lr = 0.01
I1107 11:35:16.961091 11160 solver.cpp:218] Iteration 51600 (11.5494 iter/s, 8.65848s/100 iters), loss = 0.334301
I1107 11:35:16.962091 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:35:16.962091 11160 solver.cpp:237]     Train net output #1: loss = 0.334301 (* 1 = 0.334301 loss)
I1107 11:35:16.962091 11160 sgd_solver.cpp:105] Iteration 51600, lr = 0.01
I1107 11:35:25.559059 11160 solver.cpp:218] Iteration 51700 (11.6321 iter/s, 8.59689s/100 iters), loss = 0.336433
I1107 11:35:25.559059 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:35:25.559059 11160 solver.cpp:237]     Train net output #1: loss = 0.336433 (* 1 = 0.336433 loss)
I1107 11:35:25.559059 11160 sgd_solver.cpp:105] Iteration 51700, lr = 0.01
I1107 11:35:34.137465 11160 solver.cpp:218] Iteration 51800 (11.6574 iter/s, 8.57827s/100 iters), loss = 0.326048
I1107 11:35:34.137465 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:35:34.137465 11160 solver.cpp:237]     Train net output #1: loss = 0.326048 (* 1 = 0.326048 loss)
I1107 11:35:34.137465 11160 sgd_solver.cpp:105] Iteration 51800, lr = 0.01
I1107 11:35:42.697120 11160 solver.cpp:218] Iteration 51900 (11.684 iter/s, 8.5587s/100 iters), loss = 0.218824
I1107 11:35:42.697620 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:35:42.697620 11160 solver.cpp:237]     Train net output #1: loss = 0.218824 (* 1 = 0.218824 loss)
I1107 11:35:42.697620 11160 sgd_solver.cpp:105] Iteration 51900, lr = 0.01
I1107 11:35:50.896033  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:35:51.236063 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_52000.caffemodel
I1107 11:35:51.265074 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_52000.solverstate
I1107 11:35:51.274072 11160 solver.cpp:330] Iteration 52000, Testing net (#0)
I1107 11:35:51.274072 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:35:53.295711 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:35:53.375723 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8881
I1107 11:35:53.375723 11160 solver.cpp:397]     Test net output #1: loss = 0.340815 (* 1 = 0.340815 loss)
I1107 11:35:53.457765 11160 solver.cpp:218] Iteration 52000 (9.29369 iter/s, 10.76s/100 iters), loss = 0.262212
I1107 11:35:53.457765 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:35:53.457765 11160 solver.cpp:237]     Train net output #1: loss = 0.262212 (* 1 = 0.262212 loss)
I1107 11:35:53.457765 11160 sgd_solver.cpp:105] Iteration 52000, lr = 0.01
I1107 11:36:02.062453 11160 solver.cpp:218] Iteration 52100 (11.6217 iter/s, 8.60461s/100 iters), loss = 0.338212
I1107 11:36:02.063454 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:36:02.063454 11160 solver.cpp:237]     Train net output #1: loss = 0.338212 (* 1 = 0.338212 loss)
I1107 11:36:02.063454 11160 sgd_solver.cpp:105] Iteration 52100, lr = 0.01
I1107 11:36:10.689960 11160 solver.cpp:218] Iteration 52200 (11.5916 iter/s, 8.62696s/100 iters), loss = 0.356829
I1107 11:36:10.689960 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1107 11:36:10.690960 11160 solver.cpp:237]     Train net output #1: loss = 0.35683 (* 1 = 0.35683 loss)
I1107 11:36:10.690960 11160 sgd_solver.cpp:105] Iteration 52200, lr = 0.01
I1107 11:36:19.294976 11160 solver.cpp:218] Iteration 52300 (11.6219 iter/s, 8.60448s/100 iters), loss = 0.255458
I1107 11:36:19.295977 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:36:19.295977 11160 solver.cpp:237]     Train net output #1: loss = 0.255459 (* 1 = 0.255459 loss)
I1107 11:36:19.295977 11160 sgd_solver.cpp:105] Iteration 52300, lr = 0.01
I1107 11:36:27.865341 11160 solver.cpp:218] Iteration 52400 (11.6699 iter/s, 8.56904s/100 iters), loss = 0.231369
I1107 11:36:27.865341 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:36:27.865341 11160 solver.cpp:237]     Train net output #1: loss = 0.231369 (* 1 = 0.231369 loss)
I1107 11:36:27.865341 11160 sgd_solver.cpp:105] Iteration 52400, lr = 0.01
I1107 11:36:35.981176  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:36:36.318202 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_52500.caffemodel
I1107 11:36:36.352197 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_52500.solverstate
I1107 11:36:36.361198 11160 solver.cpp:330] Iteration 52500, Testing net (#0)
I1107 11:36:36.361198 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:36:38.355391 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:36:38.434391 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8865
I1107 11:36:38.434391 11160 solver.cpp:397]     Test net output #1: loss = 0.345841 (* 1 = 0.345841 loss)
I1107 11:36:38.515395 11160 solver.cpp:218] Iteration 52500 (9.38949 iter/s, 10.6502s/100 iters), loss = 0.226757
I1107 11:36:38.515395 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:36:38.516396 11160 solver.cpp:237]     Train net output #1: loss = 0.226758 (* 1 = 0.226758 loss)
I1107 11:36:38.516396 11160 sgd_solver.cpp:105] Iteration 52500, lr = 0.01
I1107 11:36:47.052824 11160 solver.cpp:218] Iteration 52600 (11.7149 iter/s, 8.53615s/100 iters), loss = 0.314104
I1107 11:36:47.052824 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:36:47.052824 11160 solver.cpp:237]     Train net output #1: loss = 0.314104 (* 1 = 0.314104 loss)
I1107 11:36:47.052824 11160 sgd_solver.cpp:105] Iteration 52600, lr = 0.01
I1107 11:36:55.620189 11160 solver.cpp:218] Iteration 52700 (11.672 iter/s, 8.56749s/100 iters), loss = 0.356494
I1107 11:36:55.621191 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1107 11:36:55.621191 11160 solver.cpp:237]     Train net output #1: loss = 0.356495 (* 1 = 0.356495 loss)
I1107 11:36:55.621191 11160 sgd_solver.cpp:105] Iteration 52700, lr = 0.01
I1107 11:37:04.167604 11160 solver.cpp:218] Iteration 52800 (11.7012 iter/s, 8.54611s/100 iters), loss = 0.339754
I1107 11:37:04.167604 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:37:04.167604 11160 solver.cpp:237]     Train net output #1: loss = 0.339754 (* 1 = 0.339754 loss)
I1107 11:37:04.167604 11160 sgd_solver.cpp:105] Iteration 52800, lr = 0.01
I1107 11:37:12.712604 11160 solver.cpp:218] Iteration 52900 (11.7024 iter/s, 8.54525s/100 iters), loss = 0.204762
I1107 11:37:12.713605 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:37:12.713605 11160 solver.cpp:237]     Train net output #1: loss = 0.204762 (* 1 = 0.204762 loss)
I1107 11:37:12.713605 11160 sgd_solver.cpp:105] Iteration 52900, lr = 0.01
I1107 11:37:21.031211  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:37:21.381091 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_53000.caffemodel
I1107 11:37:21.412089 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_53000.solverstate
I1107 11:37:21.421090 11160 solver.cpp:330] Iteration 53000, Testing net (#0)
I1107 11:37:21.421090 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:37:23.451339 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:37:23.532343 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8764
I1107 11:37:23.532343 11160 solver.cpp:397]     Test net output #1: loss = 0.37343 (* 1 = 0.37343 loss)
I1107 11:37:23.615401 11160 solver.cpp:218] Iteration 53000 (9.17326 iter/s, 10.9012s/100 iters), loss = 0.173906
I1107 11:37:23.615401 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:37:23.615401 11160 solver.cpp:237]     Train net output #1: loss = 0.173906 (* 1 = 0.173906 loss)
I1107 11:37:23.615401 11160 sgd_solver.cpp:105] Iteration 53000, lr = 0.01
I1107 11:37:32.278959 11160 solver.cpp:218] Iteration 53100 (11.5432 iter/s, 8.66314s/100 iters), loss = 0.331377
I1107 11:37:32.278959 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:37:32.278959 11160 solver.cpp:237]     Train net output #1: loss = 0.331377 (* 1 = 0.331377 loss)
I1107 11:37:32.278959 11160 sgd_solver.cpp:105] Iteration 53100, lr = 0.01
I1107 11:37:40.926542 11160 solver.cpp:218] Iteration 53200 (11.565 iter/s, 8.64677s/100 iters), loss = 0.332827
I1107 11:37:40.926542 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:37:40.926542 11160 solver.cpp:237]     Train net output #1: loss = 0.332827 (* 1 = 0.332827 loss)
I1107 11:37:40.926542 11160 sgd_solver.cpp:105] Iteration 53200, lr = 0.01
I1107 11:37:49.488312 11160 solver.cpp:218] Iteration 53300 (11.6803 iter/s, 8.56142s/100 iters), loss = 0.298205
I1107 11:37:49.488312 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:37:49.488312 11160 solver.cpp:237]     Train net output #1: loss = 0.298205 (* 1 = 0.298205 loss)
I1107 11:37:49.488312 11160 sgd_solver.cpp:105] Iteration 53300, lr = 0.01
I1107 11:37:58.016327 11160 solver.cpp:218] Iteration 53400 (11.7268 iter/s, 8.52744s/100 iters), loss = 0.230457
I1107 11:37:58.016327 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:37:58.016327 11160 solver.cpp:237]     Train net output #1: loss = 0.230457 (* 1 = 0.230457 loss)
I1107 11:37:58.016327 11160 sgd_solver.cpp:105] Iteration 53400, lr = 0.01
I1107 11:38:06.120537  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:38:06.459568 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_53500.caffemodel
I1107 11:38:06.489575 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_53500.solverstate
I1107 11:38:06.498576 11160 solver.cpp:330] Iteration 53500, Testing net (#0)
I1107 11:38:06.498576 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:38:08.489821 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:38:08.569326 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8736
I1107 11:38:08.569826 11160 solver.cpp:397]     Test net output #1: loss = 0.390026 (* 1 = 0.390026 loss)
I1107 11:38:08.651828 11160 solver.cpp:218] Iteration 53500 (9.40305 iter/s, 10.6348s/100 iters), loss = 0.177117
I1107 11:38:08.651828 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:38:08.651828 11160 solver.cpp:237]     Train net output #1: loss = 0.177117 (* 1 = 0.177117 loss)
I1107 11:38:08.651828 11160 sgd_solver.cpp:105] Iteration 53500, lr = 0.01
I1107 11:38:17.180186 11160 solver.cpp:218] Iteration 53600 (11.7263 iter/s, 8.52782s/100 iters), loss = 0.304864
I1107 11:38:17.180186 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:38:17.180186 11160 solver.cpp:237]     Train net output #1: loss = 0.304865 (* 1 = 0.304865 loss)
I1107 11:38:17.180186 11160 sgd_solver.cpp:105] Iteration 53600, lr = 0.01
I1107 11:38:25.707696 11160 solver.cpp:218] Iteration 53700 (11.7264 iter/s, 8.52775s/100 iters), loss = 0.325834
I1107 11:38:25.707696 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:38:25.707696 11160 solver.cpp:237]     Train net output #1: loss = 0.325834 (* 1 = 0.325834 loss)
I1107 11:38:25.707696 11160 sgd_solver.cpp:105] Iteration 53700, lr = 0.01
I1107 11:38:34.229763 11160 solver.cpp:218] Iteration 53800 (11.7357 iter/s, 8.52103s/100 iters), loss = 0.318851
I1107 11:38:34.229763 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:38:34.229763 11160 solver.cpp:237]     Train net output #1: loss = 0.318852 (* 1 = 0.318852 loss)
I1107 11:38:34.229763 11160 sgd_solver.cpp:105] Iteration 53800, lr = 0.01
I1107 11:38:42.749590 11160 solver.cpp:218] Iteration 53900 (11.7375 iter/s, 8.51968s/100 iters), loss = 0.208389
I1107 11:38:42.749590 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:38:42.749590 11160 solver.cpp:237]     Train net output #1: loss = 0.208389 (* 1 = 0.208389 loss)
I1107 11:38:42.749590 11160 sgd_solver.cpp:105] Iteration 53900, lr = 0.01
I1107 11:38:50.856770  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:38:51.193791 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_54000.caffemodel
I1107 11:38:51.222795 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_54000.solverstate
I1107 11:38:51.232795 11160 solver.cpp:330] Iteration 54000, Testing net (#0)
I1107 11:38:51.232795 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:38:53.224247 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:38:53.303251 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8715
I1107 11:38:53.303251 11160 solver.cpp:397]     Test net output #1: loss = 0.397245 (* 1 = 0.397245 loss)
I1107 11:38:53.384254 11160 solver.cpp:218] Iteration 54000 (9.40416 iter/s, 10.6336s/100 iters), loss = 0.208631
I1107 11:38:53.384254 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:38:53.384254 11160 solver.cpp:237]     Train net output #1: loss = 0.208631 (* 1 = 0.208631 loss)
I1107 11:38:53.384254 11160 sgd_solver.cpp:105] Iteration 54000, lr = 0.01
I1107 11:39:01.906508 11160 solver.cpp:218] Iteration 54100 (11.7339 iter/s, 8.52232s/100 iters), loss = 0.359999
I1107 11:39:01.906508 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:39:01.906508 11160 solver.cpp:237]     Train net output #1: loss = 0.359999 (* 1 = 0.359999 loss)
I1107 11:39:01.906508 11160 sgd_solver.cpp:105] Iteration 54100, lr = 0.01
I1107 11:39:10.430927 11160 solver.cpp:218] Iteration 54200 (11.7323 iter/s, 8.52346s/100 iters), loss = 0.330238
I1107 11:39:10.430927 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:39:10.430927 11160 solver.cpp:237]     Train net output #1: loss = 0.330238 (* 1 = 0.330238 loss)
I1107 11:39:10.430927 11160 sgd_solver.cpp:105] Iteration 54200, lr = 0.01
I1107 11:39:18.952675 11160 solver.cpp:218] Iteration 54300 (11.7353 iter/s, 8.52132s/100 iters), loss = 0.233624
I1107 11:39:18.952675 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:39:18.952675 11160 solver.cpp:237]     Train net output #1: loss = 0.233624 (* 1 = 0.233624 loss)
I1107 11:39:18.952675 11160 sgd_solver.cpp:105] Iteration 54300, lr = 0.01
I1107 11:39:27.476608 11160 solver.cpp:218] Iteration 54400 (11.7329 iter/s, 8.52301s/100 iters), loss = 0.245911
I1107 11:39:27.476608 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:39:27.476608 11160 solver.cpp:237]     Train net output #1: loss = 0.245911 (* 1 = 0.245911 loss)
I1107 11:39:27.476608 11160 sgd_solver.cpp:105] Iteration 54400, lr = 0.01
I1107 11:39:35.580353  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:39:35.918378 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_54500.caffemodel
I1107 11:39:35.955379 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_54500.solverstate
I1107 11:39:35.964397 11160 solver.cpp:330] Iteration 54500, Testing net (#0)
I1107 11:39:35.964397 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:39:37.953564 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:39:38.033603 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8786
I1107 11:39:38.033603 11160 solver.cpp:397]     Test net output #1: loss = 0.378197 (* 1 = 0.378197 loss)
I1107 11:39:38.115608 11160 solver.cpp:218] Iteration 54500 (9.39967 iter/s, 10.6387s/100 iters), loss = 0.252018
I1107 11:39:38.115608 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:39:38.115608 11160 solver.cpp:237]     Train net output #1: loss = 0.252018 (* 1 = 0.252018 loss)
I1107 11:39:38.115608 11160 sgd_solver.cpp:105] Iteration 54500, lr = 0.01
I1107 11:39:46.651495 11160 solver.cpp:218] Iteration 54600 (11.7158 iter/s, 8.5355s/100 iters), loss = 0.312378
I1107 11:39:46.651495 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:39:46.651495 11160 solver.cpp:237]     Train net output #1: loss = 0.312379 (* 1 = 0.312379 loss)
I1107 11:39:46.651495 11160 sgd_solver.cpp:105] Iteration 54600, lr = 0.01
I1107 11:39:55.186112 11160 solver.cpp:218] Iteration 54700 (11.7174 iter/s, 8.53432s/100 iters), loss = 0.207798
I1107 11:39:55.186112 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:39:55.186112 11160 solver.cpp:237]     Train net output #1: loss = 0.207798 (* 1 = 0.207798 loss)
I1107 11:39:55.186112 11160 sgd_solver.cpp:105] Iteration 54700, lr = 0.01
I1107 11:40:03.749445 11160 solver.cpp:218] Iteration 54800 (11.6784 iter/s, 8.56285s/100 iters), loss = 0.274813
I1107 11:40:03.749445 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:40:03.749445 11160 solver.cpp:237]     Train net output #1: loss = 0.274813 (* 1 = 0.274813 loss)
I1107 11:40:03.749445 11160 sgd_solver.cpp:105] Iteration 54800, lr = 0.01
I1107 11:40:12.272589 11160 solver.cpp:218] Iteration 54900 (11.733 iter/s, 8.52297s/100 iters), loss = 0.20365
I1107 11:40:12.273589 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:40:12.273589 11160 solver.cpp:237]     Train net output #1: loss = 0.20365 (* 1 = 0.20365 loss)
I1107 11:40:12.273589 11160 sgd_solver.cpp:105] Iteration 54900, lr = 0.01
I1107 11:40:20.381866  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:40:20.719884 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_55000.caffemodel
I1107 11:40:20.748884 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_55000.solverstate
I1107 11:40:20.758399 11160 solver.cpp:330] Iteration 55000, Testing net (#0)
I1107 11:40:20.758890 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:40:22.751013 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:40:22.830018 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8852
I1107 11:40:22.830018 11160 solver.cpp:397]     Test net output #1: loss = 0.351896 (* 1 = 0.351896 loss)
I1107 11:40:22.912024 11160 solver.cpp:218] Iteration 55000 (9.40006 iter/s, 10.6382s/100 iters), loss = 0.20635
I1107 11:40:22.912024 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:40:22.912024 11160 solver.cpp:237]     Train net output #1: loss = 0.20635 (* 1 = 0.20635 loss)
I1107 11:40:22.912024 11160 sgd_solver.cpp:105] Iteration 55000, lr = 0.01
I1107 11:40:31.437525 11160 solver.cpp:218] Iteration 55100 (11.7296 iter/s, 8.52548s/100 iters), loss = 0.31196
I1107 11:40:31.437525 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1107 11:40:31.437525 11160 solver.cpp:237]     Train net output #1: loss = 0.311961 (* 1 = 0.311961 loss)
I1107 11:40:31.437525 11160 sgd_solver.cpp:105] Iteration 55100, lr = 0.01
I1107 11:40:39.958789 11160 solver.cpp:218] Iteration 55200 (11.7366 iter/s, 8.52037s/100 iters), loss = 0.245245
I1107 11:40:39.958789 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:40:39.958789 11160 solver.cpp:237]     Train net output #1: loss = 0.245245 (* 1 = 0.245245 loss)
I1107 11:40:39.958789 11160 sgd_solver.cpp:105] Iteration 55200, lr = 0.01
I1107 11:40:48.489779 11160 solver.cpp:218] Iteration 55300 (11.723 iter/s, 8.53024s/100 iters), loss = 0.2483
I1107 11:40:48.489779 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:40:48.489779 11160 solver.cpp:237]     Train net output #1: loss = 0.2483 (* 1 = 0.2483 loss)
I1107 11:40:48.489779 11160 sgd_solver.cpp:105] Iteration 55300, lr = 0.01
I1107 11:40:57.016697 11160 solver.cpp:218] Iteration 55400 (11.728 iter/s, 8.52662s/100 iters), loss = 0.213197
I1107 11:40:57.016697 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:40:57.016697 11160 solver.cpp:237]     Train net output #1: loss = 0.213197 (* 1 = 0.213197 loss)
I1107 11:40:57.016697 11160 sgd_solver.cpp:105] Iteration 55400, lr = 0.01
I1107 11:41:05.132508  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:41:05.470557 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_55500.caffemodel
I1107 11:41:05.501565 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_55500.solverstate
I1107 11:41:05.510565 11160 solver.cpp:330] Iteration 55500, Testing net (#0)
I1107 11:41:05.510565 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:41:07.497665 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:41:07.577678 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8727
I1107 11:41:07.577678 11160 solver.cpp:397]     Test net output #1: loss = 0.392217 (* 1 = 0.392217 loss)
I1107 11:41:07.658674 11160 solver.cpp:218] Iteration 55500 (9.39719 iter/s, 10.6415s/100 iters), loss = 0.232588
I1107 11:41:07.658674 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:41:07.658674 11160 solver.cpp:237]     Train net output #1: loss = 0.232588 (* 1 = 0.232588 loss)
I1107 11:41:07.658674 11160 sgd_solver.cpp:105] Iteration 55500, lr = 0.01
I1107 11:41:16.185004 11160 solver.cpp:218] Iteration 55600 (11.7287 iter/s, 8.52612s/100 iters), loss = 0.284193
I1107 11:41:16.185004 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:41:16.185004 11160 solver.cpp:237]     Train net output #1: loss = 0.284193 (* 1 = 0.284193 loss)
I1107 11:41:16.185004 11160 sgd_solver.cpp:105] Iteration 55600, lr = 0.01
I1107 11:41:24.720821 11160 solver.cpp:218] Iteration 55700 (11.7165 iter/s, 8.53497s/100 iters), loss = 0.294077
I1107 11:41:24.720821 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:41:24.720821 11160 solver.cpp:237]     Train net output #1: loss = 0.294077 (* 1 = 0.294077 loss)
I1107 11:41:24.720821 11160 sgd_solver.cpp:105] Iteration 55700, lr = 0.01
I1107 11:41:33.246995 11160 solver.cpp:218] Iteration 55800 (11.7291 iter/s, 8.5258s/100 iters), loss = 0.206309
I1107 11:41:33.246995 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:41:33.246995 11160 solver.cpp:237]     Train net output #1: loss = 0.206309 (* 1 = 0.206309 loss)
I1107 11:41:33.246995 11160 sgd_solver.cpp:105] Iteration 55800, lr = 0.01
I1107 11:41:41.774574 11160 solver.cpp:218] Iteration 55900 (11.728 iter/s, 8.52662s/100 iters), loss = 0.190507
I1107 11:41:41.774574 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:41:41.774574 11160 solver.cpp:237]     Train net output #1: loss = 0.190508 (* 1 = 0.190508 loss)
I1107 11:41:41.774574 11160 sgd_solver.cpp:105] Iteration 55900, lr = 0.01
I1107 11:41:49.890341  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:41:50.229393 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_56000.caffemodel
I1107 11:41:50.258394 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_56000.solverstate
I1107 11:41:50.267395 11160 solver.cpp:330] Iteration 56000, Testing net (#0)
I1107 11:41:50.267395 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:41:52.258698 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:41:52.338703 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8755
I1107 11:41:52.338703 11160 solver.cpp:397]     Test net output #1: loss = 0.380294 (* 1 = 0.380294 loss)
I1107 11:41:52.420203 11160 solver.cpp:218] Iteration 56000 (9.39395 iter/s, 10.6452s/100 iters), loss = 0.171131
I1107 11:41:52.420203 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 11:41:52.420203 11160 solver.cpp:237]     Train net output #1: loss = 0.171131 (* 1 = 0.171131 loss)
I1107 11:41:52.420203 11160 sgd_solver.cpp:105] Iteration 56000, lr = 0.01
I1107 11:42:00.944496 11160 solver.cpp:218] Iteration 56100 (11.7319 iter/s, 8.52377s/100 iters), loss = 0.316852
I1107 11:42:00.944496 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1107 11:42:00.944496 11160 solver.cpp:237]     Train net output #1: loss = 0.316852 (* 1 = 0.316852 loss)
I1107 11:42:00.944496 11160 sgd_solver.cpp:105] Iteration 56100, lr = 0.01
I1107 11:42:09.474613 11160 solver.cpp:218] Iteration 56200 (11.7233 iter/s, 8.53s/100 iters), loss = 0.227744
I1107 11:42:09.474613 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:42:09.474613 11160 solver.cpp:237]     Train net output #1: loss = 0.227744 (* 1 = 0.227744 loss)
I1107 11:42:09.474613 11160 sgd_solver.cpp:105] Iteration 56200, lr = 0.01
I1107 11:42:18.010390 11160 solver.cpp:218] Iteration 56300 (11.7166 iter/s, 8.53486s/100 iters), loss = 0.219534
I1107 11:42:18.010390 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:42:18.010390 11160 solver.cpp:237]     Train net output #1: loss = 0.219535 (* 1 = 0.219535 loss)
I1107 11:42:18.010390 11160 sgd_solver.cpp:105] Iteration 56300, lr = 0.01
I1107 11:42:26.555199 11160 solver.cpp:218] Iteration 56400 (11.7044 iter/s, 8.54383s/100 iters), loss = 0.205485
I1107 11:42:26.555199 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:42:26.555199 11160 solver.cpp:237]     Train net output #1: loss = 0.205485 (* 1 = 0.205485 loss)
I1107 11:42:26.555199 11160 sgd_solver.cpp:105] Iteration 56400, lr = 0.01
I1107 11:42:34.662734  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:42:34.999781 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_56500.caffemodel
I1107 11:42:35.027781 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_56500.solverstate
I1107 11:42:35.035781 11160 solver.cpp:330] Iteration 56500, Testing net (#0)
I1107 11:42:35.036782 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:42:37.026965 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:42:37.106969 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8738
I1107 11:42:37.106969 11160 solver.cpp:397]     Test net output #1: loss = 0.395876 (* 1 = 0.395876 loss)
I1107 11:42:37.187975 11160 solver.cpp:218] Iteration 56500 (9.40488 iter/s, 10.6328s/100 iters), loss = 0.142152
I1107 11:42:37.187975 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 11:42:37.187975 11160 solver.cpp:237]     Train net output #1: loss = 0.142152 (* 1 = 0.142152 loss)
I1107 11:42:37.187975 11160 sgd_solver.cpp:105] Iteration 56500, lr = 0.01
I1107 11:42:45.740257 11160 solver.cpp:218] Iteration 56600 (11.693 iter/s, 8.5521s/100 iters), loss = 0.260806
I1107 11:42:45.741257 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:42:45.741257 11160 solver.cpp:237]     Train net output #1: loss = 0.260806 (* 1 = 0.260806 loss)
I1107 11:42:45.741257 11160 sgd_solver.cpp:105] Iteration 56600, lr = 0.01
I1107 11:42:54.293143 11160 solver.cpp:218] Iteration 56700 (11.6926 iter/s, 8.55239s/100 iters), loss = 0.218948
I1107 11:42:54.294142 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:42:54.294142 11160 solver.cpp:237]     Train net output #1: loss = 0.218948 (* 1 = 0.218948 loss)
I1107 11:42:54.294142 11160 sgd_solver.cpp:105] Iteration 56700, lr = 0.01
I1107 11:43:02.837157 11160 solver.cpp:218] Iteration 56800 (11.7055 iter/s, 8.54298s/100 iters), loss = 0.250094
I1107 11:43:02.837157 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:43:02.837157 11160 solver.cpp:237]     Train net output #1: loss = 0.250094 (* 1 = 0.250094 loss)
I1107 11:43:02.837157 11160 sgd_solver.cpp:105] Iteration 56800, lr = 0.01
I1107 11:43:11.375049 11160 solver.cpp:218] Iteration 56900 (11.7133 iter/s, 8.5373s/100 iters), loss = 0.220497
I1107 11:43:11.375049 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:43:11.375049 11160 solver.cpp:237]     Train net output #1: loss = 0.220497 (* 1 = 0.220497 loss)
I1107 11:43:11.375049 11160 sgd_solver.cpp:105] Iteration 56900, lr = 0.01
I1107 11:43:19.490708  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:43:19.828740 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_57000.caffemodel
I1107 11:43:19.858739 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_57000.solverstate
I1107 11:43:19.867739 11160 solver.cpp:330] Iteration 57000, Testing net (#0)
I1107 11:43:19.867739 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:43:21.858219 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:43:21.938223 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8768
I1107 11:43:21.938223 11160 solver.cpp:397]     Test net output #1: loss = 0.389771 (* 1 = 0.389771 loss)
I1107 11:43:22.019234 11160 solver.cpp:218] Iteration 57000 (9.39485 iter/s, 10.6441s/100 iters), loss = 0.217496
I1107 11:43:22.019234 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:43:22.019234 11160 solver.cpp:237]     Train net output #1: loss = 0.217496 (* 1 = 0.217496 loss)
I1107 11:43:22.019234 11160 sgd_solver.cpp:105] Iteration 57000, lr = 0.01
I1107 11:43:30.548997 11160 solver.cpp:218] Iteration 57100 (11.7247 iter/s, 8.52903s/100 iters), loss = 0.267352
I1107 11:43:30.548997 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1107 11:43:30.548997 11160 solver.cpp:237]     Train net output #1: loss = 0.267352 (* 1 = 0.267352 loss)
I1107 11:43:30.548997 11160 sgd_solver.cpp:105] Iteration 57100, lr = 0.01
I1107 11:43:39.083778 11160 solver.cpp:218] Iteration 57200 (11.7183 iter/s, 8.53367s/100 iters), loss = 0.181043
I1107 11:43:39.083778 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:43:39.083778 11160 solver.cpp:237]     Train net output #1: loss = 0.181043 (* 1 = 0.181043 loss)
I1107 11:43:39.083778 11160 sgd_solver.cpp:105] Iteration 57200, lr = 0.01
I1107 11:43:47.620517 11160 solver.cpp:218] Iteration 57300 (11.7138 iter/s, 8.53694s/100 iters), loss = 0.271298
I1107 11:43:47.620517 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:43:47.620517 11160 solver.cpp:237]     Train net output #1: loss = 0.271298 (* 1 = 0.271298 loss)
I1107 11:43:47.620517 11160 sgd_solver.cpp:105] Iteration 57300, lr = 0.01
I1107 11:43:56.155268 11160 solver.cpp:218] Iteration 57400 (11.7178 iter/s, 8.53402s/100 iters), loss = 0.199561
I1107 11:43:56.155268 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:43:56.155268 11160 solver.cpp:237]     Train net output #1: loss = 0.199561 (* 1 = 0.199561 loss)
I1107 11:43:56.155268 11160 sgd_solver.cpp:105] Iteration 57400, lr = 0.01
I1107 11:44:04.268429  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:44:04.604449 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_57500.caffemodel
I1107 11:44:04.634450 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_57500.solverstate
I1107 11:44:04.643450 11160 solver.cpp:330] Iteration 57500, Testing net (#0)
I1107 11:44:04.643450 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:44:06.632598 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:44:06.712600 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8635
I1107 11:44:06.712600 11160 solver.cpp:397]     Test net output #1: loss = 0.426639 (* 1 = 0.426639 loss)
I1107 11:44:06.793606 11160 solver.cpp:218] Iteration 57500 (9.40066 iter/s, 10.6376s/100 iters), loss = 0.211304
I1107 11:44:06.793606 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:44:06.793606 11160 solver.cpp:237]     Train net output #1: loss = 0.211304 (* 1 = 0.211304 loss)
I1107 11:44:06.793606 11160 sgd_solver.cpp:105] Iteration 57500, lr = 0.01
I1107 11:44:15.333518 11160 solver.cpp:218] Iteration 57600 (11.7098 iter/s, 8.53987s/100 iters), loss = 0.314849
I1107 11:44:15.333518 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:44:15.333518 11160 solver.cpp:237]     Train net output #1: loss = 0.314849 (* 1 = 0.314849 loss)
I1107 11:44:15.333518 11160 sgd_solver.cpp:105] Iteration 57600, lr = 0.01
I1107 11:44:23.869791 11160 solver.cpp:218] Iteration 57700 (11.7163 iter/s, 8.53515s/100 iters), loss = 0.319503
I1107 11:44:23.869791 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1107 11:44:23.869791 11160 solver.cpp:237]     Train net output #1: loss = 0.319503 (* 1 = 0.319503 loss)
I1107 11:44:23.869791 11160 sgd_solver.cpp:105] Iteration 57700, lr = 0.01
I1107 11:44:32.400051 11160 solver.cpp:218] Iteration 57800 (11.7232 iter/s, 8.53013s/100 iters), loss = 0.241266
I1107 11:44:32.400051 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:44:32.400051 11160 solver.cpp:237]     Train net output #1: loss = 0.241266 (* 1 = 0.241266 loss)
I1107 11:44:32.400051 11160 sgd_solver.cpp:105] Iteration 57800, lr = 0.01
I1107 11:44:40.933187 11160 solver.cpp:218] Iteration 57900 (11.7195 iter/s, 8.5328s/100 iters), loss = 0.179766
I1107 11:44:40.933187 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:44:40.933187 11160 solver.cpp:237]     Train net output #1: loss = 0.179766 (* 1 = 0.179766 loss)
I1107 11:44:40.933187 11160 sgd_solver.cpp:105] Iteration 57900, lr = 0.01
I1107 11:44:49.042269  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:44:49.381299 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_58000.caffemodel
I1107 11:44:49.412317 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_58000.solverstate
I1107 11:44:49.420816 11160 solver.cpp:330] Iteration 58000, Testing net (#0)
I1107 11:44:49.420816 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:44:51.411461 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:44:51.490465 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8724
I1107 11:44:51.490465 11160 solver.cpp:397]     Test net output #1: loss = 0.402847 (* 1 = 0.402847 loss)
I1107 11:44:51.572968 11160 solver.cpp:218] Iteration 58000 (9.39989 iter/s, 10.6384s/100 iters), loss = 0.178854
I1107 11:44:51.572968 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:44:51.572968 11160 solver.cpp:237]     Train net output #1: loss = 0.178855 (* 1 = 0.178855 loss)
I1107 11:44:51.572968 11160 sgd_solver.cpp:105] Iteration 58000, lr = 0.01
I1107 11:45:00.095918 11160 solver.cpp:218] Iteration 58100 (11.7336 iter/s, 8.52251s/100 iters), loss = 0.226573
I1107 11:45:00.095918 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:45:00.095918 11160 solver.cpp:237]     Train net output #1: loss = 0.226573 (* 1 = 0.226573 loss)
I1107 11:45:00.095918 11160 sgd_solver.cpp:105] Iteration 58100, lr = 0.01
I1107 11:45:08.616418 11160 solver.cpp:218] Iteration 58200 (11.7365 iter/s, 8.52044s/100 iters), loss = 0.196786
I1107 11:45:08.616418 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:45:08.616418 11160 solver.cpp:237]     Train net output #1: loss = 0.196786 (* 1 = 0.196786 loss)
I1107 11:45:08.616418 11160 sgd_solver.cpp:105] Iteration 58200, lr = 0.01
I1107 11:45:17.137557 11160 solver.cpp:218] Iteration 58300 (11.7368 iter/s, 8.52023s/100 iters), loss = 0.258454
I1107 11:45:17.137557 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:45:17.137557 11160 solver.cpp:237]     Train net output #1: loss = 0.258454 (* 1 = 0.258454 loss)
I1107 11:45:17.137557 11160 sgd_solver.cpp:105] Iteration 58300, lr = 0.01
I1107 11:45:25.661705 11160 solver.cpp:218] Iteration 58400 (11.7315 iter/s, 8.52407s/100 iters), loss = 0.192767
I1107 11:45:25.661705 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:45:25.661705 11160 solver.cpp:237]     Train net output #1: loss = 0.192767 (* 1 = 0.192767 loss)
I1107 11:45:25.661705 11160 sgd_solver.cpp:105] Iteration 58400, lr = 0.01
I1107 11:45:33.765172  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:45:34.101814 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_58500.caffemodel
I1107 11:45:34.132318 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_58500.solverstate
I1107 11:45:34.141319 11160 solver.cpp:330] Iteration 58500, Testing net (#0)
I1107 11:45:34.141319 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:45:36.135782 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:45:36.215282 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8751
I1107 11:45:36.215282 11160 solver.cpp:397]     Test net output #1: loss = 0.399165 (* 1 = 0.399165 loss)
I1107 11:45:36.295784 11160 solver.cpp:218] Iteration 58500 (9.40444 iter/s, 10.6333s/100 iters), loss = 0.268513
I1107 11:45:36.295784 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:45:36.295784 11160 solver.cpp:237]     Train net output #1: loss = 0.268513 (* 1 = 0.268513 loss)
I1107 11:45:36.295784 11160 sgd_solver.cpp:105] Iteration 58500, lr = 0.01
I1107 11:45:44.826001 11160 solver.cpp:218] Iteration 58600 (11.7236 iter/s, 8.52984s/100 iters), loss = 0.251696
I1107 11:45:44.826503 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:45:44.826503 11160 solver.cpp:237]     Train net output #1: loss = 0.251696 (* 1 = 0.251696 loss)
I1107 11:45:44.826503 11160 sgd_solver.cpp:105] Iteration 58600, lr = 0.01
I1107 11:45:53.361624 11160 solver.cpp:218] Iteration 58700 (11.7168 iter/s, 8.53476s/100 iters), loss = 0.219748
I1107 11:45:53.361624 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:45:53.361624 11160 solver.cpp:237]     Train net output #1: loss = 0.219748 (* 1 = 0.219748 loss)
I1107 11:45:53.361624 11160 sgd_solver.cpp:105] Iteration 58700, lr = 0.01
I1107 11:46:01.897639 11160 solver.cpp:218] Iteration 58800 (11.7153 iter/s, 8.53587s/100 iters), loss = 0.20986
I1107 11:46:01.897639 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:46:01.897639 11160 solver.cpp:237]     Train net output #1: loss = 0.20986 (* 1 = 0.20986 loss)
I1107 11:46:01.897639 11160 sgd_solver.cpp:105] Iteration 58800, lr = 0.01
I1107 11:46:10.425511 11160 solver.cpp:218] Iteration 58900 (11.7275 iter/s, 8.52694s/100 iters), loss = 0.185051
I1107 11:46:10.426013 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:46:10.426013 11160 solver.cpp:237]     Train net output #1: loss = 0.185051 (* 1 = 0.185051 loss)
I1107 11:46:10.426013 11160 sgd_solver.cpp:105] Iteration 58900, lr = 0.01
I1107 11:46:18.534809  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:46:18.871846 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_59000.caffemodel
I1107 11:46:18.901846 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_59000.solverstate
I1107 11:46:18.910846 11160 solver.cpp:330] Iteration 59000, Testing net (#0)
I1107 11:46:18.910846 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:46:20.899997 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:46:20.979003 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8641
I1107 11:46:20.979003 11160 solver.cpp:397]     Test net output #1: loss = 0.432395 (* 1 = 0.432395 loss)
I1107 11:46:21.061007 11160 solver.cpp:218] Iteration 59000 (9.4034 iter/s, 10.6344s/100 iters), loss = 0.191534
I1107 11:46:21.061007 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 11:46:21.061007 11160 solver.cpp:237]     Train net output #1: loss = 0.191534 (* 1 = 0.191534 loss)
I1107 11:46:21.061007 11160 sgd_solver.cpp:105] Iteration 59000, lr = 0.01
I1107 11:46:29.592458 11160 solver.cpp:218] Iteration 59100 (11.7222 iter/s, 8.53085s/100 iters), loss = 0.263356
I1107 11:46:29.592458 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:46:29.592458 11160 solver.cpp:237]     Train net output #1: loss = 0.263356 (* 1 = 0.263356 loss)
I1107 11:46:29.592458 11160 sgd_solver.cpp:105] Iteration 59100, lr = 0.01
I1107 11:46:38.127518 11160 solver.cpp:218] Iteration 59200 (11.7169 iter/s, 8.53471s/100 iters), loss = 0.206464
I1107 11:46:38.127518 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:46:38.127518 11160 solver.cpp:237]     Train net output #1: loss = 0.206465 (* 1 = 0.206465 loss)
I1107 11:46:38.127518 11160 sgd_solver.cpp:105] Iteration 59200, lr = 0.01
I1107 11:46:46.653266 11160 solver.cpp:218] Iteration 59300 (11.7287 iter/s, 8.52607s/100 iters), loss = 0.219414
I1107 11:46:46.654265 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:46:46.654265 11160 solver.cpp:237]     Train net output #1: loss = 0.219414 (* 1 = 0.219414 loss)
I1107 11:46:46.654265 11160 sgd_solver.cpp:105] Iteration 59300, lr = 0.01
I1107 11:46:55.194059 11160 solver.cpp:218] Iteration 59400 (11.7094 iter/s, 8.54016s/100 iters), loss = 0.220091
I1107 11:46:55.195060 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:46:55.195060 11160 solver.cpp:237]     Train net output #1: loss = 0.220091 (* 1 = 0.220091 loss)
I1107 11:46:55.195060 11160 sgd_solver.cpp:105] Iteration 59400, lr = 0.01
I1107 11:47:03.304910  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:47:03.641930 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_59500.caffemodel
I1107 11:47:03.673933 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_59500.solverstate
I1107 11:47:03.682934 11160 solver.cpp:330] Iteration 59500, Testing net (#0)
I1107 11:47:03.682934 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:47:05.673167 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:47:05.752182 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8688
I1107 11:47:05.752182 11160 solver.cpp:397]     Test net output #1: loss = 0.419944 (* 1 = 0.419944 loss)
I1107 11:47:05.834173 11160 solver.cpp:218] Iteration 59500 (9.39966 iter/s, 10.6387s/100 iters), loss = 0.266924
I1107 11:47:05.834173 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:47:05.834173 11160 solver.cpp:237]     Train net output #1: loss = 0.266924 (* 1 = 0.266924 loss)
I1107 11:47:05.834173 11160 sgd_solver.cpp:105] Iteration 59500, lr = 0.01
I1107 11:47:14.361044 11160 solver.cpp:218] Iteration 59600 (11.7277 iter/s, 8.52682s/100 iters), loss = 0.283587
I1107 11:47:14.361044 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:47:14.361044 11160 solver.cpp:237]     Train net output #1: loss = 0.283587 (* 1 = 0.283587 loss)
I1107 11:47:14.361044 11160 sgd_solver.cpp:105] Iteration 59600, lr = 0.01
I1107 11:47:22.884145 11160 solver.cpp:218] Iteration 59700 (11.7333 iter/s, 8.52272s/100 iters), loss = 0.20972
I1107 11:47:22.884145 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:47:22.884145 11160 solver.cpp:237]     Train net output #1: loss = 0.20972 (* 1 = 0.20972 loss)
I1107 11:47:22.884145 11160 sgd_solver.cpp:105] Iteration 59700, lr = 0.01
I1107 11:47:31.408957 11160 solver.cpp:218] Iteration 59800 (11.7313 iter/s, 8.52423s/100 iters), loss = 0.254624
I1107 11:47:31.408957 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:47:31.408957 11160 solver.cpp:237]     Train net output #1: loss = 0.254625 (* 1 = 0.254625 loss)
I1107 11:47:31.408957 11160 sgd_solver.cpp:105] Iteration 59800, lr = 0.01
I1107 11:47:39.939671 11160 solver.cpp:218] Iteration 59900 (11.7238 iter/s, 8.52965s/100 iters), loss = 0.226309
I1107 11:47:39.939671 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:47:39.939671 11160 solver.cpp:237]     Train net output #1: loss = 0.22631 (* 1 = 0.22631 loss)
I1107 11:47:39.939671 11160 sgd_solver.cpp:105] Iteration 59900, lr = 0.01
I1107 11:47:48.039891  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:47:48.375931 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_60000.caffemodel
I1107 11:47:48.406931 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_60000.solverstate
I1107 11:47:48.415932 11160 solver.cpp:330] Iteration 60000, Testing net (#0)
I1107 11:47:48.415932 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:47:50.410313 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:47:50.489320 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8744
I1107 11:47:50.489320 11160 solver.cpp:397]     Test net output #1: loss = 0.40758 (* 1 = 0.40758 loss)
I1107 11:47:50.570340 11160 solver.cpp:218] Iteration 60000 (9.40693 iter/s, 10.6305s/100 iters), loss = 0.210004
I1107 11:47:50.570340 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:47:50.570340 11160 solver.cpp:237]     Train net output #1: loss = 0.210004 (* 1 = 0.210004 loss)
I1107 11:47:50.570340 11160 sgd_solver.cpp:105] Iteration 60000, lr = 0.01
I1107 11:47:59.104291 11160 solver.cpp:218] Iteration 60100 (11.7182 iter/s, 8.53372s/100 iters), loss = 0.250174
I1107 11:47:59.104291 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:47:59.104291 11160 solver.cpp:237]     Train net output #1: loss = 0.250175 (* 1 = 0.250175 loss)
I1107 11:47:59.104291 11160 sgd_solver.cpp:105] Iteration 60100, lr = 0.01
I1107 11:48:07.644603 11160 solver.cpp:218] Iteration 60200 (11.7106 iter/s, 8.53924s/100 iters), loss = 0.266838
I1107 11:48:07.644603 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:48:07.644603 11160 solver.cpp:237]     Train net output #1: loss = 0.266838 (* 1 = 0.266838 loss)
I1107 11:48:07.644603 11160 sgd_solver.cpp:105] Iteration 60200, lr = 0.01
I1107 11:48:16.181399 11160 solver.cpp:218] Iteration 60300 (11.7141 iter/s, 8.53669s/100 iters), loss = 0.177153
I1107 11:48:16.181399 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:48:16.181399 11160 solver.cpp:237]     Train net output #1: loss = 0.177153 (* 1 = 0.177153 loss)
I1107 11:48:16.181399 11160 sgd_solver.cpp:105] Iteration 60300, lr = 0.01
I1107 11:48:24.712422 11160 solver.cpp:218] Iteration 60400 (11.7226 iter/s, 8.53054s/100 iters), loss = 0.214379
I1107 11:48:24.712422 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:48:24.712422 11160 solver.cpp:237]     Train net output #1: loss = 0.214379 (* 1 = 0.214379 loss)
I1107 11:48:24.712422 11160 sgd_solver.cpp:105] Iteration 60400, lr = 0.01
I1107 11:48:32.828424  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:48:33.167394 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_60500.caffemodel
I1107 11:48:33.196393 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_60500.solverstate
I1107 11:48:33.205394 11160 solver.cpp:330] Iteration 60500, Testing net (#0)
I1107 11:48:33.206393 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:48:35.195369 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:48:35.275391 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8635
I1107 11:48:35.275391 11160 solver.cpp:397]     Test net output #1: loss = 0.429102 (* 1 = 0.429102 loss)
I1107 11:48:35.357398 11160 solver.cpp:218] Iteration 60500 (9.39513 iter/s, 10.6438s/100 iters), loss = 0.191523
I1107 11:48:35.357398 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:48:35.357398 11160 solver.cpp:237]     Train net output #1: loss = 0.191523 (* 1 = 0.191523 loss)
I1107 11:48:35.357398 11160 sgd_solver.cpp:105] Iteration 60500, lr = 0.01
I1107 11:48:43.881678 11160 solver.cpp:218] Iteration 60600 (11.7316 iter/s, 8.524s/100 iters), loss = 0.320898
I1107 11:48:43.881678 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:48:43.881678 11160 solver.cpp:237]     Train net output #1: loss = 0.320898 (* 1 = 0.320898 loss)
I1107 11:48:43.881678 11160 sgd_solver.cpp:105] Iteration 60600, lr = 0.01
I1107 11:48:52.415706 11160 solver.cpp:218] Iteration 60700 (11.7184 iter/s, 8.53358s/100 iters), loss = 0.219522
I1107 11:48:52.415706 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:48:52.415706 11160 solver.cpp:237]     Train net output #1: loss = 0.219522 (* 1 = 0.219522 loss)
I1107 11:48:52.415706 11160 sgd_solver.cpp:105] Iteration 60700, lr = 0.01
I1107 11:49:00.942649 11160 solver.cpp:218] Iteration 60800 (11.7282 iter/s, 8.52647s/100 iters), loss = 0.151782
I1107 11:49:00.942649 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 11:49:00.942649 11160 solver.cpp:237]     Train net output #1: loss = 0.151782 (* 1 = 0.151782 loss)
I1107 11:49:00.942649 11160 sgd_solver.cpp:105] Iteration 60800, lr = 0.01
I1107 11:49:09.465674 11160 solver.cpp:218] Iteration 60900 (11.7338 iter/s, 8.52237s/100 iters), loss = 0.172014
I1107 11:49:09.465674 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:49:09.465674 11160 solver.cpp:237]     Train net output #1: loss = 0.172014 (* 1 = 0.172014 loss)
I1107 11:49:09.465674 11160 sgd_solver.cpp:105] Iteration 60900, lr = 0.01
I1107 11:49:17.569439  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:49:17.906595 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_61000.caffemodel
I1107 11:49:17.934595 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_61000.solverstate
I1107 11:49:17.943611 11160 solver.cpp:330] Iteration 61000, Testing net (#0)
I1107 11:49:17.943611 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:49:19.933779 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:49:20.013317 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8687
I1107 11:49:20.013317 11160 solver.cpp:397]     Test net output #1: loss = 0.419011 (* 1 = 0.419011 loss)
I1107 11:49:20.094334 11160 solver.cpp:218] Iteration 61000 (9.40867 iter/s, 10.6285s/100 iters), loss = 0.155973
I1107 11:49:20.094334 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:49:20.094334 11160 solver.cpp:237]     Train net output #1: loss = 0.155973 (* 1 = 0.155973 loss)
I1107 11:49:20.094334 11160 sgd_solver.cpp:105] Iteration 61000, lr = 0.01
I1107 11:49:28.618752 11160 solver.cpp:218] Iteration 61100 (11.7325 iter/s, 8.52331s/100 iters), loss = 0.278588
I1107 11:49:28.618752 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:49:28.618752 11160 solver.cpp:237]     Train net output #1: loss = 0.278588 (* 1 = 0.278588 loss)
I1107 11:49:28.618752 11160 sgd_solver.cpp:105] Iteration 61100, lr = 0.01
I1107 11:49:37.149886 11160 solver.cpp:218] Iteration 61200 (11.7224 iter/s, 8.53064s/100 iters), loss = 0.186026
I1107 11:49:37.149886 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 11:49:37.149886 11160 solver.cpp:237]     Train net output #1: loss = 0.186026 (* 1 = 0.186026 loss)
I1107 11:49:37.149886 11160 sgd_solver.cpp:105] Iteration 61200, lr = 0.01
I1107 11:49:45.680855 11160 solver.cpp:218] Iteration 61300 (11.7224 iter/s, 8.53069s/100 iters), loss = 0.238879
I1107 11:49:45.680855 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:49:45.680855 11160 solver.cpp:237]     Train net output #1: loss = 0.238879 (* 1 = 0.238879 loss)
I1107 11:49:45.680855 11160 sgd_solver.cpp:105] Iteration 61300, lr = 0.01
I1107 11:49:54.208889 11160 solver.cpp:218] Iteration 61400 (11.7259 iter/s, 8.52811s/100 iters), loss = 0.200756
I1107 11:49:54.209889 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:49:54.209889 11160 solver.cpp:237]     Train net output #1: loss = 0.200756 (* 1 = 0.200756 loss)
I1107 11:49:54.209889 11160 sgd_solver.cpp:105] Iteration 61400, lr = 0.01
I1107 11:50:02.343075  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:50:02.681315 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_61500.caffemodel
I1107 11:50:02.711313 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_61500.solverstate
I1107 11:50:02.720311 11160 solver.cpp:330] Iteration 61500, Testing net (#0)
I1107 11:50:02.721312 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:50:04.715684 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:50:04.794695 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8681
I1107 11:50:04.794695 11160 solver.cpp:397]     Test net output #1: loss = 0.420753 (* 1 = 0.420753 loss)
I1107 11:50:04.876703 11160 solver.cpp:218] Iteration 61500 (9.37501 iter/s, 10.6667s/100 iters), loss = 0.131843
I1107 11:50:04.876703 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 11:50:04.876703 11160 solver.cpp:237]     Train net output #1: loss = 0.131843 (* 1 = 0.131843 loss)
I1107 11:50:04.876703 11160 sgd_solver.cpp:105] Iteration 61500, lr = 0.01
I1107 11:50:13.411476 11160 solver.cpp:218] Iteration 61600 (11.7172 iter/s, 8.53446s/100 iters), loss = 0.273046
I1107 11:50:13.411476 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:50:13.411476 11160 solver.cpp:237]     Train net output #1: loss = 0.273046 (* 1 = 0.273046 loss)
I1107 11:50:13.411476 11160 sgd_solver.cpp:105] Iteration 61600, lr = 0.01
I1107 11:50:21.948374 11160 solver.cpp:218] Iteration 61700 (11.7141 iter/s, 8.53673s/100 iters), loss = 0.188982
I1107 11:50:21.948374 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:50:21.948374 11160 solver.cpp:237]     Train net output #1: loss = 0.188982 (* 1 = 0.188982 loss)
I1107 11:50:21.948374 11160 sgd_solver.cpp:105] Iteration 61700, lr = 0.01
I1107 11:50:30.488328 11160 solver.cpp:218] Iteration 61800 (11.7108 iter/s, 8.53916s/100 iters), loss = 0.212822
I1107 11:50:30.488328 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:50:30.488328 11160 solver.cpp:237]     Train net output #1: loss = 0.212822 (* 1 = 0.212822 loss)
I1107 11:50:30.488328 11160 sgd_solver.cpp:105] Iteration 61800, lr = 0.01
I1107 11:50:39.026335 11160 solver.cpp:218] Iteration 61900 (11.7132 iter/s, 8.53738s/100 iters), loss = 0.173879
I1107 11:50:39.026335 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:50:39.026335 11160 solver.cpp:237]     Train net output #1: loss = 0.173879 (* 1 = 0.173879 loss)
I1107 11:50:39.026335 11160 sgd_solver.cpp:105] Iteration 61900, lr = 0.01
I1107 11:50:47.139417  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:50:47.475958 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_62000.caffemodel
I1107 11:50:47.504462 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_62000.solverstate
I1107 11:50:47.513463 11160 solver.cpp:330] Iteration 62000, Testing net (#0)
I1107 11:50:47.514463 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:50:49.505671 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:50:49.584677 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8572
I1107 11:50:49.584677 11160 solver.cpp:397]     Test net output #1: loss = 0.454851 (* 1 = 0.454851 loss)
I1107 11:50:49.666676 11160 solver.cpp:218] Iteration 62000 (9.39895 iter/s, 10.6395s/100 iters), loss = 0.246197
I1107 11:50:49.666676 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:50:49.666676 11160 solver.cpp:237]     Train net output #1: loss = 0.246197 (* 1 = 0.246197 loss)
I1107 11:50:49.666676 11160 sgd_solver.cpp:105] Iteration 62000, lr = 0.01
I1107 11:50:58.194989 11160 solver.cpp:218] Iteration 62100 (11.726 iter/s, 8.52804s/100 iters), loss = 0.225751
I1107 11:50:58.194989 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:50:58.194989 11160 solver.cpp:237]     Train net output #1: loss = 0.225751 (* 1 = 0.225751 loss)
I1107 11:50:58.194989 11160 sgd_solver.cpp:105] Iteration 62100, lr = 0.01
I1107 11:51:06.713886 11160 solver.cpp:218] Iteration 62200 (11.7396 iter/s, 8.51816s/100 iters), loss = 0.202138
I1107 11:51:06.713886 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:51:06.713886 11160 solver.cpp:237]     Train net output #1: loss = 0.202138 (* 1 = 0.202138 loss)
I1107 11:51:06.713886 11160 sgd_solver.cpp:105] Iteration 62200, lr = 0.01
I1107 11:51:15.237561 11160 solver.cpp:218] Iteration 62300 (11.7326 iter/s, 8.52326s/100 iters), loss = 0.190057
I1107 11:51:15.237561 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:51:15.237561 11160 solver.cpp:237]     Train net output #1: loss = 0.190057 (* 1 = 0.190057 loss)
I1107 11:51:15.237561 11160 sgd_solver.cpp:105] Iteration 62300, lr = 0.01
I1107 11:51:23.761694 11160 solver.cpp:218] Iteration 62400 (11.7317 iter/s, 8.52394s/100 iters), loss = 0.137039
I1107 11:51:23.761694 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:51:23.761694 11160 solver.cpp:237]     Train net output #1: loss = 0.137039 (* 1 = 0.137039 loss)
I1107 11:51:23.761694 11160 sgd_solver.cpp:105] Iteration 62400, lr = 0.01
I1107 11:51:31.869542  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:51:32.205576 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_62500.caffemodel
I1107 11:51:32.236577 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_62500.solverstate
I1107 11:51:32.245576 11160 solver.cpp:330] Iteration 62500, Testing net (#0)
I1107 11:51:32.245576 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:51:34.234814 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:51:34.313820 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8686
I1107 11:51:34.313820 11160 solver.cpp:397]     Test net output #1: loss = 0.429242 (* 1 = 0.429242 loss)
I1107 11:51:34.395824 11160 solver.cpp:218] Iteration 62500 (9.40417 iter/s, 10.6336s/100 iters), loss = 0.202996
I1107 11:51:34.395824 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:51:34.395824 11160 solver.cpp:237]     Train net output #1: loss = 0.202996 (* 1 = 0.202996 loss)
I1107 11:51:34.395824 11160 sgd_solver.cpp:105] Iteration 62500, lr = 0.01
I1107 11:51:42.919803 11160 solver.cpp:218] Iteration 62600 (11.7321 iter/s, 8.52359s/100 iters), loss = 0.291388
I1107 11:51:42.919803 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:51:42.919803 11160 solver.cpp:237]     Train net output #1: loss = 0.291388 (* 1 = 0.291388 loss)
I1107 11:51:42.919803 11160 sgd_solver.cpp:105] Iteration 62600, lr = 0.01
I1107 11:51:51.450033 11160 solver.cpp:218] Iteration 62700 (11.7243 iter/s, 8.5293s/100 iters), loss = 0.177502
I1107 11:51:51.450033 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:51:51.450033 11160 solver.cpp:237]     Train net output #1: loss = 0.177502 (* 1 = 0.177502 loss)
I1107 11:51:51.450033 11160 sgd_solver.cpp:105] Iteration 62700, lr = 0.01
I1107 11:51:59.987284 11160 solver.cpp:218] Iteration 62800 (11.7143 iter/s, 8.53657s/100 iters), loss = 0.247778
I1107 11:51:59.987284 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:51:59.987284 11160 solver.cpp:237]     Train net output #1: loss = 0.247778 (* 1 = 0.247778 loss)
I1107 11:51:59.987284 11160 sgd_solver.cpp:105] Iteration 62800, lr = 0.01
I1107 11:52:08.523192 11160 solver.cpp:218] Iteration 62900 (11.7155 iter/s, 8.53572s/100 iters), loss = 0.142479
I1107 11:52:08.523192 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:52:08.523192 11160 solver.cpp:237]     Train net output #1: loss = 0.142479 (* 1 = 0.142479 loss)
I1107 11:52:08.523192 11160 sgd_solver.cpp:105] Iteration 62900, lr = 0.01
I1107 11:52:16.635897  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:52:16.972378 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_63000.caffemodel
I1107 11:52:17.002388 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_63000.solverstate
I1107 11:52:17.011386 11160 solver.cpp:330] Iteration 63000, Testing net (#0)
I1107 11:52:17.011386 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:52:18.999291 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:52:19.079290 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8497
I1107 11:52:19.079290 11160 solver.cpp:397]     Test net output #1: loss = 0.492859 (* 1 = 0.492859 loss)
I1107 11:52:19.159804 11160 solver.cpp:218] Iteration 63000 (9.40162 iter/s, 10.6365s/100 iters), loss = 0.150035
I1107 11:52:19.160805 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 11:52:19.160805 11160 solver.cpp:237]     Train net output #1: loss = 0.150035 (* 1 = 0.150035 loss)
I1107 11:52:19.160805 11160 sgd_solver.cpp:105] Iteration 63000, lr = 0.01
I1107 11:52:27.682099 11160 solver.cpp:218] Iteration 63100 (11.7354 iter/s, 8.52126s/100 iters), loss = 0.406373
I1107 11:52:27.682099 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:52:27.682099 11160 solver.cpp:237]     Train net output #1: loss = 0.406373 (* 1 = 0.406373 loss)
I1107 11:52:27.682099 11160 sgd_solver.cpp:105] Iteration 63100, lr = 0.01
I1107 11:52:36.216028 11160 solver.cpp:218] Iteration 63200 (11.7184 iter/s, 8.53358s/100 iters), loss = 0.183377
I1107 11:52:36.216028 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:52:36.216028 11160 solver.cpp:237]     Train net output #1: loss = 0.183378 (* 1 = 0.183378 loss)
I1107 11:52:36.216028 11160 sgd_solver.cpp:105] Iteration 63200, lr = 0.01
I1107 11:52:44.746762 11160 solver.cpp:218] Iteration 63300 (11.7239 iter/s, 8.52959s/100 iters), loss = 0.227535
I1107 11:52:44.746762 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:52:44.746762 11160 solver.cpp:237]     Train net output #1: loss = 0.227535 (* 1 = 0.227535 loss)
I1107 11:52:44.746762 11160 sgd_solver.cpp:105] Iteration 63300, lr = 0.01
I1107 11:52:53.273710 11160 solver.cpp:218] Iteration 63400 (11.7279 iter/s, 8.52668s/100 iters), loss = 0.209692
I1107 11:52:53.273710 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:52:53.273710 11160 solver.cpp:237]     Train net output #1: loss = 0.209692 (* 1 = 0.209692 loss)
I1107 11:52:53.273710 11160 sgd_solver.cpp:105] Iteration 63400, lr = 0.01
I1107 11:53:01.368528  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:53:01.706605 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_63500.caffemodel
I1107 11:53:01.736606 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_63500.solverstate
I1107 11:53:01.745605 11160 solver.cpp:330] Iteration 63500, Testing net (#0)
I1107 11:53:01.745605 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:53:03.735765 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:53:03.815771 11160 solver.cpp:397]     Test net output #0: accuracy = 0.837
I1107 11:53:03.815771 11160 solver.cpp:397]     Test net output #1: loss = 0.534966 (* 1 = 0.534966 loss)
I1107 11:53:03.898291 11160 solver.cpp:218] Iteration 63500 (9.41275 iter/s, 10.6239s/100 iters), loss = 0.228046
I1107 11:53:03.898291 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:53:03.898291 11160 solver.cpp:237]     Train net output #1: loss = 0.228046 (* 1 = 0.228046 loss)
I1107 11:53:03.898291 11160 sgd_solver.cpp:105] Iteration 63500, lr = 0.01
I1107 11:53:12.427991 11160 solver.cpp:218] Iteration 63600 (11.7247 iter/s, 8.52901s/100 iters), loss = 0.237801
I1107 11:53:12.427991 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:53:12.427991 11160 solver.cpp:237]     Train net output #1: loss = 0.237801 (* 1 = 0.237801 loss)
I1107 11:53:12.427991 11160 sgd_solver.cpp:105] Iteration 63600, lr = 0.01
I1107 11:53:20.961416 11160 solver.cpp:218] Iteration 63700 (11.7192 iter/s, 8.53298s/100 iters), loss = 0.189795
I1107 11:53:20.961416 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:53:20.961416 11160 solver.cpp:237]     Train net output #1: loss = 0.189795 (* 1 = 0.189795 loss)
I1107 11:53:20.961416 11160 sgd_solver.cpp:105] Iteration 63700, lr = 0.01
I1107 11:53:29.486724 11160 solver.cpp:218] Iteration 63800 (11.7299 iter/s, 8.52524s/100 iters), loss = 0.149134
I1107 11:53:29.486724 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 11:53:29.486724 11160 solver.cpp:237]     Train net output #1: loss = 0.149134 (* 1 = 0.149134 loss)
I1107 11:53:29.486724 11160 sgd_solver.cpp:105] Iteration 63800, lr = 0.01
I1107 11:53:38.015971 11160 solver.cpp:218] Iteration 63900 (11.7255 iter/s, 8.52839s/100 iters), loss = 0.206906
I1107 11:53:38.015971 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:53:38.015971 11160 solver.cpp:237]     Train net output #1: loss = 0.206906 (* 1 = 0.206906 loss)
I1107 11:53:38.015971 11160 sgd_solver.cpp:105] Iteration 63900, lr = 0.01
I1107 11:53:46.127063  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:53:46.464097 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_64000.caffemodel
I1107 11:53:46.494096 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_64000.solverstate
I1107 11:53:46.503096 11160 solver.cpp:330] Iteration 64000, Testing net (#0)
I1107 11:53:46.503096 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:53:48.493286 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:53:48.573307 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8619
I1107 11:53:48.573307 11160 solver.cpp:397]     Test net output #1: loss = 0.436004 (* 1 = 0.436004 loss)
I1107 11:53:48.654808 11160 solver.cpp:218] Iteration 64000 (9.40009 iter/s, 10.6382s/100 iters), loss = 0.210264
I1107 11:53:48.654808 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:53:48.654808 11160 solver.cpp:237]     Train net output #1: loss = 0.210264 (* 1 = 0.210264 loss)
I1107 11:53:48.654808 11160 sgd_solver.cpp:105] Iteration 64000, lr = 0.01
I1107 11:53:57.179932 11160 solver.cpp:218] Iteration 64100 (11.7307 iter/s, 8.52462s/100 iters), loss = 0.22438
I1107 11:53:57.179932 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:53:57.179932 11160 solver.cpp:237]     Train net output #1: loss = 0.22438 (* 1 = 0.22438 loss)
I1107 11:53:57.179932 11160 sgd_solver.cpp:105] Iteration 64100, lr = 0.01
I1107 11:54:05.703372 11160 solver.cpp:218] Iteration 64200 (11.7332 iter/s, 8.52283s/100 iters), loss = 0.267197
I1107 11:54:05.703372 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:54:05.703372 11160 solver.cpp:237]     Train net output #1: loss = 0.267197 (* 1 = 0.267197 loss)
I1107 11:54:05.703372 11160 sgd_solver.cpp:105] Iteration 64200, lr = 0.01
I1107 11:54:14.222494 11160 solver.cpp:218] Iteration 64300 (11.7385 iter/s, 8.51896s/100 iters), loss = 0.196703
I1107 11:54:14.222494 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:54:14.222494 11160 solver.cpp:237]     Train net output #1: loss = 0.196703 (* 1 = 0.196703 loss)
I1107 11:54:14.222494 11160 sgd_solver.cpp:105] Iteration 64300, lr = 0.01
I1107 11:54:22.752369 11160 solver.cpp:218] Iteration 64400 (11.7241 iter/s, 8.52941s/100 iters), loss = 0.149098
I1107 11:54:22.752369 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:54:22.752369 11160 solver.cpp:237]     Train net output #1: loss = 0.149098 (* 1 = 0.149098 loss)
I1107 11:54:22.752369 11160 sgd_solver.cpp:105] Iteration 64400, lr = 0.01
I1107 11:54:30.863348  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:54:31.201365 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_64500.caffemodel
I1107 11:54:31.231365 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_64500.solverstate
I1107 11:54:31.240370 11160 solver.cpp:330] Iteration 64500, Testing net (#0)
I1107 11:54:31.240370 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:54:33.229590 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:54:33.308624 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8514
I1107 11:54:33.308624 11160 solver.cpp:397]     Test net output #1: loss = 0.502083 (* 1 = 0.502083 loss)
I1107 11:54:33.389612 11160 solver.cpp:218] Iteration 64500 (9.4011 iter/s, 10.6371s/100 iters), loss = 0.248339
I1107 11:54:33.389612 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:54:33.389612 11160 solver.cpp:237]     Train net output #1: loss = 0.248339 (* 1 = 0.248339 loss)
I1107 11:54:33.389612 11160 sgd_solver.cpp:105] Iteration 64500, lr = 0.01
I1107 11:54:41.919656 11160 solver.cpp:218] Iteration 64600 (11.7248 iter/s, 8.52892s/100 iters), loss = 0.29663
I1107 11:54:41.919656 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:54:41.919656 11160 solver.cpp:237]     Train net output #1: loss = 0.29663 (* 1 = 0.29663 loss)
I1107 11:54:41.919656 11160 sgd_solver.cpp:105] Iteration 64600, lr = 0.01
I1107 11:54:50.447805 11160 solver.cpp:218] Iteration 64700 (11.7266 iter/s, 8.5276s/100 iters), loss = 0.189633
I1107 11:54:50.447805 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:54:50.447805 11160 solver.cpp:237]     Train net output #1: loss = 0.189633 (* 1 = 0.189633 loss)
I1107 11:54:50.447805 11160 sgd_solver.cpp:105] Iteration 64700, lr = 0.01
I1107 11:54:59.010571 11160 solver.cpp:218] Iteration 64800 (11.6782 iter/s, 8.56296s/100 iters), loss = 0.218847
I1107 11:54:59.010571 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:54:59.010571 11160 solver.cpp:237]     Train net output #1: loss = 0.218847 (* 1 = 0.218847 loss)
I1107 11:54:59.010571 11160 sgd_solver.cpp:105] Iteration 64800, lr = 0.01
I1107 11:55:07.521783 11160 solver.cpp:218] Iteration 64900 (11.751 iter/s, 8.5099s/100 iters), loss = 0.166881
I1107 11:55:07.521783 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:55:07.521783 11160 solver.cpp:237]     Train net output #1: loss = 0.166881 (* 1 = 0.166881 loss)
I1107 11:55:07.521783 11160 sgd_solver.cpp:105] Iteration 64900, lr = 0.01
I1107 11:55:15.610716  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:55:15.947738 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_65000.caffemodel
I1107 11:55:15.976738 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_65000.solverstate
I1107 11:55:15.986249 11160 solver.cpp:330] Iteration 65000, Testing net (#0)
I1107 11:55:15.986748 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:55:17.968921 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:55:18.048926 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8676
I1107 11:55:18.048926 11160 solver.cpp:397]     Test net output #1: loss = 0.425246 (* 1 = 0.425246 loss)
I1107 11:55:18.129932 11160 solver.cpp:218] Iteration 65000 (9.42653 iter/s, 10.6084s/100 iters), loss = 0.208973
I1107 11:55:18.129932 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:55:18.129932 11160 solver.cpp:237]     Train net output #1: loss = 0.208973 (* 1 = 0.208973 loss)
I1107 11:55:18.129932 11160 sgd_solver.cpp:105] Iteration 65000, lr = 0.01
I1107 11:55:26.623356 11160 solver.cpp:218] Iteration 65100 (11.7745 iter/s, 8.49293s/100 iters), loss = 0.2135
I1107 11:55:26.624367 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:55:26.624367 11160 solver.cpp:237]     Train net output #1: loss = 0.213501 (* 1 = 0.213501 loss)
I1107 11:55:26.624367 11160 sgd_solver.cpp:105] Iteration 65100, lr = 0.01
I1107 11:55:35.126353 11160 solver.cpp:218] Iteration 65200 (11.7628 iter/s, 8.50139s/100 iters), loss = 0.154165
I1107 11:55:35.126353 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 11:55:35.126353 11160 solver.cpp:237]     Train net output #1: loss = 0.154165 (* 1 = 0.154165 loss)
I1107 11:55:35.126353 11160 sgd_solver.cpp:105] Iteration 65200, lr = 0.01
I1107 11:55:43.629089 11160 solver.cpp:218] Iteration 65300 (11.7617 iter/s, 8.50215s/100 iters), loss = 0.22028
I1107 11:55:43.629089 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:55:43.629089 11160 solver.cpp:237]     Train net output #1: loss = 0.22028 (* 1 = 0.22028 loss)
I1107 11:55:43.629089 11160 sgd_solver.cpp:105] Iteration 65300, lr = 0.01
I1107 11:55:52.127219 11160 solver.cpp:218] Iteration 65400 (11.7679 iter/s, 8.49769s/100 iters), loss = 0.212775
I1107 11:55:52.127219 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:55:52.127219 11160 solver.cpp:237]     Train net output #1: loss = 0.212775 (* 1 = 0.212775 loss)
I1107 11:55:52.127219 11160 sgd_solver.cpp:105] Iteration 65400, lr = 0.01
I1107 11:56:00.212213  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:56:00.546277 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_65500.caffemodel
I1107 11:56:00.599779 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_65500.solverstate
I1107 11:56:00.608301 11160 solver.cpp:330] Iteration 65500, Testing net (#0)
I1107 11:56:00.608301 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:56:02.593456 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:56:02.672459 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8577
I1107 11:56:02.672459 11160 solver.cpp:397]     Test net output #1: loss = 0.460741 (* 1 = 0.460741 loss)
I1107 11:56:02.754465 11160 solver.cpp:218] Iteration 65500 (9.4106 iter/s, 10.6263s/100 iters), loss = 0.164917
I1107 11:56:02.754465 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:56:02.754465 11160 solver.cpp:237]     Train net output #1: loss = 0.164917 (* 1 = 0.164917 loss)
I1107 11:56:02.754465 11160 sgd_solver.cpp:105] Iteration 65500, lr = 0.01
I1107 11:56:11.251332 11160 solver.cpp:218] Iteration 65600 (11.7695 iter/s, 8.49653s/100 iters), loss = 0.227904
I1107 11:56:11.251332 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:56:11.251332 11160 solver.cpp:237]     Train net output #1: loss = 0.227904 (* 1 = 0.227904 loss)
I1107 11:56:11.251332 11160 sgd_solver.cpp:105] Iteration 65600, lr = 0.01
I1107 11:56:19.759272 11160 solver.cpp:218] Iteration 65700 (11.7532 iter/s, 8.50829s/100 iters), loss = 0.26202
I1107 11:56:19.760273 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:56:19.760273 11160 solver.cpp:237]     Train net output #1: loss = 0.262021 (* 1 = 0.262021 loss)
I1107 11:56:19.760273 11160 sgd_solver.cpp:105] Iteration 65700, lr = 0.01
I1107 11:56:28.262094 11160 solver.cpp:218] Iteration 65800 (11.7618 iter/s, 8.50211s/100 iters), loss = 0.200528
I1107 11:56:28.263094 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:56:28.263094 11160 solver.cpp:237]     Train net output #1: loss = 0.200528 (* 1 = 0.200528 loss)
I1107 11:56:28.263094 11160 sgd_solver.cpp:105] Iteration 65800, lr = 0.01
I1107 11:56:36.782080 11160 solver.cpp:218] Iteration 65900 (11.7381 iter/s, 8.51927s/100 iters), loss = 0.191827
I1107 11:56:36.782080 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 11:56:36.782080 11160 solver.cpp:237]     Train net output #1: loss = 0.191827 (* 1 = 0.191827 loss)
I1107 11:56:36.782080 11160 sgd_solver.cpp:105] Iteration 65900, lr = 0.01
I1107 11:56:44.882100  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:56:45.217140 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_66000.caffemodel
I1107 11:56:45.243140 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_66000.solverstate
I1107 11:56:45.252140 11160 solver.cpp:330] Iteration 66000, Testing net (#0)
I1107 11:56:45.252140 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:56:47.234333 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:56:47.314347 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8464
I1107 11:56:47.314347 11160 solver.cpp:397]     Test net output #1: loss = 0.501015 (* 1 = 0.501015 loss)
I1107 11:56:47.395840 11160 solver.cpp:218] Iteration 66000 (9.42273 iter/s, 10.6126s/100 iters), loss = 0.208275
I1107 11:56:47.395840 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:56:47.395840 11160 solver.cpp:237]     Train net output #1: loss = 0.208275 (* 1 = 0.208275 loss)
I1107 11:56:47.395840 11160 sgd_solver.cpp:105] Iteration 66000, lr = 0.01
I1107 11:56:55.914052 11160 solver.cpp:218] Iteration 66100 (11.7405 iter/s, 8.51755s/100 iters), loss = 0.291539
I1107 11:56:55.914052 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:56:55.914052 11160 solver.cpp:237]     Train net output #1: loss = 0.29154 (* 1 = 0.29154 loss)
I1107 11:56:55.914052 11160 sgd_solver.cpp:105] Iteration 66100, lr = 0.01
I1107 11:57:04.420261 11160 solver.cpp:218] Iteration 66200 (11.7563 iter/s, 8.50611s/100 iters), loss = 0.204286
I1107 11:57:04.420261 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:57:04.420261 11160 solver.cpp:237]     Train net output #1: loss = 0.204286 (* 1 = 0.204286 loss)
I1107 11:57:04.420261 11160 sgd_solver.cpp:105] Iteration 66200, lr = 0.01
I1107 11:57:12.926266 11160 solver.cpp:218] Iteration 66300 (11.7565 iter/s, 8.50592s/100 iters), loss = 0.221131
I1107 11:57:12.926266 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:57:12.926266 11160 solver.cpp:237]     Train net output #1: loss = 0.221131 (* 1 = 0.221131 loss)
I1107 11:57:12.926266 11160 sgd_solver.cpp:105] Iteration 66300, lr = 0.01
I1107 11:57:21.442543 11160 solver.cpp:218] Iteration 66400 (11.7436 iter/s, 8.51529s/100 iters), loss = 0.239779
I1107 11:57:21.442543 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:57:21.442543 11160 solver.cpp:237]     Train net output #1: loss = 0.239779 (* 1 = 0.239779 loss)
I1107 11:57:21.442543 11160 sgd_solver.cpp:105] Iteration 66400, lr = 0.01
I1107 11:57:29.526127  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:57:29.862159 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_66500.caffemodel
I1107 11:57:29.891158 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_66500.solverstate
I1107 11:57:29.900159 11160 solver.cpp:330] Iteration 66500, Testing net (#0)
I1107 11:57:29.900159 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:57:31.881482 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:57:31.961496 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8735
I1107 11:57:31.961496 11160 solver.cpp:397]     Test net output #1: loss = 0.41109 (* 1 = 0.41109 loss)
I1107 11:57:32.042495 11160 solver.cpp:218] Iteration 66500 (9.43435 iter/s, 10.5996s/100 iters), loss = 0.2373
I1107 11:57:32.042495 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:57:32.042495 11160 solver.cpp:237]     Train net output #1: loss = 0.2373 (* 1 = 0.2373 loss)
I1107 11:57:32.042495 11160 sgd_solver.cpp:105] Iteration 66500, lr = 0.01
I1107 11:57:40.541301 11160 solver.cpp:218] Iteration 66600 (11.7676 iter/s, 8.4979s/100 iters), loss = 0.276228
I1107 11:57:40.541301 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 11:57:40.541301 11160 solver.cpp:237]     Train net output #1: loss = 0.276228 (* 1 = 0.276228 loss)
I1107 11:57:40.541301 11160 sgd_solver.cpp:105] Iteration 66600, lr = 0.01
I1107 11:57:49.034775 11160 solver.cpp:218] Iteration 66700 (11.7742 iter/s, 8.49316s/100 iters), loss = 0.305678
I1107 11:57:49.034775 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1107 11:57:49.034775 11160 solver.cpp:237]     Train net output #1: loss = 0.305678 (* 1 = 0.305678 loss)
I1107 11:57:49.034775 11160 sgd_solver.cpp:105] Iteration 66700, lr = 0.01
I1107 11:57:57.529788 11160 solver.cpp:218] Iteration 66800 (11.7719 iter/s, 8.49481s/100 iters), loss = 0.152519
I1107 11:57:57.529788 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:57:57.529788 11160 solver.cpp:237]     Train net output #1: loss = 0.152519 (* 1 = 0.152519 loss)
I1107 11:57:57.529788 11160 sgd_solver.cpp:105] Iteration 66800, lr = 0.01
I1107 11:58:06.023555 11160 solver.cpp:218] Iteration 66900 (11.7752 iter/s, 8.49243s/100 iters), loss = 0.20934
I1107 11:58:06.023555 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:58:06.023555 11160 solver.cpp:237]     Train net output #1: loss = 0.20934 (* 1 = 0.20934 loss)
I1107 11:58:06.023555 11160 sgd_solver.cpp:105] Iteration 66900, lr = 0.01
I1107 11:58:14.102352  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:58:14.437404 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_67000.caffemodel
I1107 11:58:14.468400 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_67000.solverstate
I1107 11:58:14.477403 11160 solver.cpp:330] Iteration 67000, Testing net (#0)
I1107 11:58:14.477403 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:58:16.459519 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:58:16.538530 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8717
I1107 11:58:16.538530 11160 solver.cpp:397]     Test net output #1: loss = 0.420272 (* 1 = 0.420272 loss)
I1107 11:58:16.620033 11160 solver.cpp:218] Iteration 67000 (9.43752 iter/s, 10.596s/100 iters), loss = 0.213044
I1107 11:58:16.620033 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:58:16.620033 11160 solver.cpp:237]     Train net output #1: loss = 0.213044 (* 1 = 0.213044 loss)
I1107 11:58:16.620033 11160 sgd_solver.cpp:105] Iteration 67000, lr = 0.01
I1107 11:58:25.124935 11160 solver.cpp:218] Iteration 67100 (11.7581 iter/s, 8.50479s/100 iters), loss = 0.287583
I1107 11:58:25.125435 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:58:25.125435 11160 solver.cpp:237]     Train net output #1: loss = 0.287583 (* 1 = 0.287583 loss)
I1107 11:58:25.125435 11160 sgd_solver.cpp:105] Iteration 67100, lr = 0.01
I1107 11:58:33.641616 11160 solver.cpp:218] Iteration 67200 (11.7423 iter/s, 8.51624s/100 iters), loss = 0.205951
I1107 11:58:33.641616 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:58:33.641616 11160 solver.cpp:237]     Train net output #1: loss = 0.205951 (* 1 = 0.205951 loss)
I1107 11:58:33.641616 11160 sgd_solver.cpp:105] Iteration 67200, lr = 0.01
I1107 11:58:42.143412 11160 solver.cpp:218] Iteration 67300 (11.7627 iter/s, 8.50148s/100 iters), loss = 0.288225
I1107 11:58:42.143412 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:58:42.143412 11160 solver.cpp:237]     Train net output #1: loss = 0.288225 (* 1 = 0.288225 loss)
I1107 11:58:42.143412 11160 sgd_solver.cpp:105] Iteration 67300, lr = 0.01
I1107 11:58:50.651767 11160 solver.cpp:218] Iteration 67400 (11.7546 iter/s, 8.5073s/100 iters), loss = 0.223868
I1107 11:58:50.651767 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 11:58:50.651767 11160 solver.cpp:237]     Train net output #1: loss = 0.223868 (* 1 = 0.223868 loss)
I1107 11:58:50.651767 11160 sgd_solver.cpp:105] Iteration 67400, lr = 0.01
I1107 11:58:58.744005  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:58:59.080682 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_67500.caffemodel
I1107 11:58:59.109680 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_67500.solverstate
I1107 11:58:59.118690 11160 solver.cpp:330] Iteration 67500, Testing net (#0)
I1107 11:58:59.119189 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:59:01.103819 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:59:01.183413 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8476
I1107 11:59:01.184413 11160 solver.cpp:397]     Test net output #1: loss = 0.510815 (* 1 = 0.510815 loss)
I1107 11:59:01.265969 11160 solver.cpp:218] Iteration 67500 (9.4219 iter/s, 10.6136s/100 iters), loss = 0.167291
I1107 11:59:01.265969 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 11:59:01.265969 11160 solver.cpp:237]     Train net output #1: loss = 0.167291 (* 1 = 0.167291 loss)
I1107 11:59:01.265969 11160 sgd_solver.cpp:105] Iteration 67500, lr = 0.01
I1107 11:59:09.762696 11160 solver.cpp:218] Iteration 67600 (11.7695 iter/s, 8.49653s/100 iters), loss = 0.249843
I1107 11:59:09.762696 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:59:09.762696 11160 solver.cpp:237]     Train net output #1: loss = 0.249843 (* 1 = 0.249843 loss)
I1107 11:59:09.762696 11160 sgd_solver.cpp:105] Iteration 67600, lr = 0.01
I1107 11:59:18.259428 11160 solver.cpp:218] Iteration 67700 (11.7694 iter/s, 8.49661s/100 iters), loss = 0.261267
I1107 11:59:18.259428 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 11:59:18.259428 11160 solver.cpp:237]     Train net output #1: loss = 0.261267 (* 1 = 0.261267 loss)
I1107 11:59:18.259428 11160 sgd_solver.cpp:105] Iteration 67700, lr = 0.01
I1107 11:59:26.761318 11160 solver.cpp:218] Iteration 67800 (11.7632 iter/s, 8.50111s/100 iters), loss = 0.237412
I1107 11:59:26.761318 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:59:26.761318 11160 solver.cpp:237]     Train net output #1: loss = 0.237413 (* 1 = 0.237413 loss)
I1107 11:59:26.761318 11160 sgd_solver.cpp:105] Iteration 67800, lr = 0.01
I1107 11:59:35.263842 11160 solver.cpp:218] Iteration 67900 (11.7626 iter/s, 8.5015s/100 iters), loss = 0.12278
I1107 11:59:35.263842 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 11:59:35.263842 11160 solver.cpp:237]     Train net output #1: loss = 0.12278 (* 1 = 0.12278 loss)
I1107 11:59:35.263842 11160 sgd_solver.cpp:105] Iteration 67900, lr = 0.01
I1107 11:59:43.343875  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:59:43.680899 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_68000.caffemodel
I1107 11:59:43.709403 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_68000.solverstate
I1107 11:59:43.718902 11160 solver.cpp:330] Iteration 68000, Testing net (#0)
I1107 11:59:43.718902 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 11:59:45.701328 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 11:59:45.780369 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8637
I1107 11:59:45.780369 11160 solver.cpp:397]     Test net output #1: loss = 0.443656 (* 1 = 0.443656 loss)
I1107 11:59:45.861374 11160 solver.cpp:218] Iteration 68000 (9.43614 iter/s, 10.5976s/100 iters), loss = 0.171357
I1107 11:59:45.861374 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 11:59:45.861374 11160 solver.cpp:237]     Train net output #1: loss = 0.171357 (* 1 = 0.171357 loss)
I1107 11:59:45.861374 11160 sgd_solver.cpp:105] Iteration 68000, lr = 0.01
I1107 11:59:54.369366 11160 solver.cpp:218] Iteration 68100 (11.7547 iter/s, 8.50723s/100 iters), loss = 0.260551
I1107 11:59:54.369366 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 11:59:54.369366 11160 solver.cpp:237]     Train net output #1: loss = 0.260551 (* 1 = 0.260551 loss)
I1107 11:59:54.369366 11160 sgd_solver.cpp:105] Iteration 68100, lr = 0.01
I1107 12:00:02.879915 11160 solver.cpp:218] Iteration 68200 (11.7511 iter/s, 8.50985s/100 iters), loss = 0.20724
I1107 12:00:02.879915 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:00:02.879915 11160 solver.cpp:237]     Train net output #1: loss = 0.207241 (* 1 = 0.207241 loss)
I1107 12:00:02.879915 11160 sgd_solver.cpp:105] Iteration 68200, lr = 0.01
I1107 12:00:11.385818 11160 solver.cpp:218] Iteration 68300 (11.7571 iter/s, 8.5055s/100 iters), loss = 0.165942
I1107 12:00:11.385818 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:00:11.385818 11160 solver.cpp:237]     Train net output #1: loss = 0.165942 (* 1 = 0.165942 loss)
I1107 12:00:11.385818 11160 sgd_solver.cpp:105] Iteration 68300, lr = 0.01
I1107 12:00:19.885769 11160 solver.cpp:218] Iteration 68400 (11.7656 iter/s, 8.49935s/100 iters), loss = 0.290921
I1107 12:00:19.885769 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:00:19.885769 11160 solver.cpp:237]     Train net output #1: loss = 0.290921 (* 1 = 0.290921 loss)
I1107 12:00:19.885769 11160 sgd_solver.cpp:105] Iteration 68400, lr = 0.01
I1107 12:00:27.966356  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:00:28.304414 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_68500.caffemodel
I1107 12:00:28.333425 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_68500.solverstate
I1107 12:00:28.343432 11160 solver.cpp:330] Iteration 68500, Testing net (#0)
I1107 12:00:28.343432 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:00:30.326720 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:00:30.406222 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8672
I1107 12:00:30.406222 11160 solver.cpp:397]     Test net output #1: loss = 0.427635 (* 1 = 0.427635 loss)
I1107 12:00:30.486723 11160 solver.cpp:218] Iteration 68500 (9.43366 iter/s, 10.6003s/100 iters), loss = 0.144667
I1107 12:00:30.486723 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:00:30.486723 11160 solver.cpp:237]     Train net output #1: loss = 0.144667 (* 1 = 0.144667 loss)
I1107 12:00:30.486723 11160 sgd_solver.cpp:105] Iteration 68500, lr = 0.01
I1107 12:00:38.988862 11160 solver.cpp:218] Iteration 68600 (11.7626 iter/s, 8.50154s/100 iters), loss = 0.245703
I1107 12:00:38.988862 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:00:38.988862 11160 solver.cpp:237]     Train net output #1: loss = 0.245703 (* 1 = 0.245703 loss)
I1107 12:00:38.988862 11160 sgd_solver.cpp:105] Iteration 68600, lr = 0.01
I1107 12:00:47.484462 11160 solver.cpp:218] Iteration 68700 (11.7713 iter/s, 8.49524s/100 iters), loss = 0.241665
I1107 12:00:47.484462 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:00:47.484462 11160 solver.cpp:237]     Train net output #1: loss = 0.241665 (* 1 = 0.241665 loss)
I1107 12:00:47.484462 11160 sgd_solver.cpp:105] Iteration 68700, lr = 0.01
I1107 12:00:55.991298 11160 solver.cpp:218] Iteration 68800 (11.756 iter/s, 8.50629s/100 iters), loss = 0.239408
I1107 12:00:55.991298 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:00:55.991298 11160 solver.cpp:237]     Train net output #1: loss = 0.239408 (* 1 = 0.239408 loss)
I1107 12:00:55.991298 11160 sgd_solver.cpp:105] Iteration 68800, lr = 0.01
I1107 12:01:04.490098 11160 solver.cpp:218] Iteration 68900 (11.767 iter/s, 8.49838s/100 iters), loss = 0.155494
I1107 12:01:04.490098 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:01:04.490098 11160 solver.cpp:237]     Train net output #1: loss = 0.155494 (* 1 = 0.155494 loss)
I1107 12:01:04.490098 11160 sgd_solver.cpp:105] Iteration 68900, lr = 0.01
I1107 12:01:12.572144  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:01:12.908175 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_69000.caffemodel
I1107 12:01:12.938184 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_69000.solverstate
I1107 12:01:12.946184 11160 solver.cpp:330] Iteration 69000, Testing net (#0)
I1107 12:01:12.947185 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:01:14.929363 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:01:15.009361 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8636
I1107 12:01:15.009361 11160 solver.cpp:397]     Test net output #1: loss = 0.46178 (* 1 = 0.46178 loss)
I1107 12:01:15.091369 11160 solver.cpp:218] Iteration 69000 (9.43373 iter/s, 10.6003s/100 iters), loss = 0.266981
I1107 12:01:15.091369 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:01:15.091369 11160 solver.cpp:237]     Train net output #1: loss = 0.266981 (* 1 = 0.266981 loss)
I1107 12:01:15.091369 11160 sgd_solver.cpp:105] Iteration 69000, lr = 0.01
I1107 12:01:23.610410 11160 solver.cpp:218] Iteration 69100 (11.7384 iter/s, 8.51904s/100 iters), loss = 0.243973
I1107 12:01:23.610410 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:01:23.610410 11160 solver.cpp:237]     Train net output #1: loss = 0.243973 (* 1 = 0.243973 loss)
I1107 12:01:23.610410 11160 sgd_solver.cpp:105] Iteration 69100, lr = 0.01
I1107 12:01:32.129452 11160 solver.cpp:218] Iteration 69200 (11.739 iter/s, 8.51859s/100 iters), loss = 0.245846
I1107 12:01:32.129452 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:01:32.129452 11160 solver.cpp:237]     Train net output #1: loss = 0.245846 (* 1 = 0.245846 loss)
I1107 12:01:32.129452 11160 sgd_solver.cpp:105] Iteration 69200, lr = 0.01
I1107 12:01:40.874146 11160 solver.cpp:218] Iteration 69300 (11.437 iter/s, 8.74356s/100 iters), loss = 0.240848
I1107 12:01:40.874146 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:01:40.874146 11160 solver.cpp:237]     Train net output #1: loss = 0.240848 (* 1 = 0.240848 loss)
I1107 12:01:40.874146 11160 sgd_solver.cpp:105] Iteration 69300, lr = 0.01
I1107 12:01:49.420095 11160 solver.cpp:218] Iteration 69400 (11.7015 iter/s, 8.5459s/100 iters), loss = 0.203899
I1107 12:01:49.420095 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:01:49.420095 11160 solver.cpp:237]     Train net output #1: loss = 0.203899 (* 1 = 0.203899 loss)
I1107 12:01:49.420095 11160 sgd_solver.cpp:105] Iteration 69400, lr = 0.01
I1107 12:01:57.584554  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:01:57.925590 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_69500.caffemodel
I1107 12:01:57.955588 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_69500.solverstate
I1107 12:01:57.964588 11160 solver.cpp:330] Iteration 69500, Testing net (#0)
I1107 12:01:57.964588 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:01:59.976070 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:02:00.057077 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8599
I1107 12:02:00.057077 11160 solver.cpp:397]     Test net output #1: loss = 0.456072 (* 1 = 0.456072 loss)
I1107 12:02:00.139096 11160 solver.cpp:218] Iteration 69500 (9.32957 iter/s, 10.7186s/100 iters), loss = 0.137987
I1107 12:02:00.139096 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:02:00.139096 11160 solver.cpp:237]     Train net output #1: loss = 0.137987 (* 1 = 0.137987 loss)
I1107 12:02:00.139096 11160 sgd_solver.cpp:105] Iteration 69500, lr = 0.01
I1107 12:02:08.744436 11160 solver.cpp:218] Iteration 69600 (11.622 iter/s, 8.60441s/100 iters), loss = 0.214616
I1107 12:02:08.744436 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:02:08.744436 11160 solver.cpp:237]     Train net output #1: loss = 0.214616 (* 1 = 0.214616 loss)
I1107 12:02:08.744436 11160 sgd_solver.cpp:105] Iteration 69600, lr = 0.01
I1107 12:02:17.307054 11160 solver.cpp:218] Iteration 69700 (11.6793 iter/s, 8.56219s/100 iters), loss = 0.217437
I1107 12:02:17.307054 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:02:17.307054 11160 solver.cpp:237]     Train net output #1: loss = 0.217437 (* 1 = 0.217437 loss)
I1107 12:02:17.307054 11160 sgd_solver.cpp:105] Iteration 69700, lr = 0.01
I1107 12:02:25.860967 11160 solver.cpp:218] Iteration 69800 (11.6912 iter/s, 8.55342s/100 iters), loss = 0.214555
I1107 12:02:25.860967 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:02:25.860967 11160 solver.cpp:237]     Train net output #1: loss = 0.214556 (* 1 = 0.214556 loss)
I1107 12:02:25.860967 11160 sgd_solver.cpp:105] Iteration 69800, lr = 0.01
I1107 12:02:34.436084 11160 solver.cpp:218] Iteration 69900 (11.6632 iter/s, 8.57399s/100 iters), loss = 0.179104
I1107 12:02:34.436084 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:02:34.436084 11160 solver.cpp:237]     Train net output #1: loss = 0.179104 (* 1 = 0.179104 loss)
I1107 12:02:34.436084 11160 sgd_solver.cpp:105] Iteration 69900, lr = 0.01
I1107 12:02:42.592619  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:02:42.930655 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_70000.caffemodel
I1107 12:02:42.960655 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_70000.solverstate
I1107 12:02:42.969655 11160 solver.cpp:330] Iteration 70000, Testing net (#0)
I1107 12:02:42.969655 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:02:44.965814 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:02:45.045819 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8568
I1107 12:02:45.045819 11160 solver.cpp:397]     Test net output #1: loss = 0.461713 (* 1 = 0.461713 loss)
I1107 12:02:45.126823 11160 solver.cpp:218] Iteration 70000 (9.35432 iter/s, 10.6903s/100 iters), loss = 0.283043
I1107 12:02:45.126823 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:02:45.126823 11160 solver.cpp:237]     Train net output #1: loss = 0.283043 (* 1 = 0.283043 loss)
I1107 12:02:45.126823 11160 sgd_solver.cpp:105] Iteration 70000, lr = 0.01
I1107 12:02:53.697486 11160 solver.cpp:218] Iteration 70100 (11.6686 iter/s, 8.57003s/100 iters), loss = 0.205936
I1107 12:02:53.697486 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:02:53.697486 11160 solver.cpp:237]     Train net output #1: loss = 0.205937 (* 1 = 0.205937 loss)
I1107 12:02:53.697486 11160 sgd_solver.cpp:105] Iteration 70100, lr = 0.01
I1107 12:03:02.272223 11160 solver.cpp:218] Iteration 70200 (11.6619 iter/s, 8.57495s/100 iters), loss = 0.234405
I1107 12:03:02.272223 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:03:02.272223 11160 solver.cpp:237]     Train net output #1: loss = 0.234405 (* 1 = 0.234405 loss)
I1107 12:03:02.272223 11160 sgd_solver.cpp:105] Iteration 70200, lr = 0.01
I1107 12:03:10.843694 11160 solver.cpp:218] Iteration 70300 (11.6681 iter/s, 8.5704s/100 iters), loss = 0.181009
I1107 12:03:10.843694 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:03:10.843694 11160 solver.cpp:237]     Train net output #1: loss = 0.181009 (* 1 = 0.181009 loss)
I1107 12:03:10.843694 11160 sgd_solver.cpp:105] Iteration 70300, lr = 0.01
I1107 12:03:19.458034 11160 solver.cpp:218] Iteration 70400 (11.6093 iter/s, 8.61377s/100 iters), loss = 0.229583
I1107 12:03:19.458034 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:03:19.458034 11160 solver.cpp:237]     Train net output #1: loss = 0.229584 (* 1 = 0.229584 loss)
I1107 12:03:19.458034 11160 sgd_solver.cpp:105] Iteration 70400, lr = 0.01
I1107 12:03:27.594863  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:03:27.932888 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_70500.caffemodel
I1107 12:03:27.962888 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_70500.solverstate
I1107 12:03:27.970887 11160 solver.cpp:330] Iteration 70500, Testing net (#0)
I1107 12:03:27.971889 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:03:29.978094 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:03:30.057108 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8468
I1107 12:03:30.058109 11160 solver.cpp:397]     Test net output #1: loss = 0.50148 (* 1 = 0.50148 loss)
I1107 12:03:30.139114 11160 solver.cpp:218] Iteration 70500 (9.36292 iter/s, 10.6804s/100 iters), loss = 0.175363
I1107 12:03:30.139114 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:03:30.139114 11160 solver.cpp:237]     Train net output #1: loss = 0.175363 (* 1 = 0.175363 loss)
I1107 12:03:30.139114 11160 sgd_solver.cpp:105] Iteration 70500, lr = 0.01
I1107 12:03:38.713834 11160 solver.cpp:218] Iteration 70600 (11.6618 iter/s, 8.57504s/100 iters), loss = 0.24034
I1107 12:03:38.714833 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:03:38.714833 11160 solver.cpp:237]     Train net output #1: loss = 0.24034 (* 1 = 0.24034 loss)
I1107 12:03:38.714833 11160 sgd_solver.cpp:105] Iteration 70600, lr = 0.01
I1107 12:03:47.299466 11160 solver.cpp:218] Iteration 70700 (11.6487 iter/s, 8.58464s/100 iters), loss = 0.17832
I1107 12:03:47.299466 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:03:47.299466 11160 solver.cpp:237]     Train net output #1: loss = 0.17832 (* 1 = 0.17832 loss)
I1107 12:03:47.299466 11160 sgd_solver.cpp:105] Iteration 70700, lr = 0.01
I1107 12:03:55.899729 11160 solver.cpp:218] Iteration 70800 (11.6289 iter/s, 8.59926s/100 iters), loss = 0.217742
I1107 12:03:55.899729 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:03:55.899729 11160 solver.cpp:237]     Train net output #1: loss = 0.217742 (* 1 = 0.217742 loss)
I1107 12:03:55.899729 11160 sgd_solver.cpp:105] Iteration 70800, lr = 0.01
I1107 12:04:04.471436 11160 solver.cpp:218] Iteration 70900 (11.6668 iter/s, 8.57129s/100 iters), loss = 0.162188
I1107 12:04:04.471436 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:04:04.471436 11160 solver.cpp:237]     Train net output #1: loss = 0.162188 (* 1 = 0.162188 loss)
I1107 12:04:04.471436 11160 sgd_solver.cpp:105] Iteration 70900, lr = 0.01
I1107 12:04:12.622509  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:04:12.961558 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_71000.caffemodel
I1107 12:04:12.998296 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_71000.solverstate
I1107 12:04:13.008296 11160 solver.cpp:330] Iteration 71000, Testing net (#0)
I1107 12:04:13.008296 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:04:15.003968 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:04:15.083971 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8656
I1107 12:04:15.083971 11160 solver.cpp:397]     Test net output #1: loss = 0.443195 (* 1 = 0.443195 loss)
I1107 12:04:15.165977 11160 solver.cpp:218] Iteration 71000 (9.3512 iter/s, 10.6938s/100 iters), loss = 0.205144
I1107 12:04:15.165977 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:04:15.165977 11160 solver.cpp:237]     Train net output #1: loss = 0.205144 (* 1 = 0.205144 loss)
I1107 12:04:15.165977 11160 sgd_solver.cpp:105] Iteration 71000, lr = 0.01
I1107 12:04:23.762435 11160 solver.cpp:218] Iteration 71100 (11.6326 iter/s, 8.59653s/100 iters), loss = 0.244643
I1107 12:04:23.762435 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:04:23.762435 11160 solver.cpp:237]     Train net output #1: loss = 0.244643 (* 1 = 0.244643 loss)
I1107 12:04:23.762435 11160 sgd_solver.cpp:105] Iteration 71100, lr = 0.01
I1107 12:04:32.319900 11160 solver.cpp:218] Iteration 71200 (11.6871 iter/s, 8.55641s/100 iters), loss = 0.162019
I1107 12:04:32.319900 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:04:32.319900 11160 solver.cpp:237]     Train net output #1: loss = 0.162019 (* 1 = 0.162019 loss)
I1107 12:04:32.319900 11160 sgd_solver.cpp:105] Iteration 71200, lr = 0.01
I1107 12:04:41.074656 11160 solver.cpp:218] Iteration 71300 (11.4228 iter/s, 8.7544s/100 iters), loss = 0.195432
I1107 12:04:41.074656 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:04:41.074656 11160 solver.cpp:237]     Train net output #1: loss = 0.195432 (* 1 = 0.195432 loss)
I1107 12:04:41.074656 11160 sgd_solver.cpp:105] Iteration 71300, lr = 0.01
I1107 12:04:49.661811 11160 solver.cpp:218] Iteration 71400 (11.6454 iter/s, 8.58708s/100 iters), loss = 0.161241
I1107 12:04:49.661811 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:04:49.661811 11160 solver.cpp:237]     Train net output #1: loss = 0.161241 (* 1 = 0.161241 loss)
I1107 12:04:49.661811 11160 sgd_solver.cpp:105] Iteration 71400, lr = 0.01
I1107 12:04:57.824054  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:04:58.163393 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_71500.caffemodel
I1107 12:04:58.200393 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_71500.solverstate
I1107 12:04:58.210393 11160 solver.cpp:330] Iteration 71500, Testing net (#0)
I1107 12:04:58.210393 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:05:00.204845 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:05:00.284610 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8128
I1107 12:05:00.284610 11160 solver.cpp:397]     Test net output #1: loss = 0.613863 (* 1 = 0.613863 loss)
I1107 12:05:00.365792 11160 solver.cpp:218] Iteration 71500 (9.34272 iter/s, 10.7035s/100 iters), loss = 0.14824
I1107 12:05:00.365792 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:05:00.365792 11160 solver.cpp:237]     Train net output #1: loss = 0.14824 (* 1 = 0.14824 loss)
I1107 12:05:00.365792 11160 sgd_solver.cpp:105] Iteration 71500, lr = 0.01
I1107 12:05:08.913709 11160 solver.cpp:218] Iteration 71600 (11.7 iter/s, 8.547s/100 iters), loss = 0.257317
I1107 12:05:08.913709 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:05:08.913709 11160 solver.cpp:237]     Train net output #1: loss = 0.257317 (* 1 = 0.257317 loss)
I1107 12:05:08.913709 11160 sgd_solver.cpp:105] Iteration 71600, lr = 0.01
I1107 12:05:17.527158 11160 solver.cpp:218] Iteration 71700 (11.6103 iter/s, 8.61306s/100 iters), loss = 0.243785
I1107 12:05:17.527158 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:05:17.527158 11160 solver.cpp:237]     Train net output #1: loss = 0.243785 (* 1 = 0.243785 loss)
I1107 12:05:17.527158 11160 sgd_solver.cpp:105] Iteration 71700, lr = 0.01
I1107 12:05:26.131988 11160 solver.cpp:218] Iteration 71800 (11.6218 iter/s, 8.60449s/100 iters), loss = 0.163474
I1107 12:05:26.131988 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:05:26.131988 11160 solver.cpp:237]     Train net output #1: loss = 0.163474 (* 1 = 0.163474 loss)
I1107 12:05:26.131988 11160 sgd_solver.cpp:105] Iteration 71800, lr = 0.01
I1107 12:05:34.665486 11160 solver.cpp:218] Iteration 71900 (11.7203 iter/s, 8.5322s/100 iters), loss = 0.139857
I1107 12:05:34.665486 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:05:34.665486 11160 solver.cpp:237]     Train net output #1: loss = 0.139857 (* 1 = 0.139857 loss)
I1107 12:05:34.665486 11160 sgd_solver.cpp:105] Iteration 71900, lr = 0.01
I1107 12:05:42.793733  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:05:43.130786 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_72000.caffemodel
I1107 12:05:43.158787 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_72000.solverstate
I1107 12:05:43.166786 11160 solver.cpp:330] Iteration 72000, Testing net (#0)
I1107 12:05:43.166786 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:05:45.162993 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:05:45.242035 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8185
I1107 12:05:45.242035 11160 solver.cpp:397]     Test net output #1: loss = 0.583619 (* 1 = 0.583619 loss)
I1107 12:05:45.324041 11160 solver.cpp:218] Iteration 72000 (9.38248 iter/s, 10.6582s/100 iters), loss = 0.252324
I1107 12:05:45.324041 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:05:45.324041 11160 solver.cpp:237]     Train net output #1: loss = 0.252324 (* 1 = 0.252324 loss)
I1107 12:05:45.324041 11160 sgd_solver.cpp:105] Iteration 72000, lr = 0.01
I1107 12:05:53.871213 11160 solver.cpp:218] Iteration 72100 (11.7005 iter/s, 8.54665s/100 iters), loss = 0.233669
I1107 12:05:53.871213 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:05:53.871213 11160 solver.cpp:237]     Train net output #1: loss = 0.23367 (* 1 = 0.23367 loss)
I1107 12:05:53.871213 11160 sgd_solver.cpp:105] Iteration 72100, lr = 0.01
I1107 12:06:02.416882 11160 solver.cpp:218] Iteration 72200 (11.7021 iter/s, 8.54545s/100 iters), loss = 0.127147
I1107 12:06:02.416882 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:06:02.416882 11160 solver.cpp:237]     Train net output #1: loss = 0.127147 (* 1 = 0.127147 loss)
I1107 12:06:02.416882 11160 sgd_solver.cpp:105] Iteration 72200, lr = 0.01
I1107 12:06:11.070804 11160 solver.cpp:218] Iteration 72300 (11.5565 iter/s, 8.65314s/100 iters), loss = 0.171024
I1107 12:06:11.070804 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:06:11.070804 11160 solver.cpp:237]     Train net output #1: loss = 0.171024 (* 1 = 0.171024 loss)
I1107 12:06:11.070804 11160 sgd_solver.cpp:105] Iteration 72300, lr = 0.01
I1107 12:06:19.690933 11160 solver.cpp:218] Iteration 72400 (11.6006 iter/s, 8.62027s/100 iters), loss = 0.106383
I1107 12:06:19.690933 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:06:19.690933 11160 solver.cpp:237]     Train net output #1: loss = 0.106383 (* 1 = 0.106383 loss)
I1107 12:06:19.690933 11160 sgd_solver.cpp:105] Iteration 72400, lr = 0.01
I1107 12:06:27.835300  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:06:28.175355 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_72500.caffemodel
I1107 12:06:28.206358 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_72500.solverstate
I1107 12:06:28.215358 11160 solver.cpp:330] Iteration 72500, Testing net (#0)
I1107 12:06:28.215358 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:06:30.226508 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:06:30.306514 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8499
I1107 12:06:30.306514 11160 solver.cpp:397]     Test net output #1: loss = 0.492457 (* 1 = 0.492457 loss)
I1107 12:06:30.388514 11160 solver.cpp:218] Iteration 72500 (9.34908 iter/s, 10.6962s/100 iters), loss = 0.183804
I1107 12:06:30.388514 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:06:30.388514 11160 solver.cpp:237]     Train net output #1: loss = 0.183804 (* 1 = 0.183804 loss)
I1107 12:06:30.388514 11160 sgd_solver.cpp:105] Iteration 72500, lr = 0.01
I1107 12:06:38.956284 11160 solver.cpp:218] Iteration 72600 (11.6719 iter/s, 8.56756s/100 iters), loss = 0.186725
I1107 12:06:38.956284 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:06:38.956284 11160 solver.cpp:237]     Train net output #1: loss = 0.186725 (* 1 = 0.186725 loss)
I1107 12:06:38.956284 11160 sgd_solver.cpp:105] Iteration 72600, lr = 0.01
I1107 12:06:47.525866 11160 solver.cpp:218] Iteration 72700 (11.6695 iter/s, 8.56938s/100 iters), loss = 0.194198
I1107 12:06:47.525866 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:06:47.525866 11160 solver.cpp:237]     Train net output #1: loss = 0.194198 (* 1 = 0.194198 loss)
I1107 12:06:47.525866 11160 sgd_solver.cpp:105] Iteration 72700, lr = 0.01
I1107 12:06:56.085821 11160 solver.cpp:218] Iteration 72800 (11.6836 iter/s, 8.55904s/100 iters), loss = 0.193768
I1107 12:06:56.086321 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:06:56.086321 11160 solver.cpp:237]     Train net output #1: loss = 0.193768 (* 1 = 0.193768 loss)
I1107 12:06:56.086321 11160 sgd_solver.cpp:105] Iteration 72800, lr = 0.01
I1107 12:07:04.626132 11160 solver.cpp:218] Iteration 72900 (11.7099 iter/s, 8.53978s/100 iters), loss = 0.153846
I1107 12:07:04.626132 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:07:04.626132 11160 solver.cpp:237]     Train net output #1: loss = 0.153846 (* 1 = 0.153846 loss)
I1107 12:07:04.626132 11160 sgd_solver.cpp:105] Iteration 72900, lr = 0.01
I1107 12:07:12.762470  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:07:13.101516 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_73000.caffemodel
I1107 12:07:13.130507 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_73000.solverstate
I1107 12:07:13.139509 11160 solver.cpp:330] Iteration 73000, Testing net (#0)
I1107 12:07:13.139509 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:07:15.131680 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:07:15.210685 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8712
I1107 12:07:15.210685 11160 solver.cpp:397]     Test net output #1: loss = 0.411724 (* 1 = 0.411724 loss)
I1107 12:07:15.291687 11160 solver.cpp:218] Iteration 73000 (9.37648 iter/s, 10.665s/100 iters), loss = 0.222379
I1107 12:07:15.292186 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:07:15.292186 11160 solver.cpp:237]     Train net output #1: loss = 0.222379 (* 1 = 0.222379 loss)
I1107 12:07:15.292186 11160 sgd_solver.cpp:105] Iteration 73000, lr = 0.01
I1107 12:07:23.850208 11160 solver.cpp:218] Iteration 73100 (11.685 iter/s, 8.55796s/100 iters), loss = 0.286945
I1107 12:07:23.850208 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:07:23.850208 11160 solver.cpp:237]     Train net output #1: loss = 0.286945 (* 1 = 0.286945 loss)
I1107 12:07:23.850208 11160 sgd_solver.cpp:105] Iteration 73100, lr = 0.01
I1107 12:07:32.462781 11160 solver.cpp:218] Iteration 73200 (11.6111 iter/s, 8.61247s/100 iters), loss = 0.219097
I1107 12:07:32.462781 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:07:32.462781 11160 solver.cpp:237]     Train net output #1: loss = 0.219097 (* 1 = 0.219097 loss)
I1107 12:07:32.462781 11160 sgd_solver.cpp:105] Iteration 73200, lr = 0.01
I1107 12:07:41.064285 11160 solver.cpp:218] Iteration 73300 (11.6268 iter/s, 8.60079s/100 iters), loss = 0.162207
I1107 12:07:41.064285 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:07:41.064285 11160 solver.cpp:237]     Train net output #1: loss = 0.162207 (* 1 = 0.162207 loss)
I1107 12:07:41.064285 11160 sgd_solver.cpp:105] Iteration 73300, lr = 0.01
I1107 12:07:49.613456 11160 solver.cpp:218] Iteration 73400 (11.6974 iter/s, 8.54888s/100 iters), loss = 0.199615
I1107 12:07:49.613456 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:07:49.613456 11160 solver.cpp:237]     Train net output #1: loss = 0.199615 (* 1 = 0.199615 loss)
I1107 12:07:49.613456 11160 sgd_solver.cpp:105] Iteration 73400, lr = 0.01
I1107 12:07:57.781431  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:07:58.120494 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_73500.caffemodel
I1107 12:07:58.150494 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_73500.solverstate
I1107 12:07:58.159494 11160 solver.cpp:330] Iteration 73500, Testing net (#0)
I1107 12:07:58.159494 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:08:00.152624 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:08:00.232631 11160 solver.cpp:397]     Test net output #0: accuracy = 0.859
I1107 12:08:00.232631 11160 solver.cpp:397]     Test net output #1: loss = 0.479538 (* 1 = 0.479538 loss)
I1107 12:08:00.313645 11160 solver.cpp:218] Iteration 73500 (9.34617 iter/s, 10.6996s/100 iters), loss = 0.166538
I1107 12:08:00.314641 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:08:00.314641 11160 solver.cpp:237]     Train net output #1: loss = 0.166538 (* 1 = 0.166538 loss)
I1107 12:08:00.314641 11160 sgd_solver.cpp:105] Iteration 73500, lr = 0.01
I1107 12:08:08.868813 11160 solver.cpp:218] Iteration 73600 (11.6901 iter/s, 8.55424s/100 iters), loss = 0.269002
I1107 12:08:08.868813 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:08:08.868813 11160 solver.cpp:237]     Train net output #1: loss = 0.269002 (* 1 = 0.269002 loss)
I1107 12:08:08.868813 11160 sgd_solver.cpp:105] Iteration 73600, lr = 0.01
I1107 12:08:17.411558 11160 solver.cpp:218] Iteration 73700 (11.7067 iter/s, 8.54208s/100 iters), loss = 0.302944
I1107 12:08:17.412060 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1107 12:08:17.412060 11160 solver.cpp:237]     Train net output #1: loss = 0.302944 (* 1 = 0.302944 loss)
I1107 12:08:17.412060 11160 sgd_solver.cpp:105] Iteration 73700, lr = 0.01
I1107 12:08:25.944079 11160 solver.cpp:218] Iteration 73800 (11.72 iter/s, 8.53243s/100 iters), loss = 0.237853
I1107 12:08:25.944079 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:08:25.944079 11160 solver.cpp:237]     Train net output #1: loss = 0.237853 (* 1 = 0.237853 loss)
I1107 12:08:25.944079 11160 sgd_solver.cpp:105] Iteration 73800, lr = 0.01
I1107 12:08:34.482779 11160 solver.cpp:218] Iteration 73900 (11.7119 iter/s, 8.5383s/100 iters), loss = 0.227501
I1107 12:08:34.483780 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:08:34.483780 11160 solver.cpp:237]     Train net output #1: loss = 0.227501 (* 1 = 0.227501 loss)
I1107 12:08:34.483780 11160 sgd_solver.cpp:105] Iteration 73900, lr = 0.01
I1107 12:08:42.639688  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:08:42.981706 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_74000.caffemodel
I1107 12:08:43.011709 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_74000.solverstate
I1107 12:08:43.020714 11160 solver.cpp:330] Iteration 74000, Testing net (#0)
I1107 12:08:43.020714 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:08:45.018364 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:08:45.098364 11160 solver.cpp:397]     Test net output #0: accuracy = 0.842
I1107 12:08:45.098364 11160 solver.cpp:397]     Test net output #1: loss = 0.519065 (* 1 = 0.519065 loss)
I1107 12:08:45.179369 11160 solver.cpp:218] Iteration 74000 (9.34938 iter/s, 10.6959s/100 iters), loss = 0.180149
I1107 12:08:45.179369 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:08:45.179369 11160 solver.cpp:237]     Train net output #1: loss = 0.180149 (* 1 = 0.180149 loss)
I1107 12:08:45.179369 11160 sgd_solver.cpp:105] Iteration 74000, lr = 0.01
I1107 12:08:53.819764 11160 solver.cpp:218] Iteration 74100 (11.5751 iter/s, 8.63921s/100 iters), loss = 0.300326
I1107 12:08:53.819764 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 12:08:53.819764 11160 solver.cpp:237]     Train net output #1: loss = 0.300326 (* 1 = 0.300326 loss)
I1107 12:08:53.819764 11160 sgd_solver.cpp:105] Iteration 74100, lr = 0.01
I1107 12:09:02.385601 11160 solver.cpp:218] Iteration 74200 (11.6746 iter/s, 8.56557s/100 iters), loss = 0.256223
I1107 12:09:02.385601 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:09:02.385601 11160 solver.cpp:237]     Train net output #1: loss = 0.256223 (* 1 = 0.256223 loss)
I1107 12:09:02.385601 11160 sgd_solver.cpp:105] Iteration 74200, lr = 0.01
I1107 12:09:10.934587 11160 solver.cpp:218] Iteration 74300 (11.6976 iter/s, 8.54873s/100 iters), loss = 0.113672
I1107 12:09:10.934587 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:09:10.934587 11160 solver.cpp:237]     Train net output #1: loss = 0.113672 (* 1 = 0.113672 loss)
I1107 12:09:10.934587 11160 sgd_solver.cpp:105] Iteration 74300, lr = 0.01
I1107 12:09:19.529609 11160 solver.cpp:218] Iteration 74400 (11.6354 iter/s, 8.59445s/100 iters), loss = 0.168247
I1107 12:09:19.529609 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:09:19.529609 11160 solver.cpp:237]     Train net output #1: loss = 0.168247 (* 1 = 0.168247 loss)
I1107 12:09:19.529609 11160 sgd_solver.cpp:105] Iteration 74400, lr = 0.01
I1107 12:09:27.863143  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:09:28.198941 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_74500.caffemodel
I1107 12:09:28.226893 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_74500.solverstate
I1107 12:09:28.235893 11160 solver.cpp:330] Iteration 74500, Testing net (#0)
I1107 12:09:28.235893 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:09:30.236049 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:09:30.316057 11160 solver.cpp:397]     Test net output #0: accuracy = 0.876
I1107 12:09:30.316057 11160 solver.cpp:397]     Test net output #1: loss = 0.410809 (* 1 = 0.410809 loss)
I1107 12:09:30.397055 11160 solver.cpp:218] Iteration 74500 (9.20251 iter/s, 10.8666s/100 iters), loss = 0.170933
I1107 12:09:30.397055 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:09:30.397055 11160 solver.cpp:237]     Train net output #1: loss = 0.170933 (* 1 = 0.170933 loss)
I1107 12:09:30.397055 11160 sgd_solver.cpp:105] Iteration 74500, lr = 0.01
I1107 12:09:38.940907 11160 solver.cpp:218] Iteration 74600 (11.7058 iter/s, 8.54275s/100 iters), loss = 0.221661
I1107 12:09:38.940907 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:09:38.940907 11160 solver.cpp:237]     Train net output #1: loss = 0.221661 (* 1 = 0.221661 loss)
I1107 12:09:38.940907 11160 sgd_solver.cpp:105] Iteration 74600, lr = 0.01
I1107 12:09:47.508203 11160 solver.cpp:218] Iteration 74700 (11.6721 iter/s, 8.56743s/100 iters), loss = 0.138247
I1107 12:09:47.508203 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:09:47.508203 11160 solver.cpp:237]     Train net output #1: loss = 0.138247 (* 1 = 0.138247 loss)
I1107 12:09:47.508203 11160 sgd_solver.cpp:105] Iteration 74700, lr = 0.01
I1107 12:09:56.050077 11160 solver.cpp:218] Iteration 74800 (11.709 iter/s, 8.54047s/100 iters), loss = 0.172746
I1107 12:09:56.050077 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:09:56.050077 11160 solver.cpp:237]     Train net output #1: loss = 0.172746 (* 1 = 0.172746 loss)
I1107 12:09:56.050077 11160 sgd_solver.cpp:105] Iteration 74800, lr = 0.01
I1107 12:10:04.623591 11160 solver.cpp:218] Iteration 74900 (11.6638 iter/s, 8.57351s/100 iters), loss = 0.229876
I1107 12:10:04.623591 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:10:04.623591 11160 solver.cpp:237]     Train net output #1: loss = 0.229876 (* 1 = 0.229876 loss)
I1107 12:10:04.623591 11160 sgd_solver.cpp:105] Iteration 74900, lr = 0.01
I1107 12:10:12.739728  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:10:13.076774 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_75000.caffemodel
I1107 12:10:13.113775 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_75000.solverstate
I1107 12:10:13.122776 11160 solver.cpp:330] Iteration 75000, Testing net (#0)
I1107 12:10:13.122776 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:10:15.116114 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:10:15.195122 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8523
I1107 12:10:15.195122 11160 solver.cpp:397]     Test net output #1: loss = 0.503299 (* 1 = 0.503299 loss)
I1107 12:10:15.276126 11160 solver.cpp:218] Iteration 75000 (9.38799 iter/s, 10.6519s/100 iters), loss = 0.164036
I1107 12:10:15.276126 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:10:15.276126 11160 solver.cpp:237]     Train net output #1: loss = 0.164037 (* 1 = 0.164037 loss)
I1107 12:10:15.276126 11160 sgd_solver.cpp:105] Iteration 75000, lr = 0.01
I1107 12:10:23.874147 11160 solver.cpp:218] Iteration 75100 (11.6308 iter/s, 8.59788s/100 iters), loss = 0.23434
I1107 12:10:23.874147 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:10:23.874147 11160 solver.cpp:237]     Train net output #1: loss = 0.23434 (* 1 = 0.23434 loss)
I1107 12:10:23.874147 11160 sgd_solver.cpp:105] Iteration 75100, lr = 0.01
I1107 12:10:32.426908 11160 solver.cpp:218] Iteration 75200 (11.6936 iter/s, 8.55172s/100 iters), loss = 0.21217
I1107 12:10:32.426908 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:10:32.426908 11160 solver.cpp:237]     Train net output #1: loss = 0.21217 (* 1 = 0.21217 loss)
I1107 12:10:32.426908 11160 sgd_solver.cpp:105] Iteration 75200, lr = 0.01
I1107 12:10:41.073271 11160 solver.cpp:218] Iteration 75300 (11.5667 iter/s, 8.64552s/100 iters), loss = 0.143907
I1107 12:10:41.073271 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:10:41.073271 11160 solver.cpp:237]     Train net output #1: loss = 0.143907 (* 1 = 0.143907 loss)
I1107 12:10:41.073271 11160 sgd_solver.cpp:105] Iteration 75300, lr = 0.01
I1107 12:10:49.653177 11160 solver.cpp:218] Iteration 75400 (11.6555 iter/s, 8.57961s/100 iters), loss = 0.25278
I1107 12:10:49.653177 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:10:49.653177 11160 solver.cpp:237]     Train net output #1: loss = 0.252781 (* 1 = 0.252781 loss)
I1107 12:10:49.653177 11160 sgd_solver.cpp:105] Iteration 75400, lr = 0.01
I1107 12:10:57.783465  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:10:58.122535 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_75500.caffemodel
I1107 12:10:58.153540 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_75500.solverstate
I1107 12:10:58.162521 11160 solver.cpp:330] Iteration 75500, Testing net (#0)
I1107 12:10:58.162521 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:11:00.155787 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:11:00.235793 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8472
I1107 12:11:00.235793 11160 solver.cpp:397]     Test net output #1: loss = 0.496797 (* 1 = 0.496797 loss)
I1107 12:11:00.317802 11160 solver.cpp:218] Iteration 75500 (9.3775 iter/s, 10.6638s/100 iters), loss = 0.125994
I1107 12:11:00.317802 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:11:00.317802 11160 solver.cpp:237]     Train net output #1: loss = 0.125994 (* 1 = 0.125994 loss)
I1107 12:11:00.317802 11160 sgd_solver.cpp:105] Iteration 75500, lr = 0.01
I1107 12:11:09.062414 11160 solver.cpp:218] Iteration 75600 (11.4356 iter/s, 8.74462s/100 iters), loss = 0.183126
I1107 12:11:09.062414 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:11:09.062414 11160 solver.cpp:237]     Train net output #1: loss = 0.183126 (* 1 = 0.183126 loss)
I1107 12:11:09.062414 11160 sgd_solver.cpp:105] Iteration 75600, lr = 0.01
I1107 12:11:17.613450 11160 solver.cpp:218] Iteration 75700 (11.6953 iter/s, 8.55042s/100 iters), loss = 0.19492
I1107 12:11:17.613450 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:11:17.613450 11160 solver.cpp:237]     Train net output #1: loss = 0.19492 (* 1 = 0.19492 loss)
I1107 12:11:17.613450 11160 sgd_solver.cpp:105] Iteration 75700, lr = 0.01
I1107 12:11:26.149394 11160 solver.cpp:218] Iteration 75800 (11.7158 iter/s, 8.53545s/100 iters), loss = 0.163962
I1107 12:11:26.149394 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:11:26.149394 11160 solver.cpp:237]     Train net output #1: loss = 0.163963 (* 1 = 0.163963 loss)
I1107 12:11:26.149394 11160 sgd_solver.cpp:105] Iteration 75800, lr = 0.01
I1107 12:11:34.672067 11160 solver.cpp:218] Iteration 75900 (11.735 iter/s, 8.52154s/100 iters), loss = 0.164977
I1107 12:11:34.672067 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:11:34.672067 11160 solver.cpp:237]     Train net output #1: loss = 0.164977 (* 1 = 0.164977 loss)
I1107 12:11:34.672067 11160 sgd_solver.cpp:105] Iteration 75900, lr = 0.01
I1107 12:11:42.772228  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:11:43.106748 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_76000.caffemodel
I1107 12:11:43.136764 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_76000.solverstate
I1107 12:11:43.145778 11160 solver.cpp:330] Iteration 76000, Testing net (#0)
I1107 12:11:43.145778 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:11:45.147984 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:11:45.229025 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8462
I1107 12:11:45.229025 11160 solver.cpp:397]     Test net output #1: loss = 0.514507 (* 1 = 0.514507 loss)
I1107 12:11:45.311036 11160 solver.cpp:218] Iteration 76000 (9.39999 iter/s, 10.6383s/100 iters), loss = 0.18409
I1107 12:11:45.311036 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:11:45.311036 11160 solver.cpp:237]     Train net output #1: loss = 0.18409 (* 1 = 0.18409 loss)
I1107 12:11:45.311036 11160 sgd_solver.cpp:105] Iteration 76000, lr = 0.01
I1107 12:11:53.874851 11160 solver.cpp:218] Iteration 76100 (11.6775 iter/s, 8.5635s/100 iters), loss = 0.212673
I1107 12:11:53.875351 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:11:53.875351 11160 solver.cpp:237]     Train net output #1: loss = 0.212673 (* 1 = 0.212673 loss)
I1107 12:11:53.875351 11160 sgd_solver.cpp:105] Iteration 76100, lr = 0.01
I1107 12:12:02.448254 11160 solver.cpp:218] Iteration 76200 (11.6647 iter/s, 8.57286s/100 iters), loss = 0.206834
I1107 12:12:02.448254 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:12:02.448254 11160 solver.cpp:237]     Train net output #1: loss = 0.206834 (* 1 = 0.206834 loss)
I1107 12:12:02.448254 11160 sgd_solver.cpp:105] Iteration 76200, lr = 0.01
I1107 12:12:11.195518 11160 solver.cpp:218] Iteration 76300 (11.4323 iter/s, 8.74712s/100 iters), loss = 0.181618
I1107 12:12:11.195518 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:12:11.195518 11160 solver.cpp:237]     Train net output #1: loss = 0.181618 (* 1 = 0.181618 loss)
I1107 12:12:11.195518 11160 sgd_solver.cpp:105] Iteration 76300, lr = 0.01
I1107 12:12:19.759286 11160 solver.cpp:218] Iteration 76400 (11.678 iter/s, 8.56312s/100 iters), loss = 0.193383
I1107 12:12:19.759286 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:12:19.759286 11160 solver.cpp:237]     Train net output #1: loss = 0.193383 (* 1 = 0.193383 loss)
I1107 12:12:19.759286 11160 sgd_solver.cpp:105] Iteration 76400, lr = 0.01
I1107 12:12:27.919109  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:12:28.259135 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_76500.caffemodel
I1107 12:12:28.288138 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_76500.solverstate
I1107 12:12:28.296139 11160 solver.cpp:330] Iteration 76500, Testing net (#0)
I1107 12:12:28.297142 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:12:30.292261 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:12:30.373270 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8566
I1107 12:12:30.373270 11160 solver.cpp:397]     Test net output #1: loss = 0.479496 (* 1 = 0.479496 loss)
I1107 12:12:30.455267 11160 solver.cpp:218] Iteration 76500 (9.34977 iter/s, 10.6954s/100 iters), loss = 0.183535
I1107 12:12:30.455267 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:12:30.455267 11160 solver.cpp:237]     Train net output #1: loss = 0.183535 (* 1 = 0.183535 loss)
I1107 12:12:30.455267 11160 sgd_solver.cpp:105] Iteration 76500, lr = 0.01
I1107 12:12:38.997045 11160 solver.cpp:218] Iteration 76600 (11.7088 iter/s, 8.54058s/100 iters), loss = 0.188761
I1107 12:12:38.997045 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:12:38.997045 11160 solver.cpp:237]     Train net output #1: loss = 0.188761 (* 1 = 0.188761 loss)
I1107 12:12:38.997045 11160 sgd_solver.cpp:105] Iteration 76600, lr = 0.01
I1107 12:12:47.552887 11160 solver.cpp:218] Iteration 76700 (11.6875 iter/s, 8.55616s/100 iters), loss = 0.138351
I1107 12:12:47.552887 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:12:47.552887 11160 solver.cpp:237]     Train net output #1: loss = 0.138351 (* 1 = 0.138351 loss)
I1107 12:12:47.552887 11160 sgd_solver.cpp:105] Iteration 76700, lr = 0.01
I1107 12:12:56.085950 11160 solver.cpp:218] Iteration 76800 (11.7209 iter/s, 8.5318s/100 iters), loss = 0.200811
I1107 12:12:56.085950 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:12:56.085950 11160 solver.cpp:237]     Train net output #1: loss = 0.200811 (* 1 = 0.200811 loss)
I1107 12:12:56.085950 11160 sgd_solver.cpp:105] Iteration 76800, lr = 0.01
I1107 12:13:04.621546 11160 solver.cpp:218] Iteration 76900 (11.7161 iter/s, 8.53523s/100 iters), loss = 0.182246
I1107 12:13:04.621546 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:13:04.621546 11160 solver.cpp:237]     Train net output #1: loss = 0.182247 (* 1 = 0.182247 loss)
I1107 12:13:04.621546 11160 sgd_solver.cpp:105] Iteration 76900, lr = 0.01
I1107 12:13:12.780539  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:13:13.118567 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_77000.caffemodel
I1107 12:13:13.149571 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_77000.solverstate
I1107 12:13:13.157568 11160 solver.cpp:330] Iteration 77000, Testing net (#0)
I1107 12:13:13.158587 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:13:15.149796 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:13:15.229799 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8383
I1107 12:13:15.229799 11160 solver.cpp:397]     Test net output #1: loss = 0.540392 (* 1 = 0.540392 loss)
I1107 12:13:15.310804 11160 solver.cpp:218] Iteration 77000 (9.35534 iter/s, 10.6891s/100 iters), loss = 0.179091
I1107 12:13:15.310804 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:13:15.310804 11160 solver.cpp:237]     Train net output #1: loss = 0.179092 (* 1 = 0.179092 loss)
I1107 12:13:15.310804 11160 sgd_solver.cpp:105] Iteration 77000, lr = 0.01
I1107 12:13:23.894330 11160 solver.cpp:218] Iteration 77100 (11.6519 iter/s, 8.58226s/100 iters), loss = 0.297699
I1107 12:13:23.894330 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:13:23.894330 11160 solver.cpp:237]     Train net output #1: loss = 0.297699 (* 1 = 0.297699 loss)
I1107 12:13:23.894330 11160 sgd_solver.cpp:105] Iteration 77100, lr = 0.01
I1107 12:13:32.458139 11160 solver.cpp:218] Iteration 77200 (11.6772 iter/s, 8.56371s/100 iters), loss = 0.136829
I1107 12:13:32.458139 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:13:32.458139 11160 solver.cpp:237]     Train net output #1: loss = 0.13683 (* 1 = 0.13683 loss)
I1107 12:13:32.458139 11160 sgd_solver.cpp:105] Iteration 77200, lr = 0.01
I1107 12:13:41.022378 11160 solver.cpp:218] Iteration 77300 (11.6773 iter/s, 8.5636s/100 iters), loss = 0.131401
I1107 12:13:41.022378 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:13:41.022378 11160 solver.cpp:237]     Train net output #1: loss = 0.131402 (* 1 = 0.131402 loss)
I1107 12:13:41.022378 11160 sgd_solver.cpp:105] Iteration 77300, lr = 0.01
I1107 12:13:49.569461 11160 solver.cpp:218] Iteration 77400 (11.7005 iter/s, 8.54667s/100 iters), loss = 0.127497
I1107 12:13:49.569461 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:13:49.569461 11160 solver.cpp:237]     Train net output #1: loss = 0.127497 (* 1 = 0.127497 loss)
I1107 12:13:49.569461 11160 sgd_solver.cpp:105] Iteration 77400, lr = 0.01
I1107 12:13:57.706122  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:13:58.042192 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_77500.caffemodel
I1107 12:13:58.071202 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_77500.solverstate
I1107 12:13:58.079201 11160 solver.cpp:330] Iteration 77500, Testing net (#0)
I1107 12:13:58.080190 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:14:00.073498 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:14:00.153162 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8744
I1107 12:14:00.153162 11160 solver.cpp:397]     Test net output #1: loss = 0.411835 (* 1 = 0.411835 loss)
I1107 12:14:00.235452 11160 solver.cpp:218] Iteration 77500 (9.37637 iter/s, 10.6651s/100 iters), loss = 0.152377
I1107 12:14:00.235452 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:14:00.235452 11160 solver.cpp:237]     Train net output #1: loss = 0.152377 (* 1 = 0.152377 loss)
I1107 12:14:00.235452 11160 sgd_solver.cpp:105] Iteration 77500, lr = 0.01
I1107 12:14:08.783129 11160 solver.cpp:218] Iteration 77600 (11.6987 iter/s, 8.54794s/100 iters), loss = 0.195173
I1107 12:14:08.783129 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:14:08.783129 11160 solver.cpp:237]     Train net output #1: loss = 0.195173 (* 1 = 0.195173 loss)
I1107 12:14:08.784131 11160 sgd_solver.cpp:105] Iteration 77600, lr = 0.01
I1107 12:14:17.312984 11160 solver.cpp:218] Iteration 77700 (11.7254 iter/s, 8.52849s/100 iters), loss = 0.177025
I1107 12:14:17.312984 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:14:17.312984 11160 solver.cpp:237]     Train net output #1: loss = 0.177025 (* 1 = 0.177025 loss)
I1107 12:14:17.312984 11160 sgd_solver.cpp:105] Iteration 77700, lr = 0.01
I1107 12:14:25.854229 11160 solver.cpp:218] Iteration 77800 (11.7086 iter/s, 8.54073s/100 iters), loss = 0.183711
I1107 12:14:25.854229 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:14:25.854229 11160 solver.cpp:237]     Train net output #1: loss = 0.183711 (* 1 = 0.183711 loss)
I1107 12:14:25.854229 11160 sgd_solver.cpp:105] Iteration 77800, lr = 0.01
I1107 12:14:34.383152 11160 solver.cpp:218] Iteration 77900 (11.7259 iter/s, 8.52813s/100 iters), loss = 0.219415
I1107 12:14:34.383152 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:14:34.383152 11160 solver.cpp:237]     Train net output #1: loss = 0.219415 (* 1 = 0.219415 loss)
I1107 12:14:34.383152 11160 sgd_solver.cpp:105] Iteration 77900, lr = 0.01
I1107 12:14:42.491099  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:14:42.828615 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_78000.caffemodel
I1107 12:14:42.858624 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_78000.solverstate
I1107 12:14:42.866623 11160 solver.cpp:330] Iteration 78000, Testing net (#0)
I1107 12:14:42.866623 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:14:44.857565 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:14:44.937067 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8337
I1107 12:14:44.937067 11160 solver.cpp:397]     Test net output #1: loss = 0.55605 (* 1 = 0.55605 loss)
I1107 12:14:45.018568 11160 solver.cpp:218] Iteration 78000 (9.40296 iter/s, 10.635s/100 iters), loss = 0.177381
I1107 12:14:45.018568 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:14:45.018568 11160 solver.cpp:237]     Train net output #1: loss = 0.177381 (* 1 = 0.177381 loss)
I1107 12:14:45.018568 11160 sgd_solver.cpp:105] Iteration 78000, lr = 0.01
I1107 12:14:53.540330 11160 solver.cpp:218] Iteration 78100 (11.7353 iter/s, 8.52131s/100 iters), loss = 0.199482
I1107 12:14:53.540330 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:14:53.540330 11160 solver.cpp:237]     Train net output #1: loss = 0.199482 (* 1 = 0.199482 loss)
I1107 12:14:53.540330 11160 sgd_solver.cpp:105] Iteration 78100, lr = 0.01
I1107 12:15:02.065675 11160 solver.cpp:218] Iteration 78200 (11.7305 iter/s, 8.52475s/100 iters), loss = 0.179263
I1107 12:15:02.065675 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:15:02.065675 11160 solver.cpp:237]     Train net output #1: loss = 0.179263 (* 1 = 0.179263 loss)
I1107 12:15:02.065675 11160 sgd_solver.cpp:105] Iteration 78200, lr = 0.01
I1107 12:15:10.583653 11160 solver.cpp:218] Iteration 78300 (11.74 iter/s, 8.51792s/100 iters), loss = 0.132352
I1107 12:15:10.583653 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:15:10.583653 11160 solver.cpp:237]     Train net output #1: loss = 0.132352 (* 1 = 0.132352 loss)
I1107 12:15:10.583653 11160 sgd_solver.cpp:105] Iteration 78300, lr = 0.01
I1107 12:15:19.101682 11160 solver.cpp:218] Iteration 78400 (11.7404 iter/s, 8.51763s/100 iters), loss = 0.152376
I1107 12:15:19.101682 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:15:19.101682 11160 solver.cpp:237]     Train net output #1: loss = 0.152377 (* 1 = 0.152377 loss)
I1107 12:15:19.101682 11160 sgd_solver.cpp:105] Iteration 78400, lr = 0.01
I1107 12:15:27.204316  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:15:27.544333 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_78500.caffemodel
I1107 12:15:27.573343 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_78500.solverstate
I1107 12:15:27.581343 11160 solver.cpp:330] Iteration 78500, Testing net (#0)
I1107 12:15:27.582352 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:15:29.570472 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:15:29.649475 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8726
I1107 12:15:29.649475 11160 solver.cpp:397]     Test net output #1: loss = 0.424567 (* 1 = 0.424567 loss)
I1107 12:15:29.731482 11160 solver.cpp:218] Iteration 78500 (9.40849 iter/s, 10.6287s/100 iters), loss = 0.260124
I1107 12:15:29.731482 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:15:29.731482 11160 solver.cpp:237]     Train net output #1: loss = 0.260124 (* 1 = 0.260124 loss)
I1107 12:15:29.731482 11160 sgd_solver.cpp:105] Iteration 78500, lr = 0.01
I1107 12:15:38.262197 11160 solver.cpp:218] Iteration 78600 (11.7226 iter/s, 8.53049s/100 iters), loss = 0.269021
I1107 12:15:38.262698 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:15:38.262698 11160 solver.cpp:237]     Train net output #1: loss = 0.269021 (* 1 = 0.269021 loss)
I1107 12:15:38.262698 11160 sgd_solver.cpp:105] Iteration 78600, lr = 0.01
I1107 12:15:46.972918 11160 solver.cpp:218] Iteration 78700 (11.4801 iter/s, 8.7107s/100 iters), loss = 0.241291
I1107 12:15:46.973918 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 12:15:46.973918 11160 solver.cpp:237]     Train net output #1: loss = 0.241291 (* 1 = 0.241291 loss)
I1107 12:15:46.973918 11160 sgd_solver.cpp:105] Iteration 78700, lr = 0.01
I1107 12:15:55.570068 11160 solver.cpp:218] Iteration 78800 (11.6333 iter/s, 8.59598s/100 iters), loss = 0.191437
I1107 12:15:55.570068 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:15:55.570068 11160 solver.cpp:237]     Train net output #1: loss = 0.191437 (* 1 = 0.191437 loss)
I1107 12:15:55.570068 11160 sgd_solver.cpp:105] Iteration 78800, lr = 0.01
I1107 12:16:04.104249 11160 solver.cpp:218] Iteration 78900 (11.7185 iter/s, 8.53354s/100 iters), loss = 0.228504
I1107 12:16:04.104249 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:16:04.104249 11160 solver.cpp:237]     Train net output #1: loss = 0.228504 (* 1 = 0.228504 loss)
I1107 12:16:04.104249 11160 sgd_solver.cpp:105] Iteration 78900, lr = 0.01
I1107 12:16:12.233546  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:16:12.571563 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_79000.caffemodel
I1107 12:16:12.603066 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_79000.solverstate
I1107 12:16:12.611572 11160 solver.cpp:330] Iteration 79000, Testing net (#0)
I1107 12:16:12.611572 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:16:14.612658 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:16:14.693686 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8798
I1107 12:16:14.693686 11160 solver.cpp:397]     Test net output #1: loss = 0.397549 (* 1 = 0.397549 loss)
I1107 12:16:14.774660 11160 solver.cpp:218] Iteration 79000 (9.37144 iter/s, 10.6707s/100 iters), loss = 0.187001
I1107 12:16:14.775660 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:16:14.775660 11160 solver.cpp:237]     Train net output #1: loss = 0.187001 (* 1 = 0.187001 loss)
I1107 12:16:14.775660 11160 sgd_solver.cpp:105] Iteration 79000, lr = 0.01
I1107 12:16:23.318400 11160 solver.cpp:218] Iteration 79100 (11.7064 iter/s, 8.54235s/100 iters), loss = 0.255223
I1107 12:16:23.318400 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:16:23.318400 11160 solver.cpp:237]     Train net output #1: loss = 0.255223 (* 1 = 0.255223 loss)
I1107 12:16:23.318400 11160 sgd_solver.cpp:105] Iteration 79100, lr = 0.01
I1107 12:16:31.843295 11160 solver.cpp:218] Iteration 79200 (11.7308 iter/s, 8.52454s/100 iters), loss = 0.209263
I1107 12:16:31.843295 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:16:31.843295 11160 solver.cpp:237]     Train net output #1: loss = 0.209263 (* 1 = 0.209263 loss)
I1107 12:16:31.843295 11160 sgd_solver.cpp:105] Iteration 79200, lr = 0.01
I1107 12:16:40.399055 11160 solver.cpp:218] Iteration 79300 (11.6894 iter/s, 8.55479s/100 iters), loss = 0.236625
I1107 12:16:40.399055 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:16:40.399055 11160 solver.cpp:237]     Train net output #1: loss = 0.236625 (* 1 = 0.236625 loss)
I1107 12:16:40.399055 11160 sgd_solver.cpp:105] Iteration 79300, lr = 0.01
I1107 12:16:48.940050 11160 solver.cpp:218] Iteration 79400 (11.7077 iter/s, 8.54137s/100 iters), loss = 0.195608
I1107 12:16:48.941051 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:16:48.941051 11160 solver.cpp:237]     Train net output #1: loss = 0.195608 (* 1 = 0.195608 loss)
I1107 12:16:48.941051 11160 sgd_solver.cpp:105] Iteration 79400, lr = 0.01
I1107 12:16:57.075297  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:16:57.413887 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_79500.caffemodel
I1107 12:16:57.441387 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_79500.solverstate
I1107 12:16:57.449399 11160 solver.cpp:330] Iteration 79500, Testing net (#0)
I1107 12:16:57.449399 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:16:59.445535 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:16:59.524538 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8642
I1107 12:16:59.524538 11160 solver.cpp:397]     Test net output #1: loss = 0.439642 (* 1 = 0.439642 loss)
I1107 12:16:59.606540 11160 solver.cpp:218] Iteration 79500 (9.37607 iter/s, 10.6654s/100 iters), loss = 0.187992
I1107 12:16:59.607041 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:16:59.607041 11160 solver.cpp:237]     Train net output #1: loss = 0.187993 (* 1 = 0.187993 loss)
I1107 12:16:59.607041 11160 sgd_solver.cpp:105] Iteration 79500, lr = 0.01
I1107 12:17:08.153204 11160 solver.cpp:218] Iteration 79600 (11.701 iter/s, 8.54626s/100 iters), loss = 0.156621
I1107 12:17:08.153204 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:17:08.153204 11160 solver.cpp:237]     Train net output #1: loss = 0.156621 (* 1 = 0.156621 loss)
I1107 12:17:08.153204 11160 sgd_solver.cpp:105] Iteration 79600, lr = 0.01
I1107 12:17:16.678400 11160 solver.cpp:218] Iteration 79700 (11.7312 iter/s, 8.52426s/100 iters), loss = 0.198996
I1107 12:17:16.678400 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:17:16.678400 11160 solver.cpp:237]     Train net output #1: loss = 0.198996 (* 1 = 0.198996 loss)
I1107 12:17:16.678400 11160 sgd_solver.cpp:105] Iteration 79700, lr = 0.01
I1107 12:17:25.221176 11160 solver.cpp:218] Iteration 79800 (11.7062 iter/s, 8.54245s/100 iters), loss = 0.151611
I1107 12:17:25.221176 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:17:25.221176 11160 solver.cpp:237]     Train net output #1: loss = 0.151612 (* 1 = 0.151612 loss)
I1107 12:17:25.221176 11160 sgd_solver.cpp:105] Iteration 79800, lr = 0.01
I1107 12:17:33.758280 11160 solver.cpp:218] Iteration 79900 (11.7135 iter/s, 8.53715s/100 iters), loss = 0.170658
I1107 12:17:33.758280 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:17:33.758280 11160 solver.cpp:237]     Train net output #1: loss = 0.170659 (* 1 = 0.170659 loss)
I1107 12:17:33.758280 11160 sgd_solver.cpp:105] Iteration 79900, lr = 0.01
I1107 12:17:41.889097  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:17:42.228147 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_80000.caffemodel
I1107 12:17:42.258152 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_80000.solverstate
I1107 12:17:42.267151 11160 solver.cpp:330] Iteration 80000, Testing net (#0)
I1107 12:17:42.267151 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:17:44.273586 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:17:44.354591 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8443
I1107 12:17:44.354591 11160 solver.cpp:397]     Test net output #1: loss = 0.521345 (* 1 = 0.521345 loss)
I1107 12:17:44.436594 11160 solver.cpp:218] Iteration 80000 (9.3659 iter/s, 10.677s/100 iters), loss = 0.183128
I1107 12:17:44.436594 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:17:44.436594 11160 solver.cpp:237]     Train net output #1: loss = 0.183128 (* 1 = 0.183128 loss)
I1107 12:17:44.436594 11160 sgd_solver.cpp:105] Iteration 80000, lr = 0.01
I1107 12:17:52.981914 11160 solver.cpp:218] Iteration 80100 (11.7031 iter/s, 8.54472s/100 iters), loss = 0.211799
I1107 12:17:52.981914 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:17:52.981914 11160 solver.cpp:237]     Train net output #1: loss = 0.211799 (* 1 = 0.211799 loss)
I1107 12:17:52.981914 11160 sgd_solver.cpp:105] Iteration 80100, lr = 0.01
I1107 12:18:01.527307 11160 solver.cpp:218] Iteration 80200 (11.7025 iter/s, 8.54516s/100 iters), loss = 0.192131
I1107 12:18:01.527307 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:18:01.527307 11160 solver.cpp:237]     Train net output #1: loss = 0.192131 (* 1 = 0.192131 loss)
I1107 12:18:01.527307 11160 sgd_solver.cpp:105] Iteration 80200, lr = 0.01
I1107 12:18:10.267302 11160 solver.cpp:218] Iteration 80300 (11.4421 iter/s, 8.73969s/100 iters), loss = 0.22942
I1107 12:18:10.267302 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:18:10.267302 11160 solver.cpp:237]     Train net output #1: loss = 0.22942 (* 1 = 0.22942 loss)
I1107 12:18:10.267302 11160 sgd_solver.cpp:105] Iteration 80300, lr = 0.01
I1107 12:18:18.822641 11160 solver.cpp:218] Iteration 80400 (11.6892 iter/s, 8.55493s/100 iters), loss = 0.147949
I1107 12:18:18.822641 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:18:18.822641 11160 solver.cpp:237]     Train net output #1: loss = 0.14795 (* 1 = 0.14795 loss)
I1107 12:18:18.822641 11160 sgd_solver.cpp:105] Iteration 80400, lr = 0.01
I1107 12:18:27.039577  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:18:27.377591 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_80500.caffemodel
I1107 12:18:27.405591 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_80500.solverstate
I1107 12:18:27.414590 11160 solver.cpp:330] Iteration 80500, Testing net (#0)
I1107 12:18:27.414590 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:18:29.414772 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:18:29.494782 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8578
I1107 12:18:29.494782 11160 solver.cpp:397]     Test net output #1: loss = 0.483359 (* 1 = 0.483359 loss)
I1107 12:18:29.576817 11160 solver.cpp:218] Iteration 80500 (9.29963 iter/s, 10.7531s/100 iters), loss = 0.242521
I1107 12:18:29.576817 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 12:18:29.576817 11160 solver.cpp:237]     Train net output #1: loss = 0.242521 (* 1 = 0.242521 loss)
I1107 12:18:29.576817 11160 sgd_solver.cpp:105] Iteration 80500, lr = 0.01
I1107 12:18:38.164096 11160 solver.cpp:218] Iteration 80600 (11.6453 iter/s, 8.58718s/100 iters), loss = 0.235331
I1107 12:18:38.164096 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:18:38.164096 11160 solver.cpp:237]     Train net output #1: loss = 0.235332 (* 1 = 0.235332 loss)
I1107 12:18:38.164096 11160 sgd_solver.cpp:105] Iteration 80600, lr = 0.01
I1107 12:18:46.716645 11160 solver.cpp:218] Iteration 80700 (11.6938 iter/s, 8.55157s/100 iters), loss = 0.275228
I1107 12:18:46.716645 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:18:46.716645 11160 solver.cpp:237]     Train net output #1: loss = 0.275228 (* 1 = 0.275228 loss)
I1107 12:18:46.716645 11160 sgd_solver.cpp:105] Iteration 80700, lr = 0.01
I1107 12:18:55.388453 11160 solver.cpp:218] Iteration 80800 (11.5319 iter/s, 8.67159s/100 iters), loss = 0.149849
I1107 12:18:55.388453 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:18:55.388453 11160 solver.cpp:237]     Train net output #1: loss = 0.149849 (* 1 = 0.149849 loss)
I1107 12:18:55.388453 11160 sgd_solver.cpp:105] Iteration 80800, lr = 0.01
I1107 12:19:03.952515 11160 solver.cpp:218] Iteration 80900 (11.6777 iter/s, 8.56337s/100 iters), loss = 0.148367
I1107 12:19:03.952515 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:19:03.952515 11160 solver.cpp:237]     Train net output #1: loss = 0.148368 (* 1 = 0.148368 loss)
I1107 12:19:03.952515 11160 sgd_solver.cpp:105] Iteration 80900, lr = 0.01
I1107 12:19:12.089998  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:19:12.427044 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_81000.caffemodel
I1107 12:19:12.457049 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_81000.solverstate
I1107 12:19:12.465062 11160 solver.cpp:330] Iteration 81000, Testing net (#0)
I1107 12:19:12.466054 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:19:14.464287 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:19:14.546288 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8616
I1107 12:19:14.546789 11160 solver.cpp:397]     Test net output #1: loss = 0.464417 (* 1 = 0.464417 loss)
I1107 12:19:14.630290 11160 solver.cpp:218] Iteration 81000 (9.36579 iter/s, 10.6772s/100 iters), loss = 0.232569
I1107 12:19:14.630290 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:19:14.630290 11160 solver.cpp:237]     Train net output #1: loss = 0.232569 (* 1 = 0.232569 loss)
I1107 12:19:14.630290 11160 sgd_solver.cpp:105] Iteration 81000, lr = 0.01
I1107 12:19:23.187063 11160 solver.cpp:218] Iteration 81100 (11.6869 iter/s, 8.5566s/100 iters), loss = 0.205365
I1107 12:19:23.187063 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:19:23.187063 11160 solver.cpp:237]     Train net output #1: loss = 0.205365 (* 1 = 0.205365 loss)
I1107 12:19:23.187063 11160 sgd_solver.cpp:105] Iteration 81100, lr = 0.01
I1107 12:19:31.725185 11160 solver.cpp:218] Iteration 81200 (11.7132 iter/s, 8.53736s/100 iters), loss = 0.211027
I1107 12:19:31.725185 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:19:31.725185 11160 solver.cpp:237]     Train net output #1: loss = 0.211027 (* 1 = 0.211027 loss)
I1107 12:19:31.725185 11160 sgd_solver.cpp:105] Iteration 81200, lr = 0.01
I1107 12:19:40.261327 11160 solver.cpp:218] Iteration 81300 (11.7162 iter/s, 8.53519s/100 iters), loss = 0.154441
I1107 12:19:40.261327 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:19:40.261327 11160 solver.cpp:237]     Train net output #1: loss = 0.154441 (* 1 = 0.154441 loss)
I1107 12:19:40.261327 11160 sgd_solver.cpp:105] Iteration 81300, lr = 0.01
I1107 12:19:48.812670 11160 solver.cpp:218] Iteration 81400 (11.694 iter/s, 8.5514s/100 iters), loss = 0.185566
I1107 12:19:48.812670 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:19:48.812670 11160 solver.cpp:237]     Train net output #1: loss = 0.185566 (* 1 = 0.185566 loss)
I1107 12:19:48.812670 11160 sgd_solver.cpp:105] Iteration 81400, lr = 0.01
I1107 12:19:56.960352  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:19:57.296871 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_81500.caffemodel
I1107 12:19:57.329870 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_81500.solverstate
I1107 12:19:57.338871 11160 solver.cpp:330] Iteration 81500, Testing net (#0)
I1107 12:19:57.339871 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:19:59.339187 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:19:59.419193 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8377
I1107 12:19:59.419193 11160 solver.cpp:397]     Test net output #1: loss = 0.545959 (* 1 = 0.545959 loss)
I1107 12:19:59.500211 11160 solver.cpp:218] Iteration 81500 (9.35731 iter/s, 10.6868s/100 iters), loss = 0.0961448
I1107 12:19:59.500211 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:19:59.500211 11160 solver.cpp:237]     Train net output #1: loss = 0.0961451 (* 1 = 0.0961451 loss)
I1107 12:19:59.500211 11160 sgd_solver.cpp:105] Iteration 81500, lr = 0.01
I1107 12:20:08.128846 11160 solver.cpp:218] Iteration 81600 (11.5899 iter/s, 8.6282s/100 iters), loss = 0.308803
I1107 12:20:08.128846 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:20:08.128846 11160 solver.cpp:237]     Train net output #1: loss = 0.308804 (* 1 = 0.308804 loss)
I1107 12:20:08.128846 11160 sgd_solver.cpp:105] Iteration 81600, lr = 0.01
I1107 12:20:16.693238 11160 solver.cpp:218] Iteration 81700 (11.6775 iter/s, 8.56347s/100 iters), loss = 0.198097
I1107 12:20:16.693238 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:20:16.693238 11160 solver.cpp:237]     Train net output #1: loss = 0.198097 (* 1 = 0.198097 loss)
I1107 12:20:16.693238 11160 sgd_solver.cpp:105] Iteration 81700, lr = 0.01
I1107 12:20:25.309195 11160 solver.cpp:218] Iteration 81800 (11.6064 iter/s, 8.61596s/100 iters), loss = 0.132419
I1107 12:20:25.309195 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:20:25.309195 11160 solver.cpp:237]     Train net output #1: loss = 0.132419 (* 1 = 0.132419 loss)
I1107 12:20:25.309195 11160 sgd_solver.cpp:105] Iteration 81800, lr = 0.01
I1107 12:20:33.973377 11160 solver.cpp:218] Iteration 81900 (11.5426 iter/s, 8.66353s/100 iters), loss = 0.132684
I1107 12:20:33.973377 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:20:33.973377 11160 solver.cpp:237]     Train net output #1: loss = 0.132684 (* 1 = 0.132684 loss)
I1107 12:20:33.973377 11160 sgd_solver.cpp:105] Iteration 81900, lr = 0.01
I1107 12:20:42.107028  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:20:42.444564 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_82000.caffemodel
I1107 12:20:42.472565 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_82000.solverstate
I1107 12:20:42.481575 11160 solver.cpp:330] Iteration 82000, Testing net (#0)
I1107 12:20:42.481575 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:20:44.490787 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:20:44.570792 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8543
I1107 12:20:44.570792 11160 solver.cpp:397]     Test net output #1: loss = 0.489267 (* 1 = 0.489267 loss)
I1107 12:20:44.651798 11160 solver.cpp:218] Iteration 82000 (9.36491 iter/s, 10.6782s/100 iters), loss = 0.164941
I1107 12:20:44.651798 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:20:44.652797 11160 solver.cpp:237]     Train net output #1: loss = 0.164941 (* 1 = 0.164941 loss)
I1107 12:20:44.652797 11160 sgd_solver.cpp:105] Iteration 82000, lr = 0.01
I1107 12:20:53.199609 11160 solver.cpp:218] Iteration 82100 (11.7006 iter/s, 8.54655s/100 iters), loss = 0.271067
I1107 12:20:53.199609 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:20:53.199609 11160 solver.cpp:237]     Train net output #1: loss = 0.271067 (* 1 = 0.271067 loss)
I1107 12:20:53.199609 11160 sgd_solver.cpp:105] Iteration 82100, lr = 0.01
I1107 12:21:01.752720 11160 solver.cpp:218] Iteration 82200 (11.6925 iter/s, 8.55252s/100 iters), loss = 0.221684
I1107 12:21:01.752720 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:21:01.752720 11160 solver.cpp:237]     Train net output #1: loss = 0.221684 (* 1 = 0.221684 loss)
I1107 12:21:01.752720 11160 sgd_solver.cpp:105] Iteration 82200, lr = 0.01
I1107 12:21:10.311321 11160 solver.cpp:218] Iteration 82300 (11.6851 iter/s, 8.55794s/100 iters), loss = 0.195095
I1107 12:21:10.311321 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:21:10.311321 11160 solver.cpp:237]     Train net output #1: loss = 0.195096 (* 1 = 0.195096 loss)
I1107 12:21:10.311321 11160 sgd_solver.cpp:105] Iteration 82300, lr = 0.01
I1107 12:21:18.855864 11160 solver.cpp:218] Iteration 82400 (11.7041 iter/s, 8.54399s/100 iters), loss = 0.162648
I1107 12:21:18.855864 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:21:18.855864 11160 solver.cpp:237]     Train net output #1: loss = 0.162648 (* 1 = 0.162648 loss)
I1107 12:21:18.855864 11160 sgd_solver.cpp:105] Iteration 82400, lr = 0.01
I1107 12:21:26.983423  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:21:27.321445 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_82500.caffemodel
I1107 12:21:27.355949 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_82500.solverstate
I1107 12:21:27.364449 11160 solver.cpp:330] Iteration 82500, Testing net (#0)
I1107 12:21:27.364949 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:21:29.353143 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:21:29.431645 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8508
I1107 12:21:29.432646 11160 solver.cpp:397]     Test net output #1: loss = 0.494574 (* 1 = 0.494574 loss)
I1107 12:21:29.513653 11160 solver.cpp:218] Iteration 82500 (9.38277 iter/s, 10.6578s/100 iters), loss = 0.169406
I1107 12:21:29.514653 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:21:29.514653 11160 solver.cpp:237]     Train net output #1: loss = 0.169406 (* 1 = 0.169406 loss)
I1107 12:21:29.514653 11160 sgd_solver.cpp:105] Iteration 82500, lr = 0.01
I1107 12:21:38.069849 11160 solver.cpp:218] Iteration 82600 (11.6893 iter/s, 8.55482s/100 iters), loss = 0.269497
I1107 12:21:38.069849 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:21:38.069849 11160 solver.cpp:237]     Train net output #1: loss = 0.269498 (* 1 = 0.269498 loss)
I1107 12:21:38.069849 11160 sgd_solver.cpp:105] Iteration 82600, lr = 0.01
I1107 12:21:46.629765 11160 solver.cpp:218] Iteration 82700 (11.6822 iter/s, 8.56001s/100 iters), loss = 0.134024
I1107 12:21:46.629765 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:21:46.629765 11160 solver.cpp:237]     Train net output #1: loss = 0.134024 (* 1 = 0.134024 loss)
I1107 12:21:46.629765 11160 sgd_solver.cpp:105] Iteration 82700, lr = 0.01
I1107 12:21:55.166903 11160 solver.cpp:218] Iteration 82800 (11.7148 iter/s, 8.53618s/100 iters), loss = 0.118745
I1107 12:21:55.166903 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:21:55.166903 11160 solver.cpp:237]     Train net output #1: loss = 0.118746 (* 1 = 0.118746 loss)
I1107 12:21:55.166903 11160 sgd_solver.cpp:105] Iteration 82800, lr = 0.01
I1107 12:22:03.689635 11160 solver.cpp:218] Iteration 82900 (11.7339 iter/s, 8.52233s/100 iters), loss = 0.1687
I1107 12:22:03.689635 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:22:03.689635 11160 solver.cpp:237]     Train net output #1: loss = 0.168701 (* 1 = 0.168701 loss)
I1107 12:22:03.689635 11160 sgd_solver.cpp:105] Iteration 82900, lr = 0.01
I1107 12:22:11.799769  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:22:12.136782 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_83000.caffemodel
I1107 12:22:12.166782 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_83000.solverstate
I1107 12:22:12.175287 11160 solver.cpp:330] Iteration 83000, Testing net (#0)
I1107 12:22:12.175287 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:22:14.166059 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:22:14.246080 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8623
I1107 12:22:14.246080 11160 solver.cpp:397]     Test net output #1: loss = 0.445226 (* 1 = 0.445226 loss)
I1107 12:22:14.327087 11160 solver.cpp:218] Iteration 83000 (9.40138 iter/s, 10.6367s/100 iters), loss = 0.134458
I1107 12:22:14.327087 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:22:14.327087 11160 solver.cpp:237]     Train net output #1: loss = 0.134458 (* 1 = 0.134458 loss)
I1107 12:22:14.327087 11160 sgd_solver.cpp:105] Iteration 83000, lr = 0.01
I1107 12:22:22.856325 11160 solver.cpp:218] Iteration 83100 (11.7245 iter/s, 8.52913s/100 iters), loss = 0.255088
I1107 12:22:22.856325 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:22:22.856325 11160 solver.cpp:237]     Train net output #1: loss = 0.255088 (* 1 = 0.255088 loss)
I1107 12:22:22.856325 11160 sgd_solver.cpp:105] Iteration 83100, lr = 0.01
I1107 12:22:31.380710 11160 solver.cpp:218] Iteration 83200 (11.7323 iter/s, 8.52349s/100 iters), loss = 0.249289
I1107 12:22:31.380710 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:22:31.380710 11160 solver.cpp:237]     Train net output #1: loss = 0.249289 (* 1 = 0.249289 loss)
I1107 12:22:31.380710 11160 sgd_solver.cpp:105] Iteration 83200, lr = 0.01
I1107 12:22:39.907982 11160 solver.cpp:218] Iteration 83300 (11.7272 iter/s, 8.52717s/100 iters), loss = 0.13212
I1107 12:22:39.907982 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:22:39.907982 11160 solver.cpp:237]     Train net output #1: loss = 0.132121 (* 1 = 0.132121 loss)
I1107 12:22:39.907982 11160 sgd_solver.cpp:105] Iteration 83300, lr = 0.01
I1107 12:22:48.433784 11160 solver.cpp:218] Iteration 83400 (11.7303 iter/s, 8.52491s/100 iters), loss = 0.201172
I1107 12:22:48.433784 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:22:48.433784 11160 solver.cpp:237]     Train net output #1: loss = 0.201172 (* 1 = 0.201172 loss)
I1107 12:22:48.433784 11160 sgd_solver.cpp:105] Iteration 83400, lr = 0.01
I1107 12:22:56.541620  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:22:56.878634 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_83500.caffemodel
I1107 12:22:56.909643 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_83500.solverstate
I1107 12:22:56.917645 11160 solver.cpp:330] Iteration 83500, Testing net (#0)
I1107 12:22:56.917645 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:22:58.907878 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:22:58.988380 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8701
I1107 12:22:58.988380 11160 solver.cpp:397]     Test net output #1: loss = 0.438602 (* 1 = 0.438602 loss)
I1107 12:22:59.069883 11160 solver.cpp:218] Iteration 83500 (9.40248 iter/s, 10.6355s/100 iters), loss = 0.147266
I1107 12:22:59.069883 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:22:59.069883 11160 solver.cpp:237]     Train net output #1: loss = 0.147266 (* 1 = 0.147266 loss)
I1107 12:22:59.069883 11160 sgd_solver.cpp:105] Iteration 83500, lr = 0.01
I1107 12:23:07.598750 11160 solver.cpp:218] Iteration 83600 (11.7259 iter/s, 8.52811s/100 iters), loss = 0.381477
I1107 12:23:07.598750 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:23:07.598750 11160 solver.cpp:237]     Train net output #1: loss = 0.381477 (* 1 = 0.381477 loss)
I1107 12:23:07.598750 11160 sgd_solver.cpp:105] Iteration 83600, lr = 0.01
I1107 12:23:16.126672 11160 solver.cpp:218] Iteration 83700 (11.7258 iter/s, 8.52822s/100 iters), loss = 0.174663
I1107 12:23:16.127673 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:23:16.127673 11160 solver.cpp:237]     Train net output #1: loss = 0.174663 (* 1 = 0.174663 loss)
I1107 12:23:16.127673 11160 sgd_solver.cpp:105] Iteration 83700, lr = 0.01
I1107 12:23:24.639621 11160 solver.cpp:218] Iteration 83800 (11.7475 iter/s, 8.51242s/100 iters), loss = 0.203687
I1107 12:23:24.640609 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:23:24.640609 11160 solver.cpp:237]     Train net output #1: loss = 0.203687 (* 1 = 0.203687 loss)
I1107 12:23:24.640609 11160 sgd_solver.cpp:105] Iteration 83800, lr = 0.01
I1107 12:23:33.152986 11160 solver.cpp:218] Iteration 83900 (11.747 iter/s, 8.51283s/100 iters), loss = 0.138212
I1107 12:23:33.152986 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:23:33.152986 11160 solver.cpp:237]     Train net output #1: loss = 0.138212 (* 1 = 0.138212 loss)
I1107 12:23:33.152986 11160 sgd_solver.cpp:105] Iteration 83900, lr = 0.01
I1107 12:23:41.257231  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:23:41.592763 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_84000.caffemodel
I1107 12:23:41.621275 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_84000.solverstate
I1107 12:23:41.630276 11160 solver.cpp:330] Iteration 84000, Testing net (#0)
I1107 12:23:41.630276 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:23:43.619436 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:23:43.699437 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8246
I1107 12:23:43.699437 11160 solver.cpp:397]     Test net output #1: loss = 0.601214 (* 1 = 0.601214 loss)
I1107 12:23:43.780441 11160 solver.cpp:218] Iteration 84000 (9.41044 iter/s, 10.6265s/100 iters), loss = 0.182476
I1107 12:23:43.780441 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:23:43.780441 11160 solver.cpp:237]     Train net output #1: loss = 0.182477 (* 1 = 0.182477 loss)
I1107 12:23:43.780441 11160 sgd_solver.cpp:105] Iteration 84000, lr = 0.01
I1107 12:23:52.316718 11160 solver.cpp:218] Iteration 84100 (11.716 iter/s, 8.53534s/100 iters), loss = 0.22622
I1107 12:23:52.316718 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:23:52.316718 11160 solver.cpp:237]     Train net output #1: loss = 0.226221 (* 1 = 0.226221 loss)
I1107 12:23:52.316718 11160 sgd_solver.cpp:105] Iteration 84100, lr = 0.01
I1107 12:24:00.844821 11160 solver.cpp:218] Iteration 84200 (11.7255 iter/s, 8.52839s/100 iters), loss = 0.222262
I1107 12:24:00.844821 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:24:00.845821 11160 solver.cpp:237]     Train net output #1: loss = 0.222262 (* 1 = 0.222262 loss)
I1107 12:24:00.845821 11160 sgd_solver.cpp:105] Iteration 84200, lr = 0.01
I1107 12:24:09.364501 11160 solver.cpp:218] Iteration 84300 (11.7385 iter/s, 8.51895s/100 iters), loss = 0.211527
I1107 12:24:09.364501 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:24:09.364501 11160 solver.cpp:237]     Train net output #1: loss = 0.211527 (* 1 = 0.211527 loss)
I1107 12:24:09.364501 11160 sgd_solver.cpp:105] Iteration 84300, lr = 0.01
I1107 12:24:17.889225 11160 solver.cpp:218] Iteration 84400 (11.7322 iter/s, 8.52359s/100 iters), loss = 0.158284
I1107 12:24:17.889225 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:24:17.889225 11160 solver.cpp:237]     Train net output #1: loss = 0.158284 (* 1 = 0.158284 loss)
I1107 12:24:17.889225 11160 sgd_solver.cpp:105] Iteration 84400, lr = 0.01
I1107 12:24:25.999179  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:24:26.337206 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_84500.caffemodel
I1107 12:24:26.368214 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_84500.solverstate
I1107 12:24:26.376214 11160 solver.cpp:330] Iteration 84500, Testing net (#0)
I1107 12:24:26.377215 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:24:28.364344 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:24:28.444846 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8449
I1107 12:24:28.444846 11160 solver.cpp:397]     Test net output #1: loss = 0.520339 (* 1 = 0.520339 loss)
I1107 12:24:28.526363 11160 solver.cpp:218] Iteration 84500 (9.40125 iter/s, 10.6369s/100 iters), loss = 0.107959
I1107 12:24:28.526363 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:24:28.526363 11160 solver.cpp:237]     Train net output #1: loss = 0.107959 (* 1 = 0.107959 loss)
I1107 12:24:28.526363 11160 sgd_solver.cpp:105] Iteration 84500, lr = 0.01
I1107 12:24:37.053022 11160 solver.cpp:218] Iteration 84600 (11.7286 iter/s, 8.5262s/100 iters), loss = 0.296292
I1107 12:24:37.053526 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:24:37.053526 11160 solver.cpp:237]     Train net output #1: loss = 0.296293 (* 1 = 0.296293 loss)
I1107 12:24:37.053526 11160 sgd_solver.cpp:105] Iteration 84600, lr = 0.01
I1107 12:24:45.578178 11160 solver.cpp:218] Iteration 84700 (11.7307 iter/s, 8.52463s/100 iters), loss = 0.167252
I1107 12:24:45.578178 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:24:45.578178 11160 solver.cpp:237]     Train net output #1: loss = 0.167252 (* 1 = 0.167252 loss)
I1107 12:24:45.578178 11160 sgd_solver.cpp:105] Iteration 84700, lr = 0.01
I1107 12:24:54.101873 11160 solver.cpp:218] Iteration 84800 (11.733 iter/s, 8.523s/100 iters), loss = 0.148553
I1107 12:24:54.101873 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:24:54.101873 11160 solver.cpp:237]     Train net output #1: loss = 0.148554 (* 1 = 0.148554 loss)
I1107 12:24:54.101873 11160 sgd_solver.cpp:105] Iteration 84800, lr = 0.01
I1107 12:25:02.635149 11160 solver.cpp:218] Iteration 84900 (11.7199 iter/s, 8.53251s/100 iters), loss = 0.237689
I1107 12:25:02.635149 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:25:02.635149 11160 solver.cpp:237]     Train net output #1: loss = 0.237689 (* 1 = 0.237689 loss)
I1107 12:25:02.635149 11160 sgd_solver.cpp:105] Iteration 84900, lr = 0.01
I1107 12:25:10.747699  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:25:11.084751 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_85000.caffemodel
I1107 12:25:11.116751 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_85000.solverstate
I1107 12:25:11.124752 11160 solver.cpp:330] Iteration 85000, Testing net (#0)
I1107 12:25:11.125751 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:25:13.114931 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:25:13.193938 11160 solver.cpp:397]     Test net output #0: accuracy = 0.858
I1107 12:25:13.193938 11160 solver.cpp:397]     Test net output #1: loss = 0.470963 (* 1 = 0.470963 loss)
I1107 12:25:13.275941 11160 solver.cpp:218] Iteration 85000 (9.39796 iter/s, 10.6406s/100 iters), loss = 0.293061
I1107 12:25:13.275941 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:25:13.275941 11160 solver.cpp:237]     Train net output #1: loss = 0.293061 (* 1 = 0.293061 loss)
I1107 12:25:13.275941 11160 sgd_solver.cpp:105] Iteration 85000, lr = 0.01
I1107 12:25:21.804002 11160 solver.cpp:218] Iteration 85100 (11.7273 iter/s, 8.52713s/100 iters), loss = 0.190745
I1107 12:25:21.804002 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:25:21.804002 11160 solver.cpp:237]     Train net output #1: loss = 0.190745 (* 1 = 0.190745 loss)
I1107 12:25:21.804002 11160 sgd_solver.cpp:105] Iteration 85100, lr = 0.01
I1107 12:25:30.323698 11160 solver.cpp:218] Iteration 85200 (11.7383 iter/s, 8.5191s/100 iters), loss = 0.230297
I1107 12:25:30.323698 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:25:30.323698 11160 solver.cpp:237]     Train net output #1: loss = 0.230297 (* 1 = 0.230297 loss)
I1107 12:25:30.323698 11160 sgd_solver.cpp:105] Iteration 85200, lr = 0.01
I1107 12:25:38.845616 11160 solver.cpp:218] Iteration 85300 (11.7351 iter/s, 8.52144s/100 iters), loss = 0.17489
I1107 12:25:38.845616 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:25:38.845616 11160 solver.cpp:237]     Train net output #1: loss = 0.174891 (* 1 = 0.174891 loss)
I1107 12:25:38.845616 11160 sgd_solver.cpp:105] Iteration 85300, lr = 0.01
I1107 12:25:47.371697 11160 solver.cpp:218] Iteration 85400 (11.7299 iter/s, 8.52525s/100 iters), loss = 0.166124
I1107 12:25:47.371697 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:25:47.371697 11160 solver.cpp:237]     Train net output #1: loss = 0.166124 (* 1 = 0.166124 loss)
I1107 12:25:47.371697 11160 sgd_solver.cpp:105] Iteration 85400, lr = 0.01
I1107 12:25:55.479689  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:25:55.818718 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_85500.caffemodel
I1107 12:25:55.849725 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_85500.solverstate
I1107 12:25:55.857725 11160 solver.cpp:330] Iteration 85500, Testing net (#0)
I1107 12:25:55.858726 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:25:57.845902 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:25:57.924903 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8459
I1107 12:25:57.924903 11160 solver.cpp:397]     Test net output #1: loss = 0.520864 (* 1 = 0.520864 loss)
I1107 12:25:58.006909 11160 solver.cpp:218] Iteration 85500 (9.40301 iter/s, 10.6349s/100 iters), loss = 0.260317
I1107 12:25:58.006909 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:25:58.006909 11160 solver.cpp:237]     Train net output #1: loss = 0.260317 (* 1 = 0.260317 loss)
I1107 12:25:58.006909 11160 sgd_solver.cpp:105] Iteration 85500, lr = 0.01
I1107 12:26:06.541999 11160 solver.cpp:218] Iteration 85600 (11.717 iter/s, 8.53462s/100 iters), loss = 0.157089
I1107 12:26:06.541999 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:26:06.541999 11160 solver.cpp:237]     Train net output #1: loss = 0.157089 (* 1 = 0.157089 loss)
I1107 12:26:06.541999 11160 sgd_solver.cpp:105] Iteration 85600, lr = 0.01
I1107 12:26:15.071465 11160 solver.cpp:218] Iteration 85700 (11.7252 iter/s, 8.52865s/100 iters), loss = 0.135246
I1107 12:26:15.071465 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:26:15.071465 11160 solver.cpp:237]     Train net output #1: loss = 0.135246 (* 1 = 0.135246 loss)
I1107 12:26:15.071465 11160 sgd_solver.cpp:105] Iteration 85700, lr = 0.01
I1107 12:26:23.599684 11160 solver.cpp:218] Iteration 85800 (11.7266 iter/s, 8.52764s/100 iters), loss = 0.218615
I1107 12:26:23.599684 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:26:23.599684 11160 solver.cpp:237]     Train net output #1: loss = 0.218616 (* 1 = 0.218616 loss)
I1107 12:26:23.599684 11160 sgd_solver.cpp:105] Iteration 85800, lr = 0.01
I1107 12:26:32.119091 11160 solver.cpp:218] Iteration 85900 (11.7385 iter/s, 8.51895s/100 iters), loss = 0.136538
I1107 12:26:32.119091 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:26:32.119091 11160 solver.cpp:237]     Train net output #1: loss = 0.136538 (* 1 = 0.136538 loss)
I1107 12:26:32.119091 11160 sgd_solver.cpp:105] Iteration 85900, lr = 0.01
I1107 12:26:40.218806  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:26:40.555827 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_86000.caffemodel
I1107 12:26:40.585827 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_86000.solverstate
I1107 12:26:40.594333 11160 solver.cpp:330] Iteration 86000, Testing net (#0)
I1107 12:26:40.594333 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:26:42.584957 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:26:42.663964 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8562
I1107 12:26:42.663964 11160 solver.cpp:397]     Test net output #1: loss = 0.470194 (* 1 = 0.470194 loss)
I1107 12:26:42.744968 11160 solver.cpp:218] Iteration 86000 (9.41098 iter/s, 10.6259s/100 iters), loss = 0.179216
I1107 12:26:42.744968 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:26:42.744968 11160 solver.cpp:237]     Train net output #1: loss = 0.179216 (* 1 = 0.179216 loss)
I1107 12:26:42.744968 11160 sgd_solver.cpp:105] Iteration 86000, lr = 0.01
I1107 12:26:51.284687 11160 solver.cpp:218] Iteration 86100 (11.7105 iter/s, 8.53932s/100 iters), loss = 0.244032
I1107 12:26:51.284687 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:26:51.285688 11160 solver.cpp:237]     Train net output #1: loss = 0.244033 (* 1 = 0.244033 loss)
I1107 12:26:51.285688 11160 sgd_solver.cpp:105] Iteration 86100, lr = 0.01
I1107 12:26:59.820652 11160 solver.cpp:218] Iteration 86200 (11.7159 iter/s, 8.5354s/100 iters), loss = 0.19514
I1107 12:26:59.820652 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:26:59.820652 11160 solver.cpp:237]     Train net output #1: loss = 0.19514 (* 1 = 0.19514 loss)
I1107 12:26:59.820652 11160 sgd_solver.cpp:105] Iteration 86200, lr = 0.01
I1107 12:27:08.348640 11160 solver.cpp:218] Iteration 86300 (11.7279 iter/s, 8.5267s/100 iters), loss = 0.177579
I1107 12:27:08.348640 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:27:08.348640 11160 solver.cpp:237]     Train net output #1: loss = 0.177579 (* 1 = 0.177579 loss)
I1107 12:27:08.348640 11160 sgd_solver.cpp:105] Iteration 86300, lr = 0.01
I1107 12:27:16.876533 11160 solver.cpp:218] Iteration 86400 (11.7259 iter/s, 8.52812s/100 iters), loss = 0.150747
I1107 12:27:16.876533 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:27:16.876533 11160 solver.cpp:237]     Train net output #1: loss = 0.150747 (* 1 = 0.150747 loss)
I1107 12:27:16.876533 11160 sgd_solver.cpp:105] Iteration 86400, lr = 0.01
I1107 12:27:24.984488  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:27:25.321552 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_86500.caffemodel
I1107 12:27:25.351550 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_86500.solverstate
I1107 12:27:25.360551 11160 solver.cpp:330] Iteration 86500, Testing net (#0)
I1107 12:27:25.360551 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:27:27.349704 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:27:27.429708 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8616
I1107 12:27:27.429708 11160 solver.cpp:397]     Test net output #1: loss = 0.450299 (* 1 = 0.450299 loss)
I1107 12:27:27.510710 11160 solver.cpp:218] Iteration 86500 (9.40416 iter/s, 10.6336s/100 iters), loss = 0.182011
I1107 12:27:27.510710 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:27:27.510710 11160 solver.cpp:237]     Train net output #1: loss = 0.182011 (* 1 = 0.182011 loss)
I1107 12:27:27.510710 11160 sgd_solver.cpp:105] Iteration 86500, lr = 0.01
I1107 12:27:36.043545 11160 solver.cpp:218] Iteration 86600 (11.7205 iter/s, 8.53207s/100 iters), loss = 0.178342
I1107 12:27:36.043545 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:27:36.043545 11160 solver.cpp:237]     Train net output #1: loss = 0.178343 (* 1 = 0.178343 loss)
I1107 12:27:36.043545 11160 sgd_solver.cpp:105] Iteration 86600, lr = 0.01
I1107 12:27:44.570477 11160 solver.cpp:218] Iteration 86700 (11.7286 iter/s, 8.52617s/100 iters), loss = 0.117389
I1107 12:27:44.570477 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:27:44.570477 11160 solver.cpp:237]     Train net output #1: loss = 0.117389 (* 1 = 0.117389 loss)
I1107 12:27:44.570477 11160 sgd_solver.cpp:105] Iteration 86700, lr = 0.01
I1107 12:27:53.096303 11160 solver.cpp:218] Iteration 86800 (11.7301 iter/s, 8.5251s/100 iters), loss = 0.163751
I1107 12:27:53.096303 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:27:53.096303 11160 solver.cpp:237]     Train net output #1: loss = 0.163751 (* 1 = 0.163751 loss)
I1107 12:27:53.096303 11160 sgd_solver.cpp:105] Iteration 86800, lr = 0.01
I1107 12:28:01.618595 11160 solver.cpp:218] Iteration 86900 (11.7341 iter/s, 8.52215s/100 iters), loss = 0.233583
I1107 12:28:01.618595 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:28:01.618595 11160 solver.cpp:237]     Train net output #1: loss = 0.233583 (* 1 = 0.233583 loss)
I1107 12:28:01.618595 11160 sgd_solver.cpp:105] Iteration 86900, lr = 0.01
I1107 12:28:09.727452  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:28:10.064471 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_87000.caffemodel
I1107 12:28:10.093472 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_87000.solverstate
I1107 12:28:10.102473 11160 solver.cpp:330] Iteration 87000, Testing net (#0)
I1107 12:28:10.102473 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:28:12.089612 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:28:12.168618 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8679
I1107 12:28:12.168618 11160 solver.cpp:397]     Test net output #1: loss = 0.429513 (* 1 = 0.429513 loss)
I1107 12:28:12.250639 11160 solver.cpp:218] Iteration 87000 (9.4063 iter/s, 10.6312s/100 iters), loss = 0.139563
I1107 12:28:12.250639 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:28:12.250639 11160 solver.cpp:237]     Train net output #1: loss = 0.139563 (* 1 = 0.139563 loss)
I1107 12:28:12.250639 11160 sgd_solver.cpp:105] Iteration 87000, lr = 0.01
I1107 12:28:20.774632 11160 solver.cpp:218] Iteration 87100 (11.7323 iter/s, 8.52349s/100 iters), loss = 0.320579
I1107 12:28:20.774632 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:28:20.774632 11160 solver.cpp:237]     Train net output #1: loss = 0.320579 (* 1 = 0.320579 loss)
I1107 12:28:20.774632 11160 sgd_solver.cpp:105] Iteration 87100, lr = 0.01
I1107 12:28:29.296660 11160 solver.cpp:218] Iteration 87200 (11.7339 iter/s, 8.52229s/100 iters), loss = 0.304128
I1107 12:28:29.296660 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 12:28:29.296660 11160 solver.cpp:237]     Train net output #1: loss = 0.304128 (* 1 = 0.304128 loss)
I1107 12:28:29.296660 11160 sgd_solver.cpp:105] Iteration 87200, lr = 0.01
I1107 12:28:37.821842 11160 solver.cpp:218] Iteration 87300 (11.7317 iter/s, 8.52394s/100 iters), loss = 0.19423
I1107 12:28:37.821842 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 12:28:37.821842 11160 solver.cpp:237]     Train net output #1: loss = 0.19423 (* 1 = 0.19423 loss)
I1107 12:28:37.821842 11160 sgd_solver.cpp:105] Iteration 87300, lr = 0.01
I1107 12:28:46.341948 11160 solver.cpp:218] Iteration 87400 (11.7365 iter/s, 8.52045s/100 iters), loss = 0.128273
I1107 12:28:46.342948 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:28:46.342948 11160 solver.cpp:237]     Train net output #1: loss = 0.128274 (* 1 = 0.128274 loss)
I1107 12:28:46.342948 11160 sgd_solver.cpp:105] Iteration 87400, lr = 0.01
I1107 12:28:54.455906  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:28:54.792919 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_87500.caffemodel
I1107 12:28:54.822423 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_87500.solverstate
I1107 12:28:54.831423 11160 solver.cpp:330] Iteration 87500, Testing net (#0)
I1107 12:28:54.831423 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:28:56.820510 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:28:56.899513 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8476
I1107 12:28:56.899513 11160 solver.cpp:397]     Test net output #1: loss = 0.51408 (* 1 = 0.51408 loss)
I1107 12:28:56.981521 11160 solver.cpp:218] Iteration 87500 (9.39998 iter/s, 10.6383s/100 iters), loss = 0.158072
I1107 12:28:56.981521 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:28:56.981521 11160 solver.cpp:237]     Train net output #1: loss = 0.158072 (* 1 = 0.158072 loss)
I1107 12:28:56.981521 11160 sgd_solver.cpp:105] Iteration 87500, lr = 0.01
I1107 12:29:05.517621 11160 solver.cpp:218] Iteration 87600 (11.715 iter/s, 8.53605s/100 iters), loss = 0.21584
I1107 12:29:05.517621 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:29:05.517621 11160 solver.cpp:237]     Train net output #1: loss = 0.215841 (* 1 = 0.215841 loss)
I1107 12:29:05.517621 11160 sgd_solver.cpp:105] Iteration 87600, lr = 0.01
I1107 12:29:14.042086 11160 solver.cpp:218] Iteration 87700 (11.7315 iter/s, 8.52403s/100 iters), loss = 0.221587
I1107 12:29:14.042086 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:29:14.042086 11160 solver.cpp:237]     Train net output #1: loss = 0.221587 (* 1 = 0.221587 loss)
I1107 12:29:14.042086 11160 sgd_solver.cpp:105] Iteration 87700, lr = 0.01
I1107 12:29:22.575484 11160 solver.cpp:218] Iteration 87800 (11.7205 iter/s, 8.53207s/100 iters), loss = 0.228674
I1107 12:29:22.575484 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:29:22.575484 11160 solver.cpp:237]     Train net output #1: loss = 0.228674 (* 1 = 0.228674 loss)
I1107 12:29:22.575484 11160 sgd_solver.cpp:105] Iteration 87800, lr = 0.01
I1107 12:29:31.105109 11160 solver.cpp:218] Iteration 87900 (11.7245 iter/s, 8.52916s/100 iters), loss = 0.152872
I1107 12:29:31.105109 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:29:31.105109 11160 solver.cpp:237]     Train net output #1: loss = 0.152872 (* 1 = 0.152872 loss)
I1107 12:29:31.105109 11160 sgd_solver.cpp:105] Iteration 87900, lr = 0.01
I1107 12:29:39.214380  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:29:39.551426 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_88000.caffemodel
I1107 12:29:39.581426 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_88000.solverstate
I1107 12:29:39.591426 11160 solver.cpp:330] Iteration 88000, Testing net (#0)
I1107 12:29:39.591426 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:29:41.579602 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:29:41.659606 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8381
I1107 12:29:41.659606 11160 solver.cpp:397]     Test net output #1: loss = 0.54765 (* 1 = 0.54765 loss)
I1107 12:29:41.741111 11160 solver.cpp:218] Iteration 88000 (9.40227 iter/s, 10.6357s/100 iters), loss = 0.167387
I1107 12:29:41.741111 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:29:41.741111 11160 solver.cpp:237]     Train net output #1: loss = 0.167387 (* 1 = 0.167387 loss)
I1107 12:29:41.741111 11160 sgd_solver.cpp:105] Iteration 88000, lr = 0.01
I1107 12:29:50.260450 11160 solver.cpp:218] Iteration 88100 (11.7391 iter/s, 8.51855s/100 iters), loss = 0.202202
I1107 12:29:50.260450 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:29:50.260450 11160 solver.cpp:237]     Train net output #1: loss = 0.202202 (* 1 = 0.202202 loss)
I1107 12:29:50.260450 11160 sgd_solver.cpp:105] Iteration 88100, lr = 0.01
I1107 12:29:58.789031 11160 solver.cpp:218] Iteration 88200 (11.7258 iter/s, 8.52818s/100 iters), loss = 0.203951
I1107 12:29:58.789031 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:29:58.789031 11160 solver.cpp:237]     Train net output #1: loss = 0.203951 (* 1 = 0.203951 loss)
I1107 12:29:58.789031 11160 sgd_solver.cpp:105] Iteration 88200, lr = 0.01
I1107 12:30:07.363214 11160 solver.cpp:218] Iteration 88300 (11.6629 iter/s, 8.57423s/100 iters), loss = 0.200716
I1107 12:30:07.363214 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:30:07.363214 11160 solver.cpp:237]     Train net output #1: loss = 0.200716 (* 1 = 0.200716 loss)
I1107 12:30:07.363214 11160 sgd_solver.cpp:105] Iteration 88300, lr = 0.01
I1107 12:30:15.893321 11160 solver.cpp:218] Iteration 88400 (11.7249 iter/s, 8.52887s/100 iters), loss = 0.188925
I1107 12:30:15.893321 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:30:15.893321 11160 solver.cpp:237]     Train net output #1: loss = 0.188925 (* 1 = 0.188925 loss)
I1107 12:30:15.893321 11160 sgd_solver.cpp:105] Iteration 88400, lr = 0.01
I1107 12:30:24.004637  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:30:24.341715 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_88500.caffemodel
I1107 12:30:24.371752 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_88500.solverstate
I1107 12:30:24.380756 11160 solver.cpp:330] Iteration 88500, Testing net (#0)
I1107 12:30:24.380756 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:30:26.372396 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:30:26.452906 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8457
I1107 12:30:26.452906 11160 solver.cpp:397]     Test net output #1: loss = 0.521816 (* 1 = 0.521816 loss)
I1107 12:30:26.533426 11160 solver.cpp:218] Iteration 88500 (9.39822 iter/s, 10.6403s/100 iters), loss = 0.142095
I1107 12:30:26.533426 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:30:26.533426 11160 solver.cpp:237]     Train net output #1: loss = 0.142095 (* 1 = 0.142095 loss)
I1107 12:30:26.533426 11160 sgd_solver.cpp:105] Iteration 88500, lr = 0.01
I1107 12:30:35.051564 11160 solver.cpp:218] Iteration 88600 (11.7414 iter/s, 8.51685s/100 iters), loss = 0.164893
I1107 12:30:35.051564 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:30:35.051564 11160 solver.cpp:237]     Train net output #1: loss = 0.164893 (* 1 = 0.164893 loss)
I1107 12:30:35.051564 11160 sgd_solver.cpp:105] Iteration 88600, lr = 0.01
I1107 12:30:43.560199 11160 solver.cpp:218] Iteration 88700 (11.7533 iter/s, 8.50824s/100 iters), loss = 0.180837
I1107 12:30:43.560199 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:30:43.560199 11160 solver.cpp:237]     Train net output #1: loss = 0.180837 (* 1 = 0.180837 loss)
I1107 12:30:43.560199 11160 sgd_solver.cpp:105] Iteration 88700, lr = 0.01
I1107 12:30:52.077620 11160 solver.cpp:218] Iteration 88800 (11.7413 iter/s, 8.51694s/100 iters), loss = 0.135674
I1107 12:30:52.077620 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:30:52.077620 11160 solver.cpp:237]     Train net output #1: loss = 0.135674 (* 1 = 0.135674 loss)
I1107 12:30:52.077620 11160 sgd_solver.cpp:105] Iteration 88800, lr = 0.01
I1107 12:31:00.601260 11160 solver.cpp:218] Iteration 88900 (11.7322 iter/s, 8.52355s/100 iters), loss = 0.206661
I1107 12:31:00.601260 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:31:00.601260 11160 solver.cpp:237]     Train net output #1: loss = 0.206661 (* 1 = 0.206661 loss)
I1107 12:31:00.601260 11160 sgd_solver.cpp:105] Iteration 88900, lr = 0.01
I1107 12:31:08.706439  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:31:09.043453 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_89000.caffemodel
I1107 12:31:09.073460 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_89000.solverstate
I1107 12:31:09.081460 11160 solver.cpp:330] Iteration 89000, Testing net (#0)
I1107 12:31:09.082461 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:31:11.070616 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:31:11.149619 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8594
I1107 12:31:11.149619 11160 solver.cpp:397]     Test net output #1: loss = 0.467482 (* 1 = 0.467482 loss)
I1107 12:31:11.231626 11160 solver.cpp:218] Iteration 89000 (9.40755 iter/s, 10.6298s/100 iters), loss = 0.177216
I1107 12:31:11.231626 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:31:11.231626 11160 solver.cpp:237]     Train net output #1: loss = 0.177217 (* 1 = 0.177217 loss)
I1107 12:31:11.231626 11160 sgd_solver.cpp:105] Iteration 89000, lr = 0.01
I1107 12:31:19.764111 11160 solver.cpp:218] Iteration 89100 (11.7212 iter/s, 8.53153s/100 iters), loss = 0.211001
I1107 12:31:19.764111 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:31:19.764111 11160 solver.cpp:237]     Train net output #1: loss = 0.211001 (* 1 = 0.211001 loss)
I1107 12:31:19.764111 11160 sgd_solver.cpp:105] Iteration 89100, lr = 0.01
I1107 12:31:28.287502 11160 solver.cpp:218] Iteration 89200 (11.7322 iter/s, 8.52359s/100 iters), loss = 0.195491
I1107 12:31:28.287502 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:31:28.287502 11160 solver.cpp:237]     Train net output #1: loss = 0.195492 (* 1 = 0.195492 loss)
I1107 12:31:28.287502 11160 sgd_solver.cpp:105] Iteration 89200, lr = 0.01
I1107 12:31:36.806524 11160 solver.cpp:218] Iteration 89300 (11.7399 iter/s, 8.51793s/100 iters), loss = 0.154939
I1107 12:31:36.806524 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:31:36.806524 11160 solver.cpp:237]     Train net output #1: loss = 0.15494 (* 1 = 0.15494 loss)
I1107 12:31:36.806524 11160 sgd_solver.cpp:105] Iteration 89300, lr = 0.01
I1107 12:31:45.326509 11160 solver.cpp:218] Iteration 89400 (11.7368 iter/s, 8.52018s/100 iters), loss = 0.186674
I1107 12:31:45.327509 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:31:45.327509 11160 solver.cpp:237]     Train net output #1: loss = 0.186674 (* 1 = 0.186674 loss)
I1107 12:31:45.327509 11160 sgd_solver.cpp:105] Iteration 89400, lr = 0.01
I1107 12:31:53.448551  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:31:53.785573 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_89500.caffemodel
I1107 12:31:53.818572 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_89500.solverstate
I1107 12:31:53.827572 11160 solver.cpp:330] Iteration 89500, Testing net (#0)
I1107 12:31:53.827572 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:31:55.816747 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:31:55.895754 11160 solver.cpp:397]     Test net output #0: accuracy = 0.861
I1107 12:31:55.895754 11160 solver.cpp:397]     Test net output #1: loss = 0.477293 (* 1 = 0.477293 loss)
I1107 12:31:55.976758 11160 solver.cpp:218] Iteration 89500 (9.3906 iter/s, 10.6489s/100 iters), loss = 0.149197
I1107 12:31:55.976758 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:31:55.976758 11160 solver.cpp:237]     Train net output #1: loss = 0.149197 (* 1 = 0.149197 loss)
I1107 12:31:55.976758 11160 sgd_solver.cpp:105] Iteration 89500, lr = 0.01
I1107 12:32:04.506772 11160 solver.cpp:218] Iteration 89600 (11.7237 iter/s, 8.52973s/100 iters), loss = 0.148277
I1107 12:32:04.506772 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:32:04.506772 11160 solver.cpp:237]     Train net output #1: loss = 0.148278 (* 1 = 0.148278 loss)
I1107 12:32:04.506772 11160 sgd_solver.cpp:105] Iteration 89600, lr = 0.01
I1107 12:32:13.040040 11160 solver.cpp:218] Iteration 89700 (11.719 iter/s, 8.53313s/100 iters), loss = 0.134607
I1107 12:32:13.040040 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:32:13.040040 11160 solver.cpp:237]     Train net output #1: loss = 0.134607 (* 1 = 0.134607 loss)
I1107 12:32:13.040040 11160 sgd_solver.cpp:105] Iteration 89700, lr = 0.01
I1107 12:32:21.585196 11160 solver.cpp:218] Iteration 89800 (11.7031 iter/s, 8.54474s/100 iters), loss = 0.194768
I1107 12:32:21.586199 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:32:21.586199 11160 solver.cpp:237]     Train net output #1: loss = 0.194768 (* 1 = 0.194768 loss)
I1107 12:32:21.586199 11160 sgd_solver.cpp:105] Iteration 89800, lr = 0.01
I1107 12:32:30.127590 11160 solver.cpp:218] Iteration 89900 (11.7082 iter/s, 8.54105s/100 iters), loss = 0.161501
I1107 12:32:30.127590 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:32:30.127590 11160 solver.cpp:237]     Train net output #1: loss = 0.161501 (* 1 = 0.161501 loss)
I1107 12:32:30.127590 11160 sgd_solver.cpp:105] Iteration 89900, lr = 0.01
I1107 12:32:38.244369  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:32:38.582887 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_90000.caffemodel
I1107 12:32:38.612391 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_90000.solverstate
I1107 12:32:38.621392 11160 solver.cpp:330] Iteration 90000, Testing net (#0)
I1107 12:32:38.622393 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:32:40.611572 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:32:40.691581 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8211
I1107 12:32:40.691581 11160 solver.cpp:397]     Test net output #1: loss = 0.614796 (* 1 = 0.614796 loss)
I1107 12:32:40.773583 11160 solver.cpp:218] Iteration 90000 (9.39342 iter/s, 10.6457s/100 iters), loss = 0.207794
I1107 12:32:40.773583 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:32:40.773583 11160 solver.cpp:237]     Train net output #1: loss = 0.207795 (* 1 = 0.207795 loss)
I1107 12:32:40.774083 11160 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1107 12:32:49.318507 11160 solver.cpp:218] Iteration 90100 (11.7037 iter/s, 8.54428s/100 iters), loss = 0.149805
I1107 12:32:49.318507 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:32:49.318507 11160 solver.cpp:237]     Train net output #1: loss = 0.149805 (* 1 = 0.149805 loss)
I1107 12:32:49.318507 11160 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1107 12:32:57.845692 11160 solver.cpp:218] Iteration 90200 (11.7273 iter/s, 8.52711s/100 iters), loss = 0.23054
I1107 12:32:57.845692 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:32:57.845692 11160 solver.cpp:237]     Train net output #1: loss = 0.23054 (* 1 = 0.23054 loss)
I1107 12:32:57.845692 11160 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1107 12:33:06.365020 11160 solver.cpp:218] Iteration 90300 (11.7386 iter/s, 8.51892s/100 iters), loss = 0.188691
I1107 12:33:06.366024 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:33:06.366024 11160 solver.cpp:237]     Train net output #1: loss = 0.188691 (* 1 = 0.188691 loss)
I1107 12:33:06.366024 11160 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1107 12:33:14.887125 11160 solver.cpp:218] Iteration 90400 (11.7359 iter/s, 8.52087s/100 iters), loss = 0.21259
I1107 12:33:14.887125 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:33:14.887125 11160 solver.cpp:237]     Train net output #1: loss = 0.212591 (* 1 = 0.212591 loss)
I1107 12:33:14.887125 11160 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1107 12:33:22.993882  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:33:23.329916 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_90500.caffemodel
I1107 12:33:23.358917 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_90500.solverstate
I1107 12:33:23.368918 11160 solver.cpp:330] Iteration 90500, Testing net (#0)
I1107 12:33:23.368918 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:33:25.357115 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:33:25.436126 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8479
I1107 12:33:25.436126 11160 solver.cpp:397]     Test net output #1: loss = 0.508814 (* 1 = 0.508814 loss)
I1107 12:33:25.518133 11160 solver.cpp:218] Iteration 90500 (9.40677 iter/s, 10.6306s/100 iters), loss = 0.202221
I1107 12:33:25.518133 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:33:25.518133 11160 solver.cpp:237]     Train net output #1: loss = 0.202222 (* 1 = 0.202222 loss)
I1107 12:33:25.518133 11160 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1107 12:33:34.040678 11160 solver.cpp:218] Iteration 90600 (11.7352 iter/s, 8.52137s/100 iters), loss = 0.244164
I1107 12:33:34.040678 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:33:34.040678 11160 solver.cpp:237]     Train net output #1: loss = 0.244164 (* 1 = 0.244164 loss)
I1107 12:33:34.040678 11160 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1107 12:33:42.567513 11160 solver.cpp:218] Iteration 90700 (11.7274 iter/s, 8.52706s/100 iters), loss = 0.132866
I1107 12:33:42.567513 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:33:42.567513 11160 solver.cpp:237]     Train net output #1: loss = 0.132866 (* 1 = 0.132866 loss)
I1107 12:33:42.567513 11160 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1107 12:33:51.093480 11160 solver.cpp:218] Iteration 90800 (11.7309 iter/s, 8.52452s/100 iters), loss = 0.180176
I1107 12:33:51.093480 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:33:51.093480 11160 solver.cpp:237]     Train net output #1: loss = 0.180176 (* 1 = 0.180176 loss)
I1107 12:33:51.093480 11160 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1107 12:33:59.612741 11160 solver.cpp:218] Iteration 90900 (11.7382 iter/s, 8.51922s/100 iters), loss = 0.186042
I1107 12:33:59.612741 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:33:59.612741 11160 solver.cpp:237]     Train net output #1: loss = 0.186043 (* 1 = 0.186043 loss)
I1107 12:33:59.612741 11160 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1107 12:34:07.711455  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:34:08.047472 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_91000.caffemodel
I1107 12:34:08.075472 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_91000.solverstate
I1107 12:34:08.083977 11160 solver.cpp:330] Iteration 91000, Testing net (#0)
I1107 12:34:08.083977 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:34:10.071655 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:34:10.151659 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8568
I1107 12:34:10.151659 11160 solver.cpp:397]     Test net output #1: loss = 0.487118 (* 1 = 0.487118 loss)
I1107 12:34:10.233662 11160 solver.cpp:218] Iteration 91000 (9.41603 iter/s, 10.6202s/100 iters), loss = 0.17861
I1107 12:34:10.233662 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:34:10.233662 11160 solver.cpp:237]     Train net output #1: loss = 0.178611 (* 1 = 0.178611 loss)
I1107 12:34:10.233662 11160 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1107 12:34:18.750025 11160 solver.cpp:218] Iteration 91100 (11.7419 iter/s, 8.51648s/100 iters), loss = 0.217065
I1107 12:34:18.750025 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:34:18.750025 11160 solver.cpp:237]     Train net output #1: loss = 0.217065 (* 1 = 0.217065 loss)
I1107 12:34:18.750025 11160 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1107 12:34:27.272934 11160 solver.cpp:218] Iteration 91200 (11.7342 iter/s, 8.52208s/100 iters), loss = 0.20154
I1107 12:34:27.272934 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:34:27.272934 11160 solver.cpp:237]     Train net output #1: loss = 0.201541 (* 1 = 0.201541 loss)
I1107 12:34:27.272934 11160 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1107 12:34:35.790339 11160 solver.cpp:218] Iteration 91300 (11.7412 iter/s, 8.51701s/100 iters), loss = 0.187373
I1107 12:34:35.790339 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:34:35.790339 11160 solver.cpp:237]     Train net output #1: loss = 0.187373 (* 1 = 0.187373 loss)
I1107 12:34:35.790339 11160 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1107 12:34:44.315047 11160 solver.cpp:218] Iteration 91400 (11.7318 iter/s, 8.52384s/100 iters), loss = 0.160555
I1107 12:34:44.315047 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:34:44.315047 11160 solver.cpp:237]     Train net output #1: loss = 0.160555 (* 1 = 0.160555 loss)
I1107 12:34:44.315047 11160 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1107 12:34:52.415287  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:34:52.752321 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_91500.caffemodel
I1107 12:34:52.781324 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_91500.solverstate
I1107 12:34:52.790324 11160 solver.cpp:330] Iteration 91500, Testing net (#0)
I1107 12:34:52.790324 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:34:54.778435 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:34:54.857441 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8308
I1107 12:34:54.857441 11160 solver.cpp:397]     Test net output #1: loss = 0.58785 (* 1 = 0.58785 loss)
I1107 12:34:54.939441 11160 solver.cpp:218] Iteration 91500 (9.41291 iter/s, 10.6237s/100 iters), loss = 0.173434
I1107 12:34:54.939441 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:34:54.939441 11160 solver.cpp:237]     Train net output #1: loss = 0.173434 (* 1 = 0.173434 loss)
I1107 12:34:54.939441 11160 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1107 12:35:03.476107 11160 solver.cpp:218] Iteration 91600 (11.7141 iter/s, 8.53674s/100 iters), loss = 0.302379
I1107 12:35:03.476107 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:35:03.476107 11160 solver.cpp:237]     Train net output #1: loss = 0.302379 (* 1 = 0.302379 loss)
I1107 12:35:03.476107 11160 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1107 12:35:12.005118 11160 solver.cpp:218] Iteration 91700 (11.7256 iter/s, 8.52833s/100 iters), loss = 0.181431
I1107 12:35:12.005118 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:35:12.005118 11160 solver.cpp:237]     Train net output #1: loss = 0.181431 (* 1 = 0.181431 loss)
I1107 12:35:12.005118 11160 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1107 12:35:20.535583 11160 solver.cpp:218] Iteration 91800 (11.7238 iter/s, 8.52966s/100 iters), loss = 0.181327
I1107 12:35:20.535583 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:35:20.535583 11160 solver.cpp:237]     Train net output #1: loss = 0.181327 (* 1 = 0.181327 loss)
I1107 12:35:20.535583 11160 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1107 12:35:29.061175 11160 solver.cpp:218] Iteration 91900 (11.7302 iter/s, 8.52503s/100 iters), loss = 0.190216
I1107 12:35:29.061175 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:35:29.061175 11160 solver.cpp:237]     Train net output #1: loss = 0.190216 (* 1 = 0.190216 loss)
I1107 12:35:29.061175 11160 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1107 12:35:37.165971  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:35:37.502987 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_92000.caffemodel
I1107 12:35:37.530987 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_92000.solverstate
I1107 12:35:37.539991 11160 solver.cpp:330] Iteration 92000, Testing net (#0)
I1107 12:35:37.540493 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:35:39.528122 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:35:39.608177 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8727
I1107 12:35:39.608177 11160 solver.cpp:397]     Test net output #1: loss = 0.420195 (* 1 = 0.420195 loss)
I1107 12:35:39.689177 11160 solver.cpp:218] Iteration 92000 (9.40898 iter/s, 10.6281s/100 iters), loss = 0.154203
I1107 12:35:39.690178 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:35:39.690178 11160 solver.cpp:237]     Train net output #1: loss = 0.154204 (* 1 = 0.154204 loss)
I1107 12:35:39.690178 11160 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1107 12:35:48.220988 11160 solver.cpp:218] Iteration 92100 (11.7226 iter/s, 8.53053s/100 iters), loss = 0.154686
I1107 12:35:48.220988 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:35:48.220988 11160 solver.cpp:237]     Train net output #1: loss = 0.154686 (* 1 = 0.154686 loss)
I1107 12:35:48.220988 11160 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1107 12:35:56.742182 11160 solver.cpp:218] Iteration 92200 (11.7365 iter/s, 8.52046s/100 iters), loss = 0.1519
I1107 12:35:56.742182 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:35:56.742182 11160 solver.cpp:237]     Train net output #1: loss = 0.1519 (* 1 = 0.1519 loss)
I1107 12:35:56.742182 11160 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1107 12:36:05.269068 11160 solver.cpp:218] Iteration 92300 (11.7284 iter/s, 8.52634s/100 iters), loss = 0.208074
I1107 12:36:05.269068 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:36:05.269068 11160 solver.cpp:237]     Train net output #1: loss = 0.208075 (* 1 = 0.208075 loss)
I1107 12:36:05.269068 11160 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1107 12:36:13.796213 11160 solver.cpp:218] Iteration 92400 (11.7283 iter/s, 8.52642s/100 iters), loss = 0.153189
I1107 12:36:13.796213 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:36:13.796213 11160 solver.cpp:237]     Train net output #1: loss = 0.153189 (* 1 = 0.153189 loss)
I1107 12:36:13.796213 11160 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1107 12:36:21.897786  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:36:22.234805 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_92500.caffemodel
I1107 12:36:22.265815 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_92500.solverstate
I1107 12:36:22.273814 11160 solver.cpp:330] Iteration 92500, Testing net (#0)
I1107 12:36:22.273814 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:36:24.261947 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:36:24.341966 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8599
I1107 12:36:24.341966 11160 solver.cpp:397]     Test net output #1: loss = 0.451641 (* 1 = 0.451641 loss)
I1107 12:36:24.422986 11160 solver.cpp:218] Iteration 92500 (9.41001 iter/s, 10.627s/100 iters), loss = 0.173041
I1107 12:36:24.422986 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:36:24.422986 11160 solver.cpp:237]     Train net output #1: loss = 0.173042 (* 1 = 0.173042 loss)
I1107 12:36:24.422986 11160 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1107 12:36:32.952980 11160 solver.cpp:218] Iteration 92600 (11.7243 iter/s, 8.52932s/100 iters), loss = 0.23354
I1107 12:36:32.952980 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:36:32.952980 11160 solver.cpp:237]     Train net output #1: loss = 0.23354 (* 1 = 0.23354 loss)
I1107 12:36:32.952980 11160 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1107 12:36:41.475822 11160 solver.cpp:218] Iteration 92700 (11.7342 iter/s, 8.52208s/100 iters), loss = 0.170798
I1107 12:36:41.475822 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:36:41.475822 11160 solver.cpp:237]     Train net output #1: loss = 0.170798 (* 1 = 0.170798 loss)
I1107 12:36:41.475822 11160 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1107 12:36:50.040621 11160 solver.cpp:218] Iteration 92800 (11.6768 iter/s, 8.56401s/100 iters), loss = 0.178636
I1107 12:36:50.040621 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:36:50.040621 11160 solver.cpp:237]     Train net output #1: loss = 0.178636 (* 1 = 0.178636 loss)
I1107 12:36:50.040621 11160 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1107 12:36:58.544620 11160 solver.cpp:218] Iteration 92900 (11.7596 iter/s, 8.50371s/100 iters), loss = 0.196275
I1107 12:36:58.544620 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:36:58.544620 11160 solver.cpp:237]     Train net output #1: loss = 0.196275 (* 1 = 0.196275 loss)
I1107 12:36:58.544620 11160 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1107 12:37:06.635395  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:37:06.972416 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_93000.caffemodel
I1107 12:37:07.001437 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_93000.solverstate
I1107 12:37:07.010438 11160 solver.cpp:330] Iteration 93000, Testing net (#0)
I1107 12:37:07.010438 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:37:08.992561 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:37:09.071559 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8381
I1107 12:37:09.071559 11160 solver.cpp:397]     Test net output #1: loss = 0.54542 (* 1 = 0.54542 loss)
I1107 12:37:09.152566 11160 solver.cpp:218] Iteration 93000 (9.42702 iter/s, 10.6078s/100 iters), loss = 0.19245
I1107 12:37:09.152566 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:37:09.152566 11160 solver.cpp:237]     Train net output #1: loss = 0.19245 (* 1 = 0.19245 loss)
I1107 12:37:09.152566 11160 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1107 12:37:17.655402 11160 solver.cpp:218] Iteration 93100 (11.7618 iter/s, 8.50212s/100 iters), loss = 0.216791
I1107 12:37:17.655402 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:37:17.655402 11160 solver.cpp:237]     Train net output #1: loss = 0.216791 (* 1 = 0.216791 loss)
I1107 12:37:17.655402 11160 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1107 12:37:26.154251 11160 solver.cpp:218] Iteration 93200 (11.7668 iter/s, 8.49845s/100 iters), loss = 0.202839
I1107 12:37:26.154251 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:37:26.154251 11160 solver.cpp:237]     Train net output #1: loss = 0.202839 (* 1 = 0.202839 loss)
I1107 12:37:26.154251 11160 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1107 12:37:34.650341 11160 solver.cpp:218] Iteration 93300 (11.7707 iter/s, 8.49567s/100 iters), loss = 0.237613
I1107 12:37:34.651341 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:37:34.651341 11160 solver.cpp:237]     Train net output #1: loss = 0.237613 (* 1 = 0.237613 loss)
I1107 12:37:34.651341 11160 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1107 12:37:43.159847 11160 solver.cpp:218] Iteration 93400 (11.7535 iter/s, 8.50811s/100 iters), loss = 0.0960681
I1107 12:37:43.159847 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:37:43.159847 11160 solver.cpp:237]     Train net output #1: loss = 0.0960683 (* 1 = 0.0960683 loss)
I1107 12:37:43.159847 11160 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1107 12:37:51.248595  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:37:51.585145 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_93500.caffemodel
I1107 12:37:51.614650 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_93500.solverstate
I1107 12:37:51.623649 11160 solver.cpp:330] Iteration 93500, Testing net (#0)
I1107 12:37:51.623649 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:37:53.607167 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:37:53.686669 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8593
I1107 12:37:53.686669 11160 solver.cpp:397]     Test net output #1: loss = 0.468618 (* 1 = 0.468618 loss)
I1107 12:37:53.767172 11160 solver.cpp:218] Iteration 93500 (9.42752 iter/s, 10.6072s/100 iters), loss = 0.124991
I1107 12:37:53.767172 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:37:53.767172 11160 solver.cpp:237]     Train net output #1: loss = 0.124991 (* 1 = 0.124991 loss)
I1107 12:37:53.767172 11160 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1107 12:38:02.275277 11160 solver.cpp:218] Iteration 93600 (11.7545 iter/s, 8.50739s/100 iters), loss = 0.192334
I1107 12:38:02.275277 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:38:02.275277 11160 solver.cpp:237]     Train net output #1: loss = 0.192334 (* 1 = 0.192334 loss)
I1107 12:38:02.275277 11160 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1107 12:38:10.782186 11160 solver.cpp:218] Iteration 93700 (11.7564 iter/s, 8.50599s/100 iters), loss = 0.254011
I1107 12:38:10.782186 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 12:38:10.782186 11160 solver.cpp:237]     Train net output #1: loss = 0.254011 (* 1 = 0.254011 loss)
I1107 12:38:10.782186 11160 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1107 12:38:19.290380 11160 solver.cpp:218] Iteration 93800 (11.7529 iter/s, 8.50854s/100 iters), loss = 0.225481
I1107 12:38:19.290380 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:38:19.291379 11160 solver.cpp:237]     Train net output #1: loss = 0.225481 (* 1 = 0.225481 loss)
I1107 12:38:19.291379 11160 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1107 12:38:27.790506 11160 solver.cpp:218] Iteration 93900 (11.7662 iter/s, 8.49892s/100 iters), loss = 0.154256
I1107 12:38:27.790506 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:38:27.790506 11160 solver.cpp:237]     Train net output #1: loss = 0.154256 (* 1 = 0.154256 loss)
I1107 12:38:27.790506 11160 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1107 12:38:35.876265  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:38:36.212347 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_94000.caffemodel
I1107 12:38:36.241358 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_94000.solverstate
I1107 12:38:36.250370 11160 solver.cpp:330] Iteration 94000, Testing net (#0)
I1107 12:38:36.250370 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:38:38.232650 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:38:38.312661 11160 solver.cpp:397]     Test net output #0: accuracy = 0.859
I1107 12:38:38.312661 11160 solver.cpp:397]     Test net output #1: loss = 0.464756 (* 1 = 0.464756 loss)
I1107 12:38:38.392666 11160 solver.cpp:218] Iteration 94000 (9.4321 iter/s, 10.6021s/100 iters), loss = 0.176176
I1107 12:38:38.392666 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:38:38.392666 11160 solver.cpp:237]     Train net output #1: loss = 0.176176 (* 1 = 0.176176 loss)
I1107 12:38:38.392666 11160 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1107 12:38:46.908798 11160 solver.cpp:218] Iteration 94100 (11.7441 iter/s, 8.51495s/100 iters), loss = 0.20072
I1107 12:38:46.908798 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 12:38:46.908798 11160 solver.cpp:237]     Train net output #1: loss = 0.200721 (* 1 = 0.200721 loss)
I1107 12:38:46.908798 11160 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1107 12:38:55.411020 11160 solver.cpp:218] Iteration 94200 (11.7612 iter/s, 8.50254s/100 iters), loss = 0.186035
I1107 12:38:55.411020 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:38:55.411020 11160 solver.cpp:237]     Train net output #1: loss = 0.186035 (* 1 = 0.186035 loss)
I1107 12:38:55.411020 11160 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1107 12:39:03.923069 11160 solver.cpp:218] Iteration 94300 (11.7487 iter/s, 8.51161s/100 iters), loss = 0.185848
I1107 12:39:03.923069 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:39:03.924069 11160 solver.cpp:237]     Train net output #1: loss = 0.185848 (* 1 = 0.185848 loss)
I1107 12:39:03.924069 11160 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1107 12:39:12.436424 11160 solver.cpp:218] Iteration 94400 (11.748 iter/s, 8.51206s/100 iters), loss = 0.215097
I1107 12:39:12.436424 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:39:12.436424 11160 solver.cpp:237]     Train net output #1: loss = 0.215097 (* 1 = 0.215097 loss)
I1107 12:39:12.436424 11160 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1107 12:39:20.522306  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:39:20.857328 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_94500.caffemodel
I1107 12:39:20.888329 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_94500.solverstate
I1107 12:39:20.897330 11160 solver.cpp:330] Iteration 94500, Testing net (#0)
I1107 12:39:20.897330 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:39:22.879499 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:39:22.959532 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8482
I1107 12:39:22.959532 11160 solver.cpp:397]     Test net output #1: loss = 0.513998 (* 1 = 0.513998 loss)
I1107 12:39:23.040522 11160 solver.cpp:218] Iteration 94500 (9.43056 iter/s, 10.6038s/100 iters), loss = 0.251009
I1107 12:39:23.040522 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:39:23.040522 11160 solver.cpp:237]     Train net output #1: loss = 0.251009 (* 1 = 0.251009 loss)
I1107 12:39:23.040522 11160 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1107 12:39:31.538882 11160 solver.cpp:218] Iteration 94600 (11.7676 iter/s, 8.49792s/100 iters), loss = 0.180335
I1107 12:39:31.538882 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:39:31.539383 11160 solver.cpp:237]     Train net output #1: loss = 0.180335 (* 1 = 0.180335 loss)
I1107 12:39:31.539383 11160 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1107 12:39:40.039203 11160 solver.cpp:218] Iteration 94700 (11.7654 iter/s, 8.49952s/100 iters), loss = 0.272429
I1107 12:39:40.039203 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 12:39:40.039203 11160 solver.cpp:237]     Train net output #1: loss = 0.27243 (* 1 = 0.27243 loss)
I1107 12:39:40.039203 11160 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1107 12:39:48.541527 11160 solver.cpp:218] Iteration 94800 (11.762 iter/s, 8.50197s/100 iters), loss = 0.221542
I1107 12:39:48.541527 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:39:48.541527 11160 solver.cpp:237]     Train net output #1: loss = 0.221542 (* 1 = 0.221542 loss)
I1107 12:39:48.541527 11160 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1107 12:39:57.046330 11160 solver.cpp:218] Iteration 94900 (11.7592 iter/s, 8.504s/100 iters), loss = 0.140742
I1107 12:39:57.046330 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:39:57.046330 11160 solver.cpp:237]     Train net output #1: loss = 0.140742 (* 1 = 0.140742 loss)
I1107 12:39:57.046330 11160 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1107 12:40:05.131572  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:40:05.468588 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_95000.caffemodel
I1107 12:40:05.496588 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_95000.solverstate
I1107 12:40:05.504587 11160 solver.cpp:330] Iteration 95000, Testing net (#0)
I1107 12:40:05.505590 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:40:07.487108 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:40:07.567113 11160 solver.cpp:397]     Test net output #0: accuracy = 0.8405
I1107 12:40:07.567113 11160 solver.cpp:397]     Test net output #1: loss = 0.559356 (* 1 = 0.559356 loss)
I1107 12:40:07.648119 11160 solver.cpp:218] Iteration 95000 (9.43263 iter/s, 10.6015s/100 iters), loss = 0.182189
I1107 12:40:07.648119 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:40:07.648119 11160 solver.cpp:237]     Train net output #1: loss = 0.182189 (* 1 = 0.182189 loss)
I1107 12:40:07.648119 11160 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1107 12:40:07.648119 11160 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1107 12:40:16.154027 11160 solver.cpp:218] Iteration 95100 (11.7576 iter/s, 8.50511s/100 iters), loss = 0.256964
I1107 12:40:16.154027 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 12:40:16.154027 11160 solver.cpp:237]     Train net output #1: loss = 0.256964 (* 1 = 0.256964 loss)
I1107 12:40:16.154027 11160 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1107 12:40:24.657263 11160 solver.cpp:218] Iteration 95200 (11.7609 iter/s, 8.50275s/100 iters), loss = 0.115416
I1107 12:40:24.657263 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:40:24.657263 11160 solver.cpp:237]     Train net output #1: loss = 0.115416 (* 1 = 0.115416 loss)
I1107 12:40:24.657263 11160 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1107 12:40:33.161211 11160 solver.cpp:218] Iteration 95300 (11.76 iter/s, 8.50337s/100 iters), loss = 0.117388
I1107 12:40:33.161211 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:40:33.161211 11160 solver.cpp:237]     Train net output #1: loss = 0.117388 (* 1 = 0.117388 loss)
I1107 12:40:33.161211 11160 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1107 12:40:41.666563 11160 solver.cpp:218] Iteration 95400 (11.7579 iter/s, 8.50495s/100 iters), loss = 0.102394
I1107 12:40:41.666563 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:40:41.666563 11160 solver.cpp:237]     Train net output #1: loss = 0.102394 (* 1 = 0.102394 loss)
I1107 12:40:41.666563 11160 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1107 12:40:49.770615  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:40:50.107739 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_95500.caffemodel
I1107 12:40:50.135294 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_95500.solverstate
I1107 12:40:50.165562 11160 solver.cpp:330] Iteration 95500, Testing net (#0)
I1107 12:40:50.166548 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:40:52.150522 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:40:52.230037 11160 solver.cpp:397]     Test net output #0: accuracy = 0.917
I1107 12:40:52.230037 11160 solver.cpp:397]     Test net output #1: loss = 0.283052 (* 1 = 0.283052 loss)
I1107 12:40:52.311127 11160 solver.cpp:218] Iteration 95500 (9.39443 iter/s, 10.6446s/100 iters), loss = 0.20944
I1107 12:40:52.311127 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:40:52.312124 11160 solver.cpp:237]     Train net output #1: loss = 0.209441 (* 1 = 0.209441 loss)
I1107 12:40:52.312124 11160 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1107 12:41:00.819376 11160 solver.cpp:218] Iteration 95600 (11.755 iter/s, 8.50704s/100 iters), loss = 0.132938
I1107 12:41:00.819376 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:41:00.819376 11160 solver.cpp:237]     Train net output #1: loss = 0.132938 (* 1 = 0.132938 loss)
I1107 12:41:00.819376 11160 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1107 12:41:09.326898 11160 solver.cpp:218] Iteration 95700 (11.7554 iter/s, 8.50671s/100 iters), loss = 0.0606885
I1107 12:41:09.326898 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:41:09.326898 11160 solver.cpp:237]     Train net output #1: loss = 0.0606887 (* 1 = 0.0606887 loss)
I1107 12:41:09.326898 11160 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1107 12:41:17.843866 11160 solver.cpp:218] Iteration 95800 (11.7418 iter/s, 8.51659s/100 iters), loss = 0.0961189
I1107 12:41:17.843866 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:41:17.843866 11160 solver.cpp:237]     Train net output #1: loss = 0.0961191 (* 1 = 0.0961191 loss)
I1107 12:41:17.843866 11160 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1107 12:41:26.351514 11160 solver.cpp:218] Iteration 95900 (11.7543 iter/s, 8.50755s/100 iters), loss = 0.0873805
I1107 12:41:26.351514 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:41:26.351514 11160 solver.cpp:237]     Train net output #1: loss = 0.0873807 (* 1 = 0.0873807 loss)
I1107 12:41:26.351514 11160 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1107 12:41:34.453189  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:41:34.791689 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_96000.caffemodel
I1107 12:41:34.818689 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_96000.solverstate
I1107 12:41:34.827690 11160 solver.cpp:330] Iteration 96000, Testing net (#0)
I1107 12:41:34.827690 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:41:36.809859 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:41:36.889864 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9166
I1107 12:41:36.889864 11160 solver.cpp:397]     Test net output #1: loss = 0.283888 (* 1 = 0.283888 loss)
I1107 12:41:36.970870 11160 solver.cpp:218] Iteration 96000 (9.41728 iter/s, 10.6188s/100 iters), loss = 0.129591
I1107 12:41:36.970870 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:41:36.970870 11160 solver.cpp:237]     Train net output #1: loss = 0.129592 (* 1 = 0.129592 loss)
I1107 12:41:36.970870 11160 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1107 12:41:45.474900 11160 solver.cpp:218] Iteration 96100 (11.7595 iter/s, 8.50374s/100 iters), loss = 0.140906
I1107 12:41:45.474900 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:41:45.474900 11160 solver.cpp:237]     Train net output #1: loss = 0.140906 (* 1 = 0.140906 loss)
I1107 12:41:45.474900 11160 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1107 12:41:53.986845 11160 solver.cpp:218] Iteration 96200 (11.7489 iter/s, 8.51142s/100 iters), loss = 0.0636838
I1107 12:41:53.986845 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:41:53.986845 11160 solver.cpp:237]     Train net output #1: loss = 0.063684 (* 1 = 0.063684 loss)
I1107 12:41:53.986845 11160 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1107 12:42:02.499219 11160 solver.cpp:218] Iteration 96300 (11.7486 iter/s, 8.51163s/100 iters), loss = 0.0568481
I1107 12:42:02.499219 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:42:02.499219 11160 solver.cpp:237]     Train net output #1: loss = 0.0568483 (* 1 = 0.0568483 loss)
I1107 12:42:02.499219 11160 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1107 12:42:11.011335 11160 solver.cpp:218] Iteration 96400 (11.7486 iter/s, 8.51167s/100 iters), loss = 0.104254
I1107 12:42:11.011335 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:42:11.011335 11160 solver.cpp:237]     Train net output #1: loss = 0.104254 (* 1 = 0.104254 loss)
I1107 12:42:11.011335 11160 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1107 12:42:19.112622  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:42:19.451143 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_96500.caffemodel
I1107 12:42:19.481145 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_96500.solverstate
I1107 12:42:19.515159 11160 solver.cpp:330] Iteration 96500, Testing net (#0)
I1107 12:42:19.515660 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:42:21.498332 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:42:21.577335 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 12:42:21.577335 11160 solver.cpp:397]     Test net output #1: loss = 0.278356 (* 1 = 0.278356 loss)
I1107 12:42:21.658357 11160 solver.cpp:218] Iteration 96500 (9.39261 iter/s, 10.6467s/100 iters), loss = 0.0763276
I1107 12:42:21.658357 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:42:21.658357 11160 solver.cpp:237]     Train net output #1: loss = 0.0763278 (* 1 = 0.0763278 loss)
I1107 12:42:21.658357 11160 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1107 12:42:30.172374 11160 solver.cpp:218] Iteration 96600 (11.7461 iter/s, 8.51348s/100 iters), loss = 0.120119
I1107 12:42:30.172374 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:42:30.172374 11160 solver.cpp:237]     Train net output #1: loss = 0.120119 (* 1 = 0.120119 loss)
I1107 12:42:30.172374 11160 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1107 12:42:38.672462 11160 solver.cpp:218] Iteration 96700 (11.766 iter/s, 8.49907s/100 iters), loss = 0.0498338
I1107 12:42:38.672462 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:42:38.672462 11160 solver.cpp:237]     Train net output #1: loss = 0.049834 (* 1 = 0.049834 loss)
I1107 12:42:38.672462 11160 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1107 12:42:47.178548 11160 solver.cpp:218] Iteration 96800 (11.7566 iter/s, 8.50586s/100 iters), loss = 0.0613512
I1107 12:42:47.178548 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:42:47.178548 11160 solver.cpp:237]     Train net output #1: loss = 0.0613514 (* 1 = 0.0613514 loss)
I1107 12:42:47.178548 11160 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1107 12:42:55.690511 11160 solver.cpp:218] Iteration 96900 (11.7483 iter/s, 8.5119s/100 iters), loss = 0.0840759
I1107 12:42:55.690511 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:42:55.690511 11160 solver.cpp:237]     Train net output #1: loss = 0.0840761 (* 1 = 0.0840761 loss)
I1107 12:42:55.690511 11160 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1107 12:43:03.787549  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:43:04.125068 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_97000.caffemodel
I1107 12:43:04.154573 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_97000.solverstate
I1107 12:43:04.162573 11160 solver.cpp:330] Iteration 97000, Testing net (#0)
I1107 12:43:04.162573 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:43:06.147837 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:43:06.228377 11160 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 12:43:06.228377 11160 solver.cpp:397]     Test net output #1: loss = 0.275874 (* 1 = 0.275874 loss)
I1107 12:43:06.309664 11160 solver.cpp:218] Iteration 97000 (9.41812 iter/s, 10.6178s/100 iters), loss = 0.0794157
I1107 12:43:06.309664 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:43:06.309664 11160 solver.cpp:237]     Train net output #1: loss = 0.0794158 (* 1 = 0.0794158 loss)
I1107 12:43:06.309664 11160 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1107 12:43:14.802317 11160 solver.cpp:218] Iteration 97100 (11.7752 iter/s, 8.49243s/100 iters), loss = 0.0955859
I1107 12:43:14.802317 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:43:14.802317 11160 solver.cpp:237]     Train net output #1: loss = 0.0955861 (* 1 = 0.0955861 loss)
I1107 12:43:14.802317 11160 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1107 12:43:23.302414 11160 solver.cpp:218] Iteration 97200 (11.7656 iter/s, 8.49936s/100 iters), loss = 0.0823609
I1107 12:43:23.302414 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:43:23.302414 11160 solver.cpp:237]     Train net output #1: loss = 0.082361 (* 1 = 0.082361 loss)
I1107 12:43:23.302414 11160 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1107 12:43:31.801888 11160 solver.cpp:218] Iteration 97300 (11.7659 iter/s, 8.49912s/100 iters), loss = 0.0684279
I1107 12:43:31.801888 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:43:31.801888 11160 solver.cpp:237]     Train net output #1: loss = 0.068428 (* 1 = 0.068428 loss)
I1107 12:43:31.801888 11160 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1107 12:43:40.309345 11160 solver.cpp:218] Iteration 97400 (11.7555 iter/s, 8.50665s/100 iters), loss = 0.0571634
I1107 12:43:40.309345 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:43:40.309345 11160 solver.cpp:237]     Train net output #1: loss = 0.0571635 (* 1 = 0.0571635 loss)
I1107 12:43:40.309345 11160 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1107 12:43:48.390066  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:43:48.725618 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_97500.caffemodel
I1107 12:43:48.753127 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_97500.solverstate
I1107 12:43:48.781496 11160 solver.cpp:330] Iteration 97500, Testing net (#0)
I1107 12:43:48.781496 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:43:50.765310 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:43:50.844312 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 12:43:50.844312 11160 solver.cpp:397]     Test net output #1: loss = 0.278431 (* 1 = 0.278431 loss)
I1107 12:43:50.926314 11160 solver.cpp:218] Iteration 97500 (9.41937 iter/s, 10.6164s/100 iters), loss = 0.0778101
I1107 12:43:50.926314 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:43:50.926314 11160 solver.cpp:237]     Train net output #1: loss = 0.0778103 (* 1 = 0.0778103 loss)
I1107 12:43:50.926314 11160 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1107 12:43:59.446969 11160 solver.cpp:218] Iteration 97600 (11.736 iter/s, 8.52077s/100 iters), loss = 0.143741
I1107 12:43:59.446969 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 12:43:59.446969 11160 solver.cpp:237]     Train net output #1: loss = 0.143741 (* 1 = 0.143741 loss)
I1107 12:43:59.446969 11160 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1107 12:44:07.942042 11160 solver.cpp:218] Iteration 97700 (11.7726 iter/s, 8.4943s/100 iters), loss = 0.0711096
I1107 12:44:07.942042 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:44:07.942042 11160 solver.cpp:237]     Train net output #1: loss = 0.0711097 (* 1 = 0.0711097 loss)
I1107 12:44:07.942042 11160 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1107 12:44:16.455231 11160 solver.cpp:218] Iteration 97800 (11.7473 iter/s, 8.51262s/100 iters), loss = 0.0437535
I1107 12:44:16.455231 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:44:16.455231 11160 solver.cpp:237]     Train net output #1: loss = 0.0437536 (* 1 = 0.0437536 loss)
I1107 12:44:16.455231 11160 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1107 12:44:24.973678 11160 solver.cpp:218] Iteration 97900 (11.7404 iter/s, 8.51756s/100 iters), loss = 0.0555239
I1107 12:44:24.973678 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:44:24.973678 11160 solver.cpp:237]     Train net output #1: loss = 0.055524 (* 1 = 0.055524 loss)
I1107 12:44:24.973678 11160 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1107 12:44:33.072640  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:44:33.407755 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_98000.caffemodel
I1107 12:44:33.437754 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_98000.solverstate
I1107 12:44:33.447755 11160 solver.cpp:330] Iteration 98000, Testing net (#0)
I1107 12:44:33.447755 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:44:35.433378 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:44:35.512919 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 12:44:35.512919 11160 solver.cpp:397]     Test net output #1: loss = 0.279582 (* 1 = 0.279582 loss)
I1107 12:44:35.593892 11160 solver.cpp:218] Iteration 98000 (9.4165 iter/s, 10.6197s/100 iters), loss = 0.0636275
I1107 12:44:35.593892 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:44:35.593892 11160 solver.cpp:237]     Train net output #1: loss = 0.0636277 (* 1 = 0.0636277 loss)
I1107 12:44:35.593892 11160 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1107 12:44:44.103482 11160 solver.cpp:218] Iteration 98100 (11.7521 iter/s, 8.50912s/100 iters), loss = 0.103713
I1107 12:44:44.103482 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:44:44.103482 11160 solver.cpp:237]     Train net output #1: loss = 0.103713 (* 1 = 0.103713 loss)
I1107 12:44:44.103482 11160 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1107 12:44:52.615898 11160 solver.cpp:218] Iteration 98200 (11.7477 iter/s, 8.51231s/100 iters), loss = 0.0461094
I1107 12:44:52.615898 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:44:52.615898 11160 solver.cpp:237]     Train net output #1: loss = 0.0461095 (* 1 = 0.0461095 loss)
I1107 12:44:52.615898 11160 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1107 12:45:01.146370 11160 solver.cpp:218] Iteration 98300 (11.7235 iter/s, 8.5299s/100 iters), loss = 0.0484099
I1107 12:45:01.146370 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:45:01.146370 11160 solver.cpp:237]     Train net output #1: loss = 0.0484101 (* 1 = 0.0484101 loss)
I1107 12:45:01.146370 11160 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1107 12:45:09.660609 11160 solver.cpp:218] Iteration 98400 (11.7464 iter/s, 8.51327s/100 iters), loss = 0.0566355
I1107 12:45:09.660609 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:45:09.660609 11160 solver.cpp:237]     Train net output #1: loss = 0.0566357 (* 1 = 0.0566357 loss)
I1107 12:45:09.660609 11160 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1107 12:45:17.764683  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:45:18.102701 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_98500.caffemodel
I1107 12:45:18.132700 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_98500.solverstate
I1107 12:45:18.189723 11160 solver.cpp:330] Iteration 98500, Testing net (#0)
I1107 12:45:18.190723 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:45:20.176913 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:45:20.256919 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 12:45:20.256919 11160 solver.cpp:397]     Test net output #1: loss = 0.279215 (* 1 = 0.279215 loss)
I1107 12:45:20.338421 11160 solver.cpp:218] Iteration 98500 (9.36586 iter/s, 10.6771s/100 iters), loss = 0.0811314
I1107 12:45:20.338421 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:45:20.338421 11160 solver.cpp:237]     Train net output #1: loss = 0.0811316 (* 1 = 0.0811316 loss)
I1107 12:45:20.338421 11160 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1107 12:45:28.961092 11160 solver.cpp:218] Iteration 98600 (11.5971 iter/s, 8.62288s/100 iters), loss = 0.0857792
I1107 12:45:28.961092 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:45:28.961092 11160 solver.cpp:237]     Train net output #1: loss = 0.0857794 (* 1 = 0.0857794 loss)
I1107 12:45:28.961092 11160 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1107 12:45:37.462896 11160 solver.cpp:218] Iteration 98700 (11.7635 iter/s, 8.50086s/100 iters), loss = 0.0381962
I1107 12:45:37.462896 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:45:37.462896 11160 solver.cpp:237]     Train net output #1: loss = 0.0381963 (* 1 = 0.0381963 loss)
I1107 12:45:37.462896 11160 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1107 12:45:45.977581 11160 solver.cpp:218] Iteration 98800 (11.7452 iter/s, 8.51415s/100 iters), loss = 0.0596965
I1107 12:45:45.977581 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:45:45.977581 11160 solver.cpp:237]     Train net output #1: loss = 0.0596966 (* 1 = 0.0596966 loss)
I1107 12:45:45.977581 11160 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1107 12:45:54.494552 11160 solver.cpp:218] Iteration 98900 (11.7416 iter/s, 8.5167s/100 iters), loss = 0.0558437
I1107 12:45:54.494552 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:45:54.494552 11160 solver.cpp:237]     Train net output #1: loss = 0.0558439 (* 1 = 0.0558439 loss)
I1107 12:45:54.494552 11160 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1107 12:46:02.604910  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:46:02.940960 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_99000.caffemodel
I1107 12:46:02.971978 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_99000.solverstate
I1107 12:46:02.980000 11160 solver.cpp:330] Iteration 99000, Testing net (#0)
I1107 12:46:02.980000 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:46:04.968230 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:46:05.047230 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 12:46:05.047230 11160 solver.cpp:397]     Test net output #1: loss = 0.278257 (* 1 = 0.278257 loss)
I1107 12:46:05.128233 11160 solver.cpp:218] Iteration 99000 (9.40438 iter/s, 10.6333s/100 iters), loss = 0.0672381
I1107 12:46:05.128233 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:46:05.128233 11160 solver.cpp:237]     Train net output #1: loss = 0.0672382 (* 1 = 0.0672382 loss)
I1107 12:46:05.128233 11160 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1107 12:46:13.637176 11160 solver.cpp:218] Iteration 99100 (11.7529 iter/s, 8.50853s/100 iters), loss = 0.0629513
I1107 12:46:13.638180 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:46:13.638180 11160 solver.cpp:237]     Train net output #1: loss = 0.0629514 (* 1 = 0.0629514 loss)
I1107 12:46:13.638180 11160 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1107 12:46:22.153223 11160 solver.cpp:218] Iteration 99200 (11.7438 iter/s, 8.51512s/100 iters), loss = 0.0527618
I1107 12:46:22.153223 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:46:22.153223 11160 solver.cpp:237]     Train net output #1: loss = 0.052762 (* 1 = 0.052762 loss)
I1107 12:46:22.153223 11160 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1107 12:46:30.660354 11160 solver.cpp:218] Iteration 99300 (11.7558 iter/s, 8.50646s/100 iters), loss = 0.059287
I1107 12:46:30.660856 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:46:30.660856 11160 solver.cpp:237]     Train net output #1: loss = 0.0592872 (* 1 = 0.0592872 loss)
I1107 12:46:30.660856 11160 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1107 12:46:39.161795 11160 solver.cpp:218] Iteration 99400 (11.7634 iter/s, 8.50091s/100 iters), loss = 0.0783764
I1107 12:46:39.162295 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:46:39.162295 11160 solver.cpp:237]     Train net output #1: loss = 0.0783766 (* 1 = 0.0783766 loss)
I1107 12:46:39.162295 11160 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1107 12:46:47.242055  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:46:47.579373 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_99500.caffemodel
I1107 12:46:47.609380 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_99500.solverstate
I1107 12:46:47.618379 11160 solver.cpp:330] Iteration 99500, Testing net (#0)
I1107 12:46:47.618379 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:46:49.602536 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:46:49.682549 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9185
I1107 12:46:49.682549 11160 solver.cpp:397]     Test net output #1: loss = 0.277785 (* 1 = 0.277785 loss)
I1107 12:46:49.763547 11160 solver.cpp:218] Iteration 99500 (9.43325 iter/s, 10.6008s/100 iters), loss = 0.0524536
I1107 12:46:49.763547 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:46:49.763547 11160 solver.cpp:237]     Train net output #1: loss = 0.0524538 (* 1 = 0.0524538 loss)
I1107 12:46:49.763547 11160 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1107 12:46:58.281785 11160 solver.cpp:218] Iteration 99600 (11.7399 iter/s, 8.51798s/100 iters), loss = 0.0741008
I1107 12:46:58.281785 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:46:58.281785 11160 solver.cpp:237]     Train net output #1: loss = 0.074101 (* 1 = 0.074101 loss)
I1107 12:46:58.281785 11160 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1107 12:47:06.782411 11160 solver.cpp:218] Iteration 99700 (11.7644 iter/s, 8.50024s/100 iters), loss = 0.0584071
I1107 12:47:06.782411 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:47:06.782411 11160 solver.cpp:237]     Train net output #1: loss = 0.0584073 (* 1 = 0.0584073 loss)
I1107 12:47:06.782411 11160 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1107 12:47:15.292470 11160 solver.cpp:218] Iteration 99800 (11.7515 iter/s, 8.50957s/100 iters), loss = 0.0753113
I1107 12:47:15.292470 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:47:15.292470 11160 solver.cpp:237]     Train net output #1: loss = 0.0753115 (* 1 = 0.0753115 loss)
I1107 12:47:15.292470 11160 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1107 12:47:23.792309 11160 solver.cpp:218] Iteration 99900 (11.765 iter/s, 8.49981s/100 iters), loss = 0.0444847
I1107 12:47:23.793310 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:47:23.793310 11160 solver.cpp:237]     Train net output #1: loss = 0.0444849 (* 1 = 0.0444849 loss)
I1107 12:47:23.793310 11160 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1107 12:47:32.123363  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:47:32.464409 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_100000.caffemodel
I1107 12:47:32.496409 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_100000.solverstate
I1107 12:47:32.506409 11160 solver.cpp:330] Iteration 100000, Testing net (#0)
I1107 12:47:32.506409 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:47:34.522608 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:47:34.601611 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 12:47:34.601611 11160 solver.cpp:397]     Test net output #1: loss = 0.281487 (* 1 = 0.281487 loss)
I1107 12:47:34.683634 11160 solver.cpp:218] Iteration 100000 (9.18291 iter/s, 10.8898s/100 iters), loss = 0.0599204
I1107 12:47:34.683634 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:47:34.683634 11160 solver.cpp:237]     Train net output #1: loss = 0.0599206 (* 1 = 0.0599206 loss)
I1107 12:47:34.683634 11160 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1107 12:47:43.229405 11160 solver.cpp:218] Iteration 100100 (11.7015 iter/s, 8.54589s/100 iters), loss = 0.0752369
I1107 12:47:43.229405 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:47:43.229405 11160 solver.cpp:237]     Train net output #1: loss = 0.0752371 (* 1 = 0.0752371 loss)
I1107 12:47:43.229405 11160 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1107 12:47:51.815801 11160 solver.cpp:218] Iteration 100200 (11.6477 iter/s, 8.58536s/100 iters), loss = 0.0610309
I1107 12:47:51.815801 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:47:51.815801 11160 solver.cpp:237]     Train net output #1: loss = 0.0610311 (* 1 = 0.0610311 loss)
I1107 12:47:51.815801 11160 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1107 12:48:00.456332 11160 solver.cpp:218] Iteration 100300 (11.5739 iter/s, 8.64011s/100 iters), loss = 0.0414558
I1107 12:48:00.456332 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:48:00.456332 11160 solver.cpp:237]     Train net output #1: loss = 0.041456 (* 1 = 0.041456 loss)
I1107 12:48:00.456332 11160 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1107 12:48:09.024722 11160 solver.cpp:218] Iteration 100400 (11.6713 iter/s, 8.56802s/100 iters), loss = 0.074814
I1107 12:48:09.024722 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:48:09.024722 11160 solver.cpp:237]     Train net output #1: loss = 0.0748142 (* 1 = 0.0748142 loss)
I1107 12:48:09.024722 11160 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1107 12:48:17.140121  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:48:17.477949 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_100500.caffemodel
I1107 12:48:17.506945 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_100500.solverstate
I1107 12:48:17.515950 11160 solver.cpp:330] Iteration 100500, Testing net (#0)
I1107 12:48:17.515950 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:48:19.522097 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:48:19.603037 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 12:48:19.603037 11160 solver.cpp:397]     Test net output #1: loss = 0.277575 (* 1 = 0.277575 loss)
I1107 12:48:19.685642 11160 solver.cpp:218] Iteration 100500 (9.38084 iter/s, 10.66s/100 iters), loss = 0.082126
I1107 12:48:19.685642 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:48:19.685642 11160 solver.cpp:237]     Train net output #1: loss = 0.0821262 (* 1 = 0.0821262 loss)
I1107 12:48:19.685642 11160 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1107 12:48:28.239064 11160 solver.cpp:218] Iteration 100600 (11.6909 iter/s, 8.55366s/100 iters), loss = 0.0667225
I1107 12:48:28.239064 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:48:28.239064 11160 solver.cpp:237]     Train net output #1: loss = 0.0667226 (* 1 = 0.0667226 loss)
I1107 12:48:28.239064 11160 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1107 12:48:36.827322 11160 solver.cpp:218] Iteration 100700 (11.6449 iter/s, 8.58744s/100 iters), loss = 0.0304368
I1107 12:48:36.827322 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:48:36.827322 11160 solver.cpp:237]     Train net output #1: loss = 0.0304369 (* 1 = 0.0304369 loss)
I1107 12:48:36.827322 11160 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1107 12:48:45.372923 11160 solver.cpp:218] Iteration 100800 (11.7032 iter/s, 8.54468s/100 iters), loss = 0.0407925
I1107 12:48:45.372923 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:48:45.372923 11160 solver.cpp:237]     Train net output #1: loss = 0.0407927 (* 1 = 0.0407927 loss)
I1107 12:48:45.372923 11160 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1107 12:48:53.925374 11160 solver.cpp:218] Iteration 100900 (11.6929 iter/s, 8.55221s/100 iters), loss = 0.0487032
I1107 12:48:53.925374 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:48:53.925374 11160 solver.cpp:237]     Train net output #1: loss = 0.0487034 (* 1 = 0.0487034 loss)
I1107 12:48:53.925374 11160 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1107 12:49:02.092306  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:49:02.429321 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_101000.caffemodel
I1107 12:49:02.460326 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_101000.solverstate
I1107 12:49:02.469331 11160 solver.cpp:330] Iteration 101000, Testing net (#0)
I1107 12:49:02.469331 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:49:04.467530 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:49:04.547528 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9176
I1107 12:49:04.547528 11160 solver.cpp:397]     Test net output #1: loss = 0.28055 (* 1 = 0.28055 loss)
I1107 12:49:04.628533 11160 solver.cpp:218] Iteration 101000 (9.34333 iter/s, 10.7028s/100 iters), loss = 0.120408
I1107 12:49:04.628533 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 12:49:04.628533 11160 solver.cpp:237]     Train net output #1: loss = 0.120409 (* 1 = 0.120409 loss)
I1107 12:49:04.628533 11160 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1107 12:49:13.195459 11160 solver.cpp:218] Iteration 101100 (11.6739 iter/s, 8.56609s/100 iters), loss = 0.0781657
I1107 12:49:13.195459 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:49:13.195459 11160 solver.cpp:237]     Train net output #1: loss = 0.0781659 (* 1 = 0.0781659 loss)
I1107 12:49:13.195459 11160 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1107 12:49:21.752681 11160 solver.cpp:218] Iteration 101200 (11.6871 iter/s, 8.55642s/100 iters), loss = 0.0475786
I1107 12:49:21.752681 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:49:21.752681 11160 solver.cpp:237]     Train net output #1: loss = 0.0475788 (* 1 = 0.0475788 loss)
I1107 12:49:21.752681 11160 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1107 12:49:30.311780 11160 solver.cpp:218] Iteration 101300 (11.6836 iter/s, 8.55899s/100 iters), loss = 0.093782
I1107 12:49:30.311780 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:49:30.311780 11160 solver.cpp:237]     Train net output #1: loss = 0.0937822 (* 1 = 0.0937822 loss)
I1107 12:49:30.311780 11160 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1107 12:49:38.857113 11160 solver.cpp:218] Iteration 101400 (11.704 iter/s, 8.5441s/100 iters), loss = 0.0483801
I1107 12:49:38.857113 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:49:38.857113 11160 solver.cpp:237]     Train net output #1: loss = 0.0483803 (* 1 = 0.0483803 loss)
I1107 12:49:38.857113 11160 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1107 12:49:46.975378  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:49:47.313406 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_101500.caffemodel
I1107 12:49:47.345396 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_101500.solverstate
I1107 12:49:47.354396 11160 solver.cpp:330] Iteration 101500, Testing net (#0)
I1107 12:49:47.354396 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:49:49.347913 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:49:49.428917 11160 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 12:49:49.428917 11160 solver.cpp:397]     Test net output #1: loss = 0.282975 (* 1 = 0.282975 loss)
I1107 12:49:49.510938 11160 solver.cpp:218] Iteration 101500 (9.38664 iter/s, 10.6534s/100 iters), loss = 0.0363805
I1107 12:49:49.510938 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:49:49.510938 11160 solver.cpp:237]     Train net output #1: loss = 0.0363806 (* 1 = 0.0363806 loss)
I1107 12:49:49.510938 11160 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1107 12:49:58.257457 11160 solver.cpp:218] Iteration 101600 (11.4331 iter/s, 8.74656s/100 iters), loss = 0.0875746
I1107 12:49:58.258457 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:49:58.258457 11160 solver.cpp:237]     Train net output #1: loss = 0.0875748 (* 1 = 0.0875748 loss)
I1107 12:49:58.258457 11160 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1107 12:50:06.864078 11160 solver.cpp:218] Iteration 101700 (11.6209 iter/s, 8.60519s/100 iters), loss = 0.037923
I1107 12:50:06.864078 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:50:06.864078 11160 solver.cpp:237]     Train net output #1: loss = 0.0379232 (* 1 = 0.0379232 loss)
I1107 12:50:06.864078 11160 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1107 12:50:15.604856 11160 solver.cpp:218] Iteration 101800 (11.4407 iter/s, 8.74074s/100 iters), loss = 0.0344659
I1107 12:50:15.604856 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:50:15.604856 11160 solver.cpp:237]     Train net output #1: loss = 0.0344661 (* 1 = 0.0344661 loss)
I1107 12:50:15.604856 11160 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1107 12:50:24.142551 11160 solver.cpp:218] Iteration 101900 (11.7131 iter/s, 8.53742s/100 iters), loss = 0.0591778
I1107 12:50:24.142551 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:50:24.142551 11160 solver.cpp:237]     Train net output #1: loss = 0.0591779 (* 1 = 0.0591779 loss)
I1107 12:50:24.142551 11160 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1107 12:50:32.264711  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:50:32.601790 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_102000.caffemodel
I1107 12:50:32.648790 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_102000.solverstate
I1107 12:50:32.657791 11160 solver.cpp:330] Iteration 102000, Testing net (#0)
I1107 12:50:32.657791 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:50:34.650203 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:50:34.730207 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 12:50:34.730207 11160 solver.cpp:397]     Test net output #1: loss = 0.284592 (* 1 = 0.284592 loss)
I1107 12:50:34.811213 11160 solver.cpp:218] Iteration 102000 (9.37368 iter/s, 10.6682s/100 iters), loss = 0.0664551
I1107 12:50:34.811213 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:50:34.811213 11160 solver.cpp:237]     Train net output #1: loss = 0.0664552 (* 1 = 0.0664552 loss)
I1107 12:50:34.812213 11160 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1107 12:50:43.342164 11160 solver.cpp:218] Iteration 102100 (11.7227 iter/s, 8.53049s/100 iters), loss = 0.167457
I1107 12:50:43.343165 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 12:50:43.343165 11160 solver.cpp:237]     Train net output #1: loss = 0.167457 (* 1 = 0.167457 loss)
I1107 12:50:43.343165 11160 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1107 12:50:51.872920 11160 solver.cpp:218] Iteration 102200 (11.7241 iter/s, 8.52941s/100 iters), loss = 0.0429276
I1107 12:50:51.872920 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:50:51.872920 11160 solver.cpp:237]     Train net output #1: loss = 0.0429278 (* 1 = 0.0429278 loss)
I1107 12:50:51.872920 11160 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1107 12:51:00.415796 11160 solver.cpp:218] Iteration 102300 (11.7055 iter/s, 8.54299s/100 iters), loss = 0.0243023
I1107 12:51:00.415796 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:51:00.415796 11160 solver.cpp:237]     Train net output #1: loss = 0.0243025 (* 1 = 0.0243025 loss)
I1107 12:51:00.415796 11160 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1107 12:51:08.985839 11160 solver.cpp:218] Iteration 102400 (11.6691 iter/s, 8.56964s/100 iters), loss = 0.0476008
I1107 12:51:08.986840 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:51:08.986840 11160 solver.cpp:237]     Train net output #1: loss = 0.0476009 (* 1 = 0.0476009 loss)
I1107 12:51:08.986840 11160 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1107 12:51:17.134786  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:51:17.471814 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_102500.caffemodel
I1107 12:51:17.501818 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_102500.solverstate
I1107 12:51:17.510819 11160 solver.cpp:330] Iteration 102500, Testing net (#0)
I1107 12:51:17.510819 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:51:19.505641 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:51:19.586163 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9185
I1107 12:51:19.586163 11160 solver.cpp:397]     Test net output #1: loss = 0.285578 (* 1 = 0.285578 loss)
I1107 12:51:19.667152 11160 solver.cpp:218] Iteration 102500 (9.36289 iter/s, 10.6805s/100 iters), loss = 0.0551626
I1107 12:51:19.667152 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:51:19.667152 11160 solver.cpp:237]     Train net output #1: loss = 0.0551627 (* 1 = 0.0551627 loss)
I1107 12:51:19.667152 11160 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1107 12:51:28.201146 11160 solver.cpp:218] Iteration 102600 (11.719 iter/s, 8.53318s/100 iters), loss = 0.0964957
I1107 12:51:28.201146 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:51:28.201146 11160 solver.cpp:237]     Train net output #1: loss = 0.0964958 (* 1 = 0.0964958 loss)
I1107 12:51:28.201146 11160 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1107 12:51:36.777305 11160 solver.cpp:218] Iteration 102700 (11.6609 iter/s, 8.57568s/100 iters), loss = 0.0559018
I1107 12:51:36.777305 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:51:36.777305 11160 solver.cpp:237]     Train net output #1: loss = 0.055902 (* 1 = 0.055902 loss)
I1107 12:51:36.777305 11160 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1107 12:51:45.342206 11160 solver.cpp:218] Iteration 102800 (11.6757 iter/s, 8.56482s/100 iters), loss = 0.0554105
I1107 12:51:45.342206 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:51:45.342206 11160 solver.cpp:237]     Train net output #1: loss = 0.0554107 (* 1 = 0.0554107 loss)
I1107 12:51:45.342206 11160 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1107 12:51:53.892271 11160 solver.cpp:218] Iteration 102900 (11.6963 iter/s, 8.54968s/100 iters), loss = 0.0517878
I1107 12:51:53.893271 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:51:53.893271 11160 solver.cpp:237]     Train net output #1: loss = 0.0517879 (* 1 = 0.0517879 loss)
I1107 12:51:53.893271 11160 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1107 12:52:02.075721  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:52:02.412760 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_103000.caffemodel
I1107 12:52:02.442762 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_103000.solverstate
I1107 12:52:02.457762 11160 solver.cpp:330] Iteration 103000, Testing net (#0)
I1107 12:52:02.458763 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:52:04.456959 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:52:04.535965 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9178
I1107 12:52:04.535965 11160 solver.cpp:397]     Test net output #1: loss = 0.285667 (* 1 = 0.285667 loss)
I1107 12:52:04.616971 11160 solver.cpp:218] Iteration 103000 (9.32483 iter/s, 10.7241s/100 iters), loss = 0.0650504
I1107 12:52:04.617970 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:52:04.617970 11160 solver.cpp:237]     Train net output #1: loss = 0.0650505 (* 1 = 0.0650505 loss)
I1107 12:52:04.617970 11160 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1107 12:52:13.211607 11160 solver.cpp:218] Iteration 103100 (11.6368 iter/s, 8.59345s/100 iters), loss = 0.0730033
I1107 12:52:13.211607 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:52:13.211607 11160 solver.cpp:237]     Train net output #1: loss = 0.0730035 (* 1 = 0.0730035 loss)
I1107 12:52:13.211607 11160 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1107 12:52:21.772848 11160 solver.cpp:218] Iteration 103200 (11.6805 iter/s, 8.56128s/100 iters), loss = 0.0434985
I1107 12:52:21.772848 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:52:21.772848 11160 solver.cpp:237]     Train net output #1: loss = 0.0434986 (* 1 = 0.0434986 loss)
I1107 12:52:21.772848 11160 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1107 12:52:30.329807 11160 solver.cpp:218] Iteration 103300 (11.6877 iter/s, 8.55601s/100 iters), loss = 0.0383526
I1107 12:52:30.329807 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:52:30.329807 11160 solver.cpp:237]     Train net output #1: loss = 0.0383527 (* 1 = 0.0383527 loss)
I1107 12:52:30.329807 11160 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1107 12:52:38.893738 11160 solver.cpp:218] Iteration 103400 (11.6782 iter/s, 8.56299s/100 iters), loss = 0.038967
I1107 12:52:38.893738 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:52:38.893738 11160 solver.cpp:237]     Train net output #1: loss = 0.0389672 (* 1 = 0.0389672 loss)
I1107 12:52:38.893738 11160 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1107 12:52:47.051926  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:52:47.393957 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_103500.caffemodel
I1107 12:52:47.428977 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_103500.solverstate
I1107 12:52:47.436975 11160 solver.cpp:330] Iteration 103500, Testing net (#0)
I1107 12:52:47.437979 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:52:49.431318 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:52:49.511322 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 12:52:49.511322 11160 solver.cpp:397]     Test net output #1: loss = 0.287355 (* 1 = 0.287355 loss)
I1107 12:52:49.593325 11160 solver.cpp:218] Iteration 103500 (9.34666 iter/s, 10.699s/100 iters), loss = 0.0584546
I1107 12:52:49.593325 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:52:49.593325 11160 solver.cpp:237]     Train net output #1: loss = 0.0584547 (* 1 = 0.0584547 loss)
I1107 12:52:49.593325 11160 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1107 12:52:58.168303 11160 solver.cpp:218] Iteration 103600 (11.6617 iter/s, 8.57509s/100 iters), loss = 0.0572154
I1107 12:52:58.168303 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:52:58.169303 11160 solver.cpp:237]     Train net output #1: loss = 0.0572156 (* 1 = 0.0572156 loss)
I1107 12:52:58.169303 11160 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1107 12:53:06.720712 11160 solver.cpp:218] Iteration 103700 (11.6941 iter/s, 8.5513s/100 iters), loss = 0.0382123
I1107 12:53:06.720712 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:53:06.720712 11160 solver.cpp:237]     Train net output #1: loss = 0.0382125 (* 1 = 0.0382125 loss)
I1107 12:53:06.720712 11160 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1107 12:53:15.261981 11160 solver.cpp:218] Iteration 103800 (11.7078 iter/s, 8.54132s/100 iters), loss = 0.0869057
I1107 12:53:15.261981 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:53:15.261981 11160 solver.cpp:237]     Train net output #1: loss = 0.0869058 (* 1 = 0.0869058 loss)
I1107 12:53:15.261981 11160 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1107 12:53:23.812157 11160 solver.cpp:218] Iteration 103900 (11.6972 iter/s, 8.54909s/100 iters), loss = 0.0466104
I1107 12:53:23.812157 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:53:23.812157 11160 solver.cpp:237]     Train net output #1: loss = 0.0466106 (* 1 = 0.0466106 loss)
I1107 12:53:23.812157 11160 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1107 12:53:31.968005  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:53:32.306020 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_104000.caffemodel
I1107 12:53:32.362028 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_104000.solverstate
I1107 12:53:32.384029 11160 solver.cpp:330] Iteration 104000, Testing net (#0)
I1107 12:53:32.384029 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:53:34.399195 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:53:34.479199 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9175
I1107 12:53:34.479199 11160 solver.cpp:397]     Test net output #1: loss = 0.2847 (* 1 = 0.2847 loss)
I1107 12:53:34.560206 11160 solver.cpp:218] Iteration 104000 (9.30439 iter/s, 10.7476s/100 iters), loss = 0.0392229
I1107 12:53:34.560206 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:53:34.560206 11160 solver.cpp:237]     Train net output #1: loss = 0.0392231 (* 1 = 0.0392231 loss)
I1107 12:53:34.560206 11160 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1107 12:53:43.180208 11160 solver.cpp:218] Iteration 104100 (11.6019 iter/s, 8.61927s/100 iters), loss = 0.0695018
I1107 12:53:43.180208 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 12:53:43.180208 11160 solver.cpp:237]     Train net output #1: loss = 0.069502 (* 1 = 0.069502 loss)
I1107 12:53:43.180208 11160 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1107 12:53:51.747838 11160 solver.cpp:218] Iteration 104200 (11.6724 iter/s, 8.56725s/100 iters), loss = 0.0421223
I1107 12:53:51.747838 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:53:51.747838 11160 solver.cpp:237]     Train net output #1: loss = 0.0421225 (* 1 = 0.0421225 loss)
I1107 12:53:51.747838 11160 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1107 12:54:00.316777 11160 solver.cpp:218] Iteration 104300 (11.6714 iter/s, 8.56796s/100 iters), loss = 0.0303607
I1107 12:54:00.316777 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:54:00.316777 11160 solver.cpp:237]     Train net output #1: loss = 0.0303609 (* 1 = 0.0303609 loss)
I1107 12:54:00.316777 11160 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1107 12:54:08.888255 11160 solver.cpp:218] Iteration 104400 (11.6672 iter/s, 8.57104s/100 iters), loss = 0.0351726
I1107 12:54:08.888255 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:54:08.888255 11160 solver.cpp:237]     Train net output #1: loss = 0.0351728 (* 1 = 0.0351728 loss)
I1107 12:54:08.888255 11160 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1107 12:54:17.013955  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:54:17.356081 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_104500.caffemodel
I1107 12:54:17.386081 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_104500.solverstate
I1107 12:54:17.394080 11160 solver.cpp:330] Iteration 104500, Testing net (#0)
I1107 12:54:17.395081 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:54:19.424729 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:54:19.508244 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 12:54:19.508244 11160 solver.cpp:397]     Test net output #1: loss = 0.285039 (* 1 = 0.285039 loss)
I1107 12:54:19.592105 11160 solver.cpp:218] Iteration 104500 (9.34256 iter/s, 10.7037s/100 iters), loss = 0.0393458
I1107 12:54:19.592105 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:54:19.592105 11160 solver.cpp:237]     Train net output #1: loss = 0.039346 (* 1 = 0.039346 loss)
I1107 12:54:19.592105 11160 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1107 12:54:28.159797 11160 solver.cpp:218] Iteration 104600 (11.6721 iter/s, 8.56746s/100 iters), loss = 0.0465403
I1107 12:54:28.159797 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:54:28.159797 11160 solver.cpp:237]     Train net output #1: loss = 0.0465405 (* 1 = 0.0465405 loss)
I1107 12:54:28.159797 11160 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1107 12:54:36.719458 11160 solver.cpp:218] Iteration 104700 (11.6845 iter/s, 8.55837s/100 iters), loss = 0.0745037
I1107 12:54:36.719458 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:54:36.719458 11160 solver.cpp:237]     Train net output #1: loss = 0.0745039 (* 1 = 0.0745039 loss)
I1107 12:54:36.719458 11160 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1107 12:54:45.275333 11160 solver.cpp:218] Iteration 104800 (11.6883 iter/s, 8.55555s/100 iters), loss = 0.0295091
I1107 12:54:45.275333 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:54:45.275333 11160 solver.cpp:237]     Train net output #1: loss = 0.0295093 (* 1 = 0.0295093 loss)
I1107 12:54:45.275333 11160 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1107 12:54:53.860244 11160 solver.cpp:218] Iteration 104900 (11.6481 iter/s, 8.58507s/100 iters), loss = 0.0472639
I1107 12:54:53.861243 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:54:53.861243 11160 solver.cpp:237]     Train net output #1: loss = 0.0472641 (* 1 = 0.0472641 loss)
I1107 12:54:53.861243 11160 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1107 12:55:01.981856  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:55:02.320401 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_105000.caffemodel
I1107 12:55:02.389905 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_105000.solverstate
I1107 12:55:02.399410 11160 solver.cpp:330] Iteration 105000, Testing net (#0)
I1107 12:55:02.399410 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:55:04.394687 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:55:04.474671 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1107 12:55:04.474671 11160 solver.cpp:397]     Test net output #1: loss = 0.288438 (* 1 = 0.288438 loss)
I1107 12:55:04.556679 11160 solver.cpp:218] Iteration 105000 (9.35005 iter/s, 10.6951s/100 iters), loss = 0.0412172
I1107 12:55:04.556679 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:55:04.556679 11160 solver.cpp:237]     Train net output #1: loss = 0.0412174 (* 1 = 0.0412174 loss)
I1107 12:55:04.556679 11160 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1107 12:55:13.123822 11160 solver.cpp:218] Iteration 105100 (11.6725 iter/s, 8.56711s/100 iters), loss = 0.0458395
I1107 12:55:13.123822 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:55:13.123822 11160 solver.cpp:237]     Train net output #1: loss = 0.0458396 (* 1 = 0.0458396 loss)
I1107 12:55:13.123822 11160 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1107 12:55:21.695186 11160 solver.cpp:218] Iteration 105200 (11.668 iter/s, 8.57049s/100 iters), loss = 0.0345774
I1107 12:55:21.695186 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:55:21.695186 11160 solver.cpp:237]     Train net output #1: loss = 0.0345776 (* 1 = 0.0345776 loss)
I1107 12:55:21.695688 11160 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1107 12:55:30.261068 11160 solver.cpp:218] Iteration 105300 (11.6751 iter/s, 8.56525s/100 iters), loss = 0.0439785
I1107 12:55:30.261068 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:55:30.261068 11160 solver.cpp:237]     Train net output #1: loss = 0.0439787 (* 1 = 0.0439787 loss)
I1107 12:55:30.261068 11160 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1107 12:55:38.796887 11160 solver.cpp:218] Iteration 105400 (11.7163 iter/s, 8.53511s/100 iters), loss = 0.0465886
I1107 12:55:38.796887 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:55:38.796887 11160 solver.cpp:237]     Train net output #1: loss = 0.0465888 (* 1 = 0.0465888 loss)
I1107 12:55:38.796887 11160 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1107 12:55:47.065863  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:55:47.404888 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_105500.caffemodel
I1107 12:55:47.436888 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_105500.solverstate
I1107 12:55:47.458906 11160 solver.cpp:330] Iteration 105500, Testing net (#0)
I1107 12:55:47.458906 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:55:49.455250 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:55:49.535253 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1107 12:55:49.535253 11160 solver.cpp:397]     Test net output #1: loss = 0.28818 (* 1 = 0.28818 loss)
I1107 12:55:49.617267 11160 solver.cpp:218] Iteration 105500 (9.24156 iter/s, 10.8207s/100 iters), loss = 0.0339829
I1107 12:55:49.618266 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:55:49.618266 11160 solver.cpp:237]     Train net output #1: loss = 0.0339831 (* 1 = 0.0339831 loss)
I1107 12:55:49.618266 11160 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1107 12:55:58.182255 11160 solver.cpp:218] Iteration 105600 (11.6765 iter/s, 8.56418s/100 iters), loss = 0.042276
I1107 12:55:58.182255 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:55:58.182255 11160 solver.cpp:237]     Train net output #1: loss = 0.0422762 (* 1 = 0.0422762 loss)
I1107 12:55:58.182255 11160 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1107 12:56:06.799504 11160 solver.cpp:218] Iteration 105700 (11.6058 iter/s, 8.61637s/100 iters), loss = 0.0419149
I1107 12:56:06.799504 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:56:06.799504 11160 solver.cpp:237]     Train net output #1: loss = 0.0419151 (* 1 = 0.0419151 loss)
I1107 12:56:06.799504 11160 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1107 12:56:15.376554 11160 solver.cpp:218] Iteration 105800 (11.66 iter/s, 8.57634s/100 iters), loss = 0.0364537
I1107 12:56:15.376554 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:56:15.376554 11160 solver.cpp:237]     Train net output #1: loss = 0.0364539 (* 1 = 0.0364539 loss)
I1107 12:56:15.376554 11160 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1107 12:56:24.003548 11160 solver.cpp:218] Iteration 105900 (11.5924 iter/s, 8.62634s/100 iters), loss = 0.0415931
I1107 12:56:24.003548 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:56:24.003548 11160 solver.cpp:237]     Train net output #1: loss = 0.0415933 (* 1 = 0.0415933 loss)
I1107 12:56:24.003548 11160 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1107 12:56:32.195498  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:56:32.533511 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_106000.caffemodel
I1107 12:56:32.563519 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_106000.solverstate
I1107 12:56:32.608525 11160 solver.cpp:330] Iteration 106000, Testing net (#0)
I1107 12:56:32.608525 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:56:34.637733 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:56:34.717738 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9178
I1107 12:56:34.717738 11160 solver.cpp:397]     Test net output #1: loss = 0.288776 (* 1 = 0.288776 loss)
I1107 12:56:34.798744 11160 solver.cpp:218] Iteration 106000 (9.26361 iter/s, 10.7949s/100 iters), loss = 0.0437911
I1107 12:56:34.798744 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:56:34.798744 11160 solver.cpp:237]     Train net output #1: loss = 0.0437913 (* 1 = 0.0437913 loss)
I1107 12:56:34.798744 11160 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1107 12:56:43.381291 11160 solver.cpp:218] Iteration 106100 (11.6514 iter/s, 8.58266s/100 iters), loss = 0.041593
I1107 12:56:43.381291 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:56:43.381291 11160 solver.cpp:237]     Train net output #1: loss = 0.0415932 (* 1 = 0.0415932 loss)
I1107 12:56:43.382292 11160 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1107 12:56:51.919024 11160 solver.cpp:218] Iteration 106200 (11.7144 iter/s, 8.53648s/100 iters), loss = 0.0400667
I1107 12:56:51.919024 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:56:51.919024 11160 solver.cpp:237]     Train net output #1: loss = 0.0400669 (* 1 = 0.0400669 loss)
I1107 12:56:51.919024 11160 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1107 12:57:00.457784 11160 solver.cpp:218] Iteration 106300 (11.7119 iter/s, 8.53831s/100 iters), loss = 0.0472358
I1107 12:57:00.457784 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:57:00.457784 11160 solver.cpp:237]     Train net output #1: loss = 0.047236 (* 1 = 0.047236 loss)
I1107 12:57:00.457784 11160 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1107 12:57:09.130249 11160 solver.cpp:218] Iteration 106400 (11.5311 iter/s, 8.67221s/100 iters), loss = 0.0498281
I1107 12:57:09.130249 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:57:09.130249 11160 solver.cpp:237]     Train net output #1: loss = 0.0498283 (* 1 = 0.0498283 loss)
I1107 12:57:09.130249 11160 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1107 12:57:17.279355  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:57:17.618391 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_106500.caffemodel
I1107 12:57:17.650389 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_106500.solverstate
I1107 12:57:17.683423 11160 solver.cpp:330] Iteration 106500, Testing net (#0)
I1107 12:57:17.684423 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:57:19.689586 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:57:19.770092 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9201
I1107 12:57:19.770092 11160 solver.cpp:397]     Test net output #1: loss = 0.281527 (* 1 = 0.281527 loss)
I1107 12:57:19.851594 11160 solver.cpp:218] Iteration 106500 (9.32799 iter/s, 10.7204s/100 iters), loss = 0.0600708
I1107 12:57:19.851594 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:57:19.851594 11160 solver.cpp:237]     Train net output #1: loss = 0.060071 (* 1 = 0.060071 loss)
I1107 12:57:19.851594 11160 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1107 12:57:28.392441 11160 solver.cpp:218] Iteration 106600 (11.7083 iter/s, 8.54095s/100 iters), loss = 0.0658566
I1107 12:57:28.392441 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:57:28.392441 11160 solver.cpp:237]     Train net output #1: loss = 0.0658568 (* 1 = 0.0658568 loss)
I1107 12:57:28.392441 11160 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1107 12:57:36.934309 11160 solver.cpp:218] Iteration 106700 (11.7081 iter/s, 8.54112s/100 iters), loss = 0.0539654
I1107 12:57:36.934309 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:57:36.934309 11160 solver.cpp:237]     Train net output #1: loss = 0.0539656 (* 1 = 0.0539656 loss)
I1107 12:57:36.934309 11160 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1107 12:57:45.496428 11160 solver.cpp:218] Iteration 106800 (11.6798 iter/s, 8.56178s/100 iters), loss = 0.0406266
I1107 12:57:45.496428 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:57:45.496428 11160 solver.cpp:237]     Train net output #1: loss = 0.0406268 (* 1 = 0.0406268 loss)
I1107 12:57:45.496428 11160 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1107 12:57:54.148421 11160 solver.cpp:218] Iteration 106900 (11.5587 iter/s, 8.65149s/100 iters), loss = 0.0353321
I1107 12:57:54.148421 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:57:54.148421 11160 solver.cpp:237]     Train net output #1: loss = 0.0353323 (* 1 = 0.0353323 loss)
I1107 12:57:54.148421 11160 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1107 12:58:02.315698  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:58:02.654712 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_107000.caffemodel
I1107 12:58:02.715734 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_107000.solverstate
I1107 12:58:02.724732 11160 solver.cpp:330] Iteration 107000, Testing net (#0)
I1107 12:58:02.724732 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:58:04.721879 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:58:04.801897 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9203
I1107 12:58:04.801897 11160 solver.cpp:397]     Test net output #1: loss = 0.286211 (* 1 = 0.286211 loss)
I1107 12:58:04.883901 11160 solver.cpp:218] Iteration 107000 (9.31588 iter/s, 10.7344s/100 iters), loss = 0.0515084
I1107 12:58:04.883901 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:58:04.883901 11160 solver.cpp:237]     Train net output #1: loss = 0.0515086 (* 1 = 0.0515086 loss)
I1107 12:58:04.883901 11160 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1107 12:58:13.413631 11160 solver.cpp:218] Iteration 107100 (11.7238 iter/s, 8.52965s/100 iters), loss = 0.0688728
I1107 12:58:13.413631 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 12:58:13.413631 11160 solver.cpp:237]     Train net output #1: loss = 0.068873 (* 1 = 0.068873 loss)
I1107 12:58:13.413631 11160 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1107 12:58:21.961941 11160 solver.cpp:218] Iteration 107200 (11.6997 iter/s, 8.54721s/100 iters), loss = 0.0625373
I1107 12:58:21.961941 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:58:21.961941 11160 solver.cpp:237]     Train net output #1: loss = 0.0625375 (* 1 = 0.0625375 loss)
I1107 12:58:21.961941 11160 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1107 12:58:30.500180 11160 solver.cpp:218] Iteration 107300 (11.7128 iter/s, 8.53767s/100 iters), loss = 0.0498505
I1107 12:58:30.500180 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:58:30.500180 11160 solver.cpp:237]     Train net output #1: loss = 0.0498508 (* 1 = 0.0498508 loss)
I1107 12:58:30.500180 11160 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1107 12:58:39.045383 11160 solver.cpp:218] Iteration 107400 (11.702 iter/s, 8.54554s/100 iters), loss = 0.0588922
I1107 12:58:39.046383 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:58:39.046383 11160 solver.cpp:237]     Train net output #1: loss = 0.0588924 (* 1 = 0.0588924 loss)
I1107 12:58:39.046383 11160 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1107 12:58:47.172559  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:58:47.510578 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_107500.caffemodel
I1107 12:58:47.569509 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_107500.solverstate
I1107 12:58:47.577512 11160 solver.cpp:330] Iteration 107500, Testing net (#0)
I1107 12:58:47.577512 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:58:49.569265 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:58:49.649010 11160 solver.cpp:397]     Test net output #0: accuracy = 0.917
I1107 12:58:49.649010 11160 solver.cpp:397]     Test net output #1: loss = 0.28888 (* 1 = 0.28888 loss)
I1107 12:58:49.730303 11160 solver.cpp:218] Iteration 107500 (9.35976 iter/s, 10.684s/100 iters), loss = 0.0369181
I1107 12:58:49.730303 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:58:49.730303 11160 solver.cpp:237]     Train net output #1: loss = 0.0369183 (* 1 = 0.0369183 loss)
I1107 12:58:49.730303 11160 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1107 12:58:58.248270 11160 solver.cpp:218] Iteration 107600 (11.7409 iter/s, 8.51723s/100 iters), loss = 0.0463283
I1107 12:58:58.248270 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:58:58.248270 11160 solver.cpp:237]     Train net output #1: loss = 0.0463285 (* 1 = 0.0463285 loss)
I1107 12:58:58.248270 11160 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1107 12:59:06.790858 11160 solver.cpp:218] Iteration 107700 (11.7071 iter/s, 8.54182s/100 iters), loss = 0.0401094
I1107 12:59:06.790858 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:59:06.790858 11160 solver.cpp:237]     Train net output #1: loss = 0.0401096 (* 1 = 0.0401096 loss)
I1107 12:59:06.790858 11160 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1107 12:59:15.366843 11160 solver.cpp:218] Iteration 107800 (11.6611 iter/s, 8.57555s/100 iters), loss = 0.0214898
I1107 12:59:15.366843 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:59:15.366843 11160 solver.cpp:237]     Train net output #1: loss = 0.02149 (* 1 = 0.02149 loss)
I1107 12:59:15.366843 11160 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1107 12:59:23.944656 11160 solver.cpp:218] Iteration 107900 (11.6588 iter/s, 8.57718s/100 iters), loss = 0.0465479
I1107 12:59:23.944656 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:59:23.944656 11160 solver.cpp:237]     Train net output #1: loss = 0.0465481 (* 1 = 0.0465481 loss)
I1107 12:59:23.944656 11160 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1107 12:59:32.110697  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:59:32.447737 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_108000.caffemodel
I1107 12:59:32.502756 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_108000.solverstate
I1107 12:59:32.511756 11160 solver.cpp:330] Iteration 108000, Testing net (#0)
I1107 12:59:32.511756 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 12:59:34.502991 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 12:59:34.582995 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9195
I1107 12:59:34.582995 11160 solver.cpp:397]     Test net output #1: loss = 0.28752 (* 1 = 0.28752 loss)
I1107 12:59:34.664497 11160 solver.cpp:218] Iteration 108000 (9.32883 iter/s, 10.7195s/100 iters), loss = 0.0416131
I1107 12:59:34.664497 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:59:34.664497 11160 solver.cpp:237]     Train net output #1: loss = 0.0416133 (* 1 = 0.0416133 loss)
I1107 12:59:34.664497 11160 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1107 12:59:43.211822 11160 solver.cpp:218] Iteration 108100 (11.7005 iter/s, 8.54666s/100 iters), loss = 0.0510882
I1107 12:59:43.211822 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 12:59:43.211822 11160 solver.cpp:237]     Train net output #1: loss = 0.0510884 (* 1 = 0.0510884 loss)
I1107 12:59:43.211822 11160 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1107 12:59:51.769783 11160 solver.cpp:218] Iteration 108200 (11.6858 iter/s, 8.55738s/100 iters), loss = 0.0282321
I1107 12:59:51.769783 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 12:59:51.770253 11160 solver.cpp:237]     Train net output #1: loss = 0.0282324 (* 1 = 0.0282324 loss)
I1107 12:59:51.770253 11160 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1107 13:00:00.373873 11160 solver.cpp:218] Iteration 108300 (11.6225 iter/s, 8.60402s/100 iters), loss = 0.0459385
I1107 13:00:00.374863 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:00:00.374863 11160 solver.cpp:237]     Train net output #1: loss = 0.0459387 (* 1 = 0.0459387 loss)
I1107 13:00:00.374863 11160 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1107 13:00:09.019951 11160 solver.cpp:218] Iteration 108400 (11.5672 iter/s, 8.64516s/100 iters), loss = 0.0503223
I1107 13:00:09.019951 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:00:09.019951 11160 solver.cpp:237]     Train net output #1: loss = 0.0503225 (* 1 = 0.0503225 loss)
I1107 13:00:09.019951 11160 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1107 13:00:17.159767  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:00:17.497807 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_108500.caffemodel
I1107 13:00:17.525813 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_108500.solverstate
I1107 13:00:17.534813 11160 solver.cpp:330] Iteration 108500, Testing net (#0)
I1107 13:00:17.534813 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:00:19.565984 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:00:19.646021 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 13:00:19.646021 11160 solver.cpp:397]     Test net output #1: loss = 0.291017 (* 1 = 0.291017 loss)
I1107 13:00:19.729053 11160 solver.cpp:218] Iteration 108500 (9.33888 iter/s, 10.7079s/100 iters), loss = 0.0475015
I1107 13:00:19.729053 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:00:19.729053 11160 solver.cpp:237]     Train net output #1: loss = 0.0475017 (* 1 = 0.0475017 loss)
I1107 13:00:19.729053 11160 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1107 13:00:28.360164 11160 solver.cpp:218] Iteration 108600 (11.586 iter/s, 8.63113s/100 iters), loss = 0.0392668
I1107 13:00:28.360164 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:00:28.360164 11160 solver.cpp:237]     Train net output #1: loss = 0.039267 (* 1 = 0.039267 loss)
I1107 13:00:28.360164 11160 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1107 13:00:36.991937 11160 solver.cpp:218] Iteration 108700 (11.5853 iter/s, 8.63166s/100 iters), loss = 0.0379
I1107 13:00:36.992938 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:00:36.992938 11160 solver.cpp:237]     Train net output #1: loss = 0.0379002 (* 1 = 0.0379002 loss)
I1107 13:00:36.992938 11160 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1107 13:00:45.777671 11160 solver.cpp:218] Iteration 108800 (11.3838 iter/s, 8.78442s/100 iters), loss = 0.0327918
I1107 13:00:45.777671 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:00:45.777671 11160 solver.cpp:237]     Train net output #1: loss = 0.032792 (* 1 = 0.032792 loss)
I1107 13:00:45.777671 11160 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1107 13:00:54.555493 11160 solver.cpp:218] Iteration 108900 (11.393 iter/s, 8.77732s/100 iters), loss = 0.0602118
I1107 13:00:54.555493 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:00:54.555493 11160 solver.cpp:237]     Train net output #1: loss = 0.060212 (* 1 = 0.060212 loss)
I1107 13:00:54.555493 11160 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1107 13:01:02.670855  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:01:03.007866 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_109000.caffemodel
I1107 13:01:03.038872 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_109000.solverstate
I1107 13:01:03.080958 11160 solver.cpp:330] Iteration 109000, Testing net (#0)
I1107 13:01:03.080958 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:01:05.069064 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:01:05.148566 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 13:01:05.148566 11160 solver.cpp:397]     Test net output #1: loss = 0.288539 (* 1 = 0.288539 loss)
I1107 13:01:05.230077 11160 solver.cpp:218] Iteration 109000 (9.36825 iter/s, 10.6744s/100 iters), loss = 0.0470471
I1107 13:01:05.230077 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:01:05.230077 11160 solver.cpp:237]     Train net output #1: loss = 0.0470473 (* 1 = 0.0470473 loss)
I1107 13:01:05.230077 11160 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1107 13:01:13.776863 11160 solver.cpp:218] Iteration 109100 (11.7016 iter/s, 8.54585s/100 iters), loss = 0.0776482
I1107 13:01:13.776863 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:01:13.776863 11160 solver.cpp:237]     Train net output #1: loss = 0.0776484 (* 1 = 0.0776484 loss)
I1107 13:01:13.776863 11160 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1107 13:01:22.322952 11160 solver.cpp:218] Iteration 109200 (11.7015 iter/s, 8.54592s/100 iters), loss = 0.0274364
I1107 13:01:22.322952 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:01:22.322952 11160 solver.cpp:237]     Train net output #1: loss = 0.0274366 (* 1 = 0.0274366 loss)
I1107 13:01:22.322952 11160 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1107 13:01:30.921264 11160 solver.cpp:218] Iteration 109300 (11.6315 iter/s, 8.59738s/100 iters), loss = 0.0345126
I1107 13:01:30.921264 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:01:30.921264 11160 solver.cpp:237]     Train net output #1: loss = 0.0345128 (* 1 = 0.0345128 loss)
I1107 13:01:30.921264 11160 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1107 13:01:39.532945 11160 solver.cpp:218] Iteration 109400 (11.6128 iter/s, 8.61116s/100 iters), loss = 0.0444476
I1107 13:01:39.532945 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:01:39.532945 11160 solver.cpp:237]     Train net output #1: loss = 0.0444478 (* 1 = 0.0444478 loss)
I1107 13:01:39.532945 11160 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1107 13:01:47.622834  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:01:47.960367 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_109500.caffemodel
I1107 13:01:47.993870 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_109500.solverstate
I1107 13:01:48.002871 11160 solver.cpp:330] Iteration 109500, Testing net (#0)
I1107 13:01:48.002871 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:01:49.993043 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:01:50.073045 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 13:01:50.073045 11160 solver.cpp:397]     Test net output #1: loss = 0.289092 (* 1 = 0.289092 loss)
I1107 13:01:50.154045 11160 solver.cpp:218] Iteration 109500 (9.41511 iter/s, 10.6212s/100 iters), loss = 0.0432812
I1107 13:01:50.154045 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:01:50.154045 11160 solver.cpp:237]     Train net output #1: loss = 0.0432814 (* 1 = 0.0432814 loss)
I1107 13:01:50.154045 11160 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1107 13:01:58.684098 11160 solver.cpp:218] Iteration 109600 (11.7247 iter/s, 8.529s/100 iters), loss = 0.0523203
I1107 13:01:58.684098 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:01:58.684098 11160 solver.cpp:237]     Train net output #1: loss = 0.0523205 (* 1 = 0.0523205 loss)
I1107 13:01:58.684098 11160 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1107 13:02:07.207923 11160 solver.cpp:218] Iteration 109700 (11.7323 iter/s, 8.52345s/100 iters), loss = 0.0365656
I1107 13:02:07.207923 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:02:07.207923 11160 solver.cpp:237]     Train net output #1: loss = 0.0365658 (* 1 = 0.0365658 loss)
I1107 13:02:07.207923 11160 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1107 13:02:15.721643 11160 solver.cpp:218] Iteration 109800 (11.7465 iter/s, 8.5132s/100 iters), loss = 0.0394508
I1107 13:02:15.721643 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:02:15.721643 11160 solver.cpp:237]     Train net output #1: loss = 0.039451 (* 1 = 0.039451 loss)
I1107 13:02:15.721643 11160 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1107 13:02:24.250694 11160 solver.cpp:218] Iteration 109900 (11.7255 iter/s, 8.52844s/100 iters), loss = 0.0391198
I1107 13:02:24.250694 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:02:24.250694 11160 solver.cpp:237]     Train net output #1: loss = 0.03912 (* 1 = 0.03912 loss)
I1107 13:02:24.250694 11160 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1107 13:02:32.355572  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:02:32.691645 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_110000.caffemodel
I1107 13:02:32.721647 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_110000.solverstate
I1107 13:02:32.746645 11160 solver.cpp:330] Iteration 110000, Testing net (#0)
I1107 13:02:32.746645 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:02:34.732820 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:02:34.812822 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 13:02:34.812822 11160 solver.cpp:397]     Test net output #1: loss = 0.291645 (* 1 = 0.291645 loss)
I1107 13:02:34.894839 11160 solver.cpp:218] Iteration 110000 (9.39558 iter/s, 10.6433s/100 iters), loss = 0.0514704
I1107 13:02:34.894839 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:02:34.894839 11160 solver.cpp:237]     Train net output #1: loss = 0.0514706 (* 1 = 0.0514706 loss)
I1107 13:02:34.894839 11160 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1107 13:02:43.422093 11160 solver.cpp:218] Iteration 110100 (11.7278 iter/s, 8.52674s/100 iters), loss = 0.0525039
I1107 13:02:43.422093 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:02:43.422093 11160 solver.cpp:237]     Train net output #1: loss = 0.0525042 (* 1 = 0.0525042 loss)
I1107 13:02:43.422093 11160 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1107 13:02:51.943625 11160 solver.cpp:218] Iteration 110200 (11.7345 iter/s, 8.52185s/100 iters), loss = 0.0320012
I1107 13:02:51.943625 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:02:51.943625 11160 solver.cpp:237]     Train net output #1: loss = 0.0320015 (* 1 = 0.0320015 loss)
I1107 13:02:51.944624 11160 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1107 13:03:00.466987 11160 solver.cpp:218] Iteration 110300 (11.7337 iter/s, 8.52246s/100 iters), loss = 0.0246466
I1107 13:03:00.466987 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:03:00.466987 11160 solver.cpp:237]     Train net output #1: loss = 0.0246468 (* 1 = 0.0246468 loss)
I1107 13:03:00.466987 11160 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1107 13:03:08.987543 11160 solver.cpp:218] Iteration 110400 (11.7365 iter/s, 8.52042s/100 iters), loss = 0.041907
I1107 13:03:08.987543 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:03:08.988554 11160 solver.cpp:237]     Train net output #1: loss = 0.0419072 (* 1 = 0.0419072 loss)
I1107 13:03:08.988554 11160 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1107 13:03:17.096443  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:03:17.434460 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_110500.caffemodel
I1107 13:03:17.463459 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_110500.solverstate
I1107 13:03:17.472460 11160 solver.cpp:330] Iteration 110500, Testing net (#0)
I1107 13:03:17.472460 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:03:19.459595 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:03:19.539651 11160 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1107 13:03:19.539651 11160 solver.cpp:397]     Test net output #1: loss = 0.292246 (* 1 = 0.292246 loss)
I1107 13:03:19.620678 11160 solver.cpp:218] Iteration 110500 (9.40507 iter/s, 10.6326s/100 iters), loss = 0.0486386
I1107 13:03:19.621680 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:03:19.621680 11160 solver.cpp:237]     Train net output #1: loss = 0.0486388 (* 1 = 0.0486388 loss)
I1107 13:03:19.621680 11160 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1107 13:03:28.157804 11160 solver.cpp:218] Iteration 110600 (11.7149 iter/s, 8.53612s/100 iters), loss = 0.0743582
I1107 13:03:28.157804 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 13:03:28.157804 11160 solver.cpp:237]     Train net output #1: loss = 0.0743584 (* 1 = 0.0743584 loss)
I1107 13:03:28.157804 11160 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1107 13:03:36.688508 11160 solver.cpp:218] Iteration 110700 (11.7235 iter/s, 8.52987s/100 iters), loss = 0.0299269
I1107 13:03:36.688508 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:03:36.688508 11160 solver.cpp:237]     Train net output #1: loss = 0.0299271 (* 1 = 0.0299271 loss)
I1107 13:03:36.688508 11160 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1107 13:03:45.217669 11160 solver.cpp:218] Iteration 110800 (11.7246 iter/s, 8.52911s/100 iters), loss = 0.0217194
I1107 13:03:45.217669 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:03:45.217669 11160 solver.cpp:237]     Train net output #1: loss = 0.0217196 (* 1 = 0.0217196 loss)
I1107 13:03:45.217669 11160 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1107 13:03:53.747066 11160 solver.cpp:218] Iteration 110900 (11.7245 iter/s, 8.52912s/100 iters), loss = 0.0515827
I1107 13:03:53.747066 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:03:53.747066 11160 solver.cpp:237]     Train net output #1: loss = 0.0515829 (* 1 = 0.0515829 loss)
I1107 13:03:53.747066 11160 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1107 13:04:01.863265  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:04:02.200318 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_111000.caffemodel
I1107 13:04:02.231318 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_111000.solverstate
I1107 13:04:02.276317 11160 solver.cpp:330] Iteration 111000, Testing net (#0)
I1107 13:04:02.276317 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:04:04.265549 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:04:04.345553 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9189
I1107 13:04:04.345553 11160 solver.cpp:397]     Test net output #1: loss = 0.294695 (* 1 = 0.294695 loss)
I1107 13:04:04.426558 11160 solver.cpp:218] Iteration 111000 (9.36416 iter/s, 10.679s/100 iters), loss = 0.0327635
I1107 13:04:04.426558 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:04:04.426558 11160 solver.cpp:237]     Train net output #1: loss = 0.0327637 (* 1 = 0.0327637 loss)
I1107 13:04:04.426558 11160 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1107 13:04:12.949062 11160 solver.cpp:218] Iteration 111100 (11.7352 iter/s, 8.52138s/100 iters), loss = 0.0502234
I1107 13:04:12.949062 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:04:12.949062 11160 solver.cpp:237]     Train net output #1: loss = 0.0502236 (* 1 = 0.0502236 loss)
I1107 13:04:12.949062 11160 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1107 13:04:21.471647 11160 solver.cpp:218] Iteration 111200 (11.7334 iter/s, 8.52265s/100 iters), loss = 0.0715717
I1107 13:04:21.471647 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:04:21.471647 11160 solver.cpp:237]     Train net output #1: loss = 0.0715719 (* 1 = 0.0715719 loss)
I1107 13:04:21.471647 11160 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1107 13:04:30.000459 11160 solver.cpp:218] Iteration 111300 (11.7261 iter/s, 8.52797s/100 iters), loss = 0.0271048
I1107 13:04:30.000459 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:04:30.000459 11160 solver.cpp:237]     Train net output #1: loss = 0.027105 (* 1 = 0.027105 loss)
I1107 13:04:30.000459 11160 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1107 13:04:38.535022 11160 solver.cpp:218] Iteration 111400 (11.7182 iter/s, 8.53377s/100 iters), loss = 0.0328727
I1107 13:04:38.535022 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:04:38.535022 11160 solver.cpp:237]     Train net output #1: loss = 0.0328729 (* 1 = 0.0328729 loss)
I1107 13:04:38.535022 11160 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1107 13:04:46.652683  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:04:46.991344 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_111500.caffemodel
I1107 13:04:47.023847 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_111500.solverstate
I1107 13:04:47.032847 11160 solver.cpp:330] Iteration 111500, Testing net (#0)
I1107 13:04:47.032847 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:04:49.020050 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:04:49.100057 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1107 13:04:49.100057 11160 solver.cpp:397]     Test net output #1: loss = 0.295585 (* 1 = 0.295585 loss)
I1107 13:04:49.182562 11160 solver.cpp:218] Iteration 111500 (9.39238 iter/s, 10.6469s/100 iters), loss = 0.0380501
I1107 13:04:49.182562 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:04:49.182562 11160 solver.cpp:237]     Train net output #1: loss = 0.0380503 (* 1 = 0.0380503 loss)
I1107 13:04:49.182562 11160 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1107 13:04:57.708369 11160 solver.cpp:218] Iteration 111600 (11.7295 iter/s, 8.52555s/100 iters), loss = 0.0351962
I1107 13:04:57.708369 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:04:57.708369 11160 solver.cpp:237]     Train net output #1: loss = 0.0351964 (* 1 = 0.0351964 loss)
I1107 13:04:57.708369 11160 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1107 13:05:06.241394 11160 solver.cpp:218] Iteration 111700 (11.7203 iter/s, 8.53223s/100 iters), loss = 0.033064
I1107 13:05:06.241394 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:05:06.241394 11160 solver.cpp:237]     Train net output #1: loss = 0.0330641 (* 1 = 0.0330641 loss)
I1107 13:05:06.241394 11160 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1107 13:05:14.770539 11160 solver.cpp:218] Iteration 111800 (11.7243 iter/s, 8.52929s/100 iters), loss = 0.0297316
I1107 13:05:14.770539 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:05:14.770539 11160 solver.cpp:237]     Train net output #1: loss = 0.0297318 (* 1 = 0.0297318 loss)
I1107 13:05:14.770539 11160 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1107 13:05:23.297565 11160 solver.cpp:218] Iteration 111900 (11.7281 iter/s, 8.52651s/100 iters), loss = 0.0508252
I1107 13:05:23.297565 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:05:23.297565 11160 solver.cpp:237]     Train net output #1: loss = 0.0508254 (* 1 = 0.0508254 loss)
I1107 13:05:23.297565 11160 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1107 13:05:31.408444  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:05:31.744521 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_112000.caffemodel
I1107 13:05:31.774534 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_112000.solverstate
I1107 13:05:31.797554 11160 solver.cpp:330] Iteration 112000, Testing net (#0)
I1107 13:05:31.797554 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:05:33.788081 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:05:33.867617 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9189
I1107 13:05:33.868116 11160 solver.cpp:397]     Test net output #1: loss = 0.288863 (* 1 = 0.288863 loss)
I1107 13:05:33.949120 11160 solver.cpp:218] Iteration 112000 (9.38893 iter/s, 10.6508s/100 iters), loss = 0.0387851
I1107 13:05:33.949120 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:05:33.949120 11160 solver.cpp:237]     Train net output #1: loss = 0.0387853 (* 1 = 0.0387853 loss)
I1107 13:05:33.949120 11160 sgd_solver.cpp:105] Iteration 112000, lr = 0.001
I1107 13:05:42.481132 11160 solver.cpp:218] Iteration 112100 (11.7218 iter/s, 8.53111s/100 iters), loss = 0.0737736
I1107 13:05:42.481132 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:05:42.481132 11160 solver.cpp:237]     Train net output #1: loss = 0.0737738 (* 1 = 0.0737738 loss)
I1107 13:05:42.481132 11160 sgd_solver.cpp:105] Iteration 112100, lr = 0.001
I1107 13:05:51.002454 11160 solver.cpp:218] Iteration 112200 (11.7359 iter/s, 8.52084s/100 iters), loss = 0.0337144
I1107 13:05:51.002454 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:05:51.002454 11160 solver.cpp:237]     Train net output #1: loss = 0.0337146 (* 1 = 0.0337146 loss)
I1107 13:05:51.002454 11160 sgd_solver.cpp:105] Iteration 112200, lr = 0.001
I1107 13:05:59.523332 11160 solver.cpp:218] Iteration 112300 (11.7369 iter/s, 8.52017s/100 iters), loss = 0.0524414
I1107 13:05:59.523332 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:05:59.523332 11160 solver.cpp:237]     Train net output #1: loss = 0.0524416 (* 1 = 0.0524416 loss)
I1107 13:05:59.523332 11160 sgd_solver.cpp:105] Iteration 112300, lr = 0.001
I1107 13:06:08.037693 11160 solver.cpp:218] Iteration 112400 (11.745 iter/s, 8.51425s/100 iters), loss = 0.0327939
I1107 13:06:08.037693 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:06:08.037693 11160 solver.cpp:237]     Train net output #1: loss = 0.0327941 (* 1 = 0.0327941 loss)
I1107 13:06:08.037693 11160 sgd_solver.cpp:105] Iteration 112400, lr = 0.001
I1107 13:06:16.134726  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:06:16.471729 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_112500.caffemodel
I1107 13:06:16.526736 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_112500.solverstate
I1107 13:06:16.535737 11160 solver.cpp:330] Iteration 112500, Testing net (#0)
I1107 13:06:16.535737 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:06:18.528833 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:06:18.608839 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9175
I1107 13:06:18.608839 11160 solver.cpp:397]     Test net output #1: loss = 0.295752 (* 1 = 0.295752 loss)
I1107 13:06:18.689844 11160 solver.cpp:218] Iteration 112500 (9.38821 iter/s, 10.6517s/100 iters), loss = 0.0611117
I1107 13:06:18.689844 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:06:18.689844 11160 solver.cpp:237]     Train net output #1: loss = 0.0611119 (* 1 = 0.0611119 loss)
I1107 13:06:18.689844 11160 sgd_solver.cpp:105] Iteration 112500, lr = 0.001
I1107 13:06:27.212884 11160 solver.cpp:218] Iteration 112600 (11.7338 iter/s, 8.52241s/100 iters), loss = 0.0498151
I1107 13:06:27.212884 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:06:27.212884 11160 solver.cpp:237]     Train net output #1: loss = 0.0498153 (* 1 = 0.0498153 loss)
I1107 13:06:27.212884 11160 sgd_solver.cpp:105] Iteration 112600, lr = 0.001
I1107 13:06:35.724715 11160 solver.cpp:218] Iteration 112700 (11.7494 iter/s, 8.51108s/100 iters), loss = 0.0393707
I1107 13:06:35.724715 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:06:35.724715 11160 solver.cpp:237]     Train net output #1: loss = 0.0393708 (* 1 = 0.0393708 loss)
I1107 13:06:35.724715 11160 sgd_solver.cpp:105] Iteration 112700, lr = 0.001
I1107 13:06:44.245452 11160 solver.cpp:218] Iteration 112800 (11.7356 iter/s, 8.52107s/100 iters), loss = 0.0424975
I1107 13:06:44.245452 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:06:44.245452 11160 solver.cpp:237]     Train net output #1: loss = 0.0424977 (* 1 = 0.0424977 loss)
I1107 13:06:44.245452 11160 sgd_solver.cpp:105] Iteration 112800, lr = 0.001
I1107 13:06:52.801128 11160 solver.cpp:218] Iteration 112900 (11.6898 iter/s, 8.55449s/100 iters), loss = 0.046037
I1107 13:06:52.801128 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:06:52.801128 11160 solver.cpp:237]     Train net output #1: loss = 0.0460371 (* 1 = 0.0460371 loss)
I1107 13:06:52.801128 11160 sgd_solver.cpp:105] Iteration 112900, lr = 0.001
I1107 13:07:00.917213  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:07:01.253260 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_113000.caffemodel
I1107 13:07:01.310281 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_113000.solverstate
I1107 13:07:01.322294 11160 solver.cpp:330] Iteration 113000, Testing net (#0)
I1107 13:07:01.322294 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:07:03.314450 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:07:03.392956 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1107 13:07:03.392956 11160 solver.cpp:397]     Test net output #1: loss = 0.290701 (* 1 = 0.290701 loss)
I1107 13:07:03.474457 11160 solver.cpp:218] Iteration 113000 (9.36994 iter/s, 10.6724s/100 iters), loss = 0.050786
I1107 13:07:03.474457 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:07:03.474457 11160 solver.cpp:237]     Train net output #1: loss = 0.0507862 (* 1 = 0.0507862 loss)
I1107 13:07:03.474457 11160 sgd_solver.cpp:105] Iteration 113000, lr = 0.001
I1107 13:07:11.997638 11160 solver.cpp:218] Iteration 113100 (11.733 iter/s, 8.52299s/100 iters), loss = 0.040375
I1107 13:07:11.997638 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:07:11.997638 11160 solver.cpp:237]     Train net output #1: loss = 0.0403752 (* 1 = 0.0403752 loss)
I1107 13:07:11.997638 11160 sgd_solver.cpp:105] Iteration 113100, lr = 0.001
I1107 13:07:20.566011 11160 solver.cpp:218] Iteration 113200 (11.6717 iter/s, 8.56774s/100 iters), loss = 0.0232543
I1107 13:07:20.566011 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:07:20.566011 11160 solver.cpp:237]     Train net output #1: loss = 0.0232545 (* 1 = 0.0232545 loss)
I1107 13:07:20.566011 11160 sgd_solver.cpp:105] Iteration 113200, lr = 0.001
I1107 13:07:29.079890 11160 solver.cpp:218] Iteration 113300 (11.7466 iter/s, 8.51311s/100 iters), loss = 0.0481585
I1107 13:07:29.079890 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:07:29.079890 11160 solver.cpp:237]     Train net output #1: loss = 0.0481587 (* 1 = 0.0481587 loss)
I1107 13:07:29.079890 11160 sgd_solver.cpp:105] Iteration 113300, lr = 0.001
I1107 13:07:37.599702 11160 solver.cpp:218] Iteration 113400 (11.737 iter/s, 8.52004s/100 iters), loss = 0.0338058
I1107 13:07:37.599702 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:07:37.599702 11160 solver.cpp:237]     Train net output #1: loss = 0.0338059 (* 1 = 0.0338059 loss)
I1107 13:07:37.599702 11160 sgd_solver.cpp:105] Iteration 113400, lr = 0.001
I1107 13:07:45.706670  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:07:46.044307 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_113500.caffemodel
I1107 13:07:46.099501 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_113500.solverstate
I1107 13:07:46.108502 11160 solver.cpp:330] Iteration 113500, Testing net (#0)
I1107 13:07:46.108502 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:07:48.098533 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:07:48.177532 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1107 13:07:48.177532 11160 solver.cpp:397]     Test net output #1: loss = 0.299352 (* 1 = 0.299352 loss)
I1107 13:07:48.259577 11160 solver.cpp:218] Iteration 113500 (9.38211 iter/s, 10.6586s/100 iters), loss = 0.0359249
I1107 13:07:48.259577 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:07:48.259577 11160 solver.cpp:237]     Train net output #1: loss = 0.0359251 (* 1 = 0.0359251 loss)
I1107 13:07:48.259577 11160 sgd_solver.cpp:105] Iteration 113500, lr = 0.001
I1107 13:07:56.836838 11160 solver.cpp:218] Iteration 113600 (11.6595 iter/s, 8.57673s/100 iters), loss = 0.0388823
I1107 13:07:56.836838 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:07:56.836838 11160 solver.cpp:237]     Train net output #1: loss = 0.0388825 (* 1 = 0.0388825 loss)
I1107 13:07:56.836838 11160 sgd_solver.cpp:105] Iteration 113600, lr = 0.001
I1107 13:08:05.524098 11160 solver.cpp:218] Iteration 113700 (11.5112 iter/s, 8.68721s/100 iters), loss = 0.0274111
I1107 13:08:05.524098 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:08:05.524098 11160 solver.cpp:237]     Train net output #1: loss = 0.0274112 (* 1 = 0.0274112 loss)
I1107 13:08:05.524098 11160 sgd_solver.cpp:105] Iteration 113700, lr = 0.001
I1107 13:08:14.104267 11160 solver.cpp:218] Iteration 113800 (11.6557 iter/s, 8.57946s/100 iters), loss = 0.0267369
I1107 13:08:14.104267 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:08:14.104267 11160 solver.cpp:237]     Train net output #1: loss = 0.0267371 (* 1 = 0.0267371 loss)
I1107 13:08:14.104267 11160 sgd_solver.cpp:105] Iteration 113800, lr = 0.001
I1107 13:08:22.706210 11160 solver.cpp:218] Iteration 113900 (11.6262 iter/s, 8.60126s/100 iters), loss = 0.0333053
I1107 13:08:22.706210 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:08:22.706210 11160 solver.cpp:237]     Train net output #1: loss = 0.0333055 (* 1 = 0.0333055 loss)
I1107 13:08:22.706210 11160 sgd_solver.cpp:105] Iteration 113900, lr = 0.001
I1107 13:08:30.865808  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:08:31.205845 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_114000.caffemodel
I1107 13:08:31.262845 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_114000.solverstate
I1107 13:08:31.272863 11160 solver.cpp:330] Iteration 114000, Testing net (#0)
I1107 13:08:31.272863 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:08:33.276831 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:08:33.356848 11160 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1107 13:08:33.356848 11160 solver.cpp:397]     Test net output #1: loss = 0.298594 (* 1 = 0.298594 loss)
I1107 13:08:33.438863 11160 solver.cpp:218] Iteration 114000 (9.31802 iter/s, 10.7319s/100 iters), loss = 0.0350461
I1107 13:08:33.438863 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:08:33.438863 11160 solver.cpp:237]     Train net output #1: loss = 0.0350463 (* 1 = 0.0350463 loss)
I1107 13:08:33.438863 11160 sgd_solver.cpp:105] Iteration 114000, lr = 0.001
I1107 13:08:41.998049 11160 solver.cpp:218] Iteration 114100 (11.6838 iter/s, 8.55884s/100 iters), loss = 0.0385394
I1107 13:08:41.998049 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:08:41.998049 11160 solver.cpp:237]     Train net output #1: loss = 0.0385396 (* 1 = 0.0385396 loss)
I1107 13:08:41.998049 11160 sgd_solver.cpp:105] Iteration 114100, lr = 0.001
I1107 13:08:50.549800 11160 solver.cpp:218] Iteration 114200 (11.6937 iter/s, 8.55161s/100 iters), loss = 0.0317178
I1107 13:08:50.549800 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:08:50.549800 11160 solver.cpp:237]     Train net output #1: loss = 0.0317179 (* 1 = 0.0317179 loss)
I1107 13:08:50.549800 11160 sgd_solver.cpp:105] Iteration 114200, lr = 0.001
I1107 13:08:59.169910 11160 solver.cpp:218] Iteration 114300 (11.6019 iter/s, 8.61928s/100 iters), loss = 0.0309035
I1107 13:08:59.169910 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:08:59.169910 11160 solver.cpp:237]     Train net output #1: loss = 0.0309037 (* 1 = 0.0309037 loss)
I1107 13:08:59.169910 11160 sgd_solver.cpp:105] Iteration 114300, lr = 0.001
I1107 13:09:07.753695 11160 solver.cpp:218] Iteration 114400 (11.65 iter/s, 8.58371s/100 iters), loss = 0.0408771
I1107 13:09:07.753695 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:09:07.753695 11160 solver.cpp:237]     Train net output #1: loss = 0.0408773 (* 1 = 0.0408773 loss)
I1107 13:09:07.753695 11160 sgd_solver.cpp:105] Iteration 114400, lr = 0.001
I1107 13:09:15.908516  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:09:16.248550 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_114500.caffemodel
I1107 13:09:16.304548 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_114500.solverstate
I1107 13:09:16.315551 11160 solver.cpp:330] Iteration 114500, Testing net (#0)
I1107 13:09:16.315551 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:09:18.349117 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:09:18.434623 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9163
I1107 13:09:18.434623 11160 solver.cpp:397]     Test net output #1: loss = 0.304022 (* 1 = 0.304022 loss)
I1107 13:09:18.522125 11160 solver.cpp:218] Iteration 114500 (9.28694 iter/s, 10.7678s/100 iters), loss = 0.0383013
I1107 13:09:18.522125 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:09:18.522125 11160 solver.cpp:237]     Train net output #1: loss = 0.0383015 (* 1 = 0.0383015 loss)
I1107 13:09:18.522125 11160 sgd_solver.cpp:105] Iteration 114500, lr = 0.001
I1107 13:09:27.169392 11160 solver.cpp:218] Iteration 114600 (11.566 iter/s, 8.646s/100 iters), loss = 0.0814083
I1107 13:09:27.169392 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 13:09:27.169392 11160 solver.cpp:237]     Train net output #1: loss = 0.0814085 (* 1 = 0.0814085 loss)
I1107 13:09:27.169392 11160 sgd_solver.cpp:105] Iteration 114600, lr = 0.001
I1107 13:09:35.805027 11160 solver.cpp:218] Iteration 114700 (11.5809 iter/s, 8.63492s/100 iters), loss = 0.0298442
I1107 13:09:35.805027 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:09:35.805027 11160 solver.cpp:237]     Train net output #1: loss = 0.0298443 (* 1 = 0.0298443 loss)
I1107 13:09:35.805027 11160 sgd_solver.cpp:105] Iteration 114700, lr = 0.001
I1107 13:09:44.397563 11160 solver.cpp:218] Iteration 114800 (11.6387 iter/s, 8.59205s/100 iters), loss = 0.0336429
I1107 13:09:44.397563 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:09:44.397563 11160 solver.cpp:237]     Train net output #1: loss = 0.033643 (* 1 = 0.033643 loss)
I1107 13:09:44.397563 11160 sgd_solver.cpp:105] Iteration 114800, lr = 0.001
I1107 13:09:52.976544 11160 solver.cpp:218] Iteration 114900 (11.6568 iter/s, 8.57868s/100 iters), loss = 0.0332216
I1107 13:09:52.976544 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:09:52.976544 11160 solver.cpp:237]     Train net output #1: loss = 0.0332218 (* 1 = 0.0332218 loss)
I1107 13:09:52.976544 11160 sgd_solver.cpp:105] Iteration 114900, lr = 0.001
I1107 13:10:01.147574  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:10:01.492617 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_115000.caffemodel
I1107 13:10:01.520617 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_115000.solverstate
I1107 13:10:01.583122 11160 solver.cpp:330] Iteration 115000, Testing net (#0)
I1107 13:10:01.583122 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:10:03.599709 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:10:03.680712 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9188
I1107 13:10:03.680712 11160 solver.cpp:397]     Test net output #1: loss = 0.298628 (* 1 = 0.298628 loss)
I1107 13:10:03.762717 11160 solver.cpp:218] Iteration 115000 (9.27207 iter/s, 10.7851s/100 iters), loss = 0.0457463
I1107 13:10:03.762717 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:10:03.762717 11160 solver.cpp:237]     Train net output #1: loss = 0.0457465 (* 1 = 0.0457465 loss)
I1107 13:10:03.762717 11160 sgd_solver.cpp:105] Iteration 115000, lr = 0.001
I1107 13:10:12.320698 11160 solver.cpp:218] Iteration 115100 (11.6855 iter/s, 8.55761s/100 iters), loss = 0.0649697
I1107 13:10:12.320698 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:10:12.320698 11160 solver.cpp:237]     Train net output #1: loss = 0.0649699 (* 1 = 0.0649699 loss)
I1107 13:10:12.320698 11160 sgd_solver.cpp:105] Iteration 115100, lr = 0.001
I1107 13:10:20.863450 11160 solver.cpp:218] Iteration 115200 (11.7073 iter/s, 8.5417s/100 iters), loss = 0.0359509
I1107 13:10:20.863450 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:10:20.863450 11160 solver.cpp:237]     Train net output #1: loss = 0.0359511 (* 1 = 0.0359511 loss)
I1107 13:10:20.863450 11160 sgd_solver.cpp:105] Iteration 115200, lr = 0.001
I1107 13:10:29.426925 11160 solver.cpp:218] Iteration 115300 (11.6779 iter/s, 8.5632s/100 iters), loss = 0.0254581
I1107 13:10:29.426925 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:10:29.426925 11160 solver.cpp:237]     Train net output #1: loss = 0.0254583 (* 1 = 0.0254583 loss)
I1107 13:10:29.426925 11160 sgd_solver.cpp:105] Iteration 115300, lr = 0.001
I1107 13:10:37.969753 11160 solver.cpp:218] Iteration 115400 (11.7064 iter/s, 8.54237s/100 iters), loss = 0.0287447
I1107 13:10:37.969753 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:10:37.969753 11160 solver.cpp:237]     Train net output #1: loss = 0.0287449 (* 1 = 0.0287449 loss)
I1107 13:10:37.969753 11160 sgd_solver.cpp:105] Iteration 115400, lr = 0.001
I1107 13:10:46.111517  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:10:46.452533 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_115500.caffemodel
I1107 13:10:46.487038 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_115500.solverstate
I1107 13:10:46.495543 11160 solver.cpp:330] Iteration 115500, Testing net (#0)
I1107 13:10:46.495543 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:10:48.497746 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:10:48.577750 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1107 13:10:48.577750 11160 solver.cpp:397]     Test net output #1: loss = 0.29971 (* 1 = 0.29971 loss)
I1107 13:10:48.658756 11160 solver.cpp:218] Iteration 115500 (9.3555 iter/s, 10.6889s/100 iters), loss = 0.0355643
I1107 13:10:48.658756 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:10:48.658756 11160 solver.cpp:237]     Train net output #1: loss = 0.0355644 (* 1 = 0.0355644 loss)
I1107 13:10:48.658756 11160 sgd_solver.cpp:105] Iteration 115500, lr = 0.001
I1107 13:10:57.227521 11160 solver.cpp:218] Iteration 115600 (11.6717 iter/s, 8.56774s/100 iters), loss = 0.0555271
I1107 13:10:57.227521 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:10:57.227521 11160 solver.cpp:237]     Train net output #1: loss = 0.0555272 (* 1 = 0.0555272 loss)
I1107 13:10:57.227521 11160 sgd_solver.cpp:105] Iteration 115600, lr = 0.001
I1107 13:11:05.797459 11160 solver.cpp:218] Iteration 115700 (11.6686 iter/s, 8.56998s/100 iters), loss = 0.0286269
I1107 13:11:05.797459 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:11:05.797459 11160 solver.cpp:237]     Train net output #1: loss = 0.0286271 (* 1 = 0.0286271 loss)
I1107 13:11:05.797459 11160 sgd_solver.cpp:105] Iteration 115700, lr = 0.001
I1107 13:11:14.373765 11160 solver.cpp:218] Iteration 115800 (11.6609 iter/s, 8.57564s/100 iters), loss = 0.0241823
I1107 13:11:14.373765 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:11:14.373765 11160 solver.cpp:237]     Train net output #1: loss = 0.0241824 (* 1 = 0.0241824 loss)
I1107 13:11:14.373765 11160 sgd_solver.cpp:105] Iteration 115800, lr = 0.001
I1107 13:11:22.961066 11160 solver.cpp:218] Iteration 115900 (11.6466 iter/s, 8.58617s/100 iters), loss = 0.037718
I1107 13:11:22.961066 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:11:22.961066 11160 solver.cpp:237]     Train net output #1: loss = 0.0377182 (* 1 = 0.0377182 loss)
I1107 13:11:22.961066 11160 sgd_solver.cpp:105] Iteration 115900, lr = 0.001
I1107 13:11:31.127113  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:11:31.465137 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_116000.caffemodel
I1107 13:11:31.493142 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_116000.solverstate
I1107 13:11:31.516146 11160 solver.cpp:330] Iteration 116000, Testing net (#0)
I1107 13:11:31.516146 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:11:33.519286 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:11:33.599788 11160 solver.cpp:397]     Test net output #0: accuracy = 0.916
I1107 13:11:33.599788 11160 solver.cpp:397]     Test net output #1: loss = 0.303777 (* 1 = 0.303777 loss)
I1107 13:11:33.680292 11160 solver.cpp:218] Iteration 116000 (9.3288 iter/s, 10.7195s/100 iters), loss = 0.0455477
I1107 13:11:33.680292 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:11:33.680292 11160 solver.cpp:237]     Train net output #1: loss = 0.0455478 (* 1 = 0.0455478 loss)
I1107 13:11:33.680292 11160 sgd_solver.cpp:105] Iteration 116000, lr = 0.001
I1107 13:11:42.237324 11160 solver.cpp:218] Iteration 116100 (11.6875 iter/s, 8.55618s/100 iters), loss = 0.033705
I1107 13:11:42.237324 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:11:42.237324 11160 solver.cpp:237]     Train net output #1: loss = 0.0337051 (* 1 = 0.0337051 loss)
I1107 13:11:42.237324 11160 sgd_solver.cpp:105] Iteration 116100, lr = 0.001
I1107 13:11:50.815599 11160 solver.cpp:218] Iteration 116200 (11.6583 iter/s, 8.57755s/100 iters), loss = 0.0298171
I1107 13:11:50.815599 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:11:50.815599 11160 solver.cpp:237]     Train net output #1: loss = 0.0298172 (* 1 = 0.0298172 loss)
I1107 13:11:50.815599 11160 sgd_solver.cpp:105] Iteration 116200, lr = 0.001
I1107 13:11:59.448323 11160 solver.cpp:218] Iteration 116300 (11.5841 iter/s, 8.63256s/100 iters), loss = 0.0253483
I1107 13:11:59.448323 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:11:59.448323 11160 solver.cpp:237]     Train net output #1: loss = 0.0253485 (* 1 = 0.0253485 loss)
I1107 13:11:59.448323 11160 sgd_solver.cpp:105] Iteration 116300, lr = 0.001
I1107 13:12:08.034340 11160 solver.cpp:218] Iteration 116400 (11.6479 iter/s, 8.58522s/100 iters), loss = 0.0334289
I1107 13:12:08.034340 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:12:08.034340 11160 solver.cpp:237]     Train net output #1: loss = 0.033429 (* 1 = 0.033429 loss)
I1107 13:12:08.034340 11160 sgd_solver.cpp:105] Iteration 116400, lr = 0.001
I1107 13:12:16.174279  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:12:16.514318 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_116500.caffemodel
I1107 13:12:16.544317 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_116500.solverstate
I1107 13:12:16.553318 11160 solver.cpp:330] Iteration 116500, Testing net (#0)
I1107 13:12:16.553318 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:12:18.546617 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:12:18.625640 11160 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 13:12:18.625640 11160 solver.cpp:397]     Test net output #1: loss = 0.298011 (* 1 = 0.298011 loss)
I1107 13:12:18.707641 11160 solver.cpp:218] Iteration 116500 (9.36972 iter/s, 10.6727s/100 iters), loss = 0.0437976
I1107 13:12:18.707641 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:12:18.707641 11160 solver.cpp:237]     Train net output #1: loss = 0.0437977 (* 1 = 0.0437977 loss)
I1107 13:12:18.707641 11160 sgd_solver.cpp:105] Iteration 116500, lr = 0.001
I1107 13:12:27.233479 11160 solver.cpp:218] Iteration 116600 (11.73 iter/s, 8.52513s/100 iters), loss = 0.0447696
I1107 13:12:27.233479 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:12:27.233479 11160 solver.cpp:237]     Train net output #1: loss = 0.0447697 (* 1 = 0.0447697 loss)
I1107 13:12:27.233479 11160 sgd_solver.cpp:105] Iteration 116600, lr = 0.001
I1107 13:12:35.756906 11160 solver.cpp:218] Iteration 116700 (11.7324 iter/s, 8.52344s/100 iters), loss = 0.0403515
I1107 13:12:35.757921 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:12:35.757921 11160 solver.cpp:237]     Train net output #1: loss = 0.0403516 (* 1 = 0.0403516 loss)
I1107 13:12:35.757921 11160 sgd_solver.cpp:105] Iteration 116700, lr = 0.001
I1107 13:12:44.294212 11160 solver.cpp:218] Iteration 116800 (11.7151 iter/s, 8.53602s/100 iters), loss = 0.0269583
I1107 13:12:44.294212 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:12:44.294212 11160 solver.cpp:237]     Train net output #1: loss = 0.0269585 (* 1 = 0.0269585 loss)
I1107 13:12:44.294212 11160 sgd_solver.cpp:105] Iteration 116800, lr = 0.001
I1107 13:12:52.829015 11160 solver.cpp:218] Iteration 116900 (11.717 iter/s, 8.5346s/100 iters), loss = 0.0286262
I1107 13:12:52.829015 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:12:52.829015 11160 solver.cpp:237]     Train net output #1: loss = 0.0286264 (* 1 = 0.0286264 loss)
I1107 13:12:52.829015 11160 sgd_solver.cpp:105] Iteration 116900, lr = 0.001
I1107 13:13:00.964546  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:13:01.302887 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_117000.caffemodel
I1107 13:13:01.333910 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_117000.solverstate
I1107 13:13:01.365898 11160 solver.cpp:330] Iteration 117000, Testing net (#0)
I1107 13:13:01.365898 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:13:03.358721 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:13:03.438727 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 13:13:03.438727 11160 solver.cpp:397]     Test net output #1: loss = 0.299326 (* 1 = 0.299326 loss)
I1107 13:13:03.519758 11160 solver.cpp:218] Iteration 117000 (9.35428 iter/s, 10.6903s/100 iters), loss = 0.0429399
I1107 13:13:03.519758 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:13:03.519758 11160 solver.cpp:237]     Train net output #1: loss = 0.04294 (* 1 = 0.04294 loss)
I1107 13:13:03.519758 11160 sgd_solver.cpp:105] Iteration 117000, lr = 0.001
I1107 13:13:12.101809 11160 solver.cpp:218] Iteration 117100 (11.6532 iter/s, 8.58132s/100 iters), loss = 0.0569427
I1107 13:13:12.102311 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:13:12.102311 11160 solver.cpp:237]     Train net output #1: loss = 0.0569428 (* 1 = 0.0569428 loss)
I1107 13:13:12.102311 11160 sgd_solver.cpp:105] Iteration 117100, lr = 0.001
I1107 13:13:20.687402 11160 solver.cpp:218] Iteration 117200 (11.6486 iter/s, 8.58474s/100 iters), loss = 0.0284394
I1107 13:13:20.687402 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:13:20.687402 11160 solver.cpp:237]     Train net output #1: loss = 0.0284396 (* 1 = 0.0284396 loss)
I1107 13:13:20.687402 11160 sgd_solver.cpp:105] Iteration 117200, lr = 0.001
I1107 13:13:29.277163 11160 solver.cpp:218] Iteration 117300 (11.6419 iter/s, 8.58967s/100 iters), loss = 0.0415475
I1107 13:13:29.277163 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:13:29.277163 11160 solver.cpp:237]     Train net output #1: loss = 0.0415477 (* 1 = 0.0415477 loss)
I1107 13:13:29.277163 11160 sgd_solver.cpp:105] Iteration 117300, lr = 0.001
I1107 13:13:37.857111 11160 solver.cpp:218] Iteration 117400 (11.6557 iter/s, 8.57952s/100 iters), loss = 0.0296401
I1107 13:13:37.857111 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:13:37.857111 11160 solver.cpp:237]     Train net output #1: loss = 0.0296402 (* 1 = 0.0296402 loss)
I1107 13:13:37.857111 11160 sgd_solver.cpp:105] Iteration 117400, lr = 0.001
I1107 13:13:46.002935  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:13:46.341994 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_117500.caffemodel
I1107 13:13:46.369992 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_117500.solverstate
I1107 13:13:46.378991 11160 solver.cpp:330] Iteration 117500, Testing net (#0)
I1107 13:13:46.378991 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:13:48.375324 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:13:48.455328 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9198
I1107 13:13:48.455328 11160 solver.cpp:397]     Test net output #1: loss = 0.299004 (* 1 = 0.299004 loss)
I1107 13:13:48.537333 11160 solver.cpp:218] Iteration 117500 (9.36372 iter/s, 10.6795s/100 iters), loss = 0.0481169
I1107 13:13:48.537333 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:13:48.537333 11160 solver.cpp:237]     Train net output #1: loss = 0.0481171 (* 1 = 0.0481171 loss)
I1107 13:13:48.537333 11160 sgd_solver.cpp:105] Iteration 117500, lr = 0.001
I1107 13:13:57.101011 11160 solver.cpp:218] Iteration 117600 (11.6774 iter/s, 8.56354s/100 iters), loss = 0.0375304
I1107 13:13:57.102010 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:13:57.102010 11160 solver.cpp:237]     Train net output #1: loss = 0.0375306 (* 1 = 0.0375306 loss)
I1107 13:13:57.102010 11160 sgd_solver.cpp:105] Iteration 117600, lr = 0.001
I1107 13:14:05.657915 11160 solver.cpp:218] Iteration 117700 (11.6871 iter/s, 8.55642s/100 iters), loss = 0.0245949
I1107 13:14:05.658916 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:14:05.658916 11160 solver.cpp:237]     Train net output #1: loss = 0.024595 (* 1 = 0.024595 loss)
I1107 13:14:05.658916 11160 sgd_solver.cpp:105] Iteration 117700, lr = 0.001
I1107 13:14:14.231884 11160 solver.cpp:218] Iteration 117800 (11.6652 iter/s, 8.57249s/100 iters), loss = 0.0205291
I1107 13:14:14.231884 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:14:14.231884 11160 solver.cpp:237]     Train net output #1: loss = 0.0205293 (* 1 = 0.0205293 loss)
I1107 13:14:14.231884 11160 sgd_solver.cpp:105] Iteration 117800, lr = 0.001
I1107 13:14:22.978879 11160 solver.cpp:218] Iteration 117900 (11.4322 iter/s, 8.74725s/100 iters), loss = 0.0429481
I1107 13:14:22.978879 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:14:22.978879 11160 solver.cpp:237]     Train net output #1: loss = 0.0429482 (* 1 = 0.0429482 loss)
I1107 13:14:22.978879 11160 sgd_solver.cpp:105] Iteration 117900, lr = 0.001
I1107 13:14:31.257084  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:14:31.597120 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_118000.caffemodel
I1107 13:14:31.655144 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_118000.solverstate
I1107 13:14:31.664145 11160 solver.cpp:330] Iteration 118000, Testing net (#0)
I1107 13:14:31.664145 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:14:33.678195 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:14:33.761203 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9169
I1107 13:14:33.761203 11160 solver.cpp:397]     Test net output #1: loss = 0.306022 (* 1 = 0.306022 loss)
I1107 13:14:33.843209 11160 solver.cpp:218] Iteration 118000 (9.2049 iter/s, 10.8638s/100 iters), loss = 0.0318122
I1107 13:14:33.844211 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:14:33.844211 11160 solver.cpp:237]     Train net output #1: loss = 0.0318123 (* 1 = 0.0318123 loss)
I1107 13:14:33.844211 11160 sgd_solver.cpp:105] Iteration 118000, lr = 0.001
I1107 13:14:42.412333 11160 solver.cpp:218] Iteration 118100 (11.6709 iter/s, 8.56834s/100 iters), loss = 0.0282595
I1107 13:14:42.412333 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:14:42.412333 11160 solver.cpp:237]     Train net output #1: loss = 0.0282597 (* 1 = 0.0282597 loss)
I1107 13:14:42.412333 11160 sgd_solver.cpp:105] Iteration 118100, lr = 0.001
I1107 13:14:50.962386 11160 solver.cpp:218] Iteration 118200 (11.6978 iter/s, 8.54858s/100 iters), loss = 0.0385653
I1107 13:14:50.962386 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:14:50.962386 11160 solver.cpp:237]     Train net output #1: loss = 0.0385654 (* 1 = 0.0385654 loss)
I1107 13:14:50.962386 11160 sgd_solver.cpp:105] Iteration 118200, lr = 0.001
I1107 13:14:59.584785 11160 solver.cpp:218] Iteration 118300 (11.5981 iter/s, 8.62214s/100 iters), loss = 0.0193681
I1107 13:14:59.584785 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:14:59.584785 11160 solver.cpp:237]     Train net output #1: loss = 0.0193682 (* 1 = 0.0193682 loss)
I1107 13:14:59.584785 11160 sgd_solver.cpp:105] Iteration 118300, lr = 0.001
I1107 13:15:08.263801 11160 solver.cpp:218] Iteration 118400 (11.5228 iter/s, 8.67843s/100 iters), loss = 0.0387568
I1107 13:15:08.263801 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:15:08.263801 11160 solver.cpp:237]     Train net output #1: loss = 0.0387569 (* 1 = 0.0387569 loss)
I1107 13:15:08.263801 11160 sgd_solver.cpp:105] Iteration 118400, lr = 0.001
I1107 13:15:16.380738  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:15:16.718755 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_118500.caffemodel
I1107 13:15:16.750258 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_118500.solverstate
I1107 13:15:16.783773 11160 solver.cpp:330] Iteration 118500, Testing net (#0)
I1107 13:15:16.783773 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:15:18.772913 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:15:18.853444 11160 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1107 13:15:18.853444 11160 solver.cpp:397]     Test net output #1: loss = 0.296974 (* 1 = 0.296974 loss)
I1107 13:15:18.934934 11160 solver.cpp:218] Iteration 118500 (9.37122 iter/s, 10.671s/100 iters), loss = 0.0359756
I1107 13:15:18.934934 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:15:18.934934 11160 solver.cpp:237]     Train net output #1: loss = 0.0359757 (* 1 = 0.0359757 loss)
I1107 13:15:18.934934 11160 sgd_solver.cpp:105] Iteration 118500, lr = 0.001
I1107 13:15:27.473879 11160 solver.cpp:218] Iteration 118600 (11.7129 iter/s, 8.53763s/100 iters), loss = 0.0641607
I1107 13:15:27.473879 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:15:27.473879 11160 solver.cpp:237]     Train net output #1: loss = 0.0641608 (* 1 = 0.0641608 loss)
I1107 13:15:27.473879 11160 sgd_solver.cpp:105] Iteration 118600, lr = 0.001
I1107 13:15:36.018587 11160 solver.cpp:218] Iteration 118700 (11.7028 iter/s, 8.54499s/100 iters), loss = 0.0528729
I1107 13:15:36.018587 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:15:36.018587 11160 solver.cpp:237]     Train net output #1: loss = 0.052873 (* 1 = 0.052873 loss)
I1107 13:15:36.018587 11160 sgd_solver.cpp:105] Iteration 118700, lr = 0.001
I1107 13:15:44.551700 11160 solver.cpp:218] Iteration 118800 (11.7201 iter/s, 8.53237s/100 iters), loss = 0.0252734
I1107 13:15:44.551700 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:15:44.551700 11160 solver.cpp:237]     Train net output #1: loss = 0.0252735 (* 1 = 0.0252735 loss)
I1107 13:15:44.551700 11160 sgd_solver.cpp:105] Iteration 118800, lr = 0.001
I1107 13:15:53.086561 11160 solver.cpp:218] Iteration 118900 (11.7177 iter/s, 8.53412s/100 iters), loss = 0.0273159
I1107 13:15:53.086561 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:15:53.086561 11160 solver.cpp:237]     Train net output #1: loss = 0.0273161 (* 1 = 0.0273161 loss)
I1107 13:15:53.086561 11160 sgd_solver.cpp:105] Iteration 118900, lr = 0.001
I1107 13:16:01.206324  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:16:01.543846 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_119000.caffemodel
I1107 13:16:01.588845 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_119000.solverstate
I1107 13:16:01.597349 11160 solver.cpp:330] Iteration 119000, Testing net (#0)
I1107 13:16:01.597349 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:16:03.586041 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:16:03.666045 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1107 13:16:03.666045 11160 solver.cpp:397]     Test net output #1: loss = 0.30003 (* 1 = 0.30003 loss)
I1107 13:16:03.748066 11160 solver.cpp:218] Iteration 119000 (9.38027 iter/s, 10.6607s/100 iters), loss = 0.0392359
I1107 13:16:03.748066 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:16:03.748066 11160 solver.cpp:237]     Train net output #1: loss = 0.0392361 (* 1 = 0.0392361 loss)
I1107 13:16:03.748066 11160 sgd_solver.cpp:105] Iteration 119000, lr = 0.001
I1107 13:16:12.286625 11160 solver.cpp:218] Iteration 119100 (11.7112 iter/s, 8.53882s/100 iters), loss = 0.0650823
I1107 13:16:12.286625 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:16:12.286625 11160 solver.cpp:237]     Train net output #1: loss = 0.0650824 (* 1 = 0.0650824 loss)
I1107 13:16:12.286625 11160 sgd_solver.cpp:105] Iteration 119100, lr = 0.001
I1107 13:16:20.825521 11160 solver.cpp:218] Iteration 119200 (11.7118 iter/s, 8.53841s/100 iters), loss = 0.0330731
I1107 13:16:20.826522 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:16:20.826522 11160 solver.cpp:237]     Train net output #1: loss = 0.0330732 (* 1 = 0.0330732 loss)
I1107 13:16:20.826522 11160 sgd_solver.cpp:105] Iteration 119200, lr = 0.001
I1107 13:16:29.366248 11160 solver.cpp:218] Iteration 119300 (11.7105 iter/s, 8.53933s/100 iters), loss = 0.02926
I1107 13:16:29.366248 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:16:29.366248 11160 solver.cpp:237]     Train net output #1: loss = 0.0292601 (* 1 = 0.0292601 loss)
I1107 13:16:29.366248 11160 sgd_solver.cpp:105] Iteration 119300, lr = 0.001
I1107 13:16:37.900269 11160 solver.cpp:218] Iteration 119400 (11.7186 iter/s, 8.53348s/100 iters), loss = 0.0313181
I1107 13:16:37.900269 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:16:37.900269 11160 solver.cpp:237]     Train net output #1: loss = 0.0313182 (* 1 = 0.0313182 loss)
I1107 13:16:37.900269 11160 sgd_solver.cpp:105] Iteration 119400, lr = 0.001
I1107 13:16:46.019409  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:16:46.357950 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_119500.caffemodel
I1107 13:16:46.388950 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_119500.solverstate
I1107 13:16:46.397950 11160 solver.cpp:330] Iteration 119500, Testing net (#0)
I1107 13:16:46.397950 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:16:48.385087 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:16:48.465091 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9144
I1107 13:16:48.466094 11160 solver.cpp:397]     Test net output #1: loss = 0.312384 (* 1 = 0.312384 loss)
I1107 13:16:48.548100 11160 solver.cpp:218] Iteration 119500 (9.39201 iter/s, 10.6474s/100 iters), loss = 0.035131
I1107 13:16:48.548100 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:16:48.548100 11160 solver.cpp:237]     Train net output #1: loss = 0.0351312 (* 1 = 0.0351312 loss)
I1107 13:16:48.548100 11160 sgd_solver.cpp:105] Iteration 119500, lr = 0.001
I1107 13:16:57.100797 11160 solver.cpp:218] Iteration 119600 (11.6929 iter/s, 8.55223s/100 iters), loss = 0.0382984
I1107 13:16:57.100797 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:16:57.100797 11160 solver.cpp:237]     Train net output #1: loss = 0.0382985 (* 1 = 0.0382985 loss)
I1107 13:16:57.100797 11160 sgd_solver.cpp:105] Iteration 119600, lr = 0.001
I1107 13:17:05.648632 11160 solver.cpp:218] Iteration 119700 (11.6989 iter/s, 8.54778s/100 iters), loss = 0.0287863
I1107 13:17:05.648632 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:17:05.648632 11160 solver.cpp:237]     Train net output #1: loss = 0.0287864 (* 1 = 0.0287864 loss)
I1107 13:17:05.648632 11160 sgd_solver.cpp:105] Iteration 119700, lr = 0.001
I1107 13:17:14.191714 11160 solver.cpp:218] Iteration 119800 (11.7065 iter/s, 8.54227s/100 iters), loss = 0.0435915
I1107 13:17:14.191714 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:17:14.191714 11160 solver.cpp:237]     Train net output #1: loss = 0.0435916 (* 1 = 0.0435916 loss)
I1107 13:17:14.191714 11160 sgd_solver.cpp:105] Iteration 119800, lr = 0.001
I1107 13:17:22.729413 11160 solver.cpp:218] Iteration 119900 (11.7124 iter/s, 8.53798s/100 iters), loss = 0.032905
I1107 13:17:22.730412 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:17:22.730412 11160 solver.cpp:237]     Train net output #1: loss = 0.0329051 (* 1 = 0.0329051 loss)
I1107 13:17:22.730412 11160 sgd_solver.cpp:105] Iteration 119900, lr = 0.001
I1107 13:17:30.845434  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:17:31.184520 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_120000.caffemodel
I1107 13:17:31.239271 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_120000.solverstate
I1107 13:17:31.248272 11160 solver.cpp:330] Iteration 120000, Testing net (#0)
I1107 13:17:31.248272 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:17:33.241211 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:17:33.321218 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9149
I1107 13:17:33.321218 11160 solver.cpp:397]     Test net output #1: loss = 0.304956 (* 1 = 0.304956 loss)
I1107 13:17:33.403247 11160 solver.cpp:218] Iteration 120000 (9.37003 iter/s, 10.6723s/100 iters), loss = 0.0668869
I1107 13:17:33.403247 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:17:33.403247 11160 solver.cpp:237]     Train net output #1: loss = 0.0668871 (* 1 = 0.0668871 loss)
I1107 13:17:33.403247 11160 sgd_solver.cpp:105] Iteration 120000, lr = 0.001
I1107 13:17:41.943650 11160 solver.cpp:218] Iteration 120100 (11.7089 iter/s, 8.54048s/100 iters), loss = 0.0591087
I1107 13:17:41.943650 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:17:41.943650 11160 solver.cpp:237]     Train net output #1: loss = 0.0591088 (* 1 = 0.0591088 loss)
I1107 13:17:41.943650 11160 sgd_solver.cpp:105] Iteration 120100, lr = 0.001
I1107 13:17:50.486059 11160 solver.cpp:218] Iteration 120200 (11.7069 iter/s, 8.54195s/100 iters), loss = 0.0268274
I1107 13:17:50.486059 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:17:50.486059 11160 solver.cpp:237]     Train net output #1: loss = 0.0268275 (* 1 = 0.0268275 loss)
I1107 13:17:50.486059 11160 sgd_solver.cpp:105] Iteration 120200, lr = 0.001
I1107 13:17:59.025653 11160 solver.cpp:218] Iteration 120300 (11.7113 iter/s, 8.53874s/100 iters), loss = 0.0360836
I1107 13:17:59.025653 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:17:59.025653 11160 solver.cpp:237]     Train net output #1: loss = 0.0360837 (* 1 = 0.0360837 loss)
I1107 13:17:59.025653 11160 sgd_solver.cpp:105] Iteration 120300, lr = 0.001
I1107 13:18:07.566596 11160 solver.cpp:218] Iteration 120400 (11.7086 iter/s, 8.54074s/100 iters), loss = 0.0317342
I1107 13:18:07.566596 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:18:07.566596 11160 solver.cpp:237]     Train net output #1: loss = 0.0317344 (* 1 = 0.0317344 loss)
I1107 13:18:07.566596 11160 sgd_solver.cpp:105] Iteration 120400, lr = 0.001
I1107 13:18:15.684834  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:18:16.024876 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_120500.caffemodel
I1107 13:18:16.053886 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_120500.solverstate
I1107 13:18:16.061887 11160 solver.cpp:330] Iteration 120500, Testing net (#0)
I1107 13:18:16.061887 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:18:18.052279 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:18:18.131284 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1107 13:18:18.131284 11160 solver.cpp:397]     Test net output #1: loss = 0.299007 (* 1 = 0.299007 loss)
I1107 13:18:18.212785 11160 solver.cpp:218] Iteration 120500 (9.39381 iter/s, 10.6453s/100 iters), loss = 0.0425449
I1107 13:18:18.212785 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:18:18.212785 11160 solver.cpp:237]     Train net output #1: loss = 0.042545 (* 1 = 0.042545 loss)
I1107 13:18:18.212785 11160 sgd_solver.cpp:105] Iteration 120500, lr = 0.001
I1107 13:18:26.754606 11160 solver.cpp:218] Iteration 120600 (11.7071 iter/s, 8.54185s/100 iters), loss = 0.0468986
I1107 13:18:26.754606 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:18:26.754606 11160 solver.cpp:237]     Train net output #1: loss = 0.0468987 (* 1 = 0.0468987 loss)
I1107 13:18:26.754606 11160 sgd_solver.cpp:105] Iteration 120600, lr = 0.001
I1107 13:18:35.303721 11160 solver.cpp:218] Iteration 120700 (11.6988 iter/s, 8.54785s/100 iters), loss = 0.0200224
I1107 13:18:35.303721 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:18:35.303721 11160 solver.cpp:237]     Train net output #1: loss = 0.0200226 (* 1 = 0.0200226 loss)
I1107 13:18:35.303721 11160 sgd_solver.cpp:105] Iteration 120700, lr = 0.001
I1107 13:18:43.846096 11160 solver.cpp:218] Iteration 120800 (11.706 iter/s, 8.54266s/100 iters), loss = 0.0282089
I1107 13:18:43.846096 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:18:43.846096 11160 solver.cpp:237]     Train net output #1: loss = 0.028209 (* 1 = 0.028209 loss)
I1107 13:18:43.846096 11160 sgd_solver.cpp:105] Iteration 120800, lr = 0.001
I1107 13:18:52.386097 11160 solver.cpp:218] Iteration 120900 (11.7105 iter/s, 8.53936s/100 iters), loss = 0.0314037
I1107 13:18:52.386097 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:18:52.386097 11160 solver.cpp:237]     Train net output #1: loss = 0.0314038 (* 1 = 0.0314038 loss)
I1107 13:18:52.386097 11160 sgd_solver.cpp:105] Iteration 120900, lr = 0.001
I1107 13:19:00.504011  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:19:00.839053 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_121000.caffemodel
I1107 13:19:00.897047 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_121000.solverstate
I1107 13:19:00.905046 11160 solver.cpp:330] Iteration 121000, Testing net (#0)
I1107 13:19:00.905046 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:19:02.897243 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:19:02.976245 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 13:19:02.976245 11160 solver.cpp:397]     Test net output #1: loss = 0.303413 (* 1 = 0.303413 loss)
I1107 13:19:03.058251 11160 solver.cpp:218] Iteration 121000 (9.37123 iter/s, 10.671s/100 iters), loss = 0.0454041
I1107 13:19:03.058251 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:19:03.058251 11160 solver.cpp:237]     Train net output #1: loss = 0.0454042 (* 1 = 0.0454042 loss)
I1107 13:19:03.058251 11160 sgd_solver.cpp:105] Iteration 121000, lr = 0.001
I1107 13:19:11.638043 11160 solver.cpp:218] Iteration 121100 (11.655 iter/s, 8.57998s/100 iters), loss = 0.0563505
I1107 13:19:11.638043 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:19:11.638043 11160 solver.cpp:237]     Train net output #1: loss = 0.0563507 (* 1 = 0.0563507 loss)
I1107 13:19:11.638043 11160 sgd_solver.cpp:105] Iteration 121100, lr = 0.001
I1107 13:19:20.273702 11160 solver.cpp:218] Iteration 121200 (11.581 iter/s, 8.63485s/100 iters), loss = 0.022764
I1107 13:19:20.273702 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:19:20.273702 11160 solver.cpp:237]     Train net output #1: loss = 0.0227641 (* 1 = 0.0227641 loss)
I1107 13:19:20.273702 11160 sgd_solver.cpp:105] Iteration 121200, lr = 0.001
I1107 13:19:28.833358 11160 solver.cpp:218] Iteration 121300 (11.6829 iter/s, 8.55952s/100 iters), loss = 0.0461328
I1107 13:19:28.834357 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:19:28.834357 11160 solver.cpp:237]     Train net output #1: loss = 0.0461329 (* 1 = 0.0461329 loss)
I1107 13:19:28.834357 11160 sgd_solver.cpp:105] Iteration 121300, lr = 0.001
I1107 13:19:37.366469 11160 solver.cpp:218] Iteration 121400 (11.72 iter/s, 8.53242s/100 iters), loss = 0.0385608
I1107 13:19:37.366469 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:19:37.367470 11160 solver.cpp:237]     Train net output #1: loss = 0.0385609 (* 1 = 0.0385609 loss)
I1107 13:19:37.367470 11160 sgd_solver.cpp:105] Iteration 121400, lr = 0.001
I1107 13:19:45.511333  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:19:45.851351 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_121500.caffemodel
I1107 13:19:45.885351 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_121500.solverstate
I1107 13:19:45.894366 11160 solver.cpp:330] Iteration 121500, Testing net (#0)
I1107 13:19:45.894366 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:19:47.894564 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:19:47.973569 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9189
I1107 13:19:47.973569 11160 solver.cpp:397]     Test net output #1: loss = 0.299776 (* 1 = 0.299776 loss)
I1107 13:19:48.055579 11160 solver.cpp:218] Iteration 121500 (9.35585 iter/s, 10.6885s/100 iters), loss = 0.0275374
I1107 13:19:48.055579 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:19:48.055579 11160 solver.cpp:237]     Train net output #1: loss = 0.0275375 (* 1 = 0.0275375 loss)
I1107 13:19:48.055579 11160 sgd_solver.cpp:105] Iteration 121500, lr = 0.001
I1107 13:19:56.686751 11160 solver.cpp:218] Iteration 121600 (11.5869 iter/s, 8.63042s/100 iters), loss = 0.0337513
I1107 13:19:56.686751 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:19:56.686751 11160 solver.cpp:237]     Train net output #1: loss = 0.0337515 (* 1 = 0.0337515 loss)
I1107 13:19:56.686751 11160 sgd_solver.cpp:105] Iteration 121600, lr = 0.001
I1107 13:20:05.330286 11160 solver.cpp:218] Iteration 121700 (11.5704 iter/s, 8.64271s/100 iters), loss = 0.0245435
I1107 13:20:05.330286 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:20:05.330286 11160 solver.cpp:237]     Train net output #1: loss = 0.0245436 (* 1 = 0.0245436 loss)
I1107 13:20:05.330286 11160 sgd_solver.cpp:105] Iteration 121700, lr = 0.001
I1107 13:20:13.974460 11160 solver.cpp:218] Iteration 121800 (11.5686 iter/s, 8.64407s/100 iters), loss = 0.0218915
I1107 13:20:13.974460 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:20:13.974460 11160 solver.cpp:237]     Train net output #1: loss = 0.0218916 (* 1 = 0.0218916 loss)
I1107 13:20:13.974460 11160 sgd_solver.cpp:105] Iteration 121800, lr = 0.001
I1107 13:20:22.504565 11160 solver.cpp:218] Iteration 121900 (11.725 iter/s, 8.52877s/100 iters), loss = 0.0303044
I1107 13:20:22.504565 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:20:22.504565 11160 solver.cpp:237]     Train net output #1: loss = 0.0303045 (* 1 = 0.0303045 loss)
I1107 13:20:22.504565 11160 sgd_solver.cpp:105] Iteration 121900, lr = 0.001
I1107 13:20:30.625412  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:20:30.963496 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_122000.caffemodel
I1107 13:20:31.022497 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_122000.solverstate
I1107 13:20:31.030498 11160 solver.cpp:330] Iteration 122000, Testing net (#0)
I1107 13:20:31.031498 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:20:33.021827 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:20:33.101785 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9178
I1107 13:20:33.101785 11160 solver.cpp:397]     Test net output #1: loss = 0.302541 (* 1 = 0.302541 loss)
I1107 13:20:33.182817 11160 solver.cpp:218] Iteration 122000 (9.36528 iter/s, 10.6777s/100 iters), loss = 0.0314601
I1107 13:20:33.182817 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:20:33.182817 11160 solver.cpp:237]     Train net output #1: loss = 0.0314602 (* 1 = 0.0314602 loss)
I1107 13:20:33.182817 11160 sgd_solver.cpp:105] Iteration 122000, lr = 0.001
I1107 13:20:41.703502 11160 solver.cpp:218] Iteration 122100 (11.7367 iter/s, 8.52027s/100 iters), loss = 0.0476866
I1107 13:20:41.703502 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:20:41.703502 11160 solver.cpp:237]     Train net output #1: loss = 0.0476867 (* 1 = 0.0476867 loss)
I1107 13:20:41.703502 11160 sgd_solver.cpp:105] Iteration 122100, lr = 0.001
I1107 13:20:50.224220 11160 solver.cpp:218] Iteration 122200 (11.7361 iter/s, 8.52071s/100 iters), loss = 0.024532
I1107 13:20:50.224220 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:20:50.224220 11160 solver.cpp:237]     Train net output #1: loss = 0.0245322 (* 1 = 0.0245322 loss)
I1107 13:20:50.224220 11160 sgd_solver.cpp:105] Iteration 122200, lr = 0.001
I1107 13:20:58.756309 11160 solver.cpp:218] Iteration 122300 (11.7214 iter/s, 8.53137s/100 iters), loss = 0.0300619
I1107 13:20:58.756309 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:20:58.756309 11160 solver.cpp:237]     Train net output #1: loss = 0.030062 (* 1 = 0.030062 loss)
I1107 13:20:58.756309 11160 sgd_solver.cpp:105] Iteration 122300, lr = 0.001
I1107 13:21:07.284369 11160 solver.cpp:218] Iteration 122400 (11.7269 iter/s, 8.52738s/100 iters), loss = 0.0281104
I1107 13:21:07.284369 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:21:07.284369 11160 solver.cpp:237]     Train net output #1: loss = 0.0281105 (* 1 = 0.0281105 loss)
I1107 13:21:07.284369 11160 sgd_solver.cpp:105] Iteration 122400, lr = 0.001
I1107 13:21:15.392849  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:21:15.729878 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_122500.caffemodel
I1107 13:21:15.758878 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_122500.solverstate
I1107 13:21:15.766878 11160 solver.cpp:330] Iteration 122500, Testing net (#0)
I1107 13:21:15.766878 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:21:17.756089 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:21:17.836094 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9167
I1107 13:21:17.836094 11160 solver.cpp:397]     Test net output #1: loss = 0.303729 (* 1 = 0.303729 loss)
I1107 13:21:17.917100 11160 solver.cpp:218] Iteration 122500 (9.40547 iter/s, 10.6321s/100 iters), loss = 0.0333233
I1107 13:21:17.917100 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:21:17.917100 11160 solver.cpp:237]     Train net output #1: loss = 0.0333235 (* 1 = 0.0333235 loss)
I1107 13:21:17.917100 11160 sgd_solver.cpp:105] Iteration 122500, lr = 0.001
I1107 13:21:26.449877 11160 solver.cpp:218] Iteration 122600 (11.7196 iter/s, 8.53271s/100 iters), loss = 0.0386858
I1107 13:21:26.449877 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:21:26.449877 11160 solver.cpp:237]     Train net output #1: loss = 0.038686 (* 1 = 0.038686 loss)
I1107 13:21:26.449877 11160 sgd_solver.cpp:105] Iteration 122600, lr = 0.001
I1107 13:21:34.973551 11160 solver.cpp:218] Iteration 122700 (11.733 iter/s, 8.52298s/100 iters), loss = 0.0222806
I1107 13:21:34.973551 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:21:34.974054 11160 solver.cpp:237]     Train net output #1: loss = 0.0222807 (* 1 = 0.0222807 loss)
I1107 13:21:34.974054 11160 sgd_solver.cpp:105] Iteration 122700, lr = 0.001
I1107 13:21:43.486143 11160 solver.cpp:218] Iteration 122800 (11.7484 iter/s, 8.51182s/100 iters), loss = 0.0208599
I1107 13:21:43.486143 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:21:43.486143 11160 solver.cpp:237]     Train net output #1: loss = 0.0208601 (* 1 = 0.0208601 loss)
I1107 13:21:43.486143 11160 sgd_solver.cpp:105] Iteration 122800, lr = 0.001
I1107 13:21:52.022517 11160 solver.cpp:218] Iteration 122900 (11.7153 iter/s, 8.53584s/100 iters), loss = 0.0331446
I1107 13:21:52.022517 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:21:52.022517 11160 solver.cpp:237]     Train net output #1: loss = 0.0331447 (* 1 = 0.0331447 loss)
I1107 13:21:52.022517 11160 sgd_solver.cpp:105] Iteration 122900, lr = 0.001
I1107 13:22:00.134356  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:22:00.472383 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_123000.caffemodel
I1107 13:22:00.529403 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_123000.solverstate
I1107 13:22:00.538424 11160 solver.cpp:330] Iteration 123000, Testing net (#0)
I1107 13:22:00.538424 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:22:02.525539 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:22:02.605571 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9172
I1107 13:22:02.605571 11160 solver.cpp:397]     Test net output #1: loss = 0.304376 (* 1 = 0.304376 loss)
I1107 13:22:02.687105 11160 solver.cpp:218] Iteration 123000 (9.3773 iter/s, 10.664s/100 iters), loss = 0.0473137
I1107 13:22:02.687105 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:22:02.687105 11160 solver.cpp:237]     Train net output #1: loss = 0.0473139 (* 1 = 0.0473139 loss)
I1107 13:22:02.687105 11160 sgd_solver.cpp:105] Iteration 123000, lr = 0.001
I1107 13:22:11.219605 11160 solver.cpp:218] Iteration 123100 (11.7195 iter/s, 8.53279s/100 iters), loss = 0.037383
I1107 13:22:11.220605 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:22:11.220605 11160 solver.cpp:237]     Train net output #1: loss = 0.0373831 (* 1 = 0.0373831 loss)
I1107 13:22:11.220605 11160 sgd_solver.cpp:105] Iteration 123100, lr = 0.001
I1107 13:22:19.750847 11160 solver.cpp:218] Iteration 123200 (11.7235 iter/s, 8.52987s/100 iters), loss = 0.0303374
I1107 13:22:19.750847 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:22:19.750847 11160 solver.cpp:237]     Train net output #1: loss = 0.0303376 (* 1 = 0.0303376 loss)
I1107 13:22:19.750847 11160 sgd_solver.cpp:105] Iteration 123200, lr = 0.001
I1107 13:22:28.281688 11160 solver.cpp:218] Iteration 123300 (11.7227 iter/s, 8.53044s/100 iters), loss = 0.0216981
I1107 13:22:28.281688 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:22:28.281688 11160 solver.cpp:237]     Train net output #1: loss = 0.0216982 (* 1 = 0.0216982 loss)
I1107 13:22:28.281688 11160 sgd_solver.cpp:105] Iteration 123300, lr = 0.001
I1107 13:22:36.829355 11160 solver.cpp:218] Iteration 123400 (11.6993 iter/s, 8.5475s/100 iters), loss = 0.0291825
I1107 13:22:36.829355 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:22:36.829355 11160 solver.cpp:237]     Train net output #1: loss = 0.0291827 (* 1 = 0.0291827 loss)
I1107 13:22:36.829355 11160 sgd_solver.cpp:105] Iteration 123400, lr = 0.001
I1107 13:22:44.987243  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:22:45.323760 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_123500.caffemodel
I1107 13:22:45.353760 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_123500.solverstate
I1107 13:22:45.382761 11160 solver.cpp:330] Iteration 123500, Testing net (#0)
I1107 13:22:45.382761 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:22:47.381880 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:22:47.460894 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9157
I1107 13:22:47.460894 11160 solver.cpp:397]     Test net output #1: loss = 0.309954 (* 1 = 0.309954 loss)
I1107 13:22:47.541893 11160 solver.cpp:218] Iteration 123500 (9.33496 iter/s, 10.7124s/100 iters), loss = 0.0298567
I1107 13:22:47.542893 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:22:47.542893 11160 solver.cpp:237]     Train net output #1: loss = 0.0298568 (* 1 = 0.0298568 loss)
I1107 13:22:47.542893 11160 sgd_solver.cpp:105] Iteration 123500, lr = 0.001
I1107 13:22:56.101310 11160 solver.cpp:218] Iteration 123600 (11.6851 iter/s, 8.55791s/100 iters), loss = 0.0328776
I1107 13:22:56.101310 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:22:56.101310 11160 solver.cpp:237]     Train net output #1: loss = 0.0328777 (* 1 = 0.0328777 loss)
I1107 13:22:56.101310 11160 sgd_solver.cpp:105] Iteration 123600, lr = 0.001
I1107 13:23:04.794932 11160 solver.cpp:218] Iteration 123700 (11.5034 iter/s, 8.69307s/100 iters), loss = 0.0260004
I1107 13:23:04.794932 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:23:04.794932 11160 solver.cpp:237]     Train net output #1: loss = 0.0260005 (* 1 = 0.0260005 loss)
I1107 13:23:04.794932 11160 sgd_solver.cpp:105] Iteration 123700, lr = 0.001
I1107 13:23:13.479869 11160 solver.cpp:218] Iteration 123800 (11.5141 iter/s, 8.68503s/100 iters), loss = 0.0294183
I1107 13:23:13.479869 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:23:13.479869 11160 solver.cpp:237]     Train net output #1: loss = 0.0294184 (* 1 = 0.0294184 loss)
I1107 13:23:13.479869 11160 sgd_solver.cpp:105] Iteration 123800, lr = 0.001
I1107 13:23:22.083803 11160 solver.cpp:218] Iteration 123900 (11.6242 iter/s, 8.60275s/100 iters), loss = 0.0305256
I1107 13:23:22.083803 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:23:22.083803 11160 solver.cpp:237]     Train net output #1: loss = 0.0305257 (* 1 = 0.0305257 loss)
I1107 13:23:22.083803 11160 sgd_solver.cpp:105] Iteration 123900, lr = 0.001
I1107 13:23:30.321305  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:23:30.660435 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_124000.caffemodel
I1107 13:23:30.689435 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_124000.solverstate
I1107 13:23:30.699440 11160 solver.cpp:330] Iteration 124000, Testing net (#0)
I1107 13:23:30.699440 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:23:32.723040 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:23:32.803077 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9159
I1107 13:23:32.803077 11160 solver.cpp:397]     Test net output #1: loss = 0.309916 (* 1 = 0.309916 loss)
I1107 13:23:32.884083 11160 solver.cpp:218] Iteration 124000 (9.259 iter/s, 10.8003s/100 iters), loss = 0.0365174
I1107 13:23:32.884083 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:23:32.884083 11160 solver.cpp:237]     Train net output #1: loss = 0.0365175 (* 1 = 0.0365175 loss)
I1107 13:23:32.884083 11160 sgd_solver.cpp:105] Iteration 124000, lr = 0.001
I1107 13:23:41.618494 11160 solver.cpp:218] Iteration 124100 (11.4496 iter/s, 8.7339s/100 iters), loss = 0.0710199
I1107 13:23:41.618494 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:23:41.618494 11160 solver.cpp:237]     Train net output #1: loss = 0.07102 (* 1 = 0.07102 loss)
I1107 13:23:41.618494 11160 sgd_solver.cpp:105] Iteration 124100, lr = 0.001
I1107 13:23:50.315578 11160 solver.cpp:218] Iteration 124200 (11.4989 iter/s, 8.69647s/100 iters), loss = 0.0219484
I1107 13:23:50.315578 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:23:50.315578 11160 solver.cpp:237]     Train net output #1: loss = 0.0219485 (* 1 = 0.0219485 loss)
I1107 13:23:50.315578 11160 sgd_solver.cpp:105] Iteration 124200, lr = 0.001
I1107 13:23:58.937364 11160 solver.cpp:218] Iteration 124300 (11.5989 iter/s, 8.62149s/100 iters), loss = 0.0179548
I1107 13:23:58.937364 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:23:58.938365 11160 solver.cpp:237]     Train net output #1: loss = 0.0179549 (* 1 = 0.0179549 loss)
I1107 13:23:58.938365 11160 sgd_solver.cpp:105] Iteration 124300, lr = 0.001
I1107 13:24:07.506577 11160 solver.cpp:218] Iteration 124400 (11.6706 iter/s, 8.56856s/100 iters), loss = 0.0293963
I1107 13:24:07.506577 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:24:07.506577 11160 solver.cpp:237]     Train net output #1: loss = 0.0293963 (* 1 = 0.0293963 loss)
I1107 13:24:07.506577 11160 sgd_solver.cpp:105] Iteration 124400, lr = 0.001
I1107 13:24:15.667268  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:24:16.009542 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_124500.caffemodel
I1107 13:24:16.040522 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_124500.solverstate
I1107 13:24:16.048527 11160 solver.cpp:330] Iteration 124500, Testing net (#0)
I1107 13:24:16.048527 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:24:18.047379 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:24:18.127410 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9155
I1107 13:24:18.127410 11160 solver.cpp:397]     Test net output #1: loss = 0.314268 (* 1 = 0.314268 loss)
I1107 13:24:18.209446 11160 solver.cpp:218] Iteration 124500 (9.34439 iter/s, 10.7016s/100 iters), loss = 0.0443214
I1107 13:24:18.209446 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:24:18.209446 11160 solver.cpp:237]     Train net output #1: loss = 0.0443215 (* 1 = 0.0443215 loss)
I1107 13:24:18.209446 11160 sgd_solver.cpp:105] Iteration 124500, lr = 0.001
I1107 13:24:26.783768 11160 solver.cpp:218] Iteration 124600 (11.6632 iter/s, 8.57396s/100 iters), loss = 0.0397643
I1107 13:24:26.783768 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:24:26.783768 11160 solver.cpp:237]     Train net output #1: loss = 0.0397644 (* 1 = 0.0397644 loss)
I1107 13:24:26.783768 11160 sgd_solver.cpp:105] Iteration 124600, lr = 0.001
I1107 13:24:35.419275 11160 solver.cpp:218] Iteration 124700 (11.581 iter/s, 8.63485s/100 iters), loss = 0.0264825
I1107 13:24:35.419275 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:24:35.419275 11160 solver.cpp:237]     Train net output #1: loss = 0.0264826 (* 1 = 0.0264826 loss)
I1107 13:24:35.419275 11160 sgd_solver.cpp:105] Iteration 124700, lr = 0.001
I1107 13:24:43.954877 11160 solver.cpp:218] Iteration 124800 (11.7153 iter/s, 8.53585s/100 iters), loss = 0.0213256
I1107 13:24:43.955878 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:24:43.955878 11160 solver.cpp:237]     Train net output #1: loss = 0.0213256 (* 1 = 0.0213256 loss)
I1107 13:24:43.955878 11160 sgd_solver.cpp:105] Iteration 124800, lr = 0.001
I1107 13:24:52.493579 11160 solver.cpp:218] Iteration 124900 (11.7132 iter/s, 8.53737s/100 iters), loss = 0.0558442
I1107 13:24:52.493579 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:24:52.493579 11160 solver.cpp:237]     Train net output #1: loss = 0.0558443 (* 1 = 0.0558443 loss)
I1107 13:24:52.493579 11160 sgd_solver.cpp:105] Iteration 124900, lr = 0.001
I1107 13:25:00.674278  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:25:01.015311 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_125000.caffemodel
I1107 13:25:01.044311 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_125000.solverstate
I1107 13:25:01.082310 11160 solver.cpp:330] Iteration 125000, Testing net (#0)
I1107 13:25:01.083312 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:25:03.105458 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:25:03.186960 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9176
I1107 13:25:03.186960 11160 solver.cpp:397]     Test net output #1: loss = 0.308742 (* 1 = 0.308742 loss)
I1107 13:25:03.269462 11160 solver.cpp:218] Iteration 125000 (9.28016 iter/s, 10.7757s/100 iters), loss = 0.0317674
I1107 13:25:03.269462 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:25:03.269462 11160 solver.cpp:237]     Train net output #1: loss = 0.0317675 (* 1 = 0.0317675 loss)
I1107 13:25:03.269462 11160 sgd_solver.cpp:105] Iteration 125000, lr = 0.001
I1107 13:25:11.875774 11160 solver.cpp:218] Iteration 125100 (11.6196 iter/s, 8.60616s/100 iters), loss = 0.0263334
I1107 13:25:11.876775 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:25:11.876775 11160 solver.cpp:237]     Train net output #1: loss = 0.0263334 (* 1 = 0.0263334 loss)
I1107 13:25:11.876775 11160 sgd_solver.cpp:105] Iteration 125100, lr = 0.001
I1107 13:25:20.419239 11160 solver.cpp:218] Iteration 125200 (11.7068 iter/s, 8.54205s/100 iters), loss = 0.0284755
I1107 13:25:20.419239 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:25:20.419239 11160 solver.cpp:237]     Train net output #1: loss = 0.0284756 (* 1 = 0.0284756 loss)
I1107 13:25:20.419239 11160 sgd_solver.cpp:105] Iteration 125200, lr = 0.001
I1107 13:25:29.009348 11160 solver.cpp:218] Iteration 125300 (11.6408 iter/s, 8.59048s/100 iters), loss = 0.0236321
I1107 13:25:29.010349 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:25:29.010349 11160 solver.cpp:237]     Train net output #1: loss = 0.0236322 (* 1 = 0.0236322 loss)
I1107 13:25:29.010349 11160 sgd_solver.cpp:105] Iteration 125300, lr = 0.001
I1107 13:25:37.680516 11160 solver.cpp:218] Iteration 125400 (11.5336 iter/s, 8.67035s/100 iters), loss = 0.0278044
I1107 13:25:37.680516 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:25:37.680516 11160 solver.cpp:237]     Train net output #1: loss = 0.0278045 (* 1 = 0.0278045 loss)
I1107 13:25:37.680516 11160 sgd_solver.cpp:105] Iteration 125400, lr = 0.001
I1107 13:25:45.979770  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:25:46.327945 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_125500.caffemodel
I1107 13:25:46.361948 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_125500.solverstate
I1107 13:25:46.370944 11160 solver.cpp:330] Iteration 125500, Testing net (#0)
I1107 13:25:46.370944 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:25:48.382562 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:25:48.463580 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9151
I1107 13:25:48.464570 11160 solver.cpp:397]     Test net output #1: loss = 0.311027 (* 1 = 0.311027 loss)
I1107 13:25:48.548591 11160 solver.cpp:218] Iteration 125500 (9.20221 iter/s, 10.867s/100 iters), loss = 0.0461599
I1107 13:25:48.548591 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:25:48.548591 11160 solver.cpp:237]     Train net output #1: loss = 0.0461599 (* 1 = 0.0461599 loss)
I1107 13:25:48.548591 11160 sgd_solver.cpp:105] Iteration 125500, lr = 0.001
I1107 13:25:57.300103 11160 solver.cpp:218] Iteration 125600 (11.4271 iter/s, 8.75112s/100 iters), loss = 0.0358048
I1107 13:25:57.300103 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:25:57.300606 11160 solver.cpp:237]     Train net output #1: loss = 0.0358048 (* 1 = 0.0358048 loss)
I1107 13:25:57.300606 11160 sgd_solver.cpp:105] Iteration 125600, lr = 0.001
I1107 13:26:06.044962 11160 solver.cpp:218] Iteration 125700 (11.4356 iter/s, 8.74462s/100 iters), loss = 0.0222715
I1107 13:26:06.044962 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:26:06.044962 11160 solver.cpp:237]     Train net output #1: loss = 0.0222716 (* 1 = 0.0222716 loss)
I1107 13:26:06.044962 11160 sgd_solver.cpp:105] Iteration 125700, lr = 0.001
I1107 13:26:14.628018 11160 solver.cpp:218] Iteration 125800 (11.6516 iter/s, 8.58251s/100 iters), loss = 0.0480206
I1107 13:26:14.628018 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:26:14.628018 11160 solver.cpp:237]     Train net output #1: loss = 0.0480207 (* 1 = 0.0480207 loss)
I1107 13:26:14.628018 11160 sgd_solver.cpp:105] Iteration 125800, lr = 0.001
I1107 13:26:23.265978 11160 solver.cpp:218] Iteration 125900 (11.5783 iter/s, 8.63682s/100 iters), loss = 0.0399327
I1107 13:26:23.265978 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:26:23.265978 11160 solver.cpp:237]     Train net output #1: loss = 0.0399328 (* 1 = 0.0399328 loss)
I1107 13:26:23.265978 11160 sgd_solver.cpp:105] Iteration 125900, lr = 0.001
I1107 13:26:31.522141  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:26:31.869194 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_126000.caffemodel
I1107 13:26:31.924409 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_126000.solverstate
I1107 13:26:31.942428 11160 solver.cpp:330] Iteration 126000, Testing net (#0)
I1107 13:26:31.942428 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:26:33.965512 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:26:34.045034 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9173
I1107 13:26:34.045034 11160 solver.cpp:397]     Test net output #1: loss = 0.306045 (* 1 = 0.306045 loss)
I1107 13:26:34.127053 11160 solver.cpp:218] Iteration 126000 (9.20743 iter/s, 10.8608s/100 iters), loss = 0.0311574
I1107 13:26:34.127053 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:26:34.127053 11160 solver.cpp:237]     Train net output #1: loss = 0.0311575 (* 1 = 0.0311575 loss)
I1107 13:26:34.127053 11160 sgd_solver.cpp:105] Iteration 126000, lr = 0.001
I1107 13:26:42.741152 11160 solver.cpp:218] Iteration 126100 (11.6098 iter/s, 8.6134s/100 iters), loss = 0.0526926
I1107 13:26:42.741152 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:26:42.741152 11160 solver.cpp:237]     Train net output #1: loss = 0.0526927 (* 1 = 0.0526927 loss)
I1107 13:26:42.741152 11160 sgd_solver.cpp:105] Iteration 126100, lr = 0.001
I1107 13:26:51.425734 11160 solver.cpp:218] Iteration 126200 (11.5156 iter/s, 8.68387s/100 iters), loss = 0.0266165
I1107 13:26:51.425734 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:26:51.425734 11160 solver.cpp:237]     Train net output #1: loss = 0.0266166 (* 1 = 0.0266166 loss)
I1107 13:26:51.425734 11160 sgd_solver.cpp:105] Iteration 126200, lr = 0.001
I1107 13:27:00.002203 11160 solver.cpp:218] Iteration 126300 (11.6604 iter/s, 8.57607s/100 iters), loss = 0.0201815
I1107 13:27:00.002203 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:27:00.002203 11160 solver.cpp:237]     Train net output #1: loss = 0.0201816 (* 1 = 0.0201816 loss)
I1107 13:27:00.002203 11160 sgd_solver.cpp:105] Iteration 126300, lr = 0.001
I1107 13:27:08.586499 11160 solver.cpp:218] Iteration 126400 (11.651 iter/s, 8.58298s/100 iters), loss = 0.0248996
I1107 13:27:08.586499 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:27:08.586499 11160 solver.cpp:237]     Train net output #1: loss = 0.0248997 (* 1 = 0.0248997 loss)
I1107 13:27:08.586499 11160 sgd_solver.cpp:105] Iteration 126400, lr = 0.001
I1107 13:27:16.756774  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:27:17.096808 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_126500.caffemodel
I1107 13:27:17.156828 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_126500.solverstate
I1107 13:27:17.165827 11160 solver.cpp:330] Iteration 126500, Testing net (#0)
I1107 13:27:17.165827 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:27:19.168037 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:27:19.248049 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9169
I1107 13:27:19.248049 11160 solver.cpp:397]     Test net output #1: loss = 0.305448 (* 1 = 0.305448 loss)
I1107 13:27:19.330054 11160 solver.cpp:218] Iteration 126500 (9.30791 iter/s, 10.7436s/100 iters), loss = 0.033027
I1107 13:27:19.330054 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:27:19.330054 11160 solver.cpp:237]     Train net output #1: loss = 0.0330272 (* 1 = 0.0330272 loss)
I1107 13:27:19.330054 11160 sgd_solver.cpp:105] Iteration 126500, lr = 0.001
I1107 13:27:27.876451 11160 solver.cpp:218] Iteration 126600 (11.7012 iter/s, 8.54613s/100 iters), loss = 0.0384653
I1107 13:27:27.876451 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:27:27.876451 11160 solver.cpp:237]     Train net output #1: loss = 0.0384655 (* 1 = 0.0384655 loss)
I1107 13:27:27.876451 11160 sgd_solver.cpp:105] Iteration 126600, lr = 0.001
I1107 13:27:36.452453 11160 solver.cpp:218] Iteration 126700 (11.6619 iter/s, 8.57496s/100 iters), loss = 0.0327832
I1107 13:27:36.452453 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:27:36.452453 11160 solver.cpp:237]     Train net output #1: loss = 0.0327833 (* 1 = 0.0327833 loss)
I1107 13:27:36.452453 11160 sgd_solver.cpp:105] Iteration 126700, lr = 0.001
I1107 13:27:45.101809 11160 solver.cpp:218] Iteration 126800 (11.5616 iter/s, 8.64933s/100 iters), loss = 0.0241092
I1107 13:27:45.101809 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:27:45.101809 11160 solver.cpp:237]     Train net output #1: loss = 0.0241093 (* 1 = 0.0241093 loss)
I1107 13:27:45.101809 11160 sgd_solver.cpp:105] Iteration 126800, lr = 0.001
I1107 13:27:53.774703 11160 solver.cpp:218] Iteration 126900 (11.5312 iter/s, 8.67215s/100 iters), loss = 0.0291233
I1107 13:27:53.774703 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:27:53.774703 11160 solver.cpp:237]     Train net output #1: loss = 0.0291234 (* 1 = 0.0291234 loss)
I1107 13:27:53.774703 11160 sgd_solver.cpp:105] Iteration 126900, lr = 0.001
I1107 13:28:02.071117  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:28:02.417201 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_127000.caffemodel
I1107 13:28:02.445019 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_127000.solverstate
I1107 13:28:02.475039 11160 solver.cpp:330] Iteration 127000, Testing net (#0)
I1107 13:28:02.476022 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:28:04.502032 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:28:04.583554 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1107 13:28:04.583554 11160 solver.cpp:397]     Test net output #1: loss = 0.304428 (* 1 = 0.304428 loss)
I1107 13:28:04.668294 11160 solver.cpp:218] Iteration 127000 (9.18076 iter/s, 10.8923s/100 iters), loss = 0.0337613
I1107 13:28:04.668294 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:28:04.668294 11160 solver.cpp:237]     Train net output #1: loss = 0.0337614 (* 1 = 0.0337614 loss)
I1107 13:28:04.668294 11160 sgd_solver.cpp:105] Iteration 127000, lr = 0.001
I1107 13:28:13.294383 11160 solver.cpp:218] Iteration 127100 (11.5927 iter/s, 8.62612s/100 iters), loss = 0.0357564
I1107 13:28:13.294383 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:28:13.294383 11160 solver.cpp:237]     Train net output #1: loss = 0.0357565 (* 1 = 0.0357565 loss)
I1107 13:28:13.294383 11160 sgd_solver.cpp:105] Iteration 127100, lr = 0.001
I1107 13:28:21.967133 11160 solver.cpp:218] Iteration 127200 (11.5317 iter/s, 8.67174s/100 iters), loss = 0.0243073
I1107 13:28:21.967133 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:28:21.967133 11160 solver.cpp:237]     Train net output #1: loss = 0.0243074 (* 1 = 0.0243074 loss)
I1107 13:28:21.967133 11160 sgd_solver.cpp:105] Iteration 127200, lr = 0.001
I1107 13:28:30.514843 11160 solver.cpp:218] Iteration 127300 (11.6997 iter/s, 8.54725s/100 iters), loss = 0.0371369
I1107 13:28:30.514843 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:28:30.514843 11160 solver.cpp:237]     Train net output #1: loss = 0.037137 (* 1 = 0.037137 loss)
I1107 13:28:30.514843 11160 sgd_solver.cpp:105] Iteration 127300, lr = 0.001
I1107 13:28:39.104190 11160 solver.cpp:218] Iteration 127400 (11.6428 iter/s, 8.58897s/100 iters), loss = 0.0273841
I1107 13:28:39.104190 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:28:39.104190 11160 solver.cpp:237]     Train net output #1: loss = 0.0273841 (* 1 = 0.0273841 loss)
I1107 13:28:39.104190 11160 sgd_solver.cpp:105] Iteration 127400, lr = 0.001
I1107 13:28:47.268004  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:28:47.609057 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_127500.caffemodel
I1107 13:28:47.639560 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_127500.solverstate
I1107 13:28:47.648063 11160 solver.cpp:330] Iteration 127500, Testing net (#0)
I1107 13:28:47.648063 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:28:49.662294 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:28:49.744303 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1107 13:28:49.744303 11160 solver.cpp:397]     Test net output #1: loss = 0.312265 (* 1 = 0.312265 loss)
I1107 13:28:49.828303 11160 solver.cpp:218] Iteration 127500 (9.32555 iter/s, 10.7232s/100 iters), loss = 0.0278928
I1107 13:28:49.828303 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:28:49.828303 11160 solver.cpp:237]     Train net output #1: loss = 0.0278929 (* 1 = 0.0278929 loss)
I1107 13:28:49.828303 11160 sgd_solver.cpp:105] Iteration 127500, lr = 0.001
I1107 13:28:58.407505 11160 solver.cpp:218] Iteration 127600 (11.6563 iter/s, 8.57904s/100 iters), loss = 0.0435393
I1107 13:28:58.407505 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:28:58.407505 11160 solver.cpp:237]     Train net output #1: loss = 0.0435394 (* 1 = 0.0435394 loss)
I1107 13:28:58.407505 11160 sgd_solver.cpp:105] Iteration 127600, lr = 0.001
I1107 13:29:06.955746 11160 solver.cpp:218] Iteration 127700 (11.6992 iter/s, 8.54758s/100 iters), loss = 0.0199337
I1107 13:29:06.955746 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:29:06.955746 11160 solver.cpp:237]     Train net output #1: loss = 0.0199338 (* 1 = 0.0199338 loss)
I1107 13:29:06.955746 11160 sgd_solver.cpp:105] Iteration 127700, lr = 0.001
I1107 13:29:15.493968 11160 solver.cpp:218] Iteration 127800 (11.7119 iter/s, 8.5383s/100 iters), loss = 0.0309024
I1107 13:29:15.493968 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:29:15.493968 11160 solver.cpp:237]     Train net output #1: loss = 0.0309025 (* 1 = 0.0309025 loss)
I1107 13:29:15.493968 11160 sgd_solver.cpp:105] Iteration 127800, lr = 0.001
I1107 13:29:24.048353 11160 solver.cpp:218] Iteration 127900 (11.6916 iter/s, 8.55315s/100 iters), loss = 0.0301276
I1107 13:29:24.048353 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:29:24.048353 11160 solver.cpp:237]     Train net output #1: loss = 0.0301277 (* 1 = 0.0301277 loss)
I1107 13:29:24.048353 11160 sgd_solver.cpp:105] Iteration 127900, lr = 0.001
I1107 13:29:32.178108  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:29:32.514127 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_128000.caffemodel
I1107 13:29:32.544129 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_128000.solverstate
I1107 13:29:32.575145 11160 solver.cpp:330] Iteration 128000, Testing net (#0)
I1107 13:29:32.575145 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:29:34.564553 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:29:34.644554 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9178
I1107 13:29:34.644554 11160 solver.cpp:397]     Test net output #1: loss = 0.311552 (* 1 = 0.311552 loss)
I1107 13:29:34.725556 11160 solver.cpp:218] Iteration 128000 (9.36569 iter/s, 10.6773s/100 iters), loss = 0.0379453
I1107 13:29:34.725556 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:29:34.725556 11160 solver.cpp:237]     Train net output #1: loss = 0.0379454 (* 1 = 0.0379454 loss)
I1107 13:29:34.725556 11160 sgd_solver.cpp:105] Iteration 128000, lr = 0.001
I1107 13:29:43.257722 11160 solver.cpp:218] Iteration 128100 (11.7222 iter/s, 8.53085s/100 iters), loss = 0.0288058
I1107 13:29:43.257722 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:29:43.257722 11160 solver.cpp:237]     Train net output #1: loss = 0.0288059 (* 1 = 0.0288059 loss)
I1107 13:29:43.257722 11160 sgd_solver.cpp:105] Iteration 128100, lr = 0.001
I1107 13:29:51.796756 11160 solver.cpp:218] Iteration 128200 (11.7107 iter/s, 8.5392s/100 iters), loss = 0.0184458
I1107 13:29:51.796756 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:29:51.796756 11160 solver.cpp:237]     Train net output #1: loss = 0.0184459 (* 1 = 0.0184459 loss)
I1107 13:29:51.796756 11160 sgd_solver.cpp:105] Iteration 128200, lr = 0.001
I1107 13:30:00.341900 11160 solver.cpp:218] Iteration 128300 (11.7042 iter/s, 8.54394s/100 iters), loss = 0.018714
I1107 13:30:00.341900 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:30:00.341900 11160 solver.cpp:237]     Train net output #1: loss = 0.0187141 (* 1 = 0.0187141 loss)
I1107 13:30:00.341900 11160 sgd_solver.cpp:105] Iteration 128300, lr = 0.001
I1107 13:30:08.923998 11160 solver.cpp:218] Iteration 128400 (11.6524 iter/s, 8.58196s/100 iters), loss = 0.0341677
I1107 13:30:08.923998 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:30:08.923998 11160 solver.cpp:237]     Train net output #1: loss = 0.0341678 (* 1 = 0.0341678 loss)
I1107 13:30:08.923998 11160 sgd_solver.cpp:105] Iteration 128400, lr = 0.001
I1107 13:30:17.045428  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:30:17.383474 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_128500.caffemodel
I1107 13:30:17.412475 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_128500.solverstate
I1107 13:30:17.421473 11160 solver.cpp:330] Iteration 128500, Testing net (#0)
I1107 13:30:17.421473 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:30:19.411783 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:30:19.491842 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9172
I1107 13:30:19.491842 11160 solver.cpp:397]     Test net output #1: loss = 0.313351 (* 1 = 0.313351 loss)
I1107 13:30:19.573328 11160 solver.cpp:218] Iteration 128500 (9.39079 iter/s, 10.6487s/100 iters), loss = 0.0246661
I1107 13:30:19.573328 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:30:19.573328 11160 solver.cpp:237]     Train net output #1: loss = 0.0246662 (* 1 = 0.0246662 loss)
I1107 13:30:19.573829 11160 sgd_solver.cpp:105] Iteration 128500, lr = 0.001
I1107 13:30:28.116835 11160 solver.cpp:218] Iteration 128600 (11.7055 iter/s, 8.54296s/100 iters), loss = 0.0508419
I1107 13:30:28.116835 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:30:28.116835 11160 solver.cpp:237]     Train net output #1: loss = 0.0508421 (* 1 = 0.0508421 loss)
I1107 13:30:28.116835 11160 sgd_solver.cpp:105] Iteration 128600, lr = 0.001
I1107 13:30:36.667428 11160 solver.cpp:218] Iteration 128700 (11.696 iter/s, 8.54992s/100 iters), loss = 0.0365199
I1107 13:30:36.667428 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:30:36.667428 11160 solver.cpp:237]     Train net output #1: loss = 0.0365201 (* 1 = 0.0365201 loss)
I1107 13:30:36.667428 11160 sgd_solver.cpp:105] Iteration 128700, lr = 0.001
I1107 13:30:45.203858 11160 solver.cpp:218] Iteration 128800 (11.7154 iter/s, 8.53581s/100 iters), loss = 0.0290918
I1107 13:30:45.203858 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:30:45.203858 11160 solver.cpp:237]     Train net output #1: loss = 0.029092 (* 1 = 0.029092 loss)
I1107 13:30:45.203858 11160 sgd_solver.cpp:105] Iteration 128800, lr = 0.001
I1107 13:30:53.735663 11160 solver.cpp:218] Iteration 128900 (11.7215 iter/s, 8.53131s/100 iters), loss = 0.0343277
I1107 13:30:53.735663 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:30:53.735663 11160 solver.cpp:237]     Train net output #1: loss = 0.0343278 (* 1 = 0.0343278 loss)
I1107 13:30:53.735663 11160 sgd_solver.cpp:105] Iteration 128900, lr = 0.001
I1107 13:31:01.864763  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:31:02.203774 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_129000.caffemodel
I1107 13:31:02.237783 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_129000.solverstate
I1107 13:31:02.258785 11160 solver.cpp:330] Iteration 129000, Testing net (#0)
I1107 13:31:02.258785 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:31:04.250114 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:31:04.330139 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9174
I1107 13:31:04.330139 11160 solver.cpp:397]     Test net output #1: loss = 0.316522 (* 1 = 0.316522 loss)
I1107 13:31:04.412142 11160 solver.cpp:218] Iteration 129000 (9.36704 iter/s, 10.6757s/100 iters), loss = 0.032016
I1107 13:31:04.412142 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:31:04.412142 11160 solver.cpp:237]     Train net output #1: loss = 0.0320161 (* 1 = 0.0320161 loss)
I1107 13:31:04.412142 11160 sgd_solver.cpp:105] Iteration 129000, lr = 0.001
I1107 13:31:12.947098 11160 solver.cpp:218] Iteration 129100 (11.7171 iter/s, 8.53452s/100 iters), loss = 0.026736
I1107 13:31:12.947098 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:31:12.947098 11160 solver.cpp:237]     Train net output #1: loss = 0.0267361 (* 1 = 0.0267361 loss)
I1107 13:31:12.947098 11160 sgd_solver.cpp:105] Iteration 129100, lr = 0.001
I1107 13:31:21.492703 11160 solver.cpp:218] Iteration 129200 (11.7027 iter/s, 8.54502s/100 iters), loss = 0.0275944
I1107 13:31:21.492703 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:31:21.492703 11160 solver.cpp:237]     Train net output #1: loss = 0.0275945 (* 1 = 0.0275945 loss)
I1107 13:31:21.492703 11160 sgd_solver.cpp:105] Iteration 129200, lr = 0.001
I1107 13:31:30.044668 11160 solver.cpp:218] Iteration 129300 (11.6934 iter/s, 8.55182s/100 iters), loss = 0.0365815
I1107 13:31:30.044668 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:31:30.044668 11160 solver.cpp:237]     Train net output #1: loss = 0.0365816 (* 1 = 0.0365816 loss)
I1107 13:31:30.044668 11160 sgd_solver.cpp:105] Iteration 129300, lr = 0.001
I1107 13:31:38.598793 11160 solver.cpp:218] Iteration 129400 (11.6911 iter/s, 8.5535s/100 iters), loss = 0.0249272
I1107 13:31:38.598793 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:31:38.598793 11160 solver.cpp:237]     Train net output #1: loss = 0.0249272 (* 1 = 0.0249272 loss)
I1107 13:31:38.598793 11160 sgd_solver.cpp:105] Iteration 129400, lr = 0.001
I1107 13:31:46.734516  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:31:47.072530 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_129500.caffemodel
I1107 13:31:47.123551 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_129500.solverstate
I1107 13:31:47.132550 11160 solver.cpp:330] Iteration 129500, Testing net (#0)
I1107 13:31:47.132550 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:31:49.124716 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:31:49.204720 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 13:31:49.204720 11160 solver.cpp:397]     Test net output #1: loss = 0.311866 (* 1 = 0.311866 loss)
I1107 13:31:49.286722 11160 solver.cpp:218] Iteration 129500 (9.35652 iter/s, 10.6877s/100 iters), loss = 0.0300648
I1107 13:31:49.286722 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:31:49.286722 11160 solver.cpp:237]     Train net output #1: loss = 0.0300649 (* 1 = 0.0300649 loss)
I1107 13:31:49.286722 11160 sgd_solver.cpp:105] Iteration 129500, lr = 0.001
I1107 13:31:57.828572 11160 solver.cpp:218] Iteration 129600 (11.7075 iter/s, 8.54157s/100 iters), loss = 0.0280406
I1107 13:31:57.829571 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:31:57.829571 11160 solver.cpp:237]     Train net output #1: loss = 0.0280407 (* 1 = 0.0280407 loss)
I1107 13:31:57.829571 11160 sgd_solver.cpp:105] Iteration 129600, lr = 0.001
I1107 13:32:06.370405 11160 solver.cpp:218] Iteration 129700 (11.7077 iter/s, 8.54137s/100 iters), loss = 0.0384334
I1107 13:32:06.371405 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:32:06.371405 11160 solver.cpp:237]     Train net output #1: loss = 0.0384335 (* 1 = 0.0384335 loss)
I1107 13:32:06.371405 11160 sgd_solver.cpp:105] Iteration 129700, lr = 0.001
I1107 13:32:14.929345 11160 solver.cpp:218] Iteration 129800 (11.6848 iter/s, 8.55812s/100 iters), loss = 0.0227811
I1107 13:32:14.929345 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:32:14.929345 11160 solver.cpp:237]     Train net output #1: loss = 0.0227812 (* 1 = 0.0227812 loss)
I1107 13:32:14.929345 11160 sgd_solver.cpp:105] Iteration 129800, lr = 0.001
I1107 13:32:23.467115 11160 solver.cpp:218] Iteration 129900 (11.7133 iter/s, 8.53731s/100 iters), loss = 0.0472341
I1107 13:32:23.467115 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:32:23.467115 11160 solver.cpp:237]     Train net output #1: loss = 0.0472342 (* 1 = 0.0472342 loss)
I1107 13:32:23.467115 11160 sgd_solver.cpp:105] Iteration 129900, lr = 0.001
I1107 13:32:31.590339  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:32:31.929373 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_130000.caffemodel
I1107 13:32:31.961372 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_130000.solverstate
I1107 13:32:31.971374 11160 solver.cpp:330] Iteration 130000, Testing net (#0)
I1107 13:32:31.971374 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:32:33.960536 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:32:34.039541 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1107 13:32:34.039541 11160 solver.cpp:397]     Test net output #1: loss = 0.30776 (* 1 = 0.30776 loss)
I1107 13:32:34.121546 11160 solver.cpp:218] Iteration 130000 (9.38687 iter/s, 10.6532s/100 iters), loss = 0.0281814
I1107 13:32:34.121546 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:32:34.121546 11160 solver.cpp:237]     Train net output #1: loss = 0.0281815 (* 1 = 0.0281815 loss)
I1107 13:32:34.121546 11160 sgd_solver.cpp:105] Iteration 130000, lr = 0.001
I1107 13:32:42.657145 11160 solver.cpp:218] Iteration 130100 (11.7154 iter/s, 8.53578s/100 iters), loss = 0.0333153
I1107 13:32:42.657145 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:32:42.657145 11160 solver.cpp:237]     Train net output #1: loss = 0.0333154 (* 1 = 0.0333154 loss)
I1107 13:32:42.657145 11160 sgd_solver.cpp:105] Iteration 130100, lr = 0.001
I1107 13:32:51.189400 11160 solver.cpp:218] Iteration 130200 (11.7206 iter/s, 8.53197s/100 iters), loss = 0.029831
I1107 13:32:51.189400 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:32:51.189400 11160 solver.cpp:237]     Train net output #1: loss = 0.0298312 (* 1 = 0.0298312 loss)
I1107 13:32:51.190400 11160 sgd_solver.cpp:105] Iteration 130200, lr = 0.001
I1107 13:32:59.723742 11160 solver.cpp:218] Iteration 130300 (11.7191 iter/s, 8.53306s/100 iters), loss = 0.0441123
I1107 13:32:59.723742 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:32:59.723742 11160 solver.cpp:237]     Train net output #1: loss = 0.0441124 (* 1 = 0.0441124 loss)
I1107 13:32:59.723742 11160 sgd_solver.cpp:105] Iteration 130300, lr = 0.001
I1107 13:33:08.254801 11160 solver.cpp:218] Iteration 130400 (11.7217 iter/s, 8.5312s/100 iters), loss = 0.0312002
I1107 13:33:08.254801 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:33:08.254801 11160 solver.cpp:237]     Train net output #1: loss = 0.0312003 (* 1 = 0.0312003 loss)
I1107 13:33:08.254801 11160 sgd_solver.cpp:105] Iteration 130400, lr = 0.001
I1107 13:33:16.412797  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:33:16.749956 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_130500.caffemodel
I1107 13:33:16.779958 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_130500.solverstate
I1107 13:33:16.788959 11160 solver.cpp:330] Iteration 130500, Testing net (#0)
I1107 13:33:16.788959 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:33:18.797215 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:33:18.876235 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 13:33:18.876235 11160 solver.cpp:397]     Test net output #1: loss = 0.311811 (* 1 = 0.311811 loss)
I1107 13:33:18.960553 11160 solver.cpp:218] Iteration 130500 (9.34192 iter/s, 10.7044s/100 iters), loss = 0.0258552
I1107 13:33:18.960553 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:33:18.960553 11160 solver.cpp:237]     Train net output #1: loss = 0.0258553 (* 1 = 0.0258553 loss)
I1107 13:33:18.960553 11160 sgd_solver.cpp:105] Iteration 130500, lr = 0.001
I1107 13:33:27.642156 11160 solver.cpp:218] Iteration 130600 (11.5193 iter/s, 8.68108s/100 iters), loss = 0.0299425
I1107 13:33:27.642156 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:33:27.642156 11160 solver.cpp:237]     Train net output #1: loss = 0.0299426 (* 1 = 0.0299426 loss)
I1107 13:33:27.642156 11160 sgd_solver.cpp:105] Iteration 130600, lr = 0.001
I1107 13:33:36.221453 11160 solver.cpp:218] Iteration 130700 (11.6568 iter/s, 8.5787s/100 iters), loss = 0.0204683
I1107 13:33:36.221453 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:33:36.221453 11160 solver.cpp:237]     Train net output #1: loss = 0.0204684 (* 1 = 0.0204684 loss)
I1107 13:33:36.221453 11160 sgd_solver.cpp:105] Iteration 130700, lr = 0.001
I1107 13:33:44.796352 11160 solver.cpp:218] Iteration 130800 (11.6618 iter/s, 8.57502s/100 iters), loss = 0.021814
I1107 13:33:44.796352 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:33:44.796352 11160 solver.cpp:237]     Train net output #1: loss = 0.0218141 (* 1 = 0.0218141 loss)
I1107 13:33:44.796352 11160 sgd_solver.cpp:105] Iteration 130800, lr = 0.001
I1107 13:33:53.412690 11160 solver.cpp:218] Iteration 130900 (11.6069 iter/s, 8.61554s/100 iters), loss = 0.029513
I1107 13:33:53.412690 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:33:53.412690 11160 solver.cpp:237]     Train net output #1: loss = 0.029513 (* 1 = 0.029513 loss)
I1107 13:33:53.412690 11160 sgd_solver.cpp:105] Iteration 130900, lr = 0.001
I1107 13:34:01.610747  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:34:01.950773 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_131000.caffemodel
I1107 13:34:01.982772 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_131000.solverstate
I1107 13:34:01.991772 11160 solver.cpp:330] Iteration 131000, Testing net (#0)
I1107 13:34:01.991772 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:34:03.984964 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:34:04.064985 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9169
I1107 13:34:04.064985 11160 solver.cpp:397]     Test net output #1: loss = 0.309775 (* 1 = 0.309775 loss)
I1107 13:34:04.145977 11160 solver.cpp:218] Iteration 131000 (9.31708 iter/s, 10.733s/100 iters), loss = 0.0412086
I1107 13:34:04.145977 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:34:04.145977 11160 solver.cpp:237]     Train net output #1: loss = 0.0412087 (* 1 = 0.0412087 loss)
I1107 13:34:04.145977 11160 sgd_solver.cpp:105] Iteration 131000, lr = 0.001
I1107 13:34:12.733216 11160 solver.cpp:218] Iteration 131100 (11.6468 iter/s, 8.58602s/100 iters), loss = 0.0278022
I1107 13:34:12.733216 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:34:12.733216 11160 solver.cpp:237]     Train net output #1: loss = 0.0278023 (* 1 = 0.0278023 loss)
I1107 13:34:12.733216 11160 sgd_solver.cpp:105] Iteration 131100, lr = 0.001
I1107 13:34:21.354607 11160 solver.cpp:218] Iteration 131200 (11.5988 iter/s, 8.62158s/100 iters), loss = 0.0224798
I1107 13:34:21.354607 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:34:21.355608 11160 solver.cpp:237]     Train net output #1: loss = 0.0224799 (* 1 = 0.0224799 loss)
I1107 13:34:21.355608 11160 sgd_solver.cpp:105] Iteration 131200, lr = 0.001
I1107 13:34:29.985494 11160 solver.cpp:218] Iteration 131300 (11.5878 iter/s, 8.62978s/100 iters), loss = 0.0198975
I1107 13:34:29.985494 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:34:29.985494 11160 solver.cpp:237]     Train net output #1: loss = 0.0198976 (* 1 = 0.0198976 loss)
I1107 13:34:29.985494 11160 sgd_solver.cpp:105] Iteration 131300, lr = 0.001
I1107 13:34:38.618616 11160 solver.cpp:218] Iteration 131400 (11.5847 iter/s, 8.63211s/100 iters), loss = 0.0382804
I1107 13:34:38.618616 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:34:38.618616 11160 solver.cpp:237]     Train net output #1: loss = 0.0382805 (* 1 = 0.0382805 loss)
I1107 13:34:38.618616 11160 sgd_solver.cpp:105] Iteration 131400, lr = 0.001
I1107 13:34:46.790793  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:34:47.131639 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_131500.caffemodel
I1107 13:34:47.160640 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_131500.solverstate
I1107 13:34:47.170644 11160 solver.cpp:330] Iteration 131500, Testing net (#0)
I1107 13:34:47.170644 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:34:49.171093 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:34:49.250594 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 13:34:49.250594 11160 solver.cpp:397]     Test net output #1: loss = 0.311921 (* 1 = 0.311921 loss)
I1107 13:34:49.334602 11160 solver.cpp:218] Iteration 131500 (9.33175 iter/s, 10.7161s/100 iters), loss = 0.0338602
I1107 13:34:49.334602 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:34:49.334602 11160 solver.cpp:237]     Train net output #1: loss = 0.0338603 (* 1 = 0.0338603 loss)
I1107 13:34:49.334602 11160 sgd_solver.cpp:105] Iteration 131500, lr = 0.001
I1107 13:34:57.942736 11160 solver.cpp:218] Iteration 131600 (11.6184 iter/s, 8.60705s/100 iters), loss = 0.0316547
I1107 13:34:57.942736 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:34:57.942736 11160 solver.cpp:237]     Train net output #1: loss = 0.0316548 (* 1 = 0.0316548 loss)
I1107 13:34:57.942736 11160 sgd_solver.cpp:105] Iteration 131600, lr = 0.001
I1107 13:35:06.536963 11160 solver.cpp:218] Iteration 131700 (11.6367 iter/s, 8.59353s/100 iters), loss = 0.0351036
I1107 13:35:06.536963 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:35:06.536963 11160 solver.cpp:237]     Train net output #1: loss = 0.0351036 (* 1 = 0.0351036 loss)
I1107 13:35:06.536963 11160 sgd_solver.cpp:105] Iteration 131700, lr = 0.001
I1107 13:35:15.148221 11160 solver.cpp:218] Iteration 131800 (11.6128 iter/s, 8.61117s/100 iters), loss = 0.0191167
I1107 13:35:15.148221 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:35:15.148221 11160 solver.cpp:237]     Train net output #1: loss = 0.0191167 (* 1 = 0.0191167 loss)
I1107 13:35:15.148221 11160 sgd_solver.cpp:105] Iteration 131800, lr = 0.001
I1107 13:35:23.716881 11160 solver.cpp:218] Iteration 131900 (11.6713 iter/s, 8.56799s/100 iters), loss = 0.0469472
I1107 13:35:23.716881 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:35:23.716881 11160 solver.cpp:237]     Train net output #1: loss = 0.0469473 (* 1 = 0.0469473 loss)
I1107 13:35:23.716881 11160 sgd_solver.cpp:105] Iteration 131900, lr = 0.001
I1107 13:35:31.876703  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:35:32.214264 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_132000.caffemodel
I1107 13:35:32.245260 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_132000.solverstate
I1107 13:35:32.254261 11160 solver.cpp:330] Iteration 132000, Testing net (#0)
I1107 13:35:32.254261 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:35:34.253253 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:35:34.334257 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9178
I1107 13:35:34.334257 11160 solver.cpp:397]     Test net output #1: loss = 0.309276 (* 1 = 0.309276 loss)
I1107 13:35:34.416262 11160 solver.cpp:218] Iteration 132000 (9.34684 iter/s, 10.6988s/100 iters), loss = 0.0323871
I1107 13:35:34.416262 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:35:34.416262 11160 solver.cpp:237]     Train net output #1: loss = 0.0323872 (* 1 = 0.0323872 loss)
I1107 13:35:34.416262 11160 sgd_solver.cpp:105] Iteration 132000, lr = 0.001
I1107 13:35:42.974694 11160 solver.cpp:218] Iteration 132100 (11.6854 iter/s, 8.55772s/100 iters), loss = 0.0324177
I1107 13:35:42.974694 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:35:42.974694 11160 solver.cpp:237]     Train net output #1: loss = 0.0324177 (* 1 = 0.0324177 loss)
I1107 13:35:42.974694 11160 sgd_solver.cpp:105] Iteration 132100, lr = 0.001
I1107 13:35:51.516872 11160 solver.cpp:218] Iteration 132200 (11.7075 iter/s, 8.54151s/100 iters), loss = 0.0282472
I1107 13:35:51.516872 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:35:51.516872 11160 solver.cpp:237]     Train net output #1: loss = 0.0282473 (* 1 = 0.0282473 loss)
I1107 13:35:51.516872 11160 sgd_solver.cpp:105] Iteration 132200, lr = 0.001
I1107 13:36:00.084293 11160 solver.cpp:218] Iteration 132300 (11.673 iter/s, 8.5668s/100 iters), loss = 0.0329289
I1107 13:36:00.084293 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:36:00.084293 11160 solver.cpp:237]     Train net output #1: loss = 0.032929 (* 1 = 0.032929 loss)
I1107 13:36:00.084293 11160 sgd_solver.cpp:105] Iteration 132300, lr = 0.001
I1107 13:36:08.637614 11160 solver.cpp:218] Iteration 132400 (11.691 iter/s, 8.55357s/100 iters), loss = 0.0349023
I1107 13:36:08.638615 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:36:08.638615 11160 solver.cpp:237]     Train net output #1: loss = 0.0349024 (* 1 = 0.0349024 loss)
I1107 13:36:08.638615 11160 sgd_solver.cpp:105] Iteration 132400, lr = 0.001
I1107 13:36:16.758211  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:36:17.096220 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_132500.caffemodel
I1107 13:36:17.132225 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_132500.solverstate
I1107 13:36:17.141239 11160 solver.cpp:330] Iteration 132500, Testing net (#0)
I1107 13:36:17.141239 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:36:19.136590 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:36:19.216604 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9172
I1107 13:36:19.216604 11160 solver.cpp:397]     Test net output #1: loss = 0.316589 (* 1 = 0.316589 loss)
I1107 13:36:19.298609 11160 solver.cpp:218] Iteration 132500 (9.38128 iter/s, 10.6595s/100 iters), loss = 0.0470043
I1107 13:36:19.298609 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:36:19.298609 11160 solver.cpp:237]     Train net output #1: loss = 0.0470044 (* 1 = 0.0470044 loss)
I1107 13:36:19.298609 11160 sgd_solver.cpp:105] Iteration 132500, lr = 0.001
I1107 13:36:27.827697 11160 solver.cpp:218] Iteration 132600 (11.7249 iter/s, 8.52888s/100 iters), loss = 0.0294889
I1107 13:36:27.827697 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:36:27.827697 11160 solver.cpp:237]     Train net output #1: loss = 0.0294889 (* 1 = 0.0294889 loss)
I1107 13:36:27.827697 11160 sgd_solver.cpp:105] Iteration 132600, lr = 0.001
I1107 13:36:36.344933 11160 solver.cpp:218] Iteration 132700 (11.7419 iter/s, 8.51651s/100 iters), loss = 0.0223004
I1107 13:36:36.344933 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:36:36.344933 11160 solver.cpp:237]     Train net output #1: loss = 0.0223005 (* 1 = 0.0223005 loss)
I1107 13:36:36.344933 11160 sgd_solver.cpp:105] Iteration 132700, lr = 0.001
I1107 13:36:44.879060 11160 solver.cpp:218] Iteration 132800 (11.7185 iter/s, 8.53354s/100 iters), loss = 0.0220382
I1107 13:36:44.879060 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:36:44.879060 11160 solver.cpp:237]     Train net output #1: loss = 0.0220382 (* 1 = 0.0220382 loss)
I1107 13:36:44.879060 11160 sgd_solver.cpp:105] Iteration 132800, lr = 0.001
I1107 13:36:53.416210 11160 solver.cpp:218] Iteration 132900 (11.7143 iter/s, 8.53658s/100 iters), loss = 0.0373204
I1107 13:36:53.416210 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:36:53.416210 11160 solver.cpp:237]     Train net output #1: loss = 0.0373205 (* 1 = 0.0373205 loss)
I1107 13:36:53.416210 11160 sgd_solver.cpp:105] Iteration 132900, lr = 0.001
I1107 13:37:01.545987  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:37:01.886020 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_133000.caffemodel
I1107 13:37:01.916020 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_133000.solverstate
I1107 13:37:01.925524 11160 solver.cpp:330] Iteration 133000, Testing net (#0)
I1107 13:37:01.925524 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:37:03.923205 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:37:04.002207 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9153
I1107 13:37:04.002207 11160 solver.cpp:397]     Test net output #1: loss = 0.313354 (* 1 = 0.313354 loss)
I1107 13:37:04.084226 11160 solver.cpp:218] Iteration 133000 (9.37372 iter/s, 10.6681s/100 iters), loss = 0.0409853
I1107 13:37:04.084226 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:37:04.084226 11160 solver.cpp:237]     Train net output #1: loss = 0.0409853 (* 1 = 0.0409853 loss)
I1107 13:37:04.084226 11160 sgd_solver.cpp:105] Iteration 133000, lr = 0.001
I1107 13:37:12.626901 11160 solver.cpp:218] Iteration 133100 (11.7074 iter/s, 8.54163s/100 iters), loss = 0.0583931
I1107 13:37:12.626901 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:37:12.626901 11160 solver.cpp:237]     Train net output #1: loss = 0.0583932 (* 1 = 0.0583932 loss)
I1107 13:37:12.626901 11160 sgd_solver.cpp:105] Iteration 133100, lr = 0.001
I1107 13:37:21.168036 11160 solver.cpp:218] Iteration 133200 (11.708 iter/s, 8.54117s/100 iters), loss = 0.0206972
I1107 13:37:21.168036 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:37:21.168036 11160 solver.cpp:237]     Train net output #1: loss = 0.0206972 (* 1 = 0.0206972 loss)
I1107 13:37:21.168036 11160 sgd_solver.cpp:105] Iteration 133200, lr = 0.001
I1107 13:37:29.805846 11160 solver.cpp:218] Iteration 133300 (11.5776 iter/s, 8.63737s/100 iters), loss = 0.024992
I1107 13:37:29.805846 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:37:29.805846 11160 solver.cpp:237]     Train net output #1: loss = 0.0249921 (* 1 = 0.0249921 loss)
I1107 13:37:29.805846 11160 sgd_solver.cpp:105] Iteration 133300, lr = 0.001
I1107 13:37:38.411423 11160 solver.cpp:218] Iteration 133400 (11.6221 iter/s, 8.60428s/100 iters), loss = 0.0304399
I1107 13:37:38.411423 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:37:38.411423 11160 solver.cpp:237]     Train net output #1: loss = 0.03044 (* 1 = 0.03044 loss)
I1107 13:37:38.411423 11160 sgd_solver.cpp:105] Iteration 133400, lr = 0.001
I1107 13:37:46.568454  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:37:46.908510 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_133500.caffemodel
I1107 13:37:46.937510 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_133500.solverstate
I1107 13:37:46.947513 11160 solver.cpp:330] Iteration 133500, Testing net (#0)
I1107 13:37:46.947513 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:37:48.942571 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:37:49.022578 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9175
I1107 13:37:49.022578 11160 solver.cpp:397]     Test net output #1: loss = 0.31607 (* 1 = 0.31607 loss)
I1107 13:37:49.103581 11160 solver.cpp:218] Iteration 133500 (9.35248 iter/s, 10.6923s/100 iters), loss = 0.0228459
I1107 13:37:49.103581 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:37:49.103581 11160 solver.cpp:237]     Train net output #1: loss = 0.022846 (* 1 = 0.022846 loss)
I1107 13:37:49.103581 11160 sgd_solver.cpp:105] Iteration 133500, lr = 0.001
I1107 13:37:57.672808 11160 solver.cpp:218] Iteration 133600 (11.6709 iter/s, 8.56828s/100 iters), loss = 0.0329559
I1107 13:37:57.672808 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:37:57.672808 11160 solver.cpp:237]     Train net output #1: loss = 0.032956 (* 1 = 0.032956 loss)
I1107 13:37:57.672808 11160 sgd_solver.cpp:105] Iteration 133600, lr = 0.001
I1107 13:38:06.241015 11160 solver.cpp:218] Iteration 133700 (11.6712 iter/s, 8.56813s/100 iters), loss = 0.0238116
I1107 13:38:06.241015 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:38:06.241015 11160 solver.cpp:237]     Train net output #1: loss = 0.0238116 (* 1 = 0.0238116 loss)
I1107 13:38:06.241015 11160 sgd_solver.cpp:105] Iteration 133700, lr = 0.001
I1107 13:38:14.837606 11160 solver.cpp:218] Iteration 133800 (11.6331 iter/s, 8.59617s/100 iters), loss = 0.0274524
I1107 13:38:14.838606 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:38:14.838606 11160 solver.cpp:237]     Train net output #1: loss = 0.0274524 (* 1 = 0.0274524 loss)
I1107 13:38:14.838606 11160 sgd_solver.cpp:105] Iteration 133800, lr = 0.001
I1107 13:38:23.395560 11160 solver.cpp:218] Iteration 133900 (11.6871 iter/s, 8.55645s/100 iters), loss = 0.0239114
I1107 13:38:23.395560 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:38:23.395560 11160 solver.cpp:237]     Train net output #1: loss = 0.0239114 (* 1 = 0.0239114 loss)
I1107 13:38:23.395560 11160 sgd_solver.cpp:105] Iteration 133900, lr = 0.001
I1107 13:38:31.542212  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:38:31.880245 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_134000.caffemodel
I1107 13:38:31.910245 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_134000.solverstate
I1107 13:38:31.920246 11160 solver.cpp:330] Iteration 134000, Testing net (#0)
I1107 13:38:31.920246 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:38:33.956744 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:38:34.038240 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9174
I1107 13:38:34.038240 11160 solver.cpp:397]     Test net output #1: loss = 0.310123 (* 1 = 0.310123 loss)
I1107 13:38:34.122247 11160 solver.cpp:218] Iteration 134000 (9.32244 iter/s, 10.7268s/100 iters), loss = 0.0258183
I1107 13:38:34.123248 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:38:34.123248 11160 solver.cpp:237]     Train net output #1: loss = 0.0258183 (* 1 = 0.0258183 loss)
I1107 13:38:34.123248 11160 sgd_solver.cpp:105] Iteration 134000, lr = 0.001
I1107 13:38:42.826640 11160 solver.cpp:218] Iteration 134100 (11.4895 iter/s, 8.70361s/100 iters), loss = 0.0399426
I1107 13:38:42.826640 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:38:42.826640 11160 solver.cpp:237]     Train net output #1: loss = 0.0399427 (* 1 = 0.0399427 loss)
I1107 13:38:42.826640 11160 sgd_solver.cpp:105] Iteration 134100, lr = 0.001
I1107 13:38:51.521359 11160 solver.cpp:218] Iteration 134200 (11.5028 iter/s, 8.69351s/100 iters), loss = 0.0467042
I1107 13:38:51.521359 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:38:51.521359 11160 solver.cpp:237]     Train net output #1: loss = 0.0467043 (* 1 = 0.0467043 loss)
I1107 13:38:51.521359 11160 sgd_solver.cpp:105] Iteration 134200, lr = 0.001
I1107 13:39:00.132432 11160 solver.cpp:218] Iteration 134300 (11.6128 iter/s, 8.61117s/100 iters), loss = 0.0312199
I1107 13:39:00.132432 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:39:00.132432 11160 solver.cpp:237]     Train net output #1: loss = 0.03122 (* 1 = 0.03122 loss)
I1107 13:39:00.132432 11160 sgd_solver.cpp:105] Iteration 134300, lr = 0.001
I1107 13:39:08.740267 11160 solver.cpp:218] Iteration 134400 (11.6182 iter/s, 8.60717s/100 iters), loss = 0.0378593
I1107 13:39:08.740267 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:39:08.740267 11160 solver.cpp:237]     Train net output #1: loss = 0.0378594 (* 1 = 0.0378594 loss)
I1107 13:39:08.740267 11160 sgd_solver.cpp:105] Iteration 134400, lr = 0.001
I1107 13:39:16.973424  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:39:17.310442 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_134500.caffemodel
I1107 13:39:17.342443 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_134500.solverstate
I1107 13:39:17.351444 11160 solver.cpp:330] Iteration 134500, Testing net (#0)
I1107 13:39:17.351444 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:39:19.368386 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:39:19.448894 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9132
I1107 13:39:19.448894 11160 solver.cpp:397]     Test net output #1: loss = 0.315296 (* 1 = 0.315296 loss)
I1107 13:39:19.531920 11160 solver.cpp:218] Iteration 134500 (9.26733 iter/s, 10.7906s/100 iters), loss = 0.0263229
I1107 13:39:19.531920 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:39:19.531920 11160 solver.cpp:237]     Train net output #1: loss = 0.026323 (* 1 = 0.026323 loss)
I1107 13:39:19.531920 11160 sgd_solver.cpp:105] Iteration 134500, lr = 0.001
I1107 13:39:28.275609 11160 solver.cpp:218] Iteration 134600 (11.4382 iter/s, 8.74265s/100 iters), loss = 0.0332919
I1107 13:39:28.275609 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:39:28.275609 11160 solver.cpp:237]     Train net output #1: loss = 0.0332919 (* 1 = 0.0332919 loss)
I1107 13:39:28.275609 11160 sgd_solver.cpp:105] Iteration 134600, lr = 0.001
I1107 13:39:36.950136 11160 solver.cpp:218] Iteration 134700 (11.5285 iter/s, 8.67417s/100 iters), loss = 0.0319634
I1107 13:39:36.950136 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:39:36.950136 11160 solver.cpp:237]     Train net output #1: loss = 0.0319635 (* 1 = 0.0319635 loss)
I1107 13:39:36.950136 11160 sgd_solver.cpp:105] Iteration 134700, lr = 0.001
I1107 13:39:45.613188 11160 solver.cpp:218] Iteration 134800 (11.5436 iter/s, 8.66283s/100 iters), loss = 0.0300862
I1107 13:39:45.613188 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:39:45.613188 11160 solver.cpp:237]     Train net output #1: loss = 0.0300862 (* 1 = 0.0300862 loss)
I1107 13:39:45.613188 11160 sgd_solver.cpp:105] Iteration 134800, lr = 0.001
I1107 13:39:54.362684 11160 solver.cpp:218] Iteration 134900 (11.4303 iter/s, 8.74871s/100 iters), loss = 0.0233108
I1107 13:39:54.362684 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:39:54.362684 11160 solver.cpp:237]     Train net output #1: loss = 0.0233109 (* 1 = 0.0233109 loss)
I1107 13:39:54.362684 11160 sgd_solver.cpp:105] Iteration 134900, lr = 0.001
I1107 13:40:02.665990  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:40:03.015041 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_135000.caffemodel
I1107 13:40:03.047039 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_135000.solverstate
I1107 13:40:03.058040 11160 solver.cpp:330] Iteration 135000, Testing net (#0)
I1107 13:40:03.058040 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:40:05.093776 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:40:05.175282 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9167
I1107 13:40:05.175282 11160 solver.cpp:397]     Test net output #1: loss = 0.311618 (* 1 = 0.311618 loss)
I1107 13:40:05.257289 11160 solver.cpp:218] Iteration 135000 (9.17917 iter/s, 10.8942s/100 iters), loss = 0.0460553
I1107 13:40:05.257289 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:40:05.257289 11160 solver.cpp:237]     Train net output #1: loss = 0.0460554 (* 1 = 0.0460554 loss)
I1107 13:40:05.257289 11160 sgd_solver.cpp:105] Iteration 135000, lr = 0.001
I1107 13:40:13.968749 11160 solver.cpp:218] Iteration 135100 (11.4797 iter/s, 8.71102s/100 iters), loss = 0.0287938
I1107 13:40:13.968749 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:40:13.968749 11160 solver.cpp:237]     Train net output #1: loss = 0.0287938 (* 1 = 0.0287938 loss)
I1107 13:40:13.968749 11160 sgd_solver.cpp:105] Iteration 135100, lr = 0.001
I1107 13:40:22.558434 11160 solver.cpp:218] Iteration 135200 (11.6424 iter/s, 8.58929s/100 iters), loss = 0.0263572
I1107 13:40:22.558434 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:40:22.558434 11160 solver.cpp:237]     Train net output #1: loss = 0.0263573 (* 1 = 0.0263573 loss)
I1107 13:40:22.558434 11160 sgd_solver.cpp:105] Iteration 135200, lr = 0.001
I1107 13:40:31.147687 11160 solver.cpp:218] Iteration 135300 (11.6435 iter/s, 8.58849s/100 iters), loss = 0.0211208
I1107 13:40:31.147687 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:40:31.147687 11160 solver.cpp:237]     Train net output #1: loss = 0.0211208 (* 1 = 0.0211208 loss)
I1107 13:40:31.147687 11160 sgd_solver.cpp:105] Iteration 135300, lr = 0.001
I1107 13:40:39.806726 11160 solver.cpp:218] Iteration 135400 (11.5489 iter/s, 8.65882s/100 iters), loss = 0.0324015
I1107 13:40:39.806726 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:40:39.806726 11160 solver.cpp:237]     Train net output #1: loss = 0.0324015 (* 1 = 0.0324015 loss)
I1107 13:40:39.806726 11160 sgd_solver.cpp:105] Iteration 135400, lr = 0.001
I1107 13:40:48.110255  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:40:48.450793 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_135500.caffemodel
I1107 13:40:48.481297 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_135500.solverstate
I1107 13:40:48.490309 11160 solver.cpp:330] Iteration 135500, Testing net (#0)
I1107 13:40:48.490309 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:40:50.522469 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:40:50.604477 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9166
I1107 13:40:50.604477 11160 solver.cpp:397]     Test net output #1: loss = 0.311962 (* 1 = 0.311962 loss)
I1107 13:40:50.687494 11160 solver.cpp:218] Iteration 135500 (9.19092 iter/s, 10.8803s/100 iters), loss = 0.0436811
I1107 13:40:50.688494 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:40:50.688494 11160 solver.cpp:237]     Train net output #1: loss = 0.0436811 (* 1 = 0.0436811 loss)
I1107 13:40:50.688494 11160 sgd_solver.cpp:105] Iteration 135500, lr = 0.001
I1107 13:40:59.281481 11160 solver.cpp:218] Iteration 135600 (11.6375 iter/s, 8.59293s/100 iters), loss = 0.0645274
I1107 13:40:59.281481 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:40:59.281481 11160 solver.cpp:237]     Train net output #1: loss = 0.0645275 (* 1 = 0.0645275 loss)
I1107 13:40:59.281481 11160 sgd_solver.cpp:105] Iteration 135600, lr = 0.001
I1107 13:41:07.921222 11160 solver.cpp:218] Iteration 135700 (11.5759 iter/s, 8.63861s/100 iters), loss = 0.0297277
I1107 13:41:07.921222 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:41:07.921222 11160 solver.cpp:237]     Train net output #1: loss = 0.0297277 (* 1 = 0.0297277 loss)
I1107 13:41:07.921222 11160 sgd_solver.cpp:105] Iteration 135700, lr = 0.001
I1107 13:41:16.635874 11160 solver.cpp:218] Iteration 135800 (11.4755 iter/s, 8.7142s/100 iters), loss = 0.0209115
I1107 13:41:16.635874 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:41:16.635874 11160 solver.cpp:237]     Train net output #1: loss = 0.0209115 (* 1 = 0.0209115 loss)
I1107 13:41:16.635874 11160 sgd_solver.cpp:105] Iteration 135800, lr = 0.001
I1107 13:41:25.450381 11160 solver.cpp:218] Iteration 135900 (11.3451 iter/s, 8.81438s/100 iters), loss = 0.0275946
I1107 13:41:25.450381 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:41:25.450381 11160 solver.cpp:237]     Train net output #1: loss = 0.0275946 (* 1 = 0.0275946 loss)
I1107 13:41:25.450381 11160 sgd_solver.cpp:105] Iteration 135900, lr = 0.001
I1107 13:41:33.605887  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:41:33.943434 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_136000.caffemodel
I1107 13:41:33.973943 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_136000.solverstate
I1107 13:41:33.982952 11160 solver.cpp:330] Iteration 136000, Testing net (#0)
I1107 13:41:33.982952 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:41:35.975445 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:41:36.055450 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9172
I1107 13:41:36.055450 11160 solver.cpp:397]     Test net output #1: loss = 0.314664 (* 1 = 0.314664 loss)
I1107 13:41:36.136950 11160 solver.cpp:218] Iteration 136000 (9.35877 iter/s, 10.6852s/100 iters), loss = 0.0345336
I1107 13:41:36.136950 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:41:36.136950 11160 solver.cpp:237]     Train net output #1: loss = 0.0345336 (* 1 = 0.0345336 loss)
I1107 13:41:36.136950 11160 sgd_solver.cpp:105] Iteration 136000, lr = 0.001
I1107 13:41:44.674314 11160 solver.cpp:218] Iteration 136100 (11.713 iter/s, 8.53749s/100 iters), loss = 0.0369951
I1107 13:41:44.674314 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:41:44.674314 11160 solver.cpp:237]     Train net output #1: loss = 0.0369952 (* 1 = 0.0369952 loss)
I1107 13:41:44.674314 11160 sgd_solver.cpp:105] Iteration 136100, lr = 0.001
I1107 13:41:53.259093 11160 solver.cpp:218] Iteration 136200 (11.6493 iter/s, 8.58421s/100 iters), loss = 0.0204458
I1107 13:41:53.259093 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:41:53.259093 11160 solver.cpp:237]     Train net output #1: loss = 0.0204458 (* 1 = 0.0204458 loss)
I1107 13:41:53.259093 11160 sgd_solver.cpp:105] Iteration 136200, lr = 0.001
I1107 13:42:01.858340 11160 solver.cpp:218] Iteration 136300 (11.6296 iter/s, 8.59876s/100 iters), loss = 0.0216279
I1107 13:42:01.858340 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:42:01.858340 11160 solver.cpp:237]     Train net output #1: loss = 0.021628 (* 1 = 0.021628 loss)
I1107 13:42:01.858340 11160 sgd_solver.cpp:105] Iteration 136300, lr = 0.001
I1107 13:42:10.625958 11160 solver.cpp:218] Iteration 136400 (11.4072 iter/s, 8.76639s/100 iters), loss = 0.0303534
I1107 13:42:10.625958 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:42:10.625958 11160 solver.cpp:237]     Train net output #1: loss = 0.0303535 (* 1 = 0.0303535 loss)
I1107 13:42:10.625958 11160 sgd_solver.cpp:105] Iteration 136400, lr = 0.001
I1107 13:42:18.814761  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:42:19.151782 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_136500.caffemodel
I1107 13:42:19.182787 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_136500.solverstate
I1107 13:42:19.191787 11160 solver.cpp:330] Iteration 136500, Testing net (#0)
I1107 13:42:19.191787 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:42:21.181428 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:42:21.260929 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 13:42:21.260929 11160 solver.cpp:397]     Test net output #1: loss = 0.31663 (* 1 = 0.31663 loss)
I1107 13:42:21.341931 11160 solver.cpp:218] Iteration 136500 (9.33171 iter/s, 10.7161s/100 iters), loss = 0.0212102
I1107 13:42:21.341931 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:42:21.341931 11160 solver.cpp:237]     Train net output #1: loss = 0.0212103 (* 1 = 0.0212103 loss)
I1107 13:42:21.341931 11160 sgd_solver.cpp:105] Iteration 136500, lr = 0.001
I1107 13:42:29.997965 11160 solver.cpp:218] Iteration 136600 (11.5536 iter/s, 8.65531s/100 iters), loss = 0.0277698
I1107 13:42:29.997965 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:42:29.997965 11160 solver.cpp:237]     Train net output #1: loss = 0.0277699 (* 1 = 0.0277699 loss)
I1107 13:42:29.997965 11160 sgd_solver.cpp:105] Iteration 136600, lr = 0.001
I1107 13:42:38.632166 11160 solver.cpp:218] Iteration 136700 (11.5824 iter/s, 8.63381s/100 iters), loss = 0.0246797
I1107 13:42:38.632166 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:42:38.632166 11160 solver.cpp:237]     Train net output #1: loss = 0.0246798 (* 1 = 0.0246798 loss)
I1107 13:42:38.632166 11160 sgd_solver.cpp:105] Iteration 136700, lr = 0.001
I1107 13:42:47.195004 11160 solver.cpp:218] Iteration 136800 (11.6802 iter/s, 8.56153s/100 iters), loss = 0.031152
I1107 13:42:47.195004 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:42:47.195004 11160 solver.cpp:237]     Train net output #1: loss = 0.0311521 (* 1 = 0.0311521 loss)
I1107 13:42:47.195004 11160 sgd_solver.cpp:105] Iteration 136800, lr = 0.001
I1107 13:42:55.781147 11160 solver.cpp:218] Iteration 136900 (11.6473 iter/s, 8.58566s/100 iters), loss = 0.038363
I1107 13:42:55.781147 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:42:55.781147 11160 solver.cpp:237]     Train net output #1: loss = 0.0383631 (* 1 = 0.0383631 loss)
I1107 13:42:55.781147 11160 sgd_solver.cpp:105] Iteration 136900, lr = 0.001
I1107 13:43:03.926625  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:43:04.264639 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_137000.caffemodel
I1107 13:43:04.294643 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_137000.solverstate
I1107 13:43:04.303647 11160 solver.cpp:330] Iteration 137000, Testing net (#0)
I1107 13:43:04.303647 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:43:06.301831 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:43:06.380825 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 13:43:06.380825 11160 solver.cpp:397]     Test net output #1: loss = 0.312071 (* 1 = 0.312071 loss)
I1107 13:43:06.462831 11160 solver.cpp:218] Iteration 137000 (9.36207 iter/s, 10.6814s/100 iters), loss = 0.039874
I1107 13:43:06.462831 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:43:06.462831 11160 solver.cpp:237]     Train net output #1: loss = 0.0398741 (* 1 = 0.0398741 loss)
I1107 13:43:06.462831 11160 sgd_solver.cpp:105] Iteration 137000, lr = 0.001
I1107 13:43:15.029808 11160 solver.cpp:218] Iteration 137100 (11.6729 iter/s, 8.56682s/100 iters), loss = 0.027373
I1107 13:43:15.029808 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:43:15.029808 11160 solver.cpp:237]     Train net output #1: loss = 0.0273731 (* 1 = 0.0273731 loss)
I1107 13:43:15.029808 11160 sgd_solver.cpp:105] Iteration 137100, lr = 0.001
I1107 13:43:23.614609 11160 solver.cpp:218] Iteration 137200 (11.6498 iter/s, 8.58386s/100 iters), loss = 0.0218688
I1107 13:43:23.614609 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:43:23.614609 11160 solver.cpp:237]     Train net output #1: loss = 0.0218689 (* 1 = 0.0218689 loss)
I1107 13:43:23.614609 11160 sgd_solver.cpp:105] Iteration 137200, lr = 0.001
I1107 13:43:32.172137 11160 solver.cpp:218] Iteration 137300 (11.6863 iter/s, 8.55705s/100 iters), loss = 0.0303502
I1107 13:43:32.172137 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:43:32.172137 11160 solver.cpp:237]     Train net output #1: loss = 0.0303503 (* 1 = 0.0303503 loss)
I1107 13:43:32.172137 11160 sgd_solver.cpp:105] Iteration 137300, lr = 0.001
I1107 13:43:40.729815 11160 solver.cpp:218] Iteration 137400 (11.6861 iter/s, 8.55721s/100 iters), loss = 0.0263289
I1107 13:43:40.729815 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:43:40.729815 11160 solver.cpp:237]     Train net output #1: loss = 0.026329 (* 1 = 0.026329 loss)
I1107 13:43:40.729815 11160 sgd_solver.cpp:105] Iteration 137400, lr = 0.001
I1107 13:43:48.892540  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:43:49.229593 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_137500.caffemodel
I1107 13:43:49.260592 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_137500.solverstate
I1107 13:43:49.268592 11160 solver.cpp:330] Iteration 137500, Testing net (#0)
I1107 13:43:49.268592 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:43:51.273916 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:43:51.354919 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 13:43:51.354919 11160 solver.cpp:397]     Test net output #1: loss = 0.311444 (* 1 = 0.311444 loss)
I1107 13:43:51.436926 11160 solver.cpp:218] Iteration 137500 (9.33972 iter/s, 10.707s/100 iters), loss = 0.0335379
I1107 13:43:51.436926 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:43:51.437927 11160 solver.cpp:237]     Train net output #1: loss = 0.033538 (* 1 = 0.033538 loss)
I1107 13:43:51.437927 11160 sgd_solver.cpp:105] Iteration 137500, lr = 0.001
I1107 13:44:00.031818 11160 solver.cpp:218] Iteration 137600 (11.6364 iter/s, 8.5937s/100 iters), loss = 0.0243459
I1107 13:44:00.031818 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:44:00.031818 11160 solver.cpp:237]     Train net output #1: loss = 0.024346 (* 1 = 0.024346 loss)
I1107 13:44:00.031818 11160 sgd_solver.cpp:105] Iteration 137600, lr = 0.001
I1107 13:44:08.646055 11160 solver.cpp:218] Iteration 137700 (11.6089 iter/s, 8.61407s/100 iters), loss = 0.0230873
I1107 13:44:08.646055 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:44:08.646055 11160 solver.cpp:237]     Train net output #1: loss = 0.0230874 (* 1 = 0.0230874 loss)
I1107 13:44:08.646055 11160 sgd_solver.cpp:105] Iteration 137700, lr = 0.001
I1107 13:44:17.278113 11160 solver.cpp:218] Iteration 137800 (11.5861 iter/s, 8.63103s/100 iters), loss = 0.0344718
I1107 13:44:17.278113 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:44:17.278113 11160 solver.cpp:237]     Train net output #1: loss = 0.0344719 (* 1 = 0.0344719 loss)
I1107 13:44:17.278113 11160 sgd_solver.cpp:105] Iteration 137800, lr = 0.001
I1107 13:44:25.861034 11160 solver.cpp:218] Iteration 137900 (11.6516 iter/s, 8.58249s/100 iters), loss = 0.036302
I1107 13:44:25.861034 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:44:25.861034 11160 solver.cpp:237]     Train net output #1: loss = 0.036302 (* 1 = 0.036302 loss)
I1107 13:44:25.861034 11160 sgd_solver.cpp:105] Iteration 137900, lr = 0.001
I1107 13:44:34.059831  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:44:34.398849 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_138000.caffemodel
I1107 13:44:34.427860 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_138000.solverstate
I1107 13:44:34.436859 11160 solver.cpp:330] Iteration 138000, Testing net (#0)
I1107 13:44:34.436859 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:44:36.431020 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:44:36.511019 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1107 13:44:36.511019 11160 solver.cpp:397]     Test net output #1: loss = 0.309799 (* 1 = 0.309799 loss)
I1107 13:44:36.592021 11160 solver.cpp:218] Iteration 138000 (9.31884 iter/s, 10.731s/100 iters), loss = 0.0362522
I1107 13:44:36.592021 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:44:36.592021 11160 solver.cpp:237]     Train net output #1: loss = 0.0362522 (* 1 = 0.0362522 loss)
I1107 13:44:36.592021 11160 sgd_solver.cpp:105] Iteration 138000, lr = 0.001
I1107 13:44:45.165261 11160 solver.cpp:218] Iteration 138100 (11.6652 iter/s, 8.57254s/100 iters), loss = 0.0192567
I1107 13:44:45.165261 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:44:45.165261 11160 solver.cpp:237]     Train net output #1: loss = 0.0192568 (* 1 = 0.0192568 loss)
I1107 13:44:45.165261 11160 sgd_solver.cpp:105] Iteration 138100, lr = 0.001
I1107 13:44:53.806622 11160 solver.cpp:218] Iteration 138200 (11.5736 iter/s, 8.64032s/100 iters), loss = 0.0220662
I1107 13:44:53.806622 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:44:53.806622 11160 solver.cpp:237]     Train net output #1: loss = 0.0220662 (* 1 = 0.0220662 loss)
I1107 13:44:53.806622 11160 sgd_solver.cpp:105] Iteration 138200, lr = 0.001
I1107 13:45:02.496127 11160 solver.cpp:218] Iteration 138300 (11.5081 iter/s, 8.68955s/100 iters), loss = 0.0230152
I1107 13:45:02.496127 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:45:02.496127 11160 solver.cpp:237]     Train net output #1: loss = 0.0230152 (* 1 = 0.0230152 loss)
I1107 13:45:02.496127 11160 sgd_solver.cpp:105] Iteration 138300, lr = 0.001
I1107 13:45:11.133842 11160 solver.cpp:218] Iteration 138400 (11.5784 iter/s, 8.63681s/100 iters), loss = 0.0264324
I1107 13:45:11.133842 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:45:11.133842 11160 solver.cpp:237]     Train net output #1: loss = 0.0264324 (* 1 = 0.0264324 loss)
I1107 13:45:11.133842 11160 sgd_solver.cpp:105] Iteration 138400, lr = 0.001
I1107 13:45:19.260681  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:45:19.597229 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_138500.caffemodel
I1107 13:45:19.626233 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_138500.solverstate
I1107 13:45:19.635231 11160 solver.cpp:330] Iteration 138500, Testing net (#0)
I1107 13:45:19.635231 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:45:21.639021 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:45:21.722044 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9173
I1107 13:45:21.722044 11160 solver.cpp:397]     Test net output #1: loss = 0.316377 (* 1 = 0.316377 loss)
I1107 13:45:21.803546 11160 solver.cpp:218] Iteration 138500 (9.37232 iter/s, 10.6697s/100 iters), loss = 0.0388691
I1107 13:45:21.803546 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:45:21.803546 11160 solver.cpp:237]     Train net output #1: loss = 0.0388692 (* 1 = 0.0388692 loss)
I1107 13:45:21.803546 11160 sgd_solver.cpp:105] Iteration 138500, lr = 0.001
I1107 13:45:30.389729 11160 solver.cpp:218] Iteration 138600 (11.6484 iter/s, 8.58485s/100 iters), loss = 0.0320742
I1107 13:45:30.389729 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:45:30.389729 11160 solver.cpp:237]     Train net output #1: loss = 0.0320743 (* 1 = 0.0320743 loss)
I1107 13:45:30.389729 11160 sgd_solver.cpp:105] Iteration 138600, lr = 0.001
I1107 13:45:38.999095 11160 solver.cpp:218] Iteration 138700 (11.6149 iter/s, 8.60961s/100 iters), loss = 0.0349038
I1107 13:45:38.999095 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:45:38.999095 11160 solver.cpp:237]     Train net output #1: loss = 0.0349039 (* 1 = 0.0349039 loss)
I1107 13:45:38.999095 11160 sgd_solver.cpp:105] Iteration 138700, lr = 0.001
I1107 13:45:47.637303 11160 solver.cpp:218] Iteration 138800 (11.5784 iter/s, 8.6368s/100 iters), loss = 0.0353773
I1107 13:45:47.637303 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:45:47.637303 11160 solver.cpp:237]     Train net output #1: loss = 0.0353773 (* 1 = 0.0353773 loss)
I1107 13:45:47.637303 11160 sgd_solver.cpp:105] Iteration 138800, lr = 0.001
I1107 13:45:56.196539 11160 solver.cpp:218] Iteration 138900 (11.6831 iter/s, 8.55936s/100 iters), loss = 0.0277648
I1107 13:45:56.196539 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:45:56.196539 11160 solver.cpp:237]     Train net output #1: loss = 0.0277648 (* 1 = 0.0277648 loss)
I1107 13:45:56.196539 11160 sgd_solver.cpp:105] Iteration 138900, lr = 0.001
I1107 13:46:04.362159  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:46:04.701478 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_139000.caffemodel
I1107 13:46:04.730492 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_139000.solverstate
I1107 13:46:04.739492 11160 solver.cpp:330] Iteration 139000, Testing net (#0)
I1107 13:46:04.739492 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:46:06.740272 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:46:06.820296 11160 solver.cpp:397]     Test net output #0: accuracy = 0.916
I1107 13:46:06.820296 11160 solver.cpp:397]     Test net output #1: loss = 0.321381 (* 1 = 0.321381 loss)
I1107 13:46:06.901335 11160 solver.cpp:218] Iteration 139000 (9.34209 iter/s, 10.7042s/100 iters), loss = 0.0259257
I1107 13:46:06.901335 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:46:06.901335 11160 solver.cpp:237]     Train net output #1: loss = 0.0259258 (* 1 = 0.0259258 loss)
I1107 13:46:06.901335 11160 sgd_solver.cpp:105] Iteration 139000, lr = 0.001
I1107 13:46:15.486264 11160 solver.cpp:218] Iteration 139100 (11.6486 iter/s, 8.58475s/100 iters), loss = 0.0535085
I1107 13:46:15.487264 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:46:15.487264 11160 solver.cpp:237]     Train net output #1: loss = 0.0535086 (* 1 = 0.0535086 loss)
I1107 13:46:15.487264 11160 sgd_solver.cpp:105] Iteration 139100, lr = 0.001
I1107 13:46:24.100419 11160 solver.cpp:218] Iteration 139200 (11.6107 iter/s, 8.61276s/100 iters), loss = 0.0271464
I1107 13:46:24.100419 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:46:24.100419 11160 solver.cpp:237]     Train net output #1: loss = 0.0271464 (* 1 = 0.0271464 loss)
I1107 13:46:24.100419 11160 sgd_solver.cpp:105] Iteration 139200, lr = 0.001
I1107 13:46:32.767990 11160 solver.cpp:218] Iteration 139300 (11.5376 iter/s, 8.66729s/100 iters), loss = 0.0200089
I1107 13:46:32.767990 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:46:32.767990 11160 solver.cpp:237]     Train net output #1: loss = 0.020009 (* 1 = 0.020009 loss)
I1107 13:46:32.767990 11160 sgd_solver.cpp:105] Iteration 139300, lr = 0.001
I1107 13:46:41.448510 11160 solver.cpp:218] Iteration 139400 (11.5206 iter/s, 8.68011s/100 iters), loss = 0.0232417
I1107 13:46:41.448510 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:46:41.448510 11160 solver.cpp:237]     Train net output #1: loss = 0.0232418 (* 1 = 0.0232418 loss)
I1107 13:46:41.448510 11160 sgd_solver.cpp:105] Iteration 139400, lr = 0.001
I1107 13:46:49.672179  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:46:50.019250 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_139500.caffemodel
I1107 13:46:50.055181 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_139500.solverstate
I1107 13:46:50.065179 11160 solver.cpp:330] Iteration 139500, Testing net (#0)
I1107 13:46:50.065179 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:46:52.085963 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:46:52.166461 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9185
I1107 13:46:52.167464 11160 solver.cpp:397]     Test net output #1: loss = 0.315634 (* 1 = 0.315634 loss)
I1107 13:46:52.250476 11160 solver.cpp:218] Iteration 139500 (9.25796 iter/s, 10.8015s/100 iters), loss = 0.0343488
I1107 13:46:52.250476 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:46:52.250476 11160 solver.cpp:237]     Train net output #1: loss = 0.0343489 (* 1 = 0.0343489 loss)
I1107 13:46:52.250476 11160 sgd_solver.cpp:105] Iteration 139500, lr = 0.001
I1107 13:47:00.942534 11160 solver.cpp:218] Iteration 139600 (11.5053 iter/s, 8.69161s/100 iters), loss = 0.0402361
I1107 13:47:00.942534 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:47:00.942534 11160 solver.cpp:237]     Train net output #1: loss = 0.0402361 (* 1 = 0.0402361 loss)
I1107 13:47:00.943536 11160 sgd_solver.cpp:105] Iteration 139600, lr = 0.001
I1107 13:47:09.513501 11160 solver.cpp:218] Iteration 139700 (11.6684 iter/s, 8.57016s/100 iters), loss = 0.025648
I1107 13:47:09.513501 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:47:09.513501 11160 solver.cpp:237]     Train net output #1: loss = 0.0256481 (* 1 = 0.0256481 loss)
I1107 13:47:09.513501 11160 sgd_solver.cpp:105] Iteration 139700, lr = 0.001
I1107 13:47:18.132887 11160 solver.cpp:218] Iteration 139800 (11.6032 iter/s, 8.61834s/100 iters), loss = 0.0187134
I1107 13:47:18.132887 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:47:18.132887 11160 solver.cpp:237]     Train net output #1: loss = 0.0187134 (* 1 = 0.0187134 loss)
I1107 13:47:18.132887 11160 sgd_solver.cpp:105] Iteration 139800, lr = 0.001
I1107 13:47:26.865298 11160 solver.cpp:218] Iteration 139900 (11.4518 iter/s, 8.73225s/100 iters), loss = 0.0275983
I1107 13:47:26.865298 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:47:26.865298 11160 solver.cpp:237]     Train net output #1: loss = 0.0275983 (* 1 = 0.0275983 loss)
I1107 13:47:26.865298 11160 sgd_solver.cpp:105] Iteration 139900, lr = 0.001
I1107 13:47:35.148512  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:47:35.491111 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_140000.caffemodel
I1107 13:47:35.521111 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_140000.solverstate
I1107 13:47:35.530112 11160 solver.cpp:330] Iteration 140000, Testing net (#0)
I1107 13:47:35.531119 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:47:37.588390 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:47:37.670398 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9165
I1107 13:47:37.670398 11160 solver.cpp:397]     Test net output #1: loss = 0.319775 (* 1 = 0.319775 loss)
I1107 13:47:37.754402 11160 solver.cpp:218] Iteration 140000 (9.1839 iter/s, 10.8886s/100 iters), loss = 0.0300464
I1107 13:47:37.754402 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:47:37.754402 11160 solver.cpp:237]     Train net output #1: loss = 0.0300464 (* 1 = 0.0300464 loss)
I1107 13:47:37.754402 11160 sgd_solver.cpp:105] Iteration 140000, lr = 0.001
I1107 13:47:46.460508 11160 solver.cpp:218] Iteration 140100 (11.4868 iter/s, 8.70568s/100 iters), loss = 0.0221843
I1107 13:47:46.460508 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:47:46.460508 11160 solver.cpp:237]     Train net output #1: loss = 0.0221844 (* 1 = 0.0221844 loss)
I1107 13:47:46.460508 11160 sgd_solver.cpp:105] Iteration 140100, lr = 0.001
I1107 13:47:55.357344 11160 solver.cpp:218] Iteration 140200 (11.2411 iter/s, 8.89596s/100 iters), loss = 0.0290747
I1107 13:47:55.357344 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:47:55.357344 11160 solver.cpp:237]     Train net output #1: loss = 0.0290748 (* 1 = 0.0290748 loss)
I1107 13:47:55.357344 11160 sgd_solver.cpp:105] Iteration 140200, lr = 0.001
I1107 13:48:04.190675 11160 solver.cpp:218] Iteration 140300 (11.3216 iter/s, 8.8327s/100 iters), loss = 0.0180736
I1107 13:48:04.190675 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:48:04.190675 11160 solver.cpp:237]     Train net output #1: loss = 0.0180737 (* 1 = 0.0180737 loss)
I1107 13:48:04.190675 11160 sgd_solver.cpp:105] Iteration 140300, lr = 0.001
I1107 13:48:13.104115 11160 solver.cpp:218] Iteration 140400 (11.2199 iter/s, 8.91277s/100 iters), loss = 0.0415947
I1107 13:48:13.104115 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:48:13.104115 11160 solver.cpp:237]     Train net output #1: loss = 0.0415947 (* 1 = 0.0415947 loss)
I1107 13:48:13.104115 11160 sgd_solver.cpp:105] Iteration 140400, lr = 0.001
I1107 13:48:21.356720  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:48:21.694779 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_140500.caffemodel
I1107 13:48:21.725778 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_140500.solverstate
I1107 13:48:21.734779 11160 solver.cpp:330] Iteration 140500, Testing net (#0)
I1107 13:48:21.734779 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:48:23.760946 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:48:23.840965 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9167
I1107 13:48:23.840965 11160 solver.cpp:397]     Test net output #1: loss = 0.314675 (* 1 = 0.314675 loss)
I1107 13:48:23.922910 11160 solver.cpp:218] Iteration 140500 (9.24395 iter/s, 10.8179s/100 iters), loss = 0.0306247
I1107 13:48:23.922910 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:48:23.922910 11160 solver.cpp:237]     Train net output #1: loss = 0.0306247 (* 1 = 0.0306247 loss)
I1107 13:48:23.922910 11160 sgd_solver.cpp:105] Iteration 140500, lr = 0.001
I1107 13:48:32.687474 11160 solver.cpp:218] Iteration 140600 (11.4103 iter/s, 8.76402s/100 iters), loss = 0.0279613
I1107 13:48:32.687474 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:48:32.687474 11160 solver.cpp:237]     Train net output #1: loss = 0.0279613 (* 1 = 0.0279613 loss)
I1107 13:48:32.687474 11160 sgd_solver.cpp:105] Iteration 140600, lr = 0.001
I1107 13:48:41.318739 11160 solver.cpp:218] Iteration 140700 (11.5862 iter/s, 8.63093s/100 iters), loss = 0.0368384
I1107 13:48:41.318739 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:48:41.318739 11160 solver.cpp:237]     Train net output #1: loss = 0.0368385 (* 1 = 0.0368385 loss)
I1107 13:48:41.318739 11160 sgd_solver.cpp:105] Iteration 140700, lr = 0.001
I1107 13:48:50.059322 11160 solver.cpp:218] Iteration 140800 (11.4421 iter/s, 8.73964s/100 iters), loss = 0.0204057
I1107 13:48:50.059322 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:48:50.059322 11160 solver.cpp:237]     Train net output #1: loss = 0.0204058 (* 1 = 0.0204058 loss)
I1107 13:48:50.059322 11160 sgd_solver.cpp:105] Iteration 140800, lr = 0.001
I1107 13:48:58.941120 11160 solver.cpp:218] Iteration 140900 (11.2593 iter/s, 8.88156s/100 iters), loss = 0.0241147
I1107 13:48:58.941120 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:48:58.941120 11160 solver.cpp:237]     Train net output #1: loss = 0.0241147 (* 1 = 0.0241147 loss)
I1107 13:48:58.941120 11160 sgd_solver.cpp:105] Iteration 140900, lr = 0.001
I1107 13:49:07.390410  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:49:07.749415 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_141000.caffemodel
I1107 13:49:07.787418 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_141000.solverstate
I1107 13:49:07.796918 11160 solver.cpp:330] Iteration 141000, Testing net (#0)
I1107 13:49:07.796918 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:49:09.825553 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:49:09.907565 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 13:49:09.907565 11160 solver.cpp:397]     Test net output #1: loss = 0.315641 (* 1 = 0.315641 loss)
I1107 13:49:09.993065 11160 solver.cpp:218] Iteration 141000 (9.04888 iter/s, 11.0511s/100 iters), loss = 0.0276138
I1107 13:49:09.993065 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:49:09.993065 11160 solver.cpp:237]     Train net output #1: loss = 0.0276138 (* 1 = 0.0276138 loss)
I1107 13:49:09.993065 11160 sgd_solver.cpp:105] Iteration 141000, lr = 0.001
I1107 13:49:18.554515 11160 solver.cpp:218] Iteration 141100 (11.6803 iter/s, 8.56141s/100 iters), loss = 0.0265169
I1107 13:49:18.554515 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:49:18.554515 11160 solver.cpp:237]     Train net output #1: loss = 0.0265169 (* 1 = 0.0265169 loss)
I1107 13:49:18.554515 11160 sgd_solver.cpp:105] Iteration 141100, lr = 0.001
I1107 13:49:27.405318 11160 solver.cpp:218] Iteration 141200 (11.2995 iter/s, 8.84995s/100 iters), loss = 0.0291607
I1107 13:49:27.405818 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:49:27.405818 11160 solver.cpp:237]     Train net output #1: loss = 0.0291608 (* 1 = 0.0291608 loss)
I1107 13:49:27.405818 11160 sgd_solver.cpp:105] Iteration 141200, lr = 0.001
I1107 13:49:36.200361 11160 solver.cpp:218] Iteration 141300 (11.371 iter/s, 8.79427s/100 iters), loss = 0.0208682
I1107 13:49:36.200361 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:49:36.200361 11160 solver.cpp:237]     Train net output #1: loss = 0.0208682 (* 1 = 0.0208682 loss)
I1107 13:49:36.200361 11160 sgd_solver.cpp:105] Iteration 141300, lr = 0.001
I1107 13:49:45.049093 11160 solver.cpp:218] Iteration 141400 (11.3014 iter/s, 8.84848s/100 iters), loss = 0.0316705
I1107 13:49:45.049093 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:49:45.049093 11160 solver.cpp:237]     Train net output #1: loss = 0.0316705 (* 1 = 0.0316705 loss)
I1107 13:49:45.049093 11160 sgd_solver.cpp:105] Iteration 141400, lr = 0.001
I1107 13:49:53.217252  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:49:53.555392 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_141500.caffemodel
I1107 13:49:53.583401 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_141500.solverstate
I1107 13:49:53.592402 11160 solver.cpp:330] Iteration 141500, Testing net (#0)
I1107 13:49:53.592402 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:49:55.590522 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:49:55.670032 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9174
I1107 13:49:55.670032 11160 solver.cpp:397]     Test net output #1: loss = 0.317932 (* 1 = 0.317932 loss)
I1107 13:49:55.752069 11160 solver.cpp:218] Iteration 141500 (9.34387 iter/s, 10.7022s/100 iters), loss = 0.0259624
I1107 13:49:55.752069 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:49:55.752069 11160 solver.cpp:237]     Train net output #1: loss = 0.0259624 (* 1 = 0.0259624 loss)
I1107 13:49:55.752069 11160 sgd_solver.cpp:105] Iteration 141500, lr = 0.001
I1107 13:50:04.480814 11160 solver.cpp:218] Iteration 141600 (11.4575 iter/s, 8.72787s/100 iters), loss = 0.0206174
I1107 13:50:04.480814 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:50:04.480814 11160 solver.cpp:237]     Train net output #1: loss = 0.0206174 (* 1 = 0.0206174 loss)
I1107 13:50:04.480814 11160 sgd_solver.cpp:105] Iteration 141600, lr = 0.001
I1107 13:50:13.214027 11160 solver.cpp:218] Iteration 141700 (11.4509 iter/s, 8.73292s/100 iters), loss = 0.0223935
I1107 13:50:13.214027 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:50:13.214027 11160 solver.cpp:237]     Train net output #1: loss = 0.0223935 (* 1 = 0.0223935 loss)
I1107 13:50:13.214027 11160 sgd_solver.cpp:105] Iteration 141700, lr = 0.001
I1107 13:50:21.785883 11160 solver.cpp:218] Iteration 141800 (11.6662 iter/s, 8.57175s/100 iters), loss = 0.0390948
I1107 13:50:21.785883 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:50:21.785883 11160 solver.cpp:237]     Train net output #1: loss = 0.0390949 (* 1 = 0.0390949 loss)
I1107 13:50:21.785883 11160 sgd_solver.cpp:105] Iteration 141800, lr = 0.001
I1107 13:50:30.361932 11160 solver.cpp:218] Iteration 141900 (11.6618 iter/s, 8.57503s/100 iters), loss = 0.0369441
I1107 13:50:30.361932 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:50:30.361932 11160 solver.cpp:237]     Train net output #1: loss = 0.0369441 (* 1 = 0.0369441 loss)
I1107 13:50:30.361932 11160 sgd_solver.cpp:105] Iteration 141900, lr = 0.001
I1107 13:50:38.562733  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:50:38.913770 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_142000.caffemodel
I1107 13:50:38.943784 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_142000.solverstate
I1107 13:50:38.953786 11160 solver.cpp:330] Iteration 142000, Testing net (#0)
I1107 13:50:38.953786 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:50:41.022569 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:50:41.104089 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 13:50:41.104089 11160 solver.cpp:397]     Test net output #1: loss = 0.317807 (* 1 = 0.317807 loss)
I1107 13:50:41.188099 11160 solver.cpp:218] Iteration 142000 (9.23698 iter/s, 10.826s/100 iters), loss = 0.0322587
I1107 13:50:41.188099 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:50:41.188099 11160 solver.cpp:237]     Train net output #1: loss = 0.0322588 (* 1 = 0.0322588 loss)
I1107 13:50:41.188099 11160 sgd_solver.cpp:105] Iteration 142000, lr = 0.001
I1107 13:50:50.069471 11160 solver.cpp:218] Iteration 142100 (11.2599 iter/s, 8.8811s/100 iters), loss = 0.0277777
I1107 13:50:50.070472 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:50:50.070472 11160 solver.cpp:237]     Train net output #1: loss = 0.0277777 (* 1 = 0.0277777 loss)
I1107 13:50:50.070472 11160 sgd_solver.cpp:105] Iteration 142100, lr = 0.001
I1107 13:50:58.841172 11160 solver.cpp:218] Iteration 142200 (11.4019 iter/s, 8.7705s/100 iters), loss = 0.0270237
I1107 13:50:58.841172 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:50:58.841172 11160 solver.cpp:237]     Train net output #1: loss = 0.0270237 (* 1 = 0.0270237 loss)
I1107 13:50:58.841172 11160 sgd_solver.cpp:105] Iteration 142200, lr = 0.001
I1107 13:51:07.714480 11160 solver.cpp:218] Iteration 142300 (11.2706 iter/s, 8.87262s/100 iters), loss = 0.0239515
I1107 13:51:07.714480 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:51:07.714480 11160 solver.cpp:237]     Train net output #1: loss = 0.0239516 (* 1 = 0.0239516 loss)
I1107 13:51:07.714480 11160 sgd_solver.cpp:105] Iteration 142300, lr = 0.001
I1107 13:51:16.303158 11160 solver.cpp:218] Iteration 142400 (11.6433 iter/s, 8.58863s/100 iters), loss = 0.0273224
I1107 13:51:16.303158 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:51:16.303158 11160 solver.cpp:237]     Train net output #1: loss = 0.0273224 (* 1 = 0.0273224 loss)
I1107 13:51:16.303158 11160 sgd_solver.cpp:105] Iteration 142400, lr = 0.001
I1107 13:51:24.504923  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:51:24.850252 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_142500.caffemodel
I1107 13:51:24.879835 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_142500.solverstate
I1107 13:51:24.887830 11160 solver.cpp:330] Iteration 142500, Testing net (#0)
I1107 13:51:24.888834 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:51:26.916363 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:51:26.997387 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 13:51:26.997387 11160 solver.cpp:397]     Test net output #1: loss = 0.319941 (* 1 = 0.319941 loss)
I1107 13:51:27.079401 11160 solver.cpp:218] Iteration 142500 (9.28006 iter/s, 10.7758s/100 iters), loss = 0.0639836
I1107 13:51:27.079401 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:51:27.079401 11160 solver.cpp:237]     Train net output #1: loss = 0.0639836 (* 1 = 0.0639836 loss)
I1107 13:51:27.079401 11160 sgd_solver.cpp:105] Iteration 142500, lr = 0.001
I1107 13:51:35.695160 11160 solver.cpp:218] Iteration 142600 (11.6086 iter/s, 8.61429s/100 iters), loss = 0.0418686
I1107 13:51:35.695160 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:51:35.695160 11160 solver.cpp:237]     Train net output #1: loss = 0.0418687 (* 1 = 0.0418687 loss)
I1107 13:51:35.695160 11160 sgd_solver.cpp:105] Iteration 142600, lr = 0.001
I1107 13:51:44.313874 11160 solver.cpp:218] Iteration 142700 (11.6027 iter/s, 8.61866s/100 iters), loss = 0.0211873
I1107 13:51:44.313874 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:51:44.313874 11160 solver.cpp:237]     Train net output #1: loss = 0.0211874 (* 1 = 0.0211874 loss)
I1107 13:51:44.313874 11160 sgd_solver.cpp:105] Iteration 142700, lr = 0.001
I1107 13:51:52.929955 11160 solver.cpp:218] Iteration 142800 (11.6082 iter/s, 8.61464s/100 iters), loss = 0.0240023
I1107 13:51:52.929955 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:51:52.929955 11160 solver.cpp:237]     Train net output #1: loss = 0.0240024 (* 1 = 0.0240024 loss)
I1107 13:51:52.929955 11160 sgd_solver.cpp:105] Iteration 142800, lr = 0.001
I1107 13:52:01.623950 11160 solver.cpp:218] Iteration 142900 (11.5024 iter/s, 8.69383s/100 iters), loss = 0.0341132
I1107 13:52:01.623950 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:52:01.623950 11160 solver.cpp:237]     Train net output #1: loss = 0.0341132 (* 1 = 0.0341132 loss)
I1107 13:52:01.623950 11160 sgd_solver.cpp:105] Iteration 142900, lr = 0.001
I1107 13:52:09.876078  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:52:10.215131 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_143000.caffemodel
I1107 13:52:10.245131 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_143000.solverstate
I1107 13:52:10.254132 11160 solver.cpp:330] Iteration 143000, Testing net (#0)
I1107 13:52:10.254132 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:52:12.248270 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:52:12.328277 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9163
I1107 13:52:12.328277 11160 solver.cpp:397]     Test net output #1: loss = 0.320133 (* 1 = 0.320133 loss)
I1107 13:52:12.410281 11160 solver.cpp:218] Iteration 143000 (9.27168 iter/s, 10.7855s/100 iters), loss = 0.0279653
I1107 13:52:12.410281 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:52:12.410281 11160 solver.cpp:237]     Train net output #1: loss = 0.0279654 (* 1 = 0.0279654 loss)
I1107 13:52:12.410281 11160 sgd_solver.cpp:105] Iteration 143000, lr = 0.001
I1107 13:52:20.976233 11160 solver.cpp:218] Iteration 143100 (11.6749 iter/s, 8.56542s/100 iters), loss = 0.0283039
I1107 13:52:20.976233 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:52:20.976233 11160 solver.cpp:237]     Train net output #1: loss = 0.0283039 (* 1 = 0.0283039 loss)
I1107 13:52:20.976233 11160 sgd_solver.cpp:105] Iteration 143100, lr = 0.001
I1107 13:52:29.674860 11160 solver.cpp:218] Iteration 143200 (11.4966 iter/s, 8.69824s/100 iters), loss = 0.0216791
I1107 13:52:29.674860 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:52:29.674860 11160 solver.cpp:237]     Train net output #1: loss = 0.0216791 (* 1 = 0.0216791 loss)
I1107 13:52:29.674860 11160 sgd_solver.cpp:105] Iteration 143200, lr = 0.001
I1107 13:52:38.513953 11160 solver.cpp:218] Iteration 143300 (11.3147 iter/s, 8.83807s/100 iters), loss = 0.0341409
I1107 13:52:38.513953 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:52:38.513953 11160 solver.cpp:237]     Train net output #1: loss = 0.0341409 (* 1 = 0.0341409 loss)
I1107 13:52:38.513953 11160 sgd_solver.cpp:105] Iteration 143300, lr = 0.001
I1107 13:52:47.425228 11160 solver.cpp:218] Iteration 143400 (11.2219 iter/s, 8.91113s/100 iters), loss = 0.0347769
I1107 13:52:47.425228 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:52:47.425228 11160 solver.cpp:237]     Train net output #1: loss = 0.0347768 (* 1 = 0.0347768 loss)
I1107 13:52:47.425228 11160 sgd_solver.cpp:105] Iteration 143400, lr = 0.001
I1107 13:52:55.733160  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:52:56.090487 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_143500.caffemodel
I1107 13:52:56.118063 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_143500.solverstate
I1107 13:52:56.127079 11160 solver.cpp:330] Iteration 143500, Testing net (#0)
I1107 13:52:56.127079 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:52:58.187839 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:52:58.269850 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9146
I1107 13:52:58.269850 11160 solver.cpp:397]     Test net output #1: loss = 0.331686 (* 1 = 0.331686 loss)
I1107 13:52:58.353873 11160 solver.cpp:218] Iteration 143500 (9.15093 iter/s, 10.9279s/100 iters), loss = 0.0381461
I1107 13:52:58.353873 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:52:58.353873 11160 solver.cpp:237]     Train net output #1: loss = 0.0381461 (* 1 = 0.0381461 loss)
I1107 13:52:58.353873 11160 sgd_solver.cpp:105] Iteration 143500, lr = 0.001
I1107 13:53:07.125950 11160 solver.cpp:218] Iteration 143600 (11.3998 iter/s, 8.77211s/100 iters), loss = 0.0323082
I1107 13:53:07.126951 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:53:07.126951 11160 solver.cpp:237]     Train net output #1: loss = 0.0323082 (* 1 = 0.0323082 loss)
I1107 13:53:07.126951 11160 sgd_solver.cpp:105] Iteration 143600, lr = 0.001
I1107 13:53:15.742744 11160 solver.cpp:218] Iteration 143700 (11.6069 iter/s, 8.61555s/100 iters), loss = 0.0339347
I1107 13:53:15.742744 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:53:15.742744 11160 solver.cpp:237]     Train net output #1: loss = 0.0339347 (* 1 = 0.0339347 loss)
I1107 13:53:15.742744 11160 sgd_solver.cpp:105] Iteration 143700, lr = 0.001
I1107 13:53:24.654989 11160 solver.cpp:218] Iteration 143800 (11.2209 iter/s, 8.91194s/100 iters), loss = 0.0179475
I1107 13:53:24.654989 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:53:24.654989 11160 solver.cpp:237]     Train net output #1: loss = 0.0179475 (* 1 = 0.0179475 loss)
I1107 13:53:24.654989 11160 sgd_solver.cpp:105] Iteration 143800, lr = 0.001
I1107 13:53:33.360998 11160 solver.cpp:218] Iteration 143900 (11.4878 iter/s, 8.70489s/100 iters), loss = 0.0302798
I1107 13:53:33.360998 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:53:33.360998 11160 solver.cpp:237]     Train net output #1: loss = 0.0302798 (* 1 = 0.0302798 loss)
I1107 13:53:33.360998 11160 sgd_solver.cpp:105] Iteration 143900, lr = 0.001
I1107 13:53:41.842653  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:53:42.194897 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_144000.caffemodel
I1107 13:53:42.225900 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_144000.solverstate
I1107 13:53:42.234899 11160 solver.cpp:330] Iteration 144000, Testing net (#0)
I1107 13:53:42.235397 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:53:44.301507 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:53:44.384008 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9163
I1107 13:53:44.384008 11160 solver.cpp:397]     Test net output #1: loss = 0.31986 (* 1 = 0.31986 loss)
I1107 13:53:44.468506 11160 solver.cpp:218] Iteration 144000 (9.00343 iter/s, 11.1069s/100 iters), loss = 0.0285303
I1107 13:53:44.468506 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:53:44.468506 11160 solver.cpp:237]     Train net output #1: loss = 0.0285303 (* 1 = 0.0285303 loss)
I1107 13:53:44.468506 11160 sgd_solver.cpp:105] Iteration 144000, lr = 0.001
I1107 13:53:53.231020 11160 solver.cpp:218] Iteration 144100 (11.4132 iter/s, 8.76177s/100 iters), loss = 0.0376018
I1107 13:53:53.231020 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:53:53.231020 11160 solver.cpp:237]     Train net output #1: loss = 0.0376018 (* 1 = 0.0376018 loss)
I1107 13:53:53.231020 11160 sgd_solver.cpp:105] Iteration 144100, lr = 0.001
I1107 13:54:01.877717 11160 solver.cpp:218] Iteration 144200 (11.5647 iter/s, 8.64698s/100 iters), loss = 0.0207637
I1107 13:54:01.877717 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:54:01.877717 11160 solver.cpp:237]     Train net output #1: loss = 0.0207637 (* 1 = 0.0207637 loss)
I1107 13:54:01.877717 11160 sgd_solver.cpp:105] Iteration 144200, lr = 0.001
I1107 13:54:10.541471 11160 solver.cpp:218] Iteration 144300 (11.5429 iter/s, 8.6633s/100 iters), loss = 0.0284799
I1107 13:54:10.542459 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:54:10.542459 11160 solver.cpp:237]     Train net output #1: loss = 0.0284799 (* 1 = 0.0284799 loss)
I1107 13:54:10.542459 11160 sgd_solver.cpp:105] Iteration 144300, lr = 0.001
I1107 13:54:19.200577 11160 solver.cpp:218] Iteration 144400 (11.5495 iter/s, 8.65838s/100 iters), loss = 0.0313985
I1107 13:54:19.201578 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:54:19.201578 11160 solver.cpp:237]     Train net output #1: loss = 0.0313985 (* 1 = 0.0313985 loss)
I1107 13:54:19.201578 11160 sgd_solver.cpp:105] Iteration 144400, lr = 0.001
I1107 13:54:27.344547  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:54:27.684602 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_144500.caffemodel
I1107 13:54:27.714104 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_144500.solverstate
I1107 13:54:27.722604 11160 solver.cpp:330] Iteration 144500, Testing net (#0)
I1107 13:54:27.722604 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:54:29.713750 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:54:29.792753 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 13:54:29.792753 11160 solver.cpp:397]     Test net output #1: loss = 0.318994 (* 1 = 0.318994 loss)
I1107 13:54:29.874755 11160 solver.cpp:218] Iteration 144500 (9.36954 iter/s, 10.6729s/100 iters), loss = 0.0521791
I1107 13:54:29.874755 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:54:29.874755 11160 solver.cpp:237]     Train net output #1: loss = 0.0521791 (* 1 = 0.0521791 loss)
I1107 13:54:29.874755 11160 sgd_solver.cpp:105] Iteration 144500, lr = 0.001
I1107 13:54:38.979887 11160 solver.cpp:218] Iteration 144600 (10.9835 iter/s, 9.10458s/100 iters), loss = 0.0315223
I1107 13:54:38.979887 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:54:38.979887 11160 solver.cpp:237]     Train net output #1: loss = 0.0315223 (* 1 = 0.0315223 loss)
I1107 13:54:38.979887 11160 sgd_solver.cpp:105] Iteration 144600, lr = 0.001
I1107 13:54:47.607240 11160 solver.cpp:218] Iteration 144700 (11.5918 iter/s, 8.62679s/100 iters), loss = 0.0273894
I1107 13:54:47.607240 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:54:47.607240 11160 solver.cpp:237]     Train net output #1: loss = 0.0273894 (* 1 = 0.0273894 loss)
I1107 13:54:47.607240 11160 sgd_solver.cpp:105] Iteration 144700, lr = 0.001
I1107 13:54:56.425184 11160 solver.cpp:218] Iteration 144800 (11.3409 iter/s, 8.81767s/100 iters), loss = 0.0206226
I1107 13:54:56.425671 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:54:56.425671 11160 solver.cpp:237]     Train net output #1: loss = 0.0206226 (* 1 = 0.0206226 loss)
I1107 13:54:56.425671 11160 sgd_solver.cpp:105] Iteration 144800, lr = 0.001
I1107 13:55:05.072283 11160 solver.cpp:218] Iteration 144900 (11.5655 iter/s, 8.6464s/100 iters), loss = 0.0276003
I1107 13:55:05.072283 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:55:05.072283 11160 solver.cpp:237]     Train net output #1: loss = 0.0276003 (* 1 = 0.0276003 loss)
I1107 13:55:05.072283 11160 sgd_solver.cpp:105] Iteration 144900, lr = 0.001
I1107 13:55:13.311359  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:55:13.653421 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_145000.caffemodel
I1107 13:55:13.684422 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_145000.solverstate
I1107 13:55:13.694425 11160 solver.cpp:330] Iteration 145000, Testing net (#0)
I1107 13:55:13.694425 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:55:15.725760 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:55:15.804764 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 13:55:15.804764 11160 solver.cpp:397]     Test net output #1: loss = 0.313932 (* 1 = 0.313932 loss)
I1107 13:55:15.885767 11160 solver.cpp:218] Iteration 145000 (9.24784 iter/s, 10.8133s/100 iters), loss = 0.0311051
I1107 13:55:15.885767 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:55:15.885767 11160 solver.cpp:237]     Train net output #1: loss = 0.0311051 (* 1 = 0.0311051 loss)
I1107 13:55:15.885767 11160 sgd_solver.cpp:105] Iteration 145000, lr = 0.001
I1107 13:55:24.464058 11160 solver.cpp:218] Iteration 145100 (11.6586 iter/s, 8.57738s/100 iters), loss = 0.0409227
I1107 13:55:24.464058 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:55:24.464058 11160 solver.cpp:237]     Train net output #1: loss = 0.0409227 (* 1 = 0.0409227 loss)
I1107 13:55:24.464058 11160 sgd_solver.cpp:105] Iteration 145100, lr = 0.001
I1107 13:55:33.094765 11160 solver.cpp:218] Iteration 145200 (11.5869 iter/s, 8.63041s/100 iters), loss = 0.0179336
I1107 13:55:33.094765 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:55:33.094765 11160 solver.cpp:237]     Train net output #1: loss = 0.0179336 (* 1 = 0.0179336 loss)
I1107 13:55:33.094765 11160 sgd_solver.cpp:105] Iteration 145200, lr = 0.001
I1107 13:55:41.739214 11160 solver.cpp:218] Iteration 145300 (11.5687 iter/s, 8.64398s/100 iters), loss = 0.0217684
I1107 13:55:41.739214 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:55:41.739214 11160 solver.cpp:237]     Train net output #1: loss = 0.0217684 (* 1 = 0.0217684 loss)
I1107 13:55:41.739214 11160 sgd_solver.cpp:105] Iteration 145300, lr = 0.001
I1107 13:55:50.315407 11160 solver.cpp:218] Iteration 145400 (11.6619 iter/s, 8.57494s/100 iters), loss = 0.0675463
I1107 13:55:50.315407 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:55:50.315407 11160 solver.cpp:237]     Train net output #1: loss = 0.0675463 (* 1 = 0.0675463 loss)
I1107 13:55:50.315407 11160 sgd_solver.cpp:105] Iteration 145400, lr = 0.001
I1107 13:55:58.452989  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:55:58.789885 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_145500.caffemodel
I1107 13:55:58.818884 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_145500.solverstate
I1107 13:55:58.828400 11160 solver.cpp:330] Iteration 145500, Testing net (#0)
I1107 13:55:58.828400 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:56:00.817672 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:56:00.897678 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9176
I1107 13:56:00.897678 11160 solver.cpp:397]     Test net output #1: loss = 0.315632 (* 1 = 0.315632 loss)
I1107 13:56:00.979698 11160 solver.cpp:218] Iteration 145500 (9.37713 iter/s, 10.6642s/100 iters), loss = 0.026089
I1107 13:56:00.979698 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:56:00.979698 11160 solver.cpp:237]     Train net output #1: loss = 0.026089 (* 1 = 0.026089 loss)
I1107 13:56:00.979698 11160 sgd_solver.cpp:105] Iteration 145500, lr = 0.001
I1107 13:56:09.517630 11160 solver.cpp:218] Iteration 145600 (11.7134 iter/s, 8.53725s/100 iters), loss = 0.0261769
I1107 13:56:09.517630 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:56:09.517630 11160 solver.cpp:237]     Train net output #1: loss = 0.0261769 (* 1 = 0.0261769 loss)
I1107 13:56:09.517630 11160 sgd_solver.cpp:105] Iteration 145600, lr = 0.001
I1107 13:56:18.065589 11160 solver.cpp:218] Iteration 145700 (11.6993 iter/s, 8.54753s/100 iters), loss = 0.0208395
I1107 13:56:18.065589 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:56:18.065589 11160 solver.cpp:237]     Train net output #1: loss = 0.0208395 (* 1 = 0.0208395 loss)
I1107 13:56:18.065589 11160 sgd_solver.cpp:105] Iteration 145700, lr = 0.001
I1107 13:56:26.699522 11160 solver.cpp:218] Iteration 145800 (11.5833 iter/s, 8.63309s/100 iters), loss = 0.0318112
I1107 13:56:26.699522 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:56:26.699522 11160 solver.cpp:237]     Train net output #1: loss = 0.0318112 (* 1 = 0.0318112 loss)
I1107 13:56:26.699522 11160 sgd_solver.cpp:105] Iteration 145800, lr = 0.001
I1107 13:56:35.414844 11160 solver.cpp:218] Iteration 145900 (11.4749 iter/s, 8.71463s/100 iters), loss = 0.0277926
I1107 13:56:35.414844 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:56:35.414844 11160 solver.cpp:237]     Train net output #1: loss = 0.0277926 (* 1 = 0.0277926 loss)
I1107 13:56:35.414844 11160 sgd_solver.cpp:105] Iteration 145900, lr = 0.001
I1107 13:56:43.663046  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:56:44.001070 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_146000.caffemodel
I1107 13:56:44.030071 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_146000.solverstate
I1107 13:56:44.039070 11160 solver.cpp:330] Iteration 146000, Testing net (#0)
I1107 13:56:44.039070 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:56:46.042229 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:56:46.121229 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9152
I1107 13:56:46.121229 11160 solver.cpp:397]     Test net output #1: loss = 0.321649 (* 1 = 0.321649 loss)
I1107 13:56:46.204232 11160 solver.cpp:218] Iteration 146000 (9.26908 iter/s, 10.7886s/100 iters), loss = 0.043673
I1107 13:56:46.204232 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:56:46.204232 11160 solver.cpp:237]     Train net output #1: loss = 0.043673 (* 1 = 0.043673 loss)
I1107 13:56:46.204232 11160 sgd_solver.cpp:105] Iteration 146000, lr = 0.001
I1107 13:56:54.782696 11160 solver.cpp:218] Iteration 146100 (11.6567 iter/s, 8.57876s/100 iters), loss = 0.0196969
I1107 13:56:54.782696 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:56:54.782696 11160 solver.cpp:237]     Train net output #1: loss = 0.0196969 (* 1 = 0.0196969 loss)
I1107 13:56:54.782696 11160 sgd_solver.cpp:105] Iteration 146100, lr = 0.001
I1107 13:57:03.383534 11160 solver.cpp:218] Iteration 146200 (11.6279 iter/s, 8.60003s/100 iters), loss = 0.026493
I1107 13:57:03.383534 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:57:03.383534 11160 solver.cpp:237]     Train net output #1: loss = 0.026493 (* 1 = 0.026493 loss)
I1107 13:57:03.383534 11160 sgd_solver.cpp:105] Iteration 146200, lr = 0.001
I1107 13:57:12.253556 11160 solver.cpp:218] Iteration 146300 (11.2749 iter/s, 8.86922s/100 iters), loss = 0.0214635
I1107 13:57:12.253556 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:57:12.253556 11160 solver.cpp:237]     Train net output #1: loss = 0.0214635 (* 1 = 0.0214635 loss)
I1107 13:57:12.253556 11160 sgd_solver.cpp:105] Iteration 146300, lr = 0.001
I1107 13:57:20.899664 11160 solver.cpp:218] Iteration 146400 (11.5669 iter/s, 8.64538s/100 iters), loss = 0.0235854
I1107 13:57:20.899664 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:57:20.899664 11160 solver.cpp:237]     Train net output #1: loss = 0.0235854 (* 1 = 0.0235854 loss)
I1107 13:57:20.899664 11160 sgd_solver.cpp:105] Iteration 146400, lr = 0.001
I1107 13:57:29.174751  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:57:29.527072 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_146500.caffemodel
I1107 13:57:29.557072 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_146500.solverstate
I1107 13:57:29.566076 11160 solver.cpp:330] Iteration 146500, Testing net (#0)
I1107 13:57:29.566076 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:57:31.628512 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:57:31.711246 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1107 13:57:31.711246 11160 solver.cpp:397]     Test net output #1: loss = 0.32062 (* 1 = 0.32062 loss)
I1107 13:57:31.795768 11160 solver.cpp:218] Iteration 146500 (9.17795 iter/s, 10.8957s/100 iters), loss = 0.0321448
I1107 13:57:31.795768 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:57:31.795768 11160 solver.cpp:237]     Train net output #1: loss = 0.0321448 (* 1 = 0.0321448 loss)
I1107 13:57:31.795768 11160 sgd_solver.cpp:105] Iteration 146500, lr = 0.001
I1107 13:57:40.594646 11160 solver.cpp:218] Iteration 146600 (11.3652 iter/s, 8.79881s/100 iters), loss = 0.0249905
I1107 13:57:40.594646 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:57:40.595649 11160 solver.cpp:237]     Train net output #1: loss = 0.0249905 (* 1 = 0.0249905 loss)
I1107 13:57:40.595649 11160 sgd_solver.cpp:105] Iteration 146600, lr = 0.001
I1107 13:57:49.257851 11160 solver.cpp:218] Iteration 146700 (11.5442 iter/s, 8.66233s/100 iters), loss = 0.0194664
I1107 13:57:49.257851 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:57:49.257851 11160 solver.cpp:237]     Train net output #1: loss = 0.0194664 (* 1 = 0.0194664 loss)
I1107 13:57:49.257851 11160 sgd_solver.cpp:105] Iteration 146700, lr = 0.001
I1107 13:57:57.992485 11160 solver.cpp:218] Iteration 146800 (11.4495 iter/s, 8.73398s/100 iters), loss = 0.0230687
I1107 13:57:57.992485 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:57:57.992485 11160 solver.cpp:237]     Train net output #1: loss = 0.0230687 (* 1 = 0.0230687 loss)
I1107 13:57:57.992485 11160 sgd_solver.cpp:105] Iteration 146800, lr = 0.001
I1107 13:58:06.554832 11160 solver.cpp:218] Iteration 146900 (11.6805 iter/s, 8.56128s/100 iters), loss = 0.0283022
I1107 13:58:06.554832 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:58:06.554832 11160 solver.cpp:237]     Train net output #1: loss = 0.0283022 (* 1 = 0.0283022 loss)
I1107 13:58:06.554832 11160 sgd_solver.cpp:105] Iteration 146900, lr = 0.001
I1107 13:58:14.669867  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:58:15.011884 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_147000.caffemodel
I1107 13:58:15.041887 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_147000.solverstate
I1107 13:58:15.050891 11160 solver.cpp:330] Iteration 147000, Testing net (#0)
I1107 13:58:15.050891 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:58:17.108045 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:58:17.191048 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 13:58:17.191048 11160 solver.cpp:397]     Test net output #1: loss = 0.320303 (* 1 = 0.320303 loss)
I1107 13:58:17.280056 11160 solver.cpp:218] Iteration 147000 (9.32383 iter/s, 10.7252s/100 iters), loss = 0.0412529
I1107 13:58:17.280056 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:58:17.280056 11160 solver.cpp:237]     Train net output #1: loss = 0.041253 (* 1 = 0.041253 loss)
I1107 13:58:17.280056 11160 sgd_solver.cpp:105] Iteration 147000, lr = 0.001
I1107 13:58:26.134555 11160 solver.cpp:218] Iteration 147100 (11.2955 iter/s, 8.85306s/100 iters), loss = 0.0380322
I1107 13:58:26.134555 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:58:26.134555 11160 solver.cpp:237]     Train net output #1: loss = 0.0380323 (* 1 = 0.0380323 loss)
I1107 13:58:26.134555 11160 sgd_solver.cpp:105] Iteration 147100, lr = 0.001
I1107 13:58:34.888654 11160 solver.cpp:218] Iteration 147200 (11.4236 iter/s, 8.75381s/100 iters), loss = 0.031361
I1107 13:58:34.888654 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:58:34.888654 11160 solver.cpp:237]     Train net output #1: loss = 0.031361 (* 1 = 0.031361 loss)
I1107 13:58:34.888654 11160 sgd_solver.cpp:105] Iteration 147200, lr = 0.001
I1107 13:58:43.447515 11160 solver.cpp:218] Iteration 147300 (11.685 iter/s, 8.55799s/100 iters), loss = 0.0217935
I1107 13:58:43.447515 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:58:43.447515 11160 solver.cpp:237]     Train net output #1: loss = 0.0217935 (* 1 = 0.0217935 loss)
I1107 13:58:43.447515 11160 sgd_solver.cpp:105] Iteration 147300, lr = 0.001
I1107 13:58:51.992992 11160 solver.cpp:218] Iteration 147400 (11.7016 iter/s, 8.5458s/100 iters), loss = 0.0231248
I1107 13:58:51.992992 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:58:51.992992 11160 solver.cpp:237]     Train net output #1: loss = 0.0231248 (* 1 = 0.0231248 loss)
I1107 13:58:51.992992 11160 sgd_solver.cpp:105] Iteration 147400, lr = 0.001
I1107 13:59:00.102788  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:59:00.439806 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_147500.caffemodel
I1107 13:59:00.470809 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_147500.solverstate
I1107 13:59:00.479811 11160 solver.cpp:330] Iteration 147500, Testing net (#0)
I1107 13:59:00.479811 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:59:02.468971 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:59:02.548972 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9172
I1107 13:59:02.548972 11160 solver.cpp:397]     Test net output #1: loss = 0.321738 (* 1 = 0.321738 loss)
I1107 13:59:02.630973 11160 solver.cpp:218] Iteration 147500 (9.40153 iter/s, 10.6366s/100 iters), loss = 0.028477
I1107 13:59:02.630973 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:59:02.630973 11160 solver.cpp:237]     Train net output #1: loss = 0.028477 (* 1 = 0.028477 loss)
I1107 13:59:02.630973 11160 sgd_solver.cpp:105] Iteration 147500, lr = 0.001
I1107 13:59:11.165041 11160 solver.cpp:218] Iteration 147600 (11.7181 iter/s, 8.53378s/100 iters), loss = 0.0544105
I1107 13:59:11.165041 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 13:59:11.165041 11160 solver.cpp:237]     Train net output #1: loss = 0.0544105 (* 1 = 0.0544105 loss)
I1107 13:59:11.165041 11160 sgd_solver.cpp:105] Iteration 147600, lr = 0.001
I1107 13:59:19.755690 11160 solver.cpp:218] Iteration 147700 (11.6407 iter/s, 8.59051s/100 iters), loss = 0.0240084
I1107 13:59:19.755690 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:59:19.755690 11160 solver.cpp:237]     Train net output #1: loss = 0.0240084 (* 1 = 0.0240084 loss)
I1107 13:59:19.755690 11160 sgd_solver.cpp:105] Iteration 147700, lr = 0.001
I1107 13:59:28.338109 11160 solver.cpp:218] Iteration 147800 (11.6526 iter/s, 8.58179s/100 iters), loss = 0.0257475
I1107 13:59:28.338109 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:59:28.338608 11160 solver.cpp:237]     Train net output #1: loss = 0.0257475 (* 1 = 0.0257475 loss)
I1107 13:59:28.338608 11160 sgd_solver.cpp:105] Iteration 147800, lr = 0.001
I1107 13:59:36.910609 11160 solver.cpp:218] Iteration 147900 (11.6661 iter/s, 8.57185s/100 iters), loss = 0.0365363
I1107 13:59:36.910609 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:59:36.910609 11160 solver.cpp:237]     Train net output #1: loss = 0.0365363 (* 1 = 0.0365363 loss)
I1107 13:59:36.910609 11160 sgd_solver.cpp:105] Iteration 147900, lr = 0.001
I1107 13:59:45.056221  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:59:45.391772 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_148000.caffemodel
I1107 13:59:45.421772 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_148000.solverstate
I1107 13:59:45.430773 11160 solver.cpp:330] Iteration 148000, Testing net (#0)
I1107 13:59:45.430773 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 13:59:47.418891 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 13:59:47.497898 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 13:59:47.497898 11160 solver.cpp:397]     Test net output #1: loss = 0.317504 (* 1 = 0.317504 loss)
I1107 13:59:47.579917 11160 solver.cpp:218] Iteration 148000 (9.37303 iter/s, 10.6689s/100 iters), loss = 0.0342189
I1107 13:59:47.579917 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 13:59:47.579917 11160 solver.cpp:237]     Train net output #1: loss = 0.0342189 (* 1 = 0.0342189 loss)
I1107 13:59:47.579917 11160 sgd_solver.cpp:105] Iteration 148000, lr = 0.001
I1107 13:59:56.181133 11160 solver.cpp:218] Iteration 148100 (11.6267 iter/s, 8.60086s/100 iters), loss = 0.0201857
I1107 13:59:56.181133 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 13:59:56.181133 11160 solver.cpp:237]     Train net output #1: loss = 0.0201857 (* 1 = 0.0201857 loss)
I1107 13:59:56.181133 11160 sgd_solver.cpp:105] Iteration 148100, lr = 0.001
I1107 14:00:04.768729 11160 solver.cpp:218] Iteration 148200 (11.6457 iter/s, 8.58689s/100 iters), loss = 0.0190715
I1107 14:00:04.768729 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:00:04.768729 11160 solver.cpp:237]     Train net output #1: loss = 0.0190715 (* 1 = 0.0190715 loss)
I1107 14:00:04.768729 11160 sgd_solver.cpp:105] Iteration 148200, lr = 0.001
I1107 14:00:13.316365 11160 solver.cpp:218] Iteration 148300 (11.7006 iter/s, 8.54658s/100 iters), loss = 0.0236751
I1107 14:00:13.316365 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:00:13.316365 11160 solver.cpp:237]     Train net output #1: loss = 0.0236751 (* 1 = 0.0236751 loss)
I1107 14:00:13.316365 11160 sgd_solver.cpp:105] Iteration 148300, lr = 0.001
I1107 14:00:21.896354 11160 solver.cpp:218] Iteration 148400 (11.6549 iter/s, 8.58009s/100 iters), loss = 0.024696
I1107 14:00:21.896354 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:00:21.896354 11160 solver.cpp:237]     Train net output #1: loss = 0.024696 (* 1 = 0.024696 loss)
I1107 14:00:21.896354 11160 sgd_solver.cpp:105] Iteration 148400, lr = 0.001
I1107 14:00:30.029388  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:00:30.364933 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_148500.caffemodel
I1107 14:00:30.396934 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_148500.solverstate
I1107 14:00:30.405935 11160 solver.cpp:330] Iteration 148500, Testing net (#0)
I1107 14:00:30.406934 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:00:32.400166 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:00:32.480171 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9169
I1107 14:00:32.480171 11160 solver.cpp:397]     Test net output #1: loss = 0.319561 (* 1 = 0.319561 loss)
I1107 14:00:32.561192 11160 solver.cpp:218] Iteration 148500 (9.37716 iter/s, 10.6642s/100 iters), loss = 0.0370423
I1107 14:00:32.561192 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:00:32.561192 11160 solver.cpp:237]     Train net output #1: loss = 0.0370422 (* 1 = 0.0370422 loss)
I1107 14:00:32.561192 11160 sgd_solver.cpp:105] Iteration 148500, lr = 0.001
I1107 14:00:41.114439 11160 solver.cpp:218] Iteration 148600 (11.6926 iter/s, 8.55245s/100 iters), loss = 0.0367679
I1107 14:00:41.114439 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:00:41.114439 11160 solver.cpp:237]     Train net output #1: loss = 0.0367679 (* 1 = 0.0367679 loss)
I1107 14:00:41.114439 11160 sgd_solver.cpp:105] Iteration 148600, lr = 0.001
I1107 14:00:49.698323 11160 solver.cpp:218] Iteration 148700 (11.6514 iter/s, 8.58266s/100 iters), loss = 0.0252784
I1107 14:00:49.698323 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:00:49.698323 11160 solver.cpp:237]     Train net output #1: loss = 0.0252784 (* 1 = 0.0252784 loss)
I1107 14:00:49.698323 11160 sgd_solver.cpp:105] Iteration 148700, lr = 0.001
I1107 14:00:58.269852 11160 solver.cpp:218] Iteration 148800 (11.6669 iter/s, 8.57126s/100 iters), loss = 0.0318263
I1107 14:00:58.269852 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:00:58.269852 11160 solver.cpp:237]     Train net output #1: loss = 0.0318263 (* 1 = 0.0318263 loss)
I1107 14:00:58.269852 11160 sgd_solver.cpp:105] Iteration 148800, lr = 0.001
I1107 14:01:06.818189 11160 solver.cpp:218] Iteration 148900 (11.6988 iter/s, 8.54788s/100 iters), loss = 0.0257257
I1107 14:01:06.818189 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:01:06.818189 11160 solver.cpp:237]     Train net output #1: loss = 0.0257257 (* 1 = 0.0257257 loss)
I1107 14:01:06.818189 11160 sgd_solver.cpp:105] Iteration 148900, lr = 0.001
I1107 14:01:14.936770  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:01:15.271492 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_149000.caffemodel
I1107 14:01:15.301549 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_149000.solverstate
I1107 14:01:15.309545 11160 solver.cpp:330] Iteration 149000, Testing net (#0)
I1107 14:01:15.310561 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:01:17.306110 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:01:17.386109 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9175
I1107 14:01:17.386109 11160 solver.cpp:397]     Test net output #1: loss = 0.320121 (* 1 = 0.320121 loss)
I1107 14:01:17.467108 11160 solver.cpp:218] Iteration 149000 (9.39083 iter/s, 10.6487s/100 iters), loss = 0.0255739
I1107 14:01:17.467108 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:01:17.467108 11160 solver.cpp:237]     Train net output #1: loss = 0.0255739 (* 1 = 0.0255739 loss)
I1107 14:01:17.467108 11160 sgd_solver.cpp:105] Iteration 149000, lr = 0.001
I1107 14:01:26.049347 11160 solver.cpp:218] Iteration 149100 (11.6529 iter/s, 8.58157s/100 iters), loss = 0.0275326
I1107 14:01:26.049347 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:01:26.049347 11160 solver.cpp:237]     Train net output #1: loss = 0.0275325 (* 1 = 0.0275325 loss)
I1107 14:01:26.049347 11160 sgd_solver.cpp:105] Iteration 149100, lr = 0.001
I1107 14:01:34.582581 11160 solver.cpp:218] Iteration 149200 (11.7194 iter/s, 8.53288s/100 iters), loss = 0.0180756
I1107 14:01:34.582581 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:01:34.582581 11160 solver.cpp:237]     Train net output #1: loss = 0.0180756 (* 1 = 0.0180756 loss)
I1107 14:01:34.582581 11160 sgd_solver.cpp:105] Iteration 149200, lr = 0.001
I1107 14:01:43.114620 11160 solver.cpp:218] Iteration 149300 (11.7221 iter/s, 8.53089s/100 iters), loss = 0.02255
I1107 14:01:43.114620 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:01:43.114620 11160 solver.cpp:237]     Train net output #1: loss = 0.02255 (* 1 = 0.02255 loss)
I1107 14:01:43.114620 11160 sgd_solver.cpp:105] Iteration 149300, lr = 0.001
I1107 14:01:51.648792 11160 solver.cpp:218] Iteration 149400 (11.718 iter/s, 8.53391s/100 iters), loss = 0.025984
I1107 14:01:51.648792 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:01:51.648792 11160 solver.cpp:237]     Train net output #1: loss = 0.025984 (* 1 = 0.025984 loss)
I1107 14:01:51.648792 11160 sgd_solver.cpp:105] Iteration 149400, lr = 0.001
I1107 14:01:59.757776  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:02:00.097295 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_149500.caffemodel
I1107 14:02:00.127799 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_149500.solverstate
I1107 14:02:00.136798 11160 solver.cpp:330] Iteration 149500, Testing net (#0)
I1107 14:02:00.136798 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:02:02.126919 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:02:02.207422 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9174
I1107 14:02:02.207422 11160 solver.cpp:397]     Test net output #1: loss = 0.323431 (* 1 = 0.323431 loss)
I1107 14:02:02.288925 11160 solver.cpp:218] Iteration 149500 (9.39881 iter/s, 10.6396s/100 iters), loss = 0.0227413
I1107 14:02:02.288925 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:02:02.288925 11160 solver.cpp:237]     Train net output #1: loss = 0.0227412 (* 1 = 0.0227412 loss)
I1107 14:02:02.288925 11160 sgd_solver.cpp:105] Iteration 149500, lr = 0.001
I1107 14:02:10.818732 11160 solver.cpp:218] Iteration 149600 (11.7243 iter/s, 8.52931s/100 iters), loss = 0.023103
I1107 14:02:10.818732 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:02:10.818732 11160 solver.cpp:237]     Train net output #1: loss = 0.023103 (* 1 = 0.023103 loss)
I1107 14:02:10.818732 11160 sgd_solver.cpp:105] Iteration 149600, lr = 0.001
I1107 14:02:19.362685 11160 solver.cpp:218] Iteration 149700 (11.7047 iter/s, 8.54358s/100 iters), loss = 0.0201804
I1107 14:02:19.362685 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:02:19.362685 11160 solver.cpp:237]     Train net output #1: loss = 0.0201804 (* 1 = 0.0201804 loss)
I1107 14:02:19.362685 11160 sgd_solver.cpp:105] Iteration 149700, lr = 0.001
I1107 14:02:27.901449 11160 solver.cpp:218] Iteration 149800 (11.7124 iter/s, 8.53797s/100 iters), loss = 0.0225768
I1107 14:02:27.901449 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:02:27.901449 11160 solver.cpp:237]     Train net output #1: loss = 0.0225768 (* 1 = 0.0225768 loss)
I1107 14:02:27.901449 11160 sgd_solver.cpp:105] Iteration 149800, lr = 0.001
I1107 14:02:36.438496 11160 solver.cpp:218] Iteration 149900 (11.7145 iter/s, 8.53643s/100 iters), loss = 0.0244593
I1107 14:02:36.438496 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:02:36.438496 11160 solver.cpp:237]     Train net output #1: loss = 0.0244592 (* 1 = 0.0244592 loss)
I1107 14:02:36.438496 11160 sgd_solver.cpp:105] Iteration 149900, lr = 0.001
I1107 14:02:44.561055  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:02:44.900070 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_150000.caffemodel
I1107 14:02:44.928077 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_150000.solverstate
I1107 14:02:44.937077 11160 solver.cpp:330] Iteration 150000, Testing net (#0)
I1107 14:02:44.937077 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:02:46.929234 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:02:47.008733 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9164
I1107 14:02:47.009234 11160 solver.cpp:397]     Test net output #1: loss = 0.322288 (* 1 = 0.322288 loss)
I1107 14:02:47.090237 11160 solver.cpp:218] Iteration 150000 (9.38851 iter/s, 10.6513s/100 iters), loss = 0.0224189
I1107 14:02:47.090237 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:02:47.090237 11160 solver.cpp:237]     Train net output #1: loss = 0.0224189 (* 1 = 0.0224189 loss)
I1107 14:02:47.090237 11160 sgd_solver.cpp:105] Iteration 150000, lr = 0.001
I1107 14:02:55.642505 11160 solver.cpp:218] Iteration 150100 (11.6938 iter/s, 8.55156s/100 iters), loss = 0.0244074
I1107 14:02:55.642505 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:02:55.642505 11160 solver.cpp:237]     Train net output #1: loss = 0.0244073 (* 1 = 0.0244073 loss)
I1107 14:02:55.642505 11160 sgd_solver.cpp:105] Iteration 150100, lr = 0.001
I1107 14:03:04.204432 11160 solver.cpp:218] Iteration 150200 (11.6795 iter/s, 8.56203s/100 iters), loss = 0.0301274
I1107 14:03:04.204432 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:03:04.204432 11160 solver.cpp:237]     Train net output #1: loss = 0.0301273 (* 1 = 0.0301273 loss)
I1107 14:03:04.204432 11160 sgd_solver.cpp:105] Iteration 150200, lr = 0.001
I1107 14:03:12.752758 11160 solver.cpp:218] Iteration 150300 (11.6989 iter/s, 8.54783s/100 iters), loss = 0.0401992
I1107 14:03:12.752758 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 14:03:12.752758 11160 solver.cpp:237]     Train net output #1: loss = 0.0401991 (* 1 = 0.0401991 loss)
I1107 14:03:12.752758 11160 sgd_solver.cpp:105] Iteration 150300, lr = 0.001
I1107 14:03:21.288370 11160 solver.cpp:218] Iteration 150400 (11.7167 iter/s, 8.53484s/100 iters), loss = 0.0219066
I1107 14:03:21.288370 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:03:21.288370 11160 solver.cpp:237]     Train net output #1: loss = 0.0219066 (* 1 = 0.0219066 loss)
I1107 14:03:21.288370 11160 sgd_solver.cpp:105] Iteration 150400, lr = 0.001
I1107 14:03:29.391855  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:03:29.728890 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_150500.caffemodel
I1107 14:03:29.758890 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_150500.solverstate
I1107 14:03:29.767891 11160 solver.cpp:330] Iteration 150500, Testing net (#0)
I1107 14:03:29.767891 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:03:31.755117 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:03:31.835124 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9154
I1107 14:03:31.835124 11160 solver.cpp:397]     Test net output #1: loss = 0.319067 (* 1 = 0.319067 loss)
I1107 14:03:31.916129 11160 solver.cpp:218] Iteration 150500 (9.40958 iter/s, 10.6275s/100 iters), loss = 0.0365274
I1107 14:03:31.916129 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:03:31.916129 11160 solver.cpp:237]     Train net output #1: loss = 0.0365274 (* 1 = 0.0365274 loss)
I1107 14:03:31.916129 11160 sgd_solver.cpp:105] Iteration 150500, lr = 0.001
I1107 14:03:40.452622 11160 solver.cpp:218] Iteration 150600 (11.7156 iter/s, 8.53566s/100 iters), loss = 0.0362971
I1107 14:03:40.452622 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:03:40.452622 11160 solver.cpp:237]     Train net output #1: loss = 0.0362971 (* 1 = 0.0362971 loss)
I1107 14:03:40.452622 11160 sgd_solver.cpp:105] Iteration 150600, lr = 0.001
I1107 14:03:48.980803 11160 solver.cpp:218] Iteration 150700 (11.7271 iter/s, 8.52729s/100 iters), loss = 0.0227735
I1107 14:03:48.980803 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:03:48.980803 11160 solver.cpp:237]     Train net output #1: loss = 0.0227735 (* 1 = 0.0227735 loss)
I1107 14:03:48.980803 11160 sgd_solver.cpp:105] Iteration 150700, lr = 0.001
I1107 14:03:57.513962 11160 solver.cpp:218] Iteration 150800 (11.7196 iter/s, 8.53269s/100 iters), loss = 0.0193119
I1107 14:03:57.513962 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:03:57.513962 11160 solver.cpp:237]     Train net output #1: loss = 0.0193119 (* 1 = 0.0193119 loss)
I1107 14:03:57.513962 11160 sgd_solver.cpp:105] Iteration 150800, lr = 0.001
I1107 14:04:06.039891 11160 solver.cpp:218] Iteration 150900 (11.729 iter/s, 8.5259s/100 iters), loss = 0.0300297
I1107 14:04:06.039891 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:04:06.039891 11160 solver.cpp:237]     Train net output #1: loss = 0.0300297 (* 1 = 0.0300297 loss)
I1107 14:04:06.039891 11160 sgd_solver.cpp:105] Iteration 150900, lr = 0.001
I1107 14:04:14.152092  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:04:14.491169 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_151000.caffemodel
I1107 14:04:14.520190 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_151000.solverstate
I1107 14:04:14.529186 11160 solver.cpp:330] Iteration 151000, Testing net (#0)
I1107 14:04:14.529186 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:04:16.521471 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:04:16.601470 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9157
I1107 14:04:16.601470 11160 solver.cpp:397]     Test net output #1: loss = 0.327665 (* 1 = 0.327665 loss)
I1107 14:04:16.682482 11160 solver.cpp:218] Iteration 151000 (9.39661 iter/s, 10.6421s/100 iters), loss = 0.0320661
I1107 14:04:16.682482 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:04:16.682482 11160 solver.cpp:237]     Train net output #1: loss = 0.0320661 (* 1 = 0.0320661 loss)
I1107 14:04:16.682482 11160 sgd_solver.cpp:105] Iteration 151000, lr = 0.001
I1107 14:04:25.228385 11160 solver.cpp:218] Iteration 151100 (11.702 iter/s, 8.54554s/100 iters), loss = 0.0226549
I1107 14:04:25.228385 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:04:25.228385 11160 solver.cpp:237]     Train net output #1: loss = 0.0226548 (* 1 = 0.0226548 loss)
I1107 14:04:25.228385 11160 sgd_solver.cpp:105] Iteration 151100, lr = 0.001
I1107 14:04:33.771435 11160 solver.cpp:218] Iteration 151200 (11.707 iter/s, 8.54192s/100 iters), loss = 0.0254227
I1107 14:04:33.771435 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:04:33.771435 11160 solver.cpp:237]     Train net output #1: loss = 0.0254226 (* 1 = 0.0254226 loss)
I1107 14:04:33.771435 11160 sgd_solver.cpp:105] Iteration 151200, lr = 0.001
I1107 14:04:42.316689 11160 solver.cpp:218] Iteration 151300 (11.7032 iter/s, 8.5447s/100 iters), loss = 0.0325203
I1107 14:04:42.316689 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:04:42.316689 11160 solver.cpp:237]     Train net output #1: loss = 0.0325202 (* 1 = 0.0325202 loss)
I1107 14:04:42.316689 11160 sgd_solver.cpp:105] Iteration 151300, lr = 0.001
I1107 14:04:50.842075 11160 solver.cpp:218] Iteration 151400 (11.731 iter/s, 8.52444s/100 iters), loss = 0.023333
I1107 14:04:50.842075 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:04:50.842075 11160 solver.cpp:237]     Train net output #1: loss = 0.023333 (* 1 = 0.023333 loss)
I1107 14:04:50.842075 11160 sgd_solver.cpp:105] Iteration 151400, lr = 0.001
I1107 14:04:58.968849  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:04:59.304862 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_151500.caffemodel
I1107 14:04:59.336868 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_151500.solverstate
I1107 14:04:59.345868 11160 solver.cpp:330] Iteration 151500, Testing net (#0)
I1107 14:04:59.345868 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:05:01.335633 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:05:01.415138 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9162
I1107 14:05:01.415138 11160 solver.cpp:397]     Test net output #1: loss = 0.326562 (* 1 = 0.326562 loss)
I1107 14:05:01.496139 11160 solver.cpp:218] Iteration 151500 (9.3865 iter/s, 10.6536s/100 iters), loss = 0.0244119
I1107 14:05:01.496139 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:05:01.496139 11160 solver.cpp:237]     Train net output #1: loss = 0.0244119 (* 1 = 0.0244119 loss)
I1107 14:05:01.496139 11160 sgd_solver.cpp:105] Iteration 151500, lr = 0.001
I1107 14:05:10.027339 11160 solver.cpp:218] Iteration 151600 (11.7223 iter/s, 8.53074s/100 iters), loss = 0.0256391
I1107 14:05:10.027339 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:05:10.027339 11160 solver.cpp:237]     Train net output #1: loss = 0.0256391 (* 1 = 0.0256391 loss)
I1107 14:05:10.027339 11160 sgd_solver.cpp:105] Iteration 151600, lr = 0.001
I1107 14:05:18.558758 11160 solver.cpp:218] Iteration 151700 (11.7224 iter/s, 8.53067s/100 iters), loss = 0.0216451
I1107 14:05:18.558758 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:05:18.558758 11160 solver.cpp:237]     Train net output #1: loss = 0.0216451 (* 1 = 0.0216451 loss)
I1107 14:05:18.558758 11160 sgd_solver.cpp:105] Iteration 151700, lr = 0.001
I1107 14:05:27.087471 11160 solver.cpp:218] Iteration 151800 (11.7252 iter/s, 8.52865s/100 iters), loss = 0.0347016
I1107 14:05:27.087471 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:05:27.087471 11160 solver.cpp:237]     Train net output #1: loss = 0.0347016 (* 1 = 0.0347016 loss)
I1107 14:05:27.087471 11160 sgd_solver.cpp:105] Iteration 151800, lr = 0.001
I1107 14:05:35.614467 11160 solver.cpp:218] Iteration 151900 (11.7283 iter/s, 8.52642s/100 iters), loss = 0.0258142
I1107 14:05:35.614467 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:05:35.614467 11160 solver.cpp:237]     Train net output #1: loss = 0.0258141 (* 1 = 0.0258141 loss)
I1107 14:05:35.614467 11160 sgd_solver.cpp:105] Iteration 151900, lr = 0.001
I1107 14:05:43.718956  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:05:44.055986 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_152000.caffemodel
I1107 14:05:44.088989 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_152000.solverstate
I1107 14:05:44.097987 11160 solver.cpp:330] Iteration 152000, Testing net (#0)
I1107 14:05:44.097987 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:05:46.086169 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:05:46.166174 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9171
I1107 14:05:46.166174 11160 solver.cpp:397]     Test net output #1: loss = 0.322959 (* 1 = 0.322959 loss)
I1107 14:05:46.247180 11160 solver.cpp:218] Iteration 152000 (9.40534 iter/s, 10.6323s/100 iters), loss = 0.0284546
I1107 14:05:46.247180 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:05:46.247180 11160 solver.cpp:237]     Train net output #1: loss = 0.0284546 (* 1 = 0.0284546 loss)
I1107 14:05:46.247180 11160 sgd_solver.cpp:105] Iteration 152000, lr = 0.001
I1107 14:05:54.777093 11160 solver.cpp:218] Iteration 152100 (11.7238 iter/s, 8.52967s/100 iters), loss = 0.0379378
I1107 14:05:54.777093 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:05:54.777093 11160 solver.cpp:237]     Train net output #1: loss = 0.0379378 (* 1 = 0.0379378 loss)
I1107 14:05:54.777093 11160 sgd_solver.cpp:105] Iteration 152100, lr = 0.001
I1107 14:06:03.314121 11160 solver.cpp:218] Iteration 152200 (11.7153 iter/s, 8.53586s/100 iters), loss = 0.0244238
I1107 14:06:03.314121 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:06:03.314121 11160 solver.cpp:237]     Train net output #1: loss = 0.0244238 (* 1 = 0.0244238 loss)
I1107 14:06:03.314121 11160 sgd_solver.cpp:105] Iteration 152200, lr = 0.001
I1107 14:06:11.851653 11160 solver.cpp:218] Iteration 152300 (11.7132 iter/s, 8.53739s/100 iters), loss = 0.0167435
I1107 14:06:11.851653 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:06:11.851653 11160 solver.cpp:237]     Train net output #1: loss = 0.0167435 (* 1 = 0.0167435 loss)
I1107 14:06:11.851653 11160 sgd_solver.cpp:105] Iteration 152300, lr = 0.001
I1107 14:06:20.389262 11160 solver.cpp:218] Iteration 152400 (11.7136 iter/s, 8.53708s/100 iters), loss = 0.0247577
I1107 14:06:20.389262 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:06:20.389262 11160 solver.cpp:237]     Train net output #1: loss = 0.0247577 (* 1 = 0.0247577 loss)
I1107 14:06:20.389262 11160 sgd_solver.cpp:105] Iteration 152400, lr = 0.001
I1107 14:06:28.506266  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:06:28.845304 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_152500.caffemodel
I1107 14:06:28.874305 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_152500.solverstate
I1107 14:06:28.883304 11160 solver.cpp:330] Iteration 152500, Testing net (#0)
I1107 14:06:28.883304 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:06:30.872442 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:06:30.953446 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9171
I1107 14:06:30.953446 11160 solver.cpp:397]     Test net output #1: loss = 0.318066 (* 1 = 0.318066 loss)
I1107 14:06:31.034451 11160 solver.cpp:218] Iteration 152500 (9.39423 iter/s, 10.6448s/100 iters), loss = 0.0285175
I1107 14:06:31.034451 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:06:31.034451 11160 solver.cpp:237]     Train net output #1: loss = 0.0285175 (* 1 = 0.0285175 loss)
I1107 14:06:31.034451 11160 sgd_solver.cpp:105] Iteration 152500, lr = 0.001
I1107 14:06:39.584872 11160 solver.cpp:218] Iteration 152600 (11.696 iter/s, 8.54996s/100 iters), loss = 0.0316371
I1107 14:06:39.584872 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:06:39.584872 11160 solver.cpp:237]     Train net output #1: loss = 0.0316371 (* 1 = 0.0316371 loss)
I1107 14:06:39.584872 11160 sgd_solver.cpp:105] Iteration 152600, lr = 0.001
I1107 14:06:48.139920 11160 solver.cpp:218] Iteration 152700 (11.6901 iter/s, 8.55425s/100 iters), loss = 0.0433348
I1107 14:06:48.139920 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:06:48.139920 11160 solver.cpp:237]     Train net output #1: loss = 0.0433348 (* 1 = 0.0433348 loss)
I1107 14:06:48.139920 11160 sgd_solver.cpp:105] Iteration 152700, lr = 0.001
I1107 14:06:56.680727 11160 solver.cpp:218] Iteration 152800 (11.7095 iter/s, 8.54007s/100 iters), loss = 0.0295249
I1107 14:06:56.680727 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:06:56.680727 11160 solver.cpp:237]     Train net output #1: loss = 0.0295249 (* 1 = 0.0295249 loss)
I1107 14:06:56.680727 11160 sgd_solver.cpp:105] Iteration 152800, lr = 0.001
I1107 14:07:05.220829 11160 solver.cpp:218] Iteration 152900 (11.7102 iter/s, 8.53958s/100 iters), loss = 0.0264815
I1107 14:07:05.220829 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:07:05.220829 11160 solver.cpp:237]     Train net output #1: loss = 0.0264815 (* 1 = 0.0264815 loss)
I1107 14:07:05.220829 11160 sgd_solver.cpp:105] Iteration 152900, lr = 0.001
I1107 14:07:13.336832  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:07:13.675849 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_153000.caffemodel
I1107 14:07:13.706848 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_153000.solverstate
I1107 14:07:13.715353 11160 solver.cpp:330] Iteration 153000, Testing net (#0)
I1107 14:07:13.715353 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:07:15.713040 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:07:15.792042 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 14:07:15.792042 11160 solver.cpp:397]     Test net output #1: loss = 0.316299 (* 1 = 0.316299 loss)
I1107 14:07:15.874063 11160 solver.cpp:218] Iteration 153000 (9.38668 iter/s, 10.6534s/100 iters), loss = 0.0256433
I1107 14:07:15.874063 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:07:15.875063 11160 solver.cpp:237]     Train net output #1: loss = 0.0256433 (* 1 = 0.0256433 loss)
I1107 14:07:15.875063 11160 sgd_solver.cpp:46] MultiStep Status: Iteration 153000, step = 3
I1107 14:07:15.875063 11160 sgd_solver.cpp:105] Iteration 153000, lr = 0.0001
I1107 14:07:24.523893 11160 solver.cpp:218] Iteration 153100 (11.5624 iter/s, 8.64875s/100 iters), loss = 0.0321193
I1107 14:07:24.523893 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:07:24.524395 11160 solver.cpp:237]     Train net output #1: loss = 0.0321193 (* 1 = 0.0321193 loss)
I1107 14:07:24.524395 11160 sgd_solver.cpp:105] Iteration 153100, lr = 0.0001
I1107 14:07:33.192955 11160 solver.cpp:218] Iteration 153200 (11.5362 iter/s, 8.66833s/100 iters), loss = 0.0231847
I1107 14:07:33.192955 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:07:33.192955 11160 solver.cpp:237]     Train net output #1: loss = 0.0231847 (* 1 = 0.0231847 loss)
I1107 14:07:33.192955 11160 sgd_solver.cpp:105] Iteration 153200, lr = 0.0001
I1107 14:07:41.798161 11160 solver.cpp:218] Iteration 153300 (11.6215 iter/s, 8.60473s/100 iters), loss = 0.0227111
I1107 14:07:41.798161 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:07:41.798161 11160 solver.cpp:237]     Train net output #1: loss = 0.0227111 (* 1 = 0.0227111 loss)
I1107 14:07:41.798161 11160 sgd_solver.cpp:105] Iteration 153300, lr = 0.0001
I1107 14:07:50.370189 11160 solver.cpp:218] Iteration 153400 (11.6663 iter/s, 8.57173s/100 iters), loss = 0.0267373
I1107 14:07:50.370189 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:07:50.370189 11160 solver.cpp:237]     Train net output #1: loss = 0.0267373 (* 1 = 0.0267373 loss)
I1107 14:07:50.370189 11160 sgd_solver.cpp:105] Iteration 153400, lr = 0.0001
I1107 14:07:58.499125  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:07:58.838178 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_153500.caffemodel
I1107 14:07:58.868180 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_153500.solverstate
I1107 14:07:58.878165 11160 solver.cpp:330] Iteration 153500, Testing net (#0)
I1107 14:07:58.878165 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:08:00.871325 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:08:00.952332 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 14:08:00.952332 11160 solver.cpp:397]     Test net output #1: loss = 0.311134 (* 1 = 0.311134 loss)
I1107 14:08:01.034333 11160 solver.cpp:218] Iteration 153500 (9.37809 iter/s, 10.6632s/100 iters), loss = 0.0276155
I1107 14:08:01.034333 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:08:01.034333 11160 solver.cpp:237]     Train net output #1: loss = 0.0276155 (* 1 = 0.0276155 loss)
I1107 14:08:01.034333 11160 sgd_solver.cpp:105] Iteration 153500, lr = 0.0001
I1107 14:08:09.700197 11160 solver.cpp:218] Iteration 153600 (11.5401 iter/s, 8.66544s/100 iters), loss = 0.0215198
I1107 14:08:09.700197 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:08:09.700197 11160 solver.cpp:237]     Train net output #1: loss = 0.0215197 (* 1 = 0.0215197 loss)
I1107 14:08:09.700197 11160 sgd_solver.cpp:105] Iteration 153600, lr = 0.0001
I1107 14:08:18.483675 11160 solver.cpp:218] Iteration 153700 (11.3862 iter/s, 8.78257s/100 iters), loss = 0.0239089
I1107 14:08:18.483675 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:08:18.483675 11160 solver.cpp:237]     Train net output #1: loss = 0.0239089 (* 1 = 0.0239089 loss)
I1107 14:08:18.483675 11160 sgd_solver.cpp:105] Iteration 153700, lr = 0.0001
I1107 14:08:27.283582 11160 solver.cpp:218] Iteration 153800 (11.3643 iter/s, 8.79946s/100 iters), loss = 0.0171276
I1107 14:08:27.283582 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:08:27.283582 11160 solver.cpp:237]     Train net output #1: loss = 0.0171276 (* 1 = 0.0171276 loss)
I1107 14:08:27.283582 11160 sgd_solver.cpp:105] Iteration 153800, lr = 0.0001
I1107 14:08:36.079764 11160 solver.cpp:218] Iteration 153900 (11.3689 iter/s, 8.79595s/100 iters), loss = 0.0231704
I1107 14:08:36.080265 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:08:36.080265 11160 solver.cpp:237]     Train net output #1: loss = 0.0231704 (* 1 = 0.0231704 loss)
I1107 14:08:36.080265 11160 sgd_solver.cpp:105] Iteration 153900, lr = 0.0001
I1107 14:08:44.403826  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:08:44.753325 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_154000.caffemodel
I1107 14:08:44.789827 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_154000.solverstate
I1107 14:08:44.798826 11160 solver.cpp:330] Iteration 154000, Testing net (#0)
I1107 14:08:44.799326 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:08:46.844826 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:08:46.926826 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 14:08:46.926826 11160 solver.cpp:397]     Test net output #1: loss = 0.310212 (* 1 = 0.310212 loss)
I1107 14:08:47.011327 11160 solver.cpp:218] Iteration 154000 (9.14877 iter/s, 10.9304s/100 iters), loss = 0.0365563
I1107 14:08:47.011327 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:08:47.011327 11160 solver.cpp:237]     Train net output #1: loss = 0.0365563 (* 1 = 0.0365563 loss)
I1107 14:08:47.011327 11160 sgd_solver.cpp:105] Iteration 154000, lr = 0.0001
I1107 14:08:55.817715 11160 solver.cpp:218] Iteration 154100 (11.3561 iter/s, 8.8058s/100 iters), loss = 0.0286182
I1107 14:08:55.817715 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:08:55.817715 11160 solver.cpp:237]     Train net output #1: loss = 0.0286182 (* 1 = 0.0286182 loss)
I1107 14:08:55.817715 11160 sgd_solver.cpp:105] Iteration 154100, lr = 0.0001
I1107 14:09:04.604638 11160 solver.cpp:218] Iteration 154200 (11.3808 iter/s, 8.78672s/100 iters), loss = 0.0183224
I1107 14:09:04.604638 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:09:04.604638 11160 solver.cpp:237]     Train net output #1: loss = 0.0183224 (* 1 = 0.0183224 loss)
I1107 14:09:04.604638 11160 sgd_solver.cpp:105] Iteration 154200, lr = 0.0001
I1107 14:09:13.363517 11160 solver.cpp:218] Iteration 154300 (11.4176 iter/s, 8.7584s/100 iters), loss = 0.0196987
I1107 14:09:13.364032 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:09:13.364032 11160 solver.cpp:237]     Train net output #1: loss = 0.0196987 (* 1 = 0.0196987 loss)
I1107 14:09:13.364032 11160 sgd_solver.cpp:105] Iteration 154300, lr = 0.0001
I1107 14:09:22.137503 11160 solver.cpp:218] Iteration 154400 (11.3983 iter/s, 8.77324s/100 iters), loss = 0.0277549
I1107 14:09:22.137503 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:09:22.137503 11160 solver.cpp:237]     Train net output #1: loss = 0.0277549 (* 1 = 0.0277549 loss)
I1107 14:09:22.137503 11160 sgd_solver.cpp:105] Iteration 154400, lr = 0.0001
I1107 14:09:30.494717  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:09:30.840718 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_154500.caffemodel
I1107 14:09:30.869719 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_154500.solverstate
I1107 14:09:30.878717 11160 solver.cpp:330] Iteration 154500, Testing net (#0)
I1107 14:09:30.878717 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:09:32.914280 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:09:32.996284 11160 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 14:09:32.996284 11160 solver.cpp:397]     Test net output #1: loss = 0.31058 (* 1 = 0.31058 loss)
I1107 14:09:33.079293 11160 solver.cpp:218] Iteration 154500 (9.13977 iter/s, 10.9412s/100 iters), loss = 0.0282233
I1107 14:09:33.079293 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:09:33.079293 11160 solver.cpp:237]     Train net output #1: loss = 0.0282233 (* 1 = 0.0282233 loss)
I1107 14:09:33.079293 11160 sgd_solver.cpp:105] Iteration 154500, lr = 0.0001
I1107 14:09:41.855904 11160 solver.cpp:218] Iteration 154600 (11.3949 iter/s, 8.77583s/100 iters), loss = 0.0363416
I1107 14:09:41.855904 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:09:41.855904 11160 solver.cpp:237]     Train net output #1: loss = 0.0363416 (* 1 = 0.0363416 loss)
I1107 14:09:41.855904 11160 sgd_solver.cpp:105] Iteration 154600, lr = 0.0001
I1107 14:09:50.656189 11160 solver.cpp:218] Iteration 154700 (11.3638 iter/s, 8.79991s/100 iters), loss = 0.0241928
I1107 14:09:50.656189 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:09:50.656690 11160 solver.cpp:237]     Train net output #1: loss = 0.0241928 (* 1 = 0.0241928 loss)
I1107 14:09:50.656690 11160 sgd_solver.cpp:105] Iteration 154700, lr = 0.0001
I1107 14:09:59.454757 11160 solver.cpp:218] Iteration 154800 (11.3664 iter/s, 8.79783s/100 iters), loss = 0.0207303
I1107 14:09:59.454757 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:09:59.454757 11160 solver.cpp:237]     Train net output #1: loss = 0.0207303 (* 1 = 0.0207303 loss)
I1107 14:09:59.454757 11160 sgd_solver.cpp:105] Iteration 154800, lr = 0.0001
I1107 14:10:08.271205 11160 solver.cpp:218] Iteration 154900 (11.3434 iter/s, 8.81569s/100 iters), loss = 0.0349793
I1107 14:10:08.271205 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:10:08.271205 11160 solver.cpp:237]     Train net output #1: loss = 0.0349792 (* 1 = 0.0349792 loss)
I1107 14:10:08.271205 11160 sgd_solver.cpp:105] Iteration 154900, lr = 0.0001
I1107 14:10:16.504643  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:10:16.841691 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_155000.caffemodel
I1107 14:10:16.871690 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_155000.solverstate
I1107 14:10:16.880692 11160 solver.cpp:330] Iteration 155000, Testing net (#0)
I1107 14:10:16.880692 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:10:18.883947 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:10:18.962956 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 14:10:18.962956 11160 solver.cpp:397]     Test net output #1: loss = 0.310207 (* 1 = 0.310207 loss)
I1107 14:10:19.044960 11160 solver.cpp:218] Iteration 155000 (9.282 iter/s, 10.7735s/100 iters), loss = 0.0278981
I1107 14:10:19.044960 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:10:19.044960 11160 solver.cpp:237]     Train net output #1: loss = 0.0278981 (* 1 = 0.0278981 loss)
I1107 14:10:19.044960 11160 sgd_solver.cpp:105] Iteration 155000, lr = 0.0001
I1107 14:10:27.598269 11160 solver.cpp:218] Iteration 155100 (11.6914 iter/s, 8.55329s/100 iters), loss = 0.0345654
I1107 14:10:27.599256 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:10:27.599256 11160 solver.cpp:237]     Train net output #1: loss = 0.0345654 (* 1 = 0.0345654 loss)
I1107 14:10:27.599256 11160 sgd_solver.cpp:105] Iteration 155100, lr = 0.0001
I1107 14:10:36.148489 11160 solver.cpp:218] Iteration 155200 (11.6974 iter/s, 8.54893s/100 iters), loss = 0.0190825
I1107 14:10:36.148489 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:10:36.148489 11160 solver.cpp:237]     Train net output #1: loss = 0.0190825 (* 1 = 0.0190825 loss)
I1107 14:10:36.148489 11160 sgd_solver.cpp:105] Iteration 155200, lr = 0.0001
I1107 14:10:44.689400 11160 solver.cpp:218] Iteration 155300 (11.7092 iter/s, 8.54031s/100 iters), loss = 0.0197823
I1107 14:10:44.689400 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:10:44.689400 11160 solver.cpp:237]     Train net output #1: loss = 0.0197823 (* 1 = 0.0197823 loss)
I1107 14:10:44.689400 11160 sgd_solver.cpp:105] Iteration 155300, lr = 0.0001
I1107 14:10:53.250335 11160 solver.cpp:218] Iteration 155400 (11.6812 iter/s, 8.56074s/100 iters), loss = 0.0240695
I1107 14:10:53.250335 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:10:53.250335 11160 solver.cpp:237]     Train net output #1: loss = 0.0240695 (* 1 = 0.0240695 loss)
I1107 14:10:53.250335 11160 sgd_solver.cpp:105] Iteration 155400, lr = 0.0001
I1107 14:11:01.372190  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:11:01.710206 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_155500.caffemodel
I1107 14:11:01.740212 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_155500.solverstate
I1107 14:11:01.749213 11160 solver.cpp:330] Iteration 155500, Testing net (#0)
I1107 14:11:01.749213 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:11:03.738572 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:11:03.818576 11160 solver.cpp:397]     Test net output #0: accuracy = 0.92
I1107 14:11:03.818576 11160 solver.cpp:397]     Test net output #1: loss = 0.309823 (* 1 = 0.309823 loss)
I1107 14:11:03.900593 11160 solver.cpp:218] Iteration 155500 (9.39018 iter/s, 10.6494s/100 iters), loss = 0.0304866
I1107 14:11:03.900593 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:11:03.900593 11160 solver.cpp:237]     Train net output #1: loss = 0.0304866 (* 1 = 0.0304866 loss)
I1107 14:11:03.900593 11160 sgd_solver.cpp:105] Iteration 155500, lr = 0.0001
I1107 14:11:12.451974 11160 solver.cpp:218] Iteration 155600 (11.6945 iter/s, 8.55106s/100 iters), loss = 0.0232264
I1107 14:11:12.451974 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:11:12.451974 11160 solver.cpp:237]     Train net output #1: loss = 0.0232264 (* 1 = 0.0232264 loss)
I1107 14:11:12.451974 11160 sgd_solver.cpp:105] Iteration 155600, lr = 0.0001
I1107 14:11:21.002810 11160 solver.cpp:218] Iteration 155700 (11.6957 iter/s, 8.55016s/100 iters), loss = 0.0232325
I1107 14:11:21.002810 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:11:21.002810 11160 solver.cpp:237]     Train net output #1: loss = 0.0232325 (* 1 = 0.0232325 loss)
I1107 14:11:21.002810 11160 sgd_solver.cpp:105] Iteration 155700, lr = 0.0001
I1107 14:11:29.560292 11160 solver.cpp:218] Iteration 155800 (11.6864 iter/s, 8.55699s/100 iters), loss = 0.0214276
I1107 14:11:29.560292 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:11:29.560292 11160 solver.cpp:237]     Train net output #1: loss = 0.0214276 (* 1 = 0.0214276 loss)
I1107 14:11:29.560292 11160 sgd_solver.cpp:105] Iteration 155800, lr = 0.0001
I1107 14:11:38.105101 11160 solver.cpp:218] Iteration 155900 (11.7031 iter/s, 8.54477s/100 iters), loss = 0.0261823
I1107 14:11:38.105101 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:11:38.105101 11160 solver.cpp:237]     Train net output #1: loss = 0.0261822 (* 1 = 0.0261822 loss)
I1107 14:11:38.105101 11160 sgd_solver.cpp:105] Iteration 155900, lr = 0.0001
I1107 14:11:46.266489  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:11:46.604001 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_156000.caffemodel
I1107 14:11:46.633002 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_156000.solverstate
I1107 14:11:46.642002 11160 solver.cpp:330] Iteration 156000, Testing net (#0)
I1107 14:11:46.642002 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:11:48.638599 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:11:48.718621 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1107 14:11:48.718621 11160 solver.cpp:397]     Test net output #1: loss = 0.309631 (* 1 = 0.309631 loss)
I1107 14:11:48.800122 11160 solver.cpp:218] Iteration 156000 (9.35129 iter/s, 10.6937s/100 iters), loss = 0.0475932
I1107 14:11:48.800122 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:11:48.800122 11160 solver.cpp:237]     Train net output #1: loss = 0.0475932 (* 1 = 0.0475932 loss)
I1107 14:11:48.800122 11160 sgd_solver.cpp:105] Iteration 156000, lr = 0.0001
I1107 14:11:57.356222 11160 solver.cpp:218] Iteration 156100 (11.6884 iter/s, 8.5555s/100 iters), loss = 0.0303273
I1107 14:11:57.356222 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:11:57.356222 11160 solver.cpp:237]     Train net output #1: loss = 0.0303272 (* 1 = 0.0303272 loss)
I1107 14:11:57.356222 11160 sgd_solver.cpp:105] Iteration 156100, lr = 0.0001
I1107 14:12:05.913164 11160 solver.cpp:218] Iteration 156200 (11.6863 iter/s, 8.55704s/100 iters), loss = 0.0171571
I1107 14:12:05.913164 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:12:05.913164 11160 solver.cpp:237]     Train net output #1: loss = 0.0171571 (* 1 = 0.0171571 loss)
I1107 14:12:05.913164 11160 sgd_solver.cpp:105] Iteration 156200, lr = 0.0001
I1107 14:12:14.472970 11160 solver.cpp:218] Iteration 156300 (11.6836 iter/s, 8.55904s/100 iters), loss = 0.0199019
I1107 14:12:14.472970 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:12:14.472970 11160 solver.cpp:237]     Train net output #1: loss = 0.0199019 (* 1 = 0.0199019 loss)
I1107 14:12:14.472970 11160 sgd_solver.cpp:105] Iteration 156300, lr = 0.0001
I1107 14:12:23.009958 11160 solver.cpp:218] Iteration 156400 (11.714 iter/s, 8.53678s/100 iters), loss = 0.0256169
I1107 14:12:23.009958 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:12:23.009958 11160 solver.cpp:237]     Train net output #1: loss = 0.0256169 (* 1 = 0.0256169 loss)
I1107 14:12:23.009958 11160 sgd_solver.cpp:105] Iteration 156400, lr = 0.0001
I1107 14:12:31.131067  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:12:31.467105 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_156500.caffemodel
I1107 14:12:31.498108 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_156500.solverstate
I1107 14:12:31.507091 11160 solver.cpp:330] Iteration 156500, Testing net (#0)
I1107 14:12:31.507091 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:12:33.495816 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:12:33.574316 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9196
I1107 14:12:33.575317 11160 solver.cpp:397]     Test net output #1: loss = 0.309639 (* 1 = 0.309639 loss)
I1107 14:12:33.656319 11160 solver.cpp:218] Iteration 156500 (9.3934 iter/s, 10.6458s/100 iters), loss = 0.0293193
I1107 14:12:33.656319 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:12:33.656319 11160 solver.cpp:237]     Train net output #1: loss = 0.0293193 (* 1 = 0.0293193 loss)
I1107 14:12:33.656319 11160 sgd_solver.cpp:105] Iteration 156500, lr = 0.0001
I1107 14:12:42.171650 11160 solver.cpp:218] Iteration 156600 (11.7447 iter/s, 8.51445s/100 iters), loss = 0.0242764
I1107 14:12:42.171650 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:12:42.171650 11160 solver.cpp:237]     Train net output #1: loss = 0.0242764 (* 1 = 0.0242764 loss)
I1107 14:12:42.171650 11160 sgd_solver.cpp:105] Iteration 156600, lr = 0.0001
I1107 14:12:50.711524 11160 solver.cpp:218] Iteration 156700 (11.7106 iter/s, 8.53925s/100 iters), loss = 0.0198943
I1107 14:12:50.711524 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:12:50.711524 11160 solver.cpp:237]     Train net output #1: loss = 0.0198943 (* 1 = 0.0198943 loss)
I1107 14:12:50.711524 11160 sgd_solver.cpp:105] Iteration 156700, lr = 0.0001
I1107 14:12:59.275142 11160 solver.cpp:218] Iteration 156800 (11.677 iter/s, 8.56381s/100 iters), loss = 0.0233231
I1107 14:12:59.275142 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:12:59.276144 11160 solver.cpp:237]     Train net output #1: loss = 0.0233231 (* 1 = 0.0233231 loss)
I1107 14:12:59.276144 11160 sgd_solver.cpp:105] Iteration 156800, lr = 0.0001
I1107 14:13:07.923972 11160 solver.cpp:218] Iteration 156900 (11.5631 iter/s, 8.6482s/100 iters), loss = 0.0276727
I1107 14:13:07.923972 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:13:07.923972 11160 solver.cpp:237]     Train net output #1: loss = 0.0276727 (* 1 = 0.0276727 loss)
I1107 14:13:07.923972 11160 sgd_solver.cpp:105] Iteration 156900, lr = 0.0001
I1107 14:13:16.141019  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:13:16.482367 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_157000.caffemodel
I1107 14:13:16.514365 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_157000.solverstate
I1107 14:13:16.523370 11160 solver.cpp:330] Iteration 157000, Testing net (#0)
I1107 14:13:16.523370 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:13:18.526105 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:13:18.606606 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 14:13:18.606606 11160 solver.cpp:397]     Test net output #1: loss = 0.310259 (* 1 = 0.310259 loss)
I1107 14:13:18.689616 11160 solver.cpp:218] Iteration 157000 (9.28989 iter/s, 10.7644s/100 iters), loss = 0.0269163
I1107 14:13:18.689616 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:13:18.689616 11160 solver.cpp:237]     Train net output #1: loss = 0.0269163 (* 1 = 0.0269163 loss)
I1107 14:13:18.689616 11160 sgd_solver.cpp:105] Iteration 157000, lr = 0.0001
I1107 14:13:27.274593 11160 solver.cpp:218] Iteration 157100 (11.6483 iter/s, 8.58496s/100 iters), loss = 0.0228324
I1107 14:13:27.274593 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:13:27.274593 11160 solver.cpp:237]     Train net output #1: loss = 0.0228324 (* 1 = 0.0228324 loss)
I1107 14:13:27.274593 11160 sgd_solver.cpp:105] Iteration 157100, lr = 0.0001
I1107 14:13:35.858760 11160 solver.cpp:218] Iteration 157200 (11.6508 iter/s, 8.58308s/100 iters), loss = 0.0183653
I1107 14:13:35.858760 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:13:35.858760 11160 solver.cpp:237]     Train net output #1: loss = 0.0183653 (* 1 = 0.0183653 loss)
I1107 14:13:35.858760 11160 sgd_solver.cpp:105] Iteration 157200, lr = 0.0001
I1107 14:13:44.413751 11160 solver.cpp:218] Iteration 157300 (11.6891 iter/s, 8.55501s/100 iters), loss = 0.0185336
I1107 14:13:44.413751 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:13:44.413751 11160 solver.cpp:237]     Train net output #1: loss = 0.0185336 (* 1 = 0.0185336 loss)
I1107 14:13:44.413751 11160 sgd_solver.cpp:105] Iteration 157300, lr = 0.0001
I1107 14:13:52.960111 11160 solver.cpp:218] Iteration 157400 (11.7015 iter/s, 8.54591s/100 iters), loss = 0.0262695
I1107 14:13:52.960111 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:13:52.960111 11160 solver.cpp:237]     Train net output #1: loss = 0.0262695 (* 1 = 0.0262695 loss)
I1107 14:13:52.960111 11160 sgd_solver.cpp:105] Iteration 157400, lr = 0.0001
I1107 14:14:01.138402  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:14:01.476408 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_157500.caffemodel
I1107 14:14:01.506409 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_157500.solverstate
I1107 14:14:01.515413 11160 solver.cpp:330] Iteration 157500, Testing net (#0)
I1107 14:14:01.515413 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:14:03.565632 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:14:03.646641 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 14:14:03.647637 11160 solver.cpp:397]     Test net output #1: loss = 0.309932 (* 1 = 0.309932 loss)
I1107 14:14:03.732655 11160 solver.cpp:218] Iteration 157500 (9.28328 iter/s, 10.7721s/100 iters), loss = 0.0276786
I1107 14:14:03.732655 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:14:03.733654 11160 solver.cpp:237]     Train net output #1: loss = 0.0276786 (* 1 = 0.0276786 loss)
I1107 14:14:03.733654 11160 sgd_solver.cpp:105] Iteration 157500, lr = 0.0001
I1107 14:14:12.460297 11160 solver.cpp:218] Iteration 157600 (11.4593 iter/s, 8.72651s/100 iters), loss = 0.0264072
I1107 14:14:12.460297 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:14:12.460297 11160 solver.cpp:237]     Train net output #1: loss = 0.0264072 (* 1 = 0.0264072 loss)
I1107 14:14:12.460297 11160 sgd_solver.cpp:105] Iteration 157600, lr = 0.0001
I1107 14:14:21.126919 11160 solver.cpp:218] Iteration 157700 (11.5385 iter/s, 8.66667s/100 iters), loss = 0.0217128
I1107 14:14:21.127919 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:14:21.127919 11160 solver.cpp:237]     Train net output #1: loss = 0.0217128 (* 1 = 0.0217128 loss)
I1107 14:14:21.127919 11160 sgd_solver.cpp:105] Iteration 157700, lr = 0.0001
I1107 14:14:29.724850 11160 solver.cpp:218] Iteration 157800 (11.6325 iter/s, 8.59657s/100 iters), loss = 0.0158294
I1107 14:14:29.724850 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:14:29.724850 11160 solver.cpp:237]     Train net output #1: loss = 0.0158294 (* 1 = 0.0158294 loss)
I1107 14:14:29.724850 11160 sgd_solver.cpp:105] Iteration 157800, lr = 0.0001
I1107 14:14:38.280767 11160 solver.cpp:218] Iteration 157900 (11.6877 iter/s, 8.55601s/100 iters), loss = 0.0251894
I1107 14:14:38.280767 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:14:38.280767 11160 solver.cpp:237]     Train net output #1: loss = 0.0251894 (* 1 = 0.0251894 loss)
I1107 14:14:38.280767 11160 sgd_solver.cpp:105] Iteration 157900, lr = 0.0001
I1107 14:14:46.448760  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:14:46.787775 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_158000.caffemodel
I1107 14:14:46.817782 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_158000.solverstate
I1107 14:14:46.826782 11160 solver.cpp:330] Iteration 158000, Testing net (#0)
I1107 14:14:46.826782 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:14:48.825909 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:14:48.905912 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 14:14:48.905912 11160 solver.cpp:397]     Test net output #1: loss = 0.309523 (* 1 = 0.309523 loss)
I1107 14:14:48.987915 11160 solver.cpp:218] Iteration 158000 (9.34052 iter/s, 10.706s/100 iters), loss = 0.0255461
I1107 14:14:48.987915 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:14:48.987915 11160 solver.cpp:237]     Train net output #1: loss = 0.025546 (* 1 = 0.025546 loss)
I1107 14:14:48.987915 11160 sgd_solver.cpp:105] Iteration 158000, lr = 0.0001
I1107 14:14:57.586529 11160 solver.cpp:218] Iteration 158100 (11.6308 iter/s, 8.59789s/100 iters), loss = 0.0230214
I1107 14:14:57.586529 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:14:57.586529 11160 solver.cpp:237]     Train net output #1: loss = 0.0230214 (* 1 = 0.0230214 loss)
I1107 14:14:57.586529 11160 sgd_solver.cpp:105] Iteration 158100, lr = 0.0001
I1107 14:15:06.248437 11160 solver.cpp:218] Iteration 158200 (11.5446 iter/s, 8.66203s/100 iters), loss = 0.0315332
I1107 14:15:06.248437 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:15:06.248437 11160 solver.cpp:237]     Train net output #1: loss = 0.0315332 (* 1 = 0.0315332 loss)
I1107 14:15:06.248437 11160 sgd_solver.cpp:105] Iteration 158200, lr = 0.0001
I1107 14:15:14.798146 11160 solver.cpp:218] Iteration 158300 (11.6977 iter/s, 8.54871s/100 iters), loss = 0.0184258
I1107 14:15:14.798146 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:15:14.798146 11160 solver.cpp:237]     Train net output #1: loss = 0.0184257 (* 1 = 0.0184257 loss)
I1107 14:15:14.798146 11160 sgd_solver.cpp:105] Iteration 158300, lr = 0.0001
I1107 14:15:23.350740 11160 solver.cpp:218] Iteration 158400 (11.6922 iter/s, 8.5527s/100 iters), loss = 0.0230899
I1107 14:15:23.351740 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:15:23.351740 11160 solver.cpp:237]     Train net output #1: loss = 0.0230899 (* 1 = 0.0230899 loss)
I1107 14:15:23.351740 11160 sgd_solver.cpp:105] Iteration 158400, lr = 0.0001
I1107 14:15:31.476382  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:15:31.813436 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_158500.caffemodel
I1107 14:15:31.844434 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_158500.solverstate
I1107 14:15:31.853436 11160 solver.cpp:330] Iteration 158500, Testing net (#0)
I1107 14:15:31.853436 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:15:33.843560 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:15:33.923624 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9197
I1107 14:15:33.923624 11160 solver.cpp:397]     Test net output #1: loss = 0.308662 (* 1 = 0.308662 loss)
I1107 14:15:34.004597 11160 solver.cpp:218] Iteration 158500 (9.38751 iter/s, 10.6525s/100 iters), loss = 0.0266263
I1107 14:15:34.004597 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:15:34.004597 11160 solver.cpp:237]     Train net output #1: loss = 0.0266263 (* 1 = 0.0266263 loss)
I1107 14:15:34.004597 11160 sgd_solver.cpp:105] Iteration 158500, lr = 0.0001
I1107 14:15:42.539775 11160 solver.cpp:218] Iteration 158600 (11.716 iter/s, 8.53531s/100 iters), loss = 0.0450761
I1107 14:15:42.539775 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:15:42.539775 11160 solver.cpp:237]     Train net output #1: loss = 0.0450761 (* 1 = 0.0450761 loss)
I1107 14:15:42.539775 11160 sgd_solver.cpp:105] Iteration 158600, lr = 0.0001
I1107 14:15:51.084619 11160 solver.cpp:218] Iteration 158700 (11.7044 iter/s, 8.54376s/100 iters), loss = 0.0194689
I1107 14:15:51.084619 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:15:51.084619 11160 solver.cpp:237]     Train net output #1: loss = 0.0194689 (* 1 = 0.0194689 loss)
I1107 14:15:51.084619 11160 sgd_solver.cpp:105] Iteration 158700, lr = 0.0001
I1107 14:15:59.617172 11160 solver.cpp:218] Iteration 158800 (11.7198 iter/s, 8.53254s/100 iters), loss = 0.0224351
I1107 14:15:59.617172 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:15:59.617172 11160 solver.cpp:237]     Train net output #1: loss = 0.022435 (* 1 = 0.022435 loss)
I1107 14:15:59.617172 11160 sgd_solver.cpp:105] Iteration 158800, lr = 0.0001
I1107 14:16:08.148717 11160 solver.cpp:218] Iteration 158900 (11.7228 iter/s, 8.53042s/100 iters), loss = 0.0300563
I1107 14:16:08.148717 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:16:08.148717 11160 solver.cpp:237]     Train net output #1: loss = 0.0300563 (* 1 = 0.0300563 loss)
I1107 14:16:08.148717 11160 sgd_solver.cpp:105] Iteration 158900, lr = 0.0001
I1107 14:16:16.266993  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:16:16.603509 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_159000.caffemodel
I1107 14:16:16.632014 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_159000.solverstate
I1107 14:16:16.641013 11160 solver.cpp:330] Iteration 159000, Testing net (#0)
I1107 14:16:16.641013 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:16:18.631199 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:16:18.711203 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 14:16:18.711203 11160 solver.cpp:397]     Test net output #1: loss = 0.308648 (* 1 = 0.308648 loss)
I1107 14:16:18.792711 11160 solver.cpp:218] Iteration 159000 (9.39541 iter/s, 10.6435s/100 iters), loss = 0.0364099
I1107 14:16:18.792711 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:16:18.792711 11160 solver.cpp:237]     Train net output #1: loss = 0.0364099 (* 1 = 0.0364099 loss)
I1107 14:16:18.792711 11160 sgd_solver.cpp:105] Iteration 159000, lr = 0.0001
I1107 14:16:27.325191 11160 solver.cpp:218] Iteration 159100 (11.7199 iter/s, 8.5325s/100 iters), loss = 0.0205879
I1107 14:16:27.325191 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:16:27.325191 11160 solver.cpp:237]     Train net output #1: loss = 0.0205879 (* 1 = 0.0205879 loss)
I1107 14:16:27.325191 11160 sgd_solver.cpp:105] Iteration 159100, lr = 0.0001
I1107 14:16:35.853843 11160 solver.cpp:218] Iteration 159200 (11.7259 iter/s, 8.5281s/100 iters), loss = 0.0187017
I1107 14:16:35.853843 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:16:35.853843 11160 solver.cpp:237]     Train net output #1: loss = 0.0187017 (* 1 = 0.0187017 loss)
I1107 14:16:35.853843 11160 sgd_solver.cpp:105] Iteration 159200, lr = 0.0001
I1107 14:16:44.397658 11160 solver.cpp:218] Iteration 159300 (11.706 iter/s, 8.54262s/100 iters), loss = 0.0245993
I1107 14:16:44.397658 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:16:44.397658 11160 solver.cpp:237]     Train net output #1: loss = 0.0245993 (* 1 = 0.0245993 loss)
I1107 14:16:44.397658 11160 sgd_solver.cpp:105] Iteration 159300, lr = 0.0001
I1107 14:16:52.953331 11160 solver.cpp:218] Iteration 159400 (11.6885 iter/s, 8.5554s/100 iters), loss = 0.0339769
I1107 14:16:52.953331 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:16:52.953331 11160 solver.cpp:237]     Train net output #1: loss = 0.0339769 (* 1 = 0.0339769 loss)
I1107 14:16:52.953331 11160 sgd_solver.cpp:105] Iteration 159400, lr = 0.0001
I1107 14:17:01.080554  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:17:01.418581 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_159500.caffemodel
I1107 14:17:01.448581 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_159500.solverstate
I1107 14:17:01.456581 11160 solver.cpp:330] Iteration 159500, Testing net (#0)
I1107 14:17:01.456581 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:17:03.449755 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:17:03.530254 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9198
I1107 14:17:03.530254 11160 solver.cpp:397]     Test net output #1: loss = 0.309957 (* 1 = 0.309957 loss)
I1107 14:17:03.611253 11160 solver.cpp:218] Iteration 159500 (9.38347 iter/s, 10.657s/100 iters), loss = 0.0272149
I1107 14:17:03.611253 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:17:03.611253 11160 solver.cpp:237]     Train net output #1: loss = 0.0272149 (* 1 = 0.0272149 loss)
I1107 14:17:03.611253 11160 sgd_solver.cpp:105] Iteration 159500, lr = 0.0001
I1107 14:17:12.150146 11160 solver.cpp:218] Iteration 159600 (11.7117 iter/s, 8.53848s/100 iters), loss = 0.02546
I1107 14:17:12.150146 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:17:12.150146 11160 solver.cpp:237]     Train net output #1: loss = 0.0254599 (* 1 = 0.0254599 loss)
I1107 14:17:12.150146 11160 sgd_solver.cpp:105] Iteration 159600, lr = 0.0001
I1107 14:17:20.682521 11160 solver.cpp:218] Iteration 159700 (11.7201 iter/s, 8.53232s/100 iters), loss = 0.023458
I1107 14:17:20.682521 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:17:20.682521 11160 solver.cpp:237]     Train net output #1: loss = 0.023458 (* 1 = 0.023458 loss)
I1107 14:17:20.682521 11160 sgd_solver.cpp:105] Iteration 159700, lr = 0.0001
I1107 14:17:29.231585 11160 solver.cpp:218] Iteration 159800 (11.6984 iter/s, 8.54815s/100 iters), loss = 0.0187969
I1107 14:17:29.231585 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:17:29.231585 11160 solver.cpp:237]     Train net output #1: loss = 0.0187969 (* 1 = 0.0187969 loss)
I1107 14:17:29.231585 11160 sgd_solver.cpp:105] Iteration 159800, lr = 0.0001
I1107 14:17:37.767293 11160 solver.cpp:218] Iteration 159900 (11.7162 iter/s, 8.53517s/100 iters), loss = 0.0212603
I1107 14:17:37.767293 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:17:37.767293 11160 solver.cpp:237]     Train net output #1: loss = 0.0212603 (* 1 = 0.0212603 loss)
I1107 14:17:37.767293 11160 sgd_solver.cpp:105] Iteration 159900, lr = 0.0001
I1107 14:17:45.882761  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:17:46.219909 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_160000.caffemodel
I1107 14:17:46.246498 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_160000.solverstate
I1107 14:17:46.255512 11160 solver.cpp:330] Iteration 160000, Testing net (#0)
I1107 14:17:46.255512 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:17:48.244825 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:17:48.324842 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1107 14:17:48.325341 11160 solver.cpp:397]     Test net output #1: loss = 0.310545 (* 1 = 0.310545 loss)
I1107 14:17:48.406360 11160 solver.cpp:218] Iteration 160000 (9.39961 iter/s, 10.6387s/100 iters), loss = 0.0268505
I1107 14:17:48.406360 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:17:48.406360 11160 solver.cpp:237]     Train net output #1: loss = 0.0268504 (* 1 = 0.0268504 loss)
I1107 14:17:48.406360 11160 sgd_solver.cpp:105] Iteration 160000, lr = 0.0001
I1107 14:17:56.930932 11160 solver.cpp:218] Iteration 160100 (11.7323 iter/s, 8.52345s/100 iters), loss = 0.0207478
I1107 14:17:56.930932 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:17:56.930932 11160 solver.cpp:237]     Train net output #1: loss = 0.0207477 (* 1 = 0.0207477 loss)
I1107 14:17:56.930932 11160 sgd_solver.cpp:105] Iteration 160100, lr = 0.0001
I1107 14:18:05.465863 11160 solver.cpp:218] Iteration 160200 (11.716 iter/s, 8.53535s/100 iters), loss = 0.0231214
I1107 14:18:05.465863 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:18:05.465863 11160 solver.cpp:237]     Train net output #1: loss = 0.0231214 (* 1 = 0.0231214 loss)
I1107 14:18:05.466864 11160 sgd_solver.cpp:105] Iteration 160200, lr = 0.0001
I1107 14:18:13.996018 11160 solver.cpp:218] Iteration 160300 (11.7244 iter/s, 8.5292s/100 iters), loss = 0.0180237
I1107 14:18:13.996018 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:18:13.996018 11160 solver.cpp:237]     Train net output #1: loss = 0.0180237 (* 1 = 0.0180237 loss)
I1107 14:18:13.996018 11160 sgd_solver.cpp:105] Iteration 160300, lr = 0.0001
I1107 14:18:22.536767 11160 solver.cpp:218] Iteration 160400 (11.7098 iter/s, 8.53989s/100 iters), loss = 0.0295
I1107 14:18:22.536767 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:18:22.536767 11160 solver.cpp:237]     Train net output #1: loss = 0.0294999 (* 1 = 0.0294999 loss)
I1107 14:18:22.536767 11160 sgd_solver.cpp:105] Iteration 160400, lr = 0.0001
I1107 14:18:30.650058  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:18:30.987077 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_160500.caffemodel
I1107 14:18:31.017081 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_160500.solverstate
I1107 14:18:31.026079 11160 solver.cpp:330] Iteration 160500, Testing net (#0)
I1107 14:18:31.026079 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:18:33.016311 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:18:33.096315 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9204
I1107 14:18:33.096315 11160 solver.cpp:397]     Test net output #1: loss = 0.309133 (* 1 = 0.309133 loss)
I1107 14:18:33.176321 11160 solver.cpp:218] Iteration 160500 (9.39868 iter/s, 10.6398s/100 iters), loss = 0.0239989
I1107 14:18:33.176321 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:18:33.177321 11160 solver.cpp:237]     Train net output #1: loss = 0.0239989 (* 1 = 0.0239989 loss)
I1107 14:18:33.177321 11160 sgd_solver.cpp:105] Iteration 160500, lr = 0.0001
I1107 14:18:41.714427 11160 solver.cpp:218] Iteration 160600 (11.7135 iter/s, 8.53719s/100 iters), loss = 0.0314383
I1107 14:18:41.714427 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:18:41.714427 11160 solver.cpp:237]     Train net output #1: loss = 0.0314383 (* 1 = 0.0314383 loss)
I1107 14:18:41.714427 11160 sgd_solver.cpp:105] Iteration 160600, lr = 0.0001
I1107 14:18:50.240820 11160 solver.cpp:218] Iteration 160700 (11.7296 iter/s, 8.52542s/100 iters), loss = 0.0224677
I1107 14:18:50.240820 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:18:50.240820 11160 solver.cpp:237]     Train net output #1: loss = 0.0224677 (* 1 = 0.0224677 loss)
I1107 14:18:50.240820 11160 sgd_solver.cpp:105] Iteration 160700, lr = 0.0001
I1107 14:18:58.768301 11160 solver.cpp:218] Iteration 160800 (11.7273 iter/s, 8.52712s/100 iters), loss = 0.0176357
I1107 14:18:58.768301 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:18:58.768301 11160 solver.cpp:237]     Train net output #1: loss = 0.0176357 (* 1 = 0.0176357 loss)
I1107 14:18:58.768301 11160 sgd_solver.cpp:105] Iteration 160800, lr = 0.0001
I1107 14:19:07.292879 11160 solver.cpp:218] Iteration 160900 (11.7315 iter/s, 8.52405s/100 iters), loss = 0.0253104
I1107 14:19:07.292879 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:19:07.292879 11160 solver.cpp:237]     Train net output #1: loss = 0.0253103 (* 1 = 0.0253103 loss)
I1107 14:19:07.292879 11160 sgd_solver.cpp:105] Iteration 160900, lr = 0.0001
I1107 14:19:15.392616  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:19:15.730651 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_161000.caffemodel
I1107 14:19:15.759675 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_161000.solverstate
I1107 14:19:15.767675 11160 solver.cpp:330] Iteration 161000, Testing net (#0)
I1107 14:19:15.768677 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:19:17.757827 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:19:17.837829 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 14:19:17.837829 11160 solver.cpp:397]     Test net output #1: loss = 0.309563 (* 1 = 0.309563 loss)
I1107 14:19:17.919849 11160 solver.cpp:218] Iteration 161000 (9.41049 iter/s, 10.6264s/100 iters), loss = 0.0264705
I1107 14:19:17.919849 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:19:17.919849 11160 solver.cpp:237]     Train net output #1: loss = 0.0264705 (* 1 = 0.0264705 loss)
I1107 14:19:17.919849 11160 sgd_solver.cpp:105] Iteration 161000, lr = 0.0001
I1107 14:19:26.450789 11160 solver.cpp:218] Iteration 161100 (11.7226 iter/s, 8.53053s/100 iters), loss = 0.0253594
I1107 14:19:26.450789 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:19:26.450789 11160 solver.cpp:237]     Train net output #1: loss = 0.0253594 (* 1 = 0.0253594 loss)
I1107 14:19:26.450789 11160 sgd_solver.cpp:105] Iteration 161100, lr = 0.0001
I1107 14:19:34.983909 11160 solver.cpp:218] Iteration 161200 (11.7195 iter/s, 8.53279s/100 iters), loss = 0.0216779
I1107 14:19:34.983909 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:19:34.983909 11160 solver.cpp:237]     Train net output #1: loss = 0.0216779 (* 1 = 0.0216779 loss)
I1107 14:19:34.983909 11160 sgd_solver.cpp:105] Iteration 161200, lr = 0.0001
I1107 14:19:43.516629 11160 solver.cpp:218] Iteration 161300 (11.7209 iter/s, 8.53175s/100 iters), loss = 0.0248846
I1107 14:19:43.516629 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:19:43.516629 11160 solver.cpp:237]     Train net output #1: loss = 0.0248846 (* 1 = 0.0248846 loss)
I1107 14:19:43.516629 11160 sgd_solver.cpp:105] Iteration 161300, lr = 0.0001
I1107 14:19:52.053242 11160 solver.cpp:218] Iteration 161400 (11.7145 iter/s, 8.5364s/100 iters), loss = 0.0234229
I1107 14:19:52.053242 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:19:52.053242 11160 solver.cpp:237]     Train net output #1: loss = 0.0234229 (* 1 = 0.0234229 loss)
I1107 14:19:52.053242 11160 sgd_solver.cpp:105] Iteration 161400, lr = 0.0001
I1107 14:20:00.169144  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:20:00.507743 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_161500.caffemodel
I1107 14:20:00.547248 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_161500.solverstate
I1107 14:20:00.556747 11160 solver.cpp:330] Iteration 161500, Testing net (#0)
I1107 14:20:00.556747 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:20:02.576443 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:20:02.656945 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9206
I1107 14:20:02.656945 11160 solver.cpp:397]     Test net output #1: loss = 0.309504 (* 1 = 0.309504 loss)
I1107 14:20:02.738447 11160 solver.cpp:218] Iteration 161500 (9.35912 iter/s, 10.6848s/100 iters), loss = 0.0260509
I1107 14:20:02.738447 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:20:02.738447 11160 solver.cpp:237]     Train net output #1: loss = 0.0260509 (* 1 = 0.0260509 loss)
I1107 14:20:02.738447 11160 sgd_solver.cpp:105] Iteration 161500, lr = 0.0001
I1107 14:20:11.267482 11160 solver.cpp:218] Iteration 161600 (11.7248 iter/s, 8.52892s/100 iters), loss = 0.0318988
I1107 14:20:11.267482 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:20:11.267482 11160 solver.cpp:237]     Train net output #1: loss = 0.0318988 (* 1 = 0.0318988 loss)
I1107 14:20:11.268483 11160 sgd_solver.cpp:105] Iteration 161600, lr = 0.0001
I1107 14:20:19.792594 11160 solver.cpp:218] Iteration 161700 (11.7307 iter/s, 8.52462s/100 iters), loss = 0.0171242
I1107 14:20:19.792594 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:20:19.792594 11160 solver.cpp:237]     Train net output #1: loss = 0.0171242 (* 1 = 0.0171242 loss)
I1107 14:20:19.792594 11160 sgd_solver.cpp:105] Iteration 161700, lr = 0.0001
I1107 14:20:28.318372 11160 solver.cpp:218] Iteration 161800 (11.73 iter/s, 8.52514s/100 iters), loss = 0.0211651
I1107 14:20:28.318372 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:20:28.318372 11160 solver.cpp:237]     Train net output #1: loss = 0.0211651 (* 1 = 0.0211651 loss)
I1107 14:20:28.318372 11160 sgd_solver.cpp:105] Iteration 161800, lr = 0.0001
I1107 14:20:36.857161 11160 solver.cpp:218] Iteration 161900 (11.7129 iter/s, 8.53763s/100 iters), loss = 0.0290048
I1107 14:20:36.857161 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:20:36.857161 11160 solver.cpp:237]     Train net output #1: loss = 0.0290047 (* 1 = 0.0290047 loss)
I1107 14:20:36.857161 11160 sgd_solver.cpp:105] Iteration 161900, lr = 0.0001
I1107 14:20:45.017885  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:20:45.354404 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_162000.caffemodel
I1107 14:20:45.382926 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_162000.solverstate
I1107 14:20:45.391926 11160 solver.cpp:330] Iteration 162000, Testing net (#0)
I1107 14:20:45.391926 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:20:47.379070 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:20:47.459578 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9206
I1107 14:20:47.459578 11160 solver.cpp:397]     Test net output #1: loss = 0.309587 (* 1 = 0.309587 loss)
I1107 14:20:47.541082 11160 solver.cpp:218] Iteration 162000 (9.36041 iter/s, 10.6833s/100 iters), loss = 0.0338903
I1107 14:20:47.541082 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:20:47.541082 11160 solver.cpp:237]     Train net output #1: loss = 0.0338903 (* 1 = 0.0338903 loss)
I1107 14:20:47.541082 11160 sgd_solver.cpp:105] Iteration 162000, lr = 0.0001
I1107 14:20:56.095237 11160 solver.cpp:218] Iteration 162100 (11.6903 iter/s, 8.55409s/100 iters), loss = 0.0233434
I1107 14:20:56.095237 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:20:56.095237 11160 solver.cpp:237]     Train net output #1: loss = 0.0233434 (* 1 = 0.0233434 loss)
I1107 14:20:56.095237 11160 sgd_solver.cpp:105] Iteration 162100, lr = 0.0001
I1107 14:21:04.763397 11160 solver.cpp:218] Iteration 162200 (11.538 iter/s, 8.66699s/100 iters), loss = 0.0220208
I1107 14:21:04.763397 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:21:04.763397 11160 solver.cpp:237]     Train net output #1: loss = 0.0220208 (* 1 = 0.0220208 loss)
I1107 14:21:04.763397 11160 sgd_solver.cpp:105] Iteration 162200, lr = 0.0001
I1107 14:21:13.438653 11160 solver.cpp:218] Iteration 162300 (11.5276 iter/s, 8.67482s/100 iters), loss = 0.0237722
I1107 14:21:13.438653 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:21:13.438653 11160 solver.cpp:237]     Train net output #1: loss = 0.0237721 (* 1 = 0.0237721 loss)
I1107 14:21:13.438653 11160 sgd_solver.cpp:105] Iteration 162300, lr = 0.0001
I1107 14:21:22.147081 11160 solver.cpp:218] Iteration 162400 (11.4831 iter/s, 8.70845s/100 iters), loss = 0.0258131
I1107 14:21:22.148082 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:21:22.148082 11160 solver.cpp:237]     Train net output #1: loss = 0.0258131 (* 1 = 0.0258131 loss)
I1107 14:21:22.148082 11160 sgd_solver.cpp:105] Iteration 162400, lr = 0.0001
I1107 14:21:30.285373  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:21:30.624321 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_162500.caffemodel
I1107 14:21:30.654834 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_162500.solverstate
I1107 14:21:30.664322 11160 solver.cpp:330] Iteration 162500, Testing net (#0)
I1107 14:21:30.664322 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:21:32.656997 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:21:32.737002 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9205
I1107 14:21:32.737002 11160 solver.cpp:397]     Test net output #1: loss = 0.308955 (* 1 = 0.308955 loss)
I1107 14:21:32.818007 11160 solver.cpp:218] Iteration 162500 (9.37188 iter/s, 10.6702s/100 iters), loss = 0.0231248
I1107 14:21:32.818007 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:21:32.818007 11160 solver.cpp:237]     Train net output #1: loss = 0.0231248 (* 1 = 0.0231248 loss)
I1107 14:21:32.818007 11160 sgd_solver.cpp:105] Iteration 162500, lr = 0.0001
I1107 14:21:41.350896 11160 solver.cpp:218] Iteration 162600 (11.7205 iter/s, 8.53208s/100 iters), loss = 0.022812
I1107 14:21:41.350896 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:21:41.350896 11160 solver.cpp:237]     Train net output #1: loss = 0.022812 (* 1 = 0.022812 loss)
I1107 14:21:41.350896 11160 sgd_solver.cpp:105] Iteration 162600, lr = 0.0001
I1107 14:21:50.075069 11160 solver.cpp:218] Iteration 162700 (11.4632 iter/s, 8.72357s/100 iters), loss = 0.0210291
I1107 14:21:50.075069 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:21:50.075069 11160 solver.cpp:237]     Train net output #1: loss = 0.021029 (* 1 = 0.021029 loss)
I1107 14:21:50.075069 11160 sgd_solver.cpp:105] Iteration 162700, lr = 0.0001
I1107 14:21:58.664084 11160 solver.cpp:218] Iteration 162800 (11.6443 iter/s, 8.5879s/100 iters), loss = 0.0235336
I1107 14:21:58.664084 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:21:58.664084 11160 solver.cpp:237]     Train net output #1: loss = 0.0235335 (* 1 = 0.0235335 loss)
I1107 14:21:58.664084 11160 sgd_solver.cpp:105] Iteration 162800, lr = 0.0001
I1107 14:22:07.371531 11160 solver.cpp:218] Iteration 162900 (11.4852 iter/s, 8.70689s/100 iters), loss = 0.0256825
I1107 14:22:07.371531 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:22:07.371531 11160 solver.cpp:237]     Train net output #1: loss = 0.0256825 (* 1 = 0.0256825 loss)
I1107 14:22:07.371531 11160 sgd_solver.cpp:105] Iteration 162900, lr = 0.0001
I1107 14:22:15.607666  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:22:15.945704 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_163000.caffemodel
I1107 14:22:15.974704 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_163000.solverstate
I1107 14:22:15.983703 11160 solver.cpp:330] Iteration 163000, Testing net (#0)
I1107 14:22:15.983703 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:22:17.976861 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:22:18.057868 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9206
I1107 14:22:18.057868 11160 solver.cpp:397]     Test net output #1: loss = 0.309041 (* 1 = 0.309041 loss)
I1107 14:22:18.141875 11160 solver.cpp:218] Iteration 163000 (9.28484 iter/s, 10.7702s/100 iters), loss = 0.0266586
I1107 14:22:18.141875 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:22:18.141875 11160 solver.cpp:237]     Train net output #1: loss = 0.0266585 (* 1 = 0.0266585 loss)
I1107 14:22:18.141875 11160 sgd_solver.cpp:105] Iteration 163000, lr = 0.0001
I1107 14:22:26.803930 11160 solver.cpp:218] Iteration 163100 (11.5457 iter/s, 8.66126s/100 iters), loss = 0.020198
I1107 14:22:26.803930 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:22:26.803930 11160 solver.cpp:237]     Train net output #1: loss = 0.020198 (* 1 = 0.020198 loss)
I1107 14:22:26.803930 11160 sgd_solver.cpp:105] Iteration 163100, lr = 0.0001
I1107 14:22:35.473484 11160 solver.cpp:218] Iteration 163200 (11.5346 iter/s, 8.66955s/100 iters), loss = 0.0193776
I1107 14:22:35.474483 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:22:35.474483 11160 solver.cpp:237]     Train net output #1: loss = 0.0193776 (* 1 = 0.0193776 loss)
I1107 14:22:35.474483 11160 sgd_solver.cpp:105] Iteration 163200, lr = 0.0001
I1107 14:22:44.150506 11160 solver.cpp:218] Iteration 163300 (11.5266 iter/s, 8.67557s/100 iters), loss = 0.0176818
I1107 14:22:44.150506 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:22:44.150506 11160 solver.cpp:237]     Train net output #1: loss = 0.0176818 (* 1 = 0.0176818 loss)
I1107 14:22:44.150506 11160 sgd_solver.cpp:105] Iteration 163300, lr = 0.0001
I1107 14:22:52.831044 11160 solver.cpp:218] Iteration 163400 (11.5211 iter/s, 8.67972s/100 iters), loss = 0.0237064
I1107 14:22:52.831044 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:22:52.831044 11160 solver.cpp:237]     Train net output #1: loss = 0.0237064 (* 1 = 0.0237064 loss)
I1107 14:22:52.831044 11160 sgd_solver.cpp:105] Iteration 163400, lr = 0.0001
I1107 14:23:00.944538  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:23:01.281580 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_163500.caffemodel
I1107 14:23:01.311619 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_163500.solverstate
I1107 14:23:01.326611 11160 solver.cpp:330] Iteration 163500, Testing net (#0)
I1107 14:23:01.326611 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:23:03.314765 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:23:03.393263 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9201
I1107 14:23:03.393263 11160 solver.cpp:397]     Test net output #1: loss = 0.309538 (* 1 = 0.309538 loss)
I1107 14:23:03.474771 11160 solver.cpp:218] Iteration 163500 (9.39571 iter/s, 10.6432s/100 iters), loss = 0.025913
I1107 14:23:03.474771 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:23:03.474771 11160 solver.cpp:237]     Train net output #1: loss = 0.0259129 (* 1 = 0.0259129 loss)
I1107 14:23:03.474771 11160 sgd_solver.cpp:105] Iteration 163500, lr = 0.0001
I1107 14:23:12.050315 11160 solver.cpp:218] Iteration 163600 (11.6609 iter/s, 8.57569s/100 iters), loss = 0.0203987
I1107 14:23:12.050315 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:23:12.050315 11160 solver.cpp:237]     Train net output #1: loss = 0.0203987 (* 1 = 0.0203987 loss)
I1107 14:23:12.050315 11160 sgd_solver.cpp:105] Iteration 163600, lr = 0.0001
I1107 14:23:20.685372 11160 solver.cpp:218] Iteration 163700 (11.5824 iter/s, 8.63379s/100 iters), loss = 0.0198368
I1107 14:23:20.685372 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:23:20.685372 11160 solver.cpp:237]     Train net output #1: loss = 0.0198368 (* 1 = 0.0198368 loss)
I1107 14:23:20.685372 11160 sgd_solver.cpp:105] Iteration 163700, lr = 0.0001
I1107 14:23:29.358850 11160 solver.cpp:218] Iteration 163800 (11.5302 iter/s, 8.6729s/100 iters), loss = 0.0221867
I1107 14:23:29.358850 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:23:29.358850 11160 solver.cpp:237]     Train net output #1: loss = 0.0221867 (* 1 = 0.0221867 loss)
I1107 14:23:29.358850 11160 sgd_solver.cpp:105] Iteration 163800, lr = 0.0001
I1107 14:23:38.073153 11160 solver.cpp:218] Iteration 163900 (11.4762 iter/s, 8.7137s/100 iters), loss = 0.0245405
I1107 14:23:38.073153 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:23:38.073153 11160 solver.cpp:237]     Train net output #1: loss = 0.0245405 (* 1 = 0.0245405 loss)
I1107 14:23:38.073153 11160 sgd_solver.cpp:105] Iteration 163900, lr = 0.0001
I1107 14:23:46.270182  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:23:46.606209 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_164000.caffemodel
I1107 14:23:46.635212 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_164000.solverstate
I1107 14:23:46.644213 11160 solver.cpp:330] Iteration 164000, Testing net (#0)
I1107 14:23:46.644213 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:23:48.635267 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:23:48.715270 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9203
I1107 14:23:48.715270 11160 solver.cpp:397]     Test net output #1: loss = 0.309338 (* 1 = 0.309338 loss)
I1107 14:23:48.796274 11160 solver.cpp:218] Iteration 164000 (9.32582 iter/s, 10.7229s/100 iters), loss = 0.0234573
I1107 14:23:48.796274 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:23:48.796274 11160 solver.cpp:237]     Train net output #1: loss = 0.0234573 (* 1 = 0.0234573 loss)
I1107 14:23:48.796274 11160 sgd_solver.cpp:105] Iteration 164000, lr = 0.0001
I1107 14:23:57.489567 11160 solver.cpp:218] Iteration 164100 (11.5034 iter/s, 8.69304s/100 iters), loss = 0.0404249
I1107 14:23:57.489567 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:23:57.489567 11160 solver.cpp:237]     Train net output #1: loss = 0.0404248 (* 1 = 0.0404248 loss)
I1107 14:23:57.489567 11160 sgd_solver.cpp:105] Iteration 164100, lr = 0.0001
I1107 14:24:06.063220 11160 solver.cpp:218] Iteration 164200 (11.6645 iter/s, 8.57302s/100 iters), loss = 0.0185199
I1107 14:24:06.063220 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:24:06.063220 11160 solver.cpp:237]     Train net output #1: loss = 0.0185199 (* 1 = 0.0185199 loss)
I1107 14:24:06.063220 11160 sgd_solver.cpp:105] Iteration 164200, lr = 0.0001
I1107 14:24:14.703866 11160 solver.cpp:218] Iteration 164300 (11.5738 iter/s, 8.64022s/100 iters), loss = 0.0158498
I1107 14:24:14.703866 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:24:14.703866 11160 solver.cpp:237]     Train net output #1: loss = 0.0158498 (* 1 = 0.0158498 loss)
I1107 14:24:14.704867 11160 sgd_solver.cpp:105] Iteration 164300, lr = 0.0001
I1107 14:24:23.294090 11160 solver.cpp:218] Iteration 164400 (11.6423 iter/s, 8.5894s/100 iters), loss = 0.042935
I1107 14:24:23.294090 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:24:23.294090 11160 solver.cpp:237]     Train net output #1: loss = 0.042935 (* 1 = 0.042935 loss)
I1107 14:24:23.294090 11160 sgd_solver.cpp:105] Iteration 164400, lr = 0.0001
I1107 14:24:31.501885  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:24:31.844341 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_164500.caffemodel
I1107 14:24:31.874342 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_164500.solverstate
I1107 14:24:31.883342 11160 solver.cpp:330] Iteration 164500, Testing net (#0)
I1107 14:24:31.883342 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:24:33.893479 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:24:33.975500 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9207
I1107 14:24:33.975500 11160 solver.cpp:397]     Test net output #1: loss = 0.308967 (* 1 = 0.308967 loss)
I1107 14:24:34.059516 11160 solver.cpp:218] Iteration 164500 (9.28924 iter/s, 10.7651s/100 iters), loss = 0.0254975
I1107 14:24:34.059516 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:24:34.059516 11160 solver.cpp:237]     Train net output #1: loss = 0.0254975 (* 1 = 0.0254975 loss)
I1107 14:24:34.059516 11160 sgd_solver.cpp:105] Iteration 164500, lr = 0.0001
I1107 14:24:42.667131 11160 solver.cpp:218] Iteration 164600 (11.6185 iter/s, 8.60697s/100 iters), loss = 0.0403426
I1107 14:24:42.667131 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:24:42.667131 11160 solver.cpp:237]     Train net output #1: loss = 0.0403426 (* 1 = 0.0403426 loss)
I1107 14:24:42.667131 11160 sgd_solver.cpp:105] Iteration 164600, lr = 0.0001
I1107 14:24:51.387765 11160 solver.cpp:218] Iteration 164700 (11.4687 iter/s, 8.71936s/100 iters), loss = 0.0181144
I1107 14:24:51.387765 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:24:51.387765 11160 solver.cpp:237]     Train net output #1: loss = 0.0181144 (* 1 = 0.0181144 loss)
I1107 14:24:51.387765 11160 sgd_solver.cpp:105] Iteration 164700, lr = 0.0001
I1107 14:25:00.010159 11160 solver.cpp:218] Iteration 164800 (11.5987 iter/s, 8.62169s/100 iters), loss = 0.0191281
I1107 14:25:00.010159 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:25:00.010159 11160 solver.cpp:237]     Train net output #1: loss = 0.0191281 (* 1 = 0.0191281 loss)
I1107 14:25:00.010159 11160 sgd_solver.cpp:105] Iteration 164800, lr = 0.0001
I1107 14:25:08.693719 11160 solver.cpp:218] Iteration 164900 (11.5167 iter/s, 8.68306s/100 iters), loss = 0.0293044
I1107 14:25:08.693719 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:25:08.693719 11160 solver.cpp:237]     Train net output #1: loss = 0.0293043 (* 1 = 0.0293043 loss)
I1107 14:25:08.693719 11160 sgd_solver.cpp:105] Iteration 164900, lr = 0.0001
I1107 14:25:16.937842  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:25:17.284909 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_165000.caffemodel
I1107 14:25:17.318915 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_165000.solverstate
I1107 14:25:17.331912 11160 solver.cpp:330] Iteration 165000, Testing net (#0)
I1107 14:25:17.331912 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:25:19.376504 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:25:19.457504 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 14:25:19.458007 11160 solver.cpp:397]     Test net output #1: loss = 0.309851 (* 1 = 0.309851 loss)
I1107 14:25:19.540513 11160 solver.cpp:218] Iteration 165000 (9.21972 iter/s, 10.8463s/100 iters), loss = 0.0424811
I1107 14:25:19.540513 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:25:19.540513 11160 solver.cpp:237]     Train net output #1: loss = 0.0424811 (* 1 = 0.0424811 loss)
I1107 14:25:19.540513 11160 sgd_solver.cpp:105] Iteration 165000, lr = 0.0001
I1107 14:25:28.144930 11160 solver.cpp:218] Iteration 165100 (11.6225 iter/s, 8.60398s/100 iters), loss = 0.0242376
I1107 14:25:28.144930 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:25:28.144930 11160 solver.cpp:237]     Train net output #1: loss = 0.0242375 (* 1 = 0.0242375 loss)
I1107 14:25:28.144930 11160 sgd_solver.cpp:105] Iteration 165100, lr = 0.0001
I1107 14:25:36.837857 11160 solver.cpp:218] Iteration 165200 (11.5051 iter/s, 8.69183s/100 iters), loss = 0.0198358
I1107 14:25:36.837857 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:25:36.837857 11160 solver.cpp:237]     Train net output #1: loss = 0.0198358 (* 1 = 0.0198358 loss)
I1107 14:25:36.837857 11160 sgd_solver.cpp:105] Iteration 165200, lr = 0.0001
I1107 14:25:45.482409 11160 solver.cpp:218] Iteration 165300 (11.5686 iter/s, 8.64407s/100 iters), loss = 0.018457
I1107 14:25:45.482409 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:25:45.482409 11160 solver.cpp:237]     Train net output #1: loss = 0.0184569 (* 1 = 0.0184569 loss)
I1107 14:25:45.482409 11160 sgd_solver.cpp:105] Iteration 165300, lr = 0.0001
I1107 14:25:54.129680 11160 solver.cpp:218] Iteration 165400 (11.5648 iter/s, 8.64696s/100 iters), loss = 0.0254432
I1107 14:25:54.129680 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:25:54.129680 11160 solver.cpp:237]     Train net output #1: loss = 0.0254432 (* 1 = 0.0254432 loss)
I1107 14:25:54.129680 11160 sgd_solver.cpp:105] Iteration 165400, lr = 0.0001
I1107 14:26:02.400596  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:26:02.742614 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_165500.caffemodel
I1107 14:26:02.774117 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_165500.solverstate
I1107 14:26:02.783622 11160 solver.cpp:330] Iteration 165500, Testing net (#0)
I1107 14:26:02.783622 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:26:04.783987 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:26:04.864995 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9204
I1107 14:26:04.864995 11160 solver.cpp:397]     Test net output #1: loss = 0.309234 (* 1 = 0.309234 loss)
I1107 14:26:04.946996 11160 solver.cpp:218] Iteration 165500 (9.24459 iter/s, 10.8171s/100 iters), loss = 0.0268123
I1107 14:26:04.947998 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:26:04.947998 11160 solver.cpp:237]     Train net output #1: loss = 0.0268122 (* 1 = 0.0268122 loss)
I1107 14:26:04.947998 11160 sgd_solver.cpp:105] Iteration 165500, lr = 0.0001
I1107 14:26:13.623335 11160 solver.cpp:218] Iteration 165600 (11.5263 iter/s, 8.67583s/100 iters), loss = 0.0220232
I1107 14:26:13.624336 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:26:13.624336 11160 solver.cpp:237]     Train net output #1: loss = 0.0220232 (* 1 = 0.0220232 loss)
I1107 14:26:13.624336 11160 sgd_solver.cpp:105] Iteration 165600, lr = 0.0001
I1107 14:26:22.349171 11160 solver.cpp:218] Iteration 165700 (11.4617 iter/s, 8.72468s/100 iters), loss = 0.0212641
I1107 14:26:22.349171 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:26:22.349171 11160 solver.cpp:237]     Train net output #1: loss = 0.021264 (* 1 = 0.021264 loss)
I1107 14:26:22.349171 11160 sgd_solver.cpp:105] Iteration 165700, lr = 0.0001
I1107 14:26:31.050045 11160 solver.cpp:218] Iteration 165800 (11.4938 iter/s, 8.70034s/100 iters), loss = 0.0189248
I1107 14:26:31.050045 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:26:31.050045 11160 solver.cpp:237]     Train net output #1: loss = 0.0189248 (* 1 = 0.0189248 loss)
I1107 14:26:31.050045 11160 sgd_solver.cpp:105] Iteration 165800, lr = 0.0001
I1107 14:26:39.809550 11160 solver.cpp:218] Iteration 165900 (11.4173 iter/s, 8.75865s/100 iters), loss = 0.032147
I1107 14:26:39.809550 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:26:39.809550 11160 solver.cpp:237]     Train net output #1: loss = 0.032147 (* 1 = 0.032147 loss)
I1107 14:26:39.809550 11160 sgd_solver.cpp:105] Iteration 165900, lr = 0.0001
I1107 14:26:47.962481  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:26:48.305510 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_166000.caffemodel
I1107 14:26:48.342537 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_166000.solverstate
I1107 14:26:48.351552 11160 solver.cpp:330] Iteration 166000, Testing net (#0)
I1107 14:26:48.351552 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:26:50.390789 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:26:50.471796 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1107 14:26:50.471796 11160 solver.cpp:397]     Test net output #1: loss = 0.309623 (* 1 = 0.309623 loss)
I1107 14:26:50.555801 11160 solver.cpp:218] Iteration 166000 (9.30561 iter/s, 10.7462s/100 iters), loss = 0.0356368
I1107 14:26:50.555801 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:26:50.555801 11160 solver.cpp:237]     Train net output #1: loss = 0.0356368 (* 1 = 0.0356368 loss)
I1107 14:26:50.555801 11160 sgd_solver.cpp:105] Iteration 166000, lr = 0.0001
I1107 14:26:59.250681 11160 solver.cpp:218] Iteration 166100 (11.5027 iter/s, 8.69359s/100 iters), loss = 0.0254182
I1107 14:26:59.250681 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:26:59.250681 11160 solver.cpp:237]     Train net output #1: loss = 0.0254182 (* 1 = 0.0254182 loss)
I1107 14:26:59.250681 11160 sgd_solver.cpp:105] Iteration 166100, lr = 0.0001
I1107 14:27:07.889799 11160 solver.cpp:218] Iteration 166200 (11.5752 iter/s, 8.63916s/100 iters), loss = 0.0352971
I1107 14:27:07.889799 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:27:07.889799 11160 solver.cpp:237]     Train net output #1: loss = 0.0352971 (* 1 = 0.0352971 loss)
I1107 14:27:07.889799 11160 sgd_solver.cpp:105] Iteration 166200, lr = 0.0001
I1107 14:27:16.481892 11160 solver.cpp:218] Iteration 166300 (11.6393 iter/s, 8.59159s/100 iters), loss = 0.0197347
I1107 14:27:16.481892 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:27:16.481892 11160 solver.cpp:237]     Train net output #1: loss = 0.0197346 (* 1 = 0.0197346 loss)
I1107 14:27:16.481892 11160 sgd_solver.cpp:105] Iteration 166300, lr = 0.0001
I1107 14:27:25.160678 11160 solver.cpp:218] Iteration 166400 (11.5229 iter/s, 8.6784s/100 iters), loss = 0.0279514
I1107 14:27:25.161679 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:27:25.161679 11160 solver.cpp:237]     Train net output #1: loss = 0.0279514 (* 1 = 0.0279514 loss)
I1107 14:27:25.161679 11160 sgd_solver.cpp:105] Iteration 166400, lr = 0.0001
I1107 14:27:33.414185  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:27:33.750217 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_166500.caffemodel
I1107 14:27:33.779217 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_166500.solverstate
I1107 14:27:33.788218 11160 solver.cpp:330] Iteration 166500, Testing net (#0)
I1107 14:27:33.788218 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:27:35.782474 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:27:35.863490 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9198
I1107 14:27:35.863490 11160 solver.cpp:397]     Test net output #1: loss = 0.309652 (* 1 = 0.309652 loss)
I1107 14:27:35.945495 11160 solver.cpp:218] Iteration 166500 (9.27287 iter/s, 10.7841s/100 iters), loss = 0.0303603
I1107 14:27:35.945495 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:27:35.945495 11160 solver.cpp:237]     Train net output #1: loss = 0.0303603 (* 1 = 0.0303603 loss)
I1107 14:27:35.945495 11160 sgd_solver.cpp:105] Iteration 166500, lr = 0.0001
I1107 14:27:44.544785 11160 solver.cpp:218] Iteration 166600 (11.6294 iter/s, 8.59892s/100 iters), loss = 0.0240277
I1107 14:27:44.545784 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:27:44.545784 11160 solver.cpp:237]     Train net output #1: loss = 0.0240277 (* 1 = 0.0240277 loss)
I1107 14:27:44.545784 11160 sgd_solver.cpp:105] Iteration 166600, lr = 0.0001
I1107 14:27:53.100611 11160 solver.cpp:218] Iteration 166700 (11.6898 iter/s, 8.55444s/100 iters), loss = 0.027033
I1107 14:27:53.100611 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:27:53.100611 11160 solver.cpp:237]     Train net output #1: loss = 0.027033 (* 1 = 0.027033 loss)
I1107 14:27:53.100611 11160 sgd_solver.cpp:105] Iteration 166700, lr = 0.0001
I1107 14:28:01.664436 11160 solver.cpp:218] Iteration 166800 (11.6779 iter/s, 8.56315s/100 iters), loss = 0.0238259
I1107 14:28:01.664436 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:28:01.664436 11160 solver.cpp:237]     Train net output #1: loss = 0.0238259 (* 1 = 0.0238259 loss)
I1107 14:28:01.664436 11160 sgd_solver.cpp:105] Iteration 166800, lr = 0.0001
I1107 14:28:10.203362 11160 solver.cpp:218] Iteration 166900 (11.7111 iter/s, 8.53891s/100 iters), loss = 0.0216123
I1107 14:28:10.203362 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:28:10.203362 11160 solver.cpp:237]     Train net output #1: loss = 0.0216123 (* 1 = 0.0216123 loss)
I1107 14:28:10.203362 11160 sgd_solver.cpp:105] Iteration 166900, lr = 0.0001
I1107 14:28:18.333261  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:28:18.668812 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_167000.caffemodel
I1107 14:28:18.699812 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_167000.solverstate
I1107 14:28:18.708814 11160 solver.cpp:330] Iteration 167000, Testing net (#0)
I1107 14:28:18.708814 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:28:20.699035 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:28:20.779038 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 14:28:20.779038 11160 solver.cpp:397]     Test net output #1: loss = 0.310039 (* 1 = 0.310039 loss)
I1107 14:28:20.861057 11160 solver.cpp:218] Iteration 167000 (9.38373 iter/s, 10.6567s/100 iters), loss = 0.0324265
I1107 14:28:20.861057 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:28:20.861057 11160 solver.cpp:237]     Train net output #1: loss = 0.0324264 (* 1 = 0.0324264 loss)
I1107 14:28:20.861057 11160 sgd_solver.cpp:105] Iteration 167000, lr = 0.0001
I1107 14:28:29.401032 11160 solver.cpp:218] Iteration 167100 (11.7096 iter/s, 8.54004s/100 iters), loss = 0.0226857
I1107 14:28:29.401032 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:28:29.401032 11160 solver.cpp:237]     Train net output #1: loss = 0.0226857 (* 1 = 0.0226857 loss)
I1107 14:28:29.401032 11160 sgd_solver.cpp:105] Iteration 167100, lr = 0.0001
I1107 14:28:37.936494 11160 solver.cpp:218] Iteration 167200 (11.7176 iter/s, 8.53416s/100 iters), loss = 0.0152017
I1107 14:28:37.936494 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:28:37.936494 11160 solver.cpp:237]     Train net output #1: loss = 0.0152017 (* 1 = 0.0152017 loss)
I1107 14:28:37.936494 11160 sgd_solver.cpp:105] Iteration 167200, lr = 0.0001
I1107 14:28:46.473459 11160 solver.cpp:218] Iteration 167300 (11.7136 iter/s, 8.53707s/100 iters), loss = 0.0170372
I1107 14:28:46.473459 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:28:46.473459 11160 solver.cpp:237]     Train net output #1: loss = 0.0170371 (* 1 = 0.0170371 loss)
I1107 14:28:46.473459 11160 sgd_solver.cpp:105] Iteration 167300, lr = 0.0001
I1107 14:28:55.021028 11160 solver.cpp:218] Iteration 167400 (11.7006 iter/s, 8.54659s/100 iters), loss = 0.027622
I1107 14:28:55.021028 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:28:55.021028 11160 solver.cpp:237]     Train net output #1: loss = 0.027622 (* 1 = 0.027622 loss)
I1107 14:28:55.021028 11160 sgd_solver.cpp:105] Iteration 167400, lr = 0.0001
I1107 14:29:03.142210  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:29:03.478740 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_167500.caffemodel
I1107 14:29:03.507740 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_167500.solverstate
I1107 14:29:03.516741 11160 solver.cpp:330] Iteration 167500, Testing net (#0)
I1107 14:29:03.516741 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:29:05.508878 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:29:05.587882 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9196
I1107 14:29:05.587882 11160 solver.cpp:397]     Test net output #1: loss = 0.310392 (* 1 = 0.310392 loss)
I1107 14:29:05.669886 11160 solver.cpp:218] Iteration 167500 (9.39122 iter/s, 10.6482s/100 iters), loss = 0.0367068
I1107 14:29:05.669886 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:29:05.669886 11160 solver.cpp:237]     Train net output #1: loss = 0.0367068 (* 1 = 0.0367068 loss)
I1107 14:29:05.669886 11160 sgd_solver.cpp:105] Iteration 167500, lr = 0.0001
I1107 14:29:14.198460 11160 solver.cpp:218] Iteration 167600 (11.7258 iter/s, 8.52821s/100 iters), loss = 0.0268007
I1107 14:29:14.198460 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:29:14.198460 11160 solver.cpp:237]     Train net output #1: loss = 0.0268007 (* 1 = 0.0268007 loss)
I1107 14:29:14.198460 11160 sgd_solver.cpp:105] Iteration 167600, lr = 0.0001
I1107 14:29:22.724997 11160 solver.cpp:218] Iteration 167700 (11.7291 iter/s, 8.52581s/100 iters), loss = 0.0172041
I1107 14:29:22.724997 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:29:22.724997 11160 solver.cpp:237]     Train net output #1: loss = 0.0172041 (* 1 = 0.0172041 loss)
I1107 14:29:22.724997 11160 sgd_solver.cpp:105] Iteration 167700, lr = 0.0001
I1107 14:29:31.249773 11160 solver.cpp:218] Iteration 167800 (11.731 iter/s, 8.52442s/100 iters), loss = 0.0222065
I1107 14:29:31.249773 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:29:31.250273 11160 solver.cpp:237]     Train net output #1: loss = 0.0222065 (* 1 = 0.0222065 loss)
I1107 14:29:31.250273 11160 sgd_solver.cpp:105] Iteration 167800, lr = 0.0001
I1107 14:29:39.782076 11160 solver.cpp:218] Iteration 167900 (11.7208 iter/s, 8.53187s/100 iters), loss = 0.0283541
I1107 14:29:39.782076 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:29:39.782076 11160 solver.cpp:237]     Train net output #1: loss = 0.028354 (* 1 = 0.028354 loss)
I1107 14:29:39.782076 11160 sgd_solver.cpp:105] Iteration 167900, lr = 0.0001
I1107 14:29:47.910756  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:29:48.249409 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_168000.caffemodel
I1107 14:29:48.277915 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_168000.solverstate
I1107 14:29:48.286916 11160 solver.cpp:330] Iteration 168000, Testing net (#0)
I1107 14:29:48.286916 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:29:50.276161 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:29:50.357195 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9205
I1107 14:29:50.357195 11160 solver.cpp:397]     Test net output #1: loss = 0.309468 (* 1 = 0.309468 loss)
I1107 14:29:50.438812 11160 solver.cpp:218] Iteration 168000 (9.38425 iter/s, 10.6561s/100 iters), loss = 0.0234224
I1107 14:29:50.438812 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:29:50.438812 11160 solver.cpp:237]     Train net output #1: loss = 0.0234224 (* 1 = 0.0234224 loss)
I1107 14:29:50.438812 11160 sgd_solver.cpp:105] Iteration 168000, lr = 0.0001
I1107 14:29:58.997602 11160 solver.cpp:218] Iteration 168100 (11.684 iter/s, 8.55869s/100 iters), loss = 0.023745
I1107 14:29:58.997602 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:29:58.997602 11160 solver.cpp:237]     Train net output #1: loss = 0.023745 (* 1 = 0.023745 loss)
I1107 14:29:58.997602 11160 sgd_solver.cpp:105] Iteration 168100, lr = 0.0001
I1107 14:30:07.576158 11160 solver.cpp:218] Iteration 168200 (11.658 iter/s, 8.57784s/100 iters), loss = 0.0266883
I1107 14:30:07.576158 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:30:07.576158 11160 solver.cpp:237]     Train net output #1: loss = 0.0266882 (* 1 = 0.0266882 loss)
I1107 14:30:07.576158 11160 sgd_solver.cpp:105] Iteration 168200, lr = 0.0001
I1107 14:30:16.132987 11160 solver.cpp:218] Iteration 168300 (11.687 iter/s, 8.55654s/100 iters), loss = 0.0214418
I1107 14:30:16.133987 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:30:16.133987 11160 solver.cpp:237]     Train net output #1: loss = 0.0214417 (* 1 = 0.0214417 loss)
I1107 14:30:16.133987 11160 sgd_solver.cpp:105] Iteration 168300, lr = 0.0001
I1107 14:30:24.667115 11160 solver.cpp:218] Iteration 168400 (11.7195 iter/s, 8.53279s/100 iters), loss = 0.0215767
I1107 14:30:24.667115 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:30:24.667115 11160 solver.cpp:237]     Train net output #1: loss = 0.0215766 (* 1 = 0.0215766 loss)
I1107 14:30:24.667115 11160 sgd_solver.cpp:105] Iteration 168400, lr = 0.0001
I1107 14:30:32.801970  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:30:33.138986 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_168500.caffemodel
I1107 14:30:33.167989 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_168500.solverstate
I1107 14:30:33.176995 11160 solver.cpp:330] Iteration 168500, Testing net (#0)
I1107 14:30:33.176995 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:30:35.181309 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:30:35.261811 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9205
I1107 14:30:35.261811 11160 solver.cpp:397]     Test net output #1: loss = 0.309998 (* 1 = 0.309998 loss)
I1107 14:30:35.342319 11160 solver.cpp:218] Iteration 168500 (9.36731 iter/s, 10.6754s/100 iters), loss = 0.0350467
I1107 14:30:35.342319 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:30:35.343319 11160 solver.cpp:237]     Train net output #1: loss = 0.0350467 (* 1 = 0.0350467 loss)
I1107 14:30:35.343319 11160 sgd_solver.cpp:105] Iteration 168500, lr = 0.0001
I1107 14:30:43.927175 11160 solver.cpp:218] Iteration 168600 (11.6495 iter/s, 8.58403s/100 iters), loss = 0.0217694
I1107 14:30:43.927175 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:30:43.927175 11160 solver.cpp:237]     Train net output #1: loss = 0.0217693 (* 1 = 0.0217693 loss)
I1107 14:30:43.927175 11160 sgd_solver.cpp:105] Iteration 168600, lr = 0.0001
I1107 14:30:52.486861 11160 solver.cpp:218] Iteration 168700 (11.6842 iter/s, 8.55855s/100 iters), loss = 0.0195477
I1107 14:30:52.486861 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:30:52.486861 11160 solver.cpp:237]     Train net output #1: loss = 0.0195477 (* 1 = 0.0195477 loss)
I1107 14:30:52.486861 11160 sgd_solver.cpp:105] Iteration 168700, lr = 0.0001
I1107 14:31:01.209244 11160 solver.cpp:218] Iteration 168800 (11.4647 iter/s, 8.7224s/100 iters), loss = 0.0346078
I1107 14:31:01.209244 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:31:01.209244 11160 solver.cpp:237]     Train net output #1: loss = 0.0346078 (* 1 = 0.0346078 loss)
I1107 14:31:01.209244 11160 sgd_solver.cpp:105] Iteration 168800, lr = 0.0001
I1107 14:31:09.809520 11160 solver.cpp:218] Iteration 168900 (11.6283 iter/s, 8.59974s/100 iters), loss = 0.0269132
I1107 14:31:09.809520 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:31:09.809520 11160 solver.cpp:237]     Train net output #1: loss = 0.0269132 (* 1 = 0.0269132 loss)
I1107 14:31:09.809520 11160 sgd_solver.cpp:105] Iteration 168900, lr = 0.0001
I1107 14:31:18.033660  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:31:18.374188 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_169000.caffemodel
I1107 14:31:18.403692 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_169000.solverstate
I1107 14:31:18.412693 11160 solver.cpp:330] Iteration 169000, Testing net (#0)
I1107 14:31:18.412693 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:31:20.414032 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:31:20.494037 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 14:31:20.494037 11160 solver.cpp:397]     Test net output #1: loss = 0.309979 (* 1 = 0.309979 loss)
I1107 14:31:20.575038 11160 solver.cpp:218] Iteration 169000 (9.28965 iter/s, 10.7647s/100 iters), loss = 0.0260659
I1107 14:31:20.575038 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:31:20.575038 11160 solver.cpp:237]     Train net output #1: loss = 0.0260659 (* 1 = 0.0260659 loss)
I1107 14:31:20.575038 11160 sgd_solver.cpp:105] Iteration 169000, lr = 0.0001
I1107 14:31:29.236825 11160 solver.cpp:218] Iteration 169100 (11.5457 iter/s, 8.66123s/100 iters), loss = 0.0180381
I1107 14:31:29.236825 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:31:29.236825 11160 solver.cpp:237]     Train net output #1: loss = 0.0180381 (* 1 = 0.0180381 loss)
I1107 14:31:29.236825 11160 sgd_solver.cpp:105] Iteration 169100, lr = 0.0001
I1107 14:31:37.799474 11160 solver.cpp:218] Iteration 169200 (11.6791 iter/s, 8.56232s/100 iters), loss = 0.0205905
I1107 14:31:37.799474 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:31:37.799474 11160 solver.cpp:237]     Train net output #1: loss = 0.0205905 (* 1 = 0.0205905 loss)
I1107 14:31:37.799474 11160 sgd_solver.cpp:105] Iteration 169200, lr = 0.0001
I1107 14:31:46.368397 11160 solver.cpp:218] Iteration 169300 (11.6711 iter/s, 8.56816s/100 iters), loss = 0.0296441
I1107 14:31:46.368397 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:31:46.368897 11160 solver.cpp:237]     Train net output #1: loss = 0.0296441 (* 1 = 0.0296441 loss)
I1107 14:31:46.368897 11160 sgd_solver.cpp:105] Iteration 169300, lr = 0.0001
I1107 14:31:54.964318 11160 solver.cpp:218] Iteration 169400 (11.6344 iter/s, 8.59518s/100 iters), loss = 0.024234
I1107 14:31:54.964318 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:31:54.964318 11160 solver.cpp:237]     Train net output #1: loss = 0.024234 (* 1 = 0.024234 loss)
I1107 14:31:54.964318 11160 sgd_solver.cpp:105] Iteration 169400, lr = 0.0001
I1107 14:32:03.228135  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:32:03.570657 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_169500.caffemodel
I1107 14:32:03.604164 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_169500.solverstate
I1107 14:32:03.614162 11160 solver.cpp:330] Iteration 169500, Testing net (#0)
I1107 14:32:03.614162 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:32:05.653626 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:32:05.735632 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9203
I1107 14:32:05.735632 11160 solver.cpp:397]     Test net output #1: loss = 0.309725 (* 1 = 0.309725 loss)
I1107 14:32:05.820667 11160 solver.cpp:218] Iteration 169500 (9.21159 iter/s, 10.8559s/100 iters), loss = 0.0255679
I1107 14:32:05.820667 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:32:05.820667 11160 solver.cpp:237]     Train net output #1: loss = 0.0255678 (* 1 = 0.0255678 loss)
I1107 14:32:05.820667 11160 sgd_solver.cpp:105] Iteration 169500, lr = 0.0001
I1107 14:32:14.517812 11160 solver.cpp:218] Iteration 169600 (11.4981 iter/s, 8.69709s/100 iters), loss = 0.0223662
I1107 14:32:14.517812 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:32:14.517812 11160 solver.cpp:237]     Train net output #1: loss = 0.0223662 (* 1 = 0.0223662 loss)
I1107 14:32:14.517812 11160 sgd_solver.cpp:105] Iteration 169600, lr = 0.0001
I1107 14:32:23.240558 11160 solver.cpp:218] Iteration 169700 (11.4659 iter/s, 8.72154s/100 iters), loss = 0.0188405
I1107 14:32:23.240558 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:32:23.240558 11160 solver.cpp:237]     Train net output #1: loss = 0.0188405 (* 1 = 0.0188405 loss)
I1107 14:32:23.240558 11160 sgd_solver.cpp:105] Iteration 169700, lr = 0.0001
I1107 14:32:31.980077 11160 solver.cpp:218] Iteration 169800 (11.443 iter/s, 8.739s/100 iters), loss = 0.0177483
I1107 14:32:31.980077 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:32:31.980077 11160 solver.cpp:237]     Train net output #1: loss = 0.0177483 (* 1 = 0.0177483 loss)
I1107 14:32:31.980077 11160 sgd_solver.cpp:105] Iteration 169800, lr = 0.0001
I1107 14:32:40.691293 11160 solver.cpp:218] Iteration 169900 (11.4803 iter/s, 8.7106s/100 iters), loss = 0.0403624
I1107 14:32:40.691293 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:32:40.691293 11160 solver.cpp:237]     Train net output #1: loss = 0.0403624 (* 1 = 0.0403624 loss)
I1107 14:32:40.691293 11160 sgd_solver.cpp:105] Iteration 169900, lr = 0.0001
I1107 14:32:48.979840  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:32:49.318874 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_170000.caffemodel
I1107 14:32:49.354871 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_170000.solverstate
I1107 14:32:49.363871 11160 solver.cpp:330] Iteration 170000, Testing net (#0)
I1107 14:32:49.363871 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:32:51.379048 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:32:51.460072 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9201
I1107 14:32:51.460072 11160 solver.cpp:397]     Test net output #1: loss = 0.309327 (* 1 = 0.309327 loss)
I1107 14:32:51.545059 11160 solver.cpp:218] Iteration 170000 (9.21348 iter/s, 10.8537s/100 iters), loss = 0.0261188
I1107 14:32:51.545059 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:32:51.545059 11160 solver.cpp:237]     Train net output #1: loss = 0.0261188 (* 1 = 0.0261188 loss)
I1107 14:32:51.545059 11160 sgd_solver.cpp:105] Iteration 170000, lr = 0.0001
I1107 14:33:00.182476 11160 solver.cpp:218] Iteration 170100 (11.5788 iter/s, 8.63645s/100 iters), loss = 0.0224774
I1107 14:33:00.182476 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:33:00.182476 11160 solver.cpp:237]     Train net output #1: loss = 0.0224774 (* 1 = 0.0224774 loss)
I1107 14:33:00.182476 11160 sgd_solver.cpp:105] Iteration 170100, lr = 0.0001
I1107 14:33:08.831406 11160 solver.cpp:218] Iteration 170200 (11.5624 iter/s, 8.64874s/100 iters), loss = 0.0364564
I1107 14:33:08.831406 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 14:33:08.831406 11160 solver.cpp:237]     Train net output #1: loss = 0.0364564 (* 1 = 0.0364564 loss)
I1107 14:33:08.831406 11160 sgd_solver.cpp:105] Iteration 170200, lr = 0.0001
I1107 14:33:17.454147 11160 solver.cpp:218] Iteration 170300 (11.5982 iter/s, 8.62201s/100 iters), loss = 0.0156395
I1107 14:33:17.454147 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:33:17.454147 11160 solver.cpp:237]     Train net output #1: loss = 0.0156394 (* 1 = 0.0156394 loss)
I1107 14:33:17.454147 11160 sgd_solver.cpp:105] Iteration 170300, lr = 0.0001
I1107 14:33:26.097877 11160 solver.cpp:218] Iteration 170400 (11.57 iter/s, 8.64306s/100 iters), loss = 0.0275886
I1107 14:33:26.097877 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:33:26.097877 11160 solver.cpp:237]     Train net output #1: loss = 0.0275886 (* 1 = 0.0275886 loss)
I1107 14:33:26.097877 11160 sgd_solver.cpp:105] Iteration 170400, lr = 0.0001
I1107 14:33:34.407722  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:33:34.747781 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_170500.caffemodel
I1107 14:33:34.779783 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_170500.solverstate
I1107 14:33:34.788784 11160 solver.cpp:330] Iteration 170500, Testing net (#0)
I1107 14:33:34.788784 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:33:36.817087 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:33:36.896092 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9201
I1107 14:33:36.897094 11160 solver.cpp:397]     Test net output #1: loss = 0.310606 (* 1 = 0.310606 loss)
I1107 14:33:36.978096 11160 solver.cpp:218] Iteration 170500 (9.191 iter/s, 10.8802s/100 iters), loss = 0.0241006
I1107 14:33:36.978096 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:33:36.978096 11160 solver.cpp:237]     Train net output #1: loss = 0.0241005 (* 1 = 0.0241005 loss)
I1107 14:33:36.978096 11160 sgd_solver.cpp:105] Iteration 170500, lr = 0.0001
I1107 14:33:45.635004 11160 solver.cpp:218] Iteration 170600 (11.5525 iter/s, 8.65614s/100 iters), loss = 0.0319553
I1107 14:33:45.635004 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:33:45.635004 11160 solver.cpp:237]     Train net output #1: loss = 0.0319553 (* 1 = 0.0319553 loss)
I1107 14:33:45.635004 11160 sgd_solver.cpp:105] Iteration 170600, lr = 0.0001
I1107 14:33:54.312068 11160 solver.cpp:218] Iteration 170700 (11.5258 iter/s, 8.67619s/100 iters), loss = 0.0230941
I1107 14:33:54.312068 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:33:54.312068 11160 solver.cpp:237]     Train net output #1: loss = 0.0230941 (* 1 = 0.0230941 loss)
I1107 14:33:54.312068 11160 sgd_solver.cpp:105] Iteration 170700, lr = 0.0001
I1107 14:34:03.013164 11160 solver.cpp:218] Iteration 170800 (11.4934 iter/s, 8.70066s/100 iters), loss = 0.0215956
I1107 14:34:03.013164 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:34:03.013164 11160 solver.cpp:237]     Train net output #1: loss = 0.0215956 (* 1 = 0.0215956 loss)
I1107 14:34:03.013164 11160 sgd_solver.cpp:105] Iteration 170800, lr = 0.0001
I1107 14:34:11.562957 11160 solver.cpp:218] Iteration 170900 (11.6968 iter/s, 8.54938s/100 iters), loss = 0.0247667
I1107 14:34:11.562957 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:34:11.562957 11160 solver.cpp:237]     Train net output #1: loss = 0.0247667 (* 1 = 0.0247667 loss)
I1107 14:34:11.562957 11160 sgd_solver.cpp:105] Iteration 170900, lr = 0.0001
I1107 14:34:19.805392  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:34:20.149452 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_171000.caffemodel
I1107 14:34:20.179453 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_171000.solverstate
I1107 14:34:20.189455 11160 solver.cpp:330] Iteration 171000, Testing net (#0)
I1107 14:34:20.189455 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:34:22.191726 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:34:22.274749 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9192
I1107 14:34:22.274749 11160 solver.cpp:397]     Test net output #1: loss = 0.310539 (* 1 = 0.310539 loss)
I1107 14:34:22.357666 11160 solver.cpp:218] Iteration 171000 (9.26409 iter/s, 10.7944s/100 iters), loss = 0.0259369
I1107 14:34:22.358677 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:34:22.358677 11160 solver.cpp:237]     Train net output #1: loss = 0.0259368 (* 1 = 0.0259368 loss)
I1107 14:34:22.358677 11160 sgd_solver.cpp:105] Iteration 171000, lr = 0.0001
I1107 14:34:30.955814 11160 solver.cpp:218] Iteration 171100 (11.6322 iter/s, 8.59686s/100 iters), loss = 0.0207002
I1107 14:34:30.955814 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:34:30.955814 11160 solver.cpp:237]     Train net output #1: loss = 0.0207002 (* 1 = 0.0207002 loss)
I1107 14:34:30.955814 11160 sgd_solver.cpp:105] Iteration 171100, lr = 0.0001
I1107 14:34:39.566664 11160 solver.cpp:218] Iteration 171200 (11.614 iter/s, 8.61031s/100 iters), loss = 0.0194087
I1107 14:34:39.566664 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:34:39.566664 11160 solver.cpp:237]     Train net output #1: loss = 0.0194087 (* 1 = 0.0194087 loss)
I1107 14:34:39.566664 11160 sgd_solver.cpp:105] Iteration 171200, lr = 0.0001
I1107 14:34:48.228628 11160 solver.cpp:218] Iteration 171300 (11.5452 iter/s, 8.6616s/100 iters), loss = 0.0191124
I1107 14:34:48.228628 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:34:48.228628 11160 solver.cpp:237]     Train net output #1: loss = 0.0191124 (* 1 = 0.0191124 loss)
I1107 14:34:48.228628 11160 sgd_solver.cpp:105] Iteration 171300, lr = 0.0001
I1107 14:34:56.806943 11160 solver.cpp:218] Iteration 171400 (11.6588 iter/s, 8.5772s/100 iters), loss = 0.0244326
I1107 14:34:56.806943 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:34:56.806943 11160 solver.cpp:237]     Train net output #1: loss = 0.0244326 (* 1 = 0.0244326 loss)
I1107 14:34:56.806943 11160 sgd_solver.cpp:105] Iteration 171400, lr = 0.0001
I1107 14:35:05.063004  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:35:05.403043 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_171500.caffemodel
I1107 14:35:05.456084 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_171500.solverstate
I1107 14:35:05.464083 11160 solver.cpp:330] Iteration 171500, Testing net (#0)
I1107 14:35:05.465083 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:35:07.482058 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:35:07.564066 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9197
I1107 14:35:07.564066 11160 solver.cpp:397]     Test net output #1: loss = 0.309772 (* 1 = 0.309772 loss)
I1107 14:35:07.647085 11160 solver.cpp:218] Iteration 171500 (9.22505 iter/s, 10.84s/100 iters), loss = 0.0334699
I1107 14:35:07.647085 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:35:07.647085 11160 solver.cpp:237]     Train net output #1: loss = 0.0334699 (* 1 = 0.0334699 loss)
I1107 14:35:07.647085 11160 sgd_solver.cpp:105] Iteration 171500, lr = 0.0001
I1107 14:35:16.288344 11160 solver.cpp:218] Iteration 171600 (11.573 iter/s, 8.64077s/100 iters), loss = 0.0259898
I1107 14:35:16.288344 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:35:16.288344 11160 solver.cpp:237]     Train net output #1: loss = 0.0259898 (* 1 = 0.0259898 loss)
I1107 14:35:16.288344 11160 sgd_solver.cpp:105] Iteration 171600, lr = 0.0001
I1107 14:35:24.851179 11160 solver.cpp:218] Iteration 171700 (11.6796 iter/s, 8.56192s/100 iters), loss = 0.0196276
I1107 14:35:24.851179 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:35:24.851179 11160 solver.cpp:237]     Train net output #1: loss = 0.0196275 (* 1 = 0.0196275 loss)
I1107 14:35:24.851179 11160 sgd_solver.cpp:105] Iteration 171700, lr = 0.0001
I1107 14:35:33.422674 11160 solver.cpp:218] Iteration 171800 (11.6667 iter/s, 8.57142s/100 iters), loss = 0.0218269
I1107 14:35:33.422674 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:35:33.422674 11160 solver.cpp:237]     Train net output #1: loss = 0.0218269 (* 1 = 0.0218269 loss)
I1107 14:35:33.422674 11160 sgd_solver.cpp:105] Iteration 171800, lr = 0.0001
I1107 14:35:41.988966 11160 solver.cpp:218] Iteration 171900 (11.6746 iter/s, 8.56558s/100 iters), loss = 0.0312525
I1107 14:35:41.988966 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:35:41.988966 11160 solver.cpp:237]     Train net output #1: loss = 0.0312525 (* 1 = 0.0312525 loss)
I1107 14:35:41.988966 11160 sgd_solver.cpp:105] Iteration 171900, lr = 0.0001
I1107 14:35:50.122934  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:35:50.460971 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_172000.caffemodel
I1107 14:35:50.524000 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_172000.solverstate
I1107 14:35:50.549145 11160 solver.cpp:330] Iteration 172000, Testing net (#0)
I1107 14:35:50.550133 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:35:52.546674 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:35:52.626687 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9204
I1107 14:35:52.626687 11160 solver.cpp:397]     Test net output #1: loss = 0.309457 (* 1 = 0.309457 loss)
I1107 14:35:52.707734 11160 solver.cpp:218] Iteration 172000 (9.32959 iter/s, 10.7186s/100 iters), loss = 0.0273651
I1107 14:35:52.707734 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:35:52.707734 11160 solver.cpp:237]     Train net output #1: loss = 0.0273651 (* 1 = 0.0273651 loss)
I1107 14:35:52.707734 11160 sgd_solver.cpp:105] Iteration 172000, lr = 0.0001
I1107 14:36:01.251045 11160 solver.cpp:218] Iteration 172100 (11.7067 iter/s, 8.54209s/100 iters), loss = 0.0178179
I1107 14:36:01.251045 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:36:01.251045 11160 solver.cpp:237]     Train net output #1: loss = 0.0178179 (* 1 = 0.0178179 loss)
I1107 14:36:01.251045 11160 sgd_solver.cpp:105] Iteration 172100, lr = 0.0001
I1107 14:36:09.813356 11160 solver.cpp:218] Iteration 172200 (11.6795 iter/s, 8.56199s/100 iters), loss = 0.0209409
I1107 14:36:09.813356 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:36:09.813356 11160 solver.cpp:237]     Train net output #1: loss = 0.0209409 (* 1 = 0.0209409 loss)
I1107 14:36:09.813356 11160 sgd_solver.cpp:105] Iteration 172200, lr = 0.0001
I1107 14:36:18.405318 11160 solver.cpp:218] Iteration 172300 (11.6398 iter/s, 8.59123s/100 iters), loss = 0.0211879
I1107 14:36:18.405318 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:36:18.405318 11160 solver.cpp:237]     Train net output #1: loss = 0.0211879 (* 1 = 0.0211879 loss)
I1107 14:36:18.405318 11160 sgd_solver.cpp:105] Iteration 172300, lr = 0.0001
I1107 14:36:26.964416 11160 solver.cpp:218] Iteration 172400 (11.684 iter/s, 8.55874s/100 iters), loss = 0.0389277
I1107 14:36:26.964416 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:36:26.964416 11160 solver.cpp:237]     Train net output #1: loss = 0.0389277 (* 1 = 0.0389277 loss)
I1107 14:36:26.964416 11160 sgd_solver.cpp:105] Iteration 172400, lr = 0.0001
I1107 14:36:35.133714  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:36:35.470369 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_172500.caffemodel
I1107 14:36:35.525385 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_172500.solverstate
I1107 14:36:35.534386 11160 solver.cpp:330] Iteration 172500, Testing net (#0)
I1107 14:36:35.535385 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:36:37.525616 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:36:37.605655 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9202
I1107 14:36:37.605655 11160 solver.cpp:397]     Test net output #1: loss = 0.309884 (* 1 = 0.309884 loss)
I1107 14:36:37.687642 11160 solver.cpp:218] Iteration 172500 (9.32594 iter/s, 10.7228s/100 iters), loss = 0.0199165
I1107 14:36:37.687642 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:36:37.687642 11160 solver.cpp:237]     Train net output #1: loss = 0.0199165 (* 1 = 0.0199165 loss)
I1107 14:36:37.687642 11160 sgd_solver.cpp:105] Iteration 172500, lr = 0.0001
I1107 14:36:46.323916 11160 solver.cpp:218] Iteration 172600 (11.5797 iter/s, 8.6358s/100 iters), loss = 0.0435819
I1107 14:36:46.323916 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:36:46.323916 11160 solver.cpp:237]     Train net output #1: loss = 0.0435819 (* 1 = 0.0435819 loss)
I1107 14:36:46.323916 11160 sgd_solver.cpp:105] Iteration 172600, lr = 0.0001
I1107 14:36:54.887228 11160 solver.cpp:218] Iteration 172700 (11.6792 iter/s, 8.56226s/100 iters), loss = 0.0185333
I1107 14:36:54.887228 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:36:54.887228 11160 solver.cpp:237]     Train net output #1: loss = 0.0185333 (* 1 = 0.0185333 loss)
I1107 14:36:54.887228 11160 sgd_solver.cpp:105] Iteration 172700, lr = 0.0001
I1107 14:37:03.465708 11160 solver.cpp:218] Iteration 172800 (11.6579 iter/s, 8.57787s/100 iters), loss = 0.0142821
I1107 14:37:03.465708 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:37:03.465708 11160 solver.cpp:237]     Train net output #1: loss = 0.0142821 (* 1 = 0.0142821 loss)
I1107 14:37:03.465708 11160 sgd_solver.cpp:105] Iteration 172800, lr = 0.0001
I1107 14:37:12.000823 11160 solver.cpp:218] Iteration 172900 (11.7165 iter/s, 8.53497s/100 iters), loss = 0.0323343
I1107 14:37:12.000823 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:37:12.000823 11160 solver.cpp:237]     Train net output #1: loss = 0.0323343 (* 1 = 0.0323343 loss)
I1107 14:37:12.000823 11160 sgd_solver.cpp:105] Iteration 172900, lr = 0.0001
I1107 14:37:20.117554  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:37:20.454577 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_173000.caffemodel
I1107 14:37:20.506597 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_173000.solverstate
I1107 14:37:20.523598 11160 solver.cpp:330] Iteration 173000, Testing net (#0)
I1107 14:37:20.523598 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:37:22.515775 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:37:22.595795 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9197
I1107 14:37:22.595795 11160 solver.cpp:397]     Test net output #1: loss = 0.309979 (* 1 = 0.309979 loss)
I1107 14:37:22.678297 11160 solver.cpp:218] Iteration 173000 (9.36634 iter/s, 10.6765s/100 iters), loss = 0.0259708
I1107 14:37:22.678297 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:37:22.678297 11160 solver.cpp:237]     Train net output #1: loss = 0.0259708 (* 1 = 0.0259708 loss)
I1107 14:37:22.678297 11160 sgd_solver.cpp:105] Iteration 173000, lr = 0.0001
I1107 14:37:31.225708 11160 solver.cpp:218] Iteration 173100 (11.6997 iter/s, 8.54719s/100 iters), loss = 0.0343655
I1107 14:37:31.225708 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:37:31.225708 11160 solver.cpp:237]     Train net output #1: loss = 0.0343655 (* 1 = 0.0343655 loss)
I1107 14:37:31.225708 11160 sgd_solver.cpp:105] Iteration 173100, lr = 0.0001
I1107 14:37:39.783179 11160 solver.cpp:218] Iteration 173200 (11.6869 iter/s, 8.55659s/100 iters), loss = 0.0186716
I1107 14:37:39.783179 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:37:39.783179 11160 solver.cpp:237]     Train net output #1: loss = 0.0186716 (* 1 = 0.0186716 loss)
I1107 14:37:39.783179 11160 sgd_solver.cpp:105] Iteration 173200, lr = 0.0001
I1107 14:37:48.325191 11160 solver.cpp:218] Iteration 173300 (11.7073 iter/s, 8.54165s/100 iters), loss = 0.0165066
I1107 14:37:48.325191 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:37:48.325191 11160 solver.cpp:237]     Train net output #1: loss = 0.0165066 (* 1 = 0.0165066 loss)
I1107 14:37:48.325191 11160 sgd_solver.cpp:105] Iteration 173300, lr = 0.0001
I1107 14:37:56.881624 11160 solver.cpp:218] Iteration 173400 (11.6884 iter/s, 8.55547s/100 iters), loss = 0.0227196
I1107 14:37:56.881624 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:37:56.881624 11160 solver.cpp:237]     Train net output #1: loss = 0.0227196 (* 1 = 0.0227196 loss)
I1107 14:37:56.881624 11160 sgd_solver.cpp:105] Iteration 173400, lr = 0.0001
I1107 14:38:05.008460  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:38:05.346518 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_173500.caffemodel
I1107 14:38:05.399538 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_173500.solverstate
I1107 14:38:05.409528 11160 solver.cpp:330] Iteration 173500, Testing net (#0)
I1107 14:38:05.410527 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:38:07.403753 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:38:07.483754 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 14:38:07.483754 11160 solver.cpp:397]     Test net output #1: loss = 0.310836 (* 1 = 0.310836 loss)
I1107 14:38:07.565757 11160 solver.cpp:218] Iteration 173500 (9.35988 iter/s, 10.6839s/100 iters), loss = 0.0242928
I1107 14:38:07.565757 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:38:07.565757 11160 solver.cpp:237]     Train net output #1: loss = 0.0242928 (* 1 = 0.0242928 loss)
I1107 14:38:07.565757 11160 sgd_solver.cpp:105] Iteration 173500, lr = 0.0001
I1107 14:38:16.098312 11160 solver.cpp:218] Iteration 173600 (11.7201 iter/s, 8.53233s/100 iters), loss = 0.0229244
I1107 14:38:16.098312 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:38:16.098312 11160 solver.cpp:237]     Train net output #1: loss = 0.0229244 (* 1 = 0.0229244 loss)
I1107 14:38:16.098312 11160 sgd_solver.cpp:105] Iteration 173600, lr = 0.0001
I1107 14:38:24.648445 11160 solver.cpp:218] Iteration 173700 (11.6973 iter/s, 8.54899s/100 iters), loss = 0.0264424
I1107 14:38:24.648445 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:38:24.648445 11160 solver.cpp:237]     Train net output #1: loss = 0.0264424 (* 1 = 0.0264424 loss)
I1107 14:38:24.648445 11160 sgd_solver.cpp:105] Iteration 173700, lr = 0.0001
I1107 14:38:33.225044 11160 solver.cpp:218] Iteration 173800 (11.6603 iter/s, 8.57608s/100 iters), loss = 0.0211989
I1107 14:38:33.225044 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:38:33.225044 11160 solver.cpp:237]     Train net output #1: loss = 0.0211989 (* 1 = 0.0211989 loss)
I1107 14:38:33.225044 11160 sgd_solver.cpp:105] Iteration 173800, lr = 0.0001
I1107 14:38:41.817786 11160 solver.cpp:218] Iteration 173900 (11.6371 iter/s, 8.5932s/100 iters), loss = 0.0244524
I1107 14:38:41.818773 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:38:41.818773 11160 solver.cpp:237]     Train net output #1: loss = 0.0244524 (* 1 = 0.0244524 loss)
I1107 14:38:41.818773 11160 sgd_solver.cpp:105] Iteration 173900, lr = 0.0001
I1107 14:38:49.937569  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:38:50.274621 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_174000.caffemodel
I1107 14:38:50.341653 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_174000.solverstate
I1107 14:38:50.350651 11160 solver.cpp:330] Iteration 174000, Testing net (#0)
I1107 14:38:50.351652 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:38:52.350016 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:38:52.431023 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9195
I1107 14:38:52.431023 11160 solver.cpp:397]     Test net output #1: loss = 0.310158 (* 1 = 0.310158 loss)
I1107 14:38:52.514029 11160 solver.cpp:218] Iteration 174000 (9.35037 iter/s, 10.6948s/100 iters), loss = 0.0296496
I1107 14:38:52.514029 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:38:52.514029 11160 solver.cpp:237]     Train net output #1: loss = 0.0296496 (* 1 = 0.0296496 loss)
I1107 14:38:52.514029 11160 sgd_solver.cpp:105] Iteration 174000, lr = 0.0001
I1107 14:39:01.064237 11160 solver.cpp:218] Iteration 174100 (11.6962 iter/s, 8.54977s/100 iters), loss = 0.0267559
I1107 14:39:01.064237 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:39:01.064237 11160 solver.cpp:237]     Train net output #1: loss = 0.0267559 (* 1 = 0.0267559 loss)
I1107 14:39:01.064237 11160 sgd_solver.cpp:105] Iteration 174100, lr = 0.0001
I1107 14:39:09.661779 11160 solver.cpp:218] Iteration 174200 (11.6319 iter/s, 8.59702s/100 iters), loss = 0.0177388
I1107 14:39:09.661779 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:39:09.661779 11160 solver.cpp:237]     Train net output #1: loss = 0.0177387 (* 1 = 0.0177387 loss)
I1107 14:39:09.661779 11160 sgd_solver.cpp:105] Iteration 174200, lr = 0.0001
I1107 14:39:18.249089 11160 solver.cpp:218] Iteration 174300 (11.645 iter/s, 8.58735s/100 iters), loss = 0.0174454
I1107 14:39:18.249089 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:39:18.249089 11160 solver.cpp:237]     Train net output #1: loss = 0.0174454 (* 1 = 0.0174454 loss)
I1107 14:39:18.249089 11160 sgd_solver.cpp:105] Iteration 174300, lr = 0.0001
I1107 14:39:26.810575 11160 solver.cpp:218] Iteration 174400 (11.6815 iter/s, 8.56058s/100 iters), loss = 0.0244493
I1107 14:39:26.810575 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:39:26.810575 11160 solver.cpp:237]     Train net output #1: loss = 0.0244493 (* 1 = 0.0244493 loss)
I1107 14:39:26.811065 11160 sgd_solver.cpp:105] Iteration 174400, lr = 0.0001
I1107 14:39:34.924832  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:39:35.262466 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_174500.caffemodel
I1107 14:39:35.292465 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_174500.solverstate
I1107 14:39:35.302480 11160 solver.cpp:330] Iteration 174500, Testing net (#0)
I1107 14:39:35.302480 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:39:37.292327 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:39:37.371835 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 14:39:37.371835 11160 solver.cpp:397]     Test net output #1: loss = 0.310263 (* 1 = 0.310263 loss)
I1107 14:39:37.452868 11160 solver.cpp:218] Iteration 174500 (9.39651 iter/s, 10.6422s/100 iters), loss = 0.038503
I1107 14:39:37.452868 11160 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 14:39:37.452868 11160 solver.cpp:237]     Train net output #1: loss = 0.038503 (* 1 = 0.038503 loss)
I1107 14:39:37.453857 11160 sgd_solver.cpp:105] Iteration 174500, lr = 0.0001
I1107 14:39:45.993747 11160 solver.cpp:218] Iteration 174600 (11.7092 iter/s, 8.54027s/100 iters), loss = 0.025477
I1107 14:39:45.993747 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:39:45.993747 11160 solver.cpp:237]     Train net output #1: loss = 0.025477 (* 1 = 0.025477 loss)
I1107 14:39:45.993747 11160 sgd_solver.cpp:105] Iteration 174600, lr = 0.0001
I1107 14:39:54.555500 11160 solver.cpp:218] Iteration 174700 (11.6811 iter/s, 8.56083s/100 iters), loss = 0.0206916
I1107 14:39:54.555500 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:39:54.555500 11160 solver.cpp:237]     Train net output #1: loss = 0.0206916 (* 1 = 0.0206916 loss)
I1107 14:39:54.555500 11160 sgd_solver.cpp:105] Iteration 174700, lr = 0.0001
I1107 14:40:03.160724 11160 solver.cpp:218] Iteration 174800 (11.622 iter/s, 8.60437s/100 iters), loss = 0.0172058
I1107 14:40:03.160724 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:40:03.160724 11160 solver.cpp:237]     Train net output #1: loss = 0.0172058 (* 1 = 0.0172058 loss)
I1107 14:40:03.160724 11160 sgd_solver.cpp:105] Iteration 174800, lr = 0.0001
I1107 14:40:11.708536 11160 solver.cpp:218] Iteration 174900 (11.6983 iter/s, 8.54827s/100 iters), loss = 0.0260371
I1107 14:40:11.709537 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:40:11.709537 11160 solver.cpp:237]     Train net output #1: loss = 0.0260371 (* 1 = 0.0260371 loss)
I1107 14:40:11.709537 11160 sgd_solver.cpp:105] Iteration 174900, lr = 0.0001
I1107 14:40:19.836447  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:40:20.175557 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_175000.caffemodel
I1107 14:40:20.205557 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_175000.solverstate
I1107 14:40:20.214557 11160 solver.cpp:330] Iteration 175000, Testing net (#0)
I1107 14:40:20.214557 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:40:22.231760 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:40:22.310768 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 14:40:22.310768 11160 solver.cpp:397]     Test net output #1: loss = 0.310689 (* 1 = 0.310689 loss)
I1107 14:40:22.392782 11160 solver.cpp:218] Iteration 175000 (9.3602 iter/s, 10.6835s/100 iters), loss = 0.0247887
I1107 14:40:22.392782 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:40:22.392782 11160 solver.cpp:237]     Train net output #1: loss = 0.0247887 (* 1 = 0.0247887 loss)
I1107 14:40:22.392782 11160 sgd_solver.cpp:105] Iteration 175000, lr = 0.0001
I1107 14:40:30.938555 11160 solver.cpp:218] Iteration 175100 (11.7026 iter/s, 8.54514s/100 iters), loss = 0.027933
I1107 14:40:30.938555 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:40:30.938555 11160 solver.cpp:237]     Train net output #1: loss = 0.027933 (* 1 = 0.027933 loss)
I1107 14:40:30.938555 11160 sgd_solver.cpp:105] Iteration 175100, lr = 0.0001
I1107 14:40:39.483232 11160 solver.cpp:218] Iteration 175200 (11.705 iter/s, 8.54338s/100 iters), loss = 0.0162924
I1107 14:40:39.483232 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:40:39.483232 11160 solver.cpp:237]     Train net output #1: loss = 0.0162924 (* 1 = 0.0162924 loss)
I1107 14:40:39.483232 11160 sgd_solver.cpp:105] Iteration 175200, lr = 0.0001
I1107 14:40:48.049702 11160 solver.cpp:218] Iteration 175300 (11.6731 iter/s, 8.56672s/100 iters), loss = 0.0156323
I1107 14:40:48.049702 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:40:48.049702 11160 solver.cpp:237]     Train net output #1: loss = 0.0156323 (* 1 = 0.0156323 loss)
I1107 14:40:48.049702 11160 sgd_solver.cpp:105] Iteration 175300, lr = 0.0001
I1107 14:40:56.727094 11160 solver.cpp:218] Iteration 175400 (11.526 iter/s, 8.67605s/100 iters), loss = 0.0215372
I1107 14:40:56.727094 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:40:56.727094 11160 solver.cpp:237]     Train net output #1: loss = 0.0215372 (* 1 = 0.0215372 loss)
I1107 14:40:56.727094 11160 sgd_solver.cpp:105] Iteration 175400, lr = 0.0001
I1107 14:41:04.996950  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:41:05.339489 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_175500.caffemodel
I1107 14:41:05.369495 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_175500.solverstate
I1107 14:41:05.398555 11160 solver.cpp:330] Iteration 175500, Testing net (#0)
I1107 14:41:05.398555 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:41:07.436785 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:41:07.515787 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9201
I1107 14:41:07.515787 11160 solver.cpp:397]     Test net output #1: loss = 0.310336 (* 1 = 0.310336 loss)
I1107 14:41:07.596802 11160 solver.cpp:218] Iteration 175500 (9.19961 iter/s, 10.87s/100 iters), loss = 0.0264665
I1107 14:41:07.596802 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:41:07.596802 11160 solver.cpp:237]     Train net output #1: loss = 0.0264664 (* 1 = 0.0264664 loss)
I1107 14:41:07.596802 11160 sgd_solver.cpp:105] Iteration 175500, lr = 0.0001
I1107 14:41:16.288775 11160 solver.cpp:218] Iteration 175600 (11.5059 iter/s, 8.69118s/100 iters), loss = 0.0221425
I1107 14:41:16.288775 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:41:16.288775 11160 solver.cpp:237]     Train net output #1: loss = 0.0221425 (* 1 = 0.0221425 loss)
I1107 14:41:16.288775 11160 sgd_solver.cpp:105] Iteration 175600, lr = 0.0001
I1107 14:41:24.920892 11160 solver.cpp:218] Iteration 175700 (11.5856 iter/s, 8.63138s/100 iters), loss = 0.0208177
I1107 14:41:24.920892 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:41:24.920892 11160 solver.cpp:237]     Train net output #1: loss = 0.0208177 (* 1 = 0.0208177 loss)
I1107 14:41:24.920892 11160 sgd_solver.cpp:105] Iteration 175700, lr = 0.0001
I1107 14:41:33.536979 11160 solver.cpp:218] Iteration 175800 (11.6068 iter/s, 8.6156s/100 iters), loss = 0.0199259
I1107 14:41:33.537479 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:41:33.537479 11160 solver.cpp:237]     Train net output #1: loss = 0.0199258 (* 1 = 0.0199258 loss)
I1107 14:41:33.537479 11160 sgd_solver.cpp:105] Iteration 175800, lr = 0.0001
I1107 14:41:42.176002 11160 solver.cpp:218] Iteration 175900 (11.5766 iter/s, 8.6381s/100 iters), loss = 0.0234064
I1107 14:41:42.176002 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:41:42.176002 11160 solver.cpp:237]     Train net output #1: loss = 0.0234064 (* 1 = 0.0234064 loss)
I1107 14:41:42.176002 11160 sgd_solver.cpp:105] Iteration 175900, lr = 0.0001
I1107 14:41:50.427711  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:41:50.770057 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_176000.caffemodel
I1107 14:41:50.801056 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_176000.solverstate
I1107 14:41:50.810057 11160 solver.cpp:330] Iteration 176000, Testing net (#0)
I1107 14:41:50.810057 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:41:52.829809 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:41:52.910329 11160 solver.cpp:397]     Test net output #0: accuracy = 0.9195
I1107 14:41:52.910329 11160 solver.cpp:397]     Test net output #1: loss = 0.310364 (* 1 = 0.310364 loss)
I1107 14:41:52.992332 11160 solver.cpp:218] Iteration 176000 (9.24509 iter/s, 10.8166s/100 iters), loss = 0.0278901
I1107 14:41:52.992332 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:41:52.992332 11160 solver.cpp:237]     Train net output #1: loss = 0.0278901 (* 1 = 0.0278901 loss)
I1107 14:41:52.992332 11160 sgd_solver.cpp:105] Iteration 176000, lr = 0.0001
I1107 14:42:01.679383 11160 solver.cpp:218] Iteration 176100 (11.5128 iter/s, 8.68602s/100 iters), loss = 0.0241689
I1107 14:42:01.679383 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:42:01.679383 11160 solver.cpp:237]     Train net output #1: loss = 0.0241689 (* 1 = 0.0241689 loss)
I1107 14:42:01.679383 11160 sgd_solver.cpp:105] Iteration 176100, lr = 0.0001
I1107 14:42:10.280127 11160 solver.cpp:218] Iteration 176200 (11.6271 iter/s, 8.60056s/100 iters), loss = 0.019467
I1107 14:42:10.280127 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:42:10.280127 11160 solver.cpp:237]     Train net output #1: loss = 0.019467 (* 1 = 0.019467 loss)
I1107 14:42:10.280127 11160 sgd_solver.cpp:105] Iteration 176200, lr = 0.0001
I1107 14:42:18.833142 11160 solver.cpp:218] Iteration 176300 (11.6924 iter/s, 8.55256s/100 iters), loss = 0.020397
I1107 14:42:18.833142 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:42:18.833142 11160 solver.cpp:237]     Train net output #1: loss = 0.020397 (* 1 = 0.020397 loss)
I1107 14:42:18.833142 11160 sgd_solver.cpp:105] Iteration 176300, lr = 0.0001
I1107 14:42:27.403192 11160 solver.cpp:218] Iteration 176400 (11.6698 iter/s, 8.56915s/100 iters), loss = 0.0224535
I1107 14:42:27.403192 11160 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 14:42:27.403192 11160 solver.cpp:237]     Train net output #1: loss = 0.0224535 (* 1 = 0.0224535 loss)
I1107 14:42:27.403192 11160 sgd_solver.cpp:105] Iteration 176400, lr = 0.0001
I1107 14:42:35.521019  1048 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:42:35.857046 11160 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_176500.caffemodel
I1107 14:42:35.888059 11160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_176500.solverstate
I1107 14:42:35.924046 11160 solver.cpp:330] Iteration 176500, Testing net (#0)
I1107 14:42:35.924046 11160 net.cpp:676] Ignoring source layer accuracy_training
I1107 14:42:37.916213 16904 data_layer.cpp:73] Restarting data prefetching from start.
I1107 14:42:37.996219 11160 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 14:42:37.996219 11160 solver.cpp:397]     Test net output #1: loss = 0.311569 (* 1 = 0.311569 loss)
I1107 14:42:38.078225 11160 solver.cpp:218] Iteration 176500 (9.36836 iter/s, 10.6742s/100 iters), loss = 0.0429655
I1107 14:42:38.0782
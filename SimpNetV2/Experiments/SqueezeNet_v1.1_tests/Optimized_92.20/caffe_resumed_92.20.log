
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt --snapshot=examples/cifar10/snaps/squeezenet_batchnorm_iter_90000.solverstate 
I1107 16:43:07.848184  5452 caffe.cpp:219] Using GPUs 0
I1107 16:43:08.048326  5452 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1107 16:43:08.365622  5452 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1107 16:43:08.383622  5452 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/squeezenet_batchnorm"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 195000
stepvalue: 220000
stepvalue: 270000
type: "AdaDelta"
I1107 16:43:08.384624  5452 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1107 16:43:08.385620  5452 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1107 16:43:08.385620  5452 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1107 16:43:08.385620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1107 16:43:08.385620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1107 16:43:08.385620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_3
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_4
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_5
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_6
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_7
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_8
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_9
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_10
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_11
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_12
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_13
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_14
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_15
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_16
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_17
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_18
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_19
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_20
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_21
I1107 16:43:08.386620  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_22
I1107 16:43:08.387635  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_23
I1107 16:43:08.387635  5452 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1107 16:43:08.387635  5452 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_Squeezenet_1.1_Batchnorm"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "fire2/squeeze1x1"
  type: "Convolution"
  bottom: "pool1"
  top: "fire2/squeeze1x1"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire2/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire2/squeeze1x1"
  top: "fire2/squeeze1x1"
}
layer {
  name: "fire2/expand1x1"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "conv2"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "fire2/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire2/relu_expand1x1"
  type: "ReLU"
  bottom: "fire2/expand1x1"
  top: "fire2/expand1x1"
}
layer {
  name: "fire2/expand3x3"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "conv_3"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_3"
  type: "BatchNorm"
  bottom: "conv_3"
  top: "conv_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_3"
  type: "Scale"
  bottom: "conv_3"
  top: "fire2/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire2/relu_expand3x3"
  type: "ReLU"
  bottom: "fire2/expand3x3"
  top: "fire2/expand3x3"
}
layer {
  name: "fire2/concat"
  type: "Concat"
  bottom: "fire2/expand1x1"
  bottom: "fire2/expand3x3"
  top: "fire2/concat"
}
layer {
  name: "fire3/squeeze1x1"
  type: "Convolution"
  bottom: "fire2/concat"
  top: "conv_4"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_4"
  type: "BatchNorm"
  bottom: "conv_4"
  top: "conv_4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_4"
  type: "Scale"
  bottom: "conv_4"
  top: "fire3/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire3/squeeze1x1"
  top: "fire3/squeeze1x1"
}
layer {
  name: "fire3/expand1x1"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "conv_5"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_5"
  type: "BatchNorm"
  bottom: "conv_5"
  top: "conv_5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_5"
  type: "Scale"
  bottom: "conv_5"
  top: "fire3/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_expand1x1"
  type: "ReLU"
  bottom: "fire3/expand1x1"
  top: "fire3/expand1x1"
}
layer {
  name: "fire3/expand3x3"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "conv_6"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_6"
  type: "BatchNorm"
  bottom: "conv_6"
  top: "conv_6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_6"
  type: "Scale"
  bottom: "conv_6"
  top: "fire3/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_expand3x3"
  type: "ReLU"
  bottom: "fire3/expand3x3"
  top: "fire3/expand3x3"
}
layer {
  name: "fire3/concat"
  type: "Concat"
  bottom: "fire3/expand1x1"
  bottom: "fire3/expand3x3"
  top: "fire3/concat"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "fire3/concat"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire4/squeeze1x1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv_7"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_7"
  type: "BatchNorm"
  bottom: "conv_7"
  top: "conv_7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_7"
  type: "Scale"
  bottom: "conv_7"
  top: "fire4/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire4/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire4/squeeze1x1"
  top: "fire4/squeeze1x1"
}
layer {
  name: "fire4/expand1x1"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "fire4/expand1x1"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire4/relu_expand1x1"
  type: "ReLU"
  bottom: "fire4/expand1x1"
  top: "fire4/expand1x1"
}
layer {
  name: "fire4/expand3x3"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "conv_8"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_8"
  type: "BatchNorm"
  bottom: "conv_8"
  top: "conv_8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_8"
  type: "Scale"
  bottom: "conv_8"
  top: "fire4/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire4/relu_expand3x3"
  type: "ReLU"
  bottom: "fire4/expand3x3"
  top: "fire4/expand3x3"
}
layer {
  name: "fire4/concat"
  type: "Concat"
  bottom: "fire4/expand1x1"
  bottom: "fire4/expand3x3"
  top: "fire4/concat"
}
layer {
  name: "fire5/squeeze1x1"
  type: "Convolution"
  bottom: "fire4/concat"
  top: "conv_9"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_9"
  type: "BatchNorm"
  bottom: "conv_9"
  top: "conv_9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_9"
  type: "Scale"
  bottom: "conv_9"
  top: "fire5/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire5/squeeze1x1"
  top: "fire5/squeeze1x1"
}
layer {
  name: "fire5/expand1x1"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "conv_10"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_10"
  type: "BatchNorm"
  bottom: "conv_10"
  top: "conv_10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_10"
  type: "Scale"
  bottom: "conv_10"
  top: "fire5/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_expand1x1"
  type: "ReLU"
  bottom: "fire5/expand1x1"
  top: "fire5/expand1x1"
}
layer {
  name: "fire5/expand3x3"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "conv_11"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_11"
  type: "BatchNorm"
  bottom: "conv_11"
  top: "conv_11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_11"
  type: "Scale"
  bottom: "conv_11"
  top: "fire5/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_expand3x3"
  type: "ReLU"
  bottom: "fire5/expand3x3"
  top: "fire5/expand3x3"
}
layer {
  name: "fire5/concat"
  type: "Concat"
  bottom: "fire5/expand1x1"
  bottom: "fire5/expand3x3"
  top: "fire5/concat"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "fire5/concat"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire6/squeeze1x1"
  type: "Convolution"
  bottom: "pool5"
  top: "conv_12"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_12"
  type: "BatchNorm"
  bottom: "conv_12"
  top: "conv_12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_12"
  type: "Scale"
  bottom: "conv_12"
  top: "fire6/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire6/squeeze1x1"
  top: "fire6/squeeze1x1"
}
layer {
  name: "fire6/expand1x1"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "conv_13"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_13"
  type: "BatchNorm"
  bottom: "conv_13"
  top: "conv_13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_13"
  type: "Scale"
  bottom: "conv_13"
  top: "fire6/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_expand1x1"
  type: "ReLU"
  bottom: "fire6/expand1x1"
  top: "fire6/expand1x1"
}
layer {
  name: "fire6/expand3x3"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "conv_14"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_14"
  type: "BatchNorm"
  bottom: "conv_14"
  top: "conv_14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_14"
  type: "Scale"
  bottom: "conv_14"
  top: "fire6/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_expand3x3"
  type: "ReLU"
  bottom: "fire6/expand3x3"
  top: "fire6/expand3x3"
}
layer {
  name: "fire6/concat"
  type: "Concat"
  bottom: "fire6/expand1x1"
  bottom: "fire6/expand3x3"
  top: "fire6/concat"
}
layer {
  name: "fire7/squeeze1x1"
  type: "Convolution"
  bottom: "fire6/concat"
  top: "conv_15"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_15"
  type: "BatchNorm"
  bottom: "conv_15"
  top: "conv_15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_15"
  type: "Scale"
  bottom: "conv_15"
  top: "fire7/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire7/squeeze1x1"
  top: "fire7/squeeze1x1"
}
layer {
  name: "fire7/expand1x1"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "conv_16"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_16"
  type: "BatchNorm"
  bottom: "conv_16"
  top: "conv_16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_16"
  type: "Scale"
  bottom: "conv_16"
  top: "fire7/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_expand1x1"
  type: "ReLU"
  bottom: "fire7/expand1x1"
  top: "fire7/expand1x1"
}
layer {
  name: "fire7/expand3x3"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "conv_17"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_17"
  type: "BatchNorm"
  bottom: "conv_17"
  top: "conv_17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_17"
  type: "Scale"
  bottom: "conv_17"
  top: "fire7/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_expand3x3"
  type: "ReLU"
  bottom: "fire7/expand3x3"
  top: "fire7/expand3x3"
}
layer {
  name: "fire7/concat"
  type: "Concat"
  bottom: "fire7/expand1x1"
  bottom: "fire7/expand3x3"
  top: "fire7/concat"
}
layer {
  name: "fire8/squeeze1x1"
  type: "Convolution"
  bottom: "fire7/concat"
  top: "conv_18"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_18"
  type: "BatchNorm"
  bottom: "conv_18"
  top: "conv_18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_18"
  type: "Scale"
  bottom: "conv_18"
  top: "fire8/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire8/squeeze1x1"
  top: "fire8/squeeze1x1"
}
layer {
  name: "fire8/expand1x1"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "conv_19"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_19"
  type: "BatchNorm"
  bottom: "conv_19"
  top: "conv_19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_19"
  type: "Scale"
  bottom: "conv_19"
  top: "fire8/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_expand1x1"
  type: "ReLU"
  bottom: "fire8/expand1x1"
  top: "fire8/expand1x1"
}
layer {
  name: "fire8/expand3x3"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "conv_20"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_20"
  type: "BatchNorm"
  bottom: "conv_20"
  top: "conv_20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_20"
  type: "Scale"
  bottom: "conv_20"
  top: "fire8/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_expand3x3"
  type: "ReLU"
  bottom: "fire8/expand3x3"
  top: "fire8/expand3x3"
}
layer {
  name: "fire8/concat"
  type: "Concat"
  bottom: "fire8/expand1x1"
  bottom: "fire8/expand3x3"
  top: "fire8/concat"
}
layer {
  name: "fire9/squeeze1x1"
  type: "Convolution"
  bottom: "fire8/concat"
  top: "conv_21"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_21"
  type: "BatchNorm"
  bottom: "conv_21"
  top: "conv_21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_21"
  type: "Scale"
  bottom: "conv_21"
  top: "fire9/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire9/squeeze1x1"
  top: "fire9/squeeze1x1"
}
layer {
  name: "fire9/expand1x1"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "conv_22"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_22"
  type: "BatchNorm"
  bottom: "conv_22"
  top: "conv_22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_22"
  type: "Scale"
  bottom: "conv_22"
  top: "fire9/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_expand1x1"
  type: "ReLU"
  bottom: "fire9/expand1x1"
  top: "fire9/expand1x1"
}
layer {
  name: "fire9/expand3x3"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "conv_23"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_23"
  type: "BatchNorm"
  bottom: "conv_23"
  top: "conv_23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_23"
  type: "Scale"
  bottom: "conv_23"
  top: "fire9/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_expand3x3"
  type: "ReLU"
  bottom: "fire9/expand3x3"
  top: "fire9/expand3x3"
}
layer {
  name: "fire9/concat"
  type: "Concat"
  bottom: "fire9/expand1x1"
  bottom: "fire9/expand3x3"
  top: "fire9/concat"
}
layer {
  name: "drop9"
  type: "Dropout"
  bottom: "fire9/concat"
  top: "fire9/concat"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv10"
  type: "Convolution"
  bottom: "fire9/concat"
  top: "conv10"
  convolution_param {
    num_output: 10
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
  }
}
layer {
  name: "relu_conv10"
  type: "ReLU"
  bottom: "conv10"
  top: "conv10"
}
layer {
  name: "pool10"
  type: "Pooling"
  bottom: "conv10"
  top: "pool10"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "pool10"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "pool10"
  bottom: "label"
  top: "loss"
}
I1107 16:43:08.388622  5452 layer_factory.cpp:58] Creating layer cifar
I1107 16:43:08.393621  5452 db_leveldb.cpp:18] Opened leveldb examples/cifar10/cifar10_train_leveldb_padding
I1107 16:43:08.394620  5452 net.cpp:84] Creating Layer cifar
I1107 16:43:08.394620  5452 net.cpp:380] cifar -> data
I1107 16:43:08.394620  5452 net.cpp:380] cifar -> label
I1107 16:43:08.395622  5452 data_layer.cpp:45] output data size: 100,3,32,32
I1107 16:43:08.401618  5452 net.cpp:122] Setting up cifar
I1107 16:43:08.401618  5452 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1107 16:43:08.401618  5452 net.cpp:129] Top shape: 100 (100)
I1107 16:43:08.401618  5452 net.cpp:137] Memory required for data: 1229200
I1107 16:43:08.401618  5452 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1107 16:43:08.401618  5452 net.cpp:84] Creating Layer label_cifar_1_split
I1107 16:43:08.401618  5452 net.cpp:406] label_cifar_1_split <- label
I1107 16:43:08.401618  5452 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1107 16:43:08.401618  5452 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1107 16:43:08.401618  5452 net.cpp:122] Setting up label_cifar_1_split
I1107 16:43:08.401618  5452 net.cpp:129] Top shape: 100 (100)
I1107 16:43:08.401618  5452 net.cpp:129] Top shape: 100 (100)
I1107 16:43:08.401618  5452 net.cpp:137] Memory required for data: 1230000
I1107 16:43:08.401618  5452 layer_factory.cpp:58] Creating layer conv1
I1107 16:43:08.401618  5452 net.cpp:84] Creating Layer conv1
I1107 16:43:08.401618  5452 net.cpp:406] conv1 <- data
I1107 16:43:08.401618  5452 net.cpp:380] conv1 -> conv1
I1107 16:43:08.401618 18344 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1107 16:43:08.652663  5452 net.cpp:122] Setting up conv1
I1107 16:43:08.652663  5452 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 16:43:08.652663  5452 net.cpp:137] Memory required for data: 24270000
I1107 16:43:08.652663  5452 layer_factory.cpp:58] Creating layer bn1
I1107 16:43:08.652663  5452 net.cpp:84] Creating Layer bn1
I1107 16:43:08.652663  5452 net.cpp:406] bn1 <- conv1
I1107 16:43:08.652663  5452 net.cpp:367] bn1 -> conv1 (in-place)
I1107 16:43:08.652663  5452 net.cpp:122] Setting up bn1
I1107 16:43:08.652663  5452 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 16:43:08.652663  5452 net.cpp:137] Memory required for data: 47310000
I1107 16:43:08.652663  5452 layer_factory.cpp:58] Creating layer scale1
I1107 16:43:08.652663  5452 net.cpp:84] Creating Layer scale1
I1107 16:43:08.653663  5452 net.cpp:406] scale1 <- conv1
I1107 16:43:08.653663  5452 net.cpp:367] scale1 -> conv1 (in-place)
I1107 16:43:08.653663  5452 layer_factory.cpp:58] Creating layer scale1
I1107 16:43:08.653663  5452 net.cpp:122] Setting up scale1
I1107 16:43:08.653663  5452 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 16:43:08.653663  5452 net.cpp:137] Memory required for data: 70350000
I1107 16:43:08.653663  5452 layer_factory.cpp:58] Creating layer relu_conv1
I1107 16:43:08.653663  5452 net.cpp:84] Creating Layer relu_conv1
I1107 16:43:08.653663  5452 net.cpp:406] relu_conv1 <- conv1
I1107 16:43:08.653663  5452 net.cpp:367] relu_conv1 -> conv1 (in-place)
I1107 16:43:08.653663  5452 net.cpp:122] Setting up relu_conv1
I1107 16:43:08.653663  5452 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 16:43:08.653663  5452 net.cpp:137] Memory required for data: 93390000
I1107 16:43:08.653663  5452 layer_factory.cpp:58] Creating layer pool1
I1107 16:43:08.653663  5452 net.cpp:84] Creating Layer pool1
I1107 16:43:08.653663  5452 net.cpp:406] pool1 <- conv1
I1107 16:43:08.653663  5452 net.cpp:380] pool1 -> pool1
I1107 16:43:08.653663  5452 net.cpp:122] Setting up pool1
I1107 16:43:08.653663  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.653663  5452 net.cpp:137] Memory required for data: 113460400
I1107 16:43:08.653663  5452 layer_factory.cpp:58] Creating layer fire2/squeeze1x1
I1107 16:43:08.653663  5452 net.cpp:84] Creating Layer fire2/squeeze1x1
I1107 16:43:08.653663  5452 net.cpp:406] fire2/squeeze1x1 <- pool1
I1107 16:43:08.653663  5452 net.cpp:380] fire2/squeeze1x1 -> fire2/squeeze1x1
I1107 16:43:08.654664  5452 net.cpp:122] Setting up fire2/squeeze1x1
I1107 16:43:08.654664  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.654664  5452 net.cpp:137] Memory required for data: 118478000
I1107 16:43:08.654664  5452 layer_factory.cpp:58] Creating layer fire2/relu_squeeze1x1
I1107 16:43:08.654664  5452 net.cpp:84] Creating Layer fire2/relu_squeeze1x1
I1107 16:43:08.654664  5452 net.cpp:406] fire2/relu_squeeze1x1 <- fire2/squeeze1x1
I1107 16:43:08.654664  5452 net.cpp:367] fire2/relu_squeeze1x1 -> fire2/squeeze1x1 (in-place)
I1107 16:43:08.654664  5452 net.cpp:122] Setting up fire2/relu_squeeze1x1
I1107 16:43:08.654664  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.655663  5452 net.cpp:137] Memory required for data: 123495600
I1107 16:43:08.655663  5452 layer_factory.cpp:58] Creating layer fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 16:43:08.655663  5452 net.cpp:84] Creating Layer fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 16:43:08.655663  5452 net.cpp:406] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split <- fire2/squeeze1x1
I1107 16:43:08.655663  5452 net.cpp:380] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split -> fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_0
I1107 16:43:08.655663  5452 net.cpp:380] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split -> fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_1
I1107 16:43:08.655663  5452 net.cpp:122] Setting up fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 16:43:08.655663  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.655663  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.655663  5452 net.cpp:137] Memory required for data: 133530800
I1107 16:43:08.655663  5452 layer_factory.cpp:58] Creating layer fire2/expand1x1
I1107 16:43:08.655663  5452 net.cpp:84] Creating Layer fire2/expand1x1
I1107 16:43:08.655663  5452 net.cpp:406] fire2/expand1x1 <- fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_0
I1107 16:43:08.655663  5452 net.cpp:380] fire2/expand1x1 -> conv2
I1107 16:43:08.655663  5452 net.cpp:122] Setting up fire2/expand1x1
I1107 16:43:08.655663  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.655663  5452 net.cpp:137] Memory required for data: 153601200
I1107 16:43:08.655663  5452 layer_factory.cpp:58] Creating layer bn2
I1107 16:43:08.655663  5452 net.cpp:84] Creating Layer bn2
I1107 16:43:08.655663  5452 net.cpp:406] bn2 <- conv2
I1107 16:43:08.655663  5452 net.cpp:367] bn2 -> conv2 (in-place)
I1107 16:43:08.656664  5452 net.cpp:122] Setting up bn2
I1107 16:43:08.656664  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.656664  5452 net.cpp:137] Memory required for data: 173671600
I1107 16:43:08.656664  5452 layer_factory.cpp:58] Creating layer scale2
I1107 16:43:08.656664  5452 net.cpp:84] Creating Layer scale2
I1107 16:43:08.656664  5452 net.cpp:406] scale2 <- conv2
I1107 16:43:08.656664  5452 net.cpp:380] scale2 -> fire2/expand1x1
I1107 16:43:08.656664  5452 layer_factory.cpp:58] Creating layer scale2
I1107 16:43:08.656664  5452 net.cpp:122] Setting up scale2
I1107 16:43:08.656664  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.656664  5452 net.cpp:137] Memory required for data: 193742000
I1107 16:43:08.656664  5452 layer_factory.cpp:58] Creating layer fire2/relu_expand1x1
I1107 16:43:08.656664  5452 net.cpp:84] Creating Layer fire2/relu_expand1x1
I1107 16:43:08.656664  5452 net.cpp:406] fire2/relu_expand1x1 <- fire2/expand1x1
I1107 16:43:08.656664  5452 net.cpp:367] fire2/relu_expand1x1 -> fire2/expand1x1 (in-place)
I1107 16:43:08.656664  5452 net.cpp:122] Setting up fire2/relu_expand1x1
I1107 16:43:08.656664  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.656664  5452 net.cpp:137] Memory required for data: 213812400
I1107 16:43:08.656664  5452 layer_factory.cpp:58] Creating layer fire2/expand3x3
I1107 16:43:08.656664  5452 net.cpp:84] Creating Layer fire2/expand3x3
I1107 16:43:08.656664  5452 net.cpp:406] fire2/expand3x3 <- fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_1
I1107 16:43:08.656664  5452 net.cpp:380] fire2/expand3x3 -> conv_3
I1107 16:43:08.658663  5452 net.cpp:122] Setting up fire2/expand3x3
I1107 16:43:08.658663  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.658663  5452 net.cpp:137] Memory required for data: 233882800
I1107 16:43:08.658663  5452 layer_factory.cpp:58] Creating layer bn_3
I1107 16:43:08.658663  5452 net.cpp:84] Creating Layer bn_3
I1107 16:43:08.658663  5452 net.cpp:406] bn_3 <- conv_3
I1107 16:43:08.658663  5452 net.cpp:367] bn_3 -> conv_3 (in-place)
I1107 16:43:08.658663  5452 net.cpp:122] Setting up bn_3
I1107 16:43:08.658663  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.658663  5452 net.cpp:137] Memory required for data: 253953200
I1107 16:43:08.658663  5452 layer_factory.cpp:58] Creating layer scale_3
I1107 16:43:08.658663  5452 net.cpp:84] Creating Layer scale_3
I1107 16:43:08.658663  5452 net.cpp:406] scale_3 <- conv_3
I1107 16:43:08.658663  5452 net.cpp:380] scale_3 -> fire2/expand3x3
I1107 16:43:08.658663  5452 layer_factory.cpp:58] Creating layer scale_3
I1107 16:43:08.658663  5452 net.cpp:122] Setting up scale_3
I1107 16:43:08.658663  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.658663  5452 net.cpp:137] Memory required for data: 274023600
I1107 16:43:08.658663  5452 layer_factory.cpp:58] Creating layer fire2/relu_expand3x3
I1107 16:43:08.658663  5452 net.cpp:84] Creating Layer fire2/relu_expand3x3
I1107 16:43:08.658663  5452 net.cpp:406] fire2/relu_expand3x3 <- fire2/expand3x3
I1107 16:43:08.658663  5452 net.cpp:367] fire2/relu_expand3x3 -> fire2/expand3x3 (in-place)
I1107 16:43:08.659663  5452 net.cpp:122] Setting up fire2/relu_expand3x3
I1107 16:43:08.659663  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.659663  5452 net.cpp:137] Memory required for data: 294094000
I1107 16:43:08.659663  5452 layer_factory.cpp:58] Creating layer fire2/concat
I1107 16:43:08.659663  5452 net.cpp:84] Creating Layer fire2/concat
I1107 16:43:08.659663  5452 net.cpp:406] fire2/concat <- fire2/expand1x1
I1107 16:43:08.659663  5452 net.cpp:406] fire2/concat <- fire2/expand3x3
I1107 16:43:08.659663  5452 net.cpp:380] fire2/concat -> fire2/concat
I1107 16:43:08.659663  5452 net.cpp:122] Setting up fire2/concat
I1107 16:43:08.659663  5452 net.cpp:129] Top shape: 100 128 28 28 (10035200)
I1107 16:43:08.659663  5452 net.cpp:137] Memory required for data: 334234800
I1107 16:43:08.659663  5452 layer_factory.cpp:58] Creating layer fire3/squeeze1x1
I1107 16:43:08.659663  5452 net.cpp:84] Creating Layer fire3/squeeze1x1
I1107 16:43:08.659663  5452 net.cpp:406] fire3/squeeze1x1 <- fire2/concat
I1107 16:43:08.659663  5452 net.cpp:380] fire3/squeeze1x1 -> conv_4
I1107 16:43:08.660665  5452 net.cpp:122] Setting up fire3/squeeze1x1
I1107 16:43:08.660665  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.660665  5452 net.cpp:137] Memory required for data: 339252400
I1107 16:43:08.660665  5452 layer_factory.cpp:58] Creating layer bn_4
I1107 16:43:08.660665  5452 net.cpp:84] Creating Layer bn_4
I1107 16:43:08.660665  5452 net.cpp:406] bn_4 <- conv_4
I1107 16:43:08.660665  5452 net.cpp:367] bn_4 -> conv_4 (in-place)
I1107 16:43:08.660665  5452 net.cpp:122] Setting up bn_4
I1107 16:43:08.660665  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.660665  5452 net.cpp:137] Memory required for data: 344270000
I1107 16:43:08.660665  5452 layer_factory.cpp:58] Creating layer scale_4
I1107 16:43:08.660665  5452 net.cpp:84] Creating Layer scale_4
I1107 16:43:08.660665  5452 net.cpp:406] scale_4 <- conv_4
I1107 16:43:08.660665  5452 net.cpp:380] scale_4 -> fire3/squeeze1x1
I1107 16:43:08.660665  5452 layer_factory.cpp:58] Creating layer scale_4
I1107 16:43:08.660665  5452 net.cpp:122] Setting up scale_4
I1107 16:43:08.660665  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.660665  5452 net.cpp:137] Memory required for data: 349287600
I1107 16:43:08.660665  5452 layer_factory.cpp:58] Creating layer fire3/relu_squeeze1x1
I1107 16:43:08.660665  5452 net.cpp:84] Creating Layer fire3/relu_squeeze1x1
I1107 16:43:08.660665  5452 net.cpp:406] fire3/relu_squeeze1x1 <- fire3/squeeze1x1
I1107 16:43:08.660665  5452 net.cpp:367] fire3/relu_squeeze1x1 -> fire3/squeeze1x1 (in-place)
I1107 16:43:08.660665  5452 net.cpp:122] Setting up fire3/relu_squeeze1x1
I1107 16:43:08.660665  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.660665  5452 net.cpp:137] Memory required for data: 354305200
I1107 16:43:08.660665  5452 layer_factory.cpp:58] Creating layer fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 16:43:08.660665  5452 net.cpp:84] Creating Layer fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 16:43:08.660665  5452 net.cpp:406] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split <- fire3/squeeze1x1
I1107 16:43:08.660665  5452 net.cpp:380] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split -> fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_0
I1107 16:43:08.660665  5452 net.cpp:380] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split -> fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_1
I1107 16:43:08.660665  5452 net.cpp:122] Setting up fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 16:43:08.660665  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.660665  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.660665  5452 net.cpp:137] Memory required for data: 364340400
I1107 16:43:08.660665  5452 layer_factory.cpp:58] Creating layer fire3/expand1x1
I1107 16:43:08.660665  5452 net.cpp:84] Creating Layer fire3/expand1x1
I1107 16:43:08.660665  5452 net.cpp:406] fire3/expand1x1 <- fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_0
I1107 16:43:08.660665  5452 net.cpp:380] fire3/expand1x1 -> conv_5
I1107 16:43:08.661664  5452 net.cpp:122] Setting up fire3/expand1x1
I1107 16:43:08.661664  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.661664  5452 net.cpp:137] Memory required for data: 384410800
I1107 16:43:08.661664  5452 layer_factory.cpp:58] Creating layer bn_5
I1107 16:43:08.662663  5452 net.cpp:84] Creating Layer bn_5
I1107 16:43:08.662663  5452 net.cpp:406] bn_5 <- conv_5
I1107 16:43:08.662663  5452 net.cpp:367] bn_5 -> conv_5 (in-place)
I1107 16:43:08.662663  5452 net.cpp:122] Setting up bn_5
I1107 16:43:08.662663  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.662663  5452 net.cpp:137] Memory required for data: 404481200
I1107 16:43:08.662663  5452 layer_factory.cpp:58] Creating layer scale_5
I1107 16:43:08.662663  5452 net.cpp:84] Creating Layer scale_5
I1107 16:43:08.662663  5452 net.cpp:406] scale_5 <- conv_5
I1107 16:43:08.662663  5452 net.cpp:380] scale_5 -> fire3/expand1x1
I1107 16:43:08.662663  5452 layer_factory.cpp:58] Creating layer scale_5
I1107 16:43:08.662663  5452 net.cpp:122] Setting up scale_5
I1107 16:43:08.662663  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.662663  5452 net.cpp:137] Memory required for data: 424551600
I1107 16:43:08.662663  5452 layer_factory.cpp:58] Creating layer fire3/relu_expand1x1
I1107 16:43:08.662663  5452 net.cpp:84] Creating Layer fire3/relu_expand1x1
I1107 16:43:08.662663  5452 net.cpp:406] fire3/relu_expand1x1 <- fire3/expand1x1
I1107 16:43:08.662663  5452 net.cpp:367] fire3/relu_expand1x1 -> fire3/expand1x1 (in-place)
I1107 16:43:08.662663  5452 net.cpp:122] Setting up fire3/relu_expand1x1
I1107 16:43:08.662663  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.662663  5452 net.cpp:137] Memory required for data: 444622000
I1107 16:43:08.662663  5452 layer_factory.cpp:58] Creating layer fire3/expand3x3
I1107 16:43:08.662663  5452 net.cpp:84] Creating Layer fire3/expand3x3
I1107 16:43:08.662663  5452 net.cpp:406] fire3/expand3x3 <- fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_1
I1107 16:43:08.662663  5452 net.cpp:380] fire3/expand3x3 -> conv_6
I1107 16:43:08.663663  5452 net.cpp:122] Setting up fire3/expand3x3
I1107 16:43:08.663663  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.663663  5452 net.cpp:137] Memory required for data: 464692400
I1107 16:43:08.663663  5452 layer_factory.cpp:58] Creating layer bn_6
I1107 16:43:08.663663  5452 net.cpp:84] Creating Layer bn_6
I1107 16:43:08.663663  5452 net.cpp:406] bn_6 <- conv_6
I1107 16:43:08.663663  5452 net.cpp:367] bn_6 -> conv_6 (in-place)
I1107 16:43:08.663663  5452 net.cpp:122] Setting up bn_6
I1107 16:43:08.663663  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.663663  5452 net.cpp:137] Memory required for data: 484762800
I1107 16:43:08.663663  5452 layer_factory.cpp:58] Creating layer scale_6
I1107 16:43:08.663663  5452 net.cpp:84] Creating Layer scale_6
I1107 16:43:08.663663  5452 net.cpp:406] scale_6 <- conv_6
I1107 16:43:08.663663  5452 net.cpp:380] scale_6 -> fire3/expand3x3
I1107 16:43:08.663663  5452 layer_factory.cpp:58] Creating layer scale_6
I1107 16:43:08.664664  5452 net.cpp:122] Setting up scale_6
I1107 16:43:08.664664  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.664664  5452 net.cpp:137] Memory required for data: 504833200
I1107 16:43:08.664664  5452 layer_factory.cpp:58] Creating layer fire3/relu_expand3x3
I1107 16:43:08.664664  5452 net.cpp:84] Creating Layer fire3/relu_expand3x3
I1107 16:43:08.664664  5452 net.cpp:406] fire3/relu_expand3x3 <- fire3/expand3x3
I1107 16:43:08.664664  5452 net.cpp:367] fire3/relu_expand3x3 -> fire3/expand3x3 (in-place)
I1107 16:43:08.664664  5452 net.cpp:122] Setting up fire3/relu_expand3x3
I1107 16:43:08.664664  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.664664  5452 net.cpp:137] Memory required for data: 524903600
I1107 16:43:08.664664  5452 layer_factory.cpp:58] Creating layer fire3/concat
I1107 16:43:08.664664  5452 net.cpp:84] Creating Layer fire3/concat
I1107 16:43:08.664664  5452 net.cpp:406] fire3/concat <- fire3/expand1x1
I1107 16:43:08.664664  5452 net.cpp:406] fire3/concat <- fire3/expand3x3
I1107 16:43:08.664664  5452 net.cpp:380] fire3/concat -> fire3/concat
I1107 16:43:08.664664  5452 net.cpp:122] Setting up fire3/concat
I1107 16:43:08.664664  5452 net.cpp:129] Top shape: 100 128 28 28 (10035200)
I1107 16:43:08.664664  5452 net.cpp:137] Memory required for data: 565044400
I1107 16:43:08.664664  5452 layer_factory.cpp:58] Creating layer pool3
I1107 16:43:08.664664  5452 net.cpp:84] Creating Layer pool3
I1107 16:43:08.664664  5452 net.cpp:406] pool3 <- fire3/concat
I1107 16:43:08.664664  5452 net.cpp:380] pool3 -> pool3
I1107 16:43:08.664664  5452 net.cpp:122] Setting up pool3
I1107 16:43:08.664664  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.664664  5452 net.cpp:137] Memory required for data: 575079600
I1107 16:43:08.664664  5452 layer_factory.cpp:58] Creating layer fire4/squeeze1x1
I1107 16:43:08.664664  5452 net.cpp:84] Creating Layer fire4/squeeze1x1
I1107 16:43:08.664664  5452 net.cpp:406] fire4/squeeze1x1 <- pool3
I1107 16:43:08.664664  5452 net.cpp:380] fire4/squeeze1x1 -> conv_7
I1107 16:43:08.665673  5452 net.cpp:122] Setting up fire4/squeeze1x1
I1107 16:43:08.665673  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.665673  5452 net.cpp:137] Memory required for data: 577588400
I1107 16:43:08.665673  5452 layer_factory.cpp:58] Creating layer bn_7
I1107 16:43:08.665673  5452 net.cpp:84] Creating Layer bn_7
I1107 16:43:08.665673  5452 net.cpp:406] bn_7 <- conv_7
I1107 16:43:08.665673  5452 net.cpp:367] bn_7 -> conv_7 (in-place)
I1107 16:43:08.665673  5452 net.cpp:122] Setting up bn_7
I1107 16:43:08.665673  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.665673  5452 net.cpp:137] Memory required for data: 580097200
I1107 16:43:08.665673  5452 layer_factory.cpp:58] Creating layer scale_7
I1107 16:43:08.665673  5452 net.cpp:84] Creating Layer scale_7
I1107 16:43:08.665673  5452 net.cpp:406] scale_7 <- conv_7
I1107 16:43:08.665673  5452 net.cpp:380] scale_7 -> fire4/squeeze1x1
I1107 16:43:08.666663  5452 layer_factory.cpp:58] Creating layer scale_7
I1107 16:43:08.666663  5452 net.cpp:122] Setting up scale_7
I1107 16:43:08.666663  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.666663  5452 net.cpp:137] Memory required for data: 582606000
I1107 16:43:08.666663  5452 layer_factory.cpp:58] Creating layer fire4/relu_squeeze1x1
I1107 16:43:08.666663  5452 net.cpp:84] Creating Layer fire4/relu_squeeze1x1
I1107 16:43:08.666663  5452 net.cpp:406] fire4/relu_squeeze1x1 <- fire4/squeeze1x1
I1107 16:43:08.666663  5452 net.cpp:367] fire4/relu_squeeze1x1 -> fire4/squeeze1x1 (in-place)
I1107 16:43:08.666663  5452 net.cpp:122] Setting up fire4/relu_squeeze1x1
I1107 16:43:08.666663  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.666663  5452 net.cpp:137] Memory required for data: 585114800
I1107 16:43:08.666663  5452 layer_factory.cpp:58] Creating layer fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 16:43:08.666663  5452 net.cpp:84] Creating Layer fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 16:43:08.666663  5452 net.cpp:406] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split <- fire4/squeeze1x1
I1107 16:43:08.666663  5452 net.cpp:380] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split -> fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_0
I1107 16:43:08.666663  5452 net.cpp:380] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split -> fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_1
I1107 16:43:08.666663  5452 net.cpp:122] Setting up fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 16:43:08.666663  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.666663  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.666663  5452 net.cpp:137] Memory required for data: 590132400
I1107 16:43:08.666663  5452 layer_factory.cpp:58] Creating layer fire4/expand1x1
I1107 16:43:08.666663  5452 net.cpp:84] Creating Layer fire4/expand1x1
I1107 16:43:08.666663  5452 net.cpp:406] fire4/expand1x1 <- fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_0
I1107 16:43:08.666663  5452 net.cpp:380] fire4/expand1x1 -> fire4/expand1x1
I1107 16:43:08.667663  5452 net.cpp:122] Setting up fire4/expand1x1
I1107 16:43:08.667663  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.667663  5452 net.cpp:137] Memory required for data: 600167600
I1107 16:43:08.667663  5452 layer_factory.cpp:58] Creating layer fire4/relu_expand1x1
I1107 16:43:08.667663  5452 net.cpp:84] Creating Layer fire4/relu_expand1x1
I1107 16:43:08.667663  5452 net.cpp:406] fire4/relu_expand1x1 <- fire4/expand1x1
I1107 16:43:08.667663  5452 net.cpp:367] fire4/relu_expand1x1 -> fire4/expand1x1 (in-place)
I1107 16:43:08.668663  5452 net.cpp:122] Setting up fire4/relu_expand1x1
I1107 16:43:08.668663  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.668663  5452 net.cpp:137] Memory required for data: 610202800
I1107 16:43:08.668663  5452 layer_factory.cpp:58] Creating layer fire4/expand3x3
I1107 16:43:08.668663  5452 net.cpp:84] Creating Layer fire4/expand3x3
I1107 16:43:08.668663  5452 net.cpp:406] fire4/expand3x3 <- fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_1
I1107 16:43:08.668663  5452 net.cpp:380] fire4/expand3x3 -> conv_8
I1107 16:43:08.669663  5452 net.cpp:122] Setting up fire4/expand3x3
I1107 16:43:08.669663  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.669663  5452 net.cpp:137] Memory required for data: 620238000
I1107 16:43:08.669663  5452 layer_factory.cpp:58] Creating layer bn_8
I1107 16:43:08.669663  5452 net.cpp:84] Creating Layer bn_8
I1107 16:43:08.669663  5452 net.cpp:406] bn_8 <- conv_8
I1107 16:43:08.669663  5452 net.cpp:367] bn_8 -> conv_8 (in-place)
I1107 16:43:08.670663  5452 net.cpp:122] Setting up bn_8
I1107 16:43:08.670663  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.670663  5452 net.cpp:137] Memory required for data: 630273200
I1107 16:43:08.670663  5452 layer_factory.cpp:58] Creating layer scale_8
I1107 16:43:08.670663  5452 net.cpp:84] Creating Layer scale_8
I1107 16:43:08.670663  5452 net.cpp:406] scale_8 <- conv_8
I1107 16:43:08.670663  5452 net.cpp:380] scale_8 -> fire4/expand3x3
I1107 16:43:08.670663  5452 layer_factory.cpp:58] Creating layer scale_8
I1107 16:43:08.670663  5452 net.cpp:122] Setting up scale_8
I1107 16:43:08.670663  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.670663  5452 net.cpp:137] Memory required for data: 640308400
I1107 16:43:08.670663  5452 layer_factory.cpp:58] Creating layer fire4/relu_expand3x3
I1107 16:43:08.670663  5452 net.cpp:84] Creating Layer fire4/relu_expand3x3
I1107 16:43:08.670663  5452 net.cpp:406] fire4/relu_expand3x3 <- fire4/expand3x3
I1107 16:43:08.670663  5452 net.cpp:367] fire4/relu_expand3x3 -> fire4/expand3x3 (in-place)
I1107 16:43:08.670663  5452 net.cpp:122] Setting up fire4/relu_expand3x3
I1107 16:43:08.670663  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.670663  5452 net.cpp:137] Memory required for data: 650343600
I1107 16:43:08.670663  5452 layer_factory.cpp:58] Creating layer fire4/concat
I1107 16:43:08.670663  5452 net.cpp:84] Creating Layer fire4/concat
I1107 16:43:08.670663  5452 net.cpp:406] fire4/concat <- fire4/expand1x1
I1107 16:43:08.670663  5452 net.cpp:406] fire4/concat <- fire4/expand3x3
I1107 16:43:08.670663  5452 net.cpp:380] fire4/concat -> fire4/concat
I1107 16:43:08.670663  5452 net.cpp:122] Setting up fire4/concat
I1107 16:43:08.670663  5452 net.cpp:129] Top shape: 100 256 14 14 (5017600)
I1107 16:43:08.670663  5452 net.cpp:137] Memory required for data: 670414000
I1107 16:43:08.670663  5452 layer_factory.cpp:58] Creating layer fire5/squeeze1x1
I1107 16:43:08.670663  5452 net.cpp:84] Creating Layer fire5/squeeze1x1
I1107 16:43:08.670663  5452 net.cpp:406] fire5/squeeze1x1 <- fire4/concat
I1107 16:43:08.670663  5452 net.cpp:380] fire5/squeeze1x1 -> conv_9
I1107 16:43:08.671663  5452 net.cpp:122] Setting up fire5/squeeze1x1
I1107 16:43:08.671663  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.671663  5452 net.cpp:137] Memory required for data: 672922800
I1107 16:43:08.671663  5452 layer_factory.cpp:58] Creating layer bn_9
I1107 16:43:08.671663  5452 net.cpp:84] Creating Layer bn_9
I1107 16:43:08.671663  5452 net.cpp:406] bn_9 <- conv_9
I1107 16:43:08.671663  5452 net.cpp:367] bn_9 -> conv_9 (in-place)
I1107 16:43:08.671663  5452 net.cpp:122] Setting up bn_9
I1107 16:43:08.671663  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.671663  5452 net.cpp:137] Memory required for data: 675431600
I1107 16:43:08.671663  5452 layer_factory.cpp:58] Creating layer scale_9
I1107 16:43:08.671663  5452 net.cpp:84] Creating Layer scale_9
I1107 16:43:08.671663  5452 net.cpp:406] scale_9 <- conv_9
I1107 16:43:08.671663  5452 net.cpp:380] scale_9 -> fire5/squeeze1x1
I1107 16:43:08.671663  5452 layer_factory.cpp:58] Creating layer scale_9
I1107 16:43:08.672664  5452 net.cpp:122] Setting up scale_9
I1107 16:43:08.672664  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.672664  5452 net.cpp:137] Memory required for data: 677940400
I1107 16:43:08.672664  5452 layer_factory.cpp:58] Creating layer fire5/relu_squeeze1x1
I1107 16:43:08.672664  5452 net.cpp:84] Creating Layer fire5/relu_squeeze1x1
I1107 16:43:08.672664  5452 net.cpp:406] fire5/relu_squeeze1x1 <- fire5/squeeze1x1
I1107 16:43:08.672664  5452 net.cpp:367] fire5/relu_squeeze1x1 -> fire5/squeeze1x1 (in-place)
I1107 16:43:08.672664  5452 net.cpp:122] Setting up fire5/relu_squeeze1x1
I1107 16:43:08.672664  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.672664  5452 net.cpp:137] Memory required for data: 680449200
I1107 16:43:08.672664  5452 layer_factory.cpp:58] Creating layer fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 16:43:08.672664  5452 net.cpp:84] Creating Layer fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 16:43:08.672664  5452 net.cpp:406] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split <- fire5/squeeze1x1
I1107 16:43:08.672664  5452 net.cpp:380] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split -> fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_0
I1107 16:43:08.672664  5452 net.cpp:380] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split -> fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_1
I1107 16:43:08.672664  5452 net.cpp:122] Setting up fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 16:43:08.672664  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.672664  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.672664  5452 net.cpp:137] Memory required for data: 685466800
I1107 16:43:08.672664  5452 layer_factory.cpp:58] Creating layer fire5/expand1x1
I1107 16:43:08.672664  5452 net.cpp:84] Creating Layer fire5/expand1x1
I1107 16:43:08.672664  5452 net.cpp:406] fire5/expand1x1 <- fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_0
I1107 16:43:08.672664  5452 net.cpp:380] fire5/expand1x1 -> conv_10
I1107 16:43:08.673662  5452 net.cpp:122] Setting up fire5/expand1x1
I1107 16:43:08.673662  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.673662  5452 net.cpp:137] Memory required for data: 695502000
I1107 16:43:08.673662  5452 layer_factory.cpp:58] Creating layer bn_10
I1107 16:43:08.673662  5452 net.cpp:84] Creating Layer bn_10
I1107 16:43:08.673662  5452 net.cpp:406] bn_10 <- conv_10
I1107 16:43:08.673662  5452 net.cpp:367] bn_10 -> conv_10 (in-place)
I1107 16:43:08.673662  5452 net.cpp:122] Setting up bn_10
I1107 16:43:08.673662  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.673662  5452 net.cpp:137] Memory required for data: 705537200
I1107 16:43:08.673662  5452 layer_factory.cpp:58] Creating layer scale_10
I1107 16:43:08.673662  5452 net.cpp:84] Creating Layer scale_10
I1107 16:43:08.673662  5452 net.cpp:406] scale_10 <- conv_10
I1107 16:43:08.673662  5452 net.cpp:380] scale_10 -> fire5/expand1x1
I1107 16:43:08.673662  5452 layer_factory.cpp:58] Creating layer scale_10
I1107 16:43:08.673662  5452 net.cpp:122] Setting up scale_10
I1107 16:43:08.673662  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.673662  5452 net.cpp:137] Memory required for data: 715572400
I1107 16:43:08.673662  5452 layer_factory.cpp:58] Creating layer fire5/relu_expand1x1
I1107 16:43:08.673662  5452 net.cpp:84] Creating Layer fire5/relu_expand1x1
I1107 16:43:08.673662  5452 net.cpp:406] fire5/relu_expand1x1 <- fire5/expand1x1
I1107 16:43:08.673662  5452 net.cpp:367] fire5/relu_expand1x1 -> fire5/expand1x1 (in-place)
I1107 16:43:08.674664  5452 net.cpp:122] Setting up fire5/relu_expand1x1
I1107 16:43:08.674664  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.674664  5452 net.cpp:137] Memory required for data: 725607600
I1107 16:43:08.674664  5452 layer_factory.cpp:58] Creating layer fire5/expand3x3
I1107 16:43:08.674664  5452 net.cpp:84] Creating Layer fire5/expand3x3
I1107 16:43:08.674664  5452 net.cpp:406] fire5/expand3x3 <- fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_1
I1107 16:43:08.674664  5452 net.cpp:380] fire5/expand3x3 -> conv_11
I1107 16:43:08.675663  5452 net.cpp:122] Setting up fire5/expand3x3
I1107 16:43:08.675663  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.675663  5452 net.cpp:137] Memory required for data: 735642800
I1107 16:43:08.675663  5452 layer_factory.cpp:58] Creating layer bn_11
I1107 16:43:08.675663  5452 net.cpp:84] Creating Layer bn_11
I1107 16:43:08.675663  5452 net.cpp:406] bn_11 <- conv_11
I1107 16:43:08.675663  5452 net.cpp:367] bn_11 -> conv_11 (in-place)
I1107 16:43:08.675663  5452 net.cpp:122] Setting up bn_11
I1107 16:43:08.675663  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.675663  5452 net.cpp:137] Memory required for data: 745678000
I1107 16:43:08.675663  5452 layer_factory.cpp:58] Creating layer scale_11
I1107 16:43:08.675663  5452 net.cpp:84] Creating Layer scale_11
I1107 16:43:08.675663  5452 net.cpp:406] scale_11 <- conv_11
I1107 16:43:08.675663  5452 net.cpp:380] scale_11 -> fire5/expand3x3
I1107 16:43:08.675663  5452 layer_factory.cpp:58] Creating layer scale_11
I1107 16:43:08.675663  5452 net.cpp:122] Setting up scale_11
I1107 16:43:08.675663  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.675663  5452 net.cpp:137] Memory required for data: 755713200
I1107 16:43:08.675663  5452 layer_factory.cpp:58] Creating layer fire5/relu_expand3x3
I1107 16:43:08.675663  5452 net.cpp:84] Creating Layer fire5/relu_expand3x3
I1107 16:43:08.675663  5452 net.cpp:406] fire5/relu_expand3x3 <- fire5/expand3x3
I1107 16:43:08.675663  5452 net.cpp:367] fire5/relu_expand3x3 -> fire5/expand3x3 (in-place)
I1107 16:43:08.676663  5452 net.cpp:122] Setting up fire5/relu_expand3x3
I1107 16:43:08.676663  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.676663  5452 net.cpp:137] Memory required for data: 765748400
I1107 16:43:08.676663  5452 layer_factory.cpp:58] Creating layer fire5/concat
I1107 16:43:08.676663  5452 net.cpp:84] Creating Layer fire5/concat
I1107 16:43:08.676663  5452 net.cpp:406] fire5/concat <- fire5/expand1x1
I1107 16:43:08.676663  5452 net.cpp:406] fire5/concat <- fire5/expand3x3
I1107 16:43:08.676663  5452 net.cpp:380] fire5/concat -> fire5/concat
I1107 16:43:08.676663  5452 net.cpp:122] Setting up fire5/concat
I1107 16:43:08.676663  5452 net.cpp:129] Top shape: 100 256 14 14 (5017600)
I1107 16:43:08.676663  5452 net.cpp:137] Memory required for data: 785818800
I1107 16:43:08.676663  5452 layer_factory.cpp:58] Creating layer pool5
I1107 16:43:08.676663  5452 net.cpp:84] Creating Layer pool5
I1107 16:43:08.676663  5452 net.cpp:406] pool5 <- fire5/concat
I1107 16:43:08.676663  5452 net.cpp:380] pool5 -> pool5
I1107 16:43:08.676663  5452 net.cpp:122] Setting up pool5
I1107 16:43:08.676663  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.676663  5452 net.cpp:137] Memory required for data: 790836400
I1107 16:43:08.676663  5452 layer_factory.cpp:58] Creating layer fire6/squeeze1x1
I1107 16:43:08.676663  5452 net.cpp:84] Creating Layer fire6/squeeze1x1
I1107 16:43:08.676663  5452 net.cpp:406] fire6/squeeze1x1 <- pool5
I1107 16:43:08.676663  5452 net.cpp:380] fire6/squeeze1x1 -> conv_12
I1107 16:43:08.677664  5452 net.cpp:122] Setting up fire6/squeeze1x1
I1107 16:43:08.677664  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.677664  5452 net.cpp:137] Memory required for data: 791777200
I1107 16:43:08.677664  5452 layer_factory.cpp:58] Creating layer bn_12
I1107 16:43:08.677664  5452 net.cpp:84] Creating Layer bn_12
I1107 16:43:08.677664  5452 net.cpp:406] bn_12 <- conv_12
I1107 16:43:08.677664  5452 net.cpp:367] bn_12 -> conv_12 (in-place)
I1107 16:43:08.677664  5452 net.cpp:122] Setting up bn_12
I1107 16:43:08.677664  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.677664  5452 net.cpp:137] Memory required for data: 792718000
I1107 16:43:08.677664  5452 layer_factory.cpp:58] Creating layer scale_12
I1107 16:43:08.677664  5452 net.cpp:84] Creating Layer scale_12
I1107 16:43:08.677664  5452 net.cpp:406] scale_12 <- conv_12
I1107 16:43:08.677664  5452 net.cpp:380] scale_12 -> fire6/squeeze1x1
I1107 16:43:08.677664  5452 layer_factory.cpp:58] Creating layer scale_12
I1107 16:43:08.677664  5452 net.cpp:122] Setting up scale_12
I1107 16:43:08.677664  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.677664  5452 net.cpp:137] Memory required for data: 793658800
I1107 16:43:08.677664  5452 layer_factory.cpp:58] Creating layer fire6/relu_squeeze1x1
I1107 16:43:08.677664  5452 net.cpp:84] Creating Layer fire6/relu_squeeze1x1
I1107 16:43:08.677664  5452 net.cpp:406] fire6/relu_squeeze1x1 <- fire6/squeeze1x1
I1107 16:43:08.677664  5452 net.cpp:367] fire6/relu_squeeze1x1 -> fire6/squeeze1x1 (in-place)
I1107 16:43:08.678663  5452 net.cpp:122] Setting up fire6/relu_squeeze1x1
I1107 16:43:08.678663  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.678663  5452 net.cpp:137] Memory required for data: 794599600
I1107 16:43:08.678663  5452 layer_factory.cpp:58] Creating layer fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 16:43:08.678663  5452 net.cpp:84] Creating Layer fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 16:43:08.678663  5452 net.cpp:406] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split <- fire6/squeeze1x1
I1107 16:43:08.678663  5452 net.cpp:380] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split -> fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_0
I1107 16:43:08.678663  5452 net.cpp:380] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split -> fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_1
I1107 16:43:08.678663  5452 net.cpp:122] Setting up fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 16:43:08.678663  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.678663  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.678663  5452 net.cpp:137] Memory required for data: 796481200
I1107 16:43:08.678663  5452 layer_factory.cpp:58] Creating layer fire6/expand1x1
I1107 16:43:08.678663  5452 net.cpp:84] Creating Layer fire6/expand1x1
I1107 16:43:08.678663  5452 net.cpp:406] fire6/expand1x1 <- fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_0
I1107 16:43:08.678663  5452 net.cpp:380] fire6/expand1x1 -> conv_13
I1107 16:43:08.679663  5452 net.cpp:122] Setting up fire6/expand1x1
I1107 16:43:08.679663  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.679663  5452 net.cpp:137] Memory required for data: 800244400
I1107 16:43:08.679663  5452 layer_factory.cpp:58] Creating layer bn_13
I1107 16:43:08.679663  5452 net.cpp:84] Creating Layer bn_13
I1107 16:43:08.679663  5452 net.cpp:406] bn_13 <- conv_13
I1107 16:43:08.679663  5452 net.cpp:367] bn_13 -> conv_13 (in-place)
I1107 16:43:08.679663  5452 net.cpp:122] Setting up bn_13
I1107 16:43:08.679663  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.679663  5452 net.cpp:137] Memory required for data: 804007600
I1107 16:43:08.679663  5452 layer_factory.cpp:58] Creating layer scale_13
I1107 16:43:08.679663  5452 net.cpp:84] Creating Layer scale_13
I1107 16:43:08.679663  5452 net.cpp:406] scale_13 <- conv_13
I1107 16:43:08.679663  5452 net.cpp:380] scale_13 -> fire6/expand1x1
I1107 16:43:08.679663  5452 layer_factory.cpp:58] Creating layer scale_13
I1107 16:43:08.679663  5452 net.cpp:122] Setting up scale_13
I1107 16:43:08.679663  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.679663  5452 net.cpp:137] Memory required for data: 807770800
I1107 16:43:08.679663  5452 layer_factory.cpp:58] Creating layer fire6/relu_expand1x1
I1107 16:43:08.679663  5452 net.cpp:84] Creating Layer fire6/relu_expand1x1
I1107 16:43:08.680663  5452 net.cpp:406] fire6/relu_expand1x1 <- fire6/expand1x1
I1107 16:43:08.680663  5452 net.cpp:367] fire6/relu_expand1x1 -> fire6/expand1x1 (in-place)
I1107 16:43:08.680663  5452 net.cpp:122] Setting up fire6/relu_expand1x1
I1107 16:43:08.680663  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.680663  5452 net.cpp:137] Memory required for data: 811534000
I1107 16:43:08.680663  5452 layer_factory.cpp:58] Creating layer fire6/expand3x3
I1107 16:43:08.680663  5452 net.cpp:84] Creating Layer fire6/expand3x3
I1107 16:43:08.680663  5452 net.cpp:406] fire6/expand3x3 <- fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_1
I1107 16:43:08.680663  5452 net.cpp:380] fire6/expand3x3 -> conv_14
I1107 16:43:08.682663  5452 net.cpp:122] Setting up fire6/expand3x3
I1107 16:43:08.682663  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.682663  5452 net.cpp:137] Memory required for data: 815297200
I1107 16:43:08.682663  5452 layer_factory.cpp:58] Creating layer bn_14
I1107 16:43:08.682663  5452 net.cpp:84] Creating Layer bn_14
I1107 16:43:08.682663  5452 net.cpp:406] bn_14 <- conv_14
I1107 16:43:08.682663  5452 net.cpp:367] bn_14 -> conv_14 (in-place)
I1107 16:43:08.682663  5452 net.cpp:122] Setting up bn_14
I1107 16:43:08.682663  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.682663  5452 net.cpp:137] Memory required for data: 819060400
I1107 16:43:08.682663  5452 layer_factory.cpp:58] Creating layer scale_14
I1107 16:43:08.682663  5452 net.cpp:84] Creating Layer scale_14
I1107 16:43:08.682663  5452 net.cpp:406] scale_14 <- conv_14
I1107 16:43:08.682663  5452 net.cpp:380] scale_14 -> fire6/expand3x3
I1107 16:43:08.682663  5452 layer_factory.cpp:58] Creating layer scale_14
I1107 16:43:08.682663  5452 net.cpp:122] Setting up scale_14
I1107 16:43:08.682663  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.682663  5452 net.cpp:137] Memory required for data: 822823600
I1107 16:43:08.682663  5452 layer_factory.cpp:58] Creating layer fire6/relu_expand3x3
I1107 16:43:08.682663  5452 net.cpp:84] Creating Layer fire6/relu_expand3x3
I1107 16:43:08.683662  5452 net.cpp:406] fire6/relu_expand3x3 <- fire6/expand3x3
I1107 16:43:08.683662  5452 net.cpp:367] fire6/relu_expand3x3 -> fire6/expand3x3 (in-place)
I1107 16:43:08.683662  5452 net.cpp:122] Setting up fire6/relu_expand3x3
I1107 16:43:08.683662  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.683662  5452 net.cpp:137] Memory required for data: 826586800
I1107 16:43:08.683662  5452 layer_factory.cpp:58] Creating layer fire6/concat
I1107 16:43:08.683662  5452 net.cpp:84] Creating Layer fire6/concat
I1107 16:43:08.683662  5452 net.cpp:406] fire6/concat <- fire6/expand1x1
I1107 16:43:08.683662  5452 net.cpp:406] fire6/concat <- fire6/expand3x3
I1107 16:43:08.683662  5452 net.cpp:380] fire6/concat -> fire6/concat
I1107 16:43:08.683662  5452 net.cpp:122] Setting up fire6/concat
I1107 16:43:08.683662  5452 net.cpp:129] Top shape: 100 384 7 7 (1881600)
I1107 16:43:08.683662  5452 net.cpp:137] Memory required for data: 834113200
I1107 16:43:08.683662  5452 layer_factory.cpp:58] Creating layer fire7/squeeze1x1
I1107 16:43:08.683662  5452 net.cpp:84] Creating Layer fire7/squeeze1x1
I1107 16:43:08.683662  5452 net.cpp:406] fire7/squeeze1x1 <- fire6/concat
I1107 16:43:08.683662  5452 net.cpp:380] fire7/squeeze1x1 -> conv_15
I1107 16:43:08.684660  5452 net.cpp:122] Setting up fire7/squeeze1x1
I1107 16:43:08.684660  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.684660  5452 net.cpp:137] Memory required for data: 835054000
I1107 16:43:08.684660  5452 layer_factory.cpp:58] Creating layer bn_15
I1107 16:43:08.684660  5452 net.cpp:84] Creating Layer bn_15
I1107 16:43:08.684660  5452 net.cpp:406] bn_15 <- conv_15
I1107 16:43:08.684660  5452 net.cpp:367] bn_15 -> conv_15 (in-place)
I1107 16:43:08.684660  5452 net.cpp:122] Setting up bn_15
I1107 16:43:08.684660  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.684660  5452 net.cpp:137] Memory required for data: 835994800
I1107 16:43:08.684660  5452 layer_factory.cpp:58] Creating layer scale_15
I1107 16:43:08.684660  5452 net.cpp:84] Creating Layer scale_15
I1107 16:43:08.684660  5452 net.cpp:406] scale_15 <- conv_15
I1107 16:43:08.684660  5452 net.cpp:380] scale_15 -> fire7/squeeze1x1
I1107 16:43:08.684660  5452 layer_factory.cpp:58] Creating layer scale_15
I1107 16:43:08.684660  5452 net.cpp:122] Setting up scale_15
I1107 16:43:08.684660  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.685663  5452 net.cpp:137] Memory required for data: 836935600
I1107 16:43:08.685663  5452 layer_factory.cpp:58] Creating layer fire7/relu_squeeze1x1
I1107 16:43:08.685663  5452 net.cpp:84] Creating Layer fire7/relu_squeeze1x1
I1107 16:43:08.685663  5452 net.cpp:406] fire7/relu_squeeze1x1 <- fire7/squeeze1x1
I1107 16:43:08.685663  5452 net.cpp:367] fire7/relu_squeeze1x1 -> fire7/squeeze1x1 (in-place)
I1107 16:43:08.685663  5452 net.cpp:122] Setting up fire7/relu_squeeze1x1
I1107 16:43:08.685663  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.685663  5452 net.cpp:137] Memory required for data: 837876400
I1107 16:43:08.685663  5452 layer_factory.cpp:58] Creating layer fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 16:43:08.685663  5452 net.cpp:84] Creating Layer fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 16:43:08.685663  5452 net.cpp:406] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split <- fire7/squeeze1x1
I1107 16:43:08.685663  5452 net.cpp:380] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split -> fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_0
I1107 16:43:08.685663  5452 net.cpp:380] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split -> fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_1
I1107 16:43:08.685663  5452 net.cpp:122] Setting up fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 16:43:08.685663  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.685663  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.685663  5452 net.cpp:137] Memory required for data: 839758000
I1107 16:43:08.685663  5452 layer_factory.cpp:58] Creating layer fire7/expand1x1
I1107 16:43:08.685663  5452 net.cpp:84] Creating Layer fire7/expand1x1
I1107 16:43:08.685663  5452 net.cpp:406] fire7/expand1x1 <- fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_0
I1107 16:43:08.685663  5452 net.cpp:380] fire7/expand1x1 -> conv_16
I1107 16:43:08.686662  5452 net.cpp:122] Setting up fire7/expand1x1
I1107 16:43:08.686662  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.686662  5452 net.cpp:137] Memory required for data: 843521200
I1107 16:43:08.686662  5452 layer_factory.cpp:58] Creating layer bn_16
I1107 16:43:08.686662  5452 net.cpp:84] Creating Layer bn_16
I1107 16:43:08.686662  5452 net.cpp:406] bn_16 <- conv_16
I1107 16:43:08.686662  5452 net.cpp:367] bn_16 -> conv_16 (in-place)
I1107 16:43:08.686662  5452 net.cpp:122] Setting up bn_16
I1107 16:43:08.686662  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.686662  5452 net.cpp:137] Memory required for data: 847284400
I1107 16:43:08.686662  5452 layer_factory.cpp:58] Creating layer scale_16
I1107 16:43:08.686662  5452 net.cpp:84] Creating Layer scale_16
I1107 16:43:08.686662  5452 net.cpp:406] scale_16 <- conv_16
I1107 16:43:08.686662  5452 net.cpp:380] scale_16 -> fire7/expand1x1
I1107 16:43:08.686662  5452 layer_factory.cpp:58] Creating layer scale_16
I1107 16:43:08.687664  5452 net.cpp:122] Setting up scale_16
I1107 16:43:08.687664  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.687664  5452 net.cpp:137] Memory required for data: 851047600
I1107 16:43:08.687664  5452 layer_factory.cpp:58] Creating layer fire7/relu_expand1x1
I1107 16:43:08.687664  5452 net.cpp:84] Creating Layer fire7/relu_expand1x1
I1107 16:43:08.687664  5452 net.cpp:406] fire7/relu_expand1x1 <- fire7/expand1x1
I1107 16:43:08.687664  5452 net.cpp:367] fire7/relu_expand1x1 -> fire7/expand1x1 (in-place)
I1107 16:43:08.687664  5452 net.cpp:122] Setting up fire7/relu_expand1x1
I1107 16:43:08.687664  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.687664  5452 net.cpp:137] Memory required for data: 854810800
I1107 16:43:08.687664  5452 layer_factory.cpp:58] Creating layer fire7/expand3x3
I1107 16:43:08.687664  5452 net.cpp:84] Creating Layer fire7/expand3x3
I1107 16:43:08.687664  5452 net.cpp:406] fire7/expand3x3 <- fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_1
I1107 16:43:08.687664  5452 net.cpp:380] fire7/expand3x3 -> conv_17
I1107 16:43:08.689663  5452 net.cpp:122] Setting up fire7/expand3x3
I1107 16:43:08.689663  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.689663  5452 net.cpp:137] Memory required for data: 858574000
I1107 16:43:08.689663  5452 layer_factory.cpp:58] Creating layer bn_17
I1107 16:43:08.689663  5452 net.cpp:84] Creating Layer bn_17
I1107 16:43:08.689663  5452 net.cpp:406] bn_17 <- conv_17
I1107 16:43:08.689663  5452 net.cpp:367] bn_17 -> conv_17 (in-place)
I1107 16:43:08.689663  5452 net.cpp:122] Setting up bn_17
I1107 16:43:08.689663  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.689663  5452 net.cpp:137] Memory required for data: 862337200
I1107 16:43:08.689663  5452 layer_factory.cpp:58] Creating layer scale_17
I1107 16:43:08.689663  5452 net.cpp:84] Creating Layer scale_17
I1107 16:43:08.689663  5452 net.cpp:406] scale_17 <- conv_17
I1107 16:43:08.689663  5452 net.cpp:380] scale_17 -> fire7/expand3x3
I1107 16:43:08.689663  5452 layer_factory.cpp:58] Creating layer scale_17
I1107 16:43:08.689663  5452 net.cpp:122] Setting up scale_17
I1107 16:43:08.689663  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.689663  5452 net.cpp:137] Memory required for data: 866100400
I1107 16:43:08.689663  5452 layer_factory.cpp:58] Creating layer fire7/relu_expand3x3
I1107 16:43:08.689663  5452 net.cpp:84] Creating Layer fire7/relu_expand3x3
I1107 16:43:08.689663  5452 net.cpp:406] fire7/relu_expand3x3 <- fire7/expand3x3
I1107 16:43:08.689663  5452 net.cpp:367] fire7/relu_expand3x3 -> fire7/expand3x3 (in-place)
I1107 16:43:08.689663  5452 net.cpp:122] Setting up fire7/relu_expand3x3
I1107 16:43:08.689663  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.689663  5452 net.cpp:137] Memory required for data: 869863600
I1107 16:43:08.689663  5452 layer_factory.cpp:58] Creating layer fire7/concat
I1107 16:43:08.689663  5452 net.cpp:84] Creating Layer fire7/concat
I1107 16:43:08.689663  5452 net.cpp:406] fire7/concat <- fire7/expand1x1
I1107 16:43:08.689663  5452 net.cpp:406] fire7/concat <- fire7/expand3x3
I1107 16:43:08.689663  5452 net.cpp:380] fire7/concat -> fire7/concat
I1107 16:43:08.689663  5452 net.cpp:122] Setting up fire7/concat
I1107 16:43:08.689663  5452 net.cpp:129] Top shape: 100 384 7 7 (1881600)
I1107 16:43:08.689663  5452 net.cpp:137] Memory required for data: 877390000
I1107 16:43:08.690660  5452 layer_factory.cpp:58] Creating layer fire8/squeeze1x1
I1107 16:43:08.690660  5452 net.cpp:84] Creating Layer fire8/squeeze1x1
I1107 16:43:08.690660  5452 net.cpp:406] fire8/squeeze1x1 <- fire7/concat
I1107 16:43:08.690660  5452 net.cpp:380] fire8/squeeze1x1 -> conv_18
I1107 16:43:08.691663  5452 net.cpp:122] Setting up fire8/squeeze1x1
I1107 16:43:08.691663  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.691663  5452 net.cpp:137] Memory required for data: 878644400
I1107 16:43:08.691663  5452 layer_factory.cpp:58] Creating layer bn_18
I1107 16:43:08.691663  5452 net.cpp:84] Creating Layer bn_18
I1107 16:43:08.691663  5452 net.cpp:406] bn_18 <- conv_18
I1107 16:43:08.691663  5452 net.cpp:367] bn_18 -> conv_18 (in-place)
I1107 16:43:08.691663  5452 net.cpp:122] Setting up bn_18
I1107 16:43:08.691663  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.691663  5452 net.cpp:137] Memory required for data: 879898800
I1107 16:43:08.691663  5452 layer_factory.cpp:58] Creating layer scale_18
I1107 16:43:08.691663  5452 net.cpp:84] Creating Layer scale_18
I1107 16:43:08.691663  5452 net.cpp:406] scale_18 <- conv_18
I1107 16:43:08.691663  5452 net.cpp:380] scale_18 -> fire8/squeeze1x1
I1107 16:43:08.691663  5452 layer_factory.cpp:58] Creating layer scale_18
I1107 16:43:08.691663  5452 net.cpp:122] Setting up scale_18
I1107 16:43:08.691663  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.691663  5452 net.cpp:137] Memory required for data: 881153200
I1107 16:43:08.691663  5452 layer_factory.cpp:58] Creating layer fire8/relu_squeeze1x1
I1107 16:43:08.691663  5452 net.cpp:84] Creating Layer fire8/relu_squeeze1x1
I1107 16:43:08.691663  5452 net.cpp:406] fire8/relu_squeeze1x1 <- fire8/squeeze1x1
I1107 16:43:08.691663  5452 net.cpp:367] fire8/relu_squeeze1x1 -> fire8/squeeze1x1 (in-place)
I1107 16:43:08.692663  5452 net.cpp:122] Setting up fire8/relu_squeeze1x1
I1107 16:43:08.692663  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.692663  5452 net.cpp:137] Memory required for data: 882407600
I1107 16:43:08.692663  5452 layer_factory.cpp:58] Creating layer fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 16:43:08.692663  5452 net.cpp:84] Creating Layer fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 16:43:08.692663  5452 net.cpp:406] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split <- fire8/squeeze1x1
I1107 16:43:08.692663  5452 net.cpp:380] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split -> fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_0
I1107 16:43:08.692663  5452 net.cpp:380] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split -> fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_1
I1107 16:43:08.692663  5452 net.cpp:122] Setting up fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 16:43:08.692663  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.692663  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.692663  5452 net.cpp:137] Memory required for data: 884916400
I1107 16:43:08.692663  5452 layer_factory.cpp:58] Creating layer fire8/expand1x1
I1107 16:43:08.692663  5452 net.cpp:84] Creating Layer fire8/expand1x1
I1107 16:43:08.692663  5452 net.cpp:406] fire8/expand1x1 <- fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_0
I1107 16:43:08.692663  5452 net.cpp:380] fire8/expand1x1 -> conv_19
I1107 16:43:08.693665  5452 net.cpp:122] Setting up fire8/expand1x1
I1107 16:43:08.693665  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.693665  5452 net.cpp:137] Memory required for data: 889934000
I1107 16:43:08.693665  5452 layer_factory.cpp:58] Creating layer bn_19
I1107 16:43:08.693665  5452 net.cpp:84] Creating Layer bn_19
I1107 16:43:08.693665  5452 net.cpp:406] bn_19 <- conv_19
I1107 16:43:08.693665  5452 net.cpp:367] bn_19 -> conv_19 (in-place)
I1107 16:43:08.693665  5452 net.cpp:122] Setting up bn_19
I1107 16:43:08.693665  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.693665  5452 net.cpp:137] Memory required for data: 894951600
I1107 16:43:08.693665  5452 layer_factory.cpp:58] Creating layer scale_19
I1107 16:43:08.693665  5452 net.cpp:84] Creating Layer scale_19
I1107 16:43:08.693665  5452 net.cpp:406] scale_19 <- conv_19
I1107 16:43:08.693665  5452 net.cpp:380] scale_19 -> fire8/expand1x1
I1107 16:43:08.693665  5452 layer_factory.cpp:58] Creating layer scale_19
I1107 16:43:08.693665  5452 net.cpp:122] Setting up scale_19
I1107 16:43:08.693665  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.693665  5452 net.cpp:137] Memory required for data: 899969200
I1107 16:43:08.693665  5452 layer_factory.cpp:58] Creating layer fire8/relu_expand1x1
I1107 16:43:08.693665  5452 net.cpp:84] Creating Layer fire8/relu_expand1x1
I1107 16:43:08.693665  5452 net.cpp:406] fire8/relu_expand1x1 <- fire8/expand1x1
I1107 16:43:08.693665  5452 net.cpp:367] fire8/relu_expand1x1 -> fire8/expand1x1 (in-place)
I1107 16:43:08.694664  5452 net.cpp:122] Setting up fire8/relu_expand1x1
I1107 16:43:08.694664  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.694664  5452 net.cpp:137] Memory required for data: 904986800
I1107 16:43:08.694664  5452 layer_factory.cpp:58] Creating layer fire8/expand3x3
I1107 16:43:08.694664  5452 net.cpp:84] Creating Layer fire8/expand3x3
I1107 16:43:08.694664  5452 net.cpp:406] fire8/expand3x3 <- fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_1
I1107 16:43:08.694664  5452 net.cpp:380] fire8/expand3x3 -> conv_20
I1107 16:43:08.696663  5452 net.cpp:122] Setting up fire8/expand3x3
I1107 16:43:08.696663  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.696663  5452 net.cpp:137] Memory required for data: 910004400
I1107 16:43:08.696663  5452 layer_factory.cpp:58] Creating layer bn_20
I1107 16:43:08.696663  5452 net.cpp:84] Creating Layer bn_20
I1107 16:43:08.696663  5452 net.cpp:406] bn_20 <- conv_20
I1107 16:43:08.696663  5452 net.cpp:367] bn_20 -> conv_20 (in-place)
I1107 16:43:08.696663  5452 net.cpp:122] Setting up bn_20
I1107 16:43:08.696663  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.696663  5452 net.cpp:137] Memory required for data: 915022000
I1107 16:43:08.696663  5452 layer_factory.cpp:58] Creating layer scale_20
I1107 16:43:08.696663  5452 net.cpp:84] Creating Layer scale_20
I1107 16:43:08.696663  5452 net.cpp:406] scale_20 <- conv_20
I1107 16:43:08.696663  5452 net.cpp:380] scale_20 -> fire8/expand3x3
I1107 16:43:08.696663  5452 layer_factory.cpp:58] Creating layer scale_20
I1107 16:43:08.696663  5452 net.cpp:122] Setting up scale_20
I1107 16:43:08.696663  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.696663  5452 net.cpp:137] Memory required for data: 920039600
I1107 16:43:08.696663  5452 layer_factory.cpp:58] Creating layer fire8/relu_expand3x3
I1107 16:43:08.697664  5452 net.cpp:84] Creating Layer fire8/relu_expand3x3
I1107 16:43:08.697664  5452 net.cpp:406] fire8/relu_expand3x3 <- fire8/expand3x3
I1107 16:43:08.697664  5452 net.cpp:367] fire8/relu_expand3x3 -> fire8/expand3x3 (in-place)
I1107 16:43:08.697664  5452 net.cpp:122] Setting up fire8/relu_expand3x3
I1107 16:43:08.697664  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.697664  5452 net.cpp:137] Memory required for data: 925057200
I1107 16:43:08.697664  5452 layer_factory.cpp:58] Creating layer fire8/concat
I1107 16:43:08.697664  5452 net.cpp:84] Creating Layer fire8/concat
I1107 16:43:08.697664  5452 net.cpp:406] fire8/concat <- fire8/expand1x1
I1107 16:43:08.697664  5452 net.cpp:406] fire8/concat <- fire8/expand3x3
I1107 16:43:08.697664  5452 net.cpp:380] fire8/concat -> fire8/concat
I1107 16:43:08.697664  5452 net.cpp:122] Setting up fire8/concat
I1107 16:43:08.697664  5452 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 16:43:08.697664  5452 net.cpp:137] Memory required for data: 935092400
I1107 16:43:08.697664  5452 layer_factory.cpp:58] Creating layer fire9/squeeze1x1
I1107 16:43:08.697664  5452 net.cpp:84] Creating Layer fire9/squeeze1x1
I1107 16:43:08.697664  5452 net.cpp:406] fire9/squeeze1x1 <- fire8/concat
I1107 16:43:08.697664  5452 net.cpp:380] fire9/squeeze1x1 -> conv_21
I1107 16:43:08.698663  5452 net.cpp:122] Setting up fire9/squeeze1x1
I1107 16:43:08.698663  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.698663  5452 net.cpp:137] Memory required for data: 936346800
I1107 16:43:08.698663  5452 layer_factory.cpp:58] Creating layer bn_21
I1107 16:43:08.698663  5452 net.cpp:84] Creating Layer bn_21
I1107 16:43:08.698663  5452 net.cpp:406] bn_21 <- conv_21
I1107 16:43:08.698663  5452 net.cpp:367] bn_21 -> conv_21 (in-place)
I1107 16:43:08.698663  5452 net.cpp:122] Setting up bn_21
I1107 16:43:08.698663  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.698663  5452 net.cpp:137] Memory required for data: 937601200
I1107 16:43:08.698663  5452 layer_factory.cpp:58] Creating layer scale_21
I1107 16:43:08.698663  5452 net.cpp:84] Creating Layer scale_21
I1107 16:43:08.698663  5452 net.cpp:406] scale_21 <- conv_21
I1107 16:43:08.698663  5452 net.cpp:380] scale_21 -> fire9/squeeze1x1
I1107 16:43:08.698663  5452 layer_factory.cpp:58] Creating layer scale_21
I1107 16:43:08.698663  5452 net.cpp:122] Setting up scale_21
I1107 16:43:08.698663  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.698663  5452 net.cpp:137] Memory required for data: 938855600
I1107 16:43:08.698663  5452 layer_factory.cpp:58] Creating layer fire9/relu_squeeze1x1
I1107 16:43:08.698663  5452 net.cpp:84] Creating Layer fire9/relu_squeeze1x1
I1107 16:43:08.698663  5452 net.cpp:406] fire9/relu_squeeze1x1 <- fire9/squeeze1x1
I1107 16:43:08.698663  5452 net.cpp:367] fire9/relu_squeeze1x1 -> fire9/squeeze1x1 (in-place)
I1107 16:43:08.699663  5452 net.cpp:122] Setting up fire9/relu_squeeze1x1
I1107 16:43:08.699663  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.699663  5452 net.cpp:137] Memory required for data: 940110000
I1107 16:43:08.699663  5452 layer_factory.cpp:58] Creating layer fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 16:43:08.699663  5452 net.cpp:84] Creating Layer fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 16:43:08.699663  5452 net.cpp:406] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split <- fire9/squeeze1x1
I1107 16:43:08.699663  5452 net.cpp:380] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split -> fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_0
I1107 16:43:08.699663  5452 net.cpp:380] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split -> fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_1
I1107 16:43:08.699663  5452 net.cpp:122] Setting up fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 16:43:08.699663  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.699663  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.699663  5452 net.cpp:137] Memory required for data: 942618800
I1107 16:43:08.699663  5452 layer_factory.cpp:58] Creating layer fire9/expand1x1
I1107 16:43:08.699663  5452 net.cpp:84] Creating Layer fire9/expand1x1
I1107 16:43:08.699663  5452 net.cpp:406] fire9/expand1x1 <- fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_0
I1107 16:43:08.699663  5452 net.cpp:380] fire9/expand1x1 -> conv_22
I1107 16:43:08.700664  5452 net.cpp:122] Setting up fire9/expand1x1
I1107 16:43:08.700664  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.700664  5452 net.cpp:137] Memory required for data: 947636400
I1107 16:43:08.700664  5452 layer_factory.cpp:58] Creating layer bn_22
I1107 16:43:08.700664  5452 net.cpp:84] Creating Layer bn_22
I1107 16:43:08.700664  5452 net.cpp:406] bn_22 <- conv_22
I1107 16:43:08.700664  5452 net.cpp:367] bn_22 -> conv_22 (in-place)
I1107 16:43:08.700664  5452 net.cpp:122] Setting up bn_22
I1107 16:43:08.700664  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.700664  5452 net.cpp:137] Memory required for data: 952654000
I1107 16:43:08.700664  5452 layer_factory.cpp:58] Creating layer scale_22
I1107 16:43:08.700664  5452 net.cpp:84] Creating Layer scale_22
I1107 16:43:08.700664  5452 net.cpp:406] scale_22 <- conv_22
I1107 16:43:08.700664  5452 net.cpp:380] scale_22 -> fire9/expand1x1
I1107 16:43:08.700664  5452 layer_factory.cpp:58] Creating layer scale_22
I1107 16:43:08.700664  5452 net.cpp:122] Setting up scale_22
I1107 16:43:08.701661  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.701661  5452 net.cpp:137] Memory required for data: 957671600
I1107 16:43:08.701661  5452 layer_factory.cpp:58] Creating layer fire9/relu_expand1x1
I1107 16:43:08.701661  5452 net.cpp:84] Creating Layer fire9/relu_expand1x1
I1107 16:43:08.701661  5452 net.cpp:406] fire9/relu_expand1x1 <- fire9/expand1x1
I1107 16:43:08.701661  5452 net.cpp:367] fire9/relu_expand1x1 -> fire9/expand1x1 (in-place)
I1107 16:43:08.701661  5452 net.cpp:122] Setting up fire9/relu_expand1x1
I1107 16:43:08.701661  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.701661  5452 net.cpp:137] Memory required for data: 962689200
I1107 16:43:08.701661  5452 layer_factory.cpp:58] Creating layer fire9/expand3x3
I1107 16:43:08.701661  5452 net.cpp:84] Creating Layer fire9/expand3x3
I1107 16:43:08.701661  5452 net.cpp:406] fire9/expand3x3 <- fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_1
I1107 16:43:08.701661  5452 net.cpp:380] fire9/expand3x3 -> conv_23
I1107 16:43:08.703663  5452 net.cpp:122] Setting up fire9/expand3x3
I1107 16:43:08.703663  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.703663  5452 net.cpp:137] Memory required for data: 967706800
I1107 16:43:08.703663  5452 layer_factory.cpp:58] Creating layer bn_23
I1107 16:43:08.703663  5452 net.cpp:84] Creating Layer bn_23
I1107 16:43:08.703663  5452 net.cpp:406] bn_23 <- conv_23
I1107 16:43:08.703663  5452 net.cpp:367] bn_23 -> conv_23 (in-place)
I1107 16:43:08.703663  5452 net.cpp:122] Setting up bn_23
I1107 16:43:08.703663  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.703663  5452 net.cpp:137] Memory required for data: 972724400
I1107 16:43:08.703663  5452 layer_factory.cpp:58] Creating layer scale_23
I1107 16:43:08.703663  5452 net.cpp:84] Creating Layer scale_23
I1107 16:43:08.703663  5452 net.cpp:406] scale_23 <- conv_23
I1107 16:43:08.703663  5452 net.cpp:380] scale_23 -> fire9/expand3x3
I1107 16:43:08.703663  5452 layer_factory.cpp:58] Creating layer scale_23
I1107 16:43:08.703663  5452 net.cpp:122] Setting up scale_23
I1107 16:43:08.703663  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.703663  5452 net.cpp:137] Memory required for data: 977742000
I1107 16:43:08.703663  5452 layer_factory.cpp:58] Creating layer fire9/relu_expand3x3
I1107 16:43:08.703663  5452 net.cpp:84] Creating Layer fire9/relu_expand3x3
I1107 16:43:08.703663  5452 net.cpp:406] fire9/relu_expand3x3 <- fire9/expand3x3
I1107 16:43:08.703663  5452 net.cpp:367] fire9/relu_expand3x3 -> fire9/expand3x3 (in-place)
I1107 16:43:08.704663  5452 net.cpp:122] Setting up fire9/relu_expand3x3
I1107 16:43:08.704663  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.704663  5452 net.cpp:137] Memory required for data: 982759600
I1107 16:43:08.704663  5452 layer_factory.cpp:58] Creating layer fire9/concat
I1107 16:43:08.704663  5452 net.cpp:84] Creating Layer fire9/concat
I1107 16:43:08.704663  5452 net.cpp:406] fire9/concat <- fire9/expand1x1
I1107 16:43:08.704663  5452 net.cpp:406] fire9/concat <- fire9/expand3x3
I1107 16:43:08.704663  5452 net.cpp:380] fire9/concat -> fire9/concat
I1107 16:43:08.704663  5452 net.cpp:122] Setting up fire9/concat
I1107 16:43:08.704663  5452 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 16:43:08.704663  5452 net.cpp:137] Memory required for data: 992794800
I1107 16:43:08.704663  5452 layer_factory.cpp:58] Creating layer drop9
I1107 16:43:08.704663  5452 net.cpp:84] Creating Layer drop9
I1107 16:43:08.704663  5452 net.cpp:406] drop9 <- fire9/concat
I1107 16:43:08.704663  5452 net.cpp:367] drop9 -> fire9/concat (in-place)
I1107 16:43:08.704663  5452 net.cpp:122] Setting up drop9
I1107 16:43:08.704663  5452 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 16:43:08.704663  5452 net.cpp:137] Memory required for data: 1002830000
I1107 16:43:08.704663  5452 layer_factory.cpp:58] Creating layer conv10
I1107 16:43:08.704663  5452 net.cpp:84] Creating Layer conv10
I1107 16:43:08.704663  5452 net.cpp:406] conv10 <- fire9/concat
I1107 16:43:08.704663  5452 net.cpp:380] conv10 -> conv10
I1107 16:43:08.705663  5452 net.cpp:122] Setting up conv10
I1107 16:43:08.705663  5452 net.cpp:129] Top shape: 100 10 7 7 (49000)
I1107 16:43:08.705663  5452 net.cpp:137] Memory required for data: 1003026000
I1107 16:43:08.705663  5452 layer_factory.cpp:58] Creating layer relu_conv10
I1107 16:43:08.705663  5452 net.cpp:84] Creating Layer relu_conv10
I1107 16:43:08.705663  5452 net.cpp:406] relu_conv10 <- conv10
I1107 16:43:08.705663  5452 net.cpp:367] relu_conv10 -> conv10 (in-place)
I1107 16:43:08.705663  5452 net.cpp:122] Setting up relu_conv10
I1107 16:43:08.705663  5452 net.cpp:129] Top shape: 100 10 7 7 (49000)
I1107 16:43:08.705663  5452 net.cpp:137] Memory required for data: 1003222000
I1107 16:43:08.705663  5452 layer_factory.cpp:58] Creating layer pool10
I1107 16:43:08.705663  5452 net.cpp:84] Creating Layer pool10
I1107 16:43:08.705663  5452 net.cpp:406] pool10 <- conv10
I1107 16:43:08.705663  5452 net.cpp:380] pool10 -> pool10
I1107 16:43:08.706663  5452 net.cpp:122] Setting up pool10
I1107 16:43:08.706663  5452 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 16:43:08.706663  5452 net.cpp:137] Memory required for data: 1003226000
I1107 16:43:08.706663  5452 layer_factory.cpp:58] Creating layer pool10_pool10_0_split
I1107 16:43:08.706663  5452 net.cpp:84] Creating Layer pool10_pool10_0_split
I1107 16:43:08.706663  5452 net.cpp:406] pool10_pool10_0_split <- pool10
I1107 16:43:08.706663  5452 net.cpp:380] pool10_pool10_0_split -> pool10_pool10_0_split_0
I1107 16:43:08.706663  5452 net.cpp:380] pool10_pool10_0_split -> pool10_pool10_0_split_1
I1107 16:43:08.706663  5452 net.cpp:122] Setting up pool10_pool10_0_split
I1107 16:43:08.706663  5452 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 16:43:08.706663  5452 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 16:43:08.706663  5452 net.cpp:137] Memory required for data: 1003234000
I1107 16:43:08.706663  5452 layer_factory.cpp:58] Creating layer accuracy_training
I1107 16:43:08.706663  5452 net.cpp:84] Creating Layer accuracy_training
I1107 16:43:08.706663  5452 net.cpp:406] accuracy_training <- pool10_pool10_0_split_0
I1107 16:43:08.706663  5452 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1107 16:43:08.706663  5452 net.cpp:380] accuracy_training -> accuracy_training
I1107 16:43:08.706663  5452 net.cpp:122] Setting up accuracy_training
I1107 16:43:08.706663  5452 net.cpp:129] Top shape: (1)
I1107 16:43:08.706663  5452 net.cpp:137] Memory required for data: 1003234004
I1107 16:43:08.706663  5452 layer_factory.cpp:58] Creating layer loss
I1107 16:43:08.706663  5452 net.cpp:84] Creating Layer loss
I1107 16:43:08.706663  5452 net.cpp:406] loss <- pool10_pool10_0_split_1
I1107 16:43:08.706663  5452 net.cpp:406] loss <- label_cifar_1_split_1
I1107 16:43:08.706663  5452 net.cpp:380] loss -> loss
I1107 16:43:08.706663  5452 layer_factory.cpp:58] Creating layer loss
I1107 16:43:08.706663  5452 net.cpp:122] Setting up loss
I1107 16:43:08.706663  5452 net.cpp:129] Top shape: (1)
I1107 16:43:08.706663  5452 net.cpp:132]     with loss weight 1
I1107 16:43:08.706663  5452 net.cpp:137] Memory required for data: 1003234008
I1107 16:43:08.706663  5452 net.cpp:198] loss needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:200] accuracy_training does not need backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] pool10_pool10_0_split needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] pool10 needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] relu_conv10 needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] conv10 needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] drop9 needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] fire9/concat needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] fire9/relu_expand3x3 needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] scale_23 needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] bn_23 needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] fire9/expand3x3 needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] fire9/relu_expand1x1 needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] scale_22 needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] bn_22 needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] fire9/expand1x1 needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.706663  5452 net.cpp:198] fire9/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_21 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_21 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire9/squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire8/concat needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire8/relu_expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_20 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_20 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire8/expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire8/relu_expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_19 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_19 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire8/expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire8/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_18 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_18 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire8/squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire7/concat needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire7/relu_expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_17 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_17 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire7/expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire7/relu_expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_16 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_16 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire7/expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire7/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_15 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_15 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire7/squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire6/concat needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire6/relu_expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_14 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_14 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire6/expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire6/relu_expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_13 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_13 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire6/expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire6/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_12 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_12 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire6/squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] pool5 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire5/concat needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire5/relu_expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_11 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_11 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire5/expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire5/relu_expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_10 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_10 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire5/expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire5/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_9 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_9 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire5/squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire4/concat needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire4/relu_expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_8 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_8 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire4/expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire4/relu_expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire4/expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire4/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_7 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_7 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire4/squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] pool3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire3/concat needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire3/relu_expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_6 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_6 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire3/expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire3/relu_expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_5 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_5 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire3/expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire3/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_4 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_4 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire3/squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire2/concat needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire2/relu_expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale_3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn_3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire2/expand3x3 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire2/relu_expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale2 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn2 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire2/expand1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire2/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] fire2/squeeze1x1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] pool1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] relu_conv1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] scale1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] bn1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:198] conv1 needs backward computation.
I1107 16:43:08.707664  5452 net.cpp:200] label_cifar_1_split does not need backward computation.
I1107 16:43:08.707664  5452 net.cpp:200] cifar does not need backward computation.
I1107 16:43:08.707664  5452 net.cpp:242] This network produces output accuracy_training
I1107 16:43:08.707664  5452 net.cpp:242] This network produces output loss
I1107 16:43:08.707664  5452 net.cpp:255] Network initialization done.
I1107 16:43:08.709664  5452 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1107 16:43:08.709664  5452 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1107 16:43:08.709664  5452 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_3
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_4
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_5
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_6
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_7
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_8
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_9
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_10
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_11
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_12
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_13
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_14
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_15
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_16
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_17
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_18
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_19
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_20
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_21
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_22
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_23
I1107 16:43:08.709664  5452 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1107 16:43:08.709664  5452 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_Squeezenet_1.1_Batchnorm"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "fire2/squeeze1x1"
  type: "Convolution"
  bottom: "pool1"
  top: "fire2/squeeze1x1"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire2/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire2/squeeze1x1"
  top: "fire2/squeeze1x1"
}
layer {
  name: "fire2/expand1x1"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "conv2"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "fire2/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire2/relu_expand1x1"
  type: "ReLU"
  bottom: "fire2/expand1x1"
  top: "fire2/expand1x1"
}
layer {
  name: "fire2/expand3x3"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "conv_3"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_3"
  type: "BatchNorm"
  bottom: "conv_3"
  top: "conv_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_3"
  type: "Scale"
  bottom: "conv_3"
  top: "fire2/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire2/relu_expand3x3"
  type: "ReLU"
  bottom: "fire2/expand3x3"
  top: "fire2/expand3x3"
}
layer {
  name: "fire2/concat"
  type: "Concat"
  bottom: "fire2/expand1x1"
  bottom: "fire2/expand3x3"
  top: "fire2/concat"
}
layer {
  name: "fire3/squeeze1x1"
  type: "Convolution"
  bottom: "fire2/concat"
  top: "conv_4"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_4"
  type: "BatchNorm"
  bottom: "conv_4"
  top: "conv_4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_4"
  type: "Scale"
  bottom: "conv_4"
  top: "fire3/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire3/squeeze1x1"
  top: "fire3/squeeze1x1"
}
layer {
  name: "fire3/expand1x1"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "conv_5"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_5"
  type: "BatchNorm"
  bottom: "conv_5"
  top: "conv_5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_5"
  type: "Scale"
  bottom: "conv_5"
  top: "fire3/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_expand1x1"
  type: "ReLU"
  bottom: "fire3/expand1x1"
  top: "fire3/expand1x1"
}
layer {
  name: "fire3/expand3x3"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "conv_6"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_6"
  type: "BatchNorm"
  bottom: "conv_6"
  top: "conv_6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_6"
  type: "Scale"
  bottom: "conv_6"
  top: "fire3/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire3/relu_expand3x3"
  type: "ReLU"
  bottom: "fire3/expand3x3"
  top: "fire3/expand3x3"
}
layer {
  name: "fire3/concat"
  type: "Concat"
  bottom: "fire3/expand1x1"
  bottom: "fire3/expand3x3"
  top: "fire3/concat"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "fire3/concat"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire4/squeeze1x1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv_7"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_7"
  type: "BatchNorm"
  bottom: "conv_7"
  top: "conv_7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_7"
  type: "Scale"
  bottom: "conv_7"
  top: "fire4/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire4/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire4/squeeze1x1"
  top: "fire4/squeeze1x1"
}
layer {
  name: "fire4/expand1x1"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "fire4/expand1x1"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire4/relu_expand1x1"
  type: "ReLU"
  bottom: "fire4/expand1x1"
  top: "fire4/expand1x1"
}
layer {
  name: "fire4/expand3x3"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "conv_8"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_8"
  type: "BatchNorm"
  bottom: "conv_8"
  top: "conv_8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_8"
  type: "Scale"
  bottom: "conv_8"
  top: "fire4/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire4/relu_expand3x3"
  type: "ReLU"
  bottom: "fire4/expand3x3"
  top: "fire4/expand3x3"
}
layer {
  name: "fire4/concat"
  type: "Concat"
  bottom: "fire4/expand1x1"
  bottom: "fire4/expand3x3"
  top: "fire4/concat"
}
layer {
  name: "fire5/squeeze1x1"
  type: "Convolution"
  bottom: "fire4/concat"
  top: "conv_9"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_9"
  type: "BatchNorm"
  bottom: "conv_9"
  top: "conv_9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_9"
  type: "Scale"
  bottom: "conv_9"
  top: "fire5/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire5/squeeze1x1"
  top: "fire5/squeeze1x1"
}
layer {
  name: "fire5/expand1x1"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "conv_10"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_10"
  type: "BatchNorm"
  bottom: "conv_10"
  top: "conv_10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_10"
  type: "Scale"
  bottom: "conv_10"
  top: "fire5/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_expand1x1"
  type: "ReLU"
  bottom: "fire5/expand1x1"
  top: "fire5/expand1x1"
}
layer {
  name: "fire5/expand3x3"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "conv_11"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_11"
  type: "BatchNorm"
  bottom: "conv_11"
  top: "conv_11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_11"
  type: "Scale"
  bottom: "conv_11"
  top: "fire5/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire5/relu_expand3x3"
  type: "ReLU"
  bottom: "fire5/expand3x3"
  top: "fire5/expand3x3"
}
layer {
  name: "fire5/concat"
  type: "Concat"
  bottom: "fire5/expand1x1"
  bottom: "fire5/expand3x3"
  top: "fire5/concat"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "fire5/concat"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire6/squeeze1x1"
  type: "Convolution"
  bottom: "pool5"
  top: "conv_12"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_12"
  type: "BatchNorm"
  bottom: "conv_12"
  top: "conv_12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_12"
  type: "Scale"
  bottom: "conv_12"
  top: "fire6/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire6/squeeze1x1"
  top: "fire6/squeeze1x1"
}
layer {
  name: "fire6/expand1x1"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "conv_13"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_13"
  type: "BatchNorm"
  bottom: "conv_13"
  top: "conv_13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_13"
  type: "Scale"
  bottom: "conv_13"
  top: "fire6/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_expand1x1"
  type: "ReLU"
  bottom: "fire6/expand1x1"
  top: "fire6/expand1x1"
}
layer {
  name: "fire6/expand3x3"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "conv_14"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_14"
  type: "BatchNorm"
  bottom: "conv_14"
  top: "conv_14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_14"
  type: "Scale"
  bottom: "conv_14"
  top: "fire6/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire6/relu_expand3x3"
  type: "ReLU"
  bottom: "fire6/expand3x3"
  top: "fire6/expand3x3"
}
layer {
  name: "fire6/concat"
  type: "Concat"
  bottom: "fire6/expand1x1"
  bottom: "fire6/expand3x3"
  top: "fire6/concat"
}
layer {
  name: "fire7/squeeze1x1"
  type: "Convolution"
  bottom: "fire6/concat"
  top: "conv_15"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_15"
  type: "BatchNorm"
  bottom: "conv_15"
  top: "conv_15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_15"
  type: "Scale"
  bottom: "conv_15"
  top: "fire7/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire7/squeeze1x1"
  top: "fire7/squeeze1x1"
}
layer {
  name: "fire7/expand1x1"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "conv_16"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_16"
  type: "BatchNorm"
  bottom: "conv_16"
  top: "conv_16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_16"
  type: "Scale"
  bottom: "conv_16"
  top: "fire7/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_expand1x1"
  type: "ReLU"
  bottom: "fire7/expand1x1"
  top: "fire7/expand1x1"
}
layer {
  name: "fire7/expand3x3"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "conv_17"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_17"
  type: "BatchNorm"
  bottom: "conv_17"
  top: "conv_17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_17"
  type: "Scale"
  bottom: "conv_17"
  top: "fire7/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire7/relu_expand3x3"
  type: "ReLU"
  bottom: "fire7/expand3x3"
  top: "fire7/expand3x3"
}
layer {
  name: "fire7/concat"
  type: "Concat"
  bottom: "fire7/expand1x1"
  bottom: "fire7/expand3x3"
  top: "fire7/concat"
}
layer {
  name: "fire8/squeeze1x1"
  type: "Convolution"
  bottom: "fire7/concat"
  top: "conv_18"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_18"
  type: "BatchNorm"
  bottom: "conv_18"
  top: "conv_18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_18"
  type: "Scale"
  bottom: "conv_18"
  top: "fire8/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire8/squeeze1x1"
  top: "fire8/squeeze1x1"
}
layer {
  name: "fire8/expand1x1"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "conv_19"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_19"
  type: "BatchNorm"
  bottom: "conv_19"
  top: "conv_19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_19"
  type: "Scale"
  bottom: "conv_19"
  top: "fire8/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_expand1x1"
  type: "ReLU"
  bottom: "fire8/expand1x1"
  top: "fire8/expand1x1"
}
layer {
  name: "fire8/expand3x3"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "conv_20"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_20"
  type: "BatchNorm"
  bottom: "conv_20"
  top: "conv_20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_20"
  type: "Scale"
  bottom: "conv_20"
  top: "fire8/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire8/relu_expand3x3"
  type: "ReLU"
  bottom: "fire8/expand3x3"
  top: "fire8/expand3x3"
}
layer {
  name: "fire8/concat"
  type: "Concat"
  bottom: "fire8/expand1x1"
  bottom: "fire8/expand3x3"
  top: "fire8/concat"
}
layer {
  name: "fire9/squeeze1x1"
  type: "Convolution"
  bottom: "fire8/concat"
  top: "conv_21"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_21"
  type: "BatchNorm"
  bottom: "conv_21"
  top: "conv_21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_21"
  type: "Scale"
  bottom: "conv_21"
  top: "fire9/squeeze1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire9/squeeze1x1"
  top: "fire9/squeeze1x1"
}
layer {
  name: "fire9/expand1x1"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "conv_22"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_22"
  type: "BatchNorm"
  bottom: "conv_22"
  top: "conv_22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_22"
  type: "Scale"
  bottom: "conv_22"
  top: "fire9/expand1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_expand1x1"
  type: "ReLU"
  bottom: "fire9/expand1x1"
  top: "fire9/expand1x1"
}
layer {
  name: "fire9/expand3x3"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "conv_23"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_23"
  type: "BatchNorm"
  bottom: "conv_23"
  top: "conv_23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_23"
  type: "Scale"
  bottom: "conv_23"
  top: "fire9/expand3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "fire9/relu_expand3x3"
  type: "ReLU"
  bottom: "fire9/expand3x3"
  top: "fire9/expand3x3"
}
layer {
  name: "fire9/concat"
  type: "Concat"
  bottom: "fire9/expand1x1"
  bottom: "fire9/expand3x3"
  top: "fire9/concat"
}
layer {
  name: "drop9"
  type: "Dropout"
  bottom: "fire9/concat"
  top: "fire9/concat"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv10"
  type: "Convolution"
  bottom: "fire9/concat"
  top: "conv10"
  convolution_param {
    num_output: 10
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
  }
}
layer {
  name: "relu_conv10"
  type: "ReLU"
  bottom: "conv10"
  top: "conv10"
}
layer {
  name: "pool10"
  type: "Pooling"
  bottom: "conv10"
  top: "pool10"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "pool10"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "pool10"
  bottom: "label"
  top: "loss"
}
I1107 16:43:08.710664  5452 layer_factory.cpp:58] Creating layer cifar
I1107 16:43:08.714674  5452 db_leveldb.cpp:18] Opened leveldb examples/cifar10/cifar10_test_leveldb_padding
I1107 16:43:08.715663  5452 net.cpp:84] Creating Layer cifar
I1107 16:43:08.715663  5452 net.cpp:380] cifar -> data
I1107 16:43:08.715663  5452 net.cpp:380] cifar -> label
I1107 16:43:08.715663  5452 data_layer.cpp:45] output data size: 100,3,32,32
I1107 16:43:08.724668  5452 net.cpp:122] Setting up cifar
I1107 16:43:08.724668  5452 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1107 16:43:08.724668  5452 net.cpp:129] Top shape: 100 (100)
I1107 16:43:08.724668  5452 net.cpp:137] Memory required for data: 1229200
I1107 16:43:08.724668  5452 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1107 16:43:08.724668  5452 net.cpp:84] Creating Layer label_cifar_1_split
I1107 16:43:08.724668  5452 net.cpp:406] label_cifar_1_split <- label
I1107 16:43:08.724668  5452 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1107 16:43:08.724668  5452 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1107 16:43:08.724668  5452 net.cpp:122] Setting up label_cifar_1_split
I1107 16:43:08.724668  5452 net.cpp:129] Top shape: 100 (100)
I1107 16:43:08.724668  5452 net.cpp:129] Top shape: 100 (100)
I1107 16:43:08.724668  5452 net.cpp:137] Memory required for data: 1230000
I1107 16:43:08.724668  5452 layer_factory.cpp:58] Creating layer conv1
I1107 16:43:08.724668  5452 net.cpp:84] Creating Layer conv1
I1107 16:43:08.724668  5452 net.cpp:406] conv1 <- data
I1107 16:43:08.724668  5452 net.cpp:380] conv1 -> conv1
I1107 16:43:08.725666 18404 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1107 16:43:08.726663  5452 net.cpp:122] Setting up conv1
I1107 16:43:08.726663  5452 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 16:43:08.726663  5452 net.cpp:137] Memory required for data: 24270000
I1107 16:43:08.726663  5452 layer_factory.cpp:58] Creating layer bn1
I1107 16:43:08.726663  5452 net.cpp:84] Creating Layer bn1
I1107 16:43:08.726663  5452 net.cpp:406] bn1 <- conv1
I1107 16:43:08.726663  5452 net.cpp:367] bn1 -> conv1 (in-place)
I1107 16:43:08.726663  5452 net.cpp:122] Setting up bn1
I1107 16:43:08.726663  5452 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 16:43:08.726663  5452 net.cpp:137] Memory required for data: 47310000
I1107 16:43:08.726663  5452 layer_factory.cpp:58] Creating layer scale1
I1107 16:43:08.726663  5452 net.cpp:84] Creating Layer scale1
I1107 16:43:08.726663  5452 net.cpp:406] scale1 <- conv1
I1107 16:43:08.726663  5452 net.cpp:367] scale1 -> conv1 (in-place)
I1107 16:43:08.726663  5452 layer_factory.cpp:58] Creating layer scale1
I1107 16:43:08.726663  5452 net.cpp:122] Setting up scale1
I1107 16:43:08.726663  5452 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 16:43:08.726663  5452 net.cpp:137] Memory required for data: 70350000
I1107 16:43:08.726663  5452 layer_factory.cpp:58] Creating layer relu_conv1
I1107 16:43:08.726663  5452 net.cpp:84] Creating Layer relu_conv1
I1107 16:43:08.726663  5452 net.cpp:406] relu_conv1 <- conv1
I1107 16:43:08.726663  5452 net.cpp:367] relu_conv1 -> conv1 (in-place)
I1107 16:43:08.726663  5452 net.cpp:122] Setting up relu_conv1
I1107 16:43:08.726663  5452 net.cpp:129] Top shape: 100 64 30 30 (5760000)
I1107 16:43:08.726663  5452 net.cpp:137] Memory required for data: 93390000
I1107 16:43:08.726663  5452 layer_factory.cpp:58] Creating layer pool1
I1107 16:43:08.726663  5452 net.cpp:84] Creating Layer pool1
I1107 16:43:08.726663  5452 net.cpp:406] pool1 <- conv1
I1107 16:43:08.726663  5452 net.cpp:380] pool1 -> pool1
I1107 16:43:08.726663  5452 net.cpp:122] Setting up pool1
I1107 16:43:08.726663  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.726663  5452 net.cpp:137] Memory required for data: 113460400
I1107 16:43:08.726663  5452 layer_factory.cpp:58] Creating layer fire2/squeeze1x1
I1107 16:43:08.726663  5452 net.cpp:84] Creating Layer fire2/squeeze1x1
I1107 16:43:08.726663  5452 net.cpp:406] fire2/squeeze1x1 <- pool1
I1107 16:43:08.726663  5452 net.cpp:380] fire2/squeeze1x1 -> fire2/squeeze1x1
I1107 16:43:08.728672  5452 net.cpp:122] Setting up fire2/squeeze1x1
I1107 16:43:08.728672  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.728672  5452 net.cpp:137] Memory required for data: 118478000
I1107 16:43:08.728672  5452 layer_factory.cpp:58] Creating layer fire2/relu_squeeze1x1
I1107 16:43:08.728672  5452 net.cpp:84] Creating Layer fire2/relu_squeeze1x1
I1107 16:43:08.728672  5452 net.cpp:406] fire2/relu_squeeze1x1 <- fire2/squeeze1x1
I1107 16:43:08.728672  5452 net.cpp:367] fire2/relu_squeeze1x1 -> fire2/squeeze1x1 (in-place)
I1107 16:43:08.729173  5452 net.cpp:122] Setting up fire2/relu_squeeze1x1
I1107 16:43:08.729173  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.729173  5452 net.cpp:137] Memory required for data: 123495600
I1107 16:43:08.729173  5452 layer_factory.cpp:58] Creating layer fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 16:43:08.729173  5452 net.cpp:84] Creating Layer fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 16:43:08.729173  5452 net.cpp:406] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split <- fire2/squeeze1x1
I1107 16:43:08.729173  5452 net.cpp:380] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split -> fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_0
I1107 16:43:08.729173  5452 net.cpp:380] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split -> fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_1
I1107 16:43:08.729173  5452 net.cpp:122] Setting up fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split
I1107 16:43:08.729173  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.729173  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.729173  5452 net.cpp:137] Memory required for data: 133530800
I1107 16:43:08.729173  5452 layer_factory.cpp:58] Creating layer fire2/expand1x1
I1107 16:43:08.729173  5452 net.cpp:84] Creating Layer fire2/expand1x1
I1107 16:43:08.729173  5452 net.cpp:406] fire2/expand1x1 <- fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_0
I1107 16:43:08.729173  5452 net.cpp:380] fire2/expand1x1 -> conv2
I1107 16:43:08.730672  5452 net.cpp:122] Setting up fire2/expand1x1
I1107 16:43:08.730672  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.730672  5452 net.cpp:137] Memory required for data: 153601200
I1107 16:43:08.730672  5452 layer_factory.cpp:58] Creating layer bn2
I1107 16:43:08.730672  5452 net.cpp:84] Creating Layer bn2
I1107 16:43:08.730672  5452 net.cpp:406] bn2 <- conv2
I1107 16:43:08.730672  5452 net.cpp:367] bn2 -> conv2 (in-place)
I1107 16:43:08.730672  5452 net.cpp:122] Setting up bn2
I1107 16:43:08.730672  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.730672  5452 net.cpp:137] Memory required for data: 173671600
I1107 16:43:08.730672  5452 layer_factory.cpp:58] Creating layer scale2
I1107 16:43:08.730672  5452 net.cpp:84] Creating Layer scale2
I1107 16:43:08.730672  5452 net.cpp:406] scale2 <- conv2
I1107 16:43:08.730672  5452 net.cpp:380] scale2 -> fire2/expand1x1
I1107 16:43:08.730672  5452 layer_factory.cpp:58] Creating layer scale2
I1107 16:43:08.731173  5452 net.cpp:122] Setting up scale2
I1107 16:43:08.731173  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.731173  5452 net.cpp:137] Memory required for data: 193742000
I1107 16:43:08.731173  5452 layer_factory.cpp:58] Creating layer fire2/relu_expand1x1
I1107 16:43:08.731173  5452 net.cpp:84] Creating Layer fire2/relu_expand1x1
I1107 16:43:08.731173  5452 net.cpp:406] fire2/relu_expand1x1 <- fire2/expand1x1
I1107 16:43:08.731173  5452 net.cpp:367] fire2/relu_expand1x1 -> fire2/expand1x1 (in-place)
I1107 16:43:08.731673  5452 net.cpp:122] Setting up fire2/relu_expand1x1
I1107 16:43:08.731673  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.731673  5452 net.cpp:137] Memory required for data: 213812400
I1107 16:43:08.731673  5452 layer_factory.cpp:58] Creating layer fire2/expand3x3
I1107 16:43:08.731673  5452 net.cpp:84] Creating Layer fire2/expand3x3
I1107 16:43:08.731673  5452 net.cpp:406] fire2/expand3x3 <- fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split_1
I1107 16:43:08.731673  5452 net.cpp:380] fire2/expand3x3 -> conv_3
I1107 16:43:08.733180  5452 net.cpp:122] Setting up fire2/expand3x3
I1107 16:43:08.733180  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.733180  5452 net.cpp:137] Memory required for data: 233882800
I1107 16:43:08.733180  5452 layer_factory.cpp:58] Creating layer bn_3
I1107 16:43:08.733180  5452 net.cpp:84] Creating Layer bn_3
I1107 16:43:08.733180  5452 net.cpp:406] bn_3 <- conv_3
I1107 16:43:08.733180  5452 net.cpp:367] bn_3 -> conv_3 (in-place)
I1107 16:43:08.733672  5452 net.cpp:122] Setting up bn_3
I1107 16:43:08.733672  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.733672  5452 net.cpp:137] Memory required for data: 253953200
I1107 16:43:08.733672  5452 layer_factory.cpp:58] Creating layer scale_3
I1107 16:43:08.733672  5452 net.cpp:84] Creating Layer scale_3
I1107 16:43:08.733672  5452 net.cpp:406] scale_3 <- conv_3
I1107 16:43:08.733672  5452 net.cpp:380] scale_3 -> fire2/expand3x3
I1107 16:43:08.733672  5452 layer_factory.cpp:58] Creating layer scale_3
I1107 16:43:08.733672  5452 net.cpp:122] Setting up scale_3
I1107 16:43:08.733672  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.733672  5452 net.cpp:137] Memory required for data: 274023600
I1107 16:43:08.733672  5452 layer_factory.cpp:58] Creating layer fire2/relu_expand3x3
I1107 16:43:08.733672  5452 net.cpp:84] Creating Layer fire2/relu_expand3x3
I1107 16:43:08.733672  5452 net.cpp:406] fire2/relu_expand3x3 <- fire2/expand3x3
I1107 16:43:08.733672  5452 net.cpp:367] fire2/relu_expand3x3 -> fire2/expand3x3 (in-place)
I1107 16:43:08.734180  5452 net.cpp:122] Setting up fire2/relu_expand3x3
I1107 16:43:08.734180  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.734180  5452 net.cpp:137] Memory required for data: 294094000
I1107 16:43:08.734180  5452 layer_factory.cpp:58] Creating layer fire2/concat
I1107 16:43:08.734180  5452 net.cpp:84] Creating Layer fire2/concat
I1107 16:43:08.734180  5452 net.cpp:406] fire2/concat <- fire2/expand1x1
I1107 16:43:08.734180  5452 net.cpp:406] fire2/concat <- fire2/expand3x3
I1107 16:43:08.734180  5452 net.cpp:380] fire2/concat -> fire2/concat
I1107 16:43:08.734180  5452 net.cpp:122] Setting up fire2/concat
I1107 16:43:08.734180  5452 net.cpp:129] Top shape: 100 128 28 28 (10035200)
I1107 16:43:08.734180  5452 net.cpp:137] Memory required for data: 334234800
I1107 16:43:08.734180  5452 layer_factory.cpp:58] Creating layer fire3/squeeze1x1
I1107 16:43:08.734180  5452 net.cpp:84] Creating Layer fire3/squeeze1x1
I1107 16:43:08.734180  5452 net.cpp:406] fire3/squeeze1x1 <- fire2/concat
I1107 16:43:08.734180  5452 net.cpp:380] fire3/squeeze1x1 -> conv_4
I1107 16:43:08.735173  5452 net.cpp:122] Setting up fire3/squeeze1x1
I1107 16:43:08.735672  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.735672  5452 net.cpp:137] Memory required for data: 339252400
I1107 16:43:08.735672  5452 layer_factory.cpp:58] Creating layer bn_4
I1107 16:43:08.735672  5452 net.cpp:84] Creating Layer bn_4
I1107 16:43:08.735672  5452 net.cpp:406] bn_4 <- conv_4
I1107 16:43:08.735672  5452 net.cpp:367] bn_4 -> conv_4 (in-place)
I1107 16:43:08.735672  5452 net.cpp:122] Setting up bn_4
I1107 16:43:08.735672  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.735672  5452 net.cpp:137] Memory required for data: 344270000
I1107 16:43:08.735672  5452 layer_factory.cpp:58] Creating layer scale_4
I1107 16:43:08.735672  5452 net.cpp:84] Creating Layer scale_4
I1107 16:43:08.735672  5452 net.cpp:406] scale_4 <- conv_4
I1107 16:43:08.735672  5452 net.cpp:380] scale_4 -> fire3/squeeze1x1
I1107 16:43:08.735672  5452 layer_factory.cpp:58] Creating layer scale_4
I1107 16:43:08.736172  5452 net.cpp:122] Setting up scale_4
I1107 16:43:08.736172  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.736172  5452 net.cpp:137] Memory required for data: 349287600
I1107 16:43:08.736172  5452 layer_factory.cpp:58] Creating layer fire3/relu_squeeze1x1
I1107 16:43:08.736172  5452 net.cpp:84] Creating Layer fire3/relu_squeeze1x1
I1107 16:43:08.736172  5452 net.cpp:406] fire3/relu_squeeze1x1 <- fire3/squeeze1x1
I1107 16:43:08.736172  5452 net.cpp:367] fire3/relu_squeeze1x1 -> fire3/squeeze1x1 (in-place)
I1107 16:43:08.736172  5452 net.cpp:122] Setting up fire3/relu_squeeze1x1
I1107 16:43:08.736172  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.736172  5452 net.cpp:137] Memory required for data: 354305200
I1107 16:43:08.736172  5452 layer_factory.cpp:58] Creating layer fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 16:43:08.736172  5452 net.cpp:84] Creating Layer fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 16:43:08.736172  5452 net.cpp:406] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split <- fire3/squeeze1x1
I1107 16:43:08.736172  5452 net.cpp:380] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split -> fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_0
I1107 16:43:08.736172  5452 net.cpp:380] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split -> fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_1
I1107 16:43:08.736172  5452 net.cpp:122] Setting up fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split
I1107 16:43:08.736172  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.736172  5452 net.cpp:129] Top shape: 100 16 28 28 (1254400)
I1107 16:43:08.736172  5452 net.cpp:137] Memory required for data: 364340400
I1107 16:43:08.736172  5452 layer_factory.cpp:58] Creating layer fire3/expand1x1
I1107 16:43:08.736172  5452 net.cpp:84] Creating Layer fire3/expand1x1
I1107 16:43:08.736172  5452 net.cpp:406] fire3/expand1x1 <- fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_0
I1107 16:43:08.736172  5452 net.cpp:380] fire3/expand1x1 -> conv_5
I1107 16:43:08.737673  5452 net.cpp:122] Setting up fire3/expand1x1
I1107 16:43:08.737673  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.737673  5452 net.cpp:137] Memory required for data: 384410800
I1107 16:43:08.737673  5452 layer_factory.cpp:58] Creating layer bn_5
I1107 16:43:08.737673  5452 net.cpp:84] Creating Layer bn_5
I1107 16:43:08.737673  5452 net.cpp:406] bn_5 <- conv_5
I1107 16:43:08.737673  5452 net.cpp:367] bn_5 -> conv_5 (in-place)
I1107 16:43:08.737673  5452 net.cpp:122] Setting up bn_5
I1107 16:43:08.737673  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.737673  5452 net.cpp:137] Memory required for data: 404481200
I1107 16:43:08.737673  5452 layer_factory.cpp:58] Creating layer scale_5
I1107 16:43:08.737673  5452 net.cpp:84] Creating Layer scale_5
I1107 16:43:08.737673  5452 net.cpp:406] scale_5 <- conv_5
I1107 16:43:08.737673  5452 net.cpp:380] scale_5 -> fire3/expand1x1
I1107 16:43:08.738173  5452 layer_factory.cpp:58] Creating layer scale_5
I1107 16:43:08.738173  5452 net.cpp:122] Setting up scale_5
I1107 16:43:08.738173  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.738173  5452 net.cpp:137] Memory required for data: 424551600
I1107 16:43:08.738173  5452 layer_factory.cpp:58] Creating layer fire3/relu_expand1x1
I1107 16:43:08.738173  5452 net.cpp:84] Creating Layer fire3/relu_expand1x1
I1107 16:43:08.738173  5452 net.cpp:406] fire3/relu_expand1x1 <- fire3/expand1x1
I1107 16:43:08.738173  5452 net.cpp:367] fire3/relu_expand1x1 -> fire3/expand1x1 (in-place)
I1107 16:43:08.738173  5452 net.cpp:122] Setting up fire3/relu_expand1x1
I1107 16:43:08.738173  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.738173  5452 net.cpp:137] Memory required for data: 444622000
I1107 16:43:08.738173  5452 layer_factory.cpp:58] Creating layer fire3/expand3x3
I1107 16:43:08.738173  5452 net.cpp:84] Creating Layer fire3/expand3x3
I1107 16:43:08.738173  5452 net.cpp:406] fire3/expand3x3 <- fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split_1
I1107 16:43:08.738173  5452 net.cpp:380] fire3/expand3x3 -> conv_6
I1107 16:43:08.739672  5452 net.cpp:122] Setting up fire3/expand3x3
I1107 16:43:08.739672  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.739672  5452 net.cpp:137] Memory required for data: 464692400
I1107 16:43:08.739672  5452 layer_factory.cpp:58] Creating layer bn_6
I1107 16:43:08.739672  5452 net.cpp:84] Creating Layer bn_6
I1107 16:43:08.739672  5452 net.cpp:406] bn_6 <- conv_6
I1107 16:43:08.739672  5452 net.cpp:367] bn_6 -> conv_6 (in-place)
I1107 16:43:08.740172  5452 net.cpp:122] Setting up bn_6
I1107 16:43:08.740172  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.740172  5452 net.cpp:137] Memory required for data: 484762800
I1107 16:43:08.740172  5452 layer_factory.cpp:58] Creating layer scale_6
I1107 16:43:08.740172  5452 net.cpp:84] Creating Layer scale_6
I1107 16:43:08.740172  5452 net.cpp:406] scale_6 <- conv_6
I1107 16:43:08.740172  5452 net.cpp:380] scale_6 -> fire3/expand3x3
I1107 16:43:08.740172  5452 layer_factory.cpp:58] Creating layer scale_6
I1107 16:43:08.740172  5452 net.cpp:122] Setting up scale_6
I1107 16:43:08.740172  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.740172  5452 net.cpp:137] Memory required for data: 504833200
I1107 16:43:08.740172  5452 layer_factory.cpp:58] Creating layer fire3/relu_expand3x3
I1107 16:43:08.740172  5452 net.cpp:84] Creating Layer fire3/relu_expand3x3
I1107 16:43:08.740172  5452 net.cpp:406] fire3/relu_expand3x3 <- fire3/expand3x3
I1107 16:43:08.740172  5452 net.cpp:367] fire3/relu_expand3x3 -> fire3/expand3x3 (in-place)
I1107 16:43:08.740172  5452 net.cpp:122] Setting up fire3/relu_expand3x3
I1107 16:43:08.740674  5452 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1107 16:43:08.740674  5452 net.cpp:137] Memory required for data: 524903600
I1107 16:43:08.740674  5452 layer_factory.cpp:58] Creating layer fire3/concat
I1107 16:43:08.740674  5452 net.cpp:84] Creating Layer fire3/concat
I1107 16:43:08.740674  5452 net.cpp:406] fire3/concat <- fire3/expand1x1
I1107 16:43:08.740674  5452 net.cpp:406] fire3/concat <- fire3/expand3x3
I1107 16:43:08.740674  5452 net.cpp:380] fire3/concat -> fire3/concat
I1107 16:43:08.740674  5452 net.cpp:122] Setting up fire3/concat
I1107 16:43:08.740674  5452 net.cpp:129] Top shape: 100 128 28 28 (10035200)
I1107 16:43:08.740674  5452 net.cpp:137] Memory required for data: 565044400
I1107 16:43:08.740674  5452 layer_factory.cpp:58] Creating layer pool3
I1107 16:43:08.740674  5452 net.cpp:84] Creating Layer pool3
I1107 16:43:08.740674  5452 net.cpp:406] pool3 <- fire3/concat
I1107 16:43:08.740674  5452 net.cpp:380] pool3 -> pool3
I1107 16:43:08.740674  5452 net.cpp:122] Setting up pool3
I1107 16:43:08.740674  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.740674  5452 net.cpp:137] Memory required for data: 575079600
I1107 16:43:08.740674  5452 layer_factory.cpp:58] Creating layer fire4/squeeze1x1
I1107 16:43:08.740674  5452 net.cpp:84] Creating Layer fire4/squeeze1x1
I1107 16:43:08.740674  5452 net.cpp:406] fire4/squeeze1x1 <- pool3
I1107 16:43:08.740674  5452 net.cpp:380] fire4/squeeze1x1 -> conv_7
I1107 16:43:08.741672  5452 net.cpp:122] Setting up fire4/squeeze1x1
I1107 16:43:08.741672  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.741672  5452 net.cpp:137] Memory required for data: 577588400
I1107 16:43:08.741672  5452 layer_factory.cpp:58] Creating layer bn_7
I1107 16:43:08.741672  5452 net.cpp:84] Creating Layer bn_7
I1107 16:43:08.741672  5452 net.cpp:406] bn_7 <- conv_7
I1107 16:43:08.741672  5452 net.cpp:367] bn_7 -> conv_7 (in-place)
I1107 16:43:08.742172  5452 net.cpp:122] Setting up bn_7
I1107 16:43:08.742172  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.742172  5452 net.cpp:137] Memory required for data: 580097200
I1107 16:43:08.742172  5452 layer_factory.cpp:58] Creating layer scale_7
I1107 16:43:08.742172  5452 net.cpp:84] Creating Layer scale_7
I1107 16:43:08.742172  5452 net.cpp:406] scale_7 <- conv_7
I1107 16:43:08.742172  5452 net.cpp:380] scale_7 -> fire4/squeeze1x1
I1107 16:43:08.742172  5452 layer_factory.cpp:58] Creating layer scale_7
I1107 16:43:08.742172  5452 net.cpp:122] Setting up scale_7
I1107 16:43:08.742172  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.742172  5452 net.cpp:137] Memory required for data: 582606000
I1107 16:43:08.742172  5452 layer_factory.cpp:58] Creating layer fire4/relu_squeeze1x1
I1107 16:43:08.742172  5452 net.cpp:84] Creating Layer fire4/relu_squeeze1x1
I1107 16:43:08.742172  5452 net.cpp:406] fire4/relu_squeeze1x1 <- fire4/squeeze1x1
I1107 16:43:08.742172  5452 net.cpp:367] fire4/relu_squeeze1x1 -> fire4/squeeze1x1 (in-place)
I1107 16:43:08.742672  5452 net.cpp:122] Setting up fire4/relu_squeeze1x1
I1107 16:43:08.742672  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.742672  5452 net.cpp:137] Memory required for data: 585114800
I1107 16:43:08.742672  5452 layer_factory.cpp:58] Creating layer fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 16:43:08.742672  5452 net.cpp:84] Creating Layer fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 16:43:08.742672  5452 net.cpp:406] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split <- fire4/squeeze1x1
I1107 16:43:08.742672  5452 net.cpp:380] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split -> fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_0
I1107 16:43:08.742672  5452 net.cpp:380] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split -> fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_1
I1107 16:43:08.742672  5452 net.cpp:122] Setting up fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split
I1107 16:43:08.742672  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.742672  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.742672  5452 net.cpp:137] Memory required for data: 590132400
I1107 16:43:08.742672  5452 layer_factory.cpp:58] Creating layer fire4/expand1x1
I1107 16:43:08.742672  5452 net.cpp:84] Creating Layer fire4/expand1x1
I1107 16:43:08.742672  5452 net.cpp:406] fire4/expand1x1 <- fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_0
I1107 16:43:08.742672  5452 net.cpp:380] fire4/expand1x1 -> fire4/expand1x1
I1107 16:43:08.743676  5452 net.cpp:122] Setting up fire4/expand1x1
I1107 16:43:08.743676  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.743676  5452 net.cpp:137] Memory required for data: 600167600
I1107 16:43:08.743676  5452 layer_factory.cpp:58] Creating layer fire4/relu_expand1x1
I1107 16:43:08.743676  5452 net.cpp:84] Creating Layer fire4/relu_expand1x1
I1107 16:43:08.743676  5452 net.cpp:406] fire4/relu_expand1x1 <- fire4/expand1x1
I1107 16:43:08.743676  5452 net.cpp:367] fire4/relu_expand1x1 -> fire4/expand1x1 (in-place)
I1107 16:43:08.743676  5452 net.cpp:122] Setting up fire4/relu_expand1x1
I1107 16:43:08.743676  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.743676  5452 net.cpp:137] Memory required for data: 610202800
I1107 16:43:08.743676  5452 layer_factory.cpp:58] Creating layer fire4/expand3x3
I1107 16:43:08.743676  5452 net.cpp:84] Creating Layer fire4/expand3x3
I1107 16:43:08.743676  5452 net.cpp:406] fire4/expand3x3 <- fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split_1
I1107 16:43:08.743676  5452 net.cpp:380] fire4/expand3x3 -> conv_8
I1107 16:43:08.745678  5452 net.cpp:122] Setting up fire4/expand3x3
I1107 16:43:08.745678  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.745678  5452 net.cpp:137] Memory required for data: 620238000
I1107 16:43:08.745678  5452 layer_factory.cpp:58] Creating layer bn_8
I1107 16:43:08.745678  5452 net.cpp:84] Creating Layer bn_8
I1107 16:43:08.745678  5452 net.cpp:406] bn_8 <- conv_8
I1107 16:43:08.745678  5452 net.cpp:367] bn_8 -> conv_8 (in-place)
I1107 16:43:08.745678  5452 net.cpp:122] Setting up bn_8
I1107 16:43:08.745678  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.745678  5452 net.cpp:137] Memory required for data: 630273200
I1107 16:43:08.745678  5452 layer_factory.cpp:58] Creating layer scale_8
I1107 16:43:08.745678  5452 net.cpp:84] Creating Layer scale_8
I1107 16:43:08.745678  5452 net.cpp:406] scale_8 <- conv_8
I1107 16:43:08.745678  5452 net.cpp:380] scale_8 -> fire4/expand3x3
I1107 16:43:08.745678  5452 layer_factory.cpp:58] Creating layer scale_8
I1107 16:43:08.745678  5452 net.cpp:122] Setting up scale_8
I1107 16:43:08.745678  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.745678  5452 net.cpp:137] Memory required for data: 640308400
I1107 16:43:08.745678  5452 layer_factory.cpp:58] Creating layer fire4/relu_expand3x3
I1107 16:43:08.745678  5452 net.cpp:84] Creating Layer fire4/relu_expand3x3
I1107 16:43:08.745678  5452 net.cpp:406] fire4/relu_expand3x3 <- fire4/expand3x3
I1107 16:43:08.745678  5452 net.cpp:367] fire4/relu_expand3x3 -> fire4/expand3x3 (in-place)
I1107 16:43:08.745678  5452 net.cpp:122] Setting up fire4/relu_expand3x3
I1107 16:43:08.745678  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.745678  5452 net.cpp:137] Memory required for data: 650343600
I1107 16:43:08.745678  5452 layer_factory.cpp:58] Creating layer fire4/concat
I1107 16:43:08.745678  5452 net.cpp:84] Creating Layer fire4/concat
I1107 16:43:08.745678  5452 net.cpp:406] fire4/concat <- fire4/expand1x1
I1107 16:43:08.745678  5452 net.cpp:406] fire4/concat <- fire4/expand3x3
I1107 16:43:08.745678  5452 net.cpp:380] fire4/concat -> fire4/concat
I1107 16:43:08.745678  5452 net.cpp:122] Setting up fire4/concat
I1107 16:43:08.745678  5452 net.cpp:129] Top shape: 100 256 14 14 (5017600)
I1107 16:43:08.745678  5452 net.cpp:137] Memory required for data: 670414000
I1107 16:43:08.745678  5452 layer_factory.cpp:58] Creating layer fire5/squeeze1x1
I1107 16:43:08.745678  5452 net.cpp:84] Creating Layer fire5/squeeze1x1
I1107 16:43:08.745678  5452 net.cpp:406] fire5/squeeze1x1 <- fire4/concat
I1107 16:43:08.745678  5452 net.cpp:380] fire5/squeeze1x1 -> conv_9
I1107 16:43:08.747678  5452 net.cpp:122] Setting up fire5/squeeze1x1
I1107 16:43:08.747678  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.747678  5452 net.cpp:137] Memory required for data: 672922800
I1107 16:43:08.747678  5452 layer_factory.cpp:58] Creating layer bn_9
I1107 16:43:08.747678  5452 net.cpp:84] Creating Layer bn_9
I1107 16:43:08.747678  5452 net.cpp:406] bn_9 <- conv_9
I1107 16:43:08.747678  5452 net.cpp:367] bn_9 -> conv_9 (in-place)
I1107 16:43:08.747678  5452 net.cpp:122] Setting up bn_9
I1107 16:43:08.747678  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.747678  5452 net.cpp:137] Memory required for data: 675431600
I1107 16:43:08.747678  5452 layer_factory.cpp:58] Creating layer scale_9
I1107 16:43:08.747678  5452 net.cpp:84] Creating Layer scale_9
I1107 16:43:08.747678  5452 net.cpp:406] scale_9 <- conv_9
I1107 16:43:08.747678  5452 net.cpp:380] scale_9 -> fire5/squeeze1x1
I1107 16:43:08.747678  5452 layer_factory.cpp:58] Creating layer scale_9
I1107 16:43:08.747678  5452 net.cpp:122] Setting up scale_9
I1107 16:43:08.747678  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.747678  5452 net.cpp:137] Memory required for data: 677940400
I1107 16:43:08.747678  5452 layer_factory.cpp:58] Creating layer fire5/relu_squeeze1x1
I1107 16:43:08.747678  5452 net.cpp:84] Creating Layer fire5/relu_squeeze1x1
I1107 16:43:08.747678  5452 net.cpp:406] fire5/relu_squeeze1x1 <- fire5/squeeze1x1
I1107 16:43:08.747678  5452 net.cpp:367] fire5/relu_squeeze1x1 -> fire5/squeeze1x1 (in-place)
I1107 16:43:08.747678  5452 net.cpp:122] Setting up fire5/relu_squeeze1x1
I1107 16:43:08.747678  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.747678  5452 net.cpp:137] Memory required for data: 680449200
I1107 16:43:08.747678  5452 layer_factory.cpp:58] Creating layer fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 16:43:08.747678  5452 net.cpp:84] Creating Layer fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 16:43:08.747678  5452 net.cpp:406] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split <- fire5/squeeze1x1
I1107 16:43:08.747678  5452 net.cpp:380] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split -> fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_0
I1107 16:43:08.747678  5452 net.cpp:380] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split -> fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_1
I1107 16:43:08.747678  5452 net.cpp:122] Setting up fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split
I1107 16:43:08.747678  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.747678  5452 net.cpp:129] Top shape: 100 32 14 14 (627200)
I1107 16:43:08.747678  5452 net.cpp:137] Memory required for data: 685466800
I1107 16:43:08.747678  5452 layer_factory.cpp:58] Creating layer fire5/expand1x1
I1107 16:43:08.747678  5452 net.cpp:84] Creating Layer fire5/expand1x1
I1107 16:43:08.747678  5452 net.cpp:406] fire5/expand1x1 <- fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_0
I1107 16:43:08.747678  5452 net.cpp:380] fire5/expand1x1 -> conv_10
I1107 16:43:08.749680  5452 net.cpp:122] Setting up fire5/expand1x1
I1107 16:43:08.749680  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.749680  5452 net.cpp:137] Memory required for data: 695502000
I1107 16:43:08.749680  5452 layer_factory.cpp:58] Creating layer bn_10
I1107 16:43:08.749680  5452 net.cpp:84] Creating Layer bn_10
I1107 16:43:08.749680  5452 net.cpp:406] bn_10 <- conv_10
I1107 16:43:08.749680  5452 net.cpp:367] bn_10 -> conv_10 (in-place)
I1107 16:43:08.749680  5452 net.cpp:122] Setting up bn_10
I1107 16:43:08.749680  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.749680  5452 net.cpp:137] Memory required for data: 705537200
I1107 16:43:08.749680  5452 layer_factory.cpp:58] Creating layer scale_10
I1107 16:43:08.749680  5452 net.cpp:84] Creating Layer scale_10
I1107 16:43:08.749680  5452 net.cpp:406] scale_10 <- conv_10
I1107 16:43:08.749680  5452 net.cpp:380] scale_10 -> fire5/expand1x1
I1107 16:43:08.749680  5452 layer_factory.cpp:58] Creating layer scale_10
I1107 16:43:08.749680  5452 net.cpp:122] Setting up scale_10
I1107 16:43:08.749680  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.749680  5452 net.cpp:137] Memory required for data: 715572400
I1107 16:43:08.749680  5452 layer_factory.cpp:58] Creating layer fire5/relu_expand1x1
I1107 16:43:08.749680  5452 net.cpp:84] Creating Layer fire5/relu_expand1x1
I1107 16:43:08.749680  5452 net.cpp:406] fire5/relu_expand1x1 <- fire5/expand1x1
I1107 16:43:08.749680  5452 net.cpp:367] fire5/relu_expand1x1 -> fire5/expand1x1 (in-place)
I1107 16:43:08.750679  5452 net.cpp:122] Setting up fire5/relu_expand1x1
I1107 16:43:08.750679  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.750679  5452 net.cpp:137] Memory required for data: 725607600
I1107 16:43:08.750679  5452 layer_factory.cpp:58] Creating layer fire5/expand3x3
I1107 16:43:08.750679  5452 net.cpp:84] Creating Layer fire5/expand3x3
I1107 16:43:08.750679  5452 net.cpp:406] fire5/expand3x3 <- fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split_1
I1107 16:43:08.750679  5452 net.cpp:380] fire5/expand3x3 -> conv_11
I1107 16:43:08.751678  5452 net.cpp:122] Setting up fire5/expand3x3
I1107 16:43:08.751678  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.751678  5452 net.cpp:137] Memory required for data: 735642800
I1107 16:43:08.751678  5452 layer_factory.cpp:58] Creating layer bn_11
I1107 16:43:08.751678  5452 net.cpp:84] Creating Layer bn_11
I1107 16:43:08.751678  5452 net.cpp:406] bn_11 <- conv_11
I1107 16:43:08.751678  5452 net.cpp:367] bn_11 -> conv_11 (in-place)
I1107 16:43:08.751678  5452 net.cpp:122] Setting up bn_11
I1107 16:43:08.751678  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.751678  5452 net.cpp:137] Memory required for data: 745678000
I1107 16:43:08.751678  5452 layer_factory.cpp:58] Creating layer scale_11
I1107 16:43:08.751678  5452 net.cpp:84] Creating Layer scale_11
I1107 16:43:08.751678  5452 net.cpp:406] scale_11 <- conv_11
I1107 16:43:08.751678  5452 net.cpp:380] scale_11 -> fire5/expand3x3
I1107 16:43:08.751678  5452 layer_factory.cpp:58] Creating layer scale_11
I1107 16:43:08.751678  5452 net.cpp:122] Setting up scale_11
I1107 16:43:08.751678  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.751678  5452 net.cpp:137] Memory required for data: 755713200
I1107 16:43:08.751678  5452 layer_factory.cpp:58] Creating layer fire5/relu_expand3x3
I1107 16:43:08.751678  5452 net.cpp:84] Creating Layer fire5/relu_expand3x3
I1107 16:43:08.751678  5452 net.cpp:406] fire5/relu_expand3x3 <- fire5/expand3x3
I1107 16:43:08.751678  5452 net.cpp:367] fire5/relu_expand3x3 -> fire5/expand3x3 (in-place)
I1107 16:43:08.752678  5452 net.cpp:122] Setting up fire5/relu_expand3x3
I1107 16:43:08.752678  5452 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1107 16:43:08.752678  5452 net.cpp:137] Memory required for data: 765748400
I1107 16:43:08.752678  5452 layer_factory.cpp:58] Creating layer fire5/concat
I1107 16:43:08.752678  5452 net.cpp:84] Creating Layer fire5/concat
I1107 16:43:08.752678  5452 net.cpp:406] fire5/concat <- fire5/expand1x1
I1107 16:43:08.752678  5452 net.cpp:406] fire5/concat <- fire5/expand3x3
I1107 16:43:08.752678  5452 net.cpp:380] fire5/concat -> fire5/concat
I1107 16:43:08.752678  5452 net.cpp:122] Setting up fire5/concat
I1107 16:43:08.752678  5452 net.cpp:129] Top shape: 100 256 14 14 (5017600)
I1107 16:43:08.752678  5452 net.cpp:137] Memory required for data: 785818800
I1107 16:43:08.752678  5452 layer_factory.cpp:58] Creating layer pool5
I1107 16:43:08.752678  5452 net.cpp:84] Creating Layer pool5
I1107 16:43:08.752678  5452 net.cpp:406] pool5 <- fire5/concat
I1107 16:43:08.752678  5452 net.cpp:380] pool5 -> pool5
I1107 16:43:08.752678  5452 net.cpp:122] Setting up pool5
I1107 16:43:08.752678  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.752678  5452 net.cpp:137] Memory required for data: 790836400
I1107 16:43:08.752678  5452 layer_factory.cpp:58] Creating layer fire6/squeeze1x1
I1107 16:43:08.752678  5452 net.cpp:84] Creating Layer fire6/squeeze1x1
I1107 16:43:08.752678  5452 net.cpp:406] fire6/squeeze1x1 <- pool5
I1107 16:43:08.752678  5452 net.cpp:380] fire6/squeeze1x1 -> conv_12
I1107 16:43:08.753679  5452 net.cpp:122] Setting up fire6/squeeze1x1
I1107 16:43:08.753679  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.753679  5452 net.cpp:137] Memory required for data: 791777200
I1107 16:43:08.753679  5452 layer_factory.cpp:58] Creating layer bn_12
I1107 16:43:08.753679  5452 net.cpp:84] Creating Layer bn_12
I1107 16:43:08.753679  5452 net.cpp:406] bn_12 <- conv_12
I1107 16:43:08.753679  5452 net.cpp:367] bn_12 -> conv_12 (in-place)
I1107 16:43:08.754679  5452 net.cpp:122] Setting up bn_12
I1107 16:43:08.754679  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.754679  5452 net.cpp:137] Memory required for data: 792718000
I1107 16:43:08.754679  5452 layer_factory.cpp:58] Creating layer scale_12
I1107 16:43:08.754679  5452 net.cpp:84] Creating Layer scale_12
I1107 16:43:08.754679  5452 net.cpp:406] scale_12 <- conv_12
I1107 16:43:08.754679  5452 net.cpp:380] scale_12 -> fire6/squeeze1x1
I1107 16:43:08.754679  5452 layer_factory.cpp:58] Creating layer scale_12
I1107 16:43:08.754679  5452 net.cpp:122] Setting up scale_12
I1107 16:43:08.754679  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.754679  5452 net.cpp:137] Memory required for data: 793658800
I1107 16:43:08.754679  5452 layer_factory.cpp:58] Creating layer fire6/relu_squeeze1x1
I1107 16:43:08.754679  5452 net.cpp:84] Creating Layer fire6/relu_squeeze1x1
I1107 16:43:08.754679  5452 net.cpp:406] fire6/relu_squeeze1x1 <- fire6/squeeze1x1
I1107 16:43:08.754679  5452 net.cpp:367] fire6/relu_squeeze1x1 -> fire6/squeeze1x1 (in-place)
I1107 16:43:08.754679  5452 net.cpp:122] Setting up fire6/relu_squeeze1x1
I1107 16:43:08.754679  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.754679  5452 net.cpp:137] Memory required for data: 794599600
I1107 16:43:08.754679  5452 layer_factory.cpp:58] Creating layer fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 16:43:08.754679  5452 net.cpp:84] Creating Layer fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 16:43:08.754679  5452 net.cpp:406] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split <- fire6/squeeze1x1
I1107 16:43:08.754679  5452 net.cpp:380] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split -> fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_0
I1107 16:43:08.754679  5452 net.cpp:380] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split -> fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_1
I1107 16:43:08.754679  5452 net.cpp:122] Setting up fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split
I1107 16:43:08.754679  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.754679  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.754679  5452 net.cpp:137] Memory required for data: 796481200
I1107 16:43:08.754679  5452 layer_factory.cpp:58] Creating layer fire6/expand1x1
I1107 16:43:08.754679  5452 net.cpp:84] Creating Layer fire6/expand1x1
I1107 16:43:08.754679  5452 net.cpp:406] fire6/expand1x1 <- fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_0
I1107 16:43:08.754679  5452 net.cpp:380] fire6/expand1x1 -> conv_13
I1107 16:43:08.756680  5452 net.cpp:122] Setting up fire6/expand1x1
I1107 16:43:08.756680  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.756680  5452 net.cpp:137] Memory required for data: 800244400
I1107 16:43:08.756680  5452 layer_factory.cpp:58] Creating layer bn_13
I1107 16:43:08.756680  5452 net.cpp:84] Creating Layer bn_13
I1107 16:43:08.756680  5452 net.cpp:406] bn_13 <- conv_13
I1107 16:43:08.756680  5452 net.cpp:367] bn_13 -> conv_13 (in-place)
I1107 16:43:08.756680  5452 net.cpp:122] Setting up bn_13
I1107 16:43:08.756680  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.756680  5452 net.cpp:137] Memory required for data: 804007600
I1107 16:43:08.756680  5452 layer_factory.cpp:58] Creating layer scale_13
I1107 16:43:08.756680  5452 net.cpp:84] Creating Layer scale_13
I1107 16:43:08.756680  5452 net.cpp:406] scale_13 <- conv_13
I1107 16:43:08.756680  5452 net.cpp:380] scale_13 -> fire6/expand1x1
I1107 16:43:08.756680  5452 layer_factory.cpp:58] Creating layer scale_13
I1107 16:43:08.756680  5452 net.cpp:122] Setting up scale_13
I1107 16:43:08.756680  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.756680  5452 net.cpp:137] Memory required for data: 807770800
I1107 16:43:08.756680  5452 layer_factory.cpp:58] Creating layer fire6/relu_expand1x1
I1107 16:43:08.756680  5452 net.cpp:84] Creating Layer fire6/relu_expand1x1
I1107 16:43:08.756680  5452 net.cpp:406] fire6/relu_expand1x1 <- fire6/expand1x1
I1107 16:43:08.756680  5452 net.cpp:367] fire6/relu_expand1x1 -> fire6/expand1x1 (in-place)
I1107 16:43:08.757678  5452 net.cpp:122] Setting up fire6/relu_expand1x1
I1107 16:43:08.757678  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.757678  5452 net.cpp:137] Memory required for data: 811534000
I1107 16:43:08.757678  5452 layer_factory.cpp:58] Creating layer fire6/expand3x3
I1107 16:43:08.757678  5452 net.cpp:84] Creating Layer fire6/expand3x3
I1107 16:43:08.757678  5452 net.cpp:406] fire6/expand3x3 <- fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split_1
I1107 16:43:08.757678  5452 net.cpp:380] fire6/expand3x3 -> conv_14
I1107 16:43:08.759680  5452 net.cpp:122] Setting up fire6/expand3x3
I1107 16:43:08.759680  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.759680  5452 net.cpp:137] Memory required for data: 815297200
I1107 16:43:08.759680  5452 layer_factory.cpp:58] Creating layer bn_14
I1107 16:43:08.759680  5452 net.cpp:84] Creating Layer bn_14
I1107 16:43:08.759680  5452 net.cpp:406] bn_14 <- conv_14
I1107 16:43:08.759680  5452 net.cpp:367] bn_14 -> conv_14 (in-place)
I1107 16:43:08.759680  5452 net.cpp:122] Setting up bn_14
I1107 16:43:08.759680  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.759680  5452 net.cpp:137] Memory required for data: 819060400
I1107 16:43:08.759680  5452 layer_factory.cpp:58] Creating layer scale_14
I1107 16:43:08.759680  5452 net.cpp:84] Creating Layer scale_14
I1107 16:43:08.759680  5452 net.cpp:406] scale_14 <- conv_14
I1107 16:43:08.759680  5452 net.cpp:380] scale_14 -> fire6/expand3x3
I1107 16:43:08.760676  5452 layer_factory.cpp:58] Creating layer scale_14
I1107 16:43:08.760676  5452 net.cpp:122] Setting up scale_14
I1107 16:43:08.760676  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.760676  5452 net.cpp:137] Memory required for data: 822823600
I1107 16:43:08.760676  5452 layer_factory.cpp:58] Creating layer fire6/relu_expand3x3
I1107 16:43:08.760676  5452 net.cpp:84] Creating Layer fire6/relu_expand3x3
I1107 16:43:08.760676  5452 net.cpp:406] fire6/relu_expand3x3 <- fire6/expand3x3
I1107 16:43:08.760676  5452 net.cpp:367] fire6/relu_expand3x3 -> fire6/expand3x3 (in-place)
I1107 16:43:08.760676  5452 net.cpp:122] Setting up fire6/relu_expand3x3
I1107 16:43:08.760676  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.760676  5452 net.cpp:137] Memory required for data: 826586800
I1107 16:43:08.760676  5452 layer_factory.cpp:58] Creating layer fire6/concat
I1107 16:43:08.760676  5452 net.cpp:84] Creating Layer fire6/concat
I1107 16:43:08.760676  5452 net.cpp:406] fire6/concat <- fire6/expand1x1
I1107 16:43:08.760676  5452 net.cpp:406] fire6/concat <- fire6/expand3x3
I1107 16:43:08.760676  5452 net.cpp:380] fire6/concat -> fire6/concat
I1107 16:43:08.760676  5452 net.cpp:122] Setting up fire6/concat
I1107 16:43:08.760676  5452 net.cpp:129] Top shape: 100 384 7 7 (1881600)
I1107 16:43:08.760676  5452 net.cpp:137] Memory required for data: 834113200
I1107 16:43:08.760676  5452 layer_factory.cpp:58] Creating layer fire7/squeeze1x1
I1107 16:43:08.760676  5452 net.cpp:84] Creating Layer fire7/squeeze1x1
I1107 16:43:08.760676  5452 net.cpp:406] fire7/squeeze1x1 <- fire6/concat
I1107 16:43:08.760676  5452 net.cpp:380] fire7/squeeze1x1 -> conv_15
I1107 16:43:08.762677  5452 net.cpp:122] Setting up fire7/squeeze1x1
I1107 16:43:08.762677  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.762677  5452 net.cpp:137] Memory required for data: 835054000
I1107 16:43:08.762677  5452 layer_factory.cpp:58] Creating layer bn_15
I1107 16:43:08.762677  5452 net.cpp:84] Creating Layer bn_15
I1107 16:43:08.762677  5452 net.cpp:406] bn_15 <- conv_15
I1107 16:43:08.762677  5452 net.cpp:367] bn_15 -> conv_15 (in-place)
I1107 16:43:08.762677  5452 net.cpp:122] Setting up bn_15
I1107 16:43:08.762677  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.762677  5452 net.cpp:137] Memory required for data: 835994800
I1107 16:43:08.762677  5452 layer_factory.cpp:58] Creating layer scale_15
I1107 16:43:08.762677  5452 net.cpp:84] Creating Layer scale_15
I1107 16:43:08.762677  5452 net.cpp:406] scale_15 <- conv_15
I1107 16:43:08.762677  5452 net.cpp:380] scale_15 -> fire7/squeeze1x1
I1107 16:43:08.762677  5452 layer_factory.cpp:58] Creating layer scale_15
I1107 16:43:08.762677  5452 net.cpp:122] Setting up scale_15
I1107 16:43:08.762677  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.762677  5452 net.cpp:137] Memory required for data: 836935600
I1107 16:43:08.762677  5452 layer_factory.cpp:58] Creating layer fire7/relu_squeeze1x1
I1107 16:43:08.762677  5452 net.cpp:84] Creating Layer fire7/relu_squeeze1x1
I1107 16:43:08.762677  5452 net.cpp:406] fire7/relu_squeeze1x1 <- fire7/squeeze1x1
I1107 16:43:08.762677  5452 net.cpp:367] fire7/relu_squeeze1x1 -> fire7/squeeze1x1 (in-place)
I1107 16:43:08.762677  5452 net.cpp:122] Setting up fire7/relu_squeeze1x1
I1107 16:43:08.762677  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.762677  5452 net.cpp:137] Memory required for data: 837876400
I1107 16:43:08.762677  5452 layer_factory.cpp:58] Creating layer fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 16:43:08.762677  5452 net.cpp:84] Creating Layer fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 16:43:08.762677  5452 net.cpp:406] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split <- fire7/squeeze1x1
I1107 16:43:08.762677  5452 net.cpp:380] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split -> fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_0
I1107 16:43:08.762677  5452 net.cpp:380] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split -> fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_1
I1107 16:43:08.762677  5452 net.cpp:122] Setting up fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split
I1107 16:43:08.762677  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.762677  5452 net.cpp:129] Top shape: 100 48 7 7 (235200)
I1107 16:43:08.762677  5452 net.cpp:137] Memory required for data: 839758000
I1107 16:43:08.762677  5452 layer_factory.cpp:58] Creating layer fire7/expand1x1
I1107 16:43:08.762677  5452 net.cpp:84] Creating Layer fire7/expand1x1
I1107 16:43:08.762677  5452 net.cpp:406] fire7/expand1x1 <- fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_0
I1107 16:43:08.762677  5452 net.cpp:380] fire7/expand1x1 -> conv_16
I1107 16:43:08.764678  5452 net.cpp:122] Setting up fire7/expand1x1
I1107 16:43:08.764678  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.764678  5452 net.cpp:137] Memory required for data: 843521200
I1107 16:43:08.764678  5452 layer_factory.cpp:58] Creating layer bn_16
I1107 16:43:08.764678  5452 net.cpp:84] Creating Layer bn_16
I1107 16:43:08.764678  5452 net.cpp:406] bn_16 <- conv_16
I1107 16:43:08.764678  5452 net.cpp:367] bn_16 -> conv_16 (in-place)
I1107 16:43:08.764678  5452 net.cpp:122] Setting up bn_16
I1107 16:43:08.764678  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.764678  5452 net.cpp:137] Memory required for data: 847284400
I1107 16:43:08.764678  5452 layer_factory.cpp:58] Creating layer scale_16
I1107 16:43:08.764678  5452 net.cpp:84] Creating Layer scale_16
I1107 16:43:08.764678  5452 net.cpp:406] scale_16 <- conv_16
I1107 16:43:08.764678  5452 net.cpp:380] scale_16 -> fire7/expand1x1
I1107 16:43:08.764678  5452 layer_factory.cpp:58] Creating layer scale_16
I1107 16:43:08.764678  5452 net.cpp:122] Setting up scale_16
I1107 16:43:08.764678  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.764678  5452 net.cpp:137] Memory required for data: 851047600
I1107 16:43:08.764678  5452 layer_factory.cpp:58] Creating layer fire7/relu_expand1x1
I1107 16:43:08.764678  5452 net.cpp:84] Creating Layer fire7/relu_expand1x1
I1107 16:43:08.764678  5452 net.cpp:406] fire7/relu_expand1x1 <- fire7/expand1x1
I1107 16:43:08.764678  5452 net.cpp:367] fire7/relu_expand1x1 -> fire7/expand1x1 (in-place)
I1107 16:43:08.764678  5452 net.cpp:122] Setting up fire7/relu_expand1x1
I1107 16:43:08.764678  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.764678  5452 net.cpp:137] Memory required for data: 854810800
I1107 16:43:08.764678  5452 layer_factory.cpp:58] Creating layer fire7/expand3x3
I1107 16:43:08.764678  5452 net.cpp:84] Creating Layer fire7/expand3x3
I1107 16:43:08.764678  5452 net.cpp:406] fire7/expand3x3 <- fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split_1
I1107 16:43:08.764678  5452 net.cpp:380] fire7/expand3x3 -> conv_17
I1107 16:43:08.767678  5452 net.cpp:122] Setting up fire7/expand3x3
I1107 16:43:08.767678  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.767678  5452 net.cpp:137] Memory required for data: 858574000
I1107 16:43:08.767678  5452 layer_factory.cpp:58] Creating layer bn_17
I1107 16:43:08.767678  5452 net.cpp:84] Creating Layer bn_17
I1107 16:43:08.767678  5452 net.cpp:406] bn_17 <- conv_17
I1107 16:43:08.767678  5452 net.cpp:367] bn_17 -> conv_17 (in-place)
I1107 16:43:08.767678  5452 net.cpp:122] Setting up bn_17
I1107 16:43:08.767678  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.767678  5452 net.cpp:137] Memory required for data: 862337200
I1107 16:43:08.767678  5452 layer_factory.cpp:58] Creating layer scale_17
I1107 16:43:08.767678  5452 net.cpp:84] Creating Layer scale_17
I1107 16:43:08.767678  5452 net.cpp:406] scale_17 <- conv_17
I1107 16:43:08.767678  5452 net.cpp:380] scale_17 -> fire7/expand3x3
I1107 16:43:08.767678  5452 layer_factory.cpp:58] Creating layer scale_17
I1107 16:43:08.767678  5452 net.cpp:122] Setting up scale_17
I1107 16:43:08.767678  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.767678  5452 net.cpp:137] Memory required for data: 866100400
I1107 16:43:08.767678  5452 layer_factory.cpp:58] Creating layer fire7/relu_expand3x3
I1107 16:43:08.767678  5452 net.cpp:84] Creating Layer fire7/relu_expand3x3
I1107 16:43:08.767678  5452 net.cpp:406] fire7/relu_expand3x3 <- fire7/expand3x3
I1107 16:43:08.767678  5452 net.cpp:367] fire7/relu_expand3x3 -> fire7/expand3x3 (in-place)
I1107 16:43:08.768676  5452 net.cpp:122] Setting up fire7/relu_expand3x3
I1107 16:43:08.768676  5452 net.cpp:129] Top shape: 100 192 7 7 (940800)
I1107 16:43:08.768676  5452 net.cpp:137] Memory required for data: 869863600
I1107 16:43:08.768676  5452 layer_factory.cpp:58] Creating layer fire7/concat
I1107 16:43:08.768676  5452 net.cpp:84] Creating Layer fire7/concat
I1107 16:43:08.768676  5452 net.cpp:406] fire7/concat <- fire7/expand1x1
I1107 16:43:08.768676  5452 net.cpp:406] fire7/concat <- fire7/expand3x3
I1107 16:43:08.768676  5452 net.cpp:380] fire7/concat -> fire7/concat
I1107 16:43:08.768676  5452 net.cpp:122] Setting up fire7/concat
I1107 16:43:08.768676  5452 net.cpp:129] Top shape: 100 384 7 7 (1881600)
I1107 16:43:08.768676  5452 net.cpp:137] Memory required for data: 877390000
I1107 16:43:08.768676  5452 layer_factory.cpp:58] Creating layer fire8/squeeze1x1
I1107 16:43:08.768676  5452 net.cpp:84] Creating Layer fire8/squeeze1x1
I1107 16:43:08.768676  5452 net.cpp:406] fire8/squeeze1x1 <- fire7/concat
I1107 16:43:08.768676  5452 net.cpp:380] fire8/squeeze1x1 -> conv_18
I1107 16:43:08.769678  5452 net.cpp:122] Setting up fire8/squeeze1x1
I1107 16:43:08.769678  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.769678  5452 net.cpp:137] Memory required for data: 878644400
I1107 16:43:08.769678  5452 layer_factory.cpp:58] Creating layer bn_18
I1107 16:43:08.769678  5452 net.cpp:84] Creating Layer bn_18
I1107 16:43:08.769678  5452 net.cpp:406] bn_18 <- conv_18
I1107 16:43:08.769678  5452 net.cpp:367] bn_18 -> conv_18 (in-place)
I1107 16:43:08.769678  5452 net.cpp:122] Setting up bn_18
I1107 16:43:08.769678  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.769678  5452 net.cpp:137] Memory required for data: 879898800
I1107 16:43:08.769678  5452 layer_factory.cpp:58] Creating layer scale_18
I1107 16:43:08.769678  5452 net.cpp:84] Creating Layer scale_18
I1107 16:43:08.769678  5452 net.cpp:406] scale_18 <- conv_18
I1107 16:43:08.769678  5452 net.cpp:380] scale_18 -> fire8/squeeze1x1
I1107 16:43:08.769678  5452 layer_factory.cpp:58] Creating layer scale_18
I1107 16:43:08.769678  5452 net.cpp:122] Setting up scale_18
I1107 16:43:08.769678  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.769678  5452 net.cpp:137] Memory required for data: 881153200
I1107 16:43:08.769678  5452 layer_factory.cpp:58] Creating layer fire8/relu_squeeze1x1
I1107 16:43:08.769678  5452 net.cpp:84] Creating Layer fire8/relu_squeeze1x1
I1107 16:43:08.769678  5452 net.cpp:406] fire8/relu_squeeze1x1 <- fire8/squeeze1x1
I1107 16:43:08.769678  5452 net.cpp:367] fire8/relu_squeeze1x1 -> fire8/squeeze1x1 (in-place)
I1107 16:43:08.770678  5452 net.cpp:122] Setting up fire8/relu_squeeze1x1
I1107 16:43:08.770678  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.770678  5452 net.cpp:137] Memory required for data: 882407600
I1107 16:43:08.770678  5452 layer_factory.cpp:58] Creating layer fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 16:43:08.770678  5452 net.cpp:84] Creating Layer fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 16:43:08.770678  5452 net.cpp:406] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split <- fire8/squeeze1x1
I1107 16:43:08.770678  5452 net.cpp:380] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split -> fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_0
I1107 16:43:08.770678  5452 net.cpp:380] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split -> fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_1
I1107 16:43:08.770678  5452 net.cpp:122] Setting up fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split
I1107 16:43:08.770678  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.770678  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.770678  5452 net.cpp:137] Memory required for data: 884916400
I1107 16:43:08.770678  5452 layer_factory.cpp:58] Creating layer fire8/expand1x1
I1107 16:43:08.770678  5452 net.cpp:84] Creating Layer fire8/expand1x1
I1107 16:43:08.770678  5452 net.cpp:406] fire8/expand1x1 <- fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_0
I1107 16:43:08.770678  5452 net.cpp:380] fire8/expand1x1 -> conv_19
I1107 16:43:08.771678  5452 net.cpp:122] Setting up fire8/expand1x1
I1107 16:43:08.771678  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.771678  5452 net.cpp:137] Memory required for data: 889934000
I1107 16:43:08.771678  5452 layer_factory.cpp:58] Creating layer bn_19
I1107 16:43:08.771678  5452 net.cpp:84] Creating Layer bn_19
I1107 16:43:08.771678  5452 net.cpp:406] bn_19 <- conv_19
I1107 16:43:08.771678  5452 net.cpp:367] bn_19 -> conv_19 (in-place)
I1107 16:43:08.771678  5452 net.cpp:122] Setting up bn_19
I1107 16:43:08.771678  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.771678  5452 net.cpp:137] Memory required for data: 894951600
I1107 16:43:08.771678  5452 layer_factory.cpp:58] Creating layer scale_19
I1107 16:43:08.771678  5452 net.cpp:84] Creating Layer scale_19
I1107 16:43:08.771678  5452 net.cpp:406] scale_19 <- conv_19
I1107 16:43:08.771678  5452 net.cpp:380] scale_19 -> fire8/expand1x1
I1107 16:43:08.771678  5452 layer_factory.cpp:58] Creating layer scale_19
I1107 16:43:08.771678  5452 net.cpp:122] Setting up scale_19
I1107 16:43:08.771678  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.771678  5452 net.cpp:137] Memory required for data: 899969200
I1107 16:43:08.771678  5452 layer_factory.cpp:58] Creating layer fire8/relu_expand1x1
I1107 16:43:08.772678  5452 net.cpp:84] Creating Layer fire8/relu_expand1x1
I1107 16:43:08.772678  5452 net.cpp:406] fire8/relu_expand1x1 <- fire8/expand1x1
I1107 16:43:08.772678  5452 net.cpp:367] fire8/relu_expand1x1 -> fire8/expand1x1 (in-place)
I1107 16:43:08.772678  5452 net.cpp:122] Setting up fire8/relu_expand1x1
I1107 16:43:08.772678  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.772678  5452 net.cpp:137] Memory required for data: 904986800
I1107 16:43:08.772678  5452 layer_factory.cpp:58] Creating layer fire8/expand3x3
I1107 16:43:08.772678  5452 net.cpp:84] Creating Layer fire8/expand3x3
I1107 16:43:08.772678  5452 net.cpp:406] fire8/expand3x3 <- fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split_1
I1107 16:43:08.772678  5452 net.cpp:380] fire8/expand3x3 -> conv_20
I1107 16:43:08.775683  5452 net.cpp:122] Setting up fire8/expand3x3
I1107 16:43:08.775683  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.775683  5452 net.cpp:137] Memory required for data: 910004400
I1107 16:43:08.775683  5452 layer_factory.cpp:58] Creating layer bn_20
I1107 16:43:08.775683  5452 net.cpp:84] Creating Layer bn_20
I1107 16:43:08.775683  5452 net.cpp:406] bn_20 <- conv_20
I1107 16:43:08.775683  5452 net.cpp:367] bn_20 -> conv_20 (in-place)
I1107 16:43:08.775683  5452 net.cpp:122] Setting up bn_20
I1107 16:43:08.775683  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.775683  5452 net.cpp:137] Memory required for data: 915022000
I1107 16:43:08.775683  5452 layer_factory.cpp:58] Creating layer scale_20
I1107 16:43:08.775683  5452 net.cpp:84] Creating Layer scale_20
I1107 16:43:08.775683  5452 net.cpp:406] scale_20 <- conv_20
I1107 16:43:08.775683  5452 net.cpp:380] scale_20 -> fire8/expand3x3
I1107 16:43:08.775683  5452 layer_factory.cpp:58] Creating layer scale_20
I1107 16:43:08.775683  5452 net.cpp:122] Setting up scale_20
I1107 16:43:08.775683  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.775683  5452 net.cpp:137] Memory required for data: 920039600
I1107 16:43:08.775683  5452 layer_factory.cpp:58] Creating layer fire8/relu_expand3x3
I1107 16:43:08.775683  5452 net.cpp:84] Creating Layer fire8/relu_expand3x3
I1107 16:43:08.775683  5452 net.cpp:406] fire8/relu_expand3x3 <- fire8/expand3x3
I1107 16:43:08.775683  5452 net.cpp:367] fire8/relu_expand3x3 -> fire8/expand3x3 (in-place)
I1107 16:43:08.776679  5452 net.cpp:122] Setting up fire8/relu_expand3x3
I1107 16:43:08.776679  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.776679  5452 net.cpp:137] Memory required for data: 925057200
I1107 16:43:08.776679  5452 layer_factory.cpp:58] Creating layer fire8/concat
I1107 16:43:08.776679  5452 net.cpp:84] Creating Layer fire8/concat
I1107 16:43:08.776679  5452 net.cpp:406] fire8/concat <- fire8/expand1x1
I1107 16:43:08.776679  5452 net.cpp:406] fire8/concat <- fire8/expand3x3
I1107 16:43:08.776679  5452 net.cpp:380] fire8/concat -> fire8/concat
I1107 16:43:08.776679  5452 net.cpp:122] Setting up fire8/concat
I1107 16:43:08.776679  5452 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 16:43:08.776679  5452 net.cpp:137] Memory required for data: 935092400
I1107 16:43:08.776679  5452 layer_factory.cpp:58] Creating layer fire9/squeeze1x1
I1107 16:43:08.776679  5452 net.cpp:84] Creating Layer fire9/squeeze1x1
I1107 16:43:08.776679  5452 net.cpp:406] fire9/squeeze1x1 <- fire8/concat
I1107 16:43:08.776679  5452 net.cpp:380] fire9/squeeze1x1 -> conv_21
I1107 16:43:08.777678  5452 net.cpp:122] Setting up fire9/squeeze1x1
I1107 16:43:08.777678  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.777678  5452 net.cpp:137] Memory required for data: 936346800
I1107 16:43:08.777678  5452 layer_factory.cpp:58] Creating layer bn_21
I1107 16:43:08.777678  5452 net.cpp:84] Creating Layer bn_21
I1107 16:43:08.777678  5452 net.cpp:406] bn_21 <- conv_21
I1107 16:43:08.777678  5452 net.cpp:367] bn_21 -> conv_21 (in-place)
I1107 16:43:08.778676  5452 net.cpp:122] Setting up bn_21
I1107 16:43:08.778676  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.778676  5452 net.cpp:137] Memory required for data: 937601200
I1107 16:43:08.778676  5452 layer_factory.cpp:58] Creating layer scale_21
I1107 16:43:08.778676  5452 net.cpp:84] Creating Layer scale_21
I1107 16:43:08.778676  5452 net.cpp:406] scale_21 <- conv_21
I1107 16:43:08.778676  5452 net.cpp:380] scale_21 -> fire9/squeeze1x1
I1107 16:43:08.778676  5452 layer_factory.cpp:58] Creating layer scale_21
I1107 16:43:08.778676  5452 net.cpp:122] Setting up scale_21
I1107 16:43:08.778676  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.778676  5452 net.cpp:137] Memory required for data: 938855600
I1107 16:43:08.778676  5452 layer_factory.cpp:58] Creating layer fire9/relu_squeeze1x1
I1107 16:43:08.778676  5452 net.cpp:84] Creating Layer fire9/relu_squeeze1x1
I1107 16:43:08.778676  5452 net.cpp:406] fire9/relu_squeeze1x1 <- fire9/squeeze1x1
I1107 16:43:08.778676  5452 net.cpp:367] fire9/relu_squeeze1x1 -> fire9/squeeze1x1 (in-place)
I1107 16:43:08.778676  5452 net.cpp:122] Setting up fire9/relu_squeeze1x1
I1107 16:43:08.778676  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.778676  5452 net.cpp:137] Memory required for data: 940110000
I1107 16:43:08.778676  5452 layer_factory.cpp:58] Creating layer fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 16:43:08.778676  5452 net.cpp:84] Creating Layer fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 16:43:08.778676  5452 net.cpp:406] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split <- fire9/squeeze1x1
I1107 16:43:08.778676  5452 net.cpp:380] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split -> fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_0
I1107 16:43:08.778676  5452 net.cpp:380] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split -> fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_1
I1107 16:43:08.778676  5452 net.cpp:122] Setting up fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split
I1107 16:43:08.778676  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.778676  5452 net.cpp:129] Top shape: 100 64 7 7 (313600)
I1107 16:43:08.778676  5452 net.cpp:137] Memory required for data: 942618800
I1107 16:43:08.778676  5452 layer_factory.cpp:58] Creating layer fire9/expand1x1
I1107 16:43:08.778676  5452 net.cpp:84] Creating Layer fire9/expand1x1
I1107 16:43:08.778676  5452 net.cpp:406] fire9/expand1x1 <- fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_0
I1107 16:43:08.778676  5452 net.cpp:380] fire9/expand1x1 -> conv_22
I1107 16:43:08.780679  5452 net.cpp:122] Setting up fire9/expand1x1
I1107 16:43:08.780679  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.780679  5452 net.cpp:137] Memory required for data: 947636400
I1107 16:43:08.780679  5452 layer_factory.cpp:58] Creating layer bn_22
I1107 16:43:08.780679  5452 net.cpp:84] Creating Layer bn_22
I1107 16:43:08.780679  5452 net.cpp:406] bn_22 <- conv_22
I1107 16:43:08.780679  5452 net.cpp:367] bn_22 -> conv_22 (in-place)
I1107 16:43:08.780679  5452 net.cpp:122] Setting up bn_22
I1107 16:43:08.780679  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.780679  5452 net.cpp:137] Memory required for data: 952654000
I1107 16:43:08.780679  5452 layer_factory.cpp:58] Creating layer scale_22
I1107 16:43:08.780679  5452 net.cpp:84] Creating Layer scale_22
I1107 16:43:08.780679  5452 net.cpp:406] scale_22 <- conv_22
I1107 16:43:08.780679  5452 net.cpp:380] scale_22 -> fire9/expand1x1
I1107 16:43:08.780679  5452 layer_factory.cpp:58] Creating layer scale_22
I1107 16:43:08.781682  5452 net.cpp:122] Setting up scale_22
I1107 16:43:08.781682  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.781682  5452 net.cpp:137] Memory required for data: 957671600
I1107 16:43:08.781682  5452 layer_factory.cpp:58] Creating layer fire9/relu_expand1x1
I1107 16:43:08.781682  5452 net.cpp:84] Creating Layer fire9/relu_expand1x1
I1107 16:43:08.781682  5452 net.cpp:406] fire9/relu_expand1x1 <- fire9/expand1x1
I1107 16:43:08.781682  5452 net.cpp:367] fire9/relu_expand1x1 -> fire9/expand1x1 (in-place)
I1107 16:43:08.781682  5452 net.cpp:122] Setting up fire9/relu_expand1x1
I1107 16:43:08.781682  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.781682  5452 net.cpp:137] Memory required for data: 962689200
I1107 16:43:08.781682  5452 layer_factory.cpp:58] Creating layer fire9/expand3x3
I1107 16:43:08.781682  5452 net.cpp:84] Creating Layer fire9/expand3x3
I1107 16:43:08.781682  5452 net.cpp:406] fire9/expand3x3 <- fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split_1
I1107 16:43:08.781682  5452 net.cpp:380] fire9/expand3x3 -> conv_23
I1107 16:43:08.783676  5452 net.cpp:122] Setting up fire9/expand3x3
I1107 16:43:08.783676  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.783676  5452 net.cpp:137] Memory required for data: 967706800
I1107 16:43:08.783676  5452 layer_factory.cpp:58] Creating layer bn_23
I1107 16:43:08.783676  5452 net.cpp:84] Creating Layer bn_23
I1107 16:43:08.783676  5452 net.cpp:406] bn_23 <- conv_23
I1107 16:43:08.783676  5452 net.cpp:367] bn_23 -> conv_23 (in-place)
I1107 16:43:08.784678  5452 net.cpp:122] Setting up bn_23
I1107 16:43:08.784678  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.784678  5452 net.cpp:137] Memory required for data: 972724400
I1107 16:43:08.784678  5452 layer_factory.cpp:58] Creating layer scale_23
I1107 16:43:08.784678  5452 net.cpp:84] Creating Layer scale_23
I1107 16:43:08.784678  5452 net.cpp:406] scale_23 <- conv_23
I1107 16:43:08.784678  5452 net.cpp:380] scale_23 -> fire9/expand3x3
I1107 16:43:08.784678  5452 layer_factory.cpp:58] Creating layer scale_23
I1107 16:43:08.784678  5452 net.cpp:122] Setting up scale_23
I1107 16:43:08.784678  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.784678  5452 net.cpp:137] Memory required for data: 977742000
I1107 16:43:08.784678  5452 layer_factory.cpp:58] Creating layer fire9/relu_expand3x3
I1107 16:43:08.784678  5452 net.cpp:84] Creating Layer fire9/relu_expand3x3
I1107 16:43:08.784678  5452 net.cpp:406] fire9/relu_expand3x3 <- fire9/expand3x3
I1107 16:43:08.784678  5452 net.cpp:367] fire9/relu_expand3x3 -> fire9/expand3x3 (in-place)
I1107 16:43:08.784678  5452 net.cpp:122] Setting up fire9/relu_expand3x3
I1107 16:43:08.784678  5452 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1107 16:43:08.784678  5452 net.cpp:137] Memory required for data: 982759600
I1107 16:43:08.784678  5452 layer_factory.cpp:58] Creating layer fire9/concat
I1107 16:43:08.784678  5452 net.cpp:84] Creating Layer fire9/concat
I1107 16:43:08.784678  5452 net.cpp:406] fire9/concat <- fire9/expand1x1
I1107 16:43:08.784678  5452 net.cpp:406] fire9/concat <- fire9/expand3x3
I1107 16:43:08.784678  5452 net.cpp:380] fire9/concat -> fire9/concat
I1107 16:43:08.784678  5452 net.cpp:122] Setting up fire9/concat
I1107 16:43:08.784678  5452 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 16:43:08.784678  5452 net.cpp:137] Memory required for data: 992794800
I1107 16:43:08.784678  5452 layer_factory.cpp:58] Creating layer drop9
I1107 16:43:08.784678  5452 net.cpp:84] Creating Layer drop9
I1107 16:43:08.784678  5452 net.cpp:406] drop9 <- fire9/concat
I1107 16:43:08.784678  5452 net.cpp:367] drop9 -> fire9/concat (in-place)
I1107 16:43:08.784678  5452 net.cpp:122] Setting up drop9
I1107 16:43:08.784678  5452 net.cpp:129] Top shape: 100 512 7 7 (2508800)
I1107 16:43:08.784678  5452 net.cpp:137] Memory required for data: 1002830000
I1107 16:43:08.784678  5452 layer_factory.cpp:58] Creating layer conv10
I1107 16:43:08.784678  5452 net.cpp:84] Creating Layer conv10
I1107 16:43:08.784678  5452 net.cpp:406] conv10 <- fire9/concat
I1107 16:43:08.784678  5452 net.cpp:380] conv10 -> conv10
I1107 16:43:08.786684  5452 net.cpp:122] Setting up conv10
I1107 16:43:08.786684  5452 net.cpp:129] Top shape: 100 10 7 7 (49000)
I1107 16:43:08.786684  5452 net.cpp:137] Memory required for data: 1003026000
I1107 16:43:08.786684  5452 layer_factory.cpp:58] Creating layer relu_conv10
I1107 16:43:08.786684  5452 net.cpp:84] Creating Layer relu_conv10
I1107 16:43:08.786684  5452 net.cpp:406] relu_conv10 <- conv10
I1107 16:43:08.786684  5452 net.cpp:367] relu_conv10 -> conv10 (in-place)
I1107 16:43:08.786684  5452 net.cpp:122] Setting up relu_conv10
I1107 16:43:08.786684  5452 net.cpp:129] Top shape: 100 10 7 7 (49000)
I1107 16:43:08.786684  5452 net.cpp:137] Memory required for data: 1003222000
I1107 16:43:08.786684  5452 layer_factory.cpp:58] Creating layer pool10
I1107 16:43:08.786684  5452 net.cpp:84] Creating Layer pool10
I1107 16:43:08.786684  5452 net.cpp:406] pool10 <- conv10
I1107 16:43:08.786684  5452 net.cpp:380] pool10 -> pool10
I1107 16:43:08.787678  5452 net.cpp:122] Setting up pool10
I1107 16:43:08.787678  5452 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 16:43:08.787678  5452 net.cpp:137] Memory required for data: 1003226000
I1107 16:43:08.787678  5452 layer_factory.cpp:58] Creating layer pool10_pool10_0_split
I1107 16:43:08.787678  5452 net.cpp:84] Creating Layer pool10_pool10_0_split
I1107 16:43:08.787678  5452 net.cpp:406] pool10_pool10_0_split <- pool10
I1107 16:43:08.787678  5452 net.cpp:380] pool10_pool10_0_split -> pool10_pool10_0_split_0
I1107 16:43:08.787678  5452 net.cpp:380] pool10_pool10_0_split -> pool10_pool10_0_split_1
I1107 16:43:08.787678  5452 net.cpp:122] Setting up pool10_pool10_0_split
I1107 16:43:08.787678  5452 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 16:43:08.787678  5452 net.cpp:129] Top shape: 100 10 1 1 (1000)
I1107 16:43:08.787678  5452 net.cpp:137] Memory required for data: 1003234000
I1107 16:43:08.787678  5452 layer_factory.cpp:58] Creating layer accuracy
I1107 16:43:08.787678  5452 net.cpp:84] Creating Layer accuracy
I1107 16:43:08.787678  5452 net.cpp:406] accuracy <- pool10_pool10_0_split_0
I1107 16:43:08.787678  5452 net.cpp:406] accuracy <- label_cifar_1_split_0
I1107 16:43:08.787678  5452 net.cpp:380] accuracy -> accuracy
I1107 16:43:08.787678  5452 net.cpp:122] Setting up accuracy
I1107 16:43:08.787678  5452 net.cpp:129] Top shape: (1)
I1107 16:43:08.787678  5452 net.cpp:137] Memory required for data: 1003234004
I1107 16:43:08.787678  5452 layer_factory.cpp:58] Creating layer loss
I1107 16:43:08.787678  5452 net.cpp:84] Creating Layer loss
I1107 16:43:08.787678  5452 net.cpp:406] loss <- pool10_pool10_0_split_1
I1107 16:43:08.787678  5452 net.cpp:406] loss <- label_cifar_1_split_1
I1107 16:43:08.787678  5452 net.cpp:380] loss -> loss
I1107 16:43:08.787678  5452 layer_factory.cpp:58] Creating layer loss
I1107 16:43:08.787678  5452 net.cpp:122] Setting up loss
I1107 16:43:08.787678  5452 net.cpp:129] Top shape: (1)
I1107 16:43:08.787678  5452 net.cpp:132]     with loss weight 1
I1107 16:43:08.787678  5452 net.cpp:137] Memory required for data: 1003234008
I1107 16:43:08.787678  5452 net.cpp:198] loss needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:200] accuracy does not need backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] pool10_pool10_0_split needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] pool10 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] relu_conv10 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] conv10 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] drop9 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] fire9/concat needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] fire9/relu_expand3x3 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] scale_23 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] bn_23 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] fire9/expand3x3 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] fire9/relu_expand1x1 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] scale_22 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] bn_22 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] fire9/expand1x1 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] fire9/squeeze1x1_fire9/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] fire9/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] scale_21 needs backward computation.
I1107 16:43:08.787678  5452 net.cpp:198] bn_21 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire9/squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire8/concat needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire8/relu_expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_20 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_20 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire8/expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire8/relu_expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_19 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_19 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire8/expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire8/squeeze1x1_fire8/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire8/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_18 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_18 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire8/squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire7/concat needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire7/relu_expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_17 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_17 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire7/expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire7/relu_expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_16 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_16 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire7/expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire7/squeeze1x1_fire7/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire7/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_15 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_15 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire7/squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire6/concat needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire6/relu_expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_14 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_14 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire6/expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire6/relu_expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_13 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_13 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire6/expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire6/squeeze1x1_fire6/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire6/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_12 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_12 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire6/squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] pool5 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire5/concat needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire5/relu_expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_11 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_11 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire5/expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire5/relu_expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_10 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_10 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire5/expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire5/squeeze1x1_fire5/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire5/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_9 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_9 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire5/squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire4/concat needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire4/relu_expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_8 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_8 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire4/expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire4/relu_expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire4/expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire4/squeeze1x1_fire4/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire4/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_7 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_7 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire4/squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] pool3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire3/concat needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire3/relu_expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_6 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_6 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire3/expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire3/relu_expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_5 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_5 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire3/expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire3/squeeze1x1_fire3/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire3/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_4 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_4 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire3/squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire2/concat needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire2/relu_expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale_3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn_3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire2/expand3x3 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire2/relu_expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale2 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn2 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire2/expand1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire2/squeeze1x1_fire2/relu_squeeze1x1_0_split needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire2/relu_squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] fire2/squeeze1x1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] pool1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] relu_conv1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] scale1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] bn1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:198] conv1 needs backward computation.
I1107 16:43:08.788679  5452 net.cpp:200] label_cifar_1_split does not need backward computation.
I1107 16:43:08.788679  5452 net.cpp:200] cifar does not need backward computation.
I1107 16:43:08.788679  5452 net.cpp:242] This network produces output accuracy
I1107 16:43:08.788679  5452 net.cpp:242] This network produces output loss
I1107 16:43:08.788679  5452 net.cpp:255] Network initialization done.
I1107 16:43:08.789679  5452 solver.cpp:56] Solver scaffolding done.
I1107 16:43:08.797677  5452 caffe.cpp:243] Resuming from examples/cifar10/snaps/squeezenet_batchnorm_iter_90000.solverstate
I1107 16:43:08.807679  5452 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/snaps/squeezenet_batchnorm_iter_90000.caffemodel
I1107 16:43:08.807679  5452 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1107 16:43:08.807679  5452 sgd_solver.cpp:318] SGDSolver: restoring history
I1107 16:43:08.815680  5452 caffe.cpp:249] Starting Optimization
I1107 16:43:08.815680  5452 solver.cpp:272] Solving CIFAR10_Squeezenet_1.1_Batchnorm
I1107 16:43:08.815680  5452 solver.cpp:273] Learning Rate Policy: multistep
I1107 16:43:08.818678  5452 solver.cpp:330] Iteration 90000, Testing net (#0)
I1107 16:43:08.821679  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:43:10.899950 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:43:10.978960  5452 solver.cpp:397]     Test net output #0: accuracy = 0.8211
I1107 16:43:10.978960  5452 solver.cpp:397]     Test net output #1: loss = 0.614796 (* 1 = 0.614796 loss)
I1107 16:43:11.166990  5452 solver.cpp:218] Iteration 90000 (38296.9 iter/s, 2.35006s/100 iters), loss = 0.206618
I1107 16:43:11.166990  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 16:43:11.166990  5452 solver.cpp:237]     Train net output #1: loss = 0.206618 (* 1 = 0.206618 loss)
I1107 16:43:11.166990  5452 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1107 16:43:19.771442  5452 solver.cpp:218] Iteration 90100 (11.6235 iter/s, 8.60328s/100 iters), loss = 0.184818
I1107 16:43:19.771442  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:43:19.771442  5452 solver.cpp:237]     Train net output #1: loss = 0.184818 (* 1 = 0.184818 loss)
I1107 16:43:19.771442  5452 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1107 16:43:28.317811  5452 solver.cpp:218] Iteration 90200 (11.7011 iter/s, 8.54618s/100 iters), loss = 0.193079
I1107 16:43:28.317811  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:43:28.317811  5452 solver.cpp:237]     Train net output #1: loss = 0.193079 (* 1 = 0.193079 loss)
I1107 16:43:28.317811  5452 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1107 16:43:36.874676  5452 solver.cpp:218] Iteration 90300 (11.687 iter/s, 8.55653s/100 iters), loss = 0.136264
I1107 16:43:36.874676  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:43:36.874676  5452 solver.cpp:237]     Train net output #1: loss = 0.136264 (* 1 = 0.136264 loss)
I1107 16:43:36.874676  5452 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1107 16:43:45.401355  5452 solver.cpp:218] Iteration 90400 (11.7285 iter/s, 8.52622s/100 iters), loss = 0.202078
I1107 16:43:45.401355  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 16:43:45.401355  5452 solver.cpp:237]     Train net output #1: loss = 0.202078 (* 1 = 0.202078 loss)
I1107 16:43:45.401355  5452 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1107 16:43:53.499095 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:43:53.833117  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_90500.caffemodel
I1107 16:43:53.866118  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_90500.solverstate
I1107 16:43:53.875118  5452 solver.cpp:330] Iteration 90500, Testing net (#0)
I1107 16:43:53.875118  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:43:55.856256 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:43:55.935259  5452 solver.cpp:397]     Test net output #0: accuracy = 0.8363
I1107 16:43:55.936260  5452 solver.cpp:397]     Test net output #1: loss = 0.558904 (* 1 = 0.558904 loss)
I1107 16:43:56.017263  5452 solver.cpp:218] Iteration 90500 (9.42065 iter/s, 10.615s/100 iters), loss = 0.203258
I1107 16:43:56.017263  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 16:43:56.017263  5452 solver.cpp:237]     Train net output #1: loss = 0.203258 (* 1 = 0.203258 loss)
I1107 16:43:56.017263  5452 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1107 16:44:04.517105  5452 solver.cpp:218] Iteration 90600 (11.766 iter/s, 8.49908s/100 iters), loss = 0.216539
I1107 16:44:04.517105  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 16:44:04.517105  5452 solver.cpp:237]     Train net output #1: loss = 0.216539 (* 1 = 0.216539 loss)
I1107 16:44:04.517105  5452 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1107 16:44:13.039891  5452 solver.cpp:218] Iteration 90700 (11.7344 iter/s, 8.52193s/100 iters), loss = 0.150583
I1107 16:44:13.039891  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:44:13.039891  5452 solver.cpp:237]     Train net output #1: loss = 0.150583 (* 1 = 0.150583 loss)
I1107 16:44:13.039891  5452 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1107 16:44:21.563838  5452 solver.cpp:218] Iteration 90800 (11.7315 iter/s, 8.52403s/100 iters), loss = 0.164018
I1107 16:44:21.563838  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:44:21.563838  5452 solver.cpp:237]     Train net output #1: loss = 0.164018 (* 1 = 0.164018 loss)
I1107 16:44:21.563838  5452 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1107 16:44:30.137697  5452 solver.cpp:218] Iteration 90900 (11.6644 iter/s, 8.57307s/100 iters), loss = 0.181312
I1107 16:44:30.137697  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:44:30.137697  5452 solver.cpp:237]     Train net output #1: loss = 0.181312 (* 1 = 0.181312 loss)
I1107 16:44:30.137697  5452 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1107 16:44:38.331703 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:44:38.678759  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_91000.caffemodel
I1107 16:44:38.708756  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_91000.solverstate
I1107 16:44:38.718755  5452 solver.cpp:330] Iteration 91000, Testing net (#0)
I1107 16:44:38.719756  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:44:40.742086 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:44:40.824120  5452 solver.cpp:397]     Test net output #0: accuracy = 0.8542
I1107 16:44:40.824120  5452 solver.cpp:397]     Test net output #1: loss = 0.48918 (* 1 = 0.48918 loss)
I1107 16:44:40.907109  5452 solver.cpp:218] Iteration 91000 (9.2857 iter/s, 10.7692s/100 iters), loss = 0.191894
I1107 16:44:40.907109  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 16:44:40.908110  5452 solver.cpp:237]     Train net output #1: loss = 0.191894 (* 1 = 0.191894 loss)
I1107 16:44:40.908110  5452 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1107 16:44:49.469817  5452 solver.cpp:218] Iteration 91100 (11.6805 iter/s, 8.56126s/100 iters), loss = 0.246858
I1107 16:44:49.469817  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 16:44:49.469817  5452 solver.cpp:237]     Train net output #1: loss = 0.246858 (* 1 = 0.246858 loss)
I1107 16:44:49.469817  5452 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1107 16:44:57.988546  5452 solver.cpp:218] Iteration 91200 (11.7391 iter/s, 8.51856s/100 iters), loss = 0.240246
I1107 16:44:57.988546  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1107 16:44:57.988546  5452 solver.cpp:237]     Train net output #1: loss = 0.240245 (* 1 = 0.240245 loss)
I1107 16:44:57.988546  5452 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1107 16:45:06.640878  5452 solver.cpp:218] Iteration 91300 (11.5578 iter/s, 8.65214s/100 iters), loss = 0.210091
I1107 16:45:06.640878  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 16:45:06.640878  5452 solver.cpp:237]     Train net output #1: loss = 0.210091 (* 1 = 0.210091 loss)
I1107 16:45:06.640878  5452 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1107 16:45:15.336997  5452 solver.cpp:218] Iteration 91400 (11.4997 iter/s, 8.69588s/100 iters), loss = 0.240323
I1107 16:45:15.337996  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1107 16:45:15.337996  5452 solver.cpp:237]     Train net output #1: loss = 0.240323 (* 1 = 0.240323 loss)
I1107 16:45:15.337996  5452 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1107 16:45:23.638659 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:45:23.974683  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_91500.caffemodel
I1107 16:45:24.005682  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_91500.solverstate
I1107 16:45:24.015193  5452 solver.cpp:330] Iteration 91500, Testing net (#0)
I1107 16:45:24.015193  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:45:26.023681 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:45:26.105206  5452 solver.cpp:397]     Test net output #0: accuracy = 0.8684
I1107 16:45:26.105206  5452 solver.cpp:397]     Test net output #1: loss = 0.43909 (* 1 = 0.43909 loss)
I1107 16:45:26.188210  5452 solver.cpp:218] Iteration 91500 (9.21618 iter/s, 10.8505s/100 iters), loss = 0.149764
I1107 16:45:26.188210  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:45:26.188210  5452 solver.cpp:237]     Train net output #1: loss = 0.149764 (* 1 = 0.149764 loss)
I1107 16:45:26.188210  5452 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1107 16:45:34.803174  5452 solver.cpp:218] Iteration 91600 (11.6083 iter/s, 8.61452s/100 iters), loss = 0.218299
I1107 16:45:34.804175  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 16:45:34.804175  5452 solver.cpp:237]     Train net output #1: loss = 0.218299 (* 1 = 0.218299 loss)
I1107 16:45:34.804175  5452 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1107 16:45:43.417471  5452 solver.cpp:218] Iteration 91700 (11.6103 iter/s, 8.61301s/100 iters), loss = 0.158462
I1107 16:45:43.417471  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:45:43.417471  5452 solver.cpp:237]     Train net output #1: loss = 0.158462 (* 1 = 0.158462 loss)
I1107 16:45:43.417471  5452 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1107 16:45:52.039924  5452 solver.cpp:218] Iteration 91800 (11.5977 iter/s, 8.62243s/100 iters), loss = 0.175679
I1107 16:45:52.039924  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:45:52.039924  5452 solver.cpp:237]     Train net output #1: loss = 0.175679 (* 1 = 0.175679 loss)
I1107 16:45:52.039924  5452 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1107 16:46:00.594149  5452 solver.cpp:218] Iteration 91900 (11.6917 iter/s, 8.55311s/100 iters), loss = 0.122784
I1107 16:46:00.594149  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 16:46:00.594149  5452 solver.cpp:237]     Train net output #1: loss = 0.122784 (* 1 = 0.122784 loss)
I1107 16:46:00.594149  5452 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1107 16:46:08.766031 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:46:09.105047  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_92000.caffemodel
I1107 16:46:09.135552  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_92000.solverstate
I1107 16:46:09.144055  5452 solver.cpp:330] Iteration 92000, Testing net (#0)
I1107 16:46:09.144055  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:46:11.144300 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:46:11.223803  5452 solver.cpp:397]     Test net output #0: accuracy = 0.8455
I1107 16:46:11.223803  5452 solver.cpp:397]     Test net output #1: loss = 0.513322 (* 1 = 0.513322 loss)
I1107 16:46:11.304303  5452 solver.cpp:218] Iteration 92000 (9.33665 iter/s, 10.7105s/100 iters), loss = 0.155388
I1107 16:46:11.304303  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:46:11.304303  5452 solver.cpp:237]     Train net output #1: loss = 0.155388 (* 1 = 0.155388 loss)
I1107 16:46:11.304303  5452 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1107 16:46:19.827669  5452 solver.cpp:218] Iteration 92100 (11.734 iter/s, 8.52224s/100 iters), loss = 0.196398
I1107 16:46:19.827669  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:46:19.827669  5452 solver.cpp:237]     Train net output #1: loss = 0.196398 (* 1 = 0.196398 loss)
I1107 16:46:19.827669  5452 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1107 16:46:28.477037  5452 solver.cpp:218] Iteration 92200 (11.5619 iter/s, 8.64907s/100 iters), loss = 0.204252
I1107 16:46:28.477037  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 16:46:28.477037  5452 solver.cpp:237]     Train net output #1: loss = 0.204252 (* 1 = 0.204252 loss)
I1107 16:46:28.477037  5452 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1107 16:46:37.167371  5452 solver.cpp:218] Iteration 92300 (11.5079 iter/s, 8.68965s/100 iters), loss = 0.121867
I1107 16:46:37.167371  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 16:46:37.167371  5452 solver.cpp:237]     Train net output #1: loss = 0.121867 (* 1 = 0.121867 loss)
I1107 16:46:37.167371  5452 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1107 16:46:45.853930  5452 solver.cpp:218] Iteration 92400 (11.5122 iter/s, 8.68642s/100 iters), loss = 0.205124
I1107 16:46:45.854933  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 16:46:45.854933  5452 solver.cpp:237]     Train net output #1: loss = 0.205124 (* 1 = 0.205124 loss)
I1107 16:46:45.854933  5452 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1107 16:46:54.145452 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:46:54.489502  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_92500.caffemodel
I1107 16:46:54.521504  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_92500.solverstate
I1107 16:46:54.530503  5452 solver.cpp:330] Iteration 92500, Testing net (#0)
I1107 16:46:54.531008  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:46:56.559746 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:46:56.641247  5452 solver.cpp:397]     Test net output #0: accuracy = 0.7831
I1107 16:46:56.641247  5452 solver.cpp:397]     Test net output #1: loss = 0.783349 (* 1 = 0.783349 loss)
I1107 16:46:56.725764  5452 solver.cpp:218] Iteration 92500 (9.19936 iter/s, 10.8703s/100 iters), loss = 0.138703
I1107 16:46:56.725764  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:46:56.725764  5452 solver.cpp:237]     Train net output #1: loss = 0.138703 (* 1 = 0.138703 loss)
I1107 16:46:56.725764  5452 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1107 16:47:05.409133  5452 solver.cpp:218] Iteration 92600 (11.517 iter/s, 8.68284s/100 iters), loss = 0.211384
I1107 16:47:05.409133  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:47:05.409133  5452 solver.cpp:237]     Train net output #1: loss = 0.211384 (* 1 = 0.211384 loss)
I1107 16:47:05.409133  5452 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1107 16:47:14.054507  5452 solver.cpp:218] Iteration 92700 (11.5677 iter/s, 8.64477s/100 iters), loss = 0.236166
I1107 16:47:14.054507  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 16:47:14.054507  5452 solver.cpp:237]     Train net output #1: loss = 0.236166 (* 1 = 0.236166 loss)
I1107 16:47:14.054507  5452 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1107 16:47:22.672605  5452 solver.cpp:218] Iteration 92800 (11.6041 iter/s, 8.61765s/100 iters), loss = 0.187905
I1107 16:47:22.672605  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:47:22.672605  5452 solver.cpp:237]     Train net output #1: loss = 0.187905 (* 1 = 0.187905 loss)
I1107 16:47:22.672605  5452 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1107 16:47:31.257448  5452 solver.cpp:218] Iteration 92900 (11.649 iter/s, 8.58441s/100 iters), loss = 0.175723
I1107 16:47:31.257448  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 16:47:31.257448  5452 solver.cpp:237]     Train net output #1: loss = 0.175723 (* 1 = 0.175723 loss)
I1107 16:47:31.257448  5452 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1107 16:47:39.392263 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:47:39.728279  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_93000.caffemodel
I1107 16:47:39.758285  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_93000.solverstate
I1107 16:47:39.767284  5452 solver.cpp:330] Iteration 93000, Testing net (#0)
I1107 16:47:39.767786  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:47:41.758576 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:47:41.837577  5452 solver.cpp:397]     Test net output #0: accuracy = 0.8762
I1107 16:47:41.837577  5452 solver.cpp:397]     Test net output #1: loss = 0.406622 (* 1 = 0.406622 loss)
I1107 16:47:41.918594  5452 solver.cpp:218] Iteration 93000 (9.3798 iter/s, 10.6612s/100 iters), loss = 0.220669
I1107 16:47:41.918594  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 16:47:41.918594  5452 solver.cpp:237]     Train net output #1: loss = 0.220669 (* 1 = 0.220669 loss)
I1107 16:47:41.919595  5452 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1107 16:47:50.515517  5452 solver.cpp:218] Iteration 93100 (11.6328 iter/s, 8.59639s/100 iters), loss = 0.163667
I1107 16:47:50.515517  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 16:47:50.515517  5452 solver.cpp:237]     Train net output #1: loss = 0.163667 (* 1 = 0.163667 loss)
I1107 16:47:50.515517  5452 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1107 16:47:59.137603  5452 solver.cpp:218] Iteration 93200 (11.5995 iter/s, 8.62106s/100 iters), loss = 0.14066
I1107 16:47:59.137603  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:47:59.137603  5452 solver.cpp:237]     Train net output #1: loss = 0.14066 (* 1 = 0.14066 loss)
I1107 16:47:59.137603  5452 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1107 16:48:07.666332  5452 solver.cpp:218] Iteration 93300 (11.7265 iter/s, 8.5277s/100 iters), loss = 0.202404
I1107 16:48:07.666332  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:48:07.666332  5452 solver.cpp:237]     Train net output #1: loss = 0.202403 (* 1 = 0.202403 loss)
I1107 16:48:07.666332  5452 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1107 16:48:16.256003  5452 solver.cpp:218] Iteration 93400 (11.6421 iter/s, 8.58948s/100 iters), loss = 0.130093
I1107 16:48:16.256003  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:48:16.256003  5452 solver.cpp:237]     Train net output #1: loss = 0.130093 (* 1 = 0.130093 loss)
I1107 16:48:16.256505  5452 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1107 16:48:24.377748 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:48:24.712765  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_93500.caffemodel
I1107 16:48:24.743773  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_93500.solverstate
I1107 16:48:24.752773  5452 solver.cpp:330] Iteration 93500, Testing net (#0)
I1107 16:48:24.752773  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:48:26.750922 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:48:26.831421  5452 solver.cpp:397]     Test net output #0: accuracy = 0.8514
I1107 16:48:26.831421  5452 solver.cpp:397]     Test net output #1: loss = 0.497132 (* 1 = 0.497132 loss)
I1107 16:48:26.912420  5452 solver.cpp:218] Iteration 93500 (9.38488 iter/s, 10.6554s/100 iters), loss = 0.218219
I1107 16:48:26.912420  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 16:48:26.912420  5452 solver.cpp:237]     Train net output #1: loss = 0.218219 (* 1 = 0.218219 loss)
I1107 16:48:26.912420  5452 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1107 16:48:35.481367  5452 solver.cpp:218] Iteration 93600 (11.6701 iter/s, 8.56893s/100 iters), loss = 0.23705
I1107 16:48:35.481367  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 16:48:35.481367  5452 solver.cpp:237]     Train net output #1: loss = 0.23705 (* 1 = 0.23705 loss)
I1107 16:48:35.481367  5452 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1107 16:48:44.007653  5452 solver.cpp:218] Iteration 93700 (11.7288 iter/s, 8.526s/100 iters), loss = 0.238201
I1107 16:48:44.007653  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 16:48:44.007653  5452 solver.cpp:237]     Train net output #1: loss = 0.238201 (* 1 = 0.238201 loss)
I1107 16:48:44.007653  5452 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1107 16:48:52.560745  5452 solver.cpp:218] Iteration 93800 (11.6926 iter/s, 8.55243s/100 iters), loss = 0.144289
I1107 16:48:52.560745  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 16:48:52.560745  5452 solver.cpp:237]     Train net output #1: loss = 0.144288 (* 1 = 0.144288 loss)
I1107 16:48:52.560745  5452 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1107 16:49:01.178910  5452 solver.cpp:218] Iteration 93900 (11.6043 iter/s, 8.61749s/100 iters), loss = 0.234041
I1107 16:49:01.178910  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 16:49:01.178910  5452 solver.cpp:237]     Train net output #1: loss = 0.234041 (* 1 = 0.234041 loss)
I1107 16:49:01.178910  5452 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1107 16:49:09.345147 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:49:09.681164  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_94000.caffemodel
I1107 16:49:09.717172  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_94000.solverstate
I1107 16:49:09.727172  5452 solver.cpp:330] Iteration 94000, Testing net (#0)
I1107 16:49:09.727172  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:49:11.719467 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:49:11.798470  5452 solver.cpp:397]     Test net output #0: accuracy = 0.8585
I1107 16:49:11.798470  5452 solver.cpp:397]     Test net output #1: loss = 0.473908 (* 1 = 0.473908 loss)
I1107 16:49:11.879470  5452 solver.cpp:218] Iteration 94000 (9.34587 iter/s, 10.6999s/100 iters), loss = 0.192843
I1107 16:49:11.879470  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 16:49:11.879470  5452 solver.cpp:237]     Train net output #1: loss = 0.192843 (* 1 = 0.192843 loss)
I1107 16:49:11.879470  5452 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1107 16:49:20.506510  5452 solver.cpp:218] Iteration 94100 (11.5928 iter/s, 8.62603s/100 iters), loss = 0.223059
I1107 16:49:20.506510  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 16:49:20.506510  5452 solver.cpp:237]     Train net output #1: loss = 0.223059 (* 1 = 0.223059 loss)
I1107 16:49:20.506510  5452 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1107 16:49:29.132891  5452 solver.cpp:218] Iteration 94200 (11.5922 iter/s, 8.62652s/100 iters), loss = 0.270827
I1107 16:49:29.133891  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1107 16:49:29.133891  5452 solver.cpp:237]     Train net output #1: loss = 0.270827 (* 1 = 0.270827 loss)
I1107 16:49:29.133891  5452 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1107 16:49:37.656716  5452 solver.cpp:218] Iteration 94300 (11.7336 iter/s, 8.5225s/100 iters), loss = 0.111327
I1107 16:49:37.656716  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 16:49:37.656716  5452 solver.cpp:237]     Train net output #1: loss = 0.111327 (* 1 = 0.111327 loss)
I1107 16:49:37.656716  5452 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1107 16:49:46.186483  5452 solver.cpp:218] Iteration 94400 (11.7233 iter/s, 8.53004s/100 iters), loss = 0.150507
I1107 16:49:46.187484  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:49:46.187484  5452 solver.cpp:237]     Train net output #1: loss = 0.150507 (* 1 = 0.150507 loss)
I1107 16:49:46.187484  5452 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1107 16:49:54.357187 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:49:54.694236  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_94500.caffemodel
I1107 16:49:54.727239  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_94500.solverstate
I1107 16:49:54.736240  5452 solver.cpp:330] Iteration 94500, Testing net (#0)
I1107 16:49:54.736742  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:49:56.733711 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:49:56.812712  5452 solver.cpp:397]     Test net output #0: accuracy = 0.8484
I1107 16:49:56.812712  5452 solver.cpp:397]     Test net output #1: loss = 0.505985 (* 1 = 0.505985 loss)
I1107 16:49:56.893718  5452 solver.cpp:218] Iteration 94500 (9.34033 iter/s, 10.7063s/100 iters), loss = 0.22143
I1107 16:49:56.893718  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:49:56.893718  5452 solver.cpp:237]     Train net output #1: loss = 0.22143 (* 1 = 0.22143 loss)
I1107 16:49:56.893718  5452 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1107 16:50:05.497884  5452 solver.cpp:218] Iteration 94600 (11.6233 iter/s, 8.60343s/100 iters), loss = 0.197546
I1107 16:50:05.497884  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1107 16:50:05.497884  5452 solver.cpp:237]     Train net output #1: loss = 0.197546 (* 1 = 0.197546 loss)
I1107 16:50:05.497884  5452 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1107 16:50:14.103796  5452 solver.cpp:218] Iteration 94700 (11.62 iter/s, 8.60589s/100 iters), loss = 0.199227
I1107 16:50:14.103796  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:50:14.103796  5452 solver.cpp:237]     Train net output #1: loss = 0.199227 (* 1 = 0.199227 loss)
I1107 16:50:14.103796  5452 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1107 16:50:22.684658  5452 solver.cpp:218] Iteration 94800 (11.6543 iter/s, 8.58049s/100 iters), loss = 0.224996
I1107 16:50:22.684658  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 16:50:22.684658  5452 solver.cpp:237]     Train net output #1: loss = 0.224996 (* 1 = 0.224996 loss)
I1107 16:50:22.684658  5452 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1107 16:50:31.289171  5452 solver.cpp:218] Iteration 94900 (11.623 iter/s, 8.60366s/100 iters), loss = 0.182442
I1107 16:50:31.289171  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:50:31.289171  5452 solver.cpp:237]     Train net output #1: loss = 0.182441 (* 1 = 0.182441 loss)
I1107 16:50:31.289171  5452 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1107 16:50:39.369926 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:50:39.706964  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_95000.caffemodel
I1107 16:50:39.735965  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_95000.solverstate
I1107 16:50:39.744964  5452 solver.cpp:330] Iteration 95000, Testing net (#0)
I1107 16:50:39.744964  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:50:41.728307 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:50:41.807318  5452 solver.cpp:397]     Test net output #0: accuracy = 0.8497
I1107 16:50:41.807318  5452 solver.cpp:397]     Test net output #1: loss = 0.493391 (* 1 = 0.493391 loss)
I1107 16:50:41.889340  5452 solver.cpp:218] Iteration 95000 (9.43429 iter/s, 10.5996s/100 iters), loss = 0.163905
I1107 16:50:41.889340  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 16:50:41.889340  5452 solver.cpp:237]     Train net output #1: loss = 0.163905 (* 1 = 0.163905 loss)
I1107 16:50:41.889340  5452 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1107 16:50:41.889340  5452 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1107 16:50:50.425983  5452 solver.cpp:218] Iteration 95100 (11.7152 iter/s, 8.53594s/100 iters), loss = 0.181454
I1107 16:50:50.425983  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:50:50.425983  5452 solver.cpp:237]     Train net output #1: loss = 0.181454 (* 1 = 0.181454 loss)
I1107 16:50:50.425983  5452 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1107 16:50:58.970901  5452 solver.cpp:218] Iteration 95200 (11.7041 iter/s, 8.544s/100 iters), loss = 0.105194
I1107 16:50:58.970901  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:50:58.970901  5452 solver.cpp:237]     Train net output #1: loss = 0.105194 (* 1 = 0.105194 loss)
I1107 16:50:58.970901  5452 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1107 16:51:07.509960  5452 solver.cpp:218] Iteration 95300 (11.7115 iter/s, 8.53861s/100 iters), loss = 0.0754298
I1107 16:51:07.509960  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:51:07.509960  5452 solver.cpp:237]     Train net output #1: loss = 0.0754297 (* 1 = 0.0754297 loss)
I1107 16:51:07.509960  5452 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1107 16:51:16.035871  5452 solver.cpp:218] Iteration 95400 (11.7297 iter/s, 8.52537s/100 iters), loss = 0.0977708
I1107 16:51:16.035871  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 16:51:16.035871  5452 solver.cpp:237]     Train net output #1: loss = 0.0977707 (* 1 = 0.0977707 loss)
I1107 16:51:16.035871  5452 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1107 16:51:24.151631 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:51:24.488673  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_95500.caffemodel
I1107 16:51:24.518676  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_95500.solverstate
I1107 16:51:24.527676  5452 solver.cpp:330] Iteration 95500, Testing net (#0)
I1107 16:51:24.527676  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:51:26.520840 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:51:26.599906  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9145
I1107 16:51:26.599906  5452 solver.cpp:397]     Test net output #1: loss = 0.279422 (* 1 = 0.279422 loss)
I1107 16:51:26.680878  5452 solver.cpp:218] Iteration 95500 (9.39441 iter/s, 10.6446s/100 iters), loss = 0.110998
I1107 16:51:26.680878  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:51:26.680878  5452 solver.cpp:237]     Train net output #1: loss = 0.110998 (* 1 = 0.110998 loss)
I1107 16:51:26.680878  5452 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1107 16:51:35.223700  5452 solver.cpp:218] Iteration 95600 (11.706 iter/s, 8.54263s/100 iters), loss = 0.170109
I1107 16:51:35.223700  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1107 16:51:35.223700  5452 solver.cpp:237]     Train net output #1: loss = 0.170109 (* 1 = 0.170109 loss)
I1107 16:51:35.223700  5452 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1107 16:51:43.758013  5452 solver.cpp:218] Iteration 95700 (11.7176 iter/s, 8.53413s/100 iters), loss = 0.135134
I1107 16:51:43.759017  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 16:51:43.759017  5452 solver.cpp:237]     Train net output #1: loss = 0.135134 (* 1 = 0.135134 loss)
I1107 16:51:43.759017  5452 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1107 16:51:52.304618  5452 solver.cpp:218] Iteration 95800 (11.702 iter/s, 8.54553s/100 iters), loss = 0.077668
I1107 16:51:52.304618  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:51:52.304618  5452 solver.cpp:237]     Train net output #1: loss = 0.0776679 (* 1 = 0.0776679 loss)
I1107 16:51:52.304618  5452 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1107 16:52:00.816460  5452 solver.cpp:218] Iteration 95900 (11.749 iter/s, 8.51133s/100 iters), loss = 0.0951465
I1107 16:52:00.816460  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 16:52:00.816460  5452 solver.cpp:237]     Train net output #1: loss = 0.0951464 (* 1 = 0.0951464 loss)
I1107 16:52:00.816460  5452 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1107 16:52:08.937518 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:52:09.272541  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_96000.caffemodel
I1107 16:52:09.303546  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_96000.solverstate
I1107 16:52:09.314553  5452 solver.cpp:330] Iteration 96000, Testing net (#0)
I1107 16:52:09.314553  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:52:11.303936 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:52:11.383941  5452 solver.cpp:397]     Test net output #0: accuracy = 0.916
I1107 16:52:11.383941  5452 solver.cpp:397]     Test net output #1: loss = 0.276776 (* 1 = 0.276776 loss)
I1107 16:52:11.464953  5452 solver.cpp:218] Iteration 96000 (9.39169 iter/s, 10.6477s/100 iters), loss = 0.0630966
I1107 16:52:11.464953  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:52:11.464953  5452 solver.cpp:237]     Train net output #1: loss = 0.0630965 (* 1 = 0.0630965 loss)
I1107 16:52:11.464953  5452 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1107 16:52:20.042410  5452 solver.cpp:218] Iteration 96100 (11.6588 iter/s, 8.57722s/100 iters), loss = 0.143917
I1107 16:52:20.042410  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:52:20.042410  5452 solver.cpp:237]     Train net output #1: loss = 0.143917 (* 1 = 0.143917 loss)
I1107 16:52:20.042410  5452 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1107 16:52:28.590901  5452 solver.cpp:218] Iteration 96200 (11.6983 iter/s, 8.54827s/100 iters), loss = 0.0950342
I1107 16:52:28.590901  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 16:52:28.590901  5452 solver.cpp:237]     Train net output #1: loss = 0.0950341 (* 1 = 0.0950341 loss)
I1107 16:52:28.590901  5452 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1107 16:52:37.129436  5452 solver.cpp:218] Iteration 96300 (11.7123 iter/s, 8.53804s/100 iters), loss = 0.0544026
I1107 16:52:37.129436  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:52:37.129436  5452 solver.cpp:237]     Train net output #1: loss = 0.0544025 (* 1 = 0.0544025 loss)
I1107 16:52:37.129436  5452 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1107 16:52:45.696621  5452 solver.cpp:218] Iteration 96400 (11.6732 iter/s, 8.56664s/100 iters), loss = 0.0705369
I1107 16:52:45.696621  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:52:45.696621  5452 solver.cpp:237]     Train net output #1: loss = 0.0705368 (* 1 = 0.0705368 loss)
I1107 16:52:45.696621  5452 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1107 16:52:53.812407 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:52:54.151448  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_96500.caffemodel
I1107 16:52:54.188448  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_96500.solverstate
I1107 16:52:54.198447  5452 solver.cpp:330] Iteration 96500, Testing net (#0)
I1107 16:52:54.198447  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:52:56.193883 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:52:56.272888  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1107 16:52:56.272888  5452 solver.cpp:397]     Test net output #1: loss = 0.277507 (* 1 = 0.277507 loss)
I1107 16:52:56.353902  5452 solver.cpp:218] Iteration 96500 (9.3837 iter/s, 10.6568s/100 iters), loss = 0.0711461
I1107 16:52:56.353902  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:52:56.353902  5452 solver.cpp:237]     Train net output #1: loss = 0.0711459 (* 1 = 0.0711459 loss)
I1107 16:52:56.354902  5452 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1107 16:53:04.890856  5452 solver.cpp:218] Iteration 96600 (11.7158 iter/s, 8.53548s/100 iters), loss = 0.129816
I1107 16:53:04.890856  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:53:04.890856  5452 solver.cpp:237]     Train net output #1: loss = 0.129815 (* 1 = 0.129815 loss)
I1107 16:53:04.890856  5452 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1107 16:53:13.506899  5452 solver.cpp:218] Iteration 96700 (11.6072 iter/s, 8.61537s/100 iters), loss = 0.0975172
I1107 16:53:13.506899  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:53:13.506899  5452 solver.cpp:237]     Train net output #1: loss = 0.0975171 (* 1 = 0.0975171 loss)
I1107 16:53:13.506899  5452 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1107 16:53:22.068078  5452 solver.cpp:218] Iteration 96800 (11.6812 iter/s, 8.56075s/100 iters), loss = 0.092335
I1107 16:53:22.068078  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:53:22.068078  5452 solver.cpp:237]     Train net output #1: loss = 0.0923348 (* 1 = 0.0923348 loss)
I1107 16:53:22.068078  5452 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1107 16:53:30.627869  5452 solver.cpp:218] Iteration 96900 (11.6824 iter/s, 8.55987s/100 iters), loss = 0.0856378
I1107 16:53:30.627869  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:53:30.627869  5452 solver.cpp:237]     Train net output #1: loss = 0.0856377 (* 1 = 0.0856377 loss)
I1107 16:53:30.627869  5452 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1107 16:53:38.739696 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:53:39.080713  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_97000.caffemodel
I1107 16:53:39.113220  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_97000.solverstate
I1107 16:53:39.122721  5452 solver.cpp:330] Iteration 97000, Testing net (#0)
I1107 16:53:39.122721  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:53:41.109346 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:53:41.188848  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 16:53:41.188848  5452 solver.cpp:397]     Test net output #1: loss = 0.273457 (* 1 = 0.273457 loss)
I1107 16:53:41.269870  5452 solver.cpp:218] Iteration 97000 (9.39712 iter/s, 10.6416s/100 iters), loss = 0.0732561
I1107 16:53:41.269870  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:53:41.269870  5452 solver.cpp:237]     Train net output #1: loss = 0.073256 (* 1 = 0.073256 loss)
I1107 16:53:41.269870  5452 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1107 16:53:49.829670  5452 solver.cpp:218] Iteration 97100 (11.6837 iter/s, 8.55895s/100 iters), loss = 0.13982
I1107 16:53:49.830171  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:53:49.830171  5452 solver.cpp:237]     Train net output #1: loss = 0.13982 (* 1 = 0.13982 loss)
I1107 16:53:49.830171  5452 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1107 16:53:58.391352  5452 solver.cpp:218] Iteration 97200 (11.6809 iter/s, 8.561s/100 iters), loss = 0.0808596
I1107 16:53:58.391352  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:53:58.391352  5452 solver.cpp:237]     Train net output #1: loss = 0.0808595 (* 1 = 0.0808595 loss)
I1107 16:53:58.391352  5452 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1107 16:54:07.027086  5452 solver.cpp:218] Iteration 97300 (11.5801 iter/s, 8.63549s/100 iters), loss = 0.0422872
I1107 16:54:07.027086  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:54:07.027086  5452 solver.cpp:237]     Train net output #1: loss = 0.0422871 (* 1 = 0.0422871 loss)
I1107 16:54:07.027086  5452 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1107 16:54:15.585927  5452 solver.cpp:218] Iteration 97400 (11.6845 iter/s, 8.55838s/100 iters), loss = 0.0786616
I1107 16:54:15.585927  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 16:54:15.585927  5452 solver.cpp:237]     Train net output #1: loss = 0.0786615 (* 1 = 0.0786615 loss)
I1107 16:54:15.585927  5452 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1107 16:54:23.702720 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:54:24.041769  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_97500.caffemodel
I1107 16:54:24.072773  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_97500.solverstate
I1107 16:54:24.081773  5452 solver.cpp:330] Iteration 97500, Testing net (#0)
I1107 16:54:24.081773  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:54:26.077040 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:54:26.156076  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9192
I1107 16:54:26.156076  5452 solver.cpp:397]     Test net output #1: loss = 0.274248 (* 1 = 0.274248 loss)
I1107 16:54:26.238068  5452 solver.cpp:218] Iteration 97500 (9.38894 iter/s, 10.6508s/100 iters), loss = 0.0874192
I1107 16:54:26.238068  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:54:26.238068  5452 solver.cpp:237]     Train net output #1: loss = 0.0874191 (* 1 = 0.0874191 loss)
I1107 16:54:26.238068  5452 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1107 16:54:34.833221  5452 solver.cpp:218] Iteration 97600 (11.6348 iter/s, 8.59488s/100 iters), loss = 0.108032
I1107 16:54:34.833221  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:54:34.833221  5452 solver.cpp:237]     Train net output #1: loss = 0.108032 (* 1 = 0.108032 loss)
I1107 16:54:34.833221  5452 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1107 16:54:43.424389  5452 solver.cpp:218] Iteration 97700 (11.6407 iter/s, 8.59058s/100 iters), loss = 0.0554885
I1107 16:54:43.424389  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:54:43.424389  5452 solver.cpp:237]     Train net output #1: loss = 0.0554884 (* 1 = 0.0554884 loss)
I1107 16:54:43.424389  5452 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1107 16:54:52.035388  5452 solver.cpp:218] Iteration 97800 (11.6136 iter/s, 8.61056s/100 iters), loss = 0.0614703
I1107 16:54:52.035388  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:54:52.035388  5452 solver.cpp:237]     Train net output #1: loss = 0.0614702 (* 1 = 0.0614702 loss)
I1107 16:54:52.035388  5452 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1107 16:55:00.580180  5452 solver.cpp:218] Iteration 97900 (11.704 iter/s, 8.54408s/100 iters), loss = 0.0547203
I1107 16:55:00.580180  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:55:00.580180  5452 solver.cpp:237]     Train net output #1: loss = 0.0547202 (* 1 = 0.0547202 loss)
I1107 16:55:00.580180  5452 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1107 16:55:08.666388 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:55:09.002434  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_98000.caffemodel
I1107 16:55:09.034435  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_98000.solverstate
I1107 16:55:09.043434  5452 solver.cpp:330] Iteration 98000, Testing net (#0)
I1107 16:55:09.043434  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:55:11.036725 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:55:11.117735  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9198
I1107 16:55:11.117735  5452 solver.cpp:397]     Test net output #1: loss = 0.276018 (* 1 = 0.276018 loss)
I1107 16:55:11.198738  5452 solver.cpp:218] Iteration 98000 (9.41727 iter/s, 10.6188s/100 iters), loss = 0.0556924
I1107 16:55:11.198738  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:55:11.199739  5452 solver.cpp:237]     Train net output #1: loss = 0.0556923 (* 1 = 0.0556923 loss)
I1107 16:55:11.199739  5452 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1107 16:55:19.689525  5452 solver.cpp:218] Iteration 98100 (11.7795 iter/s, 8.48936s/100 iters), loss = 0.125538
I1107 16:55:19.689525  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 16:55:19.689525  5452 solver.cpp:237]     Train net output #1: loss = 0.125538 (* 1 = 0.125538 loss)
I1107 16:55:19.689525  5452 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1107 16:55:28.227494  5452 solver.cpp:218] Iteration 98200 (11.7123 iter/s, 8.53805s/100 iters), loss = 0.0579856
I1107 16:55:28.227494  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:55:28.227494  5452 solver.cpp:237]     Train net output #1: loss = 0.0579856 (* 1 = 0.0579856 loss)
I1107 16:55:28.227494  5452 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1107 16:55:36.835232  5452 solver.cpp:218] Iteration 98300 (11.6187 iter/s, 8.6068s/100 iters), loss = 0.10779
I1107 16:55:36.835232  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:55:36.835232  5452 solver.cpp:237]     Train net output #1: loss = 0.10779 (* 1 = 0.10779 loss)
I1107 16:55:36.835232  5452 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1107 16:55:45.409178  5452 solver.cpp:218] Iteration 98400 (11.6631 iter/s, 8.57403s/100 iters), loss = 0.0602777
I1107 16:55:45.409178  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:55:45.409178  5452 solver.cpp:237]     Train net output #1: loss = 0.0602776 (* 1 = 0.0602776 loss)
I1107 16:55:45.409178  5452 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1107 16:55:53.562116 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:55:53.899230  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_98500.caffemodel
I1107 16:55:53.928230  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_98500.solverstate
I1107 16:55:53.937243  5452 solver.cpp:330] Iteration 98500, Testing net (#0)
I1107 16:55:53.937243  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:55:55.943289 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:55:56.022367  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 16:55:56.022367  5452 solver.cpp:397]     Test net output #1: loss = 0.279569 (* 1 = 0.279569 loss)
I1107 16:55:56.103530  5452 solver.cpp:218] Iteration 98500 (9.35185 iter/s, 10.6931s/100 iters), loss = 0.0758313
I1107 16:55:56.103530  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:55:56.103530  5452 solver.cpp:237]     Train net output #1: loss = 0.0758312 (* 1 = 0.0758312 loss)
I1107 16:55:56.103530  5452 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1107 16:56:04.661052  5452 solver.cpp:218] Iteration 98600 (11.6856 iter/s, 8.55753s/100 iters), loss = 0.0866928
I1107 16:56:04.661052  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:56:04.661052  5452 solver.cpp:237]     Train net output #1: loss = 0.0866927 (* 1 = 0.0866927 loss)
I1107 16:56:04.661052  5452 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1107 16:56:13.203387  5452 solver.cpp:218] Iteration 98700 (11.7069 iter/s, 8.54198s/100 iters), loss = 0.0565422
I1107 16:56:13.204387  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:56:13.204387  5452 solver.cpp:237]     Train net output #1: loss = 0.0565421 (* 1 = 0.0565421 loss)
I1107 16:56:13.204387  5452 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1107 16:56:21.708206  5452 solver.cpp:218] Iteration 98800 (11.7588 iter/s, 8.50426s/100 iters), loss = 0.0592995
I1107 16:56:21.708206  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:56:21.708206  5452 solver.cpp:237]     Train net output #1: loss = 0.0592994 (* 1 = 0.0592994 loss)
I1107 16:56:21.708206  5452 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1107 16:56:30.217131  5452 solver.cpp:218] Iteration 98900 (11.7539 iter/s, 8.50784s/100 iters), loss = 0.047939
I1107 16:56:30.217131  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:56:30.217131  5452 solver.cpp:237]     Train net output #1: loss = 0.0479389 (* 1 = 0.0479389 loss)
I1107 16:56:30.217131  5452 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1107 16:56:38.321755 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:56:38.664793  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_99000.caffemodel
I1107 16:56:38.695793  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_99000.solverstate
I1107 16:56:38.705793  5452 solver.cpp:330] Iteration 99000, Testing net (#0)
I1107 16:56:38.705793  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:56:40.707944 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:56:40.787946  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 16:56:40.787946  5452 solver.cpp:397]     Test net output #1: loss = 0.272597 (* 1 = 0.272597 loss)
I1107 16:56:40.869951  5452 solver.cpp:218] Iteration 99000 (9.38756 iter/s, 10.6524s/100 iters), loss = 0.0832327
I1107 16:56:40.869951  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:56:40.869951  5452 solver.cpp:237]     Train net output #1: loss = 0.0832325 (* 1 = 0.0832325 loss)
I1107 16:56:40.869951  5452 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1107 16:56:49.453622  5452 solver.cpp:218] Iteration 99100 (11.6511 iter/s, 8.5829s/100 iters), loss = 0.0900583
I1107 16:56:49.453622  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:56:49.453622  5452 solver.cpp:237]     Train net output #1: loss = 0.0900582 (* 1 = 0.0900582 loss)
I1107 16:56:49.453622  5452 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1107 16:56:58.027654  5452 solver.cpp:218] Iteration 99200 (11.6637 iter/s, 8.57357s/100 iters), loss = 0.0614489
I1107 16:56:58.027654  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:56:58.027654  5452 solver.cpp:237]     Train net output #1: loss = 0.0614488 (* 1 = 0.0614488 loss)
I1107 16:56:58.027654  5452 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1107 16:57:06.586464  5452 solver.cpp:218] Iteration 99300 (11.6848 iter/s, 8.55811s/100 iters), loss = 0.0393366
I1107 16:57:06.586464  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:57:06.586464  5452 solver.cpp:237]     Train net output #1: loss = 0.0393365 (* 1 = 0.0393365 loss)
I1107 16:57:06.586464  5452 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1107 16:57:15.191851  5452 solver.cpp:218] Iteration 99400 (11.6205 iter/s, 8.60546s/100 iters), loss = 0.0766531
I1107 16:57:15.191851  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 16:57:15.191851  5452 solver.cpp:237]     Train net output #1: loss = 0.076653 (* 1 = 0.076653 loss)
I1107 16:57:15.191851  5452 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1107 16:57:23.373261 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:57:23.718055  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_99500.caffemodel
I1107 16:57:23.748047  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_99500.solverstate
I1107 16:57:23.757591  5452 solver.cpp:330] Iteration 99500, Testing net (#0)
I1107 16:57:23.758072  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:57:25.761835 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:57:25.841836  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 16:57:25.841836  5452 solver.cpp:397]     Test net output #1: loss = 0.275113 (* 1 = 0.275113 loss)
I1107 16:57:25.922842  5452 solver.cpp:218] Iteration 99500 (9.31924 iter/s, 10.7305s/100 iters), loss = 0.0787605
I1107 16:57:25.922842  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:57:25.923841  5452 solver.cpp:237]     Train net output #1: loss = 0.0787604 (* 1 = 0.0787604 loss)
I1107 16:57:25.923841  5452 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1107 16:57:34.505023  5452 solver.cpp:218] Iteration 99600 (11.6539 iter/s, 8.58084s/100 iters), loss = 0.121274
I1107 16:57:34.505023  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:57:34.505023  5452 solver.cpp:237]     Train net output #1: loss = 0.121274 (* 1 = 0.121274 loss)
I1107 16:57:34.505023  5452 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1107 16:57:43.162508  5452 solver.cpp:218] Iteration 99700 (11.5514 iter/s, 8.65694s/100 iters), loss = 0.0674041
I1107 16:57:43.162508  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:57:43.162508  5452 solver.cpp:237]     Train net output #1: loss = 0.067404 (* 1 = 0.067404 loss)
I1107 16:57:43.162508  5452 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1107 16:57:51.817431  5452 solver.cpp:218] Iteration 99800 (11.5549 iter/s, 8.65436s/100 iters), loss = 0.0707498
I1107 16:57:51.817431  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:57:51.817431  5452 solver.cpp:237]     Train net output #1: loss = 0.0707496 (* 1 = 0.0707496 loss)
I1107 16:57:51.817431  5452 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1107 16:58:00.380080  5452 solver.cpp:218] Iteration 99900 (11.6795 iter/s, 8.562s/100 iters), loss = 0.0561347
I1107 16:58:00.380080  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:58:00.380080  5452 solver.cpp:237]     Train net output #1: loss = 0.0561345 (* 1 = 0.0561345 loss)
I1107 16:58:00.380080  5452 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1107 16:58:08.498440 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:58:08.836467  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_100000.caffemodel
I1107 16:58:08.865468  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_100000.solverstate
I1107 16:58:08.875473  5452 solver.cpp:330] Iteration 100000, Testing net (#0)
I1107 16:58:08.875473  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:58:10.879426 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:58:10.958462  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 16:58:10.958462  5452 solver.cpp:397]     Test net output #1: loss = 0.277998 (* 1 = 0.277998 loss)
I1107 16:58:11.042465  5452 solver.cpp:218] Iteration 100000 (9.37927 iter/s, 10.6618s/100 iters), loss = 0.0507939
I1107 16:58:11.042465  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:58:11.042465  5452 solver.cpp:237]     Train net output #1: loss = 0.0507938 (* 1 = 0.0507938 loss)
I1107 16:58:11.042465  5452 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1107 16:58:19.611241  5452 solver.cpp:218] Iteration 100100 (11.6707 iter/s, 8.5685s/100 iters), loss = 0.0720885
I1107 16:58:19.611241  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:58:19.611241  5452 solver.cpp:237]     Train net output #1: loss = 0.0720883 (* 1 = 0.0720883 loss)
I1107 16:58:19.611241  5452 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1107 16:58:28.194030  5452 solver.cpp:218] Iteration 100200 (11.6521 iter/s, 8.58215s/100 iters), loss = 0.0503978
I1107 16:58:28.194030  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:58:28.194030  5452 solver.cpp:237]     Train net output #1: loss = 0.0503977 (* 1 = 0.0503977 loss)
I1107 16:58:28.194030  5452 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1107 16:58:36.806484  5452 solver.cpp:218] Iteration 100300 (11.6115 iter/s, 8.61215s/100 iters), loss = 0.0749422
I1107 16:58:36.806484  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:58:36.806484  5452 solver.cpp:237]     Train net output #1: loss = 0.0749421 (* 1 = 0.0749421 loss)
I1107 16:58:36.806484  5452 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1107 16:58:45.368681  5452 solver.cpp:218] Iteration 100400 (11.6797 iter/s, 8.56186s/100 iters), loss = 0.0663331
I1107 16:58:45.368681  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:58:45.368681  5452 solver.cpp:237]     Train net output #1: loss = 0.066333 (* 1 = 0.066333 loss)
I1107 16:58:45.368681  5452 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1107 16:58:53.523180 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:58:53.859897  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_100500.caffemodel
I1107 16:58:53.890413  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_100500.solverstate
I1107 16:58:53.899415  5452 solver.cpp:330] Iteration 100500, Testing net (#0)
I1107 16:58:53.899415  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:58:55.895023 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:58:55.975030  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 16:58:55.975030  5452 solver.cpp:397]     Test net output #1: loss = 0.276725 (* 1 = 0.276725 loss)
I1107 16:58:56.056562  5452 solver.cpp:218] Iteration 100500 (9.35726 iter/s, 10.6869s/100 iters), loss = 0.0777862
I1107 16:58:56.056562  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:58:56.056562  5452 solver.cpp:237]     Train net output #1: loss = 0.0777861 (* 1 = 0.0777861 loss)
I1107 16:58:56.056562  5452 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1107 16:59:04.604533  5452 solver.cpp:218] Iteration 100600 (11.6992 iter/s, 8.54756s/100 iters), loss = 0.0735604
I1107 16:59:04.604533  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:59:04.604533  5452 solver.cpp:237]     Train net output #1: loss = 0.0735603 (* 1 = 0.0735603 loss)
I1107 16:59:04.604533  5452 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1107 16:59:13.156795  5452 solver.cpp:218] Iteration 100700 (11.6937 iter/s, 8.55158s/100 iters), loss = 0.0399071
I1107 16:59:13.156795  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:59:13.156795  5452 solver.cpp:237]     Train net output #1: loss = 0.039907 (* 1 = 0.039907 loss)
I1107 16:59:13.156795  5452 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1107 16:59:21.693750  5452 solver.cpp:218] Iteration 100800 (11.7146 iter/s, 8.53638s/100 iters), loss = 0.0994063
I1107 16:59:21.693750  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 16:59:21.693750  5452 solver.cpp:237]     Train net output #1: loss = 0.0994062 (* 1 = 0.0994062 loss)
I1107 16:59:21.693750  5452 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1107 16:59:30.204603  5452 solver.cpp:218] Iteration 100900 (11.7495 iter/s, 8.51099s/100 iters), loss = 0.0488768
I1107 16:59:30.204603  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 16:59:30.204603  5452 solver.cpp:237]     Train net output #1: loss = 0.0488767 (* 1 = 0.0488767 loss)
I1107 16:59:30.204603  5452 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1107 16:59:38.346118 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:59:38.682153  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_101000.caffemodel
I1107 16:59:38.712154  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_101000.solverstate
I1107 16:59:38.721154  5452 solver.cpp:330] Iteration 101000, Testing net (#0)
I1107 16:59:38.721154  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 16:59:40.711369 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 16:59:40.790371  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9202
I1107 16:59:40.790371  5452 solver.cpp:397]     Test net output #1: loss = 0.276549 (* 1 = 0.276549 loss)
I1107 16:59:40.872373  5452 solver.cpp:218] Iteration 101000 (9.37476 iter/s, 10.6669s/100 iters), loss = 0.0569384
I1107 16:59:40.872373  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 16:59:40.872373  5452 solver.cpp:237]     Train net output #1: loss = 0.0569383 (* 1 = 0.0569383 loss)
I1107 16:59:40.872373  5452 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1107 16:59:49.531286  5452 solver.cpp:218] Iteration 101100 (11.5496 iter/s, 8.65833s/100 iters), loss = 0.0935975
I1107 16:59:49.531286  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 16:59:49.531286  5452 solver.cpp:237]     Train net output #1: loss = 0.0935974 (* 1 = 0.0935974 loss)
I1107 16:59:49.531286  5452 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1107 16:59:58.083215  5452 solver.cpp:218] Iteration 101200 (11.6939 iter/s, 8.55148s/100 iters), loss = 0.0746503
I1107 16:59:58.083215  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 16:59:58.083215  5452 solver.cpp:237]     Train net output #1: loss = 0.0746501 (* 1 = 0.0746501 loss)
I1107 16:59:58.083215  5452 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1107 17:00:06.684494  5452 solver.cpp:218] Iteration 101300 (11.6268 iter/s, 8.60083s/100 iters), loss = 0.0274859
I1107 17:00:06.684494  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:00:06.684494  5452 solver.cpp:237]     Train net output #1: loss = 0.0274857 (* 1 = 0.0274857 loss)
I1107 17:00:06.684494  5452 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1107 17:00:15.288596  5452 solver.cpp:218] Iteration 101400 (11.6236 iter/s, 8.60321s/100 iters), loss = 0.0483911
I1107 17:00:15.288596  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:00:15.288596  5452 solver.cpp:237]     Train net output #1: loss = 0.0483909 (* 1 = 0.0483909 loss)
I1107 17:00:15.288596  5452 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1107 17:00:23.426473 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:00:23.762534  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_101500.caffemodel
I1107 17:00:23.792534  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_101500.solverstate
I1107 17:00:23.802534  5452 solver.cpp:330] Iteration 101500, Testing net (#0)
I1107 17:00:23.802534  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:00:25.783591 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:00:25.862594  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1107 17:00:25.863603  5452 solver.cpp:397]     Test net output #1: loss = 0.275064 (* 1 = 0.275064 loss)
I1107 17:00:25.944597  5452 solver.cpp:218] Iteration 101500 (9.38491 iter/s, 10.6554s/100 iters), loss = 0.0580844
I1107 17:00:25.944597  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:00:25.944597  5452 solver.cpp:237]     Train net output #1: loss = 0.0580843 (* 1 = 0.0580843 loss)
I1107 17:00:25.944597  5452 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1107 17:00:34.462347  5452 solver.cpp:218] Iteration 101600 (11.7404 iter/s, 8.51758s/100 iters), loss = 0.109851
I1107 17:00:34.462347  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 17:00:34.462347  5452 solver.cpp:237]     Train net output #1: loss = 0.109851 (* 1 = 0.109851 loss)
I1107 17:00:34.462347  5452 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1107 17:00:43.043161  5452 solver.cpp:218] Iteration 101700 (11.6543 iter/s, 8.58053s/100 iters), loss = 0.0776093
I1107 17:00:43.043161  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:00:43.043161  5452 solver.cpp:237]     Train net output #1: loss = 0.0776092 (* 1 = 0.0776092 loss)
I1107 17:00:43.043161  5452 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1107 17:00:51.703076  5452 solver.cpp:218] Iteration 101800 (11.5489 iter/s, 8.65882s/100 iters), loss = 0.0664738
I1107 17:00:51.703076  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:00:51.703076  5452 solver.cpp:237]     Train net output #1: loss = 0.0664736 (* 1 = 0.0664736 loss)
I1107 17:00:51.703076  5452 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1107 17:01:00.413992  5452 solver.cpp:218] Iteration 101900 (11.4806 iter/s, 8.71034s/100 iters), loss = 0.0510146
I1107 17:01:00.413992  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:01:00.413992  5452 solver.cpp:237]     Train net output #1: loss = 0.0510145 (* 1 = 0.0510145 loss)
I1107 17:01:00.413992  5452 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1107 17:01:08.683250 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:01:09.022301  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_102000.caffemodel
I1107 17:01:09.052301  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_102000.solverstate
I1107 17:01:09.061301  5452 solver.cpp:330] Iteration 102000, Testing net (#0)
I1107 17:01:09.061301  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:01:11.073487 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:01:11.152492  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9197
I1107 17:01:11.152492  5452 solver.cpp:397]     Test net output #1: loss = 0.277654 (* 1 = 0.277654 loss)
I1107 17:01:11.233497  5452 solver.cpp:218] Iteration 102000 (9.24289 iter/s, 10.8191s/100 iters), loss = 0.0778799
I1107 17:01:11.233497  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:01:11.233497  5452 solver.cpp:237]     Train net output #1: loss = 0.0778798 (* 1 = 0.0778798 loss)
I1107 17:01:11.233497  5452 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1107 17:01:19.795339  5452 solver.cpp:218] Iteration 102100 (11.6806 iter/s, 8.56122s/100 iters), loss = 0.123352
I1107 17:01:19.795339  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 17:01:19.795339  5452 solver.cpp:237]     Train net output #1: loss = 0.123352 (* 1 = 0.123352 loss)
I1107 17:01:19.795339  5452 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1107 17:01:28.409474  5452 solver.cpp:218] Iteration 102200 (11.6097 iter/s, 8.61348s/100 iters), loss = 0.0906749
I1107 17:01:28.409474  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 17:01:28.409474  5452 solver.cpp:237]     Train net output #1: loss = 0.0906748 (* 1 = 0.0906748 loss)
I1107 17:01:28.409474  5452 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1107 17:01:36.992323  5452 solver.cpp:218] Iteration 102300 (11.6509 iter/s, 8.58304s/100 iters), loss = 0.0367659
I1107 17:01:36.992323  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:01:36.992323  5452 solver.cpp:237]     Train net output #1: loss = 0.0367658 (* 1 = 0.0367658 loss)
I1107 17:01:36.992323  5452 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1107 17:01:45.543599  5452 solver.cpp:218] Iteration 102400 (11.6952 iter/s, 8.55051s/100 iters), loss = 0.041459
I1107 17:01:45.543599  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:01:45.543599  5452 solver.cpp:237]     Train net output #1: loss = 0.0414589 (* 1 = 0.0414589 loss)
I1107 17:01:45.543599  5452 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1107 17:01:53.649150 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:01:53.989168  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_102500.caffemodel
I1107 17:01:54.020172  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_102500.solverstate
I1107 17:01:54.029673  5452 solver.cpp:330] Iteration 102500, Testing net (#0)
I1107 17:01:54.029673  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:01:56.028453 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:01:56.107956  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9192
I1107 17:01:56.107956  5452 solver.cpp:397]     Test net output #1: loss = 0.27992 (* 1 = 0.27992 loss)
I1107 17:01:56.189963  5452 solver.cpp:218] Iteration 102500 (9.3939 iter/s, 10.6452s/100 iters), loss = 0.0561988
I1107 17:01:56.189963  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:01:56.189963  5452 solver.cpp:237]     Train net output #1: loss = 0.0561987 (* 1 = 0.0561987 loss)
I1107 17:01:56.189963  5452 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1107 17:02:04.769232  5452 solver.cpp:218] Iteration 102600 (11.6562 iter/s, 8.57912s/100 iters), loss = 0.0941781
I1107 17:02:04.769232  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1107 17:02:04.769232  5452 solver.cpp:237]     Train net output #1: loss = 0.0941779 (* 1 = 0.0941779 loss)
I1107 17:02:04.769232  5452 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1107 17:02:13.280678  5452 solver.cpp:218] Iteration 102700 (11.7498 iter/s, 8.51077s/100 iters), loss = 0.0481896
I1107 17:02:13.280678  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:02:13.280678  5452 solver.cpp:237]     Train net output #1: loss = 0.0481895 (* 1 = 0.0481895 loss)
I1107 17:02:13.280678  5452 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1107 17:02:21.791551  5452 solver.cpp:218] Iteration 102800 (11.7506 iter/s, 8.51022s/100 iters), loss = 0.0381116
I1107 17:02:21.791551  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:02:21.791551  5452 solver.cpp:237]     Train net output #1: loss = 0.0381115 (* 1 = 0.0381115 loss)
I1107 17:02:21.791551  5452 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1107 17:02:30.385335  5452 solver.cpp:218] Iteration 102900 (11.6372 iter/s, 8.59313s/100 iters), loss = 0.0488129
I1107 17:02:30.385335  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:02:30.385335  5452 solver.cpp:237]     Train net output #1: loss = 0.0488128 (* 1 = 0.0488128 loss)
I1107 17:02:30.385335  5452 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1107 17:02:38.523524 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:02:38.865559  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_103000.caffemodel
I1107 17:02:38.895560  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_103000.solverstate
I1107 17:02:38.905560  5452 solver.cpp:330] Iteration 103000, Testing net (#0)
I1107 17:02:38.905560  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:02:40.897861 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:02:40.977865  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9212
I1107 17:02:40.977865  5452 solver.cpp:397]     Test net output #1: loss = 0.272536 (* 1 = 0.272536 loss)
I1107 17:02:41.059366  5452 solver.cpp:218] Iteration 103000 (9.36919 iter/s, 10.6733s/100 iters), loss = 0.102593
I1107 17:02:41.059366  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 17:02:41.059366  5452 solver.cpp:237]     Train net output #1: loss = 0.102593 (* 1 = 0.102593 loss)
I1107 17:02:41.059366  5452 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1107 17:02:49.577541  5452 solver.cpp:218] Iteration 103100 (11.7394 iter/s, 8.51832s/100 iters), loss = 0.0594337
I1107 17:02:49.577541  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:02:49.577541  5452 solver.cpp:237]     Train net output #1: loss = 0.0594336 (* 1 = 0.0594336 loss)
I1107 17:02:49.577541  5452 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1107 17:02:58.140312  5452 solver.cpp:218] Iteration 103200 (11.6795 iter/s, 8.56204s/100 iters), loss = 0.0712366
I1107 17:02:58.140312  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:02:58.140312  5452 solver.cpp:237]     Train net output #1: loss = 0.0712364 (* 1 = 0.0712364 loss)
I1107 17:02:58.140312  5452 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1107 17:03:06.730208  5452 solver.cpp:218] Iteration 103300 (11.6427 iter/s, 8.58909s/100 iters), loss = 0.0492682
I1107 17:03:06.730208  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:03:06.730208  5452 solver.cpp:237]     Train net output #1: loss = 0.049268 (* 1 = 0.049268 loss)
I1107 17:03:06.730208  5452 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1107 17:03:15.243108  5452 solver.cpp:218] Iteration 103400 (11.7476 iter/s, 8.51239s/100 iters), loss = 0.0457342
I1107 17:03:15.243108  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:03:15.243108  5452 solver.cpp:237]     Train net output #1: loss = 0.0457341 (* 1 = 0.0457341 loss)
I1107 17:03:15.243108  5452 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1107 17:03:23.338915 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:03:23.675916  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_103500.caffemodel
I1107 17:03:23.706923  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_103500.solverstate
I1107 17:03:23.716423  5452 solver.cpp:330] Iteration 103500, Testing net (#0)
I1107 17:03:23.716928  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:03:25.700057 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:03:25.780064  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1107 17:03:25.780064  5452 solver.cpp:397]     Test net output #1: loss = 0.277776 (* 1 = 0.277776 loss)
I1107 17:03:25.861068  5452 solver.cpp:218] Iteration 103500 (9.41864 iter/s, 10.6172s/100 iters), loss = 0.0422458
I1107 17:03:25.861068  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:03:25.861068  5452 solver.cpp:237]     Train net output #1: loss = 0.0422456 (* 1 = 0.0422456 loss)
I1107 17:03:25.861068  5452 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1107 17:03:34.414708  5452 solver.cpp:218] Iteration 103600 (11.6916 iter/s, 8.55318s/100 iters), loss = 0.0834042
I1107 17:03:34.414708  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:03:34.414708  5452 solver.cpp:237]     Train net output #1: loss = 0.0834041 (* 1 = 0.0834041 loss)
I1107 17:03:34.414708  5452 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1107 17:03:42.923971  5452 solver.cpp:218] Iteration 103700 (11.7524 iter/s, 8.5089s/100 iters), loss = 0.0364378
I1107 17:03:42.923971  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:03:42.923971  5452 solver.cpp:237]     Train net output #1: loss = 0.0364377 (* 1 = 0.0364377 loss)
I1107 17:03:42.923971  5452 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1107 17:03:51.453122  5452 solver.cpp:218] Iteration 103800 (11.7256 iter/s, 8.52836s/100 iters), loss = 0.0473366
I1107 17:03:51.453122  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:03:51.453122  5452 solver.cpp:237]     Train net output #1: loss = 0.0473364 (* 1 = 0.0473364 loss)
I1107 17:03:51.453122  5452 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1107 17:03:59.963982  5452 solver.cpp:218] Iteration 103900 (11.7497 iter/s, 8.51084s/100 iters), loss = 0.0362492
I1107 17:03:59.963982  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:03:59.963982  5452 solver.cpp:237]     Train net output #1: loss = 0.0362491 (* 1 = 0.0362491 loss)
I1107 17:03:59.963982  5452 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1107 17:04:08.061755 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:04:08.397780  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_104000.caffemodel
I1107 17:04:08.426789  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_104000.solverstate
I1107 17:04:08.434788  5452 solver.cpp:330] Iteration 104000, Testing net (#0)
I1107 17:04:08.435789  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:04:10.415951 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:04:10.494953  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 17:04:10.494953  5452 solver.cpp:397]     Test net output #1: loss = 0.281147 (* 1 = 0.281147 loss)
I1107 17:04:10.575958  5452 solver.cpp:218] Iteration 104000 (9.42384 iter/s, 10.6114s/100 iters), loss = 0.0522742
I1107 17:04:10.575958  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:04:10.575958  5452 solver.cpp:237]     Train net output #1: loss = 0.0522741 (* 1 = 0.0522741 loss)
I1107 17:04:10.575958  5452 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1107 17:04:19.100708  5452 solver.cpp:218] Iteration 104100 (11.7316 iter/s, 8.52401s/100 iters), loss = 0.141889
I1107 17:04:19.100708  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1107 17:04:19.100708  5452 solver.cpp:237]     Train net output #1: loss = 0.141888 (* 1 = 0.141888 loss)
I1107 17:04:19.100708  5452 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1107 17:04:27.625588  5452 solver.cpp:218] Iteration 104200 (11.7305 iter/s, 8.52479s/100 iters), loss = 0.0432802
I1107 17:04:27.625588  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:04:27.625588  5452 solver.cpp:237]     Train net output #1: loss = 0.0432801 (* 1 = 0.0432801 loss)
I1107 17:04:27.625588  5452 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1107 17:04:36.152314  5452 solver.cpp:218] Iteration 104300 (11.7285 iter/s, 8.5262s/100 iters), loss = 0.0334364
I1107 17:04:36.152314  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:04:36.152314  5452 solver.cpp:237]     Train net output #1: loss = 0.0334363 (* 1 = 0.0334363 loss)
I1107 17:04:36.152314  5452 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1107 17:04:44.680514  5452 solver.cpp:218] Iteration 104400 (11.727 iter/s, 8.52733s/100 iters), loss = 0.0352339
I1107 17:04:44.680514  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:04:44.680514  5452 solver.cpp:237]     Train net output #1: loss = 0.0352338 (* 1 = 0.0352338 loss)
I1107 17:04:44.680514  5452 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1107 17:04:52.791785 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:04:53.128808  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_104500.caffemodel
I1107 17:04:53.159806  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_104500.solverstate
I1107 17:04:53.168807  5452 solver.cpp:330] Iteration 104500, Testing net (#0)
I1107 17:04:53.168807  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:04:55.156412 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:04:55.235419  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 17:04:55.235419  5452 solver.cpp:397]     Test net output #1: loss = 0.281481 (* 1 = 0.281481 loss)
I1107 17:04:55.316432  5452 solver.cpp:218] Iteration 104500 (9.40254 iter/s, 10.6354s/100 iters), loss = 0.0637644
I1107 17:04:55.316432  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:04:55.316432  5452 solver.cpp:237]     Train net output #1: loss = 0.0637642 (* 1 = 0.0637642 loss)
I1107 17:04:55.316432  5452 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1107 17:05:03.851104  5452 solver.cpp:218] Iteration 104600 (11.7177 iter/s, 8.53413s/100 iters), loss = 0.0792975
I1107 17:05:03.851104  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 17:05:03.851104  5452 solver.cpp:237]     Train net output #1: loss = 0.0792974 (* 1 = 0.0792974 loss)
I1107 17:05:03.851104  5452 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1107 17:05:12.396528  5452 solver.cpp:218] Iteration 104700 (11.703 iter/s, 8.54485s/100 iters), loss = 0.0508878
I1107 17:05:12.396528  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:05:12.396528  5452 solver.cpp:237]     Train net output #1: loss = 0.0508877 (* 1 = 0.0508877 loss)
I1107 17:05:12.396528  5452 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1107 17:05:20.933280  5452 solver.cpp:218] Iteration 104800 (11.7146 iter/s, 8.53633s/100 iters), loss = 0.073515
I1107 17:05:20.933280  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:05:20.933280  5452 solver.cpp:237]     Train net output #1: loss = 0.0735148 (* 1 = 0.0735148 loss)
I1107 17:05:20.933280  5452 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1107 17:05:29.524570  5452 solver.cpp:218] Iteration 104900 (11.6412 iter/s, 8.59021s/100 iters), loss = 0.0411188
I1107 17:05:29.524570  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:05:29.524570  5452 solver.cpp:237]     Train net output #1: loss = 0.0411186 (* 1 = 0.0411186 loss)
I1107 17:05:29.524570  5452 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1107 17:05:37.674579 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:05:38.012601  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_105000.caffemodel
I1107 17:05:38.042601  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_105000.solverstate
I1107 17:05:38.052106  5452 solver.cpp:330] Iteration 105000, Testing net (#0)
I1107 17:05:38.052106  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:05:40.058899 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:05:40.138902  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9165
I1107 17:05:40.138902  5452 solver.cpp:397]     Test net output #1: loss = 0.286403 (* 1 = 0.286403 loss)
I1107 17:05:40.219908  5452 solver.cpp:218] Iteration 105000 (9.35032 iter/s, 10.6948s/100 iters), loss = 0.0617051
I1107 17:05:40.219908  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:05:40.219908  5452 solver.cpp:237]     Train net output #1: loss = 0.0617049 (* 1 = 0.0617049 loss)
I1107 17:05:40.219908  5452 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1107 17:05:48.773157  5452 solver.cpp:218] Iteration 105100 (11.6917 iter/s, 8.55311s/100 iters), loss = 0.0611946
I1107 17:05:48.773157  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 17:05:48.773157  5452 solver.cpp:237]     Train net output #1: loss = 0.0611944 (* 1 = 0.0611944 loss)
I1107 17:05:48.773157  5452 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1107 17:05:57.324194  5452 solver.cpp:218] Iteration 105200 (11.6947 iter/s, 8.55087s/100 iters), loss = 0.049952
I1107 17:05:57.325194  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:05:57.325194  5452 solver.cpp:237]     Train net output #1: loss = 0.0499518 (* 1 = 0.0499518 loss)
I1107 17:05:57.325194  5452 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1107 17:06:05.966114  5452 solver.cpp:218] Iteration 105300 (11.5724 iter/s, 8.64128s/100 iters), loss = 0.02598
I1107 17:06:05.967114  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:06:05.967114  5452 solver.cpp:237]     Train net output #1: loss = 0.0259798 (* 1 = 0.0259798 loss)
I1107 17:06:05.967114  5452 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1107 17:06:14.560828  5452 solver.cpp:218] Iteration 105400 (11.6365 iter/s, 8.59363s/100 iters), loss = 0.0598617
I1107 17:06:14.560828  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:06:14.560828  5452 solver.cpp:237]     Train net output #1: loss = 0.0598615 (* 1 = 0.0598615 loss)
I1107 17:06:14.560828  5452 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1107 17:06:22.751801 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:06:23.088817  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_105500.caffemodel
I1107 17:06:23.118321  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_105500.solverstate
I1107 17:06:23.126826  5452 solver.cpp:330] Iteration 105500, Testing net (#0)
I1107 17:06:23.126826  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:06:25.132169 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:06:25.211668  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1107 17:06:25.211668  5452 solver.cpp:397]     Test net output #1: loss = 0.281721 (* 1 = 0.281721 loss)
I1107 17:06:25.293170  5452 solver.cpp:218] Iteration 105500 (9.31795 iter/s, 10.732s/100 iters), loss = 0.0498838
I1107 17:06:25.293170  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:06:25.293170  5452 solver.cpp:237]     Train net output #1: loss = 0.0498836 (* 1 = 0.0498836 loss)
I1107 17:06:25.293170  5452 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1107 17:06:33.893045  5452 solver.cpp:218] Iteration 105600 (11.6292 iter/s, 8.59904s/100 iters), loss = 0.0522822
I1107 17:06:33.893045  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:06:33.893045  5452 solver.cpp:237]     Train net output #1: loss = 0.052282 (* 1 = 0.052282 loss)
I1107 17:06:33.893045  5452 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1107 17:06:42.489926  5452 solver.cpp:218] Iteration 105700 (11.632 iter/s, 8.59696s/100 iters), loss = 0.0315461
I1107 17:06:42.489926  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:06:42.490926  5452 solver.cpp:237]     Train net output #1: loss = 0.0315459 (* 1 = 0.0315459 loss)
I1107 17:06:42.490926  5452 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1107 17:06:51.057215  5452 solver.cpp:218] Iteration 105800 (11.6733 iter/s, 8.56654s/100 iters), loss = 0.0378862
I1107 17:06:51.057215  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:06:51.057215  5452 solver.cpp:237]     Train net output #1: loss = 0.037886 (* 1 = 0.037886 loss)
I1107 17:06:51.057215  5452 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1107 17:06:59.583920  5452 solver.cpp:218] Iteration 105900 (11.7285 iter/s, 8.52624s/100 iters), loss = 0.0551956
I1107 17:06:59.583920  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:06:59.583920  5452 solver.cpp:237]     Train net output #1: loss = 0.0551954 (* 1 = 0.0551954 loss)
I1107 17:06:59.583920  5452 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1107 17:07:07.791419 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:07:08.128540  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_106000.caffemodel
I1107 17:07:08.159060  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_106000.solverstate
I1107 17:07:08.168059  5452 solver.cpp:330] Iteration 106000, Testing net (#0)
I1107 17:07:08.168059  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:07:10.157636 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:07:10.236668  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1107 17:07:10.236668  5452 solver.cpp:397]     Test net output #1: loss = 0.284678 (* 1 = 0.284678 loss)
I1107 17:07:10.318166  5452 solver.cpp:218] Iteration 106000 (9.31704 iter/s, 10.733s/100 iters), loss = 0.0350624
I1107 17:07:10.318166  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:07:10.318166  5452 solver.cpp:237]     Train net output #1: loss = 0.0350622 (* 1 = 0.0350622 loss)
I1107 17:07:10.318166  5452 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1107 17:07:18.880758  5452 solver.cpp:218] Iteration 106100 (11.6785 iter/s, 8.56274s/100 iters), loss = 0.0982827
I1107 17:07:18.880758  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:07:18.880758  5452 solver.cpp:237]     Train net output #1: loss = 0.0982825 (* 1 = 0.0982825 loss)
I1107 17:07:18.880758  5452 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1107 17:07:27.570562  5452 solver.cpp:218] Iteration 106200 (11.5096 iter/s, 8.68842s/100 iters), loss = 0.042184
I1107 17:07:27.570562  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:07:27.570562  5452 solver.cpp:237]     Train net output #1: loss = 0.0421838 (* 1 = 0.0421838 loss)
I1107 17:07:27.570562  5452 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1107 17:07:36.258615  5452 solver.cpp:218] Iteration 106300 (11.5099 iter/s, 8.68818s/100 iters), loss = 0.0358991
I1107 17:07:36.258615  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:07:36.258615  5452 solver.cpp:237]     Train net output #1: loss = 0.0358989 (* 1 = 0.0358989 loss)
I1107 17:07:36.258615  5452 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1107 17:07:44.919795  5452 solver.cpp:218] Iteration 106400 (11.5464 iter/s, 8.66069s/100 iters), loss = 0.0466266
I1107 17:07:44.919795  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:07:44.920795  5452 solver.cpp:237]     Train net output #1: loss = 0.0466264 (* 1 = 0.0466264 loss)
I1107 17:07:44.920795  5452 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1107 17:07:53.153795 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:07:53.495832  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_106500.caffemodel
I1107 17:07:53.524816  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_106500.solverstate
I1107 17:07:53.534355  5452 solver.cpp:330] Iteration 106500, Testing net (#0)
I1107 17:07:53.534855  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:07:55.560120 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:07:55.641120  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9208
I1107 17:07:55.641120  5452 solver.cpp:397]     Test net output #1: loss = 0.278346 (* 1 = 0.278346 loss)
I1107 17:07:55.724061  5452 solver.cpp:218] Iteration 106500 (9.2567 iter/s, 10.803s/100 iters), loss = 0.0651373
I1107 17:07:55.724061  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:07:55.724061  5452 solver.cpp:237]     Train net output #1: loss = 0.0651371 (* 1 = 0.0651371 loss)
I1107 17:07:55.724061  5452 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1107 17:08:04.428269  5452 solver.cpp:218] Iteration 106600 (11.4895 iter/s, 8.70362s/100 iters), loss = 0.0811236
I1107 17:08:04.428269  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 17:08:04.428269  5452 solver.cpp:237]     Train net output #1: loss = 0.0811233 (* 1 = 0.0811233 loss)
I1107 17:08:04.428269  5452 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1107 17:08:13.095953  5452 solver.cpp:218] Iteration 106700 (11.5374 iter/s, 8.66745s/100 iters), loss = 0.0493488
I1107 17:08:13.095953  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:08:13.095953  5452 solver.cpp:237]     Train net output #1: loss = 0.0493486 (* 1 = 0.0493486 loss)
I1107 17:08:13.095953  5452 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1107 17:08:21.720083  5452 solver.cpp:218] Iteration 106800 (11.5963 iter/s, 8.62342s/100 iters), loss = 0.0488626
I1107 17:08:21.720083  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:08:21.720083  5452 solver.cpp:237]     Train net output #1: loss = 0.0488624 (* 1 = 0.0488624 loss)
I1107 17:08:21.720083  5452 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1107 17:08:30.386741  5452 solver.cpp:218] Iteration 106900 (11.5391 iter/s, 8.66622s/100 iters), loss = 0.0369997
I1107 17:08:30.386741  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:08:30.386741  5452 solver.cpp:237]     Train net output #1: loss = 0.0369995 (* 1 = 0.0369995 loss)
I1107 17:08:30.386741  5452 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1107 17:08:38.608711 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:08:38.950260  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_107000.caffemodel
I1107 17:08:38.981258  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_107000.solverstate
I1107 17:08:38.990259  5452 solver.cpp:330] Iteration 107000, Testing net (#0)
I1107 17:08:38.990259  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:08:41.010916 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:08:41.091418  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9198
I1107 17:08:41.091418  5452 solver.cpp:397]     Test net output #1: loss = 0.282872 (* 1 = 0.282872 loss)
I1107 17:08:41.173421  5452 solver.cpp:218] Iteration 107000 (9.27127 iter/s, 10.786s/100 iters), loss = 0.038674
I1107 17:08:41.173421  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:08:41.173421  5452 solver.cpp:237]     Train net output #1: loss = 0.0386738 (* 1 = 0.0386738 loss)
I1107 17:08:41.173421  5452 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1107 17:08:49.724720  5452 solver.cpp:218] Iteration 107100 (11.695 iter/s, 8.55069s/100 iters), loss = 0.101362
I1107 17:08:49.724720  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 17:08:49.724720  5452 solver.cpp:237]     Train net output #1: loss = 0.101361 (* 1 = 0.101361 loss)
I1107 17:08:49.724720  5452 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1107 17:08:58.282148  5452 solver.cpp:218] Iteration 107200 (11.6858 iter/s, 8.55742s/100 iters), loss = 0.0469381
I1107 17:08:58.282148  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:08:58.282148  5452 solver.cpp:237]     Train net output #1: loss = 0.0469378 (* 1 = 0.0469378 loss)
I1107 17:08:58.282148  5452 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1107 17:09:06.827008  5452 solver.cpp:218] Iteration 107300 (11.704 iter/s, 8.54412s/100 iters), loss = 0.0282857
I1107 17:09:06.827008  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:09:06.827008  5452 solver.cpp:237]     Train net output #1: loss = 0.0282854 (* 1 = 0.0282854 loss)
I1107 17:09:06.827008  5452 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1107 17:09:15.402973  5452 solver.cpp:218] Iteration 107400 (11.6617 iter/s, 8.57506s/100 iters), loss = 0.0491525
I1107 17:09:15.402973  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:09:15.402973  5452 solver.cpp:237]     Train net output #1: loss = 0.0491522 (* 1 = 0.0491522 loss)
I1107 17:09:15.402973  5452 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1107 17:09:23.598769 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:09:23.943809  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_107500.caffemodel
I1107 17:09:23.974809  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_107500.solverstate
I1107 17:09:23.983809  5452 solver.cpp:330] Iteration 107500, Testing net (#0)
I1107 17:09:23.984809  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:09:26.006027 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:09:26.087033  5452 solver.cpp:397]     Test net output #0: accuracy = 0.922
I1107 17:09:26.087033  5452 solver.cpp:397]     Test net output #1: loss = 0.280088 (* 1 = 0.280088 loss)
I1107 17:09:26.171038  5452 solver.cpp:218] Iteration 107500 (9.2872 iter/s, 10.7675s/100 iters), loss = 0.0522917
I1107 17:09:26.171038  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:09:26.171038  5452 solver.cpp:237]     Train net output #1: loss = 0.0522914 (* 1 = 0.0522914 loss)
I1107 17:09:26.171038  5452 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1107 17:09:34.710840  5452 solver.cpp:218] Iteration 107600 (11.7106 iter/s, 8.53925s/100 iters), loss = 0.035415
I1107 17:09:34.710840  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:09:34.710840  5452 solver.cpp:237]     Train net output #1: loss = 0.0354148 (* 1 = 0.0354148 loss)
I1107 17:09:34.710840  5452 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1107 17:09:43.304507  5452 solver.cpp:218] Iteration 107700 (11.6366 iter/s, 8.59358s/100 iters), loss = 0.0454316
I1107 17:09:43.304507  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:09:43.304507  5452 solver.cpp:237]     Train net output #1: loss = 0.0454313 (* 1 = 0.0454313 loss)
I1107 17:09:43.304507  5452 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1107 17:09:51.990376  5452 solver.cpp:218] Iteration 107800 (11.5143 iter/s, 8.68482s/100 iters), loss = 0.0264899
I1107 17:09:51.990376  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:09:51.990376  5452 solver.cpp:237]     Train net output #1: loss = 0.0264896 (* 1 = 0.0264896 loss)
I1107 17:09:51.990376  5452 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1107 17:10:00.626834  5452 solver.cpp:218] Iteration 107900 (11.5796 iter/s, 8.63588s/100 iters), loss = 0.0558594
I1107 17:10:00.626834  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:10:00.626834  5452 solver.cpp:237]     Train net output #1: loss = 0.0558592 (* 1 = 0.0558592 loss)
I1107 17:10:00.626834  5452 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1107 17:10:08.832866 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:10:09.179883  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_108000.caffemodel
I1107 17:10:09.209883  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_108000.solverstate
I1107 17:10:09.218899  5452 solver.cpp:330] Iteration 108000, Testing net (#0)
I1107 17:10:09.218899  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:10:11.254415 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:10:11.336427  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9178
I1107 17:10:11.336427  5452 solver.cpp:397]     Test net output #1: loss = 0.287176 (* 1 = 0.287176 loss)
I1107 17:10:11.420428  5452 solver.cpp:218] Iteration 108000 (9.26529 iter/s, 10.793s/100 iters), loss = 0.0371139
I1107 17:10:11.420428  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:10:11.420428  5452 solver.cpp:237]     Train net output #1: loss = 0.0371137 (* 1 = 0.0371137 loss)
I1107 17:10:11.420428  5452 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1107 17:10:19.999162  5452 solver.cpp:218] Iteration 108100 (11.6577 iter/s, 8.57801s/100 iters), loss = 0.0829256
I1107 17:10:19.999162  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:10:19.999162  5452 solver.cpp:237]     Train net output #1: loss = 0.0829253 (* 1 = 0.0829253 loss)
I1107 17:10:19.999162  5452 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1107 17:10:28.663283  5452 solver.cpp:218] Iteration 108200 (11.5426 iter/s, 8.66353s/100 iters), loss = 0.0600874
I1107 17:10:28.663283  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:10:28.663283  5452 solver.cpp:237]     Train net output #1: loss = 0.0600871 (* 1 = 0.0600871 loss)
I1107 17:10:28.663283  5452 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1107 17:10:37.357424  5452 solver.cpp:218] Iteration 108300 (11.5018 iter/s, 8.69427s/100 iters), loss = 0.0281617
I1107 17:10:37.357424  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:10:37.357424  5452 solver.cpp:237]     Train net output #1: loss = 0.0281615 (* 1 = 0.0281615 loss)
I1107 17:10:37.357424  5452 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1107 17:10:45.875357  5452 solver.cpp:218] Iteration 108400 (11.7412 iter/s, 8.51701s/100 iters), loss = 0.0535553
I1107 17:10:45.875357  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:10:45.875357  5452 solver.cpp:237]     Train net output #1: loss = 0.0535551 (* 1 = 0.0535551 loss)
I1107 17:10:45.875357  5452 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1107 17:10:54.069941 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:10:54.410490  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_108500.caffemodel
I1107 17:10:54.440495  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_108500.solverstate
I1107 17:10:54.449491  5452 solver.cpp:330] Iteration 108500, Testing net (#0)
I1107 17:10:54.449491  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:10:56.461683 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:10:56.543696  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9202
I1107 17:10:56.543696  5452 solver.cpp:397]     Test net output #1: loss = 0.287216 (* 1 = 0.287216 loss)
I1107 17:10:56.626714  5452 solver.cpp:218] Iteration 108500 (9.30153 iter/s, 10.7509s/100 iters), loss = 0.0529842
I1107 17:10:56.626714  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:10:56.626714  5452 solver.cpp:237]     Train net output #1: loss = 0.052984 (* 1 = 0.052984 loss)
I1107 17:10:56.626714  5452 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1107 17:11:05.303339  5452 solver.cpp:218] Iteration 108600 (11.5269 iter/s, 8.67539s/100 iters), loss = 0.0670525
I1107 17:11:05.303339  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1107 17:11:05.303339  5452 solver.cpp:237]     Train net output #1: loss = 0.0670522 (* 1 = 0.0670522 loss)
I1107 17:11:05.303339  5452 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1107 17:11:13.918212  5452 solver.cpp:218] Iteration 108700 (11.6084 iter/s, 8.61443s/100 iters), loss = 0.0403773
I1107 17:11:13.918212  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:11:13.918212  5452 solver.cpp:237]     Train net output #1: loss = 0.0403771 (* 1 = 0.0403771 loss)
I1107 17:11:13.918212  5452 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1107 17:11:22.657655  5452 solver.cpp:218] Iteration 108800 (11.4432 iter/s, 8.73884s/100 iters), loss = 0.0513309
I1107 17:11:22.657655  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:11:22.657655  5452 solver.cpp:237]     Train net output #1: loss = 0.0513306 (* 1 = 0.0513306 loss)
I1107 17:11:22.657655  5452 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1107 17:11:31.306808  5452 solver.cpp:218] Iteration 108900 (11.563 iter/s, 8.6483s/100 iters), loss = 0.0331109
I1107 17:11:31.306808  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:11:31.306808  5452 solver.cpp:237]     Train net output #1: loss = 0.0331106 (* 1 = 0.0331106 loss)
I1107 17:11:31.306808  5452 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1107 17:11:39.436228 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:11:39.772253  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_109000.caffemodel
I1107 17:11:39.803262  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_109000.solverstate
I1107 17:11:39.812263  5452 solver.cpp:330] Iteration 109000, Testing net (#0)
I1107 17:11:39.812263  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:11:41.818460 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:11:41.899466  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 17:11:41.899466  5452 solver.cpp:397]     Test net output #1: loss = 0.285612 (* 1 = 0.285612 loss)
I1107 17:11:41.981465  5452 solver.cpp:218] Iteration 109000 (9.36813 iter/s, 10.6745s/100 iters), loss = 0.0565056
I1107 17:11:41.981966  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:11:41.981966  5452 solver.cpp:237]     Train net output #1: loss = 0.0565053 (* 1 = 0.0565053 loss)
I1107 17:11:41.981966  5452 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1107 17:11:50.539453  5452 solver.cpp:218] Iteration 109100 (11.6861 iter/s, 8.55716s/100 iters), loss = 0.0883858
I1107 17:11:50.539453  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:11:50.539453  5452 solver.cpp:237]     Train net output #1: loss = 0.0883855 (* 1 = 0.0883855 loss)
I1107 17:11:50.539453  5452 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1107 17:11:59.106812  5452 solver.cpp:218] Iteration 109200 (11.673 iter/s, 8.56676s/100 iters), loss = 0.0537566
I1107 17:11:59.106812  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:11:59.106812  5452 solver.cpp:237]     Train net output #1: loss = 0.0537564 (* 1 = 0.0537564 loss)
I1107 17:11:59.106812  5452 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1107 17:12:07.648746  5452 solver.cpp:218] Iteration 109300 (11.7074 iter/s, 8.5416s/100 iters), loss = 0.0284636
I1107 17:12:07.648746  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:12:07.648746  5452 solver.cpp:237]     Train net output #1: loss = 0.0284633 (* 1 = 0.0284633 loss)
I1107 17:12:07.648746  5452 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1107 17:12:16.195529  5452 solver.cpp:218] Iteration 109400 (11.7005 iter/s, 8.54661s/100 iters), loss = 0.0311423
I1107 17:12:16.195529  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:12:16.195529  5452 solver.cpp:237]     Train net output #1: loss = 0.0311421 (* 1 = 0.0311421 loss)
I1107 17:12:16.195529  5452 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1107 17:12:24.337260 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:12:24.673787  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_109500.caffemodel
I1107 17:12:24.704787  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_109500.solverstate
I1107 17:12:24.714788  5452 solver.cpp:330] Iteration 109500, Testing net (#0)
I1107 17:12:24.714788  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:12:26.713992 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:12:26.794500  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 17:12:26.794500  5452 solver.cpp:397]     Test net output #1: loss = 0.290725 (* 1 = 0.290725 loss)
I1107 17:12:26.876003  5452 solver.cpp:218] Iteration 109500 (9.36387 iter/s, 10.6793s/100 iters), loss = 0.0500197
I1107 17:12:26.876003  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:12:26.876003  5452 solver.cpp:237]     Train net output #1: loss = 0.0500194 (* 1 = 0.0500194 loss)
I1107 17:12:26.876003  5452 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1107 17:12:35.397855  5452 solver.cpp:218] Iteration 109600 (11.7347 iter/s, 8.52176s/100 iters), loss = 0.068951
I1107 17:12:35.397855  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:12:35.397855  5452 solver.cpp:237]     Train net output #1: loss = 0.0689507 (* 1 = 0.0689507 loss)
I1107 17:12:35.397855  5452 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1107 17:12:43.947778  5452 solver.cpp:218] Iteration 109700 (11.6961 iter/s, 8.54983s/100 iters), loss = 0.0383737
I1107 17:12:43.948779  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:12:43.948779  5452 solver.cpp:237]     Train net output #1: loss = 0.0383734 (* 1 = 0.0383734 loss)
I1107 17:12:43.948779  5452 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1107 17:12:52.531075  5452 solver.cpp:218] Iteration 109800 (11.6521 iter/s, 8.58213s/100 iters), loss = 0.0537466
I1107 17:12:52.531075  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:12:52.531075  5452 solver.cpp:237]     Train net output #1: loss = 0.0537463 (* 1 = 0.0537463 loss)
I1107 17:12:52.531075  5452 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1107 17:13:01.064455  5452 solver.cpp:218] Iteration 109900 (11.7194 iter/s, 8.53283s/100 iters), loss = 0.0376904
I1107 17:13:01.064455  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:13:01.064455  5452 solver.cpp:237]     Train net output #1: loss = 0.0376901 (* 1 = 0.0376901 loss)
I1107 17:13:01.064455  5452 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1107 17:13:09.165830 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:13:09.503845  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_110000.caffemodel
I1107 17:13:09.534348  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_110000.solverstate
I1107 17:13:09.543349  5452 solver.cpp:330] Iteration 110000, Testing net (#0)
I1107 17:13:09.543349  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:13:11.542510 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:13:11.621011  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9195
I1107 17:13:11.621011  5452 solver.cpp:397]     Test net output #1: loss = 0.282665 (* 1 = 0.282665 loss)
I1107 17:13:11.702014  5452 solver.cpp:218] Iteration 110000 (9.40085 iter/s, 10.6373s/100 iters), loss = 0.0475498
I1107 17:13:11.702014  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:13:11.702014  5452 solver.cpp:237]     Train net output #1: loss = 0.0475495 (* 1 = 0.0475495 loss)
I1107 17:13:11.702014  5452 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1107 17:13:20.338243  5452 solver.cpp:218] Iteration 110100 (11.5802 iter/s, 8.63546s/100 iters), loss = 0.0452705
I1107 17:13:20.338243  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:13:20.338243  5452 solver.cpp:237]     Train net output #1: loss = 0.0452702 (* 1 = 0.0452702 loss)
I1107 17:13:20.338243  5452 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1107 17:13:28.946074  5452 solver.cpp:218] Iteration 110200 (11.6185 iter/s, 8.60699s/100 iters), loss = 0.0594826
I1107 17:13:28.946074  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:13:28.946074  5452 solver.cpp:237]     Train net output #1: loss = 0.0594823 (* 1 = 0.0594823 loss)
I1107 17:13:28.946074  5452 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1107 17:13:37.493489  5452 solver.cpp:218] Iteration 110300 (11.6997 iter/s, 8.54724s/100 iters), loss = 0.054087
I1107 17:13:37.493489  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:13:37.493489  5452 solver.cpp:237]     Train net output #1: loss = 0.0540867 (* 1 = 0.0540867 loss)
I1107 17:13:37.493489  5452 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1107 17:13:46.120370  5452 solver.cpp:218] Iteration 110400 (11.5932 iter/s, 8.62572s/100 iters), loss = 0.0556795
I1107 17:13:46.120370  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:13:46.120370  5452 solver.cpp:237]     Train net output #1: loss = 0.0556791 (* 1 = 0.0556791 loss)
I1107 17:13:46.120370  5452 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1107 17:13:54.245105 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:13:54.582137  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_110500.caffemodel
I1107 17:13:54.612138  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_110500.solverstate
I1107 17:13:54.621139  5452 solver.cpp:330] Iteration 110500, Testing net (#0)
I1107 17:13:54.621139  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:13:56.618309 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:13:56.699316  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1107 17:13:56.699316  5452 solver.cpp:397]     Test net output #1: loss = 0.285815 (* 1 = 0.285815 loss)
I1107 17:13:56.780820  5452 solver.cpp:218] Iteration 110500 (9.38111 iter/s, 10.6597s/100 iters), loss = 0.0506548
I1107 17:13:56.780820  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:13:56.780820  5452 solver.cpp:237]     Train net output #1: loss = 0.0506545 (* 1 = 0.0506545 loss)
I1107 17:13:56.780820  5452 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1107 17:14:05.319612  5452 solver.cpp:218] Iteration 110600 (11.7116 iter/s, 8.53851s/100 iters), loss = 0.0617211
I1107 17:14:05.319612  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:14:05.319612  5452 solver.cpp:237]     Train net output #1: loss = 0.0617208 (* 1 = 0.0617208 loss)
I1107 17:14:05.319612  5452 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1107 17:14:13.913867  5452 solver.cpp:218] Iteration 110700 (11.6358 iter/s, 8.59417s/100 iters), loss = 0.032365
I1107 17:14:13.913867  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:14:13.914868  5452 solver.cpp:237]     Train net output #1: loss = 0.0323647 (* 1 = 0.0323647 loss)
I1107 17:14:13.914868  5452 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1107 17:14:22.422629  5452 solver.cpp:218] Iteration 110800 (11.7538 iter/s, 8.50788s/100 iters), loss = 0.0555792
I1107 17:14:22.422629  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:14:22.422629  5452 solver.cpp:237]     Train net output #1: loss = 0.0555788 (* 1 = 0.0555788 loss)
I1107 17:14:22.422629  5452 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1107 17:14:30.945466  5452 solver.cpp:218] Iteration 110900 (11.7344 iter/s, 8.52197s/100 iters), loss = 0.0355458
I1107 17:14:30.945466  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:14:30.945466  5452 solver.cpp:237]     Train net output #1: loss = 0.0355455 (* 1 = 0.0355455 loss)
I1107 17:14:30.945466  5452 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1107 17:14:39.059473 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:14:39.396507  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_111000.caffemodel
I1107 17:14:39.427515  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_111000.solverstate
I1107 17:14:39.435516  5452 solver.cpp:330] Iteration 111000, Testing net (#0)
I1107 17:14:39.436517  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:14:41.439033 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:14:41.520534  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 17:14:41.520534  5452 solver.cpp:397]     Test net output #1: loss = 0.29007 (* 1 = 0.29007 loss)
I1107 17:14:41.601035  5452 solver.cpp:218] Iteration 111000 (9.38473 iter/s, 10.6556s/100 iters), loss = 0.0380128
I1107 17:14:41.601035  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:14:41.601035  5452 solver.cpp:237]     Train net output #1: loss = 0.0380125 (* 1 = 0.0380125 loss)
I1107 17:14:41.601035  5452 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1107 17:14:50.168932  5452 solver.cpp:218] Iteration 111100 (11.6725 iter/s, 8.56717s/100 iters), loss = 0.0512545
I1107 17:14:50.168932  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:14:50.168932  5452 solver.cpp:237]     Train net output #1: loss = 0.0512542 (* 1 = 0.0512542 loss)
I1107 17:14:50.168932  5452 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1107 17:14:58.720573  5452 solver.cpp:218] Iteration 111200 (11.6946 iter/s, 8.55095s/100 iters), loss = 0.0399237
I1107 17:14:58.720573  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:14:58.721073  5452 solver.cpp:237]     Train net output #1: loss = 0.0399233 (* 1 = 0.0399233 loss)
I1107 17:14:58.721073  5452 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1107 17:15:07.351353  5452 solver.cpp:218] Iteration 111300 (11.5876 iter/s, 8.62991s/100 iters), loss = 0.0440397
I1107 17:15:07.351353  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:15:07.351353  5452 solver.cpp:237]     Train net output #1: loss = 0.0440394 (* 1 = 0.0440394 loss)
I1107 17:15:07.351353  5452 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1107 17:15:16.047693  5452 solver.cpp:218] Iteration 111400 (11.4989 iter/s, 8.69645s/100 iters), loss = 0.0341097
I1107 17:15:16.047693  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:15:16.047693  5452 solver.cpp:237]     Train net output #1: loss = 0.0341093 (* 1 = 0.0341093 loss)
I1107 17:15:16.047693  5452 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1107 17:15:24.142421 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:15:24.477452  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_111500.caffemodel
I1107 17:15:24.506451  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_111500.solverstate
I1107 17:15:24.515452  5452 solver.cpp:330] Iteration 111500, Testing net (#0)
I1107 17:15:24.515452  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:15:26.498642 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:15:26.577647  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 17:15:26.577647  5452 solver.cpp:397]     Test net output #1: loss = 0.289611 (* 1 = 0.289611 loss)
I1107 17:15:26.658668  5452 solver.cpp:218] Iteration 111500 (9.42472 iter/s, 10.6104s/100 iters), loss = 0.0451373
I1107 17:15:26.658668  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:15:26.658668  5452 solver.cpp:237]     Train net output #1: loss = 0.045137 (* 1 = 0.045137 loss)
I1107 17:15:26.658668  5452 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1107 17:15:35.206713  5452 solver.cpp:218] Iteration 111600 (11.7004 iter/s, 8.54672s/100 iters), loss = 0.0547923
I1107 17:15:35.206713  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:15:35.206713  5452 solver.cpp:237]     Train net output #1: loss = 0.0547919 (* 1 = 0.0547919 loss)
I1107 17:15:35.206713  5452 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1107 17:15:43.773130  5452 solver.cpp:218] Iteration 111700 (11.6747 iter/s, 8.56557s/100 iters), loss = 0.0262031
I1107 17:15:43.773130  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:15:43.773130  5452 solver.cpp:237]     Train net output #1: loss = 0.0262027 (* 1 = 0.0262027 loss)
I1107 17:15:43.773130  5452 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1107 17:15:52.344229  5452 solver.cpp:218] Iteration 111800 (11.6674 iter/s, 8.5709s/100 iters), loss = 0.0592891
I1107 17:15:52.344229  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:15:52.344229  5452 solver.cpp:237]     Train net output #1: loss = 0.0592888 (* 1 = 0.0592888 loss)
I1107 17:15:52.344229  5452 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1107 17:16:01.108464  5452 solver.cpp:218] Iteration 111900 (11.411 iter/s, 8.76347s/100 iters), loss = 0.0391626
I1107 17:16:01.108464  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:16:01.108464  5452 solver.cpp:237]     Train net output #1: loss = 0.0391623 (* 1 = 0.0391623 loss)
I1107 17:16:01.108464  5452 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1107 17:16:09.418911 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:16:09.763773  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_112000.caffemodel
I1107 17:16:09.793773  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_112000.solverstate
I1107 17:16:09.803788  5452 solver.cpp:330] Iteration 112000, Testing net (#0)
I1107 17:16:09.804771  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:16:11.822252 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:16:11.903271  5452 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 17:16:11.903271  5452 solver.cpp:397]     Test net output #1: loss = 0.284463 (* 1 = 0.284463 loss)
I1107 17:16:11.984966  5452 solver.cpp:218] Iteration 112000 (9.19406 iter/s, 10.8766s/100 iters), loss = 0.0309629
I1107 17:16:11.984966  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:16:11.984966  5452 solver.cpp:237]     Train net output #1: loss = 0.0309625 (* 1 = 0.0309625 loss)
I1107 17:16:11.984966  5452 sgd_solver.cpp:105] Iteration 112000, lr = 0.001
I1107 17:16:20.638837  5452 solver.cpp:218] Iteration 112100 (11.5568 iter/s, 8.6529s/100 iters), loss = 0.0479608
I1107 17:16:20.638837  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:16:20.638837  5452 solver.cpp:237]     Train net output #1: loss = 0.0479605 (* 1 = 0.0479605 loss)
I1107 17:16:20.638837  5452 sgd_solver.cpp:105] Iteration 112100, lr = 0.001
I1107 17:16:29.249024  5452 solver.cpp:218] Iteration 112200 (11.6151 iter/s, 8.60951s/100 iters), loss = 0.0449401
I1107 17:16:29.249024  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:16:29.249024  5452 solver.cpp:237]     Train net output #1: loss = 0.0449397 (* 1 = 0.0449397 loss)
I1107 17:16:29.249024  5452 sgd_solver.cpp:105] Iteration 112200, lr = 0.001
I1107 17:16:37.747236  5452 solver.cpp:218] Iteration 112300 (11.7678 iter/s, 8.49775s/100 iters), loss = 0.0409916
I1107 17:16:37.747236  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:16:37.747236  5452 solver.cpp:237]     Train net output #1: loss = 0.0409913 (* 1 = 0.0409913 loss)
I1107 17:16:37.747236  5452 sgd_solver.cpp:105] Iteration 112300, lr = 0.001
I1107 17:16:46.250147  5452 solver.cpp:218] Iteration 112400 (11.7602 iter/s, 8.50323s/100 iters), loss = 0.0331666
I1107 17:16:46.251147  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:16:46.251147  5452 solver.cpp:237]     Train net output #1: loss = 0.0331663 (* 1 = 0.0331663 loss)
I1107 17:16:46.251147  5452 sgd_solver.cpp:105] Iteration 112400, lr = 0.001
I1107 17:16:54.405833 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:16:54.741880  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_112500.caffemodel
I1107 17:16:54.771880  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_112500.solverstate
I1107 17:16:54.781880  5452 solver.cpp:330] Iteration 112500, Testing net (#0)
I1107 17:16:54.781880  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:16:56.770123 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:16:56.848134  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9204
I1107 17:16:56.848134  5452 solver.cpp:397]     Test net output #1: loss = 0.290692 (* 1 = 0.290692 loss)
I1107 17:16:56.930136  5452 solver.cpp:218] Iteration 112500 (9.36454 iter/s, 10.6786s/100 iters), loss = 0.051421
I1107 17:16:56.930136  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:16:56.930136  5452 solver.cpp:237]     Train net output #1: loss = 0.0514207 (* 1 = 0.0514207 loss)
I1107 17:16:56.930136  5452 sgd_solver.cpp:105] Iteration 112500, lr = 0.001
I1107 17:17:05.448812  5452 solver.cpp:218] Iteration 112600 (11.7395 iter/s, 8.51827s/100 iters), loss = 0.0343176
I1107 17:17:05.448812  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:17:05.448812  5452 solver.cpp:237]     Train net output #1: loss = 0.0343172 (* 1 = 0.0343172 loss)
I1107 17:17:05.448812  5452 sgd_solver.cpp:105] Iteration 112600, lr = 0.001
I1107 17:17:14.034704  5452 solver.cpp:218] Iteration 112700 (11.648 iter/s, 8.58516s/100 iters), loss = 0.0557247
I1107 17:17:14.034704  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:17:14.034704  5452 solver.cpp:237]     Train net output #1: loss = 0.0557243 (* 1 = 0.0557243 loss)
I1107 17:17:14.034704  5452 sgd_solver.cpp:105] Iteration 112700, lr = 0.001
I1107 17:17:22.597772  5452 solver.cpp:218] Iteration 112800 (11.6787 iter/s, 8.56262s/100 iters), loss = 0.0484503
I1107 17:17:22.598281  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:17:22.598281  5452 solver.cpp:237]     Train net output #1: loss = 0.04845 (* 1 = 0.04845 loss)
I1107 17:17:22.598281  5452 sgd_solver.cpp:105] Iteration 112800, lr = 0.001
I1107 17:17:31.148867  5452 solver.cpp:218] Iteration 112900 (11.6952 iter/s, 8.55054s/100 iters), loss = 0.0387263
I1107 17:17:31.148867  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:17:31.148867  5452 solver.cpp:237]     Train net output #1: loss = 0.038726 (* 1 = 0.038726 loss)
I1107 17:17:31.148867  5452 sgd_solver.cpp:105] Iteration 112900, lr = 0.001
I1107 17:17:39.239001 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:17:39.575218  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_113000.caffemodel
I1107 17:17:39.605737  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_113000.solverstate
I1107 17:17:39.614742  5452 solver.cpp:330] Iteration 113000, Testing net (#0)
I1107 17:17:39.614742  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:17:41.601058 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:17:41.680662  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9174
I1107 17:17:41.680662  5452 solver.cpp:397]     Test net output #1: loss = 0.29131 (* 1 = 0.29131 loss)
I1107 17:17:41.762186  5452 solver.cpp:218] Iteration 113000 (9.42218 iter/s, 10.6132s/100 iters), loss = 0.0548226
I1107 17:17:41.762186  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:17:41.763183  5452 solver.cpp:237]     Train net output #1: loss = 0.0548222 (* 1 = 0.0548222 loss)
I1107 17:17:41.763183  5452 sgd_solver.cpp:105] Iteration 113000, lr = 0.001
I1107 17:17:50.290419  5452 solver.cpp:218] Iteration 113100 (11.7276 iter/s, 8.52689s/100 iters), loss = 0.0425988
I1107 17:17:50.290419  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:17:50.290419  5452 solver.cpp:237]     Train net output #1: loss = 0.0425985 (* 1 = 0.0425985 loss)
I1107 17:17:50.290419  5452 sgd_solver.cpp:105] Iteration 113100, lr = 0.001
I1107 17:17:58.802816  5452 solver.cpp:218] Iteration 113200 (11.7475 iter/s, 8.51243s/100 iters), loss = 0.0452273
I1107 17:17:58.802816  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:17:58.802816  5452 solver.cpp:237]     Train net output #1: loss = 0.0452269 (* 1 = 0.0452269 loss)
I1107 17:17:58.802816  5452 sgd_solver.cpp:105] Iteration 113200, lr = 0.001
I1107 17:18:07.325703  5452 solver.cpp:218] Iteration 113300 (11.7347 iter/s, 8.52174s/100 iters), loss = 0.0335507
I1107 17:18:07.325703  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:18:07.325703  5452 solver.cpp:237]     Train net output #1: loss = 0.0335504 (* 1 = 0.0335504 loss)
I1107 17:18:07.325703  5452 sgd_solver.cpp:105] Iteration 113300, lr = 0.001
I1107 17:18:15.824182  5452 solver.cpp:218] Iteration 113400 (11.7667 iter/s, 8.49856s/100 iters), loss = 0.0366346
I1107 17:18:15.824182  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:18:15.824182  5452 solver.cpp:237]     Train net output #1: loss = 0.0366342 (* 1 = 0.0366342 loss)
I1107 17:18:15.824182  5452 sgd_solver.cpp:105] Iteration 113400, lr = 0.001
I1107 17:18:23.916115 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:18:24.251127  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_113500.caffemodel
I1107 17:18:24.281630  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_113500.solverstate
I1107 17:18:24.290632  5452 solver.cpp:330] Iteration 113500, Testing net (#0)
I1107 17:18:24.290632  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:18:26.272253 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:18:26.352262  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 17:18:26.352262  5452 solver.cpp:397]     Test net output #1: loss = 0.293334 (* 1 = 0.293334 loss)
I1107 17:18:26.433264  5452 solver.cpp:218] Iteration 113500 (9.42636 iter/s, 10.6086s/100 iters), loss = 0.0546575
I1107 17:18:26.433264  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:18:26.433264  5452 solver.cpp:237]     Train net output #1: loss = 0.0546572 (* 1 = 0.0546572 loss)
I1107 17:18:26.433264  5452 sgd_solver.cpp:105] Iteration 113500, lr = 0.001
I1107 17:18:34.939059  5452 solver.cpp:218] Iteration 113600 (11.7576 iter/s, 8.50513s/100 iters), loss = 0.0442606
I1107 17:18:34.939059  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:18:34.939059  5452 solver.cpp:237]     Train net output #1: loss = 0.0442602 (* 1 = 0.0442602 loss)
I1107 17:18:34.939059  5452 sgd_solver.cpp:105] Iteration 113600, lr = 0.001
I1107 17:18:43.448016  5452 solver.cpp:218] Iteration 113700 (11.7534 iter/s, 8.5082s/100 iters), loss = 0.0399501
I1107 17:18:43.448016  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:18:43.448016  5452 solver.cpp:237]     Train net output #1: loss = 0.0399497 (* 1 = 0.0399497 loss)
I1107 17:18:43.448016  5452 sgd_solver.cpp:105] Iteration 113700, lr = 0.001
I1107 17:18:51.954733  5452 solver.cpp:218] Iteration 113800 (11.7554 iter/s, 8.50676s/100 iters), loss = 0.0233232
I1107 17:18:51.955734  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:18:51.955734  5452 solver.cpp:237]     Train net output #1: loss = 0.0233228 (* 1 = 0.0233228 loss)
I1107 17:18:51.955734  5452 sgd_solver.cpp:105] Iteration 113800, lr = 0.001
I1107 17:19:00.458556  5452 solver.cpp:218] Iteration 113900 (11.7608 iter/s, 8.5028s/100 iters), loss = 0.0325067
I1107 17:19:00.458556  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:19:00.458556  5452 solver.cpp:237]     Train net output #1: loss = 0.0325063 (* 1 = 0.0325063 loss)
I1107 17:19:00.458556  5452 sgd_solver.cpp:105] Iteration 113900, lr = 0.001
I1107 17:19:08.634162 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:19:08.978315  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_114000.caffemodel
I1107 17:19:09.008317  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_114000.solverstate
I1107 17:19:09.018321  5452 solver.cpp:330] Iteration 114000, Testing net (#0)
I1107 17:19:09.018821  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:19:11.033577 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:19:11.114084  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9165
I1107 17:19:11.114084  5452 solver.cpp:397]     Test net output #1: loss = 0.297335 (* 1 = 0.297335 loss)
I1107 17:19:11.196583  5452 solver.cpp:218] Iteration 114000 (9.31303 iter/s, 10.7376s/100 iters), loss = 0.0460544
I1107 17:19:11.196583  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:19:11.196583  5452 solver.cpp:237]     Train net output #1: loss = 0.046054 (* 1 = 0.046054 loss)
I1107 17:19:11.196583  5452 sgd_solver.cpp:105] Iteration 114000, lr = 0.001
I1107 17:19:19.843514  5452 solver.cpp:218] Iteration 114100 (11.5658 iter/s, 8.64618s/100 iters), loss = 0.0467743
I1107 17:19:19.843514  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:19:19.843514  5452 solver.cpp:237]     Train net output #1: loss = 0.0467739 (* 1 = 0.0467739 loss)
I1107 17:19:19.843514  5452 sgd_solver.cpp:105] Iteration 114100, lr = 0.001
I1107 17:19:28.496634  5452 solver.cpp:218] Iteration 114200 (11.5569 iter/s, 8.65282s/100 iters), loss = 0.0516954
I1107 17:19:28.496634  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:19:28.496634  5452 solver.cpp:237]     Train net output #1: loss = 0.051695 (* 1 = 0.051695 loss)
I1107 17:19:28.496634  5452 sgd_solver.cpp:105] Iteration 114200, lr = 0.001
I1107 17:19:37.151413  5452 solver.cpp:218] Iteration 114300 (11.5549 iter/s, 8.65431s/100 iters), loss = 0.0410131
I1107 17:19:37.151413  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:19:37.151413  5452 solver.cpp:237]     Train net output #1: loss = 0.0410128 (* 1 = 0.0410128 loss)
I1107 17:19:37.151413  5452 sgd_solver.cpp:105] Iteration 114300, lr = 0.001
I1107 17:19:45.818171  5452 solver.cpp:218] Iteration 114400 (11.5398 iter/s, 8.66568s/100 iters), loss = 0.0326357
I1107 17:19:45.818171  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:19:45.818171  5452 solver.cpp:237]     Train net output #1: loss = 0.0326353 (* 1 = 0.0326353 loss)
I1107 17:19:45.818171  5452 sgd_solver.cpp:105] Iteration 114400, lr = 0.001
I1107 17:19:54.035533 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:19:54.375303  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_114500.caffemodel
I1107 17:19:54.405802  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_114500.solverstate
I1107 17:19:54.415318  5452 solver.cpp:330] Iteration 114500, Testing net (#0)
I1107 17:19:54.415318  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:19:56.419317 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:19:56.500313  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1107 17:19:56.500313  5452 solver.cpp:397]     Test net output #1: loss = 0.291619 (* 1 = 0.291619 loss)
I1107 17:19:56.582314  5452 solver.cpp:218] Iteration 114500 (9.29097 iter/s, 10.7631s/100 iters), loss = 0.0461407
I1107 17:19:56.582314  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:19:56.582314  5452 solver.cpp:237]     Train net output #1: loss = 0.0461403 (* 1 = 0.0461403 loss)
I1107 17:19:56.582314  5452 sgd_solver.cpp:105] Iteration 114500, lr = 0.001
I1107 17:20:05.182392  5452 solver.cpp:218] Iteration 114600 (11.6278 iter/s, 8.60008s/100 iters), loss = 0.044281
I1107 17:20:05.182392  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:20:05.182392  5452 solver.cpp:237]     Train net output #1: loss = 0.0442806 (* 1 = 0.0442806 loss)
I1107 17:20:05.182392  5452 sgd_solver.cpp:105] Iteration 114600, lr = 0.001
I1107 17:20:13.686847  5452 solver.cpp:218] Iteration 114700 (11.7596 iter/s, 8.50367s/100 iters), loss = 0.0297836
I1107 17:20:13.686847  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:20:13.686847  5452 solver.cpp:237]     Train net output #1: loss = 0.0297833 (* 1 = 0.0297833 loss)
I1107 17:20:13.686847  5452 sgd_solver.cpp:105] Iteration 114700, lr = 0.001
I1107 17:20:22.195927  5452 solver.cpp:218] Iteration 114800 (11.753 iter/s, 8.50849s/100 iters), loss = 0.0485542
I1107 17:20:22.195927  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:20:22.195927  5452 solver.cpp:237]     Train net output #1: loss = 0.0485538 (* 1 = 0.0485538 loss)
I1107 17:20:22.195927  5452 sgd_solver.cpp:105] Iteration 114800, lr = 0.001
I1107 17:20:30.702795  5452 solver.cpp:218] Iteration 114900 (11.7554 iter/s, 8.50674s/100 iters), loss = 0.0615947
I1107 17:20:30.702795  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:20:30.702795  5452 solver.cpp:237]     Train net output #1: loss = 0.0615943 (* 1 = 0.0615943 loss)
I1107 17:20:30.702795  5452 sgd_solver.cpp:105] Iteration 114900, lr = 0.001
I1107 17:20:38.790524 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:20:39.127571  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_115000.caffemodel
I1107 17:20:39.157573  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_115000.solverstate
I1107 17:20:39.166571  5452 solver.cpp:330] Iteration 115000, Testing net (#0)
I1107 17:20:39.167572  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:20:41.152860 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:20:41.231864  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 17:20:41.231864  5452 solver.cpp:397]     Test net output #1: loss = 0.296688 (* 1 = 0.296688 loss)
I1107 17:20:41.313369  5452 solver.cpp:218] Iteration 115000 (9.42524 iter/s, 10.6098s/100 iters), loss = 0.0566687
I1107 17:20:41.313868  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:20:41.313868  5452 solver.cpp:237]     Train net output #1: loss = 0.0566683 (* 1 = 0.0566683 loss)
I1107 17:20:41.313868  5452 sgd_solver.cpp:105] Iteration 115000, lr = 0.001
I1107 17:20:49.824580  5452 solver.cpp:218] Iteration 115100 (11.75 iter/s, 8.51064s/100 iters), loss = 0.0401418
I1107 17:20:49.824580  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:20:49.824580  5452 solver.cpp:237]     Train net output #1: loss = 0.0401414 (* 1 = 0.0401414 loss)
I1107 17:20:49.824580  5452 sgd_solver.cpp:105] Iteration 115100, lr = 0.001
I1107 17:20:58.330057  5452 solver.cpp:218] Iteration 115200 (11.7583 iter/s, 8.5046s/100 iters), loss = 0.0295916
I1107 17:20:58.330057  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:20:58.330057  5452 solver.cpp:237]     Train net output #1: loss = 0.0295912 (* 1 = 0.0295912 loss)
I1107 17:20:58.330057  5452 sgd_solver.cpp:105] Iteration 115200, lr = 0.001
I1107 17:21:06.848453  5452 solver.cpp:218] Iteration 115300 (11.7397 iter/s, 8.51809s/100 iters), loss = 0.0358621
I1107 17:21:06.848453  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:21:06.848453  5452 solver.cpp:237]     Train net output #1: loss = 0.0358617 (* 1 = 0.0358617 loss)
I1107 17:21:06.848453  5452 sgd_solver.cpp:105] Iteration 115300, lr = 0.001
I1107 17:21:15.367725  5452 solver.cpp:218] Iteration 115400 (11.7382 iter/s, 8.51918s/100 iters), loss = 0.0426264
I1107 17:21:15.367725  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:21:15.367725  5452 solver.cpp:237]     Train net output #1: loss = 0.042626 (* 1 = 0.042626 loss)
I1107 17:21:15.367725  5452 sgd_solver.cpp:105] Iteration 115400, lr = 0.001
I1107 17:21:23.477954 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:21:23.814999  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_115500.caffemodel
I1107 17:21:23.846005  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_115500.solverstate
I1107 17:21:23.855005  5452 solver.cpp:330] Iteration 115500, Testing net (#0)
I1107 17:21:23.855005  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:21:25.841455 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:21:25.921458  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1107 17:21:25.921458  5452 solver.cpp:397]     Test net output #1: loss = 0.291577 (* 1 = 0.291577 loss)
I1107 17:21:26.002460  5452 solver.cpp:218] Iteration 115500 (9.40361 iter/s, 10.6342s/100 iters), loss = 0.0358415
I1107 17:21:26.002460  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:21:26.003461  5452 solver.cpp:237]     Train net output #1: loss = 0.035841 (* 1 = 0.035841 loss)
I1107 17:21:26.003461  5452 sgd_solver.cpp:105] Iteration 115500, lr = 0.001
I1107 17:21:34.509289  5452 solver.cpp:218] Iteration 115600 (11.7566 iter/s, 8.50583s/100 iters), loss = 0.0442504
I1107 17:21:34.509289  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:21:34.509289  5452 solver.cpp:237]     Train net output #1: loss = 0.0442499 (* 1 = 0.0442499 loss)
I1107 17:21:34.509289  5452 sgd_solver.cpp:105] Iteration 115600, lr = 0.001
I1107 17:21:43.018878  5452 solver.cpp:218] Iteration 115700 (11.7528 iter/s, 8.50862s/100 iters), loss = 0.0255623
I1107 17:21:43.018878  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:21:43.018878  5452 solver.cpp:237]     Train net output #1: loss = 0.0255619 (* 1 = 0.0255619 loss)
I1107 17:21:43.018878  5452 sgd_solver.cpp:105] Iteration 115700, lr = 0.001
I1107 17:21:51.537065  5452 solver.cpp:218] Iteration 115800 (11.7392 iter/s, 8.5185s/100 iters), loss = 0.0529357
I1107 17:21:51.537065  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:21:51.537065  5452 solver.cpp:237]     Train net output #1: loss = 0.0529352 (* 1 = 0.0529352 loss)
I1107 17:21:51.537065  5452 sgd_solver.cpp:105] Iteration 115800, lr = 0.001
I1107 17:22:00.049540  5452 solver.cpp:218] Iteration 115900 (11.7491 iter/s, 8.51129s/100 iters), loss = 0.0386869
I1107 17:22:00.049540  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:22:00.049540  5452 solver.cpp:237]     Train net output #1: loss = 0.0386865 (* 1 = 0.0386865 loss)
I1107 17:22:00.049540  5452 sgd_solver.cpp:105] Iteration 115900, lr = 0.001
I1107 17:22:08.154299 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:22:08.491322  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_116000.caffemodel
I1107 17:22:08.518821  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_116000.solverstate
I1107 17:22:08.528321  5452 solver.cpp:330] Iteration 116000, Testing net (#0)
I1107 17:22:08.528321  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:22:10.516446 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:22:10.595454  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9176
I1107 17:22:10.595454  5452 solver.cpp:397]     Test net output #1: loss = 0.293944 (* 1 = 0.293944 loss)
I1107 17:22:10.677458  5452 solver.cpp:218] Iteration 116000 (9.40999 iter/s, 10.627s/100 iters), loss = 0.046357
I1107 17:22:10.677458  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:22:10.677458  5452 solver.cpp:237]     Train net output #1: loss = 0.0463566 (* 1 = 0.0463566 loss)
I1107 17:22:10.677458  5452 sgd_solver.cpp:105] Iteration 116000, lr = 0.001
I1107 17:22:19.184875  5452 solver.cpp:218] Iteration 116100 (11.7549 iter/s, 8.50707s/100 iters), loss = 0.0408273
I1107 17:22:19.184875  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:22:19.184875  5452 solver.cpp:237]     Train net output #1: loss = 0.0408269 (* 1 = 0.0408269 loss)
I1107 17:22:19.184875  5452 sgd_solver.cpp:105] Iteration 116100, lr = 0.001
I1107 17:22:27.688475  5452 solver.cpp:218] Iteration 116200 (11.7596 iter/s, 8.50372s/100 iters), loss = 0.0317043
I1107 17:22:27.688475  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:22:27.688475  5452 solver.cpp:237]     Train net output #1: loss = 0.0317039 (* 1 = 0.0317039 loss)
I1107 17:22:27.688475  5452 sgd_solver.cpp:105] Iteration 116200, lr = 0.001
I1107 17:22:36.191385  5452 solver.cpp:218] Iteration 116300 (11.7615 iter/s, 8.50228s/100 iters), loss = 0.0268655
I1107 17:22:36.191385  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:22:36.191385  5452 solver.cpp:237]     Train net output #1: loss = 0.0268651 (* 1 = 0.0268651 loss)
I1107 17:22:36.191385  5452 sgd_solver.cpp:105] Iteration 116300, lr = 0.001
I1107 17:22:44.700111  5452 solver.cpp:218] Iteration 116400 (11.7542 iter/s, 8.50762s/100 iters), loss = 0.03539
I1107 17:22:44.700111  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:22:44.700111  5452 solver.cpp:237]     Train net output #1: loss = 0.0353896 (* 1 = 0.0353896 loss)
I1107 17:22:44.700111  5452 sgd_solver.cpp:105] Iteration 116400, lr = 0.001
I1107 17:22:52.790865 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:22:53.127887  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_116500.caffemodel
I1107 17:22:53.158888  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_116500.solverstate
I1107 17:22:53.167888  5452 solver.cpp:330] Iteration 116500, Testing net (#0)
I1107 17:22:53.167888  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:22:55.151080 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:22:55.231089  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 17:22:55.231089  5452 solver.cpp:397]     Test net output #1: loss = 0.29656 (* 1 = 0.29656 loss)
I1107 17:22:55.312105  5452 solver.cpp:218] Iteration 116500 (9.42375 iter/s, 10.6115s/100 iters), loss = 0.0412328
I1107 17:22:55.312105  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:22:55.312105  5452 solver.cpp:237]     Train net output #1: loss = 0.0412324 (* 1 = 0.0412324 loss)
I1107 17:22:55.312105  5452 sgd_solver.cpp:105] Iteration 116500, lr = 0.001
I1107 17:23:03.821730  5452 solver.cpp:218] Iteration 116600 (11.7517 iter/s, 8.50941s/100 iters), loss = 0.0634247
I1107 17:23:03.821730  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:23:03.821730  5452 solver.cpp:237]     Train net output #1: loss = 0.0634243 (* 1 = 0.0634243 loss)
I1107 17:23:03.821730  5452 sgd_solver.cpp:105] Iteration 116600, lr = 0.001
I1107 17:23:12.344615  5452 solver.cpp:218] Iteration 116700 (11.7337 iter/s, 8.52245s/100 iters), loss = 0.0301646
I1107 17:23:12.344615  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:23:12.344615  5452 solver.cpp:237]     Train net output #1: loss = 0.0301642 (* 1 = 0.0301642 loss)
I1107 17:23:12.344615  5452 sgd_solver.cpp:105] Iteration 116700, lr = 0.001
I1107 17:23:20.855427  5452 solver.cpp:218] Iteration 116800 (11.7508 iter/s, 8.51004s/100 iters), loss = 0.0434676
I1107 17:23:20.855427  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:23:20.855427  5452 solver.cpp:237]     Train net output #1: loss = 0.0434672 (* 1 = 0.0434672 loss)
I1107 17:23:20.855427  5452 sgd_solver.cpp:105] Iteration 116800, lr = 0.001
I1107 17:23:29.349061  5452 solver.cpp:218] Iteration 116900 (11.774 iter/s, 8.49329s/100 iters), loss = 0.0472135
I1107 17:23:29.349061  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:23:29.349061  5452 solver.cpp:237]     Train net output #1: loss = 0.0472131 (* 1 = 0.0472131 loss)
I1107 17:23:29.349061  5452 sgd_solver.cpp:105] Iteration 116900, lr = 0.001
I1107 17:23:37.441018 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:23:37.778038  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_117000.caffemodel
I1107 17:23:37.814041  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_117000.solverstate
I1107 17:23:37.824043  5452 solver.cpp:330] Iteration 117000, Testing net (#0)
I1107 17:23:37.824043  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:23:39.805187 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:23:39.885211  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1107 17:23:39.885211  5452 solver.cpp:397]     Test net output #1: loss = 0.29689 (* 1 = 0.29689 loss)
I1107 17:23:39.967713  5452 solver.cpp:218] Iteration 117000 (9.41863 iter/s, 10.6173s/100 iters), loss = 0.0731607
I1107 17:23:39.967713  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:23:39.967713  5452 solver.cpp:237]     Train net output #1: loss = 0.0731604 (* 1 = 0.0731604 loss)
I1107 17:23:39.967713  5452 sgd_solver.cpp:105] Iteration 117000, lr = 0.001
I1107 17:23:48.480864  5452 solver.cpp:218] Iteration 117100 (11.7463 iter/s, 8.51331s/100 iters), loss = 0.0520564
I1107 17:23:48.480864  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:23:48.480864  5452 solver.cpp:237]     Train net output #1: loss = 0.052056 (* 1 = 0.052056 loss)
I1107 17:23:48.480864  5452 sgd_solver.cpp:105] Iteration 117100, lr = 0.001
I1107 17:23:56.992636  5452 solver.cpp:218] Iteration 117200 (11.75 iter/s, 8.51066s/100 iters), loss = 0.0263589
I1107 17:23:56.992636  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:23:56.992636  5452 solver.cpp:237]     Train net output #1: loss = 0.0263585 (* 1 = 0.0263585 loss)
I1107 17:23:56.992636  5452 sgd_solver.cpp:105] Iteration 117200, lr = 0.001
I1107 17:24:05.560268  5452 solver.cpp:218] Iteration 117300 (11.6726 iter/s, 8.56704s/100 iters), loss = 0.0344396
I1107 17:24:05.560268  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:24:05.560268  5452 solver.cpp:237]     Train net output #1: loss = 0.0344392 (* 1 = 0.0344392 loss)
I1107 17:24:05.560268  5452 sgd_solver.cpp:105] Iteration 117300, lr = 0.001
I1107 17:24:14.086884  5452 solver.cpp:218] Iteration 117400 (11.7282 iter/s, 8.52644s/100 iters), loss = 0.0280062
I1107 17:24:14.086884  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:24:14.086884  5452 solver.cpp:237]     Train net output #1: loss = 0.0280058 (* 1 = 0.0280058 loss)
I1107 17:24:14.086884  5452 sgd_solver.cpp:105] Iteration 117400, lr = 0.001
I1107 17:24:22.153939 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:24:22.491987  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_117500.caffemodel
I1107 17:24:22.523988  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_117500.solverstate
I1107 17:24:22.532989  5452 solver.cpp:330] Iteration 117500, Testing net (#0)
I1107 17:24:22.532989  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:24:24.517217 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:24:24.597220  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1107 17:24:24.597220  5452 solver.cpp:397]     Test net output #1: loss = 0.296071 (* 1 = 0.296071 loss)
I1107 17:24:24.677722  5452 solver.cpp:218] Iteration 117500 (9.44307 iter/s, 10.5898s/100 iters), loss = 0.0394104
I1107 17:24:24.677722  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:24:24.677722  5452 solver.cpp:237]     Train net output #1: loss = 0.03941 (* 1 = 0.03941 loss)
I1107 17:24:24.677722  5452 sgd_solver.cpp:105] Iteration 117500, lr = 0.001
I1107 17:24:33.181702  5452 solver.cpp:218] Iteration 117600 (11.76 iter/s, 8.5034s/100 iters), loss = 0.0392913
I1107 17:24:33.181702  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:24:33.181702  5452 solver.cpp:237]     Train net output #1: loss = 0.0392909 (* 1 = 0.0392909 loss)
I1107 17:24:33.181702  5452 sgd_solver.cpp:105] Iteration 117600, lr = 0.001
I1107 17:24:41.696688  5452 solver.cpp:218] Iteration 117700 (11.7436 iter/s, 8.51531s/100 iters), loss = 0.0265815
I1107 17:24:41.696688  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:24:41.696688  5452 solver.cpp:237]     Train net output #1: loss = 0.0265811 (* 1 = 0.0265811 loss)
I1107 17:24:41.696688  5452 sgd_solver.cpp:105] Iteration 117700, lr = 0.001
I1107 17:24:50.205548  5452 solver.cpp:218] Iteration 117800 (11.7539 iter/s, 8.50778s/100 iters), loss = 0.0293588
I1107 17:24:50.205548  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:24:50.205548  5452 solver.cpp:237]     Train net output #1: loss = 0.0293584 (* 1 = 0.0293584 loss)
I1107 17:24:50.205548  5452 sgd_solver.cpp:105] Iteration 117800, lr = 0.001
I1107 17:24:58.719529  5452 solver.cpp:218] Iteration 117900 (11.7466 iter/s, 8.51311s/100 iters), loss = 0.0355016
I1107 17:24:58.719529  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:24:58.719529  5452 solver.cpp:237]     Train net output #1: loss = 0.0355013 (* 1 = 0.0355013 loss)
I1107 17:24:58.719529  5452 sgd_solver.cpp:105] Iteration 117900, lr = 0.001
I1107 17:25:06.816239 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:25:07.154253  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_118000.caffemodel
I1107 17:25:07.190264  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_118000.solverstate
I1107 17:25:07.199282  5452 solver.cpp:330] Iteration 118000, Testing net (#0)
I1107 17:25:07.199282  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:25:09.185398 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:25:09.264400  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1107 17:25:09.264400  5452 solver.cpp:397]     Test net output #1: loss = 0.295205 (* 1 = 0.295205 loss)
I1107 17:25:09.345407  5452 solver.cpp:218] Iteration 118000 (9.41138 iter/s, 10.6254s/100 iters), loss = 0.0299816
I1107 17:25:09.345407  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:25:09.345407  5452 solver.cpp:237]     Train net output #1: loss = 0.0299812 (* 1 = 0.0299812 loss)
I1107 17:25:09.345407  5452 sgd_solver.cpp:105] Iteration 118000, lr = 0.001
I1107 17:25:17.853469  5452 solver.cpp:218] Iteration 118100 (11.7546 iter/s, 8.50731s/100 iters), loss = 0.0426066
I1107 17:25:17.853469  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:25:17.853469  5452 solver.cpp:237]     Train net output #1: loss = 0.0426062 (* 1 = 0.0426062 loss)
I1107 17:25:17.853469  5452 sgd_solver.cpp:105] Iteration 118100, lr = 0.001
I1107 17:25:26.362269  5452 solver.cpp:218] Iteration 118200 (11.7527 iter/s, 8.5087s/100 iters), loss = 0.0346078
I1107 17:25:26.362269  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:25:26.362269  5452 solver.cpp:237]     Train net output #1: loss = 0.0346074 (* 1 = 0.0346074 loss)
I1107 17:25:26.362269  5452 sgd_solver.cpp:105] Iteration 118200, lr = 0.001
I1107 17:25:34.883249  5452 solver.cpp:218] Iteration 118300 (11.7365 iter/s, 8.52041s/100 iters), loss = 0.0243217
I1107 17:25:34.883249  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:25:34.883249  5452 solver.cpp:237]     Train net output #1: loss = 0.0243213 (* 1 = 0.0243213 loss)
I1107 17:25:34.883249  5452 sgd_solver.cpp:105] Iteration 118300, lr = 0.001
I1107 17:25:43.414111  5452 solver.cpp:218] Iteration 118400 (11.7231 iter/s, 8.53014s/100 iters), loss = 0.0294431
I1107 17:25:43.414111  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:25:43.414111  5452 solver.cpp:237]     Train net output #1: loss = 0.0294426 (* 1 = 0.0294426 loss)
I1107 17:25:43.414111  5452 sgd_solver.cpp:105] Iteration 118400, lr = 0.001
I1107 17:25:51.495932 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:25:51.830960  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_118500.caffemodel
I1107 17:25:51.862968  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_118500.solverstate
I1107 17:25:51.872968  5452 solver.cpp:330] Iteration 118500, Testing net (#0)
I1107 17:25:51.872968  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:25:53.858212 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:25:53.937217  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 17:25:53.937217  5452 solver.cpp:397]     Test net output #1: loss = 0.295714 (* 1 = 0.295714 loss)
I1107 17:25:54.019227  5452 solver.cpp:218] Iteration 118500 (9.42942 iter/s, 10.6051s/100 iters), loss = 0.0426408
I1107 17:25:54.019227  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:25:54.019227  5452 solver.cpp:237]     Train net output #1: loss = 0.0426404 (* 1 = 0.0426404 loss)
I1107 17:25:54.019227  5452 sgd_solver.cpp:105] Iteration 118500, lr = 0.001
I1107 17:26:02.525446  5452 solver.cpp:218] Iteration 118600 (11.7572 iter/s, 8.5054s/100 iters), loss = 0.0547547
I1107 17:26:02.525446  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:26:02.525446  5452 solver.cpp:237]     Train net output #1: loss = 0.0547542 (* 1 = 0.0547542 loss)
I1107 17:26:02.525446  5452 sgd_solver.cpp:105] Iteration 118600, lr = 0.001
I1107 17:26:11.037480  5452 solver.cpp:218] Iteration 118700 (11.7495 iter/s, 8.51101s/100 iters), loss = 0.049747
I1107 17:26:11.037480  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:26:11.037480  5452 solver.cpp:237]     Train net output #1: loss = 0.0497466 (* 1 = 0.0497466 loss)
I1107 17:26:11.037480  5452 sgd_solver.cpp:105] Iteration 118700, lr = 0.001
I1107 17:26:19.541867  5452 solver.cpp:218] Iteration 118800 (11.7587 iter/s, 8.50438s/100 iters), loss = 0.0313354
I1107 17:26:19.541867  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:26:19.541867  5452 solver.cpp:237]     Train net output #1: loss = 0.031335 (* 1 = 0.031335 loss)
I1107 17:26:19.541867  5452 sgd_solver.cpp:105] Iteration 118800, lr = 0.001
I1107 17:26:28.060631  5452 solver.cpp:218] Iteration 118900 (11.7401 iter/s, 8.51782s/100 iters), loss = 0.0242989
I1107 17:26:28.060631  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:26:28.060631  5452 solver.cpp:237]     Train net output #1: loss = 0.0242984 (* 1 = 0.0242984 loss)
I1107 17:26:28.060631  5452 sgd_solver.cpp:105] Iteration 118900, lr = 0.001
I1107 17:26:36.133894 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:26:36.469993  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_119000.caffemodel
I1107 17:26:36.501994  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_119000.solverstate
I1107 17:26:36.510994  5452 solver.cpp:330] Iteration 119000, Testing net (#0)
I1107 17:26:36.510994  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:26:38.489961 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:26:38.569842  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 17:26:38.569842  5452 solver.cpp:397]     Test net output #1: loss = 0.294472 (* 1 = 0.294472 loss)
I1107 17:26:38.651345  5452 solver.cpp:218] Iteration 119000 (9.44279 iter/s, 10.5901s/100 iters), loss = 0.0526023
I1107 17:26:38.651345  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:26:38.651345  5452 solver.cpp:237]     Train net output #1: loss = 0.0526019 (* 1 = 0.0526019 loss)
I1107 17:26:38.651345  5452 sgd_solver.cpp:105] Iteration 119000, lr = 0.001
I1107 17:26:47.158273  5452 solver.cpp:218] Iteration 119100 (11.7553 iter/s, 8.50682s/100 iters), loss = 0.048017
I1107 17:26:47.158273  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:26:47.158773  5452 solver.cpp:237]     Train net output #1: loss = 0.0480166 (* 1 = 0.0480166 loss)
I1107 17:26:47.158773  5452 sgd_solver.cpp:105] Iteration 119100, lr = 0.001
I1107 17:26:55.681818  5452 solver.cpp:218] Iteration 119200 (11.7333 iter/s, 8.52273s/100 iters), loss = 0.0466502
I1107 17:26:55.681818  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:26:55.681818  5452 solver.cpp:237]     Train net output #1: loss = 0.0466498 (* 1 = 0.0466498 loss)
I1107 17:26:55.681818  5452 sgd_solver.cpp:105] Iteration 119200, lr = 0.001
I1107 17:27:04.231030  5452 solver.cpp:218] Iteration 119300 (11.6976 iter/s, 8.54875s/100 iters), loss = 0.0260355
I1107 17:27:04.231030  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:27:04.231030  5452 solver.cpp:237]     Train net output #1: loss = 0.0260351 (* 1 = 0.0260351 loss)
I1107 17:27:04.231030  5452 sgd_solver.cpp:105] Iteration 119300, lr = 0.001
I1107 17:27:12.727670  5452 solver.cpp:218] Iteration 119400 (11.7703 iter/s, 8.49597s/100 iters), loss = 0.0511281
I1107 17:27:12.727670  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:27:12.727670  5452 solver.cpp:237]     Train net output #1: loss = 0.0511277 (* 1 = 0.0511277 loss)
I1107 17:27:12.727670  5452 sgd_solver.cpp:105] Iteration 119400, lr = 0.001
I1107 17:27:20.833871 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:27:21.170419  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_119500.caffemodel
I1107 17:27:21.200925  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_119500.solverstate
I1107 17:27:21.210927  5452 solver.cpp:330] Iteration 119500, Testing net (#0)
I1107 17:27:21.210927  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:27:23.188247 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:27:23.267746  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9196
I1107 17:27:23.267746  5452 solver.cpp:397]     Test net output #1: loss = 0.295578 (* 1 = 0.295578 loss)
I1107 17:27:23.348886  5452 solver.cpp:218] Iteration 119500 (9.4155 iter/s, 10.6208s/100 iters), loss = 0.0437505
I1107 17:27:23.348886  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:27:23.348886  5452 solver.cpp:237]     Train net output #1: loss = 0.0437501 (* 1 = 0.0437501 loss)
I1107 17:27:23.348886  5452 sgd_solver.cpp:105] Iteration 119500, lr = 0.001
I1107 17:27:31.830245  5452 solver.cpp:218] Iteration 119600 (11.791 iter/s, 8.48105s/100 iters), loss = 0.054874
I1107 17:27:31.830245  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:27:31.830245  5452 solver.cpp:237]     Train net output #1: loss = 0.0548735 (* 1 = 0.0548735 loss)
I1107 17:27:31.830245  5452 sgd_solver.cpp:105] Iteration 119600, lr = 0.001
I1107 17:27:40.340261  5452 solver.cpp:218] Iteration 119700 (11.7515 iter/s, 8.50959s/100 iters), loss = 0.0636637
I1107 17:27:40.340261  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:27:40.340261  5452 solver.cpp:237]     Train net output #1: loss = 0.0636632 (* 1 = 0.0636632 loss)
I1107 17:27:40.340261  5452 sgd_solver.cpp:105] Iteration 119700, lr = 0.001
I1107 17:27:48.859143  5452 solver.cpp:218] Iteration 119800 (11.7402 iter/s, 8.51776s/100 iters), loss = 0.0249307
I1107 17:27:48.859143  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:27:48.859143  5452 solver.cpp:237]     Train net output #1: loss = 0.0249302 (* 1 = 0.0249302 loss)
I1107 17:27:48.859143  5452 sgd_solver.cpp:105] Iteration 119800, lr = 0.001
I1107 17:27:57.360795  5452 solver.cpp:218] Iteration 119900 (11.7625 iter/s, 8.50158s/100 iters), loss = 0.029959
I1107 17:27:57.360795  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:27:57.360795  5452 solver.cpp:237]     Train net output #1: loss = 0.0299586 (* 1 = 0.0299586 loss)
I1107 17:27:57.360795  5452 sgd_solver.cpp:105] Iteration 119900, lr = 0.001
I1107 17:28:05.443684 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:28:05.779703  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_120000.caffemodel
I1107 17:28:05.809707  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_120000.solverstate
I1107 17:28:05.818706  5452 solver.cpp:330] Iteration 120000, Testing net (#0)
I1107 17:28:05.818706  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:28:07.803189 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:28:07.883215  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9178
I1107 17:28:07.883215  5452 solver.cpp:397]     Test net output #1: loss = 0.297789 (* 1 = 0.297789 loss)
I1107 17:28:07.964212  5452 solver.cpp:218] Iteration 120000 (9.43117 iter/s, 10.6031s/100 iters), loss = 0.0541751
I1107 17:28:07.964212  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:28:07.964212  5452 solver.cpp:237]     Train net output #1: loss = 0.0541747 (* 1 = 0.0541747 loss)
I1107 17:28:07.964212  5452 sgd_solver.cpp:105] Iteration 120000, lr = 0.001
I1107 17:28:16.461922  5452 solver.cpp:218] Iteration 120100 (11.7691 iter/s, 8.49684s/100 iters), loss = 0.0380185
I1107 17:28:16.461922  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:28:16.461922  5452 solver.cpp:237]     Train net output #1: loss = 0.038018 (* 1 = 0.038018 loss)
I1107 17:28:16.461922  5452 sgd_solver.cpp:105] Iteration 120100, lr = 0.001
I1107 17:28:24.968703  5452 solver.cpp:218] Iteration 120200 (11.7561 iter/s, 8.50622s/100 iters), loss = 0.0328773
I1107 17:28:24.969203  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:28:24.969203  5452 solver.cpp:237]     Train net output #1: loss = 0.0328768 (* 1 = 0.0328768 loss)
I1107 17:28:24.969203  5452 sgd_solver.cpp:105] Iteration 120200, lr = 0.001
I1107 17:28:33.470571  5452 solver.cpp:218] Iteration 120300 (11.7632 iter/s, 8.50106s/100 iters), loss = 0.0262284
I1107 17:28:33.470571  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:28:33.470571  5452 solver.cpp:237]     Train net output #1: loss = 0.026228 (* 1 = 0.026228 loss)
I1107 17:28:33.470571  5452 sgd_solver.cpp:105] Iteration 120300, lr = 0.001
I1107 17:28:41.978623  5452 solver.cpp:218] Iteration 120400 (11.7543 iter/s, 8.50755s/100 iters), loss = 0.0331073
I1107 17:28:41.978623  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:28:41.978623  5452 solver.cpp:237]     Train net output #1: loss = 0.0331068 (* 1 = 0.0331068 loss)
I1107 17:28:41.978623  5452 sgd_solver.cpp:105] Iteration 120400, lr = 0.001
I1107 17:28:50.068476 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:28:50.404537  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_120500.caffemodel
I1107 17:28:50.434551  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_120500.solverstate
I1107 17:28:50.443555  5452 solver.cpp:330] Iteration 120500, Testing net (#0)
I1107 17:28:50.443555  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:28:52.428706 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:28:52.507712  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1107 17:28:52.508713  5452 solver.cpp:397]     Test net output #1: loss = 0.297314 (* 1 = 0.297314 loss)
I1107 17:28:52.589716  5452 solver.cpp:218] Iteration 120500 (9.42407 iter/s, 10.6111s/100 iters), loss = 0.0289245
I1107 17:28:52.589716  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:28:52.589716  5452 solver.cpp:237]     Train net output #1: loss = 0.0289241 (* 1 = 0.0289241 loss)
I1107 17:28:52.589716  5452 sgd_solver.cpp:105] Iteration 120500, lr = 0.001
I1107 17:29:01.101963  5452 solver.cpp:218] Iteration 120600 (11.7493 iter/s, 8.51115s/100 iters), loss = 0.0275228
I1107 17:29:01.101963  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:29:01.101963  5452 solver.cpp:237]     Train net output #1: loss = 0.0275224 (* 1 = 0.0275224 loss)
I1107 17:29:01.101963  5452 sgd_solver.cpp:105] Iteration 120600, lr = 0.001
I1107 17:29:09.610818  5452 solver.cpp:218] Iteration 120700 (11.7527 iter/s, 8.50865s/100 iters), loss = 0.0462998
I1107 17:29:09.610818  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:29:09.610818  5452 solver.cpp:237]     Train net output #1: loss = 0.0462994 (* 1 = 0.0462994 loss)
I1107 17:29:09.610818  5452 sgd_solver.cpp:105] Iteration 120700, lr = 0.001
I1107 17:29:18.130694  5452 solver.cpp:218] Iteration 120800 (11.7384 iter/s, 8.51901s/100 iters), loss = 0.0180997
I1107 17:29:18.130694  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:29:18.130694  5452 solver.cpp:237]     Train net output #1: loss = 0.0180993 (* 1 = 0.0180993 loss)
I1107 17:29:18.130694  5452 sgd_solver.cpp:105] Iteration 120800, lr = 0.001
I1107 17:29:26.636656  5452 solver.cpp:218] Iteration 120900 (11.7561 iter/s, 8.5062s/100 iters), loss = 0.0465535
I1107 17:29:26.637645  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:29:26.637645  5452 solver.cpp:237]     Train net output #1: loss = 0.0465531 (* 1 = 0.0465531 loss)
I1107 17:29:26.637645  5452 sgd_solver.cpp:105] Iteration 120900, lr = 0.001
I1107 17:29:34.715306 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:29:35.052327  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_121000.caffemodel
I1107 17:29:35.081830  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_121000.solverstate
I1107 17:29:35.090831  5452 solver.cpp:330] Iteration 121000, Testing net (#0)
I1107 17:29:35.090831  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:29:37.074458 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:29:37.153465  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9203
I1107 17:29:37.153465  5452 solver.cpp:397]     Test net output #1: loss = 0.294897 (* 1 = 0.294897 loss)
I1107 17:29:37.235472  5452 solver.cpp:218] Iteration 121000 (9.43594 iter/s, 10.5978s/100 iters), loss = 0.0339895
I1107 17:29:37.235472  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:29:37.235472  5452 solver.cpp:237]     Train net output #1: loss = 0.0339891 (* 1 = 0.0339891 loss)
I1107 17:29:37.235472  5452 sgd_solver.cpp:105] Iteration 121000, lr = 0.001
I1107 17:29:45.746453  5452 solver.cpp:218] Iteration 121100 (11.7501 iter/s, 8.51055s/100 iters), loss = 0.0438121
I1107 17:29:45.746453  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:29:45.746453  5452 solver.cpp:237]     Train net output #1: loss = 0.0438116 (* 1 = 0.0438116 loss)
I1107 17:29:45.746453  5452 sgd_solver.cpp:105] Iteration 121100, lr = 0.001
I1107 17:29:54.248458  5452 solver.cpp:218] Iteration 121200 (11.7622 iter/s, 8.50182s/100 iters), loss = 0.0500837
I1107 17:29:54.248458  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:29:54.248458  5452 solver.cpp:237]     Train net output #1: loss = 0.0500833 (* 1 = 0.0500833 loss)
I1107 17:29:54.248458  5452 sgd_solver.cpp:105] Iteration 121200, lr = 0.001
I1107 17:30:02.767822  5452 solver.cpp:218] Iteration 121300 (11.7397 iter/s, 8.51812s/100 iters), loss = 0.0264981
I1107 17:30:02.767822  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:30:02.767822  5452 solver.cpp:237]     Train net output #1: loss = 0.0264977 (* 1 = 0.0264977 loss)
I1107 17:30:02.767822  5452 sgd_solver.cpp:105] Iteration 121300, lr = 0.001
I1107 17:30:11.296514  5452 solver.cpp:218] Iteration 121400 (11.7254 iter/s, 8.52851s/100 iters), loss = 0.0310216
I1107 17:30:11.296514  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:30:11.296514  5452 solver.cpp:237]     Train net output #1: loss = 0.0310212 (* 1 = 0.0310212 loss)
I1107 17:30:11.296514  5452 sgd_solver.cpp:105] Iteration 121400, lr = 0.001
I1107 17:30:19.393041 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:30:19.729058  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_121500.caffemodel
I1107 17:30:19.756563  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_121500.solverstate
I1107 17:30:19.766067  5452 solver.cpp:330] Iteration 121500, Testing net (#0)
I1107 17:30:19.766067  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:30:21.747751 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:30:21.826253  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9202
I1107 17:30:21.826253  5452 solver.cpp:397]     Test net output #1: loss = 0.292059 (* 1 = 0.292059 loss)
I1107 17:30:21.908263  5452 solver.cpp:218] Iteration 121500 (9.4242 iter/s, 10.611s/100 iters), loss = 0.0372858
I1107 17:30:21.908263  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:30:21.908263  5452 solver.cpp:237]     Train net output #1: loss = 0.0372854 (* 1 = 0.0372854 loss)
I1107 17:30:21.908263  5452 sgd_solver.cpp:105] Iteration 121500, lr = 0.001
I1107 17:30:30.416009  5452 solver.cpp:218] Iteration 121600 (11.7543 iter/s, 8.5075s/100 iters), loss = 0.0510728
I1107 17:30:30.416009  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:30:30.416009  5452 solver.cpp:237]     Train net output #1: loss = 0.0510724 (* 1 = 0.0510724 loss)
I1107 17:30:30.416009  5452 sgd_solver.cpp:105] Iteration 121600, lr = 0.001
I1107 17:30:38.926023  5452 solver.cpp:218] Iteration 121700 (11.7518 iter/s, 8.50934s/100 iters), loss = 0.0503555
I1107 17:30:38.926023  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:30:38.926023  5452 solver.cpp:237]     Train net output #1: loss = 0.0503551 (* 1 = 0.0503551 loss)
I1107 17:30:38.926023  5452 sgd_solver.cpp:105] Iteration 121700, lr = 0.001
I1107 17:30:47.422842  5452 solver.cpp:218] Iteration 121800 (11.7695 iter/s, 8.49656s/100 iters), loss = 0.0340097
I1107 17:30:47.422842  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:30:47.422842  5452 solver.cpp:237]     Train net output #1: loss = 0.0340093 (* 1 = 0.0340093 loss)
I1107 17:30:47.422842  5452 sgd_solver.cpp:105] Iteration 121800, lr = 0.001
I1107 17:30:55.932449  5452 solver.cpp:218] Iteration 121900 (11.7533 iter/s, 8.50827s/100 iters), loss = 0.0331918
I1107 17:30:55.932449  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:30:55.932449  5452 solver.cpp:237]     Train net output #1: loss = 0.0331914 (* 1 = 0.0331914 loss)
I1107 17:30:55.932449  5452 sgd_solver.cpp:105] Iteration 121900, lr = 0.001
I1107 17:31:04.021179 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:31:04.358713  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_122000.caffemodel
I1107 17:31:04.389217  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_122000.solverstate
I1107 17:31:04.398217  5452 solver.cpp:330] Iteration 122000, Testing net (#0)
I1107 17:31:04.398217  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:31:06.384366 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:31:06.463870  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9196
I1107 17:31:06.464375  5452 solver.cpp:397]     Test net output #1: loss = 0.296333 (* 1 = 0.296333 loss)
I1107 17:31:06.545374  5452 solver.cpp:218] Iteration 122000 (9.4225 iter/s, 10.6129s/100 iters), loss = 0.0569072
I1107 17:31:06.545374  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:31:06.545374  5452 solver.cpp:237]     Train net output #1: loss = 0.0569068 (* 1 = 0.0569068 loss)
I1107 17:31:06.545374  5452 sgd_solver.cpp:105] Iteration 122000, lr = 0.001
I1107 17:31:15.048517  5452 solver.cpp:218] Iteration 122100 (11.7613 iter/s, 8.50249s/100 iters), loss = 0.0331805
I1107 17:31:15.048517  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:31:15.048517  5452 solver.cpp:237]     Train net output #1: loss = 0.0331801 (* 1 = 0.0331801 loss)
I1107 17:31:15.048517  5452 sgd_solver.cpp:105] Iteration 122100, lr = 0.001
I1107 17:31:23.557273  5452 solver.cpp:218] Iteration 122200 (11.7531 iter/s, 8.50839s/100 iters), loss = 0.0362096
I1107 17:31:23.557273  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:31:23.557273  5452 solver.cpp:237]     Train net output #1: loss = 0.0362092 (* 1 = 0.0362092 loss)
I1107 17:31:23.557273  5452 sgd_solver.cpp:105] Iteration 122200, lr = 0.001
I1107 17:31:32.048058  5452 solver.cpp:218] Iteration 122300 (11.7786 iter/s, 8.48999s/100 iters), loss = 0.0328876
I1107 17:31:32.048058  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:31:32.048058  5452 solver.cpp:237]     Train net output #1: loss = 0.0328872 (* 1 = 0.0328872 loss)
I1107 17:31:32.048058  5452 sgd_solver.cpp:105] Iteration 122300, lr = 0.001
I1107 17:31:40.564389  5452 solver.cpp:218] Iteration 122400 (11.7425 iter/s, 8.5161s/100 iters), loss = 0.0378311
I1107 17:31:40.564389  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:31:40.564389  5452 solver.cpp:237]     Train net output #1: loss = 0.0378307 (* 1 = 0.0378307 loss)
I1107 17:31:40.564389  5452 sgd_solver.cpp:105] Iteration 122400, lr = 0.001
I1107 17:31:48.643900 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:31:48.980942  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_122500.caffemodel
I1107 17:31:49.008949  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_122500.solverstate
I1107 17:31:49.017951  5452 solver.cpp:330] Iteration 122500, Testing net (#0)
I1107 17:31:49.017951  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:31:51.005157 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:31:51.084161  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9178
I1107 17:31:51.084161  5452 solver.cpp:397]     Test net output #1: loss = 0.303485 (* 1 = 0.303485 loss)
I1107 17:31:51.165165  5452 solver.cpp:218] Iteration 122500 (9.43369 iter/s, 10.6003s/100 iters), loss = 0.0452646
I1107 17:31:51.165165  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:31:51.165165  5452 solver.cpp:237]     Train net output #1: loss = 0.0452642 (* 1 = 0.0452642 loss)
I1107 17:31:51.165165  5452 sgd_solver.cpp:105] Iteration 122500, lr = 0.001
I1107 17:31:59.658505  5452 solver.cpp:218] Iteration 122600 (11.7755 iter/s, 8.49221s/100 iters), loss = 0.0313963
I1107 17:31:59.658505  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:31:59.658505  5452 solver.cpp:237]     Train net output #1: loss = 0.0313959 (* 1 = 0.0313959 loss)
I1107 17:31:59.658505  5452 sgd_solver.cpp:105] Iteration 122600, lr = 0.001
I1107 17:32:08.160771  5452 solver.cpp:218] Iteration 122700 (11.7619 iter/s, 8.50206s/100 iters), loss = 0.0243467
I1107 17:32:08.160771  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:32:08.160771  5452 solver.cpp:237]     Train net output #1: loss = 0.0243463 (* 1 = 0.0243463 loss)
I1107 17:32:08.160771  5452 sgd_solver.cpp:105] Iteration 122700, lr = 0.001
I1107 17:32:16.702607  5452 solver.cpp:218] Iteration 122800 (11.7085 iter/s, 8.5408s/100 iters), loss = 0.0362851
I1107 17:32:16.702607  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:32:16.702607  5452 solver.cpp:237]     Train net output #1: loss = 0.0362847 (* 1 = 0.0362847 loss)
I1107 17:32:16.702607  5452 sgd_solver.cpp:105] Iteration 122800, lr = 0.001
I1107 17:32:25.182891  5452 solver.cpp:218] Iteration 122900 (11.7922 iter/s, 8.4802s/100 iters), loss = 0.0674781
I1107 17:32:25.182891  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:32:25.182891  5452 solver.cpp:237]     Train net output #1: loss = 0.0674777 (* 1 = 0.0674777 loss)
I1107 17:32:25.182891  5452 sgd_solver.cpp:105] Iteration 122900, lr = 0.001
I1107 17:32:33.238651 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:32:33.574667  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_123000.caffemodel
I1107 17:32:33.605671  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_123000.solverstate
I1107 17:32:33.614671  5452 solver.cpp:330] Iteration 123000, Testing net (#0)
I1107 17:32:33.614671  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:32:35.592900 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:32:35.671905  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9189
I1107 17:32:35.671905  5452 solver.cpp:397]     Test net output #1: loss = 0.294572 (* 1 = 0.294572 loss)
I1107 17:32:35.753927  5452 solver.cpp:218] Iteration 123000 (9.46097 iter/s, 10.5697s/100 iters), loss = 0.0332531
I1107 17:32:35.753927  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:32:35.753927  5452 solver.cpp:237]     Train net output #1: loss = 0.0332527 (* 1 = 0.0332527 loss)
I1107 17:32:35.753927  5452 sgd_solver.cpp:105] Iteration 123000, lr = 0.001
I1107 17:32:44.229706  5452 solver.cpp:218] Iteration 123100 (11.7985 iter/s, 8.47563s/100 iters), loss = 0.0386389
I1107 17:32:44.229706  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:32:44.229706  5452 solver.cpp:237]     Train net output #1: loss = 0.0386385 (* 1 = 0.0386385 loss)
I1107 17:32:44.229706  5452 sgd_solver.cpp:105] Iteration 123100, lr = 0.001
I1107 17:32:52.714004  5452 solver.cpp:218] Iteration 123200 (11.7872 iter/s, 8.48375s/100 iters), loss = 0.0349278
I1107 17:32:52.714004  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:32:52.714004  5452 solver.cpp:237]     Train net output #1: loss = 0.0349274 (* 1 = 0.0349274 loss)
I1107 17:32:52.714004  5452 sgd_solver.cpp:105] Iteration 123200, lr = 0.001
I1107 17:33:01.193114  5452 solver.cpp:218] Iteration 123300 (11.7943 iter/s, 8.4787s/100 iters), loss = 0.0367214
I1107 17:33:01.193114  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:33:01.193114  5452 solver.cpp:237]     Train net output #1: loss = 0.036721 (* 1 = 0.036721 loss)
I1107 17:33:01.193114  5452 sgd_solver.cpp:105] Iteration 123300, lr = 0.001
I1107 17:33:09.677811  5452 solver.cpp:218] Iteration 123400 (11.7863 iter/s, 8.48445s/100 iters), loss = 0.0338193
I1107 17:33:09.677811  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:33:09.677811  5452 solver.cpp:237]     Train net output #1: loss = 0.0338189 (* 1 = 0.0338189 loss)
I1107 17:33:09.677811  5452 sgd_solver.cpp:105] Iteration 123400, lr = 0.001
I1107 17:33:17.740587 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:33:18.073642  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_123500.caffemodel
I1107 17:33:18.103641  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_123500.solverstate
I1107 17:33:18.113647  5452 solver.cpp:330] Iteration 123500, Testing net (#0)
I1107 17:33:18.114147  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:33:20.092808 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:33:20.172832  5452 solver.cpp:397]     Test net output #0: accuracy = 0.92
I1107 17:33:20.172832  5452 solver.cpp:397]     Test net output #1: loss = 0.300719 (* 1 = 0.300719 loss)
I1107 17:33:20.253829  5452 solver.cpp:218] Iteration 123500 (9.45645 iter/s, 10.5748s/100 iters), loss = 0.0272165
I1107 17:33:20.253829  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:33:20.253829  5452 solver.cpp:237]     Train net output #1: loss = 0.0272161 (* 1 = 0.0272161 loss)
I1107 17:33:20.253829  5452 sgd_solver.cpp:105] Iteration 123500, lr = 0.001
I1107 17:33:28.738716  5452 solver.cpp:218] Iteration 123600 (11.7854 iter/s, 8.4851s/100 iters), loss = 0.0396836
I1107 17:33:28.738716  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:33:28.738716  5452 solver.cpp:237]     Train net output #1: loss = 0.0396832 (* 1 = 0.0396832 loss)
I1107 17:33:28.738716  5452 sgd_solver.cpp:105] Iteration 123600, lr = 0.001
I1107 17:33:37.223510  5452 solver.cpp:218] Iteration 123700 (11.7869 iter/s, 8.48397s/100 iters), loss = 0.0553021
I1107 17:33:37.223510  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:33:37.223510  5452 solver.cpp:237]     Train net output #1: loss = 0.0553017 (* 1 = 0.0553017 loss)
I1107 17:33:37.223510  5452 sgd_solver.cpp:105] Iteration 123700, lr = 0.001
I1107 17:33:45.716814  5452 solver.cpp:218] Iteration 123800 (11.775 iter/s, 8.4926s/100 iters), loss = 0.0258156
I1107 17:33:45.716814  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:33:45.716814  5452 solver.cpp:237]     Train net output #1: loss = 0.0258152 (* 1 = 0.0258152 loss)
I1107 17:33:45.716814  5452 sgd_solver.cpp:105] Iteration 123800, lr = 0.001
I1107 17:33:54.203155  5452 solver.cpp:218] Iteration 123900 (11.7834 iter/s, 8.48649s/100 iters), loss = 0.0427072
I1107 17:33:54.203155  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:33:54.204155  5452 solver.cpp:237]     Train net output #1: loss = 0.0427068 (* 1 = 0.0427068 loss)
I1107 17:33:54.204155  5452 sgd_solver.cpp:105] Iteration 123900, lr = 0.001
I1107 17:34:02.283093 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:34:02.618127  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_124000.caffemodel
I1107 17:34:02.647135  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_124000.solverstate
I1107 17:34:02.656136  5452 solver.cpp:330] Iteration 124000, Testing net (#0)
I1107 17:34:02.656136  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:34:04.635244 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:34:04.713245  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9202
I1107 17:34:04.713245  5452 solver.cpp:397]     Test net output #1: loss = 0.295773 (* 1 = 0.295773 loss)
I1107 17:34:04.794247  5452 solver.cpp:218] Iteration 124000 (9.44262 iter/s, 10.5903s/100 iters), loss = 0.0348963
I1107 17:34:04.794247  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:34:04.794247  5452 solver.cpp:237]     Train net output #1: loss = 0.0348959 (* 1 = 0.0348959 loss)
I1107 17:34:04.794247  5452 sgd_solver.cpp:105] Iteration 124000, lr = 0.001
I1107 17:34:13.272970  5452 solver.cpp:218] Iteration 124100 (11.7951 iter/s, 8.47808s/100 iters), loss = 0.039887
I1107 17:34:13.272970  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:34:13.272970  5452 solver.cpp:237]     Train net output #1: loss = 0.0398866 (* 1 = 0.0398866 loss)
I1107 17:34:13.272970  5452 sgd_solver.cpp:105] Iteration 124100, lr = 0.001
I1107 17:34:21.753710  5452 solver.cpp:218] Iteration 124200 (11.792 iter/s, 8.48034s/100 iters), loss = 0.0293925
I1107 17:34:21.753710  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:34:21.753710  5452 solver.cpp:237]     Train net output #1: loss = 0.0293921 (* 1 = 0.0293921 loss)
I1107 17:34:21.753710  5452 sgd_solver.cpp:105] Iteration 124200, lr = 0.001
I1107 17:34:30.229063  5452 solver.cpp:218] Iteration 124300 (11.8003 iter/s, 8.47435s/100 iters), loss = 0.0258766
I1107 17:34:30.229063  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:34:30.229063  5452 solver.cpp:237]     Train net output #1: loss = 0.0258762 (* 1 = 0.0258762 loss)
I1107 17:34:30.229063  5452 sgd_solver.cpp:105] Iteration 124300, lr = 0.001
I1107 17:34:38.710173  5452 solver.cpp:218] Iteration 124400 (11.791 iter/s, 8.48106s/100 iters), loss = 0.0382251
I1107 17:34:38.710173  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:34:38.710173  5452 solver.cpp:237]     Train net output #1: loss = 0.0382247 (* 1 = 0.0382247 loss)
I1107 17:34:38.710173  5452 sgd_solver.cpp:105] Iteration 124400, lr = 0.001
I1107 17:34:46.776720 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:34:47.111327  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_124500.caffemodel
I1107 17:34:47.140889  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_124500.solverstate
I1107 17:34:47.148893  5452 solver.cpp:330] Iteration 124500, Testing net (#0)
I1107 17:34:47.149879  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:34:49.125739 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:34:49.204246  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 17:34:49.204246  5452 solver.cpp:397]     Test net output #1: loss = 0.298085 (* 1 = 0.298085 loss)
I1107 17:34:49.285254  5452 solver.cpp:218] Iteration 124500 (9.4571 iter/s, 10.5741s/100 iters), loss = 0.0266703
I1107 17:34:49.285254  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:34:49.285254  5452 solver.cpp:237]     Train net output #1: loss = 0.0266699 (* 1 = 0.0266699 loss)
I1107 17:34:49.285254  5452 sgd_solver.cpp:105] Iteration 124500, lr = 0.001
I1107 17:34:57.774047  5452 solver.cpp:218] Iteration 124600 (11.7801 iter/s, 8.48888s/100 iters), loss = 0.0397514
I1107 17:34:57.774047  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:34:57.774047  5452 solver.cpp:237]     Train net output #1: loss = 0.039751 (* 1 = 0.039751 loss)
I1107 17:34:57.774047  5452 sgd_solver.cpp:105] Iteration 124600, lr = 0.001
I1107 17:35:06.255072  5452 solver.cpp:218] Iteration 124700 (11.7921 iter/s, 8.48026s/100 iters), loss = 0.0224655
I1107 17:35:06.255072  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:35:06.255072  5452 solver.cpp:237]     Train net output #1: loss = 0.0224651 (* 1 = 0.0224651 loss)
I1107 17:35:06.255072  5452 sgd_solver.cpp:105] Iteration 124700, lr = 0.001
I1107 17:35:14.738812  5452 solver.cpp:218] Iteration 124800 (11.7872 iter/s, 8.48379s/100 iters), loss = 0.0265288
I1107 17:35:14.739812  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:35:14.739812  5452 solver.cpp:237]     Train net output #1: loss = 0.0265284 (* 1 = 0.0265284 loss)
I1107 17:35:14.739812  5452 sgd_solver.cpp:105] Iteration 124800, lr = 0.001
I1107 17:35:23.218545  5452 solver.cpp:218] Iteration 124900 (11.7946 iter/s, 8.47843s/100 iters), loss = 0.0312948
I1107 17:35:23.218545  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:35:23.218545  5452 solver.cpp:237]     Train net output #1: loss = 0.0312943 (* 1 = 0.0312943 loss)
I1107 17:35:23.218545  5452 sgd_solver.cpp:105] Iteration 124900, lr = 0.001
I1107 17:35:31.287145 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:35:31.621122  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_125000.caffemodel
I1107 17:35:31.652137  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_125000.solverstate
I1107 17:35:31.661141  5452 solver.cpp:330] Iteration 125000, Testing net (#0)
I1107 17:35:31.661141  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:35:33.638833 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:35:33.717676  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9185
I1107 17:35:33.717676  5452 solver.cpp:397]     Test net output #1: loss = 0.298722 (* 1 = 0.298722 loss)
I1107 17:35:33.798729  5452 solver.cpp:218] Iteration 125000 (9.45208 iter/s, 10.5797s/100 iters), loss = 0.029664
I1107 17:35:33.798729  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:35:33.798729  5452 solver.cpp:237]     Train net output #1: loss = 0.0296635 (* 1 = 0.0296635 loss)
I1107 17:35:33.798729  5452 sgd_solver.cpp:105] Iteration 125000, lr = 0.001
I1107 17:35:42.284174  5452 solver.cpp:218] Iteration 125100 (11.785 iter/s, 8.48538s/100 iters), loss = 0.0251233
I1107 17:35:42.284174  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:35:42.284174  5452 solver.cpp:237]     Train net output #1: loss = 0.0251229 (* 1 = 0.0251229 loss)
I1107 17:35:42.284174  5452 sgd_solver.cpp:105] Iteration 125100, lr = 0.001
I1107 17:35:50.755884  5452 solver.cpp:218] Iteration 125200 (11.8046 iter/s, 8.47131s/100 iters), loss = 0.0262474
I1107 17:35:50.755884  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:35:50.755884  5452 solver.cpp:237]     Train net output #1: loss = 0.026247 (* 1 = 0.026247 loss)
I1107 17:35:50.755884  5452 sgd_solver.cpp:105] Iteration 125200, lr = 0.001
I1107 17:35:59.235666  5452 solver.cpp:218] Iteration 125300 (11.7933 iter/s, 8.47936s/100 iters), loss = 0.0408806
I1107 17:35:59.235666  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:35:59.235666  5452 solver.cpp:237]     Train net output #1: loss = 0.0408802 (* 1 = 0.0408802 loss)
I1107 17:35:59.235666  5452 sgd_solver.cpp:105] Iteration 125300, lr = 0.001
I1107 17:36:07.701658  5452 solver.cpp:218] Iteration 125400 (11.8132 iter/s, 8.4651s/100 iters), loss = 0.025891
I1107 17:36:07.702157  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:36:07.702157  5452 solver.cpp:237]     Train net output #1: loss = 0.0258906 (* 1 = 0.0258906 loss)
I1107 17:36:07.702157  5452 sgd_solver.cpp:105] Iteration 125400, lr = 0.001
I1107 17:36:15.764468 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:36:16.100991  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_125500.caffemodel
I1107 17:36:16.129494  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_125500.solverstate
I1107 17:36:16.138495  5452 solver.cpp:330] Iteration 125500, Testing net (#0)
I1107 17:36:16.138495  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:36:18.115715 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:36:18.195219  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9185
I1107 17:36:18.195219  5452 solver.cpp:397]     Test net output #1: loss = 0.303377 (* 1 = 0.303377 loss)
I1107 17:36:18.275718  5452 solver.cpp:218] Iteration 125500 (9.45739 iter/s, 10.5737s/100 iters), loss = 0.0282581
I1107 17:36:18.275718  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:36:18.275718  5452 solver.cpp:237]     Train net output #1: loss = 0.0282577 (* 1 = 0.0282577 loss)
I1107 17:36:18.275718  5452 sgd_solver.cpp:105] Iteration 125500, lr = 0.001
I1107 17:36:26.753537  5452 solver.cpp:218] Iteration 125600 (11.7966 iter/s, 8.47703s/100 iters), loss = 0.049684
I1107 17:36:26.753537  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:36:26.753537  5452 solver.cpp:237]     Train net output #1: loss = 0.0496836 (* 1 = 0.0496836 loss)
I1107 17:36:26.753537  5452 sgd_solver.cpp:105] Iteration 125600, lr = 0.001
I1107 17:36:35.224225  5452 solver.cpp:218] Iteration 125700 (11.8065 iter/s, 8.46994s/100 iters), loss = 0.0293473
I1107 17:36:35.224225  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:36:35.224225  5452 solver.cpp:237]     Train net output #1: loss = 0.0293469 (* 1 = 0.0293469 loss)
I1107 17:36:35.224225  5452 sgd_solver.cpp:105] Iteration 125700, lr = 0.001
I1107 17:36:43.697902  5452 solver.cpp:218] Iteration 125800 (11.802 iter/s, 8.47313s/100 iters), loss = 0.0307012
I1107 17:36:43.697902  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:36:43.697902  5452 solver.cpp:237]     Train net output #1: loss = 0.0307008 (* 1 = 0.0307008 loss)
I1107 17:36:43.697902  5452 sgd_solver.cpp:105] Iteration 125800, lr = 0.001
I1107 17:36:52.168931  5452 solver.cpp:218] Iteration 125900 (11.8056 iter/s, 8.47059s/100 iters), loss = 0.0403917
I1107 17:36:52.168931  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:36:52.168931  5452 solver.cpp:237]     Train net output #1: loss = 0.0403913 (* 1 = 0.0403913 loss)
I1107 17:36:52.168931  5452 sgd_solver.cpp:105] Iteration 125900, lr = 0.001
I1107 17:37:00.223793 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:37:00.558797  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_126000.caffemodel
I1107 17:37:00.588798  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_126000.solverstate
I1107 17:37:00.598803  5452 solver.cpp:330] Iteration 126000, Testing net (#0)
I1107 17:37:00.598803  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:37:02.574053 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:37:02.653069  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9192
I1107 17:37:02.653069  5452 solver.cpp:397]     Test net output #1: loss = 0.300461 (* 1 = 0.300461 loss)
I1107 17:37:02.735074  5452 solver.cpp:218] Iteration 126000 (9.46469 iter/s, 10.5656s/100 iters), loss = 0.0696574
I1107 17:37:02.735074  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:37:02.735074  5452 solver.cpp:237]     Train net output #1: loss = 0.0696569 (* 1 = 0.0696569 loss)
I1107 17:37:02.735074  5452 sgd_solver.cpp:105] Iteration 126000, lr = 0.001
I1107 17:37:11.228426  5452 solver.cpp:218] Iteration 126100 (11.7749 iter/s, 8.49267s/100 iters), loss = 0.0347405
I1107 17:37:11.228426  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:37:11.228426  5452 solver.cpp:237]     Train net output #1: loss = 0.0347401 (* 1 = 0.0347401 loss)
I1107 17:37:11.228426  5452 sgd_solver.cpp:105] Iteration 126100, lr = 0.001
I1107 17:37:19.701942  5452 solver.cpp:218] Iteration 126200 (11.802 iter/s, 8.47311s/100 iters), loss = 0.0459125
I1107 17:37:19.701942  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:37:19.701942  5452 solver.cpp:237]     Train net output #1: loss = 0.045912 (* 1 = 0.045912 loss)
I1107 17:37:19.701942  5452 sgd_solver.cpp:105] Iteration 126200, lr = 0.001
I1107 17:37:28.177582  5452 solver.cpp:218] Iteration 126300 (11.7989 iter/s, 8.47535s/100 iters), loss = 0.0253923
I1107 17:37:28.177582  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:37:28.177582  5452 solver.cpp:237]     Train net output #1: loss = 0.0253918 (* 1 = 0.0253918 loss)
I1107 17:37:28.177582  5452 sgd_solver.cpp:105] Iteration 126300, lr = 0.001
I1107 17:37:36.652313  5452 solver.cpp:218] Iteration 126400 (11.8009 iter/s, 8.47394s/100 iters), loss = 0.0317852
I1107 17:37:36.652313  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:37:36.652313  5452 solver.cpp:237]     Train net output #1: loss = 0.0317848 (* 1 = 0.0317848 loss)
I1107 17:37:36.652313  5452 sgd_solver.cpp:105] Iteration 126400, lr = 0.001
I1107 17:37:44.713102 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:37:45.048130  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_126500.caffemodel
I1107 17:37:45.078140  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_126500.solverstate
I1107 17:37:45.087141  5452 solver.cpp:330] Iteration 126500, Testing net (#0)
I1107 17:37:45.087141  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:37:47.064291 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:37:47.143295  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1107 17:37:47.143295  5452 solver.cpp:397]     Test net output #1: loss = 0.3044 (* 1 = 0.3044 loss)
I1107 17:37:47.223309  5452 solver.cpp:218] Iteration 126500 (9.4596 iter/s, 10.5713s/100 iters), loss = 0.041145
I1107 17:37:47.224308  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:37:47.224308  5452 solver.cpp:237]     Train net output #1: loss = 0.0411445 (* 1 = 0.0411445 loss)
I1107 17:37:47.224308  5452 sgd_solver.cpp:105] Iteration 126500, lr = 0.001
I1107 17:37:55.702672  5452 solver.cpp:218] Iteration 126600 (11.794 iter/s, 8.47885s/100 iters), loss = 0.0252939
I1107 17:37:55.703673  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:37:55.703673  5452 solver.cpp:237]     Train net output #1: loss = 0.0252934 (* 1 = 0.0252934 loss)
I1107 17:37:55.703673  5452 sgd_solver.cpp:105] Iteration 126600, lr = 0.001
I1107 17:38:04.184296  5452 solver.cpp:218] Iteration 126700 (11.791 iter/s, 8.48101s/100 iters), loss = 0.0186649
I1107 17:38:04.184296  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:38:04.184296  5452 solver.cpp:237]     Train net output #1: loss = 0.0186645 (* 1 = 0.0186645 loss)
I1107 17:38:04.184296  5452 sgd_solver.cpp:105] Iteration 126700, lr = 0.001
I1107 17:38:12.659119  5452 solver.cpp:218] Iteration 126800 (11.8004 iter/s, 8.47427s/100 iters), loss = 0.0368766
I1107 17:38:12.659119  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:38:12.659119  5452 solver.cpp:237]     Train net output #1: loss = 0.0368762 (* 1 = 0.0368762 loss)
I1107 17:38:12.659119  5452 sgd_solver.cpp:105] Iteration 126800, lr = 0.001
I1107 17:38:21.447405  5452 solver.cpp:218] Iteration 126900 (11.3794 iter/s, 8.78781s/100 iters), loss = 0.0243756
I1107 17:38:21.448407  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:38:21.448407  5452 solver.cpp:237]     Train net output #1: loss = 0.0243752 (* 1 = 0.0243752 loss)
I1107 17:38:21.448407  5452 sgd_solver.cpp:105] Iteration 126900, lr = 0.001
I1107 17:38:29.620666 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:38:29.965684  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_127000.caffemodel
I1107 17:38:29.996189  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_127000.solverstate
I1107 17:38:30.005690  5452 solver.cpp:330] Iteration 127000, Testing net (#0)
I1107 17:38:30.005690  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:38:32.026885 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:38:32.106887  5452 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 17:38:32.106887  5452 solver.cpp:397]     Test net output #1: loss = 0.29925 (* 1 = 0.29925 loss)
I1107 17:38:32.187892  5452 solver.cpp:218] Iteration 127000 (9.31119 iter/s, 10.7398s/100 iters), loss = 0.0372155
I1107 17:38:32.188894  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:38:32.188894  5452 solver.cpp:237]     Train net output #1: loss = 0.0372151 (* 1 = 0.0372151 loss)
I1107 17:38:32.188894  5452 sgd_solver.cpp:105] Iteration 127000, lr = 0.001
I1107 17:38:40.807940  5452 solver.cpp:218] Iteration 127100 (11.6024 iter/s, 8.61894s/100 iters), loss = 0.0452614
I1107 17:38:40.807940  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:38:40.807940  5452 solver.cpp:237]     Train net output #1: loss = 0.045261 (* 1 = 0.045261 loss)
I1107 17:38:40.808439  5452 sgd_solver.cpp:105] Iteration 127100, lr = 0.001
I1107 17:38:49.410480  5452 solver.cpp:218] Iteration 127200 (11.6254 iter/s, 8.60188s/100 iters), loss = 0.023717
I1107 17:38:49.410480  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:38:49.410480  5452 solver.cpp:237]     Train net output #1: loss = 0.0237166 (* 1 = 0.0237166 loss)
I1107 17:38:49.410979  5452 sgd_solver.cpp:105] Iteration 127200, lr = 0.001
I1107 17:38:57.910382  5452 solver.cpp:218] Iteration 127300 (11.7656 iter/s, 8.49939s/100 iters), loss = 0.022959
I1107 17:38:57.910382  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:38:57.910881  5452 solver.cpp:237]     Train net output #1: loss = 0.0229586 (* 1 = 0.0229586 loss)
I1107 17:38:57.910881  5452 sgd_solver.cpp:105] Iteration 127300, lr = 0.001
I1107 17:39:06.414281  5452 solver.cpp:218] Iteration 127400 (11.76 iter/s, 8.50337s/100 iters), loss = 0.0229132
I1107 17:39:06.414281  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:39:06.414782  5452 solver.cpp:237]     Train net output #1: loss = 0.0229128 (* 1 = 0.0229128 loss)
I1107 17:39:06.414782  5452 sgd_solver.cpp:105] Iteration 127400, lr = 0.001
I1107 17:39:14.509697 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:39:14.847717  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_127500.caffemodel
I1107 17:39:14.878718  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_127500.solverstate
I1107 17:39:14.887718  5452 solver.cpp:330] Iteration 127500, Testing net (#0)
I1107 17:39:14.887718  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:39:16.881027 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:39:16.960044  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 17:39:16.960044  5452 solver.cpp:397]     Test net output #1: loss = 0.300816 (* 1 = 0.300816 loss)
I1107 17:39:17.043066  5452 solver.cpp:218] Iteration 127500 (9.40923 iter/s, 10.6279s/100 iters), loss = 0.0643999
I1107 17:39:17.043066  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:39:17.043066  5452 solver.cpp:237]     Train net output #1: loss = 0.0643995 (* 1 = 0.0643995 loss)
I1107 17:39:17.043066  5452 sgd_solver.cpp:105] Iteration 127500, lr = 0.001
I1107 17:39:25.544033  5452 solver.cpp:218] Iteration 127600 (11.7632 iter/s, 8.50107s/100 iters), loss = 0.0442557
I1107 17:39:25.544033  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:39:25.544033  5452 solver.cpp:237]     Train net output #1: loss = 0.0442553 (* 1 = 0.0442553 loss)
I1107 17:39:25.544033  5452 sgd_solver.cpp:105] Iteration 127600, lr = 0.001
I1107 17:39:34.060263  5452 solver.cpp:218] Iteration 127700 (11.7431 iter/s, 8.51563s/100 iters), loss = 0.0184525
I1107 17:39:34.060263  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:39:34.060263  5452 solver.cpp:237]     Train net output #1: loss = 0.0184521 (* 1 = 0.0184521 loss)
I1107 17:39:34.060263  5452 sgd_solver.cpp:105] Iteration 127700, lr = 0.001
I1107 17:39:42.566577  5452 solver.cpp:218] Iteration 127800 (11.7568 iter/s, 8.50569s/100 iters), loss = 0.0316549
I1107 17:39:42.566577  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:39:42.566577  5452 solver.cpp:237]     Train net output #1: loss = 0.0316545 (* 1 = 0.0316545 loss)
I1107 17:39:42.566577  5452 sgd_solver.cpp:105] Iteration 127800, lr = 0.001
I1107 17:39:51.083237  5452 solver.cpp:218] Iteration 127900 (11.7419 iter/s, 8.51649s/100 iters), loss = 0.0251574
I1107 17:39:51.083237  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:39:51.083237  5452 solver.cpp:237]     Train net output #1: loss = 0.025157 (* 1 = 0.025157 loss)
I1107 17:39:51.083237  5452 sgd_solver.cpp:105] Iteration 127900, lr = 0.001
I1107 17:39:59.177194 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:39:59.512917  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_128000.caffemodel
I1107 17:39:59.539711  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_128000.solverstate
I1107 17:39:59.548727  5452 solver.cpp:330] Iteration 128000, Testing net (#0)
I1107 17:39:59.548727  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:40:01.531780 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:40:01.611073  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 17:40:01.611073  5452 solver.cpp:397]     Test net output #1: loss = 0.304656 (* 1 = 0.304656 loss)
I1107 17:40:01.692260  5452 solver.cpp:218] Iteration 128000 (9.42687 iter/s, 10.608s/100 iters), loss = 0.0294284
I1107 17:40:01.692260  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:40:01.692260  5452 solver.cpp:237]     Train net output #1: loss = 0.0294279 (* 1 = 0.0294279 loss)
I1107 17:40:01.692260  5452 sgd_solver.cpp:105] Iteration 128000, lr = 0.001
I1107 17:40:10.341800  5452 solver.cpp:218] Iteration 128100 (11.5615 iter/s, 8.6494s/100 iters), loss = 0.0628409
I1107 17:40:10.341800  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:40:10.341800  5452 solver.cpp:237]     Train net output #1: loss = 0.0628405 (* 1 = 0.0628405 loss)
I1107 17:40:10.341800  5452 sgd_solver.cpp:105] Iteration 128100, lr = 0.001
I1107 17:40:18.972826  5452 solver.cpp:218] Iteration 128200 (11.5869 iter/s, 8.6304s/100 iters), loss = 0.0483033
I1107 17:40:18.972826  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:40:18.972826  5452 solver.cpp:237]     Train net output #1: loss = 0.0483029 (* 1 = 0.0483029 loss)
I1107 17:40:18.972826  5452 sgd_solver.cpp:105] Iteration 128200, lr = 0.001
I1107 17:40:27.494700  5452 solver.cpp:218] Iteration 128300 (11.7352 iter/s, 8.5214s/100 iters), loss = 0.0242709
I1107 17:40:27.494700  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:40:27.494700  5452 solver.cpp:237]     Train net output #1: loss = 0.0242705 (* 1 = 0.0242705 loss)
I1107 17:40:27.494700  5452 sgd_solver.cpp:105] Iteration 128300, lr = 0.001
I1107 17:40:36.063050  5452 solver.cpp:218] Iteration 128400 (11.672 iter/s, 8.56754s/100 iters), loss = 0.0320664
I1107 17:40:36.063050  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:40:36.063050  5452 solver.cpp:237]     Train net output #1: loss = 0.032066 (* 1 = 0.032066 loss)
I1107 17:40:36.063050  5452 sgd_solver.cpp:105] Iteration 128400, lr = 0.001
I1107 17:40:44.195031 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:40:44.531025  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_128500.caffemodel
I1107 17:40:44.560540  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_128500.solverstate
I1107 17:40:44.569046  5452 solver.cpp:330] Iteration 128500, Testing net (#0)
I1107 17:40:44.569046  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:40:46.548764 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:40:46.627766  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9169
I1107 17:40:46.627766  5452 solver.cpp:397]     Test net output #1: loss = 0.308866 (* 1 = 0.308866 loss)
I1107 17:40:46.707687  5452 solver.cpp:218] Iteration 128500 (9.39479 iter/s, 10.6442s/100 iters), loss = 0.0296567
I1107 17:40:46.707687  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:40:46.707687  5452 solver.cpp:237]     Train net output #1: loss = 0.0296563 (* 1 = 0.0296563 loss)
I1107 17:40:46.707687  5452 sgd_solver.cpp:105] Iteration 128500, lr = 0.001
I1107 17:40:55.247974  5452 solver.cpp:218] Iteration 128600 (11.7106 iter/s, 8.53931s/100 iters), loss = 0.0490781
I1107 17:40:55.247974  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:40:55.247974  5452 solver.cpp:237]     Train net output #1: loss = 0.0490777 (* 1 = 0.0490777 loss)
I1107 17:40:55.247974  5452 sgd_solver.cpp:105] Iteration 128600, lr = 0.001
I1107 17:41:03.792453  5452 solver.cpp:218] Iteration 128700 (11.7041 iter/s, 8.54398s/100 iters), loss = 0.0275184
I1107 17:41:03.792453  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:41:03.792453  5452 solver.cpp:237]     Train net output #1: loss = 0.0275179 (* 1 = 0.0275179 loss)
I1107 17:41:03.792453  5452 sgd_solver.cpp:105] Iteration 128700, lr = 0.001
I1107 17:41:12.363775  5452 solver.cpp:218] Iteration 128800 (11.6668 iter/s, 8.57136s/100 iters), loss = 0.0228572
I1107 17:41:12.363775  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:41:12.363775  5452 solver.cpp:237]     Train net output #1: loss = 0.0228568 (* 1 = 0.0228568 loss)
I1107 17:41:12.363775  5452 sgd_solver.cpp:105] Iteration 128800, lr = 0.001
I1107 17:41:20.889293  5452 solver.cpp:218] Iteration 128900 (11.7307 iter/s, 8.52462s/100 iters), loss = 0.0402554
I1107 17:41:20.889796  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:41:20.889796  5452 solver.cpp:237]     Train net output #1: loss = 0.0402549 (* 1 = 0.0402549 loss)
I1107 17:41:20.889796  5452 sgd_solver.cpp:105] Iteration 128900, lr = 0.001
I1107 17:41:29.011041 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:41:29.348052  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_129000.caffemodel
I1107 17:41:29.378059  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_129000.solverstate
I1107 17:41:29.387061  5452 solver.cpp:330] Iteration 129000, Testing net (#0)
I1107 17:41:29.387061  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:41:31.371338 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:41:31.450346  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9196
I1107 17:41:31.450346  5452 solver.cpp:397]     Test net output #1: loss = 0.304077 (* 1 = 0.304077 loss)
I1107 17:41:31.531352  5452 solver.cpp:218] Iteration 129000 (9.39679 iter/s, 10.6419s/100 iters), loss = 0.0394633
I1107 17:41:31.531352  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:41:31.531352  5452 solver.cpp:237]     Train net output #1: loss = 0.0394629 (* 1 = 0.0394629 loss)
I1107 17:41:31.531352  5452 sgd_solver.cpp:105] Iteration 129000, lr = 0.001
I1107 17:41:40.049123  5452 solver.cpp:218] Iteration 129100 (11.7417 iter/s, 8.51665s/100 iters), loss = 0.0529039
I1107 17:41:40.049123  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:41:40.049123  5452 solver.cpp:237]     Train net output #1: loss = 0.0529034 (* 1 = 0.0529034 loss)
I1107 17:41:40.049123  5452 sgd_solver.cpp:105] Iteration 129100, lr = 0.001
I1107 17:41:48.559310  5452 solver.cpp:218] Iteration 129200 (11.7511 iter/s, 8.50988s/100 iters), loss = 0.0237725
I1107 17:41:48.559810  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:41:48.559810  5452 solver.cpp:237]     Train net output #1: loss = 0.0237721 (* 1 = 0.0237721 loss)
I1107 17:41:48.559810  5452 sgd_solver.cpp:105] Iteration 129200, lr = 0.001
I1107 17:41:57.125108  5452 solver.cpp:218] Iteration 129300 (11.6746 iter/s, 8.56564s/100 iters), loss = 0.0208717
I1107 17:41:57.126104  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:41:57.126104  5452 solver.cpp:237]     Train net output #1: loss = 0.0208713 (* 1 = 0.0208713 loss)
I1107 17:41:57.126104  5452 sgd_solver.cpp:105] Iteration 129300, lr = 0.001
I1107 17:42:05.644626  5452 solver.cpp:218] Iteration 129400 (11.7388 iter/s, 8.51879s/100 iters), loss = 0.0524228
I1107 17:42:05.644626  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:42:05.644626  5452 solver.cpp:237]     Train net output #1: loss = 0.0524224 (* 1 = 0.0524224 loss)
I1107 17:42:05.644626  5452 sgd_solver.cpp:105] Iteration 129400, lr = 0.001
I1107 17:42:13.734412 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:42:14.071929  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_129500.caffemodel
I1107 17:42:14.101434  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_129500.solverstate
I1107 17:42:14.109434  5452 solver.cpp:330] Iteration 129500, Testing net (#0)
I1107 17:42:14.109434  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:42:16.099591 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:42:16.178594  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9178
I1107 17:42:16.178594  5452 solver.cpp:397]     Test net output #1: loss = 0.306618 (* 1 = 0.306618 loss)
I1107 17:42:16.260094  5452 solver.cpp:218] Iteration 129500 (9.42143 iter/s, 10.6141s/100 iters), loss = 0.0428503
I1107 17:42:16.260094  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:42:16.260094  5452 solver.cpp:237]     Train net output #1: loss = 0.0428499 (* 1 = 0.0428499 loss)
I1107 17:42:16.260094  5452 sgd_solver.cpp:105] Iteration 129500, lr = 0.001
I1107 17:42:24.775213  5452 solver.cpp:218] Iteration 129600 (11.7433 iter/s, 8.5155s/100 iters), loss = 0.0307647
I1107 17:42:24.775213  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:42:24.775213  5452 solver.cpp:237]     Train net output #1: loss = 0.0307643 (* 1 = 0.0307643 loss)
I1107 17:42:24.775213  5452 sgd_solver.cpp:105] Iteration 129600, lr = 0.001
I1107 17:42:33.282088  5452 solver.cpp:218] Iteration 129700 (11.757 iter/s, 8.50554s/100 iters), loss = 0.0250885
I1107 17:42:33.282088  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:42:33.282088  5452 solver.cpp:237]     Train net output #1: loss = 0.025088 (* 1 = 0.025088 loss)
I1107 17:42:33.282088  5452 sgd_solver.cpp:105] Iteration 129700, lr = 0.001
I1107 17:42:41.792574  5452 solver.cpp:218] Iteration 129800 (11.7498 iter/s, 8.51081s/100 iters), loss = 0.0227528
I1107 17:42:41.792574  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:42:41.792574  5452 solver.cpp:237]     Train net output #1: loss = 0.0227523 (* 1 = 0.0227523 loss)
I1107 17:42:41.792574  5452 sgd_solver.cpp:105] Iteration 129800, lr = 0.001
I1107 17:42:50.322654  5452 solver.cpp:218] Iteration 129900 (11.7248 iter/s, 8.5289s/100 iters), loss = 0.0329216
I1107 17:42:50.322654  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:42:50.322654  5452 solver.cpp:237]     Train net output #1: loss = 0.0329212 (* 1 = 0.0329212 loss)
I1107 17:42:50.322654  5452 sgd_solver.cpp:105] Iteration 129900, lr = 0.001
I1107 17:42:58.428503 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:42:58.765539  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_130000.caffemodel
I1107 17:42:58.794539  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_130000.solverstate
I1107 17:42:58.803540  5452 solver.cpp:330] Iteration 130000, Testing net (#0)
I1107 17:42:58.803540  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:43:00.785907 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:43:00.864935  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1107 17:43:00.864935  5452 solver.cpp:397]     Test net output #1: loss = 0.303395 (* 1 = 0.303395 loss)
I1107 17:43:00.946939  5452 solver.cpp:218] Iteration 130000 (9.41254 iter/s, 10.6241s/100 iters), loss = 0.0334512
I1107 17:43:00.946939  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:43:00.946939  5452 solver.cpp:237]     Train net output #1: loss = 0.0334508 (* 1 = 0.0334508 loss)
I1107 17:43:00.946939  5452 sgd_solver.cpp:105] Iteration 130000, lr = 0.001
I1107 17:43:09.449616  5452 solver.cpp:218] Iteration 130100 (11.7623 iter/s, 8.50171s/100 iters), loss = 0.044298
I1107 17:43:09.449616  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:43:09.449616  5452 solver.cpp:237]     Train net output #1: loss = 0.0442976 (* 1 = 0.0442976 loss)
I1107 17:43:09.449616  5452 sgd_solver.cpp:105] Iteration 130100, lr = 0.001
I1107 17:43:17.999173  5452 solver.cpp:218] Iteration 130200 (11.6973 iter/s, 8.54897s/100 iters), loss = 0.0196359
I1107 17:43:17.999173  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:43:17.999173  5452 solver.cpp:237]     Train net output #1: loss = 0.0196355 (* 1 = 0.0196355 loss)
I1107 17:43:17.999173  5452 sgd_solver.cpp:105] Iteration 130200, lr = 0.001
I1107 17:43:26.530968  5452 solver.cpp:218] Iteration 130300 (11.7211 iter/s, 8.5316s/100 iters), loss = 0.0314484
I1107 17:43:26.530968  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:43:26.530968  5452 solver.cpp:237]     Train net output #1: loss = 0.031448 (* 1 = 0.031448 loss)
I1107 17:43:26.530968  5452 sgd_solver.cpp:105] Iteration 130300, lr = 0.001
I1107 17:43:35.041784  5452 solver.cpp:218] Iteration 130400 (11.7506 iter/s, 8.51017s/100 iters), loss = 0.0413609
I1107 17:43:35.041784  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:43:35.041784  5452 solver.cpp:237]     Train net output #1: loss = 0.0413605 (* 1 = 0.0413605 loss)
I1107 17:43:35.041784  5452 sgd_solver.cpp:105] Iteration 130400, lr = 0.001
I1107 17:43:43.136075 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:43:43.471098  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_130500.caffemodel
I1107 17:43:43.502099  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_130500.solverstate
I1107 17:43:43.511097  5452 solver.cpp:330] Iteration 130500, Testing net (#0)
I1107 17:43:43.511097  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:43:45.494364 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:43:45.573408  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9202
I1107 17:43:45.573408  5452 solver.cpp:397]     Test net output #1: loss = 0.297832 (* 1 = 0.297832 loss)
I1107 17:43:45.655418  5452 solver.cpp:218] Iteration 130500 (9.42234 iter/s, 10.6131s/100 iters), loss = 0.0357112
I1107 17:43:45.655418  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:43:45.655418  5452 solver.cpp:237]     Train net output #1: loss = 0.0357107 (* 1 = 0.0357107 loss)
I1107 17:43:45.655418  5452 sgd_solver.cpp:105] Iteration 130500, lr = 0.001
I1107 17:43:54.178241  5452 solver.cpp:218] Iteration 130600 (11.7338 iter/s, 8.52237s/100 iters), loss = 0.0420547
I1107 17:43:54.178241  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:43:54.178241  5452 solver.cpp:237]     Train net output #1: loss = 0.0420543 (* 1 = 0.0420543 loss)
I1107 17:43:54.178241  5452 sgd_solver.cpp:105] Iteration 130600, lr = 0.001
I1107 17:44:02.681157  5452 solver.cpp:218] Iteration 130700 (11.7606 iter/s, 8.50296s/100 iters), loss = 0.0271084
I1107 17:44:02.681157  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:44:02.681157  5452 solver.cpp:237]     Train net output #1: loss = 0.027108 (* 1 = 0.027108 loss)
I1107 17:44:02.681157  5452 sgd_solver.cpp:105] Iteration 130700, lr = 0.001
I1107 17:44:11.198007  5452 solver.cpp:218] Iteration 130800 (11.742 iter/s, 8.51643s/100 iters), loss = 0.0177243
I1107 17:44:11.199007  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:44:11.199007  5452 solver.cpp:237]     Train net output #1: loss = 0.0177239 (* 1 = 0.0177239 loss)
I1107 17:44:11.199007  5452 sgd_solver.cpp:105] Iteration 130800, lr = 0.001
I1107 17:44:19.710861  5452 solver.cpp:218] Iteration 130900 (11.748 iter/s, 8.51212s/100 iters), loss = 0.0260381
I1107 17:44:19.710861  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:44:19.710861  5452 solver.cpp:237]     Train net output #1: loss = 0.0260377 (* 1 = 0.0260377 loss)
I1107 17:44:19.710861  5452 sgd_solver.cpp:105] Iteration 130900, lr = 0.001
I1107 17:44:27.896792 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:44:28.239334  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_131000.caffemodel
I1107 17:44:28.271833  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_131000.solverstate
I1107 17:44:28.282833  5452 solver.cpp:330] Iteration 131000, Testing net (#0)
I1107 17:44:28.283834  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:44:30.276046 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:44:30.356058  5452 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 17:44:30.356058  5452 solver.cpp:397]     Test net output #1: loss = 0.303651 (* 1 = 0.303651 loss)
I1107 17:44:30.438561  5452 solver.cpp:218] Iteration 131000 (9.32282 iter/s, 10.7264s/100 iters), loss = 0.0364051
I1107 17:44:30.438561  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:44:30.438561  5452 solver.cpp:237]     Train net output #1: loss = 0.0364047 (* 1 = 0.0364047 loss)
I1107 17:44:30.438561  5452 sgd_solver.cpp:105] Iteration 131000, lr = 0.001
I1107 17:44:38.948587  5452 solver.cpp:218] Iteration 131100 (11.7515 iter/s, 8.50953s/100 iters), loss = 0.0247349
I1107 17:44:38.948587  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:44:38.948587  5452 solver.cpp:237]     Train net output #1: loss = 0.0247345 (* 1 = 0.0247345 loss)
I1107 17:44:38.948587  5452 sgd_solver.cpp:105] Iteration 131100, lr = 0.001
I1107 17:44:47.597622  5452 solver.cpp:218] Iteration 131200 (11.5619 iter/s, 8.64911s/100 iters), loss = 0.0211475
I1107 17:44:47.597622  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:44:47.597622  5452 solver.cpp:237]     Train net output #1: loss = 0.0211471 (* 1 = 0.0211471 loss)
I1107 17:44:47.597622  5452 sgd_solver.cpp:105] Iteration 131200, lr = 0.001
I1107 17:44:56.195430  5452 solver.cpp:218] Iteration 131300 (11.632 iter/s, 8.597s/100 iters), loss = 0.0216037
I1107 17:44:56.195430  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:44:56.195430  5452 solver.cpp:237]     Train net output #1: loss = 0.0216033 (* 1 = 0.0216033 loss)
I1107 17:44:56.195430  5452 sgd_solver.cpp:105] Iteration 131300, lr = 0.001
I1107 17:45:04.811014  5452 solver.cpp:218] Iteration 131400 (11.6074 iter/s, 8.61523s/100 iters), loss = 0.0311882
I1107 17:45:04.811014  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:45:04.811014  5452 solver.cpp:237]     Train net output #1: loss = 0.0311878 (* 1 = 0.0311878 loss)
I1107 17:45:04.811014  5452 sgd_solver.cpp:105] Iteration 131400, lr = 0.001
I1107 17:45:13.105157 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:45:13.441680  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_131500.caffemodel
I1107 17:45:13.471688  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_131500.solverstate
I1107 17:45:13.480686  5452 solver.cpp:330] Iteration 131500, Testing net (#0)
I1107 17:45:13.480686  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:45:15.468906 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:45:15.548913  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9172
I1107 17:45:15.548913  5452 solver.cpp:397]     Test net output #1: loss = 0.311253 (* 1 = 0.311253 loss)
I1107 17:45:15.629911  5452 solver.cpp:218] Iteration 131500 (9.24356 iter/s, 10.8183s/100 iters), loss = 0.0435791
I1107 17:45:15.629911  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:45:15.629911  5452 solver.cpp:237]     Train net output #1: loss = 0.0435787 (* 1 = 0.0435787 loss)
I1107 17:45:15.629911  5452 sgd_solver.cpp:105] Iteration 131500, lr = 0.001
I1107 17:45:24.326822  5452 solver.cpp:218] Iteration 131600 (11.4991 iter/s, 8.69634s/100 iters), loss = 0.0277804
I1107 17:45:24.326822  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:45:24.326822  5452 solver.cpp:237]     Train net output #1: loss = 0.02778 (* 1 = 0.02778 loss)
I1107 17:45:24.326822  5452 sgd_solver.cpp:105] Iteration 131600, lr = 0.001
I1107 17:45:32.928221  5452 solver.cpp:218] Iteration 131700 (11.6274 iter/s, 8.60037s/100 iters), loss = 0.0253609
I1107 17:45:32.928221  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:45:32.928221  5452 solver.cpp:237]     Train net output #1: loss = 0.0253605 (* 1 = 0.0253605 loss)
I1107 17:45:32.928221  5452 sgd_solver.cpp:105] Iteration 131700, lr = 0.001
I1107 17:45:41.565351  5452 solver.cpp:218] Iteration 131800 (11.5789 iter/s, 8.63639s/100 iters), loss = 0.0207901
I1107 17:45:41.565351  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:45:41.565351  5452 solver.cpp:237]     Train net output #1: loss = 0.0207896 (* 1 = 0.0207896 loss)
I1107 17:45:41.565351  5452 sgd_solver.cpp:105] Iteration 131800, lr = 0.001
I1107 17:45:50.226013  5452 solver.cpp:218] Iteration 131900 (11.5465 iter/s, 8.66063s/100 iters), loss = 0.0281747
I1107 17:45:50.226013  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:45:50.226013  5452 solver.cpp:237]     Train net output #1: loss = 0.0281743 (* 1 = 0.0281743 loss)
I1107 17:45:50.226013  5452 sgd_solver.cpp:105] Iteration 131900, lr = 0.001
I1107 17:45:58.389714 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:45:58.726733  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_132000.caffemodel
I1107 17:45:58.754735  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_132000.solverstate
I1107 17:45:58.763736  5452 solver.cpp:330] Iteration 132000, Testing net (#0)
I1107 17:45:58.764736  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:46:00.763989 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:46:00.843509  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 17:46:00.843994  5452 solver.cpp:397]     Test net output #1: loss = 0.305651 (* 1 = 0.305651 loss)
I1107 17:46:00.924989  5452 solver.cpp:218] Iteration 132000 (9.34754 iter/s, 10.698s/100 iters), loss = 0.0262596
I1107 17:46:00.924989  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:46:00.924989  5452 solver.cpp:237]     Train net output #1: loss = 0.0262591 (* 1 = 0.0262591 loss)
I1107 17:46:00.924989  5452 sgd_solver.cpp:105] Iteration 132000, lr = 0.001
I1107 17:46:09.583973  5452 solver.cpp:218] Iteration 132100 (11.5491 iter/s, 8.65872s/100 iters), loss = 0.0373028
I1107 17:46:09.583973  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:46:09.583973  5452 solver.cpp:237]     Train net output #1: loss = 0.0373023 (* 1 = 0.0373023 loss)
I1107 17:46:09.583973  5452 sgd_solver.cpp:105] Iteration 132100, lr = 0.001
I1107 17:46:18.227537  5452 solver.cpp:218] Iteration 132200 (11.5702 iter/s, 8.6429s/100 iters), loss = 0.0260176
I1107 17:46:18.227537  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:46:18.227537  5452 solver.cpp:237]     Train net output #1: loss = 0.0260171 (* 1 = 0.0260171 loss)
I1107 17:46:18.227537  5452 sgd_solver.cpp:105] Iteration 132200, lr = 0.001
I1107 17:46:26.875270  5452 solver.cpp:218] Iteration 132300 (11.564 iter/s, 8.64751s/100 iters), loss = 0.0223925
I1107 17:46:26.875270  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:46:26.875270  5452 solver.cpp:237]     Train net output #1: loss = 0.0223921 (* 1 = 0.0223921 loss)
I1107 17:46:26.875270  5452 sgd_solver.cpp:105] Iteration 132300, lr = 0.001
I1107 17:46:35.461113  5452 solver.cpp:218] Iteration 132400 (11.6487 iter/s, 8.58463s/100 iters), loss = 0.0273828
I1107 17:46:35.461113  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:46:35.461113  5452 solver.cpp:237]     Train net output #1: loss = 0.0273824 (* 1 = 0.0273824 loss)
I1107 17:46:35.461113  5452 sgd_solver.cpp:105] Iteration 132400, lr = 0.001
I1107 17:46:43.558938 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:46:43.895972  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_132500.caffemodel
I1107 17:46:43.923971  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_132500.solverstate
I1107 17:46:43.933974  5452 solver.cpp:330] Iteration 132500, Testing net (#0)
I1107 17:46:43.933974  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:46:45.916206 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:46:45.994221  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9217
I1107 17:46:45.995214  5452 solver.cpp:397]     Test net output #1: loss = 0.301335 (* 1 = 0.301335 loss)
I1107 17:46:46.077216  5452 solver.cpp:218] Iteration 132500 (9.42018 iter/s, 10.6155s/100 iters), loss = 0.0479241
I1107 17:46:46.077216  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:46:46.077216  5452 solver.cpp:237]     Train net output #1: loss = 0.0479237 (* 1 = 0.0479237 loss)
I1107 17:46:46.077216  5452 sgd_solver.cpp:105] Iteration 132500, lr = 0.001
I1107 17:46:54.607316  5452 solver.cpp:218] Iteration 132600 (11.7238 iter/s, 8.52967s/100 iters), loss = 0.0466425
I1107 17:46:54.607316  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:46:54.607316  5452 solver.cpp:237]     Train net output #1: loss = 0.046642 (* 1 = 0.046642 loss)
I1107 17:46:54.607316  5452 sgd_solver.cpp:105] Iteration 132600, lr = 0.001
I1107 17:47:03.142658  5452 solver.cpp:218] Iteration 132700 (11.7162 iter/s, 8.53521s/100 iters), loss = 0.0368581
I1107 17:47:03.142658  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:47:03.142658  5452 solver.cpp:237]     Train net output #1: loss = 0.0368577 (* 1 = 0.0368577 loss)
I1107 17:47:03.142658  5452 sgd_solver.cpp:105] Iteration 132700, lr = 0.001
I1107 17:47:11.676825  5452 solver.cpp:218] Iteration 132800 (11.7188 iter/s, 8.53327s/100 iters), loss = 0.0382418
I1107 17:47:11.676825  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:47:11.676825  5452 solver.cpp:237]     Train net output #1: loss = 0.0382414 (* 1 = 0.0382414 loss)
I1107 17:47:11.676825  5452 sgd_solver.cpp:105] Iteration 132800, lr = 0.001
I1107 17:47:20.346315  5452 solver.cpp:218] Iteration 132900 (11.5347 iter/s, 8.66948s/100 iters), loss = 0.0256352
I1107 17:47:20.346315  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:47:20.346315  5452 solver.cpp:237]     Train net output #1: loss = 0.0256348 (* 1 = 0.0256348 loss)
I1107 17:47:20.346315  5452 sgd_solver.cpp:105] Iteration 132900, lr = 0.001
I1107 17:47:28.558171 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:47:28.893820  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_133000.caffemodel
I1107 17:47:28.921391  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_133000.solverstate
I1107 17:47:28.930403  5452 solver.cpp:330] Iteration 133000, Testing net (#0)
I1107 17:47:28.930403  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:47:30.934444 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:47:31.014467  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1107 17:47:31.014467  5452 solver.cpp:397]     Test net output #1: loss = 0.304473 (* 1 = 0.304473 loss)
I1107 17:47:31.096631  5452 solver.cpp:218] Iteration 133000 (9.30286 iter/s, 10.7494s/100 iters), loss = 0.0408728
I1107 17:47:31.096631  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:47:31.096631  5452 solver.cpp:237]     Train net output #1: loss = 0.0408724 (* 1 = 0.0408724 loss)
I1107 17:47:31.096631  5452 sgd_solver.cpp:105] Iteration 133000, lr = 0.001
I1107 17:47:39.714428  5452 solver.cpp:218] Iteration 133100 (11.6051 iter/s, 8.61691s/100 iters), loss = 0.0335777
I1107 17:47:39.714428  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:47:39.714428  5452 solver.cpp:237]     Train net output #1: loss = 0.0335772 (* 1 = 0.0335772 loss)
I1107 17:47:39.714428  5452 sgd_solver.cpp:105] Iteration 133100, lr = 0.001
I1107 17:47:48.341773  5452 solver.cpp:218] Iteration 133200 (11.5909 iter/s, 8.62746s/100 iters), loss = 0.0375904
I1107 17:47:48.342764  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:47:48.342764  5452 solver.cpp:237]     Train net output #1: loss = 0.03759 (* 1 = 0.03759 loss)
I1107 17:47:48.342764  5452 sgd_solver.cpp:105] Iteration 133200, lr = 0.001
I1107 17:47:57.008472  5452 solver.cpp:218] Iteration 133300 (11.5405 iter/s, 8.66514s/100 iters), loss = 0.0235834
I1107 17:47:57.008472  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:47:57.008472  5452 solver.cpp:237]     Train net output #1: loss = 0.023583 (* 1 = 0.023583 loss)
I1107 17:47:57.008472  5452 sgd_solver.cpp:105] Iteration 133300, lr = 0.001
I1107 17:48:05.598218  5452 solver.cpp:218] Iteration 133400 (11.6421 iter/s, 8.58951s/100 iters), loss = 0.0277297
I1107 17:48:05.598218  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:48:05.598218  5452 solver.cpp:237]     Train net output #1: loss = 0.0277293 (* 1 = 0.0277293 loss)
I1107 17:48:05.598218  5452 sgd_solver.cpp:105] Iteration 133400, lr = 0.001
I1107 17:48:13.796613 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:48:14.132647  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_133500.caffemodel
I1107 17:48:14.161648  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_133500.solverstate
I1107 17:48:14.170647  5452 solver.cpp:330] Iteration 133500, Testing net (#0)
I1107 17:48:14.170647  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:48:16.179976 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:48:16.258978  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9166
I1107 17:48:16.258978  5452 solver.cpp:397]     Test net output #1: loss = 0.314451 (* 1 = 0.314451 loss)
I1107 17:48:16.339989  5452 solver.cpp:218] Iteration 133500 (9.30985 iter/s, 10.7413s/100 iters), loss = 0.0308367
I1107 17:48:16.339989  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:48:16.339989  5452 solver.cpp:237]     Train net output #1: loss = 0.0308363 (* 1 = 0.0308363 loss)
I1107 17:48:16.339989  5452 sgd_solver.cpp:105] Iteration 133500, lr = 0.001
I1107 17:48:24.994000  5452 solver.cpp:218] Iteration 133600 (11.5567 iter/s, 8.65301s/100 iters), loss = 0.0353181
I1107 17:48:24.994000  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:48:24.994000  5452 solver.cpp:237]     Train net output #1: loss = 0.0353177 (* 1 = 0.0353177 loss)
I1107 17:48:24.994000  5452 sgd_solver.cpp:105] Iteration 133600, lr = 0.001
I1107 17:48:33.523488  5452 solver.cpp:218] Iteration 133700 (11.7244 iter/s, 8.52924s/100 iters), loss = 0.0246899
I1107 17:48:33.523989  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:48:33.523989  5452 solver.cpp:237]     Train net output #1: loss = 0.0246894 (* 1 = 0.0246894 loss)
I1107 17:48:33.523989  5452 sgd_solver.cpp:105] Iteration 133700, lr = 0.001
I1107 17:48:42.133128  5452 solver.cpp:218] Iteration 133800 (11.6152 iter/s, 8.6094s/100 iters), loss = 0.0539857
I1107 17:48:42.133128  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:48:42.133128  5452 solver.cpp:237]     Train net output #1: loss = 0.0539853 (* 1 = 0.0539853 loss)
I1107 17:48:42.133128  5452 sgd_solver.cpp:105] Iteration 133800, lr = 0.001
I1107 17:48:50.802234  5452 solver.cpp:218] Iteration 133900 (11.5365 iter/s, 8.66814s/100 iters), loss = 0.0265259
I1107 17:48:50.802234  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:48:50.802234  5452 solver.cpp:237]     Train net output #1: loss = 0.0265255 (* 1 = 0.0265255 loss)
I1107 17:48:50.802234  5452 sgd_solver.cpp:105] Iteration 133900, lr = 0.001
I1107 17:48:59.020419 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:48:59.357964  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_134000.caffemodel
I1107 17:48:59.385964  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_134000.solverstate
I1107 17:48:59.394965  5452 solver.cpp:330] Iteration 134000, Testing net (#0)
I1107 17:48:59.394965  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:49:01.399250 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:49:01.479243  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1107 17:49:01.479243  5452 solver.cpp:397]     Test net output #1: loss = 0.306661 (* 1 = 0.306661 loss)
I1107 17:49:01.560245  5452 solver.cpp:218] Iteration 134000 (9.29571 iter/s, 10.7576s/100 iters), loss = 0.0400557
I1107 17:49:01.560245  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:49:01.560245  5452 solver.cpp:237]     Train net output #1: loss = 0.0400553 (* 1 = 0.0400553 loss)
I1107 17:49:01.560245  5452 sgd_solver.cpp:105] Iteration 134000, lr = 0.001
I1107 17:49:10.107223  5452 solver.cpp:218] Iteration 134100 (11.7006 iter/s, 8.5466s/100 iters), loss = 0.0407612
I1107 17:49:10.107223  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:49:10.107223  5452 solver.cpp:237]     Train net output #1: loss = 0.0407607 (* 1 = 0.0407607 loss)
I1107 17:49:10.107223  5452 sgd_solver.cpp:105] Iteration 134100, lr = 0.001
I1107 17:49:18.729475  5452 solver.cpp:218] Iteration 134200 (11.5994 iter/s, 8.62116s/100 iters), loss = 0.0244946
I1107 17:49:18.729475  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:49:18.729475  5452 solver.cpp:237]     Train net output #1: loss = 0.0244942 (* 1 = 0.0244942 loss)
I1107 17:49:18.729475  5452 sgd_solver.cpp:105] Iteration 134200, lr = 0.001
I1107 17:49:27.262264  5452 solver.cpp:218] Iteration 134300 (11.7201 iter/s, 8.53236s/100 iters), loss = 0.0230534
I1107 17:49:27.262264  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:49:27.262264  5452 solver.cpp:237]     Train net output #1: loss = 0.023053 (* 1 = 0.023053 loss)
I1107 17:49:27.262264  5452 sgd_solver.cpp:105] Iteration 134300, lr = 0.001
I1107 17:49:35.781373  5452 solver.cpp:218] Iteration 134400 (11.7394 iter/s, 8.51834s/100 iters), loss = 0.0273878
I1107 17:49:35.781373  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:49:35.781373  5452 solver.cpp:237]     Train net output #1: loss = 0.0273874 (* 1 = 0.0273874 loss)
I1107 17:49:35.781373  5452 sgd_solver.cpp:105] Iteration 134400, lr = 0.001
I1107 17:49:43.977660 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:49:44.318743  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_134500.caffemodel
I1107 17:49:44.353754  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_134500.solverstate
I1107 17:49:44.362751  5452 solver.cpp:330] Iteration 134500, Testing net (#0)
I1107 17:49:44.362751  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:49:46.354074 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:49:46.433073  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1107 17:49:46.433073  5452 solver.cpp:397]     Test net output #1: loss = 0.307742 (* 1 = 0.307742 loss)
I1107 17:49:46.515075  5452 solver.cpp:218] Iteration 134500 (9.31695 iter/s, 10.7331s/100 iters), loss = 0.0339619
I1107 17:49:46.515075  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:49:46.515075  5452 solver.cpp:237]     Train net output #1: loss = 0.0339615 (* 1 = 0.0339615 loss)
I1107 17:49:46.515075  5452 sgd_solver.cpp:105] Iteration 134500, lr = 0.001
I1107 17:49:55.099372  5452 solver.cpp:218] Iteration 134600 (11.6498 iter/s, 8.58383s/100 iters), loss = 0.0458817
I1107 17:49:55.099372  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:49:55.099372  5452 solver.cpp:237]     Train net output #1: loss = 0.0458813 (* 1 = 0.0458813 loss)
I1107 17:49:55.099372  5452 sgd_solver.cpp:105] Iteration 134600, lr = 0.001
I1107 17:50:03.708482  5452 solver.cpp:218] Iteration 134700 (11.6154 iter/s, 8.60923s/100 iters), loss = 0.0304573
I1107 17:50:03.708482  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:50:03.708482  5452 solver.cpp:237]     Train net output #1: loss = 0.0304569 (* 1 = 0.0304569 loss)
I1107 17:50:03.708482  5452 sgd_solver.cpp:105] Iteration 134700, lr = 0.001
I1107 17:50:12.351013  5452 solver.cpp:218] Iteration 134800 (11.5713 iter/s, 8.6421s/100 iters), loss = 0.038152
I1107 17:50:12.351013  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:50:12.351013  5452 solver.cpp:237]     Train net output #1: loss = 0.0381516 (* 1 = 0.0381516 loss)
I1107 17:50:12.351013  5452 sgd_solver.cpp:105] Iteration 134800, lr = 0.001
I1107 17:50:21.012825  5452 solver.cpp:218] Iteration 134900 (11.5463 iter/s, 8.66077s/100 iters), loss = 0.031115
I1107 17:50:21.012825  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:50:21.012825  5452 solver.cpp:237]     Train net output #1: loss = 0.0311146 (* 1 = 0.0311146 loss)
I1107 17:50:21.012825  5452 sgd_solver.cpp:105] Iteration 134900, lr = 0.001
I1107 17:50:29.219782 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:50:29.566357  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_135000.caffemodel
I1107 17:50:29.596374  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_135000.solverstate
I1107 17:50:29.605370  5452 solver.cpp:330] Iteration 135000, Testing net (#0)
I1107 17:50:29.606379  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:50:31.640944 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:50:31.721956  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 17:50:31.721956  5452 solver.cpp:397]     Test net output #1: loss = 0.307431 (* 1 = 0.307431 loss)
I1107 17:50:31.804961  5452 solver.cpp:218] Iteration 135000 (9.26616 iter/s, 10.792s/100 iters), loss = 0.0320884
I1107 17:50:31.805960  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:50:31.805960  5452 solver.cpp:237]     Train net output #1: loss = 0.032088 (* 1 = 0.032088 loss)
I1107 17:50:31.805960  5452 sgd_solver.cpp:105] Iteration 135000, lr = 0.001
I1107 17:50:40.472708  5452 solver.cpp:218] Iteration 135100 (11.5389 iter/s, 8.66631s/100 iters), loss = 0.0280586
I1107 17:50:40.472708  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:50:40.472708  5452 solver.cpp:237]     Train net output #1: loss = 0.0280581 (* 1 = 0.0280581 loss)
I1107 17:50:40.472708  5452 sgd_solver.cpp:105] Iteration 135100, lr = 0.001
I1107 17:50:49.112288  5452 solver.cpp:218] Iteration 135200 (11.5747 iter/s, 8.63953s/100 iters), loss = 0.022006
I1107 17:50:49.112288  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:50:49.112288  5452 solver.cpp:237]     Train net output #1: loss = 0.0220056 (* 1 = 0.0220056 loss)
I1107 17:50:49.112288  5452 sgd_solver.cpp:105] Iteration 135200, lr = 0.001
I1107 17:50:57.677461  5452 solver.cpp:218] Iteration 135300 (11.6756 iter/s, 8.56486s/100 iters), loss = 0.0206147
I1107 17:50:57.678459  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:50:57.678459  5452 solver.cpp:237]     Train net output #1: loss = 0.0206143 (* 1 = 0.0206143 loss)
I1107 17:50:57.678459  5452 sgd_solver.cpp:105] Iteration 135300, lr = 0.001
I1107 17:51:06.195377  5452 solver.cpp:218] Iteration 135400 (11.7417 iter/s, 8.51664s/100 iters), loss = 0.0284372
I1107 17:51:06.195377  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:51:06.195377  5452 solver.cpp:237]     Train net output #1: loss = 0.0284367 (* 1 = 0.0284367 loss)
I1107 17:51:06.195377  5452 sgd_solver.cpp:105] Iteration 135400, lr = 0.001
I1107 17:51:14.283176 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:51:14.619225  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_135500.caffemodel
I1107 17:51:14.651229  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_135500.solverstate
I1107 17:51:14.659229  5452 solver.cpp:330] Iteration 135500, Testing net (#0)
I1107 17:51:14.660230  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:51:16.642499 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:51:16.722039  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9174
I1107 17:51:16.722039  5452 solver.cpp:397]     Test net output #1: loss = 0.310868 (* 1 = 0.310868 loss)
I1107 17:51:16.803525  5452 solver.cpp:218] Iteration 135500 (9.42704 iter/s, 10.6078s/100 iters), loss = 0.0313282
I1107 17:51:16.803525  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:51:16.803525  5452 solver.cpp:237]     Train net output #1: loss = 0.0313278 (* 1 = 0.0313278 loss)
I1107 17:51:16.803525  5452 sgd_solver.cpp:105] Iteration 135500, lr = 0.001
I1107 17:51:25.364554  5452 solver.cpp:218] Iteration 135600 (11.6808 iter/s, 8.56105s/100 iters), loss = 0.0316448
I1107 17:51:25.365556  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:51:25.365556  5452 solver.cpp:237]     Train net output #1: loss = 0.0316444 (* 1 = 0.0316444 loss)
I1107 17:51:25.365556  5452 sgd_solver.cpp:105] Iteration 135600, lr = 0.001
I1107 17:51:33.931244  5452 solver.cpp:218] Iteration 135700 (11.6752 iter/s, 8.56519s/100 iters), loss = 0.0222927
I1107 17:51:33.931244  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:51:33.931244  5452 solver.cpp:237]     Train net output #1: loss = 0.0222923 (* 1 = 0.0222923 loss)
I1107 17:51:33.931244  5452 sgd_solver.cpp:105] Iteration 135700, lr = 0.001
I1107 17:51:42.617661  5452 solver.cpp:218] Iteration 135800 (11.5125 iter/s, 8.68624s/100 iters), loss = 0.0205273
I1107 17:51:42.617661  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:51:42.617661  5452 solver.cpp:237]     Train net output #1: loss = 0.0205269 (* 1 = 0.0205269 loss)
I1107 17:51:42.617661  5452 sgd_solver.cpp:105] Iteration 135800, lr = 0.001
I1107 17:51:51.197098  5452 solver.cpp:218] Iteration 135900 (11.6566 iter/s, 8.57883s/100 iters), loss = 0.0248661
I1107 17:51:51.197098  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:51:51.197098  5452 solver.cpp:237]     Train net output #1: loss = 0.0248656 (* 1 = 0.0248656 loss)
I1107 17:51:51.197098  5452 sgd_solver.cpp:105] Iteration 135900, lr = 0.001
I1107 17:51:59.378304 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:51:59.719323  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_136000.caffemodel
I1107 17:51:59.749986  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_136000.solverstate
I1107 17:51:59.758982  5452 solver.cpp:330] Iteration 136000, Testing net (#0)
I1107 17:51:59.758982  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:52:01.755669 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:52:01.835672  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9161
I1107 17:52:01.835672  5452 solver.cpp:397]     Test net output #1: loss = 0.311112 (* 1 = 0.311112 loss)
I1107 17:52:01.917675  5452 solver.cpp:218] Iteration 136000 (9.32833 iter/s, 10.72s/100 iters), loss = 0.0340506
I1107 17:52:01.917675  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:52:01.917675  5452 solver.cpp:237]     Train net output #1: loss = 0.0340502 (* 1 = 0.0340502 loss)
I1107 17:52:01.917675  5452 sgd_solver.cpp:105] Iteration 136000, lr = 0.001
I1107 17:52:10.521081  5452 solver.cpp:218] Iteration 136100 (11.6242 iter/s, 8.60271s/100 iters), loss = 0.0259738
I1107 17:52:10.521081  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:52:10.521081  5452 solver.cpp:237]     Train net output #1: loss = 0.0259733 (* 1 = 0.0259733 loss)
I1107 17:52:10.521081  5452 sgd_solver.cpp:105] Iteration 136100, lr = 0.001
I1107 17:52:19.137492  5452 solver.cpp:218] Iteration 136200 (11.6054 iter/s, 8.61667s/100 iters), loss = 0.0280709
I1107 17:52:19.138492  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:52:19.138492  5452 solver.cpp:237]     Train net output #1: loss = 0.0280705 (* 1 = 0.0280705 loss)
I1107 17:52:19.138492  5452 sgd_solver.cpp:105] Iteration 136200, lr = 0.001
I1107 17:52:27.788658  5452 solver.cpp:218] Iteration 136300 (11.5604 iter/s, 8.65021s/100 iters), loss = 0.0243403
I1107 17:52:27.788658  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:52:27.788658  5452 solver.cpp:237]     Train net output #1: loss = 0.0243399 (* 1 = 0.0243399 loss)
I1107 17:52:27.788658  5452 sgd_solver.cpp:105] Iteration 136300, lr = 0.001
I1107 17:52:36.382555  5452 solver.cpp:218] Iteration 136400 (11.6368 iter/s, 8.59341s/100 iters), loss = 0.024532
I1107 17:52:36.382555  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:52:36.382555  5452 solver.cpp:237]     Train net output #1: loss = 0.0245316 (* 1 = 0.0245316 loss)
I1107 17:52:36.382555  5452 sgd_solver.cpp:105] Iteration 136400, lr = 0.001
I1107 17:52:44.574241 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:52:44.913282  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_136500.caffemodel
I1107 17:52:44.943287  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_136500.solverstate
I1107 17:52:44.952286  5452 solver.cpp:330] Iteration 136500, Testing net (#0)
I1107 17:52:44.952286  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:52:46.941504 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:52:47.020511  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9154
I1107 17:52:47.020511  5452 solver.cpp:397]     Test net output #1: loss = 0.317059 (* 1 = 0.317059 loss)
I1107 17:52:47.101511  5452 solver.cpp:218] Iteration 136500 (9.32972 iter/s, 10.7184s/100 iters), loss = 0.0278365
I1107 17:52:47.101511  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:52:47.101511  5452 solver.cpp:237]     Train net output #1: loss = 0.0278361 (* 1 = 0.0278361 loss)
I1107 17:52:47.101511  5452 sgd_solver.cpp:105] Iteration 136500, lr = 0.001
I1107 17:52:55.602192  5452 solver.cpp:218] Iteration 136600 (11.7652 iter/s, 8.49966s/100 iters), loss = 0.0398566
I1107 17:52:55.602192  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:52:55.602192  5452 solver.cpp:237]     Train net output #1: loss = 0.0398562 (* 1 = 0.0398562 loss)
I1107 17:52:55.602192  5452 sgd_solver.cpp:105] Iteration 136600, lr = 0.001
I1107 17:53:04.114058  5452 solver.cpp:218] Iteration 136700 (11.7491 iter/s, 8.51131s/100 iters), loss = 0.0214058
I1107 17:53:04.114058  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:53:04.114058  5452 solver.cpp:237]     Train net output #1: loss = 0.0214053 (* 1 = 0.0214053 loss)
I1107 17:53:04.114058  5452 sgd_solver.cpp:105] Iteration 136700, lr = 0.001
I1107 17:53:12.606876  5452 solver.cpp:218] Iteration 136800 (11.7754 iter/s, 8.49228s/100 iters), loss = 0.021963
I1107 17:53:12.606876  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:53:12.606876  5452 solver.cpp:237]     Train net output #1: loss = 0.0219625 (* 1 = 0.0219625 loss)
I1107 17:53:12.606876  5452 sgd_solver.cpp:105] Iteration 136800, lr = 0.001
I1107 17:53:21.129658  5452 solver.cpp:218] Iteration 136900 (11.734 iter/s, 8.52222s/100 iters), loss = 0.0436633
I1107 17:53:21.129658  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:53:21.129658  5452 solver.cpp:237]     Train net output #1: loss = 0.0436629 (* 1 = 0.0436629 loss)
I1107 17:53:21.129658  5452 sgd_solver.cpp:105] Iteration 136900, lr = 0.001
I1107 17:53:29.290103 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:53:29.626873  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_137000.caffemodel
I1107 17:53:29.653398  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_137000.solverstate
I1107 17:53:29.662410  5452 solver.cpp:330] Iteration 137000, Testing net (#0)
I1107 17:53:29.662410  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:53:31.649662 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:53:31.730166  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1107 17:53:31.730166  5452 solver.cpp:397]     Test net output #1: loss = 0.30859 (* 1 = 0.30859 loss)
I1107 17:53:31.810879  5452 solver.cpp:218] Iteration 137000 (9.36235 iter/s, 10.6811s/100 iters), loss = 0.0372347
I1107 17:53:31.810879  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:53:31.810879  5452 solver.cpp:237]     Train net output #1: loss = 0.0372343 (* 1 = 0.0372343 loss)
I1107 17:53:31.810879  5452 sgd_solver.cpp:105] Iteration 137000, lr = 0.001
I1107 17:53:40.463505  5452 solver.cpp:218] Iteration 137100 (11.5575 iter/s, 8.6524s/100 iters), loss = 0.0252178
I1107 17:53:40.463505  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:53:40.463505  5452 solver.cpp:237]     Train net output #1: loss = 0.0252173 (* 1 = 0.0252173 loss)
I1107 17:53:40.463505  5452 sgd_solver.cpp:105] Iteration 137100, lr = 0.001
I1107 17:53:49.115725  5452 solver.cpp:218] Iteration 137200 (11.5591 iter/s, 8.65122s/100 iters), loss = 0.0284373
I1107 17:53:49.115725  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:53:49.115725  5452 solver.cpp:237]     Train net output #1: loss = 0.0284368 (* 1 = 0.0284368 loss)
I1107 17:53:49.115725  5452 sgd_solver.cpp:105] Iteration 137200, lr = 0.001
I1107 17:53:57.607498  5452 solver.cpp:218] Iteration 137300 (11.7764 iter/s, 8.49157s/100 iters), loss = 0.0205931
I1107 17:53:57.607498  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:53:57.607498  5452 solver.cpp:237]     Train net output #1: loss = 0.0205926 (* 1 = 0.0205926 loss)
I1107 17:53:57.607498  5452 sgd_solver.cpp:105] Iteration 137300, lr = 0.001
I1107 17:54:06.109210  5452 solver.cpp:218] Iteration 137400 (11.7634 iter/s, 8.50098s/100 iters), loss = 0.0544525
I1107 17:54:06.109210  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:54:06.109210  5452 solver.cpp:237]     Train net output #1: loss = 0.054452 (* 1 = 0.054452 loss)
I1107 17:54:06.109210  5452 sgd_solver.cpp:105] Iteration 137400, lr = 0.001
I1107 17:54:14.193915 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:54:14.528990  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_137500.caffemodel
I1107 17:54:14.560030  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_137500.solverstate
I1107 17:54:14.569030  5452 solver.cpp:330] Iteration 137500, Testing net (#0)
I1107 17:54:14.569030  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:54:16.549151 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:54:16.628260  5452 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1107 17:54:16.628260  5452 solver.cpp:397]     Test net output #1: loss = 0.307986 (* 1 = 0.307986 loss)
I1107 17:54:16.709798  5452 solver.cpp:218] Iteration 137500 (9.43437 iter/s, 10.5995s/100 iters), loss = 0.0275206
I1107 17:54:16.709798  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:54:16.709798  5452 solver.cpp:237]     Train net output #1: loss = 0.0275202 (* 1 = 0.0275202 loss)
I1107 17:54:16.709798  5452 sgd_solver.cpp:105] Iteration 137500, lr = 0.001
I1107 17:54:25.206900  5452 solver.cpp:218] Iteration 137600 (11.7693 iter/s, 8.49671s/100 iters), loss = 0.0246467
I1107 17:54:25.206900  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:54:25.206900  5452 solver.cpp:237]     Train net output #1: loss = 0.0246462 (* 1 = 0.0246462 loss)
I1107 17:54:25.206900  5452 sgd_solver.cpp:105] Iteration 137600, lr = 0.001
I1107 17:54:33.706667  5452 solver.cpp:218] Iteration 137700 (11.7657 iter/s, 8.49929s/100 iters), loss = 0.0278463
I1107 17:54:33.706667  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:54:33.706667  5452 solver.cpp:237]     Train net output #1: loss = 0.0278458 (* 1 = 0.0278458 loss)
I1107 17:54:33.706667  5452 sgd_solver.cpp:105] Iteration 137700, lr = 0.001
I1107 17:54:42.207029  5452 solver.cpp:218] Iteration 137800 (11.7646 iter/s, 8.5001s/100 iters), loss = 0.0274114
I1107 17:54:42.207029  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:54:42.207029  5452 solver.cpp:237]     Train net output #1: loss = 0.0274109 (* 1 = 0.0274109 loss)
I1107 17:54:42.207029  5452 sgd_solver.cpp:105] Iteration 137800, lr = 0.001
I1107 17:54:50.717975  5452 solver.cpp:218] Iteration 137900 (11.7503 iter/s, 8.51043s/100 iters), loss = 0.0301184
I1107 17:54:50.717975  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:54:50.717975  5452 solver.cpp:237]     Train net output #1: loss = 0.0301179 (* 1 = 0.0301179 loss)
I1107 17:54:50.717975  5452 sgd_solver.cpp:105] Iteration 137900, lr = 0.001
I1107 17:54:58.797740 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:54:59.132994  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_138000.caffemodel
I1107 17:54:59.163995  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_138000.solverstate
I1107 17:54:59.172508  5452 solver.cpp:330] Iteration 138000, Testing net (#0)
I1107 17:54:59.172508  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:55:01.154031 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:55:01.232568  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9188
I1107 17:55:01.232568  5452 solver.cpp:397]     Test net output #1: loss = 0.308597 (* 1 = 0.308597 loss)
I1107 17:55:01.313627  5452 solver.cpp:218] Iteration 138000 (9.43801 iter/s, 10.5955s/100 iters), loss = 0.0326556
I1107 17:55:01.313627  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:55:01.313627  5452 solver.cpp:237]     Train net output #1: loss = 0.0326551 (* 1 = 0.0326551 loss)
I1107 17:55:01.313627  5452 sgd_solver.cpp:105] Iteration 138000, lr = 0.001
I1107 17:55:09.848654  5452 solver.cpp:218] Iteration 138100 (11.7174 iter/s, 8.53433s/100 iters), loss = 0.0250333
I1107 17:55:09.848654  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:55:09.848654  5452 solver.cpp:237]     Train net output #1: loss = 0.0250328 (* 1 = 0.0250328 loss)
I1107 17:55:09.848654  5452 sgd_solver.cpp:105] Iteration 138100, lr = 0.001
I1107 17:55:18.443720  5452 solver.cpp:218] Iteration 138200 (11.6357 iter/s, 8.59423s/100 iters), loss = 0.0448672
I1107 17:55:18.443720  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:55:18.443720  5452 solver.cpp:237]     Train net output #1: loss = 0.0448668 (* 1 = 0.0448668 loss)
I1107 17:55:18.443720  5452 sgd_solver.cpp:105] Iteration 138200, lr = 0.001
I1107 17:55:26.992267  5452 solver.cpp:218] Iteration 138300 (11.6992 iter/s, 8.54762s/100 iters), loss = 0.0254773
I1107 17:55:26.992267  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:55:26.992267  5452 solver.cpp:237]     Train net output #1: loss = 0.0254768 (* 1 = 0.0254768 loss)
I1107 17:55:26.992267  5452 sgd_solver.cpp:105] Iteration 138300, lr = 0.001
I1107 17:55:35.729742  5452 solver.cpp:218] Iteration 138400 (11.4455 iter/s, 8.73705s/100 iters), loss = 0.0493086
I1107 17:55:35.729742  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 17:55:35.729742  5452 solver.cpp:237]     Train net output #1: loss = 0.0493081 (* 1 = 0.0493081 loss)
I1107 17:55:35.729742  5452 sgd_solver.cpp:105] Iteration 138400, lr = 0.001
I1107 17:55:43.871938 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:55:44.207976  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_138500.caffemodel
I1107 17:55:44.237977  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_138500.solverstate
I1107 17:55:44.246978  5452 solver.cpp:330] Iteration 138500, Testing net (#0)
I1107 17:55:44.246978  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:55:46.245195 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:55:46.326200  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9196
I1107 17:55:46.326200  5452 solver.cpp:397]     Test net output #1: loss = 0.308215 (* 1 = 0.308215 loss)
I1107 17:55:46.408228  5452 solver.cpp:218] Iteration 138500 (9.36454 iter/s, 10.6786s/100 iters), loss = 0.0320879
I1107 17:55:46.409229  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:55:46.409229  5452 solver.cpp:237]     Train net output #1: loss = 0.0320875 (* 1 = 0.0320875 loss)
I1107 17:55:46.409229  5452 sgd_solver.cpp:105] Iteration 138500, lr = 0.001
I1107 17:55:54.978001  5452 solver.cpp:218] Iteration 138600 (11.6707 iter/s, 8.56848s/100 iters), loss = 0.0376836
I1107 17:55:54.978001  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:55:54.978001  5452 solver.cpp:237]     Train net output #1: loss = 0.0376831 (* 1 = 0.0376831 loss)
I1107 17:55:54.978001  5452 sgd_solver.cpp:105] Iteration 138600, lr = 0.001
I1107 17:56:03.529834  5452 solver.cpp:218] Iteration 138700 (11.6939 iter/s, 8.55148s/100 iters), loss = 0.0292534
I1107 17:56:03.529834  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:56:03.529834  5452 solver.cpp:237]     Train net output #1: loss = 0.029253 (* 1 = 0.029253 loss)
I1107 17:56:03.529834  5452 sgd_solver.cpp:105] Iteration 138700, lr = 0.001
I1107 17:56:12.092025  5452 solver.cpp:218] Iteration 138800 (11.6804 iter/s, 8.56134s/100 iters), loss = 0.0228985
I1107 17:56:12.092025  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:56:12.092025  5452 solver.cpp:237]     Train net output #1: loss = 0.0228981 (* 1 = 0.0228981 loss)
I1107 17:56:12.092025  5452 sgd_solver.cpp:105] Iteration 138800, lr = 0.001
I1107 17:56:20.624300  5452 solver.cpp:218] Iteration 138900 (11.7202 iter/s, 8.53227s/100 iters), loss = 0.0235707
I1107 17:56:20.624300  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:56:20.624300  5452 solver.cpp:237]     Train net output #1: loss = 0.0235703 (* 1 = 0.0235703 loss)
I1107 17:56:20.624300  5452 sgd_solver.cpp:105] Iteration 138900, lr = 0.001
I1107 17:56:28.738852 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:56:29.075886  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_139000.caffemodel
I1107 17:56:29.105886  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_139000.solverstate
I1107 17:56:29.114887  5452 solver.cpp:330] Iteration 139000, Testing net (#0)
I1107 17:56:29.114887  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:56:31.111162 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:56:31.190201  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1107 17:56:31.190201  5452 solver.cpp:397]     Test net output #1: loss = 0.308351 (* 1 = 0.308351 loss)
I1107 17:56:31.271692  5452 solver.cpp:218] Iteration 139000 (9.39266 iter/s, 10.6466s/100 iters), loss = 0.0261343
I1107 17:56:31.272193  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:56:31.272193  5452 solver.cpp:237]     Train net output #1: loss = 0.0261338 (* 1 = 0.0261338 loss)
I1107 17:56:31.272193  5452 sgd_solver.cpp:105] Iteration 139000, lr = 0.001
I1107 17:56:39.831344  5452 solver.cpp:218] Iteration 139100 (11.6834 iter/s, 8.55914s/100 iters), loss = 0.0349487
I1107 17:56:39.831344  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:56:39.831344  5452 solver.cpp:237]     Train net output #1: loss = 0.0349483 (* 1 = 0.0349483 loss)
I1107 17:56:39.831344  5452 sgd_solver.cpp:105] Iteration 139100, lr = 0.001
I1107 17:56:48.372753  5452 solver.cpp:218] Iteration 139200 (11.7088 iter/s, 8.54055s/100 iters), loss = 0.0240661
I1107 17:56:48.372753  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:56:48.372753  5452 solver.cpp:237]     Train net output #1: loss = 0.0240656 (* 1 = 0.0240656 loss)
I1107 17:56:48.372753  5452 sgd_solver.cpp:105] Iteration 139200, lr = 0.001
I1107 17:56:56.921550  5452 solver.cpp:218] Iteration 139300 (11.6978 iter/s, 8.54861s/100 iters), loss = 0.0292556
I1107 17:56:56.921550  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:56:56.921550  5452 solver.cpp:237]     Train net output #1: loss = 0.0292552 (* 1 = 0.0292552 loss)
I1107 17:56:56.921550  5452 sgd_solver.cpp:105] Iteration 139300, lr = 0.001
I1107 17:57:05.437221  5452 solver.cpp:218] Iteration 139400 (11.7433 iter/s, 8.51547s/100 iters), loss = 0.0323478
I1107 17:57:05.437221  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:57:05.437221  5452 solver.cpp:237]     Train net output #1: loss = 0.0323474 (* 1 = 0.0323474 loss)
I1107 17:57:05.437221  5452 sgd_solver.cpp:105] Iteration 139400, lr = 0.001
I1107 17:57:13.552842 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:57:13.891834  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_139500.caffemodel
I1107 17:57:13.922829  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_139500.solverstate
I1107 17:57:13.931833  5452 solver.cpp:330] Iteration 139500, Testing net (#0)
I1107 17:57:13.931833  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:57:15.939051 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:57:16.019098  5452 solver.cpp:397]     Test net output #0: accuracy = 0.917
I1107 17:57:16.019098  5452 solver.cpp:397]     Test net output #1: loss = 0.309474 (* 1 = 0.309474 loss)
I1107 17:57:16.100137  5452 solver.cpp:218] Iteration 139500 (9.37879 iter/s, 10.6624s/100 iters), loss = 0.0413301
I1107 17:57:16.100137  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:57:16.100137  5452 solver.cpp:237]     Train net output #1: loss = 0.0413297 (* 1 = 0.0413297 loss)
I1107 17:57:16.100137  5452 sgd_solver.cpp:105] Iteration 139500, lr = 0.001
I1107 17:57:24.672196  5452 solver.cpp:218] Iteration 139600 (11.6672 iter/s, 8.57105s/100 iters), loss = 0.0269385
I1107 17:57:24.672196  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:57:24.672196  5452 solver.cpp:237]     Train net output #1: loss = 0.0269381 (* 1 = 0.0269381 loss)
I1107 17:57:24.672196  5452 sgd_solver.cpp:105] Iteration 139600, lr = 0.001
I1107 17:57:33.252385  5452 solver.cpp:218] Iteration 139700 (11.6556 iter/s, 8.57956s/100 iters), loss = 0.0221011
I1107 17:57:33.252385  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:57:33.252385  5452 solver.cpp:237]     Train net output #1: loss = 0.0221007 (* 1 = 0.0221007 loss)
I1107 17:57:33.252385  5452 sgd_solver.cpp:105] Iteration 139700, lr = 0.001
I1107 17:57:41.837709  5452 solver.cpp:218] Iteration 139800 (11.6474 iter/s, 8.58564s/100 iters), loss = 0.032848
I1107 17:57:41.837709  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:57:41.837709  5452 solver.cpp:237]     Train net output #1: loss = 0.0328476 (* 1 = 0.0328476 loss)
I1107 17:57:41.837709  5452 sgd_solver.cpp:105] Iteration 139800, lr = 0.001
I1107 17:57:50.461814  5452 solver.cpp:218] Iteration 139900 (11.5969 iter/s, 8.62298s/100 iters), loss = 0.026448
I1107 17:57:50.461814  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:57:50.461814  5452 solver.cpp:237]     Train net output #1: loss = 0.0264475 (* 1 = 0.0264475 loss)
I1107 17:57:50.461814  5452 sgd_solver.cpp:105] Iteration 139900, lr = 0.001
I1107 17:57:58.643654 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:57:58.983810  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_140000.caffemodel
I1107 17:57:59.015820  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_140000.solverstate
I1107 17:57:59.024811  5452 solver.cpp:330] Iteration 140000, Testing net (#0)
I1107 17:57:59.024811  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:58:01.023893 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:58:01.102897  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1107 17:58:01.102897  5452 solver.cpp:397]     Test net output #1: loss = 0.312093 (* 1 = 0.312093 loss)
I1107 17:58:01.184902  5452 solver.cpp:218] Iteration 140000 (9.32637 iter/s, 10.7223s/100 iters), loss = 0.02445
I1107 17:58:01.184902  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:58:01.184902  5452 solver.cpp:237]     Train net output #1: loss = 0.0244495 (* 1 = 0.0244495 loss)
I1107 17:58:01.184902  5452 sgd_solver.cpp:105] Iteration 140000, lr = 0.001
I1107 17:58:09.796752  5452 solver.cpp:218] Iteration 140100 (11.6116 iter/s, 8.61207s/100 iters), loss = 0.0570933
I1107 17:58:09.796752  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:58:09.796752  5452 solver.cpp:237]     Train net output #1: loss = 0.0570928 (* 1 = 0.0570928 loss)
I1107 17:58:09.796752  5452 sgd_solver.cpp:105] Iteration 140100, lr = 0.001
I1107 17:58:18.448527  5452 solver.cpp:218] Iteration 140200 (11.5596 iter/s, 8.65086s/100 iters), loss = 0.0177933
I1107 17:58:18.448527  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:58:18.448527  5452 solver.cpp:237]     Train net output #1: loss = 0.0177929 (* 1 = 0.0177929 loss)
I1107 17:58:18.448527  5452 sgd_solver.cpp:105] Iteration 140200, lr = 0.001
I1107 17:58:27.089669  5452 solver.cpp:218] Iteration 140300 (11.5733 iter/s, 8.64057s/100 iters), loss = 0.0279165
I1107 17:58:27.089669  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:58:27.089669  5452 solver.cpp:237]     Train net output #1: loss = 0.0279161 (* 1 = 0.0279161 loss)
I1107 17:58:27.089669  5452 sgd_solver.cpp:105] Iteration 140300, lr = 0.001
I1107 17:58:35.744849  5452 solver.cpp:218] Iteration 140400 (11.5544 iter/s, 8.65475s/100 iters), loss = 0.040184
I1107 17:58:35.744849  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:58:35.744849  5452 solver.cpp:237]     Train net output #1: loss = 0.0401835 (* 1 = 0.0401835 loss)
I1107 17:58:35.744849  5452 sgd_solver.cpp:105] Iteration 140400, lr = 0.001
I1107 17:58:44.005848 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:58:44.343824  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_140500.caffemodel
I1107 17:58:44.373826  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_140500.solverstate
I1107 17:58:44.382823  5452 solver.cpp:330] Iteration 140500, Testing net (#0)
I1107 17:58:44.382823  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:58:46.386490 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:58:46.467504  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1107 17:58:46.467504  5452 solver.cpp:397]     Test net output #1: loss = 0.307686 (* 1 = 0.307686 loss)
I1107 17:58:46.552168  5452 solver.cpp:218] Iteration 140500 (9.25362 iter/s, 10.8066s/100 iters), loss = 0.0411411
I1107 17:58:46.552168  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:58:46.552168  5452 solver.cpp:237]     Train net output #1: loss = 0.0411406 (* 1 = 0.0411406 loss)
I1107 17:58:46.552168  5452 sgd_solver.cpp:105] Iteration 140500, lr = 0.001
I1107 17:58:55.192862  5452 solver.cpp:218] Iteration 140600 (11.5731 iter/s, 8.6407s/100 iters), loss = 0.0305168
I1107 17:58:55.192862  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:58:55.192862  5452 solver.cpp:237]     Train net output #1: loss = 0.0305164 (* 1 = 0.0305164 loss)
I1107 17:58:55.192862  5452 sgd_solver.cpp:105] Iteration 140600, lr = 0.001
I1107 17:59:03.859637  5452 solver.cpp:218] Iteration 140700 (11.5402 iter/s, 8.66535s/100 iters), loss = 0.0236547
I1107 17:59:03.859637  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:59:03.859637  5452 solver.cpp:237]     Train net output #1: loss = 0.0236542 (* 1 = 0.0236542 loss)
I1107 17:59:03.859637  5452 sgd_solver.cpp:105] Iteration 140700, lr = 0.001
I1107 17:59:12.557165  5452 solver.cpp:218] Iteration 140800 (11.4981 iter/s, 8.69706s/100 iters), loss = 0.0234243
I1107 17:59:12.557165  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:59:12.557165  5452 solver.cpp:237]     Train net output #1: loss = 0.0234239 (* 1 = 0.0234239 loss)
I1107 17:59:12.557165  5452 sgd_solver.cpp:105] Iteration 140800, lr = 0.001
I1107 17:59:21.233916  5452 solver.cpp:218] Iteration 140900 (11.5257 iter/s, 8.67629s/100 iters), loss = 0.0232316
I1107 17:59:21.233916  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:59:21.233916  5452 solver.cpp:237]     Train net output #1: loss = 0.0232312 (* 1 = 0.0232312 loss)
I1107 17:59:21.233916  5452 sgd_solver.cpp:105] Iteration 140900, lr = 0.001
I1107 17:59:29.399329 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:59:29.736860  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_141000.caffemodel
I1107 17:59:29.767364  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_141000.solverstate
I1107 17:59:29.775363  5452 solver.cpp:330] Iteration 141000, Testing net (#0)
I1107 17:59:29.776365  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 17:59:31.761744 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 17:59:31.841244  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1107 17:59:31.841244  5452 solver.cpp:397]     Test net output #1: loss = 0.309128 (* 1 = 0.309128 loss)
I1107 17:59:31.922746  5452 solver.cpp:218] Iteration 141000 (9.35598 iter/s, 10.6883s/100 iters), loss = 0.0308824
I1107 17:59:31.922746  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:59:31.922746  5452 solver.cpp:237]     Train net output #1: loss = 0.030882 (* 1 = 0.030882 loss)
I1107 17:59:31.922746  5452 sgd_solver.cpp:105] Iteration 141000, lr = 0.001
I1107 17:59:40.521282  5452 solver.cpp:218] Iteration 141100 (11.6304 iter/s, 8.59819s/100 iters), loss = 0.0387039
I1107 17:59:40.521282  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 17:59:40.521282  5452 solver.cpp:237]     Train net output #1: loss = 0.0387035 (* 1 = 0.0387035 loss)
I1107 17:59:40.521282  5452 sgd_solver.cpp:105] Iteration 141100, lr = 0.001
I1107 17:59:49.096861  5452 solver.cpp:218] Iteration 141200 (11.662 iter/s, 8.57487s/100 iters), loss = 0.0289532
I1107 17:59:49.096861  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:59:49.096861  5452 solver.cpp:237]     Train net output #1: loss = 0.0289527 (* 1 = 0.0289527 loss)
I1107 17:59:49.096861  5452 sgd_solver.cpp:105] Iteration 141200, lr = 0.001
I1107 17:59:57.650722  5452 solver.cpp:218] Iteration 141300 (11.6915 iter/s, 8.55325s/100 iters), loss = 0.0331473
I1107 17:59:57.650722  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 17:59:57.650722  5452 solver.cpp:237]     Train net output #1: loss = 0.0331468 (* 1 = 0.0331468 loss)
I1107 17:59:57.650722  5452 sgd_solver.cpp:105] Iteration 141300, lr = 0.001
I1107 18:00:06.271239  5452 solver.cpp:218] Iteration 141400 (11.6013 iter/s, 8.6197s/100 iters), loss = 0.0354784
I1107 18:00:06.271239  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:00:06.271239  5452 solver.cpp:237]     Train net output #1: loss = 0.035478 (* 1 = 0.035478 loss)
I1107 18:00:06.271239  5452 sgd_solver.cpp:105] Iteration 141400, lr = 0.001
I1107 18:00:14.437587 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:00:14.783527  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_141500.caffemodel
I1107 18:00:14.815527  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_141500.solverstate
I1107 18:00:14.825531  5452 solver.cpp:330] Iteration 141500, Testing net (#0)
I1107 18:00:14.826531  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:00:16.863276 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:00:16.945292  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9178
I1107 18:00:16.945792  5452 solver.cpp:397]     Test net output #1: loss = 0.309024 (* 1 = 0.309024 loss)
I1107 18:00:17.029285  5452 solver.cpp:218] Iteration 141500 (9.29591 iter/s, 10.7574s/100 iters), loss = 0.0346678
I1107 18:00:17.029285  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:00:17.029285  5452 solver.cpp:237]     Train net output #1: loss = 0.0346673 (* 1 = 0.0346673 loss)
I1107 18:00:17.029285  5452 sgd_solver.cpp:105] Iteration 141500, lr = 0.001
I1107 18:00:25.690438  5452 solver.cpp:218] Iteration 141600 (11.5455 iter/s, 8.66139s/100 iters), loss = 0.044129
I1107 18:00:25.691438  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:00:25.691438  5452 solver.cpp:237]     Train net output #1: loss = 0.0441286 (* 1 = 0.0441286 loss)
I1107 18:00:25.691438  5452 sgd_solver.cpp:105] Iteration 141600, lr = 0.001
I1107 18:00:34.427173  5452 solver.cpp:218] Iteration 141700 (11.4468 iter/s, 8.73609s/100 iters), loss = 0.021753
I1107 18:00:34.428175  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:00:34.428175  5452 solver.cpp:237]     Train net output #1: loss = 0.0217526 (* 1 = 0.0217526 loss)
I1107 18:00:34.428175  5452 sgd_solver.cpp:105] Iteration 141700, lr = 0.001
I1107 18:00:43.102310  5452 solver.cpp:218] Iteration 141800 (11.5288 iter/s, 8.67395s/100 iters), loss = 0.0383431
I1107 18:00:43.102310  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:00:43.102310  5452 solver.cpp:237]     Train net output #1: loss = 0.0383427 (* 1 = 0.0383427 loss)
I1107 18:00:43.102310  5452 sgd_solver.cpp:105] Iteration 141800, lr = 0.001
I1107 18:00:51.740475  5452 solver.cpp:218] Iteration 141900 (11.5772 iter/s, 8.63768s/100 iters), loss = 0.0307404
I1107 18:00:51.740475  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:00:51.740475  5452 solver.cpp:237]     Train net output #1: loss = 0.0307399 (* 1 = 0.0307399 loss)
I1107 18:00:51.740475  5452 sgd_solver.cpp:105] Iteration 141900, lr = 0.001
I1107 18:00:59.868782 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:01:00.207800  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_142000.caffemodel
I1107 18:01:00.237802  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_142000.solverstate
I1107 18:01:00.246801  5452 solver.cpp:330] Iteration 142000, Testing net (#0)
I1107 18:01:00.246801  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:01:02.231953 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:01:02.310959  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9163
I1107 18:01:02.310959  5452 solver.cpp:397]     Test net output #1: loss = 0.321618 (* 1 = 0.321618 loss)
I1107 18:01:02.392964  5452 solver.cpp:218] Iteration 142000 (9.38754 iter/s, 10.6524s/100 iters), loss = 0.0244357
I1107 18:01:02.392964  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:01:02.392964  5452 solver.cpp:237]     Train net output #1: loss = 0.0244353 (* 1 = 0.0244353 loss)
I1107 18:01:02.392964  5452 sgd_solver.cpp:105] Iteration 142000, lr = 0.001
I1107 18:01:10.922739  5452 solver.cpp:218] Iteration 142100 (11.7255 iter/s, 8.52845s/100 iters), loss = 0.0351595
I1107 18:01:10.922739  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:01:10.922739  5452 solver.cpp:237]     Train net output #1: loss = 0.0351591 (* 1 = 0.0351591 loss)
I1107 18:01:10.922739  5452 sgd_solver.cpp:105] Iteration 142100, lr = 0.001
I1107 18:01:19.459870  5452 solver.cpp:218] Iteration 142200 (11.7141 iter/s, 8.53671s/100 iters), loss = 0.0266008
I1107 18:01:19.459870  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:01:19.459870  5452 solver.cpp:237]     Train net output #1: loss = 0.0266004 (* 1 = 0.0266004 loss)
I1107 18:01:19.459870  5452 sgd_solver.cpp:105] Iteration 142200, lr = 0.001
I1107 18:01:27.987440  5452 solver.cpp:218] Iteration 142300 (11.7268 iter/s, 8.52747s/100 iters), loss = 0.0207989
I1107 18:01:27.987440  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:01:27.987440  5452 solver.cpp:237]     Train net output #1: loss = 0.0207985 (* 1 = 0.0207985 loss)
I1107 18:01:27.987440  5452 sgd_solver.cpp:105] Iteration 142300, lr = 0.001
I1107 18:01:36.518851  5452 solver.cpp:218] Iteration 142400 (11.7227 iter/s, 8.53043s/100 iters), loss = 0.0236013
I1107 18:01:36.518851  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:01:36.518851  5452 solver.cpp:237]     Train net output #1: loss = 0.0236009 (* 1 = 0.0236009 loss)
I1107 18:01:36.518851  5452 sgd_solver.cpp:105] Iteration 142400, lr = 0.001
I1107 18:01:44.615602 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:01:44.951637  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_142500.caffemodel
I1107 18:01:44.980644  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_142500.solverstate
I1107 18:01:44.989645  5452 solver.cpp:330] Iteration 142500, Testing net (#0)
I1107 18:01:44.989645  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:01:46.974905 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:01:47.053905  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9192
I1107 18:01:47.053905  5452 solver.cpp:397]     Test net output #1: loss = 0.307856 (* 1 = 0.307856 loss)
I1107 18:01:47.134910  5452 solver.cpp:218] Iteration 142500 (9.42003 iter/s, 10.6157s/100 iters), loss = 0.0344421
I1107 18:01:47.134910  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:01:47.134910  5452 solver.cpp:237]     Train net output #1: loss = 0.0344416 (* 1 = 0.0344416 loss)
I1107 18:01:47.134910  5452 sgd_solver.cpp:105] Iteration 142500, lr = 0.001
I1107 18:01:55.653672  5452 solver.cpp:218] Iteration 142600 (11.74 iter/s, 8.51789s/100 iters), loss = 0.0324752
I1107 18:01:55.653672  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:01:55.653672  5452 solver.cpp:237]     Train net output #1: loss = 0.0324748 (* 1 = 0.0324748 loss)
I1107 18:01:55.653672  5452 sgd_solver.cpp:105] Iteration 142600, lr = 0.001
I1107 18:02:04.183449  5452 solver.cpp:218] Iteration 142700 (11.7237 iter/s, 8.52972s/100 iters), loss = 0.0217673
I1107 18:02:04.183449  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:02:04.183449  5452 solver.cpp:237]     Train net output #1: loss = 0.0217668 (* 1 = 0.0217668 loss)
I1107 18:02:04.183449  5452 sgd_solver.cpp:105] Iteration 142700, lr = 0.001
I1107 18:02:12.708328  5452 solver.cpp:218] Iteration 142800 (11.7309 iter/s, 8.52448s/100 iters), loss = 0.029972
I1107 18:02:12.708328  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:02:12.708328  5452 solver.cpp:237]     Train net output #1: loss = 0.0299716 (* 1 = 0.0299716 loss)
I1107 18:02:12.708328  5452 sgd_solver.cpp:105] Iteration 142800, lr = 0.001
I1107 18:02:21.351522  5452 solver.cpp:218] Iteration 142900 (11.5714 iter/s, 8.64203s/100 iters), loss = 0.0276745
I1107 18:02:21.351522  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:02:21.351522  5452 solver.cpp:237]     Train net output #1: loss = 0.0276741 (* 1 = 0.0276741 loss)
I1107 18:02:21.351522  5452 sgd_solver.cpp:105] Iteration 142900, lr = 0.001
I1107 18:02:29.597105 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:02:29.934139  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_143000.caffemodel
I1107 18:02:29.965139  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_143000.solverstate
I1107 18:02:29.973140  5452 solver.cpp:330] Iteration 143000, Testing net (#0)
I1107 18:02:29.974139  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:02:31.995499 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:02:32.076500  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9175
I1107 18:02:32.077502  5452 solver.cpp:397]     Test net output #1: loss = 0.315278 (* 1 = 0.315278 loss)
I1107 18:02:32.161507  5452 solver.cpp:218] Iteration 143000 (9.25109 iter/s, 10.8095s/100 iters), loss = 0.0320032
I1107 18:02:32.161507  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:02:32.161507  5452 solver.cpp:237]     Train net output #1: loss = 0.0320028 (* 1 = 0.0320028 loss)
I1107 18:02:32.161507  5452 sgd_solver.cpp:105] Iteration 143000, lr = 0.001
I1107 18:02:40.713011  5452 solver.cpp:218] Iteration 143100 (11.6949 iter/s, 8.55075s/100 iters), loss = 0.0260305
I1107 18:02:40.713011  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:02:40.713011  5452 solver.cpp:237]     Train net output #1: loss = 0.02603 (* 1 = 0.02603 loss)
I1107 18:02:40.713011  5452 sgd_solver.cpp:105] Iteration 143100, lr = 0.001
I1107 18:02:49.306463  5452 solver.cpp:218] Iteration 143200 (11.6369 iter/s, 8.59333s/100 iters), loss = 0.0288524
I1107 18:02:49.306463  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:02:49.306463  5452 solver.cpp:237]     Train net output #1: loss = 0.028852 (* 1 = 0.028852 loss)
I1107 18:02:49.306463  5452 sgd_solver.cpp:105] Iteration 143200, lr = 0.001
I1107 18:02:57.839385  5452 solver.cpp:218] Iteration 143300 (11.72 iter/s, 8.53242s/100 iters), loss = 0.0272128
I1107 18:02:57.839385  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:02:57.839385  5452 solver.cpp:237]     Train net output #1: loss = 0.0272123 (* 1 = 0.0272123 loss)
I1107 18:02:57.839385  5452 sgd_solver.cpp:105] Iteration 143300, lr = 0.001
I1107 18:03:06.374449  5452 solver.cpp:218] Iteration 143400 (11.7179 iter/s, 8.53395s/100 iters), loss = 0.0273553
I1107 18:03:06.374449  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:03:06.374449  5452 solver.cpp:237]     Train net output #1: loss = 0.0273549 (* 1 = 0.0273549 loss)
I1107 18:03:06.374449  5452 sgd_solver.cpp:105] Iteration 143400, lr = 0.001
I1107 18:03:14.484367 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:03:14.820400  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_143500.caffemodel
I1107 18:03:14.850410  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_143500.solverstate
I1107 18:03:14.859411  5452 solver.cpp:330] Iteration 143500, Testing net (#0)
I1107 18:03:14.859411  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:03:16.846855 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:03:16.927359  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9174
I1107 18:03:16.927359  5452 solver.cpp:397]     Test net output #1: loss = 0.312748 (* 1 = 0.312748 loss)
I1107 18:03:17.010862  5452 solver.cpp:218] Iteration 143500 (9.40171 iter/s, 10.6364s/100 iters), loss = 0.0331369
I1107 18:03:17.010862  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:03:17.010862  5452 solver.cpp:237]     Train net output #1: loss = 0.0331364 (* 1 = 0.0331364 loss)
I1107 18:03:17.010862  5452 sgd_solver.cpp:105] Iteration 143500, lr = 0.001
I1107 18:03:25.543608  5452 solver.cpp:218] Iteration 143600 (11.7206 iter/s, 8.53195s/100 iters), loss = 0.0238486
I1107 18:03:25.543608  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:03:25.543608  5452 solver.cpp:237]     Train net output #1: loss = 0.0238482 (* 1 = 0.0238482 loss)
I1107 18:03:25.543608  5452 sgd_solver.cpp:105] Iteration 143600, lr = 0.001
I1107 18:03:34.080169  5452 solver.cpp:218] Iteration 143700 (11.7152 iter/s, 8.53591s/100 iters), loss = 0.0212751
I1107 18:03:34.080169  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:03:34.080169  5452 solver.cpp:237]     Train net output #1: loss = 0.0212747 (* 1 = 0.0212747 loss)
I1107 18:03:34.080169  5452 sgd_solver.cpp:105] Iteration 143700, lr = 0.001
I1107 18:03:42.594964  5452 solver.cpp:218] Iteration 143800 (11.7453 iter/s, 8.51406s/100 iters), loss = 0.0239984
I1107 18:03:42.594964  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:03:42.594964  5452 solver.cpp:237]     Train net output #1: loss = 0.0239979 (* 1 = 0.0239979 loss)
I1107 18:03:42.594964  5452 sgd_solver.cpp:105] Iteration 143800, lr = 0.001
I1107 18:03:51.116402  5452 solver.cpp:218] Iteration 143900 (11.7354 iter/s, 8.52126s/100 iters), loss = 0.021155
I1107 18:03:51.116402  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:03:51.116402  5452 solver.cpp:237]     Train net output #1: loss = 0.0211546 (* 1 = 0.0211546 loss)
I1107 18:03:51.116402  5452 sgd_solver.cpp:105] Iteration 143900, lr = 0.001
I1107 18:03:59.220172 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:03:59.557212  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_144000.caffemodel
I1107 18:03:59.587210  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_144000.solverstate
I1107 18:03:59.596211  5452 solver.cpp:330] Iteration 144000, Testing net (#0)
I1107 18:03:59.596211  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:04:01.580499 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:04:01.660542  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9158
I1107 18:04:01.660542  5452 solver.cpp:397]     Test net output #1: loss = 0.31908 (* 1 = 0.31908 loss)
I1107 18:04:01.742027  5452 solver.cpp:218] Iteration 144000 (9.41188 iter/s, 10.6249s/100 iters), loss = 0.0286406
I1107 18:04:01.742027  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:04:01.742027  5452 solver.cpp:237]     Train net output #1: loss = 0.0286401 (* 1 = 0.0286401 loss)
I1107 18:04:01.742027  5452 sgd_solver.cpp:105] Iteration 144000, lr = 0.001
I1107 18:04:10.269492  5452 solver.cpp:218] Iteration 144100 (11.7267 iter/s, 8.52752s/100 iters), loss = 0.047763
I1107 18:04:10.269492  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 18:04:10.269492  5452 solver.cpp:237]     Train net output #1: loss = 0.0477625 (* 1 = 0.0477625 loss)
I1107 18:04:10.269492  5452 sgd_solver.cpp:105] Iteration 144100, lr = 0.001
I1107 18:04:18.790339  5452 solver.cpp:218] Iteration 144200 (11.7367 iter/s, 8.5203s/100 iters), loss = 0.0274594
I1107 18:04:18.790339  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:04:18.790339  5452 solver.cpp:237]     Train net output #1: loss = 0.027459 (* 1 = 0.027459 loss)
I1107 18:04:18.790339  5452 sgd_solver.cpp:105] Iteration 144200, lr = 0.001
I1107 18:04:27.302137  5452 solver.cpp:218] Iteration 144300 (11.7493 iter/s, 8.51116s/100 iters), loss = 0.0259412
I1107 18:04:27.302137  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:04:27.302137  5452 solver.cpp:237]     Train net output #1: loss = 0.0259407 (* 1 = 0.0259407 loss)
I1107 18:04:27.302137  5452 sgd_solver.cpp:105] Iteration 144300, lr = 0.001
I1107 18:04:35.825942  5452 solver.cpp:218] Iteration 144400 (11.7329 iter/s, 8.52302s/100 iters), loss = 0.0337057
I1107 18:04:35.825942  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:04:35.825942  5452 solver.cpp:237]     Train net output #1: loss = 0.0337052 (* 1 = 0.0337052 loss)
I1107 18:04:35.825942  5452 sgd_solver.cpp:105] Iteration 144400, lr = 0.001
I1107 18:04:43.913527 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:04:44.249186  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_144500.caffemodel
I1107 18:04:44.279176  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_144500.solverstate
I1107 18:04:44.288182  5452 solver.cpp:330] Iteration 144500, Testing net (#0)
I1107 18:04:44.288182  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:04:46.273506 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:04:46.353004  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9176
I1107 18:04:46.353004  5452 solver.cpp:397]     Test net output #1: loss = 0.315755 (* 1 = 0.315755 loss)
I1107 18:04:46.433600  5452 solver.cpp:218] Iteration 144500 (9.42718 iter/s, 10.6076s/100 iters), loss = 0.0245237
I1107 18:04:46.433600  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:04:46.433600  5452 solver.cpp:237]     Train net output #1: loss = 0.0245233 (* 1 = 0.0245233 loss)
I1107 18:04:46.433600  5452 sgd_solver.cpp:105] Iteration 144500, lr = 0.001
I1107 18:04:54.958432  5452 solver.cpp:218] Iteration 144600 (11.7318 iter/s, 8.52386s/100 iters), loss = 0.0424636
I1107 18:04:54.958432  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:04:54.958432  5452 solver.cpp:237]     Train net output #1: loss = 0.0424632 (* 1 = 0.0424632 loss)
I1107 18:04:54.958432  5452 sgd_solver.cpp:105] Iteration 144600, lr = 0.001
I1107 18:05:03.491346  5452 solver.cpp:218] Iteration 144700 (11.72 iter/s, 8.53241s/100 iters), loss = 0.0285591
I1107 18:05:03.491346  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:05:03.491346  5452 solver.cpp:237]     Train net output #1: loss = 0.0285586 (* 1 = 0.0285586 loss)
I1107 18:05:03.491346  5452 sgd_solver.cpp:105] Iteration 144700, lr = 0.001
I1107 18:05:12.017175  5452 solver.cpp:218] Iteration 144800 (11.7292 iter/s, 8.52571s/100 iters), loss = 0.0288178
I1107 18:05:12.017175  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:05:12.017175  5452 solver.cpp:237]     Train net output #1: loss = 0.0288173 (* 1 = 0.0288173 loss)
I1107 18:05:12.017175  5452 sgd_solver.cpp:105] Iteration 144800, lr = 0.001
I1107 18:05:20.546103  5452 solver.cpp:218] Iteration 144900 (11.7266 iter/s, 8.52765s/100 iters), loss = 0.037186
I1107 18:05:20.546103  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:05:20.546103  5452 solver.cpp:237]     Train net output #1: loss = 0.0371856 (* 1 = 0.0371856 loss)
I1107 18:05:20.546103  5452 sgd_solver.cpp:105] Iteration 144900, lr = 0.001
I1107 18:05:28.677011 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:05:29.019045  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_145000.caffemodel
I1107 18:05:29.049046  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_145000.solverstate
I1107 18:05:29.059047  5452 solver.cpp:330] Iteration 145000, Testing net (#0)
I1107 18:05:29.059047  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:05:31.079514 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:05:31.159517  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9172
I1107 18:05:31.159517  5452 solver.cpp:397]     Test net output #1: loss = 0.313154 (* 1 = 0.313154 loss)
I1107 18:05:31.242549  5452 solver.cpp:218] Iteration 145000 (9.34947 iter/s, 10.6958s/100 iters), loss = 0.050472
I1107 18:05:31.242549  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:05:31.242549  5452 solver.cpp:237]     Train net output #1: loss = 0.0504716 (* 1 = 0.0504716 loss)
I1107 18:05:31.242549  5452 sgd_solver.cpp:105] Iteration 145000, lr = 0.001
I1107 18:05:39.778705  5452 solver.cpp:218] Iteration 145100 (11.7156 iter/s, 8.53566s/100 iters), loss = 0.0421927
I1107 18:05:39.778705  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:05:39.778705  5452 solver.cpp:237]     Train net output #1: loss = 0.0421922 (* 1 = 0.0421922 loss)
I1107 18:05:39.778705  5452 sgd_solver.cpp:105] Iteration 145100, lr = 0.001
I1107 18:05:48.328939  5452 solver.cpp:218] Iteration 145200 (11.6959 iter/s, 8.55s/100 iters), loss = 0.0244607
I1107 18:05:48.328939  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:05:48.328939  5452 solver.cpp:237]     Train net output #1: loss = 0.0244602 (* 1 = 0.0244602 loss)
I1107 18:05:48.328939  5452 sgd_solver.cpp:105] Iteration 145200, lr = 0.001
I1107 18:05:56.843854  5452 solver.cpp:218] Iteration 145300 (11.7453 iter/s, 8.51402s/100 iters), loss = 0.0176214
I1107 18:05:56.843854  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:05:56.843854  5452 solver.cpp:237]     Train net output #1: loss = 0.0176209 (* 1 = 0.0176209 loss)
I1107 18:05:56.843854  5452 sgd_solver.cpp:105] Iteration 145300, lr = 0.001
I1107 18:06:05.377915  5452 solver.cpp:218] Iteration 145400 (11.7184 iter/s, 8.53359s/100 iters), loss = 0.0590435
I1107 18:06:05.377915  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 18:06:05.377915  5452 solver.cpp:237]     Train net output #1: loss = 0.059043 (* 1 = 0.059043 loss)
I1107 18:06:05.377915  5452 sgd_solver.cpp:105] Iteration 145400, lr = 0.001
I1107 18:06:13.508616 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:06:13.843647  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_145500.caffemodel
I1107 18:06:13.874650  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_145500.solverstate
I1107 18:06:13.883651  5452 solver.cpp:330] Iteration 145500, Testing net (#0)
I1107 18:06:13.884150  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:06:15.894299 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:06:15.973809  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9163
I1107 18:06:15.973809  5452 solver.cpp:397]     Test net output #1: loss = 0.317042 (* 1 = 0.317042 loss)
I1107 18:06:16.057822  5452 solver.cpp:218] Iteration 145500 (9.3636 iter/s, 10.6797s/100 iters), loss = 0.0293202
I1107 18:06:16.057822  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:06:16.057822  5452 solver.cpp:237]     Train net output #1: loss = 0.0293197 (* 1 = 0.0293197 loss)
I1107 18:06:16.057822  5452 sgd_solver.cpp:105] Iteration 145500, lr = 0.001
I1107 18:06:24.679714  5452 solver.cpp:218] Iteration 145600 (11.5989 iter/s, 8.62152s/100 iters), loss = 0.0322798
I1107 18:06:24.679714  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:06:24.679714  5452 solver.cpp:237]     Train net output #1: loss = 0.0322793 (* 1 = 0.0322793 loss)
I1107 18:06:24.679714  5452 sgd_solver.cpp:105] Iteration 145600, lr = 0.001
I1107 18:06:33.275615  5452 solver.cpp:218] Iteration 145700 (11.6348 iter/s, 8.59492s/100 iters), loss = 0.0308512
I1107 18:06:33.275615  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:06:33.275615  5452 solver.cpp:237]     Train net output #1: loss = 0.0308507 (* 1 = 0.0308507 loss)
I1107 18:06:33.275615  5452 sgd_solver.cpp:105] Iteration 145700, lr = 0.001
I1107 18:06:41.969612  5452 solver.cpp:218] Iteration 145800 (11.5023 iter/s, 8.69393s/100 iters), loss = 0.0178684
I1107 18:06:41.969612  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:06:41.969612  5452 solver.cpp:237]     Train net output #1: loss = 0.017868 (* 1 = 0.017868 loss)
I1107 18:06:41.969612  5452 sgd_solver.cpp:105] Iteration 145800, lr = 0.001
I1107 18:06:50.690150  5452 solver.cpp:218] Iteration 145900 (11.4688 iter/s, 8.71928s/100 iters), loss = 0.0351884
I1107 18:06:50.690150  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:06:50.690150  5452 solver.cpp:237]     Train net output #1: loss = 0.035188 (* 1 = 0.035188 loss)
I1107 18:06:50.690150  5452 sgd_solver.cpp:105] Iteration 145900, lr = 0.001
I1107 18:06:58.867914 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:06:59.205968  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_146000.caffemodel
I1107 18:06:59.235976  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_146000.solverstate
I1107 18:06:59.244954  5452 solver.cpp:330] Iteration 146000, Testing net (#0)
I1107 18:06:59.244954  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:07:01.257167 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:07:01.337170  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1107 18:07:01.337170  5452 solver.cpp:397]     Test net output #1: loss = 0.310243 (* 1 = 0.310243 loss)
I1107 18:07:01.418187  5452 solver.cpp:218] Iteration 146000 (9.32162 iter/s, 10.7277s/100 iters), loss = 0.0263392
I1107 18:07:01.418187  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:07:01.418187  5452 solver.cpp:237]     Train net output #1: loss = 0.0263387 (* 1 = 0.0263387 loss)
I1107 18:07:01.418187  5452 sgd_solver.cpp:105] Iteration 146000, lr = 0.001
I1107 18:07:10.013269  5452 solver.cpp:218] Iteration 146100 (11.6353 iter/s, 8.59455s/100 iters), loss = 0.0246424
I1107 18:07:10.013269  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:07:10.013269  5452 solver.cpp:237]     Train net output #1: loss = 0.0246419 (* 1 = 0.0246419 loss)
I1107 18:07:10.013269  5452 sgd_solver.cpp:105] Iteration 146100, lr = 0.001
I1107 18:07:18.726009  5452 solver.cpp:218] Iteration 146200 (11.4788 iter/s, 8.71172s/100 iters), loss = 0.0205894
I1107 18:07:18.726009  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:07:18.726009  5452 solver.cpp:237]     Train net output #1: loss = 0.0205889 (* 1 = 0.0205889 loss)
I1107 18:07:18.726009  5452 sgd_solver.cpp:105] Iteration 146200, lr = 0.001
I1107 18:07:27.391420  5452 solver.cpp:218] Iteration 146300 (11.5412 iter/s, 8.66463s/100 iters), loss = 0.0214129
I1107 18:07:27.391420  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:07:27.391420  5452 solver.cpp:237]     Train net output #1: loss = 0.0214125 (* 1 = 0.0214125 loss)
I1107 18:07:27.391420  5452 sgd_solver.cpp:105] Iteration 146300, lr = 0.001
I1107 18:07:36.063882  5452 solver.cpp:218] Iteration 146400 (11.5311 iter/s, 8.67217s/100 iters), loss = 0.0306782
I1107 18:07:36.063882  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:07:36.063882  5452 solver.cpp:237]     Train net output #1: loss = 0.0306777 (* 1 = 0.0306777 loss)
I1107 18:07:36.063882  5452 sgd_solver.cpp:105] Iteration 146400, lr = 0.001
I1107 18:07:44.227915 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:07:44.567945  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_146500.caffemodel
I1107 18:07:44.600953  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_146500.solverstate
I1107 18:07:44.609953  5452 solver.cpp:330] Iteration 146500, Testing net (#0)
I1107 18:07:44.609953  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:07:46.616103 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:07:46.696118  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1107 18:07:46.696118  5452 solver.cpp:397]     Test net output #1: loss = 0.314105 (* 1 = 0.314105 loss)
I1107 18:07:46.778118  5452 solver.cpp:218] Iteration 146500 (9.33431 iter/s, 10.7132s/100 iters), loss = 0.0388043
I1107 18:07:46.778118  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:07:46.778118  5452 solver.cpp:237]     Train net output #1: loss = 0.0388038 (* 1 = 0.0388038 loss)
I1107 18:07:46.778118  5452 sgd_solver.cpp:105] Iteration 146500, lr = 0.001
I1107 18:07:55.339233  5452 solver.cpp:218] Iteration 146600 (11.6806 iter/s, 8.56122s/100 iters), loss = 0.0235473
I1107 18:07:55.339233  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:07:55.339233  5452 solver.cpp:237]     Train net output #1: loss = 0.0235469 (* 1 = 0.0235469 loss)
I1107 18:07:55.339233  5452 sgd_solver.cpp:105] Iteration 146600, lr = 0.001
I1107 18:08:03.946365  5452 solver.cpp:218] Iteration 146700 (11.6192 iter/s, 8.60648s/100 iters), loss = 0.0353069
I1107 18:08:03.946365  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:08:03.946365  5452 solver.cpp:237]     Train net output #1: loss = 0.0353065 (* 1 = 0.0353065 loss)
I1107 18:08:03.946365  5452 sgd_solver.cpp:105] Iteration 146700, lr = 0.001
I1107 18:08:12.450234  5452 solver.cpp:218] Iteration 146800 (11.7599 iter/s, 8.5035s/100 iters), loss = 0.018934
I1107 18:08:12.451234  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:08:12.451234  5452 solver.cpp:237]     Train net output #1: loss = 0.0189335 (* 1 = 0.0189335 loss)
I1107 18:08:12.451234  5452 sgd_solver.cpp:105] Iteration 146800, lr = 0.001
I1107 18:08:20.986666  5452 solver.cpp:218] Iteration 146900 (11.7162 iter/s, 8.5352s/100 iters), loss = 0.0286664
I1107 18:08:20.986666  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:08:20.986666  5452 solver.cpp:237]     Train net output #1: loss = 0.0286659 (* 1 = 0.0286659 loss)
I1107 18:08:20.986666  5452 sgd_solver.cpp:105] Iteration 146900, lr = 0.001
I1107 18:08:29.097054 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:08:29.443161  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_147000.caffemodel
I1107 18:08:29.472174  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_147000.solverstate
I1107 18:08:29.482161  5452 solver.cpp:330] Iteration 147000, Testing net (#0)
I1107 18:08:29.482161  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:08:31.488628 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:08:31.568632  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9189
I1107 18:08:31.568632  5452 solver.cpp:397]     Test net output #1: loss = 0.311337 (* 1 = 0.311337 loss)
I1107 18:08:31.651659  5452 solver.cpp:218] Iteration 147000 (9.37673 iter/s, 10.6647s/100 iters), loss = 0.0294522
I1107 18:08:31.651659  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:08:31.651659  5452 solver.cpp:237]     Train net output #1: loss = 0.0294518 (* 1 = 0.0294518 loss)
I1107 18:08:31.651659  5452 sgd_solver.cpp:105] Iteration 147000, lr = 0.001
I1107 18:08:40.364112  5452 solver.cpp:218] Iteration 147100 (11.4788 iter/s, 8.71172s/100 iters), loss = 0.0301108
I1107 18:08:40.364112  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:08:40.364112  5452 solver.cpp:237]     Train net output #1: loss = 0.0301103 (* 1 = 0.0301103 loss)
I1107 18:08:40.364112  5452 sgd_solver.cpp:105] Iteration 147100, lr = 0.001
I1107 18:08:49.030381  5452 solver.cpp:218] Iteration 147200 (11.5401 iter/s, 8.66541s/100 iters), loss = 0.0211839
I1107 18:08:49.030381  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:08:49.030381  5452 solver.cpp:237]     Train net output #1: loss = 0.0211834 (* 1 = 0.0211834 loss)
I1107 18:08:49.030381  5452 sgd_solver.cpp:105] Iteration 147200, lr = 0.001
I1107 18:08:57.643018  5452 solver.cpp:218] Iteration 147300 (11.6108 iter/s, 8.6127s/100 iters), loss = 0.0194427
I1107 18:08:57.643018  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:08:57.643018  5452 solver.cpp:237]     Train net output #1: loss = 0.0194422 (* 1 = 0.0194422 loss)
I1107 18:08:57.643018  5452 sgd_solver.cpp:105] Iteration 147300, lr = 0.001
I1107 18:09:06.232424  5452 solver.cpp:218] Iteration 147400 (11.6441 iter/s, 8.58803s/100 iters), loss = 0.0259368
I1107 18:09:06.232424  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:09:06.232424  5452 solver.cpp:237]     Train net output #1: loss = 0.0259364 (* 1 = 0.0259364 loss)
I1107 18:09:06.232424  5452 sgd_solver.cpp:105] Iteration 147400, lr = 0.001
I1107 18:09:14.368564 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:09:14.714601  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_147500.caffemodel
I1107 18:09:14.746603  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_147500.solverstate
I1107 18:09:14.755609  5452 solver.cpp:330] Iteration 147500, Testing net (#0)
I1107 18:09:14.755609  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:09:16.738832 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:09:16.817836  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9195
I1107 18:09:16.817836  5452 solver.cpp:397]     Test net output #1: loss = 0.312812 (* 1 = 0.312812 loss)
I1107 18:09:16.898843  5452 solver.cpp:218] Iteration 147500 (9.3749 iter/s, 10.6668s/100 iters), loss = 0.0269673
I1107 18:09:16.899843  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:09:16.899843  5452 solver.cpp:237]     Train net output #1: loss = 0.0269668 (* 1 = 0.0269668 loss)
I1107 18:09:16.899843  5452 sgd_solver.cpp:105] Iteration 147500, lr = 0.001
I1107 18:09:25.444622  5452 solver.cpp:218] Iteration 147600 (11.7026 iter/s, 8.5451s/100 iters), loss = 0.0376822
I1107 18:09:25.444622  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:09:25.444622  5452 solver.cpp:237]     Train net output #1: loss = 0.0376817 (* 1 = 0.0376817 loss)
I1107 18:09:25.444622  5452 sgd_solver.cpp:105] Iteration 147600, lr = 0.001
I1107 18:09:33.958397  5452 solver.cpp:218] Iteration 147700 (11.7473 iter/s, 8.51262s/100 iters), loss = 0.0221612
I1107 18:09:33.958397  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:09:33.958397  5452 solver.cpp:237]     Train net output #1: loss = 0.0221607 (* 1 = 0.0221607 loss)
I1107 18:09:33.958397  5452 sgd_solver.cpp:105] Iteration 147700, lr = 0.001
I1107 18:09:42.483716  5452 solver.cpp:218] Iteration 147800 (11.7305 iter/s, 8.52477s/100 iters), loss = 0.0208616
I1107 18:09:42.483716  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:09:42.483716  5452 solver.cpp:237]     Train net output #1: loss = 0.0208611 (* 1 = 0.0208611 loss)
I1107 18:09:42.483716  5452 sgd_solver.cpp:105] Iteration 147800, lr = 0.001
I1107 18:09:50.971973  5452 solver.cpp:218] Iteration 147900 (11.781 iter/s, 8.48823s/100 iters), loss = 0.0465536
I1107 18:09:50.971973  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:09:50.971973  5452 solver.cpp:237]     Train net output #1: loss = 0.0465531 (* 1 = 0.0465531 loss)
I1107 18:09:50.971973  5452 sgd_solver.cpp:105] Iteration 147900, lr = 0.001
I1107 18:09:59.111145 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:09:59.453192  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_148000.caffemodel
I1107 18:09:59.482194  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_148000.solverstate
I1107 18:09:59.490195  5452 solver.cpp:330] Iteration 148000, Testing net (#0)
I1107 18:09:59.490195  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:10:01.483625 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:10:01.563652  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9197
I1107 18:10:01.563652  5452 solver.cpp:397]     Test net output #1: loss = 0.307541 (* 1 = 0.307541 loss)
I1107 18:10:01.645133  5452 solver.cpp:218] Iteration 148000 (9.37025 iter/s, 10.6721s/100 iters), loss = 0.0239852
I1107 18:10:01.645133  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:10:01.645133  5452 solver.cpp:237]     Train net output #1: loss = 0.0239848 (* 1 = 0.0239848 loss)
I1107 18:10:01.645133  5452 sgd_solver.cpp:105] Iteration 148000, lr = 0.001
I1107 18:10:10.145663  5452 solver.cpp:218] Iteration 148100 (11.7643 iter/s, 8.50027s/100 iters), loss = 0.0231085
I1107 18:10:10.145663  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:10:10.145663  5452 solver.cpp:237]     Train net output #1: loss = 0.023108 (* 1 = 0.023108 loss)
I1107 18:10:10.145663  5452 sgd_solver.cpp:105] Iteration 148100, lr = 0.001
I1107 18:10:18.717602  5452 solver.cpp:218] Iteration 148200 (11.6669 iter/s, 8.57126s/100 iters), loss = 0.0217455
I1107 18:10:18.717602  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:10:18.717602  5452 solver.cpp:237]     Train net output #1: loss = 0.021745 (* 1 = 0.021745 loss)
I1107 18:10:18.717602  5452 sgd_solver.cpp:105] Iteration 148200, lr = 0.001
I1107 18:10:27.396522  5452 solver.cpp:218] Iteration 148300 (11.5228 iter/s, 8.67845s/100 iters), loss = 0.0431737
I1107 18:10:27.396522  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:10:27.396522  5452 solver.cpp:237]     Train net output #1: loss = 0.0431732 (* 1 = 0.0431732 loss)
I1107 18:10:27.396522  5452 sgd_solver.cpp:105] Iteration 148300, lr = 0.001
I1107 18:10:36.076580  5452 solver.cpp:218] Iteration 148400 (11.5209 iter/s, 8.67987s/100 iters), loss = 0.0234213
I1107 18:10:36.076580  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:10:36.076580  5452 solver.cpp:237]     Train net output #1: loss = 0.0234209 (* 1 = 0.0234209 loss)
I1107 18:10:36.076580  5452 sgd_solver.cpp:105] Iteration 148400, lr = 0.001
I1107 18:10:44.296463 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:10:44.636498  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_148500.caffemodel
I1107 18:10:44.667517  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_148500.solverstate
I1107 18:10:44.678517  5452 solver.cpp:330] Iteration 148500, Testing net (#0)
I1107 18:10:44.678517  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:10:46.677696 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:10:46.756697  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9185
I1107 18:10:46.756697  5452 solver.cpp:397]     Test net output #1: loss = 0.311564 (* 1 = 0.311564 loss)
I1107 18:10:46.837704  5452 solver.cpp:218] Iteration 148500 (9.29328 iter/s, 10.7605s/100 iters), loss = 0.0348791
I1107 18:10:46.837704  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:10:46.837704  5452 solver.cpp:237]     Train net output #1: loss = 0.0348787 (* 1 = 0.0348787 loss)
I1107 18:10:46.837704  5452 sgd_solver.cpp:105] Iteration 148500, lr = 0.001
I1107 18:10:55.359552  5452 solver.cpp:218] Iteration 148600 (11.735 iter/s, 8.5215s/100 iters), loss = 0.0224329
I1107 18:10:55.359552  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:10:55.359552  5452 solver.cpp:237]     Train net output #1: loss = 0.0224325 (* 1 = 0.0224325 loss)
I1107 18:10:55.359552  5452 sgd_solver.cpp:105] Iteration 148600, lr = 0.001
I1107 18:11:03.923658  5452 solver.cpp:218] Iteration 148700 (11.6777 iter/s, 8.56335s/100 iters), loss = 0.0193147
I1107 18:11:03.923658  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:11:03.923658  5452 solver.cpp:237]     Train net output #1: loss = 0.0193143 (* 1 = 0.0193143 loss)
I1107 18:11:03.923658  5452 sgd_solver.cpp:105] Iteration 148700, lr = 0.001
I1107 18:11:12.537995  5452 solver.cpp:218] Iteration 148800 (11.6099 iter/s, 8.61332s/100 iters), loss = 0.0272621
I1107 18:11:12.537995  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:11:12.537995  5452 solver.cpp:237]     Train net output #1: loss = 0.0272617 (* 1 = 0.0272617 loss)
I1107 18:11:12.537995  5452 sgd_solver.cpp:105] Iteration 148800, lr = 0.001
I1107 18:11:21.084851  5452 solver.cpp:218] Iteration 148900 (11.7004 iter/s, 8.54675s/100 iters), loss = 0.0302531
I1107 18:11:21.084851  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:11:21.084851  5452 solver.cpp:237]     Train net output #1: loss = 0.0302526 (* 1 = 0.0302526 loss)
I1107 18:11:21.084851  5452 sgd_solver.cpp:105] Iteration 148900, lr = 0.001
I1107 18:11:29.284961 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:11:29.632568  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_149000.caffemodel
I1107 18:11:29.665565  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_149000.solverstate
I1107 18:11:29.675570  5452 solver.cpp:330] Iteration 149000, Testing net (#0)
I1107 18:11:29.675570  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:11:31.694906 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:11:31.775408  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1107 18:11:31.775408  5452 solver.cpp:397]     Test net output #1: loss = 0.309646 (* 1 = 0.309646 loss)
I1107 18:11:31.855909  5452 solver.cpp:218] Iteration 149000 (9.28436 iter/s, 10.7708s/100 iters), loss = 0.0271718
I1107 18:11:31.855909  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:11:31.855909  5452 solver.cpp:237]     Train net output #1: loss = 0.0271714 (* 1 = 0.0271714 loss)
I1107 18:11:31.855909  5452 sgd_solver.cpp:105] Iteration 149000, lr = 0.001
I1107 18:11:40.590708  5452 solver.cpp:218] Iteration 149100 (11.4503 iter/s, 8.73341s/100 iters), loss = 0.0234617
I1107 18:11:40.590708  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:11:40.590708  5452 solver.cpp:237]     Train net output #1: loss = 0.0234613 (* 1 = 0.0234613 loss)
I1107 18:11:40.590708  5452 sgd_solver.cpp:105] Iteration 149100, lr = 0.001
I1107 18:11:49.194756  5452 solver.cpp:218] Iteration 149200 (11.6223 iter/s, 8.60416s/100 iters), loss = 0.017615
I1107 18:11:49.194756  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:11:49.194756  5452 solver.cpp:237]     Train net output #1: loss = 0.0176146 (* 1 = 0.0176146 loss)
I1107 18:11:49.194756  5452 sgd_solver.cpp:105] Iteration 149200, lr = 0.001
I1107 18:11:57.744307  5452 solver.cpp:218] Iteration 149300 (11.6973 iter/s, 8.54897s/100 iters), loss = 0.0158236
I1107 18:11:57.744307  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:11:57.744307  5452 solver.cpp:237]     Train net output #1: loss = 0.0158232 (* 1 = 0.0158232 loss)
I1107 18:11:57.744307  5452 sgd_solver.cpp:105] Iteration 149300, lr = 0.001
I1107 18:12:06.598412  5452 solver.cpp:218] Iteration 149400 (11.2953 iter/s, 8.85327s/100 iters), loss = 0.0245528
I1107 18:12:06.598412  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:12:06.598412  5452 solver.cpp:237]     Train net output #1: loss = 0.0245523 (* 1 = 0.0245523 loss)
I1107 18:12:06.598412  5452 sgd_solver.cpp:105] Iteration 149400, lr = 0.001
I1107 18:12:15.069481 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:12:15.422564  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_149500.caffemodel
I1107 18:12:15.459556  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_149500.solverstate
I1107 18:12:15.471554  5452 solver.cpp:330] Iteration 149500, Testing net (#0)
I1107 18:12:15.471554  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:12:17.530696 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:12:17.612715  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9189
I1107 18:12:17.612715  5452 solver.cpp:397]     Test net output #1: loss = 0.306705 (* 1 = 0.306705 loss)
I1107 18:12:17.697703  5452 solver.cpp:218] Iteration 149500 (9.00972 iter/s, 11.0991s/100 iters), loss = 0.0301182
I1107 18:12:17.697703  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:12:17.697703  5452 solver.cpp:237]     Train net output #1: loss = 0.0301178 (* 1 = 0.0301178 loss)
I1107 18:12:17.697703  5452 sgd_solver.cpp:105] Iteration 149500, lr = 0.001
I1107 18:12:26.546970  5452 solver.cpp:218] Iteration 149600 (11.3018 iter/s, 8.84813s/100 iters), loss = 0.0257503
I1107 18:12:26.546970  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:12:26.546970  5452 solver.cpp:237]     Train net output #1: loss = 0.0257498 (* 1 = 0.0257498 loss)
I1107 18:12:26.546970  5452 sgd_solver.cpp:105] Iteration 149600, lr = 0.001
I1107 18:12:35.084524  5452 solver.cpp:218] Iteration 149700 (11.7132 iter/s, 8.53734s/100 iters), loss = 0.0178576
I1107 18:12:35.084524  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:12:35.084524  5452 solver.cpp:237]     Train net output #1: loss = 0.0178572 (* 1 = 0.0178572 loss)
I1107 18:12:35.084524  5452 sgd_solver.cpp:105] Iteration 149700, lr = 0.001
I1107 18:12:43.628628  5452 solver.cpp:218] Iteration 149800 (11.7045 iter/s, 8.54373s/100 iters), loss = 0.023218
I1107 18:12:43.628628  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:12:43.628628  5452 solver.cpp:237]     Train net output #1: loss = 0.0232175 (* 1 = 0.0232175 loss)
I1107 18:12:43.628628  5452 sgd_solver.cpp:105] Iteration 149800, lr = 0.001
I1107 18:12:52.343111  5452 solver.cpp:218] Iteration 149900 (11.4758 iter/s, 8.71403s/100 iters), loss = 0.0321465
I1107 18:12:52.344120  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:12:52.344120  5452 solver.cpp:237]     Train net output #1: loss = 0.0321461 (* 1 = 0.0321461 loss)
I1107 18:12:52.344120  5452 sgd_solver.cpp:105] Iteration 149900, lr = 0.001
I1107 18:13:00.479789 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:13:00.820169  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_150000.caffemodel
I1107 18:13:00.853162  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_150000.solverstate
I1107 18:13:00.862161  5452 solver.cpp:330] Iteration 150000, Testing net (#0)
I1107 18:13:00.862161  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:13:02.850566 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:13:02.930101  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9213
I1107 18:13:02.930101  5452 solver.cpp:397]     Test net output #1: loss = 0.309881 (* 1 = 0.309881 loss)
I1107 18:13:03.011106  5452 solver.cpp:218] Iteration 150000 (9.37484 iter/s, 10.6668s/100 iters), loss = 0.0276459
I1107 18:13:03.011592  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:13:03.011592  5452 solver.cpp:237]     Train net output #1: loss = 0.0276454 (* 1 = 0.0276454 loss)
I1107 18:13:03.011592  5452 sgd_solver.cpp:105] Iteration 150000, lr = 0.001
I1107 18:13:11.561691  5452 solver.cpp:218] Iteration 150100 (11.6964 iter/s, 8.54965s/100 iters), loss = 0.0281219
I1107 18:13:11.561691  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:13:11.561691  5452 solver.cpp:237]     Train net output #1: loss = 0.0281215 (* 1 = 0.0281215 loss)
I1107 18:13:11.561691  5452 sgd_solver.cpp:105] Iteration 150100, lr = 0.001
I1107 18:13:20.101605  5452 solver.cpp:218] Iteration 150200 (11.7101 iter/s, 8.53965s/100 iters), loss = 0.0282582
I1107 18:13:20.101605  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:13:20.101605  5452 solver.cpp:237]     Train net output #1: loss = 0.0282577 (* 1 = 0.0282577 loss)
I1107 18:13:20.101605  5452 sgd_solver.cpp:105] Iteration 150200, lr = 0.001
I1107 18:13:28.647620  5452 solver.cpp:218] Iteration 150300 (11.7019 iter/s, 8.54559s/100 iters), loss = 0.0198244
I1107 18:13:28.647620  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:13:28.647620  5452 solver.cpp:237]     Train net output #1: loss = 0.019824 (* 1 = 0.019824 loss)
I1107 18:13:28.647620  5452 sgd_solver.cpp:105] Iteration 150300, lr = 0.001
I1107 18:13:37.201679  5452 solver.cpp:218] Iteration 150400 (11.6914 iter/s, 8.55328s/100 iters), loss = 0.0272056
I1107 18:13:37.201679  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:13:37.201679  5452 solver.cpp:237]     Train net output #1: loss = 0.0272052 (* 1 = 0.0272052 loss)
I1107 18:13:37.201679  5452 sgd_solver.cpp:105] Iteration 150400, lr = 0.001
I1107 18:13:45.336207 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:13:45.674226  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_150500.caffemodel
I1107 18:13:45.705227  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_150500.solverstate
I1107 18:13:45.714733  5452 solver.cpp:330] Iteration 150500, Testing net (#0)
I1107 18:13:45.715234  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:13:47.712450 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:13:47.791952  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9192
I1107 18:13:47.791952  5452 solver.cpp:397]     Test net output #1: loss = 0.305829 (* 1 = 0.305829 loss)
I1107 18:13:47.873560  5452 solver.cpp:218] Iteration 150500 (9.371 iter/s, 10.6712s/100 iters), loss = 0.023234
I1107 18:13:47.873560  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:13:47.873560  5452 solver.cpp:237]     Train net output #1: loss = 0.0232335 (* 1 = 0.0232335 loss)
I1107 18:13:47.873560  5452 sgd_solver.cpp:105] Iteration 150500, lr = 0.001
I1107 18:13:56.427791  5452 solver.cpp:218] Iteration 150600 (11.6908 iter/s, 8.55375s/100 iters), loss = 0.0268751
I1107 18:13:56.427791  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:13:56.427791  5452 solver.cpp:237]     Train net output #1: loss = 0.0268747 (* 1 = 0.0268747 loss)
I1107 18:13:56.427791  5452 sgd_solver.cpp:105] Iteration 150600, lr = 0.001
I1107 18:14:05.007774  5452 solver.cpp:218] Iteration 150700 (11.6559 iter/s, 8.57933s/100 iters), loss = 0.0218864
I1107 18:14:05.007774  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:14:05.007774  5452 solver.cpp:237]     Train net output #1: loss = 0.0218859 (* 1 = 0.0218859 loss)
I1107 18:14:05.007774  5452 sgd_solver.cpp:105] Iteration 150700, lr = 0.001
I1107 18:14:13.570418  5452 solver.cpp:218] Iteration 150800 (11.6795 iter/s, 8.56204s/100 iters), loss = 0.0301657
I1107 18:14:13.570418  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:14:13.570418  5452 solver.cpp:237]     Train net output #1: loss = 0.0301653 (* 1 = 0.0301653 loss)
I1107 18:14:13.570418  5452 sgd_solver.cpp:105] Iteration 150800, lr = 0.001
I1107 18:14:22.160357  5452 solver.cpp:218] Iteration 150900 (11.6413 iter/s, 8.59014s/100 iters), loss = 0.0291418
I1107 18:14:22.160357  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:14:22.160357  5452 solver.cpp:237]     Train net output #1: loss = 0.0291414 (* 1 = 0.0291414 loss)
I1107 18:14:22.160357  5452 sgd_solver.cpp:105] Iteration 150900, lr = 0.001
I1107 18:14:30.289623 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:14:30.627661  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_151000.caffemodel
I1107 18:14:30.658661  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_151000.solverstate
I1107 18:14:30.666661  5452 solver.cpp:330] Iteration 151000, Testing net (#0)
I1107 18:14:30.666661  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:14:32.652835 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:14:32.731839  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9197
I1107 18:14:32.731839  5452 solver.cpp:397]     Test net output #1: loss = 0.309628 (* 1 = 0.309628 loss)
I1107 18:14:32.813840  5452 solver.cpp:218] Iteration 151000 (9.38744 iter/s, 10.6525s/100 iters), loss = 0.0286027
I1107 18:14:32.813840  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:14:32.813840  5452 solver.cpp:237]     Train net output #1: loss = 0.0286023 (* 1 = 0.0286023 loss)
I1107 18:14:32.813840  5452 sgd_solver.cpp:105] Iteration 151000, lr = 0.001
I1107 18:14:41.354915  5452 solver.cpp:218] Iteration 151100 (11.7086 iter/s, 8.54076s/100 iters), loss = 0.0301571
I1107 18:14:41.354915  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:14:41.354915  5452 solver.cpp:237]     Train net output #1: loss = 0.0301567 (* 1 = 0.0301567 loss)
I1107 18:14:41.354915  5452 sgd_solver.cpp:105] Iteration 151100, lr = 0.001
I1107 18:14:49.859750  5452 solver.cpp:218] Iteration 151200 (11.7593 iter/s, 8.50387s/100 iters), loss = 0.0259141
I1107 18:14:49.859750  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:14:49.859750  5452 solver.cpp:237]     Train net output #1: loss = 0.0259136 (* 1 = 0.0259136 loss)
I1107 18:14:49.859750  5452 sgd_solver.cpp:105] Iteration 151200, lr = 0.001
I1107 18:14:58.409687  5452 solver.cpp:218] Iteration 151300 (11.6958 iter/s, 8.55005s/100 iters), loss = 0.0280346
I1107 18:14:58.409687  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:14:58.409687  5452 solver.cpp:237]     Train net output #1: loss = 0.0280341 (* 1 = 0.0280341 loss)
I1107 18:14:58.409687  5452 sgd_solver.cpp:105] Iteration 151300, lr = 0.001
I1107 18:15:06.994701  5452 solver.cpp:218] Iteration 151400 (11.6493 iter/s, 8.58419s/100 iters), loss = 0.0271376
I1107 18:15:06.994701  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:15:06.994701  5452 solver.cpp:237]     Train net output #1: loss = 0.0271371 (* 1 = 0.0271371 loss)
I1107 18:15:06.994701  5452 sgd_solver.cpp:105] Iteration 151400, lr = 0.001
I1107 18:15:15.076375 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:15:15.411617  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_151500.caffemodel
I1107 18:15:15.441231  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_151500.solverstate
I1107 18:15:15.452214  5452 solver.cpp:330] Iteration 151500, Testing net (#0)
I1107 18:15:15.452214  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:15:17.432107 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:15:17.511617  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9189
I1107 18:15:17.511617  5452 solver.cpp:397]     Test net output #1: loss = 0.312705 (* 1 = 0.312705 loss)
I1107 18:15:17.593513  5452 solver.cpp:218] Iteration 151500 (9.43592 iter/s, 10.5978s/100 iters), loss = 0.0272772
I1107 18:15:17.593513  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:15:17.593513  5452 solver.cpp:237]     Train net output #1: loss = 0.0272768 (* 1 = 0.0272768 loss)
I1107 18:15:17.593513  5452 sgd_solver.cpp:105] Iteration 151500, lr = 0.001
I1107 18:15:26.102283  5452 solver.cpp:218] Iteration 151600 (11.7528 iter/s, 8.50862s/100 iters), loss = 0.0340645
I1107 18:15:26.102283  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:15:26.102283  5452 solver.cpp:237]     Train net output #1: loss = 0.0340641 (* 1 = 0.0340641 loss)
I1107 18:15:26.102283  5452 sgd_solver.cpp:105] Iteration 151600, lr = 0.001
I1107 18:15:34.602144  5452 solver.cpp:218] Iteration 151700 (11.766 iter/s, 8.49909s/100 iters), loss = 0.0176055
I1107 18:15:34.602144  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:15:34.602144  5452 solver.cpp:237]     Train net output #1: loss = 0.0176051 (* 1 = 0.0176051 loss)
I1107 18:15:34.602144  5452 sgd_solver.cpp:105] Iteration 151700, lr = 0.001
I1107 18:15:43.108985  5452 solver.cpp:218] Iteration 151800 (11.7552 iter/s, 8.5069s/100 iters), loss = 0.0262069
I1107 18:15:43.108985  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:15:43.108985  5452 solver.cpp:237]     Train net output #1: loss = 0.0262065 (* 1 = 0.0262065 loss)
I1107 18:15:43.108985  5452 sgd_solver.cpp:105] Iteration 151800, lr = 0.001
I1107 18:15:51.624897  5452 solver.cpp:218] Iteration 151900 (11.744 iter/s, 8.51497s/100 iters), loss = 0.0223631
I1107 18:15:51.624897  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:15:51.624897  5452 solver.cpp:237]     Train net output #1: loss = 0.0223626 (* 1 = 0.0223626 loss)
I1107 18:15:51.624897  5452 sgd_solver.cpp:105] Iteration 151900, lr = 0.001
I1107 18:15:59.718626 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:16:00.055392  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_152000.caffemodel
I1107 18:16:00.086391  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_152000.solverstate
I1107 18:16:00.095393  5452 solver.cpp:330] Iteration 152000, Testing net (#0)
I1107 18:16:00.095393  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:16:02.081547 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:16:02.161598  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 18:16:02.161598  5452 solver.cpp:397]     Test net output #1: loss = 0.314465 (* 1 = 0.314465 loss)
I1107 18:16:02.243083  5452 solver.cpp:218] Iteration 152000 (9.41821 iter/s, 10.6177s/100 iters), loss = 0.0317588
I1107 18:16:02.243083  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:16:02.243083  5452 solver.cpp:237]     Train net output #1: loss = 0.0317584 (* 1 = 0.0317584 loss)
I1107 18:16:02.243083  5452 sgd_solver.cpp:105] Iteration 152000, lr = 0.001
I1107 18:16:10.761971  5452 solver.cpp:218] Iteration 152100 (11.7393 iter/s, 8.51837s/100 iters), loss = 0.0338222
I1107 18:16:10.761971  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:16:10.761971  5452 solver.cpp:237]     Train net output #1: loss = 0.0338218 (* 1 = 0.0338218 loss)
I1107 18:16:10.761971  5452 sgd_solver.cpp:105] Iteration 152100, lr = 0.001
I1107 18:16:19.276249  5452 solver.cpp:218] Iteration 152200 (11.7457 iter/s, 8.51375s/100 iters), loss = 0.0249103
I1107 18:16:19.276249  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:16:19.276249  5452 solver.cpp:237]     Train net output #1: loss = 0.0249098 (* 1 = 0.0249098 loss)
I1107 18:16:19.276249  5452 sgd_solver.cpp:105] Iteration 152200, lr = 0.001
I1107 18:16:27.789764  5452 solver.cpp:218] Iteration 152300 (11.7463 iter/s, 8.51329s/100 iters), loss = 0.0181314
I1107 18:16:27.789764  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:16:27.789764  5452 solver.cpp:237]     Train net output #1: loss = 0.0181309 (* 1 = 0.0181309 loss)
I1107 18:16:27.789764  5452 sgd_solver.cpp:105] Iteration 152300, lr = 0.001
I1107 18:16:36.298784  5452 solver.cpp:218] Iteration 152400 (11.7532 iter/s, 8.50834s/100 iters), loss = 0.0283477
I1107 18:16:36.298784  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:16:36.298784  5452 solver.cpp:237]     Train net output #1: loss = 0.0283472 (* 1 = 0.0283472 loss)
I1107 18:16:36.298784  5452 sgd_solver.cpp:105] Iteration 152400, lr = 0.001
I1107 18:16:44.391053 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:16:44.726603  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_152500.caffemodel
I1107 18:16:44.755673  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_152500.solverstate
I1107 18:16:44.764672  5452 solver.cpp:330] Iteration 152500, Testing net (#0)
I1107 18:16:44.764672  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:16:46.746405 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:16:46.825904  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1107 18:16:46.825904  5452 solver.cpp:397]     Test net output #1: loss = 0.311473 (* 1 = 0.311473 loss)
I1107 18:16:46.907419  5452 solver.cpp:218] Iteration 152500 (9.42697 iter/s, 10.6079s/100 iters), loss = 0.0430815
I1107 18:16:46.907419  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:16:46.907419  5452 solver.cpp:237]     Train net output #1: loss = 0.0430811 (* 1 = 0.0430811 loss)
I1107 18:16:46.907419  5452 sgd_solver.cpp:105] Iteration 152500, lr = 0.001
I1107 18:16:55.420758  5452 solver.cpp:218] Iteration 152600 (11.7466 iter/s, 8.51308s/100 iters), loss = 0.0321293
I1107 18:16:55.421259  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:16:55.421259  5452 solver.cpp:237]     Train net output #1: loss = 0.0321289 (* 1 = 0.0321289 loss)
I1107 18:16:55.421259  5452 sgd_solver.cpp:105] Iteration 152600, lr = 0.001
I1107 18:17:03.930610  5452 solver.cpp:218] Iteration 152700 (11.7511 iter/s, 8.50982s/100 iters), loss = 0.0178117
I1107 18:17:03.930610  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:17:03.931613  5452 solver.cpp:237]     Train net output #1: loss = 0.0178113 (* 1 = 0.0178113 loss)
I1107 18:17:03.931613  5452 sgd_solver.cpp:105] Iteration 152700, lr = 0.001
I1107 18:17:12.439409  5452 solver.cpp:218] Iteration 152800 (11.7541 iter/s, 8.50765s/100 iters), loss = 0.0255347
I1107 18:17:12.439409  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:17:12.439409  5452 solver.cpp:237]     Train net output #1: loss = 0.0255342 (* 1 = 0.0255342 loss)
I1107 18:17:12.439409  5452 sgd_solver.cpp:105] Iteration 152800, lr = 0.001
I1107 18:17:20.944350  5452 solver.cpp:218] Iteration 152900 (11.7579 iter/s, 8.50493s/100 iters), loss = 0.0224767
I1107 18:17:20.944350  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:17:20.944350  5452 solver.cpp:237]     Train net output #1: loss = 0.0224763 (* 1 = 0.0224763 loss)
I1107 18:17:20.944350  5452 sgd_solver.cpp:105] Iteration 152900, lr = 0.001
I1107 18:17:29.037703 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:17:29.372722  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_153000.caffemodel
I1107 18:17:29.403726  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_153000.solverstate
I1107 18:17:29.412727  5452 solver.cpp:330] Iteration 153000, Testing net (#0)
I1107 18:17:29.412727  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:17:31.392976 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:17:31.472479  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9192
I1107 18:17:31.472479  5452 solver.cpp:397]     Test net output #1: loss = 0.316301 (* 1 = 0.316301 loss)
I1107 18:17:31.552983  5452 solver.cpp:218] Iteration 153000 (9.42662 iter/s, 10.6083s/100 iters), loss = 0.0279341
I1107 18:17:31.553982  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:17:31.553982  5452 solver.cpp:237]     Train net output #1: loss = 0.0279337 (* 1 = 0.0279337 loss)
I1107 18:17:31.553982  5452 sgd_solver.cpp:46] MultiStep Status: Iteration 153000, step = 3
I1107 18:17:31.553982  5452 sgd_solver.cpp:105] Iteration 153000, lr = 0.0001
I1107 18:17:40.053426  5452 solver.cpp:218] Iteration 153100 (11.7661 iter/s, 8.49902s/100 iters), loss = 0.0332315
I1107 18:17:40.053426  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:17:40.053426  5452 solver.cpp:237]     Train net output #1: loss = 0.033231 (* 1 = 0.033231 loss)
I1107 18:17:40.053426  5452 sgd_solver.cpp:105] Iteration 153100, lr = 0.0001
I1107 18:17:48.569690  5452 solver.cpp:218] Iteration 153200 (11.7425 iter/s, 8.51607s/100 iters), loss = 0.0259179
I1107 18:17:48.569690  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:17:48.569690  5452 solver.cpp:237]     Train net output #1: loss = 0.0259175 (* 1 = 0.0259175 loss)
I1107 18:17:48.569690  5452 sgd_solver.cpp:105] Iteration 153200, lr = 0.0001
I1107 18:17:57.074910  5452 solver.cpp:218] Iteration 153300 (11.7586 iter/s, 8.50443s/100 iters), loss = 0.0180296
I1107 18:17:57.074910  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:17:57.074910  5452 solver.cpp:237]     Train net output #1: loss = 0.0180292 (* 1 = 0.0180292 loss)
I1107 18:17:57.074910  5452 sgd_solver.cpp:105] Iteration 153300, lr = 0.0001
I1107 18:18:05.574434  5452 solver.cpp:218] Iteration 153400 (11.7661 iter/s, 8.499s/100 iters), loss = 0.0208434
I1107 18:18:05.574434  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:18:05.574434  5452 solver.cpp:237]     Train net output #1: loss = 0.020843 (* 1 = 0.020843 loss)
I1107 18:18:05.574434  5452 sgd_solver.cpp:105] Iteration 153400, lr = 0.0001
I1107 18:18:13.711588 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:18:14.050907  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_153500.caffemodel
I1107 18:18:14.079907  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_153500.solverstate
I1107 18:18:14.087908  5452 solver.cpp:330] Iteration 153500, Testing net (#0)
I1107 18:18:14.088907  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:18:16.092066 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:18:16.172075  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9201
I1107 18:18:16.172075  5452 solver.cpp:397]     Test net output #1: loss = 0.303053 (* 1 = 0.303053 loss)
I1107 18:18:16.254081  5452 solver.cpp:218] Iteration 153500 (9.36401 iter/s, 10.6792s/100 iters), loss = 0.0407436
I1107 18:18:16.254081  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:18:16.254081  5452 solver.cpp:237]     Train net output #1: loss = 0.0407431 (* 1 = 0.0407431 loss)
I1107 18:18:16.254081  5452 sgd_solver.cpp:105] Iteration 153500, lr = 0.0001
I1107 18:18:24.801012  5452 solver.cpp:218] Iteration 153600 (11.7006 iter/s, 8.54656s/100 iters), loss = 0.0238145
I1107 18:18:24.801012  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:18:24.801012  5452 solver.cpp:237]     Train net output #1: loss = 0.023814 (* 1 = 0.023814 loss)
I1107 18:18:24.801012  5452 sgd_solver.cpp:105] Iteration 153600, lr = 0.0001
I1107 18:18:33.407639  5452 solver.cpp:218] Iteration 153700 (11.6195 iter/s, 8.60623s/100 iters), loss = 0.0210839
I1107 18:18:33.407639  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:18:33.407639  5452 solver.cpp:237]     Train net output #1: loss = 0.0210834 (* 1 = 0.0210834 loss)
I1107 18:18:33.407639  5452 sgd_solver.cpp:105] Iteration 153700, lr = 0.0001
I1107 18:18:41.967593  5452 solver.cpp:218] Iteration 153800 (11.683 iter/s, 8.55944s/100 iters), loss = 0.0180322
I1107 18:18:41.967593  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:18:41.967593  5452 solver.cpp:237]     Train net output #1: loss = 0.0180317 (* 1 = 0.0180317 loss)
I1107 18:18:41.967593  5452 sgd_solver.cpp:105] Iteration 153800, lr = 0.0001
I1107 18:18:50.548883  5452 solver.cpp:218] Iteration 153900 (11.6543 iter/s, 8.58053s/100 iters), loss = 0.0230021
I1107 18:18:50.548883  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:18:50.548883  5452 solver.cpp:237]     Train net output #1: loss = 0.0230017 (* 1 = 0.0230017 loss)
I1107 18:18:50.548883  5452 sgd_solver.cpp:105] Iteration 153900, lr = 0.0001
I1107 18:18:58.703469 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:18:59.040489  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_154000.caffemodel
I1107 18:18:59.071494  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_154000.solverstate
I1107 18:18:59.079499  5452 solver.cpp:330] Iteration 154000, Testing net (#0)
I1107 18:18:59.080498  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:19:01.065767 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:19:01.145768  5452 solver.cpp:397]     Test net output #0: accuracy = 0.921
I1107 18:19:01.145768  5452 solver.cpp:397]     Test net output #1: loss = 0.302767 (* 1 = 0.302767 loss)
I1107 18:19:01.226775  5452 solver.cpp:218] Iteration 154000 (9.36518 iter/s, 10.6779s/100 iters), loss = 0.0294226
I1107 18:19:01.226775  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:19:01.226775  5452 solver.cpp:237]     Train net output #1: loss = 0.0294222 (* 1 = 0.0294222 loss)
I1107 18:19:01.226775  5452 sgd_solver.cpp:105] Iteration 154000, lr = 0.0001
I1107 18:19:09.846801  5452 solver.cpp:218] Iteration 154100 (11.6014 iter/s, 8.61964s/100 iters), loss = 0.0219621
I1107 18:19:09.846801  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:19:09.846801  5452 solver.cpp:237]     Train net output #1: loss = 0.0219617 (* 1 = 0.0219617 loss)
I1107 18:19:09.846801  5452 sgd_solver.cpp:105] Iteration 154100, lr = 0.0001
I1107 18:19:18.373534  5452 solver.cpp:218] Iteration 154200 (11.7293 iter/s, 8.52565s/100 iters), loss = 0.0360638
I1107 18:19:18.373534  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1107 18:19:18.373534  5452 solver.cpp:237]     Train net output #1: loss = 0.0360634 (* 1 = 0.0360634 loss)
I1107 18:19:18.373534  5452 sgd_solver.cpp:105] Iteration 154200, lr = 0.0001
I1107 18:19:26.909368  5452 solver.cpp:218] Iteration 154300 (11.7166 iter/s, 8.53492s/100 iters), loss = 0.0193473
I1107 18:19:26.909368  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:19:26.909368  5452 solver.cpp:237]     Train net output #1: loss = 0.0193468 (* 1 = 0.0193468 loss)
I1107 18:19:26.909368  5452 sgd_solver.cpp:105] Iteration 154300, lr = 0.0001
I1107 18:19:35.494530  5452 solver.cpp:218] Iteration 154400 (11.6482 iter/s, 8.58504s/100 iters), loss = 0.0246614
I1107 18:19:35.494530  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:19:35.494530  5452 solver.cpp:237]     Train net output #1: loss = 0.0246609 (* 1 = 0.0246609 loss)
I1107 18:19:35.494530  5452 sgd_solver.cpp:105] Iteration 154400, lr = 0.0001
I1107 18:19:43.650758 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:19:43.986865  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_154500.caffemodel
I1107 18:19:44.016870  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_154500.solverstate
I1107 18:19:44.025373  5452 solver.cpp:330] Iteration 154500, Testing net (#0)
I1107 18:19:44.025373  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:19:46.048662 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:19:46.130674  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9207
I1107 18:19:46.130674  5452 solver.cpp:397]     Test net output #1: loss = 0.303798 (* 1 = 0.303798 loss)
I1107 18:19:46.213189  5452 solver.cpp:218] Iteration 154500 (9.33033 iter/s, 10.7177s/100 iters), loss = 0.0339
I1107 18:19:46.213189  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:19:46.213189  5452 solver.cpp:237]     Train net output #1: loss = 0.0338995 (* 1 = 0.0338995 loss)
I1107 18:19:46.213189  5452 sgd_solver.cpp:105] Iteration 154500, lr = 0.0001
I1107 18:19:54.725435  5452 solver.cpp:218] Iteration 154600 (11.748 iter/s, 8.51208s/100 iters), loss = 0.0304588
I1107 18:19:54.725435  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:19:54.725435  5452 solver.cpp:237]     Train net output #1: loss = 0.0304584 (* 1 = 0.0304584 loss)
I1107 18:19:54.725435  5452 sgd_solver.cpp:105] Iteration 154600, lr = 0.0001
I1107 18:20:03.250315  5452 solver.cpp:218] Iteration 154700 (11.7309 iter/s, 8.52453s/100 iters), loss = 0.021301
I1107 18:20:03.250315  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:20:03.250315  5452 solver.cpp:237]     Train net output #1: loss = 0.0213005 (* 1 = 0.0213005 loss)
I1107 18:20:03.250315  5452 sgd_solver.cpp:105] Iteration 154700, lr = 0.0001
I1107 18:20:11.800454  5452 solver.cpp:218] Iteration 154800 (11.6965 iter/s, 8.54956s/100 iters), loss = 0.0173406
I1107 18:20:11.800454  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:20:11.800454  5452 solver.cpp:237]     Train net output #1: loss = 0.0173402 (* 1 = 0.0173402 loss)
I1107 18:20:11.800454  5452 sgd_solver.cpp:105] Iteration 154800, lr = 0.0001
I1107 18:20:20.325232  5452 solver.cpp:218] Iteration 154900 (11.7315 iter/s, 8.52408s/100 iters), loss = 0.0201115
I1107 18:20:20.325232  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:20:20.325232  5452 solver.cpp:237]     Train net output #1: loss = 0.0201111 (* 1 = 0.0201111 loss)
I1107 18:20:20.325232  5452 sgd_solver.cpp:105] Iteration 154900, lr = 0.0001
I1107 18:20:28.434845 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:20:28.769868  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_155000.caffemodel
I1107 18:20:28.799882  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_155000.solverstate
I1107 18:20:28.808882  5452 solver.cpp:330] Iteration 155000, Testing net (#0)
I1107 18:20:28.808882  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:20:30.790071 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:20:30.869073  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 18:20:30.869073  5452 solver.cpp:397]     Test net output #1: loss = 0.30312 (* 1 = 0.30312 loss)
I1107 18:20:30.951077  5452 solver.cpp:218] Iteration 155000 (9.41144 iter/s, 10.6254s/100 iters), loss = 0.0314767
I1107 18:20:30.951077  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:20:30.951077  5452 solver.cpp:237]     Train net output #1: loss = 0.0314763 (* 1 = 0.0314763 loss)
I1107 18:20:30.951077  5452 sgd_solver.cpp:105] Iteration 155000, lr = 0.0001
I1107 18:20:39.456476  5452 solver.cpp:218] Iteration 155100 (11.758 iter/s, 8.50488s/100 iters), loss = 0.02658
I1107 18:20:39.456476  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:20:39.456476  5452 solver.cpp:237]     Train net output #1: loss = 0.0265796 (* 1 = 0.0265796 loss)
I1107 18:20:39.456476  5452 sgd_solver.cpp:105] Iteration 155100, lr = 0.0001
I1107 18:20:47.957275  5452 solver.cpp:218] Iteration 155200 (11.7636 iter/s, 8.50079s/100 iters), loss = 0.016881
I1107 18:20:47.958276  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:20:47.958276  5452 solver.cpp:237]     Train net output #1: loss = 0.0168805 (* 1 = 0.0168805 loss)
I1107 18:20:47.958276  5452 sgd_solver.cpp:105] Iteration 155200, lr = 0.0001
I1107 18:20:56.465991  5452 solver.cpp:218] Iteration 155300 (11.7534 iter/s, 8.50817s/100 iters), loss = 0.0190216
I1107 18:20:56.466991  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:20:56.466991  5452 solver.cpp:237]     Train net output #1: loss = 0.0190212 (* 1 = 0.0190212 loss)
I1107 18:20:56.466991  5452 sgd_solver.cpp:105] Iteration 155300, lr = 0.0001
I1107 18:21:04.974838  5452 solver.cpp:218] Iteration 155400 (11.7542 iter/s, 8.50762s/100 iters), loss = 0.024367
I1107 18:21:04.974838  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:21:04.974838  5452 solver.cpp:237]     Train net output #1: loss = 0.0243665 (* 1 = 0.0243665 loss)
I1107 18:21:04.974838  5452 sgd_solver.cpp:105] Iteration 155400, lr = 0.0001
I1107 18:21:13.059479 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:21:13.397513  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_155500.caffemodel
I1107 18:21:13.429512  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_155500.solverstate
I1107 18:21:13.438514  5452 solver.cpp:330] Iteration 155500, Testing net (#0)
I1107 18:21:13.438514  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:21:15.422782 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:21:15.501791  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9203
I1107 18:21:15.501791  5452 solver.cpp:397]     Test net output #1: loss = 0.302216 (* 1 = 0.302216 loss)
I1107 18:21:15.582795  5452 solver.cpp:218] Iteration 155500 (9.42742 iter/s, 10.6074s/100 iters), loss = 0.0286617
I1107 18:21:15.582795  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:21:15.582795  5452 solver.cpp:237]     Train net output #1: loss = 0.0286612 (* 1 = 0.0286612 loss)
I1107 18:21:15.582795  5452 sgd_solver.cpp:105] Iteration 155500, lr = 0.0001
I1107 18:21:24.081579  5452 solver.cpp:218] Iteration 155600 (11.7667 iter/s, 8.49856s/100 iters), loss = 0.0303721
I1107 18:21:24.081579  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:21:24.081579  5452 solver.cpp:237]     Train net output #1: loss = 0.0303716 (* 1 = 0.0303716 loss)
I1107 18:21:24.081579  5452 sgd_solver.cpp:105] Iteration 155600, lr = 0.0001
I1107 18:21:32.574549  5452 solver.cpp:218] Iteration 155700 (11.7752 iter/s, 8.49243s/100 iters), loss = 0.0234443
I1107 18:21:32.574549  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:21:32.574549  5452 solver.cpp:237]     Train net output #1: loss = 0.0234438 (* 1 = 0.0234438 loss)
I1107 18:21:32.574549  5452 sgd_solver.cpp:105] Iteration 155700, lr = 0.0001
I1107 18:21:41.075820  5452 solver.cpp:218] Iteration 155800 (11.7637 iter/s, 8.5007s/100 iters), loss = 0.0227718
I1107 18:21:41.075820  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:21:41.075820  5452 solver.cpp:237]     Train net output #1: loss = 0.0227714 (* 1 = 0.0227714 loss)
I1107 18:21:41.075820  5452 sgd_solver.cpp:105] Iteration 155800, lr = 0.0001
I1107 18:21:49.598312  5452 solver.cpp:218] Iteration 155900 (11.7346 iter/s, 8.5218s/100 iters), loss = 0.0278107
I1107 18:21:49.598312  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:21:49.598312  5452 solver.cpp:237]     Train net output #1: loss = 0.0278102 (* 1 = 0.0278102 loss)
I1107 18:21:49.598312  5452 sgd_solver.cpp:105] Iteration 155900, lr = 0.0001
I1107 18:21:57.690685 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:21:58.026742  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_156000.caffemodel
I1107 18:21:58.056743  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_156000.solverstate
I1107 18:21:58.064743  5452 solver.cpp:330] Iteration 156000, Testing net (#0)
I1107 18:21:58.065744  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:22:00.047940 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:22:00.126946  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9206
I1107 18:22:00.126946  5452 solver.cpp:397]     Test net output #1: loss = 0.302465 (* 1 = 0.302465 loss)
I1107 18:22:00.207949  5452 solver.cpp:218] Iteration 156000 (9.42573 iter/s, 10.6093s/100 iters), loss = 0.0251385
I1107 18:22:00.207949  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:22:00.207949  5452 solver.cpp:237]     Train net output #1: loss = 0.0251381 (* 1 = 0.0251381 loss)
I1107 18:22:00.207949  5452 sgd_solver.cpp:105] Iteration 156000, lr = 0.0001
I1107 18:22:08.716791  5452 solver.cpp:218] Iteration 156100 (11.7526 iter/s, 8.50876s/100 iters), loss = 0.0361075
I1107 18:22:08.716791  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:22:08.716791  5452 solver.cpp:237]     Train net output #1: loss = 0.036107 (* 1 = 0.036107 loss)
I1107 18:22:08.716791  5452 sgd_solver.cpp:105] Iteration 156100, lr = 0.0001
I1107 18:22:17.223568  5452 solver.cpp:218] Iteration 156200 (11.7569 iter/s, 8.50564s/100 iters), loss = 0.0225186
I1107 18:22:17.223568  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:22:17.223568  5452 solver.cpp:237]     Train net output #1: loss = 0.0225182 (* 1 = 0.0225182 loss)
I1107 18:22:17.223568  5452 sgd_solver.cpp:105] Iteration 156200, lr = 0.0001
I1107 18:22:25.739392  5452 solver.cpp:218] Iteration 156300 (11.7427 iter/s, 8.5159s/100 iters), loss = 0.0173142
I1107 18:22:25.739392  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:22:25.739392  5452 solver.cpp:237]     Train net output #1: loss = 0.0173137 (* 1 = 0.0173137 loss)
I1107 18:22:25.739392  5452 sgd_solver.cpp:105] Iteration 156300, lr = 0.0001
I1107 18:22:34.241101  5452 solver.cpp:218] Iteration 156400 (11.764 iter/s, 8.50053s/100 iters), loss = 0.0230339
I1107 18:22:34.241101  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:22:34.241101  5452 solver.cpp:237]     Train net output #1: loss = 0.0230334 (* 1 = 0.0230334 loss)
I1107 18:22:34.241101  5452 sgd_solver.cpp:105] Iteration 156400, lr = 0.0001
I1107 18:22:42.332917 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:22:42.669950  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_156500.caffemodel
I1107 18:22:42.700973  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_156500.solverstate
I1107 18:22:42.708966  5452 solver.cpp:330] Iteration 156500, Testing net (#0)
I1107 18:22:42.708966  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:22:44.690258 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:22:44.769317  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9204
I1107 18:22:44.769317  5452 solver.cpp:397]     Test net output #1: loss = 0.302365 (* 1 = 0.302365 loss)
I1107 18:22:44.850805  5452 solver.cpp:218] Iteration 156500 (9.42581 iter/s, 10.6092s/100 iters), loss = 0.0323584
I1107 18:22:44.850805  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:22:44.850805  5452 solver.cpp:237]     Train net output #1: loss = 0.0323579 (* 1 = 0.0323579 loss)
I1107 18:22:44.850805  5452 sgd_solver.cpp:105] Iteration 156500, lr = 0.0001
I1107 18:22:53.355113  5452 solver.cpp:218] Iteration 156600 (11.7592 iter/s, 8.50396s/100 iters), loss = 0.0275717
I1107 18:22:53.355113  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:22:53.355113  5452 solver.cpp:237]     Train net output #1: loss = 0.0275712 (* 1 = 0.0275712 loss)
I1107 18:22:53.355113  5452 sgd_solver.cpp:105] Iteration 156600, lr = 0.0001
I1107 18:23:01.857892  5452 solver.cpp:218] Iteration 156700 (11.7608 iter/s, 8.50285s/100 iters), loss = 0.0196777
I1107 18:23:01.857892  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:23:01.857892  5452 solver.cpp:237]     Train net output #1: loss = 0.0196772 (* 1 = 0.0196772 loss)
I1107 18:23:01.857892  5452 sgd_solver.cpp:105] Iteration 156700, lr = 0.0001
I1107 18:23:10.364605  5452 solver.cpp:218] Iteration 156800 (11.7568 iter/s, 8.5057s/100 iters), loss = 0.0185934
I1107 18:23:10.364605  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:23:10.364605  5452 solver.cpp:237]     Train net output #1: loss = 0.0185929 (* 1 = 0.0185929 loss)
I1107 18:23:10.364605  5452 sgd_solver.cpp:105] Iteration 156800, lr = 0.0001
I1107 18:23:18.866580  5452 solver.cpp:218] Iteration 156900 (11.7621 iter/s, 8.50191s/100 iters), loss = 0.0217641
I1107 18:23:18.866580  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:23:18.866580  5452 solver.cpp:237]     Train net output #1: loss = 0.0217637 (* 1 = 0.0217637 loss)
I1107 18:23:18.866580  5452 sgd_solver.cpp:105] Iteration 156900, lr = 0.0001
I1107 18:23:26.953269 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:23:27.289206  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_157000.caffemodel
I1107 18:23:27.321190  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_157000.solverstate
I1107 18:23:27.332191  5452 solver.cpp:330] Iteration 157000, Testing net (#0)
I1107 18:23:27.332191  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:23:29.315496 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:23:29.393534  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9195
I1107 18:23:29.393534  5452 solver.cpp:397]     Test net output #1: loss = 0.302523 (* 1 = 0.302523 loss)
I1107 18:23:29.474542  5452 solver.cpp:218] Iteration 157000 (9.42807 iter/s, 10.6066s/100 iters), loss = 0.0284234
I1107 18:23:29.474542  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:23:29.474542  5452 solver.cpp:237]     Train net output #1: loss = 0.028423 (* 1 = 0.028423 loss)
I1107 18:23:29.474542  5452 sgd_solver.cpp:105] Iteration 157000, lr = 0.0001
I1107 18:23:38.012514  5452 solver.cpp:218] Iteration 157100 (11.7127 iter/s, 8.53776s/100 iters), loss = 0.0298976
I1107 18:23:38.012514  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:23:38.012514  5452 solver.cpp:237]     Train net output #1: loss = 0.0298971 (* 1 = 0.0298971 loss)
I1107 18:23:38.012514  5452 sgd_solver.cpp:105] Iteration 157100, lr = 0.0001
I1107 18:23:46.592555  5452 solver.cpp:218] Iteration 157200 (11.6549 iter/s, 8.58009s/100 iters), loss = 0.0344128
I1107 18:23:46.592555  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:23:46.592555  5452 solver.cpp:237]     Train net output #1: loss = 0.0344123 (* 1 = 0.0344123 loss)
I1107 18:23:46.593555  5452 sgd_solver.cpp:105] Iteration 157200, lr = 0.0001
I1107 18:23:55.156770  5452 solver.cpp:218] Iteration 157300 (11.6784 iter/s, 8.56285s/100 iters), loss = 0.0198019
I1107 18:23:55.156770  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:23:55.156770  5452 solver.cpp:237]     Train net output #1: loss = 0.0198014 (* 1 = 0.0198014 loss)
I1107 18:23:55.156770  5452 sgd_solver.cpp:105] Iteration 157300, lr = 0.0001
I1107 18:24:03.726867  5452 solver.cpp:218] Iteration 157400 (11.669 iter/s, 8.5697s/100 iters), loss = 0.0377599
I1107 18:24:03.726867  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:24:03.726867  5452 solver.cpp:237]     Train net output #1: loss = 0.0377594 (* 1 = 0.0377594 loss)
I1107 18:24:03.726867  5452 sgd_solver.cpp:105] Iteration 157400, lr = 0.0001
I1107 18:24:11.829378 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:24:12.166396  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_157500.caffemodel
I1107 18:24:12.196902  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_157500.solverstate
I1107 18:24:12.205407  5452 solver.cpp:330] Iteration 157500, Testing net (#0)
I1107 18:24:12.205407  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:24:14.186697 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:24:14.265697  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9201
I1107 18:24:14.265697  5452 solver.cpp:397]     Test net output #1: loss = 0.302517 (* 1 = 0.302517 loss)
I1107 18:24:14.346701  5452 solver.cpp:218] Iteration 157500 (9.4167 iter/s, 10.6194s/100 iters), loss = 0.027429
I1107 18:24:14.346701  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:24:14.346701  5452 solver.cpp:237]     Train net output #1: loss = 0.0274286 (* 1 = 0.0274286 loss)
I1107 18:24:14.346701  5452 sgd_solver.cpp:105] Iteration 157500, lr = 0.0001
I1107 18:24:22.913612  5452 solver.cpp:218] Iteration 157600 (11.6731 iter/s, 8.56667s/100 iters), loss = 0.0206463
I1107 18:24:22.913612  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:24:22.913612  5452 solver.cpp:237]     Train net output #1: loss = 0.0206459 (* 1 = 0.0206459 loss)
I1107 18:24:22.913612  5452 sgd_solver.cpp:105] Iteration 157600, lr = 0.0001
I1107 18:24:31.459813  5452 solver.cpp:218] Iteration 157700 (11.7018 iter/s, 8.54569s/100 iters), loss = 0.0214689
I1107 18:24:31.459813  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:24:31.459813  5452 solver.cpp:237]     Train net output #1: loss = 0.0214685 (* 1 = 0.0214685 loss)
I1107 18:24:31.459813  5452 sgd_solver.cpp:105] Iteration 157700, lr = 0.0001
I1107 18:24:40.011210  5452 solver.cpp:218] Iteration 157800 (11.6956 iter/s, 8.55019s/100 iters), loss = 0.0180979
I1107 18:24:40.011210  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:24:40.011210  5452 solver.cpp:237]     Train net output #1: loss = 0.0180974 (* 1 = 0.0180974 loss)
I1107 18:24:40.011210  5452 sgd_solver.cpp:105] Iteration 157800, lr = 0.0001
I1107 18:24:48.609104  5452 solver.cpp:218] Iteration 157900 (11.6313 iter/s, 8.59752s/100 iters), loss = 0.0226051
I1107 18:24:48.609104  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:24:48.609104  5452 solver.cpp:237]     Train net output #1: loss = 0.0226047 (* 1 = 0.0226047 loss)
I1107 18:24:48.609104  5452 sgd_solver.cpp:105] Iteration 157900, lr = 0.0001
I1107 18:24:56.737395 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:24:57.083451  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_158000.caffemodel
I1107 18:24:57.115460  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_158000.solverstate
I1107 18:24:57.125460  5452 solver.cpp:330] Iteration 158000, Testing net (#0)
I1107 18:24:57.125460  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:24:59.129675 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:24:59.208679  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9198
I1107 18:24:59.208679  5452 solver.cpp:397]     Test net output #1: loss = 0.302485 (* 1 = 0.302485 loss)
I1107 18:24:59.295691  5452 solver.cpp:218] Iteration 158000 (9.35798 iter/s, 10.6861s/100 iters), loss = 0.0269517
I1107 18:24:59.295691  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:24:59.295691  5452 solver.cpp:237]     Train net output #1: loss = 0.0269513 (* 1 = 0.0269513 loss)
I1107 18:24:59.295691  5452 sgd_solver.cpp:105] Iteration 158000, lr = 0.0001
I1107 18:25:07.989019  5452 solver.cpp:218] Iteration 158100 (11.5039 iter/s, 8.69269s/100 iters), loss = 0.0307147
I1107 18:25:07.989019  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:25:07.989019  5452 solver.cpp:237]     Train net output #1: loss = 0.0307142 (* 1 = 0.0307142 loss)
I1107 18:25:07.989019  5452 sgd_solver.cpp:105] Iteration 158100, lr = 0.0001
I1107 18:25:16.505412  5452 solver.cpp:218] Iteration 158200 (11.7425 iter/s, 8.51608s/100 iters), loss = 0.0233769
I1107 18:25:16.505412  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:25:16.505412  5452 solver.cpp:237]     Train net output #1: loss = 0.0233764 (* 1 = 0.0233764 loss)
I1107 18:25:16.505412  5452 sgd_solver.cpp:105] Iteration 158200, lr = 0.0001
I1107 18:25:25.069414  5452 solver.cpp:218] Iteration 158300 (11.6773 iter/s, 8.56365s/100 iters), loss = 0.0322234
I1107 18:25:25.069414  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:25:25.069414  5452 solver.cpp:237]     Train net output #1: loss = 0.032223 (* 1 = 0.032223 loss)
I1107 18:25:25.069414  5452 sgd_solver.cpp:105] Iteration 158300, lr = 0.0001
I1107 18:25:33.731495  5452 solver.cpp:218] Iteration 158400 (11.5453 iter/s, 8.66156s/100 iters), loss = 0.0296779
I1107 18:25:33.731495  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:25:33.731495  5452 solver.cpp:237]     Train net output #1: loss = 0.0296775 (* 1 = 0.0296775 loss)
I1107 18:25:33.731495  5452 sgd_solver.cpp:105] Iteration 158400, lr = 0.0001
I1107 18:25:41.864416 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:25:42.215567  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_158500.caffemodel
I1107 18:25:42.250587  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_158500.solverstate
I1107 18:25:42.260571  5452 solver.cpp:330] Iteration 158500, Testing net (#0)
I1107 18:25:42.260571  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:25:44.296166 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:25:44.377670  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9201
I1107 18:25:44.378168  5452 solver.cpp:397]     Test net output #1: loss = 0.302554 (* 1 = 0.302554 loss)
I1107 18:25:44.460173  5452 solver.cpp:218] Iteration 158500 (9.3216 iter/s, 10.7278s/100 iters), loss = 0.0232087
I1107 18:25:44.460173  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:25:44.460173  5452 solver.cpp:237]     Train net output #1: loss = 0.0232082 (* 1 = 0.0232082 loss)
I1107 18:25:44.460173  5452 sgd_solver.cpp:105] Iteration 158500, lr = 0.0001
I1107 18:25:53.129946  5452 solver.cpp:218] Iteration 158600 (11.5347 iter/s, 8.66948s/100 iters), loss = 0.0413309
I1107 18:25:53.129946  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:25:53.129946  5452 solver.cpp:237]     Train net output #1: loss = 0.0413305 (* 1 = 0.0413305 loss)
I1107 18:25:53.129946  5452 sgd_solver.cpp:105] Iteration 158600, lr = 0.0001
I1107 18:26:01.850920  5452 solver.cpp:218] Iteration 158700 (11.4681 iter/s, 8.71984s/100 iters), loss = 0.0243592
I1107 18:26:01.850920  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:26:01.850920  5452 solver.cpp:237]     Train net output #1: loss = 0.0243588 (* 1 = 0.0243588 loss)
I1107 18:26:01.850920  5452 sgd_solver.cpp:105] Iteration 158700, lr = 0.0001
I1107 18:26:10.526021  5452 solver.cpp:218] Iteration 158800 (11.5269 iter/s, 8.67536s/100 iters), loss = 0.0163086
I1107 18:26:10.526021  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:26:10.526021  5452 solver.cpp:237]     Train net output #1: loss = 0.0163081 (* 1 = 0.0163081 loss)
I1107 18:26:10.526021  5452 sgd_solver.cpp:105] Iteration 158800, lr = 0.0001
I1107 18:26:19.102118  5452 solver.cpp:218] Iteration 158900 (11.6622 iter/s, 8.57473s/100 iters), loss = 0.0257256
I1107 18:26:19.102118  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:26:19.102118  5452 solver.cpp:237]     Train net output #1: loss = 0.0257252 (* 1 = 0.0257252 loss)
I1107 18:26:19.102118  5452 sgd_solver.cpp:105] Iteration 158900, lr = 0.0001
I1107 18:26:27.201865 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:26:27.538897  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_159000.caffemodel
I1107 18:26:27.568897  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_159000.solverstate
I1107 18:26:27.576897  5452 solver.cpp:330] Iteration 159000, Testing net (#0)
I1107 18:26:27.577898  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:26:29.569119 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:26:29.648144  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9205
I1107 18:26:29.648144  5452 solver.cpp:397]     Test net output #1: loss = 0.302664 (* 1 = 0.302664 loss)
I1107 18:26:29.729148  5452 solver.cpp:218] Iteration 159000 (9.40986 iter/s, 10.6271s/100 iters), loss = 0.0337199
I1107 18:26:29.729148  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:26:29.729148  5452 solver.cpp:237]     Train net output #1: loss = 0.0337195 (* 1 = 0.0337195 loss)
I1107 18:26:29.729148  5452 sgd_solver.cpp:105] Iteration 159000, lr = 0.0001
I1107 18:26:38.295361  5452 solver.cpp:218] Iteration 159100 (11.6742 iter/s, 8.56589s/100 iters), loss = 0.0276163
I1107 18:26:38.295361  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:26:38.295361  5452 solver.cpp:237]     Train net output #1: loss = 0.0276159 (* 1 = 0.0276159 loss)
I1107 18:26:38.295361  5452 sgd_solver.cpp:105] Iteration 159100, lr = 0.0001
I1107 18:26:46.928968  5452 solver.cpp:218] Iteration 159200 (11.5839 iter/s, 8.63271s/100 iters), loss = 0.0173364
I1107 18:26:46.929468  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:26:46.929468  5452 solver.cpp:237]     Train net output #1: loss = 0.017336 (* 1 = 0.017336 loss)
I1107 18:26:46.929468  5452 sgd_solver.cpp:105] Iteration 159200, lr = 0.0001
I1107 18:26:55.465754  5452 solver.cpp:218] Iteration 159300 (11.7146 iter/s, 8.53638s/100 iters), loss = 0.0209078
I1107 18:26:55.465754  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:26:55.465754  5452 solver.cpp:237]     Train net output #1: loss = 0.0209073 (* 1 = 0.0209073 loss)
I1107 18:26:55.465754  5452 sgd_solver.cpp:105] Iteration 159300, lr = 0.0001
I1107 18:27:04.131048  5452 solver.cpp:218] Iteration 159400 (11.5412 iter/s, 8.66458s/100 iters), loss = 0.0227042
I1107 18:27:04.131048  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:27:04.131048  5452 solver.cpp:237]     Train net output #1: loss = 0.0227038 (* 1 = 0.0227038 loss)
I1107 18:27:04.131048  5452 sgd_solver.cpp:105] Iteration 159400, lr = 0.0001
I1107 18:27:12.453359 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:27:12.795696  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_159500.caffemodel
I1107 18:27:12.828217  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_159500.solverstate
I1107 18:27:12.842739  5452 solver.cpp:330] Iteration 159500, Testing net (#0)
I1107 18:27:12.843734  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:27:14.850934 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:27:14.931437  5452 solver.cpp:397]     Test net output #0: accuracy = 0.92
I1107 18:27:14.931437  5452 solver.cpp:397]     Test net output #1: loss = 0.30284 (* 1 = 0.30284 loss)
I1107 18:27:15.013959  5452 solver.cpp:218] Iteration 159500 (9.18949 iter/s, 10.882s/100 iters), loss = 0.0225107
I1107 18:27:15.013959  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:27:15.013959  5452 solver.cpp:237]     Train net output #1: loss = 0.0225102 (* 1 = 0.0225102 loss)
I1107 18:27:15.013959  5452 sgd_solver.cpp:105] Iteration 159500, lr = 0.0001
I1107 18:27:23.649811  5452 solver.cpp:218] Iteration 159600 (11.5805 iter/s, 8.63519s/100 iters), loss = 0.0449895
I1107 18:27:23.649811  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:27:23.649811  5452 solver.cpp:237]     Train net output #1: loss = 0.0449891 (* 1 = 0.0449891 loss)
I1107 18:27:23.649811  5452 sgd_solver.cpp:105] Iteration 159600, lr = 0.0001
I1107 18:27:32.178431  5452 solver.cpp:218] Iteration 159700 (11.7251 iter/s, 8.52874s/100 iters), loss = 0.0238231
I1107 18:27:32.178431  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:27:32.178431  5452 solver.cpp:237]     Train net output #1: loss = 0.0238227 (* 1 = 0.0238227 loss)
I1107 18:27:32.178431  5452 sgd_solver.cpp:105] Iteration 159700, lr = 0.0001
I1107 18:27:40.719436  5452 solver.cpp:218] Iteration 159800 (11.7087 iter/s, 8.54068s/100 iters), loss = 0.0239811
I1107 18:27:40.719436  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:27:40.719436  5452 solver.cpp:237]     Train net output #1: loss = 0.0239807 (* 1 = 0.0239807 loss)
I1107 18:27:40.720424  5452 sgd_solver.cpp:105] Iteration 159800, lr = 0.0001
I1107 18:27:49.351068  5452 solver.cpp:218] Iteration 159900 (11.5866 iter/s, 8.63069s/100 iters), loss = 0.0254229
I1107 18:27:49.351068  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:27:49.351068  5452 solver.cpp:237]     Train net output #1: loss = 0.0254224 (* 1 = 0.0254224 loss)
I1107 18:27:49.351068  5452 sgd_solver.cpp:105] Iteration 159900, lr = 0.0001
I1107 18:27:57.532878 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:27:57.877923  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_160000.caffemodel
I1107 18:27:57.909924  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_160000.solverstate
I1107 18:27:57.918923  5452 solver.cpp:330] Iteration 160000, Testing net (#0)
I1107 18:27:57.918923  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:27:59.926208 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:28:00.007257  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9207
I1107 18:28:00.007257  5452 solver.cpp:397]     Test net output #1: loss = 0.302969 (* 1 = 0.302969 loss)
I1107 18:28:00.089320  5452 solver.cpp:218] Iteration 160000 (9.31338 iter/s, 10.7372s/100 iters), loss = 0.0347472
I1107 18:28:00.089320  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:28:00.089320  5452 solver.cpp:237]     Train net output #1: loss = 0.0347468 (* 1 = 0.0347468 loss)
I1107 18:28:00.089320  5452 sgd_solver.cpp:105] Iteration 160000, lr = 0.0001
I1107 18:28:08.638490  5452 solver.cpp:218] Iteration 160100 (11.6979 iter/s, 8.54852s/100 iters), loss = 0.0328446
I1107 18:28:08.638490  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:28:08.638490  5452 solver.cpp:237]     Train net output #1: loss = 0.0328442 (* 1 = 0.0328442 loss)
I1107 18:28:08.638490  5452 sgd_solver.cpp:105] Iteration 160100, lr = 0.0001
I1107 18:28:17.230692  5452 solver.cpp:218] Iteration 160200 (11.638 iter/s, 8.59254s/100 iters), loss = 0.0267448
I1107 18:28:17.230692  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:28:17.230692  5452 solver.cpp:237]     Train net output #1: loss = 0.0267444 (* 1 = 0.0267444 loss)
I1107 18:28:17.230692  5452 sgd_solver.cpp:105] Iteration 160200, lr = 0.0001
I1107 18:28:25.853442  5452 solver.cpp:218] Iteration 160300 (11.5989 iter/s, 8.62149s/100 iters), loss = 0.0238353
I1107 18:28:25.853442  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:28:25.853442  5452 solver.cpp:237]     Train net output #1: loss = 0.0238348 (* 1 = 0.0238348 loss)
I1107 18:28:25.853442  5452 sgd_solver.cpp:105] Iteration 160300, lr = 0.0001
I1107 18:28:34.433924  5452 solver.cpp:218] Iteration 160400 (11.6546 iter/s, 8.58028s/100 iters), loss = 0.0238574
I1107 18:28:34.433924  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:28:34.433924  5452 solver.cpp:237]     Train net output #1: loss = 0.0238569 (* 1 = 0.0238569 loss)
I1107 18:28:34.433924  5452 sgd_solver.cpp:105] Iteration 160400, lr = 0.0001
I1107 18:28:42.654999 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:28:42.992028  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_160500.caffemodel
I1107 18:28:43.022039  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_160500.solverstate
I1107 18:28:43.030036  5452 solver.cpp:330] Iteration 160500, Testing net (#0)
I1107 18:28:43.030036  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:28:45.023231 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:28:45.103235  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9206
I1107 18:28:45.103235  5452 solver.cpp:397]     Test net output #1: loss = 0.303752 (* 1 = 0.303752 loss)
I1107 18:28:45.184237  5452 solver.cpp:218] Iteration 160500 (9.30244 iter/s, 10.7499s/100 iters), loss = 0.0256823
I1107 18:28:45.184237  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:28:45.184237  5452 solver.cpp:237]     Train net output #1: loss = 0.0256819 (* 1 = 0.0256819 loss)
I1107 18:28:45.184237  5452 sgd_solver.cpp:105] Iteration 160500, lr = 0.0001
I1107 18:28:53.816118  5452 solver.cpp:218] Iteration 160600 (11.5858 iter/s, 8.63122s/100 iters), loss = 0.0363446
I1107 18:28:53.816118  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:28:53.816619  5452 solver.cpp:237]     Train net output #1: loss = 0.0363442 (* 1 = 0.0363442 loss)
I1107 18:28:53.816619  5452 sgd_solver.cpp:105] Iteration 160600, lr = 0.0001
I1107 18:29:02.363947  5452 solver.cpp:218] Iteration 160700 (11.6998 iter/s, 8.54717s/100 iters), loss = 0.0182935
I1107 18:29:02.363947  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:29:02.363947  5452 solver.cpp:237]     Train net output #1: loss = 0.0182931 (* 1 = 0.0182931 loss)
I1107 18:29:02.363947  5452 sgd_solver.cpp:105] Iteration 160700, lr = 0.0001
I1107 18:29:10.897701  5452 solver.cpp:218] Iteration 160800 (11.7185 iter/s, 8.53348s/100 iters), loss = 0.0174598
I1107 18:29:10.897701  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:29:10.897701  5452 solver.cpp:237]     Train net output #1: loss = 0.0174594 (* 1 = 0.0174594 loss)
I1107 18:29:10.897701  5452 sgd_solver.cpp:105] Iteration 160800, lr = 0.0001
I1107 18:29:19.431021  5452 solver.cpp:218] Iteration 160900 (11.7196 iter/s, 8.53272s/100 iters), loss = 0.0250128
I1107 18:29:19.431021  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:29:19.431021  5452 solver.cpp:237]     Train net output #1: loss = 0.0250124 (* 1 = 0.0250124 loss)
I1107 18:29:19.431021  5452 sgd_solver.cpp:105] Iteration 160900, lr = 0.0001
I1107 18:29:27.542728 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:29:27.881752  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_161000.caffemodel
I1107 18:29:27.912256  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_161000.solverstate
I1107 18:29:27.921756  5452 solver.cpp:330] Iteration 161000, Testing net (#0)
I1107 18:29:27.921756  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:29:29.916465 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:29:29.995965  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9206
I1107 18:29:29.995965  5452 solver.cpp:397]     Test net output #1: loss = 0.303388 (* 1 = 0.303388 loss)
I1107 18:29:30.077975  5452 solver.cpp:218] Iteration 161000 (9.3927 iter/s, 10.6466s/100 iters), loss = 0.0262668
I1107 18:29:30.077975  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:29:30.077975  5452 solver.cpp:237]     Train net output #1: loss = 0.0262663 (* 1 = 0.0262663 loss)
I1107 18:29:30.077975  5452 sgd_solver.cpp:105] Iteration 161000, lr = 0.0001
I1107 18:29:38.622433  5452 solver.cpp:218] Iteration 161100 (11.7047 iter/s, 8.54361s/100 iters), loss = 0.0349903
I1107 18:29:38.622433  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:29:38.622433  5452 solver.cpp:237]     Train net output #1: loss = 0.0349899 (* 1 = 0.0349899 loss)
I1107 18:29:38.622433  5452 sgd_solver.cpp:105] Iteration 161100, lr = 0.0001
I1107 18:29:47.173394  5452 solver.cpp:218] Iteration 161200 (11.6952 iter/s, 8.5505s/100 iters), loss = 0.0201981
I1107 18:29:47.173394  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:29:47.173394  5452 solver.cpp:237]     Train net output #1: loss = 0.0201977 (* 1 = 0.0201977 loss)
I1107 18:29:47.173394  5452 sgd_solver.cpp:105] Iteration 161200, lr = 0.0001
I1107 18:29:55.712280  5452 solver.cpp:218] Iteration 161300 (11.7124 iter/s, 8.53797s/100 iters), loss = 0.0351546
I1107 18:29:55.712280  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:29:55.712280  5452 solver.cpp:237]     Train net output #1: loss = 0.0351542 (* 1 = 0.0351542 loss)
I1107 18:29:55.712280  5452 sgd_solver.cpp:105] Iteration 161300, lr = 0.0001
I1107 18:30:04.272927  5452 solver.cpp:218] Iteration 161400 (11.6816 iter/s, 8.56045s/100 iters), loss = 0.0305833
I1107 18:30:04.272927  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:30:04.272927  5452 solver.cpp:237]     Train net output #1: loss = 0.0305829 (* 1 = 0.0305829 loss)
I1107 18:30:04.272927  5452 sgd_solver.cpp:105] Iteration 161400, lr = 0.0001
I1107 18:30:12.408591 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:30:12.746646  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_161500.caffemodel
I1107 18:30:12.776645  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_161500.solverstate
I1107 18:30:12.785645  5452 solver.cpp:330] Iteration 161500, Testing net (#0)
I1107 18:30:12.785645  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:30:14.779959 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:30:14.859962  5452 solver.cpp:397]     Test net output #0: accuracy = 0.921
I1107 18:30:14.859962  5452 solver.cpp:397]     Test net output #1: loss = 0.302678 (* 1 = 0.302678 loss)
I1107 18:30:14.941967  5452 solver.cpp:218] Iteration 161500 (9.37345 iter/s, 10.6684s/100 iters), loss = 0.0349226
I1107 18:30:14.941967  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:30:14.941967  5452 solver.cpp:237]     Train net output #1: loss = 0.0349222 (* 1 = 0.0349222 loss)
I1107 18:30:14.941967  5452 sgd_solver.cpp:105] Iteration 161500, lr = 0.0001
I1107 18:30:23.488735  5452 solver.cpp:218] Iteration 161600 (11.7002 iter/s, 8.54686s/100 iters), loss = 0.0437852
I1107 18:30:23.489737  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:30:23.489737  5452 solver.cpp:237]     Train net output #1: loss = 0.0437847 (* 1 = 0.0437847 loss)
I1107 18:30:23.489737  5452 sgd_solver.cpp:105] Iteration 161600, lr = 0.0001
I1107 18:30:32.037636  5452 solver.cpp:218] Iteration 161700 (11.6994 iter/s, 8.54742s/100 iters), loss = 0.0174038
I1107 18:30:32.037636  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:30:32.037636  5452 solver.cpp:237]     Train net output #1: loss = 0.0174033 (* 1 = 0.0174033 loss)
I1107 18:30:32.037636  5452 sgd_solver.cpp:105] Iteration 161700, lr = 0.0001
I1107 18:30:40.589696  5452 solver.cpp:218] Iteration 161800 (11.6927 iter/s, 8.55233s/100 iters), loss = 0.0201333
I1107 18:30:40.589696  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:30:40.589696  5452 solver.cpp:237]     Train net output #1: loss = 0.0201328 (* 1 = 0.0201328 loss)
I1107 18:30:40.589696  5452 sgd_solver.cpp:105] Iteration 161800, lr = 0.0001
I1107 18:30:49.139365  5452 solver.cpp:218] Iteration 161900 (11.698 iter/s, 8.54848s/100 iters), loss = 0.0240034
I1107 18:30:49.139365  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:30:49.139365  5452 solver.cpp:237]     Train net output #1: loss = 0.024003 (* 1 = 0.024003 loss)
I1107 18:30:49.139365  5452 sgd_solver.cpp:105] Iteration 161900, lr = 0.0001
I1107 18:30:57.252003 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:30:57.592015  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_162000.caffemodel
I1107 18:30:57.624017  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_162000.solverstate
I1107 18:30:57.633023  5452 solver.cpp:330] Iteration 162000, Testing net (#0)
I1107 18:30:57.633023  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:30:59.633767 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:30:59.713268  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9204
I1107 18:30:59.713268  5452 solver.cpp:397]     Test net output #1: loss = 0.302637 (* 1 = 0.302637 loss)
I1107 18:30:59.795274  5452 solver.cpp:218] Iteration 162000 (9.38496 iter/s, 10.6553s/100 iters), loss = 0.0365615
I1107 18:30:59.795274  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:30:59.795274  5452 solver.cpp:237]     Train net output #1: loss = 0.036561 (* 1 = 0.036561 loss)
I1107 18:30:59.795274  5452 sgd_solver.cpp:105] Iteration 162000, lr = 0.0001
I1107 18:31:08.353363  5452 solver.cpp:218] Iteration 162100 (11.6857 iter/s, 8.5575s/100 iters), loss = 0.0424431
I1107 18:31:08.353363  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:31:08.353363  5452 solver.cpp:237]     Train net output #1: loss = 0.0424427 (* 1 = 0.0424427 loss)
I1107 18:31:08.353363  5452 sgd_solver.cpp:105] Iteration 162100, lr = 0.0001
I1107 18:31:16.876406  5452 solver.cpp:218] Iteration 162200 (11.7326 iter/s, 8.52323s/100 iters), loss = 0.0187001
I1107 18:31:16.876406  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:31:16.876406  5452 solver.cpp:237]     Train net output #1: loss = 0.0186997 (* 1 = 0.0186997 loss)
I1107 18:31:16.876406  5452 sgd_solver.cpp:105] Iteration 162200, lr = 0.0001
I1107 18:31:25.377578  5452 solver.cpp:218] Iteration 162300 (11.7645 iter/s, 8.50012s/100 iters), loss = 0.0180165
I1107 18:31:25.377578  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:31:25.377578  5452 solver.cpp:237]     Train net output #1: loss = 0.0180161 (* 1 = 0.0180161 loss)
I1107 18:31:25.377578  5452 sgd_solver.cpp:105] Iteration 162300, lr = 0.0001
I1107 18:31:33.872388  5452 solver.cpp:218] Iteration 162400 (11.7727 iter/s, 8.49424s/100 iters), loss = 0.023519
I1107 18:31:33.872388  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:31:33.872388  5452 solver.cpp:237]     Train net output #1: loss = 0.0235186 (* 1 = 0.0235186 loss)
I1107 18:31:33.872388  5452 sgd_solver.cpp:105] Iteration 162400, lr = 0.0001
I1107 18:31:41.985539 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:31:42.328438  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_162500.caffemodel
I1107 18:31:42.360455  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_162500.solverstate
I1107 18:31:42.374459  5452 solver.cpp:330] Iteration 162500, Testing net (#0)
I1107 18:31:42.374459  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:31:44.373638 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:31:44.455655  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9205
I1107 18:31:44.455655  5452 solver.cpp:397]     Test net output #1: loss = 0.303499 (* 1 = 0.303499 loss)
I1107 18:31:44.538655  5452 solver.cpp:218] Iteration 162500 (9.37543 iter/s, 10.6662s/100 iters), loss = 0.0323719
I1107 18:31:44.538655  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:31:44.538655  5452 solver.cpp:237]     Train net output #1: loss = 0.0323714 (* 1 = 0.0323714 loss)
I1107 18:31:44.538655  5452 sgd_solver.cpp:105] Iteration 162500, lr = 0.0001
I1107 18:31:53.199564  5452 solver.cpp:218] Iteration 162600 (11.5477 iter/s, 8.65973s/100 iters), loss = 0.0264942
I1107 18:31:53.199564  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:31:53.199564  5452 solver.cpp:237]     Train net output #1: loss = 0.0264938 (* 1 = 0.0264938 loss)
I1107 18:31:53.199564  5452 sgd_solver.cpp:105] Iteration 162600, lr = 0.0001
I1107 18:32:01.883035  5452 solver.cpp:218] Iteration 162700 (11.5166 iter/s, 8.68309s/100 iters), loss = 0.0381557
I1107 18:32:01.883035  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:32:01.883035  5452 solver.cpp:237]     Train net output #1: loss = 0.0381552 (* 1 = 0.0381552 loss)
I1107 18:32:01.883035  5452 sgd_solver.cpp:105] Iteration 162700, lr = 0.0001
I1107 18:32:10.591799  5452 solver.cpp:218] Iteration 162800 (11.4837 iter/s, 8.70803s/100 iters), loss = 0.0216247
I1107 18:32:10.591799  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:32:10.591799  5452 solver.cpp:237]     Train net output #1: loss = 0.0216243 (* 1 = 0.0216243 loss)
I1107 18:32:10.591799  5452 sgd_solver.cpp:105] Iteration 162800, lr = 0.0001
I1107 18:32:19.294281  5452 solver.cpp:218] Iteration 162900 (11.4915 iter/s, 8.70205s/100 iters), loss = 0.0209772
I1107 18:32:19.294281  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:32:19.294281  5452 solver.cpp:237]     Train net output #1: loss = 0.0209768 (* 1 = 0.0209768 loss)
I1107 18:32:19.294281  5452 sgd_solver.cpp:105] Iteration 162900, lr = 0.0001
I1107 18:32:27.555826 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:32:27.899842  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_163000.caffemodel
I1107 18:32:27.930043  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_163000.solverstate
I1107 18:32:27.939052  5452 solver.cpp:330] Iteration 163000, Testing net (#0)
I1107 18:32:27.939052  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:32:29.956545 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:32:30.036558  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9203
I1107 18:32:30.037062  5452 solver.cpp:397]     Test net output #1: loss = 0.303075 (* 1 = 0.303075 loss)
I1107 18:32:30.119554  5452 solver.cpp:218] Iteration 163000 (9.23806 iter/s, 10.8248s/100 iters), loss = 0.0277869
I1107 18:32:30.119554  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:32:30.119554  5452 solver.cpp:237]     Train net output #1: loss = 0.0277865 (* 1 = 0.0277865 loss)
I1107 18:32:30.119554  5452 sgd_solver.cpp:105] Iteration 163000, lr = 0.0001
I1107 18:32:38.807204  5452 solver.cpp:218] Iteration 163100 (11.5114 iter/s, 8.68705s/100 iters), loss = 0.0300731
I1107 18:32:38.807204  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:32:38.807204  5452 solver.cpp:237]     Train net output #1: loss = 0.0300727 (* 1 = 0.0300727 loss)
I1107 18:32:38.807204  5452 sgd_solver.cpp:105] Iteration 163100, lr = 0.0001
I1107 18:32:47.499373  5452 solver.cpp:218] Iteration 163200 (11.5055 iter/s, 8.69153s/100 iters), loss = 0.0223346
I1107 18:32:47.499373  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:32:47.499373  5452 solver.cpp:237]     Train net output #1: loss = 0.0223342 (* 1 = 0.0223342 loss)
I1107 18:32:47.499373  5452 sgd_solver.cpp:105] Iteration 163200, lr = 0.0001
I1107 18:32:56.193678  5452 solver.cpp:218] Iteration 163300 (11.5023 iter/s, 8.69393s/100 iters), loss = 0.019328
I1107 18:32:56.194187  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:32:56.194187  5452 solver.cpp:237]     Train net output #1: loss = 0.0193275 (* 1 = 0.0193275 loss)
I1107 18:32:56.194187  5452 sgd_solver.cpp:105] Iteration 163300, lr = 0.0001
I1107 18:33:04.876299  5452 solver.cpp:218] Iteration 163400 (11.5181 iter/s, 8.68199s/100 iters), loss = 0.0264991
I1107 18:33:04.876299  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:33:04.876299  5452 solver.cpp:237]     Train net output #1: loss = 0.0264987 (* 1 = 0.0264987 loss)
I1107 18:33:04.876299  5452 sgd_solver.cpp:105] Iteration 163400, lr = 0.0001
I1107 18:33:13.132599 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:33:13.478147  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_163500.caffemodel
I1107 18:33:13.506145  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_163500.solverstate
I1107 18:33:13.516145  5452 solver.cpp:330] Iteration 163500, Testing net (#0)
I1107 18:33:13.516145  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:33:15.532646 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:33:15.614147  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9201
I1107 18:33:15.614647  5452 solver.cpp:397]     Test net output #1: loss = 0.302344 (* 1 = 0.302344 loss)
I1107 18:33:15.697660  5452 solver.cpp:218] Iteration 163500 (9.24153 iter/s, 10.8207s/100 iters), loss = 0.0393758
I1107 18:33:15.697660  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:33:15.697660  5452 solver.cpp:237]     Train net output #1: loss = 0.0393754 (* 1 = 0.0393754 loss)
I1107 18:33:15.697660  5452 sgd_solver.cpp:105] Iteration 163500, lr = 0.0001
I1107 18:33:24.377256  5452 solver.cpp:218] Iteration 163600 (11.5222 iter/s, 8.67888s/100 iters), loss = 0.029351
I1107 18:33:24.377256  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:33:24.377256  5452 solver.cpp:237]     Train net output #1: loss = 0.0293506 (* 1 = 0.0293506 loss)
I1107 18:33:24.377256  5452 sgd_solver.cpp:105] Iteration 163600, lr = 0.0001
I1107 18:33:33.073787  5452 solver.cpp:218] Iteration 163700 (11.4996 iter/s, 8.69596s/100 iters), loss = 0.0260259
I1107 18:33:33.073787  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:33:33.073787  5452 solver.cpp:237]     Train net output #1: loss = 0.0260254 (* 1 = 0.0260254 loss)
I1107 18:33:33.073787  5452 sgd_solver.cpp:105] Iteration 163700, lr = 0.0001
I1107 18:33:41.754475  5452 solver.cpp:218] Iteration 163800 (11.5206 iter/s, 8.68007s/100 iters), loss = 0.0186349
I1107 18:33:41.754475  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:33:41.754475  5452 solver.cpp:237]     Train net output #1: loss = 0.0186344 (* 1 = 0.0186344 loss)
I1107 18:33:41.754475  5452 sgd_solver.cpp:105] Iteration 163800, lr = 0.0001
I1107 18:33:50.439913  5452 solver.cpp:218] Iteration 163900 (11.5141 iter/s, 8.68498s/100 iters), loss = 0.0251761
I1107 18:33:50.439913  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:33:50.439913  5452 solver.cpp:237]     Train net output #1: loss = 0.0251757 (* 1 = 0.0251757 loss)
I1107 18:33:50.439913  5452 sgd_solver.cpp:105] Iteration 163900, lr = 0.0001
I1107 18:33:58.695673 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:33:59.038170  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_164000.caffemodel
I1107 18:33:59.072669  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_164000.solverstate
I1107 18:33:59.081670  5452 solver.cpp:330] Iteration 164000, Testing net (#0)
I1107 18:33:59.081670  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:34:01.098184 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:34:01.178673  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9196
I1107 18:34:01.178673  5452 solver.cpp:397]     Test net output #1: loss = 0.30334 (* 1 = 0.30334 loss)
I1107 18:34:01.261685  5452 solver.cpp:218] Iteration 164000 (9.24106 iter/s, 10.8213s/100 iters), loss = 0.0295061
I1107 18:34:01.262171  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:34:01.262171  5452 solver.cpp:237]     Train net output #1: loss = 0.0295057 (* 1 = 0.0295057 loss)
I1107 18:34:01.262171  5452 sgd_solver.cpp:105] Iteration 164000, lr = 0.0001
I1107 18:34:09.949767  5452 solver.cpp:218] Iteration 164100 (11.5111 iter/s, 8.68729s/100 iters), loss = 0.0289146
I1107 18:34:09.949767  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:34:09.949767  5452 solver.cpp:237]     Train net output #1: loss = 0.0289142 (* 1 = 0.0289142 loss)
I1107 18:34:09.949767  5452 sgd_solver.cpp:105] Iteration 164100, lr = 0.0001
I1107 18:34:18.630879  5452 solver.cpp:218] Iteration 164200 (11.5203 iter/s, 8.68032s/100 iters), loss = 0.0220621
I1107 18:34:18.630879  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:34:18.630879  5452 solver.cpp:237]     Train net output #1: loss = 0.0220617 (* 1 = 0.0220617 loss)
I1107 18:34:18.630879  5452 sgd_solver.cpp:105] Iteration 164200, lr = 0.0001
I1107 18:34:27.323624  5452 solver.cpp:218] Iteration 164300 (11.5041 iter/s, 8.69257s/100 iters), loss = 0.0222431
I1107 18:34:27.323624  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:34:27.323624  5452 solver.cpp:237]     Train net output #1: loss = 0.0222427 (* 1 = 0.0222427 loss)
I1107 18:34:27.324123  5452 sgd_solver.cpp:105] Iteration 164300, lr = 0.0001
I1107 18:34:36.039127  5452 solver.cpp:218] Iteration 164400 (11.4747 iter/s, 8.71484s/100 iters), loss = 0.0214491
I1107 18:34:36.039127  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:34:36.039127  5452 solver.cpp:237]     Train net output #1: loss = 0.0214487 (* 1 = 0.0214487 loss)
I1107 18:34:36.039127  5452 sgd_solver.cpp:105] Iteration 164400, lr = 0.0001
I1107 18:34:44.301556 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:34:44.647055  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_164500.caffemodel
I1107 18:34:44.675556  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_164500.solverstate
I1107 18:34:44.684072  5452 solver.cpp:330] Iteration 164500, Testing net (#0)
I1107 18:34:44.684557  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:34:46.697055 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:34:46.777557  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9195
I1107 18:34:46.777557  5452 solver.cpp:397]     Test net output #1: loss = 0.303686 (* 1 = 0.303686 loss)
I1107 18:34:46.861055  5452 solver.cpp:218] Iteration 164500 (9.24104 iter/s, 10.8213s/100 iters), loss = 0.0270704
I1107 18:34:46.861055  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:34:46.861055  5452 solver.cpp:237]     Train net output #1: loss = 0.02707 (* 1 = 0.02707 loss)
I1107 18:34:46.861055  5452 sgd_solver.cpp:105] Iteration 164500, lr = 0.0001
I1107 18:34:55.552542  5452 solver.cpp:218] Iteration 164600 (11.5065 iter/s, 8.69071s/100 iters), loss = 0.0217333
I1107 18:34:55.552542  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:34:55.552542  5452 solver.cpp:237]     Train net output #1: loss = 0.0217329 (* 1 = 0.0217329 loss)
I1107 18:34:55.552542  5452 sgd_solver.cpp:105] Iteration 164600, lr = 0.0001
I1107 18:35:04.250154  5452 solver.cpp:218] Iteration 164700 (11.4979 iter/s, 8.69723s/100 iters), loss = 0.0322686
I1107 18:35:04.250655  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:35:04.250655  5452 solver.cpp:237]     Train net output #1: loss = 0.0322681 (* 1 = 0.0322681 loss)
I1107 18:35:04.250655  5452 sgd_solver.cpp:105] Iteration 164700, lr = 0.0001
I1107 18:35:12.940306  5452 solver.cpp:218] Iteration 164800 (11.5081 iter/s, 8.68952s/100 iters), loss = 0.0197198
I1107 18:35:12.940306  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:35:12.940806  5452 solver.cpp:237]     Train net output #1: loss = 0.0197194 (* 1 = 0.0197194 loss)
I1107 18:35:12.940806  5452 sgd_solver.cpp:105] Iteration 164800, lr = 0.0001
I1107 18:35:21.634915  5452 solver.cpp:218] Iteration 164900 (11.5024 iter/s, 8.69387s/100 iters), loss = 0.0240828
I1107 18:35:21.634915  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:35:21.634915  5452 solver.cpp:237]     Train net output #1: loss = 0.0240823 (* 1 = 0.0240823 loss)
I1107 18:35:21.634915  5452 sgd_solver.cpp:105] Iteration 164900, lr = 0.0001
I1107 18:35:29.899396 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:35:30.242894  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_165000.caffemodel
I1107 18:35:30.271916  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_165000.solverstate
I1107 18:35:30.280913  5452 solver.cpp:330] Iteration 165000, Testing net (#0)
I1107 18:35:30.280913  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:35:32.300894 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:35:32.382395  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1107 18:35:32.382395  5452 solver.cpp:397]     Test net output #1: loss = 0.303211 (* 1 = 0.303211 loss)
I1107 18:35:32.465446  5452 solver.cpp:218] Iteration 165000 (9.23375 iter/s, 10.8298s/100 iters), loss = 0.0279014
I1107 18:35:32.465446  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:35:32.465446  5452 solver.cpp:237]     Train net output #1: loss = 0.027901 (* 1 = 0.027901 loss)
I1107 18:35:32.465446  5452 sgd_solver.cpp:105] Iteration 165000, lr = 0.0001
I1107 18:35:41.162276  5452 solver.cpp:218] Iteration 165100 (11.4993 iter/s, 8.69618s/100 iters), loss = 0.0230938
I1107 18:35:41.162276  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:35:41.162276  5452 solver.cpp:237]     Train net output #1: loss = 0.0230933 (* 1 = 0.0230933 loss)
I1107 18:35:41.162276  5452 sgd_solver.cpp:105] Iteration 165100, lr = 0.0001
I1107 18:35:49.852737  5452 solver.cpp:218] Iteration 165200 (11.5074 iter/s, 8.69002s/100 iters), loss = 0.0201483
I1107 18:35:49.852737  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:35:49.852737  5452 solver.cpp:237]     Train net output #1: loss = 0.0201479 (* 1 = 0.0201479 loss)
I1107 18:35:49.852737  5452 sgd_solver.cpp:105] Iteration 165200, lr = 0.0001
I1107 18:35:58.537606  5452 solver.cpp:218] Iteration 165300 (11.5151 iter/s, 8.68427s/100 iters), loss = 0.0304598
I1107 18:35:58.537606  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:35:58.537606  5452 solver.cpp:237]     Train net output #1: loss = 0.0304594 (* 1 = 0.0304594 loss)
I1107 18:35:58.537606  5452 sgd_solver.cpp:105] Iteration 165300, lr = 0.0001
I1107 18:36:07.232481  5452 solver.cpp:218] Iteration 165400 (11.5019 iter/s, 8.6942s/100 iters), loss = 0.0233181
I1107 18:36:07.232481  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:36:07.232481  5452 solver.cpp:237]     Train net output #1: loss = 0.0233177 (* 1 = 0.0233177 loss)
I1107 18:36:07.232481  5452 sgd_solver.cpp:105] Iteration 165400, lr = 0.0001
I1107 18:36:15.501590 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:36:15.842592  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_165500.caffemodel
I1107 18:36:15.877593  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_165500.solverstate
I1107 18:36:15.887091  5452 solver.cpp:330] Iteration 165500, Testing net (#0)
I1107 18:36:15.887091  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:36:17.904091 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:36:17.984606  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9202
I1107 18:36:17.984606  5452 solver.cpp:397]     Test net output #1: loss = 0.303571 (* 1 = 0.303571 loss)
I1107 18:36:18.067106  5452 solver.cpp:218] Iteration 165500 (9.22991 iter/s, 10.8343s/100 iters), loss = 0.0244794
I1107 18:36:18.067106  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:36:18.067106  5452 solver.cpp:237]     Train net output #1: loss = 0.024479 (* 1 = 0.024479 loss)
I1107 18:36:18.067106  5452 sgd_solver.cpp:105] Iteration 165500, lr = 0.0001
I1107 18:36:26.742372  5452 solver.cpp:218] Iteration 165600 (11.5278 iter/s, 8.67465s/100 iters), loss = 0.024166
I1107 18:36:26.742372  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:36:26.742372  5452 solver.cpp:237]     Train net output #1: loss = 0.0241655 (* 1 = 0.0241655 loss)
I1107 18:36:26.742372  5452 sgd_solver.cpp:105] Iteration 165600, lr = 0.0001
I1107 18:36:35.424031  5452 solver.cpp:218] Iteration 165700 (11.5197 iter/s, 8.68078s/100 iters), loss = 0.018405
I1107 18:36:35.424031  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:36:35.424031  5452 solver.cpp:237]     Train net output #1: loss = 0.0184045 (* 1 = 0.0184045 loss)
I1107 18:36:35.424031  5452 sgd_solver.cpp:105] Iteration 165700, lr = 0.0001
I1107 18:36:44.119262  5452 solver.cpp:218] Iteration 165800 (11.5014 iter/s, 8.69462s/100 iters), loss = 0.0216087
I1107 18:36:44.119262  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:36:44.119262  5452 solver.cpp:237]     Train net output #1: loss = 0.0216083 (* 1 = 0.0216083 loss)
I1107 18:36:44.119262  5452 sgd_solver.cpp:105] Iteration 165800, lr = 0.0001
I1107 18:36:52.810736  5452 solver.cpp:218] Iteration 165900 (11.5057 iter/s, 8.69136s/100 iters), loss = 0.0216306
I1107 18:36:52.810736  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:36:52.810736  5452 solver.cpp:237]     Train net output #1: loss = 0.0216302 (* 1 = 0.0216302 loss)
I1107 18:36:52.810736  5452 sgd_solver.cpp:105] Iteration 165900, lr = 0.0001
I1107 18:37:01.059375 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:37:01.404374  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_166000.caffemodel
I1107 18:37:01.435372  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_166000.solverstate
I1107 18:37:01.444373  5452 solver.cpp:330] Iteration 166000, Testing net (#0)
I1107 18:37:01.444373  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:37:03.462374 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:37:03.543373  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 18:37:03.543874  5452 solver.cpp:397]     Test net output #1: loss = 0.304164 (* 1 = 0.304164 loss)
I1107 18:37:03.626873  5452 solver.cpp:218] Iteration 166000 (9.24598 iter/s, 10.8155s/100 iters), loss = 0.0349267
I1107 18:37:03.626873  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:37:03.626873  5452 solver.cpp:237]     Train net output #1: loss = 0.0349262 (* 1 = 0.0349262 loss)
I1107 18:37:03.626873  5452 sgd_solver.cpp:105] Iteration 166000, lr = 0.0001
I1107 18:37:12.321375  5452 solver.cpp:218] Iteration 166100 (11.5022 iter/s, 8.69396s/100 iters), loss = 0.0213129
I1107 18:37:12.321876  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:37:12.321876  5452 solver.cpp:237]     Train net output #1: loss = 0.0213125 (* 1 = 0.0213125 loss)
I1107 18:37:12.321876  5452 sgd_solver.cpp:105] Iteration 166100, lr = 0.0001
I1107 18:37:21.015887  5452 solver.cpp:218] Iteration 166200 (11.5026 iter/s, 8.69368s/100 iters), loss = 0.0296461
I1107 18:37:21.015887  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:37:21.015887  5452 solver.cpp:237]     Train net output #1: loss = 0.0296457 (* 1 = 0.0296457 loss)
I1107 18:37:21.015887  5452 sgd_solver.cpp:105] Iteration 166200, lr = 0.0001
I1107 18:37:29.700078  5452 solver.cpp:218] Iteration 166300 (11.5163 iter/s, 8.68333s/100 iters), loss = 0.020954
I1107 18:37:29.700078  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:37:29.700078  5452 solver.cpp:237]     Train net output #1: loss = 0.0209536 (* 1 = 0.0209536 loss)
I1107 18:37:29.700078  5452 sgd_solver.cpp:105] Iteration 166300, lr = 0.0001
I1107 18:37:38.389945  5452 solver.cpp:218] Iteration 166400 (11.5079 iter/s, 8.68966s/100 iters), loss = 0.0226842
I1107 18:37:38.390432  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:37:38.390432  5452 solver.cpp:237]     Train net output #1: loss = 0.0226838 (* 1 = 0.0226838 loss)
I1107 18:37:38.390432  5452 sgd_solver.cpp:105] Iteration 166400, lr = 0.0001
I1107 18:37:46.662022 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:37:47.005520  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_166500.caffemodel
I1107 18:37:47.036530  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_166500.solverstate
I1107 18:37:47.045522  5452 solver.cpp:330] Iteration 166500, Testing net (#0)
I1107 18:37:47.045522  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:37:49.056020 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:37:49.136519  5452 solver.cpp:397]     Test net output #0: accuracy = 0.92
I1107 18:37:49.136519  5452 solver.cpp:397]     Test net output #1: loss = 0.304158 (* 1 = 0.304158 loss)
I1107 18:37:49.220034  5452 solver.cpp:218] Iteration 166500 (9.2342 iter/s, 10.8293s/100 iters), loss = 0.0310539
I1107 18:37:49.220034  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:37:49.220034  5452 solver.cpp:237]     Train net output #1: loss = 0.0310534 (* 1 = 0.0310534 loss)
I1107 18:37:49.220034  5452 sgd_solver.cpp:105] Iteration 166500, lr = 0.0001
I1107 18:37:57.899796  5452 solver.cpp:218] Iteration 166600 (11.5219 iter/s, 8.6791s/100 iters), loss = 0.0222588
I1107 18:37:57.899796  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:37:57.899796  5452 solver.cpp:237]     Train net output #1: loss = 0.0222584 (* 1 = 0.0222584 loss)
I1107 18:37:57.899796  5452 sgd_solver.cpp:105] Iteration 166600, lr = 0.0001
I1107 18:38:06.589823  5452 solver.cpp:218] Iteration 166700 (11.5081 iter/s, 8.6895s/100 iters), loss = 0.0216612
I1107 18:38:06.589823  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:38:06.589823  5452 solver.cpp:237]     Train net output #1: loss = 0.0216608 (* 1 = 0.0216608 loss)
I1107 18:38:06.589823  5452 sgd_solver.cpp:105] Iteration 166700, lr = 0.0001
I1107 18:38:15.286223  5452 solver.cpp:218] Iteration 166800 (11.4997 iter/s, 8.69587s/100 iters), loss = 0.0151618
I1107 18:38:15.286223  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:38:15.286223  5452 solver.cpp:237]     Train net output #1: loss = 0.0151613 (* 1 = 0.0151613 loss)
I1107 18:38:15.286223  5452 sgd_solver.cpp:105] Iteration 166800, lr = 0.0001
I1107 18:38:23.977835  5452 solver.cpp:218] Iteration 166900 (11.506 iter/s, 8.69114s/100 iters), loss = 0.0257586
I1107 18:38:23.977835  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:38:23.977835  5452 solver.cpp:237]     Train net output #1: loss = 0.0257582 (* 1 = 0.0257582 loss)
I1107 18:38:23.977835  5452 sgd_solver.cpp:105] Iteration 166900, lr = 0.0001
I1107 18:38:32.248447 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:38:32.595947  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_167000.caffemodel
I1107 18:38:32.633946  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_167000.solverstate
I1107 18:38:32.642952  5452 solver.cpp:330] Iteration 167000, Testing net (#0)
I1107 18:38:32.642952  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:38:34.664430 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:38:34.744451  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9202
I1107 18:38:34.744451  5452 solver.cpp:397]     Test net output #1: loss = 0.303722 (* 1 = 0.303722 loss)
I1107 18:38:34.826930  5452 solver.cpp:218] Iteration 167000 (9.21786 iter/s, 10.8485s/100 iters), loss = 0.0263114
I1107 18:38:34.826930  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:38:34.826930  5452 solver.cpp:237]     Train net output #1: loss = 0.026311 (* 1 = 0.026311 loss)
I1107 18:38:34.826930  5452 sgd_solver.cpp:105] Iteration 167000, lr = 0.0001
I1107 18:38:43.534188  5452 solver.cpp:218] Iteration 167100 (11.4855 iter/s, 8.70664s/100 iters), loss = 0.0243394
I1107 18:38:43.534188  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:38:43.534188  5452 solver.cpp:237]     Train net output #1: loss = 0.024339 (* 1 = 0.024339 loss)
I1107 18:38:43.534188  5452 sgd_solver.cpp:105] Iteration 167100, lr = 0.0001
I1107 18:38:52.255777  5452 solver.cpp:218] Iteration 167200 (11.4665 iter/s, 8.72106s/100 iters), loss = 0.0192402
I1107 18:38:52.255777  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:38:52.255777  5452 solver.cpp:237]     Train net output #1: loss = 0.0192398 (* 1 = 0.0192398 loss)
I1107 18:38:52.255777  5452 sgd_solver.cpp:105] Iteration 167200, lr = 0.0001
I1107 18:39:00.959981  5452 solver.cpp:218] Iteration 167300 (11.4896 iter/s, 8.70349s/100 iters), loss = 0.0169057
I1107 18:39:00.959981  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:39:00.959981  5452 solver.cpp:237]     Train net output #1: loss = 0.0169053 (* 1 = 0.0169053 loss)
I1107 18:39:00.959981  5452 sgd_solver.cpp:105] Iteration 167300, lr = 0.0001
I1107 18:39:09.640975  5452 solver.cpp:218] Iteration 167400 (11.5204 iter/s, 8.68027s/100 iters), loss = 0.0278888
I1107 18:39:09.640975  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:39:09.640975  5452 solver.cpp:237]     Train net output #1: loss = 0.0278884 (* 1 = 0.0278884 loss)
I1107 18:39:09.640975  5452 sgd_solver.cpp:105] Iteration 167400, lr = 0.0001
I1107 18:39:17.904784 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:39:18.249296  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_167500.caffemodel
I1107 18:39:18.278796  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_167500.solverstate
I1107 18:39:18.287811  5452 solver.cpp:330] Iteration 167500, Testing net (#0)
I1107 18:39:18.288311  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:39:20.307811 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:39:20.388799  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1107 18:39:20.388799  5452 solver.cpp:397]     Test net output #1: loss = 0.303249 (* 1 = 0.303249 loss)
I1107 18:39:20.470798  5452 solver.cpp:218] Iteration 167500 (9.23405 iter/s, 10.8295s/100 iters), loss = 0.0266497
I1107 18:39:20.470798  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:39:20.471299  5452 solver.cpp:237]     Train net output #1: loss = 0.0266493 (* 1 = 0.0266493 loss)
I1107 18:39:20.471299  5452 sgd_solver.cpp:105] Iteration 167500, lr = 0.0001
I1107 18:39:29.155246  5452 solver.cpp:218] Iteration 167600 (11.516 iter/s, 8.68357s/100 iters), loss = 0.0246908
I1107 18:39:29.155246  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:39:29.155246  5452 solver.cpp:237]     Train net output #1: loss = 0.0246904 (* 1 = 0.0246904 loss)
I1107 18:39:29.155246  5452 sgd_solver.cpp:105] Iteration 167600, lr = 0.0001
I1107 18:39:37.843533  5452 solver.cpp:218] Iteration 167700 (11.5104 iter/s, 8.68781s/100 iters), loss = 0.0197091
I1107 18:39:37.844019  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:39:37.844019  5452 solver.cpp:237]     Train net output #1: loss = 0.0197087 (* 1 = 0.0197087 loss)
I1107 18:39:37.844019  5452 sgd_solver.cpp:105] Iteration 167700, lr = 0.0001
I1107 18:39:46.533162  5452 solver.cpp:218] Iteration 167800 (11.5089 iter/s, 8.68896s/100 iters), loss = 0.0214015
I1107 18:39:46.533162  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:39:46.533162  5452 solver.cpp:237]     Train net output #1: loss = 0.0214011 (* 1 = 0.0214011 loss)
I1107 18:39:46.533162  5452 sgd_solver.cpp:105] Iteration 167800, lr = 0.0001
I1107 18:39:55.222287  5452 solver.cpp:218] Iteration 167900 (11.5098 iter/s, 8.68825s/100 iters), loss = 0.0212553
I1107 18:39:55.222287  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:39:55.222287  5452 solver.cpp:237]     Train net output #1: loss = 0.0212548 (* 1 = 0.0212548 loss)
I1107 18:39:55.222287  5452 sgd_solver.cpp:105] Iteration 167900, lr = 0.0001
I1107 18:40:03.483340 18344 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:40:03.826855  5452 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_168000.caffemodel
I1107 18:40:03.856351  5452 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/squeezenet_batchnorm_iter_168000.solverstate
I1107 18:40:03.865840  5452 solver.cpp:330] Iteration 168000, Testing net (#0)
I1107 18:40:03.865840  5452 net.cpp:676] Ignoring source layer accuracy_training
I1107 18:40:05.885390 18404 data_layer.cpp:73] Restarting data prefetching from start.
I1107 18:40:05.965899  5452 solver.cpp:397]     Test net output #0: accuracy = 0.9202
I1107 18:40:05.966400  5452 solver.cpp:397]     Test net output #1: loss = 0.303393 (* 1 = 0.303393 loss)
I1107 18:40:06.048399  5452 solver.cpp:218] Iteration 168000 (9.23713 iter/s, 10.8259s/100 iters), loss = 0.0420753
I1107 18:40:06.048399  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:40:06.048399  5452 solver.cpp:237]     Train net output #1: loss = 0.0420748 (* 1 = 0.0420748 loss)
I1107 18:40:06.048399  5452 sgd_solver.cpp:105] Iteration 168000, lr = 0.0001
I1107 18:40:14.736665  5452 solver.cpp:218] Iteration 168100 (11.5107 iter/s, 8.6876s/100 iters), loss = 0.0358483
I1107 18:40:14.736665  5452 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1107 18:40:14.736665  5452 solver.cpp:237]     Train net output #1: loss = 0.0358479 (* 1 = 0.0358479 loss)
I1107 18:40:14.736665  5452 sgd_solver.cpp:105] Iteration 168100, lr = 0.0001
I1107 18:40:23.424484  5452 solver.cpp:218] Iteration 168200 (11.5115 iter/s, 8.68695s/100 iters), loss = 0.0269363
I1107 18:40:23.424484  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:40:23.424484  5452 solver.cpp:237]     Train net output #1: loss = 0.0269359 (* 1 = 0.0269359 loss)
I1107 18:40:23.424484  5452 sgd_solver.cpp:105] Iteration 168200, lr = 0.0001
I1107 18:40:32.116065  5452 solver.cpp:218] Iteration 168300 (11.5062 iter/s, 8.69096s/100 iters), loss = 0.0159941
I1107 18:40:32.116065  5452 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1107 18:40:32.116065  5452 solver.cpp:237]     Train net output #1: loss = 0.0159937 (* 1 = 0.0159937 loss)
I1107 18:40:32.116065  5452 sgd_solver.cpp:105] Iteration 16
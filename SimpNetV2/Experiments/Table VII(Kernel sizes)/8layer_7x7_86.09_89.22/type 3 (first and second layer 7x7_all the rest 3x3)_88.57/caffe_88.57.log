
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1123 17:58:59.627542 16760 caffe.cpp:219] Using GPUs 0
I1123 17:58:59.788054 16760 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1123 17:59:00.084434 16760 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 17:59:00.100936 16760 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1123 17:59:00.101938 16760 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 17:59:00.101938 16760 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 17:59:00.101938 16760 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 17:59:00.101938 16760 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1123 17:59:00.101938 16760 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1123 17:59:00.101938 16760 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1123 17:59:00.101938 16760 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1123 17:59:00.101938 16760 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1123 17:59:00.101938 16760 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1123 17:59:00.101938 16760 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1123 17:59:00.101938 16760 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1123 17:59:00.101938 16760 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1123 17:59:00.101938 16760 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1123 17:59:00.102938 16760 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k_7x7_first2layers"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 35
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 38
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 74
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 75
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 17:59:00.150951 16760 layer_factory.cpp:58] Creating layer cifar
I1123 17:59:00.158952 16760 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1123 17:59:00.158952 16760 net.cpp:84] Creating Layer cifar
I1123 17:59:00.158952 16760 net.cpp:380] cifar -> data
I1123 17:59:00.158952 16760 net.cpp:380] cifar -> label
I1123 17:59:00.159952 16760 data_layer.cpp:45] output data size: 100,3,32,32
I1123 17:59:00.166956 16760 net.cpp:122] Setting up cifar
I1123 17:59:00.166956 16760 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 17:59:00.166956 16760 net.cpp:129] Top shape: 100 (100)
I1123 17:59:00.166956 16760 net.cpp:137] Memory required for data: 1229200
I1123 17:59:00.166956 16760 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 17:59:00.166956 16760 net.cpp:84] Creating Layer label_cifar_1_split
I1123 17:59:00.166956 16760 net.cpp:406] label_cifar_1_split <- label
I1123 17:59:00.166956 16760 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 17:59:00.166956 16760 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 17:59:00.166956 16760 net.cpp:122] Setting up label_cifar_1_split
I1123 17:59:00.166956 16760 net.cpp:129] Top shape: 100 (100)
I1123 17:59:00.166956 16760 net.cpp:129] Top shape: 100 (100)
I1123 17:59:00.166956 16760 net.cpp:137] Memory required for data: 1230000
I1123 17:59:00.166956 16760 layer_factory.cpp:58] Creating layer conv1
I1123 17:59:00.166956 16760 net.cpp:84] Creating Layer conv1
I1123 17:59:00.166956 16760 net.cpp:406] conv1 <- data
I1123 17:59:00.166956 16760 net.cpp:380] conv1 -> conv1
I1123 17:59:00.168941 27420 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 17:59:00.407129 16760 net.cpp:122] Setting up conv1
I1123 17:59:00.407129 16760 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 17:59:00.407129 16760 net.cpp:137] Memory required for data: 15566000
I1123 17:59:00.407129 16760 layer_factory.cpp:58] Creating layer bn1
I1123 17:59:00.407129 16760 net.cpp:84] Creating Layer bn1
I1123 17:59:00.407129 16760 net.cpp:406] bn1 <- conv1
I1123 17:59:00.407129 16760 net.cpp:367] bn1 -> conv1 (in-place)
I1123 17:59:00.407129 16760 net.cpp:122] Setting up bn1
I1123 17:59:00.407129 16760 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 17:59:00.407129 16760 net.cpp:137] Memory required for data: 29902000
I1123 17:59:00.407129 16760 layer_factory.cpp:58] Creating layer scale1
I1123 17:59:00.407129 16760 net.cpp:84] Creating Layer scale1
I1123 17:59:00.407129 16760 net.cpp:406] scale1 <- conv1
I1123 17:59:00.407129 16760 net.cpp:367] scale1 -> conv1 (in-place)
I1123 17:59:00.407129 16760 layer_factory.cpp:58] Creating layer scale1
I1123 17:59:00.407129 16760 net.cpp:122] Setting up scale1
I1123 17:59:00.407129 16760 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 17:59:00.407129 16760 net.cpp:137] Memory required for data: 44238000
I1123 17:59:00.407129 16760 layer_factory.cpp:58] Creating layer relu1
I1123 17:59:00.407129 16760 net.cpp:84] Creating Layer relu1
I1123 17:59:00.407129 16760 net.cpp:406] relu1 <- conv1
I1123 17:59:00.407129 16760 net.cpp:367] relu1 -> conv1 (in-place)
I1123 17:59:00.408131 16760 net.cpp:122] Setting up relu1
I1123 17:59:00.408131 16760 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 17:59:00.408131 16760 net.cpp:137] Memory required for data: 58574000
I1123 17:59:00.408131 16760 layer_factory.cpp:58] Creating layer conv2
I1123 17:59:00.408131 16760 net.cpp:84] Creating Layer conv2
I1123 17:59:00.408131 16760 net.cpp:406] conv2 <- conv1
I1123 17:59:00.408131 16760 net.cpp:380] conv2 -> conv2
I1123 17:59:00.410131 16760 net.cpp:122] Setting up conv2
I1123 17:59:00.410131 16760 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 17:59:00.410131 16760 net.cpp:137] Memory required for data: 74138800
I1123 17:59:00.410131 16760 layer_factory.cpp:58] Creating layer bn2
I1123 17:59:00.410131 16760 net.cpp:84] Creating Layer bn2
I1123 17:59:00.410131 16760 net.cpp:406] bn2 <- conv2
I1123 17:59:00.410131 16760 net.cpp:367] bn2 -> conv2 (in-place)
I1123 17:59:00.410131 16760 net.cpp:122] Setting up bn2
I1123 17:59:00.410131 16760 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 17:59:00.410131 16760 net.cpp:137] Memory required for data: 89703600
I1123 17:59:00.410131 16760 layer_factory.cpp:58] Creating layer scale2
I1123 17:59:00.410131 16760 net.cpp:84] Creating Layer scale2
I1123 17:59:00.410131 16760 net.cpp:406] scale2 <- conv2
I1123 17:59:00.410131 16760 net.cpp:367] scale2 -> conv2 (in-place)
I1123 17:59:00.410131 16760 layer_factory.cpp:58] Creating layer scale2
I1123 17:59:00.410131 16760 net.cpp:122] Setting up scale2
I1123 17:59:00.410131 16760 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 17:59:00.410131 16760 net.cpp:137] Memory required for data: 105268400
I1123 17:59:00.410131 16760 layer_factory.cpp:58] Creating layer relu2
I1123 17:59:00.410131 16760 net.cpp:84] Creating Layer relu2
I1123 17:59:00.410131 16760 net.cpp:406] relu2 <- conv2
I1123 17:59:00.410131 16760 net.cpp:367] relu2 -> conv2 (in-place)
I1123 17:59:00.410131 16760 net.cpp:122] Setting up relu2
I1123 17:59:00.410131 16760 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 17:59:00.410131 16760 net.cpp:137] Memory required for data: 120833200
I1123 17:59:00.410131 16760 layer_factory.cpp:58] Creating layer conv2_2
I1123 17:59:00.411131 16760 net.cpp:84] Creating Layer conv2_2
I1123 17:59:00.411131 16760 net.cpp:406] conv2_2 <- conv2
I1123 17:59:00.411131 16760 net.cpp:380] conv2_2 -> conv2_2
I1123 17:59:00.412130 16760 net.cpp:122] Setting up conv2_2
I1123 17:59:00.412130 16760 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 17:59:00.412130 16760 net.cpp:137] Memory required for data: 147457200
I1123 17:59:00.412130 16760 layer_factory.cpp:58] Creating layer bn2_2
I1123 17:59:00.412130 16760 net.cpp:84] Creating Layer bn2_2
I1123 17:59:00.412130 16760 net.cpp:406] bn2_2 <- conv2_2
I1123 17:59:00.412130 16760 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 17:59:00.412130 16760 net.cpp:122] Setting up bn2_2
I1123 17:59:00.412130 16760 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 17:59:00.412130 16760 net.cpp:137] Memory required for data: 174081200
I1123 17:59:00.412130 16760 layer_factory.cpp:58] Creating layer scale2_2
I1123 17:59:00.412130 16760 net.cpp:84] Creating Layer scale2_2
I1123 17:59:00.412130 16760 net.cpp:406] scale2_2 <- conv2_2
I1123 17:59:00.412130 16760 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 17:59:00.412130 16760 layer_factory.cpp:58] Creating layer scale2_2
I1123 17:59:00.412130 16760 net.cpp:122] Setting up scale2_2
I1123 17:59:00.412130 16760 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 17:59:00.412130 16760 net.cpp:137] Memory required for data: 200705200
I1123 17:59:00.412130 16760 layer_factory.cpp:58] Creating layer relu2_2
I1123 17:59:00.412130 16760 net.cpp:84] Creating Layer relu2_2
I1123 17:59:00.412130 16760 net.cpp:406] relu2_2 <- conv2_2
I1123 17:59:00.412130 16760 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 17:59:00.413130 16760 net.cpp:122] Setting up relu2_2
I1123 17:59:00.413130 16760 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 17:59:00.413130 16760 net.cpp:137] Memory required for data: 227329200
I1123 17:59:00.413130 16760 layer_factory.cpp:58] Creating layer pool2_1
I1123 17:59:00.413130 16760 net.cpp:84] Creating Layer pool2_1
I1123 17:59:00.413130 16760 net.cpp:406] pool2_1 <- conv2_2
I1123 17:59:00.413130 16760 net.cpp:380] pool2_1 -> pool2_1
I1123 17:59:00.413130 16760 net.cpp:122] Setting up pool2_1
I1123 17:59:00.413130 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.413130 16760 net.cpp:137] Memory required for data: 233985200
I1123 17:59:00.413130 16760 layer_factory.cpp:58] Creating layer conv3
I1123 17:59:00.413130 16760 net.cpp:84] Creating Layer conv3
I1123 17:59:00.413130 16760 net.cpp:406] conv3 <- pool2_1
I1123 17:59:00.413130 16760 net.cpp:380] conv3 -> conv3
I1123 17:59:00.414130 16760 net.cpp:122] Setting up conv3
I1123 17:59:00.414130 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.414130 16760 net.cpp:137] Memory required for data: 240641200
I1123 17:59:00.414130 16760 layer_factory.cpp:58] Creating layer bn3
I1123 17:59:00.414130 16760 net.cpp:84] Creating Layer bn3
I1123 17:59:00.414130 16760 net.cpp:406] bn3 <- conv3
I1123 17:59:00.414130 16760 net.cpp:367] bn3 -> conv3 (in-place)
I1123 17:59:00.415129 16760 net.cpp:122] Setting up bn3
I1123 17:59:00.415129 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.415129 16760 net.cpp:137] Memory required for data: 247297200
I1123 17:59:00.415129 16760 layer_factory.cpp:58] Creating layer scale3
I1123 17:59:00.415129 16760 net.cpp:84] Creating Layer scale3
I1123 17:59:00.415129 16760 net.cpp:406] scale3 <- conv3
I1123 17:59:00.415129 16760 net.cpp:367] scale3 -> conv3 (in-place)
I1123 17:59:00.415129 16760 layer_factory.cpp:58] Creating layer scale3
I1123 17:59:00.415129 16760 net.cpp:122] Setting up scale3
I1123 17:59:00.415129 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.415129 16760 net.cpp:137] Memory required for data: 253953200
I1123 17:59:00.415129 16760 layer_factory.cpp:58] Creating layer relu3
I1123 17:59:00.415129 16760 net.cpp:84] Creating Layer relu3
I1123 17:59:00.415129 16760 net.cpp:406] relu3 <- conv3
I1123 17:59:00.415129 16760 net.cpp:367] relu3 -> conv3 (in-place)
I1123 17:59:00.415129 16760 net.cpp:122] Setting up relu3
I1123 17:59:00.415129 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.415129 16760 net.cpp:137] Memory required for data: 260609200
I1123 17:59:00.415129 16760 layer_factory.cpp:58] Creating layer conv4
I1123 17:59:00.415129 16760 net.cpp:84] Creating Layer conv4
I1123 17:59:00.415129 16760 net.cpp:406] conv4 <- conv3
I1123 17:59:00.415129 16760 net.cpp:380] conv4 -> conv4
I1123 17:59:00.416129 16760 net.cpp:122] Setting up conv4
I1123 17:59:00.416129 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.416129 16760 net.cpp:137] Memory required for data: 267265200
I1123 17:59:00.417130 16760 layer_factory.cpp:58] Creating layer bn4
I1123 17:59:00.417130 16760 net.cpp:84] Creating Layer bn4
I1123 17:59:00.417130 16760 net.cpp:406] bn4 <- conv4
I1123 17:59:00.417130 16760 net.cpp:367] bn4 -> conv4 (in-place)
I1123 17:59:00.417130 16760 net.cpp:122] Setting up bn4
I1123 17:59:00.417130 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.417130 16760 net.cpp:137] Memory required for data: 273921200
I1123 17:59:00.417130 16760 layer_factory.cpp:58] Creating layer scale4
I1123 17:59:00.417130 16760 net.cpp:84] Creating Layer scale4
I1123 17:59:00.417130 16760 net.cpp:406] scale4 <- conv4
I1123 17:59:00.417130 16760 net.cpp:367] scale4 -> conv4 (in-place)
I1123 17:59:00.417130 16760 layer_factory.cpp:58] Creating layer scale4
I1123 17:59:00.417130 16760 net.cpp:122] Setting up scale4
I1123 17:59:00.417130 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.417130 16760 net.cpp:137] Memory required for data: 280577200
I1123 17:59:00.417130 16760 layer_factory.cpp:58] Creating layer relu4
I1123 17:59:00.417130 16760 net.cpp:84] Creating Layer relu4
I1123 17:59:00.417130 16760 net.cpp:406] relu4 <- conv4
I1123 17:59:00.417130 16760 net.cpp:367] relu4 -> conv4 (in-place)
I1123 17:59:00.417130 16760 net.cpp:122] Setting up relu4
I1123 17:59:00.417130 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.417130 16760 net.cpp:137] Memory required for data: 287233200
I1123 17:59:00.417130 16760 layer_factory.cpp:58] Creating layer conv4_1
I1123 17:59:00.417130 16760 net.cpp:84] Creating Layer conv4_1
I1123 17:59:00.417130 16760 net.cpp:406] conv4_1 <- conv4
I1123 17:59:00.417130 16760 net.cpp:380] conv4_1 -> conv4_1
I1123 17:59:00.419148 16760 net.cpp:122] Setting up conv4_1
I1123 17:59:00.419148 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.419148 16760 net.cpp:137] Memory required for data: 293889200
I1123 17:59:00.419148 16760 layer_factory.cpp:58] Creating layer bn4_1
I1123 17:59:00.419148 16760 net.cpp:84] Creating Layer bn4_1
I1123 17:59:00.419148 16760 net.cpp:406] bn4_1 <- conv4_1
I1123 17:59:00.419148 16760 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 17:59:00.419148 16760 net.cpp:122] Setting up bn4_1
I1123 17:59:00.419148 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.419148 16760 net.cpp:137] Memory required for data: 300545200
I1123 17:59:00.419148 16760 layer_factory.cpp:58] Creating layer scale4_1
I1123 17:59:00.419148 16760 net.cpp:84] Creating Layer scale4_1
I1123 17:59:00.419148 16760 net.cpp:406] scale4_1 <- conv4_1
I1123 17:59:00.419148 16760 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 17:59:00.419148 16760 layer_factory.cpp:58] Creating layer scale4_1
I1123 17:59:00.419148 16760 net.cpp:122] Setting up scale4_1
I1123 17:59:00.419148 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.419148 16760 net.cpp:137] Memory required for data: 307201200
I1123 17:59:00.419148 16760 layer_factory.cpp:58] Creating layer relu4_1
I1123 17:59:00.419148 16760 net.cpp:84] Creating Layer relu4_1
I1123 17:59:00.419148 16760 net.cpp:406] relu4_1 <- conv4_1
I1123 17:59:00.419148 16760 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 17:59:00.419148 16760 net.cpp:122] Setting up relu4_1
I1123 17:59:00.419148 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.419148 16760 net.cpp:137] Memory required for data: 313857200
I1123 17:59:00.419148 16760 layer_factory.cpp:58] Creating layer conv4_2
I1123 17:59:00.419148 16760 net.cpp:84] Creating Layer conv4_2
I1123 17:59:00.419148 16760 net.cpp:406] conv4_2 <- conv4_1
I1123 17:59:00.419148 16760 net.cpp:380] conv4_2 -> conv4_2
I1123 17:59:00.421149 16760 net.cpp:122] Setting up conv4_2
I1123 17:59:00.421149 16760 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 17:59:00.421149 16760 net.cpp:137] Memory required for data: 321434800
I1123 17:59:00.421149 16760 layer_factory.cpp:58] Creating layer bn4_2
I1123 17:59:00.421149 16760 net.cpp:84] Creating Layer bn4_2
I1123 17:59:00.421149 16760 net.cpp:406] bn4_2 <- conv4_2
I1123 17:59:00.421149 16760 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 17:59:00.421149 16760 net.cpp:122] Setting up bn4_2
I1123 17:59:00.421149 16760 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 17:59:00.421149 16760 net.cpp:137] Memory required for data: 329012400
I1123 17:59:00.421149 16760 layer_factory.cpp:58] Creating layer scale4_2
I1123 17:59:00.421149 16760 net.cpp:84] Creating Layer scale4_2
I1123 17:59:00.421149 16760 net.cpp:406] scale4_2 <- conv4_2
I1123 17:59:00.421149 16760 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 17:59:00.421149 16760 layer_factory.cpp:58] Creating layer scale4_2
I1123 17:59:00.421149 16760 net.cpp:122] Setting up scale4_2
I1123 17:59:00.421149 16760 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 17:59:00.421149 16760 net.cpp:137] Memory required for data: 336590000
I1123 17:59:00.421149 16760 layer_factory.cpp:58] Creating layer relu4_2
I1123 17:59:00.421149 16760 net.cpp:84] Creating Layer relu4_2
I1123 17:59:00.421149 16760 net.cpp:406] relu4_2 <- conv4_2
I1123 17:59:00.421149 16760 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 17:59:00.422148 16760 net.cpp:122] Setting up relu4_2
I1123 17:59:00.422148 16760 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 17:59:00.422148 16760 net.cpp:137] Memory required for data: 344167600
I1123 17:59:00.422148 16760 layer_factory.cpp:58] Creating layer pool4_2
I1123 17:59:00.422148 16760 net.cpp:84] Creating Layer pool4_2
I1123 17:59:00.422148 16760 net.cpp:406] pool4_2 <- conv4_2
I1123 17:59:00.422148 16760 net.cpp:380] pool4_2 -> pool4_2
I1123 17:59:00.422148 16760 net.cpp:122] Setting up pool4_2
I1123 17:59:00.422148 16760 net.cpp:129] Top shape: 100 74 8 8 (473600)
I1123 17:59:00.422148 16760 net.cpp:137] Memory required for data: 346062000
I1123 17:59:00.422148 16760 layer_factory.cpp:58] Creating layer conv12
I1123 17:59:00.422148 16760 net.cpp:84] Creating Layer conv12
I1123 17:59:00.422148 16760 net.cpp:406] conv12 <- pool4_2
I1123 17:59:00.422148 16760 net.cpp:380] conv12 -> conv12
I1123 17:59:00.423148 16760 net.cpp:122] Setting up conv12
I1123 17:59:00.423148 16760 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 17:59:00.423148 16760 net.cpp:137] Memory required for data: 347982000
I1123 17:59:00.423148 16760 layer_factory.cpp:58] Creating layer bn_conv12
I1123 17:59:00.423148 16760 net.cpp:84] Creating Layer bn_conv12
I1123 17:59:00.423148 16760 net.cpp:406] bn_conv12 <- conv12
I1123 17:59:00.423148 16760 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 17:59:00.424149 16760 net.cpp:122] Setting up bn_conv12
I1123 17:59:00.424149 16760 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 17:59:00.424149 16760 net.cpp:137] Memory required for data: 349902000
I1123 17:59:00.424149 16760 layer_factory.cpp:58] Creating layer scale_conv12
I1123 17:59:00.424149 16760 net.cpp:84] Creating Layer scale_conv12
I1123 17:59:00.424149 16760 net.cpp:406] scale_conv12 <- conv12
I1123 17:59:00.424149 16760 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 17:59:00.424149 16760 layer_factory.cpp:58] Creating layer scale_conv12
I1123 17:59:00.424149 16760 net.cpp:122] Setting up scale_conv12
I1123 17:59:00.424149 16760 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 17:59:00.424149 16760 net.cpp:137] Memory required for data: 351822000
I1123 17:59:00.424149 16760 layer_factory.cpp:58] Creating layer relu_conv12
I1123 17:59:00.424149 16760 net.cpp:84] Creating Layer relu_conv12
I1123 17:59:00.424149 16760 net.cpp:406] relu_conv12 <- conv12
I1123 17:59:00.424149 16760 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 17:59:00.424149 16760 net.cpp:122] Setting up relu_conv12
I1123 17:59:00.424149 16760 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 17:59:00.424149 16760 net.cpp:137] Memory required for data: 353742000
I1123 17:59:00.424149 16760 layer_factory.cpp:58] Creating layer poolcp6
I1123 17:59:00.424149 16760 net.cpp:84] Creating Layer poolcp6
I1123 17:59:00.424149 16760 net.cpp:406] poolcp6 <- conv12
I1123 17:59:00.424149 16760 net.cpp:380] poolcp6 -> poolcp6
I1123 17:59:00.424149 16760 net.cpp:122] Setting up poolcp6
I1123 17:59:00.424149 16760 net.cpp:129] Top shape: 100 75 1 1 (7500)
I1123 17:59:00.424149 16760 net.cpp:137] Memory required for data: 353772000
I1123 17:59:00.424149 16760 layer_factory.cpp:58] Creating layer ip1
I1123 17:59:00.424149 16760 net.cpp:84] Creating Layer ip1
I1123 17:59:00.424149 16760 net.cpp:406] ip1 <- poolcp6
I1123 17:59:00.424149 16760 net.cpp:380] ip1 -> ip1
I1123 17:59:00.424149 16760 net.cpp:122] Setting up ip1
I1123 17:59:00.424149 16760 net.cpp:129] Top shape: 100 10 (1000)
I1123 17:59:00.424149 16760 net.cpp:137] Memory required for data: 353776000
I1123 17:59:00.424149 16760 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 17:59:00.424149 16760 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 17:59:00.424149 16760 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 17:59:00.424149 16760 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 17:59:00.424149 16760 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 17:59:00.424149 16760 net.cpp:122] Setting up ip1_ip1_0_split
I1123 17:59:00.424149 16760 net.cpp:129] Top shape: 100 10 (1000)
I1123 17:59:00.424149 16760 net.cpp:129] Top shape: 100 10 (1000)
I1123 17:59:00.424149 16760 net.cpp:137] Memory required for data: 353784000
I1123 17:59:00.424149 16760 layer_factory.cpp:58] Creating layer accuracy_training
I1123 17:59:00.424149 16760 net.cpp:84] Creating Layer accuracy_training
I1123 17:59:00.424149 16760 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1123 17:59:00.424149 16760 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1123 17:59:00.424149 16760 net.cpp:380] accuracy_training -> accuracy_training
I1123 17:59:00.424149 16760 net.cpp:122] Setting up accuracy_training
I1123 17:59:00.424149 16760 net.cpp:129] Top shape: (1)
I1123 17:59:00.424149 16760 net.cpp:137] Memory required for data: 353784004
I1123 17:59:00.424149 16760 layer_factory.cpp:58] Creating layer loss
I1123 17:59:00.424149 16760 net.cpp:84] Creating Layer loss
I1123 17:59:00.424149 16760 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 17:59:00.424149 16760 net.cpp:406] loss <- label_cifar_1_split_1
I1123 17:59:00.424149 16760 net.cpp:380] loss -> loss
I1123 17:59:00.424149 16760 layer_factory.cpp:58] Creating layer loss
I1123 17:59:00.425148 16760 net.cpp:122] Setting up loss
I1123 17:59:00.425148 16760 net.cpp:129] Top shape: (1)
I1123 17:59:00.425148 16760 net.cpp:132]     with loss weight 1
I1123 17:59:00.425148 16760 net.cpp:137] Memory required for data: 353784008
I1123 17:59:00.425148 16760 net.cpp:198] loss needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:200] accuracy_training does not need backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] ip1 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] poolcp6 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] relu_conv12 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] scale_conv12 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] bn_conv12 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] conv12 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] pool4_2 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] relu4_2 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] scale4_2 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] bn4_2 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] conv4_2 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] relu4_1 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] scale4_1 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] bn4_1 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] conv4_1 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] relu4 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] scale4 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] bn4 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] conv4 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] relu3 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] scale3 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] bn3 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] conv3 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] pool2_1 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] relu2_2 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] scale2_2 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] bn2_2 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] conv2_2 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] relu2 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] scale2 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] bn2 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] conv2 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] relu1 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] scale1 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] bn1 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:198] conv1 needs backward computation.
I1123 17:59:00.425148 16760 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 17:59:00.425148 16760 net.cpp:200] cifar does not need backward computation.
I1123 17:59:00.425148 16760 net.cpp:242] This network produces output accuracy_training
I1123 17:59:00.425148 16760 net.cpp:242] This network produces output loss
I1123 17:59:00.425148 16760 net.cpp:255] Network initialization done.
I1123 17:59:00.426148 16760 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 17:59:00.426148 16760 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 17:59:00.426148 16760 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 17:59:00.426148 16760 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1123 17:59:00.426148 16760 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1123 17:59:00.426148 16760 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1123 17:59:00.426148 16760 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1123 17:59:00.426148 16760 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1123 17:59:00.426148 16760 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1123 17:59:00.426148 16760 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1123 17:59:00.426148 16760 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1123 17:59:00.426148 16760 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1123 17:59:00.426148 16760 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1123 17:59:00.426148 16760 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k_7x7_first2layers"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 35
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 38
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 74
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 75
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 17:59:00.426148 16760 layer_factory.cpp:58] Creating layer cifar
I1123 17:59:00.432148 16760 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1123 17:59:00.432148 16760 net.cpp:84] Creating Layer cifar
I1123 17:59:00.432148 16760 net.cpp:380] cifar -> data
I1123 17:59:00.432148 16760 net.cpp:380] cifar -> label
I1123 17:59:00.432148 16760 data_layer.cpp:45] output data size: 100,3,32,32
I1123 17:59:00.438148 16760 net.cpp:122] Setting up cifar
I1123 17:59:00.438148 16760 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 17:59:00.438148 16760 net.cpp:129] Top shape: 100 (100)
I1123 17:59:00.438148 16760 net.cpp:137] Memory required for data: 1229200
I1123 17:59:00.438148 16760 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 17:59:00.438148 16760 net.cpp:84] Creating Layer label_cifar_1_split
I1123 17:59:00.438148 16760 net.cpp:406] label_cifar_1_split <- label
I1123 17:59:00.438148 16760 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 17:59:00.438148 16760 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 17:59:00.438148 16760 net.cpp:122] Setting up label_cifar_1_split
I1123 17:59:00.438148 16760 net.cpp:129] Top shape: 100 (100)
I1123 17:59:00.438148 16760 net.cpp:129] Top shape: 100 (100)
I1123 17:59:00.438148 16760 net.cpp:137] Memory required for data: 1230000
I1123 17:59:00.438148 16760 layer_factory.cpp:58] Creating layer conv1
I1123 17:59:00.438148 16760 net.cpp:84] Creating Layer conv1
I1123 17:59:00.438148 16760 net.cpp:406] conv1 <- data
I1123 17:59:00.438148 16760 net.cpp:380] conv1 -> conv1
I1123 17:59:00.439149 35284 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 17:59:00.439149 16760 net.cpp:122] Setting up conv1
I1123 17:59:00.440150 16760 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 17:59:00.440150 16760 net.cpp:137] Memory required for data: 15566000
I1123 17:59:00.440150 16760 layer_factory.cpp:58] Creating layer bn1
I1123 17:59:00.440150 16760 net.cpp:84] Creating Layer bn1
I1123 17:59:00.440150 16760 net.cpp:406] bn1 <- conv1
I1123 17:59:00.440150 16760 net.cpp:367] bn1 -> conv1 (in-place)
I1123 17:59:00.440150 16760 net.cpp:122] Setting up bn1
I1123 17:59:00.440150 16760 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 17:59:00.440150 16760 net.cpp:137] Memory required for data: 29902000
I1123 17:59:00.440150 16760 layer_factory.cpp:58] Creating layer scale1
I1123 17:59:00.440150 16760 net.cpp:84] Creating Layer scale1
I1123 17:59:00.440150 16760 net.cpp:406] scale1 <- conv1
I1123 17:59:00.440150 16760 net.cpp:367] scale1 -> conv1 (in-place)
I1123 17:59:00.440150 16760 layer_factory.cpp:58] Creating layer scale1
I1123 17:59:00.440150 16760 net.cpp:122] Setting up scale1
I1123 17:59:00.440150 16760 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 17:59:00.440150 16760 net.cpp:137] Memory required for data: 44238000
I1123 17:59:00.440150 16760 layer_factory.cpp:58] Creating layer relu1
I1123 17:59:00.440150 16760 net.cpp:84] Creating Layer relu1
I1123 17:59:00.440150 16760 net.cpp:406] relu1 <- conv1
I1123 17:59:00.440150 16760 net.cpp:367] relu1 -> conv1 (in-place)
I1123 17:59:00.441146 16760 net.cpp:122] Setting up relu1
I1123 17:59:00.441146 16760 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 17:59:00.441146 16760 net.cpp:137] Memory required for data: 58574000
I1123 17:59:00.441146 16760 layer_factory.cpp:58] Creating layer conv2
I1123 17:59:00.441146 16760 net.cpp:84] Creating Layer conv2
I1123 17:59:00.441146 16760 net.cpp:406] conv2 <- conv1
I1123 17:59:00.441146 16760 net.cpp:380] conv2 -> conv2
I1123 17:59:00.442137 16760 net.cpp:122] Setting up conv2
I1123 17:59:00.442137 16760 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 17:59:00.442137 16760 net.cpp:137] Memory required for data: 74138800
I1123 17:59:00.442137 16760 layer_factory.cpp:58] Creating layer bn2
I1123 17:59:00.442137 16760 net.cpp:84] Creating Layer bn2
I1123 17:59:00.442137 16760 net.cpp:406] bn2 <- conv2
I1123 17:59:00.442137 16760 net.cpp:367] bn2 -> conv2 (in-place)
I1123 17:59:00.442137 16760 net.cpp:122] Setting up bn2
I1123 17:59:00.442137 16760 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 17:59:00.442137 16760 net.cpp:137] Memory required for data: 89703600
I1123 17:59:00.442137 16760 layer_factory.cpp:58] Creating layer scale2
I1123 17:59:00.442137 16760 net.cpp:84] Creating Layer scale2
I1123 17:59:00.442137 16760 net.cpp:406] scale2 <- conv2
I1123 17:59:00.442137 16760 net.cpp:367] scale2 -> conv2 (in-place)
I1123 17:59:00.442137 16760 layer_factory.cpp:58] Creating layer scale2
I1123 17:59:00.442137 16760 net.cpp:122] Setting up scale2
I1123 17:59:00.442137 16760 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 17:59:00.443145 16760 net.cpp:137] Memory required for data: 105268400
I1123 17:59:00.443145 16760 layer_factory.cpp:58] Creating layer relu2
I1123 17:59:00.443145 16760 net.cpp:84] Creating Layer relu2
I1123 17:59:00.443145 16760 net.cpp:406] relu2 <- conv2
I1123 17:59:00.443145 16760 net.cpp:367] relu2 -> conv2 (in-place)
I1123 17:59:00.443145 16760 net.cpp:122] Setting up relu2
I1123 17:59:00.443145 16760 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 17:59:00.443145 16760 net.cpp:137] Memory required for data: 120833200
I1123 17:59:00.443145 16760 layer_factory.cpp:58] Creating layer conv2_2
I1123 17:59:00.443145 16760 net.cpp:84] Creating Layer conv2_2
I1123 17:59:00.443145 16760 net.cpp:406] conv2_2 <- conv2
I1123 17:59:00.443145 16760 net.cpp:380] conv2_2 -> conv2_2
I1123 17:59:00.444145 16760 net.cpp:122] Setting up conv2_2
I1123 17:59:00.444145 16760 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 17:59:00.444145 16760 net.cpp:137] Memory required for data: 147457200
I1123 17:59:00.445145 16760 layer_factory.cpp:58] Creating layer bn2_2
I1123 17:59:00.445145 16760 net.cpp:84] Creating Layer bn2_2
I1123 17:59:00.445145 16760 net.cpp:406] bn2_2 <- conv2_2
I1123 17:59:00.445145 16760 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 17:59:00.445145 16760 net.cpp:122] Setting up bn2_2
I1123 17:59:00.445145 16760 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 17:59:00.445145 16760 net.cpp:137] Memory required for data: 174081200
I1123 17:59:00.445145 16760 layer_factory.cpp:58] Creating layer scale2_2
I1123 17:59:00.445145 16760 net.cpp:84] Creating Layer scale2_2
I1123 17:59:00.445145 16760 net.cpp:406] scale2_2 <- conv2_2
I1123 17:59:00.445145 16760 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 17:59:00.445145 16760 layer_factory.cpp:58] Creating layer scale2_2
I1123 17:59:00.445145 16760 net.cpp:122] Setting up scale2_2
I1123 17:59:00.445145 16760 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 17:59:00.445145 16760 net.cpp:137] Memory required for data: 200705200
I1123 17:59:00.445145 16760 layer_factory.cpp:58] Creating layer relu2_2
I1123 17:59:00.445145 16760 net.cpp:84] Creating Layer relu2_2
I1123 17:59:00.445145 16760 net.cpp:406] relu2_2 <- conv2_2
I1123 17:59:00.445145 16760 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 17:59:00.445145 16760 net.cpp:122] Setting up relu2_2
I1123 17:59:00.445145 16760 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 17:59:00.445145 16760 net.cpp:137] Memory required for data: 227329200
I1123 17:59:00.445145 16760 layer_factory.cpp:58] Creating layer pool2_1
I1123 17:59:00.445145 16760 net.cpp:84] Creating Layer pool2_1
I1123 17:59:00.445145 16760 net.cpp:406] pool2_1 <- conv2_2
I1123 17:59:00.445145 16760 net.cpp:380] pool2_1 -> pool2_1
I1123 17:59:00.445145 16760 net.cpp:122] Setting up pool2_1
I1123 17:59:00.445145 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.446146 16760 net.cpp:137] Memory required for data: 233985200
I1123 17:59:00.446146 16760 layer_factory.cpp:58] Creating layer conv3
I1123 17:59:00.446146 16760 net.cpp:84] Creating Layer conv3
I1123 17:59:00.446146 16760 net.cpp:406] conv3 <- pool2_1
I1123 17:59:00.446146 16760 net.cpp:380] conv3 -> conv3
I1123 17:59:00.447146 16760 net.cpp:122] Setting up conv3
I1123 17:59:00.447146 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.447146 16760 net.cpp:137] Memory required for data: 240641200
I1123 17:59:00.447146 16760 layer_factory.cpp:58] Creating layer bn3
I1123 17:59:00.447146 16760 net.cpp:84] Creating Layer bn3
I1123 17:59:00.447146 16760 net.cpp:406] bn3 <- conv3
I1123 17:59:00.447146 16760 net.cpp:367] bn3 -> conv3 (in-place)
I1123 17:59:00.448145 16760 net.cpp:122] Setting up bn3
I1123 17:59:00.448145 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.448145 16760 net.cpp:137] Memory required for data: 247297200
I1123 17:59:00.448145 16760 layer_factory.cpp:58] Creating layer scale3
I1123 17:59:00.448145 16760 net.cpp:84] Creating Layer scale3
I1123 17:59:00.448145 16760 net.cpp:406] scale3 <- conv3
I1123 17:59:00.448145 16760 net.cpp:367] scale3 -> conv3 (in-place)
I1123 17:59:00.448145 16760 layer_factory.cpp:58] Creating layer scale3
I1123 17:59:00.448145 16760 net.cpp:122] Setting up scale3
I1123 17:59:00.448145 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.448145 16760 net.cpp:137] Memory required for data: 253953200
I1123 17:59:00.448145 16760 layer_factory.cpp:58] Creating layer relu3
I1123 17:59:00.448145 16760 net.cpp:84] Creating Layer relu3
I1123 17:59:00.448145 16760 net.cpp:406] relu3 <- conv3
I1123 17:59:00.448145 16760 net.cpp:367] relu3 -> conv3 (in-place)
I1123 17:59:00.448145 16760 net.cpp:122] Setting up relu3
I1123 17:59:00.448145 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.448145 16760 net.cpp:137] Memory required for data: 260609200
I1123 17:59:00.448145 16760 layer_factory.cpp:58] Creating layer conv4
I1123 17:59:00.448145 16760 net.cpp:84] Creating Layer conv4
I1123 17:59:00.448145 16760 net.cpp:406] conv4 <- conv3
I1123 17:59:00.448145 16760 net.cpp:380] conv4 -> conv4
I1123 17:59:00.450145 16760 net.cpp:122] Setting up conv4
I1123 17:59:00.450145 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.450145 16760 net.cpp:137] Memory required for data: 267265200
I1123 17:59:00.450145 16760 layer_factory.cpp:58] Creating layer bn4
I1123 17:59:00.450145 16760 net.cpp:84] Creating Layer bn4
I1123 17:59:00.450145 16760 net.cpp:406] bn4 <- conv4
I1123 17:59:00.450145 16760 net.cpp:367] bn4 -> conv4 (in-place)
I1123 17:59:00.450145 16760 net.cpp:122] Setting up bn4
I1123 17:59:00.450145 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.450145 16760 net.cpp:137] Memory required for data: 273921200
I1123 17:59:00.450145 16760 layer_factory.cpp:58] Creating layer scale4
I1123 17:59:00.450145 16760 net.cpp:84] Creating Layer scale4
I1123 17:59:00.450145 16760 net.cpp:406] scale4 <- conv4
I1123 17:59:00.450145 16760 net.cpp:367] scale4 -> conv4 (in-place)
I1123 17:59:00.450145 16760 layer_factory.cpp:58] Creating layer scale4
I1123 17:59:00.450145 16760 net.cpp:122] Setting up scale4
I1123 17:59:00.450145 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.450145 16760 net.cpp:137] Memory required for data: 280577200
I1123 17:59:00.450145 16760 layer_factory.cpp:58] Creating layer relu4
I1123 17:59:00.450145 16760 net.cpp:84] Creating Layer relu4
I1123 17:59:00.450145 16760 net.cpp:406] relu4 <- conv4
I1123 17:59:00.450145 16760 net.cpp:367] relu4 -> conv4 (in-place)
I1123 17:59:00.450145 16760 net.cpp:122] Setting up relu4
I1123 17:59:00.450145 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.450145 16760 net.cpp:137] Memory required for data: 287233200
I1123 17:59:00.450145 16760 layer_factory.cpp:58] Creating layer conv4_1
I1123 17:59:00.450145 16760 net.cpp:84] Creating Layer conv4_1
I1123 17:59:00.450145 16760 net.cpp:406] conv4_1 <- conv4
I1123 17:59:00.450145 16760 net.cpp:380] conv4_1 -> conv4_1
I1123 17:59:00.452141 16760 net.cpp:122] Setting up conv4_1
I1123 17:59:00.452141 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.452141 16760 net.cpp:137] Memory required for data: 293889200
I1123 17:59:00.452141 16760 layer_factory.cpp:58] Creating layer bn4_1
I1123 17:59:00.452141 16760 net.cpp:84] Creating Layer bn4_1
I1123 17:59:00.452141 16760 net.cpp:406] bn4_1 <- conv4_1
I1123 17:59:00.452141 16760 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 17:59:00.452141 16760 net.cpp:122] Setting up bn4_1
I1123 17:59:00.452141 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.452141 16760 net.cpp:137] Memory required for data: 300545200
I1123 17:59:00.452141 16760 layer_factory.cpp:58] Creating layer scale4_1
I1123 17:59:00.452141 16760 net.cpp:84] Creating Layer scale4_1
I1123 17:59:00.452141 16760 net.cpp:406] scale4_1 <- conv4_1
I1123 17:59:00.452141 16760 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 17:59:00.452141 16760 layer_factory.cpp:58] Creating layer scale4_1
I1123 17:59:00.452141 16760 net.cpp:122] Setting up scale4_1
I1123 17:59:00.452141 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.452141 16760 net.cpp:137] Memory required for data: 307201200
I1123 17:59:00.452141 16760 layer_factory.cpp:58] Creating layer relu4_1
I1123 17:59:00.452141 16760 net.cpp:84] Creating Layer relu4_1
I1123 17:59:00.452141 16760 net.cpp:406] relu4_1 <- conv4_1
I1123 17:59:00.452141 16760 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 17:59:00.453141 16760 net.cpp:122] Setting up relu4_1
I1123 17:59:00.453141 16760 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 17:59:00.453141 16760 net.cpp:137] Memory required for data: 313857200
I1123 17:59:00.453141 16760 layer_factory.cpp:58] Creating layer conv4_2
I1123 17:59:00.453141 16760 net.cpp:84] Creating Layer conv4_2
I1123 17:59:00.453141 16760 net.cpp:406] conv4_2 <- conv4_1
I1123 17:59:00.453141 16760 net.cpp:380] conv4_2 -> conv4_2
I1123 17:59:00.455140 16760 net.cpp:122] Setting up conv4_2
I1123 17:59:00.455140 16760 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 17:59:00.455140 16760 net.cpp:137] Memory required for data: 321434800
I1123 17:59:00.455140 16760 layer_factory.cpp:58] Creating layer bn4_2
I1123 17:59:00.455140 16760 net.cpp:84] Creating Layer bn4_2
I1123 17:59:00.455140 16760 net.cpp:406] bn4_2 <- conv4_2
I1123 17:59:00.455140 16760 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 17:59:00.455140 16760 net.cpp:122] Setting up bn4_2
I1123 17:59:00.455140 16760 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 17:59:00.455140 16760 net.cpp:137] Memory required for data: 329012400
I1123 17:59:00.455140 16760 layer_factory.cpp:58] Creating layer scale4_2
I1123 17:59:00.455140 16760 net.cpp:84] Creating Layer scale4_2
I1123 17:59:00.455140 16760 net.cpp:406] scale4_2 <- conv4_2
I1123 17:59:00.455140 16760 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 17:59:00.455140 16760 layer_factory.cpp:58] Creating layer scale4_2
I1123 17:59:00.455140 16760 net.cpp:122] Setting up scale4_2
I1123 17:59:00.455140 16760 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 17:59:00.455140 16760 net.cpp:137] Memory required for data: 336590000
I1123 17:59:00.455140 16760 layer_factory.cpp:58] Creating layer relu4_2
I1123 17:59:00.455140 16760 net.cpp:84] Creating Layer relu4_2
I1123 17:59:00.455140 16760 net.cpp:406] relu4_2 <- conv4_2
I1123 17:59:00.455140 16760 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 17:59:00.456140 16760 net.cpp:122] Setting up relu4_2
I1123 17:59:00.456140 16760 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 17:59:00.456140 16760 net.cpp:137] Memory required for data: 344167600
I1123 17:59:00.456140 16760 layer_factory.cpp:58] Creating layer pool4_2
I1123 17:59:00.456140 16760 net.cpp:84] Creating Layer pool4_2
I1123 17:59:00.456140 16760 net.cpp:406] pool4_2 <- conv4_2
I1123 17:59:00.456140 16760 net.cpp:380] pool4_2 -> pool4_2
I1123 17:59:00.456140 16760 net.cpp:122] Setting up pool4_2
I1123 17:59:00.456140 16760 net.cpp:129] Top shape: 100 74 8 8 (473600)
I1123 17:59:00.456140 16760 net.cpp:137] Memory required for data: 346062000
I1123 17:59:00.456140 16760 layer_factory.cpp:58] Creating layer conv12
I1123 17:59:00.456140 16760 net.cpp:84] Creating Layer conv12
I1123 17:59:00.456140 16760 net.cpp:406] conv12 <- pool4_2
I1123 17:59:00.456140 16760 net.cpp:380] conv12 -> conv12
I1123 17:59:00.457140 16760 net.cpp:122] Setting up conv12
I1123 17:59:00.457140 16760 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 17:59:00.457140 16760 net.cpp:137] Memory required for data: 347982000
I1123 17:59:00.457140 16760 layer_factory.cpp:58] Creating layer bn_conv12
I1123 17:59:00.457140 16760 net.cpp:84] Creating Layer bn_conv12
I1123 17:59:00.457140 16760 net.cpp:406] bn_conv12 <- conv12
I1123 17:59:00.457140 16760 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 17:59:00.457140 16760 net.cpp:122] Setting up bn_conv12
I1123 17:59:00.457140 16760 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 17:59:00.457140 16760 net.cpp:137] Memory required for data: 349902000
I1123 17:59:00.457140 16760 layer_factory.cpp:58] Creating layer scale_conv12
I1123 17:59:00.457140 16760 net.cpp:84] Creating Layer scale_conv12
I1123 17:59:00.457140 16760 net.cpp:406] scale_conv12 <- conv12
I1123 17:59:00.457140 16760 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 17:59:00.457140 16760 layer_factory.cpp:58] Creating layer scale_conv12
I1123 17:59:00.458140 16760 net.cpp:122] Setting up scale_conv12
I1123 17:59:00.458140 16760 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 17:59:00.458140 16760 net.cpp:137] Memory required for data: 351822000
I1123 17:59:00.458140 16760 layer_factory.cpp:58] Creating layer relu_conv12
I1123 17:59:00.458140 16760 net.cpp:84] Creating Layer relu_conv12
I1123 17:59:00.458140 16760 net.cpp:406] relu_conv12 <- conv12
I1123 17:59:00.458140 16760 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 17:59:00.458140 16760 net.cpp:122] Setting up relu_conv12
I1123 17:59:00.458140 16760 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 17:59:00.458140 16760 net.cpp:137] Memory required for data: 353742000
I1123 17:59:00.458140 16760 layer_factory.cpp:58] Creating layer poolcp6
I1123 17:59:00.458140 16760 net.cpp:84] Creating Layer poolcp6
I1123 17:59:00.458140 16760 net.cpp:406] poolcp6 <- conv12
I1123 17:59:00.458140 16760 net.cpp:380] poolcp6 -> poolcp6
I1123 17:59:00.458140 16760 net.cpp:122] Setting up poolcp6
I1123 17:59:00.458140 16760 net.cpp:129] Top shape: 100 75 1 1 (7500)
I1123 17:59:00.458140 16760 net.cpp:137] Memory required for data: 353772000
I1123 17:59:00.458140 16760 layer_factory.cpp:58] Creating layer ip1
I1123 17:59:00.458140 16760 net.cpp:84] Creating Layer ip1
I1123 17:59:00.458140 16760 net.cpp:406] ip1 <- poolcp6
I1123 17:59:00.458140 16760 net.cpp:380] ip1 -> ip1
I1123 17:59:00.458140 16760 net.cpp:122] Setting up ip1
I1123 17:59:00.458140 16760 net.cpp:129] Top shape: 100 10 (1000)
I1123 17:59:00.458140 16760 net.cpp:137] Memory required for data: 353776000
I1123 17:59:00.458140 16760 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 17:59:00.458140 16760 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 17:59:00.458140 16760 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 17:59:00.458140 16760 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 17:59:00.458140 16760 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 17:59:00.458140 16760 net.cpp:122] Setting up ip1_ip1_0_split
I1123 17:59:00.458140 16760 net.cpp:129] Top shape: 100 10 (1000)
I1123 17:59:00.458140 16760 net.cpp:129] Top shape: 100 10 (1000)
I1123 17:59:00.458140 16760 net.cpp:137] Memory required for data: 353784000
I1123 17:59:00.458140 16760 layer_factory.cpp:58] Creating layer accuracy
I1123 17:59:00.458140 16760 net.cpp:84] Creating Layer accuracy
I1123 17:59:00.458140 16760 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1123 17:59:00.458140 16760 net.cpp:406] accuracy <- label_cifar_1_split_0
I1123 17:59:00.458140 16760 net.cpp:380] accuracy -> accuracy
I1123 17:59:00.458140 16760 net.cpp:122] Setting up accuracy
I1123 17:59:00.458140 16760 net.cpp:129] Top shape: (1)
I1123 17:59:00.458140 16760 net.cpp:137] Memory required for data: 353784004
I1123 17:59:00.458140 16760 layer_factory.cpp:58] Creating layer loss
I1123 17:59:00.458140 16760 net.cpp:84] Creating Layer loss
I1123 17:59:00.458140 16760 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 17:59:00.458140 16760 net.cpp:406] loss <- label_cifar_1_split_1
I1123 17:59:00.458140 16760 net.cpp:380] loss -> loss
I1123 17:59:00.458140 16760 layer_factory.cpp:58] Creating layer loss
I1123 17:59:00.459141 16760 net.cpp:122] Setting up loss
I1123 17:59:00.459141 16760 net.cpp:129] Top shape: (1)
I1123 17:59:00.459141 16760 net.cpp:132]     with loss weight 1
I1123 17:59:00.459141 16760 net.cpp:137] Memory required for data: 353784008
I1123 17:59:00.459141 16760 net.cpp:198] loss needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:200] accuracy does not need backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] ip1 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] poolcp6 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] relu_conv12 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] scale_conv12 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] bn_conv12 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] conv12 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] pool4_2 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] relu4_2 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] scale4_2 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] bn4_2 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] conv4_2 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] relu4_1 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] scale4_1 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] bn4_1 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] conv4_1 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] relu4 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] scale4 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] bn4 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] conv4 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] relu3 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] scale3 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] bn3 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] conv3 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] pool2_1 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] relu2_2 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] scale2_2 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] bn2_2 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] conv2_2 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] relu2 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] scale2 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] bn2 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] conv2 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] relu1 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] scale1 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] bn1 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:198] conv1 needs backward computation.
I1123 17:59:00.459141 16760 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 17:59:00.459141 16760 net.cpp:200] cifar does not need backward computation.
I1123 17:59:00.459141 16760 net.cpp:242] This network produces output accuracy
I1123 17:59:00.459141 16760 net.cpp:242] This network produces output loss
I1123 17:59:00.459141 16760 net.cpp:255] Network initialization done.
I1123 17:59:00.459141 16760 solver.cpp:56] Solver scaffolding done.
I1123 17:59:00.462141 16760 caffe.cpp:249] Starting Optimization
I1123 17:59:00.462141 16760 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k_7x7_first2layers
I1123 17:59:00.462141 16760 solver.cpp:273] Learning Rate Policy: multistep
I1123 17:59:00.463140 16760 solver.cpp:330] Iteration 0, Testing net (#0)
I1123 17:59:00.464144 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 17:59:01.765616 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:59:01.815791 16760 solver.cpp:397]     Test net output #0: accuracy = 0.1018
I1123 17:59:01.815791 16760 solver.cpp:397]     Test net output #1: loss = 78.4457 (* 1 = 78.4457 loss)
I1123 17:59:01.893303 16760 solver.cpp:218] Iteration 0 (0 iter/s, 1.43078s/100 iters), loss = 3.46642
I1123 17:59:01.893303 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.03
I1123 17:59:01.893303 16760 solver.cpp:237]     Train net output #1: loss = 3.46642 (* 1 = 3.46642 loss)
I1123 17:59:01.893303 16760 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1123 17:59:06.687790 16760 solver.cpp:218] Iteration 100 (20.8573 iter/s, 4.79449s/100 iters), loss = 1.73538
I1123 17:59:06.687790 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.33
I1123 17:59:06.687790 16760 solver.cpp:237]     Train net output #1: loss = 1.73538 (* 1 = 1.73538 loss)
I1123 17:59:06.687790 16760 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1123 17:59:11.478791 16760 solver.cpp:218] Iteration 200 (20.8759 iter/s, 4.7902s/100 iters), loss = 1.79683
I1123 17:59:11.478791 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.36
I1123 17:59:11.478791 16760 solver.cpp:237]     Train net output #1: loss = 1.79683 (* 1 = 1.79683 loss)
I1123 17:59:11.478791 16760 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1123 17:59:16.277568 16760 solver.cpp:218] Iteration 300 (20.8397 iter/s, 4.79854s/100 iters), loss = 1.4393
I1123 17:59:16.277568 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.53
I1123 17:59:16.277568 16760 solver.cpp:237]     Train net output #1: loss = 1.4393 (* 1 = 1.4393 loss)
I1123 17:59:16.277568 16760 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1123 17:59:21.071182 16760 solver.cpp:218] Iteration 400 (20.8642 iter/s, 4.79289s/100 iters), loss = 1.23959
I1123 17:59:21.071182 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.53
I1123 17:59:21.071182 16760 solver.cpp:237]     Train net output #1: loss = 1.23959 (* 1 = 1.23959 loss)
I1123 17:59:21.071182 16760 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1123 17:59:25.623687 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:59:25.811215 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_500.caffemodel
I1123 17:59:25.824717 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_500.solverstate
I1123 17:59:25.828718 16760 solver.cpp:330] Iteration 500, Testing net (#0)
I1123 17:59:25.828718 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 17:59:27.083957 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:59:27.134459 16760 solver.cpp:397]     Test net output #0: accuracy = 0.3988
I1123 17:59:27.134459 16760 solver.cpp:397]     Test net output #1: loss = 1.66482 (* 1 = 1.66482 loss)
I1123 17:59:27.180582 16760 solver.cpp:218] Iteration 500 (16.3689 iter/s, 6.10915s/100 iters), loss = 1.39309
I1123 17:59:27.180582 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.46
I1123 17:59:27.180582 16760 solver.cpp:237]     Train net output #1: loss = 1.39309 (* 1 = 1.39309 loss)
I1123 17:59:27.180582 16760 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1123 17:59:31.951162 16760 solver.cpp:218] Iteration 600 (20.9621 iter/s, 4.77052s/100 iters), loss = 1.31959
I1123 17:59:31.951162 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.53
I1123 17:59:31.951162 16760 solver.cpp:237]     Train net output #1: loss = 1.31959 (* 1 = 1.31959 loss)
I1123 17:59:31.951162 16760 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1123 17:59:36.719276 16760 solver.cpp:218] Iteration 700 (20.9735 iter/s, 4.76791s/100 iters), loss = 1.1948
I1123 17:59:36.719276 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.58
I1123 17:59:36.719276 16760 solver.cpp:237]     Train net output #1: loss = 1.1948 (* 1 = 1.1948 loss)
I1123 17:59:36.719276 16760 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1123 17:59:41.503466 16760 solver.cpp:218] Iteration 800 (20.9037 iter/s, 4.78385s/100 iters), loss = 1.03095
I1123 17:59:41.503466 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1123 17:59:41.503466 16760 solver.cpp:237]     Train net output #1: loss = 1.03095 (* 1 = 1.03095 loss)
I1123 17:59:41.503466 16760 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1123 17:59:46.329792 16760 solver.cpp:218] Iteration 900 (20.7237 iter/s, 4.8254s/100 iters), loss = 1.01572
I1123 17:59:46.329792 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.61
I1123 17:59:46.329792 16760 solver.cpp:237]     Train net output #1: loss = 1.01572 (* 1 = 1.01572 loss)
I1123 17:59:46.330293 16760 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1123 17:59:50.905871 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:59:51.095948 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_1000.caffemodel
I1123 17:59:51.106938 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_1000.solverstate
I1123 17:59:51.110939 16760 solver.cpp:330] Iteration 1000, Testing net (#0)
I1123 17:59:51.110939 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 17:59:52.372216 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:59:52.423216 16760 solver.cpp:397]     Test net output #0: accuracy = 0.6059
I1123 17:59:52.423216 16760 solver.cpp:397]     Test net output #1: loss = 1.1296 (* 1 = 1.1296 loss)
I1123 17:59:52.469225 16760 solver.cpp:218] Iteration 1000 (16.2896 iter/s, 6.1389s/100 iters), loss = 0.989276
I1123 17:59:52.469225 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1123 17:59:52.469225 16760 solver.cpp:237]     Train net output #1: loss = 0.989276 (* 1 = 0.989276 loss)
I1123 17:59:52.469225 16760 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1123 17:59:57.277858 16760 solver.cpp:218] Iteration 1100 (20.7959 iter/s, 4.80863s/100 iters), loss = 0.940025
I1123 17:59:57.277858 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1123 17:59:57.278857 16760 solver.cpp:237]     Train net output #1: loss = 0.940025 (* 1 = 0.940025 loss)
I1123 17:59:57.278857 16760 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1123 18:00:02.174396 16760 solver.cpp:218] Iteration 1200 (20.4282 iter/s, 4.89519s/100 iters), loss = 0.875946
I1123 18:00:02.174396 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1123 18:00:02.174396 16760 solver.cpp:237]     Train net output #1: loss = 0.875946 (* 1 = 0.875946 loss)
I1123 18:00:02.174396 16760 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1123 18:00:06.982602 16760 solver.cpp:218] Iteration 1300 (20.7972 iter/s, 4.80834s/100 iters), loss = 0.856027
I1123 18:00:06.982602 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1123 18:00:06.982602 16760 solver.cpp:237]     Train net output #1: loss = 0.856027 (* 1 = 0.856027 loss)
I1123 18:00:06.982602 16760 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1123 18:00:11.829792 16760 solver.cpp:218] Iteration 1400 (20.6328 iter/s, 4.84665s/100 iters), loss = 1.01984
I1123 18:00:11.829792 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.62
I1123 18:00:11.829792 16760 solver.cpp:237]     Train net output #1: loss = 1.01984 (* 1 = 1.01984 loss)
I1123 18:00:11.829792 16760 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1123 18:00:16.437970 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:00:16.626482 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_1500.caffemodel
I1123 18:00:16.638500 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_1500.solverstate
I1123 18:00:16.642500 16760 solver.cpp:330] Iteration 1500, Testing net (#0)
I1123 18:00:16.642500 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:00:17.915524 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:00:17.966528 16760 solver.cpp:397]     Test net output #0: accuracy = 0.6353
I1123 18:00:17.966528 16760 solver.cpp:397]     Test net output #1: loss = 1.02808 (* 1 = 1.02808 loss)
I1123 18:00:18.013566 16760 solver.cpp:218] Iteration 1500 (16.1742 iter/s, 6.18269s/100 iters), loss = 0.803494
I1123 18:00:18.013566 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1123 18:00:18.013566 16760 solver.cpp:237]     Train net output #1: loss = 0.803494 (* 1 = 0.803494 loss)
I1123 18:00:18.013566 16760 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1123 18:00:22.862426 16760 solver.cpp:218] Iteration 1600 (20.623 iter/s, 4.84897s/100 iters), loss = 0.748556
I1123 18:00:22.862426 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1123 18:00:22.862426 16760 solver.cpp:237]     Train net output #1: loss = 0.748556 (* 1 = 0.748556 loss)
I1123 18:00:22.862426 16760 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1123 18:00:27.671078 16760 solver.cpp:218] Iteration 1700 (20.7997 iter/s, 4.80776s/100 iters), loss = 0.743821
I1123 18:00:27.671078 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 18:00:27.671078 16760 solver.cpp:237]     Train net output #1: loss = 0.743821 (* 1 = 0.743821 loss)
I1123 18:00:27.671078 16760 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1123 18:00:32.481864 16760 solver.cpp:218] Iteration 1800 (20.7867 iter/s, 4.81078s/100 iters), loss = 0.747987
I1123 18:00:32.481864 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1123 18:00:32.481864 16760 solver.cpp:237]     Train net output #1: loss = 0.747987 (* 1 = 0.747987 loss)
I1123 18:00:32.481864 16760 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1123 18:00:37.342355 16760 solver.cpp:218] Iteration 1900 (20.5795 iter/s, 4.85921s/100 iters), loss = 0.721535
I1123 18:00:37.342355 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1123 18:00:37.342355 16760 solver.cpp:237]     Train net output #1: loss = 0.721535 (* 1 = 0.721535 loss)
I1123 18:00:37.342355 16760 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1123 18:00:41.955150 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:00:42.144410 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_2000.caffemodel
I1123 18:00:42.155411 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_2000.solverstate
I1123 18:00:42.159427 16760 solver.cpp:330] Iteration 2000, Testing net (#0)
I1123 18:00:42.159427 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:00:43.423535 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:00:43.474535 16760 solver.cpp:397]     Test net output #0: accuracy = 0.6289
I1123 18:00:43.474535 16760 solver.cpp:397]     Test net output #1: loss = 1.05624 (* 1 = 1.05624 loss)
I1123 18:00:43.521075 16760 solver.cpp:218] Iteration 2000 (16.1861 iter/s, 6.17815s/100 iters), loss = 0.670634
I1123 18:00:43.521075 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 18:00:43.521075 16760 solver.cpp:237]     Train net output #1: loss = 0.670634 (* 1 = 0.670634 loss)
I1123 18:00:43.521075 16760 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1123 18:00:48.349310 16760 solver.cpp:218] Iteration 2100 (20.7113 iter/s, 4.82828s/100 iters), loss = 0.581175
I1123 18:00:48.349310 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 18:00:48.349310 16760 solver.cpp:237]     Train net output #1: loss = 0.581175 (* 1 = 0.581175 loss)
I1123 18:00:48.349310 16760 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1123 18:00:53.249608 16760 solver.cpp:218] Iteration 2200 (20.4093 iter/s, 4.89973s/100 iters), loss = 0.661533
I1123 18:00:53.249608 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 18:00:53.249608 16760 solver.cpp:237]     Train net output #1: loss = 0.661533 (* 1 = 0.661533 loss)
I1123 18:00:53.249608 16760 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1123 18:00:58.100464 16760 solver.cpp:218] Iteration 2300 (20.6182 iter/s, 4.85009s/100 iters), loss = 0.743361
I1123 18:00:58.100464 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 18:00:58.100464 16760 solver.cpp:237]     Train net output #1: loss = 0.743361 (* 1 = 0.743361 loss)
I1123 18:00:58.100464 16760 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1123 18:01:02.949573 16760 solver.cpp:218] Iteration 2400 (20.6233 iter/s, 4.84889s/100 iters), loss = 0.695486
I1123 18:01:02.949573 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1123 18:01:02.949573 16760 solver.cpp:237]     Train net output #1: loss = 0.695486 (* 1 = 0.695486 loss)
I1123 18:01:02.949573 16760 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1123 18:01:07.568100 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:01:07.761363 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_2500.caffemodel
I1123 18:01:07.773363 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_2500.solverstate
I1123 18:01:07.777364 16760 solver.cpp:330] Iteration 2500, Testing net (#0)
I1123 18:01:07.777364 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:01:09.051679 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:01:09.101680 16760 solver.cpp:397]     Test net output #0: accuracy = 0.7212
I1123 18:01:09.101680 16760 solver.cpp:397]     Test net output #1: loss = 0.80491 (* 1 = 0.80491 loss)
I1123 18:01:09.148203 16760 solver.cpp:218] Iteration 2500 (16.1352 iter/s, 6.19762s/100 iters), loss = 0.654254
I1123 18:01:09.148203 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1123 18:01:09.148203 16760 solver.cpp:237]     Train net output #1: loss = 0.654254 (* 1 = 0.654254 loss)
I1123 18:01:09.148203 16760 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1123 18:01:13.982414 16760 solver.cpp:218] Iteration 2600 (20.6848 iter/s, 4.83447s/100 iters), loss = 0.517127
I1123 18:01:13.982414 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 18:01:13.982414 16760 solver.cpp:237]     Train net output #1: loss = 0.517127 (* 1 = 0.517127 loss)
I1123 18:01:13.982414 16760 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1123 18:01:18.804553 16760 solver.cpp:218] Iteration 2700 (20.7407 iter/s, 4.82144s/100 iters), loss = 0.680384
I1123 18:01:18.804553 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 18:01:18.804553 16760 solver.cpp:237]     Train net output #1: loss = 0.680384 (* 1 = 0.680384 loss)
I1123 18:01:18.804553 16760 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1123 18:01:23.650365 16760 solver.cpp:218] Iteration 2800 (20.6364 iter/s, 4.8458s/100 iters), loss = 0.615063
I1123 18:01:23.650365 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 18:01:23.650365 16760 solver.cpp:237]     Train net output #1: loss = 0.615063 (* 1 = 0.615063 loss)
I1123 18:01:23.650365 16760 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1123 18:01:28.489359 16760 solver.cpp:218] Iteration 2900 (20.6682 iter/s, 4.83835s/100 iters), loss = 0.586543
I1123 18:01:28.489359 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 18:01:28.489359 16760 solver.cpp:237]     Train net output #1: loss = 0.586543 (* 1 = 0.586543 loss)
I1123 18:01:28.489359 16760 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1123 18:01:33.059854 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:01:33.248927 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_3000.caffemodel
I1123 18:01:33.258926 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_3000.solverstate
I1123 18:01:33.262926 16760 solver.cpp:330] Iteration 3000, Testing net (#0)
I1123 18:01:33.262926 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:01:34.525941 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:01:34.574942 16760 solver.cpp:397]     Test net output #0: accuracy = 0.6827
I1123 18:01:34.574942 16760 solver.cpp:397]     Test net output #1: loss = 0.940275 (* 1 = 0.940275 loss)
I1123 18:01:34.621456 16760 solver.cpp:218] Iteration 3000 (16.3095 iter/s, 6.1314s/100 iters), loss = 0.643928
I1123 18:01:34.621456 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 18:01:34.621456 16760 solver.cpp:237]     Train net output #1: loss = 0.643928 (* 1 = 0.643928 loss)
I1123 18:01:34.621456 16760 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1123 18:01:39.456734 16760 solver.cpp:218] Iteration 3100 (20.683 iter/s, 4.8349s/100 iters), loss = 0.538658
I1123 18:01:39.456734 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 18:01:39.456734 16760 solver.cpp:237]     Train net output #1: loss = 0.538658 (* 1 = 0.538658 loss)
I1123 18:01:39.456734 16760 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1123 18:01:44.328619 16760 solver.cpp:218] Iteration 3200 (20.5275 iter/s, 4.87151s/100 iters), loss = 0.593432
I1123 18:01:44.328619 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 18:01:44.328619 16760 solver.cpp:237]     Train net output #1: loss = 0.593432 (* 1 = 0.593432 loss)
I1123 18:01:44.328619 16760 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1123 18:01:49.121686 16760 solver.cpp:218] Iteration 3300 (20.8638 iter/s, 4.79299s/100 iters), loss = 0.632178
I1123 18:01:49.121686 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 18:01:49.121686 16760 solver.cpp:237]     Train net output #1: loss = 0.632178 (* 1 = 0.632178 loss)
I1123 18:01:49.121686 16760 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1123 18:01:53.921535 16760 solver.cpp:218] Iteration 3400 (20.8362 iter/s, 4.79934s/100 iters), loss = 0.686532
I1123 18:01:53.921535 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1123 18:01:53.921535 16760 solver.cpp:237]     Train net output #1: loss = 0.686532 (* 1 = 0.686532 loss)
I1123 18:01:53.921535 16760 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1123 18:01:58.541738 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:01:58.730814 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_3500.caffemodel
I1123 18:01:58.740798 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_3500.solverstate
I1123 18:01:58.744799 16760 solver.cpp:330] Iteration 3500, Testing net (#0)
I1123 18:01:58.744799 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:02:00.010171 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:02:00.060150 16760 solver.cpp:397]     Test net output #0: accuracy = 0.7022
I1123 18:02:00.060150 16760 solver.cpp:397]     Test net output #1: loss = 0.867354 (* 1 = 0.867354 loss)
I1123 18:02:00.106753 16760 solver.cpp:218] Iteration 3500 (16.1701 iter/s, 6.18427s/100 iters), loss = 0.659419
I1123 18:02:00.106753 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 18:02:00.106753 16760 solver.cpp:237]     Train net output #1: loss = 0.659419 (* 1 = 0.659419 loss)
I1123 18:02:00.106753 16760 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1123 18:02:04.977084 16760 solver.cpp:218] Iteration 3600 (20.5337 iter/s, 4.87005s/100 iters), loss = 0.645157
I1123 18:02:04.977084 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1123 18:02:04.977084 16760 solver.cpp:237]     Train net output #1: loss = 0.645157 (* 1 = 0.645157 loss)
I1123 18:02:04.977084 16760 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1123 18:02:09.842893 16760 solver.cpp:218] Iteration 3700 (20.5514 iter/s, 4.86584s/100 iters), loss = 0.518509
I1123 18:02:09.842893 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 18:02:09.842893 16760 solver.cpp:237]     Train net output #1: loss = 0.518509 (* 1 = 0.518509 loss)
I1123 18:02:09.842893 16760 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1123 18:02:14.640823 16760 solver.cpp:218] Iteration 3800 (20.8433 iter/s, 4.7977s/100 iters), loss = 0.750566
I1123 18:02:14.640823 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 18:02:14.640823 16760 solver.cpp:237]     Train net output #1: loss = 0.750566 (* 1 = 0.750566 loss)
I1123 18:02:14.640823 16760 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1123 18:02:19.453480 16760 solver.cpp:218] Iteration 3900 (20.7808 iter/s, 4.81213s/100 iters), loss = 0.669119
I1123 18:02:19.453480 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 18:02:19.453480 16760 solver.cpp:237]     Train net output #1: loss = 0.669119 (* 1 = 0.669119 loss)
I1123 18:02:19.453480 16760 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1123 18:02:24.020299 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:02:24.209650 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_4000.caffemodel
I1123 18:02:24.219630 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_4000.solverstate
I1123 18:02:24.223634 16760 solver.cpp:330] Iteration 4000, Testing net (#0)
I1123 18:02:24.223634 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:02:25.485847 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:02:25.535866 16760 solver.cpp:397]     Test net output #0: accuracy = 0.604
I1123 18:02:25.535866 16760 solver.cpp:397]     Test net output #1: loss = 1.17958 (* 1 = 1.17958 loss)
I1123 18:02:25.581883 16760 solver.cpp:218] Iteration 4000 (16.3193 iter/s, 6.12771s/100 iters), loss = 0.552595
I1123 18:02:25.581883 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 18:02:25.581883 16760 solver.cpp:237]     Train net output #1: loss = 0.552595 (* 1 = 0.552595 loss)
I1123 18:02:25.582384 16760 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1123 18:02:30.389088 16760 solver.cpp:218] Iteration 4100 (20.8056 iter/s, 4.8064s/100 iters), loss = 0.58573
I1123 18:02:30.389088 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 18:02:30.389088 16760 solver.cpp:237]     Train net output #1: loss = 0.58573 (* 1 = 0.58573 loss)
I1123 18:02:30.389088 16760 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1123 18:02:35.240231 16760 solver.cpp:218] Iteration 4200 (20.6128 iter/s, 4.85135s/100 iters), loss = 0.559413
I1123 18:02:35.240231 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 18:02:35.240231 16760 solver.cpp:237]     Train net output #1: loss = 0.559413 (* 1 = 0.559413 loss)
I1123 18:02:35.240231 16760 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1123 18:02:40.034461 16760 solver.cpp:218] Iteration 4300 (20.8601 iter/s, 4.79384s/100 iters), loss = 0.656811
I1123 18:02:40.034461 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 18:02:40.034461 16760 solver.cpp:237]     Train net output #1: loss = 0.656811 (* 1 = 0.656811 loss)
I1123 18:02:40.034461 16760 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1123 18:02:44.830193 16760 solver.cpp:218] Iteration 4400 (20.8553 iter/s, 4.79495s/100 iters), loss = 0.549745
I1123 18:02:44.830193 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 18:02:44.830193 16760 solver.cpp:237]     Train net output #1: loss = 0.549745 (* 1 = 0.549745 loss)
I1123 18:02:44.830193 16760 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1123 18:02:49.401211 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:02:49.589768 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_4500.caffemodel
I1123 18:02:49.600265 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_4500.solverstate
I1123 18:02:49.603766 16760 solver.cpp:330] Iteration 4500, Testing net (#0)
I1123 18:02:49.604765 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:02:50.867635 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:02:50.917567 16760 solver.cpp:397]     Test net output #0: accuracy = 0.7622
I1123 18:02:50.917567 16760 solver.cpp:397]     Test net output #1: loss = 0.701949 (* 1 = 0.701949 loss)
I1123 18:02:50.963560 16760 solver.cpp:218] Iteration 4500 (16.3037 iter/s, 6.13359s/100 iters), loss = 0.570512
I1123 18:02:50.964565 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 18:02:50.964565 16760 solver.cpp:237]     Train net output #1: loss = 0.570512 (* 1 = 0.570512 loss)
I1123 18:02:50.964565 16760 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1123 18:02:55.771793 16760 solver.cpp:218] Iteration 4600 (20.8024 iter/s, 4.80713s/100 iters), loss = 0.614136
I1123 18:02:55.771793 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 18:02:55.771793 16760 solver.cpp:237]     Train net output #1: loss = 0.614136 (* 1 = 0.614136 loss)
I1123 18:02:55.771793 16760 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1123 18:03:00.627946 16760 solver.cpp:218] Iteration 4700 (20.5916 iter/s, 4.85634s/100 iters), loss = 0.525243
I1123 18:03:00.627946 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 18:03:00.627946 16760 solver.cpp:237]     Train net output #1: loss = 0.525243 (* 1 = 0.525243 loss)
I1123 18:03:00.627946 16760 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1123 18:03:05.498008 16760 solver.cpp:218] Iteration 4800 (20.5378 iter/s, 4.86906s/100 iters), loss = 0.662002
I1123 18:03:05.498008 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 18:03:05.498008 16760 solver.cpp:237]     Train net output #1: loss = 0.662002 (* 1 = 0.662002 loss)
I1123 18:03:05.498008 16760 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1123 18:03:10.297070 16760 solver.cpp:218] Iteration 4900 (20.8399 iter/s, 4.79848s/100 iters), loss = 0.572573
I1123 18:03:10.297070 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 18:03:10.297070 16760 solver.cpp:237]     Train net output #1: loss = 0.572573 (* 1 = 0.572573 loss)
I1123 18:03:10.297070 16760 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1123 18:03:14.962618 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:03:15.150295 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_5000.caffemodel
I1123 18:03:15.164315 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_5000.solverstate
I1123 18:03:15.168298 16760 solver.cpp:330] Iteration 5000, Testing net (#0)
I1123 18:03:15.168298 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:03:16.432735 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:03:16.482720 16760 solver.cpp:397]     Test net output #0: accuracy = 0.7199
I1123 18:03:16.482720 16760 solver.cpp:397]     Test net output #1: loss = 0.847196 (* 1 = 0.847196 loss)
I1123 18:03:16.529263 16760 solver.cpp:218] Iteration 5000 (16.0469 iter/s, 6.23173s/100 iters), loss = 0.565164
I1123 18:03:16.529263 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 18:03:16.529263 16760 solver.cpp:237]     Train net output #1: loss = 0.565164 (* 1 = 0.565164 loss)
I1123 18:03:16.529263 16760 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1123 18:03:16.529263 16760 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1123 18:03:21.323575 16760 solver.cpp:218] Iteration 5100 (20.8579 iter/s, 4.79435s/100 iters), loss = 0.344028
I1123 18:03:21.323575 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:03:21.323575 16760 solver.cpp:237]     Train net output #1: loss = 0.344028 (* 1 = 0.344028 loss)
I1123 18:03:21.323575 16760 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1123 18:03:26.160517 16760 solver.cpp:218] Iteration 5200 (20.6761 iter/s, 4.8365s/100 iters), loss = 0.490036
I1123 18:03:26.160517 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 18:03:26.160517 16760 solver.cpp:237]     Train net output #1: loss = 0.490036 (* 1 = 0.490036 loss)
I1123 18:03:26.160517 16760 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1123 18:03:30.985816 16760 solver.cpp:218] Iteration 5300 (20.7261 iter/s, 4.82483s/100 iters), loss = 0.421653
I1123 18:03:30.985816 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 18:03:30.985816 16760 solver.cpp:237]     Train net output #1: loss = 0.421653 (* 1 = 0.421653 loss)
I1123 18:03:30.985816 16760 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1123 18:03:35.804951 16760 solver.cpp:218] Iteration 5400 (20.7518 iter/s, 4.81885s/100 iters), loss = 0.385693
I1123 18:03:35.804951 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 18:03:35.804951 16760 solver.cpp:237]     Train net output #1: loss = 0.385693 (* 1 = 0.385693 loss)
I1123 18:03:35.804951 16760 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1123 18:03:40.387488 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:03:40.575213 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_5500.caffemodel
I1123 18:03:40.584200 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_5500.solverstate
I1123 18:03:40.588198 16760 solver.cpp:330] Iteration 5500, Testing net (#0)
I1123 18:03:40.588198 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:03:41.849895 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:03:41.898908 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8518
I1123 18:03:41.898908 16760 solver.cpp:397]     Test net output #1: loss = 0.427239 (* 1 = 0.427239 loss)
I1123 18:03:41.945930 16760 solver.cpp:218] Iteration 5500 (16.2865 iter/s, 6.14004s/100 iters), loss = 0.350795
I1123 18:03:41.945930 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 18:03:41.945930 16760 solver.cpp:237]     Train net output #1: loss = 0.350795 (* 1 = 0.350795 loss)
I1123 18:03:41.945930 16760 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1123 18:03:46.742061 16760 solver.cpp:218] Iteration 5600 (20.8515 iter/s, 4.79582s/100 iters), loss = 0.333543
I1123 18:03:46.742061 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 18:03:46.742061 16760 solver.cpp:237]     Train net output #1: loss = 0.333543 (* 1 = 0.333543 loss)
I1123 18:03:46.742061 16760 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1123 18:03:51.540242 16760 solver.cpp:218] Iteration 5700 (20.8425 iter/s, 4.79788s/100 iters), loss = 0.356866
I1123 18:03:51.540242 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 18:03:51.540242 16760 solver.cpp:237]     Train net output #1: loss = 0.356866 (* 1 = 0.356866 loss)
I1123 18:03:51.540242 16760 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1123 18:03:56.368474 16760 solver.cpp:218] Iteration 5800 (20.7116 iter/s, 4.8282s/100 iters), loss = 0.403285
I1123 18:03:56.368474 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 18:03:56.368474 16760 solver.cpp:237]     Train net output #1: loss = 0.403285 (* 1 = 0.403285 loss)
I1123 18:03:56.368474 16760 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1123 18:04:01.240084 16760 solver.cpp:218] Iteration 5900 (20.5308 iter/s, 4.87073s/100 iters), loss = 0.328803
I1123 18:04:01.240084 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 18:04:01.240084 16760 solver.cpp:237]     Train net output #1: loss = 0.328803 (* 1 = 0.328803 loss)
I1123 18:04:01.240084 16760 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1123 18:04:05.867705 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:04:06.060986 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_6000.caffemodel
I1123 18:04:06.070986 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_6000.solverstate
I1123 18:04:06.074985 16760 solver.cpp:330] Iteration 6000, Testing net (#0)
I1123 18:04:06.074985 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:04:07.352396 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:04:07.402406 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8564
I1123 18:04:07.402406 16760 solver.cpp:397]     Test net output #1: loss = 0.419553 (* 1 = 0.419553 loss)
I1123 18:04:07.448436 16760 solver.cpp:218] Iteration 6000 (16.1067 iter/s, 6.20858s/100 iters), loss = 0.338466
I1123 18:04:07.448436 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:04:07.448436 16760 solver.cpp:237]     Train net output #1: loss = 0.338466 (* 1 = 0.338466 loss)
I1123 18:04:07.448436 16760 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1123 18:04:12.283748 16760 solver.cpp:218] Iteration 6100 (20.6839 iter/s, 4.83467s/100 iters), loss = 0.333708
I1123 18:04:12.283748 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 18:04:12.283748 16760 solver.cpp:237]     Train net output #1: loss = 0.333708 (* 1 = 0.333708 loss)
I1123 18:04:12.283748 16760 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1123 18:04:17.079319 16760 solver.cpp:218] Iteration 6200 (20.8547 iter/s, 4.79509s/100 iters), loss = 0.310783
I1123 18:04:17.079319 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:04:17.079319 16760 solver.cpp:237]     Train net output #1: loss = 0.310783 (* 1 = 0.310783 loss)
I1123 18:04:17.079319 16760 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1123 18:04:21.871588 16760 solver.cpp:218] Iteration 6300 (20.8693 iter/s, 4.79173s/100 iters), loss = 0.369479
I1123 18:04:21.871588 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 18:04:21.871588 16760 solver.cpp:237]     Train net output #1: loss = 0.369479 (* 1 = 0.369479 loss)
I1123 18:04:21.871588 16760 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1123 18:04:26.662567 16760 solver.cpp:218] Iteration 6400 (20.8719 iter/s, 4.79113s/100 iters), loss = 0.303962
I1123 18:04:26.662567 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:04:26.662567 16760 solver.cpp:237]     Train net output #1: loss = 0.303962 (* 1 = 0.303962 loss)
I1123 18:04:26.662567 16760 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1123 18:04:31.232915 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:04:31.420972 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_6500.caffemodel
I1123 18:04:31.430989 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_6500.solverstate
I1123 18:04:31.435992 16760 solver.cpp:330] Iteration 6500, Testing net (#0)
I1123 18:04:31.435992 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:04:32.702720 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:04:32.752339 16760 solver.cpp:397]     Test net output #0: accuracy = 0.862
I1123 18:04:32.752339 16760 solver.cpp:397]     Test net output #1: loss = 0.398331 (* 1 = 0.398331 loss)
I1123 18:04:32.798347 16760 solver.cpp:218] Iteration 6500 (16.2993 iter/s, 6.13524s/100 iters), loss = 0.35482
I1123 18:04:32.798347 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 18:04:32.798347 16760 solver.cpp:237]     Train net output #1: loss = 0.35482 (* 1 = 0.35482 loss)
I1123 18:04:32.798347 16760 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1123 18:04:37.607611 16760 solver.cpp:218] Iteration 6600 (20.7983 iter/s, 4.80807s/100 iters), loss = 0.278495
I1123 18:04:37.607611 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:04:37.607611 16760 solver.cpp:237]     Train net output #1: loss = 0.278495 (* 1 = 0.278495 loss)
I1123 18:04:37.607611 16760 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1123 18:04:42.413573 16760 solver.cpp:218] Iteration 6700 (20.808 iter/s, 4.80585s/100 iters), loss = 0.263739
I1123 18:04:42.413573 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:04:42.413573 16760 solver.cpp:237]     Train net output #1: loss = 0.263739 (* 1 = 0.263739 loss)
I1123 18:04:42.413573 16760 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1123 18:04:47.249724 16760 solver.cpp:218] Iteration 6800 (20.6774 iter/s, 4.8362s/100 iters), loss = 0.350691
I1123 18:04:47.249724 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:04:47.249724 16760 solver.cpp:237]     Train net output #1: loss = 0.350691 (* 1 = 0.350691 loss)
I1123 18:04:47.249724 16760 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1123 18:04:52.065335 16760 solver.cpp:218] Iteration 6900 (20.7673 iter/s, 4.81527s/100 iters), loss = 0.274486
I1123 18:04:52.065335 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 18:04:52.065335 16760 solver.cpp:237]     Train net output #1: loss = 0.274486 (* 1 = 0.274486 loss)
I1123 18:04:52.065335 16760 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1123 18:04:56.637583 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:04:56.826627 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_7000.caffemodel
I1123 18:04:56.836628 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_7000.solverstate
I1123 18:04:56.840697 16760 solver.cpp:330] Iteration 7000, Testing net (#0)
I1123 18:04:56.840697 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:04:58.108361 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:04:58.158890 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8661
I1123 18:04:58.158890 16760 solver.cpp:397]     Test net output #1: loss = 0.396436 (* 1 = 0.396436 loss)
I1123 18:04:58.205895 16760 solver.cpp:218] Iteration 7000 (16.2881 iter/s, 6.13944s/100 iters), loss = 0.276023
I1123 18:04:58.205895 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 18:04:58.205895 16760 solver.cpp:237]     Train net output #1: loss = 0.276023 (* 1 = 0.276023 loss)
I1123 18:04:58.205895 16760 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1123 18:05:03.013581 16760 solver.cpp:218] Iteration 7100 (20.7992 iter/s, 4.80788s/100 iters), loss = 0.277626
I1123 18:05:03.013581 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:05:03.013581 16760 solver.cpp:237]     Train net output #1: loss = 0.277626 (* 1 = 0.277626 loss)
I1123 18:05:03.013581 16760 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1123 18:05:07.823447 16760 solver.cpp:218] Iteration 7200 (20.7924 iter/s, 4.80946s/100 iters), loss = 0.280162
I1123 18:05:07.823447 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:05:07.823447 16760 solver.cpp:237]     Train net output #1: loss = 0.280162 (* 1 = 0.280162 loss)
I1123 18:05:07.823447 16760 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1123 18:05:12.631378 16760 solver.cpp:218] Iteration 7300 (20.8019 iter/s, 4.80724s/100 iters), loss = 0.286132
I1123 18:05:12.631378 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:05:12.631378 16760 solver.cpp:237]     Train net output #1: loss = 0.286132 (* 1 = 0.286132 loss)
I1123 18:05:12.631378 16760 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1123 18:05:17.437976 16760 solver.cpp:218] Iteration 7400 (20.8044 iter/s, 4.80667s/100 iters), loss = 0.325796
I1123 18:05:17.437976 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:05:17.437976 16760 solver.cpp:237]     Train net output #1: loss = 0.325796 (* 1 = 0.325796 loss)
I1123 18:05:17.437976 16760 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1123 18:05:22.008208 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:05:22.198287 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_7500.caffemodel
I1123 18:05:22.208295 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_7500.solverstate
I1123 18:05:22.212298 16760 solver.cpp:330] Iteration 7500, Testing net (#0)
I1123 18:05:22.212298 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:05:23.479317 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:05:23.529320 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8647
I1123 18:05:23.529320 16760 solver.cpp:397]     Test net output #1: loss = 0.392944 (* 1 = 0.392944 loss)
I1123 18:05:23.575335 16760 solver.cpp:218] Iteration 7500 (16.2957 iter/s, 6.13657s/100 iters), loss = 0.354186
I1123 18:05:23.575335 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 18:05:23.575335 16760 solver.cpp:237]     Train net output #1: loss = 0.354186 (* 1 = 0.354186 loss)
I1123 18:05:23.575335 16760 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1123 18:05:28.381255 16760 solver.cpp:218] Iteration 7600 (20.8108 iter/s, 4.80519s/100 iters), loss = 0.250703
I1123 18:05:28.381255 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:05:28.381255 16760 solver.cpp:237]     Train net output #1: loss = 0.250703 (* 1 = 0.250703 loss)
I1123 18:05:28.381255 16760 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1123 18:05:33.184773 16760 solver.cpp:218] Iteration 7700 (20.8197 iter/s, 4.80315s/100 iters), loss = 0.272117
I1123 18:05:33.184773 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:05:33.184773 16760 solver.cpp:237]     Train net output #1: loss = 0.272117 (* 1 = 0.272117 loss)
I1123 18:05:33.184773 16760 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1123 18:05:37.990267 16760 solver.cpp:218] Iteration 7800 (20.8109 iter/s, 4.80516s/100 iters), loss = 0.381664
I1123 18:05:37.990267 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 18:05:37.990267 16760 solver.cpp:237]     Train net output #1: loss = 0.381664 (* 1 = 0.381664 loss)
I1123 18:05:37.990267 16760 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1123 18:05:42.797410 16760 solver.cpp:218] Iteration 7900 (20.8048 iter/s, 4.80658s/100 iters), loss = 0.245706
I1123 18:05:42.797410 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:05:42.797410 16760 solver.cpp:237]     Train net output #1: loss = 0.245706 (* 1 = 0.245706 loss)
I1123 18:05:42.797410 16760 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1123 18:05:47.370118 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:05:47.559767 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_8000.caffemodel
I1123 18:05:47.569762 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_8000.solverstate
I1123 18:05:47.573761 16760 solver.cpp:330] Iteration 8000, Testing net (#0)
I1123 18:05:47.573761 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:05:48.840580 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:05:48.890584 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8665
I1123 18:05:48.890584 16760 solver.cpp:397]     Test net output #1: loss = 0.387495 (* 1 = 0.387495 loss)
I1123 18:05:48.937585 16760 solver.cpp:218] Iteration 8000 (16.2873 iter/s, 6.13975s/100 iters), loss = 0.26413
I1123 18:05:48.937585 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:05:48.937585 16760 solver.cpp:237]     Train net output #1: loss = 0.26413 (* 1 = 0.26413 loss)
I1123 18:05:48.937585 16760 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1123 18:05:53.739544 16760 solver.cpp:218] Iteration 8100 (20.824 iter/s, 4.80215s/100 iters), loss = 0.278016
I1123 18:05:53.739544 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:05:53.739544 16760 solver.cpp:237]     Train net output #1: loss = 0.278016 (* 1 = 0.278016 loss)
I1123 18:05:53.739544 16760 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1123 18:05:58.543978 16760 solver.cpp:218] Iteration 8200 (20.8166 iter/s, 4.80386s/100 iters), loss = 0.222724
I1123 18:05:58.543978 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 18:05:58.543978 16760 solver.cpp:237]     Train net output #1: loss = 0.222724 (* 1 = 0.222724 loss)
I1123 18:05:58.543978 16760 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1123 18:06:03.350075 16760 solver.cpp:218] Iteration 8300 (20.8073 iter/s, 4.80601s/100 iters), loss = 0.314653
I1123 18:06:03.350075 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 18:06:03.350075 16760 solver.cpp:237]     Train net output #1: loss = 0.314653 (* 1 = 0.314653 loss)
I1123 18:06:03.350075 16760 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1123 18:06:08.159369 16760 solver.cpp:218] Iteration 8400 (20.7961 iter/s, 4.8086s/100 iters), loss = 0.335785
I1123 18:06:08.159369 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:06:08.159369 16760 solver.cpp:237]     Train net output #1: loss = 0.335785 (* 1 = 0.335785 loss)
I1123 18:06:08.159369 16760 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1123 18:06:12.731979 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:06:12.921525 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_8500.caffemodel
I1123 18:06:12.933040 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_8500.solverstate
I1123 18:06:12.937055 16760 solver.cpp:330] Iteration 8500, Testing net (#0)
I1123 18:06:12.937055 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:06:14.203653 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:06:14.253672 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8607
I1123 18:06:14.253672 16760 solver.cpp:397]     Test net output #1: loss = 0.397798 (* 1 = 0.397798 loss)
I1123 18:06:14.299685 16760 solver.cpp:218] Iteration 8500 (16.286 iter/s, 6.14025s/100 iters), loss = 0.292114
I1123 18:06:14.299685 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:06:14.299685 16760 solver.cpp:237]     Train net output #1: loss = 0.292114 (* 1 = 0.292114 loss)
I1123 18:06:14.299685 16760 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1123 18:06:19.108407 16760 solver.cpp:218] Iteration 8600 (20.8004 iter/s, 4.80761s/100 iters), loss = 0.234202
I1123 18:06:19.108407 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:06:19.108407 16760 solver.cpp:237]     Train net output #1: loss = 0.234202 (* 1 = 0.234202 loss)
I1123 18:06:19.108407 16760 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1123 18:06:23.917466 16760 solver.cpp:218] Iteration 8700 (20.7935 iter/s, 4.8092s/100 iters), loss = 0.237453
I1123 18:06:23.917466 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:06:23.917466 16760 solver.cpp:237]     Train net output #1: loss = 0.237453 (* 1 = 0.237453 loss)
I1123 18:06:23.917466 16760 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1123 18:06:28.728055 16760 solver.cpp:218] Iteration 8800 (20.7909 iter/s, 4.80979s/100 iters), loss = 0.289963
I1123 18:06:28.728055 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 18:06:28.728055 16760 solver.cpp:237]     Train net output #1: loss = 0.289963 (* 1 = 0.289963 loss)
I1123 18:06:28.728055 16760 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1123 18:06:33.542181 16760 solver.cpp:218] Iteration 8900 (20.7735 iter/s, 4.81382s/100 iters), loss = 0.269454
I1123 18:06:33.542181 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:06:33.542682 16760 solver.cpp:237]     Train net output #1: loss = 0.269454 (* 1 = 0.269454 loss)
I1123 18:06:33.542682 16760 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1123 18:06:38.108341 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:06:38.297402 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_9000.caffemodel
I1123 18:06:38.307387 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_9000.solverstate
I1123 18:06:38.311389 16760 solver.cpp:330] Iteration 9000, Testing net (#0)
I1123 18:06:38.311389 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:06:39.577275 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:06:39.627255 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8581
I1123 18:06:39.628255 16760 solver.cpp:397]     Test net output #1: loss = 0.415781 (* 1 = 0.415781 loss)
I1123 18:06:39.673872 16760 solver.cpp:218] Iteration 9000 (16.3087 iter/s, 6.1317s/100 iters), loss = 0.29622
I1123 18:06:39.673872 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 18:06:39.673872 16760 solver.cpp:237]     Train net output #1: loss = 0.29622 (* 1 = 0.29622 loss)
I1123 18:06:39.673872 16760 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1123 18:06:44.484633 16760 solver.cpp:218] Iteration 9100 (20.7921 iter/s, 4.80951s/100 iters), loss = 0.266426
I1123 18:06:44.484633 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:06:44.484633 16760 solver.cpp:237]     Train net output #1: loss = 0.266425 (* 1 = 0.266425 loss)
I1123 18:06:44.484633 16760 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1123 18:06:49.289554 16760 solver.cpp:218] Iteration 9200 (20.8126 iter/s, 4.80479s/100 iters), loss = 0.286513
I1123 18:06:49.289554 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 18:06:49.289554 16760 solver.cpp:237]     Train net output #1: loss = 0.286513 (* 1 = 0.286513 loss)
I1123 18:06:49.289554 16760 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1123 18:06:54.105618 16760 solver.cpp:218] Iteration 9300 (20.7671 iter/s, 4.8153s/100 iters), loss = 0.286381
I1123 18:06:54.105618 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 18:06:54.105618 16760 solver.cpp:237]     Train net output #1: loss = 0.286381 (* 1 = 0.286381 loss)
I1123 18:06:54.105618 16760 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1123 18:06:58.912453 16760 solver.cpp:218] Iteration 9400 (20.8029 iter/s, 4.80703s/100 iters), loss = 0.272912
I1123 18:06:58.912453 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:06:58.912453 16760 solver.cpp:237]     Train net output #1: loss = 0.272912 (* 1 = 0.272912 loss)
I1123 18:06:58.912453 16760 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1123 18:07:03.485316 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:07:03.673611 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_9500.caffemodel
I1123 18:07:03.686611 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_9500.solverstate
I1123 18:07:03.690611 16760 solver.cpp:330] Iteration 9500, Testing net (#0)
I1123 18:07:03.690611 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:07:04.958567 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:07:05.009081 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8601
I1123 18:07:05.009081 16760 solver.cpp:397]     Test net output #1: loss = 0.408281 (* 1 = 0.408281 loss)
I1123 18:07:05.055599 16760 solver.cpp:218] Iteration 9500 (16.2808 iter/s, 6.14221s/100 iters), loss = 0.2521
I1123 18:07:05.055599 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:07:05.055599 16760 solver.cpp:237]     Train net output #1: loss = 0.2521 (* 1 = 0.2521 loss)
I1123 18:07:05.055599 16760 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1123 18:07:05.055599 16760 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1123 18:07:09.856695 16760 solver.cpp:218] Iteration 9600 (20.8289 iter/s, 4.80101s/100 iters), loss = 0.221506
I1123 18:07:09.857194 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:07:09.857194 16760 solver.cpp:237]     Train net output #1: loss = 0.221506 (* 1 = 0.221506 loss)
I1123 18:07:09.857194 16760 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1123 18:07:14.661197 16760 solver.cpp:218] Iteration 9700 (20.8164 iter/s, 4.8039s/100 iters), loss = 0.250694
I1123 18:07:14.661197 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:07:14.661197 16760 solver.cpp:237]     Train net output #1: loss = 0.250694 (* 1 = 0.250694 loss)
I1123 18:07:14.661197 16760 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1123 18:07:19.466631 16760 solver.cpp:218] Iteration 9800 (20.811 iter/s, 4.80515s/100 iters), loss = 0.279224
I1123 18:07:19.466631 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 18:07:19.466631 16760 solver.cpp:237]     Train net output #1: loss = 0.279224 (* 1 = 0.279224 loss)
I1123 18:07:19.466631 16760 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1123 18:07:24.273772 16760 solver.cpp:218] Iteration 9900 (20.8042 iter/s, 4.80672s/100 iters), loss = 0.239334
I1123 18:07:24.273772 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:07:24.273772 16760 solver.cpp:237]     Train net output #1: loss = 0.239334 (* 1 = 0.239334 loss)
I1123 18:07:24.273772 16760 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1123 18:07:28.847323 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:07:29.035399 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_10000.caffemodel
I1123 18:07:29.046404 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_10000.solverstate
I1123 18:07:29.050397 16760 solver.cpp:330] Iteration 10000, Testing net (#0)
I1123 18:07:29.050397 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:07:30.317495 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:07:30.368002 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8818
I1123 18:07:30.368002 16760 solver.cpp:397]     Test net output #1: loss = 0.34696 (* 1 = 0.34696 loss)
I1123 18:07:30.414526 16760 solver.cpp:218] Iteration 10000 (16.2865 iter/s, 6.14004s/100 iters), loss = 0.291793
I1123 18:07:30.414526 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 18:07:30.414526 16760 solver.cpp:237]     Train net output #1: loss = 0.291793 (* 1 = 0.291793 loss)
I1123 18:07:30.414526 16760 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1123 18:07:35.221278 16760 solver.cpp:218] Iteration 10100 (20.8018 iter/s, 4.80728s/100 iters), loss = 0.247648
I1123 18:07:35.222281 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:07:35.222281 16760 solver.cpp:237]     Train net output #1: loss = 0.247648 (* 1 = 0.247648 loss)
I1123 18:07:35.222281 16760 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1123 18:07:40.028200 16760 solver.cpp:218] Iteration 10200 (20.8056 iter/s, 4.80639s/100 iters), loss = 0.264707
I1123 18:07:40.028200 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:07:40.028200 16760 solver.cpp:237]     Train net output #1: loss = 0.264707 (* 1 = 0.264707 loss)
I1123 18:07:40.028200 16760 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1123 18:07:44.835222 16760 solver.cpp:218] Iteration 10300 (20.8074 iter/s, 4.80598s/100 iters), loss = 0.233397
I1123 18:07:44.835222 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:07:44.835222 16760 solver.cpp:237]     Train net output #1: loss = 0.233397 (* 1 = 0.233397 loss)
I1123 18:07:44.835222 16760 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1123 18:07:49.640250 16760 solver.cpp:218] Iteration 10400 (20.8105 iter/s, 4.80526s/100 iters), loss = 0.195174
I1123 18:07:49.640250 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:07:49.640250 16760 solver.cpp:237]     Train net output #1: loss = 0.195174 (* 1 = 0.195174 loss)
I1123 18:07:49.641250 16760 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1123 18:07:54.213793 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:07:54.403822 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_10500.caffemodel
I1123 18:07:54.413826 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_10500.solverstate
I1123 18:07:54.417840 16760 solver.cpp:330] Iteration 10500, Testing net (#0)
I1123 18:07:54.417840 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:07:55.684092 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:07:55.734115 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8827
I1123 18:07:55.734115 16760 solver.cpp:397]     Test net output #1: loss = 0.343595 (* 1 = 0.343595 loss)
I1123 18:07:55.781123 16760 solver.cpp:218] Iteration 10500 (16.2877 iter/s, 6.13959s/100 iters), loss = 0.220411
I1123 18:07:55.781123 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:07:55.781123 16760 solver.cpp:237]     Train net output #1: loss = 0.220411 (* 1 = 0.220411 loss)
I1123 18:07:55.781123 16760 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1123 18:08:00.589351 16760 solver.cpp:218] Iteration 10600 (20.7986 iter/s, 4.80802s/100 iters), loss = 0.190085
I1123 18:08:00.589351 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:08:00.589351 16760 solver.cpp:237]     Train net output #1: loss = 0.190085 (* 1 = 0.190085 loss)
I1123 18:08:00.589351 16760 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1123 18:08:05.390312 16760 solver.cpp:218] Iteration 10700 (20.8284 iter/s, 4.80115s/100 iters), loss = 0.214671
I1123 18:08:05.390312 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:08:05.391312 16760 solver.cpp:237]     Train net output #1: loss = 0.214671 (* 1 = 0.214671 loss)
I1123 18:08:05.391312 16760 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1123 18:08:10.198873 16760 solver.cpp:218] Iteration 10800 (20.7987 iter/s, 4.80799s/100 iters), loss = 0.254346
I1123 18:08:10.198873 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:08:10.198873 16760 solver.cpp:237]     Train net output #1: loss = 0.254346 (* 1 = 0.254346 loss)
I1123 18:08:10.198873 16760 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1123 18:08:15.008251 16760 solver.cpp:218] Iteration 10900 (20.7968 iter/s, 4.80843s/100 iters), loss = 0.194515
I1123 18:08:15.008251 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:08:15.008251 16760 solver.cpp:237]     Train net output #1: loss = 0.194515 (* 1 = 0.194515 loss)
I1123 18:08:15.008251 16760 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1123 18:08:19.580943 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:08:19.769682 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_11000.caffemodel
I1123 18:08:19.779680 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_11000.solverstate
I1123 18:08:19.784682 16760 solver.cpp:330] Iteration 11000, Testing net (#0)
I1123 18:08:19.784682 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:08:21.051352 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:08:21.100375 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8829
I1123 18:08:21.100375 16760 solver.cpp:397]     Test net output #1: loss = 0.342331 (* 1 = 0.342331 loss)
I1123 18:08:21.146419 16760 solver.cpp:218] Iteration 11000 (16.291 iter/s, 6.13835s/100 iters), loss = 0.228844
I1123 18:08:21.146419 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:08:21.146419 16760 solver.cpp:237]     Train net output #1: loss = 0.228844 (* 1 = 0.228844 loss)
I1123 18:08:21.146419 16760 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1123 18:08:25.957022 16760 solver.cpp:218] Iteration 11100 (20.7919 iter/s, 4.80957s/100 iters), loss = 0.219094
I1123 18:08:25.957022 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:08:25.957022 16760 solver.cpp:237]     Train net output #1: loss = 0.219094 (* 1 = 0.219094 loss)
I1123 18:08:25.957022 16760 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1123 18:08:30.762452 16760 solver.cpp:218] Iteration 11200 (20.8113 iter/s, 4.80509s/100 iters), loss = 0.233387
I1123 18:08:30.762452 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:08:30.762452 16760 solver.cpp:237]     Train net output #1: loss = 0.233387 (* 1 = 0.233387 loss)
I1123 18:08:30.762452 16760 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1123 18:08:35.576586 16760 solver.cpp:218] Iteration 11300 (20.7752 iter/s, 4.81343s/100 iters), loss = 0.240577
I1123 18:08:35.576586 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:08:35.576586 16760 solver.cpp:237]     Train net output #1: loss = 0.240577 (* 1 = 0.240577 loss)
I1123 18:08:35.576586 16760 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1123 18:08:40.381029 16760 solver.cpp:218] Iteration 11400 (20.8118 iter/s, 4.80497s/100 iters), loss = 0.214132
I1123 18:08:40.382040 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:08:40.382040 16760 solver.cpp:237]     Train net output #1: loss = 0.214132 (* 1 = 0.214132 loss)
I1123 18:08:40.382040 16760 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1123 18:08:44.950614 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:08:45.139396 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_11500.caffemodel
I1123 18:08:45.149395 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_11500.solverstate
I1123 18:08:45.153395 16760 solver.cpp:330] Iteration 11500, Testing net (#0)
I1123 18:08:45.153395 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:08:46.419387 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:08:46.470052 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8832
I1123 18:08:46.470052 16760 solver.cpp:397]     Test net output #1: loss = 0.343833 (* 1 = 0.343833 loss)
I1123 18:08:46.516064 16760 solver.cpp:218] Iteration 11500 (16.3028 iter/s, 6.13392s/100 iters), loss = 0.247005
I1123 18:08:46.516064 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:08:46.516064 16760 solver.cpp:237]     Train net output #1: loss = 0.247005 (* 1 = 0.247005 loss)
I1123 18:08:46.516064 16760 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1123 18:08:51.324468 16760 solver.cpp:218] Iteration 11600 (20.7989 iter/s, 4.80795s/100 iters), loss = 0.188054
I1123 18:08:51.324468 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:08:51.324468 16760 solver.cpp:237]     Train net output #1: loss = 0.188054 (* 1 = 0.188054 loss)
I1123 18:08:51.324468 16760 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1123 18:08:56.128363 16760 solver.cpp:218] Iteration 11700 (20.8173 iter/s, 4.8037s/100 iters), loss = 0.218721
I1123 18:08:56.128363 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:08:56.128363 16760 solver.cpp:237]     Train net output #1: loss = 0.218721 (* 1 = 0.218721 loss)
I1123 18:08:56.128363 16760 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1123 18:09:00.935449 16760 solver.cpp:218] Iteration 11800 (20.8042 iter/s, 4.80671s/100 iters), loss = 0.26213
I1123 18:09:00.935449 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 18:09:00.935449 16760 solver.cpp:237]     Train net output #1: loss = 0.26213 (* 1 = 0.26213 loss)
I1123 18:09:00.935449 16760 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1123 18:09:05.738788 16760 solver.cpp:218] Iteration 11900 (20.8204 iter/s, 4.80298s/100 iters), loss = 0.175931
I1123 18:09:05.739289 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:09:05.739289 16760 solver.cpp:237]     Train net output #1: loss = 0.175931 (* 1 = 0.175931 loss)
I1123 18:09:05.739289 16760 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1123 18:09:10.307132 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:09:10.496194 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_12000.caffemodel
I1123 18:09:10.508193 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_12000.solverstate
I1123 18:09:10.512195 16760 solver.cpp:330] Iteration 12000, Testing net (#0)
I1123 18:09:10.512195 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:09:11.779048 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:09:11.830037 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8837
I1123 18:09:11.830037 16760 solver.cpp:397]     Test net output #1: loss = 0.342985 (* 1 = 0.342985 loss)
I1123 18:09:11.875675 16760 solver.cpp:218] Iteration 12000 (16.2949 iter/s, 6.13687s/100 iters), loss = 0.227316
I1123 18:09:11.875675 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:09:11.875675 16760 solver.cpp:237]     Train net output #1: loss = 0.227316 (* 1 = 0.227316 loss)
I1123 18:09:11.875675 16760 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1123 18:09:16.680642 16760 solver.cpp:218] Iteration 12100 (20.8132 iter/s, 4.80465s/100 iters), loss = 0.219529
I1123 18:09:16.681643 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:09:16.681643 16760 solver.cpp:237]     Train net output #1: loss = 0.219529 (* 1 = 0.219529 loss)
I1123 18:09:16.681643 16760 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1123 18:09:21.486655 16760 solver.cpp:218] Iteration 12200 (20.8117 iter/s, 4.80498s/100 iters), loss = 0.214175
I1123 18:09:21.486655 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:09:21.486655 16760 solver.cpp:237]     Train net output #1: loss = 0.214175 (* 1 = 0.214175 loss)
I1123 18:09:21.486655 16760 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1123 18:09:26.292470 16760 solver.cpp:218] Iteration 12300 (20.8112 iter/s, 4.80511s/100 iters), loss = 0.210793
I1123 18:09:26.292470 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:09:26.292470 16760 solver.cpp:237]     Train net output #1: loss = 0.210793 (* 1 = 0.210793 loss)
I1123 18:09:26.292470 16760 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1123 18:09:31.097616 16760 solver.cpp:218] Iteration 12400 (20.8121 iter/s, 4.8049s/100 iters), loss = 0.228308
I1123 18:09:31.097616 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:09:31.097616 16760 solver.cpp:237]     Train net output #1: loss = 0.228308 (* 1 = 0.228308 loss)
I1123 18:09:31.097616 16760 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1123 18:09:35.668581 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:09:35.857648 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_12500.caffemodel
I1123 18:09:35.868144 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_12500.solverstate
I1123 18:09:35.872146 16760 solver.cpp:330] Iteration 12500, Testing net (#0)
I1123 18:09:35.872146 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:09:37.139035 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:09:37.189061 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8834
I1123 18:09:37.189061 16760 solver.cpp:397]     Test net output #1: loss = 0.341788 (* 1 = 0.341788 loss)
I1123 18:09:37.235066 16760 solver.cpp:218] Iteration 12500 (16.2933 iter/s, 6.13749s/100 iters), loss = 0.217786
I1123 18:09:37.235066 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:09:37.235066 16760 solver.cpp:237]     Train net output #1: loss = 0.217786 (* 1 = 0.217786 loss)
I1123 18:09:37.235066 16760 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1123 18:09:42.044603 16760 solver.cpp:218] Iteration 12600 (20.796 iter/s, 4.80863s/100 iters), loss = 0.225906
I1123 18:09:42.044603 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:09:42.044603 16760 solver.cpp:237]     Train net output #1: loss = 0.225906 (* 1 = 0.225906 loss)
I1123 18:09:42.044603 16760 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1123 18:09:46.855140 16760 solver.cpp:218] Iteration 12700 (20.789 iter/s, 4.81024s/100 iters), loss = 0.247002
I1123 18:09:46.855655 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:09:46.855655 16760 solver.cpp:237]     Train net output #1: loss = 0.247002 (* 1 = 0.247002 loss)
I1123 18:09:46.855655 16760 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1123 18:09:51.660861 16760 solver.cpp:218] Iteration 12800 (20.8112 iter/s, 4.8051s/100 iters), loss = 0.209726
I1123 18:09:51.660861 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:09:51.660861 16760 solver.cpp:237]     Train net output #1: loss = 0.209726 (* 1 = 0.209726 loss)
I1123 18:09:51.660861 16760 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1123 18:09:56.467869 16760 solver.cpp:218] Iteration 12900 (20.8039 iter/s, 4.8068s/100 iters), loss = 0.194993
I1123 18:09:56.468371 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:09:56.468371 16760 solver.cpp:237]     Train net output #1: loss = 0.194993 (* 1 = 0.194993 loss)
I1123 18:09:56.468371 16760 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1123 18:10:01.060637 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:10:01.256327 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_13000.caffemodel
I1123 18:10:01.267326 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_13000.solverstate
I1123 18:10:01.272325 16760 solver.cpp:330] Iteration 13000, Testing net (#0)
I1123 18:10:01.272325 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:10:02.543247 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:10:02.593245 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8819
I1123 18:10:02.593245 16760 solver.cpp:397]     Test net output #1: loss = 0.340217 (* 1 = 0.340217 loss)
I1123 18:10:02.639952 16760 solver.cpp:218] Iteration 13000 (16.2037 iter/s, 6.17143s/100 iters), loss = 0.231185
I1123 18:10:02.639952 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:10:02.639952 16760 solver.cpp:237]     Train net output #1: loss = 0.231185 (* 1 = 0.231185 loss)
I1123 18:10:02.639952 16760 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1123 18:10:07.448246 16760 solver.cpp:218] Iteration 13100 (20.7975 iter/s, 4.80828s/100 iters), loss = 0.220559
I1123 18:10:07.448246 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:10:07.448246 16760 solver.cpp:237]     Train net output #1: loss = 0.22056 (* 1 = 0.22056 loss)
I1123 18:10:07.448246 16760 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1123 18:10:12.257408 16760 solver.cpp:218] Iteration 13200 (20.7962 iter/s, 4.80857s/100 iters), loss = 0.184097
I1123 18:10:12.257408 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:10:12.257408 16760 solver.cpp:237]     Train net output #1: loss = 0.184097 (* 1 = 0.184097 loss)
I1123 18:10:12.257408 16760 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1123 18:10:17.062944 16760 solver.cpp:218] Iteration 13300 (20.8127 iter/s, 4.80475s/100 iters), loss = 0.242943
I1123 18:10:17.062944 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:10:17.062944 16760 solver.cpp:237]     Train net output #1: loss = 0.242943 (* 1 = 0.242943 loss)
I1123 18:10:17.062944 16760 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1123 18:10:21.865695 16760 solver.cpp:218] Iteration 13400 (20.8198 iter/s, 4.80312s/100 iters), loss = 0.192116
I1123 18:10:21.865695 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:10:21.865695 16760 solver.cpp:237]     Train net output #1: loss = 0.192116 (* 1 = 0.192116 loss)
I1123 18:10:21.865695 16760 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1123 18:10:26.434671 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:10:26.622776 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_13500.caffemodel
I1123 18:10:26.633800 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_13500.solverstate
I1123 18:10:26.638299 16760 solver.cpp:330] Iteration 13500, Testing net (#0)
I1123 18:10:26.638299 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:10:27.904443 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:10:27.954478 16760 solver.cpp:397]     Test net output #0: accuracy = 0.885
I1123 18:10:27.954478 16760 solver.cpp:397]     Test net output #1: loss = 0.339493 (* 1 = 0.339493 loss)
I1123 18:10:28.001478 16760 solver.cpp:218] Iteration 13500 (16.3011 iter/s, 6.13456s/100 iters), loss = 0.207257
I1123 18:10:28.001478 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:10:28.001478 16760 solver.cpp:237]     Train net output #1: loss = 0.207257 (* 1 = 0.207257 loss)
I1123 18:10:28.001478 16760 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1123 18:10:32.808310 16760 solver.cpp:218] Iteration 13600 (20.8045 iter/s, 4.80666s/100 iters), loss = 0.203616
I1123 18:10:32.808310 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:10:32.808310 16760 solver.cpp:237]     Train net output #1: loss = 0.203616 (* 1 = 0.203616 loss)
I1123 18:10:32.808310 16760 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1123 18:10:37.619726 16760 solver.cpp:218] Iteration 13700 (20.7844 iter/s, 4.81131s/100 iters), loss = 0.188947
I1123 18:10:37.619726 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 18:10:37.619726 16760 solver.cpp:237]     Train net output #1: loss = 0.188948 (* 1 = 0.188948 loss)
I1123 18:10:37.619726 16760 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1123 18:10:42.425846 16760 solver.cpp:218] Iteration 13800 (20.8064 iter/s, 4.80621s/100 iters), loss = 0.245177
I1123 18:10:42.426864 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:10:42.426864 16760 solver.cpp:237]     Train net output #1: loss = 0.245177 (* 1 = 0.245177 loss)
I1123 18:10:42.426864 16760 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1123 18:10:47.233774 16760 solver.cpp:218] Iteration 13900 (20.8048 iter/s, 4.80658s/100 iters), loss = 0.183201
I1123 18:10:47.233774 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:10:47.233774 16760 solver.cpp:237]     Train net output #1: loss = 0.183201 (* 1 = 0.183201 loss)
I1123 18:10:47.233774 16760 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1123 18:10:51.807250 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:10:51.996317 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_14000.caffemodel
I1123 18:10:52.006297 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_14000.solverstate
I1123 18:10:52.010298 16760 solver.cpp:330] Iteration 14000, Testing net (#0)
I1123 18:10:52.010298 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:10:53.278102 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:10:53.328102 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8849
I1123 18:10:53.328102 16760 solver.cpp:397]     Test net output #1: loss = 0.340569 (* 1 = 0.340569 loss)
I1123 18:10:53.374135 16760 solver.cpp:218] Iteration 14000 (16.2854 iter/s, 6.14047s/100 iters), loss = 0.248772
I1123 18:10:53.374135 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 18:10:53.374135 16760 solver.cpp:237]     Train net output #1: loss = 0.248773 (* 1 = 0.248773 loss)
I1123 18:10:53.374135 16760 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1123 18:10:58.184196 16760 solver.cpp:218] Iteration 14100 (20.7931 iter/s, 4.80929s/100 iters), loss = 0.200884
I1123 18:10:58.184196 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:10:58.184196 16760 solver.cpp:237]     Train net output #1: loss = 0.200884 (* 1 = 0.200884 loss)
I1123 18:10:58.184196 16760 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1123 18:11:02.993547 16760 solver.cpp:218] Iteration 14200 (20.795 iter/s, 4.80885s/100 iters), loss = 0.196058
I1123 18:11:02.993547 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:11:02.993547 16760 solver.cpp:237]     Train net output #1: loss = 0.196058 (* 1 = 0.196058 loss)
I1123 18:11:02.993547 16760 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1123 18:11:07.804332 16760 solver.cpp:218] Iteration 14300 (20.7861 iter/s, 4.81091s/100 iters), loss = 0.223584
I1123 18:11:07.804332 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:11:07.804332 16760 solver.cpp:237]     Train net output #1: loss = 0.223584 (* 1 = 0.223584 loss)
I1123 18:11:07.804332 16760 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1123 18:11:12.614437 16760 solver.cpp:218] Iteration 14400 (20.7937 iter/s, 4.80914s/100 iters), loss = 0.126255
I1123 18:11:12.614437 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 18:11:12.614437 16760 solver.cpp:237]     Train net output #1: loss = 0.126255 (* 1 = 0.126255 loss)
I1123 18:11:12.614437 16760 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1123 18:11:17.186673 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:11:17.375308 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_14500.caffemodel
I1123 18:11:17.385305 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_14500.solverstate
I1123 18:11:17.389327 16760 solver.cpp:330] Iteration 14500, Testing net (#0)
I1123 18:11:17.389832 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:11:18.657220 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:11:18.707721 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8847
I1123 18:11:18.707721 16760 solver.cpp:397]     Test net output #1: loss = 0.340388 (* 1 = 0.340388 loss)
I1123 18:11:18.753741 16760 solver.cpp:218] Iteration 14500 (16.2889 iter/s, 6.13916s/100 iters), loss = 0.238987
I1123 18:11:18.753741 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:11:18.753741 16760 solver.cpp:237]     Train net output #1: loss = 0.238987 (* 1 = 0.238987 loss)
I1123 18:11:18.753741 16760 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1123 18:11:23.558666 16760 solver.cpp:218] Iteration 14600 (20.8119 iter/s, 4.80494s/100 iters), loss = 0.184555
I1123 18:11:23.559670 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:11:23.559670 16760 solver.cpp:237]     Train net output #1: loss = 0.184555 (* 1 = 0.184555 loss)
I1123 18:11:23.559670 16760 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1123 18:11:28.362576 16760 solver.cpp:218] Iteration 14700 (20.8209 iter/s, 4.80288s/100 iters), loss = 0.192999
I1123 18:11:28.362576 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:11:28.362576 16760 solver.cpp:237]     Train net output #1: loss = 0.192999 (* 1 = 0.192999 loss)
I1123 18:11:28.362576 16760 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1123 18:11:33.167340 16760 solver.cpp:218] Iteration 14800 (20.8115 iter/s, 4.80503s/100 iters), loss = 0.208989
I1123 18:11:33.168344 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:11:33.168344 16760 solver.cpp:237]     Train net output #1: loss = 0.20899 (* 1 = 0.20899 loss)
I1123 18:11:33.168344 16760 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1123 18:11:37.971982 16760 solver.cpp:218] Iteration 14900 (20.8171 iter/s, 4.80375s/100 iters), loss = 0.21443
I1123 18:11:37.971982 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:11:37.971982 16760 solver.cpp:237]     Train net output #1: loss = 0.21443 (* 1 = 0.21443 loss)
I1123 18:11:37.971982 16760 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1123 18:11:42.541676 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:11:42.730316 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_15000.caffemodel
I1123 18:11:42.741302 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_15000.solverstate
I1123 18:11:42.745321 16760 solver.cpp:330] Iteration 15000, Testing net (#0)
I1123 18:11:42.745321 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:11:44.011818 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:11:44.061336 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8856
I1123 18:11:44.061336 16760 solver.cpp:397]     Test net output #1: loss = 0.336763 (* 1 = 0.336763 loss)
I1123 18:11:44.107830 16760 solver.cpp:218] Iteration 15000 (16.2992 iter/s, 6.13528s/100 iters), loss = 0.227606
I1123 18:11:44.107830 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:11:44.107830 16760 solver.cpp:237]     Train net output #1: loss = 0.227606 (* 1 = 0.227606 loss)
I1123 18:11:44.107830 16760 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1123 18:11:48.913861 16760 solver.cpp:218] Iteration 15100 (20.8085 iter/s, 4.80573s/100 iters), loss = 0.162075
I1123 18:11:48.914376 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:11:48.914376 16760 solver.cpp:237]     Train net output #1: loss = 0.162075 (* 1 = 0.162075 loss)
I1123 18:11:48.914376 16760 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1123 18:11:53.720839 16760 solver.cpp:218] Iteration 15200 (20.8064 iter/s, 4.8062s/100 iters), loss = 0.181168
I1123 18:11:53.720839 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:11:53.720839 16760 solver.cpp:237]     Train net output #1: loss = 0.181168 (* 1 = 0.181168 loss)
I1123 18:11:53.720839 16760 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1123 18:11:58.525981 16760 solver.cpp:218] Iteration 15300 (20.8131 iter/s, 4.80466s/100 iters), loss = 0.208977
I1123 18:11:58.525981 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:11:58.525981 16760 solver.cpp:237]     Train net output #1: loss = 0.208977 (* 1 = 0.208977 loss)
I1123 18:11:58.525981 16760 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1123 18:11:58.525981 16760 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1123 18:12:03.333410 16760 solver.cpp:218] Iteration 15400 (20.8021 iter/s, 4.80721s/100 iters), loss = 0.208229
I1123 18:12:03.333410 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:12:03.333410 16760 solver.cpp:237]     Train net output #1: loss = 0.208229 (* 1 = 0.208229 loss)
I1123 18:12:03.333410 16760 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1123 18:12:07.904477 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:12:08.092547 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_15500.caffemodel
I1123 18:12:08.102545 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_15500.solverstate
I1123 18:12:08.106545 16760 solver.cpp:330] Iteration 15500, Testing net (#0)
I1123 18:12:08.106545 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:12:09.374030 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:12:09.424053 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8851
I1123 18:12:09.424053 16760 solver.cpp:397]     Test net output #1: loss = 0.338309 (* 1 = 0.338309 loss)
I1123 18:12:09.470618 16760 solver.cpp:218] Iteration 15500 (16.2947 iter/s, 6.13697s/100 iters), loss = 0.210186
I1123 18:12:09.470618 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:12:09.470618 16760 solver.cpp:237]     Train net output #1: loss = 0.210186 (* 1 = 0.210186 loss)
I1123 18:12:09.470618 16760 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1123 18:12:14.275638 16760 solver.cpp:218] Iteration 15600 (20.8143 iter/s, 4.80439s/100 iters), loss = 0.225511
I1123 18:12:14.275638 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:12:14.275638 16760 solver.cpp:237]     Train net output #1: loss = 0.225511 (* 1 = 0.225511 loss)
I1123 18:12:14.275638 16760 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1123 18:12:19.085754 16760 solver.cpp:218] Iteration 15700 (20.7917 iter/s, 4.80961s/100 iters), loss = 0.177631
I1123 18:12:19.085754 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:12:19.085754 16760 solver.cpp:237]     Train net output #1: loss = 0.177632 (* 1 = 0.177632 loss)
I1123 18:12:19.085754 16760 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1123 18:12:23.893576 16760 solver.cpp:218] Iteration 15800 (20.8005 iter/s, 4.80758s/100 iters), loss = 0.212931
I1123 18:12:23.893576 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:12:23.893576 16760 solver.cpp:237]     Train net output #1: loss = 0.212931 (* 1 = 0.212931 loss)
I1123 18:12:23.893576 16760 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1123 18:12:28.700237 16760 solver.cpp:218] Iteration 15900 (20.8039 iter/s, 4.80679s/100 iters), loss = 0.150649
I1123 18:12:28.700237 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:12:28.700237 16760 solver.cpp:237]     Train net output #1: loss = 0.150649 (* 1 = 0.150649 loss)
I1123 18:12:28.700237 16760 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1123 18:12:33.275604 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:12:33.463636 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_16000.caffemodel
I1123 18:12:33.474143 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_16000.solverstate
I1123 18:12:33.478142 16760 solver.cpp:330] Iteration 16000, Testing net (#0)
I1123 18:12:33.478142 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:12:34.745160 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:12:34.795197 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8857
I1123 18:12:34.795197 16760 solver.cpp:397]     Test net output #1: loss = 0.33798 (* 1 = 0.33798 loss)
I1123 18:12:34.842181 16760 solver.cpp:218] Iteration 16000 (16.2839 iter/s, 6.14104s/100 iters), loss = 0.20786
I1123 18:12:34.842181 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:12:34.842181 16760 solver.cpp:237]     Train net output #1: loss = 0.20786 (* 1 = 0.20786 loss)
I1123 18:12:34.842181 16760 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1123 18:12:39.652099 16760 solver.cpp:218] Iteration 16100 (20.7915 iter/s, 4.80965s/100 iters), loss = 0.244446
I1123 18:12:39.652099 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:12:39.652099 16760 solver.cpp:237]     Train net output #1: loss = 0.244446 (* 1 = 0.244446 loss)
I1123 18:12:39.652099 16760 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1123 18:12:44.462659 16760 solver.cpp:218] Iteration 16200 (20.7863 iter/s, 4.81085s/100 iters), loss = 0.230871
I1123 18:12:44.463661 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:12:44.463661 16760 solver.cpp:237]     Train net output #1: loss = 0.230871 (* 1 = 0.230871 loss)
I1123 18:12:44.463661 16760 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1123 18:12:49.269026 16760 solver.cpp:218] Iteration 16300 (20.8093 iter/s, 4.80555s/100 iters), loss = 0.23457
I1123 18:12:49.269026 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:12:49.269026 16760 solver.cpp:237]     Train net output #1: loss = 0.23457 (* 1 = 0.23457 loss)
I1123 18:12:49.269026 16760 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1123 18:12:54.078997 16760 solver.cpp:218] Iteration 16400 (20.7906 iter/s, 4.80986s/100 iters), loss = 0.188623
I1123 18:12:54.078997 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:12:54.078997 16760 solver.cpp:237]     Train net output #1: loss = 0.188623 (* 1 = 0.188623 loss)
I1123 18:12:54.078997 16760 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1123 18:12:58.652142 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:12:58.841166 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_16500.caffemodel
I1123 18:12:58.851169 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_16500.solverstate
I1123 18:12:58.855170 16760 solver.cpp:330] Iteration 16500, Testing net (#0)
I1123 18:12:58.855670 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:13:00.118059 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:13:00.168560 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8856
I1123 18:13:00.168560 16760 solver.cpp:397]     Test net output #1: loss = 0.337801 (* 1 = 0.337801 loss)
I1123 18:13:00.215057 16760 solver.cpp:218] Iteration 16500 (16.3004 iter/s, 6.13483s/100 iters), loss = 0.194009
I1123 18:13:00.215057 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:13:00.215057 16760 solver.cpp:237]     Train net output #1: loss = 0.194009 (* 1 = 0.194009 loss)
I1123 18:13:00.215057 16760 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1123 18:13:05.020727 16760 solver.cpp:218] Iteration 16600 (20.8102 iter/s, 4.80533s/100 iters), loss = 0.202502
I1123 18:13:05.020727 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:13:05.020727 16760 solver.cpp:237]     Train net output #1: loss = 0.202502 (* 1 = 0.202502 loss)
I1123 18:13:05.020727 16760 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1123 18:13:09.829368 16760 solver.cpp:218] Iteration 16700 (20.7969 iter/s, 4.8084s/100 iters), loss = 0.187834
I1123 18:13:09.829368 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:13:09.829368 16760 solver.cpp:237]     Train net output #1: loss = 0.187834 (* 1 = 0.187834 loss)
I1123 18:13:09.829368 16760 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1123 18:13:14.637121 16760 solver.cpp:218] Iteration 16800 (20.8021 iter/s, 4.80721s/100 iters), loss = 0.28807
I1123 18:13:14.637121 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 18:13:14.637121 16760 solver.cpp:237]     Train net output #1: loss = 0.288071 (* 1 = 0.288071 loss)
I1123 18:13:14.637121 16760 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1123 18:13:19.445148 16760 solver.cpp:218] Iteration 16900 (20.7981 iter/s, 4.80814s/100 iters), loss = 0.153476
I1123 18:13:19.445148 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:13:19.445148 16760 solver.cpp:237]     Train net output #1: loss = 0.153476 (* 1 = 0.153476 loss)
I1123 18:13:19.445148 16760 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1123 18:13:24.020227 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:13:24.209300 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_17000.caffemodel
I1123 18:13:24.218302 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_17000.solverstate
I1123 18:13:24.222285 16760 solver.cpp:330] Iteration 17000, Testing net (#0)
I1123 18:13:24.222285 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:13:25.490422 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:13:25.540936 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8857
I1123 18:13:25.540936 16760 solver.cpp:397]     Test net output #1: loss = 0.33766 (* 1 = 0.33766 loss)
I1123 18:13:25.586752 16760 solver.cpp:218] Iteration 17000 (16.2834 iter/s, 6.14121s/100 iters), loss = 0.158467
I1123 18:13:25.586752 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 18:13:25.586752 16760 solver.cpp:237]     Train net output #1: loss = 0.158467 (* 1 = 0.158467 loss)
I1123 18:13:25.586752 16760 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1123 18:13:30.398910 16760 solver.cpp:218] Iteration 17100 (20.7828 iter/s, 4.81166s/100 iters), loss = 0.189394
I1123 18:13:30.398910 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:13:30.398910 16760 solver.cpp:237]     Train net output #1: loss = 0.189394 (* 1 = 0.189394 loss)
I1123 18:13:30.398910 16760 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1123 18:13:35.208565 16760 solver.cpp:218] Iteration 17200 (20.7921 iter/s, 4.80951s/100 iters), loss = 0.212355
I1123 18:13:35.208565 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:13:35.208565 16760 solver.cpp:237]     Train net output #1: loss = 0.212355 (* 1 = 0.212355 loss)
I1123 18:13:35.208565 16760 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1123 18:13:40.014353 16760 solver.cpp:218] Iteration 17300 (20.8121 iter/s, 4.8049s/100 iters), loss = 0.244858
I1123 18:13:40.014353 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 18:13:40.014353 16760 solver.cpp:237]     Train net output #1: loss = 0.244858 (* 1 = 0.244858 loss)
I1123 18:13:40.014353 16760 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1123 18:13:44.818648 16760 solver.cpp:218] Iteration 17400 (20.8148 iter/s, 4.80427s/100 iters), loss = 0.165694
I1123 18:13:44.818648 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:13:44.818648 16760 solver.cpp:237]     Train net output #1: loss = 0.165695 (* 1 = 0.165695 loss)
I1123 18:13:44.818648 16760 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1123 18:13:49.388070 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:13:49.576952 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_17500.caffemodel
I1123 18:13:49.586951 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_17500.solverstate
I1123 18:13:49.590950 16760 solver.cpp:330] Iteration 17500, Testing net (#0)
I1123 18:13:49.590950 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:13:50.857949 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:13:50.907950 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8855
I1123 18:13:50.907950 16760 solver.cpp:397]     Test net output #1: loss = 0.337494 (* 1 = 0.337494 loss)
I1123 18:13:50.954483 16760 solver.cpp:218] Iteration 17500 (16.3006 iter/s, 6.13473s/100 iters), loss = 0.205636
I1123 18:13:50.954483 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:13:50.954483 16760 solver.cpp:237]     Train net output #1: loss = 0.205636 (* 1 = 0.205636 loss)
I1123 18:13:50.954483 16760 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1123 18:13:55.761135 16760 solver.cpp:218] Iteration 17600 (20.8057 iter/s, 4.80638s/100 iters), loss = 0.200311
I1123 18:13:55.761135 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:13:55.761135 16760 solver.cpp:237]     Train net output #1: loss = 0.200311 (* 1 = 0.200311 loss)
I1123 18:13:55.761135 16760 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1123 18:14:00.569819 16760 solver.cpp:218] Iteration 17700 (20.7941 iter/s, 4.80906s/100 iters), loss = 0.222921
I1123 18:14:00.569819 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:14:00.569819 16760 solver.cpp:237]     Train net output #1: loss = 0.222921 (* 1 = 0.222921 loss)
I1123 18:14:00.570819 16760 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1123 18:14:05.378556 16760 solver.cpp:218] Iteration 17800 (20.7989 iter/s, 4.80795s/100 iters), loss = 0.204777
I1123 18:14:05.378556 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:14:05.378556 16760 solver.cpp:237]     Train net output #1: loss = 0.204777 (* 1 = 0.204777 loss)
I1123 18:14:05.378556 16760 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1123 18:14:10.184959 16760 solver.cpp:218] Iteration 17900 (20.8083 iter/s, 4.80578s/100 iters), loss = 0.143091
I1123 18:14:10.184959 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 18:14:10.184959 16760 solver.cpp:237]     Train net output #1: loss = 0.143091 (* 1 = 0.143091 loss)
I1123 18:14:10.184959 16760 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1123 18:14:14.758349 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:14:14.948662 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_18000.caffemodel
I1123 18:14:14.959177 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_18000.solverstate
I1123 18:14:14.963178 16760 solver.cpp:330] Iteration 18000, Testing net (#0)
I1123 18:14:14.963178 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:14:16.230886 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:14:16.280907 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8855
I1123 18:14:16.280907 16760 solver.cpp:397]     Test net output #1: loss = 0.337194 (* 1 = 0.337194 loss)
I1123 18:14:16.326907 16760 solver.cpp:218] Iteration 18000 (16.2813 iter/s, 6.14201s/100 iters), loss = 0.205013
I1123 18:14:16.326907 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:14:16.326907 16760 solver.cpp:237]     Train net output #1: loss = 0.205014 (* 1 = 0.205014 loss)
I1123 18:14:16.326907 16760 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1123 18:14:21.133468 16760 solver.cpp:218] Iteration 18100 (20.8068 iter/s, 4.80611s/100 iters), loss = 0.212845
I1123 18:14:21.133468 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:14:21.133468 16760 solver.cpp:237]     Train net output #1: loss = 0.212845 (* 1 = 0.212845 loss)
I1123 18:14:21.133468 16760 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1123 18:14:25.937254 16760 solver.cpp:218] Iteration 18200 (20.8201 iter/s, 4.80304s/100 iters), loss = 0.168055
I1123 18:14:25.937254 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 18:14:25.937254 16760 solver.cpp:237]     Train net output #1: loss = 0.168055 (* 1 = 0.168055 loss)
I1123 18:14:25.937254 16760 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1123 18:14:30.745322 16760 solver.cpp:218] Iteration 18300 (20.8017 iter/s, 4.8073s/100 iters), loss = 0.262301
I1123 18:14:30.745322 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:14:30.745322 16760 solver.cpp:237]     Train net output #1: loss = 0.262301 (* 1 = 0.262301 loss)
I1123 18:14:30.745322 16760 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1123 18:14:35.552335 16760 solver.cpp:218] Iteration 18400 (20.8045 iter/s, 4.80666s/100 iters), loss = 0.185563
I1123 18:14:35.552335 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:14:35.552335 16760 solver.cpp:237]     Train net output #1: loss = 0.185563 (* 1 = 0.185563 loss)
I1123 18:14:35.552335 16760 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1123 18:14:40.122756 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:14:40.311722 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_18500.caffemodel
I1123 18:14:40.321725 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_18500.solverstate
I1123 18:14:40.325709 16760 solver.cpp:330] Iteration 18500, Testing net (#0)
I1123 18:14:40.325709 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:14:41.592499 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:14:41.641495 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8851
I1123 18:14:41.642498 16760 solver.cpp:397]     Test net output #1: loss = 0.337272 (* 1 = 0.337272 loss)
I1123 18:14:41.688079 16760 solver.cpp:218] Iteration 18500 (16.2981 iter/s, 6.13567s/100 iters), loss = 0.248369
I1123 18:14:41.688079 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:14:41.688079 16760 solver.cpp:237]     Train net output #1: loss = 0.248369 (* 1 = 0.248369 loss)
I1123 18:14:41.688079 16760 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1123 18:14:46.495882 16760 solver.cpp:218] Iteration 18600 (20.801 iter/s, 4.80747s/100 iters), loss = 0.19538
I1123 18:14:46.495882 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:14:46.495882 16760 solver.cpp:237]     Train net output #1: loss = 0.19538 (* 1 = 0.19538 loss)
I1123 18:14:46.495882 16760 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1123 18:14:51.305433 16760 solver.cpp:218] Iteration 18700 (20.794 iter/s, 4.80908s/100 iters), loss = 0.175116
I1123 18:14:51.305433 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:14:51.305433 16760 solver.cpp:237]     Train net output #1: loss = 0.175116 (* 1 = 0.175116 loss)
I1123 18:14:51.305433 16760 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1123 18:14:56.114838 16760 solver.cpp:218] Iteration 18800 (20.7961 iter/s, 4.8086s/100 iters), loss = 0.207135
I1123 18:14:56.114838 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:14:56.114838 16760 solver.cpp:237]     Train net output #1: loss = 0.207135 (* 1 = 0.207135 loss)
I1123 18:14:56.114838 16760 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1123 18:15:00.923895 16760 solver.cpp:218] Iteration 18900 (20.7941 iter/s, 4.80906s/100 iters), loss = 0.146755
I1123 18:15:00.923895 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 18:15:00.923895 16760 solver.cpp:237]     Train net output #1: loss = 0.146755 (* 1 = 0.146755 loss)
I1123 18:15:00.923895 16760 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1123 18:15:05.496739 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:15:05.686285 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_19000.caffemodel
I1123 18:15:05.695269 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_19000.solverstate
I1123 18:15:05.699270 16760 solver.cpp:330] Iteration 19000, Testing net (#0)
I1123 18:15:05.699270 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:15:06.967505 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:15:07.017141 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8855
I1123 18:15:07.017141 16760 solver.cpp:397]     Test net output #1: loss = 0.336961 (* 1 = 0.336961 loss)
I1123 18:15:07.064152 16760 solver.cpp:218] Iteration 19000 (16.2882 iter/s, 6.13942s/100 iters), loss = 0.206177
I1123 18:15:07.064152 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:15:07.064152 16760 solver.cpp:237]     Train net output #1: loss = 0.206177 (* 1 = 0.206177 loss)
I1123 18:15:07.064152 16760 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1123 18:15:11.870493 16760 solver.cpp:218] Iteration 19100 (20.8034 iter/s, 4.8069s/100 iters), loss = 0.195913
I1123 18:15:11.871497 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:15:11.871497 16760 solver.cpp:237]     Train net output #1: loss = 0.195913 (* 1 = 0.195913 loss)
I1123 18:15:11.871497 16760 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1123 18:15:16.681875 16760 solver.cpp:218] Iteration 19200 (20.7861 iter/s, 4.8109s/100 iters), loss = 0.208742
I1123 18:15:16.681875 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:15:16.682875 16760 solver.cpp:237]     Train net output #1: loss = 0.208742 (* 1 = 0.208742 loss)
I1123 18:15:16.682875 16760 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1123 18:15:21.491570 16760 solver.cpp:218] Iteration 19300 (20.7955 iter/s, 4.80873s/100 iters), loss = 0.209041
I1123 18:15:21.491570 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:15:21.491570 16760 solver.cpp:237]     Train net output #1: loss = 0.209041 (* 1 = 0.209041 loss)
I1123 18:15:21.491570 16760 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1123 18:15:26.301378 16760 solver.cpp:218] Iteration 19400 (20.7917 iter/s, 4.80962s/100 iters), loss = 0.161185
I1123 18:15:26.301378 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 18:15:26.301378 16760 solver.cpp:237]     Train net output #1: loss = 0.161185 (* 1 = 0.161185 loss)
I1123 18:15:26.301378 16760 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1123 18:15:30.874024 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:15:31.063122 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_19500.caffemodel
I1123 18:15:31.074121 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_19500.solverstate
I1123 18:15:31.078122 16760 solver.cpp:330] Iteration 19500, Testing net (#0)
I1123 18:15:31.078122 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:15:32.344707 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:15:32.394781 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8857
I1123 18:15:32.394781 16760 solver.cpp:397]     Test net output #1: loss = 0.337051 (* 1 = 0.337051 loss)
I1123 18:15:32.441287 16760 solver.cpp:218] Iteration 19500 (16.289 iter/s, 6.13912s/100 iters), loss = 0.20013
I1123 18:15:32.441287 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:15:32.441287 16760 solver.cpp:237]     Train net output #1: loss = 0.20013 (* 1 = 0.20013 loss)
I1123 18:15:32.441287 16760 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1123 18:15:32.441287 16760 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1123 18:15:37.246280 16760 solver.cpp:218] Iteration 19600 (20.8137 iter/s, 4.80453s/100 iters), loss = 0.180426
I1123 18:15:37.246280 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:15:37.246280 16760 solver.cpp:237]     Train net output #1: loss = 0.180426 (* 1 = 0.180426 loss)
I1123 18:15:37.246280 16760 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1123 18:15:42.049827 16760 solver.cpp:218] Iteration 19700 (20.8156 iter/s, 4.80408s/100 iters), loss = 0.220538
I1123 18:15:42.050832 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:15:42.050832 16760 solver.cpp:237]     Train net output #1: loss = 0.220538 (* 1 = 0.220538 loss)
I1123 18:15:42.050832 16760 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1123 18:15:46.854600 16760 solver.cpp:218] Iteration 19800 (20.8171 iter/s, 4.80374s/100 iters), loss = 0.205928
I1123 18:15:46.854600 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:15:46.854600 16760 solver.cpp:237]     Train net output #1: loss = 0.205928 (* 1 = 0.205928 loss)
I1123 18:15:46.854600 16760 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1123 18:15:51.660558 16760 solver.cpp:218] Iteration 19900 (20.8071 iter/s, 4.80604s/100 iters), loss = 0.130081
I1123 18:15:51.661572 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 18:15:51.661572 16760 solver.cpp:237]     Train net output #1: loss = 0.130081 (* 1 = 0.130081 loss)
I1123 18:15:51.661572 16760 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1123 18:15:56.228905 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:15:56.418217 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_20000.caffemodel
I1123 18:15:56.431741 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_20000.solverstate
I1123 18:15:56.436241 16760 solver.cpp:330] Iteration 20000, Testing net (#0)
I1123 18:15:56.436241 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:15:57.702033 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:15:57.751550 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8854
I1123 18:15:57.751550 16760 solver.cpp:397]     Test net output #1: loss = 0.336831 (* 1 = 0.336831 loss)
I1123 18:15:57.798549 16760 solver.cpp:218] Iteration 20000 (16.2948 iter/s, 6.13694s/100 iters), loss = 0.231845
I1123 18:15:57.798549 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:15:57.798549 16760 solver.cpp:237]     Train net output #1: loss = 0.231845 (* 1 = 0.231845 loss)
I1123 18:15:57.798549 16760 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1123 18:16:02.602408 16760 solver.cpp:218] Iteration 20100 (20.8181 iter/s, 4.80352s/100 iters), loss = 0.210539
I1123 18:16:02.602408 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:16:02.602908 16760 solver.cpp:237]     Train net output #1: loss = 0.210539 (* 1 = 0.210539 loss)
I1123 18:16:02.602908 16760 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1123 18:16:07.411308 16760 solver.cpp:218] Iteration 20200 (20.7961 iter/s, 4.80859s/100 iters), loss = 0.189864
I1123 18:16:07.411308 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 18:16:07.411308 16760 solver.cpp:237]     Train net output #1: loss = 0.189864 (* 1 = 0.189864 loss)
I1123 18:16:07.411308 16760 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1123 18:16:12.217625 16760 solver.cpp:218] Iteration 20300 (20.8091 iter/s, 4.80558s/100 iters), loss = 0.230111
I1123 18:16:12.217625 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:16:12.217625 16760 solver.cpp:237]     Train net output #1: loss = 0.230111 (* 1 = 0.230111 loss)
I1123 18:16:12.217625 16760 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1123 18:16:17.022878 16760 solver.cpp:218] Iteration 20400 (20.8117 iter/s, 4.80498s/100 iters), loss = 0.162234
I1123 18:16:17.022878 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:16:17.022878 16760 solver.cpp:237]     Train net output #1: loss = 0.162234 (* 1 = 0.162234 loss)
I1123 18:16:17.022878 16760 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1123 18:16:21.590659 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:16:21.779342 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_20500.caffemodel
I1123 18:16:21.789342 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_20500.solverstate
I1123 18:16:21.793340 16760 solver.cpp:330] Iteration 20500, Testing net (#0)
I1123 18:16:21.793340 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:16:23.060174 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:16:23.110673 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8855
I1123 18:16:23.110673 16760 solver.cpp:397]     Test net output #1: loss = 0.336851 (* 1 = 0.336851 loss)
I1123 18:16:23.157202 16760 solver.cpp:218] Iteration 20500 (16.3032 iter/s, 6.13377s/100 iters), loss = 0.219421
I1123 18:16:23.157202 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:16:23.157202 16760 solver.cpp:237]     Train net output #1: loss = 0.219421 (* 1 = 0.219421 loss)
I1123 18:16:23.157202 16760 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1123 18:16:27.965931 16760 solver.cpp:218] Iteration 20600 (20.7965 iter/s, 4.8085s/100 iters), loss = 0.175852
I1123 18:16:27.965931 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:16:27.965931 16760 solver.cpp:237]     Train net output #1: loss = 0.175852 (* 1 = 0.175852 loss)
I1123 18:16:27.965931 16760 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1123 18:16:32.774186 16760 solver.cpp:218] Iteration 20700 (20.7984 iter/s, 4.80806s/100 iters), loss = 0.216179
I1123 18:16:32.774186 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:16:32.774186 16760 solver.cpp:237]     Train net output #1: loss = 0.216179 (* 1 = 0.216179 loss)
I1123 18:16:32.774186 16760 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1123 18:16:37.582072 16760 solver.cpp:218] Iteration 20800 (20.7991 iter/s, 4.80789s/100 iters), loss = 0.229253
I1123 18:16:37.582072 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:16:37.582072 16760 solver.cpp:237]     Train net output #1: loss = 0.229253 (* 1 = 0.229253 loss)
I1123 18:16:37.582072 16760 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1123 18:16:42.386656 16760 solver.cpp:218] Iteration 20900 (20.8149 iter/s, 4.80425s/100 iters), loss = 0.166058
I1123 18:16:42.386656 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:16:42.387656 16760 solver.cpp:237]     Train net output #1: loss = 0.166058 (* 1 = 0.166058 loss)
I1123 18:16:42.387656 16760 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1123 18:16:46.963484 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:16:47.152055 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_21000.caffemodel
I1123 18:16:47.163058 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_21000.solverstate
I1123 18:16:47.167055 16760 solver.cpp:330] Iteration 21000, Testing net (#0)
I1123 18:16:47.167055 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:16:48.434531 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:16:48.484516 16760 solver.cpp:397]     Test net output #0: accuracy = 0.885
I1123 18:16:48.484516 16760 solver.cpp:397]     Test net output #1: loss = 0.33693 (* 1 = 0.33693 loss)
I1123 18:16:48.531551 16760 solver.cpp:218] Iteration 21000 (16.2769 iter/s, 6.14366s/100 iters), loss = 0.223845
I1123 18:16:48.531551 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 18:16:48.531551 16760 solver.cpp:237]     Train net output #1: loss = 0.223845 (* 1 = 0.223845 loss)
I1123 18:16:48.531551 16760 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1123 18:16:53.338332 16760 solver.cpp:218] Iteration 21100 (20.8043 iter/s, 4.8067s/100 iters), loss = 0.183111
I1123 18:16:53.338332 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:16:53.338332 16760 solver.cpp:237]     Train net output #1: loss = 0.183112 (* 1 = 0.183112 loss)
I1123 18:16:53.338332 16760 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1123 18:16:58.150468 16760 solver.cpp:218] Iteration 21200 (20.7829 iter/s, 4.81166s/100 iters), loss = 0.20979
I1123 18:16:58.150468 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:16:58.150468 16760 solver.cpp:237]     Train net output #1: loss = 0.20979 (* 1 = 0.20979 loss)
I1123 18:16:58.150468 16760 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1123 18:17:02.962700 16760 solver.cpp:218] Iteration 21300 (20.7839 iter/s, 4.81142s/100 iters), loss = 0.224472
I1123 18:17:02.962700 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:17:02.962700 16760 solver.cpp:237]     Train net output #1: loss = 0.224472 (* 1 = 0.224472 loss)
I1123 18:17:02.962700 16760 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1123 18:17:07.769702 16760 solver.cpp:218] Iteration 21400 (20.8049 iter/s, 4.80657s/100 iters), loss = 0.126815
I1123 18:17:07.769702 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 18:17:07.769702 16760 solver.cpp:237]     Train net output #1: loss = 0.126815 (* 1 = 0.126815 loss)
I1123 18:17:07.769702 16760 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1123 18:17:12.342260 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:17:12.531298 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_21500.caffemodel
I1123 18:17:12.541299 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_21500.solverstate
I1123 18:17:12.545300 16760 solver.cpp:330] Iteration 21500, Testing net (#0)
I1123 18:17:12.545300 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:17:13.810762 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:17:13.859807 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8852
I1123 18:17:13.859807 16760 solver.cpp:397]     Test net output #1: loss = 0.336938 (* 1 = 0.336938 loss)
I1123 18:17:13.906805 16760 solver.cpp:218] Iteration 21500 (16.2942 iter/s, 6.13716s/100 iters), loss = 0.216855
I1123 18:17:13.906805 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:17:13.906805 16760 solver.cpp:237]     Train net output #1: loss = 0.216856 (* 1 = 0.216856 loss)
I1123 18:17:13.906805 16760 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1123 18:17:18.711243 16760 solver.cpp:218] Iteration 21600 (20.8179 iter/s, 4.80357s/100 iters), loss = 0.20449
I1123 18:17:18.711243 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:17:18.711243 16760 solver.cpp:237]     Train net output #1: loss = 0.20449 (* 1 = 0.20449 loss)
I1123 18:17:18.711243 16760 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1123 18:17:23.517932 16760 solver.cpp:218] Iteration 21700 (20.8026 iter/s, 4.80708s/100 iters), loss = 0.196195
I1123 18:17:23.517932 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:17:23.517932 16760 solver.cpp:237]     Train net output #1: loss = 0.196195 (* 1 = 0.196195 loss)
I1123 18:17:23.517932 16760 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1123 18:17:28.324379 16760 solver.cpp:218] Iteration 21800 (20.808 iter/s, 4.80584s/100 iters), loss = 0.234688
I1123 18:17:28.324379 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 18:17:28.324379 16760 solver.cpp:237]     Train net output #1: loss = 0.234688 (* 1 = 0.234688 loss)
I1123 18:17:28.324379 16760 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1123 18:17:33.135584 16760 solver.cpp:218] Iteration 21900 (20.7856 iter/s, 4.81102s/100 iters), loss = 0.142548
I1123 18:17:33.135584 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 18:17:33.135584 16760 solver.cpp:237]     Train net output #1: loss = 0.142548 (* 1 = 0.142548 loss)
I1123 18:17:33.135584 16760 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1123 18:17:37.706820 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:17:37.896574 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_22000.caffemodel
I1123 18:17:37.906554 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_22000.solverstate
I1123 18:17:37.910553 16760 solver.cpp:330] Iteration 22000, Testing net (#0)
I1123 18:17:37.910553 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:17:39.178026 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:17:39.228021 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8852
I1123 18:17:39.228021 16760 solver.cpp:397]     Test net output #1: loss = 0.336925 (* 1 = 0.336925 loss)
I1123 18:17:39.275050 16760 solver.cpp:218] Iteration 22000 (16.2903 iter/s, 6.13862s/100 iters), loss = 0.23087
I1123 18:17:39.275050 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:17:39.275050 16760 solver.cpp:237]     Train net output #1: loss = 0.23087 (* 1 = 0.23087 loss)
I1123 18:17:39.275050 16760 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1123 18:17:39.275050 16760 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1123 18:17:44.085501 16760 solver.cpp:218] Iteration 22100 (20.7893 iter/s, 4.81017s/100 iters), loss = 0.218459
I1123 18:17:44.085501 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:17:44.085501 16760 solver.cpp:237]     Train net output #1: loss = 0.21846 (* 1 = 0.21846 loss)
I1123 18:17:44.085501 16760 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1123 18:17:48.895480 16760 solver.cpp:218] Iteration 22200 (20.7924 iter/s, 4.80944s/100 iters), loss = 0.18324
I1123 18:17:48.895480 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:17:48.895480 16760 solver.cpp:237]     Train net output #1: loss = 0.18324 (* 1 = 0.18324 loss)
I1123 18:17:48.895480 16760 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1123 18:17:53.708504 16760 solver.cpp:218] Iteration 22300 (20.7786 iter/s, 4.81265s/100 iters), loss = 0.163325
I1123 18:17:53.708504 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:17:53.708504 16760 solver.cpp:237]     Train net output #1: loss = 0.163325 (* 1 = 0.163325 loss)
I1123 18:17:53.708504 16760 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1123 18:17:58.518407 16760 solver.cpp:218] Iteration 22400 (20.7935 iter/s, 4.8092s/100 iters), loss = 0.148601
I1123 18:17:58.518407 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:17:58.518407 16760 solver.cpp:237]     Train net output #1: loss = 0.148601 (* 1 = 0.148601 loss)
I1123 18:17:58.518407 16760 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1123 18:18:03.093502 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:18:03.283030 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_22500.caffemodel
I1123 18:18:03.293028 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_22500.solverstate
I1123 18:18:03.297549 16760 solver.cpp:330] Iteration 22500, Testing net (#0)
I1123 18:18:03.297549 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:18:04.565173 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:18:04.615073 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8851
I1123 18:18:04.615073 16760 solver.cpp:397]     Test net output #1: loss = 0.336932 (* 1 = 0.336932 loss)
I1123 18:18:04.661092 16760 solver.cpp:218] Iteration 22500 (16.2793 iter/s, 6.14278s/100 iters), loss = 0.207142
I1123 18:18:04.661092 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:18:04.661092 16760 solver.cpp:237]     Train net output #1: loss = 0.207142 (* 1 = 0.207142 loss)
I1123 18:18:04.661092 16760 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1123 18:18:09.465765 16760 solver.cpp:218] Iteration 22600 (20.8173 iter/s, 4.80369s/100 iters), loss = 0.229571
I1123 18:18:09.465765 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:18:09.465765 16760 solver.cpp:237]     Train net output #1: loss = 0.229571 (* 1 = 0.229571 loss)
I1123 18:18:09.465765 16760 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1123 18:18:14.273860 16760 solver.cpp:218] Iteration 22700 (20.7977 iter/s, 4.80823s/100 iters), loss = 0.204174
I1123 18:18:14.273860 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:18:14.273860 16760 solver.cpp:237]     Train net output #1: loss = 0.204174 (* 1 = 0.204174 loss)
I1123 18:18:14.273860 16760 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1123 18:18:19.083784 16760 solver.cpp:218] Iteration 22800 (20.7945 iter/s, 4.80896s/100 iters), loss = 0.235853
I1123 18:18:19.083784 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:18:19.083784 16760 solver.cpp:237]     Train net output #1: loss = 0.235854 (* 1 = 0.235854 loss)
I1123 18:18:19.083784 16760 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1123 18:18:23.892441 16760 solver.cpp:218] Iteration 22900 (20.7974 iter/s, 4.80829s/100 iters), loss = 0.175516
I1123 18:18:23.892441 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:18:23.892441 16760 solver.cpp:237]     Train net output #1: loss = 0.175517 (* 1 = 0.175517 loss)
I1123 18:18:23.892441 16760 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1123 18:18:28.463042 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:18:28.651476 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_23000.caffemodel
I1123 18:18:28.661478 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_23000.solverstate
I1123 18:18:28.666496 16760 solver.cpp:330] Iteration 23000, Testing net (#0)
I1123 18:18:28.666496 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:18:29.932242 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:18:29.982241 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8854
I1123 18:18:29.982241 16760 solver.cpp:397]     Test net output #1: loss = 0.337041 (* 1 = 0.337041 loss)
I1123 18:18:30.029284 16760 solver.cpp:218] Iteration 23000 (16.2952 iter/s, 6.13679s/100 iters), loss = 0.229015
I1123 18:18:30.029284 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:18:30.029284 16760 solver.cpp:237]     Train net output #1: loss = 0.229016 (* 1 = 0.229016 loss)
I1123 18:18:30.029284 16760 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1123 18:18:34.836115 16760 solver.cpp:218] Iteration 23100 (20.8067 iter/s, 4.80613s/100 iters), loss = 0.182399
I1123 18:18:34.836115 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 18:18:34.836115 16760 solver.cpp:237]     Train net output #1: loss = 0.182399 (* 1 = 0.182399 loss)
I1123 18:18:34.836115 16760 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1123 18:18:39.640550 16760 solver.cpp:218] Iteration 23200 (20.8148 iter/s, 4.80428s/100 iters), loss = 0.212421
I1123 18:18:39.640550 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:18:39.641050 16760 solver.cpp:237]     Train net output #1: loss = 0.212421 (* 1 = 0.212421 loss)
I1123 18:18:39.641050 16760 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1123 18:18:44.444792 16760 solver.cpp:218] Iteration 23300 (20.8185 iter/s, 4.80343s/100 iters), loss = 0.229509
I1123 18:18:44.444792 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:18:44.444792 16760 solver.cpp:237]     Train net output #1: loss = 0.229509 (* 1 = 0.229509 loss)
I1123 18:18:44.444792 16760 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1123 18:18:49.249549 16760 solver.cpp:218] Iteration 23400 (20.8122 iter/s, 4.80487s/100 iters), loss = 0.165859
I1123 18:18:49.249549 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:18:49.249549 16760 solver.cpp:237]     Train net output #1: loss = 0.165859 (* 1 = 0.165859 loss)
I1123 18:18:49.249549 16760 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1123 18:18:53.820830 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:18:54.009380 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_23500.caffemodel
I1123 18:18:54.019381 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_23500.solverstate
I1123 18:18:54.023393 16760 solver.cpp:330] Iteration 23500, Testing net (#0)
I1123 18:18:54.023393 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:18:55.290400 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:18:55.340922 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8853
I1123 18:18:55.340922 16760 solver.cpp:397]     Test net output #1: loss = 0.336925 (* 1 = 0.336925 loss)
I1123 18:18:55.387431 16760 solver.cpp:218] Iteration 23500 (16.2946 iter/s, 6.13699s/100 iters), loss = 0.208558
I1123 18:18:55.387431 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:18:55.387431 16760 solver.cpp:237]     Train net output #1: loss = 0.208558 (* 1 = 0.208558 loss)
I1123 18:18:55.387431 16760 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1123 18:19:00.195946 16760 solver.cpp:218] Iteration 23600 (20.7974 iter/s, 4.80829s/100 iters), loss = 0.222765
I1123 18:19:00.195946 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:19:00.195946 16760 solver.cpp:237]     Train net output #1: loss = 0.222765 (* 1 = 0.222765 loss)
I1123 18:19:00.195946 16760 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1123 18:19:05.002861 16760 solver.cpp:218] Iteration 23700 (20.8024 iter/s, 4.80713s/100 iters), loss = 0.22088
I1123 18:19:05.002861 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:19:05.002861 16760 solver.cpp:237]     Train net output #1: loss = 0.22088 (* 1 = 0.22088 loss)
I1123 18:19:05.002861 16760 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1123 18:19:09.812917 16760 solver.cpp:218] Iteration 23800 (20.7937 iter/s, 4.80914s/100 iters), loss = 0.175184
I1123 18:19:09.812917 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:19:09.812917 16760 solver.cpp:237]     Train net output #1: loss = 0.175185 (* 1 = 0.175185 loss)
I1123 18:19:09.812917 16760 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1123 18:19:14.615109 16760 solver.cpp:218] Iteration 23900 (20.825 iter/s, 4.80192s/100 iters), loss = 0.181049
I1123 18:19:14.615109 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:19:14.615109 16760 solver.cpp:237]     Train net output #1: loss = 0.181049 (* 1 = 0.181049 loss)
I1123 18:19:14.615109 16760 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1123 18:19:19.188223 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:19:19.378077 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_24000.caffemodel
I1123 18:19:19.389081 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_24000.solverstate
I1123 18:19:19.393076 16760 solver.cpp:330] Iteration 24000, Testing net (#0)
I1123 18:19:19.393076 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:19:20.660662 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:19:20.710678 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8852
I1123 18:19:20.710678 16760 solver.cpp:397]     Test net output #1: loss = 0.336909 (* 1 = 0.336909 loss)
I1123 18:19:20.757205 16760 solver.cpp:218] Iteration 24000 (16.2827 iter/s, 6.14148s/100 iters), loss = 0.215904
I1123 18:19:20.757205 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:19:20.757205 16760 solver.cpp:237]     Train net output #1: loss = 0.215904 (* 1 = 0.215904 loss)
I1123 18:19:20.757205 16760 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1123 18:19:25.566337 16760 solver.cpp:218] Iteration 24100 (20.7942 iter/s, 4.80903s/100 iters), loss = 0.207874
I1123 18:19:25.566337 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:19:25.566337 16760 solver.cpp:237]     Train net output #1: loss = 0.207874 (* 1 = 0.207874 loss)
I1123 18:19:25.566337 16760 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1123 18:19:30.377269 16760 solver.cpp:218] Iteration 24200 (20.7876 iter/s, 4.81056s/100 iters), loss = 0.205311
I1123 18:19:30.377269 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:19:30.377269 16760 solver.cpp:237]     Train net output #1: loss = 0.205311 (* 1 = 0.205311 loss)
I1123 18:19:30.377269 16760 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1123 18:19:35.188259 16760 solver.cpp:218] Iteration 24300 (20.7873 iter/s, 4.81062s/100 iters), loss = 0.230962
I1123 18:19:35.188259 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:19:35.188259 16760 solver.cpp:237]     Train net output #1: loss = 0.230962 (* 1 = 0.230962 loss)
I1123 18:19:35.188259 16760 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1123 18:19:39.994386 16760 solver.cpp:218] Iteration 24400 (20.8075 iter/s, 4.80596s/100 iters), loss = 0.185582
I1123 18:19:39.995385 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:19:39.995385 16760 solver.cpp:237]     Train net output #1: loss = 0.185582 (* 1 = 0.185582 loss)
I1123 18:19:39.995385 16760 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1123 18:19:44.567101 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:19:44.756695 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_24500.caffemodel
I1123 18:19:44.769218 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_24500.solverstate
I1123 18:19:44.773217 16760 solver.cpp:330] Iteration 24500, Testing net (#0)
I1123 18:19:44.773217 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:19:46.041432 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:19:46.091454 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8852
I1123 18:19:46.091454 16760 solver.cpp:397]     Test net output #1: loss = 0.336891 (* 1 = 0.336891 loss)
I1123 18:19:46.137468 16760 solver.cpp:218] Iteration 24500 (16.2802 iter/s, 6.14244s/100 iters), loss = 0.214881
I1123 18:19:46.137468 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 18:19:46.137468 16760 solver.cpp:237]     Train net output #1: loss = 0.214881 (* 1 = 0.214881 loss)
I1123 18:19:46.137468 16760 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1123 18:19:50.941403 16760 solver.cpp:218] Iteration 24600 (20.817 iter/s, 4.80377s/100 iters), loss = 0.211705
I1123 18:19:50.942409 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:19:50.942409 16760 solver.cpp:237]     Train net output #1: loss = 0.211705 (* 1 = 0.211705 loss)
I1123 18:19:50.942409 16760 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1123 18:19:55.749009 16760 solver.cpp:218] Iteration 24700 (20.8026 iter/s, 4.8071s/100 iters), loss = 0.221024
I1123 18:19:55.749009 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:19:55.749009 16760 solver.cpp:237]     Train net output #1: loss = 0.221024 (* 1 = 0.221024 loss)
I1123 18:19:55.749009 16760 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1123 18:20:00.568053 16760 solver.cpp:218] Iteration 24800 (20.7564 iter/s, 4.8178s/100 iters), loss = 0.199984
I1123 18:20:00.568053 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:20:00.568053 16760 solver.cpp:237]     Train net output #1: loss = 0.199984 (* 1 = 0.199984 loss)
I1123 18:20:00.568053 16760 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1123 18:20:05.395056 16760 solver.cpp:218] Iteration 24900 (20.7157 iter/s, 4.82725s/100 iters), loss = 0.184351
I1123 18:20:05.395056 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 18:20:05.395056 16760 solver.cpp:237]     Train net output #1: loss = 0.184352 (* 1 = 0.184352 loss)
I1123 18:20:05.395056 16760 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1123 18:20:09.962293 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:20:10.151460 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_25000.caffemodel
I1123 18:20:10.160459 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_25000.solverstate
I1123 18:20:10.165477 16760 solver.cpp:330] Iteration 25000, Testing net (#0)
I1123 18:20:10.165477 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:20:11.430830 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:20:11.481331 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8853
I1123 18:20:11.481331 16760 solver.cpp:397]     Test net output #1: loss = 0.336944 (* 1 = 0.336944 loss)
I1123 18:20:11.527433 16760 solver.cpp:218] Iteration 25000 (16.3073 iter/s, 6.13221s/100 iters), loss = 0.209668
I1123 18:20:11.528437 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:20:11.528437 16760 solver.cpp:237]     Train net output #1: loss = 0.209668 (* 1 = 0.209668 loss)
I1123 18:20:11.528437 16760 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1123 18:20:16.336129 16760 solver.cpp:218] Iteration 25100 (20.7977 iter/s, 4.80822s/100 iters), loss = 0.185986
I1123 18:20:16.336129 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:20:16.336129 16760 solver.cpp:237]     Train net output #1: loss = 0.185986 (* 1 = 0.185986 loss)
I1123 18:20:16.336129 16760 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1123 18:20:21.144228 16760 solver.cpp:218] Iteration 25200 (20.8013 iter/s, 4.80739s/100 iters), loss = 0.219211
I1123 18:20:21.144228 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:20:21.144228 16760 solver.cpp:237]     Train net output #1: loss = 0.219211 (* 1 = 0.219211 loss)
I1123 18:20:21.144228 16760 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1123 18:20:25.950124 16760 solver.cpp:218] Iteration 25300 (20.8098 iter/s, 4.80544s/100 iters), loss = 0.217953
I1123 18:20:25.950124 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:20:25.950124 16760 solver.cpp:237]     Train net output #1: loss = 0.217954 (* 1 = 0.217954 loss)
I1123 18:20:25.950124 16760 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1123 18:20:30.753959 16760 solver.cpp:218] Iteration 25400 (20.8177 iter/s, 4.8036s/100 iters), loss = 0.171067
I1123 18:20:30.753959 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:20:30.753959 16760 solver.cpp:237]     Train net output #1: loss = 0.171068 (* 1 = 0.171068 loss)
I1123 18:20:30.753959 16760 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1123 18:20:35.325229 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:20:35.514892 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_25500.caffemodel
I1123 18:20:35.526893 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_25500.solverstate
I1123 18:20:35.531893 16760 solver.cpp:330] Iteration 25500, Testing net (#0)
I1123 18:20:35.531893 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:20:36.796295 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:20:36.847283 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8855
I1123 18:20:36.847283 16760 solver.cpp:397]     Test net output #1: loss = 0.336973 (* 1 = 0.336973 loss)
I1123 18:20:36.893306 16760 solver.cpp:218] Iteration 25500 (16.2902 iter/s, 6.13867s/100 iters), loss = 0.205683
I1123 18:20:36.893810 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:20:36.893810 16760 solver.cpp:237]     Train net output #1: loss = 0.205684 (* 1 = 0.205684 loss)
I1123 18:20:36.893810 16760 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1123 18:20:41.699820 16760 solver.cpp:218] Iteration 25600 (20.805 iter/s, 4.80653s/100 iters), loss = 0.185648
I1123 18:20:41.699820 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:20:41.699820 16760 solver.cpp:237]     Train net output #1: loss = 0.185649 (* 1 = 0.185649 loss)
I1123 18:20:41.699820 16760 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1123 18:20:46.507874 16760 solver.cpp:218] Iteration 25700 (20.8002 iter/s, 4.80764s/100 iters), loss = 0.207389
I1123 18:20:46.507874 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:20:46.507874 16760 solver.cpp:237]     Train net output #1: loss = 0.20739 (* 1 = 0.20739 loss)
I1123 18:20:46.507874 16760 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1123 18:20:51.318815 16760 solver.cpp:218] Iteration 25800 (20.7899 iter/s, 4.81003s/100 iters), loss = 0.215292
I1123 18:20:51.318815 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:20:51.318815 16760 solver.cpp:237]     Train net output #1: loss = 0.215293 (* 1 = 0.215293 loss)
I1123 18:20:51.318815 16760 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1123 18:20:56.112020 16760 solver.cpp:218] Iteration 25900 (20.8623 iter/s, 4.79334s/100 iters), loss = 0.176941
I1123 18:20:56.113021 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:20:56.113021 16760 solver.cpp:237]     Train net output #1: loss = 0.176941 (* 1 = 0.176941 loss)
I1123 18:20:56.113021 16760 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1123 18:21:00.670655 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:21:00.859233 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_26000.caffemodel
I1123 18:21:00.870723 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_26000.solverstate
I1123 18:21:00.875226 16760 solver.cpp:330] Iteration 26000, Testing net (#0)
I1123 18:21:00.875226 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:21:02.132133 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:21:02.182651 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8852
I1123 18:21:02.182651 16760 solver.cpp:397]     Test net output #1: loss = 0.336893 (* 1 = 0.336893 loss)
I1123 18:21:02.229158 16760 solver.cpp:218] Iteration 26000 (16.3508 iter/s, 6.11589s/100 iters), loss = 0.214791
I1123 18:21:02.229158 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:21:02.229158 16760 solver.cpp:237]     Train net output #1: loss = 0.214791 (* 1 = 0.214791 loss)
I1123 18:21:02.229158 16760 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1123 18:21:07.031350 16760 solver.cpp:218] Iteration 26100 (20.8224 iter/s, 4.80253s/100 iters), loss = 0.208202
I1123 18:21:07.032351 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:21:07.032351 16760 solver.cpp:237]     Train net output #1: loss = 0.208203 (* 1 = 0.208203 loss)
I1123 18:21:07.032351 16760 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1123 18:21:11.830584 16760 solver.cpp:218] Iteration 26200 (20.8403 iter/s, 4.79839s/100 iters), loss = 0.218327
I1123 18:21:11.830584 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:21:11.830584 16760 solver.cpp:237]     Train net output #1: loss = 0.218327 (* 1 = 0.218327 loss)
I1123 18:21:11.830584 16760 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1123 18:21:16.635737 16760 solver.cpp:218] Iteration 26300 (20.8119 iter/s, 4.80495s/100 iters), loss = 0.225639
I1123 18:21:16.635737 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:21:16.635737 16760 solver.cpp:237]     Train net output #1: loss = 0.22564 (* 1 = 0.22564 loss)
I1123 18:21:16.635737 16760 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1123 18:21:21.437151 16760 solver.cpp:218] Iteration 26400 (20.8282 iter/s, 4.80118s/100 iters), loss = 0.163859
I1123 18:21:21.437151 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:21:21.437151 16760 solver.cpp:237]     Train net output #1: loss = 0.16386 (* 1 = 0.16386 loss)
I1123 18:21:21.437151 16760 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1123 18:21:26.006741 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:21:26.196509 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_26500.caffemodel
I1123 18:21:26.206495 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_26500.solverstate
I1123 18:21:26.210494 16760 solver.cpp:330] Iteration 26500, Testing net (#0)
I1123 18:21:26.210494 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:21:27.477726 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:21:27.527235 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8851
I1123 18:21:27.527235 16760 solver.cpp:397]     Test net output #1: loss = 0.337007 (* 1 = 0.337007 loss)
I1123 18:21:27.572772 16760 solver.cpp:218] Iteration 26500 (16.2992 iter/s, 6.13526s/100 iters), loss = 0.21544
I1123 18:21:27.573791 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:21:27.573791 16760 solver.cpp:237]     Train net output #1: loss = 0.21544 (* 1 = 0.21544 loss)
I1123 18:21:27.573791 16760 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1123 18:21:32.383066 16760 solver.cpp:218] Iteration 26600 (20.7936 iter/s, 4.80917s/100 iters), loss = 0.226714
I1123 18:21:32.383066 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:21:32.383066 16760 solver.cpp:237]     Train net output #1: loss = 0.226715 (* 1 = 0.226715 loss)
I1123 18:21:32.383066 16760 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1123 18:21:37.188169 16760 solver.cpp:218] Iteration 26700 (20.8135 iter/s, 4.80458s/100 iters), loss = 0.171824
I1123 18:21:37.188169 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 18:21:37.188169 16760 solver.cpp:237]     Train net output #1: loss = 0.171824 (* 1 = 0.171824 loss)
I1123 18:21:37.188169 16760 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1123 18:21:41.991387 16760 solver.cpp:218] Iteration 26800 (20.8206 iter/s, 4.80293s/100 iters), loss = 0.210885
I1123 18:21:41.991387 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:21:41.991387 16760 solver.cpp:237]     Train net output #1: loss = 0.210885 (* 1 = 0.210885 loss)
I1123 18:21:41.991387 16760 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1123 18:21:46.802528 16760 solver.cpp:218] Iteration 26900 (20.7867 iter/s, 4.81077s/100 iters), loss = 0.156315
I1123 18:21:46.802528 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:21:46.802528 16760 solver.cpp:237]     Train net output #1: loss = 0.156316 (* 1 = 0.156316 loss)
I1123 18:21:46.802528 16760 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1123 18:21:51.369416 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:21:51.558446 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_27000.caffemodel
I1123 18:21:51.567446 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_27000.solverstate
I1123 18:21:51.571466 16760 solver.cpp:330] Iteration 27000, Testing net (#0)
I1123 18:21:51.571466 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:21:52.834794 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:21:52.885296 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8856
I1123 18:21:52.885296 16760 solver.cpp:397]     Test net output #1: loss = 0.336892 (* 1 = 0.336892 loss)
I1123 18:21:52.930825 16760 solver.cpp:218] Iteration 27000 (16.3178 iter/s, 6.12828s/100 iters), loss = 0.230209
I1123 18:21:52.930825 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:21:52.930825 16760 solver.cpp:237]     Train net output #1: loss = 0.230209 (* 1 = 0.230209 loss)
I1123 18:21:52.930825 16760 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1123 18:21:52.930825 16760 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1123 18:21:57.738011 16760 solver.cpp:218] Iteration 27100 (20.806 iter/s, 4.8063s/100 iters), loss = 0.263286
I1123 18:21:57.738011 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 18:21:57.738011 16760 solver.cpp:237]     Train net output #1: loss = 0.263286 (* 1 = 0.263286 loss)
I1123 18:21:57.738011 16760 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1123 18:22:02.542981 16760 solver.cpp:218] Iteration 27200 (20.8139 iter/s, 4.80448s/100 iters), loss = 0.178902
I1123 18:22:02.542981 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:22:02.542981 16760 solver.cpp:237]     Train net output #1: loss = 0.178903 (* 1 = 0.178903 loss)
I1123 18:22:02.542981 16760 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1123 18:22:07.347827 16760 solver.cpp:218] Iteration 27300 (20.8124 iter/s, 4.80484s/100 iters), loss = 0.219968
I1123 18:22:07.348331 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:22:07.348331 16760 solver.cpp:237]     Train net output #1: loss = 0.219968 (* 1 = 0.219968 loss)
I1123 18:22:07.348331 16760 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1123 18:22:12.153973 16760 solver.cpp:218] Iteration 27400 (20.8093 iter/s, 4.80553s/100 iters), loss = 0.151443
I1123 18:22:12.153973 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 18:22:12.153973 16760 solver.cpp:237]     Train net output #1: loss = 0.151444 (* 1 = 0.151444 loss)
I1123 18:22:12.153973 16760 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1123 18:22:16.728178 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:22:16.916761 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_27500.caffemodel
I1123 18:22:16.926764 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_27500.solverstate
I1123 18:22:16.930763 16760 solver.cpp:330] Iteration 27500, Testing net (#0)
I1123 18:22:16.931764 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:22:18.198637 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:22:18.248661 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8853
I1123 18:22:18.248661 16760 solver.cpp:397]     Test net output #1: loss = 0.336873 (* 1 = 0.336873 loss)
I1123 18:22:18.295192 16760 solver.cpp:218] Iteration 27500 (16.2846 iter/s, 6.14076s/100 iters), loss = 0.252088
I1123 18:22:18.295192 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:22:18.295192 16760 solver.cpp:237]     Train net output #1: loss = 0.252089 (* 1 = 0.252089 loss)
I1123 18:22:18.295192 16760 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1123 18:22:23.104487 16760 solver.cpp:218] Iteration 27600 (20.7938 iter/s, 4.80912s/100 iters), loss = 0.216673
I1123 18:22:23.104487 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:22:23.104487 16760 solver.cpp:237]     Train net output #1: loss = 0.216673 (* 1 = 0.216673 loss)
I1123 18:22:23.104487 16760 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1123 18:22:27.912396 16760 solver.cpp:218] Iteration 27700 (20.8 iter/s, 4.80768s/100 iters), loss = 0.186576
I1123 18:22:27.912396 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:22:27.912396 16760 solver.cpp:237]     Train net output #1: loss = 0.186576 (* 1 = 0.186576 loss)
I1123 18:22:27.912396 16760 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1123 18:22:32.719897 16760 solver.cpp:218] Iteration 27800 (20.8047 iter/s, 4.80662s/100 iters), loss = 0.206835
I1123 18:22:32.719897 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:22:32.719897 16760 solver.cpp:237]     Train net output #1: loss = 0.206835 (* 1 = 0.206835 loss)
I1123 18:22:32.719897 16760 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1123 18:22:37.526675 16760 solver.cpp:218] Iteration 27900 (20.8031 iter/s, 4.80697s/100 iters), loss = 0.222192
I1123 18:22:37.526675 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:22:37.526675 16760 solver.cpp:237]     Train net output #1: loss = 0.222192 (* 1 = 0.222192 loss)
I1123 18:22:37.526675 16760 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1123 18:22:42.102857 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:22:42.291932 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_28000.caffemodel
I1123 18:22:42.302932 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_28000.solverstate
I1123 18:22:42.305950 16760 solver.cpp:330] Iteration 28000, Testing net (#0)
I1123 18:22:42.306949 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:22:43.573469 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:22:43.623489 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8852
I1123 18:22:43.623489 16760 solver.cpp:397]     Test net output #1: loss = 0.336981 (* 1 = 0.336981 loss)
I1123 18:22:43.669499 16760 solver.cpp:218] Iteration 28000 (16.2808 iter/s, 6.14222s/100 iters), loss = 0.185735
I1123 18:22:43.669998 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:22:43.669998 16760 solver.cpp:237]     Train net output #1: loss = 0.185735 (* 1 = 0.185735 loss)
I1123 18:22:43.669998 16760 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1123 18:22:48.476773 16760 solver.cpp:218] Iteration 28100 (20.8053 iter/s, 4.80647s/100 iters), loss = 0.224142
I1123 18:22:48.476773 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:22:48.476773 16760 solver.cpp:237]     Train net output #1: loss = 0.224143 (* 1 = 0.224143 loss)
I1123 18:22:48.476773 16760 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1123 18:22:53.281083 16760 solver.cpp:218] Iteration 28200 (20.8143 iter/s, 4.80439s/100 iters), loss = 0.218122
I1123 18:22:53.281083 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:22:53.281083 16760 solver.cpp:237]     Train net output #1: loss = 0.218123 (* 1 = 0.218123 loss)
I1123 18:22:53.281083 16760 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1123 18:22:58.090626 16760 solver.cpp:218] Iteration 28300 (20.7948 iter/s, 4.80889s/100 iters), loss = 0.189887
I1123 18:22:58.090626 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:22:58.090626 16760 solver.cpp:237]     Train net output #1: loss = 0.189887 (* 1 = 0.189887 loss)
I1123 18:22:58.090626 16760 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1123 18:23:02.888512 16760 solver.cpp:218] Iteration 28400 (20.8438 iter/s, 4.79759s/100 iters), loss = 0.158285
I1123 18:23:02.888512 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:23:02.889014 16760 solver.cpp:237]     Train net output #1: loss = 0.158286 (* 1 = 0.158286 loss)
I1123 18:23:02.889014 16760 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1123 18:23:07.457227 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:23:07.646280 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_28500.caffemodel
I1123 18:23:07.656261 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_28500.solverstate
I1123 18:23:07.661259 16760 solver.cpp:330] Iteration 28500, Testing net (#0)
I1123 18:23:07.661259 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:23:08.928059 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:23:08.978071 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8853
I1123 18:23:08.978071 16760 solver.cpp:397]     Test net output #1: loss = 0.33693 (* 1 = 0.33693 loss)
I1123 18:23:09.025096 16760 solver.cpp:218] Iteration 28500 (16.2981 iter/s, 6.13569s/100 iters), loss = 0.200257
I1123 18:23:09.025096 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:23:09.025096 16760 solver.cpp:237]     Train net output #1: loss = 0.200258 (* 1 = 0.200258 loss)
I1123 18:23:09.025096 16760 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1123 18:23:13.837182 16760 solver.cpp:218] Iteration 28600 (20.7822 iter/s, 4.8118s/100 iters), loss = 0.220411
I1123 18:23:13.837182 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:23:13.837182 16760 solver.cpp:237]     Train net output #1: loss = 0.220411 (* 1 = 0.220411 loss)
I1123 18:23:13.837182 16760 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1123 18:23:18.646590 16760 solver.cpp:218] Iteration 28700 (20.7921 iter/s, 4.80953s/100 iters), loss = 0.212313
I1123 18:23:18.646590 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:23:18.646590 16760 solver.cpp:237]     Train net output #1: loss = 0.212313 (* 1 = 0.212313 loss)
I1123 18:23:18.646590 16760 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1123 18:23:23.452073 16760 solver.cpp:218] Iteration 28800 (20.8104 iter/s, 4.8053s/100 iters), loss = 0.210028
I1123 18:23:23.453073 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:23:23.453073 16760 solver.cpp:237]     Train net output #1: loss = 0.210028 (* 1 = 0.210028 loss)
I1123 18:23:23.453073 16760 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1123 18:23:28.308734 16760 solver.cpp:218] Iteration 28900 (20.5948 iter/s, 4.85559s/100 iters), loss = 0.177054
I1123 18:23:28.308734 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:23:28.308734 16760 solver.cpp:237]     Train net output #1: loss = 0.177055 (* 1 = 0.177055 loss)
I1123 18:23:28.308734 16760 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1123 18:23:32.876960 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:23:33.064971 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_29000.caffemodel
I1123 18:23:33.075971 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_29000.solverstate
I1123 18:23:33.079972 16760 solver.cpp:330] Iteration 29000, Testing net (#0)
I1123 18:23:33.079972 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:23:34.341572 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:23:34.392082 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8855
I1123 18:23:34.392082 16760 solver.cpp:397]     Test net output #1: loss = 0.336994 (* 1 = 0.336994 loss)
I1123 18:23:34.438074 16760 solver.cpp:218] Iteration 29000 (16.3163 iter/s, 6.12883s/100 iters), loss = 0.235209
I1123 18:23:34.438074 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:23:34.438074 16760 solver.cpp:237]     Train net output #1: loss = 0.235209 (* 1 = 0.235209 loss)
I1123 18:23:34.438074 16760 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1123 18:23:39.231362 16760 solver.cpp:218] Iteration 29100 (20.8614 iter/s, 4.79354s/100 iters), loss = 0.192576
I1123 18:23:39.231362 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:23:39.231362 16760 solver.cpp:237]     Train net output #1: loss = 0.192577 (* 1 = 0.192577 loss)
I1123 18:23:39.231362 16760 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1123 18:23:44.024713 16760 solver.cpp:218] Iteration 29200 (20.8647 iter/s, 4.79279s/100 iters), loss = 0.191213
I1123 18:23:44.024713 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:23:44.024713 16760 solver.cpp:237]     Train net output #1: loss = 0.191213 (* 1 = 0.191213 loss)
I1123 18:23:44.024713 16760 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1123 18:23:48.819166 16760 solver.cpp:218] Iteration 29300 (20.8618 iter/s, 4.79346s/100 iters), loss = 0.189635
I1123 18:23:48.819166 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:23:48.819166 16760 solver.cpp:237]     Train net output #1: loss = 0.189635 (* 1 = 0.189635 loss)
I1123 18:23:48.819166 16760 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1123 18:23:53.617436 16760 solver.cpp:218] Iteration 29400 (20.8417 iter/s, 4.79806s/100 iters), loss = 0.17146
I1123 18:23:53.617436 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 18:23:53.617436 16760 solver.cpp:237]     Train net output #1: loss = 0.17146 (* 1 = 0.17146 loss)
I1123 18:23:53.617436 16760 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1123 18:23:58.177712 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:23:58.366731 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_29500.caffemodel
I1123 18:23:58.376731 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_29500.solverstate
I1123 18:23:58.380731 16760 solver.cpp:330] Iteration 29500, Testing net (#0)
I1123 18:23:58.380731 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:23:59.641824 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:23:59.691831 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8852
I1123 18:23:59.691831 16760 solver.cpp:397]     Test net output #1: loss = 0.336914 (* 1 = 0.336914 loss)
I1123 18:23:59.738826 16760 solver.cpp:218] Iteration 29500 (16.3373 iter/s, 6.12095s/100 iters), loss = 0.236696
I1123 18:23:59.738826 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 18:23:59.738826 16760 solver.cpp:237]     Train net output #1: loss = 0.236696 (* 1 = 0.236696 loss)
I1123 18:23:59.738826 16760 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1123 18:24:04.532140 16760 solver.cpp:218] Iteration 29600 (20.8647 iter/s, 4.79278s/100 iters), loss = 0.222181
I1123 18:24:04.532140 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 18:24:04.532140 16760 solver.cpp:237]     Train net output #1: loss = 0.222181 (* 1 = 0.222181 loss)
I1123 18:24:04.532140 16760 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1123 18:24:09.326606 16760 solver.cpp:218] Iteration 29700 (20.8573 iter/s, 4.79448s/100 iters), loss = 0.182661
I1123 18:24:09.326606 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 18:24:09.326606 16760 solver.cpp:237]     Train net output #1: loss = 0.182662 (* 1 = 0.182662 loss)
I1123 18:24:09.326606 16760 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1123 18:24:14.121170 16760 solver.cpp:218] Iteration 29800 (20.8591 iter/s, 4.79407s/100 iters), loss = 0.210987
I1123 18:24:14.121170 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 18:24:14.121170 16760 solver.cpp:237]     Train net output #1: loss = 0.210987 (* 1 = 0.210987 loss)
I1123 18:24:14.121170 16760 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1123 18:24:18.910467 16760 solver.cpp:218] Iteration 29900 (20.8825 iter/s, 4.78869s/100 iters), loss = 0.161818
I1123 18:24:18.910467 16760 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 18:24:18.910467 16760 solver.cpp:237]     Train net output #1: loss = 0.161818 (* 1 = 0.161818 loss)
I1123 18:24:18.910467 16760 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1123 18:24:23.461238 27420 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:24:23.648747 16760 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_30000.caffemodel
I1123 18:24:23.659247 16760 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_30000.solverstate
I1123 18:24:23.677759 16760 solver.cpp:310] Iteration 30000, loss = 0.212817
I1123 18:24:23.677759 16760 solver.cpp:330] Iteration 30000, Testing net (#0)
I1123 18:24:23.677759 16760 net.cpp:676] Ignoring source layer accuracy_training
I1123 18:24:24.938853 35284 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:24:24.988860 16760 solver.cpp:397]     Test net output #0: accuracy = 0.8851
I1123 18:24:24.988860 16760 solver.cpp:397]     Test net output #1: loss = 0.336934 (* 1 = 0.336934 loss)
I1123 18:24:24.988860 16760 solver.cpp:315] Optimization Done.
I1123 18:24:24.988860 16760 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 
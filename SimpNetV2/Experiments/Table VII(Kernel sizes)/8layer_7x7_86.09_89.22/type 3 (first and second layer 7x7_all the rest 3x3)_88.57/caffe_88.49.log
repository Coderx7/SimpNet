
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1123 16:37:08.299648 14636 caffe.cpp:219] Using GPUs 0
I1123 16:37:08.456218 14636 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1123 16:37:08.754426 14636 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 16:37:08.770444 14636 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1123 16:37:08.771445 14636 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 16:37:08.771445 14636 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 16:37:08.771445 14636 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 16:37:08.771445 14636 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1123 16:37:08.771445 14636 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1123 16:37:08.771445 14636 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1123 16:37:08.771445 14636 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1123 16:37:08.771445 14636 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1123 16:37:08.771445 14636 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1123 16:37:08.771445 14636 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1123 16:37:08.771445 14636 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1123 16:37:08.771445 14636 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1123 16:37:08.771445 14636 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1123 16:37:08.771445 14636 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k_7x7_first2layers"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 35
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 38
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 74
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 75
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 16:37:08.804464 14636 layer_factory.cpp:58] Creating layer cifar
I1123 16:37:08.908607 14636 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1123 16:37:08.908607 14636 net.cpp:84] Creating Layer cifar
I1123 16:37:08.908607 14636 net.cpp:380] cifar -> data
I1123 16:37:08.908607 14636 net.cpp:380] cifar -> label
I1123 16:37:08.909607 14636 data_layer.cpp:45] output data size: 100,3,32,32
I1123 16:37:08.916618 14636 net.cpp:122] Setting up cifar
I1123 16:37:08.916618 14636 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 16:37:08.916618 14636 net.cpp:129] Top shape: 100 (100)
I1123 16:37:08.916618 14636 net.cpp:137] Memory required for data: 1229200
I1123 16:37:08.916618 14636 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 16:37:08.916618 14636 net.cpp:84] Creating Layer label_cifar_1_split
I1123 16:37:08.916618 14636 net.cpp:406] label_cifar_1_split <- label
I1123 16:37:08.916618 14636 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 16:37:08.916618 14636 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 16:37:08.916618 14636 net.cpp:122] Setting up label_cifar_1_split
I1123 16:37:08.916618 14636 net.cpp:129] Top shape: 100 (100)
I1123 16:37:08.916618 14636 net.cpp:129] Top shape: 100 (100)
I1123 16:37:08.916618 14636 net.cpp:137] Memory required for data: 1230000
I1123 16:37:08.916618 14636 layer_factory.cpp:58] Creating layer conv1
I1123 16:37:08.916618 14636 net.cpp:84] Creating Layer conv1
I1123 16:37:08.916618 14636 net.cpp:406] conv1 <- data
I1123 16:37:08.916618 14636 net.cpp:380] conv1 -> conv1
I1123 16:37:08.917608 22208 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 16:37:09.175657 14636 net.cpp:122] Setting up conv1
I1123 16:37:09.175657 14636 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 16:37:09.175657 14636 net.cpp:137] Memory required for data: 15566000
I1123 16:37:09.175657 14636 layer_factory.cpp:58] Creating layer bn1
I1123 16:37:09.175657 14636 net.cpp:84] Creating Layer bn1
I1123 16:37:09.175657 14636 net.cpp:406] bn1 <- conv1
I1123 16:37:09.175657 14636 net.cpp:367] bn1 -> conv1 (in-place)
I1123 16:37:09.175657 14636 net.cpp:122] Setting up bn1
I1123 16:37:09.175657 14636 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 16:37:09.175657 14636 net.cpp:137] Memory required for data: 29902000
I1123 16:37:09.175657 14636 layer_factory.cpp:58] Creating layer scale1
I1123 16:37:09.175657 14636 net.cpp:84] Creating Layer scale1
I1123 16:37:09.175657 14636 net.cpp:406] scale1 <- conv1
I1123 16:37:09.175657 14636 net.cpp:367] scale1 -> conv1 (in-place)
I1123 16:37:09.175657 14636 layer_factory.cpp:58] Creating layer scale1
I1123 16:37:09.176658 14636 net.cpp:122] Setting up scale1
I1123 16:37:09.176658 14636 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 16:37:09.176658 14636 net.cpp:137] Memory required for data: 44238000
I1123 16:37:09.176658 14636 layer_factory.cpp:58] Creating layer relu1
I1123 16:37:09.176658 14636 net.cpp:84] Creating Layer relu1
I1123 16:37:09.176658 14636 net.cpp:406] relu1 <- conv1
I1123 16:37:09.176658 14636 net.cpp:367] relu1 -> conv1 (in-place)
I1123 16:37:09.176658 14636 net.cpp:122] Setting up relu1
I1123 16:37:09.176658 14636 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 16:37:09.176658 14636 net.cpp:137] Memory required for data: 58574000
I1123 16:37:09.176658 14636 layer_factory.cpp:58] Creating layer conv2
I1123 16:37:09.176658 14636 net.cpp:84] Creating Layer conv2
I1123 16:37:09.176658 14636 net.cpp:406] conv2 <- conv1
I1123 16:37:09.176658 14636 net.cpp:380] conv2 -> conv2
I1123 16:37:09.178656 14636 net.cpp:122] Setting up conv2
I1123 16:37:09.178656 14636 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 16:37:09.178656 14636 net.cpp:137] Memory required for data: 74138800
I1123 16:37:09.178656 14636 layer_factory.cpp:58] Creating layer bn2
I1123 16:37:09.178656 14636 net.cpp:84] Creating Layer bn2
I1123 16:37:09.178656 14636 net.cpp:406] bn2 <- conv2
I1123 16:37:09.178656 14636 net.cpp:367] bn2 -> conv2 (in-place)
I1123 16:37:09.179658 14636 net.cpp:122] Setting up bn2
I1123 16:37:09.179658 14636 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 16:37:09.179658 14636 net.cpp:137] Memory required for data: 89703600
I1123 16:37:09.179658 14636 layer_factory.cpp:58] Creating layer scale2
I1123 16:37:09.179658 14636 net.cpp:84] Creating Layer scale2
I1123 16:37:09.179658 14636 net.cpp:406] scale2 <- conv2
I1123 16:37:09.179658 14636 net.cpp:367] scale2 -> conv2 (in-place)
I1123 16:37:09.179658 14636 layer_factory.cpp:58] Creating layer scale2
I1123 16:37:09.179658 14636 net.cpp:122] Setting up scale2
I1123 16:37:09.179658 14636 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 16:37:09.179658 14636 net.cpp:137] Memory required for data: 105268400
I1123 16:37:09.179658 14636 layer_factory.cpp:58] Creating layer relu2
I1123 16:37:09.179658 14636 net.cpp:84] Creating Layer relu2
I1123 16:37:09.179658 14636 net.cpp:406] relu2 <- conv2
I1123 16:37:09.179658 14636 net.cpp:367] relu2 -> conv2 (in-place)
I1123 16:37:09.179658 14636 net.cpp:122] Setting up relu2
I1123 16:37:09.179658 14636 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 16:37:09.179658 14636 net.cpp:137] Memory required for data: 120833200
I1123 16:37:09.179658 14636 layer_factory.cpp:58] Creating layer conv2_2
I1123 16:37:09.179658 14636 net.cpp:84] Creating Layer conv2_2
I1123 16:37:09.179658 14636 net.cpp:406] conv2_2 <- conv2
I1123 16:37:09.179658 14636 net.cpp:380] conv2_2 -> conv2_2
I1123 16:37:09.181656 14636 net.cpp:122] Setting up conv2_2
I1123 16:37:09.181656 14636 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 16:37:09.181656 14636 net.cpp:137] Memory required for data: 147457200
I1123 16:37:09.181656 14636 layer_factory.cpp:58] Creating layer bn2_2
I1123 16:37:09.181656 14636 net.cpp:84] Creating Layer bn2_2
I1123 16:37:09.181656 14636 net.cpp:406] bn2_2 <- conv2_2
I1123 16:37:09.181656 14636 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 16:37:09.181656 14636 net.cpp:122] Setting up bn2_2
I1123 16:37:09.181656 14636 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 16:37:09.181656 14636 net.cpp:137] Memory required for data: 174081200
I1123 16:37:09.181656 14636 layer_factory.cpp:58] Creating layer scale2_2
I1123 16:37:09.181656 14636 net.cpp:84] Creating Layer scale2_2
I1123 16:37:09.181656 14636 net.cpp:406] scale2_2 <- conv2_2
I1123 16:37:09.181656 14636 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 16:37:09.181656 14636 layer_factory.cpp:58] Creating layer scale2_2
I1123 16:37:09.181656 14636 net.cpp:122] Setting up scale2_2
I1123 16:37:09.181656 14636 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 16:37:09.181656 14636 net.cpp:137] Memory required for data: 200705200
I1123 16:37:09.181656 14636 layer_factory.cpp:58] Creating layer relu2_2
I1123 16:37:09.181656 14636 net.cpp:84] Creating Layer relu2_2
I1123 16:37:09.181656 14636 net.cpp:406] relu2_2 <- conv2_2
I1123 16:37:09.181656 14636 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 16:37:09.182658 14636 net.cpp:122] Setting up relu2_2
I1123 16:37:09.182658 14636 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 16:37:09.182658 14636 net.cpp:137] Memory required for data: 227329200
I1123 16:37:09.182658 14636 layer_factory.cpp:58] Creating layer pool2_1
I1123 16:37:09.182658 14636 net.cpp:84] Creating Layer pool2_1
I1123 16:37:09.182658 14636 net.cpp:406] pool2_1 <- conv2_2
I1123 16:37:09.182658 14636 net.cpp:380] pool2_1 -> pool2_1
I1123 16:37:09.182658 14636 net.cpp:122] Setting up pool2_1
I1123 16:37:09.182658 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.182658 14636 net.cpp:137] Memory required for data: 233985200
I1123 16:37:09.182658 14636 layer_factory.cpp:58] Creating layer conv3
I1123 16:37:09.182658 14636 net.cpp:84] Creating Layer conv3
I1123 16:37:09.182658 14636 net.cpp:406] conv3 <- pool2_1
I1123 16:37:09.182658 14636 net.cpp:380] conv3 -> conv3
I1123 16:37:09.183665 14636 net.cpp:122] Setting up conv3
I1123 16:37:09.183665 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.183665 14636 net.cpp:137] Memory required for data: 240641200
I1123 16:37:09.183665 14636 layer_factory.cpp:58] Creating layer bn3
I1123 16:37:09.183665 14636 net.cpp:84] Creating Layer bn3
I1123 16:37:09.183665 14636 net.cpp:406] bn3 <- conv3
I1123 16:37:09.183665 14636 net.cpp:367] bn3 -> conv3 (in-place)
I1123 16:37:09.184658 14636 net.cpp:122] Setting up bn3
I1123 16:37:09.184658 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.184658 14636 net.cpp:137] Memory required for data: 247297200
I1123 16:37:09.184658 14636 layer_factory.cpp:58] Creating layer scale3
I1123 16:37:09.184658 14636 net.cpp:84] Creating Layer scale3
I1123 16:37:09.184658 14636 net.cpp:406] scale3 <- conv3
I1123 16:37:09.184658 14636 net.cpp:367] scale3 -> conv3 (in-place)
I1123 16:37:09.184658 14636 layer_factory.cpp:58] Creating layer scale3
I1123 16:37:09.184658 14636 net.cpp:122] Setting up scale3
I1123 16:37:09.184658 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.184658 14636 net.cpp:137] Memory required for data: 253953200
I1123 16:37:09.184658 14636 layer_factory.cpp:58] Creating layer relu3
I1123 16:37:09.184658 14636 net.cpp:84] Creating Layer relu3
I1123 16:37:09.184658 14636 net.cpp:406] relu3 <- conv3
I1123 16:37:09.184658 14636 net.cpp:367] relu3 -> conv3 (in-place)
I1123 16:37:09.184658 14636 net.cpp:122] Setting up relu3
I1123 16:37:09.184658 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.184658 14636 net.cpp:137] Memory required for data: 260609200
I1123 16:37:09.184658 14636 layer_factory.cpp:58] Creating layer conv4
I1123 16:37:09.184658 14636 net.cpp:84] Creating Layer conv4
I1123 16:37:09.184658 14636 net.cpp:406] conv4 <- conv3
I1123 16:37:09.184658 14636 net.cpp:380] conv4 -> conv4
I1123 16:37:09.186659 14636 net.cpp:122] Setting up conv4
I1123 16:37:09.186659 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.186659 14636 net.cpp:137] Memory required for data: 267265200
I1123 16:37:09.186659 14636 layer_factory.cpp:58] Creating layer bn4
I1123 16:37:09.186659 14636 net.cpp:84] Creating Layer bn4
I1123 16:37:09.186659 14636 net.cpp:406] bn4 <- conv4
I1123 16:37:09.186659 14636 net.cpp:367] bn4 -> conv4 (in-place)
I1123 16:37:09.187667 14636 net.cpp:122] Setting up bn4
I1123 16:37:09.187667 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.187667 14636 net.cpp:137] Memory required for data: 273921200
I1123 16:37:09.187667 14636 layer_factory.cpp:58] Creating layer scale4
I1123 16:37:09.187667 14636 net.cpp:84] Creating Layer scale4
I1123 16:37:09.187667 14636 net.cpp:406] scale4 <- conv4
I1123 16:37:09.187667 14636 net.cpp:367] scale4 -> conv4 (in-place)
I1123 16:37:09.187667 14636 layer_factory.cpp:58] Creating layer scale4
I1123 16:37:09.187667 14636 net.cpp:122] Setting up scale4
I1123 16:37:09.187667 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.187667 14636 net.cpp:137] Memory required for data: 280577200
I1123 16:37:09.187667 14636 layer_factory.cpp:58] Creating layer relu4
I1123 16:37:09.187667 14636 net.cpp:84] Creating Layer relu4
I1123 16:37:09.187667 14636 net.cpp:406] relu4 <- conv4
I1123 16:37:09.187667 14636 net.cpp:367] relu4 -> conv4 (in-place)
I1123 16:37:09.188657 14636 net.cpp:122] Setting up relu4
I1123 16:37:09.188657 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.188657 14636 net.cpp:137] Memory required for data: 287233200
I1123 16:37:09.188657 14636 layer_factory.cpp:58] Creating layer conv4_1
I1123 16:37:09.188657 14636 net.cpp:84] Creating Layer conv4_1
I1123 16:37:09.188657 14636 net.cpp:406] conv4_1 <- conv4
I1123 16:37:09.188657 14636 net.cpp:380] conv4_1 -> conv4_1
I1123 16:37:09.189657 14636 net.cpp:122] Setting up conv4_1
I1123 16:37:09.189657 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.189657 14636 net.cpp:137] Memory required for data: 293889200
I1123 16:37:09.189657 14636 layer_factory.cpp:58] Creating layer bn4_1
I1123 16:37:09.189657 14636 net.cpp:84] Creating Layer bn4_1
I1123 16:37:09.189657 14636 net.cpp:406] bn4_1 <- conv4_1
I1123 16:37:09.189657 14636 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 16:37:09.189657 14636 net.cpp:122] Setting up bn4_1
I1123 16:37:09.189657 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.189657 14636 net.cpp:137] Memory required for data: 300545200
I1123 16:37:09.189657 14636 layer_factory.cpp:58] Creating layer scale4_1
I1123 16:37:09.189657 14636 net.cpp:84] Creating Layer scale4_1
I1123 16:37:09.189657 14636 net.cpp:406] scale4_1 <- conv4_1
I1123 16:37:09.189657 14636 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 16:37:09.190654 14636 layer_factory.cpp:58] Creating layer scale4_1
I1123 16:37:09.190654 14636 net.cpp:122] Setting up scale4_1
I1123 16:37:09.190654 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.190654 14636 net.cpp:137] Memory required for data: 307201200
I1123 16:37:09.190654 14636 layer_factory.cpp:58] Creating layer relu4_1
I1123 16:37:09.190654 14636 net.cpp:84] Creating Layer relu4_1
I1123 16:37:09.190654 14636 net.cpp:406] relu4_1 <- conv4_1
I1123 16:37:09.190654 14636 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 16:37:09.190654 14636 net.cpp:122] Setting up relu4_1
I1123 16:37:09.190654 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.190654 14636 net.cpp:137] Memory required for data: 313857200
I1123 16:37:09.190654 14636 layer_factory.cpp:58] Creating layer conv4_2
I1123 16:37:09.190654 14636 net.cpp:84] Creating Layer conv4_2
I1123 16:37:09.190654 14636 net.cpp:406] conv4_2 <- conv4_1
I1123 16:37:09.190654 14636 net.cpp:380] conv4_2 -> conv4_2
I1123 16:37:09.192654 14636 net.cpp:122] Setting up conv4_2
I1123 16:37:09.192654 14636 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 16:37:09.192654 14636 net.cpp:137] Memory required for data: 321434800
I1123 16:37:09.192654 14636 layer_factory.cpp:58] Creating layer bn4_2
I1123 16:37:09.192654 14636 net.cpp:84] Creating Layer bn4_2
I1123 16:37:09.192654 14636 net.cpp:406] bn4_2 <- conv4_2
I1123 16:37:09.192654 14636 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 16:37:09.192654 14636 net.cpp:122] Setting up bn4_2
I1123 16:37:09.192654 14636 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 16:37:09.192654 14636 net.cpp:137] Memory required for data: 329012400
I1123 16:37:09.192654 14636 layer_factory.cpp:58] Creating layer scale4_2
I1123 16:37:09.192654 14636 net.cpp:84] Creating Layer scale4_2
I1123 16:37:09.192654 14636 net.cpp:406] scale4_2 <- conv4_2
I1123 16:37:09.192654 14636 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 16:37:09.192654 14636 layer_factory.cpp:58] Creating layer scale4_2
I1123 16:37:09.192654 14636 net.cpp:122] Setting up scale4_2
I1123 16:37:09.192654 14636 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 16:37:09.192654 14636 net.cpp:137] Memory required for data: 336590000
I1123 16:37:09.192654 14636 layer_factory.cpp:58] Creating layer relu4_2
I1123 16:37:09.192654 14636 net.cpp:84] Creating Layer relu4_2
I1123 16:37:09.193656 14636 net.cpp:406] relu4_2 <- conv4_2
I1123 16:37:09.193656 14636 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 16:37:09.193656 14636 net.cpp:122] Setting up relu4_2
I1123 16:37:09.193656 14636 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 16:37:09.193656 14636 net.cpp:137] Memory required for data: 344167600
I1123 16:37:09.193656 14636 layer_factory.cpp:58] Creating layer pool4_2
I1123 16:37:09.193656 14636 net.cpp:84] Creating Layer pool4_2
I1123 16:37:09.193656 14636 net.cpp:406] pool4_2 <- conv4_2
I1123 16:37:09.193656 14636 net.cpp:380] pool4_2 -> pool4_2
I1123 16:37:09.193656 14636 net.cpp:122] Setting up pool4_2
I1123 16:37:09.193656 14636 net.cpp:129] Top shape: 100 74 8 8 (473600)
I1123 16:37:09.193656 14636 net.cpp:137] Memory required for data: 346062000
I1123 16:37:09.193656 14636 layer_factory.cpp:58] Creating layer conv12
I1123 16:37:09.193656 14636 net.cpp:84] Creating Layer conv12
I1123 16:37:09.193656 14636 net.cpp:406] conv12 <- pool4_2
I1123 16:37:09.193656 14636 net.cpp:380] conv12 -> conv12
I1123 16:37:09.195657 14636 net.cpp:122] Setting up conv12
I1123 16:37:09.195657 14636 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 16:37:09.195657 14636 net.cpp:137] Memory required for data: 347982000
I1123 16:37:09.195657 14636 layer_factory.cpp:58] Creating layer bn_conv12
I1123 16:37:09.195657 14636 net.cpp:84] Creating Layer bn_conv12
I1123 16:37:09.195657 14636 net.cpp:406] bn_conv12 <- conv12
I1123 16:37:09.195657 14636 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 16:37:09.195657 14636 net.cpp:122] Setting up bn_conv12
I1123 16:37:09.195657 14636 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 16:37:09.195657 14636 net.cpp:137] Memory required for data: 349902000
I1123 16:37:09.195657 14636 layer_factory.cpp:58] Creating layer scale_conv12
I1123 16:37:09.195657 14636 net.cpp:84] Creating Layer scale_conv12
I1123 16:37:09.195657 14636 net.cpp:406] scale_conv12 <- conv12
I1123 16:37:09.195657 14636 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 16:37:09.195657 14636 layer_factory.cpp:58] Creating layer scale_conv12
I1123 16:37:09.195657 14636 net.cpp:122] Setting up scale_conv12
I1123 16:37:09.195657 14636 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 16:37:09.195657 14636 net.cpp:137] Memory required for data: 351822000
I1123 16:37:09.195657 14636 layer_factory.cpp:58] Creating layer relu_conv12
I1123 16:37:09.195657 14636 net.cpp:84] Creating Layer relu_conv12
I1123 16:37:09.195657 14636 net.cpp:406] relu_conv12 <- conv12
I1123 16:37:09.195657 14636 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 16:37:09.195657 14636 net.cpp:122] Setting up relu_conv12
I1123 16:37:09.195657 14636 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 16:37:09.195657 14636 net.cpp:137] Memory required for data: 353742000
I1123 16:37:09.195657 14636 layer_factory.cpp:58] Creating layer poolcp6
I1123 16:37:09.195657 14636 net.cpp:84] Creating Layer poolcp6
I1123 16:37:09.195657 14636 net.cpp:406] poolcp6 <- conv12
I1123 16:37:09.195657 14636 net.cpp:380] poolcp6 -> poolcp6
I1123 16:37:09.195657 14636 net.cpp:122] Setting up poolcp6
I1123 16:37:09.195657 14636 net.cpp:129] Top shape: 100 75 1 1 (7500)
I1123 16:37:09.195657 14636 net.cpp:137] Memory required for data: 353772000
I1123 16:37:09.195657 14636 layer_factory.cpp:58] Creating layer ip1
I1123 16:37:09.195657 14636 net.cpp:84] Creating Layer ip1
I1123 16:37:09.195657 14636 net.cpp:406] ip1 <- poolcp6
I1123 16:37:09.195657 14636 net.cpp:380] ip1 -> ip1
I1123 16:37:09.196658 14636 net.cpp:122] Setting up ip1
I1123 16:37:09.196658 14636 net.cpp:129] Top shape: 100 10 (1000)
I1123 16:37:09.196658 14636 net.cpp:137] Memory required for data: 353776000
I1123 16:37:09.196658 14636 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 16:37:09.196658 14636 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 16:37:09.196658 14636 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 16:37:09.196658 14636 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 16:37:09.196658 14636 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 16:37:09.196658 14636 net.cpp:122] Setting up ip1_ip1_0_split
I1123 16:37:09.196658 14636 net.cpp:129] Top shape: 100 10 (1000)
I1123 16:37:09.196658 14636 net.cpp:129] Top shape: 100 10 (1000)
I1123 16:37:09.196658 14636 net.cpp:137] Memory required for data: 353784000
I1123 16:37:09.196658 14636 layer_factory.cpp:58] Creating layer accuracy_training
I1123 16:37:09.196658 14636 net.cpp:84] Creating Layer accuracy_training
I1123 16:37:09.196658 14636 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1123 16:37:09.196658 14636 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1123 16:37:09.196658 14636 net.cpp:380] accuracy_training -> accuracy_training
I1123 16:37:09.196658 14636 net.cpp:122] Setting up accuracy_training
I1123 16:37:09.196658 14636 net.cpp:129] Top shape: (1)
I1123 16:37:09.196658 14636 net.cpp:137] Memory required for data: 353784004
I1123 16:37:09.196658 14636 layer_factory.cpp:58] Creating layer loss
I1123 16:37:09.196658 14636 net.cpp:84] Creating Layer loss
I1123 16:37:09.196658 14636 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 16:37:09.196658 14636 net.cpp:406] loss <- label_cifar_1_split_1
I1123 16:37:09.196658 14636 net.cpp:380] loss -> loss
I1123 16:37:09.196658 14636 layer_factory.cpp:58] Creating layer loss
I1123 16:37:09.196658 14636 net.cpp:122] Setting up loss
I1123 16:37:09.196658 14636 net.cpp:129] Top shape: (1)
I1123 16:37:09.196658 14636 net.cpp:132]     with loss weight 1
I1123 16:37:09.196658 14636 net.cpp:137] Memory required for data: 353784008
I1123 16:37:09.196658 14636 net.cpp:198] loss needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:200] accuracy_training does not need backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] ip1 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] poolcp6 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] relu_conv12 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] scale_conv12 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] bn_conv12 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] conv12 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] pool4_2 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] relu4_2 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] scale4_2 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] bn4_2 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] conv4_2 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] relu4_1 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] scale4_1 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] bn4_1 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] conv4_1 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] relu4 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] scale4 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] bn4 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] conv4 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] relu3 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] scale3 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] bn3 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] conv3 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] pool2_1 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] relu2_2 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] scale2_2 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] bn2_2 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] conv2_2 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] relu2 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] scale2 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] bn2 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] conv2 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] relu1 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] scale1 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] bn1 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:198] conv1 needs backward computation.
I1123 16:37:09.196658 14636 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 16:37:09.196658 14636 net.cpp:200] cifar does not need backward computation.
I1123 16:37:09.196658 14636 net.cpp:242] This network produces output accuracy_training
I1123 16:37:09.196658 14636 net.cpp:242] This network produces output loss
I1123 16:37:09.196658 14636 net.cpp:255] Network initialization done.
I1123 16:37:09.197657 14636 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 16:37:09.197657 14636 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 16:37:09.197657 14636 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 16:37:09.197657 14636 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1123 16:37:09.197657 14636 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1123 16:37:09.197657 14636 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1123 16:37:09.197657 14636 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1123 16:37:09.197657 14636 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1123 16:37:09.197657 14636 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1123 16:37:09.197657 14636 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1123 16:37:09.197657 14636 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1123 16:37:09.197657 14636 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1123 16:37:09.197657 14636 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1123 16:37:09.197657 14636 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k_7x7_first2layers"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 35
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 38
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 65
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 74
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 75
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 16:37:09.197657 14636 layer_factory.cpp:58] Creating layer cifar
I1123 16:37:09.204654 14636 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1123 16:37:09.204654 14636 net.cpp:84] Creating Layer cifar
I1123 16:37:09.204654 14636 net.cpp:380] cifar -> data
I1123 16:37:09.204654 14636 net.cpp:380] cifar -> label
I1123 16:37:09.204654 14636 data_layer.cpp:45] output data size: 100,3,32,32
I1123 16:37:09.210657 14636 net.cpp:122] Setting up cifar
I1123 16:37:09.210657 14636 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 16:37:09.210657 14636 net.cpp:129] Top shape: 100 (100)
I1123 16:37:09.210657 14636 net.cpp:137] Memory required for data: 1229200
I1123 16:37:09.210657 14636 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 16:37:09.210657 14636 net.cpp:84] Creating Layer label_cifar_1_split
I1123 16:37:09.210657 14636 net.cpp:406] label_cifar_1_split <- label
I1123 16:37:09.210657 14636 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 16:37:09.210657 14636 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 16:37:09.210657 14636 net.cpp:122] Setting up label_cifar_1_split
I1123 16:37:09.210657 14636 net.cpp:129] Top shape: 100 (100)
I1123 16:37:09.210657 14636 net.cpp:129] Top shape: 100 (100)
I1123 16:37:09.210657 14636 net.cpp:137] Memory required for data: 1230000
I1123 16:37:09.210657 14636 layer_factory.cpp:58] Creating layer conv1
I1123 16:37:09.210657 14636 net.cpp:84] Creating Layer conv1
I1123 16:37:09.210657 14636 net.cpp:406] conv1 <- data
I1123 16:37:09.210657 14636 net.cpp:380] conv1 -> conv1
I1123 16:37:09.211660 14636 net.cpp:122] Setting up conv1
I1123 16:37:09.212658 14636 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 16:37:09.212658 14636 net.cpp:137] Memory required for data: 15566000
I1123 16:37:09.212658 24900 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 16:37:09.212658 14636 layer_factory.cpp:58] Creating layer bn1
I1123 16:37:09.212658 14636 net.cpp:84] Creating Layer bn1
I1123 16:37:09.212658 14636 net.cpp:406] bn1 <- conv1
I1123 16:37:09.212658 14636 net.cpp:367] bn1 -> conv1 (in-place)
I1123 16:37:09.212658 14636 net.cpp:122] Setting up bn1
I1123 16:37:09.212658 14636 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 16:37:09.212658 14636 net.cpp:137] Memory required for data: 29902000
I1123 16:37:09.212658 14636 layer_factory.cpp:58] Creating layer scale1
I1123 16:37:09.212658 14636 net.cpp:84] Creating Layer scale1
I1123 16:37:09.212658 14636 net.cpp:406] scale1 <- conv1
I1123 16:37:09.212658 14636 net.cpp:367] scale1 -> conv1 (in-place)
I1123 16:37:09.212658 14636 layer_factory.cpp:58] Creating layer scale1
I1123 16:37:09.212658 14636 net.cpp:122] Setting up scale1
I1123 16:37:09.212658 14636 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 16:37:09.212658 14636 net.cpp:137] Memory required for data: 44238000
I1123 16:37:09.212658 14636 layer_factory.cpp:58] Creating layer relu1
I1123 16:37:09.212658 14636 net.cpp:84] Creating Layer relu1
I1123 16:37:09.212658 14636 net.cpp:406] relu1 <- conv1
I1123 16:37:09.212658 14636 net.cpp:367] relu1 -> conv1 (in-place)
I1123 16:37:09.213656 14636 net.cpp:122] Setting up relu1
I1123 16:37:09.213656 14636 net.cpp:129] Top shape: 100 35 32 32 (3584000)
I1123 16:37:09.213656 14636 net.cpp:137] Memory required for data: 58574000
I1123 16:37:09.213656 14636 layer_factory.cpp:58] Creating layer conv2
I1123 16:37:09.213656 14636 net.cpp:84] Creating Layer conv2
I1123 16:37:09.213656 14636 net.cpp:406] conv2 <- conv1
I1123 16:37:09.213656 14636 net.cpp:380] conv2 -> conv2
I1123 16:37:09.214658 14636 net.cpp:122] Setting up conv2
I1123 16:37:09.214658 14636 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 16:37:09.214658 14636 net.cpp:137] Memory required for data: 74138800
I1123 16:37:09.214658 14636 layer_factory.cpp:58] Creating layer bn2
I1123 16:37:09.214658 14636 net.cpp:84] Creating Layer bn2
I1123 16:37:09.214658 14636 net.cpp:406] bn2 <- conv2
I1123 16:37:09.214658 14636 net.cpp:367] bn2 -> conv2 (in-place)
I1123 16:37:09.214658 14636 net.cpp:122] Setting up bn2
I1123 16:37:09.214658 14636 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 16:37:09.214658 14636 net.cpp:137] Memory required for data: 89703600
I1123 16:37:09.214658 14636 layer_factory.cpp:58] Creating layer scale2
I1123 16:37:09.214658 14636 net.cpp:84] Creating Layer scale2
I1123 16:37:09.214658 14636 net.cpp:406] scale2 <- conv2
I1123 16:37:09.214658 14636 net.cpp:367] scale2 -> conv2 (in-place)
I1123 16:37:09.214658 14636 layer_factory.cpp:58] Creating layer scale2
I1123 16:37:09.215657 14636 net.cpp:122] Setting up scale2
I1123 16:37:09.215657 14636 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 16:37:09.215657 14636 net.cpp:137] Memory required for data: 105268400
I1123 16:37:09.215657 14636 layer_factory.cpp:58] Creating layer relu2
I1123 16:37:09.215657 14636 net.cpp:84] Creating Layer relu2
I1123 16:37:09.215657 14636 net.cpp:406] relu2 <- conv2
I1123 16:37:09.215657 14636 net.cpp:367] relu2 -> conv2 (in-place)
I1123 16:37:09.215657 14636 net.cpp:122] Setting up relu2
I1123 16:37:09.215657 14636 net.cpp:129] Top shape: 100 38 32 32 (3891200)
I1123 16:37:09.215657 14636 net.cpp:137] Memory required for data: 120833200
I1123 16:37:09.215657 14636 layer_factory.cpp:58] Creating layer conv2_2
I1123 16:37:09.215657 14636 net.cpp:84] Creating Layer conv2_2
I1123 16:37:09.215657 14636 net.cpp:406] conv2_2 <- conv2
I1123 16:37:09.215657 14636 net.cpp:380] conv2_2 -> conv2_2
I1123 16:37:09.216657 14636 net.cpp:122] Setting up conv2_2
I1123 16:37:09.216657 14636 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 16:37:09.216657 14636 net.cpp:137] Memory required for data: 147457200
I1123 16:37:09.216657 14636 layer_factory.cpp:58] Creating layer bn2_2
I1123 16:37:09.217658 14636 net.cpp:84] Creating Layer bn2_2
I1123 16:37:09.217658 14636 net.cpp:406] bn2_2 <- conv2_2
I1123 16:37:09.217658 14636 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 16:37:09.217658 14636 net.cpp:122] Setting up bn2_2
I1123 16:37:09.217658 14636 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 16:37:09.217658 14636 net.cpp:137] Memory required for data: 174081200
I1123 16:37:09.217658 14636 layer_factory.cpp:58] Creating layer scale2_2
I1123 16:37:09.217658 14636 net.cpp:84] Creating Layer scale2_2
I1123 16:37:09.217658 14636 net.cpp:406] scale2_2 <- conv2_2
I1123 16:37:09.217658 14636 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 16:37:09.217658 14636 layer_factory.cpp:58] Creating layer scale2_2
I1123 16:37:09.217658 14636 net.cpp:122] Setting up scale2_2
I1123 16:37:09.217658 14636 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 16:37:09.217658 14636 net.cpp:137] Memory required for data: 200705200
I1123 16:37:09.217658 14636 layer_factory.cpp:58] Creating layer relu2_2
I1123 16:37:09.217658 14636 net.cpp:84] Creating Layer relu2_2
I1123 16:37:09.217658 14636 net.cpp:406] relu2_2 <- conv2_2
I1123 16:37:09.217658 14636 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 16:37:09.217658 14636 net.cpp:122] Setting up relu2_2
I1123 16:37:09.217658 14636 net.cpp:129] Top shape: 100 65 32 32 (6656000)
I1123 16:37:09.217658 14636 net.cpp:137] Memory required for data: 227329200
I1123 16:37:09.217658 14636 layer_factory.cpp:58] Creating layer pool2_1
I1123 16:37:09.217658 14636 net.cpp:84] Creating Layer pool2_1
I1123 16:37:09.217658 14636 net.cpp:406] pool2_1 <- conv2_2
I1123 16:37:09.217658 14636 net.cpp:380] pool2_1 -> pool2_1
I1123 16:37:09.217658 14636 net.cpp:122] Setting up pool2_1
I1123 16:37:09.217658 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.217658 14636 net.cpp:137] Memory required for data: 233985200
I1123 16:37:09.217658 14636 layer_factory.cpp:58] Creating layer conv3
I1123 16:37:09.217658 14636 net.cpp:84] Creating Layer conv3
I1123 16:37:09.218657 14636 net.cpp:406] conv3 <- pool2_1
I1123 16:37:09.218657 14636 net.cpp:380] conv3 -> conv3
I1123 16:37:09.219657 14636 net.cpp:122] Setting up conv3
I1123 16:37:09.219657 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.219657 14636 net.cpp:137] Memory required for data: 240641200
I1123 16:37:09.219657 14636 layer_factory.cpp:58] Creating layer bn3
I1123 16:37:09.219657 14636 net.cpp:84] Creating Layer bn3
I1123 16:37:09.219657 14636 net.cpp:406] bn3 <- conv3
I1123 16:37:09.219657 14636 net.cpp:367] bn3 -> conv3 (in-place)
I1123 16:37:09.220659 14636 net.cpp:122] Setting up bn3
I1123 16:37:09.220659 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.220659 14636 net.cpp:137] Memory required for data: 247297200
I1123 16:37:09.220659 14636 layer_factory.cpp:58] Creating layer scale3
I1123 16:37:09.220659 14636 net.cpp:84] Creating Layer scale3
I1123 16:37:09.220659 14636 net.cpp:406] scale3 <- conv3
I1123 16:37:09.220659 14636 net.cpp:367] scale3 -> conv3 (in-place)
I1123 16:37:09.220659 14636 layer_factory.cpp:58] Creating layer scale3
I1123 16:37:09.220659 14636 net.cpp:122] Setting up scale3
I1123 16:37:09.220659 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.220659 14636 net.cpp:137] Memory required for data: 253953200
I1123 16:37:09.220659 14636 layer_factory.cpp:58] Creating layer relu3
I1123 16:37:09.220659 14636 net.cpp:84] Creating Layer relu3
I1123 16:37:09.220659 14636 net.cpp:406] relu3 <- conv3
I1123 16:37:09.220659 14636 net.cpp:367] relu3 -> conv3 (in-place)
I1123 16:37:09.220659 14636 net.cpp:122] Setting up relu3
I1123 16:37:09.220659 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.220659 14636 net.cpp:137] Memory required for data: 260609200
I1123 16:37:09.220659 14636 layer_factory.cpp:58] Creating layer conv4
I1123 16:37:09.220659 14636 net.cpp:84] Creating Layer conv4
I1123 16:37:09.220659 14636 net.cpp:406] conv4 <- conv3
I1123 16:37:09.220659 14636 net.cpp:380] conv4 -> conv4
I1123 16:37:09.222657 14636 net.cpp:122] Setting up conv4
I1123 16:37:09.222657 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.222657 14636 net.cpp:137] Memory required for data: 267265200
I1123 16:37:09.222657 14636 layer_factory.cpp:58] Creating layer bn4
I1123 16:37:09.222657 14636 net.cpp:84] Creating Layer bn4
I1123 16:37:09.222657 14636 net.cpp:406] bn4 <- conv4
I1123 16:37:09.222657 14636 net.cpp:367] bn4 -> conv4 (in-place)
I1123 16:37:09.222657 14636 net.cpp:122] Setting up bn4
I1123 16:37:09.222657 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.222657 14636 net.cpp:137] Memory required for data: 273921200
I1123 16:37:09.222657 14636 layer_factory.cpp:58] Creating layer scale4
I1123 16:37:09.222657 14636 net.cpp:84] Creating Layer scale4
I1123 16:37:09.222657 14636 net.cpp:406] scale4 <- conv4
I1123 16:37:09.222657 14636 net.cpp:367] scale4 -> conv4 (in-place)
I1123 16:37:09.222657 14636 layer_factory.cpp:58] Creating layer scale4
I1123 16:37:09.222657 14636 net.cpp:122] Setting up scale4
I1123 16:37:09.222657 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.222657 14636 net.cpp:137] Memory required for data: 280577200
I1123 16:37:09.222657 14636 layer_factory.cpp:58] Creating layer relu4
I1123 16:37:09.222657 14636 net.cpp:84] Creating Layer relu4
I1123 16:37:09.223657 14636 net.cpp:406] relu4 <- conv4
I1123 16:37:09.223657 14636 net.cpp:367] relu4 -> conv4 (in-place)
I1123 16:37:09.223657 14636 net.cpp:122] Setting up relu4
I1123 16:37:09.223657 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.223657 14636 net.cpp:137] Memory required for data: 287233200
I1123 16:37:09.223657 14636 layer_factory.cpp:58] Creating layer conv4_1
I1123 16:37:09.223657 14636 net.cpp:84] Creating Layer conv4_1
I1123 16:37:09.223657 14636 net.cpp:406] conv4_1 <- conv4
I1123 16:37:09.223657 14636 net.cpp:380] conv4_1 -> conv4_1
I1123 16:37:09.224658 14636 net.cpp:122] Setting up conv4_1
I1123 16:37:09.224658 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.224658 14636 net.cpp:137] Memory required for data: 293889200
I1123 16:37:09.224658 14636 layer_factory.cpp:58] Creating layer bn4_1
I1123 16:37:09.224658 14636 net.cpp:84] Creating Layer bn4_1
I1123 16:37:09.224658 14636 net.cpp:406] bn4_1 <- conv4_1
I1123 16:37:09.224658 14636 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 16:37:09.225658 14636 net.cpp:122] Setting up bn4_1
I1123 16:37:09.225658 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.225658 14636 net.cpp:137] Memory required for data: 300545200
I1123 16:37:09.225658 14636 layer_factory.cpp:58] Creating layer scale4_1
I1123 16:37:09.225658 14636 net.cpp:84] Creating Layer scale4_1
I1123 16:37:09.225658 14636 net.cpp:406] scale4_1 <- conv4_1
I1123 16:37:09.225658 14636 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 16:37:09.225658 14636 layer_factory.cpp:58] Creating layer scale4_1
I1123 16:37:09.225658 14636 net.cpp:122] Setting up scale4_1
I1123 16:37:09.225658 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.225658 14636 net.cpp:137] Memory required for data: 307201200
I1123 16:37:09.225658 14636 layer_factory.cpp:58] Creating layer relu4_1
I1123 16:37:09.225658 14636 net.cpp:84] Creating Layer relu4_1
I1123 16:37:09.225658 14636 net.cpp:406] relu4_1 <- conv4_1
I1123 16:37:09.225658 14636 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 16:37:09.225658 14636 net.cpp:122] Setting up relu4_1
I1123 16:37:09.225658 14636 net.cpp:129] Top shape: 100 65 16 16 (1664000)
I1123 16:37:09.225658 14636 net.cpp:137] Memory required for data: 313857200
I1123 16:37:09.225658 14636 layer_factory.cpp:58] Creating layer conv4_2
I1123 16:37:09.225658 14636 net.cpp:84] Creating Layer conv4_2
I1123 16:37:09.225658 14636 net.cpp:406] conv4_2 <- conv4_1
I1123 16:37:09.225658 14636 net.cpp:380] conv4_2 -> conv4_2
I1123 16:37:09.227658 14636 net.cpp:122] Setting up conv4_2
I1123 16:37:09.227658 14636 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 16:37:09.227658 14636 net.cpp:137] Memory required for data: 321434800
I1123 16:37:09.227658 14636 layer_factory.cpp:58] Creating layer bn4_2
I1123 16:37:09.227658 14636 net.cpp:84] Creating Layer bn4_2
I1123 16:37:09.227658 14636 net.cpp:406] bn4_2 <- conv4_2
I1123 16:37:09.227658 14636 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 16:37:09.228658 14636 net.cpp:122] Setting up bn4_2
I1123 16:37:09.228658 14636 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 16:37:09.228658 14636 net.cpp:137] Memory required for data: 329012400
I1123 16:37:09.228658 14636 layer_factory.cpp:58] Creating layer scale4_2
I1123 16:37:09.228658 14636 net.cpp:84] Creating Layer scale4_2
I1123 16:37:09.228658 14636 net.cpp:406] scale4_2 <- conv4_2
I1123 16:37:09.228658 14636 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 16:37:09.228658 14636 layer_factory.cpp:58] Creating layer scale4_2
I1123 16:37:09.228658 14636 net.cpp:122] Setting up scale4_2
I1123 16:37:09.228658 14636 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 16:37:09.228658 14636 net.cpp:137] Memory required for data: 336590000
I1123 16:37:09.228658 14636 layer_factory.cpp:58] Creating layer relu4_2
I1123 16:37:09.228658 14636 net.cpp:84] Creating Layer relu4_2
I1123 16:37:09.228658 14636 net.cpp:406] relu4_2 <- conv4_2
I1123 16:37:09.228658 14636 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 16:37:09.228658 14636 net.cpp:122] Setting up relu4_2
I1123 16:37:09.228658 14636 net.cpp:129] Top shape: 100 74 16 16 (1894400)
I1123 16:37:09.228658 14636 net.cpp:137] Memory required for data: 344167600
I1123 16:37:09.228658 14636 layer_factory.cpp:58] Creating layer pool4_2
I1123 16:37:09.228658 14636 net.cpp:84] Creating Layer pool4_2
I1123 16:37:09.228658 14636 net.cpp:406] pool4_2 <- conv4_2
I1123 16:37:09.228658 14636 net.cpp:380] pool4_2 -> pool4_2
I1123 16:37:09.228658 14636 net.cpp:122] Setting up pool4_2
I1123 16:37:09.228658 14636 net.cpp:129] Top shape: 100 74 8 8 (473600)
I1123 16:37:09.228658 14636 net.cpp:137] Memory required for data: 346062000
I1123 16:37:09.228658 14636 layer_factory.cpp:58] Creating layer conv12
I1123 16:37:09.228658 14636 net.cpp:84] Creating Layer conv12
I1123 16:37:09.228658 14636 net.cpp:406] conv12 <- pool4_2
I1123 16:37:09.228658 14636 net.cpp:380] conv12 -> conv12
I1123 16:37:09.230656 14636 net.cpp:122] Setting up conv12
I1123 16:37:09.230656 14636 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 16:37:09.230656 14636 net.cpp:137] Memory required for data: 347982000
I1123 16:37:09.230656 14636 layer_factory.cpp:58] Creating layer bn_conv12
I1123 16:37:09.230656 14636 net.cpp:84] Creating Layer bn_conv12
I1123 16:37:09.230656 14636 net.cpp:406] bn_conv12 <- conv12
I1123 16:37:09.230656 14636 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 16:37:09.230656 14636 net.cpp:122] Setting up bn_conv12
I1123 16:37:09.230656 14636 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 16:37:09.230656 14636 net.cpp:137] Memory required for data: 349902000
I1123 16:37:09.230656 14636 layer_factory.cpp:58] Creating layer scale_conv12
I1123 16:37:09.230656 14636 net.cpp:84] Creating Layer scale_conv12
I1123 16:37:09.230656 14636 net.cpp:406] scale_conv12 <- conv12
I1123 16:37:09.230656 14636 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 16:37:09.230656 14636 layer_factory.cpp:58] Creating layer scale_conv12
I1123 16:37:09.230656 14636 net.cpp:122] Setting up scale_conv12
I1123 16:37:09.230656 14636 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 16:37:09.230656 14636 net.cpp:137] Memory required for data: 351822000
I1123 16:37:09.230656 14636 layer_factory.cpp:58] Creating layer relu_conv12
I1123 16:37:09.230656 14636 net.cpp:84] Creating Layer relu_conv12
I1123 16:37:09.230656 14636 net.cpp:406] relu_conv12 <- conv12
I1123 16:37:09.230656 14636 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 16:37:09.231657 14636 net.cpp:122] Setting up relu_conv12
I1123 16:37:09.231657 14636 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1123 16:37:09.231657 14636 net.cpp:137] Memory required for data: 353742000
I1123 16:37:09.231657 14636 layer_factory.cpp:58] Creating layer poolcp6
I1123 16:37:09.231657 14636 net.cpp:84] Creating Layer poolcp6
I1123 16:37:09.231657 14636 net.cpp:406] poolcp6 <- conv12
I1123 16:37:09.231657 14636 net.cpp:380] poolcp6 -> poolcp6
I1123 16:37:09.231657 14636 net.cpp:122] Setting up poolcp6
I1123 16:37:09.231657 14636 net.cpp:129] Top shape: 100 75 1 1 (7500)
I1123 16:37:09.231657 14636 net.cpp:137] Memory required for data: 353772000
I1123 16:37:09.231657 14636 layer_factory.cpp:58] Creating layer ip1
I1123 16:37:09.231657 14636 net.cpp:84] Creating Layer ip1
I1123 16:37:09.231657 14636 net.cpp:406] ip1 <- poolcp6
I1123 16:37:09.231657 14636 net.cpp:380] ip1 -> ip1
I1123 16:37:09.231657 14636 net.cpp:122] Setting up ip1
I1123 16:37:09.231657 14636 net.cpp:129] Top shape: 100 10 (1000)
I1123 16:37:09.231657 14636 net.cpp:137] Memory required for data: 353776000
I1123 16:37:09.231657 14636 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 16:37:09.231657 14636 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 16:37:09.231657 14636 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 16:37:09.231657 14636 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 16:37:09.231657 14636 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 16:37:09.231657 14636 net.cpp:122] Setting up ip1_ip1_0_split
I1123 16:37:09.231657 14636 net.cpp:129] Top shape: 100 10 (1000)
I1123 16:37:09.231657 14636 net.cpp:129] Top shape: 100 10 (1000)
I1123 16:37:09.231657 14636 net.cpp:137] Memory required for data: 353784000
I1123 16:37:09.231657 14636 layer_factory.cpp:58] Creating layer accuracy
I1123 16:37:09.231657 14636 net.cpp:84] Creating Layer accuracy
I1123 16:37:09.231657 14636 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1123 16:37:09.231657 14636 net.cpp:406] accuracy <- label_cifar_1_split_0
I1123 16:37:09.231657 14636 net.cpp:380] accuracy -> accuracy
I1123 16:37:09.231657 14636 net.cpp:122] Setting up accuracy
I1123 16:37:09.231657 14636 net.cpp:129] Top shape: (1)
I1123 16:37:09.231657 14636 net.cpp:137] Memory required for data: 353784004
I1123 16:37:09.231657 14636 layer_factory.cpp:58] Creating layer loss
I1123 16:37:09.231657 14636 net.cpp:84] Creating Layer loss
I1123 16:37:09.231657 14636 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 16:37:09.231657 14636 net.cpp:406] loss <- label_cifar_1_split_1
I1123 16:37:09.231657 14636 net.cpp:380] loss -> loss
I1123 16:37:09.231657 14636 layer_factory.cpp:58] Creating layer loss
I1123 16:37:09.232657 14636 net.cpp:122] Setting up loss
I1123 16:37:09.232657 14636 net.cpp:129] Top shape: (1)
I1123 16:37:09.232657 14636 net.cpp:132]     with loss weight 1
I1123 16:37:09.232657 14636 net.cpp:137] Memory required for data: 353784008
I1123 16:37:09.232657 14636 net.cpp:198] loss needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:200] accuracy does not need backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] ip1 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] poolcp6 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] relu_conv12 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] scale_conv12 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] bn_conv12 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] conv12 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] pool4_2 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] relu4_2 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] scale4_2 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] bn4_2 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] conv4_2 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] relu4_1 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] scale4_1 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] bn4_1 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] conv4_1 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] relu4 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] scale4 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] bn4 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] conv4 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] relu3 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] scale3 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] bn3 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] conv3 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] pool2_1 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] relu2_2 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] scale2_2 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] bn2_2 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] conv2_2 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] relu2 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] scale2 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] bn2 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] conv2 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] relu1 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] scale1 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] bn1 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:198] conv1 needs backward computation.
I1123 16:37:09.232657 14636 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 16:37:09.232657 14636 net.cpp:200] cifar does not need backward computation.
I1123 16:37:09.232657 14636 net.cpp:242] This network produces output accuracy
I1123 16:37:09.232657 14636 net.cpp:242] This network produces output loss
I1123 16:37:09.232657 14636 net.cpp:255] Network initialization done.
I1123 16:37:09.232657 14636 solver.cpp:56] Solver scaffolding done.
I1123 16:37:09.234657 14636 caffe.cpp:249] Starting Optimization
I1123 16:37:09.235657 14636 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k_7x7_first2layers
I1123 16:37:09.235657 14636 solver.cpp:273] Learning Rate Policy: multistep
I1123 16:37:09.236659 14636 solver.cpp:330] Iteration 0, Testing net (#0)
I1123 16:37:09.237658 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:37:10.548821 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:37:10.597822 14636 solver.cpp:397]     Test net output #0: accuracy = 0.1018
I1123 16:37:10.597822 14636 solver.cpp:397]     Test net output #1: loss = 78.4457 (* 1 = 78.4457 loss)
I1123 16:37:10.674832 14636 solver.cpp:218] Iteration 0 (0 iter/s, 1.43905s/100 iters), loss = 3.46642
I1123 16:37:10.674832 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.03
I1123 16:37:10.674832 14636 solver.cpp:237]     Train net output #1: loss = 3.46642 (* 1 = 3.46642 loss)
I1123 16:37:10.674832 14636 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1123 16:37:15.470683 14636 solver.cpp:218] Iteration 100 (20.8547 iter/s, 4.79507s/100 iters), loss = 1.82389
I1123 16:37:15.470683 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.26
I1123 16:37:15.470683 14636 solver.cpp:237]     Train net output #1: loss = 1.82389 (* 1 = 1.82389 loss)
I1123 16:37:15.470683 14636 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1123 16:37:20.273281 14636 solver.cpp:218] Iteration 200 (20.8227 iter/s, 4.80245s/100 iters), loss = 1.70144
I1123 16:37:20.273281 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.37
I1123 16:37:20.273281 14636 solver.cpp:237]     Train net output #1: loss = 1.70144 (* 1 = 1.70144 loss)
I1123 16:37:20.273281 14636 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1123 16:37:25.067090 14636 solver.cpp:218] Iteration 300 (20.8632 iter/s, 4.79313s/100 iters), loss = 1.48133
I1123 16:37:25.067090 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.51
I1123 16:37:25.067090 14636 solver.cpp:237]     Train net output #1: loss = 1.48133 (* 1 = 1.48133 loss)
I1123 16:37:25.067090 14636 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1123 16:37:29.854442 14636 solver.cpp:218] Iteration 400 (20.8906 iter/s, 4.78684s/100 iters), loss = 1.44393
I1123 16:37:29.854442 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.43
I1123 16:37:29.854442 14636 solver.cpp:237]     Train net output #1: loss = 1.44393 (* 1 = 1.44393 loss)
I1123 16:37:29.854442 14636 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1123 16:37:34.439174 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:37:34.627686 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_500.caffemodel
I1123 16:37:34.645192 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_500.solverstate
I1123 16:37:34.649194 14636 solver.cpp:330] Iteration 500, Testing net (#0)
I1123 16:37:34.649194 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:37:35.913319 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:37:35.963330 14636 solver.cpp:397]     Test net output #0: accuracy = 0.4532
I1123 16:37:35.963330 14636 solver.cpp:397]     Test net output #1: loss = 1.58085 (* 1 = 1.58085 loss)
I1123 16:37:36.009330 14636 solver.cpp:218] Iteration 500 (16.2478 iter/s, 6.15469s/100 iters), loss = 1.38268
I1123 16:37:36.009330 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.49
I1123 16:37:36.009330 14636 solver.cpp:237]     Train net output #1: loss = 1.38268 (* 1 = 1.38268 loss)
I1123 16:37:36.009330 14636 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1123 16:37:40.812928 14636 solver.cpp:218] Iteration 600 (20.8203 iter/s, 4.80301s/100 iters), loss = 1.23853
I1123 16:37:40.812928 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.55
I1123 16:37:40.812928 14636 solver.cpp:237]     Train net output #1: loss = 1.23853 (* 1 = 1.23853 loss)
I1123 16:37:40.812928 14636 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1123 16:37:45.627348 14636 solver.cpp:218] Iteration 700 (20.7722 iter/s, 4.81413s/100 iters), loss = 1.13501
I1123 16:37:45.627348 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.56
I1123 16:37:45.627348 14636 solver.cpp:237]     Train net output #1: loss = 1.13501 (* 1 = 1.13501 loss)
I1123 16:37:45.627348 14636 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1123 16:37:50.426494 14636 solver.cpp:218] Iteration 800 (20.8383 iter/s, 4.79886s/100 iters), loss = 0.98404
I1123 16:37:50.426494 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1123 16:37:50.426494 14636 solver.cpp:237]     Train net output #1: loss = 0.98404 (* 1 = 0.98404 loss)
I1123 16:37:50.426494 14636 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1123 16:37:55.278007 14636 solver.cpp:218] Iteration 900 (20.6139 iter/s, 4.85109s/100 iters), loss = 0.993807
I1123 16:37:55.278007 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.61
I1123 16:37:55.278007 14636 solver.cpp:237]     Train net output #1: loss = 0.993807 (* 1 = 0.993807 loss)
I1123 16:37:55.278007 14636 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1123 16:37:59.836001 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:38:00.024514 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_1000.caffemodel
I1123 16:38:00.035024 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_1000.solverstate
I1123 16:38:00.039525 14636 solver.cpp:330] Iteration 1000, Testing net (#0)
I1123 16:38:00.039525 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:38:01.299672 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:38:01.349684 14636 solver.cpp:397]     Test net output #0: accuracy = 0.562
I1123 16:38:01.349684 14636 solver.cpp:397]     Test net output #1: loss = 1.27002 (* 1 = 1.27002 loss)
I1123 16:38:01.395683 14636 solver.cpp:218] Iteration 1000 (16.3462 iter/s, 6.11763s/100 iters), loss = 0.978591
I1123 16:38:01.395683 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1123 16:38:01.395683 14636 solver.cpp:237]     Train net output #1: loss = 0.978591 (* 1 = 0.978591 loss)
I1123 16:38:01.395683 14636 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1123 16:38:06.179219 14636 solver.cpp:218] Iteration 1100 (20.9058 iter/s, 4.78336s/100 iters), loss = 0.886226
I1123 16:38:06.180222 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1123 16:38:06.180222 14636 solver.cpp:237]     Train net output #1: loss = 0.886226 (* 1 = 0.886226 loss)
I1123 16:38:06.180222 14636 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1123 16:38:10.963745 14636 solver.cpp:218] Iteration 1200 (20.9032 iter/s, 4.78395s/100 iters), loss = 0.832124
I1123 16:38:10.963745 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1123 16:38:10.963745 14636 solver.cpp:237]     Train net output #1: loss = 0.832124 (* 1 = 0.832124 loss)
I1123 16:38:10.963745 14636 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1123 16:38:15.748414 14636 solver.cpp:218] Iteration 1300 (20.9045 iter/s, 4.78365s/100 iters), loss = 0.85537
I1123 16:38:15.748414 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 16:38:15.748414 14636 solver.cpp:237]     Train net output #1: loss = 0.85537 (* 1 = 0.85537 loss)
I1123 16:38:15.748414 14636 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1123 16:38:20.532945 14636 solver.cpp:218] Iteration 1400 (20.9033 iter/s, 4.78394s/100 iters), loss = 0.931502
I1123 16:38:20.532945 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1123 16:38:20.532945 14636 solver.cpp:237]     Train net output #1: loss = 0.931502 (* 1 = 0.931502 loss)
I1123 16:38:20.532945 14636 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1123 16:38:25.084441 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:38:25.273474 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_1500.caffemodel
I1123 16:38:25.285475 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_1500.solverstate
I1123 16:38:25.290477 14636 solver.cpp:330] Iteration 1500, Testing net (#0)
I1123 16:38:25.290477 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:38:26.549772 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:38:26.599771 14636 solver.cpp:397]     Test net output #0: accuracy = 0.6537
I1123 16:38:26.599771 14636 solver.cpp:397]     Test net output #1: loss = 0.982998 (* 1 = 0.982998 loss)
I1123 16:38:26.645768 14636 solver.cpp:218] Iteration 1500 (16.358 iter/s, 6.11322s/100 iters), loss = 0.839136
I1123 16:38:26.645768 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1123 16:38:26.645768 14636 solver.cpp:237]     Train net output #1: loss = 0.839136 (* 1 = 0.839136 loss)
I1123 16:38:26.646772 14636 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1123 16:38:31.431661 14636 solver.cpp:218] Iteration 1600 (20.8988 iter/s, 4.78496s/100 iters), loss = 0.754242
I1123 16:38:31.431661 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1123 16:38:31.431661 14636 solver.cpp:237]     Train net output #1: loss = 0.754242 (* 1 = 0.754242 loss)
I1123 16:38:31.431661 14636 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1123 16:38:36.230826 14636 solver.cpp:218] Iteration 1700 (20.8404 iter/s, 4.79837s/100 iters), loss = 0.706031
I1123 16:38:36.230826 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 16:38:36.230826 14636 solver.cpp:237]     Train net output #1: loss = 0.706031 (* 1 = 0.706031 loss)
I1123 16:38:36.230826 14636 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1123 16:38:41.014561 14636 solver.cpp:218] Iteration 1800 (20.9035 iter/s, 4.78389s/100 iters), loss = 0.77287
I1123 16:38:41.014561 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1123 16:38:41.014561 14636 solver.cpp:237]     Train net output #1: loss = 0.77287 (* 1 = 0.77287 loss)
I1123 16:38:41.014561 14636 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1123 16:38:45.797101 14636 solver.cpp:218] Iteration 1900 (20.9098 iter/s, 4.78245s/100 iters), loss = 0.654328
I1123 16:38:45.797101 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 16:38:45.797101 14636 solver.cpp:237]     Train net output #1: loss = 0.654328 (* 1 = 0.654328 loss)
I1123 16:38:45.797101 14636 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1123 16:38:50.345093 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:38:50.533124 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_2000.caffemodel
I1123 16:38:50.543624 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_2000.solverstate
I1123 16:38:50.547626 14636 solver.cpp:330] Iteration 2000, Testing net (#0)
I1123 16:38:50.547626 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:38:51.807773 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:38:51.857786 14636 solver.cpp:397]     Test net output #0: accuracy = 0.5285
I1123 16:38:51.857786 14636 solver.cpp:397]     Test net output #1: loss = 1.29197 (* 1 = 1.29197 loss)
I1123 16:38:51.903784 14636 solver.cpp:218] Iteration 2000 (16.3782 iter/s, 6.10566s/100 iters), loss = 0.682516
I1123 16:38:51.903784 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 16:38:51.903784 14636 solver.cpp:237]     Train net output #1: loss = 0.682516 (* 1 = 0.682516 loss)
I1123 16:38:51.903784 14636 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1123 16:38:56.698319 14636 solver.cpp:218] Iteration 2100 (20.8581 iter/s, 4.7943s/100 iters), loss = 0.565853
I1123 16:38:56.698319 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 16:38:56.698319 14636 solver.cpp:237]     Train net output #1: loss = 0.565853 (* 1 = 0.565853 loss)
I1123 16:38:56.698319 14636 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1123 16:39:01.497887 14636 solver.cpp:218] Iteration 2200 (20.8352 iter/s, 4.79957s/100 iters), loss = 0.670222
I1123 16:39:01.497887 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 16:39:01.497887 14636 solver.cpp:237]     Train net output #1: loss = 0.670222 (* 1 = 0.670222 loss)
I1123 16:39:01.497887 14636 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1123 16:39:06.283380 14636 solver.cpp:218] Iteration 2300 (20.8997 iter/s, 4.78477s/100 iters), loss = 0.779373
I1123 16:39:06.283380 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1123 16:39:06.283380 14636 solver.cpp:237]     Train net output #1: loss = 0.779373 (* 1 = 0.779373 loss)
I1123 16:39:06.283380 14636 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1123 16:39:11.105978 14636 solver.cpp:218] Iteration 2400 (20.7395 iter/s, 4.82172s/100 iters), loss = 0.650385
I1123 16:39:11.105978 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 16:39:11.105978 14636 solver.cpp:237]     Train net output #1: loss = 0.650385 (* 1 = 0.650385 loss)
I1123 16:39:11.105978 14636 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1123 16:39:15.759816 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:39:15.952849 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_2500.caffemodel
I1123 16:39:15.963840 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_2500.solverstate
I1123 16:39:15.967839 14636 solver.cpp:330] Iteration 2500, Testing net (#0)
I1123 16:39:15.967839 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:39:17.254843 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:39:17.304847 14636 solver.cpp:397]     Test net output #0: accuracy = 0.6108
I1123 16:39:17.304847 14636 solver.cpp:397]     Test net output #1: loss = 1.10212 (* 1 = 1.10212 loss)
I1123 16:39:17.353361 14636 solver.cpp:218] Iteration 2500 (16.0082 iter/s, 6.2468s/100 iters), loss = 0.571278
I1123 16:39:17.353361 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 16:39:17.353361 14636 solver.cpp:237]     Train net output #1: loss = 0.571278 (* 1 = 0.571278 loss)
I1123 16:39:17.353361 14636 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1123 16:39:22.200788 14636 solver.cpp:218] Iteration 2600 (20.6292 iter/s, 4.84751s/100 iters), loss = 0.651477
I1123 16:39:22.200788 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 16:39:22.200788 14636 solver.cpp:237]     Train net output #1: loss = 0.651477 (* 1 = 0.651477 loss)
I1123 16:39:22.200788 14636 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1123 16:39:27.074496 14636 solver.cpp:218] Iteration 2700 (20.5215 iter/s, 4.87293s/100 iters), loss = 0.641509
I1123 16:39:27.074996 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 16:39:27.074996 14636 solver.cpp:237]     Train net output #1: loss = 0.641509 (* 1 = 0.641509 loss)
I1123 16:39:27.074996 14636 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1123 16:39:31.886085 14636 solver.cpp:218] Iteration 2800 (20.7856 iter/s, 4.81103s/100 iters), loss = 0.611448
I1123 16:39:31.886085 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 16:39:31.886085 14636 solver.cpp:237]     Train net output #1: loss = 0.611448 (* 1 = 0.611448 loss)
I1123 16:39:31.886085 14636 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1123 16:39:36.710290 14636 solver.cpp:218] Iteration 2900 (20.7304 iter/s, 4.82384s/100 iters), loss = 0.56589
I1123 16:39:36.710290 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 16:39:36.710290 14636 solver.cpp:237]     Train net output #1: loss = 0.56589 (* 1 = 0.56589 loss)
I1123 16:39:36.710290 14636 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1123 16:39:41.316025 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:39:41.510046 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_3000.caffemodel
I1123 16:39:41.520046 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_3000.solverstate
I1123 16:39:41.525055 14636 solver.cpp:330] Iteration 3000, Testing net (#0)
I1123 16:39:41.525055 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:39:42.792445 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:39:42.842453 14636 solver.cpp:397]     Test net output #0: accuracy = 0.6734
I1123 16:39:42.842453 14636 solver.cpp:397]     Test net output #1: loss = 0.945976 (* 1 = 0.945976 loss)
I1123 16:39:42.889453 14636 solver.cpp:218] Iteration 3000 (16.1849 iter/s, 6.1786s/100 iters), loss = 0.630049
I1123 16:39:42.889453 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 16:39:42.889453 14636 solver.cpp:237]     Train net output #1: loss = 0.630049 (* 1 = 0.630049 loss)
I1123 16:39:42.889453 14636 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1123 16:39:47.763347 14636 solver.cpp:218] Iteration 3100 (20.5172 iter/s, 4.87397s/100 iters), loss = 0.558929
I1123 16:39:47.763347 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 16:39:47.763347 14636 solver.cpp:237]     Train net output #1: loss = 0.558929 (* 1 = 0.558929 loss)
I1123 16:39:47.763347 14636 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1123 16:39:52.620867 14636 solver.cpp:218] Iteration 3200 (20.5886 iter/s, 4.85706s/100 iters), loss = 0.580822
I1123 16:39:52.620867 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 16:39:52.620867 14636 solver.cpp:237]     Train net output #1: loss = 0.580822 (* 1 = 0.580822 loss)
I1123 16:39:52.620867 14636 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1123 16:39:57.459578 14636 solver.cpp:218] Iteration 3300 (20.6716 iter/s, 4.83756s/100 iters), loss = 0.655483
I1123 16:39:57.459578 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 16:39:57.459578 14636 solver.cpp:237]     Train net output #1: loss = 0.655483 (* 1 = 0.655483 loss)
I1123 16:39:57.459578 14636 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1123 16:40:02.316005 14636 solver.cpp:218] Iteration 3400 (20.5933 iter/s, 4.85594s/100 iters), loss = 0.576683
I1123 16:40:02.316005 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 16:40:02.316005 14636 solver.cpp:237]     Train net output #1: loss = 0.576683 (* 1 = 0.576683 loss)
I1123 16:40:02.316005 14636 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1123 16:40:06.893025 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:40:07.082031 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_3500.caffemodel
I1123 16:40:07.092036 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_3500.solverstate
I1123 16:40:07.096036 14636 solver.cpp:330] Iteration 3500, Testing net (#0)
I1123 16:40:07.096036 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:40:08.370179 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:40:08.420192 14636 solver.cpp:397]     Test net output #0: accuracy = 0.7149
I1123 16:40:08.420192 14636 solver.cpp:397]     Test net output #1: loss = 0.845666 (* 1 = 0.845666 loss)
I1123 16:40:08.466181 14636 solver.cpp:218] Iteration 3500 (16.2583 iter/s, 6.1507s/100 iters), loss = 0.614983
I1123 16:40:08.467182 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 16:40:08.467182 14636 solver.cpp:237]     Train net output #1: loss = 0.614983 (* 1 = 0.614983 loss)
I1123 16:40:08.467182 14636 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1123 16:40:13.307803 14636 solver.cpp:218] Iteration 3600 (20.6566 iter/s, 4.84106s/100 iters), loss = 0.48781
I1123 16:40:13.307803 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 16:40:13.307803 14636 solver.cpp:237]     Train net output #1: loss = 0.48781 (* 1 = 0.48781 loss)
I1123 16:40:13.307803 14636 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1123 16:40:18.175438 14636 solver.cpp:218] Iteration 3700 (20.5483 iter/s, 4.86658s/100 iters), loss = 0.558292
I1123 16:40:18.175438 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 16:40:18.175438 14636 solver.cpp:237]     Train net output #1: loss = 0.558292 (* 1 = 0.558292 loss)
I1123 16:40:18.175438 14636 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1123 16:40:23.078779 14636 solver.cpp:218] Iteration 3800 (20.3955 iter/s, 4.90304s/100 iters), loss = 0.597612
I1123 16:40:23.078779 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 16:40:23.078779 14636 solver.cpp:237]     Train net output #1: loss = 0.597612 (* 1 = 0.597612 loss)
I1123 16:40:23.078779 14636 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1123 16:40:27.942818 14636 solver.cpp:218] Iteration 3900 (20.5595 iter/s, 4.86392s/100 iters), loss = 0.602002
I1123 16:40:27.942818 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 16:40:27.942818 14636 solver.cpp:237]     Train net output #1: loss = 0.602002 (* 1 = 0.602002 loss)
I1123 16:40:27.942818 14636 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1123 16:40:32.542688 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:40:32.731703 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_4000.caffemodel
I1123 16:40:32.742703 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_4000.solverstate
I1123 16:40:32.746704 14636 solver.cpp:330] Iteration 4000, Testing net (#0)
I1123 16:40:32.746704 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:40:34.013864 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:40:34.063868 14636 solver.cpp:397]     Test net output #0: accuracy = 0.7287
I1123 16:40:34.063868 14636 solver.cpp:397]     Test net output #1: loss = 0.773093 (* 1 = 0.773093 loss)
I1123 16:40:34.110373 14636 solver.cpp:218] Iteration 4000 (16.2164 iter/s, 6.16659s/100 iters), loss = 0.591292
I1123 16:40:34.110373 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 16:40:34.110373 14636 solver.cpp:237]     Train net output #1: loss = 0.591292 (* 1 = 0.591292 loss)
I1123 16:40:34.110373 14636 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1123 16:40:38.933421 14636 solver.cpp:218] Iteration 4100 (20.7367 iter/s, 4.82238s/100 iters), loss = 0.62788
I1123 16:40:38.933421 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 16:40:38.933421 14636 solver.cpp:237]     Train net output #1: loss = 0.62788 (* 1 = 0.62788 loss)
I1123 16:40:38.933421 14636 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1123 16:40:43.740394 14636 solver.cpp:218] Iteration 4200 (20.805 iter/s, 4.80654s/100 iters), loss = 0.534803
I1123 16:40:43.740394 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 16:40:43.740394 14636 solver.cpp:237]     Train net output #1: loss = 0.534803 (* 1 = 0.534803 loss)
I1123 16:40:43.740394 14636 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1123 16:40:48.548965 14636 solver.cpp:218] Iteration 4300 (20.797 iter/s, 4.80838s/100 iters), loss = 0.658533
I1123 16:40:48.548965 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1123 16:40:48.548965 14636 solver.cpp:237]     Train net output #1: loss = 0.658533 (* 1 = 0.658533 loss)
I1123 16:40:48.548965 14636 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1123 16:40:53.356582 14636 solver.cpp:218] Iteration 4400 (20.802 iter/s, 4.80724s/100 iters), loss = 0.524402
I1123 16:40:53.356582 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 16:40:53.356582 14636 solver.cpp:237]     Train net output #1: loss = 0.524402 (* 1 = 0.524402 loss)
I1123 16:40:53.356582 14636 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1123 16:40:57.934192 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:40:58.123210 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_4500.caffemodel
I1123 16:40:58.135211 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_4500.solverstate
I1123 16:40:58.139211 14636 solver.cpp:330] Iteration 4500, Testing net (#0)
I1123 16:40:58.139211 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:40:59.405351 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:40:59.455363 14636 solver.cpp:397]     Test net output #0: accuracy = 0.7575
I1123 16:40:59.456365 14636 solver.cpp:397]     Test net output #1: loss = 0.720375 (* 1 = 0.720375 loss)
I1123 16:40:59.502363 14636 solver.cpp:218] Iteration 4500 (16.2722 iter/s, 6.14546s/100 iters), loss = 0.514831
I1123 16:40:59.502363 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 16:40:59.502363 14636 solver.cpp:237]     Train net output #1: loss = 0.514831 (* 1 = 0.514831 loss)
I1123 16:40:59.502363 14636 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1123 16:41:04.300907 14636 solver.cpp:218] Iteration 4600 (20.8391 iter/s, 4.79868s/100 iters), loss = 0.500019
I1123 16:41:04.300907 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 16:41:04.300907 14636 solver.cpp:237]     Train net output #1: loss = 0.500019 (* 1 = 0.500019 loss)
I1123 16:41:04.300907 14636 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1123 16:41:09.099483 14636 solver.cpp:218] Iteration 4700 (20.8451 iter/s, 4.7973s/100 iters), loss = 0.565743
I1123 16:41:09.099483 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 16:41:09.099483 14636 solver.cpp:237]     Train net output #1: loss = 0.565743 (* 1 = 0.565743 loss)
I1123 16:41:09.099483 14636 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1123 16:41:13.909157 14636 solver.cpp:218] Iteration 4800 (20.7931 iter/s, 4.80928s/100 iters), loss = 0.643273
I1123 16:41:13.909157 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 16:41:13.909157 14636 solver.cpp:237]     Train net output #1: loss = 0.643273 (* 1 = 0.643273 loss)
I1123 16:41:13.909157 14636 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1123 16:41:18.712702 14636 solver.cpp:218] Iteration 4900 (20.8195 iter/s, 4.80319s/100 iters), loss = 0.474427
I1123 16:41:18.712702 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 16:41:18.712702 14636 solver.cpp:237]     Train net output #1: loss = 0.474427 (* 1 = 0.474427 loss)
I1123 16:41:18.712702 14636 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1123 16:41:23.274243 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:41:23.463263 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_5000.caffemodel
I1123 16:41:23.473260 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_5000.solverstate
I1123 16:41:23.477260 14636 solver.cpp:330] Iteration 5000, Testing net (#0)
I1123 16:41:23.477260 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:41:24.753581 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:41:24.803578 14636 solver.cpp:397]     Test net output #0: accuracy = 0.7098
I1123 16:41:24.803578 14636 solver.cpp:397]     Test net output #1: loss = 0.855313 (* 1 = 0.855313 loss)
I1123 16:41:24.850600 14636 solver.cpp:218] Iteration 5000 (16.2931 iter/s, 6.13758s/100 iters), loss = 0.460787
I1123 16:41:24.850600 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 16:41:24.850600 14636 solver.cpp:237]     Train net output #1: loss = 0.460787 (* 1 = 0.460787 loss)
I1123 16:41:24.850600 14636 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1123 16:41:24.850600 14636 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1123 16:41:29.650143 14636 solver.cpp:218] Iteration 5100 (20.8342 iter/s, 4.7998s/100 iters), loss = 0.354602
I1123 16:41:29.650143 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:41:29.650143 14636 solver.cpp:237]     Train net output #1: loss = 0.354602 (* 1 = 0.354602 loss)
I1123 16:41:29.650143 14636 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1123 16:41:34.451612 14636 solver.cpp:218] Iteration 5200 (20.8302 iter/s, 4.80073s/100 iters), loss = 0.461147
I1123 16:41:34.451612 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 16:41:34.451612 14636 solver.cpp:237]     Train net output #1: loss = 0.461147 (* 1 = 0.461147 loss)
I1123 16:41:34.451612 14636 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1123 16:41:39.253142 14636 solver.cpp:218] Iteration 5300 (20.8291 iter/s, 4.80099s/100 iters), loss = 0.410384
I1123 16:41:39.253142 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:41:39.253142 14636 solver.cpp:237]     Train net output #1: loss = 0.410384 (* 1 = 0.410384 loss)
I1123 16:41:39.253142 14636 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1123 16:41:44.052649 14636 solver.cpp:218] Iteration 5400 (20.8354 iter/s, 4.79951s/100 iters), loss = 0.396394
I1123 16:41:44.052649 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:41:44.052649 14636 solver.cpp:237]     Train net output #1: loss = 0.396394 (* 1 = 0.396394 loss)
I1123 16:41:44.052649 14636 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1123 16:41:48.631212 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:41:48.821238 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_5500.caffemodel
I1123 16:41:48.830235 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_5500.solverstate
I1123 16:41:48.834234 14636 solver.cpp:330] Iteration 5500, Testing net (#0)
I1123 16:41:48.835237 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:41:50.100370 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:41:50.150390 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8582
I1123 16:41:50.150390 14636 solver.cpp:397]     Test net output #1: loss = 0.422072 (* 1 = 0.422072 loss)
I1123 16:41:50.197389 14636 solver.cpp:218] Iteration 5500 (16.2775 iter/s, 6.14345s/100 iters), loss = 0.286562
I1123 16:41:50.197389 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:41:50.197389 14636 solver.cpp:237]     Train net output #1: loss = 0.286562 (* 1 = 0.286562 loss)
I1123 16:41:50.197389 14636 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1123 16:41:55.000999 14636 solver.cpp:218] Iteration 5600 (20.817 iter/s, 4.80376s/100 iters), loss = 0.311845
I1123 16:41:55.000999 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:41:55.000999 14636 solver.cpp:237]     Train net output #1: loss = 0.311845 (* 1 = 0.311845 loss)
I1123 16:41:55.000999 14636 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1123 16:41:59.801422 14636 solver.cpp:218] Iteration 5700 (20.8352 iter/s, 4.79957s/100 iters), loss = 0.402498
I1123 16:41:59.801422 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:41:59.801422 14636 solver.cpp:237]     Train net output #1: loss = 0.402498 (* 1 = 0.402498 loss)
I1123 16:41:59.801422 14636 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1123 16:42:04.602843 14636 solver.cpp:218] Iteration 5800 (20.8271 iter/s, 4.80144s/100 iters), loss = 0.365919
I1123 16:42:04.602843 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 16:42:04.602843 14636 solver.cpp:237]     Train net output #1: loss = 0.365919 (* 1 = 0.365919 loss)
I1123 16:42:04.602843 14636 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1123 16:42:09.406392 14636 solver.cpp:218] Iteration 5900 (20.8204 iter/s, 4.80299s/100 iters), loss = 0.309431
I1123 16:42:09.406392 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:42:09.406392 14636 solver.cpp:237]     Train net output #1: loss = 0.309431 (* 1 = 0.309431 loss)
I1123 16:42:09.406392 14636 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1123 16:42:13.972697 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:42:14.160727 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_6000.caffemodel
I1123 16:42:14.172720 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_6000.solverstate
I1123 16:42:14.178721 14636 solver.cpp:330] Iteration 6000, Testing net (#0)
I1123 16:42:14.179720 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:42:15.452853 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:42:15.502864 14636 solver.cpp:397]     Test net output #0: accuracy = 0.861
I1123 16:42:15.502864 14636 solver.cpp:397]     Test net output #1: loss = 0.403832 (* 1 = 0.403832 loss)
I1123 16:42:15.548859 14636 solver.cpp:218] Iteration 6000 (16.2799 iter/s, 6.14253s/100 iters), loss = 0.317107
I1123 16:42:15.548859 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:42:15.548859 14636 solver.cpp:237]     Train net output #1: loss = 0.317107 (* 1 = 0.317107 loss)
I1123 16:42:15.548859 14636 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1123 16:42:20.358438 14636 solver.cpp:218] Iteration 6100 (20.7932 iter/s, 4.80925s/100 iters), loss = 0.319193
I1123 16:42:20.359441 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:42:20.359441 14636 solver.cpp:237]     Train net output #1: loss = 0.319193 (* 1 = 0.319193 loss)
I1123 16:42:20.359441 14636 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1123 16:42:25.158054 14636 solver.cpp:218] Iteration 6200 (20.8398 iter/s, 4.79851s/100 iters), loss = 0.358105
I1123 16:42:25.158054 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 16:42:25.158054 14636 solver.cpp:237]     Train net output #1: loss = 0.358105 (* 1 = 0.358105 loss)
I1123 16:42:25.158054 14636 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1123 16:42:29.968745 14636 solver.cpp:218] Iteration 6300 (20.7889 iter/s, 4.81026s/100 iters), loss = 0.408745
I1123 16:42:29.968745 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:42:29.968745 14636 solver.cpp:237]     Train net output #1: loss = 0.408745 (* 1 = 0.408745 loss)
I1123 16:42:29.968745 14636 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1123 16:42:34.768311 14636 solver.cpp:218] Iteration 6400 (20.8364 iter/s, 4.79929s/100 iters), loss = 0.250182
I1123 16:42:34.768311 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:42:34.768311 14636 solver.cpp:237]     Train net output #1: loss = 0.250182 (* 1 = 0.250182 loss)
I1123 16:42:34.768311 14636 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1123 16:42:39.335057 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:42:39.523572 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_6500.caffemodel
I1123 16:42:39.534075 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_6500.solverstate
I1123 16:42:39.539088 14636 solver.cpp:330] Iteration 6500, Testing net (#0)
I1123 16:42:39.539088 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:42:40.805218 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:42:40.856230 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8615
I1123 16:42:40.856230 14636 solver.cpp:397]     Test net output #1: loss = 0.401524 (* 1 = 0.401524 loss)
I1123 16:42:40.905230 14636 solver.cpp:218] Iteration 6500 (16.2962 iter/s, 6.13642s/100 iters), loss = 0.251847
I1123 16:42:40.905230 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:42:40.905230 14636 solver.cpp:237]     Train net output #1: loss = 0.251847 (* 1 = 0.251847 loss)
I1123 16:42:40.905230 14636 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1123 16:42:45.711755 14636 solver.cpp:218] Iteration 6600 (20.807 iter/s, 4.80607s/100 iters), loss = 0.276023
I1123 16:42:45.711755 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:42:45.711755 14636 solver.cpp:237]     Train net output #1: loss = 0.276023 (* 1 = 0.276023 loss)
I1123 16:42:45.711755 14636 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1123 16:42:50.522068 14636 solver.cpp:218] Iteration 6700 (20.7896 iter/s, 4.81011s/100 iters), loss = 0.32867
I1123 16:42:50.522068 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:42:50.522568 14636 solver.cpp:237]     Train net output #1: loss = 0.32867 (* 1 = 0.32867 loss)
I1123 16:42:50.522568 14636 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1123 16:42:55.317114 14636 solver.cpp:218] Iteration 6800 (20.8585 iter/s, 4.79422s/100 iters), loss = 0.377115
I1123 16:42:55.317114 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:42:55.317114 14636 solver.cpp:237]     Train net output #1: loss = 0.377115 (* 1 = 0.377115 loss)
I1123 16:42:55.317114 14636 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1123 16:43:00.116376 14636 solver.cpp:218] Iteration 6900 (20.8368 iter/s, 4.7992s/100 iters), loss = 0.251132
I1123 16:43:00.116376 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:43:00.116376 14636 solver.cpp:237]     Train net output #1: loss = 0.251132 (* 1 = 0.251132 loss)
I1123 16:43:00.116376 14636 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1123 16:43:04.680418 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:43:04.869436 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_7000.caffemodel
I1123 16:43:04.879436 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_7000.solverstate
I1123 16:43:04.884439 14636 solver.cpp:330] Iteration 7000, Testing net (#0)
I1123 16:43:04.884439 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:43:06.148587 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:43:06.199592 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8683
I1123 16:43:06.199592 14636 solver.cpp:397]     Test net output #1: loss = 0.385495 (* 1 = 0.385495 loss)
I1123 16:43:06.245601 14636 solver.cpp:218] Iteration 7000 (16.316 iter/s, 6.12894s/100 iters), loss = 0.226333
I1123 16:43:06.245601 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:43:06.245601 14636 solver.cpp:237]     Train net output #1: loss = 0.226333 (* 1 = 0.226333 loss)
I1123 16:43:06.245601 14636 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1123 16:43:11.048176 14636 solver.cpp:218] Iteration 7100 (20.8243 iter/s, 4.80209s/100 iters), loss = 0.271322
I1123 16:43:11.048176 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:43:11.048176 14636 solver.cpp:237]     Train net output #1: loss = 0.271322 (* 1 = 0.271322 loss)
I1123 16:43:11.048176 14636 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1123 16:43:15.850697 14636 solver.cpp:218] Iteration 7200 (20.8255 iter/s, 4.80181s/100 iters), loss = 0.33626
I1123 16:43:15.850697 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:43:15.850697 14636 solver.cpp:237]     Train net output #1: loss = 0.33626 (* 1 = 0.33626 loss)
I1123 16:43:15.850697 14636 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1123 16:43:20.648169 14636 solver.cpp:218] Iteration 7300 (20.8424 iter/s, 4.79791s/100 iters), loss = 0.306826
I1123 16:43:20.648169 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:43:20.648169 14636 solver.cpp:237]     Train net output #1: loss = 0.306826 (* 1 = 0.306826 loss)
I1123 16:43:20.648169 14636 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1123 16:43:25.450711 14636 solver.cpp:218] Iteration 7400 (20.8242 iter/s, 4.8021s/100 iters), loss = 0.245998
I1123 16:43:25.450711 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:43:25.450711 14636 solver.cpp:237]     Train net output #1: loss = 0.245998 (* 1 = 0.245998 loss)
I1123 16:43:25.450711 14636 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1123 16:43:30.017693 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:43:30.206708 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_7500.caffemodel
I1123 16:43:30.217710 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_7500.solverstate
I1123 16:43:30.221212 14636 solver.cpp:330] Iteration 7500, Testing net (#0)
I1123 16:43:30.221212 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:43:31.486886 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:43:31.537396 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8679
I1123 16:43:31.537396 14636 solver.cpp:397]     Test net output #1: loss = 0.381964 (* 1 = 0.381964 loss)
I1123 16:43:31.583395 14636 solver.cpp:218] Iteration 7500 (16.3088 iter/s, 6.13167s/100 iters), loss = 0.28277
I1123 16:43:31.583395 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:43:31.583395 14636 solver.cpp:237]     Train net output #1: loss = 0.28277 (* 1 = 0.28277 loss)
I1123 16:43:31.583395 14636 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1123 16:43:36.395946 14636 solver.cpp:218] Iteration 7600 (20.778 iter/s, 4.81279s/100 iters), loss = 0.236142
I1123 16:43:36.395946 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:43:36.395946 14636 solver.cpp:237]     Train net output #1: loss = 0.236142 (* 1 = 0.236142 loss)
I1123 16:43:36.395946 14636 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1123 16:43:41.198038 14636 solver.cpp:218] Iteration 7700 (20.8288 iter/s, 4.80105s/100 iters), loss = 0.281012
I1123 16:43:41.198038 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:43:41.198038 14636 solver.cpp:237]     Train net output #1: loss = 0.281012 (* 1 = 0.281012 loss)
I1123 16:43:41.198038 14636 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1123 16:43:46.005182 14636 solver.cpp:218] Iteration 7800 (20.8007 iter/s, 4.80753s/100 iters), loss = 0.353399
I1123 16:43:46.006183 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:43:46.006183 14636 solver.cpp:237]     Train net output #1: loss = 0.3534 (* 1 = 0.3534 loss)
I1123 16:43:46.006183 14636 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1123 16:43:50.813566 14636 solver.cpp:218] Iteration 7900 (20.8005 iter/s, 4.80757s/100 iters), loss = 0.222215
I1123 16:43:50.813566 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:43:50.813566 14636 solver.cpp:237]     Train net output #1: loss = 0.222215 (* 1 = 0.222215 loss)
I1123 16:43:50.813566 14636 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1123 16:43:55.380179 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:43:55.570199 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_8000.caffemodel
I1123 16:43:55.581198 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_8000.solverstate
I1123 16:43:55.585197 14636 solver.cpp:330] Iteration 8000, Testing net (#0)
I1123 16:43:55.585197 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:43:56.851335 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:43:56.901841 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8679
I1123 16:43:56.901841 14636 solver.cpp:397]     Test net output #1: loss = 0.384492 (* 1 = 0.384492 loss)
I1123 16:43:56.948343 14636 solver.cpp:218] Iteration 8000 (16.3024 iter/s, 6.13408s/100 iters), loss = 0.249065
I1123 16:43:56.948343 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:43:56.948343 14636 solver.cpp:237]     Train net output #1: loss = 0.249065 (* 1 = 0.249065 loss)
I1123 16:43:56.948343 14636 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1123 16:44:01.754920 14636 solver.cpp:218] Iteration 8100 (20.8066 iter/s, 4.80617s/100 iters), loss = 0.255242
I1123 16:44:01.754920 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:44:01.754920 14636 solver.cpp:237]     Train net output #1: loss = 0.255242 (* 1 = 0.255242 loss)
I1123 16:44:01.754920 14636 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1123 16:44:06.555441 14636 solver.cpp:218] Iteration 8200 (20.8313 iter/s, 4.80046s/100 iters), loss = 0.307697
I1123 16:44:06.555441 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:44:06.555441 14636 solver.cpp:237]     Train net output #1: loss = 0.307697 (* 1 = 0.307697 loss)
I1123 16:44:06.555441 14636 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1123 16:44:11.353209 14636 solver.cpp:218] Iteration 8300 (20.8463 iter/s, 4.79703s/100 iters), loss = 0.360721
I1123 16:44:11.353209 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:44:11.353209 14636 solver.cpp:237]     Train net output #1: loss = 0.360721 (* 1 = 0.360721 loss)
I1123 16:44:11.353209 14636 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1123 16:44:16.178724 14636 solver.cpp:218] Iteration 8400 (20.7245 iter/s, 4.82521s/100 iters), loss = 0.25531
I1123 16:44:16.178724 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:44:16.178724 14636 solver.cpp:237]     Train net output #1: loss = 0.25531 (* 1 = 0.25531 loss)
I1123 16:44:16.178724 14636 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1123 16:44:20.796324 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:44:20.986333 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_8500.caffemodel
I1123 16:44:20.996345 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_8500.solverstate
I1123 16:44:21.000344 14636 solver.cpp:330] Iteration 8500, Testing net (#0)
I1123 16:44:21.000344 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:44:22.272505 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:44:22.321512 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8705
I1123 16:44:22.321512 14636 solver.cpp:397]     Test net output #1: loss = 0.37469 (* 1 = 0.37469 loss)
I1123 16:44:22.368512 14636 solver.cpp:218] Iteration 8500 (16.1573 iter/s, 6.18916s/100 iters), loss = 0.246537
I1123 16:44:22.368512 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:44:22.368512 14636 solver.cpp:237]     Train net output #1: loss = 0.246537 (* 1 = 0.246537 loss)
I1123 16:44:22.368512 14636 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1123 16:44:27.194305 14636 solver.cpp:218] Iteration 8600 (20.7235 iter/s, 4.82545s/100 iters), loss = 0.218845
I1123 16:44:27.194305 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:44:27.194305 14636 solver.cpp:237]     Train net output #1: loss = 0.218845 (* 1 = 0.218845 loss)
I1123 16:44:27.194305 14636 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1123 16:44:31.996836 14636 solver.cpp:218] Iteration 8700 (20.8249 iter/s, 4.80195s/100 iters), loss = 0.29828
I1123 16:44:31.996836 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:44:31.996836 14636 solver.cpp:237]     Train net output #1: loss = 0.29828 (* 1 = 0.29828 loss)
I1123 16:44:31.996836 14636 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1123 16:44:36.811400 14636 solver.cpp:218] Iteration 8800 (20.7679 iter/s, 4.81512s/100 iters), loss = 0.308253
I1123 16:44:36.812400 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:44:36.812400 14636 solver.cpp:237]     Train net output #1: loss = 0.308253 (* 1 = 0.308253 loss)
I1123 16:44:36.812400 14636 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1123 16:44:41.690105 14636 solver.cpp:218] Iteration 8900 (20.5021 iter/s, 4.87754s/100 iters), loss = 0.16982
I1123 16:44:41.690105 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:44:41.690105 14636 solver.cpp:237]     Train net output #1: loss = 0.16982 (* 1 = 0.16982 loss)
I1123 16:44:41.690105 14636 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1123 16:44:46.260695 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:44:46.449712 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_9000.caffemodel
I1123 16:44:46.462710 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_9000.solverstate
I1123 16:44:46.466724 14636 solver.cpp:330] Iteration 9000, Testing net (#0)
I1123 16:44:46.466724 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:44:47.732779 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:44:47.782775 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8659
I1123 16:44:47.782775 14636 solver.cpp:397]     Test net output #1: loss = 0.394343 (* 1 = 0.394343 loss)
I1123 16:44:47.828778 14636 solver.cpp:218] Iteration 9000 (16.2911 iter/s, 6.13831s/100 iters), loss = 0.231631
I1123 16:44:47.828778 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:44:47.828778 14636 solver.cpp:237]     Train net output #1: loss = 0.231631 (* 1 = 0.231631 loss)
I1123 16:44:47.828778 14636 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1123 16:44:52.636389 14636 solver.cpp:218] Iteration 9100 (20.7996 iter/s, 4.80778s/100 iters), loss = 0.222678
I1123 16:44:52.636389 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:44:52.637390 14636 solver.cpp:237]     Train net output #1: loss = 0.222678 (* 1 = 0.222678 loss)
I1123 16:44:52.637390 14636 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1123 16:44:57.441956 14636 solver.cpp:218] Iteration 9200 (20.8121 iter/s, 4.8049s/100 iters), loss = 0.270968
I1123 16:44:57.441956 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:44:57.441956 14636 solver.cpp:237]     Train net output #1: loss = 0.270968 (* 1 = 0.270968 loss)
I1123 16:44:57.441956 14636 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1123 16:45:02.290575 14636 solver.cpp:218] Iteration 9300 (20.6272 iter/s, 4.84796s/100 iters), loss = 0.289638
I1123 16:45:02.290575 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:45:02.290575 14636 solver.cpp:237]     Train net output #1: loss = 0.289638 (* 1 = 0.289638 loss)
I1123 16:45:02.290575 14636 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1123 16:45:07.131490 14636 solver.cpp:218] Iteration 9400 (20.6599 iter/s, 4.84028s/100 iters), loss = 0.243755
I1123 16:45:07.131490 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:45:07.131490 14636 solver.cpp:237]     Train net output #1: loss = 0.243755 (* 1 = 0.243755 loss)
I1123 16:45:07.131490 14636 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1123 16:45:11.721911 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:45:11.911424 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_9500.caffemodel
I1123 16:45:11.923930 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_9500.solverstate
I1123 16:45:11.928431 14636 solver.cpp:330] Iteration 9500, Testing net (#0)
I1123 16:45:11.928431 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:45:13.215566 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:45:13.266105 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8669
I1123 16:45:13.266105 14636 solver.cpp:397]     Test net output #1: loss = 0.393166 (* 1 = 0.393166 loss)
I1123 16:45:13.313102 14636 solver.cpp:218] Iteration 9500 (16.1786 iter/s, 6.18099s/100 iters), loss = 0.267564
I1123 16:45:13.313102 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:45:13.313102 14636 solver.cpp:237]     Train net output #1: loss = 0.267564 (* 1 = 0.267564 loss)
I1123 16:45:13.313102 14636 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1123 16:45:13.313102 14636 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1123 16:45:18.234616 14636 solver.cpp:218] Iteration 9600 (20.3206 iter/s, 4.92111s/100 iters), loss = 0.208624
I1123 16:45:18.234616 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:45:18.234616 14636 solver.cpp:237]     Train net output #1: loss = 0.208624 (* 1 = 0.208624 loss)
I1123 16:45:18.234616 14636 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1123 16:45:23.159057 14636 solver.cpp:218] Iteration 9700 (20.3055 iter/s, 4.92477s/100 iters), loss = 0.236467
I1123 16:45:23.159057 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:45:23.159057 14636 solver.cpp:237]     Train net output #1: loss = 0.236467 (* 1 = 0.236467 loss)
I1123 16:45:23.159057 14636 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1123 16:45:28.049909 14636 solver.cpp:218] Iteration 9800 (20.4512 iter/s, 4.88968s/100 iters), loss = 0.25812
I1123 16:45:28.049909 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:45:28.049909 14636 solver.cpp:237]     Train net output #1: loss = 0.25812 (* 1 = 0.25812 loss)
I1123 16:45:28.049909 14636 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1123 16:45:32.879672 14636 solver.cpp:218] Iteration 9900 (20.705 iter/s, 4.82975s/100 iters), loss = 0.161782
I1123 16:45:32.879672 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:45:32.879672 14636 solver.cpp:237]     Train net output #1: loss = 0.161782 (* 1 = 0.161782 loss)
I1123 16:45:32.879672 14636 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1123 16:45:37.446130 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:45:37.634639 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_10000.caffemodel
I1123 16:45:37.644145 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_10000.solverstate
I1123 16:45:37.648144 14636 solver.cpp:330] Iteration 10000, Testing net (#0)
I1123 16:45:37.648144 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:45:38.923290 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:45:38.973309 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8811
I1123 16:45:38.973309 14636 solver.cpp:397]     Test net output #1: loss = 0.346606 (* 1 = 0.346606 loss)
I1123 16:45:39.019299 14636 solver.cpp:218] Iteration 10000 (16.2879 iter/s, 6.13954s/100 iters), loss = 0.218832
I1123 16:45:39.019299 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:45:39.019299 14636 solver.cpp:237]     Train net output #1: loss = 0.218833 (* 1 = 0.218833 loss)
I1123 16:45:39.019299 14636 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1123 16:45:43.838335 14636 solver.cpp:218] Iteration 10100 (20.7548 iter/s, 4.81817s/100 iters), loss = 0.209974
I1123 16:45:43.838335 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:45:43.838335 14636 solver.cpp:237]     Train net output #1: loss = 0.209974 (* 1 = 0.209974 loss)
I1123 16:45:43.838335 14636 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1123 16:45:48.666401 14636 solver.cpp:218] Iteration 10200 (20.7123 iter/s, 4.82806s/100 iters), loss = 0.286029
I1123 16:45:48.666401 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:45:48.666401 14636 solver.cpp:237]     Train net output #1: loss = 0.286029 (* 1 = 0.286029 loss)
I1123 16:45:48.666401 14636 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1123 16:45:53.482148 14636 solver.cpp:218] Iteration 10300 (20.7669 iter/s, 4.81536s/100 iters), loss = 0.262983
I1123 16:45:53.482148 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:45:53.482148 14636 solver.cpp:237]     Train net output #1: loss = 0.262983 (* 1 = 0.262983 loss)
I1123 16:45:53.482148 14636 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1123 16:45:58.289722 14636 solver.cpp:218] Iteration 10400 (20.8029 iter/s, 4.80702s/100 iters), loss = 0.169965
I1123 16:45:58.289722 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:45:58.289722 14636 solver.cpp:237]     Train net output #1: loss = 0.169965 (* 1 = 0.169965 loss)
I1123 16:45:58.289722 14636 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1123 16:46:02.856271 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:46:03.045289 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_10500.caffemodel
I1123 16:46:03.055289 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_10500.solverstate
I1123 16:46:03.059288 14636 solver.cpp:330] Iteration 10500, Testing net (#0)
I1123 16:46:03.059288 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:46:04.325428 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:46:04.375444 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8797
I1123 16:46:04.375444 14636 solver.cpp:397]     Test net output #1: loss = 0.348339 (* 1 = 0.348339 loss)
I1123 16:46:04.422436 14636 solver.cpp:218] Iteration 10500 (16.3079 iter/s, 6.13198s/100 iters), loss = 0.21874
I1123 16:46:04.422436 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:46:04.422436 14636 solver.cpp:237]     Train net output #1: loss = 0.21874 (* 1 = 0.21874 loss)
I1123 16:46:04.422436 14636 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1123 16:46:09.230638 14636 solver.cpp:218] Iteration 10600 (20.799 iter/s, 4.80792s/100 iters), loss = 0.210994
I1123 16:46:09.230638 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:46:09.230638 14636 solver.cpp:237]     Train net output #1: loss = 0.210994 (* 1 = 0.210994 loss)
I1123 16:46:09.230638 14636 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1123 16:46:14.034696 14636 solver.cpp:218] Iteration 10700 (20.8172 iter/s, 4.80371s/100 iters), loss = 0.251622
I1123 16:46:14.034696 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:46:14.034696 14636 solver.cpp:237]     Train net output #1: loss = 0.251622 (* 1 = 0.251622 loss)
I1123 16:46:14.034696 14636 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1123 16:46:18.838410 14636 solver.cpp:218] Iteration 10800 (20.8205 iter/s, 4.80295s/100 iters), loss = 0.261619
I1123 16:46:18.838410 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:46:18.838410 14636 solver.cpp:237]     Train net output #1: loss = 0.261619 (* 1 = 0.261619 loss)
I1123 16:46:18.838410 14636 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1123 16:46:23.645926 14636 solver.cpp:218] Iteration 10900 (20.8019 iter/s, 4.80726s/100 iters), loss = 0.19323
I1123 16:46:23.645926 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:46:23.645926 14636 solver.cpp:237]     Train net output #1: loss = 0.19323 (* 1 = 0.19323 loss)
I1123 16:46:23.645926 14636 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1123 16:46:28.234010 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:46:28.422529 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_11000.caffemodel
I1123 16:46:28.433037 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_11000.solverstate
I1123 16:46:28.437037 14636 solver.cpp:330] Iteration 11000, Testing net (#0)
I1123 16:46:28.437037 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:46:29.707677 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:46:29.757699 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8805
I1123 16:46:29.757699 14636 solver.cpp:397]     Test net output #1: loss = 0.345171 (* 1 = 0.345171 loss)
I1123 16:46:29.804697 14636 solver.cpp:218] Iteration 11000 (16.2382 iter/s, 6.15831s/100 iters), loss = 0.218729
I1123 16:46:29.804697 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:46:29.804697 14636 solver.cpp:237]     Train net output #1: loss = 0.218729 (* 1 = 0.218729 loss)
I1123 16:46:29.804697 14636 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1123 16:46:34.619227 14636 solver.cpp:218] Iteration 11100 (20.7726 iter/s, 4.81402s/100 iters), loss = 0.20138
I1123 16:46:34.619227 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:46:34.619227 14636 solver.cpp:237]     Train net output #1: loss = 0.20138 (* 1 = 0.20138 loss)
I1123 16:46:34.619227 14636 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1123 16:46:39.452900 14636 solver.cpp:218] Iteration 11200 (20.6874 iter/s, 4.83386s/100 iters), loss = 0.257056
I1123 16:46:39.452900 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:46:39.452900 14636 solver.cpp:237]     Train net output #1: loss = 0.257056 (* 1 = 0.257056 loss)
I1123 16:46:39.452900 14636 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1123 16:46:44.332101 14636 solver.cpp:218] Iteration 11300 (20.4981 iter/s, 4.8785s/100 iters), loss = 0.252263
I1123 16:46:44.332101 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:46:44.332101 14636 solver.cpp:237]     Train net output #1: loss = 0.252264 (* 1 = 0.252264 loss)
I1123 16:46:44.332101 14636 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1123 16:46:49.150964 14636 solver.cpp:218] Iteration 11400 (20.7542 iter/s, 4.81831s/100 iters), loss = 0.186339
I1123 16:46:49.150964 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:46:49.150964 14636 solver.cpp:237]     Train net output #1: loss = 0.186339 (* 1 = 0.186339 loss)
I1123 16:46:49.150964 14636 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1123 16:46:53.736977 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:46:53.925493 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_11500.caffemodel
I1123 16:46:53.935999 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_11500.solverstate
I1123 16:46:53.940001 14636 solver.cpp:330] Iteration 11500, Testing net (#0)
I1123 16:46:53.940001 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:46:55.214656 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:46:55.264670 14636 solver.cpp:397]     Test net output #0: accuracy = 0.882
I1123 16:46:55.264670 14636 solver.cpp:397]     Test net output #1: loss = 0.344869 (* 1 = 0.344869 loss)
I1123 16:46:55.310664 14636 solver.cpp:218] Iteration 11500 (16.2342 iter/s, 6.15983s/100 iters), loss = 0.24692
I1123 16:46:55.310664 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:46:55.310664 14636 solver.cpp:237]     Train net output #1: loss = 0.24692 (* 1 = 0.24692 loss)
I1123 16:46:55.310664 14636 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1123 16:47:00.113214 14636 solver.cpp:218] Iteration 11600 (20.8274 iter/s, 4.80137s/100 iters), loss = 0.220519
I1123 16:47:00.113214 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:47:00.113214 14636 solver.cpp:237]     Train net output #1: loss = 0.220519 (* 1 = 0.220519 loss)
I1123 16:47:00.113214 14636 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1123 16:47:04.913763 14636 solver.cpp:218] Iteration 11700 (20.8296 iter/s, 4.80085s/100 iters), loss = 0.218533
I1123 16:47:04.913763 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:47:04.913763 14636 solver.cpp:237]     Train net output #1: loss = 0.218533 (* 1 = 0.218533 loss)
I1123 16:47:04.913763 14636 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1123 16:47:09.715181 14636 solver.cpp:218] Iteration 11800 (20.8315 iter/s, 4.80042s/100 iters), loss = 0.262526
I1123 16:47:09.715181 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:47:09.715181 14636 solver.cpp:237]     Train net output #1: loss = 0.262526 (* 1 = 0.262526 loss)
I1123 16:47:09.715181 14636 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1123 16:47:14.515712 14636 solver.cpp:218] Iteration 11900 (20.8331 iter/s, 4.80005s/100 iters), loss = 0.179744
I1123 16:47:14.515712 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:47:14.515712 14636 solver.cpp:237]     Train net output #1: loss = 0.179744 (* 1 = 0.179744 loss)
I1123 16:47:14.515712 14636 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1123 16:47:19.079252 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:47:19.269282 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_12000.caffemodel
I1123 16:47:19.279283 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_12000.solverstate
I1123 16:47:19.283282 14636 solver.cpp:330] Iteration 12000, Testing net (#0)
I1123 16:47:19.283282 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:47:20.550418 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:47:20.600425 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8818
I1123 16:47:20.600425 14636 solver.cpp:397]     Test net output #1: loss = 0.343447 (* 1 = 0.343447 loss)
I1123 16:47:20.646929 14636 solver.cpp:218] Iteration 12000 (16.3113 iter/s, 6.1307s/100 iters), loss = 0.207566
I1123 16:47:20.646929 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:47:20.646929 14636 solver.cpp:237]     Train net output #1: loss = 0.207566 (* 1 = 0.207566 loss)
I1123 16:47:20.646929 14636 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1123 16:47:25.445449 14636 solver.cpp:218] Iteration 12100 (20.8404 iter/s, 4.79837s/100 iters), loss = 0.220217
I1123 16:47:25.445449 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:47:25.445449 14636 solver.cpp:237]     Train net output #1: loss = 0.220217 (* 1 = 0.220217 loss)
I1123 16:47:25.445449 14636 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1123 16:47:30.242985 14636 solver.cpp:218] Iteration 12200 (20.8446 iter/s, 4.79739s/100 iters), loss = 0.259694
I1123 16:47:30.243485 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:47:30.243485 14636 solver.cpp:237]     Train net output #1: loss = 0.259694 (* 1 = 0.259694 loss)
I1123 16:47:30.243485 14636 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1123 16:47:35.046025 14636 solver.cpp:218] Iteration 12300 (20.8221 iter/s, 4.80258s/100 iters), loss = 0.188829
I1123 16:47:35.046025 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:47:35.046025 14636 solver.cpp:237]     Train net output #1: loss = 0.18883 (* 1 = 0.18883 loss)
I1123 16:47:35.046525 14636 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1123 16:47:39.851007 14636 solver.cpp:218] Iteration 12400 (20.815 iter/s, 4.80423s/100 iters), loss = 0.166058
I1123 16:47:39.851007 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:47:39.851007 14636 solver.cpp:237]     Train net output #1: loss = 0.166058 (* 1 = 0.166058 loss)
I1123 16:47:39.851007 14636 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1123 16:47:44.418957 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:47:44.607986 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_12500.caffemodel
I1123 16:47:44.617988 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_12500.solverstate
I1123 16:47:44.621987 14636 solver.cpp:330] Iteration 12500, Testing net (#0)
I1123 16:47:44.621987 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:47:45.889142 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:47:45.939141 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8822
I1123 16:47:45.939141 14636 solver.cpp:397]     Test net output #1: loss = 0.34418 (* 1 = 0.34418 loss)
I1123 16:47:45.985152 14636 solver.cpp:218] Iteration 12500 (16.3017 iter/s, 6.13431s/100 iters), loss = 0.222971
I1123 16:47:45.985152 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:47:45.985152 14636 solver.cpp:237]     Train net output #1: loss = 0.222971 (* 1 = 0.222971 loss)
I1123 16:47:45.985152 14636 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1123 16:47:50.787744 14636 solver.cpp:218] Iteration 12600 (20.8249 iter/s, 4.80193s/100 iters), loss = 0.186616
I1123 16:47:50.787744 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:47:50.787744 14636 solver.cpp:237]     Train net output #1: loss = 0.186617 (* 1 = 0.186617 loss)
I1123 16:47:50.787744 14636 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1123 16:47:55.576504 14636 solver.cpp:218] Iteration 12700 (20.8821 iter/s, 4.7888s/100 iters), loss = 0.223248
I1123 16:47:55.576504 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:47:55.576504 14636 solver.cpp:237]     Train net output #1: loss = 0.223248 (* 1 = 0.223248 loss)
I1123 16:47:55.576504 14636 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1123 16:48:00.379293 14636 solver.cpp:218] Iteration 12800 (20.8224 iter/s, 4.80252s/100 iters), loss = 0.202412
I1123 16:48:00.380293 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:48:00.380293 14636 solver.cpp:237]     Train net output #1: loss = 0.202412 (* 1 = 0.202412 loss)
I1123 16:48:00.380293 14636 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1123 16:48:05.182816 14636 solver.cpp:218] Iteration 12900 (20.8229 iter/s, 4.8024s/100 iters), loss = 0.171554
I1123 16:48:05.182816 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:48:05.182816 14636 solver.cpp:237]     Train net output #1: loss = 0.171554 (* 1 = 0.171554 loss)
I1123 16:48:05.182816 14636 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1123 16:48:09.747354 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:48:09.937371 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_13000.caffemodel
I1123 16:48:09.947371 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_13000.solverstate
I1123 16:48:09.951370 14636 solver.cpp:330] Iteration 13000, Testing net (#0)
I1123 16:48:09.951370 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:48:11.219509 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:48:11.269511 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8812
I1123 16:48:11.269511 14636 solver.cpp:397]     Test net output #1: loss = 0.343752 (* 1 = 0.343752 loss)
I1123 16:48:11.316516 14636 solver.cpp:218] Iteration 13000 (16.3052 iter/s, 6.13303s/100 iters), loss = 0.208298
I1123 16:48:11.316516 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:48:11.316516 14636 solver.cpp:237]     Train net output #1: loss = 0.208298 (* 1 = 0.208298 loss)
I1123 16:48:11.316516 14636 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1123 16:48:16.143198 14636 solver.cpp:218] Iteration 13100 (20.7174 iter/s, 4.82687s/100 iters), loss = 0.226057
I1123 16:48:16.143198 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:48:16.143198 14636 solver.cpp:237]     Train net output #1: loss = 0.226057 (* 1 = 0.226057 loss)
I1123 16:48:16.143198 14636 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1123 16:48:20.943742 14636 solver.cpp:218] Iteration 13200 (20.8311 iter/s, 4.80051s/100 iters), loss = 0.215341
I1123 16:48:20.944742 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:48:20.944742 14636 solver.cpp:237]     Train net output #1: loss = 0.215341 (* 1 = 0.215341 loss)
I1123 16:48:20.944742 14636 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1123 16:48:25.768327 14636 solver.cpp:218] Iteration 13300 (20.7295 iter/s, 4.82403s/100 iters), loss = 0.192292
I1123 16:48:25.768327 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:48:25.768327 14636 solver.cpp:237]     Train net output #1: loss = 0.192293 (* 1 = 0.192293 loss)
I1123 16:48:25.768327 14636 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1123 16:48:30.620515 14636 solver.cpp:218] Iteration 13400 (20.6147 iter/s, 4.85092s/100 iters), loss = 0.150033
I1123 16:48:30.620515 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:48:30.620515 14636 solver.cpp:237]     Train net output #1: loss = 0.150033 (* 1 = 0.150033 loss)
I1123 16:48:30.620515 14636 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1123 16:48:35.238569 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:48:35.428587 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_13500.caffemodel
I1123 16:48:35.438587 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_13500.solverstate
I1123 16:48:35.442587 14636 solver.cpp:330] Iteration 13500, Testing net (#0)
I1123 16:48:35.442587 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:48:36.710732 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:48:36.760735 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8834
I1123 16:48:36.760735 14636 solver.cpp:397]     Test net output #1: loss = 0.343218 (* 1 = 0.343218 loss)
I1123 16:48:36.808243 14636 solver.cpp:218] Iteration 13500 (16.162 iter/s, 6.18737s/100 iters), loss = 0.190013
I1123 16:48:36.808243 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:48:36.808243 14636 solver.cpp:237]     Train net output #1: loss = 0.190013 (* 1 = 0.190013 loss)
I1123 16:48:36.808243 14636 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1123 16:48:41.615253 14636 solver.cpp:218] Iteration 13600 (20.8045 iter/s, 4.80665s/100 iters), loss = 0.202836
I1123 16:48:41.615253 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:48:41.615253 14636 solver.cpp:237]     Train net output #1: loss = 0.202836 (* 1 = 0.202836 loss)
I1123 16:48:41.615253 14636 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1123 16:48:46.417672 14636 solver.cpp:218] Iteration 13700 (20.821 iter/s, 4.80285s/100 iters), loss = 0.235866
I1123 16:48:46.417672 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:48:46.418674 14636 solver.cpp:237]     Train net output #1: loss = 0.235866 (* 1 = 0.235866 loss)
I1123 16:48:46.418674 14636 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1123 16:48:51.221180 14636 solver.cpp:218] Iteration 13800 (20.8198 iter/s, 4.80311s/100 iters), loss = 0.272651
I1123 16:48:51.222183 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:48:51.222183 14636 solver.cpp:237]     Train net output #1: loss = 0.272651 (* 1 = 0.272651 loss)
I1123 16:48:51.222183 14636 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1123 16:48:56.025634 14636 solver.cpp:218] Iteration 13900 (20.8196 iter/s, 4.80316s/100 iters), loss = 0.17203
I1123 16:48:56.025634 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:48:56.025634 14636 solver.cpp:237]     Train net output #1: loss = 0.17203 (* 1 = 0.17203 loss)
I1123 16:48:56.025634 14636 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1123 16:49:00.593261 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:49:00.782276 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_14000.caffemodel
I1123 16:49:00.792277 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_14000.solverstate
I1123 16:49:00.796275 14636 solver.cpp:330] Iteration 14000, Testing net (#0)
I1123 16:49:00.796275 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:49:02.061499 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:49:02.112501 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8829
I1123 16:49:02.112501 14636 solver.cpp:397]     Test net output #1: loss = 0.344613 (* 1 = 0.344613 loss)
I1123 16:49:02.158520 14636 solver.cpp:218] Iteration 14000 (16.3043 iter/s, 6.13337s/100 iters), loss = 0.197407
I1123 16:49:02.159521 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:49:02.159521 14636 solver.cpp:237]     Train net output #1: loss = 0.197407 (* 1 = 0.197407 loss)
I1123 16:49:02.159521 14636 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1123 16:49:06.968052 14636 solver.cpp:218] Iteration 14100 (20.7943 iter/s, 4.809s/100 iters), loss = 0.241023
I1123 16:49:06.968052 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:49:06.968052 14636 solver.cpp:237]     Train net output #1: loss = 0.241023 (* 1 = 0.241023 loss)
I1123 16:49:06.968052 14636 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1123 16:49:11.773573 14636 solver.cpp:218] Iteration 14200 (20.8121 iter/s, 4.80489s/100 iters), loss = 0.18797
I1123 16:49:11.773573 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:49:11.773573 14636 solver.cpp:237]     Train net output #1: loss = 0.18797 (* 1 = 0.18797 loss)
I1123 16:49:11.773573 14636 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1123 16:49:16.581087 14636 solver.cpp:218] Iteration 14300 (20.8044 iter/s, 4.80667s/100 iters), loss = 0.236875
I1123 16:49:16.581087 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:49:16.581087 14636 solver.cpp:237]     Train net output #1: loss = 0.236875 (* 1 = 0.236875 loss)
I1123 16:49:16.581087 14636 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1123 16:49:21.385624 14636 solver.cpp:218] Iteration 14400 (20.8156 iter/s, 4.8041s/100 iters), loss = 0.166792
I1123 16:49:21.385624 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:49:21.385624 14636 solver.cpp:237]     Train net output #1: loss = 0.166793 (* 1 = 0.166793 loss)
I1123 16:49:21.385624 14636 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1123 16:49:25.953150 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:49:26.142670 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_14500.caffemodel
I1123 16:49:26.152179 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_14500.solverstate
I1123 16:49:26.156177 14636 solver.cpp:330] Iteration 14500, Testing net (#0)
I1123 16:49:26.156177 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:49:27.423336 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:49:27.474346 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8836
I1123 16:49:27.474346 14636 solver.cpp:397]     Test net output #1: loss = 0.342752 (* 1 = 0.342752 loss)
I1123 16:49:27.520345 14636 solver.cpp:218] Iteration 14500 (16.3002 iter/s, 6.1349s/100 iters), loss = 0.197421
I1123 16:49:27.520345 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:49:27.520345 14636 solver.cpp:237]     Train net output #1: loss = 0.197421 (* 1 = 0.197421 loss)
I1123 16:49:27.520345 14636 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1123 16:49:32.318874 14636 solver.cpp:218] Iteration 14600 (20.8402 iter/s, 4.79843s/100 iters), loss = 0.201719
I1123 16:49:32.318874 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:49:32.319875 14636 solver.cpp:237]     Train net output #1: loss = 0.201719 (* 1 = 0.201719 loss)
I1123 16:49:32.319875 14636 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1123 16:49:37.122401 14636 solver.cpp:218] Iteration 14700 (20.8233 iter/s, 4.80231s/100 iters), loss = 0.271309
I1123 16:49:37.122401 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:49:37.122401 14636 solver.cpp:237]     Train net output #1: loss = 0.271309 (* 1 = 0.271309 loss)
I1123 16:49:37.122401 14636 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1123 16:49:41.922889 14636 solver.cpp:218] Iteration 14800 (20.8304 iter/s, 4.80067s/100 iters), loss = 0.215806
I1123 16:49:41.922889 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:49:41.922889 14636 solver.cpp:237]     Train net output #1: loss = 0.215806 (* 1 = 0.215806 loss)
I1123 16:49:41.922889 14636 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1123 16:49:46.721372 14636 solver.cpp:218] Iteration 14900 (20.8442 iter/s, 4.79749s/100 iters), loss = 0.192978
I1123 16:49:46.721372 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:49:46.721372 14636 solver.cpp:237]     Train net output #1: loss = 0.192979 (* 1 = 0.192979 loss)
I1123 16:49:46.721372 14636 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1123 16:49:51.286895 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:49:51.474917 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_15000.caffemodel
I1123 16:49:51.493916 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_15000.solverstate
I1123 16:49:51.497925 14636 solver.cpp:330] Iteration 15000, Testing net (#0)
I1123 16:49:51.497925 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:49:52.764088 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:49:52.814086 14636 solver.cpp:397]     Test net output #0: accuracy = 0.884
I1123 16:49:52.814086 14636 solver.cpp:397]     Test net output #1: loss = 0.341511 (* 1 = 0.341511 loss)
I1123 16:49:52.861096 14636 solver.cpp:218] Iteration 15000 (16.2875 iter/s, 6.13969s/100 iters), loss = 0.146135
I1123 16:49:52.861096 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:49:52.861096 14636 solver.cpp:237]     Train net output #1: loss = 0.146135 (* 1 = 0.146135 loss)
I1123 16:49:52.861096 14636 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1123 16:49:57.661633 14636 solver.cpp:218] Iteration 15100 (20.8341 iter/s, 4.79983s/100 iters), loss = 0.146825
I1123 16:49:57.661633 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:49:57.661633 14636 solver.cpp:237]     Train net output #1: loss = 0.146825 (* 1 = 0.146825 loss)
I1123 16:49:57.661633 14636 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1123 16:50:02.495223 14636 solver.cpp:218] Iteration 15200 (20.6873 iter/s, 4.83389s/100 iters), loss = 0.170233
I1123 16:50:02.495223 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:50:02.495223 14636 solver.cpp:237]     Train net output #1: loss = 0.170233 (* 1 = 0.170233 loss)
I1123 16:50:02.495223 14636 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1123 16:50:07.299700 14636 solver.cpp:218] Iteration 15300 (20.8188 iter/s, 4.80335s/100 iters), loss = 0.198557
I1123 16:50:07.299700 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:50:07.299700 14636 solver.cpp:237]     Train net output #1: loss = 0.198557 (* 1 = 0.198557 loss)
I1123 16:50:07.299700 14636 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1123 16:50:07.299700 14636 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1123 16:50:12.102177 14636 solver.cpp:218] Iteration 15400 (20.823 iter/s, 4.80238s/100 iters), loss = 0.166129
I1123 16:50:12.102177 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:50:12.102177 14636 solver.cpp:237]     Train net output #1: loss = 0.166129 (* 1 = 0.166129 loss)
I1123 16:50:12.102177 14636 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1123 16:50:16.666173 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:50:16.854682 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_15500.caffemodel
I1123 16:50:16.865186 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_15500.solverstate
I1123 16:50:16.868687 14636 solver.cpp:330] Iteration 15500, Testing net (#0)
I1123 16:50:16.868687 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:50:18.134819 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:50:18.185834 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8839
I1123 16:50:18.185834 14636 solver.cpp:397]     Test net output #1: loss = 0.34187 (* 1 = 0.34187 loss)
I1123 16:50:18.231833 14636 solver.cpp:218] Iteration 15500 (16.3149 iter/s, 6.12938s/100 iters), loss = 0.180661
I1123 16:50:18.231833 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:50:18.231833 14636 solver.cpp:237]     Train net output #1: loss = 0.180661 (* 1 = 0.180661 loss)
I1123 16:50:18.231833 14636 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1123 16:50:23.034590 14636 solver.cpp:218] Iteration 15600 (20.8233 iter/s, 4.80231s/100 iters), loss = 0.229454
I1123 16:50:23.034590 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:50:23.034590 14636 solver.cpp:237]     Train net output #1: loss = 0.229454 (* 1 = 0.229454 loss)
I1123 16:50:23.034590 14636 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1123 16:50:27.838121 14636 solver.cpp:218] Iteration 15700 (20.8191 iter/s, 4.80328s/100 iters), loss = 0.239755
I1123 16:50:27.838121 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:50:27.838121 14636 solver.cpp:237]     Train net output #1: loss = 0.239755 (* 1 = 0.239755 loss)
I1123 16:50:27.838121 14636 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1123 16:50:32.641646 14636 solver.cpp:218] Iteration 15800 (20.8197 iter/s, 4.80315s/100 iters), loss = 0.229867
I1123 16:50:32.641646 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:50:32.641646 14636 solver.cpp:237]     Train net output #1: loss = 0.229867 (* 1 = 0.229867 loss)
I1123 16:50:32.641646 14636 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1123 16:50:37.440191 14636 solver.cpp:218] Iteration 15900 (20.8401 iter/s, 4.79845s/100 iters), loss = 0.177392
I1123 16:50:37.441192 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:50:37.441192 14636 solver.cpp:237]     Train net output #1: loss = 0.177392 (* 1 = 0.177392 loss)
I1123 16:50:37.441192 14636 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1123 16:50:42.009625 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:50:42.198647 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_16000.caffemodel
I1123 16:50:42.235646 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_16000.solverstate
I1123 16:50:42.239645 14636 solver.cpp:330] Iteration 16000, Testing net (#0)
I1123 16:50:42.239645 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:50:43.506800 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:50:43.556798 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8841
I1123 16:50:43.556798 14636 solver.cpp:397]     Test net output #1: loss = 0.341585 (* 1 = 0.341585 loss)
I1123 16:50:43.603811 14636 solver.cpp:218] Iteration 16000 (16.2269 iter/s, 6.16261s/100 iters), loss = 0.189917
I1123 16:50:43.603811 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:50:43.603811 14636 solver.cpp:237]     Train net output #1: loss = 0.189917 (* 1 = 0.189917 loss)
I1123 16:50:43.603811 14636 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1123 16:50:48.405342 14636 solver.cpp:218] Iteration 16100 (20.8263 iter/s, 4.80162s/100 iters), loss = 0.19108
I1123 16:50:48.405342 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:50:48.405342 14636 solver.cpp:237]     Train net output #1: loss = 0.19108 (* 1 = 0.19108 loss)
I1123 16:50:48.405342 14636 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1123 16:50:53.208886 14636 solver.cpp:218] Iteration 16200 (20.8199 iter/s, 4.8031s/100 iters), loss = 0.260889
I1123 16:50:53.208886 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:50:53.208886 14636 solver.cpp:237]     Train net output #1: loss = 0.260889 (* 1 = 0.260889 loss)
I1123 16:50:53.208886 14636 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1123 16:50:58.012538 14636 solver.cpp:218] Iteration 16300 (20.8206 iter/s, 4.80293s/100 iters), loss = 0.216522
I1123 16:50:58.012538 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:50:58.012538 14636 solver.cpp:237]     Train net output #1: loss = 0.216523 (* 1 = 0.216523 loss)
I1123 16:50:58.012538 14636 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1123 16:51:02.815148 14636 solver.cpp:218] Iteration 16400 (20.8242 iter/s, 4.8021s/100 iters), loss = 0.162162
I1123 16:51:02.815148 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:51:02.815148 14636 solver.cpp:237]     Train net output #1: loss = 0.162162 (* 1 = 0.162162 loss)
I1123 16:51:02.815148 14636 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1123 16:51:07.383687 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:51:07.572199 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_16500.caffemodel
I1123 16:51:07.584702 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_16500.solverstate
I1123 16:51:07.588718 14636 solver.cpp:330] Iteration 16500, Testing net (#0)
I1123 16:51:07.588718 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:51:08.855942 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:51:08.905521 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8834
I1123 16:51:08.905521 14636 solver.cpp:397]     Test net output #1: loss = 0.341389 (* 1 = 0.341389 loss)
I1123 16:51:08.952519 14636 solver.cpp:218] Iteration 16500 (16.2956 iter/s, 6.13663s/100 iters), loss = 0.142148
I1123 16:51:08.952519 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:51:08.952519 14636 solver.cpp:237]     Train net output #1: loss = 0.142148 (* 1 = 0.142148 loss)
I1123 16:51:08.952519 14636 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1123 16:51:13.759598 14636 solver.cpp:218] Iteration 16600 (20.8033 iter/s, 4.80694s/100 iters), loss = 0.200189
I1123 16:51:13.759598 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:51:13.759598 14636 solver.cpp:237]     Train net output #1: loss = 0.20019 (* 1 = 0.20019 loss)
I1123 16:51:13.759598 14636 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1123 16:51:18.563217 14636 solver.cpp:218] Iteration 16700 (20.82 iter/s, 4.80307s/100 iters), loss = 0.235712
I1123 16:51:18.563217 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:51:18.563217 14636 solver.cpp:237]     Train net output #1: loss = 0.235712 (* 1 = 0.235712 loss)
I1123 16:51:18.563217 14636 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1123 16:51:23.363736 14636 solver.cpp:218] Iteration 16800 (20.832 iter/s, 4.8003s/100 iters), loss = 0.232503
I1123 16:51:23.363736 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:51:23.363736 14636 solver.cpp:237]     Train net output #1: loss = 0.232504 (* 1 = 0.232504 loss)
I1123 16:51:23.363736 14636 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1123 16:51:28.165290 14636 solver.cpp:218] Iteration 16900 (20.8286 iter/s, 4.80109s/100 iters), loss = 0.208964
I1123 16:51:28.165290 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:51:28.165290 14636 solver.cpp:237]     Train net output #1: loss = 0.208964 (* 1 = 0.208964 loss)
I1123 16:51:28.165290 14636 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1123 16:51:32.732908 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:51:32.921926 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_17000.caffemodel
I1123 16:51:32.951926 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_17000.solverstate
I1123 16:51:32.954926 14636 solver.cpp:330] Iteration 17000, Testing net (#0)
I1123 16:51:32.954926 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:51:34.221074 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:51:34.271580 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8842
I1123 16:51:34.271580 14636 solver.cpp:397]     Test net output #1: loss = 0.341054 (* 1 = 0.341054 loss)
I1123 16:51:34.318083 14636 solver.cpp:218] Iteration 17000 (16.2546 iter/s, 6.15212s/100 iters), loss = 0.19576
I1123 16:51:34.318083 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:51:34.318083 14636 solver.cpp:237]     Train net output #1: loss = 0.19576 (* 1 = 0.19576 loss)
I1123 16:51:34.318083 14636 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1123 16:51:39.117584 14636 solver.cpp:218] Iteration 17100 (20.8355 iter/s, 4.7995s/100 iters), loss = 0.202521
I1123 16:51:39.117584 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:51:39.117584 14636 solver.cpp:237]     Train net output #1: loss = 0.202521 (* 1 = 0.202521 loss)
I1123 16:51:39.117584 14636 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1123 16:51:43.921108 14636 solver.cpp:218] Iteration 17200 (20.8191 iter/s, 4.80328s/100 iters), loss = 0.218034
I1123 16:51:43.921108 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:51:43.921108 14636 solver.cpp:237]     Train net output #1: loss = 0.218035 (* 1 = 0.218035 loss)
I1123 16:51:43.921108 14636 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1123 16:51:48.724731 14636 solver.cpp:218] Iteration 17300 (20.8199 iter/s, 4.80309s/100 iters), loss = 0.246503
I1123 16:51:48.724731 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:51:48.724731 14636 solver.cpp:237]     Train net output #1: loss = 0.246504 (* 1 = 0.246504 loss)
I1123 16:51:48.724731 14636 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1123 16:51:53.530521 14636 solver.cpp:218] Iteration 17400 (20.8104 iter/s, 4.80529s/100 iters), loss = 0.160723
I1123 16:51:53.530521 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:51:53.530521 14636 solver.cpp:237]     Train net output #1: loss = 0.160723 (* 1 = 0.160723 loss)
I1123 16:51:53.530521 14636 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1123 16:51:58.102041 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:51:58.290076 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_17500.caffemodel
I1123 16:51:58.300076 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_17500.solverstate
I1123 16:51:58.304075 14636 solver.cpp:330] Iteration 17500, Testing net (#0)
I1123 16:51:58.304075 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:51:59.571243 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:51:59.621254 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8843
I1123 16:51:59.621254 14636 solver.cpp:397]     Test net output #1: loss = 0.340786 (* 1 = 0.340786 loss)
I1123 16:51:59.668243 14636 solver.cpp:218] Iteration 17500 (16.2941 iter/s, 6.1372s/100 iters), loss = 0.154468
I1123 16:51:59.668243 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:51:59.668243 14636 solver.cpp:237]     Train net output #1: loss = 0.154469 (* 1 = 0.154469 loss)
I1123 16:51:59.668243 14636 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1123 16:52:04.472652 14636 solver.cpp:218] Iteration 17600 (20.8152 iter/s, 4.80419s/100 iters), loss = 0.173227
I1123 16:52:04.472652 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:52:04.472652 14636 solver.cpp:237]     Train net output #1: loss = 0.173227 (* 1 = 0.173227 loss)
I1123 16:52:04.472652 14636 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1123 16:52:09.278298 14636 solver.cpp:218] Iteration 17700 (20.8118 iter/s, 4.80497s/100 iters), loss = 0.211787
I1123 16:52:09.278298 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:52:09.278298 14636 solver.cpp:237]     Train net output #1: loss = 0.211787 (* 1 = 0.211787 loss)
I1123 16:52:09.278298 14636 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1123 16:52:14.081347 14636 solver.cpp:218] Iteration 17800 (20.821 iter/s, 4.80284s/100 iters), loss = 0.203275
I1123 16:52:14.081347 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:52:14.081347 14636 solver.cpp:237]     Train net output #1: loss = 0.203275 (* 1 = 0.203275 loss)
I1123 16:52:14.081347 14636 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1123 16:52:18.885462 14636 solver.cpp:218] Iteration 17900 (20.8171 iter/s, 4.80375s/100 iters), loss = 0.109504
I1123 16:52:18.885462 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 16:52:18.885462 14636 solver.cpp:237]     Train net output #1: loss = 0.109504 (* 1 = 0.109504 loss)
I1123 16:52:18.885462 14636 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1123 16:52:23.450060 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:52:23.639077 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_18000.caffemodel
I1123 16:52:23.669077 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_18000.solverstate
I1123 16:52:23.673079 14636 solver.cpp:330] Iteration 18000, Testing net (#0)
I1123 16:52:23.673079 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:52:24.939329 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:52:24.989334 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8837
I1123 16:52:24.989334 14636 solver.cpp:397]     Test net output #1: loss = 0.340846 (* 1 = 0.340846 loss)
I1123 16:52:25.036336 14636 solver.cpp:218] Iteration 18000 (16.2597 iter/s, 6.15018s/100 iters), loss = 0.182415
I1123 16:52:25.036336 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:52:25.036336 14636 solver.cpp:237]     Train net output #1: loss = 0.182415 (* 1 = 0.182415 loss)
I1123 16:52:25.036336 14636 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1123 16:52:29.837905 14636 solver.cpp:218] Iteration 18100 (20.8246 iter/s, 4.80202s/100 iters), loss = 0.217435
I1123 16:52:29.837905 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:52:29.837905 14636 solver.cpp:237]     Train net output #1: loss = 0.217435 (* 1 = 0.217435 loss)
I1123 16:52:29.837905 14636 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1123 16:52:34.639683 14636 solver.cpp:218] Iteration 18200 (20.8277 iter/s, 4.80129s/100 iters), loss = 0.263222
I1123 16:52:34.639683 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:52:34.639683 14636 solver.cpp:237]     Train net output #1: loss = 0.263222 (* 1 = 0.263222 loss)
I1123 16:52:34.639683 14636 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1123 16:52:39.439260 14636 solver.cpp:218] Iteration 18300 (20.8372 iter/s, 4.79911s/100 iters), loss = 0.264702
I1123 16:52:39.439260 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:52:39.439260 14636 solver.cpp:237]     Train net output #1: loss = 0.264702 (* 1 = 0.264702 loss)
I1123 16:52:39.439260 14636 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1123 16:52:44.314831 14636 solver.cpp:218] Iteration 18400 (20.5154 iter/s, 4.87438s/100 iters), loss = 0.143644
I1123 16:52:44.314831 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:52:44.314831 14636 solver.cpp:237]     Train net output #1: loss = 0.143644 (* 1 = 0.143644 loss)
I1123 16:52:44.314831 14636 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1123 16:52:48.884979 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:52:49.073501 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_18500.caffemodel
I1123 16:52:49.084509 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_18500.solverstate
I1123 16:52:49.088009 14636 solver.cpp:330] Iteration 18500, Testing net (#0)
I1123 16:52:49.088009 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:52:50.354562 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:52:50.404063 14636 solver.cpp:397]     Test net output #0: accuracy = 0.884
I1123 16:52:50.404063 14636 solver.cpp:397]     Test net output #1: loss = 0.340376 (* 1 = 0.340376 loss)
I1123 16:52:50.450060 14636 solver.cpp:218] Iteration 18500 (16.3 iter/s, 6.13497s/100 iters), loss = 0.212984
I1123 16:52:50.450060 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:52:50.450060 14636 solver.cpp:237]     Train net output #1: loss = 0.212984 (* 1 = 0.212984 loss)
I1123 16:52:50.450060 14636 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1123 16:52:55.253907 14636 solver.cpp:218] Iteration 18600 (20.8165 iter/s, 4.80389s/100 iters), loss = 0.195244
I1123 16:52:55.253907 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:52:55.253907 14636 solver.cpp:237]     Train net output #1: loss = 0.195244 (* 1 = 0.195244 loss)
I1123 16:52:55.253907 14636 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1123 16:53:00.060375 14636 solver.cpp:218] Iteration 18700 (20.8088 iter/s, 4.80566s/100 iters), loss = 0.178653
I1123 16:53:00.060375 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:53:00.060375 14636 solver.cpp:237]     Train net output #1: loss = 0.178653 (* 1 = 0.178653 loss)
I1123 16:53:00.060375 14636 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1123 16:53:04.900245 14636 solver.cpp:218] Iteration 18800 (20.6626 iter/s, 4.83966s/100 iters), loss = 0.225948
I1123 16:53:04.900245 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:53:04.900245 14636 solver.cpp:237]     Train net output #1: loss = 0.225948 (* 1 = 0.225948 loss)
I1123 16:53:04.900245 14636 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1123 16:53:09.707914 14636 solver.cpp:218] Iteration 18900 (20.8 iter/s, 4.8077s/100 iters), loss = 0.142304
I1123 16:53:09.708914 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:53:09.708914 14636 solver.cpp:237]     Train net output #1: loss = 0.142304 (* 1 = 0.142304 loss)
I1123 16:53:09.708914 14636 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1123 16:53:14.291513 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:53:14.479531 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_19000.caffemodel
I1123 16:53:14.502532 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_19000.solverstate
I1123 16:53:14.505532 14636 solver.cpp:330] Iteration 19000, Testing net (#0)
I1123 16:53:14.505532 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:53:15.773702 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:53:15.823705 14636 solver.cpp:397]     Test net output #0: accuracy = 0.884
I1123 16:53:15.823705 14636 solver.cpp:397]     Test net output #1: loss = 0.340479 (* 1 = 0.340479 loss)
I1123 16:53:15.870709 14636 solver.cpp:218] Iteration 19000 (16.2299 iter/s, 6.16147s/100 iters), loss = 0.187707
I1123 16:53:15.870709 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:53:15.870709 14636 solver.cpp:237]     Train net output #1: loss = 0.187707 (* 1 = 0.187707 loss)
I1123 16:53:15.870709 14636 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1123 16:53:20.678261 14636 solver.cpp:218] Iteration 19100 (20.7992 iter/s, 4.80787s/100 iters), loss = 0.187096
I1123 16:53:20.678261 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:53:20.678261 14636 solver.cpp:237]     Train net output #1: loss = 0.187096 (* 1 = 0.187096 loss)
I1123 16:53:20.678261 14636 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1123 16:53:25.485807 14636 solver.cpp:218] Iteration 19200 (20.806 iter/s, 4.8063s/100 iters), loss = 0.168823
I1123 16:53:25.485807 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:53:25.485807 14636 solver.cpp:237]     Train net output #1: loss = 0.168823 (* 1 = 0.168823 loss)
I1123 16:53:25.485807 14636 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1123 16:53:30.294329 14636 solver.cpp:218] Iteration 19300 (20.7972 iter/s, 4.80833s/100 iters), loss = 0.200697
I1123 16:53:30.294329 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:53:30.294329 14636 solver.cpp:237]     Train net output #1: loss = 0.200697 (* 1 = 0.200697 loss)
I1123 16:53:30.294329 14636 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1123 16:53:35.101047 14636 solver.cpp:218] Iteration 19400 (20.8034 iter/s, 4.8069s/100 iters), loss = 0.159562
I1123 16:53:35.101047 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:53:35.101047 14636 solver.cpp:237]     Train net output #1: loss = 0.159562 (* 1 = 0.159562 loss)
I1123 16:53:35.102048 14636 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1123 16:53:39.675175 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:53:39.865196 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_19500.caffemodel
I1123 16:53:39.875196 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_19500.solverstate
I1123 16:53:39.879703 14636 solver.cpp:330] Iteration 19500, Testing net (#0)
I1123 16:53:39.879703 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:53:41.154884 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:53:41.204896 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8845
I1123 16:53:41.204896 14636 solver.cpp:397]     Test net output #1: loss = 0.340216 (* 1 = 0.340216 loss)
I1123 16:53:41.250895 14636 solver.cpp:218] Iteration 19500 (16.2617 iter/s, 6.14942s/100 iters), loss = 0.156789
I1123 16:53:41.250895 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:53:41.251896 14636 solver.cpp:237]     Train net output #1: loss = 0.156789 (* 1 = 0.156789 loss)
I1123 16:53:41.251896 14636 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1123 16:53:41.251896 14636 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1123 16:53:46.097535 14636 solver.cpp:218] Iteration 19600 (20.6353 iter/s, 4.84607s/100 iters), loss = 0.170724
I1123 16:53:46.097535 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:53:46.097535 14636 solver.cpp:237]     Train net output #1: loss = 0.170724 (* 1 = 0.170724 loss)
I1123 16:53:46.097535 14636 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1123 16:53:50.920104 14636 solver.cpp:218] Iteration 19700 (20.7389 iter/s, 4.82187s/100 iters), loss = 0.254147
I1123 16:53:50.920104 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:53:50.920104 14636 solver.cpp:237]     Train net output #1: loss = 0.254147 (* 1 = 0.254147 loss)
I1123 16:53:50.920104 14636 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1123 16:53:55.723598 14636 solver.cpp:218] Iteration 19800 (20.821 iter/s, 4.80283s/100 iters), loss = 0.252306
I1123 16:53:55.723598 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:53:55.723598 14636 solver.cpp:237]     Train net output #1: loss = 0.252306 (* 1 = 0.252306 loss)
I1123 16:53:55.723598 14636 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1123 16:54:00.529114 14636 solver.cpp:218] Iteration 19900 (20.8117 iter/s, 4.805s/100 iters), loss = 0.125965
I1123 16:54:00.529114 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 16:54:00.529114 14636 solver.cpp:237]     Train net output #1: loss = 0.125965 (* 1 = 0.125965 loss)
I1123 16:54:00.529114 14636 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1123 16:54:05.094684 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:54:05.283692 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_20000.caffemodel
I1123 16:54:05.310693 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_20000.solverstate
I1123 16:54:05.314692 14636 solver.cpp:330] Iteration 20000, Testing net (#0)
I1123 16:54:05.314692 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:54:06.582836 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:54:06.632849 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8848
I1123 16:54:06.632849 14636 solver.cpp:397]     Test net output #1: loss = 0.340163 (* 1 = 0.340163 loss)
I1123 16:54:06.679839 14636 solver.cpp:218] Iteration 20000 (16.2596 iter/s, 6.15022s/100 iters), loss = 0.1978
I1123 16:54:06.679839 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:54:06.679839 14636 solver.cpp:237]     Train net output #1: loss = 0.197801 (* 1 = 0.197801 loss)
I1123 16:54:06.679839 14636 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1123 16:54:11.487691 14636 solver.cpp:218] Iteration 20100 (20.7997 iter/s, 4.80776s/100 iters), loss = 0.14334
I1123 16:54:11.487691 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:54:11.487691 14636 solver.cpp:237]     Train net output #1: loss = 0.143341 (* 1 = 0.143341 loss)
I1123 16:54:11.487691 14636 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1123 16:54:16.291246 14636 solver.cpp:218] Iteration 20200 (20.8206 iter/s, 4.80294s/100 iters), loss = 0.287657
I1123 16:54:16.291246 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:54:16.291246 14636 solver.cpp:237]     Train net output #1: loss = 0.287657 (* 1 = 0.287657 loss)
I1123 16:54:16.291246 14636 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1123 16:54:21.095779 14636 solver.cpp:218] Iteration 20300 (20.8131 iter/s, 4.80466s/100 iters), loss = 0.159842
I1123 16:54:21.095779 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:54:21.095779 14636 solver.cpp:237]     Train net output #1: loss = 0.159843 (* 1 = 0.159843 loss)
I1123 16:54:21.095779 14636 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1123 16:54:25.900410 14636 solver.cpp:218] Iteration 20400 (20.8141 iter/s, 4.80444s/100 iters), loss = 0.187179
I1123 16:54:25.900410 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:54:25.900410 14636 solver.cpp:237]     Train net output #1: loss = 0.187179 (* 1 = 0.187179 loss)
I1123 16:54:25.900410 14636 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1123 16:54:30.500936 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:54:30.689957 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_20500.caffemodel
I1123 16:54:30.700953 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_20500.solverstate
I1123 16:54:30.704952 14636 solver.cpp:330] Iteration 20500, Testing net (#0)
I1123 16:54:30.704952 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:54:31.971114 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:54:32.021118 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8849
I1123 16:54:32.021118 14636 solver.cpp:397]     Test net output #1: loss = 0.340126 (* 1 = 0.340126 loss)
I1123 16:54:32.067117 14636 solver.cpp:218] Iteration 20500 (16.2179 iter/s, 6.16603s/100 iters), loss = 0.189017
I1123 16:54:32.067117 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:54:32.067117 14636 solver.cpp:237]     Train net output #1: loss = 0.189017 (* 1 = 0.189017 loss)
I1123 16:54:32.067117 14636 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1123 16:54:36.883738 14636 solver.cpp:218] Iteration 20600 (20.7637 iter/s, 4.81609s/100 iters), loss = 0.192926
I1123 16:54:36.884239 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:54:36.884239 14636 solver.cpp:237]     Train net output #1: loss = 0.192926 (* 1 = 0.192926 loss)
I1123 16:54:36.884239 14636 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1123 16:54:41.693342 14636 solver.cpp:218] Iteration 20700 (20.7939 iter/s, 4.80911s/100 iters), loss = 0.203119
I1123 16:54:41.693342 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:54:41.693342 14636 solver.cpp:237]     Train net output #1: loss = 0.20312 (* 1 = 0.20312 loss)
I1123 16:54:41.693342 14636 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1123 16:54:46.529103 14636 solver.cpp:218] Iteration 20800 (20.6807 iter/s, 4.83542s/100 iters), loss = 0.238235
I1123 16:54:46.529103 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:54:46.529103 14636 solver.cpp:237]     Train net output #1: loss = 0.238235 (* 1 = 0.238235 loss)
I1123 16:54:46.529103 14636 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1123 16:54:51.350672 14636 solver.cpp:218] Iteration 20900 (20.7433 iter/s, 4.82084s/100 iters), loss = 0.145813
I1123 16:54:51.350672 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 16:54:51.350672 14636 solver.cpp:237]     Train net output #1: loss = 0.145813 (* 1 = 0.145813 loss)
I1123 16:54:51.350672 14636 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1123 16:54:55.919287 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:54:56.108314 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_21000.caffemodel
I1123 16:54:56.143314 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_21000.solverstate
I1123 16:54:56.147315 14636 solver.cpp:330] Iteration 21000, Testing net (#0)
I1123 16:54:56.147315 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:54:57.414501 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:54:57.464503 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8847
I1123 16:54:57.464503 14636 solver.cpp:397]     Test net output #1: loss = 0.340178 (* 1 = 0.340178 loss)
I1123 16:54:57.510512 14636 solver.cpp:218] Iteration 21000 (16.2347 iter/s, 6.15966s/100 iters), loss = 0.166674
I1123 16:54:57.510512 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:54:57.510512 14636 solver.cpp:237]     Train net output #1: loss = 0.166674 (* 1 = 0.166674 loss)
I1123 16:54:57.510512 14636 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1123 16:55:02.317311 14636 solver.cpp:218] Iteration 21100 (20.8073 iter/s, 4.806s/100 iters), loss = 0.173751
I1123 16:55:02.317311 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:55:02.317311 14636 solver.cpp:237]     Train net output #1: loss = 0.173751 (* 1 = 0.173751 loss)
I1123 16:55:02.317311 14636 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1123 16:55:07.123852 14636 solver.cpp:218] Iteration 21200 (20.8042 iter/s, 4.80673s/100 iters), loss = 0.189778
I1123 16:55:07.123852 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:55:07.123852 14636 solver.cpp:237]     Train net output #1: loss = 0.189778 (* 1 = 0.189778 loss)
I1123 16:55:07.123852 14636 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1123 16:55:11.929648 14636 solver.cpp:218] Iteration 21300 (20.8115 iter/s, 4.80504s/100 iters), loss = 0.206023
I1123 16:55:11.929648 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:55:11.929648 14636 solver.cpp:237]     Train net output #1: loss = 0.206024 (* 1 = 0.206024 loss)
I1123 16:55:11.929648 14636 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1123 16:55:16.737499 14636 solver.cpp:218] Iteration 21400 (20.8006 iter/s, 4.80756s/100 iters), loss = 0.130919
I1123 16:55:16.737499 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:55:16.737499 14636 solver.cpp:237]     Train net output #1: loss = 0.130919 (* 1 = 0.130919 loss)
I1123 16:55:16.737499 14636 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1123 16:55:21.305341 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:55:21.494366 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_21500.caffemodel
I1123 16:55:21.505360 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_21500.solverstate
I1123 16:55:21.509359 14636 solver.cpp:330] Iteration 21500, Testing net (#0)
I1123 16:55:21.509359 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:55:22.774047 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:55:22.823560 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8844
I1123 16:55:22.823560 14636 solver.cpp:397]     Test net output #1: loss = 0.34031 (* 1 = 0.34031 loss)
I1123 16:55:22.870560 14636 solver.cpp:218] Iteration 21500 (16.3065 iter/s, 6.13254s/100 iters), loss = 0.151311
I1123 16:55:22.870560 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:55:22.870560 14636 solver.cpp:237]     Train net output #1: loss = 0.151311 (* 1 = 0.151311 loss)
I1123 16:55:22.870560 14636 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1123 16:55:27.688968 14636 solver.cpp:218] Iteration 21600 (20.7538 iter/s, 4.8184s/100 iters), loss = 0.196611
I1123 16:55:27.688968 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:55:27.688968 14636 solver.cpp:237]     Train net output #1: loss = 0.196612 (* 1 = 0.196612 loss)
I1123 16:55:27.688968 14636 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1123 16:55:32.489473 14636 solver.cpp:218] Iteration 21700 (20.835 iter/s, 4.79963s/100 iters), loss = 0.283471
I1123 16:55:32.489473 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:55:32.489473 14636 solver.cpp:237]     Train net output #1: loss = 0.283471 (* 1 = 0.283471 loss)
I1123 16:55:32.489473 14636 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1123 16:55:37.291170 14636 solver.cpp:218] Iteration 21800 (20.8265 iter/s, 4.80157s/100 iters), loss = 0.234169
I1123 16:55:37.291677 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:55:37.291677 14636 solver.cpp:237]     Train net output #1: loss = 0.234169 (* 1 = 0.234169 loss)
I1123 16:55:37.291677 14636 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1123 16:55:42.091697 14636 solver.cpp:218] Iteration 21900 (20.8348 iter/s, 4.79965s/100 iters), loss = 0.108302
I1123 16:55:42.091697 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 16:55:42.091697 14636 solver.cpp:237]     Train net output #1: loss = 0.108302 (* 1 = 0.108302 loss)
I1123 16:55:42.091697 14636 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1123 16:55:46.657227 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:55:46.846367 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_22000.caffemodel
I1123 16:55:46.868366 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_22000.solverstate
I1123 16:55:46.872366 14636 solver.cpp:330] Iteration 22000, Testing net (#0)
I1123 16:55:46.872366 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:55:48.137506 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:55:48.187505 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8845
I1123 16:55:48.187505 14636 solver.cpp:397]     Test net output #1: loss = 0.340197 (* 1 = 0.340197 loss)
I1123 16:55:48.234514 14636 solver.cpp:218] Iteration 22000 (16.28 iter/s, 6.14249s/100 iters), loss = 0.136465
I1123 16:55:48.234514 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:55:48.234514 14636 solver.cpp:237]     Train net output #1: loss = 0.136465 (* 1 = 0.136465 loss)
I1123 16:55:48.234514 14636 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1123 16:55:48.234514 14636 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1123 16:55:53.039033 14636 solver.cpp:218] Iteration 22100 (20.8157 iter/s, 4.80407s/100 iters), loss = 0.2191
I1123 16:55:53.039033 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:55:53.039033 14636 solver.cpp:237]     Train net output #1: loss = 0.219101 (* 1 = 0.219101 loss)
I1123 16:55:53.039033 14636 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1123 16:55:57.845798 14636 solver.cpp:218] Iteration 22200 (20.8049 iter/s, 4.80655s/100 iters), loss = 0.191571
I1123 16:55:57.845798 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:55:57.845798 14636 solver.cpp:237]     Train net output #1: loss = 0.191572 (* 1 = 0.191572 loss)
I1123 16:55:57.845798 14636 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1123 16:56:02.653339 14636 solver.cpp:218] Iteration 22300 (20.8034 iter/s, 4.8069s/100 iters), loss = 0.204954
I1123 16:56:02.653339 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:56:02.653339 14636 solver.cpp:237]     Train net output #1: loss = 0.204954 (* 1 = 0.204954 loss)
I1123 16:56:02.653339 14636 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1123 16:56:07.461906 14636 solver.cpp:218] Iteration 22400 (20.7954 iter/s, 4.80876s/100 iters), loss = 0.157783
I1123 16:56:07.461906 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:56:07.461906 14636 solver.cpp:237]     Train net output #1: loss = 0.157784 (* 1 = 0.157784 loss)
I1123 16:56:07.461906 14636 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1123 16:56:12.033143 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:56:12.222159 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_22500.caffemodel
I1123 16:56:12.233162 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_22500.solverstate
I1123 16:56:12.236160 14636 solver.cpp:330] Iteration 22500, Testing net (#0)
I1123 16:56:12.236160 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:56:13.502295 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:56:13.553300 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8843
I1123 16:56:13.553300 14636 solver.cpp:397]     Test net output #1: loss = 0.340143 (* 1 = 0.340143 loss)
I1123 16:56:13.599301 14636 solver.cpp:218] Iteration 22500 (16.2954 iter/s, 6.13669s/100 iters), loss = 0.196265
I1123 16:56:13.599301 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:56:13.599301 14636 solver.cpp:237]     Train net output #1: loss = 0.196265 (* 1 = 0.196265 loss)
I1123 16:56:13.599301 14636 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1123 16:56:18.434705 14636 solver.cpp:218] Iteration 22600 (20.6838 iter/s, 4.8347s/100 iters), loss = 0.213564
I1123 16:56:18.434705 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:56:18.434705 14636 solver.cpp:237]     Train net output #1: loss = 0.213564 (* 1 = 0.213564 loss)
I1123 16:56:18.434705 14636 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1123 16:56:23.260270 14636 solver.cpp:218] Iteration 22700 (20.7243 iter/s, 4.82524s/100 iters), loss = 0.208024
I1123 16:56:23.260270 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:56:23.260270 14636 solver.cpp:237]     Train net output #1: loss = 0.208024 (* 1 = 0.208024 loss)
I1123 16:56:23.260270 14636 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1123 16:56:28.103248 14636 solver.cpp:218] Iteration 22800 (20.6493 iter/s, 4.84278s/100 iters), loss = 0.223257
I1123 16:56:28.103248 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:56:28.103248 14636 solver.cpp:237]     Train net output #1: loss = 0.223258 (* 1 = 0.223258 loss)
I1123 16:56:28.103248 14636 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1123 16:56:32.947389 14636 solver.cpp:218] Iteration 22900 (20.6446 iter/s, 4.84389s/100 iters), loss = 0.189117
I1123 16:56:32.947389 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:56:32.947389 14636 solver.cpp:237]     Train net output #1: loss = 0.189118 (* 1 = 0.189118 loss)
I1123 16:56:32.947389 14636 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1123 16:56:37.539836 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:56:37.730870 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_23000.caffemodel
I1123 16:56:37.767869 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_23000.solverstate
I1123 16:56:37.771869 14636 solver.cpp:330] Iteration 23000, Testing net (#0)
I1123 16:56:37.771869 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:56:39.039017 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:56:39.089015 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8844
I1123 16:56:39.089015 14636 solver.cpp:397]     Test net output #1: loss = 0.340272 (* 1 = 0.340272 loss)
I1123 16:56:39.136024 14636 solver.cpp:218] Iteration 23000 (16.1599 iter/s, 6.18817s/100 iters), loss = 0.132567
I1123 16:56:39.136024 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 16:56:39.136024 14636 solver.cpp:237]     Train net output #1: loss = 0.132567 (* 1 = 0.132567 loss)
I1123 16:56:39.136024 14636 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1123 16:56:43.946851 14636 solver.cpp:218] Iteration 23100 (20.7889 iter/s, 4.81025s/100 iters), loss = 0.218921
I1123 16:56:43.946851 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:56:43.946851 14636 solver.cpp:237]     Train net output #1: loss = 0.218921 (* 1 = 0.218921 loss)
I1123 16:56:43.946851 14636 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1123 16:56:48.775408 14636 solver.cpp:218] Iteration 23200 (20.7106 iter/s, 4.82845s/100 iters), loss = 0.257728
I1123 16:56:48.775408 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:56:48.775408 14636 solver.cpp:237]     Train net output #1: loss = 0.257729 (* 1 = 0.257729 loss)
I1123 16:56:48.775408 14636 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1123 16:56:53.597692 14636 solver.cpp:218] Iteration 23300 (20.7398 iter/s, 4.82164s/100 iters), loss = 0.240606
I1123 16:56:53.597692 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:56:53.597692 14636 solver.cpp:237]     Train net output #1: loss = 0.240606 (* 1 = 0.240606 loss)
I1123 16:56:53.597692 14636 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1123 16:56:58.442301 14636 solver.cpp:218] Iteration 23400 (20.6439 iter/s, 4.84406s/100 iters), loss = 0.145326
I1123 16:56:58.442301 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:56:58.442301 14636 solver.cpp:237]     Train net output #1: loss = 0.145326 (* 1 = 0.145326 loss)
I1123 16:56:58.442301 14636 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1123 16:57:03.027819 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:57:03.219833 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_23500.caffemodel
I1123 16:57:03.229833 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_23500.solverstate
I1123 16:57:03.233836 14636 solver.cpp:330] Iteration 23500, Testing net (#0)
I1123 16:57:03.233836 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:57:04.500969 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:57:04.550979 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8845
I1123 16:57:04.550979 14636 solver.cpp:397]     Test net output #1: loss = 0.340126 (* 1 = 0.340126 loss)
I1123 16:57:04.597976 14636 solver.cpp:218] Iteration 23500 (16.2475 iter/s, 6.15481s/100 iters), loss = 0.159904
I1123 16:57:04.597976 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:57:04.597976 14636 solver.cpp:237]     Train net output #1: loss = 0.159905 (* 1 = 0.159905 loss)
I1123 16:57:04.597976 14636 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1123 16:57:09.410327 14636 solver.cpp:218] Iteration 23600 (20.7812 iter/s, 4.81205s/100 iters), loss = 0.176957
I1123 16:57:09.410327 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 16:57:09.410327 14636 solver.cpp:237]     Train net output #1: loss = 0.176957 (* 1 = 0.176957 loss)
I1123 16:57:09.410327 14636 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1123 16:57:14.245200 14636 solver.cpp:218] Iteration 23700 (20.6816 iter/s, 4.83521s/100 iters), loss = 0.229733
I1123 16:57:14.245200 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:57:14.245200 14636 solver.cpp:237]     Train net output #1: loss = 0.229734 (* 1 = 0.229734 loss)
I1123 16:57:14.245200 14636 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1123 16:57:19.091747 14636 solver.cpp:218] Iteration 23800 (20.6368 iter/s, 4.84572s/100 iters), loss = 0.215914
I1123 16:57:19.091747 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:57:19.091747 14636 solver.cpp:237]     Train net output #1: loss = 0.215914 (* 1 = 0.215914 loss)
I1123 16:57:19.091747 14636 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1123 16:57:23.900315 14636 solver.cpp:218] Iteration 23900 (20.7958 iter/s, 4.80867s/100 iters), loss = 0.167143
I1123 16:57:23.901317 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:57:23.901317 14636 solver.cpp:237]     Train net output #1: loss = 0.167143 (* 1 = 0.167143 loss)
I1123 16:57:23.901317 14636 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1123 16:57:28.471863 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:57:28.660882 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_24000.caffemodel
I1123 16:57:28.693883 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_24000.solverstate
I1123 16:57:28.697885 14636 solver.cpp:330] Iteration 24000, Testing net (#0)
I1123 16:57:28.697885 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:57:29.964054 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:57:30.014564 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8842
I1123 16:57:30.014564 14636 solver.cpp:397]     Test net output #1: loss = 0.340176 (* 1 = 0.340176 loss)
I1123 16:57:30.060065 14636 solver.cpp:218] Iteration 24000 (16.2361 iter/s, 6.15912s/100 iters), loss = 0.181504
I1123 16:57:30.060065 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:57:30.060065 14636 solver.cpp:237]     Train net output #1: loss = 0.181505 (* 1 = 0.181505 loss)
I1123 16:57:30.060065 14636 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1123 16:57:34.868661 14636 solver.cpp:218] Iteration 24100 (20.8 iter/s, 4.80769s/100 iters), loss = 0.180916
I1123 16:57:34.868661 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:57:34.868661 14636 solver.cpp:237]     Train net output #1: loss = 0.180916 (* 1 = 0.180916 loss)
I1123 16:57:34.868661 14636 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1123 16:57:39.676204 14636 solver.cpp:218] Iteration 24200 (20.8015 iter/s, 4.80736s/100 iters), loss = 0.232917
I1123 16:57:39.676204 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:57:39.676204 14636 solver.cpp:237]     Train net output #1: loss = 0.232917 (* 1 = 0.232917 loss)
I1123 16:57:39.676204 14636 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1123 16:57:44.483743 14636 solver.cpp:218] Iteration 24300 (20.8032 iter/s, 4.80695s/100 iters), loss = 0.241686
I1123 16:57:44.483743 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:57:44.483743 14636 solver.cpp:237]     Train net output #1: loss = 0.241687 (* 1 = 0.241687 loss)
I1123 16:57:44.483743 14636 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1123 16:57:49.292286 14636 solver.cpp:218] Iteration 24400 (20.7979 iter/s, 4.80817s/100 iters), loss = 0.173543
I1123 16:57:49.292286 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:57:49.292286 14636 solver.cpp:237]     Train net output #1: loss = 0.173544 (* 1 = 0.173544 loss)
I1123 16:57:49.292286 14636 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1123 16:57:53.865865 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:57:54.054894 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_24500.caffemodel
I1123 16:57:54.065894 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_24500.solverstate
I1123 16:57:54.068894 14636 solver.cpp:330] Iteration 24500, Testing net (#0)
I1123 16:57:54.068894 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:57:55.334050 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:57:55.384042 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8843
I1123 16:57:55.384042 14636 solver.cpp:397]     Test net output #1: loss = 0.340114 (* 1 = 0.340114 loss)
I1123 16:57:55.431051 14636 solver.cpp:218] Iteration 24500 (16.2914 iter/s, 6.13822s/100 iters), loss = 0.130888
I1123 16:57:55.431051 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:57:55.431051 14636 solver.cpp:237]     Train net output #1: loss = 0.130888 (* 1 = 0.130888 loss)
I1123 16:57:55.431051 14636 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1123 16:58:00.238137 14636 solver.cpp:218] Iteration 24600 (20.8046 iter/s, 4.80664s/100 iters), loss = 0.161684
I1123 16:58:00.238137 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:58:00.238137 14636 solver.cpp:237]     Train net output #1: loss = 0.161684 (* 1 = 0.161684 loss)
I1123 16:58:00.238137 14636 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1123 16:58:05.042951 14636 solver.cpp:218] Iteration 24700 (20.8103 iter/s, 4.80531s/100 iters), loss = 0.2318
I1123 16:58:05.043952 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:58:05.043952 14636 solver.cpp:237]     Train net output #1: loss = 0.231801 (* 1 = 0.231801 loss)
I1123 16:58:05.043952 14636 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1123 16:58:09.846544 14636 solver.cpp:218] Iteration 24800 (20.8203 iter/s, 4.80301s/100 iters), loss = 0.191962
I1123 16:58:09.846544 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:58:09.846544 14636 solver.cpp:237]     Train net output #1: loss = 0.191962 (* 1 = 0.191962 loss)
I1123 16:58:09.846544 14636 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1123 16:58:14.676318 14636 solver.cpp:218] Iteration 24900 (20.7085 iter/s, 4.82894s/100 iters), loss = 0.170935
I1123 16:58:14.676318 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:58:14.676318 14636 solver.cpp:237]     Train net output #1: loss = 0.170935 (* 1 = 0.170935 loss)
I1123 16:58:14.676318 14636 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1123 16:58:19.248693 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:58:19.437712 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_25000.caffemodel
I1123 16:58:19.460714 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_25000.solverstate
I1123 16:58:19.464714 14636 solver.cpp:330] Iteration 25000, Testing net (#0)
I1123 16:58:19.464714 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:58:20.731874 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:58:20.781883 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8844
I1123 16:58:20.781883 14636 solver.cpp:397]     Test net output #1: loss = 0.340208 (* 1 = 0.340208 loss)
I1123 16:58:20.828882 14636 solver.cpp:218] Iteration 25000 (16.2549 iter/s, 6.15201s/100 iters), loss = 0.194411
I1123 16:58:20.828882 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:58:20.828882 14636 solver.cpp:237]     Train net output #1: loss = 0.194411 (* 1 = 0.194411 loss)
I1123 16:58:20.828882 14636 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1123 16:58:25.643730 14636 solver.cpp:218] Iteration 25100 (20.7677 iter/s, 4.81518s/100 iters), loss = 0.178116
I1123 16:58:25.644732 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:58:25.644732 14636 solver.cpp:237]     Train net output #1: loss = 0.178117 (* 1 = 0.178117 loss)
I1123 16:58:25.644732 14636 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1123 16:58:30.444540 14636 solver.cpp:218] Iteration 25200 (20.832 iter/s, 4.80031s/100 iters), loss = 0.18026
I1123 16:58:30.444540 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:58:30.444540 14636 solver.cpp:237]     Train net output #1: loss = 0.18026 (* 1 = 0.18026 loss)
I1123 16:58:30.444540 14636 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1123 16:58:35.247921 14636 solver.cpp:218] Iteration 25300 (20.8228 iter/s, 4.80243s/100 iters), loss = 0.238703
I1123 16:58:35.247921 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:58:35.247921 14636 solver.cpp:237]     Train net output #1: loss = 0.238703 (* 1 = 0.238703 loss)
I1123 16:58:35.247921 14636 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1123 16:58:40.045543 14636 solver.cpp:218] Iteration 25400 (20.845 iter/s, 4.7973s/100 iters), loss = 0.142505
I1123 16:58:40.045543 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:58:40.045543 14636 solver.cpp:237]     Train net output #1: loss = 0.142506 (* 1 = 0.142506 loss)
I1123 16:58:40.045543 14636 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1123 16:58:44.609906 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:58:44.798929 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_25500.caffemodel
I1123 16:58:44.808437 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_25500.solverstate
I1123 16:58:44.812438 14636 solver.cpp:330] Iteration 25500, Testing net (#0)
I1123 16:58:44.812438 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:58:46.078107 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:58:46.129124 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8845
I1123 16:58:46.129124 14636 solver.cpp:397]     Test net output #1: loss = 0.340301 (* 1 = 0.340301 loss)
I1123 16:58:46.175118 14636 solver.cpp:218] Iteration 25500 (16.3155 iter/s, 6.12916s/100 iters), loss = 0.158428
I1123 16:58:46.175118 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:58:46.175118 14636 solver.cpp:237]     Train net output #1: loss = 0.158429 (* 1 = 0.158429 loss)
I1123 16:58:46.175118 14636 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1123 16:58:50.980281 14636 solver.cpp:218] Iteration 25600 (20.8119 iter/s, 4.80495s/100 iters), loss = 0.244543
I1123 16:58:50.980281 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:58:50.980281 14636 solver.cpp:237]     Train net output #1: loss = 0.244543 (* 1 = 0.244543 loss)
I1123 16:58:50.980281 14636 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1123 16:58:55.786810 14636 solver.cpp:218] Iteration 25700 (20.8099 iter/s, 4.80541s/100 iters), loss = 0.189308
I1123 16:58:55.786810 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:58:55.786810 14636 solver.cpp:237]     Train net output #1: loss = 0.189309 (* 1 = 0.189309 loss)
I1123 16:58:55.786810 14636 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1123 16:59:00.603360 14636 solver.cpp:218] Iteration 25800 (20.7629 iter/s, 4.81628s/100 iters), loss = 0.198757
I1123 16:59:00.603360 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:59:00.603360 14636 solver.cpp:237]     Train net output #1: loss = 0.198757 (* 1 = 0.198757 loss)
I1123 16:59:00.603360 14636 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1123 16:59:05.409493 14636 solver.cpp:218] Iteration 25900 (20.8078 iter/s, 4.80588s/100 iters), loss = 0.157318
I1123 16:59:05.409493 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 16:59:05.409998 14636 solver.cpp:237]     Train net output #1: loss = 0.157319 (* 1 = 0.157319 loss)
I1123 16:59:05.409998 14636 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1123 16:59:09.978000 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:59:10.166021 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_26000.caffemodel
I1123 16:59:10.204021 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_26000.solverstate
I1123 16:59:10.208021 14636 solver.cpp:330] Iteration 26000, Testing net (#0)
I1123 16:59:10.208021 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:59:11.473189 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:59:11.524206 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8845
I1123 16:59:11.524206 14636 solver.cpp:397]     Test net output #1: loss = 0.340198 (* 1 = 0.340198 loss)
I1123 16:59:11.570200 14636 solver.cpp:218] Iteration 26000 (16.2339 iter/s, 6.15996s/100 iters), loss = 0.210324
I1123 16:59:11.570200 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:59:11.570200 14636 solver.cpp:237]     Train net output #1: loss = 0.210324 (* 1 = 0.210324 loss)
I1123 16:59:11.570200 14636 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1123 16:59:16.378433 14636 solver.cpp:218] Iteration 26100 (20.7962 iter/s, 4.80857s/100 iters), loss = 0.164298
I1123 16:59:16.378433 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:59:16.378433 14636 solver.cpp:237]     Train net output #1: loss = 0.164299 (* 1 = 0.164299 loss)
I1123 16:59:16.378433 14636 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1123 16:59:21.179502 14636 solver.cpp:218] Iteration 26200 (20.8325 iter/s, 4.80019s/100 iters), loss = 0.210029
I1123 16:59:21.179502 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:59:21.179502 14636 solver.cpp:237]     Train net output #1: loss = 0.21003 (* 1 = 0.21003 loss)
I1123 16:59:21.179502 14636 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1123 16:59:25.986055 14636 solver.cpp:218] Iteration 26300 (20.8058 iter/s, 4.80635s/100 iters), loss = 0.207662
I1123 16:59:25.986055 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:59:25.986055 14636 solver.cpp:237]     Train net output #1: loss = 0.207662 (* 1 = 0.207662 loss)
I1123 16:59:25.986055 14636 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1123 16:59:30.786608 14636 solver.cpp:218] Iteration 26400 (20.8352 iter/s, 4.79957s/100 iters), loss = 0.159789
I1123 16:59:30.786608 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:59:30.786608 14636 solver.cpp:237]     Train net output #1: loss = 0.15979 (* 1 = 0.15979 loss)
I1123 16:59:30.786608 14636 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1123 16:59:35.358690 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:59:35.546710 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_26500.caffemodel
I1123 16:59:35.558710 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_26500.solverstate
I1123 16:59:35.562710 14636 solver.cpp:330] Iteration 26500, Testing net (#0)
I1123 16:59:35.562710 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:59:36.828006 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:59:36.877008 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8845
I1123 16:59:36.877008 14636 solver.cpp:397]     Test net output #1: loss = 0.340205 (* 1 = 0.340205 loss)
I1123 16:59:36.924052 14636 solver.cpp:218] Iteration 26500 (16.2943 iter/s, 6.13713s/100 iters), loss = 0.202802
I1123 16:59:36.924052 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:59:36.924052 14636 solver.cpp:237]     Train net output #1: loss = 0.202803 (* 1 = 0.202803 loss)
I1123 16:59:36.924052 14636 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1123 16:59:41.731150 14636 solver.cpp:218] Iteration 26600 (20.8027 iter/s, 4.80708s/100 iters), loss = 0.230727
I1123 16:59:41.731150 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:59:41.731150 14636 solver.cpp:237]     Train net output #1: loss = 0.230727 (* 1 = 0.230727 loss)
I1123 16:59:41.731150 14636 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1123 16:59:46.539618 14636 solver.cpp:218] Iteration 26700 (20.7997 iter/s, 4.80777s/100 iters), loss = 0.192181
I1123 16:59:46.539618 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 16:59:46.539618 14636 solver.cpp:237]     Train net output #1: loss = 0.192181 (* 1 = 0.192181 loss)
I1123 16:59:46.539618 14636 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1123 16:59:51.348300 14636 solver.cpp:218] Iteration 26800 (20.7991 iter/s, 4.8079s/100 iters), loss = 0.220193
I1123 16:59:51.348300 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:59:51.348300 14636 solver.cpp:237]     Train net output #1: loss = 0.220193 (* 1 = 0.220193 loss)
I1123 16:59:51.348300 14636 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1123 16:59:56.160393 14636 solver.cpp:218] Iteration 26900 (20.7814 iter/s, 4.812s/100 iters), loss = 0.136463
I1123 16:59:56.160393 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:59:56.160393 14636 solver.cpp:237]     Train net output #1: loss = 0.136463 (* 1 = 0.136463 loss)
I1123 16:59:56.160393 14636 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1123 17:00:00.745378 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:00:00.939406 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_27000.caffemodel
I1123 17:00:00.993422 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_27000.solverstate
I1123 17:00:00.996920 14636 solver.cpp:330] Iteration 27000, Testing net (#0)
I1123 17:00:00.996920 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 17:00:02.273929 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:00:02.323935 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8846
I1123 17:00:02.323935 14636 solver.cpp:397]     Test net output #1: loss = 0.34009 (* 1 = 0.34009 loss)
I1123 17:00:02.370934 14636 solver.cpp:218] Iteration 27000 (16.1033 iter/s, 6.2099s/100 iters), loss = 0.171927
I1123 17:00:02.370934 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 17:00:02.370934 14636 solver.cpp:237]     Train net output #1: loss = 0.171927 (* 1 = 0.171927 loss)
I1123 17:00:02.370934 14636 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1123 17:00:02.370934 14636 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1123 17:00:07.184856 14636 solver.cpp:218] Iteration 27100 (20.7719 iter/s, 4.8142s/100 iters), loss = 0.206039
I1123 17:00:07.184856 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 17:00:07.184856 14636 solver.cpp:237]     Train net output #1: loss = 0.206039 (* 1 = 0.206039 loss)
I1123 17:00:07.184856 14636 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1123 17:00:11.986901 14636 solver.cpp:218] Iteration 27200 (20.8267 iter/s, 4.80154s/100 iters), loss = 0.234847
I1123 17:00:11.986901 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 17:00:11.986901 14636 solver.cpp:237]     Train net output #1: loss = 0.234847 (* 1 = 0.234847 loss)
I1123 17:00:11.986901 14636 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1123 17:00:16.788398 14636 solver.cpp:218] Iteration 27300 (20.8298 iter/s, 4.80082s/100 iters), loss = 0.236713
I1123 17:00:16.788398 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 17:00:16.788398 14636 solver.cpp:237]     Train net output #1: loss = 0.236714 (* 1 = 0.236714 loss)
I1123 17:00:16.788398 14636 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1123 17:00:21.600594 14636 solver.cpp:218] Iteration 27400 (20.7848 iter/s, 4.8112s/100 iters), loss = 0.120533
I1123 17:00:21.600594 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 17:00:21.600594 14636 solver.cpp:237]     Train net output #1: loss = 0.120534 (* 1 = 0.120534 loss)
I1123 17:00:21.600594 14636 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1123 17:00:26.173410 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:00:26.361429 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_27500.caffemodel
I1123 17:00:26.372421 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_27500.solverstate
I1123 17:00:26.375422 14636 solver.cpp:330] Iteration 27500, Testing net (#0)
I1123 17:00:26.375422 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 17:00:27.643591 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:00:27.694094 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8844
I1123 17:00:27.694094 14636 solver.cpp:397]     Test net output #1: loss = 0.340161 (* 1 = 0.340161 loss)
I1123 17:00:27.740624 14636 solver.cpp:218] Iteration 27500 (16.2875 iter/s, 6.13969s/100 iters), loss = 0.185451
I1123 17:00:27.740624 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 17:00:27.740624 14636 solver.cpp:237]     Train net output #1: loss = 0.185452 (* 1 = 0.185452 loss)
I1123 17:00:27.740624 14636 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1123 17:00:32.554193 14636 solver.cpp:218] Iteration 27600 (20.7724 iter/s, 4.81408s/100 iters), loss = 0.232463
I1123 17:00:32.555193 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 17:00:32.555193 14636 solver.cpp:237]     Train net output #1: loss = 0.232463 (* 1 = 0.232463 loss)
I1123 17:00:32.555193 14636 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1123 17:00:37.362089 14636 solver.cpp:218] Iteration 27700 (20.8026 iter/s, 4.8071s/100 iters), loss = 0.226942
I1123 17:00:37.362089 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 17:00:37.362089 14636 solver.cpp:237]     Train net output #1: loss = 0.226942 (* 1 = 0.226942 loss)
I1123 17:00:37.362089 14636 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1123 17:00:42.164029 14636 solver.cpp:218] Iteration 27800 (20.8263 iter/s, 4.80162s/100 iters), loss = 0.230564
I1123 17:00:42.164029 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 17:00:42.164029 14636 solver.cpp:237]     Train net output #1: loss = 0.230565 (* 1 = 0.230565 loss)
I1123 17:00:42.164029 14636 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1123 17:00:46.968323 14636 solver.cpp:218] Iteration 27900 (20.8179 iter/s, 4.80355s/100 iters), loss = 0.167372
I1123 17:00:46.968323 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 17:00:46.968323 14636 solver.cpp:237]     Train net output #1: loss = 0.167373 (* 1 = 0.167373 loss)
I1123 17:00:46.968323 14636 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1123 17:00:51.537441 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:00:51.726461 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_28000.caffemodel
I1123 17:00:51.760462 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_28000.solverstate
I1123 17:00:51.764461 14636 solver.cpp:330] Iteration 28000, Testing net (#0)
I1123 17:00:51.764461 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 17:00:53.030609 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:00:53.080610 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8846
I1123 17:00:53.080610 14636 solver.cpp:397]     Test net output #1: loss = 0.340207 (* 1 = 0.340207 loss)
I1123 17:00:53.127629 14636 solver.cpp:218] Iteration 28000 (16.2374 iter/s, 6.15863s/100 iters), loss = 0.190961
I1123 17:00:53.127629 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 17:00:53.127629 14636 solver.cpp:237]     Train net output #1: loss = 0.190961 (* 1 = 0.190961 loss)
I1123 17:00:53.127629 14636 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1123 17:00:57.934610 14636 solver.cpp:218] Iteration 28100 (20.8051 iter/s, 4.80651s/100 iters), loss = 0.232208
I1123 17:00:57.934610 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 17:00:57.934610 14636 solver.cpp:237]     Train net output #1: loss = 0.232208 (* 1 = 0.232208 loss)
I1123 17:00:57.934610 14636 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1123 17:01:02.737079 14636 solver.cpp:218] Iteration 28200 (20.8238 iter/s, 4.80221s/100 iters), loss = 0.199473
I1123 17:01:02.737079 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 17:01:02.737079 14636 solver.cpp:237]     Train net output #1: loss = 0.199474 (* 1 = 0.199474 loss)
I1123 17:01:02.737079 14636 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1123 17:01:07.534925 14636 solver.cpp:218] Iteration 28300 (20.8446 iter/s, 4.79741s/100 iters), loss = 0.213476
I1123 17:01:07.534925 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 17:01:07.534925 14636 solver.cpp:237]     Train net output #1: loss = 0.213476 (* 1 = 0.213476 loss)
I1123 17:01:07.534925 14636 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1123 17:01:12.339767 14636 solver.cpp:218] Iteration 28400 (20.8114 iter/s, 4.80507s/100 iters), loss = 0.183087
I1123 17:01:12.339767 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 17:01:12.340770 14636 solver.cpp:237]     Train net output #1: loss = 0.183087 (* 1 = 0.183087 loss)
I1123 17:01:12.340770 14636 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1123 17:01:16.914932 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:01:17.103950 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_28500.caffemodel
I1123 17:01:17.114450 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_28500.solverstate
I1123 17:01:17.117456 14636 solver.cpp:330] Iteration 28500, Testing net (#0)
I1123 17:01:17.117456 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 17:01:18.381108 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:01:18.431387 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8845
I1123 17:01:18.431387 14636 solver.cpp:397]     Test net output #1: loss = 0.340145 (* 1 = 0.340145 loss)
I1123 17:01:18.477386 14636 solver.cpp:218] Iteration 28500 (16.2955 iter/s, 6.13668s/100 iters), loss = 0.162508
I1123 17:01:18.477386 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 17:01:18.477386 14636 solver.cpp:237]     Train net output #1: loss = 0.162508 (* 1 = 0.162508 loss)
I1123 17:01:18.477386 14636 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1123 17:01:23.280890 14636 solver.cpp:218] Iteration 28600 (20.8219 iter/s, 4.80264s/100 iters), loss = 0.223529
I1123 17:01:23.280890 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 17:01:23.280890 14636 solver.cpp:237]     Train net output #1: loss = 0.223529 (* 1 = 0.223529 loss)
I1123 17:01:23.280890 14636 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1123 17:01:28.079300 14636 solver.cpp:218] Iteration 28700 (20.8413 iter/s, 4.79816s/100 iters), loss = 0.234933
I1123 17:01:28.079300 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 17:01:28.079300 14636 solver.cpp:237]     Train net output #1: loss = 0.234934 (* 1 = 0.234934 loss)
I1123 17:01:28.079300 14636 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1123 17:01:32.880970 14636 solver.cpp:218] Iteration 28800 (20.8275 iter/s, 4.80135s/100 iters), loss = 0.189899
I1123 17:01:32.880970 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 17:01:32.880970 14636 solver.cpp:237]     Train net output #1: loss = 0.189899 (* 1 = 0.189899 loss)
I1123 17:01:32.880970 14636 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1123 17:01:37.682355 14636 solver.cpp:218] Iteration 28900 (20.8272 iter/s, 4.80141s/100 iters), loss = 0.184131
I1123 17:01:37.682355 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 17:01:37.683357 14636 solver.cpp:237]     Train net output #1: loss = 0.184132 (* 1 = 0.184132 loss)
I1123 17:01:37.683357 14636 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1123 17:01:42.250277 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:01:42.439229 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_29000.caffemodel
I1123 17:01:42.520239 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_29000.solverstate
I1123 17:01:42.524245 14636 solver.cpp:330] Iteration 29000, Testing net (#0)
I1123 17:01:42.524245 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 17:01:43.788277 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:01:43.838796 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8845
I1123 17:01:43.838796 14636 solver.cpp:397]     Test net output #1: loss = 0.340174 (* 1 = 0.340174 loss)
I1123 17:01:43.884784 14636 solver.cpp:218] Iteration 29000 (16.1241 iter/s, 6.20191s/100 iters), loss = 0.174556
I1123 17:01:43.884784 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 17:01:43.884784 14636 solver.cpp:237]     Train net output #1: loss = 0.174557 (* 1 = 0.174557 loss)
I1123 17:01:43.884784 14636 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1123 17:01:48.691541 14636 solver.cpp:218] Iteration 29100 (20.8073 iter/s, 4.80601s/100 iters), loss = 0.20294
I1123 17:01:48.691541 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 17:01:48.691541 14636 solver.cpp:237]     Train net output #1: loss = 0.202941 (* 1 = 0.202941 loss)
I1123 17:01:48.691541 14636 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1123 17:01:53.502641 14636 solver.cpp:218] Iteration 29200 (20.7882 iter/s, 4.81043s/100 iters), loss = 0.218749
I1123 17:01:53.502641 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 17:01:53.502641 14636 solver.cpp:237]     Train net output #1: loss = 0.218749 (* 1 = 0.218749 loss)
I1123 17:01:53.502641 14636 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1123 17:01:58.309967 14636 solver.cpp:218] Iteration 29300 (20.8055 iter/s, 4.80643s/100 iters), loss = 0.201154
I1123 17:01:58.309967 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 17:01:58.309967 14636 solver.cpp:237]     Train net output #1: loss = 0.201154 (* 1 = 0.201154 loss)
I1123 17:01:58.309967 14636 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1123 17:02:03.114696 14636 solver.cpp:218] Iteration 29400 (20.8135 iter/s, 4.80456s/100 iters), loss = 0.151158
I1123 17:02:03.114696 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 17:02:03.114696 14636 solver.cpp:237]     Train net output #1: loss = 0.151158 (* 1 = 0.151158 loss)
I1123 17:02:03.114696 14636 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1123 17:02:07.694463 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:02:07.884486 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_29500.caffemodel
I1123 17:02:07.893486 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_29500.solverstate
I1123 17:02:07.897487 14636 solver.cpp:330] Iteration 29500, Testing net (#0)
I1123 17:02:07.897487 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 17:02:09.163555 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:02:09.214061 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8843
I1123 17:02:09.214061 14636 solver.cpp:397]     Test net output #1: loss = 0.340164 (* 1 = 0.340164 loss)
I1123 17:02:09.260565 14636 solver.cpp:218] Iteration 29500 (16.2729 iter/s, 6.14518s/100 iters), loss = 0.166823
I1123 17:02:09.260565 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 17:02:09.260565 14636 solver.cpp:237]     Train net output #1: loss = 0.166823 (* 1 = 0.166823 loss)
I1123 17:02:09.260565 14636 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1123 17:02:14.064589 14636 solver.cpp:218] Iteration 29600 (20.8142 iter/s, 4.80441s/100 iters), loss = 0.169139
I1123 17:02:14.064589 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 17:02:14.064589 14636 solver.cpp:237]     Train net output #1: loss = 0.169139 (* 1 = 0.169139 loss)
I1123 17:02:14.064589 14636 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1123 17:02:18.871330 14636 solver.cpp:218] Iteration 29700 (20.8068 iter/s, 4.80611s/100 iters), loss = 0.26216
I1123 17:02:18.871330 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 17:02:18.871330 14636 solver.cpp:237]     Train net output #1: loss = 0.262161 (* 1 = 0.262161 loss)
I1123 17:02:18.871330 14636 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1123 17:02:23.681900 14636 solver.cpp:218] Iteration 29800 (20.7921 iter/s, 4.80951s/100 iters), loss = 0.177308
I1123 17:02:23.681900 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 17:02:23.681900 14636 solver.cpp:237]     Train net output #1: loss = 0.177309 (* 1 = 0.177309 loss)
I1123 17:02:23.681900 14636 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1123 17:02:28.486613 14636 solver.cpp:218] Iteration 29900 (20.813 iter/s, 4.80469s/100 iters), loss = 0.175715
I1123 17:02:28.486613 14636 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 17:02:28.486613 14636 solver.cpp:237]     Train net output #1: loss = 0.175715 (* 1 = 0.175715 loss)
I1123 17:02:28.486613 14636 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1123 17:02:33.067160 22208 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:02:33.258767 14636 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_30000.caffemodel
I1123 17:02:33.276777 14636 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_F2L_iter_30000.solverstate
I1123 17:02:33.294277 14636 solver.cpp:310] Iteration 30000, loss = 0.1898
I1123 17:02:33.294277 14636 solver.cpp:330] Iteration 30000, Testing net (#0)
I1123 17:02:33.295279 14636 net.cpp:676] Ignoring source layer accuracy_training
I1123 17:02:34.560494 24900 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:02:34.611521 14636 solver.cpp:397]     Test net output #0: accuracy = 0.8845
I1123 17:02:34.611521 14636 solver.cpp:397]     Test net output #1: loss = 0.340068 (* 1 = 0.340068 loss)
I1123 17:02:34.611521 14636 solver.cpp:315] Optimization Done.
I1123 17:02:34.611521 14636 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 
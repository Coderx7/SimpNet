
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1123 14:46:22.147929 25976 caffe.cpp:219] Using GPUs 0
I1123 14:46:22.305948 25976 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1123 14:46:22.607758 25976 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 14:46:22.623764 25976 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_300k_8L_7x7"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1123 14:46:22.623764 25976 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 14:46:22.625747 25976 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 14:46:22.625747 25976 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 14:46:22.625747 25976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1123 14:46:22.625747 25976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1123 14:46:22.625747 25976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1123 14:46:22.625747 25976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1123 14:46:22.625747 25976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1123 14:46:22.625747 25976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1123 14:46:22.625747 25976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1123 14:46:22.625747 25976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1123 14:46:22.625747 25976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1123 14:46:22.625747 25976 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1123 14:46:22.625747 25976 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_7x7_300K"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 35
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 38
    pad: 3
    kernel_size: 7
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 14:46:22.658751 25976 layer_factory.cpp:58] Creating layer cifar
I1123 14:46:22.741482 25976 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1123 14:46:22.742476 25976 net.cpp:84] Creating Layer cifar
I1123 14:46:22.742476 25976 net.cpp:380] cifar -> data
I1123 14:46:22.742476 25976 net.cpp:380] cifar -> label
I1123 14:46:22.743469 25976 data_layer.cpp:45] output data size: 100,3,32,32
I1123 14:46:22.749469 25976 net.cpp:122] Setting up cifar
I1123 14:46:22.749469 25976 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 14:46:22.749469 25976 net.cpp:129] Top shape: 100 (100)
I1123 14:46:22.749469 25976 net.cpp:137] Memory required for data: 1229200
I1123 14:46:22.749469 25976 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 14:46:22.749469 25976 net.cpp:84] Creating Layer label_cifar_1_split
I1123 14:46:22.749469 25976 net.cpp:406] label_cifar_1_split <- label
I1123 14:46:22.749469 25976 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 14:46:22.749469 25976 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 14:46:22.749469 25976 net.cpp:122] Setting up label_cifar_1_split
I1123 14:46:22.749469 25976 net.cpp:129] Top shape: 100 (100)
I1123 14:46:22.749469 25976 net.cpp:129] Top shape: 100 (100)
I1123 14:46:22.749469 25976 net.cpp:137] Memory required for data: 1230000
I1123 14:46:22.749469 25976 layer_factory.cpp:58] Creating layer conv1
I1123 14:46:22.749469 25976 net.cpp:84] Creating Layer conv1
I1123 14:46:22.749469 25976 net.cpp:406] conv1 <- data
I1123 14:46:22.749469 25976 net.cpp:380] conv1 -> conv1
I1123 14:46:22.750468 24360 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 14:46:23.221674 25976 net.cpp:122] Setting up conv1
I1123 14:46:23.221674 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.221674 25976 net.cpp:137] Memory required for data: 9422000
I1123 14:46:23.221674 25976 layer_factory.cpp:58] Creating layer bn1
I1123 14:46:23.221674 25976 net.cpp:84] Creating Layer bn1
I1123 14:46:23.221674 25976 net.cpp:406] bn1 <- conv1
I1123 14:46:23.221674 25976 net.cpp:367] bn1 -> conv1 (in-place)
I1123 14:46:23.221674 25976 net.cpp:122] Setting up bn1
I1123 14:46:23.221674 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.221674 25976 net.cpp:137] Memory required for data: 17614000
I1123 14:46:23.221674 25976 layer_factory.cpp:58] Creating layer scale1
I1123 14:46:23.221674 25976 net.cpp:84] Creating Layer scale1
I1123 14:46:23.221674 25976 net.cpp:406] scale1 <- conv1
I1123 14:46:23.221674 25976 net.cpp:367] scale1 -> conv1 (in-place)
I1123 14:46:23.221674 25976 layer_factory.cpp:58] Creating layer scale1
I1123 14:46:23.221674 25976 net.cpp:122] Setting up scale1
I1123 14:46:23.221674 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.221674 25976 net.cpp:137] Memory required for data: 25806000
I1123 14:46:23.221674 25976 layer_factory.cpp:58] Creating layer relu1
I1123 14:46:23.221674 25976 net.cpp:84] Creating Layer relu1
I1123 14:46:23.221674 25976 net.cpp:406] relu1 <- conv1
I1123 14:46:23.221674 25976 net.cpp:367] relu1 -> conv1 (in-place)
I1123 14:46:23.221674 25976 net.cpp:122] Setting up relu1
I1123 14:46:23.221674 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.221674 25976 net.cpp:137] Memory required for data: 33998000
I1123 14:46:23.221674 25976 layer_factory.cpp:58] Creating layer conv2
I1123 14:46:23.221674 25976 net.cpp:84] Creating Layer conv2
I1123 14:46:23.221674 25976 net.cpp:406] conv2 <- conv1
I1123 14:46:23.221674 25976 net.cpp:380] conv2 -> conv2
I1123 14:46:23.284476 25976 net.cpp:122] Setting up conv2
I1123 14:46:23.284976 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.284976 25976 net.cpp:137] Memory required for data: 42190000
I1123 14:46:23.284976 25976 layer_factory.cpp:58] Creating layer bn2
I1123 14:46:23.284976 25976 net.cpp:84] Creating Layer bn2
I1123 14:46:23.284976 25976 net.cpp:406] bn2 <- conv2
I1123 14:46:23.284976 25976 net.cpp:367] bn2 -> conv2 (in-place)
I1123 14:46:23.284976 25976 net.cpp:122] Setting up bn2
I1123 14:46:23.284976 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.284976 25976 net.cpp:137] Memory required for data: 50382000
I1123 14:46:23.284976 25976 layer_factory.cpp:58] Creating layer scale2
I1123 14:46:23.284976 25976 net.cpp:84] Creating Layer scale2
I1123 14:46:23.284976 25976 net.cpp:406] scale2 <- conv2
I1123 14:46:23.284976 25976 net.cpp:367] scale2 -> conv2 (in-place)
I1123 14:46:23.284976 25976 layer_factory.cpp:58] Creating layer scale2
I1123 14:46:23.284976 25976 net.cpp:122] Setting up scale2
I1123 14:46:23.284976 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.284976 25976 net.cpp:137] Memory required for data: 58574000
I1123 14:46:23.284976 25976 layer_factory.cpp:58] Creating layer relu2
I1123 14:46:23.284976 25976 net.cpp:84] Creating Layer relu2
I1123 14:46:23.284976 25976 net.cpp:406] relu2 <- conv2
I1123 14:46:23.284976 25976 net.cpp:367] relu2 -> conv2 (in-place)
I1123 14:46:23.285475 25976 net.cpp:122] Setting up relu2
I1123 14:46:23.285475 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.285475 25976 net.cpp:137] Memory required for data: 66766000
I1123 14:46:23.285475 25976 layer_factory.cpp:58] Creating layer conv2_2
I1123 14:46:23.285475 25976 net.cpp:84] Creating Layer conv2_2
I1123 14:46:23.285475 25976 net.cpp:406] conv2_2 <- conv2
I1123 14:46:23.285475 25976 net.cpp:380] conv2_2 -> conv2_2
I1123 14:46:23.286975 25976 net.cpp:122] Setting up conv2_2
I1123 14:46:23.286975 25976 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 14:46:23.286975 25976 net.cpp:137] Memory required for data: 79054000
I1123 14:46:23.286975 25976 layer_factory.cpp:58] Creating layer bn2_2
I1123 14:46:23.286975 25976 net.cpp:84] Creating Layer bn2_2
I1123 14:46:23.286975 25976 net.cpp:406] bn2_2 <- conv2_2
I1123 14:46:23.286975 25976 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 14:46:23.286975 25976 net.cpp:122] Setting up bn2_2
I1123 14:46:23.286975 25976 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 14:46:23.286975 25976 net.cpp:137] Memory required for data: 91342000
I1123 14:46:23.286975 25976 layer_factory.cpp:58] Creating layer scale2_2
I1123 14:46:23.286975 25976 net.cpp:84] Creating Layer scale2_2
I1123 14:46:23.286975 25976 net.cpp:406] scale2_2 <- conv2_2
I1123 14:46:23.286975 25976 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 14:46:23.287475 25976 layer_factory.cpp:58] Creating layer scale2_2
I1123 14:46:23.287475 25976 net.cpp:122] Setting up scale2_2
I1123 14:46:23.287475 25976 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 14:46:23.287475 25976 net.cpp:137] Memory required for data: 103630000
I1123 14:46:23.287475 25976 layer_factory.cpp:58] Creating layer relu2_2
I1123 14:46:23.287475 25976 net.cpp:84] Creating Layer relu2_2
I1123 14:46:23.287475 25976 net.cpp:406] relu2_2 <- conv2_2
I1123 14:46:23.287475 25976 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 14:46:23.287475 25976 net.cpp:122] Setting up relu2_2
I1123 14:46:23.287475 25976 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 14:46:23.287475 25976 net.cpp:137] Memory required for data: 115918000
I1123 14:46:23.287475 25976 layer_factory.cpp:58] Creating layer pool2_1
I1123 14:46:23.287475 25976 net.cpp:84] Creating Layer pool2_1
I1123 14:46:23.287475 25976 net.cpp:406] pool2_1 <- conv2_2
I1123 14:46:23.287475 25976 net.cpp:380] pool2_1 -> pool2_1
I1123 14:46:23.287475 25976 net.cpp:122] Setting up pool2_1
I1123 14:46:23.287475 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.287475 25976 net.cpp:137] Memory required for data: 118990000
I1123 14:46:23.287475 25976 layer_factory.cpp:58] Creating layer conv3
I1123 14:46:23.287475 25976 net.cpp:84] Creating Layer conv3
I1123 14:46:23.287475 25976 net.cpp:406] conv3 <- pool2_1
I1123 14:46:23.287475 25976 net.cpp:380] conv3 -> conv3
I1123 14:46:23.288477 25976 net.cpp:122] Setting up conv3
I1123 14:46:23.288477 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.288477 25976 net.cpp:137] Memory required for data: 122062000
I1123 14:46:23.288477 25976 layer_factory.cpp:58] Creating layer bn3
I1123 14:46:23.288477 25976 net.cpp:84] Creating Layer bn3
I1123 14:46:23.288477 25976 net.cpp:406] bn3 <- conv3
I1123 14:46:23.288477 25976 net.cpp:367] bn3 -> conv3 (in-place)
I1123 14:46:23.288477 25976 net.cpp:122] Setting up bn3
I1123 14:46:23.288477 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.288477 25976 net.cpp:137] Memory required for data: 125134000
I1123 14:46:23.288477 25976 layer_factory.cpp:58] Creating layer scale3
I1123 14:46:23.288477 25976 net.cpp:84] Creating Layer scale3
I1123 14:46:23.288477 25976 net.cpp:406] scale3 <- conv3
I1123 14:46:23.288477 25976 net.cpp:367] scale3 -> conv3 (in-place)
I1123 14:46:23.289549 25976 layer_factory.cpp:58] Creating layer scale3
I1123 14:46:23.289549 25976 net.cpp:122] Setting up scale3
I1123 14:46:23.289549 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.289549 25976 net.cpp:137] Memory required for data: 128206000
I1123 14:46:23.289549 25976 layer_factory.cpp:58] Creating layer relu3
I1123 14:46:23.289549 25976 net.cpp:84] Creating Layer relu3
I1123 14:46:23.289549 25976 net.cpp:406] relu3 <- conv3
I1123 14:46:23.289549 25976 net.cpp:367] relu3 -> conv3 (in-place)
I1123 14:46:23.289549 25976 net.cpp:122] Setting up relu3
I1123 14:46:23.289549 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.289549 25976 net.cpp:137] Memory required for data: 131278000
I1123 14:46:23.289549 25976 layer_factory.cpp:58] Creating layer conv4
I1123 14:46:23.289549 25976 net.cpp:84] Creating Layer conv4
I1123 14:46:23.289549 25976 net.cpp:406] conv4 <- conv3
I1123 14:46:23.289549 25976 net.cpp:380] conv4 -> conv4
I1123 14:46:23.290549 25976 net.cpp:122] Setting up conv4
I1123 14:46:23.290549 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.290549 25976 net.cpp:137] Memory required for data: 134350000
I1123 14:46:23.290549 25976 layer_factory.cpp:58] Creating layer bn4
I1123 14:46:23.290549 25976 net.cpp:84] Creating Layer bn4
I1123 14:46:23.290549 25976 net.cpp:406] bn4 <- conv4
I1123 14:46:23.290549 25976 net.cpp:367] bn4 -> conv4 (in-place)
I1123 14:46:23.291550 25976 net.cpp:122] Setting up bn4
I1123 14:46:23.291550 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.291550 25976 net.cpp:137] Memory required for data: 137422000
I1123 14:46:23.291550 25976 layer_factory.cpp:58] Creating layer scale4
I1123 14:46:23.291550 25976 net.cpp:84] Creating Layer scale4
I1123 14:46:23.291550 25976 net.cpp:406] scale4 <- conv4
I1123 14:46:23.291550 25976 net.cpp:367] scale4 -> conv4 (in-place)
I1123 14:46:23.291550 25976 layer_factory.cpp:58] Creating layer scale4
I1123 14:46:23.291550 25976 net.cpp:122] Setting up scale4
I1123 14:46:23.291550 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.291550 25976 net.cpp:137] Memory required for data: 140494000
I1123 14:46:23.291550 25976 layer_factory.cpp:58] Creating layer relu4
I1123 14:46:23.291550 25976 net.cpp:84] Creating Layer relu4
I1123 14:46:23.291550 25976 net.cpp:406] relu4 <- conv4
I1123 14:46:23.291550 25976 net.cpp:367] relu4 -> conv4 (in-place)
I1123 14:46:23.291550 25976 net.cpp:122] Setting up relu4
I1123 14:46:23.291550 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.291550 25976 net.cpp:137] Memory required for data: 143566000
I1123 14:46:23.291550 25976 layer_factory.cpp:58] Creating layer conv4_1
I1123 14:46:23.291550 25976 net.cpp:84] Creating Layer conv4_1
I1123 14:46:23.291550 25976 net.cpp:406] conv4_1 <- conv4
I1123 14:46:23.291550 25976 net.cpp:380] conv4_1 -> conv4_1
I1123 14:46:23.292549 25976 net.cpp:122] Setting up conv4_1
I1123 14:46:23.292549 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.292549 25976 net.cpp:137] Memory required for data: 146638000
I1123 14:46:23.292549 25976 layer_factory.cpp:58] Creating layer bn4_1
I1123 14:46:23.292549 25976 net.cpp:84] Creating Layer bn4_1
I1123 14:46:23.292549 25976 net.cpp:406] bn4_1 <- conv4_1
I1123 14:46:23.292549 25976 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 14:46:23.293550 25976 net.cpp:122] Setting up bn4_1
I1123 14:46:23.293550 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.293550 25976 net.cpp:137] Memory required for data: 149710000
I1123 14:46:23.293550 25976 layer_factory.cpp:58] Creating layer scale4_1
I1123 14:46:23.293550 25976 net.cpp:84] Creating Layer scale4_1
I1123 14:46:23.293550 25976 net.cpp:406] scale4_1 <- conv4_1
I1123 14:46:23.293550 25976 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 14:46:23.293550 25976 layer_factory.cpp:58] Creating layer scale4_1
I1123 14:46:23.293550 25976 net.cpp:122] Setting up scale4_1
I1123 14:46:23.293550 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.293550 25976 net.cpp:137] Memory required for data: 152782000
I1123 14:46:23.293550 25976 layer_factory.cpp:58] Creating layer relu4_1
I1123 14:46:23.293550 25976 net.cpp:84] Creating Layer relu4_1
I1123 14:46:23.293550 25976 net.cpp:406] relu4_1 <- conv4_1
I1123 14:46:23.293550 25976 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 14:46:23.293550 25976 net.cpp:122] Setting up relu4_1
I1123 14:46:23.293550 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.293550 25976 net.cpp:137] Memory required for data: 155854000
I1123 14:46:23.293550 25976 layer_factory.cpp:58] Creating layer conv4_2
I1123 14:46:23.293550 25976 net.cpp:84] Creating Layer conv4_2
I1123 14:46:23.293550 25976 net.cpp:406] conv4_2 <- conv4_1
I1123 14:46:23.293550 25976 net.cpp:380] conv4_2 -> conv4_2
I1123 14:46:23.295549 25976 net.cpp:122] Setting up conv4_2
I1123 14:46:23.295549 25976 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 14:46:23.295549 25976 net.cpp:137] Memory required for data: 159438000
I1123 14:46:23.295549 25976 layer_factory.cpp:58] Creating layer bn4_2
I1123 14:46:23.295549 25976 net.cpp:84] Creating Layer bn4_2
I1123 14:46:23.295549 25976 net.cpp:406] bn4_2 <- conv4_2
I1123 14:46:23.295549 25976 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 14:46:23.295549 25976 net.cpp:122] Setting up bn4_2
I1123 14:46:23.295549 25976 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 14:46:23.295549 25976 net.cpp:137] Memory required for data: 163022000
I1123 14:46:23.295549 25976 layer_factory.cpp:58] Creating layer scale4_2
I1123 14:46:23.295549 25976 net.cpp:84] Creating Layer scale4_2
I1123 14:46:23.295549 25976 net.cpp:406] scale4_2 <- conv4_2
I1123 14:46:23.295549 25976 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 14:46:23.295549 25976 layer_factory.cpp:58] Creating layer scale4_2
I1123 14:46:23.295549 25976 net.cpp:122] Setting up scale4_2
I1123 14:46:23.295549 25976 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 14:46:23.295549 25976 net.cpp:137] Memory required for data: 166606000
I1123 14:46:23.295549 25976 layer_factory.cpp:58] Creating layer relu4_2
I1123 14:46:23.295549 25976 net.cpp:84] Creating Layer relu4_2
I1123 14:46:23.295549 25976 net.cpp:406] relu4_2 <- conv4_2
I1123 14:46:23.295549 25976 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 14:46:23.295549 25976 net.cpp:122] Setting up relu4_2
I1123 14:46:23.295549 25976 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 14:46:23.295549 25976 net.cpp:137] Memory required for data: 170190000
I1123 14:46:23.295549 25976 layer_factory.cpp:58] Creating layer pool4_2
I1123 14:46:23.295549 25976 net.cpp:84] Creating Layer pool4_2
I1123 14:46:23.295549 25976 net.cpp:406] pool4_2 <- conv4_2
I1123 14:46:23.295549 25976 net.cpp:380] pool4_2 -> pool4_2
I1123 14:46:23.295549 25976 net.cpp:122] Setting up pool4_2
I1123 14:46:23.295549 25976 net.cpp:129] Top shape: 100 35 8 8 (224000)
I1123 14:46:23.295549 25976 net.cpp:137] Memory required for data: 171086000
I1123 14:46:23.295549 25976 layer_factory.cpp:58] Creating layer conv12
I1123 14:46:23.295549 25976 net.cpp:84] Creating Layer conv12
I1123 14:46:23.295549 25976 net.cpp:406] conv12 <- pool4_2
I1123 14:46:23.295549 25976 net.cpp:380] conv12 -> conv12
I1123 14:46:23.297583 25976 net.cpp:122] Setting up conv12
I1123 14:46:23.297583 25976 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 14:46:23.297583 25976 net.cpp:137] Memory required for data: 172058800
I1123 14:46:23.297583 25976 layer_factory.cpp:58] Creating layer bn_conv12
I1123 14:46:23.297583 25976 net.cpp:84] Creating Layer bn_conv12
I1123 14:46:23.297583 25976 net.cpp:406] bn_conv12 <- conv12
I1123 14:46:23.297583 25976 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 14:46:23.297583 25976 net.cpp:122] Setting up bn_conv12
I1123 14:46:23.297583 25976 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 14:46:23.297583 25976 net.cpp:137] Memory required for data: 173031600
I1123 14:46:23.297583 25976 layer_factory.cpp:58] Creating layer scale_conv12
I1123 14:46:23.297583 25976 net.cpp:84] Creating Layer scale_conv12
I1123 14:46:23.298549 25976 net.cpp:406] scale_conv12 <- conv12
I1123 14:46:23.298549 25976 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 14:46:23.298549 25976 layer_factory.cpp:58] Creating layer scale_conv12
I1123 14:46:23.298549 25976 net.cpp:122] Setting up scale_conv12
I1123 14:46:23.298549 25976 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 14:46:23.298549 25976 net.cpp:137] Memory required for data: 174004400
I1123 14:46:23.298549 25976 layer_factory.cpp:58] Creating layer relu_conv12
I1123 14:46:23.298549 25976 net.cpp:84] Creating Layer relu_conv12
I1123 14:46:23.298549 25976 net.cpp:406] relu_conv12 <- conv12
I1123 14:46:23.298549 25976 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 14:46:23.298549 25976 net.cpp:122] Setting up relu_conv12
I1123 14:46:23.298549 25976 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 14:46:23.298549 25976 net.cpp:137] Memory required for data: 174977200
I1123 14:46:23.298549 25976 layer_factory.cpp:58] Creating layer poolcp6
I1123 14:46:23.298549 25976 net.cpp:84] Creating Layer poolcp6
I1123 14:46:23.298549 25976 net.cpp:406] poolcp6 <- conv12
I1123 14:46:23.298549 25976 net.cpp:380] poolcp6 -> poolcp6
I1123 14:46:23.298549 25976 net.cpp:122] Setting up poolcp6
I1123 14:46:23.298549 25976 net.cpp:129] Top shape: 100 38 1 1 (3800)
I1123 14:46:23.298549 25976 net.cpp:137] Memory required for data: 174992400
I1123 14:46:23.298549 25976 layer_factory.cpp:58] Creating layer ip1
I1123 14:46:23.298549 25976 net.cpp:84] Creating Layer ip1
I1123 14:46:23.298549 25976 net.cpp:406] ip1 <- poolcp6
I1123 14:46:23.298549 25976 net.cpp:380] ip1 -> ip1
I1123 14:46:23.298549 25976 net.cpp:122] Setting up ip1
I1123 14:46:23.298549 25976 net.cpp:129] Top shape: 100 10 (1000)
I1123 14:46:23.298549 25976 net.cpp:137] Memory required for data: 174996400
I1123 14:46:23.298549 25976 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 14:46:23.298549 25976 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 14:46:23.298549 25976 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 14:46:23.298549 25976 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 14:46:23.298549 25976 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 14:46:23.298549 25976 net.cpp:122] Setting up ip1_ip1_0_split
I1123 14:46:23.298549 25976 net.cpp:129] Top shape: 100 10 (1000)
I1123 14:46:23.298549 25976 net.cpp:129] Top shape: 100 10 (1000)
I1123 14:46:23.298549 25976 net.cpp:137] Memory required for data: 175004400
I1123 14:46:23.298549 25976 layer_factory.cpp:58] Creating layer accuracy_training
I1123 14:46:23.298549 25976 net.cpp:84] Creating Layer accuracy_training
I1123 14:46:23.298549 25976 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1123 14:46:23.298549 25976 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1123 14:46:23.298549 25976 net.cpp:380] accuracy_training -> accuracy_training
I1123 14:46:23.298549 25976 net.cpp:122] Setting up accuracy_training
I1123 14:46:23.298549 25976 net.cpp:129] Top shape: (1)
I1123 14:46:23.298549 25976 net.cpp:137] Memory required for data: 175004404
I1123 14:46:23.298549 25976 layer_factory.cpp:58] Creating layer loss
I1123 14:46:23.298549 25976 net.cpp:84] Creating Layer loss
I1123 14:46:23.298549 25976 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 14:46:23.298549 25976 net.cpp:406] loss <- label_cifar_1_split_1
I1123 14:46:23.298549 25976 net.cpp:380] loss -> loss
I1123 14:46:23.298549 25976 layer_factory.cpp:58] Creating layer loss
I1123 14:46:23.299549 25976 net.cpp:122] Setting up loss
I1123 14:46:23.299549 25976 net.cpp:129] Top shape: (1)
I1123 14:46:23.299549 25976 net.cpp:132]     with loss weight 1
I1123 14:46:23.299549 25976 net.cpp:137] Memory required for data: 175004408
I1123 14:46:23.299549 25976 net.cpp:198] loss needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:200] accuracy_training does not need backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] ip1 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] poolcp6 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] relu_conv12 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] scale_conv12 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] bn_conv12 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] conv12 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] pool4_2 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] relu4_2 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] scale4_2 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] bn4_2 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] conv4_2 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] relu4_1 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] scale4_1 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] bn4_1 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] conv4_1 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] relu4 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] scale4 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] bn4 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] conv4 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] relu3 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] scale3 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] bn3 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] conv3 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] pool2_1 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] relu2_2 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] scale2_2 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] bn2_2 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] conv2_2 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] relu2 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] scale2 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] bn2 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] conv2 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] relu1 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] scale1 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] bn1 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:198] conv1 needs backward computation.
I1123 14:46:23.299549 25976 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 14:46:23.299549 25976 net.cpp:200] cifar does not need backward computation.
I1123 14:46:23.299549 25976 net.cpp:242] This network produces output accuracy_training
I1123 14:46:23.299549 25976 net.cpp:242] This network produces output loss
I1123 14:46:23.299549 25976 net.cpp:255] Network initialization done.
I1123 14:46:23.300549 25976 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 14:46:23.300549 25976 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 14:46:23.300549 25976 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 14:46:23.300549 25976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1123 14:46:23.300549 25976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1123 14:46:23.300549 25976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1123 14:46:23.300549 25976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1123 14:46:23.300549 25976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1123 14:46:23.300549 25976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1123 14:46:23.300549 25976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1123 14:46:23.300549 25976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1123 14:46:23.300549 25976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1123 14:46:23.300549 25976 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1123 14:46:23.300549 25976 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_7x7_300K"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 35
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 38
    pad: 3
    kernel_size: 7
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 14:46:23.300549 25976 layer_factory.cpp:58] Creating layer cifar
I1123 14:46:23.374802 25976 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1123 14:46:23.454828 25976 net.cpp:84] Creating Layer cifar
I1123 14:46:23.454828 25976 net.cpp:380] cifar -> data
I1123 14:46:23.454828 25976 net.cpp:380] cifar -> label
I1123 14:46:23.454828 25976 data_layer.cpp:45] output data size: 100,3,32,32
I1123 14:46:23.460814 25976 net.cpp:122] Setting up cifar
I1123 14:46:23.460814 25976 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 14:46:23.460814 25976 net.cpp:129] Top shape: 100 (100)
I1123 14:46:23.460814 25976 net.cpp:137] Memory required for data: 1229200
I1123 14:46:23.460814 25976 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 14:46:23.460814 25976 net.cpp:84] Creating Layer label_cifar_1_split
I1123 14:46:23.460814 25976 net.cpp:406] label_cifar_1_split <- label
I1123 14:46:23.460814 25976 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 14:46:23.460814 25976 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 14:46:23.460814 25976 net.cpp:122] Setting up label_cifar_1_split
I1123 14:46:23.460814 25976 net.cpp:129] Top shape: 100 (100)
I1123 14:46:23.460814 25976 net.cpp:129] Top shape: 100 (100)
I1123 14:46:23.460814 25976 net.cpp:137] Memory required for data: 1230000
I1123 14:46:23.460814 25976 layer_factory.cpp:58] Creating layer conv1
I1123 14:46:23.460814 25976 net.cpp:84] Creating Layer conv1
I1123 14:46:23.460814 25976 net.cpp:406] conv1 <- data
I1123 14:46:23.460814 25976 net.cpp:380] conv1 -> conv1
I1123 14:46:23.461815 35784 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 14:46:23.461815 25976 net.cpp:122] Setting up conv1
I1123 14:46:23.461815 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.461815 25976 net.cpp:137] Memory required for data: 9422000
I1123 14:46:23.461815 25976 layer_factory.cpp:58] Creating layer bn1
I1123 14:46:23.461815 25976 net.cpp:84] Creating Layer bn1
I1123 14:46:23.461815 25976 net.cpp:406] bn1 <- conv1
I1123 14:46:23.461815 25976 net.cpp:367] bn1 -> conv1 (in-place)
I1123 14:46:23.462815 25976 net.cpp:122] Setting up bn1
I1123 14:46:23.462815 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.462815 25976 net.cpp:137] Memory required for data: 17614000
I1123 14:46:23.462815 25976 layer_factory.cpp:58] Creating layer scale1
I1123 14:46:23.462815 25976 net.cpp:84] Creating Layer scale1
I1123 14:46:23.462815 25976 net.cpp:406] scale1 <- conv1
I1123 14:46:23.462815 25976 net.cpp:367] scale1 -> conv1 (in-place)
I1123 14:46:23.462815 25976 layer_factory.cpp:58] Creating layer scale1
I1123 14:46:23.462815 25976 net.cpp:122] Setting up scale1
I1123 14:46:23.462815 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.462815 25976 net.cpp:137] Memory required for data: 25806000
I1123 14:46:23.462815 25976 layer_factory.cpp:58] Creating layer relu1
I1123 14:46:23.462815 25976 net.cpp:84] Creating Layer relu1
I1123 14:46:23.462815 25976 net.cpp:406] relu1 <- conv1
I1123 14:46:23.462815 25976 net.cpp:367] relu1 -> conv1 (in-place)
I1123 14:46:23.462815 25976 net.cpp:122] Setting up relu1
I1123 14:46:23.462815 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.462815 25976 net.cpp:137] Memory required for data: 33998000
I1123 14:46:23.462815 25976 layer_factory.cpp:58] Creating layer conv2
I1123 14:46:23.462815 25976 net.cpp:84] Creating Layer conv2
I1123 14:46:23.462815 25976 net.cpp:406] conv2 <- conv1
I1123 14:46:23.462815 25976 net.cpp:380] conv2 -> conv2
I1123 14:46:23.463814 25976 net.cpp:122] Setting up conv2
I1123 14:46:23.463814 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.463814 25976 net.cpp:137] Memory required for data: 42190000
I1123 14:46:23.463814 25976 layer_factory.cpp:58] Creating layer bn2
I1123 14:46:23.463814 25976 net.cpp:84] Creating Layer bn2
I1123 14:46:23.463814 25976 net.cpp:406] bn2 <- conv2
I1123 14:46:23.463814 25976 net.cpp:367] bn2 -> conv2 (in-place)
I1123 14:46:23.463814 25976 net.cpp:122] Setting up bn2
I1123 14:46:23.463814 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.463814 25976 net.cpp:137] Memory required for data: 50382000
I1123 14:46:23.463814 25976 layer_factory.cpp:58] Creating layer scale2
I1123 14:46:23.463814 25976 net.cpp:84] Creating Layer scale2
I1123 14:46:23.463814 25976 net.cpp:406] scale2 <- conv2
I1123 14:46:23.463814 25976 net.cpp:367] scale2 -> conv2 (in-place)
I1123 14:46:23.463814 25976 layer_factory.cpp:58] Creating layer scale2
I1123 14:46:23.464813 25976 net.cpp:122] Setting up scale2
I1123 14:46:23.464813 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.464813 25976 net.cpp:137] Memory required for data: 58574000
I1123 14:46:23.464813 25976 layer_factory.cpp:58] Creating layer relu2
I1123 14:46:23.464813 25976 net.cpp:84] Creating Layer relu2
I1123 14:46:23.464813 25976 net.cpp:406] relu2 <- conv2
I1123 14:46:23.464813 25976 net.cpp:367] relu2 -> conv2 (in-place)
I1123 14:46:23.464813 25976 net.cpp:122] Setting up relu2
I1123 14:46:23.464813 25976 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 14:46:23.464813 25976 net.cpp:137] Memory required for data: 66766000
I1123 14:46:23.464813 25976 layer_factory.cpp:58] Creating layer conv2_2
I1123 14:46:23.464813 25976 net.cpp:84] Creating Layer conv2_2
I1123 14:46:23.464813 25976 net.cpp:406] conv2_2 <- conv2
I1123 14:46:23.464813 25976 net.cpp:380] conv2_2 -> conv2_2
I1123 14:46:23.465814 25976 net.cpp:122] Setting up conv2_2
I1123 14:46:23.465814 25976 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 14:46:23.465814 25976 net.cpp:137] Memory required for data: 79054000
I1123 14:46:23.465814 25976 layer_factory.cpp:58] Creating layer bn2_2
I1123 14:46:23.465814 25976 net.cpp:84] Creating Layer bn2_2
I1123 14:46:23.465814 25976 net.cpp:406] bn2_2 <- conv2_2
I1123 14:46:23.465814 25976 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 14:46:23.466814 25976 net.cpp:122] Setting up bn2_2
I1123 14:46:23.466814 25976 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 14:46:23.466814 25976 net.cpp:137] Memory required for data: 91342000
I1123 14:46:23.466814 25976 layer_factory.cpp:58] Creating layer scale2_2
I1123 14:46:23.466814 25976 net.cpp:84] Creating Layer scale2_2
I1123 14:46:23.466814 25976 net.cpp:406] scale2_2 <- conv2_2
I1123 14:46:23.466814 25976 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 14:46:23.466814 25976 layer_factory.cpp:58] Creating layer scale2_2
I1123 14:46:23.466814 25976 net.cpp:122] Setting up scale2_2
I1123 14:46:23.466814 25976 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 14:46:23.466814 25976 net.cpp:137] Memory required for data: 103630000
I1123 14:46:23.466814 25976 layer_factory.cpp:58] Creating layer relu2_2
I1123 14:46:23.466814 25976 net.cpp:84] Creating Layer relu2_2
I1123 14:46:23.466814 25976 net.cpp:406] relu2_2 <- conv2_2
I1123 14:46:23.466814 25976 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 14:46:23.466814 25976 net.cpp:122] Setting up relu2_2
I1123 14:46:23.466814 25976 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 14:46:23.466814 25976 net.cpp:137] Memory required for data: 115918000
I1123 14:46:23.466814 25976 layer_factory.cpp:58] Creating layer pool2_1
I1123 14:46:23.466814 25976 net.cpp:84] Creating Layer pool2_1
I1123 14:46:23.466814 25976 net.cpp:406] pool2_1 <- conv2_2
I1123 14:46:23.466814 25976 net.cpp:380] pool2_1 -> pool2_1
I1123 14:46:23.466814 25976 net.cpp:122] Setting up pool2_1
I1123 14:46:23.466814 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.466814 25976 net.cpp:137] Memory required for data: 118990000
I1123 14:46:23.466814 25976 layer_factory.cpp:58] Creating layer conv3
I1123 14:46:23.466814 25976 net.cpp:84] Creating Layer conv3
I1123 14:46:23.466814 25976 net.cpp:406] conv3 <- pool2_1
I1123 14:46:23.466814 25976 net.cpp:380] conv3 -> conv3
I1123 14:46:23.468814 25976 net.cpp:122] Setting up conv3
I1123 14:46:23.468814 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.468814 25976 net.cpp:137] Memory required for data: 122062000
I1123 14:46:23.468814 25976 layer_factory.cpp:58] Creating layer bn3
I1123 14:46:23.468814 25976 net.cpp:84] Creating Layer bn3
I1123 14:46:23.468814 25976 net.cpp:406] bn3 <- conv3
I1123 14:46:23.468814 25976 net.cpp:367] bn3 -> conv3 (in-place)
I1123 14:46:23.468814 25976 net.cpp:122] Setting up bn3
I1123 14:46:23.468814 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.468814 25976 net.cpp:137] Memory required for data: 125134000
I1123 14:46:23.468814 25976 layer_factory.cpp:58] Creating layer scale3
I1123 14:46:23.468814 25976 net.cpp:84] Creating Layer scale3
I1123 14:46:23.468814 25976 net.cpp:406] scale3 <- conv3
I1123 14:46:23.468814 25976 net.cpp:367] scale3 -> conv3 (in-place)
I1123 14:46:23.468814 25976 layer_factory.cpp:58] Creating layer scale3
I1123 14:46:23.468814 25976 net.cpp:122] Setting up scale3
I1123 14:46:23.468814 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.468814 25976 net.cpp:137] Memory required for data: 128206000
I1123 14:46:23.468814 25976 layer_factory.cpp:58] Creating layer relu3
I1123 14:46:23.468814 25976 net.cpp:84] Creating Layer relu3
I1123 14:46:23.468814 25976 net.cpp:406] relu3 <- conv3
I1123 14:46:23.468814 25976 net.cpp:367] relu3 -> conv3 (in-place)
I1123 14:46:23.469813 25976 net.cpp:122] Setting up relu3
I1123 14:46:23.469813 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.469813 25976 net.cpp:137] Memory required for data: 131278000
I1123 14:46:23.469813 25976 layer_factory.cpp:58] Creating layer conv4
I1123 14:46:23.469813 25976 net.cpp:84] Creating Layer conv4
I1123 14:46:23.469813 25976 net.cpp:406] conv4 <- conv3
I1123 14:46:23.469813 25976 net.cpp:380] conv4 -> conv4
I1123 14:46:23.470813 25976 net.cpp:122] Setting up conv4
I1123 14:46:23.470813 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.470813 25976 net.cpp:137] Memory required for data: 134350000
I1123 14:46:23.470813 25976 layer_factory.cpp:58] Creating layer bn4
I1123 14:46:23.470813 25976 net.cpp:84] Creating Layer bn4
I1123 14:46:23.470813 25976 net.cpp:406] bn4 <- conv4
I1123 14:46:23.470813 25976 net.cpp:367] bn4 -> conv4 (in-place)
I1123 14:46:23.470813 25976 net.cpp:122] Setting up bn4
I1123 14:46:23.470813 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.470813 25976 net.cpp:137] Memory required for data: 137422000
I1123 14:46:23.470813 25976 layer_factory.cpp:58] Creating layer scale4
I1123 14:46:23.470813 25976 net.cpp:84] Creating Layer scale4
I1123 14:46:23.470813 25976 net.cpp:406] scale4 <- conv4
I1123 14:46:23.470813 25976 net.cpp:367] scale4 -> conv4 (in-place)
I1123 14:46:23.470813 25976 layer_factory.cpp:58] Creating layer scale4
I1123 14:46:23.471813 25976 net.cpp:122] Setting up scale4
I1123 14:46:23.471813 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.471813 25976 net.cpp:137] Memory required for data: 140494000
I1123 14:46:23.471813 25976 layer_factory.cpp:58] Creating layer relu4
I1123 14:46:23.471813 25976 net.cpp:84] Creating Layer relu4
I1123 14:46:23.471813 25976 net.cpp:406] relu4 <- conv4
I1123 14:46:23.471813 25976 net.cpp:367] relu4 -> conv4 (in-place)
I1123 14:46:23.471813 25976 net.cpp:122] Setting up relu4
I1123 14:46:23.471813 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.471813 25976 net.cpp:137] Memory required for data: 143566000
I1123 14:46:23.471813 25976 layer_factory.cpp:58] Creating layer conv4_1
I1123 14:46:23.471813 25976 net.cpp:84] Creating Layer conv4_1
I1123 14:46:23.471813 25976 net.cpp:406] conv4_1 <- conv4
I1123 14:46:23.471813 25976 net.cpp:380] conv4_1 -> conv4_1
I1123 14:46:23.473320 25976 net.cpp:122] Setting up conv4_1
I1123 14:46:23.473320 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.473320 25976 net.cpp:137] Memory required for data: 146638000
I1123 14:46:23.473320 25976 layer_factory.cpp:58] Creating layer bn4_1
I1123 14:46:23.473320 25976 net.cpp:84] Creating Layer bn4_1
I1123 14:46:23.473320 25976 net.cpp:406] bn4_1 <- conv4_1
I1123 14:46:23.473320 25976 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 14:46:23.473320 25976 net.cpp:122] Setting up bn4_1
I1123 14:46:23.473320 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.473320 25976 net.cpp:137] Memory required for data: 149710000
I1123 14:46:23.473320 25976 layer_factory.cpp:58] Creating layer scale4_1
I1123 14:46:23.473320 25976 net.cpp:84] Creating Layer scale4_1
I1123 14:46:23.473320 25976 net.cpp:406] scale4_1 <- conv4_1
I1123 14:46:23.473320 25976 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 14:46:23.473320 25976 layer_factory.cpp:58] Creating layer scale4_1
I1123 14:46:23.473320 25976 net.cpp:122] Setting up scale4_1
I1123 14:46:23.473320 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.473819 25976 net.cpp:137] Memory required for data: 152782000
I1123 14:46:23.473819 25976 layer_factory.cpp:58] Creating layer relu4_1
I1123 14:46:23.473819 25976 net.cpp:84] Creating Layer relu4_1
I1123 14:46:23.473819 25976 net.cpp:406] relu4_1 <- conv4_1
I1123 14:46:23.473819 25976 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 14:46:23.473819 25976 net.cpp:122] Setting up relu4_1
I1123 14:46:23.473819 25976 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 14:46:23.473819 25976 net.cpp:137] Memory required for data: 155854000
I1123 14:46:23.473819 25976 layer_factory.cpp:58] Creating layer conv4_2
I1123 14:46:23.473819 25976 net.cpp:84] Creating Layer conv4_2
I1123 14:46:23.473819 25976 net.cpp:406] conv4_2 <- conv4_1
I1123 14:46:23.473819 25976 net.cpp:380] conv4_2 -> conv4_2
I1123 14:46:23.475819 25976 net.cpp:122] Setting up conv4_2
I1123 14:46:23.475819 25976 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 14:46:23.475819 25976 net.cpp:137] Memory required for data: 159438000
I1123 14:46:23.475819 25976 layer_factory.cpp:58] Creating layer bn4_2
I1123 14:46:23.475819 25976 net.cpp:84] Creating Layer bn4_2
I1123 14:46:23.475819 25976 net.cpp:406] bn4_2 <- conv4_2
I1123 14:46:23.475819 25976 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 14:46:23.475819 25976 net.cpp:122] Setting up bn4_2
I1123 14:46:23.475819 25976 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 14:46:23.475819 25976 net.cpp:137] Memory required for data: 163022000
I1123 14:46:23.476320 25976 layer_factory.cpp:58] Creating layer scale4_2
I1123 14:46:23.476320 25976 net.cpp:84] Creating Layer scale4_2
I1123 14:46:23.476320 25976 net.cpp:406] scale4_2 <- conv4_2
I1123 14:46:23.476320 25976 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 14:46:23.476320 25976 layer_factory.cpp:58] Creating layer scale4_2
I1123 14:46:23.476320 25976 net.cpp:122] Setting up scale4_2
I1123 14:46:23.476320 25976 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 14:46:23.476320 25976 net.cpp:137] Memory required for data: 166606000
I1123 14:46:23.476320 25976 layer_factory.cpp:58] Creating layer relu4_2
I1123 14:46:23.476320 25976 net.cpp:84] Creating Layer relu4_2
I1123 14:46:23.476320 25976 net.cpp:406] relu4_2 <- conv4_2
I1123 14:46:23.476320 25976 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 14:46:23.476822 25976 net.cpp:122] Setting up relu4_2
I1123 14:46:23.476822 25976 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 14:46:23.476822 25976 net.cpp:137] Memory required for data: 170190000
I1123 14:46:23.476822 25976 layer_factory.cpp:58] Creating layer pool4_2
I1123 14:46:23.476822 25976 net.cpp:84] Creating Layer pool4_2
I1123 14:46:23.476822 25976 net.cpp:406] pool4_2 <- conv4_2
I1123 14:46:23.476822 25976 net.cpp:380] pool4_2 -> pool4_2
I1123 14:46:23.476822 25976 net.cpp:122] Setting up pool4_2
I1123 14:46:23.476822 25976 net.cpp:129] Top shape: 100 35 8 8 (224000)
I1123 14:46:23.476822 25976 net.cpp:137] Memory required for data: 171086000
I1123 14:46:23.476822 25976 layer_factory.cpp:58] Creating layer conv12
I1123 14:46:23.476822 25976 net.cpp:84] Creating Layer conv12
I1123 14:46:23.476822 25976 net.cpp:406] conv12 <- pool4_2
I1123 14:46:23.476822 25976 net.cpp:380] conv12 -> conv12
I1123 14:46:23.477829 25976 net.cpp:122] Setting up conv12
I1123 14:46:23.477829 25976 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 14:46:23.477829 25976 net.cpp:137] Memory required for data: 172058800
I1123 14:46:23.477829 25976 layer_factory.cpp:58] Creating layer bn_conv12
I1123 14:46:23.477829 25976 net.cpp:84] Creating Layer bn_conv12
I1123 14:46:23.477829 25976 net.cpp:406] bn_conv12 <- conv12
I1123 14:46:23.477829 25976 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 14:46:23.478328 25976 net.cpp:122] Setting up bn_conv12
I1123 14:46:23.478328 25976 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 14:46:23.478328 25976 net.cpp:137] Memory required for data: 173031600
I1123 14:46:23.478328 25976 layer_factory.cpp:58] Creating layer scale_conv12
I1123 14:46:23.478328 25976 net.cpp:84] Creating Layer scale_conv12
I1123 14:46:23.478328 25976 net.cpp:406] scale_conv12 <- conv12
I1123 14:46:23.478328 25976 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 14:46:23.478328 25976 layer_factory.cpp:58] Creating layer scale_conv12
I1123 14:46:23.478328 25976 net.cpp:122] Setting up scale_conv12
I1123 14:46:23.478328 25976 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 14:46:23.478328 25976 net.cpp:137] Memory required for data: 174004400
I1123 14:46:23.478328 25976 layer_factory.cpp:58] Creating layer relu_conv12
I1123 14:46:23.478328 25976 net.cpp:84] Creating Layer relu_conv12
I1123 14:46:23.478328 25976 net.cpp:406] relu_conv12 <- conv12
I1123 14:46:23.478328 25976 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 14:46:23.478961 25976 net.cpp:122] Setting up relu_conv12
I1123 14:46:23.478961 25976 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 14:46:23.478961 25976 net.cpp:137] Memory required for data: 174977200
I1123 14:46:23.478961 25976 layer_factory.cpp:58] Creating layer poolcp6
I1123 14:46:23.478961 25976 net.cpp:84] Creating Layer poolcp6
I1123 14:46:23.478961 25976 net.cpp:406] poolcp6 <- conv12
I1123 14:46:23.478961 25976 net.cpp:380] poolcp6 -> poolcp6
I1123 14:46:23.478961 25976 net.cpp:122] Setting up poolcp6
I1123 14:46:23.478961 25976 net.cpp:129] Top shape: 100 38 1 1 (3800)
I1123 14:46:23.478961 25976 net.cpp:137] Memory required for data: 174992400
I1123 14:46:23.478961 25976 layer_factory.cpp:58] Creating layer ip1
I1123 14:46:23.478961 25976 net.cpp:84] Creating Layer ip1
I1123 14:46:23.478961 25976 net.cpp:406] ip1 <- poolcp6
I1123 14:46:23.478961 25976 net.cpp:380] ip1 -> ip1
I1123 14:46:23.478961 25976 net.cpp:122] Setting up ip1
I1123 14:46:23.478961 25976 net.cpp:129] Top shape: 100 10 (1000)
I1123 14:46:23.478961 25976 net.cpp:137] Memory required for data: 174996400
I1123 14:46:23.478961 25976 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 14:46:23.478961 25976 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 14:46:23.478961 25976 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 14:46:23.478961 25976 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 14:46:23.478961 25976 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 14:46:23.479496 25976 net.cpp:122] Setting up ip1_ip1_0_split
I1123 14:46:23.479496 25976 net.cpp:129] Top shape: 100 10 (1000)
I1123 14:46:23.479496 25976 net.cpp:129] Top shape: 100 10 (1000)
I1123 14:46:23.479496 25976 net.cpp:137] Memory required for data: 175004400
I1123 14:46:23.479496 25976 layer_factory.cpp:58] Creating layer accuracy
I1123 14:46:23.479496 25976 net.cpp:84] Creating Layer accuracy
I1123 14:46:23.479496 25976 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1123 14:46:23.479496 25976 net.cpp:406] accuracy <- label_cifar_1_split_0
I1123 14:46:23.479496 25976 net.cpp:380] accuracy -> accuracy
I1123 14:46:23.479496 25976 net.cpp:122] Setting up accuracy
I1123 14:46:23.479496 25976 net.cpp:129] Top shape: (1)
I1123 14:46:23.479496 25976 net.cpp:137] Memory required for data: 175004404
I1123 14:46:23.479496 25976 layer_factory.cpp:58] Creating layer loss
I1123 14:46:23.479496 25976 net.cpp:84] Creating Layer loss
I1123 14:46:23.479496 25976 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 14:46:23.479496 25976 net.cpp:406] loss <- label_cifar_1_split_1
I1123 14:46:23.479496 25976 net.cpp:380] loss -> loss
I1123 14:46:23.479496 25976 layer_factory.cpp:58] Creating layer loss
I1123 14:46:23.479496 25976 net.cpp:122] Setting up loss
I1123 14:46:23.479496 25976 net.cpp:129] Top shape: (1)
I1123 14:46:23.479496 25976 net.cpp:132]     with loss weight 1
I1123 14:46:23.479496 25976 net.cpp:137] Memory required for data: 175004408
I1123 14:46:23.479496 25976 net.cpp:198] loss needs backward computation.
I1123 14:46:23.479496 25976 net.cpp:200] accuracy does not need backward computation.
I1123 14:46:23.479496 25976 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 14:46:23.479496 25976 net.cpp:198] ip1 needs backward computation.
I1123 14:46:23.479496 25976 net.cpp:198] poolcp6 needs backward computation.
I1123 14:46:23.479496 25976 net.cpp:198] relu_conv12 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] scale_conv12 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] bn_conv12 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] conv12 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] pool4_2 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] relu4_2 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] scale4_2 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] bn4_2 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] conv4_2 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] relu4_1 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] scale4_1 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] bn4_1 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] conv4_1 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] relu4 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] scale4 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] bn4 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] conv4 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] relu3 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] scale3 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] bn3 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] conv3 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] pool2_1 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] relu2_2 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] scale2_2 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] bn2_2 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] conv2_2 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] relu2 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] scale2 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] bn2 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] conv2 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] relu1 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] scale1 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] bn1 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:198] conv1 needs backward computation.
I1123 14:46:23.479995 25976 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 14:46:23.479995 25976 net.cpp:200] cifar does not need backward computation.
I1123 14:46:23.479995 25976 net.cpp:242] This network produces output accuracy
I1123 14:46:23.479995 25976 net.cpp:242] This network produces output loss
I1123 14:46:23.479995 25976 net.cpp:255] Network initialization done.
I1123 14:46:23.479995 25976 solver.cpp:56] Solver scaffolding done.
I1123 14:46:23.482496 25976 caffe.cpp:249] Starting Optimization
I1123 14:46:23.482496 25976 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_7x7_300K
I1123 14:46:23.482496 25976 solver.cpp:273] Learning Rate Policy: multistep
I1123 14:46:23.484027 25976 solver.cpp:330] Iteration 0, Testing net (#0)
I1123 14:46:23.485530 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:46:23.485530 25976 blocking_queue.cpp:49] Waiting for data
I1123 14:46:30.426918 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:46:30.446916 25976 solver.cpp:397]     Test net output #0: accuracy = 0.1006
I1123 14:46:30.446916 25976 solver.cpp:397]     Test net output #1: loss = 78.5505 (* 1 = 78.5505 loss)
I1123 14:46:30.505923 25976 solver.cpp:218] Iteration 0 (0 iter/s, 7.02296s/100 iters), loss = 3.59469
I1123 14:46:30.505923 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.05
I1123 14:46:30.505923 25976 solver.cpp:237]     Train net output #1: loss = 3.59469 (* 1 = 3.59469 loss)
I1123 14:46:30.505923 25976 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1123 14:46:34.324213 25976 solver.cpp:218] Iteration 100 (26.1922 iter/s, 3.81793s/100 iters), loss = 1.6925
I1123 14:46:34.324213 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.37
I1123 14:46:34.324213 25976 solver.cpp:237]     Train net output #1: loss = 1.6925 (* 1 = 1.6925 loss)
I1123 14:46:34.324213 25976 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1123 14:46:38.145536 25976 solver.cpp:218] Iteration 200 (26.1683 iter/s, 3.82142s/100 iters), loss = 1.95436
I1123 14:46:38.145536 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.31
I1123 14:46:38.145536 25976 solver.cpp:237]     Train net output #1: loss = 1.95436 (* 1 = 1.95436 loss)
I1123 14:46:38.145536 25976 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1123 14:46:41.962808 25976 solver.cpp:218] Iteration 300 (26.2047 iter/s, 3.81611s/100 iters), loss = 1.44376
I1123 14:46:41.962808 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.5
I1123 14:46:41.962808 25976 solver.cpp:237]     Train net output #1: loss = 1.44376 (* 1 = 1.44376 loss)
I1123 14:46:41.962808 25976 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1123 14:46:45.782596 25976 solver.cpp:218] Iteration 400 (26.1825 iter/s, 3.81934s/100 iters), loss = 1.29124
I1123 14:46:45.782596 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.48
I1123 14:46:45.782596 25976 solver.cpp:237]     Train net output #1: loss = 1.29124 (* 1 = 1.29124 loss)
I1123 14:46:45.782596 25976 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1123 14:46:49.442436 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:46:49.592447 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_500.caffemodel
I1123 14:46:49.606441 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_500.solverstate
I1123 14:46:49.610945 25976 solver.cpp:330] Iteration 500, Testing net (#0)
I1123 14:46:49.610945 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:46:50.671545 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:46:50.714051 25976 solver.cpp:397]     Test net output #0: accuracy = 0.4798
I1123 14:46:50.714051 25976 solver.cpp:397]     Test net output #1: loss = 1.45612 (* 1 = 1.45612 loss)
I1123 14:46:50.750556 25976 solver.cpp:218] Iteration 500 (20.1272 iter/s, 4.96839s/100 iters), loss = 1.39428
I1123 14:46:50.750556 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.52
I1123 14:46:50.750556 25976 solver.cpp:237]     Train net output #1: loss = 1.39428 (* 1 = 1.39428 loss)
I1123 14:46:50.751556 25976 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1123 14:46:54.597859 25976 solver.cpp:218] Iteration 600 (25.9981 iter/s, 3.84643s/100 iters), loss = 1.32142
I1123 14:46:54.597859 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.53
I1123 14:46:54.597859 25976 solver.cpp:237]     Train net output #1: loss = 1.32142 (* 1 = 1.32142 loss)
I1123 14:46:54.597859 25976 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1123 14:46:58.438161 25976 solver.cpp:218] Iteration 700 (26.0428 iter/s, 3.83983s/100 iters), loss = 1.22743
I1123 14:46:58.438161 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.54
I1123 14:46:58.438161 25976 solver.cpp:237]     Train net output #1: loss = 1.22743 (* 1 = 1.22743 loss)
I1123 14:46:58.438161 25976 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1123 14:47:02.269464 25976 solver.cpp:218] Iteration 800 (26.1044 iter/s, 3.83077s/100 iters), loss = 0.988883
I1123 14:47:02.269464 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1123 14:47:02.269464 25976 solver.cpp:237]     Train net output #1: loss = 0.988883 (* 1 = 0.988883 loss)
I1123 14:47:02.269464 25976 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1123 14:47:06.097728 25976 solver.cpp:218] Iteration 900 (26.12 iter/s, 3.82848s/100 iters), loss = 1.02794
I1123 14:47:06.097728 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1123 14:47:06.097728 25976 solver.cpp:237]     Train net output #1: loss = 1.02794 (* 1 = 1.02794 loss)
I1123 14:47:06.097728 25976 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1123 14:47:09.758049 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:47:09.908560 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_1000.caffemodel
I1123 14:47:09.920063 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_1000.solverstate
I1123 14:47:09.925062 25976 solver.cpp:330] Iteration 1000, Testing net (#0)
I1123 14:47:09.925062 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:47:10.989168 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:47:11.031184 25976 solver.cpp:397]     Test net output #0: accuracy = 0.557
I1123 14:47:11.031184 25976 solver.cpp:397]     Test net output #1: loss = 1.22352 (* 1 = 1.22352 loss)
I1123 14:47:11.068182 25976 solver.cpp:218] Iteration 1000 (20.1231 iter/s, 4.96942s/100 iters), loss = 1.08227
I1123 14:47:11.068182 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1123 14:47:11.068182 25976 solver.cpp:237]     Train net output #1: loss = 1.08227 (* 1 = 1.08227 loss)
I1123 14:47:11.068182 25976 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1123 14:47:14.914121 25976 solver.cpp:218] Iteration 1100 (26.0029 iter/s, 3.84572s/100 iters), loss = 0.973662
I1123 14:47:14.914121 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1123 14:47:14.914121 25976 solver.cpp:237]     Train net output #1: loss = 0.973662 (* 1 = 0.973662 loss)
I1123 14:47:14.914121 25976 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1123 14:47:18.764911 25976 solver.cpp:218] Iteration 1200 (25.9715 iter/s, 3.85038s/100 iters), loss = 0.988844
I1123 14:47:18.764911 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.62
I1123 14:47:18.764911 25976 solver.cpp:237]     Train net output #1: loss = 0.988844 (* 1 = 0.988844 loss)
I1123 14:47:18.764911 25976 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1123 14:47:22.618221 25976 solver.cpp:218] Iteration 1300 (25.9546 iter/s, 3.85287s/100 iters), loss = 0.951877
I1123 14:47:22.618221 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1123 14:47:22.618221 25976 solver.cpp:237]     Train net output #1: loss = 0.951877 (* 1 = 0.951877 loss)
I1123 14:47:22.618221 25976 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1123 14:47:26.485510 25976 solver.cpp:218] Iteration 1400 (25.8571 iter/s, 3.86741s/100 iters), loss = 0.994119
I1123 14:47:26.485510 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1123 14:47:26.485510 25976 solver.cpp:237]     Train net output #1: loss = 0.994119 (* 1 = 0.994119 loss)
I1123 14:47:26.485510 25976 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1123 14:47:30.148751 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:47:30.299757 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_1500.caffemodel
I1123 14:47:30.308756 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_1500.solverstate
I1123 14:47:30.312757 25976 solver.cpp:330] Iteration 1500, Testing net (#0)
I1123 14:47:30.312757 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:47:31.380821 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:47:31.422824 25976 solver.cpp:397]     Test net output #0: accuracy = 0.5795
I1123 14:47:31.422824 25976 solver.cpp:397]     Test net output #1: loss = 1.20528 (* 1 = 1.20528 loss)
I1123 14:47:31.459826 25976 solver.cpp:218] Iteration 1500 (20.1062 iter/s, 4.97358s/100 iters), loss = 0.893853
I1123 14:47:31.459826 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1123 14:47:31.459826 25976 solver.cpp:237]     Train net output #1: loss = 0.893853 (* 1 = 0.893853 loss)
I1123 14:47:31.459826 25976 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1123 14:47:35.306542 25976 solver.cpp:218] Iteration 1600 (25.9984 iter/s, 3.8464s/100 iters), loss = 0.774827
I1123 14:47:35.306542 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1123 14:47:35.306542 25976 solver.cpp:237]     Train net output #1: loss = 0.774827 (* 1 = 0.774827 loss)
I1123 14:47:35.306542 25976 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1123 14:47:39.169360 25976 solver.cpp:218] Iteration 1700 (25.892 iter/s, 3.86219s/100 iters), loss = 0.843849
I1123 14:47:39.169360 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1123 14:47:39.169360 25976 solver.cpp:237]     Train net output #1: loss = 0.843849 (* 1 = 0.843849 loss)
I1123 14:47:39.169360 25976 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1123 14:47:43.025674 25976 solver.cpp:218] Iteration 1800 (25.9327 iter/s, 3.85613s/100 iters), loss = 0.762576
I1123 14:47:43.025674 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 14:47:43.025674 25976 solver.cpp:237]     Train net output #1: loss = 0.762576 (* 1 = 0.762576 loss)
I1123 14:47:43.025674 25976 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1123 14:47:46.869148 25976 solver.cpp:218] Iteration 1900 (26.024 iter/s, 3.84261s/100 iters), loss = 0.802492
I1123 14:47:46.869148 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 14:47:46.869148 25976 solver.cpp:237]     Train net output #1: loss = 0.802492 (* 1 = 0.802492 loss)
I1123 14:47:46.869148 25976 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1123 14:47:50.529423 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:47:50.681931 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_2000.caffemodel
I1123 14:47:50.694434 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_2000.solverstate
I1123 14:47:50.698434 25976 solver.cpp:330] Iteration 2000, Testing net (#0)
I1123 14:47:50.698434 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:47:51.775044 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:47:51.816545 25976 solver.cpp:397]     Test net output #0: accuracy = 0.662
I1123 14:47:51.816545 25976 solver.cpp:397]     Test net output #1: loss = 0.950639 (* 1 = 0.950639 loss)
I1123 14:47:51.853545 25976 solver.cpp:218] Iteration 2000 (20.0615 iter/s, 4.98468s/100 iters), loss = 0.774784
I1123 14:47:51.853545 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1123 14:47:51.853545 25976 solver.cpp:237]     Train net output #1: loss = 0.774784 (* 1 = 0.774784 loss)
I1123 14:47:51.853545 25976 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1123 14:47:55.757843 25976 solver.cpp:218] Iteration 2100 (25.6172 iter/s, 3.90363s/100 iters), loss = 0.743057
I1123 14:47:55.757843 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1123 14:47:55.757843 25976 solver.cpp:237]     Train net output #1: loss = 0.743057 (* 1 = 0.743057 loss)
I1123 14:47:55.757843 25976 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1123 14:47:59.624136 25976 solver.cpp:218] Iteration 2200 (25.8671 iter/s, 3.86591s/100 iters), loss = 0.855141
I1123 14:47:59.624136 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1123 14:47:59.624136 25976 solver.cpp:237]     Train net output #1: loss = 0.855141 (* 1 = 0.855141 loss)
I1123 14:47:59.624136 25976 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1123 14:48:03.491502 25976 solver.cpp:218] Iteration 2300 (25.8556 iter/s, 3.86764s/100 iters), loss = 0.784206
I1123 14:48:03.491502 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1123 14:48:03.492502 25976 solver.cpp:237]     Train net output #1: loss = 0.784206 (* 1 = 0.784206 loss)
I1123 14:48:03.492502 25976 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1123 14:48:07.354787 25976 solver.cpp:218] Iteration 2400 (25.8906 iter/s, 3.86241s/100 iters), loss = 0.841686
I1123 14:48:07.354787 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1123 14:48:07.354787 25976 solver.cpp:237]     Train net output #1: loss = 0.841686 (* 1 = 0.841686 loss)
I1123 14:48:07.354787 25976 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1123 14:48:11.062850 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:48:11.216351 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_2500.caffemodel
I1123 14:48:11.226850 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_2500.solverstate
I1123 14:48:11.231350 25976 solver.cpp:330] Iteration 2500, Testing net (#0)
I1123 14:48:11.231350 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:48:12.317421 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:48:12.359419 25976 solver.cpp:397]     Test net output #0: accuracy = 0.6556
I1123 14:48:12.359419 25976 solver.cpp:397]     Test net output #1: loss = 0.994463 (* 1 = 0.994463 loss)
I1123 14:48:12.396937 25976 solver.cpp:218] Iteration 2500 (19.8347 iter/s, 5.04166s/100 iters), loss = 0.68464
I1123 14:48:12.397440 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1123 14:48:12.397440 25976 solver.cpp:237]     Train net output #1: loss = 0.68464 (* 1 = 0.68464 loss)
I1123 14:48:12.397440 25976 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1123 14:48:16.273691 25976 solver.cpp:218] Iteration 2600 (25.7945 iter/s, 3.87679s/100 iters), loss = 0.612334
I1123 14:48:16.273691 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 14:48:16.273691 25976 solver.cpp:237]     Train net output #1: loss = 0.612334 (* 1 = 0.612334 loss)
I1123 14:48:16.273691 25976 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1123 14:48:20.139969 25976 solver.cpp:218] Iteration 2700 (25.8676 iter/s, 3.86585s/100 iters), loss = 0.705635
I1123 14:48:20.139969 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 14:48:20.139969 25976 solver.cpp:237]     Train net output #1: loss = 0.705635 (* 1 = 0.705635 loss)
I1123 14:48:20.139969 25976 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1123 14:48:24.024257 25976 solver.cpp:218] Iteration 2800 (25.7481 iter/s, 3.88379s/100 iters), loss = 0.697778
I1123 14:48:24.024257 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 14:48:24.024257 25976 solver.cpp:237]     Train net output #1: loss = 0.697778 (* 1 = 0.697778 loss)
I1123 14:48:24.024257 25976 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1123 14:48:27.916534 25976 solver.cpp:218] Iteration 2900 (25.6944 iter/s, 3.89189s/100 iters), loss = 0.680169
I1123 14:48:27.916534 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 14:48:27.916534 25976 solver.cpp:237]     Train net output #1: loss = 0.680169 (* 1 = 0.680169 loss)
I1123 14:48:27.916534 25976 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1123 14:48:31.675846 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:48:31.831871 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_3000.caffemodel
I1123 14:48:31.841871 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_3000.solverstate
I1123 14:48:31.845871 25976 solver.cpp:330] Iteration 3000, Testing net (#0)
I1123 14:48:31.845871 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:48:32.933944 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:48:32.975952 25976 solver.cpp:397]     Test net output #0: accuracy = 0.6588
I1123 14:48:32.975952 25976 solver.cpp:397]     Test net output #1: loss = 1.00077 (* 1 = 1.00077 loss)
I1123 14:48:33.012949 25976 solver.cpp:218] Iteration 3000 (19.6265 iter/s, 5.09515s/100 iters), loss = 0.760055
I1123 14:48:33.012949 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 14:48:33.012949 25976 solver.cpp:237]     Train net output #1: loss = 0.760055 (* 1 = 0.760055 loss)
I1123 14:48:33.012949 25976 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1123 14:48:36.863198 25976 solver.cpp:218] Iteration 3100 (25.9748 iter/s, 3.84988s/100 iters), loss = 0.684068
I1123 14:48:36.863198 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 14:48:36.863198 25976 solver.cpp:237]     Train net output #1: loss = 0.684068 (* 1 = 0.684068 loss)
I1123 14:48:36.863198 25976 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1123 14:48:40.713487 25976 solver.cpp:218] Iteration 3200 (25.9737 iter/s, 3.85005s/100 iters), loss = 0.758473
I1123 14:48:40.713487 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1123 14:48:40.713487 25976 solver.cpp:237]     Train net output #1: loss = 0.758473 (* 1 = 0.758473 loss)
I1123 14:48:40.713487 25976 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1123 14:48:44.600878 25976 solver.cpp:218] Iteration 3300 (25.725 iter/s, 3.88727s/100 iters), loss = 0.6781
I1123 14:48:44.600878 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 14:48:44.600878 25976 solver.cpp:237]     Train net output #1: loss = 0.6781 (* 1 = 0.6781 loss)
I1123 14:48:44.600878 25976 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1123 14:48:48.494186 25976 solver.cpp:218] Iteration 3400 (25.6885 iter/s, 3.8928s/100 iters), loss = 0.737291
I1123 14:48:48.494186 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 14:48:48.494186 25976 solver.cpp:237]     Train net output #1: loss = 0.737291 (* 1 = 0.737291 loss)
I1123 14:48:48.494186 25976 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1123 14:48:52.182685 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:48:52.335693 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_3500.caffemodel
I1123 14:48:52.345692 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_3500.solverstate
I1123 14:48:52.349692 25976 solver.cpp:330] Iteration 3500, Testing net (#0)
I1123 14:48:52.349692 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:48:53.418790 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:48:53.460791 25976 solver.cpp:397]     Test net output #0: accuracy = 0.6734
I1123 14:48:53.460791 25976 solver.cpp:397]     Test net output #1: loss = 0.931044 (* 1 = 0.931044 loss)
I1123 14:48:53.498798 25976 solver.cpp:218] Iteration 3500 (19.9841 iter/s, 5.00398s/100 iters), loss = 0.739898
I1123 14:48:53.498798 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 14:48:53.498798 25976 solver.cpp:237]     Train net output #1: loss = 0.739898 (* 1 = 0.739898 loss)
I1123 14:48:53.498798 25976 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1123 14:48:57.514997 25976 solver.cpp:218] Iteration 3600 (24.8997 iter/s, 4.01611s/100 iters), loss = 0.596347
I1123 14:48:57.514997 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 14:48:57.514997 25976 solver.cpp:237]     Train net output #1: loss = 0.596347 (* 1 = 0.596347 loss)
I1123 14:48:57.515497 25976 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1123 14:49:01.443253 25976 solver.cpp:218] Iteration 3700 (25.4586 iter/s, 3.92795s/100 iters), loss = 0.650835
I1123 14:49:01.443253 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 14:49:01.443253 25976 solver.cpp:237]     Train net output #1: loss = 0.650835 (* 1 = 0.650835 loss)
I1123 14:49:01.443253 25976 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1123 14:49:05.392536 25976 solver.cpp:218] Iteration 3800 (25.3219 iter/s, 3.94915s/100 iters), loss = 0.649185
I1123 14:49:05.393541 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 14:49:05.393541 25976 solver.cpp:237]     Train net output #1: loss = 0.649185 (* 1 = 0.649185 loss)
I1123 14:49:05.393541 25976 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1123 14:49:09.290968 25976 solver.cpp:218] Iteration 3900 (25.6582 iter/s, 3.8974s/100 iters), loss = 0.766641
I1123 14:49:09.290968 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1123 14:49:09.290968 25976 solver.cpp:237]     Train net output #1: loss = 0.766641 (* 1 = 0.766641 loss)
I1123 14:49:09.290968 25976 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1123 14:49:12.970216 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:49:13.120221 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_4000.caffemodel
I1123 14:49:13.130221 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_4000.solverstate
I1123 14:49:13.134222 25976 solver.cpp:330] Iteration 4000, Testing net (#0)
I1123 14:49:13.134222 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:49:14.215297 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:49:14.257305 25976 solver.cpp:397]     Test net output #0: accuracy = 0.5726
I1123 14:49:14.257305 25976 solver.cpp:397]     Test net output #1: loss = 1.37546 (* 1 = 1.37546 loss)
I1123 14:49:14.294303 25976 solver.cpp:218] Iteration 4000 (19.9865 iter/s, 5.00338s/100 iters), loss = 0.722956
I1123 14:49:14.294303 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 14:49:14.294303 25976 solver.cpp:237]     Train net output #1: loss = 0.722956 (* 1 = 0.722956 loss)
I1123 14:49:14.294303 25976 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1123 14:49:18.183554 25976 solver.cpp:218] Iteration 4100 (25.7163 iter/s, 3.88858s/100 iters), loss = 0.625129
I1123 14:49:18.183554 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 14:49:18.183554 25976 solver.cpp:237]     Train net output #1: loss = 0.625129 (* 1 = 0.625129 loss)
I1123 14:49:18.183554 25976 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1123 14:49:22.072793 25976 solver.cpp:218] Iteration 4200 (25.7114 iter/s, 3.88933s/100 iters), loss = 0.482962
I1123 14:49:22.073793 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 14:49:22.073793 25976 solver.cpp:237]     Train net output #1: loss = 0.482962 (* 1 = 0.482962 loss)
I1123 14:49:22.073793 25976 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1123 14:49:25.952121 25976 solver.cpp:218] Iteration 4300 (25.785 iter/s, 3.87823s/100 iters), loss = 0.664943
I1123 14:49:25.952121 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 14:49:25.952121 25976 solver.cpp:237]     Train net output #1: loss = 0.664943 (* 1 = 0.664943 loss)
I1123 14:49:25.952121 25976 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1123 14:49:29.835762 25976 solver.cpp:218] Iteration 4400 (25.7513 iter/s, 3.88329s/100 iters), loss = 0.639023
I1123 14:49:29.835762 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 14:49:29.835762 25976 solver.cpp:237]     Train net output #1: loss = 0.639023 (* 1 = 0.639023 loss)
I1123 14:49:29.835762 25976 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1123 14:49:33.542870 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:49:33.697878 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_4500.caffemodel
I1123 14:49:33.708879 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_4500.solverstate
I1123 14:49:33.712879 25976 solver.cpp:330] Iteration 4500, Testing net (#0)
I1123 14:49:33.712879 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:49:34.791952 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:49:34.833951 25976 solver.cpp:397]     Test net output #0: accuracy = 0.6799
I1123 14:49:34.833951 25976 solver.cpp:397]     Test net output #1: loss = 0.922809 (* 1 = 0.922809 loss)
I1123 14:49:34.870957 25976 solver.cpp:218] Iteration 4500 (19.8603 iter/s, 5.03518s/100 iters), loss = 0.69207
I1123 14:49:34.870957 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 14:49:34.870957 25976 solver.cpp:237]     Train net output #1: loss = 0.69207 (* 1 = 0.69207 loss)
I1123 14:49:34.870957 25976 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1123 14:49:38.762224 25976 solver.cpp:218] Iteration 4600 (25.7019 iter/s, 3.89076s/100 iters), loss = 0.507354
I1123 14:49:38.762725 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 14:49:38.762725 25976 solver.cpp:237]     Train net output #1: loss = 0.507354 (* 1 = 0.507354 loss)
I1123 14:49:38.762725 25976 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1123 14:49:42.625471 25976 solver.cpp:218] Iteration 4700 (25.8875 iter/s, 3.86286s/100 iters), loss = 0.604155
I1123 14:49:42.625471 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 14:49:42.625471 25976 solver.cpp:237]     Train net output #1: loss = 0.604155 (* 1 = 0.604155 loss)
I1123 14:49:42.625471 25976 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1123 14:49:46.500712 25976 solver.cpp:218] Iteration 4800 (25.8094 iter/s, 3.87455s/100 iters), loss = 0.650616
I1123 14:49:46.500712 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 14:49:46.500712 25976 solver.cpp:237]     Train net output #1: loss = 0.650616 (* 1 = 0.650616 loss)
I1123 14:49:46.500712 25976 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1123 14:49:50.537132 25976 solver.cpp:218] Iteration 4900 (24.7722 iter/s, 4.03679s/100 iters), loss = 0.581584
I1123 14:49:50.538132 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 14:49:50.538132 25976 solver.cpp:237]     Train net output #1: loss = 0.581584 (* 1 = 0.581584 loss)
I1123 14:49:50.538132 25976 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1123 14:49:54.225596 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:49:54.376607 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_5000.caffemodel
I1123 14:49:54.388607 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_5000.solverstate
I1123 14:49:54.392607 25976 solver.cpp:330] Iteration 5000, Testing net (#0)
I1123 14:49:54.392607 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:49:55.468706 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:49:55.510706 25976 solver.cpp:397]     Test net output #0: accuracy = 0.6955
I1123 14:49:55.510706 25976 solver.cpp:397]     Test net output #1: loss = 0.897861 (* 1 = 0.897861 loss)
I1123 14:49:55.548708 25976 solver.cpp:218] Iteration 5000 (19.9584 iter/s, 5.01043s/100 iters), loss = 0.531224
I1123 14:49:55.548708 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 14:49:55.548708 25976 solver.cpp:237]     Train net output #1: loss = 0.531224 (* 1 = 0.531224 loss)
I1123 14:49:55.548708 25976 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1123 14:49:55.548708 25976 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1123 14:49:59.420989 25976 solver.cpp:218] Iteration 5100 (25.8232 iter/s, 3.87249s/100 iters), loss = 0.369042
I1123 14:49:59.420989 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 14:49:59.420989 25976 solver.cpp:237]     Train net output #1: loss = 0.369042 (* 1 = 0.369042 loss)
I1123 14:49:59.420989 25976 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1123 14:50:03.341462 25976 solver.cpp:218] Iteration 5200 (25.5124 iter/s, 3.91966s/100 iters), loss = 0.51411
I1123 14:50:03.341462 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 14:50:03.341462 25976 solver.cpp:237]     Train net output #1: loss = 0.51411 (* 1 = 0.51411 loss)
I1123 14:50:03.341462 25976 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1123 14:50:07.228957 25976 solver.cpp:218] Iteration 5300 (25.7268 iter/s, 3.887s/100 iters), loss = 0.518846
I1123 14:50:07.228957 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 14:50:07.228957 25976 solver.cpp:237]     Train net output #1: loss = 0.518846 (* 1 = 0.518846 loss)
I1123 14:50:07.228957 25976 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1123 14:50:11.099428 25976 solver.cpp:218] Iteration 5400 (25.8379 iter/s, 3.87028s/100 iters), loss = 0.412611
I1123 14:50:11.099428 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 14:50:11.099428 25976 solver.cpp:237]     Train net output #1: loss = 0.412611 (* 1 = 0.412611 loss)
I1123 14:50:11.099428 25976 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1123 14:50:14.783618 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:50:14.934628 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_5500.caffemodel
I1123 14:50:14.944628 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_5500.solverstate
I1123 14:50:14.948629 25976 solver.cpp:330] Iteration 5500, Testing net (#0)
I1123 14:50:14.948629 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:50:16.019206 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:50:16.061717 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8223
I1123 14:50:16.061717 25976 solver.cpp:397]     Test net output #1: loss = 0.510326 (* 1 = 0.510326 loss)
I1123 14:50:16.098708 25976 solver.cpp:218] Iteration 5500 (20.0046 iter/s, 4.99885s/100 iters), loss = 0.425017
I1123 14:50:16.098708 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 14:50:16.098708 25976 solver.cpp:237]     Train net output #1: loss = 0.425017 (* 1 = 0.425017 loss)
I1123 14:50:16.098708 25976 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1123 14:50:20.143348 25976 solver.cpp:218] Iteration 5600 (24.7283 iter/s, 4.04394s/100 iters), loss = 0.374963
I1123 14:50:20.143348 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 14:50:20.143348 25976 solver.cpp:237]     Train net output #1: loss = 0.374963 (* 1 = 0.374963 loss)
I1123 14:50:20.143348 25976 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1123 14:50:24.007119 25976 solver.cpp:218] Iteration 5700 (25.884 iter/s, 3.86339s/100 iters), loss = 0.440617
I1123 14:50:24.007119 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 14:50:24.007119 25976 solver.cpp:237]     Train net output #1: loss = 0.440617 (* 1 = 0.440617 loss)
I1123 14:50:24.007119 25976 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1123 14:50:27.878403 25976 solver.cpp:218] Iteration 5800 (25.8337 iter/s, 3.87091s/100 iters), loss = 0.480097
I1123 14:50:27.878403 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 14:50:27.878403 25976 solver.cpp:237]     Train net output #1: loss = 0.480097 (* 1 = 0.480097 loss)
I1123 14:50:27.878403 25976 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1123 14:50:31.914629 25976 solver.cpp:218] Iteration 5900 (24.7776 iter/s, 4.0359s/100 iters), loss = 0.37593
I1123 14:50:31.914629 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 14:50:31.914629 25976 solver.cpp:237]     Train net output #1: loss = 0.37593 (* 1 = 0.37593 loss)
I1123 14:50:31.914629 25976 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1123 14:50:35.618899 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:50:35.769911 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_6000.caffemodel
I1123 14:50:35.779907 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_6000.solverstate
I1123 14:50:35.783908 25976 solver.cpp:330] Iteration 6000, Testing net (#0)
I1123 14:50:35.783908 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:50:36.856997 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:50:36.899006 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8298
I1123 14:50:36.899006 25976 solver.cpp:397]     Test net output #1: loss = 0.48765 (* 1 = 0.48765 loss)
I1123 14:50:36.935001 25976 solver.cpp:218] Iteration 6000 (19.9177 iter/s, 5.02065s/100 iters), loss = 0.44453
I1123 14:50:36.935001 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 14:50:36.935001 25976 solver.cpp:237]     Train net output #1: loss = 0.44453 (* 1 = 0.44453 loss)
I1123 14:50:36.935001 25976 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1123 14:50:40.831302 25976 solver.cpp:218] Iteration 6100 (25.6689 iter/s, 3.89576s/100 iters), loss = 0.44456
I1123 14:50:40.831302 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 14:50:40.831302 25976 solver.cpp:237]     Train net output #1: loss = 0.44456 (* 1 = 0.44456 loss)
I1123 14:50:40.831302 25976 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1123 14:50:44.733589 25976 solver.cpp:218] Iteration 6200 (25.6309 iter/s, 3.90154s/100 iters), loss = 0.422256
I1123 14:50:44.733589 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 14:50:44.733589 25976 solver.cpp:237]     Train net output #1: loss = 0.422256 (* 1 = 0.422256 loss)
I1123 14:50:44.733589 25976 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1123 14:50:48.612867 25976 solver.cpp:218] Iteration 6300 (25.7814 iter/s, 3.87876s/100 iters), loss = 0.442477
I1123 14:50:48.612867 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 14:50:48.612867 25976 solver.cpp:237]     Train net output #1: loss = 0.442477 (* 1 = 0.442477 loss)
I1123 14:50:48.612867 25976 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1123 14:50:52.662612 25976 solver.cpp:218] Iteration 6400 (24.695 iter/s, 4.0494s/100 iters), loss = 0.375698
I1123 14:50:52.662612 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 14:50:52.662612 25976 solver.cpp:237]     Train net output #1: loss = 0.375698 (* 1 = 0.375698 loss)
I1123 14:50:52.662612 25976 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1123 14:50:56.349391 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:50:56.501406 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_6500.caffemodel
I1123 14:50:56.511401 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_6500.solverstate
I1123 14:50:56.515401 25976 solver.cpp:330] Iteration 6500, Testing net (#0)
I1123 14:50:56.515401 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:50:57.617491 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:50:57.662495 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8317
I1123 14:50:57.662495 25976 solver.cpp:397]     Test net output #1: loss = 0.483038 (* 1 = 0.483038 loss)
I1123 14:50:57.702498 25976 solver.cpp:218] Iteration 6500 (19.8433 iter/s, 5.03949s/100 iters), loss = 0.39109
I1123 14:50:57.702498 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 14:50:57.702498 25976 solver.cpp:237]     Train net output #1: loss = 0.39109 (* 1 = 0.39109 loss)
I1123 14:50:57.702498 25976 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1123 14:51:01.658809 25976 solver.cpp:218] Iteration 6600 (25.2785 iter/s, 3.95594s/100 iters), loss = 0.321872
I1123 14:51:01.659302 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 14:51:01.659302 25976 solver.cpp:237]     Train net output #1: loss = 0.321872 (* 1 = 0.321872 loss)
I1123 14:51:01.659302 25976 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1123 14:51:05.643095 25976 solver.cpp:218] Iteration 6700 (25.101 iter/s, 3.9839s/100 iters), loss = 0.395229
I1123 14:51:05.643095 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 14:51:05.643095 25976 solver.cpp:237]     Train net output #1: loss = 0.395229 (* 1 = 0.395229 loss)
I1123 14:51:05.643095 25976 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1123 14:51:09.596499 25976 solver.cpp:218] Iteration 6800 (25.2941 iter/s, 3.9535s/100 iters), loss = 0.418319
I1123 14:51:09.597499 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 14:51:09.597499 25976 solver.cpp:237]     Train net output #1: loss = 0.418319 (* 1 = 0.418319 loss)
I1123 14:51:09.597499 25976 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1123 14:51:13.510757 25976 solver.cpp:218] Iteration 6900 (25.5504 iter/s, 3.91384s/100 iters), loss = 0.306745
I1123 14:51:13.510757 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 14:51:13.510757 25976 solver.cpp:237]     Train net output #1: loss = 0.306745 (* 1 = 0.306745 loss)
I1123 14:51:13.510757 25976 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1123 14:51:17.201025 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:51:17.354032 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_7000.caffemodel
I1123 14:51:17.364038 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_7000.solverstate
I1123 14:51:17.367537 25976 solver.cpp:330] Iteration 7000, Testing net (#0)
I1123 14:51:17.368037 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:51:18.475117 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:51:18.521117 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8304
I1123 14:51:18.521117 25976 solver.cpp:397]     Test net output #1: loss = 0.491056 (* 1 = 0.491056 loss)
I1123 14:51:18.561625 25976 solver.cpp:218] Iteration 7000 (19.8031 iter/s, 5.04971s/100 iters), loss = 0.362037
I1123 14:51:18.561625 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:51:18.561625 25976 solver.cpp:237]     Train net output #1: loss = 0.362037 (* 1 = 0.362037 loss)
I1123 14:51:18.561625 25976 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1123 14:51:22.536425 25976 solver.cpp:218] Iteration 7100 (25.1558 iter/s, 3.97523s/100 iters), loss = 0.346725
I1123 14:51:22.537426 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 14:51:22.537426 25976 solver.cpp:237]     Train net output #1: loss = 0.346725 (* 1 = 0.346725 loss)
I1123 14:51:22.537426 25976 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1123 14:51:26.396673 25976 solver.cpp:218] Iteration 7200 (25.914 iter/s, 3.85892s/100 iters), loss = 0.417323
I1123 14:51:26.396673 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 14:51:26.396673 25976 solver.cpp:237]     Train net output #1: loss = 0.417323 (* 1 = 0.417323 loss)
I1123 14:51:26.396673 25976 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1123 14:51:30.264003 25976 solver.cpp:218] Iteration 7300 (25.8548 iter/s, 3.86775s/100 iters), loss = 0.434234
I1123 14:51:30.264003 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 14:51:30.264003 25976 solver.cpp:237]     Train net output #1: loss = 0.434234 (* 1 = 0.434234 loss)
I1123 14:51:30.264003 25976 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1123 14:51:34.296738 25976 solver.cpp:218] Iteration 7400 (24.8033 iter/s, 4.03172s/100 iters), loss = 0.299583
I1123 14:51:34.296738 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:51:34.296738 25976 solver.cpp:237]     Train net output #1: loss = 0.299583 (* 1 = 0.299583 loss)
I1123 14:51:34.296738 25976 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1123 14:51:37.966042 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:51:38.117067 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_7500.caffemodel
I1123 14:51:38.127058 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_7500.solverstate
I1123 14:51:38.131057 25976 solver.cpp:330] Iteration 7500, Testing net (#0)
I1123 14:51:38.131057 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:51:39.200649 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:51:39.242153 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8241
I1123 14:51:39.242153 25976 solver.cpp:397]     Test net output #1: loss = 0.497494 (* 1 = 0.497494 loss)
I1123 14:51:39.279152 25976 solver.cpp:218] Iteration 7500 (20.0707 iter/s, 4.98238s/100 iters), loss = 0.400379
I1123 14:51:39.279152 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 14:51:39.279152 25976 solver.cpp:237]     Train net output #1: loss = 0.400379 (* 1 = 0.400379 loss)
I1123 14:51:39.279152 25976 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1123 14:51:43.326723 25976 solver.cpp:218] Iteration 7600 (24.7073 iter/s, 4.04739s/100 iters), loss = 0.329243
I1123 14:51:43.326723 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 14:51:43.326723 25976 solver.cpp:237]     Train net output #1: loss = 0.329243 (* 1 = 0.329243 loss)
I1123 14:51:43.326723 25976 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1123 14:51:47.197978 25976 solver.cpp:218] Iteration 7700 (25.8391 iter/s, 3.87011s/100 iters), loss = 0.353639
I1123 14:51:47.197978 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 14:51:47.197978 25976 solver.cpp:237]     Train net output #1: loss = 0.353639 (* 1 = 0.353639 loss)
I1123 14:51:47.197978 25976 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1123 14:51:51.070317 25976 solver.cpp:218] Iteration 7800 (25.8233 iter/s, 3.87247s/100 iters), loss = 0.49874
I1123 14:51:51.070317 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 14:51:51.070317 25976 solver.cpp:237]     Train net output #1: loss = 0.49874 (* 1 = 0.49874 loss)
I1123 14:51:51.070317 25976 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1123 14:51:55.099637 25976 solver.cpp:218] Iteration 7900 (24.8187 iter/s, 4.02922s/100 iters), loss = 0.292393
I1123 14:51:55.100638 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:51:55.100638 25976 solver.cpp:237]     Train net output #1: loss = 0.292393 (* 1 = 0.292393 loss)
I1123 14:51:55.100638 25976 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1123 14:51:58.771363 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:51:58.922888 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_8000.caffemodel
I1123 14:51:58.932881 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_8000.solverstate
I1123 14:51:58.936882 25976 solver.cpp:330] Iteration 8000, Testing net (#0)
I1123 14:51:58.936882 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:52:00.004978 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:52:00.047976 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8388
I1123 14:52:00.047976 25976 solver.cpp:397]     Test net output #1: loss = 0.465454 (* 1 = 0.465454 loss)
I1123 14:52:00.084980 25976 solver.cpp:218] Iteration 8000 (20.0643 iter/s, 4.98399s/100 iters), loss = 0.377583
I1123 14:52:00.084980 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 14:52:00.084980 25976 solver.cpp:237]     Train net output #1: loss = 0.377583 (* 1 = 0.377583 loss)
I1123 14:52:00.084980 25976 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1123 14:52:04.001328 25976 solver.cpp:218] Iteration 8100 (25.536 iter/s, 3.91604s/100 iters), loss = 0.32234
I1123 14:52:04.001328 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:52:04.001328 25976 solver.cpp:237]     Train net output #1: loss = 0.32234 (* 1 = 0.32234 loss)
I1123 14:52:04.001328 25976 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1123 14:52:07.864598 25976 solver.cpp:218] Iteration 8200 (25.8821 iter/s, 3.86368s/100 iters), loss = 0.375823
I1123 14:52:07.864598 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 14:52:07.864598 25976 solver.cpp:237]     Train net output #1: loss = 0.375823 (* 1 = 0.375823 loss)
I1123 14:52:07.864598 25976 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1123 14:52:11.721915 25976 solver.cpp:218] Iteration 8300 (25.9286 iter/s, 3.85674s/100 iters), loss = 0.447496
I1123 14:52:11.721915 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 14:52:11.721915 25976 solver.cpp:237]     Train net output #1: loss = 0.447496 (* 1 = 0.447496 loss)
I1123 14:52:11.721915 25976 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1123 14:52:15.585250 25976 solver.cpp:218] Iteration 8400 (25.8888 iter/s, 3.86267s/100 iters), loss = 0.328526
I1123 14:52:15.585250 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 14:52:15.585250 25976 solver.cpp:237]     Train net output #1: loss = 0.328526 (* 1 = 0.328526 loss)
I1123 14:52:15.585250 25976 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1123 14:52:19.360797 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:52:19.520807 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_8500.caffemodel
I1123 14:52:19.532817 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_8500.solverstate
I1123 14:52:19.536809 25976 solver.cpp:330] Iteration 8500, Testing net (#0)
I1123 14:52:19.536809 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:52:20.628895 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:52:20.670398 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8294
I1123 14:52:20.670398 25976 solver.cpp:397]     Test net output #1: loss = 0.485184 (* 1 = 0.485184 loss)
I1123 14:52:20.707901 25976 solver.cpp:218] Iteration 8500 (19.5203 iter/s, 5.12288s/100 iters), loss = 0.295387
I1123 14:52:20.708901 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:52:20.708901 25976 solver.cpp:237]     Train net output #1: loss = 0.295387 (* 1 = 0.295387 loss)
I1123 14:52:20.708901 25976 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1123 14:52:24.585443 25976 solver.cpp:218] Iteration 8600 (25.7973 iter/s, 3.87638s/100 iters), loss = 0.266428
I1123 14:52:24.585443 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:52:24.585443 25976 solver.cpp:237]     Train net output #1: loss = 0.266428 (* 1 = 0.266428 loss)
I1123 14:52:24.585443 25976 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1123 14:52:28.447865 25976 solver.cpp:218] Iteration 8700 (25.892 iter/s, 3.86219s/100 iters), loss = 0.405946
I1123 14:52:28.447865 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 14:52:28.447865 25976 solver.cpp:237]     Train net output #1: loss = 0.405946 (* 1 = 0.405946 loss)
I1123 14:52:28.447865 25976 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1123 14:52:32.453325 25976 solver.cpp:218] Iteration 8800 (24.9675 iter/s, 4.00521s/100 iters), loss = 0.403798
I1123 14:52:32.453325 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 14:52:32.453325 25976 solver.cpp:237]     Train net output #1: loss = 0.403798 (* 1 = 0.403798 loss)
I1123 14:52:32.453325 25976 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1123 14:52:36.359493 25976 solver.cpp:218] Iteration 8900 (25.6048 iter/s, 3.90551s/100 iters), loss = 0.303929
I1123 14:52:36.359493 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:52:36.359493 25976 solver.cpp:237]     Train net output #1: loss = 0.303929 (* 1 = 0.303929 loss)
I1123 14:52:36.359493 25976 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1123 14:52:40.044203 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:52:40.195714 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_9000.caffemodel
I1123 14:52:40.205713 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_9000.solverstate
I1123 14:52:40.209714 25976 solver.cpp:330] Iteration 9000, Testing net (#0)
I1123 14:52:40.209714 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:52:41.314813 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:52:41.357317 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8339
I1123 14:52:41.357317 25976 solver.cpp:397]     Test net output #1: loss = 0.483498 (* 1 = 0.483498 loss)
I1123 14:52:41.393822 25976 solver.cpp:218] Iteration 9000 (19.8644 iter/s, 5.03413s/100 iters), loss = 0.351282
I1123 14:52:41.393822 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 14:52:41.393822 25976 solver.cpp:237]     Train net output #1: loss = 0.351282 (* 1 = 0.351282 loss)
I1123 14:52:41.393822 25976 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1123 14:52:45.259146 25976 solver.cpp:218] Iteration 9100 (25.8731 iter/s, 3.86501s/100 iters), loss = 0.365062
I1123 14:52:45.259146 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 14:52:45.259146 25976 solver.cpp:237]     Train net output #1: loss = 0.365062 (* 1 = 0.365062 loss)
I1123 14:52:45.259146 25976 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1123 14:52:49.119457 25976 solver.cpp:218] Iteration 9200 (25.9078 iter/s, 3.85983s/100 iters), loss = 0.33756
I1123 14:52:49.119457 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 14:52:49.119457 25976 solver.cpp:237]     Train net output #1: loss = 0.33756 (* 1 = 0.33756 loss)
I1123 14:52:49.119457 25976 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1123 14:52:52.979745 25976 solver.cpp:218] Iteration 9300 (25.9108 iter/s, 3.85939s/100 iters), loss = 0.374454
I1123 14:52:52.979745 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:52:52.979745 25976 solver.cpp:237]     Train net output #1: loss = 0.374454 (* 1 = 0.374454 loss)
I1123 14:52:52.979745 25976 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1123 14:52:56.842010 25976 solver.cpp:218] Iteration 9400 (25.8943 iter/s, 3.86185s/100 iters), loss = 0.210177
I1123 14:52:56.842010 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 14:52:56.842010 25976 solver.cpp:237]     Train net output #1: loss = 0.210177 (* 1 = 0.210177 loss)
I1123 14:52:56.842010 25976 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1123 14:53:00.528270 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:53:00.680284 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_9500.caffemodel
I1123 14:53:00.693284 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_9500.solverstate
I1123 14:53:00.697283 25976 solver.cpp:330] Iteration 9500, Testing net (#0)
I1123 14:53:00.697283 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:53:01.768400 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:53:01.810400 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8422
I1123 14:53:01.810400 25976 solver.cpp:397]     Test net output #1: loss = 0.45233 (* 1 = 0.45233 loss)
I1123 14:53:01.847400 25976 solver.cpp:218] Iteration 9500 (19.98 iter/s, 5.005s/100 iters), loss = 0.306911
I1123 14:53:01.847400 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:53:01.847400 25976 solver.cpp:237]     Train net output #1: loss = 0.306911 (* 1 = 0.306911 loss)
I1123 14:53:01.847400 25976 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1123 14:53:01.847400 25976 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1123 14:53:05.710741 25976 solver.cpp:218] Iteration 9600 (25.885 iter/s, 3.86324s/100 iters), loss = 0.271582
I1123 14:53:05.710741 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 14:53:05.710741 25976 solver.cpp:237]     Train net output #1: loss = 0.271582 (* 1 = 0.271582 loss)
I1123 14:53:05.710741 25976 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1123 14:53:09.576056 25976 solver.cpp:218] Iteration 9700 (25.8709 iter/s, 3.86534s/100 iters), loss = 0.326394
I1123 14:53:09.576056 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 14:53:09.577057 25976 solver.cpp:237]     Train net output #1: loss = 0.326394 (* 1 = 0.326394 loss)
I1123 14:53:09.577057 25976 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1123 14:53:13.442235 25976 solver.cpp:218] Iteration 9800 (25.8726 iter/s, 3.86509s/100 iters), loss = 0.331174
I1123 14:53:13.442235 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:53:13.442235 25976 solver.cpp:237]     Train net output #1: loss = 0.331174 (* 1 = 0.331174 loss)
I1123 14:53:13.442235 25976 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1123 14:53:17.355593 25976 solver.cpp:218] Iteration 9900 (25.5577 iter/s, 3.91272s/100 iters), loss = 0.294023
I1123 14:53:17.355593 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:53:17.355593 25976 solver.cpp:237]     Train net output #1: loss = 0.294023 (* 1 = 0.294023 loss)
I1123 14:53:17.355593 25976 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1123 14:53:21.053881 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:53:21.205885 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_10000.caffemodel
I1123 14:53:21.215884 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_10000.solverstate
I1123 14:53:21.219885 25976 solver.cpp:330] Iteration 10000, Testing net (#0)
I1123 14:53:21.219885 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:53:22.287873 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:53:22.330873 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8521
I1123 14:53:22.330873 25976 solver.cpp:397]     Test net output #1: loss = 0.42845 (* 1 = 0.42845 loss)
I1123 14:53:22.368880 25976 solver.cpp:218] Iteration 10000 (19.9477 iter/s, 5.01311s/100 iters), loss = 0.246888
I1123 14:53:22.368880 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:53:22.368880 25976 solver.cpp:237]     Train net output #1: loss = 0.246888 (* 1 = 0.246888 loss)
I1123 14:53:22.368880 25976 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1123 14:53:26.265156 25976 solver.cpp:218] Iteration 10100 (25.6642 iter/s, 3.89649s/100 iters), loss = 0.316675
I1123 14:53:26.266156 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:53:26.266156 25976 solver.cpp:237]     Train net output #1: loss = 0.316675 (* 1 = 0.316675 loss)
I1123 14:53:26.266156 25976 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1123 14:53:30.123395 25976 solver.cpp:218] Iteration 10200 (25.9241 iter/s, 3.85742s/100 iters), loss = 0.337508
I1123 14:53:30.123395 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:53:30.123395 25976 solver.cpp:237]     Train net output #1: loss = 0.337508 (* 1 = 0.337508 loss)
I1123 14:53:30.123395 25976 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1123 14:53:33.974676 25976 solver.cpp:218] Iteration 10300 (25.9665 iter/s, 3.85112s/100 iters), loss = 0.408959
I1123 14:53:33.974676 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 14:53:33.974676 25976 solver.cpp:237]     Train net output #1: loss = 0.408959 (* 1 = 0.408959 loss)
I1123 14:53:33.974676 25976 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1123 14:53:37.848412 25976 solver.cpp:218] Iteration 10400 (25.8211 iter/s, 3.8728s/100 iters), loss = 0.277736
I1123 14:53:37.848412 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 14:53:37.848412 25976 solver.cpp:237]     Train net output #1: loss = 0.277736 (* 1 = 0.277736 loss)
I1123 14:53:37.848412 25976 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1123 14:53:41.514174 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:53:41.665186 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_10500.caffemodel
I1123 14:53:41.674185 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_10500.solverstate
I1123 14:53:41.678186 25976 solver.cpp:330] Iteration 10500, Testing net (#0)
I1123 14:53:41.678186 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:53:42.746765 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:53:42.788277 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8523
I1123 14:53:42.788277 25976 solver.cpp:397]     Test net output #1: loss = 0.42726 (* 1 = 0.42726 loss)
I1123 14:53:42.825268 25976 solver.cpp:218] Iteration 10500 (20.0935 iter/s, 4.97673s/100 iters), loss = 0.329537
I1123 14:53:42.825268 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 14:53:42.825268 25976 solver.cpp:237]     Train net output #1: loss = 0.329537 (* 1 = 0.329537 loss)
I1123 14:53:42.825268 25976 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1123 14:53:46.694514 25976 solver.cpp:218] Iteration 10600 (25.8451 iter/s, 3.86921s/100 iters), loss = 0.270775
I1123 14:53:46.694514 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:53:46.694514 25976 solver.cpp:237]     Train net output #1: loss = 0.270774 (* 1 = 0.270774 loss)
I1123 14:53:46.694514 25976 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1123 14:53:50.552789 25976 solver.cpp:218] Iteration 10700 (25.921 iter/s, 3.85788s/100 iters), loss = 0.285537
I1123 14:53:50.552789 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:53:50.552789 25976 solver.cpp:237]     Train net output #1: loss = 0.285537 (* 1 = 0.285537 loss)
I1123 14:53:50.552789 25976 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1123 14:53:54.410064 25976 solver.cpp:218] Iteration 10800 (25.9282 iter/s, 3.8568s/100 iters), loss = 0.319307
I1123 14:53:54.410064 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:53:54.410064 25976 solver.cpp:237]     Train net output #1: loss = 0.319307 (* 1 = 0.319307 loss)
I1123 14:53:54.410064 25976 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1123 14:53:58.276449 25976 solver.cpp:218] Iteration 10900 (25.866 iter/s, 3.86608s/100 iters), loss = 0.274509
I1123 14:53:58.276449 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 14:53:58.276449 25976 solver.cpp:237]     Train net output #1: loss = 0.274509 (* 1 = 0.274509 loss)
I1123 14:53:58.276449 25976 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1123 14:54:01.945713 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:54:02.096719 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_11000.caffemodel
I1123 14:54:02.105720 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_11000.solverstate
I1123 14:54:02.109719 25976 solver.cpp:330] Iteration 11000, Testing net (#0)
I1123 14:54:02.109719 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:54:03.176813 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:54:03.218803 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8535
I1123 14:54:03.218803 25976 solver.cpp:397]     Test net output #1: loss = 0.425954 (* 1 = 0.425954 loss)
I1123 14:54:03.255808 25976 solver.cpp:218] Iteration 11000 (20.0869 iter/s, 4.97838s/100 iters), loss = 0.255176
I1123 14:54:03.255808 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 14:54:03.255808 25976 solver.cpp:237]     Train net output #1: loss = 0.255176 (* 1 = 0.255176 loss)
I1123 14:54:03.255808 25976 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1123 14:54:07.119071 25976 solver.cpp:218] Iteration 11100 (25.8863 iter/s, 3.86305s/100 iters), loss = 0.250214
I1123 14:54:07.119071 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:54:07.119071 25976 solver.cpp:237]     Train net output #1: loss = 0.250214 (* 1 = 0.250214 loss)
I1123 14:54:07.119071 25976 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1123 14:54:11.004400 25976 solver.cpp:218] Iteration 11200 (25.7368 iter/s, 3.88549s/100 iters), loss = 0.302552
I1123 14:54:11.004400 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:54:11.004400 25976 solver.cpp:237]     Train net output #1: loss = 0.302552 (* 1 = 0.302552 loss)
I1123 14:54:11.004400 25976 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1123 14:54:14.891710 25976 solver.cpp:218] Iteration 11300 (25.7284 iter/s, 3.88675s/100 iters), loss = 0.344733
I1123 14:54:14.891710 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 14:54:14.891710 25976 solver.cpp:237]     Train net output #1: loss = 0.344733 (* 1 = 0.344733 loss)
I1123 14:54:14.891710 25976 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1123 14:54:18.798122 25976 solver.cpp:218] Iteration 11400 (25.5999 iter/s, 3.90627s/100 iters), loss = 0.253073
I1123 14:54:18.798122 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:54:18.798122 25976 solver.cpp:237]     Train net output #1: loss = 0.253072 (* 1 = 0.253072 loss)
I1123 14:54:18.798122 25976 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1123 14:54:22.512466 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:54:22.667477 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_11500.caffemodel
I1123 14:54:22.677477 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_11500.solverstate
I1123 14:54:22.680477 25976 solver.cpp:330] Iteration 11500, Testing net (#0)
I1123 14:54:22.681478 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:54:23.755563 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:54:23.797567 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8534
I1123 14:54:23.797567 25976 solver.cpp:397]     Test net output #1: loss = 0.426357 (* 1 = 0.426357 loss)
I1123 14:54:23.834565 25976 solver.cpp:218] Iteration 11500 (19.8598 iter/s, 5.03531s/100 iters), loss = 0.322361
I1123 14:54:23.834565 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 14:54:23.834565 25976 solver.cpp:237]     Train net output #1: loss = 0.32236 (* 1 = 0.32236 loss)
I1123 14:54:23.834565 25976 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1123 14:54:27.697794 25976 solver.cpp:218] Iteration 11600 (25.8838 iter/s, 3.86341s/100 iters), loss = 0.243634
I1123 14:54:27.697794 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:54:27.697794 25976 solver.cpp:237]     Train net output #1: loss = 0.243634 (* 1 = 0.243634 loss)
I1123 14:54:27.697794 25976 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1123 14:54:31.569113 25976 solver.cpp:218] Iteration 11700 (25.8332 iter/s, 3.87098s/100 iters), loss = 0.310322
I1123 14:54:31.569113 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 14:54:31.569113 25976 solver.cpp:237]     Train net output #1: loss = 0.310322 (* 1 = 0.310322 loss)
I1123 14:54:31.569113 25976 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1123 14:54:35.424417 25976 solver.cpp:218] Iteration 11800 (25.9424 iter/s, 3.8547s/100 iters), loss = 0.354323
I1123 14:54:35.424417 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 14:54:35.424417 25976 solver.cpp:237]     Train net output #1: loss = 0.354323 (* 1 = 0.354323 loss)
I1123 14:54:35.424417 25976 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1123 14:54:39.283725 25976 solver.cpp:218] Iteration 11900 (25.9171 iter/s, 3.85845s/100 iters), loss = 0.221411
I1123 14:54:39.283725 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:54:39.283725 25976 solver.cpp:237]     Train net output #1: loss = 0.221411 (* 1 = 0.221411 loss)
I1123 14:54:39.283725 25976 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1123 14:54:42.953492 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:54:43.105002 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_12000.caffemodel
I1123 14:54:43.115002 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_12000.solverstate
I1123 14:54:43.119002 25976 solver.cpp:330] Iteration 12000, Testing net (#0)
I1123 14:54:43.119002 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:54:44.185087 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:54:44.227087 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8527
I1123 14:54:44.227087 25976 solver.cpp:397]     Test net output #1: loss = 0.426772 (* 1 = 0.426772 loss)
I1123 14:54:44.264109 25976 solver.cpp:218] Iteration 12000 (20.0796 iter/s, 4.98017s/100 iters), loss = 0.278408
I1123 14:54:44.264109 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 14:54:44.264109 25976 solver.cpp:237]     Train net output #1: loss = 0.278408 (* 1 = 0.278408 loss)
I1123 14:54:44.264109 25976 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1123 14:54:48.111359 25976 solver.cpp:218] Iteration 12100 (25.9943 iter/s, 3.847s/100 iters), loss = 0.316724
I1123 14:54:48.111359 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 14:54:48.111359 25976 solver.cpp:237]     Train net output #1: loss = 0.316724 (* 1 = 0.316724 loss)
I1123 14:54:48.111359 25976 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1123 14:54:51.961585 25976 solver.cpp:218] Iteration 12200 (25.9769 iter/s, 3.84958s/100 iters), loss = 0.293945
I1123 14:54:51.961585 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:54:51.961585 25976 solver.cpp:237]     Train net output #1: loss = 0.293945 (* 1 = 0.293945 loss)
I1123 14:54:51.961585 25976 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1123 14:54:55.814832 25976 solver.cpp:218] Iteration 12300 (25.9514 iter/s, 3.85336s/100 iters), loss = 0.276177
I1123 14:54:55.814832 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:54:55.814832 25976 solver.cpp:237]     Train net output #1: loss = 0.276176 (* 1 = 0.276176 loss)
I1123 14:54:55.814832 25976 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1123 14:54:59.676090 25976 solver.cpp:218] Iteration 12400 (25.9039 iter/s, 3.86042s/100 iters), loss = 0.266475
I1123 14:54:59.676090 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:54:59.676090 25976 solver.cpp:237]     Train net output #1: loss = 0.266475 (* 1 = 0.266475 loss)
I1123 14:54:59.676090 25976 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1123 14:55:03.349860 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:55:03.502372 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_12500.caffemodel
I1123 14:55:03.512372 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_12500.solverstate
I1123 14:55:03.516373 25976 solver.cpp:330] Iteration 12500, Testing net (#0)
I1123 14:55:03.516373 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:55:04.582495 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:55:04.624495 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8537
I1123 14:55:04.624495 25976 solver.cpp:397]     Test net output #1: loss = 0.423062 (* 1 = 0.423062 loss)
I1123 14:55:04.661499 25976 solver.cpp:218] Iteration 12500 (20.0592 iter/s, 4.98525s/100 iters), loss = 0.257164
I1123 14:55:04.661499 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:55:04.661499 25976 solver.cpp:237]     Train net output #1: loss = 0.257164 (* 1 = 0.257164 loss)
I1123 14:55:04.661499 25976 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1123 14:55:08.523764 25976 solver.cpp:218] Iteration 12600 (25.8932 iter/s, 3.86202s/100 iters), loss = 0.302218
I1123 14:55:08.523764 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:55:08.523764 25976 solver.cpp:237]     Train net output #1: loss = 0.302218 (* 1 = 0.302218 loss)
I1123 14:55:08.523764 25976 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1123 14:55:12.381043 25976 solver.cpp:218] Iteration 12700 (25.9249 iter/s, 3.85729s/100 iters), loss = 0.268475
I1123 14:55:12.381043 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:55:12.381043 25976 solver.cpp:237]     Train net output #1: loss = 0.268475 (* 1 = 0.268475 loss)
I1123 14:55:12.381043 25976 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1123 14:55:16.241288 25976 solver.cpp:218] Iteration 12800 (25.9111 iter/s, 3.85935s/100 iters), loss = 0.310091
I1123 14:55:16.241288 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:55:16.241288 25976 solver.cpp:237]     Train net output #1: loss = 0.31009 (* 1 = 0.31009 loss)
I1123 14:55:16.241288 25976 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1123 14:55:20.098559 25976 solver.cpp:218] Iteration 12900 (25.9271 iter/s, 3.85697s/100 iters), loss = 0.229806
I1123 14:55:20.098559 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 14:55:20.098559 25976 solver.cpp:237]     Train net output #1: loss = 0.229806 (* 1 = 0.229806 loss)
I1123 14:55:20.098559 25976 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1123 14:55:23.769843 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:55:23.919850 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_13000.caffemodel
I1123 14:55:23.929850 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_13000.solverstate
I1123 14:55:23.933851 25976 solver.cpp:330] Iteration 13000, Testing net (#0)
I1123 14:55:23.933851 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:55:24.999989 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:55:25.041996 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8519
I1123 14:55:25.041996 25976 solver.cpp:397]     Test net output #1: loss = 0.427387 (* 1 = 0.427387 loss)
I1123 14:55:25.078995 25976 solver.cpp:218] Iteration 13000 (20.0795 iter/s, 4.9802s/100 iters), loss = 0.284392
I1123 14:55:25.078995 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 14:55:25.078995 25976 solver.cpp:237]     Train net output #1: loss = 0.284392 (* 1 = 0.284392 loss)
I1123 14:55:25.078995 25976 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1123 14:55:28.943307 25976 solver.cpp:218] Iteration 13100 (25.8781 iter/s, 3.86427s/100 iters), loss = 0.255816
I1123 14:55:28.943307 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:55:28.943307 25976 solver.cpp:237]     Train net output #1: loss = 0.255816 (* 1 = 0.255816 loss)
I1123 14:55:28.943307 25976 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1123 14:55:32.804565 25976 solver.cpp:218] Iteration 13200 (25.9048 iter/s, 3.8603s/100 iters), loss = 0.310138
I1123 14:55:32.804565 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:55:32.804565 25976 solver.cpp:237]     Train net output #1: loss = 0.310137 (* 1 = 0.310137 loss)
I1123 14:55:32.804565 25976 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1123 14:55:36.667837 25976 solver.cpp:218] Iteration 13300 (25.8868 iter/s, 3.86297s/100 iters), loss = 0.299401
I1123 14:55:36.667837 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:55:36.667837 25976 solver.cpp:237]     Train net output #1: loss = 0.299401 (* 1 = 0.299401 loss)
I1123 14:55:36.667837 25976 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1123 14:55:40.531095 25976 solver.cpp:218] Iteration 13400 (25.8888 iter/s, 3.86268s/100 iters), loss = 0.266972
I1123 14:55:40.531095 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 14:55:40.531095 25976 solver.cpp:237]     Train net output #1: loss = 0.266972 (* 1 = 0.266972 loss)
I1123 14:55:40.531095 25976 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1123 14:55:44.211374 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:55:44.363391 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_13500.caffemodel
I1123 14:55:44.373389 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_13500.solverstate
I1123 14:55:44.377390 25976 solver.cpp:330] Iteration 13500, Testing net (#0)
I1123 14:55:44.377390 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:55:45.444483 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:55:45.486488 25976 solver.cpp:397]     Test net output #0: accuracy = 0.855
I1123 14:55:45.486488 25976 solver.cpp:397]     Test net output #1: loss = 0.424574 (* 1 = 0.424574 loss)
I1123 14:55:45.523488 25976 solver.cpp:218] Iteration 13500 (20.0292 iter/s, 4.99271s/100 iters), loss = 0.237128
I1123 14:55:45.523488 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:55:45.523488 25976 solver.cpp:237]     Train net output #1: loss = 0.237128 (* 1 = 0.237128 loss)
I1123 14:55:45.523488 25976 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1123 14:55:49.388780 25976 solver.cpp:218] Iteration 13600 (25.8736 iter/s, 3.86494s/100 iters), loss = 0.21034
I1123 14:55:49.388780 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 14:55:49.388780 25976 solver.cpp:237]     Train net output #1: loss = 0.21034 (* 1 = 0.21034 loss)
I1123 14:55:49.388780 25976 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1123 14:55:53.245028 25976 solver.cpp:218] Iteration 13700 (25.9384 iter/s, 3.85528s/100 iters), loss = 0.337116
I1123 14:55:53.245028 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:55:53.245028 25976 solver.cpp:237]     Train net output #1: loss = 0.337116 (* 1 = 0.337116 loss)
I1123 14:55:53.245028 25976 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1123 14:55:57.109277 25976 solver.cpp:218] Iteration 13800 (25.8793 iter/s, 3.86409s/100 iters), loss = 0.323414
I1123 14:55:57.109277 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:55:57.109277 25976 solver.cpp:237]     Train net output #1: loss = 0.323414 (* 1 = 0.323414 loss)
I1123 14:55:57.109277 25976 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1123 14:56:00.976554 25976 solver.cpp:218] Iteration 13900 (25.8583 iter/s, 3.86722s/100 iters), loss = 0.232596
I1123 14:56:00.976554 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:56:00.976554 25976 solver.cpp:237]     Train net output #1: loss = 0.232595 (* 1 = 0.232595 loss)
I1123 14:56:00.976554 25976 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1123 14:56:04.643784 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:56:04.793802 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_14000.caffemodel
I1123 14:56:04.803794 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_14000.solverstate
I1123 14:56:04.807795 25976 solver.cpp:330] Iteration 14000, Testing net (#0)
I1123 14:56:04.807795 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:56:05.872777 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:56:05.914779 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8555
I1123 14:56:05.914779 25976 solver.cpp:397]     Test net output #1: loss = 0.42231 (* 1 = 0.42231 loss)
I1123 14:56:05.951778 25976 solver.cpp:218] Iteration 14000 (20.1024 iter/s, 4.97452s/100 iters), loss = 0.215638
I1123 14:56:05.952280 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 14:56:05.952280 25976 solver.cpp:237]     Train net output #1: loss = 0.215638 (* 1 = 0.215638 loss)
I1123 14:56:05.952280 25976 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1123 14:56:09.804008 25976 solver.cpp:218] Iteration 14100 (25.9585 iter/s, 3.8523s/100 iters), loss = 0.248191
I1123 14:56:09.804008 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:56:09.804008 25976 solver.cpp:237]     Train net output #1: loss = 0.248191 (* 1 = 0.248191 loss)
I1123 14:56:09.805008 25976 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1123 14:56:13.661798 25976 solver.cpp:218] Iteration 14200 (25.9305 iter/s, 3.85646s/100 iters), loss = 0.327642
I1123 14:56:13.661798 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:56:13.661798 25976 solver.cpp:237]     Train net output #1: loss = 0.327641 (* 1 = 0.327641 loss)
I1123 14:56:13.661798 25976 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1123 14:56:17.525241 25976 solver.cpp:218] Iteration 14300 (25.8833 iter/s, 3.8635s/100 iters), loss = 0.296276
I1123 14:56:17.525241 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 14:56:17.525241 25976 solver.cpp:237]     Train net output #1: loss = 0.296276 (* 1 = 0.296276 loss)
I1123 14:56:17.525241 25976 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1123 14:56:21.380198 25976 solver.cpp:218] Iteration 14400 (25.9447 iter/s, 3.85435s/100 iters), loss = 0.254095
I1123 14:56:21.380198 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:56:21.380198 25976 solver.cpp:237]     Train net output #1: loss = 0.254094 (* 1 = 0.254094 loss)
I1123 14:56:21.380198 25976 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1123 14:56:25.053956 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:56:25.205473 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_14500.caffemodel
I1123 14:56:25.214464 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_14500.solverstate
I1123 14:56:25.218464 25976 solver.cpp:330] Iteration 14500, Testing net (#0)
I1123 14:56:25.218464 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:56:26.285544 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:56:26.327544 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8552
I1123 14:56:26.327544 25976 solver.cpp:397]     Test net output #1: loss = 0.422601 (* 1 = 0.422601 loss)
I1123 14:56:26.365557 25976 solver.cpp:218] Iteration 14500 (20.0609 iter/s, 4.98482s/100 iters), loss = 0.302552
I1123 14:56:26.365557 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 14:56:26.365557 25976 solver.cpp:237]     Train net output #1: loss = 0.302552 (* 1 = 0.302552 loss)
I1123 14:56:26.365557 25976 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1123 14:56:30.231828 25976 solver.cpp:218] Iteration 14600 (25.8634 iter/s, 3.86646s/100 iters), loss = 0.225149
I1123 14:56:30.231828 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 14:56:30.231828 25976 solver.cpp:237]     Train net output #1: loss = 0.225149 (* 1 = 0.225149 loss)
I1123 14:56:30.231828 25976 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1123 14:56:34.094080 25976 solver.cpp:218] Iteration 14700 (25.8971 iter/s, 3.86144s/100 iters), loss = 0.284256
I1123 14:56:34.094080 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:56:34.094080 25976 solver.cpp:237]     Train net output #1: loss = 0.284256 (* 1 = 0.284256 loss)
I1123 14:56:34.094080 25976 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1123 14:56:37.952338 25976 solver.cpp:218] Iteration 14800 (25.9167 iter/s, 3.85851s/100 iters), loss = 0.342125
I1123 14:56:37.952338 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:56:37.952338 25976 solver.cpp:237]     Train net output #1: loss = 0.342125 (* 1 = 0.342125 loss)
I1123 14:56:37.952338 25976 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1123 14:56:41.806602 25976 solver.cpp:218] Iteration 14900 (25.9472 iter/s, 3.85398s/100 iters), loss = 0.277084
I1123 14:56:41.807602 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:56:41.807602 25976 solver.cpp:237]     Train net output #1: loss = 0.277083 (* 1 = 0.277083 loss)
I1123 14:56:41.807602 25976 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1123 14:56:45.480842 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:56:45.632853 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_15000.caffemodel
I1123 14:56:45.643852 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_15000.solverstate
I1123 14:56:45.647855 25976 solver.cpp:330] Iteration 15000, Testing net (#0)
I1123 14:56:45.647855 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:56:46.714452 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:56:46.755954 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8542
I1123 14:56:46.755954 25976 solver.cpp:397]     Test net output #1: loss = 0.423835 (* 1 = 0.423835 loss)
I1123 14:56:46.793954 25976 solver.cpp:218] Iteration 15000 (20.0565 iter/s, 4.98592s/100 iters), loss = 0.245192
I1123 14:56:46.793954 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:56:46.793954 25976 solver.cpp:237]     Train net output #1: loss = 0.245192 (* 1 = 0.245192 loss)
I1123 14:56:46.793954 25976 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1123 14:56:50.665288 25976 solver.cpp:218] Iteration 15100 (25.8275 iter/s, 3.87184s/100 iters), loss = 0.2458
I1123 14:56:50.665288 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 14:56:50.665288 25976 solver.cpp:237]     Train net output #1: loss = 0.2458 (* 1 = 0.2458 loss)
I1123 14:56:50.665288 25976 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1123 14:56:54.523602 25976 solver.cpp:218] Iteration 15200 (25.9219 iter/s, 3.85774s/100 iters), loss = 0.308594
I1123 14:56:54.523602 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 14:56:54.523602 25976 solver.cpp:237]     Train net output #1: loss = 0.308594 (* 1 = 0.308594 loss)
I1123 14:56:54.523602 25976 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1123 14:56:58.384382 25976 solver.cpp:218] Iteration 15300 (25.9062 iter/s, 3.86008s/100 iters), loss = 0.341741
I1123 14:56:58.384382 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 14:56:58.384382 25976 solver.cpp:237]     Train net output #1: loss = 0.341741 (* 1 = 0.341741 loss)
I1123 14:56:58.384382 25976 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1123 14:56:58.384382 25976 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1123 14:57:02.251346 25976 solver.cpp:218] Iteration 15400 (25.8596 iter/s, 3.86703s/100 iters), loss = 0.255692
I1123 14:57:02.251346 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:57:02.251346 25976 solver.cpp:237]     Train net output #1: loss = 0.255692 (* 1 = 0.255692 loss)
I1123 14:57:02.251346 25976 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1123 14:57:05.932611 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:57:06.084120 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_15500.caffemodel
I1123 14:57:06.096120 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_15500.solverstate
I1123 14:57:06.099624 25976 solver.cpp:330] Iteration 15500, Testing net (#0)
I1123 14:57:06.099624 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:57:07.167688 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:57:07.209693 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8554
I1123 14:57:07.209693 25976 solver.cpp:397]     Test net output #1: loss = 0.423832 (* 1 = 0.423832 loss)
I1123 14:57:07.246693 25976 solver.cpp:218] Iteration 15500 (20.0201 iter/s, 4.99499s/100 iters), loss = 0.239403
I1123 14:57:07.246693 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:57:07.246693 25976 solver.cpp:237]     Train net output #1: loss = 0.239403 (* 1 = 0.239403 loss)
I1123 14:57:07.246693 25976 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1123 14:57:11.108022 25976 solver.cpp:218] Iteration 15600 (25.9063 iter/s, 3.86007s/100 iters), loss = 0.25219
I1123 14:57:11.108022 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 14:57:11.108022 25976 solver.cpp:237]     Train net output #1: loss = 0.252189 (* 1 = 0.252189 loss)
I1123 14:57:11.108022 25976 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1123 14:57:14.972427 25976 solver.cpp:218] Iteration 15700 (25.8795 iter/s, 3.86406s/100 iters), loss = 0.238572
I1123 14:57:14.972427 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:57:14.972427 25976 solver.cpp:237]     Train net output #1: loss = 0.238572 (* 1 = 0.238572 loss)
I1123 14:57:14.972427 25976 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1123 14:57:18.824934 25976 solver.cpp:218] Iteration 15800 (25.9578 iter/s, 3.85241s/100 iters), loss = 0.269512
I1123 14:57:18.824934 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 14:57:18.824934 25976 solver.cpp:237]     Train net output #1: loss = 0.269512 (* 1 = 0.269512 loss)
I1123 14:57:18.825433 25976 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1123 14:57:22.687232 25976 solver.cpp:218] Iteration 15900 (25.8933 iter/s, 3.86201s/100 iters), loss = 0.205136
I1123 14:57:22.687232 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 14:57:22.687232 25976 solver.cpp:237]     Train net output #1: loss = 0.205136 (* 1 = 0.205136 loss)
I1123 14:57:22.687232 25976 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1123 14:57:26.362516 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:57:26.514530 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_16000.caffemodel
I1123 14:57:26.526031 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_16000.solverstate
I1123 14:57:26.530030 25976 solver.cpp:330] Iteration 16000, Testing net (#0)
I1123 14:57:26.530030 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:57:27.596601 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:57:27.638103 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8554
I1123 14:57:27.638103 25976 solver.cpp:397]     Test net output #1: loss = 0.423418 (* 1 = 0.423418 loss)
I1123 14:57:27.674608 25976 solver.cpp:218] Iteration 16000 (20.0507 iter/s, 4.98734s/100 iters), loss = 0.239625
I1123 14:57:27.674608 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:57:27.674608 25976 solver.cpp:237]     Train net output #1: loss = 0.239625 (* 1 = 0.239625 loss)
I1123 14:57:27.674608 25976 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1123 14:57:31.524863 25976 solver.cpp:218] Iteration 16100 (25.9775 iter/s, 3.84949s/100 iters), loss = 0.263785
I1123 14:57:31.524863 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 14:57:31.525363 25976 solver.cpp:237]     Train net output #1: loss = 0.263785 (* 1 = 0.263785 loss)
I1123 14:57:31.525363 25976 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1123 14:57:35.376099 25976 solver.cpp:218] Iteration 16200 (25.971 iter/s, 3.85044s/100 iters), loss = 0.309704
I1123 14:57:35.376099 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:57:35.376099 25976 solver.cpp:237]     Train net output #1: loss = 0.309703 (* 1 = 0.309703 loss)
I1123 14:57:35.376099 25976 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1123 14:57:39.227365 25976 solver.cpp:218] Iteration 16300 (25.9662 iter/s, 3.85117s/100 iters), loss = 0.350978
I1123 14:57:39.227365 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:57:39.227365 25976 solver.cpp:237]     Train net output #1: loss = 0.350978 (* 1 = 0.350978 loss)
I1123 14:57:39.227365 25976 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1123 14:57:43.083643 25976 solver.cpp:218] Iteration 16400 (25.9359 iter/s, 3.85566s/100 iters), loss = 0.230398
I1123 14:57:43.083643 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:57:43.083643 25976 solver.cpp:237]     Train net output #1: loss = 0.230398 (* 1 = 0.230398 loss)
I1123 14:57:43.083643 25976 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1123 14:57:46.757905 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:57:46.909910 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_16500.caffemodel
I1123 14:57:46.919916 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_16500.solverstate
I1123 14:57:46.923918 25976 solver.cpp:330] Iteration 16500, Testing net (#0)
I1123 14:57:46.923918 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:57:47.990070 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:57:48.032083 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8554
I1123 14:57:48.032083 25976 solver.cpp:397]     Test net output #1: loss = 0.422787 (* 1 = 0.422787 loss)
I1123 14:57:48.069073 25976 solver.cpp:218] Iteration 16500 (20.0584 iter/s, 4.98544s/100 iters), loss = 0.230557
I1123 14:57:48.069073 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:57:48.069073 25976 solver.cpp:237]     Train net output #1: loss = 0.230557 (* 1 = 0.230557 loss)
I1123 14:57:48.069073 25976 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1123 14:57:51.930338 25976 solver.cpp:218] Iteration 16600 (25.9037 iter/s, 3.86045s/100 iters), loss = 0.242475
I1123 14:57:51.930338 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:57:51.930338 25976 solver.cpp:237]     Train net output #1: loss = 0.242475 (* 1 = 0.242475 loss)
I1123 14:57:51.930338 25976 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1123 14:57:55.784488 25976 solver.cpp:218] Iteration 16700 (25.9484 iter/s, 3.8538s/100 iters), loss = 0.298213
I1123 14:57:55.784488 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:57:55.784488 25976 solver.cpp:237]     Train net output #1: loss = 0.298213 (* 1 = 0.298213 loss)
I1123 14:57:55.784488 25976 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1123 14:57:59.637785 25976 solver.cpp:218] Iteration 16800 (25.9524 iter/s, 3.85321s/100 iters), loss = 0.372478
I1123 14:57:59.637785 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:57:59.637785 25976 solver.cpp:237]     Train net output #1: loss = 0.372478 (* 1 = 0.372478 loss)
I1123 14:57:59.637785 25976 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1123 14:58:03.500087 25976 solver.cpp:218] Iteration 16900 (25.8926 iter/s, 3.8621s/100 iters), loss = 0.190972
I1123 14:58:03.500087 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:58:03.500087 25976 solver.cpp:237]     Train net output #1: loss = 0.190972 (* 1 = 0.190972 loss)
I1123 14:58:03.500087 25976 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1123 14:58:07.172524 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:58:07.322522 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_17000.caffemodel
I1123 14:58:07.333528 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_17000.solverstate
I1123 14:58:07.337538 25976 solver.cpp:330] Iteration 17000, Testing net (#0)
I1123 14:58:07.337538 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:58:08.405612 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:58:08.447623 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8558
I1123 14:58:08.447623 25976 solver.cpp:397]     Test net output #1: loss = 0.422622 (* 1 = 0.422622 loss)
I1123 14:58:08.484622 25976 solver.cpp:218] Iteration 17000 (20.0644 iter/s, 4.98395s/100 iters), loss = 0.290748
I1123 14:58:08.484622 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:58:08.484622 25976 solver.cpp:237]     Train net output #1: loss = 0.290748 (* 1 = 0.290748 loss)
I1123 14:58:08.484622 25976 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1123 14:58:12.336503 25976 solver.cpp:218] Iteration 17100 (25.9636 iter/s, 3.85155s/100 iters), loss = 0.269296
I1123 14:58:12.336503 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:58:12.336503 25976 solver.cpp:237]     Train net output #1: loss = 0.269296 (* 1 = 0.269296 loss)
I1123 14:58:12.336503 25976 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1123 14:58:16.189229 25976 solver.cpp:218] Iteration 17200 (25.961 iter/s, 3.85192s/100 iters), loss = 0.29146
I1123 14:58:16.189229 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:58:16.189229 25976 solver.cpp:237]     Train net output #1: loss = 0.29146 (* 1 = 0.29146 loss)
I1123 14:58:16.189229 25976 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1123 14:58:20.048530 25976 solver.cpp:218] Iteration 17300 (25.9142 iter/s, 3.85889s/100 iters), loss = 0.330715
I1123 14:58:20.048530 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:58:20.048530 25976 solver.cpp:237]     Train net output #1: loss = 0.330715 (* 1 = 0.330715 loss)
I1123 14:58:20.048530 25976 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1123 14:58:23.909809 25976 solver.cpp:218] Iteration 17400 (25.9007 iter/s, 3.86091s/100 iters), loss = 0.234738
I1123 14:58:23.909809 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:58:23.909809 25976 solver.cpp:237]     Train net output #1: loss = 0.234738 (* 1 = 0.234738 loss)
I1123 14:58:23.909809 25976 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1123 14:58:27.571082 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:58:27.723086 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_17500.caffemodel
I1123 14:58:27.735090 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_17500.solverstate
I1123 14:58:27.739091 25976 solver.cpp:330] Iteration 17500, Testing net (#0)
I1123 14:58:27.739091 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:58:28.805182 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:58:28.846187 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8556
I1123 14:58:28.846187 25976 solver.cpp:397]     Test net output #1: loss = 0.422829 (* 1 = 0.422829 loss)
I1123 14:58:28.885187 25976 solver.cpp:218] Iteration 17500 (20.1003 iter/s, 4.97505s/100 iters), loss = 0.235358
I1123 14:58:28.885187 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:58:28.885187 25976 solver.cpp:237]     Train net output #1: loss = 0.235358 (* 1 = 0.235358 loss)
I1123 14:58:28.885187 25976 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1123 14:58:32.745441 25976 solver.cpp:218] Iteration 17600 (25.9018 iter/s, 3.86073s/100 iters), loss = 0.234261
I1123 14:58:32.746441 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:58:32.746441 25976 solver.cpp:237]     Train net output #1: loss = 0.234261 (* 1 = 0.234261 loss)
I1123 14:58:32.746441 25976 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1123 14:58:36.609738 25976 solver.cpp:218] Iteration 17700 (25.8829 iter/s, 3.86355s/100 iters), loss = 0.279907
I1123 14:58:36.609738 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:58:36.609738 25976 solver.cpp:237]     Train net output #1: loss = 0.279907 (* 1 = 0.279907 loss)
I1123 14:58:36.609738 25976 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1123 14:58:40.466025 25976 solver.cpp:218] Iteration 17800 (25.9349 iter/s, 3.85582s/100 iters), loss = 0.303697
I1123 14:58:40.466025 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:58:40.466025 25976 solver.cpp:237]     Train net output #1: loss = 0.303696 (* 1 = 0.303696 loss)
I1123 14:58:40.466025 25976 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1123 14:58:44.333308 25976 solver.cpp:218] Iteration 17900 (25.8643 iter/s, 3.86633s/100 iters), loss = 0.247208
I1123 14:58:44.333308 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:58:44.333308 25976 solver.cpp:237]     Train net output #1: loss = 0.247208 (* 1 = 0.247208 loss)
I1123 14:58:44.333308 25976 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1123 14:58:48.001595 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:58:48.153611 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_18000.caffemodel
I1123 14:58:48.163610 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_18000.solverstate
I1123 14:58:48.168612 25976 solver.cpp:330] Iteration 18000, Testing net (#0)
I1123 14:58:48.169615 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:58:49.236688 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:58:49.278618 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8562
I1123 14:58:49.278618 25976 solver.cpp:397]     Test net output #1: loss = 0.42278 (* 1 = 0.42278 loss)
I1123 14:58:49.315603 25976 solver.cpp:218] Iteration 18000 (20.0703 iter/s, 4.98248s/100 iters), loss = 0.210336
I1123 14:58:49.315603 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 14:58:49.315603 25976 solver.cpp:237]     Train net output #1: loss = 0.210336 (* 1 = 0.210336 loss)
I1123 14:58:49.315603 25976 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1123 14:58:53.186923 25976 solver.cpp:218] Iteration 18100 (25.8312 iter/s, 3.87128s/100 iters), loss = 0.334282
I1123 14:58:53.187923 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:58:53.187923 25976 solver.cpp:237]     Train net output #1: loss = 0.334281 (* 1 = 0.334281 loss)
I1123 14:58:53.187923 25976 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1123 14:58:57.050202 25976 solver.cpp:218] Iteration 18200 (25.8937 iter/s, 3.86195s/100 iters), loss = 0.258441
I1123 14:58:57.050202 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 14:58:57.050202 25976 solver.cpp:237]     Train net output #1: loss = 0.25844 (* 1 = 0.25844 loss)
I1123 14:58:57.050202 25976 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1123 14:59:00.916436 25976 solver.cpp:218] Iteration 18300 (25.8663 iter/s, 3.86603s/100 iters), loss = 0.33109
I1123 14:59:00.916436 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 14:59:00.916436 25976 solver.cpp:237]     Train net output #1: loss = 0.33109 (* 1 = 0.33109 loss)
I1123 14:59:00.916436 25976 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1123 14:59:04.779738 25976 solver.cpp:218] Iteration 18400 (25.8846 iter/s, 3.8633s/100 iters), loss = 0.230262
I1123 14:59:04.779738 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:59:04.779738 25976 solver.cpp:237]     Train net output #1: loss = 0.230262 (* 1 = 0.230262 loss)
I1123 14:59:04.779738 25976 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1123 14:59:08.457976 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:59:08.609985 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_18500.caffemodel
I1123 14:59:08.619987 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_18500.solverstate
I1123 14:59:08.624989 25976 solver.cpp:330] Iteration 18500, Testing net (#0)
I1123 14:59:08.624989 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:59:09.692087 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:59:09.734593 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8557
I1123 14:59:09.734593 25976 solver.cpp:397]     Test net output #1: loss = 0.422664 (* 1 = 0.422664 loss)
I1123 14:59:09.771096 25976 solver.cpp:218] Iteration 18500 (20.036 iter/s, 4.99102s/100 iters), loss = 0.30665
I1123 14:59:09.771096 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:59:09.771096 25976 solver.cpp:237]     Train net output #1: loss = 0.30665 (* 1 = 0.30665 loss)
I1123 14:59:09.771096 25976 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1123 14:59:13.637362 25976 solver.cpp:218] Iteration 18600 (25.8692 iter/s, 3.8656s/100 iters), loss = 0.226267
I1123 14:59:13.637362 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 14:59:13.637362 25976 solver.cpp:237]     Train net output #1: loss = 0.226267 (* 1 = 0.226267 loss)
I1123 14:59:13.637362 25976 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1123 14:59:17.505650 25976 solver.cpp:218] Iteration 18700 (25.8547 iter/s, 3.86777s/100 iters), loss = 0.286856
I1123 14:59:17.505650 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:59:17.505650 25976 solver.cpp:237]     Train net output #1: loss = 0.286856 (* 1 = 0.286856 loss)
I1123 14:59:17.505650 25976 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1123 14:59:21.362926 25976 solver.cpp:218] Iteration 18800 (25.9288 iter/s, 3.85671s/100 iters), loss = 0.308402
I1123 14:59:21.362926 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 14:59:21.362926 25976 solver.cpp:237]     Train net output #1: loss = 0.308402 (* 1 = 0.308402 loss)
I1123 14:59:21.362926 25976 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1123 14:59:25.222235 25976 solver.cpp:218] Iteration 18900 (25.9111 iter/s, 3.85935s/100 iters), loss = 0.213336
I1123 14:59:25.222235 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 14:59:25.222235 25976 solver.cpp:237]     Train net output #1: loss = 0.213335 (* 1 = 0.213335 loss)
I1123 14:59:25.222235 25976 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1123 14:59:28.888559 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:59:29.041066 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_19000.caffemodel
I1123 14:59:29.050570 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_19000.solverstate
I1123 14:59:29.054571 25976 solver.cpp:330] Iteration 19000, Testing net (#0)
I1123 14:59:29.054571 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:59:30.122673 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:59:30.164688 25976 solver.cpp:397]     Test net output #0: accuracy = 0.856
I1123 14:59:30.164688 25976 solver.cpp:397]     Test net output #1: loss = 0.422884 (* 1 = 0.422884 loss)
I1123 14:59:30.203680 25976 solver.cpp:218] Iteration 19000 (20.0777 iter/s, 4.98065s/100 iters), loss = 0.240573
I1123 14:59:30.203680 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 14:59:30.203680 25976 solver.cpp:237]     Train net output #1: loss = 0.240573 (* 1 = 0.240573 loss)
I1123 14:59:30.203680 25976 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1123 14:59:34.063982 25976 solver.cpp:218] Iteration 19100 (25.9074 iter/s, 3.85989s/100 iters), loss = 0.301334
I1123 14:59:34.063982 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:59:34.063982 25976 solver.cpp:237]     Train net output #1: loss = 0.301334 (* 1 = 0.301334 loss)
I1123 14:59:34.063982 25976 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1123 14:59:37.929203 25976 solver.cpp:218] Iteration 19200 (25.8719 iter/s, 3.86519s/100 iters), loss = 0.312211
I1123 14:59:37.929203 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 14:59:37.929203 25976 solver.cpp:237]     Train net output #1: loss = 0.312211 (* 1 = 0.312211 loss)
I1123 14:59:37.929203 25976 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1123 14:59:41.785509 25976 solver.cpp:218] Iteration 19300 (25.9355 iter/s, 3.85573s/100 iters), loss = 0.273211
I1123 14:59:41.785509 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 14:59:41.785509 25976 solver.cpp:237]     Train net output #1: loss = 0.273211 (* 1 = 0.273211 loss)
I1123 14:59:41.785509 25976 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1123 14:59:45.654798 25976 solver.cpp:218] Iteration 19400 (25.8459 iter/s, 3.86909s/100 iters), loss = 0.244144
I1123 14:59:45.654798 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:59:45.654798 25976 solver.cpp:237]     Train net output #1: loss = 0.244143 (* 1 = 0.244143 loss)
I1123 14:59:45.654798 25976 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1123 14:59:49.337934 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:59:49.488941 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_19500.caffemodel
I1123 14:59:49.499941 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_19500.solverstate
I1123 14:59:49.503942 25976 solver.cpp:330] Iteration 19500, Testing net (#0)
I1123 14:59:49.503942 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 14:59:50.569041 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:59:50.611040 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8558
I1123 14:59:50.611040 25976 solver.cpp:397]     Test net output #1: loss = 0.422819 (* 1 = 0.422819 loss)
I1123 14:59:50.648542 25976 solver.cpp:218] Iteration 19500 (20.0265 iter/s, 4.99338s/100 iters), loss = 0.21346
I1123 14:59:50.648542 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 14:59:50.648542 25976 solver.cpp:237]     Train net output #1: loss = 0.21346 (* 1 = 0.21346 loss)
I1123 14:59:50.648542 25976 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1123 14:59:50.648542 25976 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1123 14:59:54.525272 25976 solver.cpp:218] Iteration 19600 (25.799 iter/s, 3.87611s/100 iters), loss = 0.28056
I1123 14:59:54.525272 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 14:59:54.525272 25976 solver.cpp:237]     Train net output #1: loss = 0.280559 (* 1 = 0.280559 loss)
I1123 14:59:54.525272 25976 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1123 14:59:58.395563 25976 solver.cpp:218] Iteration 19700 (25.8386 iter/s, 3.87018s/100 iters), loss = 0.263292
I1123 14:59:58.395563 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 14:59:58.395563 25976 solver.cpp:237]     Train net output #1: loss = 0.263292 (* 1 = 0.263292 loss)
I1123 14:59:58.395563 25976 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1123 15:00:02.296797 25976 solver.cpp:218] Iteration 19800 (25.6365 iter/s, 3.90068s/100 iters), loss = 0.305238
I1123 15:00:02.296797 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:00:02.296797 25976 solver.cpp:237]     Train net output #1: loss = 0.305238 (* 1 = 0.305238 loss)
I1123 15:00:02.296797 25976 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1123 15:00:06.155092 25976 solver.cpp:218] Iteration 19900 (25.9189 iter/s, 3.85819s/100 iters), loss = 0.212993
I1123 15:00:06.155092 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:00:06.155092 25976 solver.cpp:237]     Train net output #1: loss = 0.212993 (* 1 = 0.212993 loss)
I1123 15:00:06.155092 25976 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1123 15:00:09.827409 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:00:09.979429 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_20000.caffemodel
I1123 15:00:09.990429 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_20000.solverstate
I1123 15:00:09.993429 25976 solver.cpp:330] Iteration 20000, Testing net (#0)
I1123 15:00:09.994431 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:00:11.062513 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:00:11.103515 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8559
I1123 15:00:11.103515 25976 solver.cpp:397]     Test net output #1: loss = 0.423018 (* 1 = 0.423018 loss)
I1123 15:00:11.140516 25976 solver.cpp:218] Iteration 20000 (20.059 iter/s, 4.9853s/100 iters), loss = 0.259256
I1123 15:00:11.140516 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:00:11.140516 25976 solver.cpp:237]     Train net output #1: loss = 0.259256 (* 1 = 0.259256 loss)
I1123 15:00:11.140516 25976 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1123 15:00:14.985801 25976 solver.cpp:218] Iteration 20100 (26.0085 iter/s, 3.8449s/100 iters), loss = 0.24741
I1123 15:00:14.985801 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:00:14.985801 25976 solver.cpp:237]     Train net output #1: loss = 0.24741 (* 1 = 0.24741 loss)
I1123 15:00:14.985801 25976 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1123 15:00:18.838060 25976 solver.cpp:218] Iteration 20200 (25.9624 iter/s, 3.85173s/100 iters), loss = 0.281854
I1123 15:00:18.838060 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:00:18.838060 25976 solver.cpp:237]     Train net output #1: loss = 0.281854 (* 1 = 0.281854 loss)
I1123 15:00:18.838060 25976 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1123 15:00:22.694371 25976 solver.cpp:218] Iteration 20300 (25.935 iter/s, 3.8558s/100 iters), loss = 0.38983
I1123 15:00:22.694371 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 15:00:22.694371 25976 solver.cpp:237]     Train net output #1: loss = 0.389829 (* 1 = 0.389829 loss)
I1123 15:00:22.694371 25976 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1123 15:00:26.551651 25976 solver.cpp:218] Iteration 20400 (25.9261 iter/s, 3.85712s/100 iters), loss = 0.189198
I1123 15:00:26.551651 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 15:00:26.551651 25976 solver.cpp:237]     Train net output #1: loss = 0.189197 (* 1 = 0.189197 loss)
I1123 15:00:26.551651 25976 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1123 15:00:30.214912 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:00:30.367923 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_20500.caffemodel
I1123 15:00:30.380928 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_20500.solverstate
I1123 15:00:30.384927 25976 solver.cpp:330] Iteration 20500, Testing net (#0)
I1123 15:00:30.384927 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:00:31.455004 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:00:31.498010 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8559
I1123 15:00:31.498010 25976 solver.cpp:397]     Test net output #1: loss = 0.422922 (* 1 = 0.422922 loss)
I1123 15:00:31.536010 25976 solver.cpp:218] Iteration 20500 (20.0636 iter/s, 4.98416s/100 iters), loss = 0.276986
I1123 15:00:31.536010 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:00:31.536010 25976 solver.cpp:237]     Train net output #1: loss = 0.276986 (* 1 = 0.276986 loss)
I1123 15:00:31.536010 25976 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1123 15:00:35.410310 25976 solver.cpp:218] Iteration 20600 (25.8173 iter/s, 3.87338s/100 iters), loss = 0.266207
I1123 15:00:35.410310 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 15:00:35.410310 25976 solver.cpp:237]     Train net output #1: loss = 0.266206 (* 1 = 0.266206 loss)
I1123 15:00:35.410310 25976 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1123 15:00:39.260591 25976 solver.cpp:218] Iteration 20700 (25.9771 iter/s, 3.84954s/100 iters), loss = 0.266132
I1123 15:00:39.260591 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:00:39.260591 25976 solver.cpp:237]     Train net output #1: loss = 0.266132 (* 1 = 0.266132 loss)
I1123 15:00:39.260591 25976 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1123 15:00:43.125905 25976 solver.cpp:218] Iteration 20800 (25.8727 iter/s, 3.86507s/100 iters), loss = 0.301415
I1123 15:00:43.125905 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:00:43.125905 25976 solver.cpp:237]     Train net output #1: loss = 0.301415 (* 1 = 0.301415 loss)
I1123 15:00:43.125905 25976 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1123 15:00:46.995156 25976 solver.cpp:218] Iteration 20900 (25.8433 iter/s, 3.86947s/100 iters), loss = 0.207462
I1123 15:00:46.995156 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:00:46.995156 25976 solver.cpp:237]     Train net output #1: loss = 0.207461 (* 1 = 0.207461 loss)
I1123 15:00:46.995156 25976 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1123 15:00:50.686425 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:00:50.838438 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_21000.caffemodel
I1123 15:00:50.851439 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_21000.solverstate
I1123 15:00:50.855439 25976 solver.cpp:330] Iteration 21000, Testing net (#0)
I1123 15:00:50.855439 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:00:51.923562 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:00:51.965564 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8559
I1123 15:00:51.965564 25976 solver.cpp:397]     Test net output #1: loss = 0.423048 (* 1 = 0.423048 loss)
I1123 15:00:52.002562 25976 solver.cpp:218] Iteration 21000 (19.9726 iter/s, 5.00686s/100 iters), loss = 0.222847
I1123 15:00:52.002562 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:00:52.002562 25976 solver.cpp:237]     Train net output #1: loss = 0.222846 (* 1 = 0.222846 loss)
I1123 15:00:52.002562 25976 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1123 15:00:55.870102 25976 solver.cpp:218] Iteration 21100 (25.8634 iter/s, 3.86646s/100 iters), loss = 0.303758
I1123 15:00:55.870102 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:00:55.870102 25976 solver.cpp:237]     Train net output #1: loss = 0.303757 (* 1 = 0.303757 loss)
I1123 15:00:55.870102 25976 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1123 15:00:59.731411 25976 solver.cpp:218] Iteration 21200 (25.8947 iter/s, 3.86179s/100 iters), loss = 0.302453
I1123 15:00:59.731411 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:00:59.731411 25976 solver.cpp:237]     Train net output #1: loss = 0.302453 (* 1 = 0.302453 loss)
I1123 15:00:59.731411 25976 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1123 15:01:03.598657 25976 solver.cpp:218] Iteration 21300 (25.8636 iter/s, 3.86644s/100 iters), loss = 0.342113
I1123 15:01:03.598657 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:01:03.598657 25976 solver.cpp:237]     Train net output #1: loss = 0.342113 (* 1 = 0.342113 loss)
I1123 15:01:03.598657 25976 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1123 15:01:07.457970 25976 solver.cpp:218] Iteration 21400 (25.9115 iter/s, 3.85929s/100 iters), loss = 0.189185
I1123 15:01:07.457970 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 15:01:07.457970 25976 solver.cpp:237]     Train net output #1: loss = 0.189185 (* 1 = 0.189185 loss)
I1123 15:01:07.457970 25976 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1123 15:01:11.147228 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:01:11.298231 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_21500.caffemodel
I1123 15:01:11.308233 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_21500.solverstate
I1123 15:01:11.312232 25976 solver.cpp:330] Iteration 21500, Testing net (#0)
I1123 15:01:11.312232 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:01:12.380311 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:01:12.422814 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8557
I1123 15:01:12.422814 25976 solver.cpp:397]     Test net output #1: loss = 0.423092 (* 1 = 0.423092 loss)
I1123 15:01:12.459314 25976 solver.cpp:218] Iteration 21500 (19.996 iter/s, 5.00101s/100 iters), loss = 0.222892
I1123 15:01:12.459314 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:01:12.460315 25976 solver.cpp:237]     Train net output #1: loss = 0.222892 (* 1 = 0.222892 loss)
I1123 15:01:12.460315 25976 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1123 15:01:16.331599 25976 solver.cpp:218] Iteration 21600 (25.8298 iter/s, 3.8715s/100 iters), loss = 0.267627
I1123 15:01:16.331599 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:01:16.331599 25976 solver.cpp:237]     Train net output #1: loss = 0.267627 (* 1 = 0.267627 loss)
I1123 15:01:16.331599 25976 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1123 15:01:20.181864 25976 solver.cpp:218] Iteration 21700 (25.9758 iter/s, 3.84973s/100 iters), loss = 0.28
I1123 15:01:20.181864 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:01:20.181864 25976 solver.cpp:237]     Train net output #1: loss = 0.279999 (* 1 = 0.279999 loss)
I1123 15:01:20.181864 25976 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1123 15:01:24.040177 25976 solver.cpp:218] Iteration 21800 (25.918 iter/s, 3.85832s/100 iters), loss = 0.341702
I1123 15:01:24.040177 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:01:24.040177 25976 solver.cpp:237]     Train net output #1: loss = 0.341702 (* 1 = 0.341702 loss)
I1123 15:01:24.040177 25976 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1123 15:01:27.901415 25976 solver.cpp:218] Iteration 21900 (25.9069 iter/s, 3.85998s/100 iters), loss = 0.217213
I1123 15:01:27.901415 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:01:27.901415 25976 solver.cpp:237]     Train net output #1: loss = 0.217212 (* 1 = 0.217212 loss)
I1123 15:01:27.901415 25976 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1123 15:01:31.563921 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:01:31.714926 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_22000.caffemodel
I1123 15:01:31.724931 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_22000.solverstate
I1123 15:01:31.728930 25976 solver.cpp:330] Iteration 22000, Testing net (#0)
I1123 15:01:31.729431 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:01:32.795006 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:01:32.837013 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8555
I1123 15:01:32.837013 25976 solver.cpp:397]     Test net output #1: loss = 0.422968 (* 1 = 0.422968 loss)
I1123 15:01:32.874012 25976 solver.cpp:218] Iteration 22000 (20.1117 iter/s, 4.97222s/100 iters), loss = 0.205292
I1123 15:01:32.874012 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:01:32.874012 25976 solver.cpp:237]     Train net output #1: loss = 0.205291 (* 1 = 0.205291 loss)
I1123 15:01:32.874012 25976 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1123 15:01:32.874012 25976 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1123 15:01:36.724268 25976 solver.cpp:218] Iteration 22100 (25.9743 iter/s, 3.84996s/100 iters), loss = 0.255808
I1123 15:01:36.724268 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:01:36.724268 25976 solver.cpp:237]     Train net output #1: loss = 0.255807 (* 1 = 0.255807 loss)
I1123 15:01:36.724268 25976 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1123 15:01:40.575505 25976 solver.cpp:218] Iteration 22200 (25.9653 iter/s, 3.85129s/100 iters), loss = 0.257609
I1123 15:01:40.575505 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:01:40.575505 25976 solver.cpp:237]     Train net output #1: loss = 0.257608 (* 1 = 0.257608 loss)
I1123 15:01:40.575505 25976 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1123 15:01:44.434783 25976 solver.cpp:218] Iteration 22300 (25.9169 iter/s, 3.85848s/100 iters), loss = 0.334738
I1123 15:01:44.434783 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:01:44.434783 25976 solver.cpp:237]     Train net output #1: loss = 0.334738 (* 1 = 0.334738 loss)
I1123 15:01:44.434783 25976 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1123 15:01:48.290014 25976 solver.cpp:218] Iteration 22400 (25.936 iter/s, 3.85564s/100 iters), loss = 0.188651
I1123 15:01:48.290014 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:01:48.290014 25976 solver.cpp:237]     Train net output #1: loss = 0.188651 (* 1 = 0.188651 loss)
I1123 15:01:48.290014 25976 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1123 15:01:51.967262 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:01:52.118270 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_22500.caffemodel
I1123 15:01:52.128271 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_22500.solverstate
I1123 15:01:52.131770 25976 solver.cpp:330] Iteration 22500, Testing net (#0)
I1123 15:01:52.132272 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:01:53.201355 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:01:53.243362 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8555
I1123 15:01:53.243362 25976 solver.cpp:397]     Test net output #1: loss = 0.423036 (* 1 = 0.423036 loss)
I1123 15:01:53.280361 25976 solver.cpp:218] Iteration 22500 (20.0438 iter/s, 4.98907s/100 iters), loss = 0.221127
I1123 15:01:53.280361 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:01:53.280361 25976 solver.cpp:237]     Train net output #1: loss = 0.221126 (* 1 = 0.221126 loss)
I1123 15:01:53.280361 25976 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1123 15:01:57.142623 25976 solver.cpp:218] Iteration 22600 (25.8895 iter/s, 3.86257s/100 iters), loss = 0.282766
I1123 15:01:57.142623 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:01:57.142623 25976 solver.cpp:237]     Train net output #1: loss = 0.282766 (* 1 = 0.282766 loss)
I1123 15:01:57.142623 25976 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1123 15:02:01.009783 25976 solver.cpp:218] Iteration 22700 (25.864 iter/s, 3.86638s/100 iters), loss = 0.326615
I1123 15:02:01.009783 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:02:01.009783 25976 solver.cpp:237]     Train net output #1: loss = 0.326614 (* 1 = 0.326614 loss)
I1123 15:02:01.009783 25976 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1123 15:02:04.871031 25976 solver.cpp:218] Iteration 22800 (25.9019 iter/s, 3.86072s/100 iters), loss = 0.325164
I1123 15:02:04.871031 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:02:04.871031 25976 solver.cpp:237]     Train net output #1: loss = 0.325164 (* 1 = 0.325164 loss)
I1123 15:02:04.871031 25976 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1123 15:02:08.737291 25976 solver.cpp:218] Iteration 22900 (25.8657 iter/s, 3.86613s/100 iters), loss = 0.222457
I1123 15:02:08.737291 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:02:08.737291 25976 solver.cpp:237]     Train net output #1: loss = 0.222456 (* 1 = 0.222456 loss)
I1123 15:02:08.737291 25976 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1123 15:02:12.418051 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:02:12.570554 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_23000.caffemodel
I1123 15:02:12.579552 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_23000.solverstate
I1123 15:02:12.583571 25976 solver.cpp:330] Iteration 23000, Testing net (#0)
I1123 15:02:12.583571 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:02:13.652640 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:02:13.694640 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8559
I1123 15:02:13.694640 25976 solver.cpp:397]     Test net output #1: loss = 0.42304 (* 1 = 0.42304 loss)
I1123 15:02:13.731644 25976 solver.cpp:218] Iteration 23000 (20.0249 iter/s, 4.99377s/100 iters), loss = 0.246269
I1123 15:02:13.731644 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:02:13.731644 25976 solver.cpp:237]     Train net output #1: loss = 0.246269 (* 1 = 0.246269 loss)
I1123 15:02:13.731644 25976 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1123 15:02:17.583899 25976 solver.cpp:218] Iteration 23100 (25.959 iter/s, 3.85222s/100 iters), loss = 0.260228
I1123 15:02:17.583899 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:02:17.583899 25976 solver.cpp:237]     Train net output #1: loss = 0.260228 (* 1 = 0.260228 loss)
I1123 15:02:17.583899 25976 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1123 15:02:21.446177 25976 solver.cpp:218] Iteration 23200 (25.8905 iter/s, 3.86242s/100 iters), loss = 0.27485
I1123 15:02:21.446177 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:02:21.446177 25976 solver.cpp:237]     Train net output #1: loss = 0.27485 (* 1 = 0.27485 loss)
I1123 15:02:21.446177 25976 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1123 15:02:25.309480 25976 solver.cpp:218] Iteration 23300 (25.8901 iter/s, 3.86247s/100 iters), loss = 0.354798
I1123 15:02:25.309983 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:02:25.309983 25976 solver.cpp:237]     Train net output #1: loss = 0.354798 (* 1 = 0.354798 loss)
I1123 15:02:25.309983 25976 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1123 15:02:29.160728 25976 solver.cpp:218] Iteration 23400 (25.9644 iter/s, 3.85143s/100 iters), loss = 0.302216
I1123 15:02:29.161728 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:02:29.161728 25976 solver.cpp:237]     Train net output #1: loss = 0.302216 (* 1 = 0.302216 loss)
I1123 15:02:29.161728 25976 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1123 15:02:32.835963 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:02:32.988971 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_23500.caffemodel
I1123 15:02:32.998972 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_23500.solverstate
I1123 15:02:33.002971 25976 solver.cpp:330] Iteration 23500, Testing net (#0)
I1123 15:02:33.002971 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:02:34.071033 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:02:34.113034 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8555
I1123 15:02:34.113034 25976 solver.cpp:397]     Test net output #1: loss = 0.42299 (* 1 = 0.42299 loss)
I1123 15:02:34.151538 25976 solver.cpp:218] Iteration 23500 (20.042 iter/s, 4.98951s/100 iters), loss = 0.216119
I1123 15:02:34.151538 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:02:34.151538 25976 solver.cpp:237]     Train net output #1: loss = 0.216119 (* 1 = 0.216119 loss)
I1123 15:02:34.151538 25976 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1123 15:02:38.030899 25976 solver.cpp:218] Iteration 23600 (25.7795 iter/s, 3.87905s/100 iters), loss = 0.296613
I1123 15:02:38.030899 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:02:38.030899 25976 solver.cpp:237]     Train net output #1: loss = 0.296612 (* 1 = 0.296612 loss)
I1123 15:02:38.030899 25976 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1123 15:02:41.909984 25976 solver.cpp:218] Iteration 23700 (25.7814 iter/s, 3.87876s/100 iters), loss = 0.303121
I1123 15:02:41.909984 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:02:41.909984 25976 solver.cpp:237]     Train net output #1: loss = 0.30312 (* 1 = 0.30312 loss)
I1123 15:02:41.909984 25976 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1123 15:02:45.770256 25976 solver.cpp:218] Iteration 23800 (25.9045 iter/s, 3.86034s/100 iters), loss = 0.33393
I1123 15:02:45.770256 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:02:45.770256 25976 solver.cpp:237]     Train net output #1: loss = 0.33393 (* 1 = 0.33393 loss)
I1123 15:02:45.770256 25976 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1123 15:02:49.633419 25976 solver.cpp:218] Iteration 23900 (25.8918 iter/s, 3.86222s/100 iters), loss = 0.242842
I1123 15:02:49.633419 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:02:49.633419 25976 solver.cpp:237]     Train net output #1: loss = 0.242842 (* 1 = 0.242842 loss)
I1123 15:02:49.633419 25976 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1123 15:02:53.304744 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:02:53.455751 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_24000.caffemodel
I1123 15:02:53.465751 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_24000.solverstate
I1123 15:02:53.469751 25976 solver.cpp:330] Iteration 24000, Testing net (#0)
I1123 15:02:53.469751 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:02:54.536857 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:02:54.578856 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8559
I1123 15:02:54.578856 25976 solver.cpp:397]     Test net output #1: loss = 0.423087 (* 1 = 0.423087 loss)
I1123 15:02:54.615875 25976 solver.cpp:218] Iteration 24000 (20.0698 iter/s, 4.98262s/100 iters), loss = 0.248645
I1123 15:02:54.615875 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:02:54.615875 25976 solver.cpp:237]     Train net output #1: loss = 0.248644 (* 1 = 0.248644 loss)
I1123 15:02:54.615875 25976 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1123 15:02:58.474126 25976 solver.cpp:218] Iteration 24100 (25.9213 iter/s, 3.85783s/100 iters), loss = 0.239868
I1123 15:02:58.474126 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:02:58.474126 25976 solver.cpp:237]     Train net output #1: loss = 0.239868 (* 1 = 0.239868 loss)
I1123 15:02:58.474126 25976 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1123 15:03:02.333401 25976 solver.cpp:218] Iteration 24200 (25.9151 iter/s, 3.85875s/100 iters), loss = 0.296021
I1123 15:03:02.333401 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:03:02.333401 25976 solver.cpp:237]     Train net output #1: loss = 0.296021 (* 1 = 0.296021 loss)
I1123 15:03:02.333401 25976 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1123 15:03:06.188621 25976 solver.cpp:218] Iteration 24300 (25.941 iter/s, 3.8549s/100 iters), loss = 0.343962
I1123 15:03:06.188621 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 15:03:06.188621 25976 solver.cpp:237]     Train net output #1: loss = 0.343962 (* 1 = 0.343962 loss)
I1123 15:03:06.188621 25976 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1123 15:03:10.045877 25976 solver.cpp:218] Iteration 24400 (25.9309 iter/s, 3.8564s/100 iters), loss = 0.283613
I1123 15:03:10.045877 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:03:10.045877 25976 solver.cpp:237]     Train net output #1: loss = 0.283612 (* 1 = 0.283612 loss)
I1123 15:03:10.045877 25976 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1123 15:03:13.714097 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:03:13.866101 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_24500.caffemodel
I1123 15:03:13.876101 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_24500.solverstate
I1123 15:03:13.881101 25976 solver.cpp:330] Iteration 24500, Testing net (#0)
I1123 15:03:13.881101 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:03:14.947219 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:03:14.989219 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8558
I1123 15:03:14.989219 25976 solver.cpp:397]     Test net output #1: loss = 0.422951 (* 1 = 0.422951 loss)
I1123 15:03:15.026226 25976 solver.cpp:218] Iteration 24500 (20.0801 iter/s, 4.98004s/100 iters), loss = 0.249038
I1123 15:03:15.026226 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:03:15.026226 25976 solver.cpp:237]     Train net output #1: loss = 0.249038 (* 1 = 0.249038 loss)
I1123 15:03:15.026226 25976 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1123 15:03:18.870494 25976 solver.cpp:218] Iteration 24600 (26.0129 iter/s, 3.84425s/100 iters), loss = 0.27285
I1123 15:03:18.870494 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:03:18.870494 25976 solver.cpp:237]     Train net output #1: loss = 0.27285 (* 1 = 0.27285 loss)
I1123 15:03:18.870494 25976 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1123 15:03:22.733772 25976 solver.cpp:218] Iteration 24700 (25.8861 iter/s, 3.86308s/100 iters), loss = 0.280808
I1123 15:03:22.733772 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:03:22.733772 25976 solver.cpp:237]     Train net output #1: loss = 0.280808 (* 1 = 0.280808 loss)
I1123 15:03:22.733772 25976 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1123 15:03:26.601047 25976 solver.cpp:218] Iteration 24800 (25.8661 iter/s, 3.86606s/100 iters), loss = 0.362728
I1123 15:03:26.601047 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:03:26.601047 25976 solver.cpp:237]     Train net output #1: loss = 0.362727 (* 1 = 0.362727 loss)
I1123 15:03:26.601047 25976 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1123 15:03:30.465473 25976 solver.cpp:218] Iteration 24900 (25.8758 iter/s, 3.86462s/100 iters), loss = 0.225879
I1123 15:03:30.465473 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:03:30.465473 25976 solver.cpp:237]     Train net output #1: loss = 0.225878 (* 1 = 0.225878 loss)
I1123 15:03:30.465473 25976 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1123 15:03:34.134697 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:03:34.290707 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_25000.caffemodel
I1123 15:03:34.301211 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_25000.solverstate
I1123 15:03:34.305210 25976 solver.cpp:330] Iteration 25000, Testing net (#0)
I1123 15:03:34.305210 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:03:35.372789 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:03:35.414798 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8558
I1123 15:03:35.414798 25976 solver.cpp:397]     Test net output #1: loss = 0.422982 (* 1 = 0.422982 loss)
I1123 15:03:35.451797 25976 solver.cpp:218] Iteration 25000 (20.0554 iter/s, 4.9862s/100 iters), loss = 0.314327
I1123 15:03:35.451797 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:03:35.451797 25976 solver.cpp:237]     Train net output #1: loss = 0.314327 (* 1 = 0.314327 loss)
I1123 15:03:35.451797 25976 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1123 15:03:39.321118 25976 solver.cpp:218] Iteration 25100 (25.8513 iter/s, 3.86828s/100 iters), loss = 0.332527
I1123 15:03:39.321118 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:03:39.321118 25976 solver.cpp:237]     Train net output #1: loss = 0.332526 (* 1 = 0.332526 loss)
I1123 15:03:39.321118 25976 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1123 15:03:43.196923 25976 solver.cpp:218] Iteration 25200 (25.8032 iter/s, 3.8755s/100 iters), loss = 0.253014
I1123 15:03:43.196923 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:03:43.196923 25976 solver.cpp:237]     Train net output #1: loss = 0.253014 (* 1 = 0.253014 loss)
I1123 15:03:43.196923 25976 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1123 15:03:47.078750 25976 solver.cpp:218] Iteration 25300 (25.7602 iter/s, 3.88195s/100 iters), loss = 0.295853
I1123 15:03:47.078750 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:03:47.078750 25976 solver.cpp:237]     Train net output #1: loss = 0.295853 (* 1 = 0.295853 loss)
I1123 15:03:47.078750 25976 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1123 15:03:50.986047 25976 solver.cpp:218] Iteration 25400 (25.6 iter/s, 3.90625s/100 iters), loss = 0.231975
I1123 15:03:50.986047 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:03:50.986047 25976 solver.cpp:237]     Train net output #1: loss = 0.231975 (* 1 = 0.231975 loss)
I1123 15:03:50.986047 25976 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1123 15:03:54.682350 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:03:54.835364 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_25500.caffemodel
I1123 15:03:54.844363 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_25500.solverstate
I1123 15:03:54.848364 25976 solver.cpp:330] Iteration 25500, Testing net (#0)
I1123 15:03:54.848364 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:03:55.918462 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:03:55.960465 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8558
I1123 15:03:55.960465 25976 solver.cpp:397]     Test net output #1: loss = 0.422907 (* 1 = 0.422907 loss)
I1123 15:03:55.997462 25976 solver.cpp:218] Iteration 25500 (19.9569 iter/s, 5.01079s/100 iters), loss = 0.21391
I1123 15:03:55.997462 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:03:55.997462 25976 solver.cpp:237]     Train net output #1: loss = 0.21391 (* 1 = 0.21391 loss)
I1123 15:03:55.997963 25976 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1123 15:03:59.850728 25976 solver.cpp:218] Iteration 25600 (25.9527 iter/s, 3.85317s/100 iters), loss = 0.311533
I1123 15:03:59.850728 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:03:59.850728 25976 solver.cpp:237]     Train net output #1: loss = 0.311533 (* 1 = 0.311533 loss)
I1123 15:03:59.850728 25976 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1123 15:04:03.727015 25976 solver.cpp:218] Iteration 25700 (25.8037 iter/s, 3.87541s/100 iters), loss = 0.251344
I1123 15:04:03.727015 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:04:03.727015 25976 solver.cpp:237]     Train net output #1: loss = 0.251343 (* 1 = 0.251343 loss)
I1123 15:04:03.727015 25976 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1123 15:04:07.614327 25976 solver.cpp:218] Iteration 25800 (25.727 iter/s, 3.88697s/100 iters), loss = 0.337338
I1123 15:04:07.614327 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:04:07.614327 25976 solver.cpp:237]     Train net output #1: loss = 0.337337 (* 1 = 0.337337 loss)
I1123 15:04:07.614327 25976 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1123 15:04:11.493597 25976 solver.cpp:218] Iteration 25900 (25.7763 iter/s, 3.87954s/100 iters), loss = 0.229914
I1123 15:04:11.493597 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:04:11.493597 25976 solver.cpp:237]     Train net output #1: loss = 0.229914 (* 1 = 0.229914 loss)
I1123 15:04:11.493597 25976 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1123 15:04:15.231879 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:04:15.383882 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_26000.caffemodel
I1123 15:04:15.397883 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_26000.solverstate
I1123 15:04:15.401887 25976 solver.cpp:330] Iteration 26000, Testing net (#0)
I1123 15:04:15.401887 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:04:16.467981 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:04:16.510973 25976 solver.cpp:397]     Test net output #0: accuracy = 0.856
I1123 15:04:16.510973 25976 solver.cpp:397]     Test net output #1: loss = 0.423016 (* 1 = 0.423016 loss)
I1123 15:04:16.547976 25976 solver.cpp:218] Iteration 26000 (19.7883 iter/s, 5.05349s/100 iters), loss = 0.257481
I1123 15:04:16.547976 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:04:16.547976 25976 solver.cpp:237]     Train net output #1: loss = 0.257481 (* 1 = 0.257481 loss)
I1123 15:04:16.547976 25976 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1123 15:04:20.407742 25976 solver.cpp:218] Iteration 26100 (25.9128 iter/s, 3.8591s/100 iters), loss = 0.315723
I1123 15:04:20.407742 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:04:20.407742 25976 solver.cpp:237]     Train net output #1: loss = 0.315723 (* 1 = 0.315723 loss)
I1123 15:04:20.407742 25976 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1123 15:04:24.275516 25976 solver.cpp:218] Iteration 26200 (25.853 iter/s, 3.86802s/100 iters), loss = 0.262392
I1123 15:04:24.275516 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:04:24.275516 25976 solver.cpp:237]     Train net output #1: loss = 0.262392 (* 1 = 0.262392 loss)
I1123 15:04:24.275516 25976 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1123 15:04:28.174723 25976 solver.cpp:218] Iteration 26300 (25.6477 iter/s, 3.89898s/100 iters), loss = 0.262341
I1123 15:04:28.174723 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:04:28.174723 25976 solver.cpp:237]     Train net output #1: loss = 0.262341 (* 1 = 0.262341 loss)
I1123 15:04:28.174723 25976 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1123 15:04:32.047974 25976 solver.cpp:218] Iteration 26400 (25.8224 iter/s, 3.87261s/100 iters), loss = 0.254121
I1123 15:04:32.047974 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:04:32.047974 25976 solver.cpp:237]     Train net output #1: loss = 0.254121 (* 1 = 0.254121 loss)
I1123 15:04:32.047974 25976 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1123 15:04:35.803562 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:04:35.954572 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_26500.caffemodel
I1123 15:04:35.964573 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_26500.solverstate
I1123 15:04:35.968574 25976 solver.cpp:330] Iteration 26500, Testing net (#0)
I1123 15:04:35.968574 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:04:37.037659 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:04:37.079659 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8557
I1123 15:04:37.079659 25976 solver.cpp:397]     Test net output #1: loss = 0.422993 (* 1 = 0.422993 loss)
I1123 15:04:37.116663 25976 solver.cpp:218] Iteration 26500 (19.7296 iter/s, 5.06852s/100 iters), loss = 0.252158
I1123 15:04:37.117663 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:04:37.117663 25976 solver.cpp:237]     Train net output #1: loss = 0.252157 (* 1 = 0.252157 loss)
I1123 15:04:37.117663 25976 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1123 15:04:40.992981 25976 solver.cpp:218] Iteration 26600 (25.8006 iter/s, 3.87589s/100 iters), loss = 0.287341
I1123 15:04:40.993983 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:04:40.993983 25976 solver.cpp:237]     Train net output #1: loss = 0.287341 (* 1 = 0.287341 loss)
I1123 15:04:40.993983 25976 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1123 15:04:45.022812 25976 solver.cpp:218] Iteration 26700 (24.8209 iter/s, 4.02886s/100 iters), loss = 0.249444
I1123 15:04:45.022812 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:04:45.022812 25976 solver.cpp:237]     Train net output #1: loss = 0.249444 (* 1 = 0.249444 loss)
I1123 15:04:45.022812 25976 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1123 15:04:48.879550 25976 solver.cpp:218] Iteration 26800 (25.927 iter/s, 3.85698s/100 iters), loss = 0.293243
I1123 15:04:48.880550 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:04:48.880550 25976 solver.cpp:237]     Train net output #1: loss = 0.293243 (* 1 = 0.293243 loss)
I1123 15:04:48.880550 25976 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1123 15:04:52.759887 25976 solver.cpp:218] Iteration 26900 (25.7783 iter/s, 3.87923s/100 iters), loss = 0.234179
I1123 15:04:52.759887 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:04:52.759887 25976 solver.cpp:237]     Train net output #1: loss = 0.234179 (* 1 = 0.234179 loss)
I1123 15:04:52.759887 25976 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1123 15:04:56.427135 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:04:56.577144 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_27000.caffemodel
I1123 15:04:56.588145 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_27000.solverstate
I1123 15:04:56.592145 25976 solver.cpp:330] Iteration 27000, Testing net (#0)
I1123 15:04:56.592145 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:04:57.670223 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:04:57.713227 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8553
I1123 15:04:57.713227 25976 solver.cpp:397]     Test net output #1: loss = 0.422864 (* 1 = 0.422864 loss)
I1123 15:04:57.750228 25976 solver.cpp:218] Iteration 27000 (20.0411 iter/s, 4.98975s/100 iters), loss = 0.252726
I1123 15:04:57.750228 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:04:57.750228 25976 solver.cpp:237]     Train net output #1: loss = 0.252726 (* 1 = 0.252726 loss)
I1123 15:04:57.750228 25976 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1123 15:04:57.750228 25976 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1123 15:05:01.666487 25976 solver.cpp:218] Iteration 27100 (25.533 iter/s, 3.9165s/100 iters), loss = 0.286845
I1123 15:05:01.666487 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:05:01.666487 25976 solver.cpp:237]     Train net output #1: loss = 0.286844 (* 1 = 0.286844 loss)
I1123 15:05:01.666487 25976 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1123 15:05:05.523231 25976 solver.cpp:218] Iteration 27200 (25.9319 iter/s, 3.85626s/100 iters), loss = 0.328538
I1123 15:05:05.523730 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:05:05.523730 25976 solver.cpp:237]     Train net output #1: loss = 0.328538 (* 1 = 0.328538 loss)
I1123 15:05:05.523730 25976 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1123 15:05:09.442013 25976 solver.cpp:218] Iteration 27300 (25.5207 iter/s, 3.91838s/100 iters), loss = 0.440363
I1123 15:05:09.442013 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 15:05:09.442013 25976 solver.cpp:237]     Train net output #1: loss = 0.440363 (* 1 = 0.440363 loss)
I1123 15:05:09.442013 25976 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1123 15:05:13.326741 25976 solver.cpp:218] Iteration 27400 (25.7457 iter/s, 3.88414s/100 iters), loss = 0.245354
I1123 15:05:13.326741 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:05:13.326741 25976 solver.cpp:237]     Train net output #1: loss = 0.245353 (* 1 = 0.245353 loss)
I1123 15:05:13.326741 25976 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1123 15:05:17.006490 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:05:17.158509 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_27500.caffemodel
I1123 15:05:17.168507 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_27500.solverstate
I1123 15:05:17.174510 25976 solver.cpp:330] Iteration 27500, Testing net (#0)
I1123 15:05:17.174510 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:05:18.239058 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:05:18.280059 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8557
I1123 15:05:18.280059 25976 solver.cpp:397]     Test net output #1: loss = 0.423008 (* 1 = 0.423008 loss)
I1123 15:05:18.317059 25976 solver.cpp:218] Iteration 27500 (20.0406 iter/s, 4.98988s/100 iters), loss = 0.2594
I1123 15:05:18.317059 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:05:18.317059 25976 solver.cpp:237]     Train net output #1: loss = 0.2594 (* 1 = 0.2594 loss)
I1123 15:05:18.317059 25976 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1123 15:05:22.194984 25976 solver.cpp:218] Iteration 27600 (25.7884 iter/s, 3.87772s/100 iters), loss = 0.238867
I1123 15:05:22.195487 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:05:22.195487 25976 solver.cpp:237]     Train net output #1: loss = 0.238867 (* 1 = 0.238867 loss)
I1123 15:05:22.195487 25976 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1123 15:05:26.079638 25976 solver.cpp:218] Iteration 27700 (25.7469 iter/s, 3.88396s/100 iters), loss = 0.290297
I1123 15:05:26.079638 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:05:26.079638 25976 solver.cpp:237]     Train net output #1: loss = 0.290297 (* 1 = 0.290297 loss)
I1123 15:05:26.079638 25976 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1123 15:05:29.976011 25976 solver.cpp:218] Iteration 27800 (25.6682 iter/s, 3.89587s/100 iters), loss = 0.34126
I1123 15:05:29.976011 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:05:29.976011 25976 solver.cpp:237]     Train net output #1: loss = 0.34126 (* 1 = 0.34126 loss)
I1123 15:05:29.976011 25976 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1123 15:05:34.013649 25976 solver.cpp:218] Iteration 27900 (24.7691 iter/s, 4.03729s/100 iters), loss = 0.188677
I1123 15:05:34.013649 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:05:34.013649 25976 solver.cpp:237]     Train net output #1: loss = 0.188677 (* 1 = 0.188677 loss)
I1123 15:05:34.013649 25976 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1123 15:05:37.709854 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:05:37.861862 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_28000.caffemodel
I1123 15:05:37.872864 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_28000.solverstate
I1123 15:05:37.877862 25976 solver.cpp:330] Iteration 28000, Testing net (#0)
I1123 15:05:37.877862 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:05:38.965984 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:05:39.009984 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8558
I1123 15:05:39.009984 25976 solver.cpp:397]     Test net output #1: loss = 0.423149 (* 1 = 0.423149 loss)
I1123 15:05:39.048986 25976 solver.cpp:218] Iteration 28000 (19.8615 iter/s, 5.03488s/100 iters), loss = 0.255534
I1123 15:05:39.048986 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:05:39.048986 25976 solver.cpp:237]     Train net output #1: loss = 0.255534 (* 1 = 0.255534 loss)
I1123 15:05:39.048986 25976 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1123 15:05:42.974325 25976 solver.cpp:218] Iteration 28100 (25.4778 iter/s, 3.92499s/100 iters), loss = 0.270945
I1123 15:05:42.974325 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:05:42.974325 25976 solver.cpp:237]     Train net output #1: loss = 0.270945 (* 1 = 0.270945 loss)
I1123 15:05:42.974325 25976 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1123 15:05:46.859644 25976 solver.cpp:218] Iteration 28200 (25.743 iter/s, 3.88455s/100 iters), loss = 0.291261
I1123 15:05:46.859644 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:05:46.859644 25976 solver.cpp:237]     Train net output #1: loss = 0.29126 (* 1 = 0.29126 loss)
I1123 15:05:46.859644 25976 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1123 15:05:50.803994 25976 solver.cpp:218] Iteration 28300 (25.3527 iter/s, 3.94435s/100 iters), loss = 0.349919
I1123 15:05:50.803994 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:05:50.803994 25976 solver.cpp:237]     Train net output #1: loss = 0.349919 (* 1 = 0.349919 loss)
I1123 15:05:50.803994 25976 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1123 15:05:54.735283 25976 solver.cpp:218] Iteration 28400 (25.4394 iter/s, 3.93092s/100 iters), loss = 0.22147
I1123 15:05:54.735283 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:05:54.735283 25976 solver.cpp:237]     Train net output #1: loss = 0.22147 (* 1 = 0.22147 loss)
I1123 15:05:54.735283 25976 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1123 15:05:58.403548 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:05:58.555560 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_28500.caffemodel
I1123 15:05:58.565560 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_28500.solverstate
I1123 15:05:58.569561 25976 solver.cpp:330] Iteration 28500, Testing net (#0)
I1123 15:05:58.569561 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:05:59.636653 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:05:59.678663 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8556
I1123 15:05:59.678663 25976 solver.cpp:397]     Test net output #1: loss = 0.423101 (* 1 = 0.423101 loss)
I1123 15:05:59.715652 25976 solver.cpp:218] Iteration 28500 (20.0819 iter/s, 4.9796s/100 iters), loss = 0.21516
I1123 15:05:59.715652 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:05:59.715652 25976 solver.cpp:237]     Train net output #1: loss = 0.21516 (* 1 = 0.21516 loss)
I1123 15:05:59.715652 25976 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1123 15:06:03.583917 25976 solver.cpp:218] Iteration 28600 (25.8544 iter/s, 3.86781s/100 iters), loss = 0.325633
I1123 15:06:03.583917 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:06:03.583917 25976 solver.cpp:237]     Train net output #1: loss = 0.325633 (* 1 = 0.325633 loss)
I1123 15:06:03.583917 25976 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1123 15:06:07.624161 25976 solver.cpp:218] Iteration 28700 (24.7544 iter/s, 4.03969s/100 iters), loss = 0.295445
I1123 15:06:07.624161 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:06:07.624161 25976 solver.cpp:237]     Train net output #1: loss = 0.295445 (* 1 = 0.295445 loss)
I1123 15:06:07.624161 25976 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1123 15:06:11.484920 25976 solver.cpp:218] Iteration 28800 (25.9035 iter/s, 3.86048s/100 iters), loss = 0.303986
I1123 15:06:11.484920 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 15:06:11.484920 25976 solver.cpp:237]     Train net output #1: loss = 0.303986 (* 1 = 0.303986 loss)
I1123 15:06:11.484920 25976 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1123 15:06:15.353233 25976 solver.cpp:218] Iteration 28900 (25.8486 iter/s, 3.86869s/100 iters), loss = 0.221276
I1123 15:06:15.354233 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:06:15.354233 25976 solver.cpp:237]     Train net output #1: loss = 0.221276 (* 1 = 0.221276 loss)
I1123 15:06:15.354233 25976 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1123 15:06:19.073047 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:06:19.242547 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_29000.caffemodel
I1123 15:06:19.252547 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_29000.solverstate
I1123 15:06:19.256546 25976 solver.cpp:330] Iteration 29000, Testing net (#0)
I1123 15:06:19.256546 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:06:20.447650 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:06:20.489667 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8555
I1123 15:06:20.489667 25976 solver.cpp:397]     Test net output #1: loss = 0.423059 (* 1 = 0.423059 loss)
I1123 15:06:20.526657 25976 solver.cpp:218] Iteration 29000 (19.3338 iter/s, 5.1723s/100 iters), loss = 0.220006
I1123 15:06:20.526657 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:06:20.526657 25976 solver.cpp:237]     Train net output #1: loss = 0.220006 (* 1 = 0.220006 loss)
I1123 15:06:20.526657 25976 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1123 15:06:24.391938 25976 solver.cpp:218] Iteration 29100 (25.8712 iter/s, 3.8653s/100 iters), loss = 0.29548
I1123 15:06:24.391938 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:06:24.391938 25976 solver.cpp:237]     Train net output #1: loss = 0.29548 (* 1 = 0.29548 loss)
I1123 15:06:24.391938 25976 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1123 15:06:28.255172 25976 solver.cpp:218] Iteration 29200 (25.8908 iter/s, 3.86238s/100 iters), loss = 0.253692
I1123 15:06:28.255172 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:06:28.255172 25976 solver.cpp:237]     Train net output #1: loss = 0.253691 (* 1 = 0.253691 loss)
I1123 15:06:28.255172 25976 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1123 15:06:32.110407 25976 solver.cpp:218] Iteration 29300 (25.9378 iter/s, 3.85537s/100 iters), loss = 0.292046
I1123 15:06:32.110407 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:06:32.110407 25976 solver.cpp:237]     Train net output #1: loss = 0.292046 (* 1 = 0.292046 loss)
I1123 15:06:32.110407 25976 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1123 15:06:35.961694 25976 solver.cpp:218] Iteration 29400 (25.9699 iter/s, 3.85061s/100 iters), loss = 0.298184
I1123 15:06:35.961694 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:06:35.961694 25976 solver.cpp:237]     Train net output #1: loss = 0.298184 (* 1 = 0.298184 loss)
I1123 15:06:35.961694 25976 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1123 15:06:39.807451 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:06:39.969964 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_29500.caffemodel
I1123 15:06:39.980970 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_29500.solverstate
I1123 15:06:39.984966 25976 solver.cpp:330] Iteration 29500, Testing net (#0)
I1123 15:06:39.984966 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:06:41.052052 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:06:41.094058 25976 solver.cpp:397]     Test net output #0: accuracy = 0.856
I1123 15:06:41.094058 25976 solver.cpp:397]     Test net output #1: loss = 0.42293 (* 1 = 0.42293 loss)
I1123 15:06:41.130053 25976 solver.cpp:218] Iteration 29500 (19.349 iter/s, 5.16822s/100 iters), loss = 0.274258
I1123 15:06:41.130053 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:06:41.130053 25976 solver.cpp:237]     Train net output #1: loss = 0.274258 (* 1 = 0.274258 loss)
I1123 15:06:41.130053 25976 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1123 15:06:45.006325 25976 solver.cpp:218] Iteration 29600 (25.8016 iter/s, 3.87572s/100 iters), loss = 0.265959
I1123 15:06:45.006325 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:06:45.006325 25976 solver.cpp:237]     Train net output #1: loss = 0.265959 (* 1 = 0.265959 loss)
I1123 15:06:45.006325 25976 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1123 15:06:48.888602 25976 solver.cpp:218] Iteration 29700 (25.763 iter/s, 3.88153s/100 iters), loss = 0.251318
I1123 15:06:48.888602 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:06:48.888602 25976 solver.cpp:237]     Train net output #1: loss = 0.251318 (* 1 = 0.251318 loss)
I1123 15:06:48.888602 25976 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1123 15:06:52.761885 25976 solver.cpp:218] Iteration 29800 (25.8155 iter/s, 3.87364s/100 iters), loss = 0.299746
I1123 15:06:52.761885 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:06:52.761885 25976 solver.cpp:237]     Train net output #1: loss = 0.299746 (* 1 = 0.299746 loss)
I1123 15:06:52.761885 25976 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1123 15:06:56.628120 25976 solver.cpp:218] Iteration 29900 (25.8692 iter/s, 3.8656s/100 iters), loss = 0.211182
I1123 15:06:56.628120 25976 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 15:06:56.628120 25976 solver.cpp:237]     Train net output #1: loss = 0.211182 (* 1 = 0.211182 loss)
I1123 15:06:56.628120 25976 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1123 15:07:00.302356 24360 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:07:00.454371 25976 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_30000.caffemodel
I1123 15:07:00.465376 25976 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_30000.solverstate
I1123 15:07:00.481376 25976 solver.cpp:310] Iteration 30000, loss = 0.271379
I1123 15:07:00.481376 25976 solver.cpp:330] Iteration 30000, Testing net (#0)
I1123 15:07:00.481376 25976 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:07:01.549479 35784 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:07:01.591485 25976 solver.cpp:397]     Test net output #0: accuracy = 0.8554
I1123 15:07:01.591485 25976 solver.cpp:397]     Test net output #1: loss = 0.422961 (* 1 = 0.422961 loss)
I1123 15:07:01.591485 25976 solver.cpp:315] Optimization Done.
I1123 15:07:01.591485 25976 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 
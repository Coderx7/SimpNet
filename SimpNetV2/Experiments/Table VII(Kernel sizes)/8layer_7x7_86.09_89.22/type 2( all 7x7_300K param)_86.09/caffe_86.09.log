
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1123 16:10:41.125263  5096 caffe.cpp:219] Using GPUs 0
I1123 16:10:41.285696  5096 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1123 16:10:41.595741  5096 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 16:10:41.613757  5096 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_300k_8L_7x7"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1123 16:10:41.613757  5096 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 16:10:41.614765  5096 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 16:10:41.614765  5096 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 16:10:41.614765  5096 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1123 16:10:41.614765  5096 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1123 16:10:41.614765  5096 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1123 16:10:41.614765  5096 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1123 16:10:41.614765  5096 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1123 16:10:41.614765  5096 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1123 16:10:41.614765  5096 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1123 16:10:41.614765  5096 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1123 16:10:41.614765  5096 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1123 16:10:41.614765  5096 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1123 16:10:41.614765  5096 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_7x7_300K"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 35
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 38
    pad: 3
    kernel_size: 7
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 16:10:41.643298  5096 layer_factory.cpp:58] Creating layer cifar
I1123 16:10:41.650804  5096 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1123 16:10:41.650804  5096 net.cpp:84] Creating Layer cifar
I1123 16:10:41.650804  5096 net.cpp:380] cifar -> data
I1123 16:10:41.650804  5096 net.cpp:380] cifar -> label
I1123 16:10:41.651814  5096 data_layer.cpp:45] output data size: 100,3,32,32
I1123 16:10:41.657819  5096 net.cpp:122] Setting up cifar
I1123 16:10:41.658814  5096 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 16:10:41.658814  5096 net.cpp:129] Top shape: 100 (100)
I1123 16:10:41.658814  5096 net.cpp:137] Memory required for data: 1229200
I1123 16:10:41.658814  5096 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 16:10:41.658814  5096 net.cpp:84] Creating Layer label_cifar_1_split
I1123 16:10:41.658814  5096 net.cpp:406] label_cifar_1_split <- label
I1123 16:10:41.658814  5096 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 16:10:41.658814  5096 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 16:10:41.658814  5096 net.cpp:122] Setting up label_cifar_1_split
I1123 16:10:41.658814  5096 net.cpp:129] Top shape: 100 (100)
I1123 16:10:41.658814  5096 net.cpp:129] Top shape: 100 (100)
I1123 16:10:41.658814  5096 net.cpp:137] Memory required for data: 1230000
I1123 16:10:41.658814  5096 layer_factory.cpp:58] Creating layer conv1
I1123 16:10:41.658814  5096 net.cpp:84] Creating Layer conv1
I1123 16:10:41.658814  5096 net.cpp:406] conv1 <- data
I1123 16:10:41.658814  5096 net.cpp:380] conv1 -> conv1
I1123 16:10:41.660815 27880 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 16:10:41.900863  5096 net.cpp:122] Setting up conv1
I1123 16:10:41.900863  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.900863  5096 net.cpp:137] Memory required for data: 9422000
I1123 16:10:41.900863  5096 layer_factory.cpp:58] Creating layer bn1
I1123 16:10:41.900863  5096 net.cpp:84] Creating Layer bn1
I1123 16:10:41.900863  5096 net.cpp:406] bn1 <- conv1
I1123 16:10:41.900863  5096 net.cpp:367] bn1 -> conv1 (in-place)
I1123 16:10:41.900863  5096 net.cpp:122] Setting up bn1
I1123 16:10:41.900863  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.900863  5096 net.cpp:137] Memory required for data: 17614000
I1123 16:10:41.900863  5096 layer_factory.cpp:58] Creating layer scale1
I1123 16:10:41.900863  5096 net.cpp:84] Creating Layer scale1
I1123 16:10:41.900863  5096 net.cpp:406] scale1 <- conv1
I1123 16:10:41.900863  5096 net.cpp:367] scale1 -> conv1 (in-place)
I1123 16:10:41.900863  5096 layer_factory.cpp:58] Creating layer scale1
I1123 16:10:41.900863  5096 net.cpp:122] Setting up scale1
I1123 16:10:41.900863  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.900863  5096 net.cpp:137] Memory required for data: 25806000
I1123 16:10:41.900863  5096 layer_factory.cpp:58] Creating layer relu1
I1123 16:10:41.900863  5096 net.cpp:84] Creating Layer relu1
I1123 16:10:41.900863  5096 net.cpp:406] relu1 <- conv1
I1123 16:10:41.900863  5096 net.cpp:367] relu1 -> conv1 (in-place)
I1123 16:10:41.900863  5096 net.cpp:122] Setting up relu1
I1123 16:10:41.900863  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.900863  5096 net.cpp:137] Memory required for data: 33998000
I1123 16:10:41.900863  5096 layer_factory.cpp:58] Creating layer conv2
I1123 16:10:41.900863  5096 net.cpp:84] Creating Layer conv2
I1123 16:10:41.900863  5096 net.cpp:406] conv2 <- conv1
I1123 16:10:41.900863  5096 net.cpp:380] conv2 -> conv2
I1123 16:10:41.902861  5096 net.cpp:122] Setting up conv2
I1123 16:10:41.902861  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.902861  5096 net.cpp:137] Memory required for data: 42190000
I1123 16:10:41.902861  5096 layer_factory.cpp:58] Creating layer bn2
I1123 16:10:41.902861  5096 net.cpp:84] Creating Layer bn2
I1123 16:10:41.902861  5096 net.cpp:406] bn2 <- conv2
I1123 16:10:41.902861  5096 net.cpp:367] bn2 -> conv2 (in-place)
I1123 16:10:41.902861  5096 net.cpp:122] Setting up bn2
I1123 16:10:41.902861  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.902861  5096 net.cpp:137] Memory required for data: 50382000
I1123 16:10:41.902861  5096 layer_factory.cpp:58] Creating layer scale2
I1123 16:10:41.902861  5096 net.cpp:84] Creating Layer scale2
I1123 16:10:41.902861  5096 net.cpp:406] scale2 <- conv2
I1123 16:10:41.902861  5096 net.cpp:367] scale2 -> conv2 (in-place)
I1123 16:10:41.902861  5096 layer_factory.cpp:58] Creating layer scale2
I1123 16:10:41.902861  5096 net.cpp:122] Setting up scale2
I1123 16:10:41.902861  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.903880  5096 net.cpp:137] Memory required for data: 58574000
I1123 16:10:41.903880  5096 layer_factory.cpp:58] Creating layer relu2
I1123 16:10:41.903880  5096 net.cpp:84] Creating Layer relu2
I1123 16:10:41.903880  5096 net.cpp:406] relu2 <- conv2
I1123 16:10:41.903880  5096 net.cpp:367] relu2 -> conv2 (in-place)
I1123 16:10:41.903880  5096 net.cpp:122] Setting up relu2
I1123 16:10:41.903880  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.903880  5096 net.cpp:137] Memory required for data: 66766000
I1123 16:10:41.903880  5096 layer_factory.cpp:58] Creating layer conv2_2
I1123 16:10:41.903880  5096 net.cpp:84] Creating Layer conv2_2
I1123 16:10:41.903880  5096 net.cpp:406] conv2_2 <- conv2
I1123 16:10:41.903880  5096 net.cpp:380] conv2_2 -> conv2_2
I1123 16:10:41.904875  5096 net.cpp:122] Setting up conv2_2
I1123 16:10:41.904875  5096 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 16:10:41.904875  5096 net.cpp:137] Memory required for data: 79054000
I1123 16:10:41.904875  5096 layer_factory.cpp:58] Creating layer bn2_2
I1123 16:10:41.904875  5096 net.cpp:84] Creating Layer bn2_2
I1123 16:10:41.904875  5096 net.cpp:406] bn2_2 <- conv2_2
I1123 16:10:41.904875  5096 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 16:10:41.904875  5096 net.cpp:122] Setting up bn2_2
I1123 16:10:41.904875  5096 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 16:10:41.904875  5096 net.cpp:137] Memory required for data: 91342000
I1123 16:10:41.904875  5096 layer_factory.cpp:58] Creating layer scale2_2
I1123 16:10:41.904875  5096 net.cpp:84] Creating Layer scale2_2
I1123 16:10:41.904875  5096 net.cpp:406] scale2_2 <- conv2_2
I1123 16:10:41.904875  5096 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 16:10:41.904875  5096 layer_factory.cpp:58] Creating layer scale2_2
I1123 16:10:41.904875  5096 net.cpp:122] Setting up scale2_2
I1123 16:10:41.904875  5096 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 16:10:41.904875  5096 net.cpp:137] Memory required for data: 103630000
I1123 16:10:41.904875  5096 layer_factory.cpp:58] Creating layer relu2_2
I1123 16:10:41.904875  5096 net.cpp:84] Creating Layer relu2_2
I1123 16:10:41.904875  5096 net.cpp:406] relu2_2 <- conv2_2
I1123 16:10:41.904875  5096 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 16:10:41.905875  5096 net.cpp:122] Setting up relu2_2
I1123 16:10:41.905875  5096 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 16:10:41.905875  5096 net.cpp:137] Memory required for data: 115918000
I1123 16:10:41.905875  5096 layer_factory.cpp:58] Creating layer pool2_1
I1123 16:10:41.905875  5096 net.cpp:84] Creating Layer pool2_1
I1123 16:10:41.905875  5096 net.cpp:406] pool2_1 <- conv2_2
I1123 16:10:41.905875  5096 net.cpp:380] pool2_1 -> pool2_1
I1123 16:10:41.905875  5096 net.cpp:122] Setting up pool2_1
I1123 16:10:41.905875  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.905875  5096 net.cpp:137] Memory required for data: 118990000
I1123 16:10:41.905875  5096 layer_factory.cpp:58] Creating layer conv3
I1123 16:10:41.905875  5096 net.cpp:84] Creating Layer conv3
I1123 16:10:41.905875  5096 net.cpp:406] conv3 <- pool2_1
I1123 16:10:41.905875  5096 net.cpp:380] conv3 -> conv3
I1123 16:10:41.906882  5096 net.cpp:122] Setting up conv3
I1123 16:10:41.906882  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.906882  5096 net.cpp:137] Memory required for data: 122062000
I1123 16:10:41.906882  5096 layer_factory.cpp:58] Creating layer bn3
I1123 16:10:41.906882  5096 net.cpp:84] Creating Layer bn3
I1123 16:10:41.906882  5096 net.cpp:406] bn3 <- conv3
I1123 16:10:41.906882  5096 net.cpp:367] bn3 -> conv3 (in-place)
I1123 16:10:41.906882  5096 net.cpp:122] Setting up bn3
I1123 16:10:41.906882  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.906882  5096 net.cpp:137] Memory required for data: 125134000
I1123 16:10:41.906882  5096 layer_factory.cpp:58] Creating layer scale3
I1123 16:10:41.906882  5096 net.cpp:84] Creating Layer scale3
I1123 16:10:41.906882  5096 net.cpp:406] scale3 <- conv3
I1123 16:10:41.906882  5096 net.cpp:367] scale3 -> conv3 (in-place)
I1123 16:10:41.906882  5096 layer_factory.cpp:58] Creating layer scale3
I1123 16:10:41.906882  5096 net.cpp:122] Setting up scale3
I1123 16:10:41.906882  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.906882  5096 net.cpp:137] Memory required for data: 128206000
I1123 16:10:41.906882  5096 layer_factory.cpp:58] Creating layer relu3
I1123 16:10:41.906882  5096 net.cpp:84] Creating Layer relu3
I1123 16:10:41.906882  5096 net.cpp:406] relu3 <- conv3
I1123 16:10:41.906882  5096 net.cpp:367] relu3 -> conv3 (in-place)
I1123 16:10:41.907881  5096 net.cpp:122] Setting up relu3
I1123 16:10:41.907881  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.907881  5096 net.cpp:137] Memory required for data: 131278000
I1123 16:10:41.907881  5096 layer_factory.cpp:58] Creating layer conv4
I1123 16:10:41.907881  5096 net.cpp:84] Creating Layer conv4
I1123 16:10:41.907881  5096 net.cpp:406] conv4 <- conv3
I1123 16:10:41.907881  5096 net.cpp:380] conv4 -> conv4
I1123 16:10:41.908879  5096 net.cpp:122] Setting up conv4
I1123 16:10:41.908879  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.908879  5096 net.cpp:137] Memory required for data: 134350000
I1123 16:10:41.908879  5096 layer_factory.cpp:58] Creating layer bn4
I1123 16:10:41.908879  5096 net.cpp:84] Creating Layer bn4
I1123 16:10:41.908879  5096 net.cpp:406] bn4 <- conv4
I1123 16:10:41.908879  5096 net.cpp:367] bn4 -> conv4 (in-place)
I1123 16:10:41.908879  5096 net.cpp:122] Setting up bn4
I1123 16:10:41.908879  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.908879  5096 net.cpp:137] Memory required for data: 137422000
I1123 16:10:41.908879  5096 layer_factory.cpp:58] Creating layer scale4
I1123 16:10:41.908879  5096 net.cpp:84] Creating Layer scale4
I1123 16:10:41.908879  5096 net.cpp:406] scale4 <- conv4
I1123 16:10:41.908879  5096 net.cpp:367] scale4 -> conv4 (in-place)
I1123 16:10:41.908879  5096 layer_factory.cpp:58] Creating layer scale4
I1123 16:10:41.908879  5096 net.cpp:122] Setting up scale4
I1123 16:10:41.908879  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.908879  5096 net.cpp:137] Memory required for data: 140494000
I1123 16:10:41.908879  5096 layer_factory.cpp:58] Creating layer relu4
I1123 16:10:41.908879  5096 net.cpp:84] Creating Layer relu4
I1123 16:10:41.908879  5096 net.cpp:406] relu4 <- conv4
I1123 16:10:41.908879  5096 net.cpp:367] relu4 -> conv4 (in-place)
I1123 16:10:41.909879  5096 net.cpp:122] Setting up relu4
I1123 16:10:41.909879  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.909879  5096 net.cpp:137] Memory required for data: 143566000
I1123 16:10:41.909879  5096 layer_factory.cpp:58] Creating layer conv4_1
I1123 16:10:41.909879  5096 net.cpp:84] Creating Layer conv4_1
I1123 16:10:41.909879  5096 net.cpp:406] conv4_1 <- conv4
I1123 16:10:41.909879  5096 net.cpp:380] conv4_1 -> conv4_1
I1123 16:10:41.910879  5096 net.cpp:122] Setting up conv4_1
I1123 16:10:41.910879  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.910879  5096 net.cpp:137] Memory required for data: 146638000
I1123 16:10:41.910879  5096 layer_factory.cpp:58] Creating layer bn4_1
I1123 16:10:41.910879  5096 net.cpp:84] Creating Layer bn4_1
I1123 16:10:41.910879  5096 net.cpp:406] bn4_1 <- conv4_1
I1123 16:10:41.910879  5096 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 16:10:41.910879  5096 net.cpp:122] Setting up bn4_1
I1123 16:10:41.910879  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.910879  5096 net.cpp:137] Memory required for data: 149710000
I1123 16:10:41.910879  5096 layer_factory.cpp:58] Creating layer scale4_1
I1123 16:10:41.910879  5096 net.cpp:84] Creating Layer scale4_1
I1123 16:10:41.910879  5096 net.cpp:406] scale4_1 <- conv4_1
I1123 16:10:41.910879  5096 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 16:10:41.910879  5096 layer_factory.cpp:58] Creating layer scale4_1
I1123 16:10:41.910879  5096 net.cpp:122] Setting up scale4_1
I1123 16:10:41.910879  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.910879  5096 net.cpp:137] Memory required for data: 152782000
I1123 16:10:41.910879  5096 layer_factory.cpp:58] Creating layer relu4_1
I1123 16:10:41.910879  5096 net.cpp:84] Creating Layer relu4_1
I1123 16:10:41.910879  5096 net.cpp:406] relu4_1 <- conv4_1
I1123 16:10:41.910879  5096 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 16:10:41.911880  5096 net.cpp:122] Setting up relu4_1
I1123 16:10:41.911880  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.911880  5096 net.cpp:137] Memory required for data: 155854000
I1123 16:10:41.911880  5096 layer_factory.cpp:58] Creating layer conv4_2
I1123 16:10:41.911880  5096 net.cpp:84] Creating Layer conv4_2
I1123 16:10:41.911880  5096 net.cpp:406] conv4_2 <- conv4_1
I1123 16:10:41.911880  5096 net.cpp:380] conv4_2 -> conv4_2
I1123 16:10:41.912880  5096 net.cpp:122] Setting up conv4_2
I1123 16:10:41.912880  5096 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 16:10:41.912880  5096 net.cpp:137] Memory required for data: 159438000
I1123 16:10:41.912880  5096 layer_factory.cpp:58] Creating layer bn4_2
I1123 16:10:41.912880  5096 net.cpp:84] Creating Layer bn4_2
I1123 16:10:41.912880  5096 net.cpp:406] bn4_2 <- conv4_2
I1123 16:10:41.912880  5096 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 16:10:41.912880  5096 net.cpp:122] Setting up bn4_2
I1123 16:10:41.912880  5096 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 16:10:41.912880  5096 net.cpp:137] Memory required for data: 163022000
I1123 16:10:41.912880  5096 layer_factory.cpp:58] Creating layer scale4_2
I1123 16:10:41.912880  5096 net.cpp:84] Creating Layer scale4_2
I1123 16:10:41.912880  5096 net.cpp:406] scale4_2 <- conv4_2
I1123 16:10:41.912880  5096 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 16:10:41.912880  5096 layer_factory.cpp:58] Creating layer scale4_2
I1123 16:10:41.913880  5096 net.cpp:122] Setting up scale4_2
I1123 16:10:41.913880  5096 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 16:10:41.913880  5096 net.cpp:137] Memory required for data: 166606000
I1123 16:10:41.913880  5096 layer_factory.cpp:58] Creating layer relu4_2
I1123 16:10:41.913880  5096 net.cpp:84] Creating Layer relu4_2
I1123 16:10:41.913880  5096 net.cpp:406] relu4_2 <- conv4_2
I1123 16:10:41.913880  5096 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 16:10:41.913880  5096 net.cpp:122] Setting up relu4_2
I1123 16:10:41.913880  5096 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 16:10:41.913880  5096 net.cpp:137] Memory required for data: 170190000
I1123 16:10:41.913880  5096 layer_factory.cpp:58] Creating layer pool4_2
I1123 16:10:41.913880  5096 net.cpp:84] Creating Layer pool4_2
I1123 16:10:41.913880  5096 net.cpp:406] pool4_2 <- conv4_2
I1123 16:10:41.913880  5096 net.cpp:380] pool4_2 -> pool4_2
I1123 16:10:41.913880  5096 net.cpp:122] Setting up pool4_2
I1123 16:10:41.913880  5096 net.cpp:129] Top shape: 100 35 8 8 (224000)
I1123 16:10:41.913880  5096 net.cpp:137] Memory required for data: 171086000
I1123 16:10:41.913880  5096 layer_factory.cpp:58] Creating layer conv12
I1123 16:10:41.913880  5096 net.cpp:84] Creating Layer conv12
I1123 16:10:41.913880  5096 net.cpp:406] conv12 <- pool4_2
I1123 16:10:41.913880  5096 net.cpp:380] conv12 -> conv12
I1123 16:10:41.914880  5096 net.cpp:122] Setting up conv12
I1123 16:10:41.914880  5096 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 16:10:41.914880  5096 net.cpp:137] Memory required for data: 172058800
I1123 16:10:41.914880  5096 layer_factory.cpp:58] Creating layer bn_conv12
I1123 16:10:41.914880  5096 net.cpp:84] Creating Layer bn_conv12
I1123 16:10:41.915879  5096 net.cpp:406] bn_conv12 <- conv12
I1123 16:10:41.915879  5096 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 16:10:41.915879  5096 net.cpp:122] Setting up bn_conv12
I1123 16:10:41.915879  5096 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 16:10:41.915879  5096 net.cpp:137] Memory required for data: 173031600
I1123 16:10:41.915879  5096 layer_factory.cpp:58] Creating layer scale_conv12
I1123 16:10:41.915879  5096 net.cpp:84] Creating Layer scale_conv12
I1123 16:10:41.915879  5096 net.cpp:406] scale_conv12 <- conv12
I1123 16:10:41.915879  5096 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 16:10:41.915879  5096 layer_factory.cpp:58] Creating layer scale_conv12
I1123 16:10:41.915879  5096 net.cpp:122] Setting up scale_conv12
I1123 16:10:41.915879  5096 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 16:10:41.915879  5096 net.cpp:137] Memory required for data: 174004400
I1123 16:10:41.915879  5096 layer_factory.cpp:58] Creating layer relu_conv12
I1123 16:10:41.915879  5096 net.cpp:84] Creating Layer relu_conv12
I1123 16:10:41.915879  5096 net.cpp:406] relu_conv12 <- conv12
I1123 16:10:41.915879  5096 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 16:10:41.915879  5096 net.cpp:122] Setting up relu_conv12
I1123 16:10:41.915879  5096 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 16:10:41.915879  5096 net.cpp:137] Memory required for data: 174977200
I1123 16:10:41.915879  5096 layer_factory.cpp:58] Creating layer poolcp6
I1123 16:10:41.915879  5096 net.cpp:84] Creating Layer poolcp6
I1123 16:10:41.915879  5096 net.cpp:406] poolcp6 <- conv12
I1123 16:10:41.915879  5096 net.cpp:380] poolcp6 -> poolcp6
I1123 16:10:41.915879  5096 net.cpp:122] Setting up poolcp6
I1123 16:10:41.915879  5096 net.cpp:129] Top shape: 100 38 1 1 (3800)
I1123 16:10:41.915879  5096 net.cpp:137] Memory required for data: 174992400
I1123 16:10:41.915879  5096 layer_factory.cpp:58] Creating layer ip1
I1123 16:10:41.915879  5096 net.cpp:84] Creating Layer ip1
I1123 16:10:41.915879  5096 net.cpp:406] ip1 <- poolcp6
I1123 16:10:41.915879  5096 net.cpp:380] ip1 -> ip1
I1123 16:10:41.915879  5096 net.cpp:122] Setting up ip1
I1123 16:10:41.915879  5096 net.cpp:129] Top shape: 100 10 (1000)
I1123 16:10:41.915879  5096 net.cpp:137] Memory required for data: 174996400
I1123 16:10:41.915879  5096 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 16:10:41.915879  5096 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 16:10:41.915879  5096 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 16:10:41.915879  5096 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 16:10:41.915879  5096 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 16:10:41.916879  5096 net.cpp:122] Setting up ip1_ip1_0_split
I1123 16:10:41.916879  5096 net.cpp:129] Top shape: 100 10 (1000)
I1123 16:10:41.916879  5096 net.cpp:129] Top shape: 100 10 (1000)
I1123 16:10:41.916879  5096 net.cpp:137] Memory required for data: 175004400
I1123 16:10:41.916879  5096 layer_factory.cpp:58] Creating layer accuracy_training
I1123 16:10:41.916879  5096 net.cpp:84] Creating Layer accuracy_training
I1123 16:10:41.916879  5096 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1123 16:10:41.916879  5096 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1123 16:10:41.916879  5096 net.cpp:380] accuracy_training -> accuracy_training
I1123 16:10:41.916879  5096 net.cpp:122] Setting up accuracy_training
I1123 16:10:41.916879  5096 net.cpp:129] Top shape: (1)
I1123 16:10:41.916879  5096 net.cpp:137] Memory required for data: 175004404
I1123 16:10:41.916879  5096 layer_factory.cpp:58] Creating layer loss
I1123 16:10:41.916879  5096 net.cpp:84] Creating Layer loss
I1123 16:10:41.916879  5096 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 16:10:41.916879  5096 net.cpp:406] loss <- label_cifar_1_split_1
I1123 16:10:41.916879  5096 net.cpp:380] loss -> loss
I1123 16:10:41.916879  5096 layer_factory.cpp:58] Creating layer loss
I1123 16:10:41.916879  5096 net.cpp:122] Setting up loss
I1123 16:10:41.916879  5096 net.cpp:129] Top shape: (1)
I1123 16:10:41.916879  5096 net.cpp:132]     with loss weight 1
I1123 16:10:41.916879  5096 net.cpp:137] Memory required for data: 175004408
I1123 16:10:41.916879  5096 net.cpp:198] loss needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:200] accuracy_training does not need backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] ip1 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] poolcp6 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] relu_conv12 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] scale_conv12 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] bn_conv12 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] conv12 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] pool4_2 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] relu4_2 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] scale4_2 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] bn4_2 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] conv4_2 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] relu4_1 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] scale4_1 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] bn4_1 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] conv4_1 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] relu4 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] scale4 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] bn4 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] conv4 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] relu3 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] scale3 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] bn3 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] conv3 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] pool2_1 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] relu2_2 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] scale2_2 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] bn2_2 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] conv2_2 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] relu2 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] scale2 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] bn2 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] conv2 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] relu1 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] scale1 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] bn1 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:198] conv1 needs backward computation.
I1123 16:10:41.916879  5096 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 16:10:41.916879  5096 net.cpp:200] cifar does not need backward computation.
I1123 16:10:41.916879  5096 net.cpp:242] This network produces output accuracy_training
I1123 16:10:41.916879  5096 net.cpp:242] This network produces output loss
I1123 16:10:41.916879  5096 net.cpp:255] Network initialization done.
I1123 16:10:41.917879  5096 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 16:10:41.917879  5096 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 16:10:41.917879  5096 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 16:10:41.917879  5096 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1123 16:10:41.917879  5096 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1123 16:10:41.917879  5096 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1123 16:10:41.917879  5096 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1123 16:10:41.917879  5096 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1123 16:10:41.917879  5096 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1123 16:10:41.917879  5096 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1123 16:10:41.917879  5096 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1123 16:10:41.917879  5096 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1123 16:10:41.917879  5096 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1123 16:10:41.917879  5096 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_7x7_300K"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 35
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 38
    pad: 3
    kernel_size: 7
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 16:10:41.917879  5096 layer_factory.cpp:58] Creating layer cifar
I1123 16:10:41.923863  5096 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1123 16:10:41.923863  5096 net.cpp:84] Creating Layer cifar
I1123 16:10:41.923863  5096 net.cpp:380] cifar -> data
I1123 16:10:41.923863  5096 net.cpp:380] cifar -> label
I1123 16:10:41.923863  5096 data_layer.cpp:45] output data size: 100,3,32,32
I1123 16:10:41.929898  5096 net.cpp:122] Setting up cifar
I1123 16:10:41.929898  5096 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 16:10:41.929898  5096 net.cpp:129] Top shape: 100 (100)
I1123 16:10:41.929898  5096 net.cpp:137] Memory required for data: 1229200
I1123 16:10:41.929898  5096 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 16:10:41.930390  5096 net.cpp:84] Creating Layer label_cifar_1_split
I1123 16:10:41.930390  5096 net.cpp:406] label_cifar_1_split <- label
I1123 16:10:41.930390  5096 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 16:10:41.930390  5096 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 16:10:41.930390  5096 net.cpp:122] Setting up label_cifar_1_split
I1123 16:10:41.930390  5096 net.cpp:129] Top shape: 100 (100)
I1123 16:10:41.930390  5096 net.cpp:129] Top shape: 100 (100)
I1123 16:10:41.930390  5096 net.cpp:137] Memory required for data: 1230000
I1123 16:10:41.930390  5096 layer_factory.cpp:58] Creating layer conv1
I1123 16:10:41.930390  5096 net.cpp:84] Creating Layer conv1
I1123 16:10:41.930390  5096 net.cpp:406] conv1 <- data
I1123 16:10:41.930390  5096 net.cpp:380] conv1 -> conv1
I1123 16:10:41.930891 30692 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 16:10:41.931890  5096 net.cpp:122] Setting up conv1
I1123 16:10:41.931890  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.931890  5096 net.cpp:137] Memory required for data: 9422000
I1123 16:10:41.931890  5096 layer_factory.cpp:58] Creating layer bn1
I1123 16:10:41.931890  5096 net.cpp:84] Creating Layer bn1
I1123 16:10:41.931890  5096 net.cpp:406] bn1 <- conv1
I1123 16:10:41.931890  5096 net.cpp:367] bn1 -> conv1 (in-place)
I1123 16:10:41.931890  5096 net.cpp:122] Setting up bn1
I1123 16:10:41.931890  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.931890  5096 net.cpp:137] Memory required for data: 17614000
I1123 16:10:41.932390  5096 layer_factory.cpp:58] Creating layer scale1
I1123 16:10:41.932390  5096 net.cpp:84] Creating Layer scale1
I1123 16:10:41.932390  5096 net.cpp:406] scale1 <- conv1
I1123 16:10:41.932390  5096 net.cpp:367] scale1 -> conv1 (in-place)
I1123 16:10:41.932390  5096 layer_factory.cpp:58] Creating layer scale1
I1123 16:10:41.932390  5096 net.cpp:122] Setting up scale1
I1123 16:10:41.932390  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.932390  5096 net.cpp:137] Memory required for data: 25806000
I1123 16:10:41.932390  5096 layer_factory.cpp:58] Creating layer relu1
I1123 16:10:41.932390  5096 net.cpp:84] Creating Layer relu1
I1123 16:10:41.932390  5096 net.cpp:406] relu1 <- conv1
I1123 16:10:41.932390  5096 net.cpp:367] relu1 -> conv1 (in-place)
I1123 16:10:41.932889  5096 net.cpp:122] Setting up relu1
I1123 16:10:41.932889  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.932889  5096 net.cpp:137] Memory required for data: 33998000
I1123 16:10:41.932889  5096 layer_factory.cpp:58] Creating layer conv2
I1123 16:10:41.932889  5096 net.cpp:84] Creating Layer conv2
I1123 16:10:41.932889  5096 net.cpp:406] conv2 <- conv1
I1123 16:10:41.932889  5096 net.cpp:380] conv2 -> conv2
I1123 16:10:41.933890  5096 net.cpp:122] Setting up conv2
I1123 16:10:41.933890  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.933890  5096 net.cpp:137] Memory required for data: 42190000
I1123 16:10:41.933890  5096 layer_factory.cpp:58] Creating layer bn2
I1123 16:10:41.933890  5096 net.cpp:84] Creating Layer bn2
I1123 16:10:41.933890  5096 net.cpp:406] bn2 <- conv2
I1123 16:10:41.933890  5096 net.cpp:367] bn2 -> conv2 (in-place)
I1123 16:10:41.934391  5096 net.cpp:122] Setting up bn2
I1123 16:10:41.934391  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.934391  5096 net.cpp:137] Memory required for data: 50382000
I1123 16:10:41.934391  5096 layer_factory.cpp:58] Creating layer scale2
I1123 16:10:41.934391  5096 net.cpp:84] Creating Layer scale2
I1123 16:10:41.934391  5096 net.cpp:406] scale2 <- conv2
I1123 16:10:41.934391  5096 net.cpp:367] scale2 -> conv2 (in-place)
I1123 16:10:41.934391  5096 layer_factory.cpp:58] Creating layer scale2
I1123 16:10:41.934391  5096 net.cpp:122] Setting up scale2
I1123 16:10:41.934391  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.934391  5096 net.cpp:137] Memory required for data: 58574000
I1123 16:10:41.934890  5096 layer_factory.cpp:58] Creating layer relu2
I1123 16:10:41.934890  5096 net.cpp:84] Creating Layer relu2
I1123 16:10:41.934890  5096 net.cpp:406] relu2 <- conv2
I1123 16:10:41.934890  5096 net.cpp:367] relu2 -> conv2 (in-place)
I1123 16:10:41.934890  5096 net.cpp:122] Setting up relu2
I1123 16:10:41.935389  5096 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 16:10:41.935389  5096 net.cpp:137] Memory required for data: 66766000
I1123 16:10:41.935389  5096 layer_factory.cpp:58] Creating layer conv2_2
I1123 16:10:41.935389  5096 net.cpp:84] Creating Layer conv2_2
I1123 16:10:41.935389  5096 net.cpp:406] conv2_2 <- conv2
I1123 16:10:41.935389  5096 net.cpp:380] conv2_2 -> conv2_2
I1123 16:10:41.936390  5096 net.cpp:122] Setting up conv2_2
I1123 16:10:41.936390  5096 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 16:10:41.936390  5096 net.cpp:137] Memory required for data: 79054000
I1123 16:10:41.936390  5096 layer_factory.cpp:58] Creating layer bn2_2
I1123 16:10:41.936390  5096 net.cpp:84] Creating Layer bn2_2
I1123 16:10:41.936390  5096 net.cpp:406] bn2_2 <- conv2_2
I1123 16:10:41.936390  5096 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 16:10:41.936890  5096 net.cpp:122] Setting up bn2_2
I1123 16:10:41.936890  5096 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 16:10:41.936890  5096 net.cpp:137] Memory required for data: 91342000
I1123 16:10:41.936890  5096 layer_factory.cpp:58] Creating layer scale2_2
I1123 16:10:41.936890  5096 net.cpp:84] Creating Layer scale2_2
I1123 16:10:41.936890  5096 net.cpp:406] scale2_2 <- conv2_2
I1123 16:10:41.936890  5096 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 16:10:41.936890  5096 layer_factory.cpp:58] Creating layer scale2_2
I1123 16:10:41.936890  5096 net.cpp:122] Setting up scale2_2
I1123 16:10:41.936890  5096 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 16:10:41.936890  5096 net.cpp:137] Memory required for data: 103630000
I1123 16:10:41.937391  5096 layer_factory.cpp:58] Creating layer relu2_2
I1123 16:10:41.937391  5096 net.cpp:84] Creating Layer relu2_2
I1123 16:10:41.937391  5096 net.cpp:406] relu2_2 <- conv2_2
I1123 16:10:41.937391  5096 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 16:10:41.937391  5096 net.cpp:122] Setting up relu2_2
I1123 16:10:41.937391  5096 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 16:10:41.937391  5096 net.cpp:137] Memory required for data: 115918000
I1123 16:10:41.937391  5096 layer_factory.cpp:58] Creating layer pool2_1
I1123 16:10:41.937391  5096 net.cpp:84] Creating Layer pool2_1
I1123 16:10:41.937391  5096 net.cpp:406] pool2_1 <- conv2_2
I1123 16:10:41.937391  5096 net.cpp:380] pool2_1 -> pool2_1
I1123 16:10:41.937391  5096 net.cpp:122] Setting up pool2_1
I1123 16:10:41.937391  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.937391  5096 net.cpp:137] Memory required for data: 118990000
I1123 16:10:41.937391  5096 layer_factory.cpp:58] Creating layer conv3
I1123 16:10:41.937391  5096 net.cpp:84] Creating Layer conv3
I1123 16:10:41.937391  5096 net.cpp:406] conv3 <- pool2_1
I1123 16:10:41.937391  5096 net.cpp:380] conv3 -> conv3
I1123 16:10:41.938889  5096 net.cpp:122] Setting up conv3
I1123 16:10:41.938889  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.938889  5096 net.cpp:137] Memory required for data: 122062000
I1123 16:10:41.938889  5096 layer_factory.cpp:58] Creating layer bn3
I1123 16:10:41.938889  5096 net.cpp:84] Creating Layer bn3
I1123 16:10:41.938889  5096 net.cpp:406] bn3 <- conv3
I1123 16:10:41.938889  5096 net.cpp:367] bn3 -> conv3 (in-place)
I1123 16:10:41.938889  5096 net.cpp:122] Setting up bn3
I1123 16:10:41.938889  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.938889  5096 net.cpp:137] Memory required for data: 125134000
I1123 16:10:41.938889  5096 layer_factory.cpp:58] Creating layer scale3
I1123 16:10:41.938889  5096 net.cpp:84] Creating Layer scale3
I1123 16:10:41.938889  5096 net.cpp:406] scale3 <- conv3
I1123 16:10:41.938889  5096 net.cpp:367] scale3 -> conv3 (in-place)
I1123 16:10:41.939390  5096 layer_factory.cpp:58] Creating layer scale3
I1123 16:10:41.939390  5096 net.cpp:122] Setting up scale3
I1123 16:10:41.939390  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.939390  5096 net.cpp:137] Memory required for data: 128206000
I1123 16:10:41.939390  5096 layer_factory.cpp:58] Creating layer relu3
I1123 16:10:41.939390  5096 net.cpp:84] Creating Layer relu3
I1123 16:10:41.939390  5096 net.cpp:406] relu3 <- conv3
I1123 16:10:41.939390  5096 net.cpp:367] relu3 -> conv3 (in-place)
I1123 16:10:41.939890  5096 net.cpp:122] Setting up relu3
I1123 16:10:41.939890  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.939890  5096 net.cpp:137] Memory required for data: 131278000
I1123 16:10:41.939890  5096 layer_factory.cpp:58] Creating layer conv4
I1123 16:10:41.939890  5096 net.cpp:84] Creating Layer conv4
I1123 16:10:41.939890  5096 net.cpp:406] conv4 <- conv3
I1123 16:10:41.939890  5096 net.cpp:380] conv4 -> conv4
I1123 16:10:41.941391  5096 net.cpp:122] Setting up conv4
I1123 16:10:41.941391  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.941391  5096 net.cpp:137] Memory required for data: 134350000
I1123 16:10:41.941391  5096 layer_factory.cpp:58] Creating layer bn4
I1123 16:10:41.941391  5096 net.cpp:84] Creating Layer bn4
I1123 16:10:41.941391  5096 net.cpp:406] bn4 <- conv4
I1123 16:10:41.941391  5096 net.cpp:367] bn4 -> conv4 (in-place)
I1123 16:10:41.941391  5096 net.cpp:122] Setting up bn4
I1123 16:10:41.941391  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.941391  5096 net.cpp:137] Memory required for data: 137422000
I1123 16:10:41.941391  5096 layer_factory.cpp:58] Creating layer scale4
I1123 16:10:41.941391  5096 net.cpp:84] Creating Layer scale4
I1123 16:10:41.941391  5096 net.cpp:406] scale4 <- conv4
I1123 16:10:41.941391  5096 net.cpp:367] scale4 -> conv4 (in-place)
I1123 16:10:41.941391  5096 layer_factory.cpp:58] Creating layer scale4
I1123 16:10:41.941391  5096 net.cpp:122] Setting up scale4
I1123 16:10:41.941391  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.941391  5096 net.cpp:137] Memory required for data: 140494000
I1123 16:10:41.941910  5096 layer_factory.cpp:58] Creating layer relu4
I1123 16:10:41.941910  5096 net.cpp:84] Creating Layer relu4
I1123 16:10:41.941910  5096 net.cpp:406] relu4 <- conv4
I1123 16:10:41.941910  5096 net.cpp:367] relu4 -> conv4 (in-place)
I1123 16:10:41.941910  5096 net.cpp:122] Setting up relu4
I1123 16:10:41.941910  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.941910  5096 net.cpp:137] Memory required for data: 143566000
I1123 16:10:41.941910  5096 layer_factory.cpp:58] Creating layer conv4_1
I1123 16:10:41.941910  5096 net.cpp:84] Creating Layer conv4_1
I1123 16:10:41.941910  5096 net.cpp:406] conv4_1 <- conv4
I1123 16:10:41.941910  5096 net.cpp:380] conv4_1 -> conv4_1
I1123 16:10:41.943393  5096 net.cpp:122] Setting up conv4_1
I1123 16:10:41.943393  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.943393  5096 net.cpp:137] Memory required for data: 146638000
I1123 16:10:41.943393  5096 layer_factory.cpp:58] Creating layer bn4_1
I1123 16:10:41.943393  5096 net.cpp:84] Creating Layer bn4_1
I1123 16:10:41.943393  5096 net.cpp:406] bn4_1 <- conv4_1
I1123 16:10:41.943393  5096 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 16:10:41.943393  5096 net.cpp:122] Setting up bn4_1
I1123 16:10:41.943393  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.943393  5096 net.cpp:137] Memory required for data: 149710000
I1123 16:10:41.943393  5096 layer_factory.cpp:58] Creating layer scale4_1
I1123 16:10:41.943393  5096 net.cpp:84] Creating Layer scale4_1
I1123 16:10:41.943393  5096 net.cpp:406] scale4_1 <- conv4_1
I1123 16:10:41.943393  5096 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 16:10:41.943393  5096 layer_factory.cpp:58] Creating layer scale4_1
I1123 16:10:41.943393  5096 net.cpp:122] Setting up scale4_1
I1123 16:10:41.943393  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.943393  5096 net.cpp:137] Memory required for data: 152782000
I1123 16:10:41.943393  5096 layer_factory.cpp:58] Creating layer relu4_1
I1123 16:10:41.943393  5096 net.cpp:84] Creating Layer relu4_1
I1123 16:10:41.943393  5096 net.cpp:406] relu4_1 <- conv4_1
I1123 16:10:41.943393  5096 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 16:10:41.943393  5096 net.cpp:122] Setting up relu4_1
I1123 16:10:41.943393  5096 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 16:10:41.943393  5096 net.cpp:137] Memory required for data: 155854000
I1123 16:10:41.943393  5096 layer_factory.cpp:58] Creating layer conv4_2
I1123 16:10:41.943393  5096 net.cpp:84] Creating Layer conv4_2
I1123 16:10:41.943393  5096 net.cpp:406] conv4_2 <- conv4_1
I1123 16:10:41.943393  5096 net.cpp:380] conv4_2 -> conv4_2
I1123 16:10:41.945497  5096 net.cpp:122] Setting up conv4_2
I1123 16:10:41.945497  5096 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 16:10:41.945497  5096 net.cpp:137] Memory required for data: 159438000
I1123 16:10:41.945497  5096 layer_factory.cpp:58] Creating layer bn4_2
I1123 16:10:41.945497  5096 net.cpp:84] Creating Layer bn4_2
I1123 16:10:41.945497  5096 net.cpp:406] bn4_2 <- conv4_2
I1123 16:10:41.945497  5096 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 16:10:41.946496  5096 net.cpp:122] Setting up bn4_2
I1123 16:10:41.946496  5096 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 16:10:41.946496  5096 net.cpp:137] Memory required for data: 163022000
I1123 16:10:41.946496  5096 layer_factory.cpp:58] Creating layer scale4_2
I1123 16:10:41.946496  5096 net.cpp:84] Creating Layer scale4_2
I1123 16:10:41.946496  5096 net.cpp:406] scale4_2 <- conv4_2
I1123 16:10:41.946496  5096 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 16:10:41.946496  5096 layer_factory.cpp:58] Creating layer scale4_2
I1123 16:10:41.946496  5096 net.cpp:122] Setting up scale4_2
I1123 16:10:41.946496  5096 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 16:10:41.946496  5096 net.cpp:137] Memory required for data: 166606000
I1123 16:10:41.946496  5096 layer_factory.cpp:58] Creating layer relu4_2
I1123 16:10:41.946496  5096 net.cpp:84] Creating Layer relu4_2
I1123 16:10:41.946496  5096 net.cpp:406] relu4_2 <- conv4_2
I1123 16:10:41.946496  5096 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 16:10:41.946496  5096 net.cpp:122] Setting up relu4_2
I1123 16:10:41.946496  5096 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 16:10:41.946496  5096 net.cpp:137] Memory required for data: 170190000
I1123 16:10:41.946496  5096 layer_factory.cpp:58] Creating layer pool4_2
I1123 16:10:41.946496  5096 net.cpp:84] Creating Layer pool4_2
I1123 16:10:41.946496  5096 net.cpp:406] pool4_2 <- conv4_2
I1123 16:10:41.946496  5096 net.cpp:380] pool4_2 -> pool4_2
I1123 16:10:41.946496  5096 net.cpp:122] Setting up pool4_2
I1123 16:10:41.946496  5096 net.cpp:129] Top shape: 100 35 8 8 (224000)
I1123 16:10:41.946496  5096 net.cpp:137] Memory required for data: 171086000
I1123 16:10:41.946496  5096 layer_factory.cpp:58] Creating layer conv12
I1123 16:10:41.946496  5096 net.cpp:84] Creating Layer conv12
I1123 16:10:41.946496  5096 net.cpp:406] conv12 <- pool4_2
I1123 16:10:41.946496  5096 net.cpp:380] conv12 -> conv12
I1123 16:10:41.947496  5096 net.cpp:122] Setting up conv12
I1123 16:10:41.947496  5096 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 16:10:41.947496  5096 net.cpp:137] Memory required for data: 172058800
I1123 16:10:41.948496  5096 layer_factory.cpp:58] Creating layer bn_conv12
I1123 16:10:41.948496  5096 net.cpp:84] Creating Layer bn_conv12
I1123 16:10:41.948496  5096 net.cpp:406] bn_conv12 <- conv12
I1123 16:10:41.948496  5096 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 16:10:41.948496  5096 net.cpp:122] Setting up bn_conv12
I1123 16:10:41.948496  5096 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 16:10:41.948496  5096 net.cpp:137] Memory required for data: 173031600
I1123 16:10:41.948496  5096 layer_factory.cpp:58] Creating layer scale_conv12
I1123 16:10:41.948496  5096 net.cpp:84] Creating Layer scale_conv12
I1123 16:10:41.948496  5096 net.cpp:406] scale_conv12 <- conv12
I1123 16:10:41.948496  5096 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 16:10:41.948496  5096 layer_factory.cpp:58] Creating layer scale_conv12
I1123 16:10:41.948496  5096 net.cpp:122] Setting up scale_conv12
I1123 16:10:41.948496  5096 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 16:10:41.948496  5096 net.cpp:137] Memory required for data: 174004400
I1123 16:10:41.948496  5096 layer_factory.cpp:58] Creating layer relu_conv12
I1123 16:10:41.948496  5096 net.cpp:84] Creating Layer relu_conv12
I1123 16:10:41.948496  5096 net.cpp:406] relu_conv12 <- conv12
I1123 16:10:41.948496  5096 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 16:10:41.948496  5096 net.cpp:122] Setting up relu_conv12
I1123 16:10:41.948496  5096 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 16:10:41.948496  5096 net.cpp:137] Memory required for data: 174977200
I1123 16:10:41.948496  5096 layer_factory.cpp:58] Creating layer poolcp6
I1123 16:10:41.949496  5096 net.cpp:84] Creating Layer poolcp6
I1123 16:10:41.949496  5096 net.cpp:406] poolcp6 <- conv12
I1123 16:10:41.949496  5096 net.cpp:380] poolcp6 -> poolcp6
I1123 16:10:41.949496  5096 net.cpp:122] Setting up poolcp6
I1123 16:10:41.949496  5096 net.cpp:129] Top shape: 100 38 1 1 (3800)
I1123 16:10:41.949496  5096 net.cpp:137] Memory required for data: 174992400
I1123 16:10:41.949496  5096 layer_factory.cpp:58] Creating layer ip1
I1123 16:10:41.949496  5096 net.cpp:84] Creating Layer ip1
I1123 16:10:41.949496  5096 net.cpp:406] ip1 <- poolcp6
I1123 16:10:41.949496  5096 net.cpp:380] ip1 -> ip1
I1123 16:10:41.949496  5096 net.cpp:122] Setting up ip1
I1123 16:10:41.949496  5096 net.cpp:129] Top shape: 100 10 (1000)
I1123 16:10:41.949496  5096 net.cpp:137] Memory required for data: 174996400
I1123 16:10:41.949496  5096 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 16:10:41.949496  5096 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 16:10:41.949496  5096 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 16:10:41.949496  5096 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 16:10:41.949496  5096 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 16:10:41.949496  5096 net.cpp:122] Setting up ip1_ip1_0_split
I1123 16:10:41.949496  5096 net.cpp:129] Top shape: 100 10 (1000)
I1123 16:10:41.949496  5096 net.cpp:129] Top shape: 100 10 (1000)
I1123 16:10:41.949496  5096 net.cpp:137] Memory required for data: 175004400
I1123 16:10:41.949496  5096 layer_factory.cpp:58] Creating layer accuracy
I1123 16:10:41.949496  5096 net.cpp:84] Creating Layer accuracy
I1123 16:10:41.949496  5096 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1123 16:10:41.949496  5096 net.cpp:406] accuracy <- label_cifar_1_split_0
I1123 16:10:41.949496  5096 net.cpp:380] accuracy -> accuracy
I1123 16:10:41.949496  5096 net.cpp:122] Setting up accuracy
I1123 16:10:41.949496  5096 net.cpp:129] Top shape: (1)
I1123 16:10:41.949496  5096 net.cpp:137] Memory required for data: 175004404
I1123 16:10:41.949496  5096 layer_factory.cpp:58] Creating layer loss
I1123 16:10:41.949496  5096 net.cpp:84] Creating Layer loss
I1123 16:10:41.949496  5096 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 16:10:41.949496  5096 net.cpp:406] loss <- label_cifar_1_split_1
I1123 16:10:41.949496  5096 net.cpp:380] loss -> loss
I1123 16:10:41.949496  5096 layer_factory.cpp:58] Creating layer loss
I1123 16:10:41.949496  5096 net.cpp:122] Setting up loss
I1123 16:10:41.949496  5096 net.cpp:129] Top shape: (1)
I1123 16:10:41.949496  5096 net.cpp:132]     with loss weight 1
I1123 16:10:41.949496  5096 net.cpp:137] Memory required for data: 175004408
I1123 16:10:41.949496  5096 net.cpp:198] loss needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:200] accuracy does not need backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] ip1 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] poolcp6 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] relu_conv12 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] scale_conv12 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] bn_conv12 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] conv12 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] pool4_2 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] relu4_2 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] scale4_2 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] bn4_2 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] conv4_2 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] relu4_1 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] scale4_1 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] bn4_1 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] conv4_1 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] relu4 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] scale4 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] bn4 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] conv4 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] relu3 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] scale3 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] bn3 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] conv3 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] pool2_1 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] relu2_2 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] scale2_2 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] bn2_2 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] conv2_2 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] relu2 needs backward computation.
I1123 16:10:41.949496  5096 net.cpp:198] scale2 needs backward computation.
I1123 16:10:41.950496  5096 net.cpp:198] bn2 needs backward computation.
I1123 16:10:41.950496  5096 net.cpp:198] conv2 needs backward computation.
I1123 16:10:41.950496  5096 net.cpp:198] relu1 needs backward computation.
I1123 16:10:41.950496  5096 net.cpp:198] scale1 needs backward computation.
I1123 16:10:41.950496  5096 net.cpp:198] bn1 needs backward computation.
I1123 16:10:41.950496  5096 net.cpp:198] conv1 needs backward computation.
I1123 16:10:41.950496  5096 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 16:10:41.950496  5096 net.cpp:200] cifar does not need backward computation.
I1123 16:10:41.950496  5096 net.cpp:242] This network produces output accuracy
I1123 16:10:41.950496  5096 net.cpp:242] This network produces output loss
I1123 16:10:41.950496  5096 net.cpp:255] Network initialization done.
I1123 16:10:41.950496  5096 solver.cpp:56] Solver scaffolding done.
I1123 16:10:41.952478  5096 caffe.cpp:249] Starting Optimization
I1123 16:10:41.952478  5096 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_7x7_300K
I1123 16:10:41.952478  5096 solver.cpp:273] Learning Rate Policy: multistep
I1123 16:10:41.954493  5096 solver.cpp:330] Iteration 0, Testing net (#0)
I1123 16:10:41.955492  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:10:43.057008 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:10:43.098011  5096 solver.cpp:397]     Test net output #0: accuracy = 0.1006
I1123 16:10:43.098011  5096 solver.cpp:397]     Test net output #1: loss = 78.5505 (* 1 = 78.5505 loss)
I1123 16:10:43.156064  5096 solver.cpp:218] Iteration 0 (0 iter/s, 1.20298s/100 iters), loss = 3.59469
I1123 16:10:43.156064  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.05
I1123 16:10:43.156064  5096 solver.cpp:237]     Train net output #1: loss = 3.59469 (* 1 = 3.59469 loss)
I1123 16:10:43.156064  5096 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1123 16:10:47.019073  5096 solver.cpp:218] Iteration 100 (25.8882 iter/s, 3.86277s/100 iters), loss = 1.75484
I1123 16:10:47.019073  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.36
I1123 16:10:47.019073  5096 solver.cpp:237]     Train net output #1: loss = 1.75484 (* 1 = 1.75484 loss)
I1123 16:10:47.019073  5096 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1123 16:10:50.847182  5096 solver.cpp:218] Iteration 200 (26.1307 iter/s, 3.82691s/100 iters), loss = 1.82091
I1123 16:10:50.847182  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.35
I1123 16:10:50.847182  5096 solver.cpp:237]     Train net output #1: loss = 1.82091 (* 1 = 1.82091 loss)
I1123 16:10:50.847182  5096 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1123 16:10:54.667397  5096 solver.cpp:218] Iteration 300 (26.1744 iter/s, 3.82053s/100 iters), loss = 1.40267
I1123 16:10:54.667397  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.55
I1123 16:10:54.667397  5096 solver.cpp:237]     Train net output #1: loss = 1.40267 (* 1 = 1.40267 loss)
I1123 16:10:54.667397  5096 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1123 16:10:58.489532  5096 solver.cpp:218] Iteration 400 (26.1701 iter/s, 3.82115s/100 iters), loss = 1.25586
I1123 16:10:58.489532  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.5
I1123 16:10:58.489532  5096 solver.cpp:237]     Train net output #1: loss = 1.25586 (* 1 = 1.25586 loss)
I1123 16:10:58.489532  5096 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1123 16:11:02.122355 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:11:02.273047  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_500.caffemodel
I1123 16:11:02.286067  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_500.solverstate
I1123 16:11:02.291075  5096 solver.cpp:330] Iteration 500, Testing net (#0)
I1123 16:11:02.291075  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:11:03.347898 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:11:03.388931  5096 solver.cpp:397]     Test net output #0: accuracy = 0.4598
I1123 16:11:03.388931  5096 solver.cpp:397]     Test net output #1: loss = 1.47303 (* 1 = 1.47303 loss)
I1123 16:11:03.425930  5096 solver.cpp:218] Iteration 500 (20.258 iter/s, 4.93631s/100 iters), loss = 1.36634
I1123 16:11:03.425930  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.49
I1123 16:11:03.425930  5096 solver.cpp:237]     Train net output #1: loss = 1.36634 (* 1 = 1.36634 loss)
I1123 16:11:03.425930  5096 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1123 16:11:07.257992  5096 solver.cpp:218] Iteration 600 (26.0999 iter/s, 3.83143s/100 iters), loss = 1.27589
I1123 16:11:07.257992  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.57
I1123 16:11:07.257992  5096 solver.cpp:237]     Train net output #1: loss = 1.27589 (* 1 = 1.27589 loss)
I1123 16:11:07.257992  5096 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1123 16:11:11.084738  5096 solver.cpp:218] Iteration 700 (26.1338 iter/s, 3.82646s/100 iters), loss = 1.22968
I1123 16:11:11.084738  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.56
I1123 16:11:11.084738  5096 solver.cpp:237]     Train net output #1: loss = 1.22968 (* 1 = 1.22968 loss)
I1123 16:11:11.084738  5096 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1123 16:11:14.901118  5096 solver.cpp:218] Iteration 800 (26.2032 iter/s, 3.81633s/100 iters), loss = 1.0561
I1123 16:11:14.901118  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1123 16:11:14.901118  5096 solver.cpp:237]     Train net output #1: loss = 1.0561 (* 1 = 1.0561 loss)
I1123 16:11:14.901118  5096 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1123 16:11:18.728842  5096 solver.cpp:218] Iteration 900 (26.1243 iter/s, 3.82786s/100 iters), loss = 0.981806
I1123 16:11:18.729846  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1123 16:11:18.729846  5096 solver.cpp:237]     Train net output #1: loss = 0.981806 (* 1 = 0.981806 loss)
I1123 16:11:18.729846  5096 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1123 16:11:22.355147 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:11:22.505151  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_1000.caffemodel
I1123 16:11:22.514138  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_1000.solverstate
I1123 16:11:22.519160  5096 solver.cpp:330] Iteration 1000, Testing net (#0)
I1123 16:11:22.519160  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:11:23.579493 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:11:23.621521  5096 solver.cpp:397]     Test net output #0: accuracy = 0.501
I1123 16:11:23.621521  5096 solver.cpp:397]     Test net output #1: loss = 1.42272 (* 1 = 1.42272 loss)
I1123 16:11:23.657508  5096 solver.cpp:218] Iteration 1000 (20.2918 iter/s, 4.92809s/100 iters), loss = 1.16413
I1123 16:11:23.657508  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.57
I1123 16:11:23.657508  5096 solver.cpp:237]     Train net output #1: loss = 1.16413 (* 1 = 1.16413 loss)
I1123 16:11:23.657508  5096 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1123 16:11:27.499608  5096 solver.cpp:218] Iteration 1100 (26.0337 iter/s, 3.84117s/100 iters), loss = 0.940512
I1123 16:11:27.499608  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1123 16:11:27.499608  5096 solver.cpp:237]     Train net output #1: loss = 0.940512 (* 1 = 0.940512 loss)
I1123 16:11:27.499608  5096 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1123 16:11:31.337996  5096 solver.cpp:218] Iteration 1200 (26.0529 iter/s, 3.83834s/100 iters), loss = 1.03232
I1123 16:11:31.337996  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.56
I1123 16:11:31.337996  5096 solver.cpp:237]     Train net output #1: loss = 1.03232 (* 1 = 1.03232 loss)
I1123 16:11:31.337996  5096 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1123 16:11:35.189296  5096 solver.cpp:218] Iteration 1300 (25.9698 iter/s, 3.85063s/100 iters), loss = 0.959482
I1123 16:11:35.189296  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1123 16:11:35.189296  5096 solver.cpp:237]     Train net output #1: loss = 0.959482 (* 1 = 0.959482 loss)
I1123 16:11:35.189296  5096 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1123 16:11:39.032698  5096 solver.cpp:218] Iteration 1400 (26.0182 iter/s, 3.84346s/100 iters), loss = 1.01188
I1123 16:11:39.032698  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1123 16:11:39.032698  5096 solver.cpp:237]     Train net output #1: loss = 1.01188 (* 1 = 1.01188 loss)
I1123 16:11:39.032698  5096 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1123 16:11:42.684345 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:11:42.834872  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_1500.caffemodel
I1123 16:11:42.845901  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_1500.solverstate
I1123 16:11:42.850905  5096 solver.cpp:330] Iteration 1500, Testing net (#0)
I1123 16:11:42.850905  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:11:43.914186 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:11:43.956212  5096 solver.cpp:397]     Test net output #0: accuracy = 0.5511
I1123 16:11:43.956212  5096 solver.cpp:397]     Test net output #1: loss = 1.28815 (* 1 = 1.28815 loss)
I1123 16:11:43.992221  5096 solver.cpp:218] Iteration 1500 (20.1641 iter/s, 4.95931s/100 iters), loss = 0.873637
I1123 16:11:43.992221  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1123 16:11:43.992221  5096 solver.cpp:237]     Train net output #1: loss = 0.873637 (* 1 = 0.873637 loss)
I1123 16:11:43.992221  5096 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1123 16:11:47.839447  5096 solver.cpp:218] Iteration 1600 (25.9945 iter/s, 3.84697s/100 iters), loss = 0.784565
I1123 16:11:47.840452  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1123 16:11:47.840452  5096 solver.cpp:237]     Train net output #1: loss = 0.784565 (* 1 = 0.784565 loss)
I1123 16:11:47.840452  5096 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1123 16:11:51.678844  5096 solver.cpp:218] Iteration 1700 (26.0539 iter/s, 3.8382s/100 iters), loss = 0.807678
I1123 16:11:51.678844  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1123 16:11:51.678844  5096 solver.cpp:237]     Train net output #1: loss = 0.807678 (* 1 = 0.807678 loss)
I1123 16:11:51.678844  5096 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1123 16:11:55.515369  5096 solver.cpp:218] Iteration 1800 (26.0666 iter/s, 3.83633s/100 iters), loss = 0.780043
I1123 16:11:55.515369  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 16:11:55.515369  5096 solver.cpp:237]     Train net output #1: loss = 0.780043 (* 1 = 0.780043 loss)
I1123 16:11:55.515369  5096 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1123 16:11:59.350626  5096 solver.cpp:218] Iteration 1900 (26.0768 iter/s, 3.83482s/100 iters), loss = 0.78361
I1123 16:11:59.350626  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 16:11:59.350626  5096 solver.cpp:237]     Train net output #1: loss = 0.78361 (* 1 = 0.78361 loss)
I1123 16:11:59.350626  5096 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1123 16:12:03.005065 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:12:03.156086  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_2000.caffemodel
I1123 16:12:03.166091  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_2000.solverstate
I1123 16:12:03.172075  5096 solver.cpp:330] Iteration 2000, Testing net (#0)
I1123 16:12:03.172075  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:12:04.230792 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:12:04.272794  5096 solver.cpp:397]     Test net output #0: accuracy = 0.6699
I1123 16:12:04.272794  5096 solver.cpp:397]     Test net output #1: loss = 0.937065 (* 1 = 0.937065 loss)
I1123 16:12:04.309849  5096 solver.cpp:218] Iteration 2000 (20.1666 iter/s, 4.9587s/100 iters), loss = 0.622472
I1123 16:12:04.309849  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 16:12:04.309849  5096 solver.cpp:237]     Train net output #1: loss = 0.622472 (* 1 = 0.622472 loss)
I1123 16:12:04.309849  5096 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1123 16:12:08.142990  5096 solver.cpp:218] Iteration 2100 (26.0869 iter/s, 3.83335s/100 iters), loss = 0.700445
I1123 16:12:08.142990  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 16:12:08.142990  5096 solver.cpp:237]     Train net output #1: loss = 0.700445 (* 1 = 0.700445 loss)
I1123 16:12:08.142990  5096 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1123 16:12:11.994976  5096 solver.cpp:218] Iteration 2200 (25.9644 iter/s, 3.85143s/100 iters), loss = 0.735817
I1123 16:12:11.994976  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 16:12:11.994976  5096 solver.cpp:237]     Train net output #1: loss = 0.735817 (* 1 = 0.735817 loss)
I1123 16:12:11.994976  5096 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1123 16:12:15.849539  5096 solver.cpp:218] Iteration 2300 (25.944 iter/s, 3.85446s/100 iters), loss = 0.808101
I1123 16:12:15.849539  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 16:12:15.849539  5096 solver.cpp:237]     Train net output #1: loss = 0.808101 (* 1 = 0.808101 loss)
I1123 16:12:15.849539  5096 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1123 16:12:19.707079  5096 solver.cpp:218] Iteration 2400 (25.9297 iter/s, 3.85658s/100 iters), loss = 0.866334
I1123 16:12:19.707079  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 16:12:19.707079  5096 solver.cpp:237]     Train net output #1: loss = 0.866334 (* 1 = 0.866334 loss)
I1123 16:12:19.707079  5096 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1123 16:12:23.371721 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:12:23.523861  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_2500.caffemodel
I1123 16:12:23.533854  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_2500.solverstate
I1123 16:12:23.537853  5096 solver.cpp:330] Iteration 2500, Testing net (#0)
I1123 16:12:23.538851  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:12:24.602125 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:12:24.644142  5096 solver.cpp:397]     Test net output #0: accuracy = 0.5647
I1123 16:12:24.644142  5096 solver.cpp:397]     Test net output #1: loss = 1.29365 (* 1 = 1.29365 loss)
I1123 16:12:24.681157  5096 solver.cpp:218] Iteration 2500 (20.1063 iter/s, 4.97355s/100 iters), loss = 0.633038
I1123 16:12:24.681157  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 16:12:24.681157  5096 solver.cpp:237]     Train net output #1: loss = 0.633038 (* 1 = 0.633038 loss)
I1123 16:12:24.681157  5096 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1123 16:12:28.521430  5096 solver.cpp:218] Iteration 2600 (26.0365 iter/s, 3.84076s/100 iters), loss = 0.733152
I1123 16:12:28.521430  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 16:12:28.521430  5096 solver.cpp:237]     Train net output #1: loss = 0.733152 (* 1 = 0.733152 loss)
I1123 16:12:28.521430  5096 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1123 16:12:32.371433  5096 solver.cpp:218] Iteration 2700 (25.9771 iter/s, 3.84955s/100 iters), loss = 0.687388
I1123 16:12:32.371433  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1123 16:12:32.371433  5096 solver.cpp:237]     Train net output #1: loss = 0.687388 (* 1 = 0.687388 loss)
I1123 16:12:32.371433  5096 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1123 16:12:36.212286  5096 solver.cpp:218] Iteration 2800 (26.0409 iter/s, 3.84011s/100 iters), loss = 0.733702
I1123 16:12:36.212286  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1123 16:12:36.212286  5096 solver.cpp:237]     Train net output #1: loss = 0.733702 (* 1 = 0.733702 loss)
I1123 16:12:36.212786  5096 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1123 16:12:40.053580  5096 solver.cpp:218] Iteration 2900 (26.0356 iter/s, 3.84089s/100 iters), loss = 0.65127
I1123 16:12:40.053580  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 16:12:40.053580  5096 solver.cpp:237]     Train net output #1: loss = 0.65127 (* 1 = 0.65127 loss)
I1123 16:12:40.053580  5096 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1123 16:12:43.718207 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:12:43.869206  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_3000.caffemodel
I1123 16:12:43.883208  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_3000.solverstate
I1123 16:12:43.887707  5096 solver.cpp:330] Iteration 3000, Testing net (#0)
I1123 16:12:43.888207  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:12:44.949242 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:12:44.990742  5096 solver.cpp:397]     Test net output #0: accuracy = 0.7051
I1123 16:12:44.990742  5096 solver.cpp:397]     Test net output #1: loss = 0.880943 (* 1 = 0.880943 loss)
I1123 16:12:45.027742  5096 solver.cpp:218] Iteration 3000 (20.106 iter/s, 4.97365s/100 iters), loss = 0.677361
I1123 16:12:45.027742  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 16:12:45.027742  5096 solver.cpp:237]     Train net output #1: loss = 0.677361 (* 1 = 0.677361 loss)
I1123 16:12:45.027742  5096 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1123 16:12:48.878773  5096 solver.cpp:218] Iteration 3100 (25.9682 iter/s, 3.85087s/100 iters), loss = 0.695964
I1123 16:12:48.878773  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 16:12:48.878773  5096 solver.cpp:237]     Train net output #1: loss = 0.695964 (* 1 = 0.695964 loss)
I1123 16:12:48.878773  5096 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1123 16:12:52.751802  5096 solver.cpp:218] Iteration 3200 (25.8221 iter/s, 3.87265s/100 iters), loss = 0.759567
I1123 16:12:52.751802  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1123 16:12:52.751802  5096 solver.cpp:237]     Train net output #1: loss = 0.759567 (* 1 = 0.759567 loss)
I1123 16:12:52.751802  5096 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1123 16:12:56.625104  5096 solver.cpp:218] Iteration 3300 (25.8195 iter/s, 3.87304s/100 iters), loss = 0.75768
I1123 16:12:56.625104  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 16:12:56.625104  5096 solver.cpp:237]     Train net output #1: loss = 0.75768 (* 1 = 0.75768 loss)
I1123 16:12:56.625104  5096 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1123 16:13:00.487802  5096 solver.cpp:218] Iteration 3400 (25.8893 iter/s, 3.86259s/100 iters), loss = 0.665562
I1123 16:13:00.487802  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 16:13:00.487802  5096 solver.cpp:237]     Train net output #1: loss = 0.665562 (* 1 = 0.665562 loss)
I1123 16:13:00.487802  5096 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1123 16:13:04.178849 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:13:04.330391  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_3500.caffemodel
I1123 16:13:04.340380  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_3500.solverstate
I1123 16:13:04.344904  5096 solver.cpp:330] Iteration 3500, Testing net (#0)
I1123 16:13:04.344904  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:13:05.411898 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:13:05.454419  5096 solver.cpp:397]     Test net output #0: accuracy = 0.474
I1123 16:13:05.454419  5096 solver.cpp:397]     Test net output #1: loss = 1.78332 (* 1 = 1.78332 loss)
I1123 16:13:05.491940  5096 solver.cpp:218] Iteration 3500 (19.9852 iter/s, 5.00371s/100 iters), loss = 0.616425
I1123 16:13:05.491940  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 16:13:05.491940  5096 solver.cpp:237]     Train net output #1: loss = 0.616425 (* 1 = 0.616425 loss)
I1123 16:13:05.491940  5096 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1123 16:13:09.423486  5096 solver.cpp:218] Iteration 3600 (25.4409 iter/s, 3.93068s/100 iters), loss = 0.549435
I1123 16:13:09.423486  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 16:13:09.423486  5096 solver.cpp:237]     Train net output #1: loss = 0.549435 (* 1 = 0.549435 loss)
I1123 16:13:09.423486  5096 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1123 16:13:13.356925  5096 solver.cpp:218] Iteration 3700 (25.4284 iter/s, 3.93261s/100 iters), loss = 0.695402
I1123 16:13:13.356925  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 16:13:13.356925  5096 solver.cpp:237]     Train net output #1: loss = 0.695402 (* 1 = 0.695402 loss)
I1123 16:13:13.356925  5096 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1123 16:13:17.275054  5096 solver.cpp:218] Iteration 3800 (25.5233 iter/s, 3.918s/100 iters), loss = 0.695354
I1123 16:13:17.275054  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1123 16:13:17.275054  5096 solver.cpp:237]     Train net output #1: loss = 0.695354 (* 1 = 0.695354 loss)
I1123 16:13:17.275054  5096 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1123 16:13:21.184173  5096 solver.cpp:218] Iteration 3900 (25.5795 iter/s, 3.90939s/100 iters), loss = 0.678552
I1123 16:13:21.185178  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 16:13:21.185178  5096 solver.cpp:237]     Train net output #1: loss = 0.678552 (* 1 = 0.678552 loss)
I1123 16:13:21.185178  5096 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1123 16:13:24.899889 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:13:25.051409  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_4000.caffemodel
I1123 16:13:25.061415  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_4000.solverstate
I1123 16:13:25.065914  5096 solver.cpp:330] Iteration 4000, Testing net (#0)
I1123 16:13:25.065914  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:13:26.129721 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:13:26.171730  5096 solver.cpp:397]     Test net output #0: accuracy = 0.7236
I1123 16:13:26.171730  5096 solver.cpp:397]     Test net output #1: loss = 0.804462 (* 1 = 0.804462 loss)
I1123 16:13:26.208725  5096 solver.cpp:218] Iteration 4000 (19.9042 iter/s, 5.02405s/100 iters), loss = 0.579295
I1123 16:13:26.209735  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 16:13:26.209735  5096 solver.cpp:237]     Train net output #1: loss = 0.579295 (* 1 = 0.579295 loss)
I1123 16:13:26.209735  5096 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1123 16:13:30.133765  5096 solver.cpp:218] Iteration 4100 (25.482 iter/s, 3.92434s/100 iters), loss = 0.567677
I1123 16:13:30.133765  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 16:13:30.133765  5096 solver.cpp:237]     Train net output #1: loss = 0.567677 (* 1 = 0.567677 loss)
I1123 16:13:30.133765  5096 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1123 16:13:34.071909  5096 solver.cpp:218] Iteration 4200 (25.3958 iter/s, 3.93765s/100 iters), loss = 0.647355
I1123 16:13:34.071909  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 16:13:34.071909  5096 solver.cpp:237]     Train net output #1: loss = 0.647355 (* 1 = 0.647355 loss)
I1123 16:13:34.071909  5096 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1123 16:13:37.998921  5096 solver.cpp:218] Iteration 4300 (25.4666 iter/s, 3.92671s/100 iters), loss = 0.606485
I1123 16:13:37.998921  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 16:13:37.998921  5096 solver.cpp:237]     Train net output #1: loss = 0.606485 (* 1 = 0.606485 loss)
I1123 16:13:37.998921  5096 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1123 16:13:41.907650  5096 solver.cpp:218] Iteration 4400 (25.5903 iter/s, 3.90774s/100 iters), loss = 0.588417
I1123 16:13:41.907650  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 16:13:41.907650  5096 solver.cpp:237]     Train net output #1: loss = 0.588417 (* 1 = 0.588417 loss)
I1123 16:13:41.907650  5096 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1123 16:13:45.641504 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:13:45.796326  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_4500.caffemodel
I1123 16:13:45.807339  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_4500.solverstate
I1123 16:13:45.811324  5096 solver.cpp:330] Iteration 4500, Testing net (#0)
I1123 16:13:45.811324  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:13:46.891971 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:13:46.934968  5096 solver.cpp:397]     Test net output #0: accuracy = 0.627
I1123 16:13:46.934968  5096 solver.cpp:397]     Test net output #1: loss = 1.13228 (* 1 = 1.13228 loss)
I1123 16:13:46.973487  5096 solver.cpp:218] Iteration 4500 (19.7399 iter/s, 5.06588s/100 iters), loss = 0.554525
I1123 16:13:46.973987  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 16:13:46.973987  5096 solver.cpp:237]     Train net output #1: loss = 0.554525 (* 1 = 0.554525 loss)
I1123 16:13:46.973987  5096 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1123 16:13:50.900928  5096 solver.cpp:218] Iteration 4600 (25.4664 iter/s, 3.92674s/100 iters), loss = 0.583846
I1123 16:13:50.900928  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 16:13:50.900928  5096 solver.cpp:237]     Train net output #1: loss = 0.583846 (* 1 = 0.583846 loss)
I1123 16:13:50.900928  5096 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1123 16:13:54.828933  5096 solver.cpp:218] Iteration 4700 (25.4578 iter/s, 3.92808s/100 iters), loss = 0.620363
I1123 16:13:54.828933  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 16:13:54.828933  5096 solver.cpp:237]     Train net output #1: loss = 0.620363 (* 1 = 0.620363 loss)
I1123 16:13:54.828933  5096 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1123 16:13:58.716945  5096 solver.cpp:218] Iteration 4800 (25.7211 iter/s, 3.88785s/100 iters), loss = 0.648297
I1123 16:13:58.717944  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 16:13:58.717944  5096 solver.cpp:237]     Train net output #1: loss = 0.648297 (* 1 = 0.648297 loss)
I1123 16:13:58.717944  5096 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1123 16:14:02.633939  5096 solver.cpp:218] Iteration 4900 (25.5373 iter/s, 3.91585s/100 iters), loss = 0.54133
I1123 16:14:02.633939  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 16:14:02.633939  5096 solver.cpp:237]     Train net output #1: loss = 0.54133 (* 1 = 0.54133 loss)
I1123 16:14:02.633939  5096 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1123 16:14:06.304066 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:14:06.459072  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_5000.caffemodel
I1123 16:14:06.470073  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_5000.solverstate
I1123 16:14:06.474079  5096 solver.cpp:330] Iteration 5000, Testing net (#0)
I1123 16:14:06.474079  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:14:07.552254 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:14:07.593832  5096 solver.cpp:397]     Test net output #0: accuracy = 0.6058
I1123 16:14:07.593832  5096 solver.cpp:397]     Test net output #1: loss = 1.21243 (* 1 = 1.21243 loss)
I1123 16:14:07.631846  5096 solver.cpp:218] Iteration 5000 (20.0081 iter/s, 4.99799s/100 iters), loss = 0.560552
I1123 16:14:07.631846  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 16:14:07.631846  5096 solver.cpp:237]     Train net output #1: loss = 0.560552 (* 1 = 0.560552 loss)
I1123 16:14:07.631846  5096 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1123 16:14:07.631846  5096 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1123 16:14:11.562523  5096 solver.cpp:218] Iteration 5100 (25.444 iter/s, 3.9302s/100 iters), loss = 0.471729
I1123 16:14:11.562523  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 16:14:11.562523  5096 solver.cpp:237]     Train net output #1: loss = 0.471729 (* 1 = 0.471729 loss)
I1123 16:14:11.562523  5096 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1123 16:14:15.418685  5096 solver.cpp:218] Iteration 5200 (25.9378 iter/s, 3.85537s/100 iters), loss = 0.463817
I1123 16:14:15.418685  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 16:14:15.418685  5096 solver.cpp:237]     Train net output #1: loss = 0.463817 (* 1 = 0.463817 loss)
I1123 16:14:15.418685  5096 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1123 16:14:19.279580  5096 solver.cpp:218] Iteration 5300 (25.8999 iter/s, 3.86101s/100 iters), loss = 0.463563
I1123 16:14:19.279580  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:14:19.279580  5096 solver.cpp:237]     Train net output #1: loss = 0.463563 (* 1 = 0.463563 loss)
I1123 16:14:19.279580  5096 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1123 16:14:23.195015  5096 solver.cpp:218] Iteration 5400 (25.5421 iter/s, 3.9151s/100 iters), loss = 0.412917
I1123 16:14:23.195015  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 16:14:23.195015  5096 solver.cpp:237]     Train net output #1: loss = 0.412917 (* 1 = 0.412917 loss)
I1123 16:14:23.195015  5096 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1123 16:14:26.866902 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:14:27.018462  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_5500.caffemodel
I1123 16:14:27.028564  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_5500.solverstate
I1123 16:14:27.032579  5096 solver.cpp:330] Iteration 5500, Testing net (#0)
I1123 16:14:27.032579  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:14:28.097800 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:14:28.138854  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8244
I1123 16:14:28.139870  5096 solver.cpp:397]     Test net output #1: loss = 0.509944 (* 1 = 0.509944 loss)
I1123 16:14:28.175843  5096 solver.cpp:218] Iteration 5500 (20.0775 iter/s, 4.98071s/100 iters), loss = 0.4214
I1123 16:14:28.176841  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 16:14:28.176841  5096 solver.cpp:237]     Train net output #1: loss = 0.4214 (* 1 = 0.4214 loss)
I1123 16:14:28.176841  5096 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1123 16:14:32.070904  5096 solver.cpp:218] Iteration 5600 (25.6826 iter/s, 3.89368s/100 iters), loss = 0.337875
I1123 16:14:32.070904  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:14:32.070904  5096 solver.cpp:237]     Train net output #1: loss = 0.337875 (* 1 = 0.337875 loss)
I1123 16:14:32.070904  5096 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1123 16:14:35.971905  5096 solver.cpp:218] Iteration 5700 (25.6376 iter/s, 3.90053s/100 iters), loss = 0.428474
I1123 16:14:35.971905  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 16:14:35.971905  5096 solver.cpp:237]     Train net output #1: loss = 0.428474 (* 1 = 0.428474 loss)
I1123 16:14:35.971905  5096 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1123 16:14:39.863880  5096 solver.cpp:218] Iteration 5800 (25.6954 iter/s, 3.89175s/100 iters), loss = 0.455621
I1123 16:14:39.863880  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 16:14:39.863880  5096 solver.cpp:237]     Train net output #1: loss = 0.455621 (* 1 = 0.455621 loss)
I1123 16:14:39.863880  5096 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1123 16:14:43.793931  5096 solver.cpp:218] Iteration 5900 (25.4461 iter/s, 3.92987s/100 iters), loss = 0.363091
I1123 16:14:43.793931  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:14:43.793931  5096 solver.cpp:237]     Train net output #1: loss = 0.363091 (* 1 = 0.363091 loss)
I1123 16:14:43.793931  5096 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1123 16:14:47.523610 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:14:47.673652  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_6000.caffemodel
I1123 16:14:47.684641  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_6000.solverstate
I1123 16:14:47.689627  5096 solver.cpp:330] Iteration 6000, Testing net (#0)
I1123 16:14:47.689627  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:14:48.754376 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:14:48.796376  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8337
I1123 16:14:48.796376  5096 solver.cpp:397]     Test net output #1: loss = 0.485571 (* 1 = 0.485571 loss)
I1123 16:14:48.834393  5096 solver.cpp:218] Iteration 6000 (19.842 iter/s, 5.03982s/100 iters), loss = 0.370434
I1123 16:14:48.834393  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:14:48.834393  5096 solver.cpp:237]     Train net output #1: loss = 0.370434 (* 1 = 0.370434 loss)
I1123 16:14:48.834393  5096 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1123 16:14:52.733002  5096 solver.cpp:218] Iteration 6100 (25.6481 iter/s, 3.89893s/100 iters), loss = 0.431391
I1123 16:14:52.733002  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 16:14:52.733002  5096 solver.cpp:237]     Train net output #1: loss = 0.431391 (* 1 = 0.431391 loss)
I1123 16:14:52.733002  5096 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1123 16:14:56.628345  5096 solver.cpp:218] Iteration 6200 (25.6779 iter/s, 3.8944s/100 iters), loss = 0.336755
I1123 16:14:56.628345  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:14:56.628345  5096 solver.cpp:237]     Train net output #1: loss = 0.336755 (* 1 = 0.336755 loss)
I1123 16:14:56.628345  5096 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1123 16:15:00.524060  5096 solver.cpp:218] Iteration 6300 (25.6733 iter/s, 3.89509s/100 iters), loss = 0.397991
I1123 16:15:00.524060  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:15:00.524060  5096 solver.cpp:237]     Train net output #1: loss = 0.397991 (* 1 = 0.397991 loss)
I1123 16:15:00.524060  5096 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1123 16:15:04.433861  5096 solver.cpp:218] Iteration 6400 (25.5781 iter/s, 3.90959s/100 iters), loss = 0.354927
I1123 16:15:04.433861  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 16:15:04.433861  5096 solver.cpp:237]     Train net output #1: loss = 0.354927 (* 1 = 0.354927 loss)
I1123 16:15:04.433861  5096 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1123 16:15:08.150683 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:15:08.302250  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_6500.caffemodel
I1123 16:15:08.312250  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_6500.solverstate
I1123 16:15:08.316251  5096 solver.cpp:330] Iteration 6500, Testing net (#0)
I1123 16:15:08.316251  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:15:09.382164 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:15:09.423178  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8306
I1123 16:15:09.423178  5096 solver.cpp:397]     Test net output #1: loss = 0.490131 (* 1 = 0.490131 loss)
I1123 16:15:09.460731  5096 solver.cpp:218] Iteration 6500 (19.8923 iter/s, 5.02708s/100 iters), loss = 0.36709
I1123 16:15:09.460731  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 16:15:09.460731  5096 solver.cpp:237]     Train net output #1: loss = 0.36709 (* 1 = 0.36709 loss)
I1123 16:15:09.460731  5096 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1123 16:15:13.325570  5096 solver.cpp:218] Iteration 6600 (25.8789 iter/s, 3.86415s/100 iters), loss = 0.33094
I1123 16:15:13.325570  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:15:13.325570  5096 solver.cpp:237]     Train net output #1: loss = 0.33094 (* 1 = 0.33094 loss)
I1123 16:15:13.325570  5096 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1123 16:15:17.253722  5096 solver.cpp:218] Iteration 6700 (25.4569 iter/s, 3.92821s/100 iters), loss = 0.456144
I1123 16:15:17.254727  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:15:17.254727  5096 solver.cpp:237]     Train net output #1: loss = 0.456144 (* 1 = 0.456144 loss)
I1123 16:15:17.254727  5096 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1123 16:15:21.149742  5096 solver.cpp:218] Iteration 6800 (25.6715 iter/s, 3.89538s/100 iters), loss = 0.401943
I1123 16:15:21.149742  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:15:21.149742  5096 solver.cpp:237]     Train net output #1: loss = 0.401943 (* 1 = 0.401943 loss)
I1123 16:15:21.149742  5096 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1123 16:15:25.027578  5096 solver.cpp:218] Iteration 6900 (25.7914 iter/s, 3.87727s/100 iters), loss = 0.342352
I1123 16:15:25.027578  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 16:15:25.027578  5096 solver.cpp:237]     Train net output #1: loss = 0.342352 (* 1 = 0.342352 loss)
I1123 16:15:25.027578  5096 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1123 16:15:28.787479 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:15:28.941999  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_7000.caffemodel
I1123 16:15:28.952010  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_7000.solverstate
I1123 16:15:28.956010  5096 solver.cpp:330] Iteration 7000, Testing net (#0)
I1123 16:15:28.956010  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:15:30.032824 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:15:30.074846  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8339
I1123 16:15:30.074846  5096 solver.cpp:397]     Test net output #1: loss = 0.491688 (* 1 = 0.491688 loss)
I1123 16:15:30.111848  5096 solver.cpp:218] Iteration 7000 (19.669 iter/s, 5.08414s/100 iters), loss = 0.344918
I1123 16:15:30.111848  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:15:30.111848  5096 solver.cpp:237]     Train net output #1: loss = 0.344918 (* 1 = 0.344918 loss)
I1123 16:15:30.111848  5096 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1123 16:15:33.963332  5096 solver.cpp:218] Iteration 7100 (25.9683 iter/s, 3.85084s/100 iters), loss = 0.358056
I1123 16:15:33.963332  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:15:33.963332  5096 solver.cpp:237]     Train net output #1: loss = 0.358056 (* 1 = 0.358056 loss)
I1123 16:15:33.963332  5096 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1123 16:15:37.812949  5096 solver.cpp:218] Iteration 7200 (25.9781 iter/s, 3.8494s/100 iters), loss = 0.335659
I1123 16:15:37.812949  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:15:37.812949  5096 solver.cpp:237]     Train net output #1: loss = 0.335659 (* 1 = 0.335659 loss)
I1123 16:15:37.812949  5096 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1123 16:15:41.663444  5096 solver.cpp:218] Iteration 7300 (25.978 iter/s, 3.84941s/100 iters), loss = 0.407338
I1123 16:15:41.663444  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 16:15:41.663444  5096 solver.cpp:237]     Train net output #1: loss = 0.407338 (* 1 = 0.407338 loss)
I1123 16:15:41.663444  5096 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1123 16:15:45.513717  5096 solver.cpp:218] Iteration 7400 (25.9751 iter/s, 3.84985s/100 iters), loss = 0.347777
I1123 16:15:45.513717  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:15:45.513717  5096 solver.cpp:237]     Train net output #1: loss = 0.347777 (* 1 = 0.347777 loss)
I1123 16:15:45.513717  5096 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1123 16:15:49.175834 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:15:49.327385  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_7500.caffemodel
I1123 16:15:49.338366  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_7500.solverstate
I1123 16:15:49.341908  5096 solver.cpp:330] Iteration 7500, Testing net (#0)
I1123 16:15:49.341908  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:15:50.406196 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:15:50.447235  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8303
I1123 16:15:50.447235  5096 solver.cpp:397]     Test net output #1: loss = 0.504787 (* 1 = 0.504787 loss)
I1123 16:15:50.484236  5096 solver.cpp:218] Iteration 7500 (20.1167 iter/s, 4.971s/100 iters), loss = 0.375583
I1123 16:15:50.484236  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:15:50.484236  5096 solver.cpp:237]     Train net output #1: loss = 0.375583 (* 1 = 0.375583 loss)
I1123 16:15:50.484236  5096 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1123 16:15:54.345180  5096 solver.cpp:218] Iteration 7600 (25.9037 iter/s, 3.86046s/100 iters), loss = 0.295049
I1123 16:15:54.345180  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:15:54.345180  5096 solver.cpp:237]     Train net output #1: loss = 0.295049 (* 1 = 0.295049 loss)
I1123 16:15:54.345180  5096 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1123 16:15:58.195804  5096 solver.cpp:218] Iteration 7700 (25.9769 iter/s, 3.84957s/100 iters), loss = 0.293679
I1123 16:15:58.195804  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:15:58.195804  5096 solver.cpp:237]     Train net output #1: loss = 0.293679 (* 1 = 0.293679 loss)
I1123 16:15:58.195804  5096 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1123 16:16:02.072003  5096 solver.cpp:218] Iteration 7800 (25.8009 iter/s, 3.87584s/100 iters), loss = 0.432093
I1123 16:16:02.072003  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 16:16:02.072003  5096 solver.cpp:237]     Train net output #1: loss = 0.432093 (* 1 = 0.432093 loss)
I1123 16:16:02.072003  5096 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1123 16:16:06.008987  5096 solver.cpp:218] Iteration 7900 (25.4012 iter/s, 3.93682s/100 iters), loss = 0.33229
I1123 16:16:06.008987  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:16:06.008987  5096 solver.cpp:237]     Train net output #1: loss = 0.33229 (* 1 = 0.33229 loss)
I1123 16:16:06.008987  5096 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1123 16:16:09.732430 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:16:09.887629  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_8000.caffemodel
I1123 16:16:09.896632  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_8000.solverstate
I1123 16:16:09.901636  5096 solver.cpp:330] Iteration 8000, Testing net (#0)
I1123 16:16:09.901636  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:16:10.972792 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:16:11.014832  5096 solver.cpp:397]     Test net output #0: accuracy = 0.832
I1123 16:16:11.014832  5096 solver.cpp:397]     Test net output #1: loss = 0.48428 (* 1 = 0.48428 loss)
I1123 16:16:11.050834  5096 solver.cpp:218] Iteration 8000 (19.8338 iter/s, 5.04189s/100 iters), loss = 0.293091
I1123 16:16:11.050834  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:16:11.050834  5096 solver.cpp:237]     Train net output #1: loss = 0.293091 (* 1 = 0.293091 loss)
I1123 16:16:11.050834  5096 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1123 16:16:14.949106  5096 solver.cpp:218] Iteration 8100 (25.6578 iter/s, 3.89746s/100 iters), loss = 0.339739
I1123 16:16:14.949106  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:16:14.949106  5096 solver.cpp:237]     Train net output #1: loss = 0.339739 (* 1 = 0.339739 loss)
I1123 16:16:14.949106  5096 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1123 16:16:18.842905  5096 solver.cpp:218] Iteration 8200 (25.6849 iter/s, 3.89333s/100 iters), loss = 0.335102
I1123 16:16:18.842905  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:16:18.842905  5096 solver.cpp:237]     Train net output #1: loss = 0.335102 (* 1 = 0.335102 loss)
I1123 16:16:18.842905  5096 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1123 16:16:22.743875  5096 solver.cpp:218] Iteration 8300 (25.6385 iter/s, 3.90039s/100 iters), loss = 0.410324
I1123 16:16:22.743875  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:16:22.743875  5096 solver.cpp:237]     Train net output #1: loss = 0.410324 (* 1 = 0.410324 loss)
I1123 16:16:22.743875  5096 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1123 16:16:26.672843  5096 solver.cpp:218] Iteration 8400 (25.454 iter/s, 3.92865s/100 iters), loss = 0.326832
I1123 16:16:26.672843  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:16:26.672843  5096 solver.cpp:237]     Train net output #1: loss = 0.326832 (* 1 = 0.326832 loss)
I1123 16:16:26.672843  5096 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1123 16:16:30.411458 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:16:30.562209  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_8500.caffemodel
I1123 16:16:30.572207  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_8500.solverstate
I1123 16:16:30.577229  5096 solver.cpp:330] Iteration 8500, Testing net (#0)
I1123 16:16:30.577229  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:16:31.652829 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:16:31.695341  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8248
I1123 16:16:31.695341  5096 solver.cpp:397]     Test net output #1: loss = 0.518436 (* 1 = 0.518436 loss)
I1123 16:16:31.732353  5096 solver.cpp:218] Iteration 8500 (19.7637 iter/s, 5.05979s/100 iters), loss = 0.314328
I1123 16:16:31.733357  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:16:31.733357  5096 solver.cpp:237]     Train net output #1: loss = 0.314328 (* 1 = 0.314328 loss)
I1123 16:16:31.733357  5096 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1123 16:16:35.630369  5096 solver.cpp:218] Iteration 8600 (25.6645 iter/s, 3.89643s/100 iters), loss = 0.364356
I1123 16:16:35.630369  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:16:35.630369  5096 solver.cpp:237]     Train net output #1: loss = 0.364356 (* 1 = 0.364356 loss)
I1123 16:16:35.630369  5096 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1123 16:16:39.496412  5096 solver.cpp:218] Iteration 8700 (25.866 iter/s, 3.86608s/100 iters), loss = 0.337353
I1123 16:16:39.496412  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:16:39.496412  5096 solver.cpp:237]     Train net output #1: loss = 0.337352 (* 1 = 0.337352 loss)
I1123 16:16:39.496412  5096 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1123 16:16:43.354228  5096 solver.cpp:218] Iteration 8800 (25.9255 iter/s, 3.85721s/100 iters), loss = 0.431393
I1123 16:16:43.354228  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 16:16:43.354228  5096 solver.cpp:237]     Train net output #1: loss = 0.431393 (* 1 = 0.431393 loss)
I1123 16:16:43.354228  5096 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1123 16:16:47.201505  5096 solver.cpp:218] Iteration 8900 (25.9915 iter/s, 3.84741s/100 iters), loss = 0.313072
I1123 16:16:47.201505  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 16:16:47.201505  5096 solver.cpp:237]     Train net output #1: loss = 0.313072 (* 1 = 0.313072 loss)
I1123 16:16:47.201505  5096 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1123 16:16:50.862414 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:16:51.012958  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_9000.caffemodel
I1123 16:16:51.022945  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_9000.solverstate
I1123 16:16:51.027946  5096 solver.cpp:330] Iteration 9000, Testing net (#0)
I1123 16:16:51.027946  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:16:52.092692 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:16:52.134706  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8353
I1123 16:16:52.134706  5096 solver.cpp:397]     Test net output #1: loss = 0.482973 (* 1 = 0.482973 loss)
I1123 16:16:52.172708  5096 solver.cpp:218] Iteration 9000 (20.1197 iter/s, 4.97024s/100 iters), loss = 0.297222
I1123 16:16:52.172708  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:16:52.172708  5096 solver.cpp:237]     Train net output #1: loss = 0.297222 (* 1 = 0.297222 loss)
I1123 16:16:52.172708  5096 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1123 16:16:56.031534  5096 solver.cpp:218] Iteration 9100 (25.9171 iter/s, 3.85845s/100 iters), loss = 0.331475
I1123 16:16:56.031534  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:16:56.031534  5096 solver.cpp:237]     Train net output #1: loss = 0.331475 (* 1 = 0.331475 loss)
I1123 16:16:56.031534  5096 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1123 16:16:59.888383  5096 solver.cpp:218] Iteration 9200 (25.9302 iter/s, 3.85651s/100 iters), loss = 0.289001
I1123 16:16:59.888383  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:16:59.888383  5096 solver.cpp:237]     Train net output #1: loss = 0.289001 (* 1 = 0.289001 loss)
I1123 16:16:59.888383  5096 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1123 16:17:03.752426  5096 solver.cpp:218] Iteration 9300 (25.8817 iter/s, 3.86374s/100 iters), loss = 0.372733
I1123 16:17:03.752426  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:17:03.752426  5096 solver.cpp:237]     Train net output #1: loss = 0.372733 (* 1 = 0.372733 loss)
I1123 16:17:03.752426  5096 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1123 16:17:07.615444  5096 solver.cpp:218] Iteration 9400 (25.8912 iter/s, 3.86232s/100 iters), loss = 0.276865
I1123 16:17:07.615444  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:17:07.615444  5096 solver.cpp:237]     Train net output #1: loss = 0.276865 (* 1 = 0.276865 loss)
I1123 16:17:07.615444  5096 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1123 16:17:11.287972 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:17:11.438614  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_9500.caffemodel
I1123 16:17:11.449618  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_9500.solverstate
I1123 16:17:11.453619  5096 solver.cpp:330] Iteration 9500, Testing net (#0)
I1123 16:17:11.453619  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:17:12.518587 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:17:12.560595  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8328
I1123 16:17:12.560595  5096 solver.cpp:397]     Test net output #1: loss = 0.483074 (* 1 = 0.483074 loss)
I1123 16:17:12.598122  5096 solver.cpp:218] Iteration 9500 (20.0712 iter/s, 4.98226s/100 iters), loss = 0.343504
I1123 16:17:12.598122  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:17:12.598122  5096 solver.cpp:237]     Train net output #1: loss = 0.343504 (* 1 = 0.343504 loss)
I1123 16:17:12.598122  5096 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1123 16:17:12.598122  5096 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1123 16:17:16.445610  5096 solver.cpp:218] Iteration 9600 (25.9898 iter/s, 3.84766s/100 iters), loss = 0.313295
I1123 16:17:16.445610  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:17:16.445610  5096 solver.cpp:237]     Train net output #1: loss = 0.313295 (* 1 = 0.313295 loss)
I1123 16:17:16.445610  5096 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1123 16:17:20.306424  5096 solver.cpp:218] Iteration 9700 (25.9048 iter/s, 3.86029s/100 iters), loss = 0.29876
I1123 16:17:20.306424  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:17:20.306424  5096 solver.cpp:237]     Train net output #1: loss = 0.29876 (* 1 = 0.29876 loss)
I1123 16:17:20.306424  5096 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1123 16:17:24.166800  5096 solver.cpp:218] Iteration 9800 (25.906 iter/s, 3.86011s/100 iters), loss = 0.368152
I1123 16:17:24.166800  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:17:24.166800  5096 solver.cpp:237]     Train net output #1: loss = 0.368151 (* 1 = 0.368151 loss)
I1123 16:17:24.166800  5096 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1123 16:17:28.047190  5096 solver.cpp:218] Iteration 9900 (25.776 iter/s, 3.87958s/100 iters), loss = 0.317387
I1123 16:17:28.047190  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:17:28.047190  5096 solver.cpp:237]     Train net output #1: loss = 0.317387 (* 1 = 0.317387 loss)
I1123 16:17:28.047190  5096 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1123 16:17:31.722326 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:17:31.874330  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_10000.caffemodel
I1123 16:17:31.885308  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_10000.solverstate
I1123 16:17:31.889308  5096 solver.cpp:330] Iteration 10000, Testing net (#0)
I1123 16:17:31.889308  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:17:32.958689 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:17:33.001767  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8554
I1123 16:17:33.001767  5096 solver.cpp:397]     Test net output #1: loss = 0.426947 (* 1 = 0.426947 loss)
I1123 16:17:33.038331  5096 solver.cpp:218] Iteration 10000 (20.0347 iter/s, 4.99135s/100 iters), loss = 0.261274
I1123 16:17:33.038331  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:17:33.038331  5096 solver.cpp:237]     Train net output #1: loss = 0.261274 (* 1 = 0.261274 loss)
I1123 16:17:33.038331  5096 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1123 16:17:36.910152  5096 solver.cpp:218] Iteration 10100 (25.8333 iter/s, 3.87097s/100 iters), loss = 0.329152
I1123 16:17:36.910152  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:17:36.910152  5096 solver.cpp:237]     Train net output #1: loss = 0.329152 (* 1 = 0.329152 loss)
I1123 16:17:36.910152  5096 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1123 16:17:40.772120  5096 solver.cpp:218] Iteration 10200 (25.8941 iter/s, 3.86188s/100 iters), loss = 0.273086
I1123 16:17:40.772120  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:17:40.772120  5096 solver.cpp:237]     Train net output #1: loss = 0.273086 (* 1 = 0.273086 loss)
I1123 16:17:40.772120  5096 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1123 16:17:44.633023  5096 solver.cpp:218] Iteration 10300 (25.9031 iter/s, 3.86055s/100 iters), loss = 0.365246
I1123 16:17:44.633023  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:17:44.633023  5096 solver.cpp:237]     Train net output #1: loss = 0.365246 (* 1 = 0.365246 loss)
I1123 16:17:44.633023  5096 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1123 16:17:48.496083  5096 solver.cpp:218] Iteration 10400 (25.8903 iter/s, 3.86245s/100 iters), loss = 0.245571
I1123 16:17:48.496083  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:17:48.496083  5096 solver.cpp:237]     Train net output #1: loss = 0.245571 (* 1 = 0.245571 loss)
I1123 16:17:48.496083  5096 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1123 16:17:52.171785 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:17:52.322852  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_10500.caffemodel
I1123 16:17:52.333837  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_10500.solverstate
I1123 16:17:52.337857  5096 solver.cpp:330] Iteration 10500, Testing net (#0)
I1123 16:17:52.337857  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:17:53.406677 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:17:53.448210  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8573
I1123 16:17:53.448210  5096 solver.cpp:397]     Test net output #1: loss = 0.424451 (* 1 = 0.424451 loss)
I1123 16:17:53.486209  5096 solver.cpp:218] Iteration 10500 (20.0414 iter/s, 4.98966s/100 iters), loss = 0.266767
I1123 16:17:53.486209  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:17:53.486209  5096 solver.cpp:237]     Train net output #1: loss = 0.266767 (* 1 = 0.266767 loss)
I1123 16:17:53.486209  5096 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1123 16:17:57.370026  5096 solver.cpp:218] Iteration 10600 (25.7485 iter/s, 3.88372s/100 iters), loss = 0.309896
I1123 16:17:57.370026  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:17:57.370026  5096 solver.cpp:237]     Train net output #1: loss = 0.309896 (* 1 = 0.309896 loss)
I1123 16:17:57.370026  5096 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1123 16:18:01.254421  5096 solver.cpp:218] Iteration 10700 (25.7482 iter/s, 3.88377s/100 iters), loss = 0.227603
I1123 16:18:01.254421  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:18:01.254421  5096 solver.cpp:237]     Train net output #1: loss = 0.227603 (* 1 = 0.227603 loss)
I1123 16:18:01.254421  5096 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1123 16:18:05.116911  5096 solver.cpp:218] Iteration 10800 (25.8894 iter/s, 3.86259s/100 iters), loss = 0.405521
I1123 16:18:05.116911  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 16:18:05.116911  5096 solver.cpp:237]     Train net output #1: loss = 0.405521 (* 1 = 0.405521 loss)
I1123 16:18:05.116911  5096 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1123 16:18:08.989434  5096 solver.cpp:218] Iteration 10900 (25.8265 iter/s, 3.87199s/100 iters), loss = 0.287176
I1123 16:18:08.989434  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:18:08.989434  5096 solver.cpp:237]     Train net output #1: loss = 0.287176 (* 1 = 0.287176 loss)
I1123 16:18:08.989434  5096 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1123 16:18:12.667244 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:18:12.819200  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_11000.caffemodel
I1123 16:18:12.829185  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_11000.solverstate
I1123 16:18:12.833204  5096 solver.cpp:330] Iteration 11000, Testing net (#0)
I1123 16:18:12.833204  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:18:13.903731 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:18:13.945268  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8593
I1123 16:18:13.945268  5096 solver.cpp:397]     Test net output #1: loss = 0.423703 (* 1 = 0.423703 loss)
I1123 16:18:13.982261  5096 solver.cpp:218] Iteration 11000 (20.0285 iter/s, 4.99289s/100 iters), loss = 0.265046
I1123 16:18:13.982261  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:18:13.982261  5096 solver.cpp:237]     Train net output #1: loss = 0.265046 (* 1 = 0.265046 loss)
I1123 16:18:13.982261  5096 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1123 16:18:17.892194  5096 solver.cpp:218] Iteration 11100 (25.5817 iter/s, 3.90904s/100 iters), loss = 0.277672
I1123 16:18:17.892194  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:18:17.892194  5096 solver.cpp:237]     Train net output #1: loss = 0.277672 (* 1 = 0.277672 loss)
I1123 16:18:17.892194  5096 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1123 16:18:21.811087  5096 solver.cpp:218] Iteration 11200 (25.5187 iter/s, 3.9187s/100 iters), loss = 0.282915
I1123 16:18:21.811588  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:18:21.811588  5096 solver.cpp:237]     Train net output #1: loss = 0.282915 (* 1 = 0.282915 loss)
I1123 16:18:21.811588  5096 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1123 16:18:25.716608  5096 solver.cpp:218] Iteration 11300 (25.6088 iter/s, 3.9049s/100 iters), loss = 0.328902
I1123 16:18:25.716608  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:18:25.716608  5096 solver.cpp:237]     Train net output #1: loss = 0.328902 (* 1 = 0.328902 loss)
I1123 16:18:25.716608  5096 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1123 16:18:29.639106  5096 solver.cpp:218] Iteration 11400 (25.4965 iter/s, 3.92211s/100 iters), loss = 0.28168
I1123 16:18:29.639106  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:18:29.639106  5096 solver.cpp:237]     Train net output #1: loss = 0.28168 (* 1 = 0.28168 loss)
I1123 16:18:29.639106  5096 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1123 16:18:33.374112 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:18:33.528883  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_11500.caffemodel
I1123 16:18:33.539088  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_11500.solverstate
I1123 16:18:33.543107  5096 solver.cpp:330] Iteration 11500, Testing net (#0)
I1123 16:18:33.543107  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:18:34.622122 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:18:34.664497  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8595
I1123 16:18:34.664497  5096 solver.cpp:397]     Test net output #1: loss = 0.422943 (* 1 = 0.422943 loss)
I1123 16:18:34.702512  5096 solver.cpp:218] Iteration 11500 (19.7514 iter/s, 5.06292s/100 iters), loss = 0.33401
I1123 16:18:34.702512  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:18:34.702512  5096 solver.cpp:237]     Train net output #1: loss = 0.33401 (* 1 = 0.33401 loss)
I1123 16:18:34.702512  5096 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1123 16:18:38.667708  5096 solver.cpp:218] Iteration 11600 (25.2177 iter/s, 3.96546s/100 iters), loss = 0.288274
I1123 16:18:38.667708  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:18:38.667708  5096 solver.cpp:237]     Train net output #1: loss = 0.288274 (* 1 = 0.288274 loss)
I1123 16:18:38.668709  5096 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1123 16:18:42.650183  5096 solver.cpp:218] Iteration 11700 (25.115 iter/s, 3.98169s/100 iters), loss = 0.275234
I1123 16:18:42.650183  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:18:42.650183  5096 solver.cpp:237]     Train net output #1: loss = 0.275234 (* 1 = 0.275234 loss)
I1123 16:18:42.650183  5096 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1123 16:18:46.524247  5096 solver.cpp:218] Iteration 11800 (25.8128 iter/s, 3.87404s/100 iters), loss = 0.367899
I1123 16:18:46.524247  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:18:46.525249  5096 solver.cpp:237]     Train net output #1: loss = 0.367899 (* 1 = 0.367899 loss)
I1123 16:18:46.525249  5096 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1123 16:18:50.424353  5096 solver.cpp:218] Iteration 11900 (25.6465 iter/s, 3.89917s/100 iters), loss = 0.222907
I1123 16:18:50.424353  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:18:50.424353  5096 solver.cpp:237]     Train net output #1: loss = 0.222907 (* 1 = 0.222907 loss)
I1123 16:18:50.424353  5096 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1123 16:18:54.094815 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:18:54.245329  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_12000.caffemodel
I1123 16:18:54.256330  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_12000.solverstate
I1123 16:18:54.260329  5096 solver.cpp:330] Iteration 12000, Testing net (#0)
I1123 16:18:54.260329  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:18:55.321800 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:18:55.363821  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8577
I1123 16:18:55.363821  5096 solver.cpp:397]     Test net output #1: loss = 0.423528 (* 1 = 0.423528 loss)
I1123 16:18:55.400852  5096 solver.cpp:218] Iteration 12000 (20.095 iter/s, 4.97636s/100 iters), loss = 0.273401
I1123 16:18:55.400852  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:18:55.400852  5096 solver.cpp:237]     Train net output #1: loss = 0.273401 (* 1 = 0.273401 loss)
I1123 16:18:55.400852  5096 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1123 16:18:59.244091  5096 solver.cpp:218] Iteration 12100 (26.0256 iter/s, 3.84236s/100 iters), loss = 0.267094
I1123 16:18:59.244091  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:18:59.244091  5096 solver.cpp:237]     Train net output #1: loss = 0.267094 (* 1 = 0.267094 loss)
I1123 16:18:59.244091  5096 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1123 16:19:03.084050  5096 solver.cpp:218] Iteration 12200 (26.0397 iter/s, 3.84029s/100 iters), loss = 0.261393
I1123 16:19:03.084050  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:19:03.084050  5096 solver.cpp:237]     Train net output #1: loss = 0.261392 (* 1 = 0.261392 loss)
I1123 16:19:03.084050  5096 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1123 16:19:06.925984  5096 solver.cpp:218] Iteration 12300 (26.0301 iter/s, 3.84171s/100 iters), loss = 0.284919
I1123 16:19:06.926990  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:19:06.926990  5096 solver.cpp:237]     Train net output #1: loss = 0.284919 (* 1 = 0.284919 loss)
I1123 16:19:06.926990  5096 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1123 16:19:10.769171  5096 solver.cpp:218] Iteration 12400 (26.0267 iter/s, 3.84221s/100 iters), loss = 0.284394
I1123 16:19:10.769171  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:19:10.769171  5096 solver.cpp:237]     Train net output #1: loss = 0.284394 (* 1 = 0.284394 loss)
I1123 16:19:10.769171  5096 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1123 16:19:14.423559 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:19:14.576571  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_12500.caffemodel
I1123 16:19:14.586572  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_12500.solverstate
I1123 16:19:14.591073  5096 solver.cpp:330] Iteration 12500, Testing net (#0)
I1123 16:19:14.591073  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:19:15.646311 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:19:15.687851  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8586
I1123 16:19:15.687851  5096 solver.cpp:397]     Test net output #1: loss = 0.421033 (* 1 = 0.421033 loss)
I1123 16:19:15.724644  5096 solver.cpp:218] Iteration 12500 (20.1816 iter/s, 4.955s/100 iters), loss = 0.313401
I1123 16:19:15.724644  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 16:19:15.724644  5096 solver.cpp:237]     Train net output #1: loss = 0.313401 (* 1 = 0.313401 loss)
I1123 16:19:15.724644  5096 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1123 16:19:19.554482  5096 solver.cpp:218] Iteration 12600 (26.1124 iter/s, 3.82959s/100 iters), loss = 0.317224
I1123 16:19:19.554482  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:19:19.554981  5096 solver.cpp:237]     Train net output #1: loss = 0.317224 (* 1 = 0.317224 loss)
I1123 16:19:19.554981  5096 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1123 16:19:23.390842  5096 solver.cpp:218] Iteration 12700 (26.0685 iter/s, 3.83605s/100 iters), loss = 0.272267
I1123 16:19:23.390842  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:19:23.390842  5096 solver.cpp:237]     Train net output #1: loss = 0.272267 (* 1 = 0.272267 loss)
I1123 16:19:23.390842  5096 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1123 16:19:27.246731  5096 solver.cpp:218] Iteration 12800 (25.936 iter/s, 3.85564s/100 iters), loss = 0.339015
I1123 16:19:27.246731  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:19:27.246731  5096 solver.cpp:237]     Train net output #1: loss = 0.339015 (* 1 = 0.339015 loss)
I1123 16:19:27.246731  5096 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1123 16:19:31.108832  5096 solver.cpp:218] Iteration 12900 (25.8939 iter/s, 3.86191s/100 iters), loss = 0.260038
I1123 16:19:31.108832  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:19:31.108832  5096 solver.cpp:237]     Train net output #1: loss = 0.260038 (* 1 = 0.260038 loss)
I1123 16:19:31.108832  5096 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1123 16:19:34.776162 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:19:34.927238  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_13000.caffemodel
I1123 16:19:34.937225  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_13000.solverstate
I1123 16:19:34.941226  5096 solver.cpp:330] Iteration 13000, Testing net (#0)
I1123 16:19:34.941226  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:19:36.010934 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:19:36.052933  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8594
I1123 16:19:36.052933  5096 solver.cpp:397]     Test net output #1: loss = 0.419652 (* 1 = 0.419652 loss)
I1123 16:19:36.089952  5096 solver.cpp:218] Iteration 13000 (20.0796 iter/s, 4.98018s/100 iters), loss = 0.310601
I1123 16:19:36.089952  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:19:36.089952  5096 solver.cpp:237]     Train net output #1: loss = 0.310601 (* 1 = 0.310601 loss)
I1123 16:19:36.089952  5096 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1123 16:19:39.953410  5096 solver.cpp:218] Iteration 13100 (25.8873 iter/s, 3.8629s/100 iters), loss = 0.320481
I1123 16:19:39.953410  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:19:39.953410  5096 solver.cpp:237]     Train net output #1: loss = 0.320481 (* 1 = 0.320481 loss)
I1123 16:19:39.953410  5096 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1123 16:19:43.813251  5096 solver.cpp:218] Iteration 13200 (25.9081 iter/s, 3.85979s/100 iters), loss = 0.280818
I1123 16:19:43.813251  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:19:43.813251  5096 solver.cpp:237]     Train net output #1: loss = 0.280818 (* 1 = 0.280818 loss)
I1123 16:19:43.813251  5096 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1123 16:19:47.679455  5096 solver.cpp:218] Iteration 13300 (25.8685 iter/s, 3.86571s/100 iters), loss = 0.37067
I1123 16:19:47.679455  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:19:47.679455  5096 solver.cpp:237]     Train net output #1: loss = 0.37067 (* 1 = 0.37067 loss)
I1123 16:19:47.679455  5096 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1123 16:19:51.535228  5096 solver.cpp:218] Iteration 13400 (25.9398 iter/s, 3.85507s/100 iters), loss = 0.265341
I1123 16:19:51.535228  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:19:51.535228  5096 solver.cpp:237]     Train net output #1: loss = 0.265341 (* 1 = 0.265341 loss)
I1123 16:19:51.535228  5096 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1123 16:19:55.192884 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:19:55.345042  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_13500.caffemodel
I1123 16:19:55.355047  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_13500.solverstate
I1123 16:19:55.359047  5096 solver.cpp:330] Iteration 13500, Testing net (#0)
I1123 16:19:55.359047  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:19:56.422338 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:19:56.464357  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8595
I1123 16:19:56.464357  5096 solver.cpp:397]     Test net output #1: loss = 0.419558 (* 1 = 0.419558 loss)
I1123 16:19:56.502866  5096 solver.cpp:218] Iteration 13500 (20.1325 iter/s, 4.9671s/100 iters), loss = 0.262073
I1123 16:19:56.502866  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:19:56.502866  5096 solver.cpp:237]     Train net output #1: loss = 0.262073 (* 1 = 0.262073 loss)
I1123 16:19:56.502866  5096 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1123 16:20:00.382393  5096 solver.cpp:218] Iteration 13600 (25.7774 iter/s, 3.87936s/100 iters), loss = 0.262383
I1123 16:20:00.382393  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:20:00.382393  5096 solver.cpp:237]     Train net output #1: loss = 0.262383 (* 1 = 0.262383 loss)
I1123 16:20:00.382393  5096 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1123 16:20:04.258162  5096 solver.cpp:218] Iteration 13700 (25.802 iter/s, 3.87567s/100 iters), loss = 0.270426
I1123 16:20:04.258162  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:20:04.258162  5096 solver.cpp:237]     Train net output #1: loss = 0.270426 (* 1 = 0.270426 loss)
I1123 16:20:04.258162  5096 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1123 16:20:08.132066  5096 solver.cpp:218] Iteration 13800 (25.8141 iter/s, 3.87386s/100 iters), loss = 0.321446
I1123 16:20:08.132066  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:20:08.133080  5096 solver.cpp:237]     Train net output #1: loss = 0.321445 (* 1 = 0.321445 loss)
I1123 16:20:08.133080  5096 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1123 16:20:11.998117  5096 solver.cpp:218] Iteration 13900 (25.8696 iter/s, 3.86555s/100 iters), loss = 0.266249
I1123 16:20:11.998117  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:20:11.998117  5096 solver.cpp:237]     Train net output #1: loss = 0.266249 (* 1 = 0.266249 loss)
I1123 16:20:11.998117  5096 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1123 16:20:15.669873 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:20:15.820436  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_14000.caffemodel
I1123 16:20:15.830432  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_14000.solverstate
I1123 16:20:15.834434  5096 solver.cpp:330] Iteration 14000, Testing net (#0)
I1123 16:20:15.834434  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:20:16.902575 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:20:16.944597  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8588
I1123 16:20:16.944597  5096 solver.cpp:397]     Test net output #1: loss = 0.421464 (* 1 = 0.421464 loss)
I1123 16:20:16.980631  5096 solver.cpp:218] Iteration 14000 (20.0721 iter/s, 4.98205s/100 iters), loss = 0.278143
I1123 16:20:16.980631  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:20:16.980631  5096 solver.cpp:237]     Train net output #1: loss = 0.278142 (* 1 = 0.278142 loss)
I1123 16:20:16.980631  5096 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1123 16:20:20.851876  5096 solver.cpp:218] Iteration 14100 (25.837 iter/s, 3.87042s/100 iters), loss = 0.311678
I1123 16:20:20.851876  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:20:20.851876  5096 solver.cpp:237]     Train net output #1: loss = 0.311678 (* 1 = 0.311678 loss)
I1123 16:20:20.851876  5096 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1123 16:20:24.719954  5096 solver.cpp:218] Iteration 14200 (25.8571 iter/s, 3.86741s/100 iters), loss = 0.264207
I1123 16:20:24.719954  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:20:24.719954  5096 solver.cpp:237]     Train net output #1: loss = 0.264207 (* 1 = 0.264207 loss)
I1123 16:20:24.719954  5096 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1123 16:20:28.589329  5096 solver.cpp:218] Iteration 14300 (25.8441 iter/s, 3.86935s/100 iters), loss = 0.328197
I1123 16:20:28.589329  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:20:28.589329  5096 solver.cpp:237]     Train net output #1: loss = 0.328197 (* 1 = 0.328197 loss)
I1123 16:20:28.589329  5096 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1123 16:20:32.457751  5096 solver.cpp:218] Iteration 14400 (25.855 iter/s, 3.86772s/100 iters), loss = 0.202285
I1123 16:20:32.457751  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:20:32.457751  5096 solver.cpp:237]     Train net output #1: loss = 0.202285 (* 1 = 0.202285 loss)
I1123 16:20:32.457751  5096 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1123 16:20:36.137603 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:20:36.288168  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_14500.caffemodel
I1123 16:20:36.298154  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_14500.solverstate
I1123 16:20:36.302175  5096 solver.cpp:330] Iteration 14500, Testing net (#0)
I1123 16:20:36.302175  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:20:37.364934 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:20:37.406445  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8603
I1123 16:20:37.406445  5096 solver.cpp:397]     Test net output #1: loss = 0.420061 (* 1 = 0.420061 loss)
I1123 16:20:37.443450  5096 solver.cpp:218] Iteration 14500 (20.0585 iter/s, 4.98542s/100 iters), loss = 0.229231
I1123 16:20:37.443450  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:20:37.443450  5096 solver.cpp:237]     Train net output #1: loss = 0.229231 (* 1 = 0.229231 loss)
I1123 16:20:37.443450  5096 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1123 16:20:41.299100  5096 solver.cpp:218] Iteration 14600 (25.938 iter/s, 3.85535s/100 iters), loss = 0.24284
I1123 16:20:41.299100  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:20:41.299100  5096 solver.cpp:237]     Train net output #1: loss = 0.24284 (* 1 = 0.24284 loss)
I1123 16:20:41.299100  5096 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1123 16:20:45.167413  5096 solver.cpp:218] Iteration 14700 (25.8533 iter/s, 3.86798s/100 iters), loss = 0.286071
I1123 16:20:45.167413  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:20:45.167413  5096 solver.cpp:237]     Train net output #1: loss = 0.286071 (* 1 = 0.286071 loss)
I1123 16:20:45.167413  5096 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1123 16:20:49.030779  5096 solver.cpp:218] Iteration 14800 (25.8878 iter/s, 3.86282s/100 iters), loss = 0.34158
I1123 16:20:49.030779  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:20:49.030779  5096 solver.cpp:237]     Train net output #1: loss = 0.34158 (* 1 = 0.34158 loss)
I1123 16:20:49.030779  5096 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1123 16:20:52.888676  5096 solver.cpp:218] Iteration 14900 (25.9225 iter/s, 3.85766s/100 iters), loss = 0.257322
I1123 16:20:52.888676  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:20:52.888676  5096 solver.cpp:237]     Train net output #1: loss = 0.257322 (* 1 = 0.257322 loss)
I1123 16:20:52.888676  5096 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1123 16:20:56.559345 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:20:56.710194  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_15000.caffemodel
I1123 16:20:56.721179  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_15000.solverstate
I1123 16:20:56.725179  5096 solver.cpp:330] Iteration 15000, Testing net (#0)
I1123 16:20:56.725179  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:20:57.790361 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:20:57.832376  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8594
I1123 16:20:57.833390  5096 solver.cpp:397]     Test net output #1: loss = 0.421146 (* 1 = 0.421146 loss)
I1123 16:20:57.869895  5096 solver.cpp:218] Iteration 15000 (20.0774 iter/s, 4.98071s/100 iters), loss = 0.258753
I1123 16:20:57.869895  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:20:57.869895  5096 solver.cpp:237]     Train net output #1: loss = 0.258753 (* 1 = 0.258753 loss)
I1123 16:20:57.869895  5096 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1123 16:21:01.727932  5096 solver.cpp:218] Iteration 15100 (25.9203 iter/s, 3.85798s/100 iters), loss = 0.25366
I1123 16:21:01.727932  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:21:01.727932  5096 solver.cpp:237]     Train net output #1: loss = 0.25366 (* 1 = 0.25366 loss)
I1123 16:21:01.727932  5096 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1123 16:21:05.595942  5096 solver.cpp:218] Iteration 15200 (25.8539 iter/s, 3.86788s/100 iters), loss = 0.258355
I1123 16:21:05.595942  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 16:21:05.595942  5096 solver.cpp:237]     Train net output #1: loss = 0.258355 (* 1 = 0.258355 loss)
I1123 16:21:05.595942  5096 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1123 16:21:09.466372  5096 solver.cpp:218] Iteration 15300 (25.8436 iter/s, 3.86942s/100 iters), loss = 0.355851
I1123 16:21:09.466372  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 16:21:09.466372  5096 solver.cpp:237]     Train net output #1: loss = 0.355851 (* 1 = 0.355851 loss)
I1123 16:21:09.466372  5096 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1123 16:21:09.466372  5096 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1123 16:21:13.322784  5096 solver.cpp:218] Iteration 15400 (25.9344 iter/s, 3.85589s/100 iters), loss = 0.320684
I1123 16:21:13.322784  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:21:13.322784  5096 solver.cpp:237]     Train net output #1: loss = 0.320684 (* 1 = 0.320684 loss)
I1123 16:21:13.322784  5096 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1123 16:21:16.984401 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:21:17.135468  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_15500.caffemodel
I1123 16:21:17.148470  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_15500.solverstate
I1123 16:21:17.152451  5096 solver.cpp:330] Iteration 15500, Testing net (#0)
I1123 16:21:17.152451  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:21:18.214646 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:21:18.256649  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8599
I1123 16:21:18.256649  5096 solver.cpp:397]     Test net output #1: loss = 0.419686 (* 1 = 0.419686 loss)
I1123 16:21:18.294375  5096 solver.cpp:218] Iteration 15500 (20.1153 iter/s, 4.97133s/100 iters), loss = 0.204526
I1123 16:21:18.294375  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:21:18.294375  5096 solver.cpp:237]     Train net output #1: loss = 0.204526 (* 1 = 0.204526 loss)
I1123 16:21:18.294375  5096 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1123 16:21:22.172670  5096 solver.cpp:218] Iteration 15600 (25.7823 iter/s, 3.87863s/100 iters), loss = 0.323459
I1123 16:21:22.173671  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:21:22.173671  5096 solver.cpp:237]     Train net output #1: loss = 0.323459 (* 1 = 0.323459 loss)
I1123 16:21:22.173671  5096 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1123 16:21:26.035763  5096 solver.cpp:218] Iteration 15700 (25.8937 iter/s, 3.86194s/100 iters), loss = 0.277117
I1123 16:21:26.035763  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:21:26.035763  5096 solver.cpp:237]     Train net output #1: loss = 0.277116 (* 1 = 0.277116 loss)
I1123 16:21:26.035763  5096 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1123 16:21:29.893364  5096 solver.cpp:218] Iteration 15800 (25.9278 iter/s, 3.85686s/100 iters), loss = 0.310442
I1123 16:21:29.893364  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:21:29.893364  5096 solver.cpp:237]     Train net output #1: loss = 0.310442 (* 1 = 0.310442 loss)
I1123 16:21:29.893364  5096 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1123 16:21:33.753116  5096 solver.cpp:218] Iteration 15900 (25.9077 iter/s, 3.85985s/100 iters), loss = 0.270734
I1123 16:21:33.753116  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:21:33.753116  5096 solver.cpp:237]     Train net output #1: loss = 0.270734 (* 1 = 0.270734 loss)
I1123 16:21:33.753116  5096 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1123 16:21:37.427230 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:21:37.580281  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_16000.caffemodel
I1123 16:21:37.591255  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_16000.solverstate
I1123 16:21:37.595773  5096 solver.cpp:330] Iteration 16000, Testing net (#0)
I1123 16:21:37.596274  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:21:38.665915 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:21:38.707942  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8604
I1123 16:21:38.707942  5096 solver.cpp:397]     Test net output #1: loss = 0.419032 (* 1 = 0.419032 loss)
I1123 16:21:38.743937  5096 solver.cpp:218] Iteration 16000 (20.0377 iter/s, 4.99058s/100 iters), loss = 0.29827
I1123 16:21:38.743937  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:21:38.743937  5096 solver.cpp:237]     Train net output #1: loss = 0.298269 (* 1 = 0.298269 loss)
I1123 16:21:38.743937  5096 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1123 16:21:42.598212  5096 solver.cpp:218] Iteration 16100 (25.9542 iter/s, 3.85294s/100 iters), loss = 0.319489
I1123 16:21:42.598212  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:21:42.598212  5096 solver.cpp:237]     Train net output #1: loss = 0.319489 (* 1 = 0.319489 loss)
I1123 16:21:42.598212  5096 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1123 16:21:46.516762  5096 solver.cpp:218] Iteration 16200 (25.5185 iter/s, 3.91872s/100 iters), loss = 0.254606
I1123 16:21:46.516762  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:21:46.516762  5096 solver.cpp:237]     Train net output #1: loss = 0.254606 (* 1 = 0.254606 loss)
I1123 16:21:46.516762  5096 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1123 16:21:50.378717  5096 solver.cpp:218] Iteration 16300 (25.8944 iter/s, 3.86184s/100 iters), loss = 0.324063
I1123 16:21:50.378717  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:21:50.378717  5096 solver.cpp:237]     Train net output #1: loss = 0.324063 (* 1 = 0.324063 loss)
I1123 16:21:50.378717  5096 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1123 16:21:54.249940  5096 solver.cpp:218] Iteration 16400 (25.8374 iter/s, 3.87035s/100 iters), loss = 0.279826
I1123 16:21:54.249940  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:21:54.249940  5096 solver.cpp:237]     Train net output #1: loss = 0.279826 (* 1 = 0.279826 loss)
I1123 16:21:54.249940  5096 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1123 16:21:57.922454 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:21:58.074271  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_16500.caffemodel
I1123 16:21:58.083276  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_16500.solverstate
I1123 16:21:58.088276  5096 solver.cpp:330] Iteration 16500, Testing net (#0)
I1123 16:21:58.088276  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:21:59.155169 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:21:59.196691  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8605
I1123 16:21:59.196691  5096 solver.cpp:397]     Test net output #1: loss = 0.418838 (* 1 = 0.418838 loss)
I1123 16:21:59.234210  5096 solver.cpp:218] Iteration 16500 (20.0659 iter/s, 4.98358s/100 iters), loss = 0.225602
I1123 16:21:59.234210  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:21:59.234210  5096 solver.cpp:237]     Train net output #1: loss = 0.225602 (* 1 = 0.225602 loss)
I1123 16:21:59.234210  5096 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1123 16:22:03.086226  5096 solver.cpp:218] Iteration 16600 (25.9627 iter/s, 3.85168s/100 iters), loss = 0.2802
I1123 16:22:03.086226  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:22:03.086226  5096 solver.cpp:237]     Train net output #1: loss = 0.2802 (* 1 = 0.2802 loss)
I1123 16:22:03.086226  5096 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1123 16:22:06.962018  5096 solver.cpp:218] Iteration 16700 (25.8002 iter/s, 3.87594s/100 iters), loss = 0.240601
I1123 16:22:06.962018  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:22:06.962018  5096 solver.cpp:237]     Train net output #1: loss = 0.240601 (* 1 = 0.240601 loss)
I1123 16:22:06.962018  5096 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1123 16:22:10.832051  5096 solver.cpp:218] Iteration 16800 (25.8415 iter/s, 3.86974s/100 iters), loss = 0.267757
I1123 16:22:10.832051  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:22:10.833050  5096 solver.cpp:237]     Train net output #1: loss = 0.267757 (* 1 = 0.267757 loss)
I1123 16:22:10.833050  5096 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1123 16:22:14.723779  5096 solver.cpp:218] Iteration 16900 (25.6984 iter/s, 3.89129s/100 iters), loss = 0.274796
I1123 16:22:14.724761  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:22:14.724761  5096 solver.cpp:237]     Train net output #1: loss = 0.274796 (* 1 = 0.274796 loss)
I1123 16:22:14.724761  5096 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1123 16:22:18.393985 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:22:18.545043  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_17000.caffemodel
I1123 16:22:18.555042  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_17000.solverstate
I1123 16:22:18.560041  5096 solver.cpp:330] Iteration 17000, Testing net (#0)
I1123 16:22:18.560041  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:22:19.621238 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:22:19.663254  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8609
I1123 16:22:19.663254  5096 solver.cpp:397]     Test net output #1: loss = 0.418717 (* 1 = 0.418717 loss)
I1123 16:22:19.700258  5096 solver.cpp:218] Iteration 17000 (20.0968 iter/s, 4.97592s/100 iters), loss = 0.284367
I1123 16:22:19.701258  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:22:19.701258  5096 solver.cpp:237]     Train net output #1: loss = 0.284367 (* 1 = 0.284367 loss)
I1123 16:22:19.701258  5096 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1123 16:22:23.544391  5096 solver.cpp:218] Iteration 17100 (26.0212 iter/s, 3.84302s/100 iters), loss = 0.29509
I1123 16:22:23.544391  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:22:23.544391  5096 solver.cpp:237]     Train net output #1: loss = 0.29509 (* 1 = 0.29509 loss)
I1123 16:22:23.544391  5096 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1123 16:22:27.387820  5096 solver.cpp:218] Iteration 17200 (26.022 iter/s, 3.84291s/100 iters), loss = 0.228133
I1123 16:22:27.387820  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:22:27.387820  5096 solver.cpp:237]     Train net output #1: loss = 0.228133 (* 1 = 0.228133 loss)
I1123 16:22:27.387820  5096 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1123 16:22:31.233726  5096 solver.cpp:218] Iteration 17300 (26.0042 iter/s, 3.84554s/100 iters), loss = 0.318489
I1123 16:22:31.233726  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:22:31.233726  5096 solver.cpp:237]     Train net output #1: loss = 0.318489 (* 1 = 0.318489 loss)
I1123 16:22:31.233726  5096 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1123 16:22:35.076982  5096 solver.cpp:218] Iteration 17400 (26.0183 iter/s, 3.84346s/100 iters), loss = 0.258409
I1123 16:22:35.076982  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:22:35.076982  5096 solver.cpp:237]     Train net output #1: loss = 0.258409 (* 1 = 0.258409 loss)
I1123 16:22:35.076982  5096 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1123 16:22:38.737617 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:22:38.887681  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_17500.caffemodel
I1123 16:22:38.898167  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_17500.solverstate
I1123 16:22:38.902665  5096 solver.cpp:330] Iteration 17500, Testing net (#0)
I1123 16:22:38.902665  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:22:39.964882 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:22:40.006444  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8603
I1123 16:22:40.006444  5096 solver.cpp:397]     Test net output #1: loss = 0.418645 (* 1 = 0.418645 loss)
I1123 16:22:40.042933  5096 solver.cpp:218] Iteration 17500 (20.1393 iter/s, 4.96543s/100 iters), loss = 0.242194
I1123 16:22:40.042933  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:22:40.042933  5096 solver.cpp:237]     Train net output #1: loss = 0.242194 (* 1 = 0.242194 loss)
I1123 16:22:40.042933  5096 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1123 16:22:43.894600  5096 solver.cpp:218] Iteration 17600 (25.9641 iter/s, 3.85148s/100 iters), loss = 0.260369
I1123 16:22:43.894600  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:22:43.894600  5096 solver.cpp:237]     Train net output #1: loss = 0.260369 (* 1 = 0.260369 loss)
I1123 16:22:43.894600  5096 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1123 16:22:47.743988  5096 solver.cpp:218] Iteration 17700 (25.9829 iter/s, 3.84869s/100 iters), loss = 0.259454
I1123 16:22:47.743988  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:22:47.743988  5096 solver.cpp:237]     Train net output #1: loss = 0.259454 (* 1 = 0.259454 loss)
I1123 16:22:47.743988  5096 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1123 16:22:51.589221  5096 solver.cpp:218] Iteration 17800 (26.006 iter/s, 3.84527s/100 iters), loss = 0.334303
I1123 16:22:51.589221  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:22:51.589221  5096 solver.cpp:237]     Train net output #1: loss = 0.334303 (* 1 = 0.334303 loss)
I1123 16:22:51.589221  5096 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1123 16:22:55.439628  5096 solver.cpp:218] Iteration 17900 (25.9753 iter/s, 3.84981s/100 iters), loss = 0.209797
I1123 16:22:55.439628  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:22:55.439628  5096 solver.cpp:237]     Train net output #1: loss = 0.209797 (* 1 = 0.209797 loss)
I1123 16:22:55.439628  5096 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1123 16:22:59.099894 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:22:59.250458  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_18000.caffemodel
I1123 16:22:59.260455  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_18000.solverstate
I1123 16:22:59.264472  5096 solver.cpp:330] Iteration 18000, Testing net (#0)
I1123 16:22:59.264472  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:23:00.328945 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:23:00.370945  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8607
I1123 16:23:00.370945  5096 solver.cpp:397]     Test net output #1: loss = 0.418735 (* 1 = 0.418735 loss)
I1123 16:23:00.407959  5096 solver.cpp:218] Iteration 18000 (20.1303 iter/s, 4.96763s/100 iters), loss = 0.254158
I1123 16:23:00.407959  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:23:00.407959  5096 solver.cpp:237]     Train net output #1: loss = 0.254158 (* 1 = 0.254158 loss)
I1123 16:23:00.407959  5096 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1123 16:23:04.254909  5096 solver.cpp:218] Iteration 18100 (25.9971 iter/s, 3.84659s/100 iters), loss = 0.293837
I1123 16:23:04.254909  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:23:04.254909  5096 solver.cpp:237]     Train net output #1: loss = 0.293837 (* 1 = 0.293837 loss)
I1123 16:23:04.254909  5096 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1123 16:23:08.101188  5096 solver.cpp:218] Iteration 18200 (26.0017 iter/s, 3.84591s/100 iters), loss = 0.269162
I1123 16:23:08.101188  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:23:08.101188  5096 solver.cpp:237]     Train net output #1: loss = 0.269162 (* 1 = 0.269162 loss)
I1123 16:23:08.101188  5096 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1123 16:23:11.949311  5096 solver.cpp:218] Iteration 18300 (25.99 iter/s, 3.84763s/100 iters), loss = 0.323925
I1123 16:23:11.949311  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:23:11.949311  5096 solver.cpp:237]     Train net output #1: loss = 0.323925 (* 1 = 0.323925 loss)
I1123 16:23:11.949311  5096 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1123 16:23:15.795596  5096 solver.cpp:218] Iteration 18400 (25.9993 iter/s, 3.84626s/100 iters), loss = 0.274728
I1123 16:23:15.795596  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:23:15.795596  5096 solver.cpp:237]     Train net output #1: loss = 0.274728 (* 1 = 0.274728 loss)
I1123 16:23:15.795596  5096 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1123 16:23:19.450937 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:23:19.602967  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_18500.caffemodel
I1123 16:23:19.611968  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_18500.solverstate
I1123 16:23:19.615968  5096 solver.cpp:330] Iteration 18500, Testing net (#0)
I1123 16:23:19.615968  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:23:20.682025 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:23:20.724033  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8609
I1123 16:23:20.724033  5096 solver.cpp:397]     Test net output #1: loss = 0.418783 (* 1 = 0.418783 loss)
I1123 16:23:20.761085  5096 solver.cpp:218] Iteration 18500 (20.1419 iter/s, 4.96477s/100 iters), loss = 0.30466
I1123 16:23:20.761085  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:23:20.761085  5096 solver.cpp:237]     Train net output #1: loss = 0.30466 (* 1 = 0.30466 loss)
I1123 16:23:20.761085  5096 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1123 16:23:24.610158  5096 solver.cpp:218] Iteration 18600 (25.9835 iter/s, 3.84859s/100 iters), loss = 0.257252
I1123 16:23:24.610158  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:23:24.610158  5096 solver.cpp:237]     Train net output #1: loss = 0.257252 (* 1 = 0.257252 loss)
I1123 16:23:24.610158  5096 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1123 16:23:28.456027  5096 solver.cpp:218] Iteration 18700 (26.0016 iter/s, 3.84591s/100 iters), loss = 0.306893
I1123 16:23:28.456027  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:23:28.456027  5096 solver.cpp:237]     Train net output #1: loss = 0.306893 (* 1 = 0.306893 loss)
I1123 16:23:28.456027  5096 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1123 16:23:32.302279  5096 solver.cpp:218] Iteration 18800 (26.0039 iter/s, 3.84558s/100 iters), loss = 0.296022
I1123 16:23:32.302279  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:23:32.302279  5096 solver.cpp:237]     Train net output #1: loss = 0.296022 (* 1 = 0.296022 loss)
I1123 16:23:32.302279  5096 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1123 16:23:36.149734  5096 solver.cpp:218] Iteration 18900 (25.9939 iter/s, 3.84706s/100 iters), loss = 0.272295
I1123 16:23:36.149734  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:23:36.149734  5096 solver.cpp:237]     Train net output #1: loss = 0.272295 (* 1 = 0.272295 loss)
I1123 16:23:36.149734  5096 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1123 16:23:39.809046 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:23:39.960710  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_19000.caffemodel
I1123 16:23:39.970700  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_19000.solverstate
I1123 16:23:39.974702  5096 solver.cpp:330] Iteration 19000, Testing net (#0)
I1123 16:23:39.974702  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:23:41.039506 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:23:41.080580  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8603
I1123 16:23:41.080580  5096 solver.cpp:397]     Test net output #1: loss = 0.418836 (* 1 = 0.418836 loss)
I1123 16:23:41.117581  5096 solver.cpp:218] Iteration 19000 (20.1294 iter/s, 4.96787s/100 iters), loss = 0.272981
I1123 16:23:41.117581  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:23:41.117581  5096 solver.cpp:237]     Train net output #1: loss = 0.272981 (* 1 = 0.272981 loss)
I1123 16:23:41.117581  5096 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1123 16:23:44.971937  5096 solver.cpp:218] Iteration 19100 (25.9439 iter/s, 3.85446s/100 iters), loss = 0.300271
I1123 16:23:44.971937  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:23:44.972939  5096 solver.cpp:237]     Train net output #1: loss = 0.300271 (* 1 = 0.300271 loss)
I1123 16:23:44.972939  5096 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1123 16:23:48.830078  5096 solver.cpp:218] Iteration 19200 (25.9244 iter/s, 3.85737s/100 iters), loss = 0.282965
I1123 16:23:48.830078  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:23:48.830078  5096 solver.cpp:237]     Train net output #1: loss = 0.282965 (* 1 = 0.282965 loss)
I1123 16:23:48.830078  5096 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1123 16:23:52.685693  5096 solver.cpp:218] Iteration 19300 (25.939 iter/s, 3.8552s/100 iters), loss = 0.302
I1123 16:23:52.685693  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:23:52.685693  5096 solver.cpp:237]     Train net output #1: loss = 0.302 (* 1 = 0.302 loss)
I1123 16:23:52.685693  5096 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1123 16:23:56.541185  5096 solver.cpp:218] Iteration 19400 (25.9372 iter/s, 3.85547s/100 iters), loss = 0.271854
I1123 16:23:56.541185  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:23:56.541185  5096 solver.cpp:237]     Train net output #1: loss = 0.271854 (* 1 = 0.271854 loss)
I1123 16:23:56.541185  5096 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1123 16:24:00.210670 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:24:00.361937  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_19500.caffemodel
I1123 16:24:00.371472  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_19500.solverstate
I1123 16:24:00.375473  5096 solver.cpp:330] Iteration 19500, Testing net (#0)
I1123 16:24:00.376472  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:24:01.441336 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:24:01.482396  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8605
I1123 16:24:01.482396  5096 solver.cpp:397]     Test net output #1: loss = 0.418489 (* 1 = 0.418489 loss)
I1123 16:24:01.520397  5096 solver.cpp:218] Iteration 19500 (20.0883 iter/s, 4.97803s/100 iters), loss = 0.24998
I1123 16:24:01.520397  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:24:01.520397  5096 solver.cpp:237]     Train net output #1: loss = 0.24998 (* 1 = 0.24998 loss)
I1123 16:24:01.520397  5096 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1123 16:24:01.520397  5096 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1123 16:24:05.369699  5096 solver.cpp:218] Iteration 19600 (25.9806 iter/s, 3.84902s/100 iters), loss = 0.248387
I1123 16:24:05.369699  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:24:05.369699  5096 solver.cpp:237]     Train net output #1: loss = 0.248387 (* 1 = 0.248387 loss)
I1123 16:24:05.369699  5096 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1123 16:24:09.217242  5096 solver.cpp:218] Iteration 19700 (25.9885 iter/s, 3.84785s/100 iters), loss = 0.287587
I1123 16:24:09.217242  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:24:09.217242  5096 solver.cpp:237]     Train net output #1: loss = 0.287587 (* 1 = 0.287587 loss)
I1123 16:24:09.217242  5096 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1123 16:24:13.062065  5096 solver.cpp:218] Iteration 19800 (26.0176 iter/s, 3.84356s/100 iters), loss = 0.354164
I1123 16:24:13.062065  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:24:13.062065  5096 solver.cpp:237]     Train net output #1: loss = 0.354164 (* 1 = 0.354164 loss)
I1123 16:24:13.062065  5096 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1123 16:24:16.909224  5096 solver.cpp:218] Iteration 19900 (25.9953 iter/s, 3.84685s/100 iters), loss = 0.230641
I1123 16:24:16.909224  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:24:16.909224  5096 solver.cpp:237]     Train net output #1: loss = 0.230641 (* 1 = 0.230641 loss)
I1123 16:24:16.909224  5096 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1123 16:24:20.568558 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:24:20.718580  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_20000.caffemodel
I1123 16:24:20.729576  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_20000.solverstate
I1123 16:24:20.733569  5096 solver.cpp:330] Iteration 20000, Testing net (#0)
I1123 16:24:20.733569  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:24:21.798831 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:24:21.840839  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8604
I1123 16:24:21.840839  5096 solver.cpp:397]     Test net output #1: loss = 0.418512 (* 1 = 0.418512 loss)
I1123 16:24:21.877866  5096 solver.cpp:218] Iteration 20000 (20.128 iter/s, 4.9682s/100 iters), loss = 0.25538
I1123 16:24:21.877866  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:24:21.877866  5096 solver.cpp:237]     Train net output #1: loss = 0.25538 (* 1 = 0.25538 loss)
I1123 16:24:21.877866  5096 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1123 16:24:25.718713  5096 solver.cpp:218] Iteration 20100 (26.0368 iter/s, 3.84071s/100 iters), loss = 0.22664
I1123 16:24:25.718713  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:24:25.718713  5096 solver.cpp:237]     Train net output #1: loss = 0.22664 (* 1 = 0.22664 loss)
I1123 16:24:25.718713  5096 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1123 16:24:29.561084  5096 solver.cpp:218] Iteration 20200 (26.0229 iter/s, 3.84277s/100 iters), loss = 0.292769
I1123 16:24:29.562090  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:24:29.562090  5096 solver.cpp:237]     Train net output #1: loss = 0.292769 (* 1 = 0.292769 loss)
I1123 16:24:29.562090  5096 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1123 16:24:33.401444  5096 solver.cpp:218] Iteration 20300 (26.0416 iter/s, 3.84001s/100 iters), loss = 0.304782
I1123 16:24:33.402453  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:24:33.402453  5096 solver.cpp:237]     Train net output #1: loss = 0.304782 (* 1 = 0.304782 loss)
I1123 16:24:33.402453  5096 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1123 16:24:37.244876  5096 solver.cpp:218] Iteration 20400 (26.0214 iter/s, 3.84299s/100 iters), loss = 0.24573
I1123 16:24:37.244876  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:24:37.245877  5096 solver.cpp:237]     Train net output #1: loss = 0.24573 (* 1 = 0.24573 loss)
I1123 16:24:37.245877  5096 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1123 16:24:40.906746 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:24:41.058769  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_20500.caffemodel
I1123 16:24:41.068765  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_20500.solverstate
I1123 16:24:41.072787  5096 solver.cpp:330] Iteration 20500, Testing net (#0)
I1123 16:24:41.072787  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:24:42.142109 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:24:42.184123  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8605
I1123 16:24:42.184123  5096 solver.cpp:397]     Test net output #1: loss = 0.418394 (* 1 = 0.418394 loss)
I1123 16:24:42.220887  5096 solver.cpp:218] Iteration 20500 (20.101 iter/s, 4.97488s/100 iters), loss = 0.292617
I1123 16:24:42.220887  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 16:24:42.220887  5096 solver.cpp:237]     Train net output #1: loss = 0.292617 (* 1 = 0.292617 loss)
I1123 16:24:42.220887  5096 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1123 16:24:46.085742  5096 solver.cpp:218] Iteration 20600 (25.8778 iter/s, 3.86432s/100 iters), loss = 0.309669
I1123 16:24:46.085742  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:24:46.085742  5096 solver.cpp:237]     Train net output #1: loss = 0.309669 (* 1 = 0.309669 loss)
I1123 16:24:46.085742  5096 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1123 16:24:49.951892  5096 solver.cpp:218] Iteration 20700 (25.8682 iter/s, 3.86576s/100 iters), loss = 0.279783
I1123 16:24:49.951892  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:24:49.951892  5096 solver.cpp:237]     Train net output #1: loss = 0.279783 (* 1 = 0.279783 loss)
I1123 16:24:49.951892  5096 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1123 16:24:53.813060  5096 solver.cpp:218] Iteration 20800 (25.8982 iter/s, 3.86127s/100 iters), loss = 0.341677
I1123 16:24:53.813060  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:24:53.813060  5096 solver.cpp:237]     Train net output #1: loss = 0.341677 (* 1 = 0.341677 loss)
I1123 16:24:53.813060  5096 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1123 16:24:57.675755  5096 solver.cpp:218] Iteration 20900 (25.8941 iter/s, 3.86189s/100 iters), loss = 0.22218
I1123 16:24:57.675755  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:24:57.675755  5096 solver.cpp:237]     Train net output #1: loss = 0.22218 (* 1 = 0.22218 loss)
I1123 16:24:57.675755  5096 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1123 16:25:01.353046 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:25:01.504756  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_21000.caffemodel
I1123 16:25:01.514753  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_21000.solverstate
I1123 16:25:01.518755  5096 solver.cpp:330] Iteration 21000, Testing net (#0)
I1123 16:25:01.518755  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:25:02.588585 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:25:02.630602  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8603
I1123 16:25:02.630602  5096 solver.cpp:397]     Test net output #1: loss = 0.418384 (* 1 = 0.418384 loss)
I1123 16:25:02.667618  5096 solver.cpp:218] Iteration 21000 (20.032 iter/s, 4.99202s/100 iters), loss = 0.241572
I1123 16:25:02.667618  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:25:02.667618  5096 solver.cpp:237]     Train net output #1: loss = 0.241572 (* 1 = 0.241572 loss)
I1123 16:25:02.667618  5096 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1123 16:25:06.537678  5096 solver.cpp:218] Iteration 21100 (25.8449 iter/s, 3.86923s/100 iters), loss = 0.254072
I1123 16:25:06.537678  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:25:06.537678  5096 solver.cpp:237]     Train net output #1: loss = 0.254071 (* 1 = 0.254071 loss)
I1123 16:25:06.537678  5096 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1123 16:25:10.406623  5096 solver.cpp:218] Iteration 21200 (25.8467 iter/s, 3.86897s/100 iters), loss = 0.246377
I1123 16:25:10.406623  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:25:10.406623  5096 solver.cpp:237]     Train net output #1: loss = 0.246377 (* 1 = 0.246377 loss)
I1123 16:25:10.406623  5096 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1123 16:25:14.272768  5096 solver.cpp:218] Iteration 21300 (25.8653 iter/s, 3.86619s/100 iters), loss = 0.36901
I1123 16:25:14.273769  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 16:25:14.273769  5096 solver.cpp:237]     Train net output #1: loss = 0.36901 (* 1 = 0.36901 loss)
I1123 16:25:14.273769  5096 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1123 16:25:18.142287  5096 solver.cpp:218] Iteration 21400 (25.8488 iter/s, 3.86866s/100 iters), loss = 0.245336
I1123 16:25:18.142287  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:25:18.142287  5096 solver.cpp:237]     Train net output #1: loss = 0.245336 (* 1 = 0.245336 loss)
I1123 16:25:18.142287  5096 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1123 16:25:21.823927 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:25:21.975543  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_21500.caffemodel
I1123 16:25:21.988543  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_21500.solverstate
I1123 16:25:21.992542  5096 solver.cpp:330] Iteration 21500, Testing net (#0)
I1123 16:25:21.992542  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:25:23.062337 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:25:23.104346  5096 solver.cpp:397]     Test net output #0: accuracy = 0.86
I1123 16:25:23.104346  5096 solver.cpp:397]     Test net output #1: loss = 0.418441 (* 1 = 0.418441 loss)
I1123 16:25:23.141343  5096 solver.cpp:218] Iteration 21500 (20.0069 iter/s, 4.99828s/100 iters), loss = 0.232104
I1123 16:25:23.141343  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:25:23.141343  5096 solver.cpp:237]     Train net output #1: loss = 0.232104 (* 1 = 0.232104 loss)
I1123 16:25:23.141343  5096 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1123 16:25:27.001559  5096 solver.cpp:218] Iteration 21600 (25.9068 iter/s, 3.86s/100 iters), loss = 0.267213
I1123 16:25:27.001559  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:25:27.001559  5096 solver.cpp:237]     Train net output #1: loss = 0.267213 (* 1 = 0.267213 loss)
I1123 16:25:27.001559  5096 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1123 16:25:30.863916  5096 solver.cpp:218] Iteration 21700 (25.889 iter/s, 3.86264s/100 iters), loss = 0.251245
I1123 16:25:30.863916  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:25:30.863916  5096 solver.cpp:237]     Train net output #1: loss = 0.251245 (* 1 = 0.251245 loss)
I1123 16:25:30.864912  5096 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1123 16:25:34.726444  5096 solver.cpp:218] Iteration 21800 (25.8944 iter/s, 3.86184s/100 iters), loss = 0.299385
I1123 16:25:34.726444  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:25:34.726444  5096 solver.cpp:237]     Train net output #1: loss = 0.299385 (* 1 = 0.299385 loss)
I1123 16:25:34.726444  5096 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1123 16:25:38.590528  5096 solver.cpp:218] Iteration 21900 (25.8833 iter/s, 3.86349s/100 iters), loss = 0.250017
I1123 16:25:38.590528  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:25:38.590528  5096 solver.cpp:237]     Train net output #1: loss = 0.250017 (* 1 = 0.250017 loss)
I1123 16:25:38.590528  5096 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1123 16:25:42.265375 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:25:42.416427  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_22000.caffemodel
I1123 16:25:42.425429  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_22000.solverstate
I1123 16:25:42.430428  5096 solver.cpp:330] Iteration 22000, Testing net (#0)
I1123 16:25:42.430428  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:25:43.500671 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:25:43.542695  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8603
I1123 16:25:43.542695  5096 solver.cpp:397]     Test net output #1: loss = 0.418419 (* 1 = 0.418419 loss)
I1123 16:25:43.579710  5096 solver.cpp:218] Iteration 22000 (20.0463 iter/s, 4.98844s/100 iters), loss = 0.251259
I1123 16:25:43.579710  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:25:43.579710  5096 solver.cpp:237]     Train net output #1: loss = 0.251259 (* 1 = 0.251259 loss)
I1123 16:25:43.579710  5096 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1123 16:25:43.579710  5096 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1123 16:25:47.449661  5096 solver.cpp:218] Iteration 22100 (25.8378 iter/s, 3.8703s/100 iters), loss = 0.325799
I1123 16:25:47.449661  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:25:47.449661  5096 solver.cpp:237]     Train net output #1: loss = 0.325799 (* 1 = 0.325799 loss)
I1123 16:25:47.449661  5096 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1123 16:25:51.316900  5096 solver.cpp:218] Iteration 22200 (25.863 iter/s, 3.86652s/100 iters), loss = 0.266733
I1123 16:25:51.316900  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:25:51.317411  5096 solver.cpp:237]     Train net output #1: loss = 0.266733 (* 1 = 0.266733 loss)
I1123 16:25:51.317411  5096 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1123 16:25:55.187917  5096 solver.cpp:218] Iteration 22300 (25.8331 iter/s, 3.871s/100 iters), loss = 0.275077
I1123 16:25:55.187917  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:25:55.187917  5096 solver.cpp:237]     Train net output #1: loss = 0.275077 (* 1 = 0.275077 loss)
I1123 16:25:55.187917  5096 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1123 16:25:59.057242  5096 solver.cpp:218] Iteration 22400 (25.8479 iter/s, 3.86878s/100 iters), loss = 0.263186
I1123 16:25:59.057242  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:25:59.057242  5096 solver.cpp:237]     Train net output #1: loss = 0.263186 (* 1 = 0.263186 loss)
I1123 16:25:59.057242  5096 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1123 16:26:02.736810 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:26:02.888866  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_22500.caffemodel
I1123 16:26:02.898866  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_22500.solverstate
I1123 16:26:02.903867  5096 solver.cpp:330] Iteration 22500, Testing net (#0)
I1123 16:26:02.903867  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:26:03.972187 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:26:04.014191  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8607
I1123 16:26:04.014191  5096 solver.cpp:397]     Test net output #1: loss = 0.418355 (* 1 = 0.418355 loss)
I1123 16:26:04.051189  5096 solver.cpp:218] Iteration 22500 (20.027 iter/s, 4.99326s/100 iters), loss = 0.265461
I1123 16:26:04.051189  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:26:04.051189  5096 solver.cpp:237]     Train net output #1: loss = 0.265461 (* 1 = 0.265461 loss)
I1123 16:26:04.051189  5096 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1123 16:26:07.909149  5096 solver.cpp:218] Iteration 22600 (25.923 iter/s, 3.85758s/100 iters), loss = 0.26292
I1123 16:26:07.909149  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:26:07.909149  5096 solver.cpp:237]     Train net output #1: loss = 0.26292 (* 1 = 0.26292 loss)
I1123 16:26:07.909149  5096 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1123 16:26:11.765473  5096 solver.cpp:218] Iteration 22700 (25.9312 iter/s, 3.85636s/100 iters), loss = 0.276861
I1123 16:26:11.765473  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:26:11.765473  5096 solver.cpp:237]     Train net output #1: loss = 0.276861 (* 1 = 0.276861 loss)
I1123 16:26:11.765473  5096 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1123 16:26:15.629861  5096 solver.cpp:218] Iteration 22800 (25.8833 iter/s, 3.86349s/100 iters), loss = 0.333212
I1123 16:26:15.629861  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:26:15.629861  5096 solver.cpp:237]     Train net output #1: loss = 0.333212 (* 1 = 0.333212 loss)
I1123 16:26:15.629861  5096 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1123 16:26:19.495792  5096 solver.cpp:218] Iteration 22900 (25.8658 iter/s, 3.86611s/100 iters), loss = 0.229271
I1123 16:26:19.495792  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:26:19.495792  5096 solver.cpp:237]     Train net output #1: loss = 0.229271 (* 1 = 0.229271 loss)
I1123 16:26:19.495792  5096 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1123 16:26:23.171015 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:26:23.322528  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_23000.caffemodel
I1123 16:26:23.332020  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_23000.solverstate
I1123 16:26:23.336519  5096 solver.cpp:330] Iteration 23000, Testing net (#0)
I1123 16:26:23.336519  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:26:24.405894 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:26:24.448961  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8601
I1123 16:26:24.448961  5096 solver.cpp:397]     Test net output #1: loss = 0.418412 (* 1 = 0.418412 loss)
I1123 16:26:24.484954  5096 solver.cpp:218] Iteration 23000 (20.0443 iter/s, 4.98895s/100 iters), loss = 0.239792
I1123 16:26:24.485958  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:26:24.485958  5096 solver.cpp:237]     Train net output #1: loss = 0.239792 (* 1 = 0.239792 loss)
I1123 16:26:24.485958  5096 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1123 16:26:28.343273  5096 solver.cpp:218] Iteration 23100 (25.9227 iter/s, 3.85762s/100 iters), loss = 0.277796
I1123 16:26:28.343273  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:26:28.343273  5096 solver.cpp:237]     Train net output #1: loss = 0.277796 (* 1 = 0.277796 loss)
I1123 16:26:28.343273  5096 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1123 16:26:32.206044  5096 solver.cpp:218] Iteration 23200 (25.889 iter/s, 3.86265s/100 iters), loss = 0.264594
I1123 16:26:32.207044  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:26:32.207044  5096 solver.cpp:237]     Train net output #1: loss = 0.264594 (* 1 = 0.264594 loss)
I1123 16:26:32.207044  5096 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1123 16:26:36.072049  5096 solver.cpp:218] Iteration 23300 (25.875 iter/s, 3.86473s/100 iters), loss = 0.31721
I1123 16:26:36.072049  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:26:36.072049  5096 solver.cpp:237]     Train net output #1: loss = 0.31721 (* 1 = 0.31721 loss)
I1123 16:26:36.072049  5096 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1123 16:26:39.936861  5096 solver.cpp:218] Iteration 23400 (25.8769 iter/s, 3.86445s/100 iters), loss = 0.306193
I1123 16:26:39.936861  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:26:39.936861  5096 solver.cpp:237]     Train net output #1: loss = 0.306193 (* 1 = 0.306193 loss)
I1123 16:26:39.936861  5096 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1123 16:26:43.611588 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:26:43.763619  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_23500.caffemodel
I1123 16:26:43.774619  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_23500.solverstate
I1123 16:26:43.778638  5096 solver.cpp:330] Iteration 23500, Testing net (#0)
I1123 16:26:43.779624  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:26:44.848325 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:26:44.889843  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8604
I1123 16:26:44.889843  5096 solver.cpp:397]     Test net output #1: loss = 0.418414 (* 1 = 0.418414 loss)
I1123 16:26:44.926857  5096 solver.cpp:218] Iteration 23500 (20.0406 iter/s, 4.98986s/100 iters), loss = 0.235635
I1123 16:26:44.926857  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:26:44.926857  5096 solver.cpp:237]     Train net output #1: loss = 0.235635 (* 1 = 0.235635 loss)
I1123 16:26:44.926857  5096 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1123 16:26:48.787622  5096 solver.cpp:218] Iteration 23600 (25.904 iter/s, 3.8604s/100 iters), loss = 0.31358
I1123 16:26:48.787622  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 16:26:48.787622  5096 solver.cpp:237]     Train net output #1: loss = 0.31358 (* 1 = 0.31358 loss)
I1123 16:26:48.787622  5096 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1123 16:26:52.648576  5096 solver.cpp:218] Iteration 23700 (25.9024 iter/s, 3.86065s/100 iters), loss = 0.250243
I1123 16:26:52.648576  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:26:52.648576  5096 solver.cpp:237]     Train net output #1: loss = 0.250243 (* 1 = 0.250243 loss)
I1123 16:26:52.648576  5096 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1123 16:26:56.514272  5096 solver.cpp:218] Iteration 23800 (25.8723 iter/s, 3.86514s/100 iters), loss = 0.344227
I1123 16:26:56.514272  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:26:56.514272  5096 solver.cpp:237]     Train net output #1: loss = 0.344227 (* 1 = 0.344227 loss)
I1123 16:26:56.514272  5096 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1123 16:27:00.378262  5096 solver.cpp:218] Iteration 23900 (25.8836 iter/s, 3.86344s/100 iters), loss = 0.288765
I1123 16:27:00.378262  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 16:27:00.378262  5096 solver.cpp:237]     Train net output #1: loss = 0.288765 (* 1 = 0.288765 loss)
I1123 16:27:00.378262  5096 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1123 16:27:04.058356 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:27:04.209884  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_24000.caffemodel
I1123 16:27:04.219884  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_24000.solverstate
I1123 16:27:04.224903  5096 solver.cpp:330] Iteration 24000, Testing net (#0)
I1123 16:27:04.224903  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:27:05.294155 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:27:05.337173  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8602
I1123 16:27:05.337173  5096 solver.cpp:397]     Test net output #1: loss = 0.418456 (* 1 = 0.418456 loss)
I1123 16:27:05.374198  5096 solver.cpp:218] Iteration 24000 (20.0164 iter/s, 4.9959s/100 iters), loss = 0.293998
I1123 16:27:05.374198  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:27:05.374198  5096 solver.cpp:237]     Train net output #1: loss = 0.293998 (* 1 = 0.293998 loss)
I1123 16:27:05.374198  5096 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1123 16:27:09.240258  5096 solver.cpp:218] Iteration 24100 (25.8669 iter/s, 3.86594s/100 iters), loss = 0.308891
I1123 16:27:09.240258  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:27:09.240258  5096 solver.cpp:237]     Train net output #1: loss = 0.308891 (* 1 = 0.308891 loss)
I1123 16:27:09.240258  5096 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1123 16:27:13.113481  5096 solver.cpp:218] Iteration 24200 (25.8191 iter/s, 3.8731s/100 iters), loss = 0.2682
I1123 16:27:13.113481  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:27:13.113481  5096 solver.cpp:237]     Train net output #1: loss = 0.2682 (* 1 = 0.2682 loss)
I1123 16:27:13.113481  5096 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1123 16:27:16.978385  5096 solver.cpp:218] Iteration 24300 (25.8756 iter/s, 3.86464s/100 iters), loss = 0.336838
I1123 16:27:16.978385  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:27:16.978385  5096 solver.cpp:237]     Train net output #1: loss = 0.336838 (* 1 = 0.336838 loss)
I1123 16:27:16.978385  5096 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1123 16:27:20.847137  5096 solver.cpp:218] Iteration 24400 (25.8536 iter/s, 3.86793s/100 iters), loss = 0.235918
I1123 16:27:20.847137  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:27:20.847137  5096 solver.cpp:237]     Train net output #1: loss = 0.235918 (* 1 = 0.235918 loss)
I1123 16:27:20.847137  5096 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1123 16:27:24.527904 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:27:24.679949  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_24500.caffemodel
I1123 16:27:24.689949  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_24500.solverstate
I1123 16:27:24.694952  5096 solver.cpp:330] Iteration 24500, Testing net (#0)
I1123 16:27:24.694952  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:27:25.764024 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:27:25.806042  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8599
I1123 16:27:25.806042  5096 solver.cpp:397]     Test net output #1: loss = 0.418333 (* 1 = 0.418333 loss)
I1123 16:27:25.843056  5096 solver.cpp:218] Iteration 24500 (20.0163 iter/s, 4.99593s/100 iters), loss = 0.233672
I1123 16:27:25.843056  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:27:25.843056  5096 solver.cpp:237]     Train net output #1: loss = 0.233672 (* 1 = 0.233672 loss)
I1123 16:27:25.843056  5096 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1123 16:27:29.704804  5096 solver.cpp:218] Iteration 24600 (25.9025 iter/s, 3.86063s/100 iters), loss = 0.269799
I1123 16:27:29.704804  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:27:29.704804  5096 solver.cpp:237]     Train net output #1: loss = 0.269799 (* 1 = 0.269799 loss)
I1123 16:27:29.704804  5096 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1123 16:27:33.572361  5096 solver.cpp:218] Iteration 24700 (25.8582 iter/s, 3.86724s/100 iters), loss = 0.266253
I1123 16:27:33.572361  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:27:33.572361  5096 solver.cpp:237]     Train net output #1: loss = 0.266253 (* 1 = 0.266253 loss)
I1123 16:27:33.572361  5096 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1123 16:27:37.434414  5096 solver.cpp:218] Iteration 24800 (25.8956 iter/s, 3.86166s/100 iters), loss = 0.296522
I1123 16:27:37.434414  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:27:37.434414  5096 solver.cpp:237]     Train net output #1: loss = 0.296522 (* 1 = 0.296522 loss)
I1123 16:27:37.434414  5096 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1123 16:27:41.302253  5096 solver.cpp:218] Iteration 24900 (25.8517 iter/s, 3.86822s/100 iters), loss = 0.269758
I1123 16:27:41.302253  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:27:41.302253  5096 solver.cpp:237]     Train net output #1: loss = 0.269758 (* 1 = 0.269758 loss)
I1123 16:27:41.302253  5096 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1123 16:27:44.978031 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:27:45.130156  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_25000.caffemodel
I1123 16:27:45.141182  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_25000.solverstate
I1123 16:27:45.145176  5096 solver.cpp:330] Iteration 25000, Testing net (#0)
I1123 16:27:45.146181  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:27:46.214615 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:27:46.256626  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8599
I1123 16:27:46.256626  5096 solver.cpp:397]     Test net output #1: loss = 0.418445 (* 1 = 0.418445 loss)
I1123 16:27:46.293624  5096 solver.cpp:218] Iteration 25000 (20.0381 iter/s, 4.9905s/100 iters), loss = 0.246422
I1123 16:27:46.293624  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:27:46.293624  5096 solver.cpp:237]     Train net output #1: loss = 0.246422 (* 1 = 0.246422 loss)
I1123 16:27:46.293624  5096 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1123 16:27:50.150663  5096 solver.cpp:218] Iteration 25100 (25.9297 iter/s, 3.85658s/100 iters), loss = 0.253206
I1123 16:27:50.150663  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:27:50.150663  5096 solver.cpp:237]     Train net output #1: loss = 0.253206 (* 1 = 0.253206 loss)
I1123 16:27:50.150663  5096 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1123 16:27:54.013555  5096 solver.cpp:218] Iteration 25200 (25.8873 iter/s, 3.8629s/100 iters), loss = 0.223283
I1123 16:27:54.013555  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:27:54.013555  5096 solver.cpp:237]     Train net output #1: loss = 0.223283 (* 1 = 0.223283 loss)
I1123 16:27:54.013555  5096 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1123 16:27:57.880502  5096 solver.cpp:218] Iteration 25300 (25.8628 iter/s, 3.86655s/100 iters), loss = 0.321365
I1123 16:27:57.880502  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:27:57.880502  5096 solver.cpp:237]     Train net output #1: loss = 0.321365 (* 1 = 0.321365 loss)
I1123 16:27:57.880502  5096 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1123 16:28:01.743007  5096 solver.cpp:218] Iteration 25400 (25.8946 iter/s, 3.86181s/100 iters), loss = 0.224165
I1123 16:28:01.743007  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:28:01.743007  5096 solver.cpp:237]     Train net output #1: loss = 0.224165 (* 1 = 0.224165 loss)
I1123 16:28:01.743007  5096 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1123 16:28:05.417342 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:28:05.568632  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_25500.caffemodel
I1123 16:28:05.578630  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_25500.solverstate
I1123 16:28:05.582633  5096 solver.cpp:330] Iteration 25500, Testing net (#0)
I1123 16:28:05.582633  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:28:06.650933 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:28:06.691952  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8598
I1123 16:28:06.691952  5096 solver.cpp:397]     Test net output #1: loss = 0.4185 (* 1 = 0.4185 loss)
I1123 16:28:06.728970  5096 solver.cpp:218] Iteration 25500 (20.0563 iter/s, 4.98596s/100 iters), loss = 0.257542
I1123 16:28:06.728970  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:28:06.728970  5096 solver.cpp:237]     Train net output #1: loss = 0.257542 (* 1 = 0.257542 loss)
I1123 16:28:06.728970  5096 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1123 16:28:10.584808  5096 solver.cpp:218] Iteration 25600 (25.9385 iter/s, 3.85527s/100 iters), loss = 0.282314
I1123 16:28:10.584808  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:28:10.584808  5096 solver.cpp:237]     Train net output #1: loss = 0.282314 (* 1 = 0.282314 loss)
I1123 16:28:10.584808  5096 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1123 16:28:14.441184  5096 solver.cpp:218] Iteration 25700 (25.9361 iter/s, 3.85563s/100 iters), loss = 0.252761
I1123 16:28:14.441184  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:28:14.441184  5096 solver.cpp:237]     Train net output #1: loss = 0.252761 (* 1 = 0.252761 loss)
I1123 16:28:14.441184  5096 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1123 16:28:18.301434  5096 solver.cpp:218] Iteration 25800 (25.9081 iter/s, 3.85979s/100 iters), loss = 0.300105
I1123 16:28:18.301434  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:28:18.301434  5096 solver.cpp:237]     Train net output #1: loss = 0.300105 (* 1 = 0.300105 loss)
I1123 16:28:18.301434  5096 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1123 16:28:22.155470  5096 solver.cpp:218] Iteration 25900 (25.9469 iter/s, 3.85402s/100 iters), loss = 0.291153
I1123 16:28:22.155470  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:28:22.155470  5096 solver.cpp:237]     Train net output #1: loss = 0.291153 (* 1 = 0.291153 loss)
I1123 16:28:22.155470  5096 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1123 16:28:25.829123 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:28:25.981174  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_26000.caffemodel
I1123 16:28:25.990162  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_26000.solverstate
I1123 16:28:25.995163  5096 solver.cpp:330] Iteration 26000, Testing net (#0)
I1123 16:28:25.995163  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:28:27.065563 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:28:27.107563  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8602
I1123 16:28:27.107563  5096 solver.cpp:397]     Test net output #1: loss = 0.418474 (* 1 = 0.418474 loss)
I1123 16:28:27.144578  5096 solver.cpp:218] Iteration 26000 (20.0451 iter/s, 4.98875s/100 iters), loss = 0.269919
I1123 16:28:27.144578  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:28:27.144578  5096 solver.cpp:237]     Train net output #1: loss = 0.269919 (* 1 = 0.269919 loss)
I1123 16:28:27.144578  5096 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1123 16:28:31.012547  5096 solver.cpp:218] Iteration 26100 (25.8582 iter/s, 3.86724s/100 iters), loss = 0.329546
I1123 16:28:31.012547  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:28:31.012547  5096 solver.cpp:237]     Train net output #1: loss = 0.329546 (* 1 = 0.329546 loss)
I1123 16:28:31.012547  5096 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1123 16:28:34.881556  5096 solver.cpp:218] Iteration 26200 (25.8459 iter/s, 3.86909s/100 iters), loss = 0.245031
I1123 16:28:34.881556  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:28:34.881556  5096 solver.cpp:237]     Train net output #1: loss = 0.245031 (* 1 = 0.245031 loss)
I1123 16:28:34.881556  5096 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1123 16:28:38.747021  5096 solver.cpp:218] Iteration 26300 (25.8707 iter/s, 3.86537s/100 iters), loss = 0.304974
I1123 16:28:38.747021  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:28:38.747021  5096 solver.cpp:237]     Train net output #1: loss = 0.304973 (* 1 = 0.304973 loss)
I1123 16:28:38.747021  5096 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1123 16:28:42.618418  5096 solver.cpp:218] Iteration 26400 (25.836 iter/s, 3.87057s/100 iters), loss = 0.24542
I1123 16:28:42.618418  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:28:42.618418  5096 solver.cpp:237]     Train net output #1: loss = 0.24542 (* 1 = 0.24542 loss)
I1123 16:28:42.618418  5096 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1123 16:28:46.298619 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:28:46.450117  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_26500.caffemodel
I1123 16:28:46.461648  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_26500.solverstate
I1123 16:28:46.465636  5096 solver.cpp:330] Iteration 26500, Testing net (#0)
I1123 16:28:46.465636  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:28:47.535492 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:28:47.577548  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8599
I1123 16:28:47.577548  5096 solver.cpp:397]     Test net output #1: loss = 0.418526 (* 1 = 0.418526 loss)
I1123 16:28:47.614536  5096 solver.cpp:218] Iteration 26500 (20.0152 iter/s, 4.99621s/100 iters), loss = 0.223194
I1123 16:28:47.614536  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:28:47.614536  5096 solver.cpp:237]     Train net output #1: loss = 0.223194 (* 1 = 0.223194 loss)
I1123 16:28:47.614536  5096 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1123 16:28:51.483839  5096 solver.cpp:218] Iteration 26600 (25.8474 iter/s, 3.86887s/100 iters), loss = 0.300207
I1123 16:28:51.483839  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:28:51.483839  5096 solver.cpp:237]     Train net output #1: loss = 0.300207 (* 1 = 0.300207 loss)
I1123 16:28:51.483839  5096 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1123 16:28:55.353142  5096 solver.cpp:218] Iteration 26700 (25.8514 iter/s, 3.86826s/100 iters), loss = 0.266658
I1123 16:28:55.353142  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:28:55.353142  5096 solver.cpp:237]     Train net output #1: loss = 0.266658 (* 1 = 0.266658 loss)
I1123 16:28:55.353142  5096 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1123 16:28:59.222657  5096 solver.cpp:218] Iteration 26800 (25.8429 iter/s, 3.86954s/100 iters), loss = 0.328934
I1123 16:28:59.222657  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:28:59.222657  5096 solver.cpp:237]     Train net output #1: loss = 0.328934 (* 1 = 0.328934 loss)
I1123 16:28:59.222657  5096 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1123 16:29:03.092171  5096 solver.cpp:218] Iteration 26900 (25.8436 iter/s, 3.86943s/100 iters), loss = 0.288712
I1123 16:29:03.092171  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:29:03.092171  5096 solver.cpp:237]     Train net output #1: loss = 0.288712 (* 1 = 0.288712 loss)
I1123 16:29:03.092171  5096 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1123 16:29:06.771178 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:29:06.923329  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_27000.caffemodel
I1123 16:29:06.933329  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_27000.solverstate
I1123 16:29:06.937330  5096 solver.cpp:330] Iteration 27000, Testing net (#0)
I1123 16:29:06.937330  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:29:08.005426 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:29:08.047427  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8602
I1123 16:29:08.047427  5096 solver.cpp:397]     Test net output #1: loss = 0.418452 (* 1 = 0.418452 loss)
I1123 16:29:08.084044  5096 solver.cpp:218] Iteration 27000 (20.0344 iter/s, 4.99141s/100 iters), loss = 0.232616
I1123 16:29:08.084044  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:29:08.084044  5096 solver.cpp:237]     Train net output #1: loss = 0.232616 (* 1 = 0.232616 loss)
I1123 16:29:08.084044  5096 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1123 16:29:08.084044  5096 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1123 16:29:11.940441  5096 solver.cpp:218] Iteration 27100 (25.9347 iter/s, 3.85584s/100 iters), loss = 0.31384
I1123 16:29:11.940441  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:29:11.940441  5096 solver.cpp:237]     Train net output #1: loss = 0.31384 (* 1 = 0.31384 loss)
I1123 16:29:11.940441  5096 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1123 16:29:15.798506  5096 solver.cpp:218] Iteration 27200 (25.9204 iter/s, 3.85797s/100 iters), loss = 0.326718
I1123 16:29:15.798506  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:29:15.798506  5096 solver.cpp:237]     Train net output #1: loss = 0.326718 (* 1 = 0.326718 loss)
I1123 16:29:15.798506  5096 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1123 16:29:19.661952  5096 solver.cpp:218] Iteration 27300 (25.8861 iter/s, 3.86308s/100 iters), loss = 0.318722
I1123 16:29:19.661952  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:29:19.661952  5096 solver.cpp:237]     Train net output #1: loss = 0.318722 (* 1 = 0.318722 loss)
I1123 16:29:19.661952  5096 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1123 16:29:23.524663  5096 solver.cpp:218] Iteration 27400 (25.8952 iter/s, 3.86172s/100 iters), loss = 0.233843
I1123 16:29:23.524663  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:29:23.524663  5096 solver.cpp:237]     Train net output #1: loss = 0.233843 (* 1 = 0.233843 loss)
I1123 16:29:23.524663  5096 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1123 16:29:27.196290 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:29:27.348341  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_27500.caffemodel
I1123 16:29:27.360342  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_27500.solverstate
I1123 16:29:27.365341  5096 solver.cpp:330] Iteration 27500, Testing net (#0)
I1123 16:29:27.365341  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:29:28.434577 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:29:28.476606  5096 solver.cpp:397]     Test net output #0: accuracy = 0.86
I1123 16:29:28.476606  5096 solver.cpp:397]     Test net output #1: loss = 0.418382 (* 1 = 0.418382 loss)
I1123 16:29:28.513633  5096 solver.cpp:218] Iteration 27500 (20.0462 iter/s, 4.98847s/100 iters), loss = 0.267398
I1123 16:29:28.513633  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:29:28.513633  5096 solver.cpp:237]     Train net output #1: loss = 0.267398 (* 1 = 0.267398 loss)
I1123 16:29:28.513633  5096 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1123 16:29:32.373783  5096 solver.cpp:218] Iteration 27600 (25.9075 iter/s, 3.85988s/100 iters), loss = 0.278811
I1123 16:29:32.373783  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:29:32.373783  5096 solver.cpp:237]     Train net output #1: loss = 0.278811 (* 1 = 0.278811 loss)
I1123 16:29:32.373783  5096 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1123 16:29:36.227336  5096 solver.cpp:218] Iteration 27700 (25.9489 iter/s, 3.85373s/100 iters), loss = 0.275989
I1123 16:29:36.227336  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:29:36.227336  5096 solver.cpp:237]     Train net output #1: loss = 0.275989 (* 1 = 0.275989 loss)
I1123 16:29:36.227336  5096 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1123 16:29:40.088580  5096 solver.cpp:218] Iteration 27800 (25.9033 iter/s, 3.86051s/100 iters), loss = 0.343323
I1123 16:29:40.088580  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:29:40.088580  5096 solver.cpp:237]     Train net output #1: loss = 0.343324 (* 1 = 0.343324 loss)
I1123 16:29:40.088580  5096 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1123 16:29:43.943251  5096 solver.cpp:218] Iteration 27900 (25.9467 iter/s, 3.85406s/100 iters), loss = 0.269007
I1123 16:29:43.943251  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:29:43.943251  5096 solver.cpp:237]     Train net output #1: loss = 0.269007 (* 1 = 0.269007 loss)
I1123 16:29:43.943251  5096 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1123 16:29:47.610122 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:29:47.762235  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_28000.caffemodel
I1123 16:29:47.774245  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_28000.solverstate
I1123 16:29:47.778769  5096 solver.cpp:330] Iteration 28000, Testing net (#0)
I1123 16:29:47.778769  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:29:48.848778 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:29:48.891336  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8599
I1123 16:29:48.891336  5096 solver.cpp:397]     Test net output #1: loss = 0.418519 (* 1 = 0.418519 loss)
I1123 16:29:48.928828  5096 solver.cpp:218] Iteration 28000 (20.0596 iter/s, 4.98514s/100 iters), loss = 0.271833
I1123 16:29:48.928828  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:29:48.928828  5096 solver.cpp:237]     Train net output #1: loss = 0.271833 (* 1 = 0.271833 loss)
I1123 16:29:48.928828  5096 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1123 16:29:52.796977  5096 solver.cpp:218] Iteration 28100 (25.8513 iter/s, 3.86828s/100 iters), loss = 0.22114
I1123 16:29:52.796977  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:29:52.796977  5096 solver.cpp:237]     Train net output #1: loss = 0.22114 (* 1 = 0.22114 loss)
I1123 16:29:52.796977  5096 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1123 16:29:56.661083  5096 solver.cpp:218] Iteration 28200 (25.8799 iter/s, 3.864s/100 iters), loss = 0.246808
I1123 16:29:56.661083  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:29:56.661083  5096 solver.cpp:237]     Train net output #1: loss = 0.246808 (* 1 = 0.246808 loss)
I1123 16:29:56.661083  5096 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1123 16:30:00.534466  5096 solver.cpp:218] Iteration 28300 (25.8249 iter/s, 3.87223s/100 iters), loss = 0.33353
I1123 16:30:00.534466  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:30:00.534466  5096 solver.cpp:237]     Train net output #1: loss = 0.33353 (* 1 = 0.33353 loss)
I1123 16:30:00.534466  5096 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1123 16:30:04.421160  5096 solver.cpp:218] Iteration 28400 (25.727 iter/s, 3.88696s/100 iters), loss = 0.244701
I1123 16:30:04.421160  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 16:30:04.421160  5096 solver.cpp:237]     Train net output #1: loss = 0.244701 (* 1 = 0.244701 loss)
I1123 16:30:04.421160  5096 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1123 16:30:08.097476 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:30:08.249474  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_28500.caffemodel
I1123 16:30:08.259474  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_28500.solverstate
I1123 16:30:08.263473  5096 solver.cpp:330] Iteration 28500, Testing net (#0)
I1123 16:30:08.263473  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:30:09.332895 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:30:09.373909  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8597
I1123 16:30:09.373909  5096 solver.cpp:397]     Test net output #1: loss = 0.418421 (* 1 = 0.418421 loss)
I1123 16:30:09.410930  5096 solver.cpp:218] Iteration 28500 (20.0434 iter/s, 4.98917s/100 iters), loss = 0.263956
I1123 16:30:09.410930  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:30:09.410930  5096 solver.cpp:237]     Train net output #1: loss = 0.263956 (* 1 = 0.263956 loss)
I1123 16:30:09.410930  5096 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1123 16:30:13.272444  5096 solver.cpp:218] Iteration 28600 (25.8978 iter/s, 3.86133s/100 iters), loss = 0.287389
I1123 16:30:13.272444  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:30:13.272444  5096 solver.cpp:237]     Train net output #1: loss = 0.28739 (* 1 = 0.28739 loss)
I1123 16:30:13.272444  5096 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1123 16:30:17.134639  5096 solver.cpp:218] Iteration 28700 (25.8939 iter/s, 3.86191s/100 iters), loss = 0.262801
I1123 16:30:17.134639  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 16:30:17.134639  5096 solver.cpp:237]     Train net output #1: loss = 0.262801 (* 1 = 0.262801 loss)
I1123 16:30:17.134639  5096 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1123 16:30:21.000780  5096 solver.cpp:218] Iteration 28800 (25.8703 iter/s, 3.86544s/100 iters), loss = 0.348222
I1123 16:30:21.000780  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:30:21.000780  5096 solver.cpp:237]     Train net output #1: loss = 0.348222 (* 1 = 0.348222 loss)
I1123 16:30:21.000780  5096 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1123 16:30:24.865382  5096 solver.cpp:218] Iteration 28900 (25.8796 iter/s, 3.86404s/100 iters), loss = 0.25744
I1123 16:30:24.865382  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:30:24.865382  5096 solver.cpp:237]     Train net output #1: loss = 0.25744 (* 1 = 0.25744 loss)
I1123 16:30:24.865382  5096 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1123 16:30:28.535393 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:30:28.687412  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_29000.caffemodel
I1123 16:30:28.698412  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_29000.solverstate
I1123 16:30:28.702430  5096 solver.cpp:330] Iteration 29000, Testing net (#0)
I1123 16:30:28.702430  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:30:29.770612 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:30:29.813135  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8602
I1123 16:30:29.813135  5096 solver.cpp:397]     Test net output #1: loss = 0.418502 (* 1 = 0.418502 loss)
I1123 16:30:29.849663  5096 solver.cpp:218] Iteration 29000 (20.0634 iter/s, 4.98419s/100 iters), loss = 0.258582
I1123 16:30:29.849663  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 16:30:29.849663  5096 solver.cpp:237]     Train net output #1: loss = 0.258583 (* 1 = 0.258583 loss)
I1123 16:30:29.849663  5096 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1123 16:30:33.710783  5096 solver.cpp:218] Iteration 29100 (25.9018 iter/s, 3.86073s/100 iters), loss = 0.275334
I1123 16:30:33.710783  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:30:33.710783  5096 solver.cpp:237]     Train net output #1: loss = 0.275334 (* 1 = 0.275334 loss)
I1123 16:30:33.711282  5096 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1123 16:30:37.575773  5096 solver.cpp:218] Iteration 29200 (25.8789 iter/s, 3.86416s/100 iters), loss = 0.221172
I1123 16:30:37.575773  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 16:30:37.575773  5096 solver.cpp:237]     Train net output #1: loss = 0.221172 (* 1 = 0.221172 loss)
I1123 16:30:37.575773  5096 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1123 16:30:41.436802  5096 solver.cpp:218] Iteration 29300 (25.896 iter/s, 3.8616s/100 iters), loss = 0.304286
I1123 16:30:41.437808  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:30:41.437808  5096 solver.cpp:237]     Train net output #1: loss = 0.304286 (* 1 = 0.304286 loss)
I1123 16:30:41.437808  5096 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1123 16:30:45.299451  5096 solver.cpp:218] Iteration 29400 (25.8926 iter/s, 3.86211s/100 iters), loss = 0.228188
I1123 16:30:45.299451  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:30:45.299451  5096 solver.cpp:237]     Train net output #1: loss = 0.228188 (* 1 = 0.228188 loss)
I1123 16:30:45.299451  5096 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1123 16:30:48.971381 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:30:49.123582  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_29500.caffemodel
I1123 16:30:49.134279  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_29500.solverstate
I1123 16:30:49.139281  5096 solver.cpp:330] Iteration 29500, Testing net (#0)
I1123 16:30:49.139281  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:30:50.208148 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:30:50.250159  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8599
I1123 16:30:50.250159  5096 solver.cpp:397]     Test net output #1: loss = 0.418409 (* 1 = 0.418409 loss)
I1123 16:30:50.287163  5096 solver.cpp:218] Iteration 29500 (20.0523 iter/s, 4.98696s/100 iters), loss = 0.231279
I1123 16:30:50.287163  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 16:30:50.287163  5096 solver.cpp:237]     Train net output #1: loss = 0.231279 (* 1 = 0.231279 loss)
I1123 16:30:50.287163  5096 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1123 16:30:54.151927  5096 solver.cpp:218] Iteration 29600 (25.8793 iter/s, 3.86409s/100 iters), loss = 0.284553
I1123 16:30:54.151927  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 16:30:54.151927  5096 solver.cpp:237]     Train net output #1: loss = 0.284553 (* 1 = 0.284553 loss)
I1123 16:30:54.151927  5096 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1123 16:30:58.010076  5096 solver.cpp:218] Iteration 29700 (25.9184 iter/s, 3.85826s/100 iters), loss = 0.270408
I1123 16:30:58.010076  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 16:30:58.010076  5096 solver.cpp:237]     Train net output #1: loss = 0.270408 (* 1 = 0.270408 loss)
I1123 16:30:58.010076  5096 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1123 16:31:01.867915  5096 solver.cpp:218] Iteration 29800 (25.9228 iter/s, 3.85761s/100 iters), loss = 0.311813
I1123 16:31:01.867915  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:31:01.867915  5096 solver.cpp:237]     Train net output #1: loss = 0.311813 (* 1 = 0.311813 loss)
I1123 16:31:01.867915  5096 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1123 16:31:05.721261  5096 solver.cpp:218] Iteration 29900 (25.9594 iter/s, 3.85217s/100 iters), loss = 0.224165
I1123 16:31:05.721261  5096 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 16:31:05.721261  5096 solver.cpp:237]     Train net output #1: loss = 0.224165 (* 1 = 0.224165 loss)
I1123 16:31:05.721261  5096 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1123 16:31:09.392283 27880 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:31:09.544353  5096 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_30000.caffemodel
I1123 16:31:09.553334  5096 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_30000.solverstate
I1123 16:31:09.569347  5096 solver.cpp:310] Iteration 30000, loss = 0.252171
I1123 16:31:09.569347  5096 solver.cpp:330] Iteration 30000, Testing net (#0)
I1123 16:31:09.570348  5096 net.cpp:676] Ignoring source layer accuracy_training
I1123 16:31:10.640138 30692 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:31:10.681637  5096 solver.cpp:397]     Test net output #0: accuracy = 0.8607
I1123 16:31:10.681637  5096 solver.cpp:397]     Test net output #1: loss = 0.418355 (* 1 = 0.418355 loss)
I1123 16:31:10.681637  5096 solver.cpp:315] Optimization Done.
I1123 16:31:10.681637  5096 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 

G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1123 15:19:17.593518 22196 caffe.cpp:219] Using GPUs 0
I1123 15:19:17.767658 22196 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1123 15:19:18.065373 22196 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 15:19:18.082373 22196 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_300k_8L_7x7"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1123 15:19:18.082373 22196 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 15:19:18.083374 22196 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 15:19:18.083374 22196 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 15:19:18.083374 22196 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1123 15:19:18.083374 22196 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1123 15:19:18.083374 22196 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1123 15:19:18.083374 22196 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1123 15:19:18.083374 22196 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1123 15:19:18.083374 22196 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1123 15:19:18.083374 22196 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1123 15:19:18.083374 22196 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1123 15:19:18.083374 22196 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1123 15:19:18.083374 22196 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1123 15:19:18.083374 22196 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_7x7_300K"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 35
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 38
    pad: 3
    kernel_size: 7
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 15:19:18.145931 22196 layer_factory.cpp:58] Creating layer cifar
I1123 15:19:18.153920 22196 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1123 15:19:18.153920 22196 net.cpp:84] Creating Layer cifar
I1123 15:19:18.153920 22196 net.cpp:380] cifar -> data
I1123 15:19:18.153920 22196 net.cpp:380] cifar -> label
I1123 15:19:18.154918 22196 data_layer.cpp:45] output data size: 100,3,32,32
I1123 15:19:18.159947 22196 net.cpp:122] Setting up cifar
I1123 15:19:18.159947 22196 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 15:19:18.159947 22196 net.cpp:129] Top shape: 100 (100)
I1123 15:19:18.159947 22196 net.cpp:137] Memory required for data: 1229200
I1123 15:19:18.159947 22196 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 15:19:18.159947 22196 net.cpp:84] Creating Layer label_cifar_1_split
I1123 15:19:18.159947 22196 net.cpp:406] label_cifar_1_split <- label
I1123 15:19:18.159947 22196 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 15:19:18.159947 22196 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 15:19:18.160446 22196 net.cpp:122] Setting up label_cifar_1_split
I1123 15:19:18.160446 22196 net.cpp:129] Top shape: 100 (100)
I1123 15:19:18.160446 22196 net.cpp:129] Top shape: 100 (100)
I1123 15:19:18.160446 22196 net.cpp:137] Memory required for data: 1230000
I1123 15:19:18.160446 22196 layer_factory.cpp:58] Creating layer conv1
I1123 15:19:18.160446 22196 net.cpp:84] Creating Layer conv1
I1123 15:19:18.160446 22196 net.cpp:406] conv1 <- data
I1123 15:19:18.160446 22196 net.cpp:380] conv1 -> conv1
I1123 15:19:18.160946 20256 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 15:19:18.410270 22196 net.cpp:122] Setting up conv1
I1123 15:19:18.410270 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.410270 22196 net.cpp:137] Memory required for data: 9422000
I1123 15:19:18.410270 22196 layer_factory.cpp:58] Creating layer bn1
I1123 15:19:18.410270 22196 net.cpp:84] Creating Layer bn1
I1123 15:19:18.410270 22196 net.cpp:406] bn1 <- conv1
I1123 15:19:18.410270 22196 net.cpp:367] bn1 -> conv1 (in-place)
I1123 15:19:18.410270 22196 net.cpp:122] Setting up bn1
I1123 15:19:18.410270 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.410270 22196 net.cpp:137] Memory required for data: 17614000
I1123 15:19:18.410270 22196 layer_factory.cpp:58] Creating layer scale1
I1123 15:19:18.410270 22196 net.cpp:84] Creating Layer scale1
I1123 15:19:18.410270 22196 net.cpp:406] scale1 <- conv1
I1123 15:19:18.410270 22196 net.cpp:367] scale1 -> conv1 (in-place)
I1123 15:19:18.410270 22196 layer_factory.cpp:58] Creating layer scale1
I1123 15:19:18.410270 22196 net.cpp:122] Setting up scale1
I1123 15:19:18.410270 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.410270 22196 net.cpp:137] Memory required for data: 25806000
I1123 15:19:18.410270 22196 layer_factory.cpp:58] Creating layer relu1
I1123 15:19:18.410270 22196 net.cpp:84] Creating Layer relu1
I1123 15:19:18.410270 22196 net.cpp:406] relu1 <- conv1
I1123 15:19:18.410270 22196 net.cpp:367] relu1 -> conv1 (in-place)
I1123 15:19:18.410270 22196 net.cpp:122] Setting up relu1
I1123 15:19:18.410270 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.410270 22196 net.cpp:137] Memory required for data: 33998000
I1123 15:19:18.410270 22196 layer_factory.cpp:58] Creating layer conv2
I1123 15:19:18.410270 22196 net.cpp:84] Creating Layer conv2
I1123 15:19:18.410270 22196 net.cpp:406] conv2 <- conv1
I1123 15:19:18.410270 22196 net.cpp:380] conv2 -> conv2
I1123 15:19:18.412273 22196 net.cpp:122] Setting up conv2
I1123 15:19:18.412273 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.412273 22196 net.cpp:137] Memory required for data: 42190000
I1123 15:19:18.412273 22196 layer_factory.cpp:58] Creating layer bn2
I1123 15:19:18.412273 22196 net.cpp:84] Creating Layer bn2
I1123 15:19:18.412273 22196 net.cpp:406] bn2 <- conv2
I1123 15:19:18.412273 22196 net.cpp:367] bn2 -> conv2 (in-place)
I1123 15:19:18.412273 22196 net.cpp:122] Setting up bn2
I1123 15:19:18.412273 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.412273 22196 net.cpp:137] Memory required for data: 50382000
I1123 15:19:18.412273 22196 layer_factory.cpp:58] Creating layer scale2
I1123 15:19:18.412273 22196 net.cpp:84] Creating Layer scale2
I1123 15:19:18.412273 22196 net.cpp:406] scale2 <- conv2
I1123 15:19:18.412273 22196 net.cpp:367] scale2 -> conv2 (in-place)
I1123 15:19:18.412273 22196 layer_factory.cpp:58] Creating layer scale2
I1123 15:19:18.412273 22196 net.cpp:122] Setting up scale2
I1123 15:19:18.412273 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.412273 22196 net.cpp:137] Memory required for data: 58574000
I1123 15:19:18.412273 22196 layer_factory.cpp:58] Creating layer relu2
I1123 15:19:18.412273 22196 net.cpp:84] Creating Layer relu2
I1123 15:19:18.412273 22196 net.cpp:406] relu2 <- conv2
I1123 15:19:18.412273 22196 net.cpp:367] relu2 -> conv2 (in-place)
I1123 15:19:18.413275 22196 net.cpp:122] Setting up relu2
I1123 15:19:18.413275 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.413275 22196 net.cpp:137] Memory required for data: 66766000
I1123 15:19:18.413275 22196 layer_factory.cpp:58] Creating layer conv2_2
I1123 15:19:18.413275 22196 net.cpp:84] Creating Layer conv2_2
I1123 15:19:18.413275 22196 net.cpp:406] conv2_2 <- conv2
I1123 15:19:18.413275 22196 net.cpp:380] conv2_2 -> conv2_2
I1123 15:19:18.414270 22196 net.cpp:122] Setting up conv2_2
I1123 15:19:18.414270 22196 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 15:19:18.414270 22196 net.cpp:137] Memory required for data: 79054000
I1123 15:19:18.414270 22196 layer_factory.cpp:58] Creating layer bn2_2
I1123 15:19:18.414270 22196 net.cpp:84] Creating Layer bn2_2
I1123 15:19:18.414270 22196 net.cpp:406] bn2_2 <- conv2_2
I1123 15:19:18.414270 22196 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 15:19:18.414270 22196 net.cpp:122] Setting up bn2_2
I1123 15:19:18.414270 22196 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 15:19:18.414270 22196 net.cpp:137] Memory required for data: 91342000
I1123 15:19:18.414270 22196 layer_factory.cpp:58] Creating layer scale2_2
I1123 15:19:18.414270 22196 net.cpp:84] Creating Layer scale2_2
I1123 15:19:18.414270 22196 net.cpp:406] scale2_2 <- conv2_2
I1123 15:19:18.414270 22196 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 15:19:18.414270 22196 layer_factory.cpp:58] Creating layer scale2_2
I1123 15:19:18.414270 22196 net.cpp:122] Setting up scale2_2
I1123 15:19:18.414270 22196 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 15:19:18.414270 22196 net.cpp:137] Memory required for data: 103630000
I1123 15:19:18.414270 22196 layer_factory.cpp:58] Creating layer relu2_2
I1123 15:19:18.414270 22196 net.cpp:84] Creating Layer relu2_2
I1123 15:19:18.414270 22196 net.cpp:406] relu2_2 <- conv2_2
I1123 15:19:18.414270 22196 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 15:19:18.414270 22196 net.cpp:122] Setting up relu2_2
I1123 15:19:18.414270 22196 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 15:19:18.414270 22196 net.cpp:137] Memory required for data: 115918000
I1123 15:19:18.414270 22196 layer_factory.cpp:58] Creating layer pool2_1
I1123 15:19:18.414270 22196 net.cpp:84] Creating Layer pool2_1
I1123 15:19:18.414270 22196 net.cpp:406] pool2_1 <- conv2_2
I1123 15:19:18.414270 22196 net.cpp:380] pool2_1 -> pool2_1
I1123 15:19:18.415271 22196 net.cpp:122] Setting up pool2_1
I1123 15:19:18.415271 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.415271 22196 net.cpp:137] Memory required for data: 118990000
I1123 15:19:18.415271 22196 layer_factory.cpp:58] Creating layer conv3
I1123 15:19:18.415271 22196 net.cpp:84] Creating Layer conv3
I1123 15:19:18.415271 22196 net.cpp:406] conv3 <- pool2_1
I1123 15:19:18.415271 22196 net.cpp:380] conv3 -> conv3
I1123 15:19:18.416270 22196 net.cpp:122] Setting up conv3
I1123 15:19:18.416270 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.416270 22196 net.cpp:137] Memory required for data: 122062000
I1123 15:19:18.416270 22196 layer_factory.cpp:58] Creating layer bn3
I1123 15:19:18.416270 22196 net.cpp:84] Creating Layer bn3
I1123 15:19:18.416270 22196 net.cpp:406] bn3 <- conv3
I1123 15:19:18.416270 22196 net.cpp:367] bn3 -> conv3 (in-place)
I1123 15:19:18.416270 22196 net.cpp:122] Setting up bn3
I1123 15:19:18.416270 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.416270 22196 net.cpp:137] Memory required for data: 125134000
I1123 15:19:18.416270 22196 layer_factory.cpp:58] Creating layer scale3
I1123 15:19:18.416270 22196 net.cpp:84] Creating Layer scale3
I1123 15:19:18.416270 22196 net.cpp:406] scale3 <- conv3
I1123 15:19:18.416270 22196 net.cpp:367] scale3 -> conv3 (in-place)
I1123 15:19:18.416270 22196 layer_factory.cpp:58] Creating layer scale3
I1123 15:19:18.416270 22196 net.cpp:122] Setting up scale3
I1123 15:19:18.416270 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.416270 22196 net.cpp:137] Memory required for data: 128206000
I1123 15:19:18.416270 22196 layer_factory.cpp:58] Creating layer relu3
I1123 15:19:18.416270 22196 net.cpp:84] Creating Layer relu3
I1123 15:19:18.416270 22196 net.cpp:406] relu3 <- conv3
I1123 15:19:18.416270 22196 net.cpp:367] relu3 -> conv3 (in-place)
I1123 15:19:18.416270 22196 net.cpp:122] Setting up relu3
I1123 15:19:18.416270 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.416270 22196 net.cpp:137] Memory required for data: 131278000
I1123 15:19:18.416270 22196 layer_factory.cpp:58] Creating layer conv4
I1123 15:19:18.416270 22196 net.cpp:84] Creating Layer conv4
I1123 15:19:18.416270 22196 net.cpp:406] conv4 <- conv3
I1123 15:19:18.416270 22196 net.cpp:380] conv4 -> conv4
I1123 15:19:18.418270 22196 net.cpp:122] Setting up conv4
I1123 15:19:18.418270 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.418270 22196 net.cpp:137] Memory required for data: 134350000
I1123 15:19:18.418270 22196 layer_factory.cpp:58] Creating layer bn4
I1123 15:19:18.418270 22196 net.cpp:84] Creating Layer bn4
I1123 15:19:18.418270 22196 net.cpp:406] bn4 <- conv4
I1123 15:19:18.418270 22196 net.cpp:367] bn4 -> conv4 (in-place)
I1123 15:19:18.418270 22196 net.cpp:122] Setting up bn4
I1123 15:19:18.418270 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.418270 22196 net.cpp:137] Memory required for data: 137422000
I1123 15:19:18.418270 22196 layer_factory.cpp:58] Creating layer scale4
I1123 15:19:18.418270 22196 net.cpp:84] Creating Layer scale4
I1123 15:19:18.418270 22196 net.cpp:406] scale4 <- conv4
I1123 15:19:18.418270 22196 net.cpp:367] scale4 -> conv4 (in-place)
I1123 15:19:18.418270 22196 layer_factory.cpp:58] Creating layer scale4
I1123 15:19:18.418270 22196 net.cpp:122] Setting up scale4
I1123 15:19:18.418270 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.418270 22196 net.cpp:137] Memory required for data: 140494000
I1123 15:19:18.418270 22196 layer_factory.cpp:58] Creating layer relu4
I1123 15:19:18.418270 22196 net.cpp:84] Creating Layer relu4
I1123 15:19:18.418270 22196 net.cpp:406] relu4 <- conv4
I1123 15:19:18.418270 22196 net.cpp:367] relu4 -> conv4 (in-place)
I1123 15:19:18.419270 22196 net.cpp:122] Setting up relu4
I1123 15:19:18.419270 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.419270 22196 net.cpp:137] Memory required for data: 143566000
I1123 15:19:18.419270 22196 layer_factory.cpp:58] Creating layer conv4_1
I1123 15:19:18.419270 22196 net.cpp:84] Creating Layer conv4_1
I1123 15:19:18.419270 22196 net.cpp:406] conv4_1 <- conv4
I1123 15:19:18.419270 22196 net.cpp:380] conv4_1 -> conv4_1
I1123 15:19:18.420269 22196 net.cpp:122] Setting up conv4_1
I1123 15:19:18.420269 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.420269 22196 net.cpp:137] Memory required for data: 146638000
I1123 15:19:18.420269 22196 layer_factory.cpp:58] Creating layer bn4_1
I1123 15:19:18.420269 22196 net.cpp:84] Creating Layer bn4_1
I1123 15:19:18.420269 22196 net.cpp:406] bn4_1 <- conv4_1
I1123 15:19:18.420269 22196 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 15:19:18.420269 22196 net.cpp:122] Setting up bn4_1
I1123 15:19:18.420269 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.420269 22196 net.cpp:137] Memory required for data: 149710000
I1123 15:19:18.420269 22196 layer_factory.cpp:58] Creating layer scale4_1
I1123 15:19:18.420269 22196 net.cpp:84] Creating Layer scale4_1
I1123 15:19:18.420269 22196 net.cpp:406] scale4_1 <- conv4_1
I1123 15:19:18.420269 22196 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 15:19:18.420269 22196 layer_factory.cpp:58] Creating layer scale4_1
I1123 15:19:18.420269 22196 net.cpp:122] Setting up scale4_1
I1123 15:19:18.420269 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.420269 22196 net.cpp:137] Memory required for data: 152782000
I1123 15:19:18.420269 22196 layer_factory.cpp:58] Creating layer relu4_1
I1123 15:19:18.420269 22196 net.cpp:84] Creating Layer relu4_1
I1123 15:19:18.420269 22196 net.cpp:406] relu4_1 <- conv4_1
I1123 15:19:18.420269 22196 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 15:19:18.421272 22196 net.cpp:122] Setting up relu4_1
I1123 15:19:18.421272 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.421272 22196 net.cpp:137] Memory required for data: 155854000
I1123 15:19:18.421272 22196 layer_factory.cpp:58] Creating layer conv4_2
I1123 15:19:18.421272 22196 net.cpp:84] Creating Layer conv4_2
I1123 15:19:18.421272 22196 net.cpp:406] conv4_2 <- conv4_1
I1123 15:19:18.421272 22196 net.cpp:380] conv4_2 -> conv4_2
I1123 15:19:18.422271 22196 net.cpp:122] Setting up conv4_2
I1123 15:19:18.422271 22196 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 15:19:18.422271 22196 net.cpp:137] Memory required for data: 159438000
I1123 15:19:18.422271 22196 layer_factory.cpp:58] Creating layer bn4_2
I1123 15:19:18.422271 22196 net.cpp:84] Creating Layer bn4_2
I1123 15:19:18.422271 22196 net.cpp:406] bn4_2 <- conv4_2
I1123 15:19:18.422271 22196 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 15:19:18.422271 22196 net.cpp:122] Setting up bn4_2
I1123 15:19:18.422271 22196 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 15:19:18.422271 22196 net.cpp:137] Memory required for data: 163022000
I1123 15:19:18.422271 22196 layer_factory.cpp:58] Creating layer scale4_2
I1123 15:19:18.422271 22196 net.cpp:84] Creating Layer scale4_2
I1123 15:19:18.422271 22196 net.cpp:406] scale4_2 <- conv4_2
I1123 15:19:18.422271 22196 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 15:19:18.422271 22196 layer_factory.cpp:58] Creating layer scale4_2
I1123 15:19:18.422271 22196 net.cpp:122] Setting up scale4_2
I1123 15:19:18.422271 22196 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 15:19:18.422271 22196 net.cpp:137] Memory required for data: 166606000
I1123 15:19:18.422271 22196 layer_factory.cpp:58] Creating layer relu4_2
I1123 15:19:18.422271 22196 net.cpp:84] Creating Layer relu4_2
I1123 15:19:18.422271 22196 net.cpp:406] relu4_2 <- conv4_2
I1123 15:19:18.422271 22196 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 15:19:18.423270 22196 net.cpp:122] Setting up relu4_2
I1123 15:19:18.423270 22196 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 15:19:18.423270 22196 net.cpp:137] Memory required for data: 170190000
I1123 15:19:18.423270 22196 layer_factory.cpp:58] Creating layer pool4_2
I1123 15:19:18.423270 22196 net.cpp:84] Creating Layer pool4_2
I1123 15:19:18.423270 22196 net.cpp:406] pool4_2 <- conv4_2
I1123 15:19:18.423270 22196 net.cpp:380] pool4_2 -> pool4_2
I1123 15:19:18.423270 22196 net.cpp:122] Setting up pool4_2
I1123 15:19:18.423270 22196 net.cpp:129] Top shape: 100 35 8 8 (224000)
I1123 15:19:18.423270 22196 net.cpp:137] Memory required for data: 171086000
I1123 15:19:18.423270 22196 layer_factory.cpp:58] Creating layer conv12
I1123 15:19:18.423270 22196 net.cpp:84] Creating Layer conv12
I1123 15:19:18.423270 22196 net.cpp:406] conv12 <- pool4_2
I1123 15:19:18.423270 22196 net.cpp:380] conv12 -> conv12
I1123 15:19:18.424269 22196 net.cpp:122] Setting up conv12
I1123 15:19:18.424269 22196 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 15:19:18.424269 22196 net.cpp:137] Memory required for data: 172058800
I1123 15:19:18.424269 22196 layer_factory.cpp:58] Creating layer bn_conv12
I1123 15:19:18.424269 22196 net.cpp:84] Creating Layer bn_conv12
I1123 15:19:18.424269 22196 net.cpp:406] bn_conv12 <- conv12
I1123 15:19:18.424269 22196 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 15:19:18.425271 22196 net.cpp:122] Setting up bn_conv12
I1123 15:19:18.425271 22196 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 15:19:18.425271 22196 net.cpp:137] Memory required for data: 173031600
I1123 15:19:18.425271 22196 layer_factory.cpp:58] Creating layer scale_conv12
I1123 15:19:18.425271 22196 net.cpp:84] Creating Layer scale_conv12
I1123 15:19:18.425271 22196 net.cpp:406] scale_conv12 <- conv12
I1123 15:19:18.425271 22196 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 15:19:18.425271 22196 layer_factory.cpp:58] Creating layer scale_conv12
I1123 15:19:18.425271 22196 net.cpp:122] Setting up scale_conv12
I1123 15:19:18.425271 22196 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 15:19:18.425271 22196 net.cpp:137] Memory required for data: 174004400
I1123 15:19:18.425271 22196 layer_factory.cpp:58] Creating layer relu_conv12
I1123 15:19:18.425271 22196 net.cpp:84] Creating Layer relu_conv12
I1123 15:19:18.425271 22196 net.cpp:406] relu_conv12 <- conv12
I1123 15:19:18.425271 22196 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 15:19:18.425271 22196 net.cpp:122] Setting up relu_conv12
I1123 15:19:18.425271 22196 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 15:19:18.425271 22196 net.cpp:137] Memory required for data: 174977200
I1123 15:19:18.425271 22196 layer_factory.cpp:58] Creating layer poolcp6
I1123 15:19:18.425271 22196 net.cpp:84] Creating Layer poolcp6
I1123 15:19:18.425271 22196 net.cpp:406] poolcp6 <- conv12
I1123 15:19:18.425271 22196 net.cpp:380] poolcp6 -> poolcp6
I1123 15:19:18.425271 22196 net.cpp:122] Setting up poolcp6
I1123 15:19:18.425271 22196 net.cpp:129] Top shape: 100 38 1 1 (3800)
I1123 15:19:18.425271 22196 net.cpp:137] Memory required for data: 174992400
I1123 15:19:18.425271 22196 layer_factory.cpp:58] Creating layer ip1
I1123 15:19:18.425271 22196 net.cpp:84] Creating Layer ip1
I1123 15:19:18.425271 22196 net.cpp:406] ip1 <- poolcp6
I1123 15:19:18.425271 22196 net.cpp:380] ip1 -> ip1
I1123 15:19:18.426270 22196 net.cpp:122] Setting up ip1
I1123 15:19:18.426270 22196 net.cpp:129] Top shape: 100 10 (1000)
I1123 15:19:18.426270 22196 net.cpp:137] Memory required for data: 174996400
I1123 15:19:18.426270 22196 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 15:19:18.426270 22196 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 15:19:18.426270 22196 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 15:19:18.426270 22196 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 15:19:18.426270 22196 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 15:19:18.426270 22196 net.cpp:122] Setting up ip1_ip1_0_split
I1123 15:19:18.426270 22196 net.cpp:129] Top shape: 100 10 (1000)
I1123 15:19:18.426270 22196 net.cpp:129] Top shape: 100 10 (1000)
I1123 15:19:18.426270 22196 net.cpp:137] Memory required for data: 175004400
I1123 15:19:18.426270 22196 layer_factory.cpp:58] Creating layer accuracy_training
I1123 15:19:18.426270 22196 net.cpp:84] Creating Layer accuracy_training
I1123 15:19:18.426270 22196 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1123 15:19:18.426270 22196 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1123 15:19:18.426270 22196 net.cpp:380] accuracy_training -> accuracy_training
I1123 15:19:18.426270 22196 net.cpp:122] Setting up accuracy_training
I1123 15:19:18.426270 22196 net.cpp:129] Top shape: (1)
I1123 15:19:18.426270 22196 net.cpp:137] Memory required for data: 175004404
I1123 15:19:18.426270 22196 layer_factory.cpp:58] Creating layer loss
I1123 15:19:18.426270 22196 net.cpp:84] Creating Layer loss
I1123 15:19:18.426270 22196 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 15:19:18.426270 22196 net.cpp:406] loss <- label_cifar_1_split_1
I1123 15:19:18.426270 22196 net.cpp:380] loss -> loss
I1123 15:19:18.426270 22196 layer_factory.cpp:58] Creating layer loss
I1123 15:19:18.426270 22196 net.cpp:122] Setting up loss
I1123 15:19:18.426270 22196 net.cpp:129] Top shape: (1)
I1123 15:19:18.426270 22196 net.cpp:132]     with loss weight 1
I1123 15:19:18.426270 22196 net.cpp:137] Memory required for data: 175004408
I1123 15:19:18.426270 22196 net.cpp:198] loss needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:200] accuracy_training does not need backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] ip1 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] poolcp6 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] relu_conv12 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] scale_conv12 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] bn_conv12 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] conv12 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] pool4_2 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] relu4_2 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] scale4_2 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] bn4_2 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] conv4_2 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] relu4_1 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] scale4_1 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] bn4_1 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] conv4_1 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] relu4 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] scale4 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] bn4 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] conv4 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] relu3 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] scale3 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] bn3 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] conv3 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] pool2_1 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] relu2_2 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] scale2_2 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] bn2_2 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] conv2_2 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] relu2 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] scale2 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] bn2 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] conv2 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] relu1 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] scale1 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] bn1 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:198] conv1 needs backward computation.
I1123 15:19:18.426270 22196 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 15:19:18.426270 22196 net.cpp:200] cifar does not need backward computation.
I1123 15:19:18.426270 22196 net.cpp:242] This network produces output accuracy_training
I1123 15:19:18.426270 22196 net.cpp:242] This network produces output loss
I1123 15:19:18.426270 22196 net.cpp:255] Network initialization done.
I1123 15:19:18.427269 22196 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 15:19:18.427269 22196 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 15:19:18.427269 22196 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 15:19:18.427269 22196 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1123 15:19:18.427269 22196 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1123 15:19:18.427269 22196 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1123 15:19:18.427269 22196 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1123 15:19:18.427269 22196 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1123 15:19:18.427269 22196 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1123 15:19:18.427269 22196 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1123 15:19:18.427269 22196 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1123 15:19:18.427269 22196 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1123 15:19:18.427269 22196 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1123 15:19:18.427269 22196 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_7x7_300K"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 35
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 38
    pad: 3
    kernel_size: 7
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 15:19:18.427269 22196 layer_factory.cpp:58] Creating layer cifar
I1123 15:19:18.433290 22196 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1123 15:19:18.433290 22196 net.cpp:84] Creating Layer cifar
I1123 15:19:18.433290 22196 net.cpp:380] cifar -> data
I1123 15:19:18.433290 22196 net.cpp:380] cifar -> label
I1123 15:19:18.433290 22196 data_layer.cpp:45] output data size: 100,3,32,32
I1123 15:19:18.439270 22196 net.cpp:122] Setting up cifar
I1123 15:19:18.439270 22196 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 15:19:18.439270 22196 net.cpp:129] Top shape: 100 (100)
I1123 15:19:18.439270 22196 net.cpp:137] Memory required for data: 1229200
I1123 15:19:18.439270 22196 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 15:19:18.439270 22196 net.cpp:84] Creating Layer label_cifar_1_split
I1123 15:19:18.439270 22196 net.cpp:406] label_cifar_1_split <- label
I1123 15:19:18.439270 22196 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 15:19:18.439270 22196 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 15:19:18.439270 22196 net.cpp:122] Setting up label_cifar_1_split
I1123 15:19:18.440269 22196 net.cpp:129] Top shape: 100 (100)
I1123 15:19:18.440269 22196 net.cpp:129] Top shape: 100 (100)
I1123 15:19:18.440269 22196 net.cpp:137] Memory required for data: 1230000
I1123 15:19:18.440269 22196 layer_factory.cpp:58] Creating layer conv1
I1123 15:19:18.440269 22196 net.cpp:84] Creating Layer conv1
I1123 15:19:18.440269 22196 net.cpp:406] conv1 <- data
I1123 15:19:18.440269 22196 net.cpp:380] conv1 -> conv1
I1123 15:19:18.442270 22196 net.cpp:122] Setting up conv1
I1123 15:19:18.442270 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.442270 22196 net.cpp:137] Memory required for data: 9422000
I1123 15:19:18.442270 22196 layer_factory.cpp:58] Creating layer bn1
I1123 15:19:18.442270 22196 net.cpp:84] Creating Layer bn1
I1123 15:19:18.442270 22196 net.cpp:406] bn1 <- conv1
I1123 15:19:18.442270 22196 net.cpp:367] bn1 -> conv1 (in-place)
I1123 15:19:18.442270 22196 net.cpp:122] Setting up bn1
I1123 15:19:18.443272 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.443272 22196 net.cpp:137] Memory required for data: 17614000
I1123 15:19:18.443272 22196 layer_factory.cpp:58] Creating layer scale1
I1123 15:19:18.443272 22196 net.cpp:84] Creating Layer scale1
I1123 15:19:18.443272 22196 net.cpp:406] scale1 <- conv1
I1123 15:19:18.443272 22196 net.cpp:367] scale1 -> conv1 (in-place)
I1123 15:19:18.443272 16188 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 15:19:18.443272 22196 layer_factory.cpp:58] Creating layer scale1
I1123 15:19:18.443272 22196 net.cpp:122] Setting up scale1
I1123 15:19:18.443272 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.443272 22196 net.cpp:137] Memory required for data: 25806000
I1123 15:19:18.443272 22196 layer_factory.cpp:58] Creating layer relu1
I1123 15:19:18.443272 22196 net.cpp:84] Creating Layer relu1
I1123 15:19:18.443272 22196 net.cpp:406] relu1 <- conv1
I1123 15:19:18.443272 22196 net.cpp:367] relu1 -> conv1 (in-place)
I1123 15:19:18.444272 22196 net.cpp:122] Setting up relu1
I1123 15:19:18.444272 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.444272 22196 net.cpp:137] Memory required for data: 33998000
I1123 15:19:18.444272 22196 layer_factory.cpp:58] Creating layer conv2
I1123 15:19:18.444272 22196 net.cpp:84] Creating Layer conv2
I1123 15:19:18.444272 22196 net.cpp:406] conv2 <- conv1
I1123 15:19:18.444272 22196 net.cpp:380] conv2 -> conv2
I1123 15:19:18.445271 22196 net.cpp:122] Setting up conv2
I1123 15:19:18.445271 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.445271 22196 net.cpp:137] Memory required for data: 42190000
I1123 15:19:18.445271 22196 layer_factory.cpp:58] Creating layer bn2
I1123 15:19:18.445271 22196 net.cpp:84] Creating Layer bn2
I1123 15:19:18.445271 22196 net.cpp:406] bn2 <- conv2
I1123 15:19:18.445271 22196 net.cpp:367] bn2 -> conv2 (in-place)
I1123 15:19:18.445271 22196 net.cpp:122] Setting up bn2
I1123 15:19:18.445271 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.445271 22196 net.cpp:137] Memory required for data: 50382000
I1123 15:19:18.445271 22196 layer_factory.cpp:58] Creating layer scale2
I1123 15:19:18.445271 22196 net.cpp:84] Creating Layer scale2
I1123 15:19:18.445271 22196 net.cpp:406] scale2 <- conv2
I1123 15:19:18.445271 22196 net.cpp:367] scale2 -> conv2 (in-place)
I1123 15:19:18.445271 22196 layer_factory.cpp:58] Creating layer scale2
I1123 15:19:18.445777 22196 net.cpp:122] Setting up scale2
I1123 15:19:18.445777 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.445777 22196 net.cpp:137] Memory required for data: 58574000
I1123 15:19:18.445777 22196 layer_factory.cpp:58] Creating layer relu2
I1123 15:19:18.445777 22196 net.cpp:84] Creating Layer relu2
I1123 15:19:18.445777 22196 net.cpp:406] relu2 <- conv2
I1123 15:19:18.445777 22196 net.cpp:367] relu2 -> conv2 (in-place)
I1123 15:19:18.446277 22196 net.cpp:122] Setting up relu2
I1123 15:19:18.446277 22196 net.cpp:129] Top shape: 100 20 32 32 (2048000)
I1123 15:19:18.446277 22196 net.cpp:137] Memory required for data: 66766000
I1123 15:19:18.446277 22196 layer_factory.cpp:58] Creating layer conv2_2
I1123 15:19:18.446277 22196 net.cpp:84] Creating Layer conv2_2
I1123 15:19:18.446277 22196 net.cpp:406] conv2_2 <- conv2
I1123 15:19:18.446277 22196 net.cpp:380] conv2_2 -> conv2_2
I1123 15:19:18.447777 22196 net.cpp:122] Setting up conv2_2
I1123 15:19:18.447777 22196 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 15:19:18.447777 22196 net.cpp:137] Memory required for data: 79054000
I1123 15:19:18.447777 22196 layer_factory.cpp:58] Creating layer bn2_2
I1123 15:19:18.447777 22196 net.cpp:84] Creating Layer bn2_2
I1123 15:19:18.447777 22196 net.cpp:406] bn2_2 <- conv2_2
I1123 15:19:18.447777 22196 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 15:19:18.448276 22196 net.cpp:122] Setting up bn2_2
I1123 15:19:18.448276 22196 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 15:19:18.448276 22196 net.cpp:137] Memory required for data: 91342000
I1123 15:19:18.448276 22196 layer_factory.cpp:58] Creating layer scale2_2
I1123 15:19:18.448276 22196 net.cpp:84] Creating Layer scale2_2
I1123 15:19:18.448276 22196 net.cpp:406] scale2_2 <- conv2_2
I1123 15:19:18.448276 22196 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 15:19:18.448276 22196 layer_factory.cpp:58] Creating layer scale2_2
I1123 15:19:18.448276 22196 net.cpp:122] Setting up scale2_2
I1123 15:19:18.448276 22196 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 15:19:18.448276 22196 net.cpp:137] Memory required for data: 103630000
I1123 15:19:18.448276 22196 layer_factory.cpp:58] Creating layer relu2_2
I1123 15:19:18.448276 22196 net.cpp:84] Creating Layer relu2_2
I1123 15:19:18.448276 22196 net.cpp:406] relu2_2 <- conv2_2
I1123 15:19:18.448276 22196 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 15:19:18.448777 22196 net.cpp:122] Setting up relu2_2
I1123 15:19:18.448777 22196 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1123 15:19:18.448777 22196 net.cpp:137] Memory required for data: 115918000
I1123 15:19:18.448777 22196 layer_factory.cpp:58] Creating layer pool2_1
I1123 15:19:18.448777 22196 net.cpp:84] Creating Layer pool2_1
I1123 15:19:18.448777 22196 net.cpp:406] pool2_1 <- conv2_2
I1123 15:19:18.448777 22196 net.cpp:380] pool2_1 -> pool2_1
I1123 15:19:18.448777 22196 net.cpp:122] Setting up pool2_1
I1123 15:19:18.448777 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.448777 22196 net.cpp:137] Memory required for data: 118990000
I1123 15:19:18.448777 22196 layer_factory.cpp:58] Creating layer conv3
I1123 15:19:18.448777 22196 net.cpp:84] Creating Layer conv3
I1123 15:19:18.448777 22196 net.cpp:406] conv3 <- pool2_1
I1123 15:19:18.448777 22196 net.cpp:380] conv3 -> conv3
I1123 15:19:18.450778 22196 net.cpp:122] Setting up conv3
I1123 15:19:18.450778 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.450778 22196 net.cpp:137] Memory required for data: 122062000
I1123 15:19:18.450778 22196 layer_factory.cpp:58] Creating layer bn3
I1123 15:19:18.450778 22196 net.cpp:84] Creating Layer bn3
I1123 15:19:18.450778 22196 net.cpp:406] bn3 <- conv3
I1123 15:19:18.450778 22196 net.cpp:367] bn3 -> conv3 (in-place)
I1123 15:19:18.450778 22196 net.cpp:122] Setting up bn3
I1123 15:19:18.450778 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.450778 22196 net.cpp:137] Memory required for data: 125134000
I1123 15:19:18.450778 22196 layer_factory.cpp:58] Creating layer scale3
I1123 15:19:18.450778 22196 net.cpp:84] Creating Layer scale3
I1123 15:19:18.450778 22196 net.cpp:406] scale3 <- conv3
I1123 15:19:18.450778 22196 net.cpp:367] scale3 -> conv3 (in-place)
I1123 15:19:18.451277 22196 layer_factory.cpp:58] Creating layer scale3
I1123 15:19:18.451277 22196 net.cpp:122] Setting up scale3
I1123 15:19:18.451277 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.451277 22196 net.cpp:137] Memory required for data: 128206000
I1123 15:19:18.451277 22196 layer_factory.cpp:58] Creating layer relu3
I1123 15:19:18.451277 22196 net.cpp:84] Creating Layer relu3
I1123 15:19:18.451277 22196 net.cpp:406] relu3 <- conv3
I1123 15:19:18.451277 22196 net.cpp:367] relu3 -> conv3 (in-place)
I1123 15:19:18.451776 22196 net.cpp:122] Setting up relu3
I1123 15:19:18.451776 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.451776 22196 net.cpp:137] Memory required for data: 131278000
I1123 15:19:18.451776 22196 layer_factory.cpp:58] Creating layer conv4
I1123 15:19:18.451776 22196 net.cpp:84] Creating Layer conv4
I1123 15:19:18.451776 22196 net.cpp:406] conv4 <- conv3
I1123 15:19:18.451776 22196 net.cpp:380] conv4 -> conv4
I1123 15:19:18.453781 22196 net.cpp:122] Setting up conv4
I1123 15:19:18.453781 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.453781 22196 net.cpp:137] Memory required for data: 134350000
I1123 15:19:18.453781 22196 layer_factory.cpp:58] Creating layer bn4
I1123 15:19:18.453781 22196 net.cpp:84] Creating Layer bn4
I1123 15:19:18.453781 22196 net.cpp:406] bn4 <- conv4
I1123 15:19:18.453781 22196 net.cpp:367] bn4 -> conv4 (in-place)
I1123 15:19:18.454277 22196 net.cpp:122] Setting up bn4
I1123 15:19:18.454277 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.454277 22196 net.cpp:137] Memory required for data: 137422000
I1123 15:19:18.454277 22196 layer_factory.cpp:58] Creating layer scale4
I1123 15:19:18.454277 22196 net.cpp:84] Creating Layer scale4
I1123 15:19:18.454277 22196 net.cpp:406] scale4 <- conv4
I1123 15:19:18.454277 22196 net.cpp:367] scale4 -> conv4 (in-place)
I1123 15:19:18.454277 22196 layer_factory.cpp:58] Creating layer scale4
I1123 15:19:18.454277 22196 net.cpp:122] Setting up scale4
I1123 15:19:18.454277 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.454277 22196 net.cpp:137] Memory required for data: 140494000
I1123 15:19:18.454277 22196 layer_factory.cpp:58] Creating layer relu4
I1123 15:19:18.454277 22196 net.cpp:84] Creating Layer relu4
I1123 15:19:18.454277 22196 net.cpp:406] relu4 <- conv4
I1123 15:19:18.454277 22196 net.cpp:367] relu4 -> conv4 (in-place)
I1123 15:19:18.454777 22196 net.cpp:122] Setting up relu4
I1123 15:19:18.454777 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.454777 22196 net.cpp:137] Memory required for data: 143566000
I1123 15:19:18.454777 22196 layer_factory.cpp:58] Creating layer conv4_1
I1123 15:19:18.454777 22196 net.cpp:84] Creating Layer conv4_1
I1123 15:19:18.454777 22196 net.cpp:406] conv4_1 <- conv4
I1123 15:19:18.454777 22196 net.cpp:380] conv4_1 -> conv4_1
I1123 15:19:18.456791 22196 net.cpp:122] Setting up conv4_1
I1123 15:19:18.456791 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.456791 22196 net.cpp:137] Memory required for data: 146638000
I1123 15:19:18.456791 22196 layer_factory.cpp:58] Creating layer bn4_1
I1123 15:19:18.456791 22196 net.cpp:84] Creating Layer bn4_1
I1123 15:19:18.456791 22196 net.cpp:406] bn4_1 <- conv4_1
I1123 15:19:18.456791 22196 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 15:19:18.457278 22196 net.cpp:122] Setting up bn4_1
I1123 15:19:18.457278 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.457278 22196 net.cpp:137] Memory required for data: 149710000
I1123 15:19:18.457278 22196 layer_factory.cpp:58] Creating layer scale4_1
I1123 15:19:18.457278 22196 net.cpp:84] Creating Layer scale4_1
I1123 15:19:18.457278 22196 net.cpp:406] scale4_1 <- conv4_1
I1123 15:19:18.457278 22196 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 15:19:18.457278 22196 layer_factory.cpp:58] Creating layer scale4_1
I1123 15:19:18.457278 22196 net.cpp:122] Setting up scale4_1
I1123 15:19:18.457278 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.457278 22196 net.cpp:137] Memory required for data: 152782000
I1123 15:19:18.457278 22196 layer_factory.cpp:58] Creating layer relu4_1
I1123 15:19:18.457278 22196 net.cpp:84] Creating Layer relu4_1
I1123 15:19:18.457278 22196 net.cpp:406] relu4_1 <- conv4_1
I1123 15:19:18.457278 22196 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 15:19:18.457777 22196 net.cpp:122] Setting up relu4_1
I1123 15:19:18.457777 22196 net.cpp:129] Top shape: 100 30 16 16 (768000)
I1123 15:19:18.457777 22196 net.cpp:137] Memory required for data: 155854000
I1123 15:19:18.457777 22196 layer_factory.cpp:58] Creating layer conv4_2
I1123 15:19:18.457777 22196 net.cpp:84] Creating Layer conv4_2
I1123 15:19:18.457777 22196 net.cpp:406] conv4_2 <- conv4_1
I1123 15:19:18.457777 22196 net.cpp:380] conv4_2 -> conv4_2
I1123 15:19:18.459277 22196 net.cpp:122] Setting up conv4_2
I1123 15:19:18.459277 22196 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 15:19:18.459277 22196 net.cpp:137] Memory required for data: 159438000
I1123 15:19:18.459277 22196 layer_factory.cpp:58] Creating layer bn4_2
I1123 15:19:18.459277 22196 net.cpp:84] Creating Layer bn4_2
I1123 15:19:18.459777 22196 net.cpp:406] bn4_2 <- conv4_2
I1123 15:19:18.459777 22196 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 15:19:18.459777 22196 net.cpp:122] Setting up bn4_2
I1123 15:19:18.459777 22196 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 15:19:18.459777 22196 net.cpp:137] Memory required for data: 163022000
I1123 15:19:18.459777 22196 layer_factory.cpp:58] Creating layer scale4_2
I1123 15:19:18.459777 22196 net.cpp:84] Creating Layer scale4_2
I1123 15:19:18.459777 22196 net.cpp:406] scale4_2 <- conv4_2
I1123 15:19:18.459777 22196 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 15:19:18.459777 22196 layer_factory.cpp:58] Creating layer scale4_2
I1123 15:19:18.459777 22196 net.cpp:122] Setting up scale4_2
I1123 15:19:18.459777 22196 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 15:19:18.459777 22196 net.cpp:137] Memory required for data: 166606000
I1123 15:19:18.459777 22196 layer_factory.cpp:58] Creating layer relu4_2
I1123 15:19:18.459777 22196 net.cpp:84] Creating Layer relu4_2
I1123 15:19:18.459777 22196 net.cpp:406] relu4_2 <- conv4_2
I1123 15:19:18.459777 22196 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 15:19:18.460276 22196 net.cpp:122] Setting up relu4_2
I1123 15:19:18.460276 22196 net.cpp:129] Top shape: 100 35 16 16 (896000)
I1123 15:19:18.460276 22196 net.cpp:137] Memory required for data: 170190000
I1123 15:19:18.460276 22196 layer_factory.cpp:58] Creating layer pool4_2
I1123 15:19:18.460276 22196 net.cpp:84] Creating Layer pool4_2
I1123 15:19:18.460276 22196 net.cpp:406] pool4_2 <- conv4_2
I1123 15:19:18.460276 22196 net.cpp:380] pool4_2 -> pool4_2
I1123 15:19:18.460276 22196 net.cpp:122] Setting up pool4_2
I1123 15:19:18.460276 22196 net.cpp:129] Top shape: 100 35 8 8 (224000)
I1123 15:19:18.460276 22196 net.cpp:137] Memory required for data: 171086000
I1123 15:19:18.460276 22196 layer_factory.cpp:58] Creating layer conv12
I1123 15:19:18.460276 22196 net.cpp:84] Creating Layer conv12
I1123 15:19:18.460276 22196 net.cpp:406] conv12 <- pool4_2
I1123 15:19:18.460276 22196 net.cpp:380] conv12 -> conv12
I1123 15:19:18.461279 22196 net.cpp:122] Setting up conv12
I1123 15:19:18.461279 22196 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 15:19:18.461279 22196 net.cpp:137] Memory required for data: 172058800
I1123 15:19:18.461279 22196 layer_factory.cpp:58] Creating layer bn_conv12
I1123 15:19:18.461279 22196 net.cpp:84] Creating Layer bn_conv12
I1123 15:19:18.461279 22196 net.cpp:406] bn_conv12 <- conv12
I1123 15:19:18.461279 22196 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 15:19:18.461279 22196 net.cpp:122] Setting up bn_conv12
I1123 15:19:18.461279 22196 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 15:19:18.461279 22196 net.cpp:137] Memory required for data: 173031600
I1123 15:19:18.461279 22196 layer_factory.cpp:58] Creating layer scale_conv12
I1123 15:19:18.461279 22196 net.cpp:84] Creating Layer scale_conv12
I1123 15:19:18.461279 22196 net.cpp:406] scale_conv12 <- conv12
I1123 15:19:18.461279 22196 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 15:19:18.461279 22196 layer_factory.cpp:58] Creating layer scale_conv12
I1123 15:19:18.462280 22196 net.cpp:122] Setting up scale_conv12
I1123 15:19:18.462280 22196 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 15:19:18.462280 22196 net.cpp:137] Memory required for data: 174004400
I1123 15:19:18.462280 22196 layer_factory.cpp:58] Creating layer relu_conv12
I1123 15:19:18.462280 22196 net.cpp:84] Creating Layer relu_conv12
I1123 15:19:18.462280 22196 net.cpp:406] relu_conv12 <- conv12
I1123 15:19:18.462280 22196 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 15:19:18.462280 22196 net.cpp:122] Setting up relu_conv12
I1123 15:19:18.462280 22196 net.cpp:129] Top shape: 100 38 8 8 (243200)
I1123 15:19:18.462280 22196 net.cpp:137] Memory required for data: 174977200
I1123 15:19:18.462280 22196 layer_factory.cpp:58] Creating layer poolcp6
I1123 15:19:18.462280 22196 net.cpp:84] Creating Layer poolcp6
I1123 15:19:18.462280 22196 net.cpp:406] poolcp6 <- conv12
I1123 15:19:18.462280 22196 net.cpp:380] poolcp6 -> poolcp6
I1123 15:19:18.462280 22196 net.cpp:122] Setting up poolcp6
I1123 15:19:18.462280 22196 net.cpp:129] Top shape: 100 38 1 1 (3800)
I1123 15:19:18.462280 22196 net.cpp:137] Memory required for data: 174992400
I1123 15:19:18.462280 22196 layer_factory.cpp:58] Creating layer ip1
I1123 15:19:18.462280 22196 net.cpp:84] Creating Layer ip1
I1123 15:19:18.462280 22196 net.cpp:406] ip1 <- poolcp6
I1123 15:19:18.462280 22196 net.cpp:380] ip1 -> ip1
I1123 15:19:18.462280 22196 net.cpp:122] Setting up ip1
I1123 15:19:18.462280 22196 net.cpp:129] Top shape: 100 10 (1000)
I1123 15:19:18.462280 22196 net.cpp:137] Memory required for data: 174996400
I1123 15:19:18.462280 22196 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 15:19:18.462280 22196 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 15:19:18.462280 22196 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 15:19:18.462280 22196 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 15:19:18.462280 22196 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 15:19:18.463280 22196 net.cpp:122] Setting up ip1_ip1_0_split
I1123 15:19:18.463280 22196 net.cpp:129] Top shape: 100 10 (1000)
I1123 15:19:18.463280 22196 net.cpp:129] Top shape: 100 10 (1000)
I1123 15:19:18.463280 22196 net.cpp:137] Memory required for data: 175004400
I1123 15:19:18.463280 22196 layer_factory.cpp:58] Creating layer accuracy
I1123 15:19:18.463280 22196 net.cpp:84] Creating Layer accuracy
I1123 15:19:18.463280 22196 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1123 15:19:18.463280 22196 net.cpp:406] accuracy <- label_cifar_1_split_0
I1123 15:19:18.463280 22196 net.cpp:380] accuracy -> accuracy
I1123 15:19:18.463280 22196 net.cpp:122] Setting up accuracy
I1123 15:19:18.463280 22196 net.cpp:129] Top shape: (1)
I1123 15:19:18.463280 22196 net.cpp:137] Memory required for data: 175004404
I1123 15:19:18.463280 22196 layer_factory.cpp:58] Creating layer loss
I1123 15:19:18.463280 22196 net.cpp:84] Creating Layer loss
I1123 15:19:18.463280 22196 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 15:19:18.463280 22196 net.cpp:406] loss <- label_cifar_1_split_1
I1123 15:19:18.463280 22196 net.cpp:380] loss -> loss
I1123 15:19:18.463280 22196 layer_factory.cpp:58] Creating layer loss
I1123 15:19:18.463280 22196 net.cpp:122] Setting up loss
I1123 15:19:18.463280 22196 net.cpp:129] Top shape: (1)
I1123 15:19:18.463280 22196 net.cpp:132]     with loss weight 1
I1123 15:19:18.463280 22196 net.cpp:137] Memory required for data: 175004408
I1123 15:19:18.463280 22196 net.cpp:198] loss needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:200] accuracy does not need backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] ip1 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] poolcp6 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] relu_conv12 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] scale_conv12 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] bn_conv12 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] conv12 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] pool4_2 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] relu4_2 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] scale4_2 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] bn4_2 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] conv4_2 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] relu4_1 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] scale4_1 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] bn4_1 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] conv4_1 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] relu4 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] scale4 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] bn4 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] conv4 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] relu3 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] scale3 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] bn3 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] conv3 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] pool2_1 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] relu2_2 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] scale2_2 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] bn2_2 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] conv2_2 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] relu2 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] scale2 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] bn2 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] conv2 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] relu1 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] scale1 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] bn1 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:198] conv1 needs backward computation.
I1123 15:19:18.463280 22196 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 15:19:18.463280 22196 net.cpp:200] cifar does not need backward computation.
I1123 15:19:18.463280 22196 net.cpp:242] This network produces output accuracy
I1123 15:19:18.463280 22196 net.cpp:242] This network produces output loss
I1123 15:19:18.463280 22196 net.cpp:255] Network initialization done.
I1123 15:19:18.463280 22196 solver.cpp:56] Solver scaffolding done.
I1123 15:19:18.466281 22196 caffe.cpp:249] Starting Optimization
I1123 15:19:18.466281 22196 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_7x7_300K
I1123 15:19:18.466281 22196 solver.cpp:273] Learning Rate Policy: multistep
I1123 15:19:18.468281 22196 solver.cpp:330] Iteration 0, Testing net (#0)
I1123 15:19:18.469280 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:19:19.573403 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:19:19.615416 22196 solver.cpp:397]     Test net output #0: accuracy = 0.1006
I1123 15:19:19.615416 22196 solver.cpp:397]     Test net output #1: loss = 78.5505 (* 1 = 78.5505 loss)
I1123 15:19:19.672916 22196 solver.cpp:218] Iteration 0 (0 iter/s, 1.20583s/100 iters), loss = 3.59469
I1123 15:19:19.672916 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.05
I1123 15:19:19.672916 22196 solver.cpp:237]     Train net output #1: loss = 3.59469 (* 1 = 3.59469 loss)
I1123 15:19:19.672916 22196 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1123 15:19:23.511888 22196 solver.cpp:218] Iteration 100 (26.0507 iter/s, 3.83867s/100 iters), loss = 1.68827
I1123 15:19:23.511888 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.37
I1123 15:19:23.511888 22196 solver.cpp:237]     Train net output #1: loss = 1.68827 (* 1 = 1.68827 loss)
I1123 15:19:23.511888 22196 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1123 15:19:27.355082 22196 solver.cpp:218] Iteration 200 (26.0253 iter/s, 3.84242s/100 iters), loss = 1.91106
I1123 15:19:27.355082 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.3
I1123 15:19:27.355082 22196 solver.cpp:237]     Train net output #1: loss = 1.91106 (* 1 = 1.91106 loss)
I1123 15:19:27.355082 22196 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1123 15:19:31.189064 22196 solver.cpp:218] Iteration 300 (26.0856 iter/s, 3.83353s/100 iters), loss = 1.51492
I1123 15:19:31.189064 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.46
I1123 15:19:31.189064 22196 solver.cpp:237]     Train net output #1: loss = 1.51492 (* 1 = 1.51492 loss)
I1123 15:19:31.189064 22196 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1123 15:19:35.018332 22196 solver.cpp:218] Iteration 400 (26.1129 iter/s, 3.82952s/100 iters), loss = 1.27058
I1123 15:19:35.018332 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.55
I1123 15:19:35.018332 22196 solver.cpp:237]     Train net output #1: loss = 1.27058 (* 1 = 1.27058 loss)
I1123 15:19:35.018332 22196 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1123 15:19:38.670562 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:19:38.820469 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_500.caffemodel
I1123 15:19:38.834458 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_500.solverstate
I1123 15:19:38.838457 22196 solver.cpp:330] Iteration 500, Testing net (#0)
I1123 15:19:38.838457 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:19:39.897976 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:19:39.939956 22196 solver.cpp:397]     Test net output #0: accuracy = 0.4751
I1123 15:19:39.939956 22196 solver.cpp:397]     Test net output #1: loss = 1.44306 (* 1 = 1.44306 loss)
I1123 15:19:39.976078 22196 solver.cpp:218] Iteration 500 (20.1722 iter/s, 4.95731s/100 iters), loss = 1.44001
I1123 15:19:39.976078 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.5
I1123 15:19:39.976078 22196 solver.cpp:237]     Train net output #1: loss = 1.44001 (* 1 = 1.44001 loss)
I1123 15:19:39.976078 22196 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1123 15:19:43.814821 22196 solver.cpp:218] Iteration 600 (26.0568 iter/s, 3.83777s/100 iters), loss = 1.24618
I1123 15:19:43.814821 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.56
I1123 15:19:43.814821 22196 solver.cpp:237]     Train net output #1: loss = 1.24618 (* 1 = 1.24618 loss)
I1123 15:19:43.814821 22196 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1123 15:19:47.666009 22196 solver.cpp:218] Iteration 700 (25.9674 iter/s, 3.85098s/100 iters), loss = 1.19322
I1123 15:19:47.666509 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.57
I1123 15:19:47.666509 22196 solver.cpp:237]     Train net output #1: loss = 1.19322 (* 1 = 1.19322 loss)
I1123 15:19:47.666509 22196 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1123 15:19:51.519160 22196 solver.cpp:218] Iteration 800 (25.9533 iter/s, 3.85307s/100 iters), loss = 1.0389
I1123 15:19:51.519160 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1123 15:19:51.519160 22196 solver.cpp:237]     Train net output #1: loss = 1.0389 (* 1 = 1.0389 loss)
I1123 15:19:51.519160 22196 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1123 15:19:55.401381 22196 solver.cpp:218] Iteration 900 (25.7652 iter/s, 3.8812s/100 iters), loss = 0.999449
I1123 15:19:55.401381 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1123 15:19:55.401381 22196 solver.cpp:237]     Train net output #1: loss = 0.999449 (* 1 = 0.999449 loss)
I1123 15:19:55.401381 22196 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1123 15:19:59.078265 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:19:59.229295 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_1000.caffemodel
I1123 15:19:59.239300 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_1000.solverstate
I1123 15:19:59.244293 22196 solver.cpp:330] Iteration 1000, Testing net (#0)
I1123 15:19:59.244293 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:20:00.310220 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:20:00.352217 22196 solver.cpp:397]     Test net output #0: accuracy = 0.5852
I1123 15:20:00.352217 22196 solver.cpp:397]     Test net output #1: loss = 1.15609 (* 1 = 1.15609 loss)
I1123 15:20:00.390228 22196 solver.cpp:218] Iteration 1000 (20.0439 iter/s, 4.98905s/100 iters), loss = 1.15592
I1123 15:20:00.390228 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.61
I1123 15:20:00.390228 22196 solver.cpp:237]     Train net output #1: loss = 1.15592 (* 1 = 1.15592 loss)
I1123 15:20:00.390228 22196 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1123 15:20:04.292608 22196 solver.cpp:218] Iteration 1100 (25.6339 iter/s, 3.90109s/100 iters), loss = 1.00428
I1123 15:20:04.292608 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1123 15:20:04.292608 22196 solver.cpp:237]     Train net output #1: loss = 1.00428 (* 1 = 1.00428 loss)
I1123 15:20:04.292608 22196 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1123 15:20:08.162183 22196 solver.cpp:218] Iteration 1200 (25.8391 iter/s, 3.8701s/100 iters), loss = 1.00865
I1123 15:20:08.163173 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1123 15:20:08.163173 22196 solver.cpp:237]     Train net output #1: loss = 1.00865 (* 1 = 1.00865 loss)
I1123 15:20:08.163173 22196 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1123 15:20:12.027052 22196 solver.cpp:218] Iteration 1300 (25.8772 iter/s, 3.86441s/100 iters), loss = 0.958709
I1123 15:20:12.027052 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1123 15:20:12.027052 22196 solver.cpp:237]     Train net output #1: loss = 0.958709 (* 1 = 0.958709 loss)
I1123 15:20:12.027052 22196 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1123 15:20:15.916661 22196 solver.cpp:218] Iteration 1400 (25.7129 iter/s, 3.8891s/100 iters), loss = 0.959574
I1123 15:20:15.916661 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1123 15:20:15.916661 22196 solver.cpp:237]     Train net output #1: loss = 0.959574 (* 1 = 0.959574 loss)
I1123 15:20:15.916661 22196 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1123 15:20:19.586673 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:20:19.737181 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_1500.caffemodel
I1123 15:20:19.747180 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_1500.solverstate
I1123 15:20:19.750181 22196 solver.cpp:330] Iteration 1500, Testing net (#0)
I1123 15:20:19.751183 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:20:20.812983 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:20:20.855978 22196 solver.cpp:397]     Test net output #0: accuracy = 0.4902
I1123 15:20:20.855978 22196 solver.cpp:397]     Test net output #1: loss = 1.44389 (* 1 = 1.44389 loss)
I1123 15:20:20.892489 22196 solver.cpp:218] Iteration 1500 (20.1013 iter/s, 4.9748s/100 iters), loss = 0.963655
I1123 15:20:20.892489 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1123 15:20:20.892489 22196 solver.cpp:237]     Train net output #1: loss = 0.963655 (* 1 = 0.963655 loss)
I1123 15:20:20.892489 22196 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1123 15:20:24.740058 22196 solver.cpp:218] Iteration 1600 (25.9868 iter/s, 3.8481s/100 iters), loss = 0.796174
I1123 15:20:24.741044 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1123 15:20:24.741044 22196 solver.cpp:237]     Train net output #1: loss = 0.796174 (* 1 = 0.796174 loss)
I1123 15:20:24.741044 22196 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1123 15:20:28.595983 22196 solver.cpp:218] Iteration 1700 (25.9367 iter/s, 3.85553s/100 iters), loss = 0.798352
I1123 15:20:28.596982 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1123 15:20:28.596982 22196 solver.cpp:237]     Train net output #1: loss = 0.798352 (* 1 = 0.798352 loss)
I1123 15:20:28.596982 22196 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1123 15:20:32.452690 22196 solver.cpp:218] Iteration 1800 (25.9329 iter/s, 3.85611s/100 iters), loss = 0.750067
I1123 15:20:32.452690 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 15:20:32.452690 22196 solver.cpp:237]     Train net output #1: loss = 0.750067 (* 1 = 0.750067 loss)
I1123 15:20:32.452690 22196 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1123 15:20:36.317551 22196 solver.cpp:218] Iteration 1900 (25.8799 iter/s, 3.864s/100 iters), loss = 0.822583
I1123 15:20:36.317551 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 15:20:36.317551 22196 solver.cpp:237]     Train net output #1: loss = 0.822583 (* 1 = 0.822583 loss)
I1123 15:20:36.317551 22196 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1123 15:20:39.997098 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:20:40.150619 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_2000.caffemodel
I1123 15:20:40.160619 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_2000.solverstate
I1123 15:20:40.164619 22196 solver.cpp:330] Iteration 2000, Testing net (#0)
I1123 15:20:40.164619 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:20:41.228891 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:20:41.270876 22196 solver.cpp:397]     Test net output #0: accuracy = 0.652
I1123 15:20:41.270876 22196 solver.cpp:397]     Test net output #1: loss = 1.00409 (* 1 = 1.00409 loss)
I1123 15:20:41.307909 22196 solver.cpp:218] Iteration 2000 (20.0415 iter/s, 4.98964s/100 iters), loss = 0.747246
I1123 15:20:41.307909 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1123 15:20:41.307909 22196 solver.cpp:237]     Train net output #1: loss = 0.747246 (* 1 = 0.747246 loss)
I1123 15:20:41.307909 22196 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1123 15:20:45.176852 22196 solver.cpp:218] Iteration 2100 (25.8492 iter/s, 3.8686s/100 iters), loss = 0.711136
I1123 15:20:45.176852 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1123 15:20:45.176852 22196 solver.cpp:237]     Train net output #1: loss = 0.711136 (* 1 = 0.711136 loss)
I1123 15:20:45.176852 22196 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1123 15:20:49.052386 22196 solver.cpp:218] Iteration 2200 (25.8014 iter/s, 3.87576s/100 iters), loss = 0.809966
I1123 15:20:49.052386 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1123 15:20:49.053375 22196 solver.cpp:237]     Train net output #1: loss = 0.809966 (* 1 = 0.809966 loss)
I1123 15:20:49.053375 22196 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1123 15:20:52.908682 22196 solver.cpp:218] Iteration 2300 (25.934 iter/s, 3.85594s/100 iters), loss = 0.792734
I1123 15:20:52.909669 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 15:20:52.909669 22196 solver.cpp:237]     Train net output #1: loss = 0.792734 (* 1 = 0.792734 loss)
I1123 15:20:52.909669 22196 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1123 15:20:56.775420 22196 solver.cpp:218] Iteration 2400 (25.8695 iter/s, 3.86556s/100 iters), loss = 0.787485
I1123 15:20:56.775420 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 15:20:56.775420 22196 solver.cpp:237]     Train net output #1: loss = 0.787485 (* 1 = 0.787485 loss)
I1123 15:20:56.775420 22196 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1123 15:21:00.451875 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:21:00.602412 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_2500.caffemodel
I1123 15:21:00.612395 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_2500.solverstate
I1123 15:21:00.616901 22196 solver.cpp:330] Iteration 2500, Testing net (#0)
I1123 15:21:00.616901 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:21:01.678774 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:21:01.721284 22196 solver.cpp:397]     Test net output #0: accuracy = 0.6875
I1123 15:21:01.721284 22196 solver.cpp:397]     Test net output #1: loss = 0.900681 (* 1 = 0.900681 loss)
I1123 15:21:01.757802 22196 solver.cpp:218] Iteration 2500 (20.0712 iter/s, 4.98226s/100 iters), loss = 0.710067
I1123 15:21:01.757802 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 15:21:01.757802 22196 solver.cpp:237]     Train net output #1: loss = 0.710067 (* 1 = 0.710067 loss)
I1123 15:21:01.757802 22196 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1123 15:21:05.607086 22196 solver.cpp:218] Iteration 2600 (25.9813 iter/s, 3.84892s/100 iters), loss = 0.683464
I1123 15:21:05.607086 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 15:21:05.607086 22196 solver.cpp:237]     Train net output #1: loss = 0.683464 (* 1 = 0.683464 loss)
I1123 15:21:05.607086 22196 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1123 15:21:09.459300 22196 solver.cpp:218] Iteration 2700 (25.9617 iter/s, 3.85183s/100 iters), loss = 0.758387
I1123 15:21:09.459300 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1123 15:21:09.459300 22196 solver.cpp:237]     Train net output #1: loss = 0.758387 (* 1 = 0.758387 loss)
I1123 15:21:09.459300 22196 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1123 15:21:13.313058 22196 solver.cpp:218] Iteration 2800 (25.9508 iter/s, 3.85345s/100 iters), loss = 0.72552
I1123 15:21:13.313058 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1123 15:21:13.313544 22196 solver.cpp:237]     Train net output #1: loss = 0.72552 (* 1 = 0.72552 loss)
I1123 15:21:13.313544 22196 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1123 15:21:17.169735 22196 solver.cpp:218] Iteration 2900 (25.932 iter/s, 3.85624s/100 iters), loss = 0.699295
I1123 15:21:17.169735 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 15:21:17.169735 22196 solver.cpp:237]     Train net output #1: loss = 0.699295 (* 1 = 0.699295 loss)
I1123 15:21:17.169735 22196 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1123 15:21:20.844188 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:21:20.996206 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_3000.caffemodel
I1123 15:21:21.007207 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_3000.solverstate
I1123 15:21:21.011217 22196 solver.cpp:330] Iteration 3000, Testing net (#0)
I1123 15:21:21.011217 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:21:22.075356 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:21:22.117352 22196 solver.cpp:397]     Test net output #0: accuracy = 0.6082
I1123 15:21:22.117352 22196 solver.cpp:397]     Test net output #1: loss = 1.18276 (* 1 = 1.18276 loss)
I1123 15:21:22.154880 22196 solver.cpp:218] Iteration 3000 (20.0616 iter/s, 4.98464s/100 iters), loss = 0.660917
I1123 15:21:22.154880 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 15:21:22.155364 22196 solver.cpp:237]     Train net output #1: loss = 0.660917 (* 1 = 0.660917 loss)
I1123 15:21:22.155364 22196 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1123 15:21:26.007766 22196 solver.cpp:218] Iteration 3100 (25.9541 iter/s, 3.85295s/100 iters), loss = 0.624492
I1123 15:21:26.007766 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 15:21:26.007766 22196 solver.cpp:237]     Train net output #1: loss = 0.624492 (* 1 = 0.624492 loss)
I1123 15:21:26.007766 22196 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1123 15:21:29.858592 22196 solver.cpp:218] Iteration 3200 (25.9749 iter/s, 3.84988s/100 iters), loss = 0.730978
I1123 15:21:29.858592 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 15:21:29.858592 22196 solver.cpp:237]     Train net output #1: loss = 0.730978 (* 1 = 0.730978 loss)
I1123 15:21:29.858592 22196 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1123 15:21:33.707515 22196 solver.cpp:218] Iteration 3300 (25.9822 iter/s, 3.84879s/100 iters), loss = 0.709064
I1123 15:21:33.707515 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 15:21:33.707515 22196 solver.cpp:237]     Train net output #1: loss = 0.709064 (* 1 = 0.709064 loss)
I1123 15:21:33.707515 22196 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1123 15:21:37.555694 22196 solver.cpp:218] Iteration 3400 (25.9873 iter/s, 3.84803s/100 iters), loss = 0.67215
I1123 15:21:37.555694 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1123 15:21:37.555694 22196 solver.cpp:237]     Train net output #1: loss = 0.67215 (* 1 = 0.67215 loss)
I1123 15:21:37.555694 22196 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1123 15:21:41.213327 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:21:41.365032 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_3500.caffemodel
I1123 15:21:41.374531 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_3500.solverstate
I1123 15:21:41.378536 22196 solver.cpp:330] Iteration 3500, Testing net (#0)
I1123 15:21:41.378536 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:21:42.438431 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:21:42.480422 22196 solver.cpp:397]     Test net output #0: accuracy = 0.6213
I1123 15:21:42.480422 22196 solver.cpp:397]     Test net output #1: loss = 1.15903 (* 1 = 1.15903 loss)
I1123 15:21:42.517416 22196 solver.cpp:218] Iteration 3500 (20.1575 iter/s, 4.96093s/100 iters), loss = 0.710579
I1123 15:21:42.517416 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 15:21:42.517416 22196 solver.cpp:237]     Train net output #1: loss = 0.710579 (* 1 = 0.710579 loss)
I1123 15:21:42.517416 22196 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1123 15:21:46.369460 22196 solver.cpp:218] Iteration 3600 (25.963 iter/s, 3.85164s/100 iters), loss = 0.604905
I1123 15:21:46.369460 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 15:21:46.369460 22196 solver.cpp:237]     Train net output #1: loss = 0.604905 (* 1 = 0.604905 loss)
I1123 15:21:46.369460 22196 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1123 15:21:50.222793 22196 solver.cpp:218] Iteration 3700 (25.9519 iter/s, 3.85328s/100 iters), loss = 0.74438
I1123 15:21:50.222793 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 15:21:50.222793 22196 solver.cpp:237]     Train net output #1: loss = 0.74438 (* 1 = 0.74438 loss)
I1123 15:21:50.222793 22196 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1123 15:21:54.083370 22196 solver.cpp:218] Iteration 3800 (25.9061 iter/s, 3.86009s/100 iters), loss = 0.715743
I1123 15:21:54.083370 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 15:21:54.083370 22196 solver.cpp:237]     Train net output #1: loss = 0.715743 (* 1 = 0.715743 loss)
I1123 15:21:54.083370 22196 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1123 15:21:57.946584 22196 solver.cpp:218] Iteration 3900 (25.8892 iter/s, 3.86262s/100 iters), loss = 0.621135
I1123 15:21:57.946584 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 15:21:57.947085 22196 solver.cpp:237]     Train net output #1: loss = 0.621135 (* 1 = 0.621135 loss)
I1123 15:21:57.947085 22196 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1123 15:22:01.621237 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:22:01.771277 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_4000.caffemodel
I1123 15:22:01.781263 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_4000.solverstate
I1123 15:22:01.785262 22196 solver.cpp:330] Iteration 4000, Testing net (#0)
I1123 15:22:01.785262 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:22:02.846940 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:22:02.888222 22196 solver.cpp:397]     Test net output #0: accuracy = 0.5704
I1123 15:22:02.889212 22196 solver.cpp:397]     Test net output #1: loss = 1.30323 (* 1 = 1.30323 loss)
I1123 15:22:02.925231 22196 solver.cpp:218] Iteration 4000 (20.0864 iter/s, 4.9785s/100 iters), loss = 0.699051
I1123 15:22:02.925231 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 15:22:02.925231 22196 solver.cpp:237]     Train net output #1: loss = 0.699051 (* 1 = 0.699051 loss)
I1123 15:22:02.925231 22196 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1123 15:22:06.799984 22196 solver.cpp:218] Iteration 4100 (25.8147 iter/s, 3.87376s/100 iters), loss = 0.535885
I1123 15:22:06.799984 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 15:22:06.799984 22196 solver.cpp:237]     Train net output #1: loss = 0.535885 (* 1 = 0.535885 loss)
I1123 15:22:06.799984 22196 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1123 15:22:10.647909 22196 solver.cpp:218] Iteration 4200 (25.9916 iter/s, 3.8474s/100 iters), loss = 0.651355
I1123 15:22:10.647909 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 15:22:10.647909 22196 solver.cpp:237]     Train net output #1: loss = 0.651355 (* 1 = 0.651355 loss)
I1123 15:22:10.647909 22196 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1123 15:22:14.497078 22196 solver.cpp:218] Iteration 4300 (25.9808 iter/s, 3.84899s/100 iters), loss = 0.64718
I1123 15:22:14.497078 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 15:22:14.497078 22196 solver.cpp:237]     Train net output #1: loss = 0.64718 (* 1 = 0.64718 loss)
I1123 15:22:14.497078 22196 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1123 15:22:18.354619 22196 solver.cpp:218] Iteration 4400 (25.9243 iter/s, 3.85739s/100 iters), loss = 0.659392
I1123 15:22:18.354619 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 15:22:18.354619 22196 solver.cpp:237]     Train net output #1: loss = 0.659392 (* 1 = 0.659392 loss)
I1123 15:22:18.354619 22196 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1123 15:22:22.037142 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:22:22.189183 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_4500.caffemodel
I1123 15:22:22.199184 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_4500.solverstate
I1123 15:22:22.203188 22196 solver.cpp:330] Iteration 4500, Testing net (#0)
I1123 15:22:22.203188 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:22:23.265498 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:22:23.308140 22196 solver.cpp:397]     Test net output #0: accuracy = 0.7003
I1123 15:22:23.308140 22196 solver.cpp:397]     Test net output #1: loss = 0.877229 (* 1 = 0.877229 loss)
I1123 15:22:23.345152 22196 solver.cpp:218] Iteration 4500 (20.0412 iter/s, 4.98973s/100 iters), loss = 0.589337
I1123 15:22:23.345152 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 15:22:23.345152 22196 solver.cpp:237]     Train net output #1: loss = 0.589337 (* 1 = 0.589337 loss)
I1123 15:22:23.345152 22196 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1123 15:22:27.204025 22196 solver.cpp:218] Iteration 4600 (25.9134 iter/s, 3.859s/100 iters), loss = 0.42273
I1123 15:22:27.204025 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 15:22:27.204025 22196 solver.cpp:237]     Train net output #1: loss = 0.42273 (* 1 = 0.42273 loss)
I1123 15:22:27.204025 22196 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1123 15:22:31.059049 22196 solver.cpp:218] Iteration 4700 (25.9454 iter/s, 3.85425s/100 iters), loss = 0.672589
I1123 15:22:31.059049 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 15:22:31.059049 22196 solver.cpp:237]     Train net output #1: loss = 0.672589 (* 1 = 0.672589 loss)
I1123 15:22:31.059049 22196 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1123 15:22:34.917420 22196 solver.cpp:218] Iteration 4800 (25.9193 iter/s, 3.85813s/100 iters), loss = 0.625525
I1123 15:22:34.917420 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 15:22:34.917420 22196 solver.cpp:237]     Train net output #1: loss = 0.625525 (* 1 = 0.625525 loss)
I1123 15:22:34.917420 22196 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1123 15:22:38.767060 22196 solver.cpp:218] Iteration 4900 (25.9793 iter/s, 3.84921s/100 iters), loss = 0.618379
I1123 15:22:38.767060 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 15:22:38.767060 22196 solver.cpp:237]     Train net output #1: loss = 0.618379 (* 1 = 0.618379 loss)
I1123 15:22:38.767060 22196 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1123 15:22:42.425220 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:22:42.577802 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_5000.caffemodel
I1123 15:22:42.588804 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_5000.solverstate
I1123 15:22:42.592803 22196 solver.cpp:330] Iteration 5000, Testing net (#0)
I1123 15:22:42.592803 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:22:43.653604 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:22:43.695629 22196 solver.cpp:397]     Test net output #0: accuracy = 0.6249
I1123 15:22:43.695629 22196 solver.cpp:397]     Test net output #1: loss = 1.16271 (* 1 = 1.16271 loss)
I1123 15:22:43.732131 22196 solver.cpp:218] Iteration 5000 (20.1403 iter/s, 4.96517s/100 iters), loss = 0.549165
I1123 15:22:43.732131 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 15:22:43.732131 22196 solver.cpp:237]     Train net output #1: loss = 0.549165 (* 1 = 0.549165 loss)
I1123 15:22:43.732131 22196 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1123 15:22:43.732131 22196 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1123 15:22:47.588228 22196 solver.cpp:218] Iteration 5100 (25.9384 iter/s, 3.85529s/100 iters), loss = 0.396984
I1123 15:22:47.588228 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:22:47.588228 22196 solver.cpp:237]     Train net output #1: loss = 0.396984 (* 1 = 0.396984 loss)
I1123 15:22:47.588228 22196 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1123 15:22:51.429407 22196 solver.cpp:218] Iteration 5200 (26.0338 iter/s, 3.84116s/100 iters), loss = 0.510363
I1123 15:22:51.429407 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 15:22:51.429407 22196 solver.cpp:237]     Train net output #1: loss = 0.510363 (* 1 = 0.510363 loss)
I1123 15:22:51.429407 22196 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1123 15:22:55.276156 22196 solver.cpp:218] Iteration 5300 (25.9977 iter/s, 3.8465s/100 iters), loss = 0.488485
I1123 15:22:55.276156 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 15:22:55.276156 22196 solver.cpp:237]     Train net output #1: loss = 0.488485 (* 1 = 0.488485 loss)
I1123 15:22:55.276156 22196 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1123 15:22:59.122849 22196 solver.cpp:218] Iteration 5400 (25.9975 iter/s, 3.84653s/100 iters), loss = 0.456595
I1123 15:22:59.122849 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:22:59.122849 22196 solver.cpp:237]     Train net output #1: loss = 0.456595 (* 1 = 0.456595 loss)
I1123 15:22:59.123837 22196 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1123 15:23:02.793447 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:23:02.945508 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_5500.caffemodel
I1123 15:23:02.955494 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_5500.solverstate
I1123 15:23:02.959513 22196 solver.cpp:330] Iteration 5500, Testing net (#0)
I1123 15:23:02.959513 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:23:04.022191 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:23:04.064206 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8316
I1123 15:23:04.064206 22196 solver.cpp:397]     Test net output #1: loss = 0.507311 (* 1 = 0.507311 loss)
I1123 15:23:04.101200 22196 solver.cpp:218] Iteration 5500 (20.0904 iter/s, 4.97749s/100 iters), loss = 0.423317
I1123 15:23:04.101200 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 15:23:04.101200 22196 solver.cpp:237]     Train net output #1: loss = 0.423317 (* 1 = 0.423317 loss)
I1123 15:23:04.101706 22196 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1123 15:23:07.947666 22196 solver.cpp:218] Iteration 5600 (26.002 iter/s, 3.84585s/100 iters), loss = 0.404785
I1123 15:23:07.947666 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 15:23:07.947666 22196 solver.cpp:237]     Train net output #1: loss = 0.404785 (* 1 = 0.404785 loss)
I1123 15:23:07.947666 22196 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1123 15:23:11.801661 22196 solver.cpp:218] Iteration 5700 (25.9444 iter/s, 3.85439s/100 iters), loss = 0.511541
I1123 15:23:11.801661 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 15:23:11.801661 22196 solver.cpp:237]     Train net output #1: loss = 0.511541 (* 1 = 0.511541 loss)
I1123 15:23:11.801661 22196 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1123 15:23:15.652930 22196 solver.cpp:218] Iteration 5800 (25.9725 iter/s, 3.85022s/100 iters), loss = 0.444472
I1123 15:23:15.652930 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 15:23:15.652930 22196 solver.cpp:237]     Train net output #1: loss = 0.444472 (* 1 = 0.444472 loss)
I1123 15:23:15.652930 22196 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1123 15:23:19.505262 22196 solver.cpp:218] Iteration 5900 (25.9593 iter/s, 3.85219s/100 iters), loss = 0.392771
I1123 15:23:19.505262 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:23:19.505262 22196 solver.cpp:237]     Train net output #1: loss = 0.392771 (* 1 = 0.392771 loss)
I1123 15:23:19.505262 22196 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1123 15:23:23.167495 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:23:23.319511 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_6000.caffemodel
I1123 15:23:23.329000 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_6000.solverstate
I1123 15:23:23.333499 22196 solver.cpp:330] Iteration 6000, Testing net (#0)
I1123 15:23:23.333499 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:23:24.396723 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:23:24.438735 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8351
I1123 15:23:24.438735 22196 solver.cpp:397]     Test net output #1: loss = 0.492481 (* 1 = 0.492481 loss)
I1123 15:23:24.476239 22196 solver.cpp:218] Iteration 6000 (20.1191 iter/s, 4.97039s/100 iters), loss = 0.406128
I1123 15:23:24.476239 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:23:24.476239 22196 solver.cpp:237]     Train net output #1: loss = 0.406128 (* 1 = 0.406128 loss)
I1123 15:23:24.476239 22196 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1123 15:23:28.339284 22196 solver.cpp:218] Iteration 6100 (25.8911 iter/s, 3.86234s/100 iters), loss = 0.360734
I1123 15:23:28.339284 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 15:23:28.339284 22196 solver.cpp:237]     Train net output #1: loss = 0.360734 (* 1 = 0.360734 loss)
I1123 15:23:28.339284 22196 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1123 15:23:32.197820 22196 solver.cpp:218] Iteration 6200 (25.9143 iter/s, 3.85887s/100 iters), loss = 0.44936
I1123 15:23:32.197820 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 15:23:32.197820 22196 solver.cpp:237]     Train net output #1: loss = 0.44936 (* 1 = 0.44936 loss)
I1123 15:23:32.197820 22196 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1123 15:23:36.064751 22196 solver.cpp:218] Iteration 6300 (25.8621 iter/s, 3.86666s/100 iters), loss = 0.420058
I1123 15:23:36.064751 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 15:23:36.064751 22196 solver.cpp:237]     Train net output #1: loss = 0.420058 (* 1 = 0.420058 loss)
I1123 15:23:36.064751 22196 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1123 15:23:39.927861 22196 solver.cpp:218] Iteration 6400 (25.8888 iter/s, 3.86267s/100 iters), loss = 0.374495
I1123 15:23:39.927861 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:23:39.927861 22196 solver.cpp:237]     Train net output #1: loss = 0.374495 (* 1 = 0.374495 loss)
I1123 15:23:39.927861 22196 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1123 15:23:43.594789 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:23:43.746825 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_6500.caffemodel
I1123 15:23:43.757895 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_6500.solverstate
I1123 15:23:43.761905 22196 solver.cpp:330] Iteration 6500, Testing net (#0)
I1123 15:23:43.761905 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:23:44.824659 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:23:44.865681 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8381
I1123 15:23:44.865681 22196 solver.cpp:397]     Test net output #1: loss = 0.479552 (* 1 = 0.479552 loss)
I1123 15:23:44.902685 22196 solver.cpp:218] Iteration 6500 (20.104 iter/s, 4.97414s/100 iters), loss = 0.388594
I1123 15:23:44.902685 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 15:23:44.902685 22196 solver.cpp:237]     Train net output #1: loss = 0.388594 (* 1 = 0.388594 loss)
I1123 15:23:44.902685 22196 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1123 15:23:48.759747 22196 solver.cpp:218] Iteration 6600 (25.9313 iter/s, 3.85634s/100 iters), loss = 0.318049
I1123 15:23:48.759747 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:23:48.759747 22196 solver.cpp:237]     Train net output #1: loss = 0.318049 (* 1 = 0.318049 loss)
I1123 15:23:48.759747 22196 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1123 15:23:52.617439 22196 solver.cpp:218] Iteration 6700 (25.925 iter/s, 3.85728s/100 iters), loss = 0.52675
I1123 15:23:52.617439 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 15:23:52.617439 22196 solver.cpp:237]     Train net output #1: loss = 0.52675 (* 1 = 0.52675 loss)
I1123 15:23:52.617439 22196 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1123 15:23:56.476526 22196 solver.cpp:218] Iteration 6800 (25.9107 iter/s, 3.85941s/100 iters), loss = 0.396457
I1123 15:23:56.476526 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:23:56.476526 22196 solver.cpp:237]     Train net output #1: loss = 0.396457 (* 1 = 0.396457 loss)
I1123 15:23:56.476526 22196 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1123 15:24:00.355546 22196 solver.cpp:218] Iteration 6900 (25.7885 iter/s, 3.87769s/100 iters), loss = 0.359405
I1123 15:24:00.355546 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:24:00.355546 22196 solver.cpp:237]     Train net output #1: loss = 0.359405 (* 1 = 0.359405 loss)
I1123 15:24:00.355546 22196 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1123 15:24:04.031730 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:24:04.187928 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_7000.caffemodel
I1123 15:24:04.197927 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_7000.solverstate
I1123 15:24:04.201927 22196 solver.cpp:330] Iteration 7000, Testing net (#0)
I1123 15:24:04.201927 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:24:05.267297 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:24:05.308595 22196 solver.cpp:397]     Test net output #0: accuracy = 0.837
I1123 15:24:05.308595 22196 solver.cpp:397]     Test net output #1: loss = 0.488392 (* 1 = 0.488392 loss)
I1123 15:24:05.345609 22196 solver.cpp:218] Iteration 7000 (20.0379 iter/s, 4.99054s/100 iters), loss = 0.272739
I1123 15:24:05.345609 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:24:05.346609 22196 solver.cpp:237]     Train net output #1: loss = 0.272739 (* 1 = 0.272739 loss)
I1123 15:24:05.346609 22196 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1123 15:24:09.208887 22196 solver.cpp:218] Iteration 7100 (25.8883 iter/s, 3.86275s/100 iters), loss = 0.292186
I1123 15:24:09.208887 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:24:09.208887 22196 solver.cpp:237]     Train net output #1: loss = 0.292186 (* 1 = 0.292186 loss)
I1123 15:24:09.208887 22196 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1123 15:24:13.055057 22196 solver.cpp:218] Iteration 7200 (26.0069 iter/s, 3.84514s/100 iters), loss = 0.464019
I1123 15:24:13.055057 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 15:24:13.055057 22196 solver.cpp:237]     Train net output #1: loss = 0.464019 (* 1 = 0.464019 loss)
I1123 15:24:13.055057 22196 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1123 15:24:16.899142 22196 solver.cpp:218] Iteration 7300 (26.0132 iter/s, 3.8442s/100 iters), loss = 0.366227
I1123 15:24:16.899142 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:24:16.899142 22196 solver.cpp:237]     Train net output #1: loss = 0.366227 (* 1 = 0.366227 loss)
I1123 15:24:16.899142 22196 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1123 15:24:20.746598 22196 solver.cpp:218] Iteration 7400 (25.9929 iter/s, 3.8472s/100 iters), loss = 0.358101
I1123 15:24:20.746598 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:24:20.746598 22196 solver.cpp:237]     Train net output #1: loss = 0.358101 (* 1 = 0.358101 loss)
I1123 15:24:20.746598 22196 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1123 15:24:24.410953 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:24:24.564127 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_7500.caffemodel
I1123 15:24:24.575134 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_7500.solverstate
I1123 15:24:24.579129 22196 solver.cpp:330] Iteration 7500, Testing net (#0)
I1123 15:24:24.579129 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:24:25.644179 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:24:25.686702 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8347
I1123 15:24:25.686702 22196 solver.cpp:397]     Test net output #1: loss = 0.495466 (* 1 = 0.495466 loss)
I1123 15:24:25.723094 22196 solver.cpp:218] Iteration 7500 (20.0952 iter/s, 4.9763s/100 iters), loss = 0.357227
I1123 15:24:25.723094 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:24:25.723094 22196 solver.cpp:237]     Train net output #1: loss = 0.357227 (* 1 = 0.357227 loss)
I1123 15:24:25.723094 22196 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1123 15:24:29.583542 22196 solver.cpp:218] Iteration 7600 (25.9098 iter/s, 3.85954s/100 iters), loss = 0.306954
I1123 15:24:29.583542 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:24:29.583542 22196 solver.cpp:237]     Train net output #1: loss = 0.306954 (* 1 = 0.306954 loss)
I1123 15:24:29.583542 22196 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1123 15:24:33.439736 22196 solver.cpp:218] Iteration 7700 (25.9372 iter/s, 3.85547s/100 iters), loss = 0.40905
I1123 15:24:33.439736 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 15:24:33.439736 22196 solver.cpp:237]     Train net output #1: loss = 0.40905 (* 1 = 0.40905 loss)
I1123 15:24:33.439736 22196 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1123 15:24:37.297873 22196 solver.cpp:218] Iteration 7800 (25.9182 iter/s, 3.85829s/100 iters), loss = 0.404547
I1123 15:24:37.297873 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:24:37.297873 22196 solver.cpp:237]     Train net output #1: loss = 0.404547 (* 1 = 0.404547 loss)
I1123 15:24:37.297873 22196 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1123 15:24:41.152820 22196 solver.cpp:218] Iteration 7900 (25.941 iter/s, 3.85491s/100 iters), loss = 0.354752
I1123 15:24:41.152820 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:24:41.153820 22196 solver.cpp:237]     Train net output #1: loss = 0.354752 (* 1 = 0.354752 loss)
I1123 15:24:41.153820 22196 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1123 15:24:44.819700 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:24:44.971233 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_8000.caffemodel
I1123 15:24:44.981233 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_8000.solverstate
I1123 15:24:44.986238 22196 solver.cpp:330] Iteration 8000, Testing net (#0)
I1123 15:24:44.986238 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:24:46.049762 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:24:46.092280 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8389
I1123 15:24:46.092780 22196 solver.cpp:397]     Test net output #1: loss = 0.470628 (* 1 = 0.470628 loss)
I1123 15:24:46.129523 22196 solver.cpp:218] Iteration 8000 (20.0977 iter/s, 4.97569s/100 iters), loss = 0.340026
I1123 15:24:46.129523 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 15:24:46.129523 22196 solver.cpp:237]     Train net output #1: loss = 0.340026 (* 1 = 0.340026 loss)
I1123 15:24:46.129523 22196 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1123 15:24:49.987957 22196 solver.cpp:218] Iteration 8100 (25.9196 iter/s, 3.85808s/100 iters), loss = 0.370809
I1123 15:24:49.987957 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:24:49.987957 22196 solver.cpp:237]     Train net output #1: loss = 0.370809 (* 1 = 0.370809 loss)
I1123 15:24:49.987957 22196 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1123 15:24:53.848160 22196 solver.cpp:218] Iteration 8200 (25.906 iter/s, 3.86012s/100 iters), loss = 0.417713
I1123 15:24:53.848160 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 15:24:53.848160 22196 solver.cpp:237]     Train net output #1: loss = 0.417713 (* 1 = 0.417713 loss)
I1123 15:24:53.848160 22196 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1123 15:24:57.715988 22196 solver.cpp:218] Iteration 8300 (25.8567 iter/s, 3.86746s/100 iters), loss = 0.351011
I1123 15:24:57.715988 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:24:57.715988 22196 solver.cpp:237]     Train net output #1: loss = 0.351011 (* 1 = 0.351011 loss)
I1123 15:24:57.715988 22196 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1123 15:25:01.576979 22196 solver.cpp:218] Iteration 8400 (25.9025 iter/s, 3.86063s/100 iters), loss = 0.336687
I1123 15:25:01.576979 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:25:01.576979 22196 solver.cpp:237]     Train net output #1: loss = 0.336687 (* 1 = 0.336687 loss)
I1123 15:25:01.576979 22196 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1123 15:25:05.236208 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:25:05.388273 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_8500.caffemodel
I1123 15:25:05.398763 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_8500.solverstate
I1123 15:25:05.402763 22196 solver.cpp:330] Iteration 8500, Testing net (#0)
I1123 15:25:05.403262 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:25:06.468168 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:25:06.510167 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8403
I1123 15:25:06.510167 22196 solver.cpp:397]     Test net output #1: loss = 0.476473 (* 1 = 0.476473 loss)
I1123 15:25:06.547202 22196 solver.cpp:218] Iteration 8500 (20.1238 iter/s, 4.96923s/100 iters), loss = 0.367389
I1123 15:25:06.547202 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:25:06.547202 22196 solver.cpp:237]     Train net output #1: loss = 0.367389 (* 1 = 0.367389 loss)
I1123 15:25:06.547202 22196 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1123 15:25:10.399111 22196 solver.cpp:218] Iteration 8600 (25.9584 iter/s, 3.85232s/100 iters), loss = 0.302576
I1123 15:25:10.399111 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 15:25:10.399111 22196 solver.cpp:237]     Train net output #1: loss = 0.302576 (* 1 = 0.302576 loss)
I1123 15:25:10.399111 22196 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1123 15:25:14.250272 22196 solver.cpp:218] Iteration 8700 (25.9716 iter/s, 3.85037s/100 iters), loss = 0.470868
I1123 15:25:14.250272 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:25:14.250272 22196 solver.cpp:237]     Train net output #1: loss = 0.470868 (* 1 = 0.470868 loss)
I1123 15:25:14.250272 22196 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1123 15:25:18.097092 22196 solver.cpp:218] Iteration 8800 (25.9953 iter/s, 3.84685s/100 iters), loss = 0.319666
I1123 15:25:18.097092 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:25:18.098079 22196 solver.cpp:237]     Train net output #1: loss = 0.319666 (* 1 = 0.319666 loss)
I1123 15:25:18.098079 22196 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1123 15:25:21.975525 22196 solver.cpp:218] Iteration 8900 (25.7908 iter/s, 3.87736s/100 iters), loss = 0.315496
I1123 15:25:21.975525 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:25:21.975525 22196 solver.cpp:237]     Train net output #1: loss = 0.315496 (* 1 = 0.315496 loss)
I1123 15:25:21.975525 22196 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1123 15:25:25.651275 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:25:25.802438 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_9000.caffemodel
I1123 15:25:25.812419 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_9000.solverstate
I1123 15:25:25.816418 22196 solver.cpp:330] Iteration 9000, Testing net (#0)
I1123 15:25:25.816418 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:25:26.879573 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:25:26.921569 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8409
I1123 15:25:26.921569 22196 solver.cpp:397]     Test net output #1: loss = 0.464231 (* 1 = 0.464231 loss)
I1123 15:25:26.959022 22196 solver.cpp:218] Iteration 9000 (20.0677 iter/s, 4.98313s/100 iters), loss = 0.372241
I1123 15:25:26.959022 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:25:26.959022 22196 solver.cpp:237]     Train net output #1: loss = 0.372241 (* 1 = 0.372241 loss)
I1123 15:25:26.959022 22196 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1123 15:25:30.819795 22196 solver.cpp:218] Iteration 9100 (25.9027 iter/s, 3.8606s/100 iters), loss = 0.312815
I1123 15:25:30.819795 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:25:30.819795 22196 solver.cpp:237]     Train net output #1: loss = 0.312815 (* 1 = 0.312815 loss)
I1123 15:25:30.819795 22196 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1123 15:25:34.667230 22196 solver.cpp:218] Iteration 9200 (25.9926 iter/s, 3.84725s/100 iters), loss = 0.340168
I1123 15:25:34.667230 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:25:34.667230 22196 solver.cpp:237]     Train net output #1: loss = 0.340168 (* 1 = 0.340168 loss)
I1123 15:25:34.667230 22196 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1123 15:25:38.513665 22196 solver.cpp:218] Iteration 9300 (26.0045 iter/s, 3.84549s/100 iters), loss = 0.342582
I1123 15:25:38.513665 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:25:38.513665 22196 solver.cpp:237]     Train net output #1: loss = 0.342582 (* 1 = 0.342582 loss)
I1123 15:25:38.513665 22196 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1123 15:25:42.359233 22196 solver.cpp:218] Iteration 9400 (26.0024 iter/s, 3.8458s/100 iters), loss = 0.262398
I1123 15:25:42.359233 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:25:42.359233 22196 solver.cpp:237]     Train net output #1: loss = 0.262398 (* 1 = 0.262398 loss)
I1123 15:25:42.359233 22196 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1123 15:25:46.030786 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:25:46.181401 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_9500.caffemodel
I1123 15:25:46.192385 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_9500.solverstate
I1123 15:25:46.196388 22196 solver.cpp:330] Iteration 9500, Testing net (#0)
I1123 15:25:46.196388 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:25:47.258704 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:25:47.300688 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8362
I1123 15:25:47.300688 22196 solver.cpp:397]     Test net output #1: loss = 0.480826 (* 1 = 0.480826 loss)
I1123 15:25:47.337688 22196 solver.cpp:218] Iteration 9500 (20.0892 iter/s, 4.9778s/100 iters), loss = 0.333791
I1123 15:25:47.337688 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:25:47.337688 22196 solver.cpp:237]     Train net output #1: loss = 0.333791 (* 1 = 0.333791 loss)
I1123 15:25:47.337688 22196 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1123 15:25:47.337688 22196 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1123 15:25:51.184741 22196 solver.cpp:218] Iteration 9600 (25.9981 iter/s, 3.84643s/100 iters), loss = 0.277214
I1123 15:25:51.184741 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:25:51.184741 22196 solver.cpp:237]     Train net output #1: loss = 0.277214 (* 1 = 0.277214 loss)
I1123 15:25:51.184741 22196 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1123 15:25:55.035908 22196 solver.cpp:218] Iteration 9700 (25.9646 iter/s, 3.85139s/100 iters), loss = 0.340324
I1123 15:25:55.035908 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:25:55.035908 22196 solver.cpp:237]     Train net output #1: loss = 0.340324 (* 1 = 0.340324 loss)
I1123 15:25:55.035908 22196 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1123 15:25:58.894466 22196 solver.cpp:218] Iteration 9800 (25.9249 iter/s, 3.8573s/100 iters), loss = 0.326382
I1123 15:25:58.894466 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:25:58.894466 22196 solver.cpp:237]     Train net output #1: loss = 0.326382 (* 1 = 0.326382 loss)
I1123 15:25:58.894466 22196 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1123 15:26:02.743348 22196 solver.cpp:218] Iteration 9900 (25.9812 iter/s, 3.84894s/100 iters), loss = 0.336947
I1123 15:26:02.743348 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:26:02.743348 22196 solver.cpp:237]     Train net output #1: loss = 0.336947 (* 1 = 0.336947 loss)
I1123 15:26:02.743348 22196 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1123 15:26:06.404803 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:26:06.555349 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_10000.caffemodel
I1123 15:26:06.566354 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_10000.solverstate
I1123 15:26:06.570354 22196 solver.cpp:330] Iteration 10000, Testing net (#0)
I1123 15:26:06.570354 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:26:07.635880 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:26:07.677894 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8546
I1123 15:26:07.677894 22196 solver.cpp:397]     Test net output #1: loss = 0.428695 (* 1 = 0.428695 loss)
I1123 15:26:07.714536 22196 solver.cpp:218] Iteration 10000 (20.117 iter/s, 4.97091s/100 iters), loss = 0.267242
I1123 15:26:07.714536 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:26:07.714536 22196 solver.cpp:237]     Train net output #1: loss = 0.267242 (* 1 = 0.267242 loss)
I1123 15:26:07.714536 22196 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1123 15:26:11.567380 22196 solver.cpp:218] Iteration 10100 (25.9591 iter/s, 3.85222s/100 iters), loss = 0.285474
I1123 15:26:11.567380 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:26:11.567380 22196 solver.cpp:237]     Train net output #1: loss = 0.285474 (* 1 = 0.285474 loss)
I1123 15:26:11.567380 22196 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1123 15:26:15.418521 22196 solver.cpp:218] Iteration 10200 (25.9682 iter/s, 3.85087s/100 iters), loss = 0.362037
I1123 15:26:15.418521 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:26:15.418521 22196 solver.cpp:237]     Train net output #1: loss = 0.362037 (* 1 = 0.362037 loss)
I1123 15:26:15.418521 22196 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1123 15:26:19.269568 22196 solver.cpp:218] Iteration 10300 (25.9673 iter/s, 3.851s/100 iters), loss = 0.310979
I1123 15:26:19.269568 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:26:19.269568 22196 solver.cpp:237]     Train net output #1: loss = 0.310979 (* 1 = 0.310979 loss)
I1123 15:26:19.269568 22196 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1123 15:26:23.124255 22196 solver.cpp:218] Iteration 10400 (25.9458 iter/s, 3.85419s/100 iters), loss = 0.212783
I1123 15:26:23.124255 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 15:26:23.124255 22196 solver.cpp:237]     Train net output #1: loss = 0.212783 (* 1 = 0.212783 loss)
I1123 15:26:23.124255 22196 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1123 15:26:26.799058 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:26:26.952174 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_10500.caffemodel
I1123 15:26:26.962151 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_10500.solverstate
I1123 15:26:26.966150 22196 solver.cpp:330] Iteration 10500, Testing net (#0)
I1123 15:26:26.966150 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:26:28.029196 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:26:28.071179 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8556
I1123 15:26:28.071179 22196 solver.cpp:397]     Test net output #1: loss = 0.427641 (* 1 = 0.427641 loss)
I1123 15:26:28.107414 22196 solver.cpp:218] Iteration 10500 (20.0677 iter/s, 4.98314s/100 iters), loss = 0.310118
I1123 15:26:28.108395 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:26:28.108395 22196 solver.cpp:237]     Train net output #1: loss = 0.310118 (* 1 = 0.310118 loss)
I1123 15:26:28.108395 22196 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1123 15:26:31.962963 22196 solver.cpp:218] Iteration 10600 (25.9391 iter/s, 3.85518s/100 iters), loss = 0.280343
I1123 15:26:31.963951 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:26:31.963951 22196 solver.cpp:237]     Train net output #1: loss = 0.280343 (* 1 = 0.280343 loss)
I1123 15:26:31.963951 22196 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1123 15:26:35.822314 22196 solver.cpp:218] Iteration 10700 (25.9192 iter/s, 3.85814s/100 iters), loss = 0.285437
I1123 15:26:35.822314 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:26:35.822314 22196 solver.cpp:237]     Train net output #1: loss = 0.285437 (* 1 = 0.285437 loss)
I1123 15:26:35.822314 22196 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1123 15:26:39.683990 22196 solver.cpp:218] Iteration 10800 (25.8926 iter/s, 3.86211s/100 iters), loss = 0.309381
I1123 15:26:39.683990 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:26:39.683990 22196 solver.cpp:237]     Train net output #1: loss = 0.309381 (* 1 = 0.309381 loss)
I1123 15:26:39.683990 22196 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1123 15:26:43.539940 22196 solver.cpp:218] Iteration 10900 (25.9388 iter/s, 3.85523s/100 iters), loss = 0.284503
I1123 15:26:43.539940 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:26:43.539940 22196 solver.cpp:237]     Train net output #1: loss = 0.284503 (* 1 = 0.284503 loss)
I1123 15:26:43.539940 22196 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1123 15:26:47.225070 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:26:47.376458 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_11000.caffemodel
I1123 15:26:47.386457 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_11000.solverstate
I1123 15:26:47.390460 22196 solver.cpp:330] Iteration 11000, Testing net (#0)
I1123 15:26:47.390460 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:26:48.452816 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:26:48.494822 22196 solver.cpp:397]     Test net output #0: accuracy = 0.854
I1123 15:26:48.494822 22196 solver.cpp:397]     Test net output #1: loss = 0.428346 (* 1 = 0.428346 loss)
I1123 15:26:48.531042 22196 solver.cpp:218] Iteration 11000 (20.0367 iter/s, 4.99084s/100 iters), loss = 0.258041
I1123 15:26:48.531042 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:26:48.531042 22196 solver.cpp:237]     Train net output #1: loss = 0.258041 (* 1 = 0.258041 loss)
I1123 15:26:48.531042 22196 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1123 15:26:52.406515 22196 solver.cpp:218] Iteration 11100 (25.8094 iter/s, 3.87455s/100 iters), loss = 0.325259
I1123 15:26:52.406515 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:26:52.406515 22196 solver.cpp:237]     Train net output #1: loss = 0.325259 (* 1 = 0.325259 loss)
I1123 15:26:52.406515 22196 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1123 15:26:56.258142 22196 solver.cpp:218] Iteration 11200 (25.9647 iter/s, 3.85138s/100 iters), loss = 0.295372
I1123 15:26:56.258142 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:26:56.258142 22196 solver.cpp:237]     Train net output #1: loss = 0.295372 (* 1 = 0.295372 loss)
I1123 15:26:56.258142 22196 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1123 15:27:00.107214 22196 solver.cpp:218] Iteration 11300 (25.9826 iter/s, 3.84873s/100 iters), loss = 0.289833
I1123 15:27:00.107214 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:27:00.107214 22196 solver.cpp:237]     Train net output #1: loss = 0.289833 (* 1 = 0.289833 loss)
I1123 15:27:00.107214 22196 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1123 15:27:03.959743 22196 solver.cpp:218] Iteration 11400 (25.9557 iter/s, 3.85273s/100 iters), loss = 0.289572
I1123 15:27:03.959743 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:27:03.960729 22196 solver.cpp:237]     Train net output #1: loss = 0.289572 (* 1 = 0.289572 loss)
I1123 15:27:03.960729 22196 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1123 15:27:07.633585 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:27:07.788599 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_11500.caffemodel
I1123 15:27:07.798606 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_11500.solverstate
I1123 15:27:07.802599 22196 solver.cpp:330] Iteration 11500, Testing net (#0)
I1123 15:27:07.802599 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:27:08.865422 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:27:08.907447 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8546
I1123 15:27:08.907918 22196 solver.cpp:397]     Test net output #1: loss = 0.427033 (* 1 = 0.427033 loss)
I1123 15:27:08.944051 22196 solver.cpp:218] Iteration 11500 (20.0658 iter/s, 4.98361s/100 iters), loss = 0.258386
I1123 15:27:08.944051 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:27:08.944051 22196 solver.cpp:237]     Train net output #1: loss = 0.258386 (* 1 = 0.258386 loss)
I1123 15:27:08.944051 22196 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1123 15:27:12.796964 22196 solver.cpp:218] Iteration 11600 (25.958 iter/s, 3.85238s/100 iters), loss = 0.309272
I1123 15:27:12.796964 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:27:12.796964 22196 solver.cpp:237]     Train net output #1: loss = 0.309272 (* 1 = 0.309272 loss)
I1123 15:27:12.796964 22196 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1123 15:27:16.657778 22196 solver.cpp:218] Iteration 11700 (25.9009 iter/s, 3.86087s/100 iters), loss = 0.378173
I1123 15:27:16.657778 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:27:16.657778 22196 solver.cpp:237]     Train net output #1: loss = 0.378173 (* 1 = 0.378173 loss)
I1123 15:27:16.657778 22196 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1123 15:27:20.513315 22196 solver.cpp:218] Iteration 11800 (25.9439 iter/s, 3.85447s/100 iters), loss = 0.300063
I1123 15:27:20.513315 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:27:20.513315 22196 solver.cpp:237]     Train net output #1: loss = 0.300063 (* 1 = 0.300063 loss)
I1123 15:27:20.513315 22196 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1123 15:27:24.369874 22196 solver.cpp:218] Iteration 11900 (25.9314 iter/s, 3.85633s/100 iters), loss = 0.274815
I1123 15:27:24.369874 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:27:24.369874 22196 solver.cpp:237]     Train net output #1: loss = 0.274815 (* 1 = 0.274815 loss)
I1123 15:27:24.369874 22196 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1123 15:27:28.028195 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:27:28.179236 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_12000.caffemodel
I1123 15:27:28.190218 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_12000.solverstate
I1123 15:27:28.194218 22196 solver.cpp:330] Iteration 12000, Testing net (#0)
I1123 15:27:28.194218 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:27:29.256386 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:27:29.298384 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8561
I1123 15:27:29.298384 22196 solver.cpp:397]     Test net output #1: loss = 0.426671 (* 1 = 0.426671 loss)
I1123 15:27:29.334874 22196 solver.cpp:218] Iteration 12000 (20.1388 iter/s, 4.96554s/100 iters), loss = 0.306613
I1123 15:27:29.334874 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:27:29.334874 22196 solver.cpp:237]     Train net output #1: loss = 0.306613 (* 1 = 0.306613 loss)
I1123 15:27:29.334874 22196 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1123 15:27:33.177659 22196 solver.cpp:218] Iteration 12100 (26.0327 iter/s, 3.84133s/100 iters), loss = 0.265663
I1123 15:27:33.177659 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:27:33.177659 22196 solver.cpp:237]     Train net output #1: loss = 0.265663 (* 1 = 0.265663 loss)
I1123 15:27:33.177659 22196 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1123 15:27:37.026715 22196 solver.cpp:218] Iteration 12200 (25.9832 iter/s, 3.84864s/100 iters), loss = 0.295779
I1123 15:27:37.026715 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:27:37.026715 22196 solver.cpp:237]     Train net output #1: loss = 0.295779 (* 1 = 0.295779 loss)
I1123 15:27:37.026715 22196 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1123 15:27:40.881711 22196 solver.cpp:218] Iteration 12300 (25.9424 iter/s, 3.85469s/100 iters), loss = 0.266796
I1123 15:27:40.881711 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:27:40.881711 22196 solver.cpp:237]     Train net output #1: loss = 0.266796 (* 1 = 0.266796 loss)
I1123 15:27:40.881711 22196 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1123 15:27:44.791362 22196 solver.cpp:218] Iteration 12400 (25.5806 iter/s, 3.90921s/100 iters), loss = 0.246403
I1123 15:27:44.791362 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:27:44.791362 22196 solver.cpp:237]     Train net output #1: loss = 0.246403 (* 1 = 0.246403 loss)
I1123 15:27:44.791362 22196 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1123 15:27:48.528713 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:27:48.683235 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_12500.caffemodel
I1123 15:27:48.696235 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_12500.solverstate
I1123 15:27:48.700237 22196 solver.cpp:330] Iteration 12500, Testing net (#0)
I1123 15:27:48.700237 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:27:49.777477 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:27:49.819468 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8549
I1123 15:27:49.819468 22196 solver.cpp:397]     Test net output #1: loss = 0.426021 (* 1 = 0.426021 loss)
I1123 15:27:49.856485 22196 solver.cpp:218] Iteration 12500 (19.7431 iter/s, 5.06507s/100 iters), loss = 0.232983
I1123 15:27:49.856485 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:27:49.856485 22196 solver.cpp:237]     Train net output #1: loss = 0.232983 (* 1 = 0.232983 loss)
I1123 15:27:49.856485 22196 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1123 15:27:53.735813 22196 solver.cpp:218] Iteration 12600 (25.7822 iter/s, 3.87865s/100 iters), loss = 0.266909
I1123 15:27:53.735813 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:27:53.735813 22196 solver.cpp:237]     Train net output #1: loss = 0.266909 (* 1 = 0.266909 loss)
I1123 15:27:53.735813 22196 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1123 15:27:57.620895 22196 solver.cpp:218] Iteration 12700 (25.7415 iter/s, 3.88478s/100 iters), loss = 0.305393
I1123 15:27:57.620895 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:27:57.620895 22196 solver.cpp:237]     Train net output #1: loss = 0.305393 (* 1 = 0.305393 loss)
I1123 15:27:57.620895 22196 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1123 15:28:01.494863 22196 solver.cpp:218] Iteration 12800 (25.8163 iter/s, 3.87352s/100 iters), loss = 0.284347
I1123 15:28:01.494863 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:28:01.494863 22196 solver.cpp:237]     Train net output #1: loss = 0.284347 (* 1 = 0.284347 loss)
I1123 15:28:01.494863 22196 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1123 15:28:05.395393 22196 solver.cpp:218] Iteration 12900 (25.641 iter/s, 3.9s/100 iters), loss = 0.239569
I1123 15:28:05.395393 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:28:05.395393 22196 solver.cpp:237]     Train net output #1: loss = 0.239568 (* 1 = 0.239568 loss)
I1123 15:28:05.395393 22196 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1123 15:28:09.104488 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:28:09.254984 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_13000.caffemodel
I1123 15:28:09.264982 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_13000.solverstate
I1123 15:28:09.270983 22196 solver.cpp:330] Iteration 13000, Testing net (#0)
I1123 15:28:09.270983 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:28:10.341578 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:28:10.383608 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8558
I1123 15:28:10.383608 22196 solver.cpp:397]     Test net output #1: loss = 0.425035 (* 1 = 0.425035 loss)
I1123 15:28:10.419610 22196 solver.cpp:218] Iteration 13000 (19.9032 iter/s, 5.02431s/100 iters), loss = 0.345563
I1123 15:28:10.419610 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:28:10.419610 22196 solver.cpp:237]     Train net output #1: loss = 0.345563 (* 1 = 0.345563 loss)
I1123 15:28:10.419610 22196 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1123 15:28:14.279949 22196 solver.cpp:218] Iteration 13100 (25.9121 iter/s, 3.85921s/100 iters), loss = 0.269912
I1123 15:28:14.279949 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:28:14.279949 22196 solver.cpp:237]     Train net output #1: loss = 0.269912 (* 1 = 0.269912 loss)
I1123 15:28:14.279949 22196 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1123 15:28:18.137771 22196 solver.cpp:218] Iteration 13200 (25.9237 iter/s, 3.85748s/100 iters), loss = 0.343979
I1123 15:28:18.137771 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:28:18.137771 22196 solver.cpp:237]     Train net output #1: loss = 0.343978 (* 1 = 0.343978 loss)
I1123 15:28:18.137771 22196 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1123 15:28:22.021209 22196 solver.cpp:218] Iteration 13300 (25.753 iter/s, 3.88304s/100 iters), loss = 0.298535
I1123 15:28:22.021209 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:28:22.021209 22196 solver.cpp:237]     Train net output #1: loss = 0.298535 (* 1 = 0.298535 loss)
I1123 15:28:22.021209 22196 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1123 15:28:25.866484 22196 solver.cpp:218] Iteration 13400 (26.0047 iter/s, 3.84547s/100 iters), loss = 0.239721
I1123 15:28:25.866484 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:28:25.866484 22196 solver.cpp:237]     Train net output #1: loss = 0.23972 (* 1 = 0.23972 loss)
I1123 15:28:25.866484 22196 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1123 15:28:29.542465 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:28:29.694322 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_13500.caffemodel
I1123 15:28:29.704917 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_13500.solverstate
I1123 15:28:29.708916 22196 solver.cpp:330] Iteration 13500, Testing net (#0)
I1123 15:28:29.708916 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:28:30.771230 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:28:30.813112 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8556
I1123 15:28:30.813112 22196 solver.cpp:397]     Test net output #1: loss = 0.427003 (* 1 = 0.427003 loss)
I1123 15:28:30.850111 22196 solver.cpp:218] Iteration 13500 (20.0692 iter/s, 4.98276s/100 iters), loss = 0.234094
I1123 15:28:30.850111 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:28:30.850111 22196 solver.cpp:237]     Train net output #1: loss = 0.234094 (* 1 = 0.234094 loss)
I1123 15:28:30.850111 22196 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1123 15:28:34.705356 22196 solver.cpp:218] Iteration 13600 (25.9383 iter/s, 3.8553s/100 iters), loss = 0.311516
I1123 15:28:34.705356 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:28:34.705356 22196 solver.cpp:237]     Train net output #1: loss = 0.311516 (* 1 = 0.311516 loss)
I1123 15:28:34.705356 22196 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1123 15:28:38.548048 22196 solver.cpp:218] Iteration 13700 (26.0257 iter/s, 3.84235s/100 iters), loss = 0.317621
I1123 15:28:38.548048 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:28:38.548048 22196 solver.cpp:237]     Train net output #1: loss = 0.317621 (* 1 = 0.317621 loss)
I1123 15:28:38.548048 22196 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1123 15:28:42.394778 22196 solver.cpp:218] Iteration 13800 (26.0018 iter/s, 3.84589s/100 iters), loss = 0.288749
I1123 15:28:42.394778 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:28:42.394778 22196 solver.cpp:237]     Train net output #1: loss = 0.288749 (* 1 = 0.288749 loss)
I1123 15:28:42.394778 22196 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1123 15:28:46.248898 22196 solver.cpp:218] Iteration 13900 (25.9451 iter/s, 3.85429s/100 iters), loss = 0.239223
I1123 15:28:46.248898 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:28:46.248898 22196 solver.cpp:237]     Train net output #1: loss = 0.239223 (* 1 = 0.239223 loss)
I1123 15:28:46.248898 22196 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1123 15:28:49.914983 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:28:50.065505 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_14000.caffemodel
I1123 15:28:50.075485 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_14000.solverstate
I1123 15:28:50.079484 22196 solver.cpp:330] Iteration 14000, Testing net (#0)
I1123 15:28:50.079484 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:28:51.141892 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:28:51.182879 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8562
I1123 15:28:51.182879 22196 solver.cpp:397]     Test net output #1: loss = 0.424116 (* 1 = 0.424116 loss)
I1123 15:28:51.220433 22196 solver.cpp:218] Iteration 14000 (20.1187 iter/s, 4.97051s/100 iters), loss = 0.242805
I1123 15:28:51.220433 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:28:51.220433 22196 solver.cpp:237]     Train net output #1: loss = 0.242804 (* 1 = 0.242804 loss)
I1123 15:28:51.220433 22196 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1123 15:28:55.076089 22196 solver.cpp:218] Iteration 14100 (25.9327 iter/s, 3.85614s/100 iters), loss = 0.321651
I1123 15:28:55.076089 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:28:55.076089 22196 solver.cpp:237]     Train net output #1: loss = 0.321651 (* 1 = 0.321651 loss)
I1123 15:28:55.077088 22196 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1123 15:28:58.922183 22196 solver.cpp:218] Iteration 14200 (26.0086 iter/s, 3.84488s/100 iters), loss = 0.295939
I1123 15:28:58.922183 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:28:58.922183 22196 solver.cpp:237]     Train net output #1: loss = 0.295939 (* 1 = 0.295939 loss)
I1123 15:28:58.922183 22196 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1123 15:29:02.763386 22196 solver.cpp:218] Iteration 14300 (26.0326 iter/s, 3.84134s/100 iters), loss = 0.266104
I1123 15:29:02.763386 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:29:02.763386 22196 solver.cpp:237]     Train net output #1: loss = 0.266103 (* 1 = 0.266103 loss)
I1123 15:29:02.763386 22196 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1123 15:29:06.607264 22196 solver.cpp:218] Iteration 14400 (26.0215 iter/s, 3.84297s/100 iters), loss = 0.227831
I1123 15:29:06.607264 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 15:29:06.607264 22196 solver.cpp:237]     Train net output #1: loss = 0.227831 (* 1 = 0.227831 loss)
I1123 15:29:06.607264 22196 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1123 15:29:10.265992 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:29:10.416468 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_14500.caffemodel
I1123 15:29:10.425954 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_14500.solverstate
I1123 15:29:10.429972 22196 solver.cpp:330] Iteration 14500, Testing net (#0)
I1123 15:29:10.429972 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:29:11.491150 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:29:11.533162 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8556
I1123 15:29:11.533162 22196 solver.cpp:397]     Test net output #1: loss = 0.423856 (* 1 = 0.423856 loss)
I1123 15:29:11.570176 22196 solver.cpp:218] Iteration 14500 (20.1493 iter/s, 4.96295s/100 iters), loss = 0.27576
I1123 15:29:11.570176 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:29:11.570176 22196 solver.cpp:237]     Train net output #1: loss = 0.27576 (* 1 = 0.27576 loss)
I1123 15:29:11.570176 22196 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1123 15:29:15.414280 22196 solver.cpp:218] Iteration 14600 (26.0188 iter/s, 3.84337s/100 iters), loss = 0.295269
I1123 15:29:15.414280 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:29:15.414280 22196 solver.cpp:237]     Train net output #1: loss = 0.295269 (* 1 = 0.295269 loss)
I1123 15:29:15.414280 22196 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1123 15:29:19.258183 22196 solver.cpp:218] Iteration 14700 (26.014 iter/s, 3.84408s/100 iters), loss = 0.316673
I1123 15:29:19.258183 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:29:19.258183 22196 solver.cpp:237]     Train net output #1: loss = 0.316673 (* 1 = 0.316673 loss)
I1123 15:29:19.258183 22196 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1123 15:29:23.107673 22196 solver.cpp:218] Iteration 14800 (25.9811 iter/s, 3.84895s/100 iters), loss = 0.274266
I1123 15:29:23.107673 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:29:23.107673 22196 solver.cpp:237]     Train net output #1: loss = 0.274266 (* 1 = 0.274266 loss)
I1123 15:29:23.107673 22196 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1123 15:29:26.950801 22196 solver.cpp:218] Iteration 14900 (26.0214 iter/s, 3.84299s/100 iters), loss = 0.277193
I1123 15:29:26.950801 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:29:26.950801 22196 solver.cpp:237]     Train net output #1: loss = 0.277193 (* 1 = 0.277193 loss)
I1123 15:29:26.950801 22196 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1123 15:29:30.607247 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:29:30.757309 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_15000.caffemodel
I1123 15:29:30.767309 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_15000.solverstate
I1123 15:29:30.771309 22196 solver.cpp:330] Iteration 15000, Testing net (#0)
I1123 15:29:30.771309 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:29:31.834995 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:29:31.877979 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8541
I1123 15:29:31.877979 22196 solver.cpp:397]     Test net output #1: loss = 0.424218 (* 1 = 0.424218 loss)
I1123 15:29:31.915989 22196 solver.cpp:218] Iteration 15000 (20.1437 iter/s, 4.96432s/100 iters), loss = 0.314758
I1123 15:29:31.915989 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:29:31.915989 22196 solver.cpp:237]     Train net output #1: loss = 0.314758 (* 1 = 0.314758 loss)
I1123 15:29:31.915989 22196 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1123 15:29:35.765607 22196 solver.cpp:218] Iteration 15100 (25.9775 iter/s, 3.84948s/100 iters), loss = 0.173639
I1123 15:29:35.765607 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 15:29:35.765607 22196 solver.cpp:237]     Train net output #1: loss = 0.173639 (* 1 = 0.173639 loss)
I1123 15:29:35.765607 22196 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1123 15:29:39.612713 22196 solver.cpp:218] Iteration 15200 (25.9988 iter/s, 3.84632s/100 iters), loss = 0.359816
I1123 15:29:39.612713 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:29:39.612713 22196 solver.cpp:237]     Train net output #1: loss = 0.359816 (* 1 = 0.359816 loss)
I1123 15:29:39.612713 22196 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1123 15:29:43.459547 22196 solver.cpp:218] Iteration 15300 (25.9954 iter/s, 3.84683s/100 iters), loss = 0.303159
I1123 15:29:43.459547 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:29:43.459547 22196 solver.cpp:237]     Train net output #1: loss = 0.303158 (* 1 = 0.303158 loss)
I1123 15:29:43.459547 22196 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1123 15:29:43.459547 22196 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1123 15:29:47.310256 22196 solver.cpp:218] Iteration 15400 (25.9724 iter/s, 3.85024s/100 iters), loss = 0.26724
I1123 15:29:47.310256 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:29:47.310256 22196 solver.cpp:237]     Train net output #1: loss = 0.26724 (* 1 = 0.26724 loss)
I1123 15:29:47.310256 22196 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1123 15:29:50.967213 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:29:51.118424 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_15500.caffemodel
I1123 15:29:51.128410 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_15500.solverstate
I1123 15:29:51.132918 22196 solver.cpp:330] Iteration 15500, Testing net (#0)
I1123 15:29:51.132918 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:29:52.196564 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:29:52.238065 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8577
I1123 15:29:52.238065 22196 solver.cpp:397]     Test net output #1: loss = 0.422687 (* 1 = 0.422687 loss)
I1123 15:29:52.274592 22196 solver.cpp:218] Iteration 15500 (20.1436 iter/s, 4.96436s/100 iters), loss = 0.265034
I1123 15:29:52.274592 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:29:52.274592 22196 solver.cpp:237]     Train net output #1: loss = 0.265034 (* 1 = 0.265034 loss)
I1123 15:29:52.274592 22196 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1123 15:29:56.130439 22196 solver.cpp:218] Iteration 15600 (25.9381 iter/s, 3.85534s/100 iters), loss = 0.228183
I1123 15:29:56.130439 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 15:29:56.130439 22196 solver.cpp:237]     Train net output #1: loss = 0.228183 (* 1 = 0.228183 loss)
I1123 15:29:56.130439 22196 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1123 15:29:59.976915 22196 solver.cpp:218] Iteration 15700 (26.0041 iter/s, 3.84554s/100 iters), loss = 0.319588
I1123 15:29:59.976915 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:29:59.976915 22196 solver.cpp:237]     Train net output #1: loss = 0.319587 (* 1 = 0.319587 loss)
I1123 15:29:59.976915 22196 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1123 15:30:03.856197 22196 solver.cpp:218] Iteration 15800 (25.7734 iter/s, 3.87997s/100 iters), loss = 0.244997
I1123 15:30:03.856197 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:30:03.856197 22196 solver.cpp:237]     Train net output #1: loss = 0.244996 (* 1 = 0.244996 loss)
I1123 15:30:03.856197 22196 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1123 15:30:07.702525 22196 solver.cpp:218] Iteration 15900 (26.0037 iter/s, 3.84561s/100 iters), loss = 0.225
I1123 15:30:07.702525 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:30:07.702525 22196 solver.cpp:237]     Train net output #1: loss = 0.225 (* 1 = 0.225 loss)
I1123 15:30:07.702525 22196 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1123 15:30:11.363145 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:30:11.514144 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_16000.caffemodel
I1123 15:30:11.524147 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_16000.solverstate
I1123 15:30:11.528159 22196 solver.cpp:330] Iteration 16000, Testing net (#0)
I1123 15:30:11.528159 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:30:12.589565 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:30:12.630565 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8572
I1123 15:30:12.630565 22196 solver.cpp:397]     Test net output #1: loss = 0.422261 (* 1 = 0.422261 loss)
I1123 15:30:12.667125 22196 solver.cpp:218] Iteration 16000 (20.1442 iter/s, 4.96422s/100 iters), loss = 0.267582
I1123 15:30:12.667125 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:30:12.667125 22196 solver.cpp:237]     Train net output #1: loss = 0.267582 (* 1 = 0.267582 loss)
I1123 15:30:12.667125 22196 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1123 15:30:16.513701 22196 solver.cpp:218] Iteration 16100 (26.0027 iter/s, 3.84576s/100 iters), loss = 0.282983
I1123 15:30:16.513701 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:30:16.513701 22196 solver.cpp:237]     Train net output #1: loss = 0.282983 (* 1 = 0.282983 loss)
I1123 15:30:16.513701 22196 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1123 15:30:20.355078 22196 solver.cpp:218] Iteration 16200 (26.0357 iter/s, 3.84088s/100 iters), loss = 0.322039
I1123 15:30:20.355078 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:30:20.355078 22196 solver.cpp:237]     Train net output #1: loss = 0.322038 (* 1 = 0.322038 loss)
I1123 15:30:20.355078 22196 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1123 15:30:24.196983 22196 solver.cpp:218] Iteration 16300 (26.0307 iter/s, 3.84162s/100 iters), loss = 0.291278
I1123 15:30:24.196983 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:30:24.196983 22196 solver.cpp:237]     Train net output #1: loss = 0.291278 (* 1 = 0.291278 loss)
I1123 15:30:24.196983 22196 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1123 15:30:28.041365 22196 solver.cpp:218] Iteration 16400 (26.0163 iter/s, 3.84374s/100 iters), loss = 0.239857
I1123 15:30:28.041365 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:30:28.041365 22196 solver.cpp:237]     Train net output #1: loss = 0.239856 (* 1 = 0.239856 loss)
I1123 15:30:28.041365 22196 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1123 15:30:31.698679 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:30:31.849792 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_16500.caffemodel
I1123 15:30:31.860293 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_16500.solverstate
I1123 15:30:31.864295 22196 solver.cpp:330] Iteration 16500, Testing net (#0)
I1123 15:30:31.864295 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:30:32.924687 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:30:32.967193 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8579
I1123 15:30:32.967193 22196 solver.cpp:397]     Test net output #1: loss = 0.421845 (* 1 = 0.421845 loss)
I1123 15:30:33.003720 22196 solver.cpp:218] Iteration 16500 (20.1518 iter/s, 4.96234s/100 iters), loss = 0.293828
I1123 15:30:33.003720 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:30:33.003720 22196 solver.cpp:237]     Train net output #1: loss = 0.293828 (* 1 = 0.293828 loss)
I1123 15:30:33.003720 22196 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1123 15:30:36.858309 22196 solver.cpp:218] Iteration 16600 (25.946 iter/s, 3.85415s/100 iters), loss = 0.292974
I1123 15:30:36.858309 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:30:36.858825 22196 solver.cpp:237]     Train net output #1: loss = 0.292974 (* 1 = 0.292974 loss)
I1123 15:30:36.858825 22196 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1123 15:30:40.716519 22196 solver.cpp:218] Iteration 16700 (25.9246 iter/s, 3.85734s/100 iters), loss = 0.334328
I1123 15:30:40.716519 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:30:40.716519 22196 solver.cpp:237]     Train net output #1: loss = 0.334327 (* 1 = 0.334327 loss)
I1123 15:30:40.716519 22196 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1123 15:30:44.569236 22196 solver.cpp:218] Iteration 16800 (25.9561 iter/s, 3.85266s/100 iters), loss = 0.284991
I1123 15:30:44.569236 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:30:44.569236 22196 solver.cpp:237]     Train net output #1: loss = 0.284991 (* 1 = 0.284991 loss)
I1123 15:30:44.569236 22196 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1123 15:30:48.420254 22196 solver.cpp:218] Iteration 16900 (25.9665 iter/s, 3.85112s/100 iters), loss = 0.27571
I1123 15:30:48.420254 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:30:48.420254 22196 solver.cpp:237]     Train net output #1: loss = 0.27571 (* 1 = 0.27571 loss)
I1123 15:30:48.420254 22196 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1123 15:30:52.101143 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:30:52.253458 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_17000.caffemodel
I1123 15:30:52.263448 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_17000.solverstate
I1123 15:30:52.267449 22196 solver.cpp:330] Iteration 17000, Testing net (#0)
I1123 15:30:52.267449 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:30:53.328462 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:30:53.370476 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8584
I1123 15:30:53.370476 22196 solver.cpp:397]     Test net output #1: loss = 0.421214 (* 1 = 0.421214 loss)
I1123 15:30:53.406996 22196 solver.cpp:218] Iteration 17000 (20.0552 iter/s, 4.98623s/100 iters), loss = 0.253904
I1123 15:30:53.406996 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:30:53.406996 22196 solver.cpp:237]     Train net output #1: loss = 0.253904 (* 1 = 0.253904 loss)
I1123 15:30:53.406996 22196 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1123 15:30:57.262130 22196 solver.cpp:218] Iteration 17100 (25.9423 iter/s, 3.85471s/100 iters), loss = 0.270959
I1123 15:30:57.262130 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:30:57.262130 22196 solver.cpp:237]     Train net output #1: loss = 0.270959 (* 1 = 0.270959 loss)
I1123 15:30:57.262130 22196 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1123 15:31:01.126191 22196 solver.cpp:218] Iteration 17200 (25.8813 iter/s, 3.86379s/100 iters), loss = 0.289302
I1123 15:31:01.127182 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:31:01.127182 22196 solver.cpp:237]     Train net output #1: loss = 0.289301 (* 1 = 0.289301 loss)
I1123 15:31:01.127182 22196 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1123 15:31:04.983767 22196 solver.cpp:218] Iteration 17300 (25.9308 iter/s, 3.85641s/100 iters), loss = 0.252175
I1123 15:31:04.983767 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:31:04.983767 22196 solver.cpp:237]     Train net output #1: loss = 0.252174 (* 1 = 0.252174 loss)
I1123 15:31:04.983767 22196 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1123 15:31:08.840132 22196 solver.cpp:218] Iteration 17400 (25.9326 iter/s, 3.85615s/100 iters), loss = 0.300338
I1123 15:31:08.840132 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:31:08.840132 22196 solver.cpp:237]     Train net output #1: loss = 0.300338 (* 1 = 0.300338 loss)
I1123 15:31:08.840132 22196 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1123 15:31:12.505731 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:31:12.657351 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_17500.caffemodel
I1123 15:31:12.667348 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_17500.solverstate
I1123 15:31:12.671349 22196 solver.cpp:330] Iteration 17500, Testing net (#0)
I1123 15:31:12.671349 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:31:13.733191 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:31:13.774204 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8582
I1123 15:31:13.774204 22196 solver.cpp:397]     Test net output #1: loss = 0.421255 (* 1 = 0.421255 loss)
I1123 15:31:13.811391 22196 solver.cpp:218] Iteration 17500 (20.1165 iter/s, 4.97104s/100 iters), loss = 0.287409
I1123 15:31:13.811391 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:31:13.811391 22196 solver.cpp:237]     Train net output #1: loss = 0.287409 (* 1 = 0.287409 loss)
I1123 15:31:13.811391 22196 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1123 15:31:17.672869 22196 solver.cpp:218] Iteration 17600 (25.9022 iter/s, 3.86068s/100 iters), loss = 0.306983
I1123 15:31:17.672869 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:31:17.672869 22196 solver.cpp:237]     Train net output #1: loss = 0.306983 (* 1 = 0.306983 loss)
I1123 15:31:17.672869 22196 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1123 15:31:21.521523 22196 solver.cpp:218] Iteration 17700 (25.9862 iter/s, 3.8482s/100 iters), loss = 0.228575
I1123 15:31:21.521523 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:31:21.521523 22196 solver.cpp:237]     Train net output #1: loss = 0.228574 (* 1 = 0.228574 loss)
I1123 15:31:21.521523 22196 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1123 15:31:25.369638 22196 solver.cpp:218] Iteration 17800 (25.9884 iter/s, 3.84788s/100 iters), loss = 0.291391
I1123 15:31:25.369638 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:31:25.369638 22196 solver.cpp:237]     Train net output #1: loss = 0.291391 (* 1 = 0.291391 loss)
I1123 15:31:25.369638 22196 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1123 15:31:29.207998 22196 solver.cpp:218] Iteration 17900 (26.054 iter/s, 3.83819s/100 iters), loss = 0.202722
I1123 15:31:29.208487 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:31:29.208487 22196 solver.cpp:237]     Train net output #1: loss = 0.202722 (* 1 = 0.202722 loss)
I1123 15:31:29.208487 22196 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1123 15:31:32.865223 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:31:33.015882 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_18000.caffemodel
I1123 15:31:33.025882 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_18000.solverstate
I1123 15:31:33.029881 22196 solver.cpp:330] Iteration 18000, Testing net (#0)
I1123 15:31:33.029881 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:31:34.089639 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:31:34.132159 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8576
I1123 15:31:34.132159 22196 solver.cpp:397]     Test net output #1: loss = 0.42132 (* 1 = 0.42132 loss)
I1123 15:31:34.169132 22196 solver.cpp:218] Iteration 18000 (20.1601 iter/s, 4.9603s/100 iters), loss = 0.268292
I1123 15:31:34.169132 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:31:34.169132 22196 solver.cpp:237]     Train net output #1: loss = 0.268292 (* 1 = 0.268292 loss)
I1123 15:31:34.169132 22196 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1123 15:31:38.020766 22196 solver.cpp:218] Iteration 18100 (25.9667 iter/s, 3.85109s/100 iters), loss = 0.278286
I1123 15:31:38.020766 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:31:38.020766 22196 solver.cpp:237]     Train net output #1: loss = 0.278286 (* 1 = 0.278286 loss)
I1123 15:31:38.020766 22196 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1123 15:31:41.869485 22196 solver.cpp:218] Iteration 18200 (25.9862 iter/s, 3.84819s/100 iters), loss = 0.299288
I1123 15:31:41.869485 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:31:41.869485 22196 solver.cpp:237]     Train net output #1: loss = 0.299288 (* 1 = 0.299288 loss)
I1123 15:31:41.869485 22196 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1123 15:31:45.718765 22196 solver.cpp:218] Iteration 18300 (25.9807 iter/s, 3.849s/100 iters), loss = 0.283057
I1123 15:31:45.718765 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:31:45.718765 22196 solver.cpp:237]     Train net output #1: loss = 0.283057 (* 1 = 0.283057 loss)
I1123 15:31:45.718765 22196 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1123 15:31:49.564530 22196 solver.cpp:218] Iteration 18400 (26.0046 iter/s, 3.84547s/100 iters), loss = 0.305229
I1123 15:31:49.564530 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:31:49.564530 22196 solver.cpp:237]     Train net output #1: loss = 0.305229 (* 1 = 0.305229 loss)
I1123 15:31:49.564530 22196 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1123 15:31:53.217838 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:31:53.367861 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_18500.caffemodel
I1123 15:31:53.380859 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_18500.solverstate
I1123 15:31:53.384860 22196 solver.cpp:330] Iteration 18500, Testing net (#0)
I1123 15:31:53.384860 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:31:54.443871 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:31:54.486855 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8582
I1123 15:31:54.486855 22196 solver.cpp:397]     Test net output #1: loss = 0.421007 (* 1 = 0.421007 loss)
I1123 15:31:54.523373 22196 solver.cpp:218] Iteration 18500 (20.1684 iter/s, 4.95825s/100 iters), loss = 0.287024
I1123 15:31:54.523373 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:31:54.523373 22196 solver.cpp:237]     Train net output #1: loss = 0.287023 (* 1 = 0.287023 loss)
I1123 15:31:54.523373 22196 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1123 15:31:58.372428 22196 solver.cpp:218] Iteration 18600 (25.9793 iter/s, 3.84922s/100 iters), loss = 0.265887
I1123 15:31:58.372428 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:31:58.372428 22196 solver.cpp:237]     Train net output #1: loss = 0.265886 (* 1 = 0.265886 loss)
I1123 15:31:58.372428 22196 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1123 15:32:02.227402 22196 solver.cpp:218] Iteration 18700 (25.9452 iter/s, 3.85427s/100 iters), loss = 0.302132
I1123 15:32:02.227402 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:32:02.227402 22196 solver.cpp:237]     Train net output #1: loss = 0.302132 (* 1 = 0.302132 loss)
I1123 15:32:02.227402 22196 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1123 15:32:06.084111 22196 solver.cpp:218] Iteration 18800 (25.928 iter/s, 3.85683s/100 iters), loss = 0.313565
I1123 15:32:06.084111 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:32:06.084111 22196 solver.cpp:237]     Train net output #1: loss = 0.313565 (* 1 = 0.313565 loss)
I1123 15:32:06.084111 22196 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1123 15:32:09.936722 22196 solver.cpp:218] Iteration 18900 (25.9621 iter/s, 3.85177s/100 iters), loss = 0.250227
I1123 15:32:09.936722 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:32:09.936722 22196 solver.cpp:237]     Train net output #1: loss = 0.250227 (* 1 = 0.250227 loss)
I1123 15:32:09.936722 22196 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1123 15:32:13.597769 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:32:13.748781 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_19000.caffemodel
I1123 15:32:13.757781 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_19000.solverstate
I1123 15:32:13.762780 22196 solver.cpp:330] Iteration 19000, Testing net (#0)
I1123 15:32:13.762780 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:32:14.824290 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:32:14.865816 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8582
I1123 15:32:14.865816 22196 solver.cpp:397]     Test net output #1: loss = 0.42097 (* 1 = 0.42097 loss)
I1123 15:32:14.902815 22196 solver.cpp:218] Iteration 19000 (20.1377 iter/s, 4.96582s/100 iters), loss = 0.274528
I1123 15:32:14.902815 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:32:14.902815 22196 solver.cpp:237]     Train net output #1: loss = 0.274528 (* 1 = 0.274528 loss)
I1123 15:32:14.902815 22196 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1123 15:32:18.811224 22196 solver.cpp:218] Iteration 19100 (25.5886 iter/s, 3.90799s/100 iters), loss = 0.314625
I1123 15:32:18.811224 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:32:18.811224 22196 solver.cpp:237]     Train net output #1: loss = 0.314625 (* 1 = 0.314625 loss)
I1123 15:32:18.811224 22196 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1123 15:32:22.733309 22196 solver.cpp:218] Iteration 19200 (25.5004 iter/s, 3.92151s/100 iters), loss = 0.327338
I1123 15:32:22.733309 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:32:22.733309 22196 solver.cpp:237]     Train net output #1: loss = 0.327338 (* 1 = 0.327338 loss)
I1123 15:32:22.733309 22196 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1123 15:32:26.686574 22196 solver.cpp:218] Iteration 19300 (25.2936 iter/s, 3.95357s/100 iters), loss = 0.325631
I1123 15:32:26.686574 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:32:26.686574 22196 solver.cpp:237]     Train net output #1: loss = 0.325631 (* 1 = 0.325631 loss)
I1123 15:32:26.686574 22196 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1123 15:32:30.568544 22196 solver.cpp:218] Iteration 19400 (25.7676 iter/s, 3.88085s/100 iters), loss = 0.236563
I1123 15:32:30.568544 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:32:30.568544 22196 solver.cpp:237]     Train net output #1: loss = 0.236563 (* 1 = 0.236563 loss)
I1123 15:32:30.568544 22196 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1123 15:32:34.282603 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:32:34.438218 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_19500.caffemodel
I1123 15:32:34.448220 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_19500.solverstate
I1123 15:32:34.452219 22196 solver.cpp:330] Iteration 19500, Testing net (#0)
I1123 15:32:34.453217 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:32:35.522208 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:32:35.565208 22196 solver.cpp:397]     Test net output #0: accuracy = 0.858
I1123 15:32:35.565208 22196 solver.cpp:397]     Test net output #1: loss = 0.420762 (* 1 = 0.420762 loss)
I1123 15:32:35.601727 22196 solver.cpp:218] Iteration 19500 (19.868 iter/s, 5.03322s/100 iters), loss = 0.240641
I1123 15:32:35.601727 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:32:35.601727 22196 solver.cpp:237]     Train net output #1: loss = 0.240641 (* 1 = 0.240641 loss)
I1123 15:32:35.601727 22196 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1123 15:32:35.601727 22196 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1123 15:32:39.492585 22196 solver.cpp:218] Iteration 19600 (25.7072 iter/s, 3.88996s/100 iters), loss = 0.256779
I1123 15:32:39.492585 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:32:39.492585 22196 solver.cpp:237]     Train net output #1: loss = 0.256779 (* 1 = 0.256779 loss)
I1123 15:32:39.492585 22196 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1123 15:32:43.390635 22196 solver.cpp:218] Iteration 19700 (25.6558 iter/s, 3.89775s/100 iters), loss = 0.343408
I1123 15:32:43.390635 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:32:43.390635 22196 solver.cpp:237]     Train net output #1: loss = 0.343407 (* 1 = 0.343407 loss)
I1123 15:32:43.390635 22196 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1123 15:32:47.317068 22196 solver.cpp:218] Iteration 19800 (25.4682 iter/s, 3.92646s/100 iters), loss = 0.289123
I1123 15:32:47.317068 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:32:47.317068 22196 solver.cpp:237]     Train net output #1: loss = 0.289123 (* 1 = 0.289123 loss)
I1123 15:32:47.317068 22196 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1123 15:32:51.203356 22196 solver.cpp:218] Iteration 19900 (25.7369 iter/s, 3.88547s/100 iters), loss = 0.273717
I1123 15:32:51.203356 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:32:51.203356 22196 solver.cpp:237]     Train net output #1: loss = 0.273717 (* 1 = 0.273717 loss)
I1123 15:32:51.203356 22196 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1123 15:32:54.901116 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:32:55.052381 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_20000.caffemodel
I1123 15:32:55.062381 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_20000.solverstate
I1123 15:32:55.066395 22196 solver.cpp:330] Iteration 20000, Testing net (#0)
I1123 15:32:55.066395 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:32:56.132520 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:32:56.173710 22196 solver.cpp:397]     Test net output #0: accuracy = 0.858
I1123 15:32:56.173710 22196 solver.cpp:397]     Test net output #1: loss = 0.420793 (* 1 = 0.420793 loss)
I1123 15:32:56.213704 22196 solver.cpp:218] Iteration 20000 (19.9587 iter/s, 5.01035s/100 iters), loss = 0.270073
I1123 15:32:56.213704 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:32:56.214705 22196 solver.cpp:237]     Train net output #1: loss = 0.270073 (* 1 = 0.270073 loss)
I1123 15:32:56.214705 22196 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1123 15:33:00.114605 22196 solver.cpp:218] Iteration 20100 (25.6405 iter/s, 3.90008s/100 iters), loss = 0.221443
I1123 15:33:00.114605 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 15:33:00.114605 22196 solver.cpp:237]     Train net output #1: loss = 0.221442 (* 1 = 0.221442 loss)
I1123 15:33:00.114605 22196 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1123 15:33:04.003748 22196 solver.cpp:218] Iteration 20200 (25.7173 iter/s, 3.88844s/100 iters), loss = 0.348099
I1123 15:33:04.003748 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:33:04.003748 22196 solver.cpp:237]     Train net output #1: loss = 0.348099 (* 1 = 0.348099 loss)
I1123 15:33:04.003748 22196 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1123 15:33:07.910940 22196 solver.cpp:218] Iteration 20300 (25.5966 iter/s, 3.90677s/100 iters), loss = 0.321325
I1123 15:33:07.910940 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:33:07.910940 22196 solver.cpp:237]     Train net output #1: loss = 0.321325 (* 1 = 0.321325 loss)
I1123 15:33:07.910940 22196 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1123 15:33:11.769206 22196 solver.cpp:218] Iteration 20400 (25.9214 iter/s, 3.85782s/100 iters), loss = 0.262587
I1123 15:33:11.769206 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:33:11.769206 22196 solver.cpp:237]     Train net output #1: loss = 0.262587 (* 1 = 0.262587 loss)
I1123 15:33:11.769206 22196 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1123 15:33:15.432385 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:33:15.584113 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_20500.caffemodel
I1123 15:33:15.594112 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_20500.solverstate
I1123 15:33:15.598112 22196 solver.cpp:330] Iteration 20500, Testing net (#0)
I1123 15:33:15.598112 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:33:16.661412 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:33:16.702917 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8578
I1123 15:33:16.702917 22196 solver.cpp:397]     Test net output #1: loss = 0.420717 (* 1 = 0.420717 loss)
I1123 15:33:16.739910 22196 solver.cpp:218] Iteration 20500 (20.117 iter/s, 4.97091s/100 iters), loss = 0.326453
I1123 15:33:16.739910 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:33:16.739910 22196 solver.cpp:237]     Train net output #1: loss = 0.326453 (* 1 = 0.326453 loss)
I1123 15:33:16.739910 22196 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1123 15:33:20.590951 22196 solver.cpp:218] Iteration 20600 (25.9675 iter/s, 3.85097s/100 iters), loss = 0.271965
I1123 15:33:20.590951 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:33:20.590951 22196 solver.cpp:237]     Train net output #1: loss = 0.271965 (* 1 = 0.271965 loss)
I1123 15:33:20.590951 22196 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1123 15:33:24.445461 22196 solver.cpp:218] Iteration 20700 (25.9493 iter/s, 3.85366s/100 iters), loss = 0.289671
I1123 15:33:24.445461 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:33:24.445957 22196 solver.cpp:237]     Train net output #1: loss = 0.289671 (* 1 = 0.289671 loss)
I1123 15:33:24.445957 22196 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1123 15:33:28.295838 22196 solver.cpp:218] Iteration 20800 (25.9752 iter/s, 3.84982s/100 iters), loss = 0.288047
I1123 15:33:28.295838 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:33:28.295838 22196 solver.cpp:237]     Train net output #1: loss = 0.288047 (* 1 = 0.288047 loss)
I1123 15:33:28.295838 22196 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1123 15:33:32.144165 22196 solver.cpp:218] Iteration 20900 (25.9864 iter/s, 3.84816s/100 iters), loss = 0.211654
I1123 15:33:32.144650 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 15:33:32.144650 22196 solver.cpp:237]     Train net output #1: loss = 0.211653 (* 1 = 0.211653 loss)
I1123 15:33:32.144650 22196 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1123 15:33:35.803086 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:33:35.954607 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_21000.caffemodel
I1123 15:33:35.964609 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_21000.solverstate
I1123 15:33:35.968624 22196 solver.cpp:330] Iteration 21000, Testing net (#0)
I1123 15:33:35.968624 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:33:37.031436 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:33:37.073302 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8576
I1123 15:33:37.073302 22196 solver.cpp:397]     Test net output #1: loss = 0.42075 (* 1 = 0.42075 loss)
I1123 15:33:37.110312 22196 solver.cpp:218] Iteration 21000 (20.1385 iter/s, 4.96561s/100 iters), loss = 0.256101
I1123 15:33:37.110312 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:33:37.110312 22196 solver.cpp:237]     Train net output #1: loss = 0.256101 (* 1 = 0.256101 loss)
I1123 15:33:37.110312 22196 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1123 15:33:40.957793 22196 solver.cpp:218] Iteration 21100 (25.9945 iter/s, 3.84697s/100 iters), loss = 0.257357
I1123 15:33:40.957793 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:33:40.957793 22196 solver.cpp:237]     Train net output #1: loss = 0.257357 (* 1 = 0.257357 loss)
I1123 15:33:40.957793 22196 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1123 15:33:44.801820 22196 solver.cpp:218] Iteration 21200 (26.0115 iter/s, 3.84445s/100 iters), loss = 0.287958
I1123 15:33:44.801820 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:33:44.802809 22196 solver.cpp:237]     Train net output #1: loss = 0.287958 (* 1 = 0.287958 loss)
I1123 15:33:44.802809 22196 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1123 15:33:48.647112 22196 solver.cpp:218] Iteration 21300 (26.014 iter/s, 3.84408s/100 iters), loss = 0.308463
I1123 15:33:48.647112 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:33:48.647112 22196 solver.cpp:237]     Train net output #1: loss = 0.308463 (* 1 = 0.308463 loss)
I1123 15:33:48.647112 22196 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1123 15:33:52.492463 22196 solver.cpp:218] Iteration 21400 (26.0066 iter/s, 3.84518s/100 iters), loss = 0.232166
I1123 15:33:52.492463 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:33:52.492463 22196 solver.cpp:237]     Train net output #1: loss = 0.232166 (* 1 = 0.232166 loss)
I1123 15:33:52.492463 22196 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1123 15:33:56.149840 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:33:56.300369 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_21500.caffemodel
I1123 15:33:56.312369 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_21500.solverstate
I1123 15:33:56.316370 22196 solver.cpp:330] Iteration 21500, Testing net (#0)
I1123 15:33:56.317370 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:33:57.379851 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:33:57.420841 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8578
I1123 15:33:57.420841 22196 solver.cpp:397]     Test net output #1: loss = 0.420776 (* 1 = 0.420776 loss)
I1123 15:33:57.457856 22196 solver.cpp:218] Iteration 21500 (20.1394 iter/s, 4.96539s/100 iters), loss = 0.249112
I1123 15:33:57.457856 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:33:57.457856 22196 solver.cpp:237]     Train net output #1: loss = 0.249112 (* 1 = 0.249112 loss)
I1123 15:33:57.457856 22196 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1123 15:34:01.306560 22196 solver.cpp:218] Iteration 21600 (25.9875 iter/s, 3.84801s/100 iters), loss = 0.284256
I1123 15:34:01.306560 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:34:01.306560 22196 solver.cpp:237]     Train net output #1: loss = 0.284256 (* 1 = 0.284256 loss)
I1123 15:34:01.306560 22196 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1123 15:34:05.152679 22196 solver.cpp:218] Iteration 21700 (25.9993 iter/s, 3.84626s/100 iters), loss = 0.321677
I1123 15:34:05.152679 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:34:05.152679 22196 solver.cpp:237]     Train net output #1: loss = 0.321676 (* 1 = 0.321676 loss)
I1123 15:34:05.152679 22196 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1123 15:34:09.002882 22196 solver.cpp:218] Iteration 21800 (25.9804 iter/s, 3.84906s/100 iters), loss = 0.288984
I1123 15:34:09.002882 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:34:09.002882 22196 solver.cpp:237]     Train net output #1: loss = 0.288983 (* 1 = 0.288983 loss)
I1123 15:34:09.002882 22196 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1123 15:34:12.846623 22196 solver.cpp:218] Iteration 21900 (26.0178 iter/s, 3.84352s/100 iters), loss = 0.233864
I1123 15:34:12.846623 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:34:12.846623 22196 solver.cpp:237]     Train net output #1: loss = 0.233864 (* 1 = 0.233864 loss)
I1123 15:34:12.846623 22196 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1123 15:34:16.505139 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:34:16.656231 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_22000.caffemodel
I1123 15:34:16.666221 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_22000.solverstate
I1123 15:34:16.670222 22196 solver.cpp:330] Iteration 22000, Testing net (#0)
I1123 15:34:16.670222 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:34:17.731874 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:34:17.773874 22196 solver.cpp:397]     Test net output #0: accuracy = 0.858
I1123 15:34:17.773874 22196 solver.cpp:397]     Test net output #1: loss = 0.420792 (* 1 = 0.420792 loss)
I1123 15:34:17.810894 22196 solver.cpp:218] Iteration 22000 (20.1453 iter/s, 4.96393s/100 iters), loss = 0.217096
I1123 15:34:17.810894 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 15:34:17.810894 22196 solver.cpp:237]     Train net output #1: loss = 0.217096 (* 1 = 0.217096 loss)
I1123 15:34:17.810894 22196 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1123 15:34:17.810894 22196 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1123 15:34:21.664743 22196 solver.cpp:218] Iteration 22100 (25.9509 iter/s, 3.85343s/100 iters), loss = 0.283704
I1123 15:34:21.664743 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:34:21.664743 22196 solver.cpp:237]     Train net output #1: loss = 0.283704 (* 1 = 0.283704 loss)
I1123 15:34:21.664743 22196 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1123 15:34:25.520903 22196 solver.cpp:218] Iteration 22200 (25.9355 iter/s, 3.85572s/100 iters), loss = 0.365458
I1123 15:34:25.520903 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:34:25.520903 22196 solver.cpp:237]     Train net output #1: loss = 0.365458 (* 1 = 0.365458 loss)
I1123 15:34:25.520903 22196 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1123 15:34:29.374053 22196 solver.cpp:218] Iteration 22300 (25.954 iter/s, 3.85297s/100 iters), loss = 0.272545
I1123 15:34:29.374053 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:34:29.374053 22196 solver.cpp:237]     Train net output #1: loss = 0.272545 (* 1 = 0.272545 loss)
I1123 15:34:29.374053 22196 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1123 15:34:33.225678 22196 solver.cpp:218] Iteration 22400 (25.9677 iter/s, 3.85094s/100 iters), loss = 0.256653
I1123 15:34:33.225678 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:34:33.225678 22196 solver.cpp:237]     Train net output #1: loss = 0.256653 (* 1 = 0.256653 loss)
I1123 15:34:33.225678 22196 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1123 15:34:36.890199 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:34:37.041234 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_22500.caffemodel
I1123 15:34:37.052232 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_22500.solverstate
I1123 15:34:37.056232 22196 solver.cpp:330] Iteration 22500, Testing net (#0)
I1123 15:34:37.056232 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:34:38.119830 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:34:38.161303 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8582
I1123 15:34:38.161303 22196 solver.cpp:397]     Test net output #1: loss = 0.420684 (* 1 = 0.420684 loss)
I1123 15:34:38.198318 22196 solver.cpp:218] Iteration 22500 (20.1083 iter/s, 4.97307s/100 iters), loss = 0.200189
I1123 15:34:38.198318 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 15:34:38.198318 22196 solver.cpp:237]     Train net output #1: loss = 0.200189 (* 1 = 0.200189 loss)
I1123 15:34:38.198318 22196 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1123 15:34:42.044435 22196 solver.cpp:218] Iteration 22600 (26.003 iter/s, 3.84572s/100 iters), loss = 0.258553
I1123 15:34:42.044435 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:34:42.044435 22196 solver.cpp:237]     Train net output #1: loss = 0.258552 (* 1 = 0.258552 loss)
I1123 15:34:42.044435 22196 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1123 15:34:45.894531 22196 solver.cpp:218] Iteration 22700 (25.9805 iter/s, 3.84905s/100 iters), loss = 0.280888
I1123 15:34:45.894531 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:34:45.894531 22196 solver.cpp:237]     Train net output #1: loss = 0.280888 (* 1 = 0.280888 loss)
I1123 15:34:45.894531 22196 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1123 15:34:49.741514 22196 solver.cpp:218] Iteration 22800 (25.994 iter/s, 3.84704s/100 iters), loss = 0.267201
I1123 15:34:49.741514 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:34:49.741514 22196 solver.cpp:237]     Train net output #1: loss = 0.267201 (* 1 = 0.267201 loss)
I1123 15:34:49.741514 22196 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1123 15:34:53.603855 22196 solver.cpp:218] Iteration 22900 (25.8958 iter/s, 3.86162s/100 iters), loss = 0.230252
I1123 15:34:53.603855 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:34:53.603855 22196 solver.cpp:237]     Train net output #1: loss = 0.230252 (* 1 = 0.230252 loss)
I1123 15:34:53.603855 22196 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1123 15:34:57.277673 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:34:57.429352 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_23000.caffemodel
I1123 15:34:57.438838 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_23000.solverstate
I1123 15:34:57.443336 22196 solver.cpp:330] Iteration 23000, Testing net (#0)
I1123 15:34:57.443336 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:34:58.512578 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:34:58.554744 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8578
I1123 15:34:58.554744 22196 solver.cpp:397]     Test net output #1: loss = 0.42093 (* 1 = 0.42093 loss)
I1123 15:34:58.590759 22196 solver.cpp:218] Iteration 23000 (20.0508 iter/s, 4.98732s/100 iters), loss = 0.235288
I1123 15:34:58.590759 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:34:58.591745 22196 solver.cpp:237]     Train net output #1: loss = 0.235288 (* 1 = 0.235288 loss)
I1123 15:34:58.591745 22196 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1123 15:35:02.458786 22196 solver.cpp:218] Iteration 23100 (25.8596 iter/s, 3.86704s/100 iters), loss = 0.225273
I1123 15:35:02.458786 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 15:35:02.458786 22196 solver.cpp:237]     Train net output #1: loss = 0.225272 (* 1 = 0.225272 loss)
I1123 15:35:02.458786 22196 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1123 15:35:06.326644 22196 solver.cpp:218] Iteration 23200 (25.8555 iter/s, 3.86765s/100 iters), loss = 0.297128
I1123 15:35:06.326644 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:35:06.326644 22196 solver.cpp:237]     Train net output #1: loss = 0.297127 (* 1 = 0.297127 loss)
I1123 15:35:06.326644 22196 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1123 15:35:10.188781 22196 solver.cpp:218] Iteration 23300 (25.8935 iter/s, 3.86197s/100 iters), loss = 0.290426
I1123 15:35:10.188781 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:35:10.188781 22196 solver.cpp:237]     Train net output #1: loss = 0.290426 (* 1 = 0.290426 loss)
I1123 15:35:10.188781 22196 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1123 15:35:14.057492 22196 solver.cpp:218] Iteration 23400 (25.8541 iter/s, 3.86786s/100 iters), loss = 0.303937
I1123 15:35:14.057492 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:35:14.057492 22196 solver.cpp:237]     Train net output #1: loss = 0.303937 (* 1 = 0.303937 loss)
I1123 15:35:14.057492 22196 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1123 15:35:17.737264 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:35:17.888825 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_23500.caffemodel
I1123 15:35:17.899824 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_23500.solverstate
I1123 15:35:17.903823 22196 solver.cpp:330] Iteration 23500, Testing net (#0)
I1123 15:35:17.903823 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:35:18.971889 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:35:19.014881 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8577
I1123 15:35:19.014881 22196 solver.cpp:397]     Test net output #1: loss = 0.420811 (* 1 = 0.420811 loss)
I1123 15:35:19.051898 22196 solver.cpp:218] Iteration 23500 (20.0232 iter/s, 4.99421s/100 iters), loss = 0.264361
I1123 15:35:19.052387 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:35:19.052387 22196 solver.cpp:237]     Train net output #1: loss = 0.264361 (* 1 = 0.264361 loss)
I1123 15:35:19.052387 22196 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1123 15:35:22.915294 22196 solver.cpp:218] Iteration 23600 (25.889 iter/s, 3.86264s/100 iters), loss = 0.278755
I1123 15:35:22.915294 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:35:22.915294 22196 solver.cpp:237]     Train net output #1: loss = 0.278755 (* 1 = 0.278755 loss)
I1123 15:35:22.915294 22196 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1123 15:35:26.778337 22196 solver.cpp:218] Iteration 23700 (25.8844 iter/s, 3.86333s/100 iters), loss = 0.303085
I1123 15:35:26.778337 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:35:26.778337 22196 solver.cpp:237]     Train net output #1: loss = 0.303085 (* 1 = 0.303085 loss)
I1123 15:35:26.778337 22196 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1123 15:35:30.643224 22196 solver.cpp:218] Iteration 23800 (25.881 iter/s, 3.86384s/100 iters), loss = 0.264753
I1123 15:35:30.643224 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:35:30.643224 22196 solver.cpp:237]     Train net output #1: loss = 0.264753 (* 1 = 0.264753 loss)
I1123 15:35:30.643224 22196 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1123 15:35:34.505805 22196 solver.cpp:218] Iteration 23900 (25.8909 iter/s, 3.86235s/100 iters), loss = 0.329393
I1123 15:35:34.505805 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:35:34.505805 22196 solver.cpp:237]     Train net output #1: loss = 0.329393 (* 1 = 0.329393 loss)
I1123 15:35:34.505805 22196 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1123 15:35:38.173648 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:35:38.324328 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_24000.caffemodel
I1123 15:35:38.334312 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_24000.solverstate
I1123 15:35:38.339311 22196 solver.cpp:330] Iteration 24000, Testing net (#0)
I1123 15:35:38.339311 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:35:39.405692 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:35:39.448678 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8579
I1123 15:35:39.448678 22196 solver.cpp:397]     Test net output #1: loss = 0.420837 (* 1 = 0.420837 loss)
I1123 15:35:39.485146 22196 solver.cpp:218] Iteration 24000 (20.0828 iter/s, 4.97938s/100 iters), loss = 0.284959
I1123 15:35:39.485146 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:35:39.485146 22196 solver.cpp:237]     Train net output #1: loss = 0.284959 (* 1 = 0.284959 loss)
I1123 15:35:39.485146 22196 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1123 15:35:43.339234 22196 solver.cpp:218] Iteration 24100 (25.9476 iter/s, 3.85393s/100 iters), loss = 0.286833
I1123 15:35:43.339234 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:35:43.339234 22196 solver.cpp:237]     Train net output #1: loss = 0.286832 (* 1 = 0.286832 loss)
I1123 15:35:43.339234 22196 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1123 15:35:47.192989 22196 solver.cpp:218] Iteration 24200 (25.9521 iter/s, 3.85326s/100 iters), loss = 0.297304
I1123 15:35:47.192989 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:35:47.192989 22196 solver.cpp:237]     Train net output #1: loss = 0.297304 (* 1 = 0.297304 loss)
I1123 15:35:47.192989 22196 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1123 15:35:51.054215 22196 solver.cpp:218] Iteration 24300 (25.9016 iter/s, 3.86077s/100 iters), loss = 0.268934
I1123 15:35:51.054215 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:35:51.054215 22196 solver.cpp:237]     Train net output #1: loss = 0.268934 (* 1 = 0.268934 loss)
I1123 15:35:51.054215 22196 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1123 15:35:54.917078 22196 solver.cpp:218] Iteration 24400 (25.8912 iter/s, 3.86231s/100 iters), loss = 0.265795
I1123 15:35:54.917078 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:35:54.917078 22196 solver.cpp:237]     Train net output #1: loss = 0.265795 (* 1 = 0.265795 loss)
I1123 15:35:54.917078 22196 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1123 15:35:58.590656 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:35:58.741662 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_24500.caffemodel
I1123 15:35:58.751660 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_24500.solverstate
I1123 15:35:58.755661 22196 solver.cpp:330] Iteration 24500, Testing net (#0)
I1123 15:35:58.755661 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:35:59.822265 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:35:59.864279 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8578
I1123 15:35:59.864279 22196 solver.cpp:397]     Test net output #1: loss = 0.420776 (* 1 = 0.420776 loss)
I1123 15:35:59.901281 22196 solver.cpp:218] Iteration 24500 (20.065 iter/s, 4.98379s/100 iters), loss = 0.318692
I1123 15:35:59.901281 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:35:59.901281 22196 solver.cpp:237]     Train net output #1: loss = 0.318692 (* 1 = 0.318692 loss)
I1123 15:35:59.901281 22196 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1123 15:36:03.757418 22196 solver.cpp:218] Iteration 24600 (25.9337 iter/s, 3.85599s/100 iters), loss = 0.304847
I1123 15:36:03.757418 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:36:03.757418 22196 solver.cpp:237]     Train net output #1: loss = 0.304847 (* 1 = 0.304847 loss)
I1123 15:36:03.757418 22196 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1123 15:36:07.603279 22196 solver.cpp:218] Iteration 24700 (26.0029 iter/s, 3.84572s/100 iters), loss = 0.368102
I1123 15:36:07.603279 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 15:36:07.603279 22196 solver.cpp:237]     Train net output #1: loss = 0.368101 (* 1 = 0.368101 loss)
I1123 15:36:07.603279 22196 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1123 15:36:11.448529 22196 solver.cpp:218] Iteration 24800 (26.0102 iter/s, 3.84465s/100 iters), loss = 0.306701
I1123 15:36:11.448529 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:36:11.448529 22196 solver.cpp:237]     Train net output #1: loss = 0.306701 (* 1 = 0.306701 loss)
I1123 15:36:11.448529 22196 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1123 15:36:15.286865 22196 solver.cpp:218] Iteration 24900 (26.0594 iter/s, 3.83739s/100 iters), loss = 0.210824
I1123 15:36:15.286865 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:36:15.286865 22196 solver.cpp:237]     Train net output #1: loss = 0.210824 (* 1 = 0.210824 loss)
I1123 15:36:15.286865 22196 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1123 15:36:18.944324 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:36:19.095368 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_25000.caffemodel
I1123 15:36:19.105872 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_25000.solverstate
I1123 15:36:19.109889 22196 solver.cpp:330] Iteration 25000, Testing net (#0)
I1123 15:36:19.109889 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:36:20.173636 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:36:20.214165 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8578
I1123 15:36:20.214165 22196 solver.cpp:397]     Test net output #1: loss = 0.420917 (* 1 = 0.420917 loss)
I1123 15:36:20.251180 22196 solver.cpp:218] Iteration 25000 (20.1436 iter/s, 4.96435s/100 iters), loss = 0.267998
I1123 15:36:20.251180 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:36:20.251180 22196 solver.cpp:237]     Train net output #1: loss = 0.267998 (* 1 = 0.267998 loss)
I1123 15:36:20.251180 22196 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1123 15:36:24.092784 22196 solver.cpp:218] Iteration 25100 (26.0357 iter/s, 3.84088s/100 iters), loss = 0.253873
I1123 15:36:24.092784 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:36:24.092784 22196 solver.cpp:237]     Train net output #1: loss = 0.253873 (* 1 = 0.253873 loss)
I1123 15:36:24.092784 22196 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1123 15:36:27.938782 22196 solver.cpp:218] Iteration 25200 (25.9979 iter/s, 3.84647s/100 iters), loss = 0.34114
I1123 15:36:27.938782 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:36:27.938782 22196 solver.cpp:237]     Train net output #1: loss = 0.34114 (* 1 = 0.34114 loss)
I1123 15:36:27.939769 22196 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1123 15:36:31.784534 22196 solver.cpp:218] Iteration 25300 (26.0065 iter/s, 3.8452s/100 iters), loss = 0.262466
I1123 15:36:31.784534 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:36:31.784534 22196 solver.cpp:237]     Train net output #1: loss = 0.262466 (* 1 = 0.262466 loss)
I1123 15:36:31.784534 22196 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1123 15:36:35.626447 22196 solver.cpp:218] Iteration 25400 (26.0319 iter/s, 3.84143s/100 iters), loss = 0.28722
I1123 15:36:35.626447 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:36:35.626447 22196 solver.cpp:237]     Train net output #1: loss = 0.28722 (* 1 = 0.28722 loss)
I1123 15:36:35.626447 22196 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1123 15:36:39.282593 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:36:39.434142 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_25500.caffemodel
I1123 15:36:39.444123 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_25500.solverstate
I1123 15:36:39.448125 22196 solver.cpp:330] Iteration 25500, Testing net (#0)
I1123 15:36:39.448125 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:36:40.510841 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:36:40.552829 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8578
I1123 15:36:40.552829 22196 solver.cpp:397]     Test net output #1: loss = 0.420813 (* 1 = 0.420813 loss)
I1123 15:36:40.589408 22196 solver.cpp:218] Iteration 25500 (20.1506 iter/s, 4.96262s/100 iters), loss = 0.259078
I1123 15:36:40.589408 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:36:40.589408 22196 solver.cpp:237]     Train net output #1: loss = 0.259078 (* 1 = 0.259078 loss)
I1123 15:36:40.589408 22196 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1123 15:36:44.438563 22196 solver.cpp:218] Iteration 25600 (25.9824 iter/s, 3.84875s/100 iters), loss = 0.282744
I1123 15:36:44.438563 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:36:44.438563 22196 solver.cpp:237]     Train net output #1: loss = 0.282744 (* 1 = 0.282744 loss)
I1123 15:36:44.438563 22196 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1123 15:36:48.286463 22196 solver.cpp:218] Iteration 25700 (25.9915 iter/s, 3.84741s/100 iters), loss = 0.289239
I1123 15:36:48.286463 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:36:48.286463 22196 solver.cpp:237]     Train net output #1: loss = 0.289239 (* 1 = 0.289239 loss)
I1123 15:36:48.286463 22196 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1123 15:36:52.136379 22196 solver.cpp:218] Iteration 25800 (25.979 iter/s, 3.84926s/100 iters), loss = 0.311705
I1123 15:36:52.136379 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:36:52.136379 22196 solver.cpp:237]     Train net output #1: loss = 0.311705 (* 1 = 0.311705 loss)
I1123 15:36:52.136379 22196 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1123 15:36:55.982996 22196 solver.cpp:218] Iteration 25900 (25.9991 iter/s, 3.84629s/100 iters), loss = 0.269772
I1123 15:36:55.982996 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:36:55.982996 22196 solver.cpp:237]     Train net output #1: loss = 0.269772 (* 1 = 0.269772 loss)
I1123 15:36:55.982996 22196 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1123 15:36:59.640488 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:36:59.792614 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_26000.caffemodel
I1123 15:36:59.801585 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_26000.solverstate
I1123 15:36:59.805586 22196 solver.cpp:330] Iteration 26000, Testing net (#0)
I1123 15:36:59.805586 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:37:00.869963 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:37:00.912459 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8579
I1123 15:37:00.912459 22196 solver.cpp:397]     Test net output #1: loss = 0.420831 (* 1 = 0.420831 loss)
I1123 15:37:00.948472 22196 solver.cpp:218] Iteration 26000 (20.1385 iter/s, 4.96562s/100 iters), loss = 0.284525
I1123 15:37:00.948472 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:37:00.948472 22196 solver.cpp:237]     Train net output #1: loss = 0.284525 (* 1 = 0.284525 loss)
I1123 15:37:00.948472 22196 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1123 15:37:04.798653 22196 solver.cpp:218] Iteration 26100 (25.9767 iter/s, 3.8496s/100 iters), loss = 0.285453
I1123 15:37:04.798653 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:37:04.798653 22196 solver.cpp:237]     Train net output #1: loss = 0.285452 (* 1 = 0.285452 loss)
I1123 15:37:04.798653 22196 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1123 15:37:08.649646 22196 solver.cpp:218] Iteration 26200 (25.9674 iter/s, 3.85098s/100 iters), loss = 0.257005
I1123 15:37:08.649646 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:37:08.649646 22196 solver.cpp:237]     Train net output #1: loss = 0.257005 (* 1 = 0.257005 loss)
I1123 15:37:08.649646 22196 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1123 15:37:12.502025 22196 solver.cpp:218] Iteration 26300 (25.9653 iter/s, 3.85129s/100 iters), loss = 0.307215
I1123 15:37:12.502025 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:37:12.502025 22196 solver.cpp:237]     Train net output #1: loss = 0.307215 (* 1 = 0.307215 loss)
I1123 15:37:12.502025 22196 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1123 15:37:16.354046 22196 solver.cpp:218] Iteration 26400 (25.9598 iter/s, 3.85211s/100 iters), loss = 0.240835
I1123 15:37:16.354046 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:37:16.354046 22196 solver.cpp:237]     Train net output #1: loss = 0.240835 (* 1 = 0.240835 loss)
I1123 15:37:16.354046 22196 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1123 15:37:20.035116 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:37:20.187134 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_26500.caffemodel
I1123 15:37:20.197638 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_26500.solverstate
I1123 15:37:20.201638 22196 solver.cpp:330] Iteration 26500, Testing net (#0)
I1123 15:37:20.201638 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:37:21.267269 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:37:21.308816 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8579
I1123 15:37:21.308816 22196 solver.cpp:397]     Test net output #1: loss = 0.420874 (* 1 = 0.420874 loss)
I1123 15:37:21.346820 22196 solver.cpp:218] Iteration 26500 (20.0335 iter/s, 4.99163s/100 iters), loss = 0.204735
I1123 15:37:21.346820 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:37:21.346820 22196 solver.cpp:237]     Train net output #1: loss = 0.204735 (* 1 = 0.204735 loss)
I1123 15:37:21.346820 22196 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1123 15:37:25.205305 22196 solver.cpp:218] Iteration 26600 (25.9172 iter/s, 3.85844s/100 iters), loss = 0.24658
I1123 15:37:25.205791 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:37:25.205791 22196 solver.cpp:237]     Train net output #1: loss = 0.246579 (* 1 = 0.246579 loss)
I1123 15:37:25.205791 22196 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1123 15:37:29.066727 22196 solver.cpp:218] Iteration 26700 (25.8981 iter/s, 3.86129s/100 iters), loss = 0.275362
I1123 15:37:29.066727 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:37:29.066727 22196 solver.cpp:237]     Train net output #1: loss = 0.275362 (* 1 = 0.275362 loss)
I1123 15:37:29.066727 22196 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1123 15:37:32.929329 22196 solver.cpp:218] Iteration 26800 (25.8911 iter/s, 3.86234s/100 iters), loss = 0.250288
I1123 15:37:32.929329 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:37:32.929329 22196 solver.cpp:237]     Train net output #1: loss = 0.250288 (* 1 = 0.250288 loss)
I1123 15:37:32.929329 22196 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1123 15:37:36.788108 22196 solver.cpp:218] Iteration 26900 (25.9163 iter/s, 3.85857s/100 iters), loss = 0.249806
I1123 15:37:36.789093 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:37:36.789093 22196 solver.cpp:237]     Train net output #1: loss = 0.249806 (* 1 = 0.249806 loss)
I1123 15:37:36.789093 22196 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1123 15:37:40.463369 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:37:40.614467 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_27000.caffemodel
I1123 15:37:40.623970 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_27000.solverstate
I1123 15:37:40.627970 22196 solver.cpp:330] Iteration 27000, Testing net (#0)
I1123 15:37:40.627970 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:37:41.696494 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:37:41.738529 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8579
I1123 15:37:41.738529 22196 solver.cpp:397]     Test net output #1: loss = 0.420657 (* 1 = 0.420657 loss)
I1123 15:37:41.774523 22196 solver.cpp:218] Iteration 27000 (20.0564 iter/s, 4.98594s/100 iters), loss = 0.262435
I1123 15:37:41.774523 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:37:41.774523 22196 solver.cpp:237]     Train net output #1: loss = 0.262435 (* 1 = 0.262435 loss)
I1123 15:37:41.774523 22196 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1123 15:37:41.774523 22196 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1123 15:37:45.631996 22196 solver.cpp:218] Iteration 27100 (25.9283 iter/s, 3.85679s/100 iters), loss = 0.268339
I1123 15:37:45.631996 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:37:45.631996 22196 solver.cpp:237]     Train net output #1: loss = 0.268339 (* 1 = 0.268339 loss)
I1123 15:37:45.631996 22196 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1123 15:37:49.489490 22196 solver.cpp:218] Iteration 27200 (25.9247 iter/s, 3.85733s/100 iters), loss = 0.295857
I1123 15:37:49.489490 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:37:49.489490 22196 solver.cpp:237]     Train net output #1: loss = 0.295856 (* 1 = 0.295856 loss)
I1123 15:37:49.489490 22196 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1123 15:37:53.350313 22196 solver.cpp:218] Iteration 27300 (25.907 iter/s, 3.85995s/100 iters), loss = 0.295845
I1123 15:37:53.350313 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:37:53.350313 22196 solver.cpp:237]     Train net output #1: loss = 0.295845 (* 1 = 0.295845 loss)
I1123 15:37:53.350313 22196 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1123 15:37:57.206326 22196 solver.cpp:218] Iteration 27400 (25.9348 iter/s, 3.85582s/100 iters), loss = 0.262169
I1123 15:37:57.206326 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:37:57.206326 22196 solver.cpp:237]     Train net output #1: loss = 0.262169 (* 1 = 0.262169 loss)
I1123 15:37:57.206326 22196 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1123 15:38:00.873580 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:38:01.024621 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_27500.caffemodel
I1123 15:38:01.034883 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_27500.solverstate
I1123 15:38:01.038883 22196 solver.cpp:330] Iteration 27500, Testing net (#0)
I1123 15:38:01.038883 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:38:02.104895 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:38:02.147914 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8578
I1123 15:38:02.147914 22196 solver.cpp:397]     Test net output #1: loss = 0.420827 (* 1 = 0.420827 loss)
I1123 15:38:02.183928 22196 solver.cpp:218] Iteration 27500 (20.0899 iter/s, 4.97762s/100 iters), loss = 0.276731
I1123 15:38:02.183928 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:38:02.183928 22196 solver.cpp:237]     Train net output #1: loss = 0.276731 (* 1 = 0.276731 loss)
I1123 15:38:02.183928 22196 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1123 15:38:06.043246 22196 solver.cpp:218] Iteration 27600 (25.9184 iter/s, 3.85827s/100 iters), loss = 0.266802
I1123 15:38:06.043246 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:38:06.043246 22196 solver.cpp:237]     Train net output #1: loss = 0.266802 (* 1 = 0.266802 loss)
I1123 15:38:06.043246 22196 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1123 15:38:09.904907 22196 solver.cpp:218] Iteration 27700 (25.895 iter/s, 3.86174s/100 iters), loss = 0.298128
I1123 15:38:09.904907 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:38:09.904907 22196 solver.cpp:237]     Train net output #1: loss = 0.298128 (* 1 = 0.298128 loss)
I1123 15:38:09.904907 22196 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1123 15:38:13.764716 22196 solver.cpp:218] Iteration 27800 (25.9148 iter/s, 3.8588s/100 iters), loss = 0.281758
I1123 15:38:13.764716 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:38:13.764716 22196 solver.cpp:237]     Train net output #1: loss = 0.281758 (* 1 = 0.281758 loss)
I1123 15:38:13.764716 22196 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1123 15:38:17.619556 22196 solver.cpp:218] Iteration 27900 (25.9417 iter/s, 3.8548s/100 iters), loss = 0.231458
I1123 15:38:17.619556 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:38:17.619556 22196 solver.cpp:237]     Train net output #1: loss = 0.231458 (* 1 = 0.231458 loss)
I1123 15:38:17.619556 22196 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1123 15:38:21.291849 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:38:21.443114 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_28000.caffemodel
I1123 15:38:21.453114 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_28000.solverstate
I1123 15:38:21.457119 22196 solver.cpp:330] Iteration 28000, Testing net (#0)
I1123 15:38:21.457119 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:38:22.524755 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:38:22.566756 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8575
I1123 15:38:22.566756 22196 solver.cpp:397]     Test net output #1: loss = 0.420979 (* 1 = 0.420979 loss)
I1123 15:38:22.603530 22196 solver.cpp:218] Iteration 28000 (20.0652 iter/s, 4.98375s/100 iters), loss = 0.235329
I1123 15:38:22.603530 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 15:38:22.603530 22196 solver.cpp:237]     Train net output #1: loss = 0.235329 (* 1 = 0.235329 loss)
I1123 15:38:22.603530 22196 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1123 15:38:26.459374 22196 solver.cpp:218] Iteration 28100 (25.9365 iter/s, 3.85557s/100 iters), loss = 0.268628
I1123 15:38:26.459374 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:38:26.459374 22196 solver.cpp:237]     Train net output #1: loss = 0.268628 (* 1 = 0.268628 loss)
I1123 15:38:26.459374 22196 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1123 15:38:30.313832 22196 solver.cpp:218] Iteration 28200 (25.9446 iter/s, 3.85437s/100 iters), loss = 0.310382
I1123 15:38:30.313832 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:38:30.313832 22196 solver.cpp:237]     Train net output #1: loss = 0.310382 (* 1 = 0.310382 loss)
I1123 15:38:30.313832 22196 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1123 15:38:34.168864 22196 solver.cpp:218] Iteration 28300 (25.9458 iter/s, 3.85419s/100 iters), loss = 0.315314
I1123 15:38:34.169353 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:38:34.169353 22196 solver.cpp:237]     Train net output #1: loss = 0.315314 (* 1 = 0.315314 loss)
I1123 15:38:34.169353 22196 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1123 15:38:38.027978 22196 solver.cpp:218] Iteration 28400 (25.914 iter/s, 3.85891s/100 iters), loss = 0.239073
I1123 15:38:38.027978 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 15:38:38.027978 22196 solver.cpp:237]     Train net output #1: loss = 0.239073 (* 1 = 0.239073 loss)
I1123 15:38:38.027978 22196 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1123 15:38:41.695384 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:38:41.847923 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_28500.caffemodel
I1123 15:38:41.857409 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_28500.solverstate
I1123 15:38:41.861408 22196 solver.cpp:330] Iteration 28500, Testing net (#0)
I1123 15:38:41.861408 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:38:42.928973 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:38:42.971699 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8579
I1123 15:38:42.971699 22196 solver.cpp:397]     Test net output #1: loss = 0.420873 (* 1 = 0.420873 loss)
I1123 15:38:43.007702 22196 solver.cpp:218] Iteration 28500 (20.0824 iter/s, 4.97949s/100 iters), loss = 0.30135
I1123 15:38:43.007702 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 15:38:43.007702 22196 solver.cpp:237]     Train net output #1: loss = 0.30135 (* 1 = 0.30135 loss)
I1123 15:38:43.007702 22196 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1123 15:38:46.870533 22196 solver.cpp:218] Iteration 28600 (25.8926 iter/s, 3.8621s/100 iters), loss = 0.227325
I1123 15:38:46.870533 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 15:38:46.870533 22196 solver.cpp:237]     Train net output #1: loss = 0.227325 (* 1 = 0.227325 loss)
I1123 15:38:46.870533 22196 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1123 15:38:50.731737 22196 solver.cpp:218] Iteration 28700 (25.9038 iter/s, 3.86043s/100 iters), loss = 0.305541
I1123 15:38:50.731737 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:38:50.731737 22196 solver.cpp:237]     Train net output #1: loss = 0.305541 (* 1 = 0.305541 loss)
I1123 15:38:50.731737 22196 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1123 15:38:54.598186 22196 solver.cpp:218] Iteration 28800 (25.8604 iter/s, 3.86692s/100 iters), loss = 0.31214
I1123 15:38:54.599174 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:38:54.599174 22196 solver.cpp:237]     Train net output #1: loss = 0.31214 (* 1 = 0.31214 loss)
I1123 15:38:54.599174 22196 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1123 15:38:58.460414 22196 solver.cpp:218] Iteration 28900 (25.8984 iter/s, 3.86125s/100 iters), loss = 0.212151
I1123 15:38:58.460414 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:38:58.460902 22196 solver.cpp:237]     Train net output #1: loss = 0.212151 (* 1 = 0.212151 loss)
I1123 15:38:58.460902 22196 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1123 15:39:02.138698 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:39:02.289754 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_29000.caffemodel
I1123 15:39:02.299736 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_29000.solverstate
I1123 15:39:02.303736 22196 solver.cpp:330] Iteration 29000, Testing net (#0)
I1123 15:39:02.303736 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:39:03.369802 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:39:03.412326 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8577
I1123 15:39:03.412326 22196 solver.cpp:397]     Test net output #1: loss = 0.420886 (* 1 = 0.420886 loss)
I1123 15:39:03.448329 22196 solver.cpp:218] Iteration 29000 (20.0485 iter/s, 4.98791s/100 iters), loss = 0.210893
I1123 15:39:03.448329 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:39:03.448329 22196 solver.cpp:237]     Train net output #1: loss = 0.210892 (* 1 = 0.210892 loss)
I1123 15:39:03.448329 22196 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1123 15:39:07.308686 22196 solver.cpp:218] Iteration 29100 (25.9057 iter/s, 3.86015s/100 iters), loss = 0.291665
I1123 15:39:07.309674 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:39:07.309674 22196 solver.cpp:237]     Train net output #1: loss = 0.291665 (* 1 = 0.291665 loss)
I1123 15:39:07.309674 22196 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1123 15:39:11.165503 22196 solver.cpp:218] Iteration 29200 (25.9343 iter/s, 3.8559s/100 iters), loss = 0.245028
I1123 15:39:11.165503 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 15:39:11.165503 22196 solver.cpp:237]     Train net output #1: loss = 0.245028 (* 1 = 0.245028 loss)
I1123 15:39:11.165503 22196 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1123 15:39:15.022037 22196 solver.cpp:218] Iteration 29300 (25.9325 iter/s, 3.85616s/100 iters), loss = 0.320775
I1123 15:39:15.022037 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:39:15.022037 22196 solver.cpp:237]     Train net output #1: loss = 0.320775 (* 1 = 0.320775 loss)
I1123 15:39:15.022037 22196 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1123 15:39:18.878373 22196 solver.cpp:218] Iteration 29400 (25.934 iter/s, 3.85595s/100 iters), loss = 0.288033
I1123 15:39:18.878373 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:39:18.878373 22196 solver.cpp:237]     Train net output #1: loss = 0.288033 (* 1 = 0.288033 loss)
I1123 15:39:18.878373 22196 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1123 15:39:22.546334 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:39:22.697360 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_29500.caffemodel
I1123 15:39:22.707360 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_29500.solverstate
I1123 15:39:22.711360 22196 solver.cpp:330] Iteration 29500, Testing net (#0)
I1123 15:39:22.711360 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:39:23.779026 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:39:23.820638 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8578
I1123 15:39:23.820638 22196 solver.cpp:397]     Test net output #1: loss = 0.420809 (* 1 = 0.420809 loss)
I1123 15:39:23.857638 22196 solver.cpp:218] Iteration 29500 (20.0838 iter/s, 4.97913s/100 iters), loss = 0.268457
I1123 15:39:23.857638 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 15:39:23.857638 22196 solver.cpp:237]     Train net output #1: loss = 0.268457 (* 1 = 0.268457 loss)
I1123 15:39:23.857638 22196 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1123 15:39:27.716964 22196 solver.cpp:218] Iteration 29600 (25.9133 iter/s, 3.85902s/100 iters), loss = 0.291686
I1123 15:39:27.716964 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 15:39:27.716964 22196 solver.cpp:237]     Train net output #1: loss = 0.291686 (* 1 = 0.291686 loss)
I1123 15:39:27.716964 22196 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1123 15:39:31.581049 22196 solver.cpp:218] Iteration 29700 (25.883 iter/s, 3.86354s/100 iters), loss = 0.323275
I1123 15:39:31.581538 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 15:39:31.581538 22196 solver.cpp:237]     Train net output #1: loss = 0.323275 (* 1 = 0.323275 loss)
I1123 15:39:31.581538 22196 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1123 15:39:35.446581 22196 solver.cpp:218] Iteration 29800 (25.8724 iter/s, 3.86512s/100 iters), loss = 0.26193
I1123 15:39:35.446581 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 15:39:35.446581 22196 solver.cpp:237]     Train net output #1: loss = 0.26193 (* 1 = 0.26193 loss)
I1123 15:39:35.446581 22196 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1123 15:39:39.309046 22196 solver.cpp:218] Iteration 29900 (25.8939 iter/s, 3.86192s/100 iters), loss = 0.202461
I1123 15:39:39.309046 22196 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 15:39:39.309046 22196 solver.cpp:237]     Train net output #1: loss = 0.202461 (* 1 = 0.202461 loss)
I1123 15:39:39.309046 22196 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1123 15:39:42.985867 20256 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:39:43.137908 22196 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_30000.caffemodel
I1123 15:39:43.147892 22196 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_7x7_iter_30000.solverstate
I1123 15:39:43.163607 22196 solver.cpp:310] Iteration 30000, loss = 0.288064
I1123 15:39:43.163607 22196 solver.cpp:330] Iteration 30000, Testing net (#0)
I1123 15:39:43.163607 22196 net.cpp:676] Ignoring source layer accuracy_training
I1123 15:39:44.233120 16188 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:39:44.275149 22196 solver.cpp:397]     Test net output #0: accuracy = 0.8576
I1123 15:39:44.275149 22196 solver.cpp:397]     Test net output #1: loss = 0.420896 (* 1 = 0.420896 loss)
I1123 15:39:44.275149 22196 solver.cpp:315] Optimization Done.
I1123 15:39:44.275149 22196 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 
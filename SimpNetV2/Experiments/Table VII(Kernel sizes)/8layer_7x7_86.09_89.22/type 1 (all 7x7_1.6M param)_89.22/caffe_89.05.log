
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1123 19:59:24.868363 34600 caffe.cpp:219] Using GPUs 0
I1123 19:59:25.031389 34600 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1123 19:59:25.336205 34600 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 19:59:25.352233 34600 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_1.6M_8L_7x7"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1123 19:59:25.352233 34600 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 19:59:25.353233 34600 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 19:59:25.353233 34600 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 19:59:25.353233 34600 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1123 19:59:25.353233 34600 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1123 19:59:25.353233 34600 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1123 19:59:25.353233 34600 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1123 19:59:25.353233 34600 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1123 19:59:25.353233 34600 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1123 19:59:25.353233 34600 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1123 19:59:25.353233 34600 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1123 19:59:25.353233 34600 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1123 19:59:25.353233 34600 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1123 19:59:25.353233 34600 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_7x7_1.6M"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 41
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 43
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 85
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 3
    kernel_size: 7
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 19:59:25.357272 34600 layer_factory.cpp:58] Creating layer cifar
I1123 19:59:25.365253 34600 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1123 19:59:25.365253 34600 net.cpp:84] Creating Layer cifar
I1123 19:59:25.365253 34600 net.cpp:380] cifar -> data
I1123 19:59:25.365253 34600 net.cpp:380] cifar -> label
I1123 19:59:25.366264 34600 data_layer.cpp:45] output data size: 100,3,32,32
I1123 19:59:25.372234 34600 net.cpp:122] Setting up cifar
I1123 19:59:25.372234 34600 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 19:59:25.373234 34600 net.cpp:129] Top shape: 100 (100)
I1123 19:59:25.373234 34600 net.cpp:137] Memory required for data: 1229200
I1123 19:59:25.373234 34600 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 19:59:25.373234 34600 net.cpp:84] Creating Layer label_cifar_1_split
I1123 19:59:25.373234 34600 net.cpp:406] label_cifar_1_split <- label
I1123 19:59:25.373234 34600 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 19:59:25.373234 34600 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 19:59:25.373234 34600 net.cpp:122] Setting up label_cifar_1_split
I1123 19:59:25.373234 34600 net.cpp:129] Top shape: 100 (100)
I1123 19:59:25.373234 34600 net.cpp:129] Top shape: 100 (100)
I1123 19:59:25.373234 34600 net.cpp:137] Memory required for data: 1230000
I1123 19:59:25.373234 34600 layer_factory.cpp:58] Creating layer conv1
I1123 19:59:25.373234 34600 net.cpp:84] Creating Layer conv1
I1123 19:59:25.373234 34600 net.cpp:406] conv1 <- data
I1123 19:59:25.373234 34600 net.cpp:380] conv1 -> conv1
I1123 19:59:25.374234 27844 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 19:59:25.621840 34600 net.cpp:122] Setting up conv1
I1123 19:59:25.621840 34600 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 19:59:25.621840 34600 net.cpp:137] Memory required for data: 18023600
I1123 19:59:25.621840 34600 layer_factory.cpp:58] Creating layer bn1
I1123 19:59:25.621840 34600 net.cpp:84] Creating Layer bn1
I1123 19:59:25.621840 34600 net.cpp:406] bn1 <- conv1
I1123 19:59:25.621840 34600 net.cpp:367] bn1 -> conv1 (in-place)
I1123 19:59:25.621840 34600 net.cpp:122] Setting up bn1
I1123 19:59:25.621840 34600 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 19:59:25.621840 34600 net.cpp:137] Memory required for data: 34817200
I1123 19:59:25.621840 34600 layer_factory.cpp:58] Creating layer scale1
I1123 19:59:25.621840 34600 net.cpp:84] Creating Layer scale1
I1123 19:59:25.621840 34600 net.cpp:406] scale1 <- conv1
I1123 19:59:25.621840 34600 net.cpp:367] scale1 -> conv1 (in-place)
I1123 19:59:25.621840 34600 layer_factory.cpp:58] Creating layer scale1
I1123 19:59:25.621840 34600 net.cpp:122] Setting up scale1
I1123 19:59:25.621840 34600 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 19:59:25.621840 34600 net.cpp:137] Memory required for data: 51610800
I1123 19:59:25.621840 34600 layer_factory.cpp:58] Creating layer relu1
I1123 19:59:25.621840 34600 net.cpp:84] Creating Layer relu1
I1123 19:59:25.621840 34600 net.cpp:406] relu1 <- conv1
I1123 19:59:25.622836 34600 net.cpp:367] relu1 -> conv1 (in-place)
I1123 19:59:25.622836 34600 net.cpp:122] Setting up relu1
I1123 19:59:25.622836 34600 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 19:59:25.622836 34600 net.cpp:137] Memory required for data: 68404400
I1123 19:59:25.622836 34600 layer_factory.cpp:58] Creating layer conv2
I1123 19:59:25.622836 34600 net.cpp:84] Creating Layer conv2
I1123 19:59:25.622836 34600 net.cpp:406] conv2 <- conv1
I1123 19:59:25.622836 34600 net.cpp:380] conv2 -> conv2
I1123 19:59:25.623836 34600 net.cpp:122] Setting up conv2
I1123 19:59:25.623836 34600 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 19:59:25.623836 34600 net.cpp:137] Memory required for data: 86017200
I1123 19:59:25.623836 34600 layer_factory.cpp:58] Creating layer bn2
I1123 19:59:25.623836 34600 net.cpp:84] Creating Layer bn2
I1123 19:59:25.623836 34600 net.cpp:406] bn2 <- conv2
I1123 19:59:25.623836 34600 net.cpp:367] bn2 -> conv2 (in-place)
I1123 19:59:25.623836 34600 net.cpp:122] Setting up bn2
I1123 19:59:25.623836 34600 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 19:59:25.623836 34600 net.cpp:137] Memory required for data: 103630000
I1123 19:59:25.623836 34600 layer_factory.cpp:58] Creating layer scale2
I1123 19:59:25.623836 34600 net.cpp:84] Creating Layer scale2
I1123 19:59:25.623836 34600 net.cpp:406] scale2 <- conv2
I1123 19:59:25.624835 34600 net.cpp:367] scale2 -> conv2 (in-place)
I1123 19:59:25.624835 34600 layer_factory.cpp:58] Creating layer scale2
I1123 19:59:25.624835 34600 net.cpp:122] Setting up scale2
I1123 19:59:25.624835 34600 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 19:59:25.624835 34600 net.cpp:137] Memory required for data: 121242800
I1123 19:59:25.624835 34600 layer_factory.cpp:58] Creating layer relu2
I1123 19:59:25.624835 34600 net.cpp:84] Creating Layer relu2
I1123 19:59:25.624835 34600 net.cpp:406] relu2 <- conv2
I1123 19:59:25.624835 34600 net.cpp:367] relu2 -> conv2 (in-place)
I1123 19:59:25.624835 34600 net.cpp:122] Setting up relu2
I1123 19:59:25.624835 34600 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 19:59:25.624835 34600 net.cpp:137] Memory required for data: 138855600
I1123 19:59:25.624835 34600 layer_factory.cpp:58] Creating layer conv2_2
I1123 19:59:25.624835 34600 net.cpp:84] Creating Layer conv2_2
I1123 19:59:25.624835 34600 net.cpp:406] conv2_2 <- conv2
I1123 19:59:25.624835 34600 net.cpp:380] conv2_2 -> conv2_2
I1123 19:59:25.626835 34600 net.cpp:122] Setting up conv2_2
I1123 19:59:25.626835 34600 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 19:59:25.626835 34600 net.cpp:137] Memory required for data: 167527600
I1123 19:59:25.626835 34600 layer_factory.cpp:58] Creating layer bn2_2
I1123 19:59:25.626835 34600 net.cpp:84] Creating Layer bn2_2
I1123 19:59:25.626835 34600 net.cpp:406] bn2_2 <- conv2_2
I1123 19:59:25.626835 34600 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 19:59:25.626835 34600 net.cpp:122] Setting up bn2_2
I1123 19:59:25.626835 34600 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 19:59:25.626835 34600 net.cpp:137] Memory required for data: 196199600
I1123 19:59:25.626835 34600 layer_factory.cpp:58] Creating layer scale2_2
I1123 19:59:25.626835 34600 net.cpp:84] Creating Layer scale2_2
I1123 19:59:25.626835 34600 net.cpp:406] scale2_2 <- conv2_2
I1123 19:59:25.626835 34600 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 19:59:25.626835 34600 layer_factory.cpp:58] Creating layer scale2_2
I1123 19:59:25.626835 34600 net.cpp:122] Setting up scale2_2
I1123 19:59:25.626835 34600 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 19:59:25.626835 34600 net.cpp:137] Memory required for data: 224871600
I1123 19:59:25.626835 34600 layer_factory.cpp:58] Creating layer relu2_2
I1123 19:59:25.626835 34600 net.cpp:84] Creating Layer relu2_2
I1123 19:59:25.626835 34600 net.cpp:406] relu2_2 <- conv2_2
I1123 19:59:25.626835 34600 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 19:59:25.626835 34600 net.cpp:122] Setting up relu2_2
I1123 19:59:25.626835 34600 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 19:59:25.626835 34600 net.cpp:137] Memory required for data: 253543600
I1123 19:59:25.627837 34600 layer_factory.cpp:58] Creating layer pool2_1
I1123 19:59:25.627837 34600 net.cpp:84] Creating Layer pool2_1
I1123 19:59:25.627837 34600 net.cpp:406] pool2_1 <- conv2_2
I1123 19:59:25.627837 34600 net.cpp:380] pool2_1 -> pool2_1
I1123 19:59:25.627837 34600 net.cpp:122] Setting up pool2_1
I1123 19:59:25.627837 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.627837 34600 net.cpp:137] Memory required for data: 260711600
I1123 19:59:25.627837 34600 layer_factory.cpp:58] Creating layer conv3
I1123 19:59:25.627837 34600 net.cpp:84] Creating Layer conv3
I1123 19:59:25.627837 34600 net.cpp:406] conv3 <- pool2_1
I1123 19:59:25.627837 34600 net.cpp:380] conv3 -> conv3
I1123 19:59:25.630836 34600 net.cpp:122] Setting up conv3
I1123 19:59:25.630836 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.630836 34600 net.cpp:137] Memory required for data: 267879600
I1123 19:59:25.630836 34600 layer_factory.cpp:58] Creating layer bn3
I1123 19:59:25.630836 34600 net.cpp:84] Creating Layer bn3
I1123 19:59:25.630836 34600 net.cpp:406] bn3 <- conv3
I1123 19:59:25.630836 34600 net.cpp:367] bn3 -> conv3 (in-place)
I1123 19:59:25.630836 34600 net.cpp:122] Setting up bn3
I1123 19:59:25.630836 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.630836 34600 net.cpp:137] Memory required for data: 275047600
I1123 19:59:25.630836 34600 layer_factory.cpp:58] Creating layer scale3
I1123 19:59:25.630836 34600 net.cpp:84] Creating Layer scale3
I1123 19:59:25.630836 34600 net.cpp:406] scale3 <- conv3
I1123 19:59:25.630836 34600 net.cpp:367] scale3 -> conv3 (in-place)
I1123 19:59:25.630836 34600 layer_factory.cpp:58] Creating layer scale3
I1123 19:59:25.630836 34600 net.cpp:122] Setting up scale3
I1123 19:59:25.630836 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.630836 34600 net.cpp:137] Memory required for data: 282215600
I1123 19:59:25.630836 34600 layer_factory.cpp:58] Creating layer relu3
I1123 19:59:25.630836 34600 net.cpp:84] Creating Layer relu3
I1123 19:59:25.630836 34600 net.cpp:406] relu3 <- conv3
I1123 19:59:25.630836 34600 net.cpp:367] relu3 -> conv3 (in-place)
I1123 19:59:25.630836 34600 net.cpp:122] Setting up relu3
I1123 19:59:25.630836 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.630836 34600 net.cpp:137] Memory required for data: 289383600
I1123 19:59:25.630836 34600 layer_factory.cpp:58] Creating layer conv4
I1123 19:59:25.630836 34600 net.cpp:84] Creating Layer conv4
I1123 19:59:25.630836 34600 net.cpp:406] conv4 <- conv3
I1123 19:59:25.630836 34600 net.cpp:380] conv4 -> conv4
I1123 19:59:25.633852 34600 net.cpp:122] Setting up conv4
I1123 19:59:25.633852 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.633852 34600 net.cpp:137] Memory required for data: 296551600
I1123 19:59:25.633852 34600 layer_factory.cpp:58] Creating layer bn4
I1123 19:59:25.633852 34600 net.cpp:84] Creating Layer bn4
I1123 19:59:25.633852 34600 net.cpp:406] bn4 <- conv4
I1123 19:59:25.633852 34600 net.cpp:367] bn4 -> conv4 (in-place)
I1123 19:59:25.633852 34600 net.cpp:122] Setting up bn4
I1123 19:59:25.633852 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.633852 34600 net.cpp:137] Memory required for data: 303719600
I1123 19:59:25.633852 34600 layer_factory.cpp:58] Creating layer scale4
I1123 19:59:25.633852 34600 net.cpp:84] Creating Layer scale4
I1123 19:59:25.633852 34600 net.cpp:406] scale4 <- conv4
I1123 19:59:25.633852 34600 net.cpp:367] scale4 -> conv4 (in-place)
I1123 19:59:25.633852 34600 layer_factory.cpp:58] Creating layer scale4
I1123 19:59:25.633852 34600 net.cpp:122] Setting up scale4
I1123 19:59:25.633852 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.633852 34600 net.cpp:137] Memory required for data: 310887600
I1123 19:59:25.633852 34600 layer_factory.cpp:58] Creating layer relu4
I1123 19:59:25.633852 34600 net.cpp:84] Creating Layer relu4
I1123 19:59:25.633852 34600 net.cpp:406] relu4 <- conv4
I1123 19:59:25.633852 34600 net.cpp:367] relu4 -> conv4 (in-place)
I1123 19:59:25.634852 34600 net.cpp:122] Setting up relu4
I1123 19:59:25.634852 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.634852 34600 net.cpp:137] Memory required for data: 318055600
I1123 19:59:25.634852 34600 layer_factory.cpp:58] Creating layer conv4_1
I1123 19:59:25.634852 34600 net.cpp:84] Creating Layer conv4_1
I1123 19:59:25.634852 34600 net.cpp:406] conv4_1 <- conv4
I1123 19:59:25.634852 34600 net.cpp:380] conv4_1 -> conv4_1
I1123 19:59:25.637843 34600 net.cpp:122] Setting up conv4_1
I1123 19:59:25.637843 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.637843 34600 net.cpp:137] Memory required for data: 325223600
I1123 19:59:25.637843 34600 layer_factory.cpp:58] Creating layer bn4_1
I1123 19:59:25.637843 34600 net.cpp:84] Creating Layer bn4_1
I1123 19:59:25.637843 34600 net.cpp:406] bn4_1 <- conv4_1
I1123 19:59:25.637843 34600 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 19:59:25.638344 34600 net.cpp:122] Setting up bn4_1
I1123 19:59:25.638344 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.638344 34600 net.cpp:137] Memory required for data: 332391600
I1123 19:59:25.638344 34600 layer_factory.cpp:58] Creating layer scale4_1
I1123 19:59:25.638344 34600 net.cpp:84] Creating Layer scale4_1
I1123 19:59:25.638344 34600 net.cpp:406] scale4_1 <- conv4_1
I1123 19:59:25.638344 34600 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 19:59:25.638344 34600 layer_factory.cpp:58] Creating layer scale4_1
I1123 19:59:25.638344 34600 net.cpp:122] Setting up scale4_1
I1123 19:59:25.638344 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.638344 34600 net.cpp:137] Memory required for data: 339559600
I1123 19:59:25.638344 34600 layer_factory.cpp:58] Creating layer relu4_1
I1123 19:59:25.638344 34600 net.cpp:84] Creating Layer relu4_1
I1123 19:59:25.638344 34600 net.cpp:406] relu4_1 <- conv4_1
I1123 19:59:25.638344 34600 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 19:59:25.638857 34600 net.cpp:122] Setting up relu4_1
I1123 19:59:25.638857 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.638857 34600 net.cpp:137] Memory required for data: 346727600
I1123 19:59:25.638857 34600 layer_factory.cpp:58] Creating layer conv4_2
I1123 19:59:25.638857 34600 net.cpp:84] Creating Layer conv4_2
I1123 19:59:25.638857 34600 net.cpp:406] conv4_2 <- conv4_1
I1123 19:59:25.638857 34600 net.cpp:380] conv4_2 -> conv4_2
I1123 19:59:25.641861 34600 net.cpp:122] Setting up conv4_2
I1123 19:59:25.641861 34600 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 19:59:25.641861 34600 net.cpp:137] Memory required for data: 355431600
I1123 19:59:25.641861 34600 layer_factory.cpp:58] Creating layer bn4_2
I1123 19:59:25.642357 34600 net.cpp:84] Creating Layer bn4_2
I1123 19:59:25.642357 34600 net.cpp:406] bn4_2 <- conv4_2
I1123 19:59:25.642357 34600 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 19:59:25.642357 34600 net.cpp:122] Setting up bn4_2
I1123 19:59:25.642357 34600 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 19:59:25.642357 34600 net.cpp:137] Memory required for data: 364135600
I1123 19:59:25.642357 34600 layer_factory.cpp:58] Creating layer scale4_2
I1123 19:59:25.642357 34600 net.cpp:84] Creating Layer scale4_2
I1123 19:59:25.642357 34600 net.cpp:406] scale4_2 <- conv4_2
I1123 19:59:25.642357 34600 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 19:59:25.642357 34600 layer_factory.cpp:58] Creating layer scale4_2
I1123 19:59:25.642357 34600 net.cpp:122] Setting up scale4_2
I1123 19:59:25.642357 34600 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 19:59:25.642357 34600 net.cpp:137] Memory required for data: 372839600
I1123 19:59:25.642357 34600 layer_factory.cpp:58] Creating layer relu4_2
I1123 19:59:25.642357 34600 net.cpp:84] Creating Layer relu4_2
I1123 19:59:25.642357 34600 net.cpp:406] relu4_2 <- conv4_2
I1123 19:59:25.642357 34600 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 19:59:25.642858 34600 net.cpp:122] Setting up relu4_2
I1123 19:59:25.642858 34600 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 19:59:25.642858 34600 net.cpp:137] Memory required for data: 381543600
I1123 19:59:25.642858 34600 layer_factory.cpp:58] Creating layer pool4_2
I1123 19:59:25.642858 34600 net.cpp:84] Creating Layer pool4_2
I1123 19:59:25.642858 34600 net.cpp:406] pool4_2 <- conv4_2
I1123 19:59:25.642858 34600 net.cpp:380] pool4_2 -> pool4_2
I1123 19:59:25.642858 34600 net.cpp:122] Setting up pool4_2
I1123 19:59:25.642858 34600 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1123 19:59:25.642858 34600 net.cpp:137] Memory required for data: 383719600
I1123 19:59:25.642858 34600 layer_factory.cpp:58] Creating layer conv12
I1123 19:59:25.642858 34600 net.cpp:84] Creating Layer conv12
I1123 19:59:25.642858 34600 net.cpp:406] conv12 <- pool4_2
I1123 19:59:25.642858 34600 net.cpp:380] conv12 -> conv12
I1123 19:59:25.647361 34600 net.cpp:122] Setting up conv12
I1123 19:59:25.647361 34600 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 19:59:25.647361 34600 net.cpp:137] Memory required for data: 386023600
I1123 19:59:25.647361 34600 layer_factory.cpp:58] Creating layer bn_conv12
I1123 19:59:25.647361 34600 net.cpp:84] Creating Layer bn_conv12
I1123 19:59:25.647361 34600 net.cpp:406] bn_conv12 <- conv12
I1123 19:59:25.647361 34600 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 19:59:25.647856 34600 net.cpp:122] Setting up bn_conv12
I1123 19:59:25.647856 34600 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 19:59:25.647856 34600 net.cpp:137] Memory required for data: 388327600
I1123 19:59:25.647856 34600 layer_factory.cpp:58] Creating layer scale_conv12
I1123 19:59:25.647856 34600 net.cpp:84] Creating Layer scale_conv12
I1123 19:59:25.647856 34600 net.cpp:406] scale_conv12 <- conv12
I1123 19:59:25.647856 34600 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 19:59:25.647856 34600 layer_factory.cpp:58] Creating layer scale_conv12
I1123 19:59:25.647856 34600 net.cpp:122] Setting up scale_conv12
I1123 19:59:25.647856 34600 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 19:59:25.647856 34600 net.cpp:137] Memory required for data: 390631600
I1123 19:59:25.647856 34600 layer_factory.cpp:58] Creating layer relu_conv12
I1123 19:59:25.647856 34600 net.cpp:84] Creating Layer relu_conv12
I1123 19:59:25.647856 34600 net.cpp:406] relu_conv12 <- conv12
I1123 19:59:25.647856 34600 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 19:59:25.648357 34600 net.cpp:122] Setting up relu_conv12
I1123 19:59:25.648357 34600 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 19:59:25.648357 34600 net.cpp:137] Memory required for data: 392935600
I1123 19:59:25.648357 34600 layer_factory.cpp:58] Creating layer poolcp6
I1123 19:59:25.648357 34600 net.cpp:84] Creating Layer poolcp6
I1123 19:59:25.648357 34600 net.cpp:406] poolcp6 <- conv12
I1123 19:59:25.648357 34600 net.cpp:380] poolcp6 -> poolcp6
I1123 19:59:25.648357 34600 net.cpp:122] Setting up poolcp6
I1123 19:59:25.648357 34600 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1123 19:59:25.648357 34600 net.cpp:137] Memory required for data: 392971600
I1123 19:59:25.648357 34600 layer_factory.cpp:58] Creating layer ip1
I1123 19:59:25.648357 34600 net.cpp:84] Creating Layer ip1
I1123 19:59:25.648357 34600 net.cpp:406] ip1 <- poolcp6
I1123 19:59:25.648357 34600 net.cpp:380] ip1 -> ip1
I1123 19:59:25.648357 34600 net.cpp:122] Setting up ip1
I1123 19:59:25.648357 34600 net.cpp:129] Top shape: 100 10 (1000)
I1123 19:59:25.648357 34600 net.cpp:137] Memory required for data: 392975600
I1123 19:59:25.648357 34600 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 19:59:25.648357 34600 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 19:59:25.648357 34600 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 19:59:25.648357 34600 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 19:59:25.648357 34600 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 19:59:25.648357 34600 net.cpp:122] Setting up ip1_ip1_0_split
I1123 19:59:25.648856 34600 net.cpp:129] Top shape: 100 10 (1000)
I1123 19:59:25.648856 34600 net.cpp:129] Top shape: 100 10 (1000)
I1123 19:59:25.648856 34600 net.cpp:137] Memory required for data: 392983600
I1123 19:59:25.648856 34600 layer_factory.cpp:58] Creating layer accuracy_training
I1123 19:59:25.648856 34600 net.cpp:84] Creating Layer accuracy_training
I1123 19:59:25.648856 34600 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1123 19:59:25.648856 34600 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1123 19:59:25.648856 34600 net.cpp:380] accuracy_training -> accuracy_training
I1123 19:59:25.648856 34600 net.cpp:122] Setting up accuracy_training
I1123 19:59:25.648856 34600 net.cpp:129] Top shape: (1)
I1123 19:59:25.648856 34600 net.cpp:137] Memory required for data: 392983604
I1123 19:59:25.648856 34600 layer_factory.cpp:58] Creating layer loss
I1123 19:59:25.648856 34600 net.cpp:84] Creating Layer loss
I1123 19:59:25.648856 34600 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 19:59:25.648856 34600 net.cpp:406] loss <- label_cifar_1_split_1
I1123 19:59:25.648856 34600 net.cpp:380] loss -> loss
I1123 19:59:25.648856 34600 layer_factory.cpp:58] Creating layer loss
I1123 19:59:25.648856 34600 net.cpp:122] Setting up loss
I1123 19:59:25.648856 34600 net.cpp:129] Top shape: (1)
I1123 19:59:25.648856 34600 net.cpp:132]     with loss weight 1
I1123 19:59:25.648856 34600 net.cpp:137] Memory required for data: 392983608
I1123 19:59:25.648856 34600 net.cpp:198] loss needs backward computation.
I1123 19:59:25.648856 34600 net.cpp:200] accuracy_training does not need backward computation.
I1123 19:59:25.648856 34600 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 19:59:25.648856 34600 net.cpp:198] ip1 needs backward computation.
I1123 19:59:25.648856 34600 net.cpp:198] poolcp6 needs backward computation.
I1123 19:59:25.648856 34600 net.cpp:198] relu_conv12 needs backward computation.
I1123 19:59:25.648856 34600 net.cpp:198] scale_conv12 needs backward computation.
I1123 19:59:25.648856 34600 net.cpp:198] bn_conv12 needs backward computation.
I1123 19:59:25.648856 34600 net.cpp:198] conv12 needs backward computation.
I1123 19:59:25.648856 34600 net.cpp:198] pool4_2 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] relu4_2 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] scale4_2 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] bn4_2 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] conv4_2 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] relu4_1 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] scale4_1 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] bn4_1 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] conv4_1 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] relu4 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] scale4 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] bn4 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] conv4 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] relu3 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] scale3 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] bn3 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] conv3 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] pool2_1 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] relu2_2 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] scale2_2 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] bn2_2 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] conv2_2 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] relu2 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] scale2 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] bn2 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] conv2 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] relu1 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] scale1 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] bn1 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:198] conv1 needs backward computation.
I1123 19:59:25.649358 34600 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 19:59:25.649358 34600 net.cpp:200] cifar does not need backward computation.
I1123 19:59:25.649358 34600 net.cpp:242] This network produces output accuracy_training
I1123 19:59:25.649358 34600 net.cpp:242] This network produces output loss
I1123 19:59:25.649358 34600 net.cpp:255] Network initialization done.
I1123 19:59:25.649857 34600 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 19:59:25.649857 34600 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 19:59:25.649857 34600 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 19:59:25.649857 34600 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1123 19:59:25.649857 34600 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1123 19:59:25.649857 34600 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1123 19:59:25.649857 34600 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1123 19:59:25.649857 34600 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1123 19:59:25.649857 34600 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1123 19:59:25.649857 34600 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1123 19:59:25.649857 34600 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1123 19:59:25.649857 34600 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1123 19:59:25.649857 34600 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1123 19:59:25.650357 34600 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_7x7_1.6M"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 41
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 43
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 85
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 3
    kernel_size: 7
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 19:59:25.650357 34600 layer_factory.cpp:58] Creating layer cifar
I1123 19:59:25.656903 34600 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1123 19:59:25.656903 34600 net.cpp:84] Creating Layer cifar
I1123 19:59:25.656903 34600 net.cpp:380] cifar -> data
I1123 19:59:25.656903 34600 net.cpp:380] cifar -> label
I1123 19:59:25.656903 34600 data_layer.cpp:45] output data size: 100,3,32,32
I1123 19:59:25.662883 34600 net.cpp:122] Setting up cifar
I1123 19:59:25.662883 34600 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 19:59:25.662883 34600 net.cpp:129] Top shape: 100 (100)
I1123 19:59:25.662883 34600 net.cpp:137] Memory required for data: 1229200
I1123 19:59:25.662883 34600 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 19:59:25.662883 34600 net.cpp:84] Creating Layer label_cifar_1_split
I1123 19:59:25.662883 34600 net.cpp:406] label_cifar_1_split <- label
I1123 19:59:25.662883 34600 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 19:59:25.662883 34600 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 19:59:25.662883 34600 net.cpp:122] Setting up label_cifar_1_split
I1123 19:59:25.662883 34600 net.cpp:129] Top shape: 100 (100)
I1123 19:59:25.662883 34600 net.cpp:129] Top shape: 100 (100)
I1123 19:59:25.662883 34600 net.cpp:137] Memory required for data: 1230000
I1123 19:59:25.662883 34600 layer_factory.cpp:58] Creating layer conv1
I1123 19:59:25.662883 34600 net.cpp:84] Creating Layer conv1
I1123 19:59:25.662883 34600 net.cpp:406] conv1 <- data
I1123 19:59:25.662883 34600 net.cpp:380] conv1 -> conv1
I1123 19:59:25.663885 30128 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 19:59:25.664901 34600 net.cpp:122] Setting up conv1
I1123 19:59:25.664901 34600 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 19:59:25.664901 34600 net.cpp:137] Memory required for data: 18023600
I1123 19:59:25.664901 34600 layer_factory.cpp:58] Creating layer bn1
I1123 19:59:25.664901 34600 net.cpp:84] Creating Layer bn1
I1123 19:59:25.664901 34600 net.cpp:406] bn1 <- conv1
I1123 19:59:25.664901 34600 net.cpp:367] bn1 -> conv1 (in-place)
I1123 19:59:25.664901 34600 net.cpp:122] Setting up bn1
I1123 19:59:25.664901 34600 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 19:59:25.664901 34600 net.cpp:137] Memory required for data: 34817200
I1123 19:59:25.664901 34600 layer_factory.cpp:58] Creating layer scale1
I1123 19:59:25.664901 34600 net.cpp:84] Creating Layer scale1
I1123 19:59:25.664901 34600 net.cpp:406] scale1 <- conv1
I1123 19:59:25.664901 34600 net.cpp:367] scale1 -> conv1 (in-place)
I1123 19:59:25.664901 34600 layer_factory.cpp:58] Creating layer scale1
I1123 19:59:25.664901 34600 net.cpp:122] Setting up scale1
I1123 19:59:25.664901 34600 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 19:59:25.664901 34600 net.cpp:137] Memory required for data: 51610800
I1123 19:59:25.664901 34600 layer_factory.cpp:58] Creating layer relu1
I1123 19:59:25.664901 34600 net.cpp:84] Creating Layer relu1
I1123 19:59:25.664901 34600 net.cpp:406] relu1 <- conv1
I1123 19:59:25.664901 34600 net.cpp:367] relu1 -> conv1 (in-place)
I1123 19:59:25.665899 34600 net.cpp:122] Setting up relu1
I1123 19:59:25.665899 34600 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 19:59:25.665899 34600 net.cpp:137] Memory required for data: 68404400
I1123 19:59:25.665899 34600 layer_factory.cpp:58] Creating layer conv2
I1123 19:59:25.665899 34600 net.cpp:84] Creating Layer conv2
I1123 19:59:25.665899 34600 net.cpp:406] conv2 <- conv1
I1123 19:59:25.665899 34600 net.cpp:380] conv2 -> conv2
I1123 19:59:25.667899 34600 net.cpp:122] Setting up conv2
I1123 19:59:25.667899 34600 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 19:59:25.667899 34600 net.cpp:137] Memory required for data: 86017200
I1123 19:59:25.667899 34600 layer_factory.cpp:58] Creating layer bn2
I1123 19:59:25.667899 34600 net.cpp:84] Creating Layer bn2
I1123 19:59:25.667899 34600 net.cpp:406] bn2 <- conv2
I1123 19:59:25.667899 34600 net.cpp:367] bn2 -> conv2 (in-place)
I1123 19:59:25.667899 34600 net.cpp:122] Setting up bn2
I1123 19:59:25.667899 34600 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 19:59:25.667899 34600 net.cpp:137] Memory required for data: 103630000
I1123 19:59:25.667899 34600 layer_factory.cpp:58] Creating layer scale2
I1123 19:59:25.667899 34600 net.cpp:84] Creating Layer scale2
I1123 19:59:25.667899 34600 net.cpp:406] scale2 <- conv2
I1123 19:59:25.667899 34600 net.cpp:367] scale2 -> conv2 (in-place)
I1123 19:59:25.667899 34600 layer_factory.cpp:58] Creating layer scale2
I1123 19:59:25.667899 34600 net.cpp:122] Setting up scale2
I1123 19:59:25.667899 34600 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 19:59:25.667899 34600 net.cpp:137] Memory required for data: 121242800
I1123 19:59:25.667899 34600 layer_factory.cpp:58] Creating layer relu2
I1123 19:59:25.667899 34600 net.cpp:84] Creating Layer relu2
I1123 19:59:25.667899 34600 net.cpp:406] relu2 <- conv2
I1123 19:59:25.667899 34600 net.cpp:367] relu2 -> conv2 (in-place)
I1123 19:59:25.668893 34600 net.cpp:122] Setting up relu2
I1123 19:59:25.668893 34600 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 19:59:25.668893 34600 net.cpp:137] Memory required for data: 138855600
I1123 19:59:25.668893 34600 layer_factory.cpp:58] Creating layer conv2_2
I1123 19:59:25.668893 34600 net.cpp:84] Creating Layer conv2_2
I1123 19:59:25.668893 34600 net.cpp:406] conv2_2 <- conv2
I1123 19:59:25.668893 34600 net.cpp:380] conv2_2 -> conv2_2
I1123 19:59:25.670900 34600 net.cpp:122] Setting up conv2_2
I1123 19:59:25.670900 34600 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 19:59:25.670900 34600 net.cpp:137] Memory required for data: 167527600
I1123 19:59:25.670900 34600 layer_factory.cpp:58] Creating layer bn2_2
I1123 19:59:25.670900 34600 net.cpp:84] Creating Layer bn2_2
I1123 19:59:25.670900 34600 net.cpp:406] bn2_2 <- conv2_2
I1123 19:59:25.670900 34600 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 19:59:25.670900 34600 net.cpp:122] Setting up bn2_2
I1123 19:59:25.670900 34600 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 19:59:25.670900 34600 net.cpp:137] Memory required for data: 196199600
I1123 19:59:25.670900 34600 layer_factory.cpp:58] Creating layer scale2_2
I1123 19:59:25.670900 34600 net.cpp:84] Creating Layer scale2_2
I1123 19:59:25.670900 34600 net.cpp:406] scale2_2 <- conv2_2
I1123 19:59:25.670900 34600 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 19:59:25.670900 34600 layer_factory.cpp:58] Creating layer scale2_2
I1123 19:59:25.670900 34600 net.cpp:122] Setting up scale2_2
I1123 19:59:25.670900 34600 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 19:59:25.670900 34600 net.cpp:137] Memory required for data: 224871600
I1123 19:59:25.670900 34600 layer_factory.cpp:58] Creating layer relu2_2
I1123 19:59:25.670900 34600 net.cpp:84] Creating Layer relu2_2
I1123 19:59:25.670900 34600 net.cpp:406] relu2_2 <- conv2_2
I1123 19:59:25.670900 34600 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 19:59:25.671895 34600 net.cpp:122] Setting up relu2_2
I1123 19:59:25.671895 34600 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 19:59:25.671895 34600 net.cpp:137] Memory required for data: 253543600
I1123 19:59:25.671895 34600 layer_factory.cpp:58] Creating layer pool2_1
I1123 19:59:25.671895 34600 net.cpp:84] Creating Layer pool2_1
I1123 19:59:25.671895 34600 net.cpp:406] pool2_1 <- conv2_2
I1123 19:59:25.671895 34600 net.cpp:380] pool2_1 -> pool2_1
I1123 19:59:25.671895 34600 net.cpp:122] Setting up pool2_1
I1123 19:59:25.671895 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.671895 34600 net.cpp:137] Memory required for data: 260711600
I1123 19:59:25.671895 34600 layer_factory.cpp:58] Creating layer conv3
I1123 19:59:25.671895 34600 net.cpp:84] Creating Layer conv3
I1123 19:59:25.671895 34600 net.cpp:406] conv3 <- pool2_1
I1123 19:59:25.671895 34600 net.cpp:380] conv3 -> conv3
I1123 19:59:25.673899 34600 net.cpp:122] Setting up conv3
I1123 19:59:25.673899 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.673899 34600 net.cpp:137] Memory required for data: 267879600
I1123 19:59:25.673899 34600 layer_factory.cpp:58] Creating layer bn3
I1123 19:59:25.673899 34600 net.cpp:84] Creating Layer bn3
I1123 19:59:25.673899 34600 net.cpp:406] bn3 <- conv3
I1123 19:59:25.673899 34600 net.cpp:367] bn3 -> conv3 (in-place)
I1123 19:59:25.673899 34600 net.cpp:122] Setting up bn3
I1123 19:59:25.673899 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.673899 34600 net.cpp:137] Memory required for data: 275047600
I1123 19:59:25.673899 34600 layer_factory.cpp:58] Creating layer scale3
I1123 19:59:25.674898 34600 net.cpp:84] Creating Layer scale3
I1123 19:59:25.674898 34600 net.cpp:406] scale3 <- conv3
I1123 19:59:25.674898 34600 net.cpp:367] scale3 -> conv3 (in-place)
I1123 19:59:25.674898 34600 layer_factory.cpp:58] Creating layer scale3
I1123 19:59:25.674898 34600 net.cpp:122] Setting up scale3
I1123 19:59:25.674898 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.674898 34600 net.cpp:137] Memory required for data: 282215600
I1123 19:59:25.674898 34600 layer_factory.cpp:58] Creating layer relu3
I1123 19:59:25.674898 34600 net.cpp:84] Creating Layer relu3
I1123 19:59:25.674898 34600 net.cpp:406] relu3 <- conv3
I1123 19:59:25.674898 34600 net.cpp:367] relu3 -> conv3 (in-place)
I1123 19:59:25.674898 34600 net.cpp:122] Setting up relu3
I1123 19:59:25.674898 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.674898 34600 net.cpp:137] Memory required for data: 289383600
I1123 19:59:25.674898 34600 layer_factory.cpp:58] Creating layer conv4
I1123 19:59:25.674898 34600 net.cpp:84] Creating Layer conv4
I1123 19:59:25.674898 34600 net.cpp:406] conv4 <- conv3
I1123 19:59:25.674898 34600 net.cpp:380] conv4 -> conv4
I1123 19:59:25.678886 34600 net.cpp:122] Setting up conv4
I1123 19:59:25.678886 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.678886 34600 net.cpp:137] Memory required for data: 296551600
I1123 19:59:25.678886 34600 layer_factory.cpp:58] Creating layer bn4
I1123 19:59:25.678886 34600 net.cpp:84] Creating Layer bn4
I1123 19:59:25.678886 34600 net.cpp:406] bn4 <- conv4
I1123 19:59:25.678886 34600 net.cpp:367] bn4 -> conv4 (in-place)
I1123 19:59:25.678886 34600 net.cpp:122] Setting up bn4
I1123 19:59:25.678886 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.678886 34600 net.cpp:137] Memory required for data: 303719600
I1123 19:59:25.678886 34600 layer_factory.cpp:58] Creating layer scale4
I1123 19:59:25.678886 34600 net.cpp:84] Creating Layer scale4
I1123 19:59:25.678886 34600 net.cpp:406] scale4 <- conv4
I1123 19:59:25.678886 34600 net.cpp:367] scale4 -> conv4 (in-place)
I1123 19:59:25.678886 34600 layer_factory.cpp:58] Creating layer scale4
I1123 19:59:25.678886 34600 net.cpp:122] Setting up scale4
I1123 19:59:25.678886 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.678886 34600 net.cpp:137] Memory required for data: 310887600
I1123 19:59:25.678886 34600 layer_factory.cpp:58] Creating layer relu4
I1123 19:59:25.678886 34600 net.cpp:84] Creating Layer relu4
I1123 19:59:25.678886 34600 net.cpp:406] relu4 <- conv4
I1123 19:59:25.678886 34600 net.cpp:367] relu4 -> conv4 (in-place)
I1123 19:59:25.678886 34600 net.cpp:122] Setting up relu4
I1123 19:59:25.678886 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.678886 34600 net.cpp:137] Memory required for data: 318055600
I1123 19:59:25.678886 34600 layer_factory.cpp:58] Creating layer conv4_1
I1123 19:59:25.678886 34600 net.cpp:84] Creating Layer conv4_1
I1123 19:59:25.678886 34600 net.cpp:406] conv4_1 <- conv4
I1123 19:59:25.678886 34600 net.cpp:380] conv4_1 -> conv4_1
I1123 19:59:25.681898 34600 net.cpp:122] Setting up conv4_1
I1123 19:59:25.681898 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.681898 34600 net.cpp:137] Memory required for data: 325223600
I1123 19:59:25.681898 34600 layer_factory.cpp:58] Creating layer bn4_1
I1123 19:59:25.681898 34600 net.cpp:84] Creating Layer bn4_1
I1123 19:59:25.681898 34600 net.cpp:406] bn4_1 <- conv4_1
I1123 19:59:25.681898 34600 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 19:59:25.681898 34600 net.cpp:122] Setting up bn4_1
I1123 19:59:25.681898 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.681898 34600 net.cpp:137] Memory required for data: 332391600
I1123 19:59:25.681898 34600 layer_factory.cpp:58] Creating layer scale4_1
I1123 19:59:25.681898 34600 net.cpp:84] Creating Layer scale4_1
I1123 19:59:25.681898 34600 net.cpp:406] scale4_1 <- conv4_1
I1123 19:59:25.681898 34600 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 19:59:25.681898 34600 layer_factory.cpp:58] Creating layer scale4_1
I1123 19:59:25.681898 34600 net.cpp:122] Setting up scale4_1
I1123 19:59:25.681898 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.681898 34600 net.cpp:137] Memory required for data: 339559600
I1123 19:59:25.681898 34600 layer_factory.cpp:58] Creating layer relu4_1
I1123 19:59:25.681898 34600 net.cpp:84] Creating Layer relu4_1
I1123 19:59:25.681898 34600 net.cpp:406] relu4_1 <- conv4_1
I1123 19:59:25.681898 34600 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 19:59:25.681898 34600 net.cpp:122] Setting up relu4_1
I1123 19:59:25.681898 34600 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 19:59:25.681898 34600 net.cpp:137] Memory required for data: 346727600
I1123 19:59:25.681898 34600 layer_factory.cpp:58] Creating layer conv4_2
I1123 19:59:25.681898 34600 net.cpp:84] Creating Layer conv4_2
I1123 19:59:25.681898 34600 net.cpp:406] conv4_2 <- conv4_1
I1123 19:59:25.681898 34600 net.cpp:380] conv4_2 -> conv4_2
I1123 19:59:25.685885 34600 net.cpp:122] Setting up conv4_2
I1123 19:59:25.685885 34600 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 19:59:25.685885 34600 net.cpp:137] Memory required for data: 355431600
I1123 19:59:25.685885 34600 layer_factory.cpp:58] Creating layer bn4_2
I1123 19:59:25.685885 34600 net.cpp:84] Creating Layer bn4_2
I1123 19:59:25.685885 34600 net.cpp:406] bn4_2 <- conv4_2
I1123 19:59:25.685885 34600 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 19:59:25.685885 34600 net.cpp:122] Setting up bn4_2
I1123 19:59:25.685885 34600 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 19:59:25.685885 34600 net.cpp:137] Memory required for data: 364135600
I1123 19:59:25.685885 34600 layer_factory.cpp:58] Creating layer scale4_2
I1123 19:59:25.685885 34600 net.cpp:84] Creating Layer scale4_2
I1123 19:59:25.685885 34600 net.cpp:406] scale4_2 <- conv4_2
I1123 19:59:25.685885 34600 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 19:59:25.685885 34600 layer_factory.cpp:58] Creating layer scale4_2
I1123 19:59:25.685885 34600 net.cpp:122] Setting up scale4_2
I1123 19:59:25.685885 34600 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 19:59:25.685885 34600 net.cpp:137] Memory required for data: 372839600
I1123 19:59:25.685885 34600 layer_factory.cpp:58] Creating layer relu4_2
I1123 19:59:25.685885 34600 net.cpp:84] Creating Layer relu4_2
I1123 19:59:25.685885 34600 net.cpp:406] relu4_2 <- conv4_2
I1123 19:59:25.685885 34600 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 19:59:25.686900 34600 net.cpp:122] Setting up relu4_2
I1123 19:59:25.686900 34600 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 19:59:25.686900 34600 net.cpp:137] Memory required for data: 381543600
I1123 19:59:25.686900 34600 layer_factory.cpp:58] Creating layer pool4_2
I1123 19:59:25.686900 34600 net.cpp:84] Creating Layer pool4_2
I1123 19:59:25.686900 34600 net.cpp:406] pool4_2 <- conv4_2
I1123 19:59:25.686900 34600 net.cpp:380] pool4_2 -> pool4_2
I1123 19:59:25.686900 34600 net.cpp:122] Setting up pool4_2
I1123 19:59:25.686900 34600 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1123 19:59:25.686900 34600 net.cpp:137] Memory required for data: 383719600
I1123 19:59:25.686900 34600 layer_factory.cpp:58] Creating layer conv12
I1123 19:59:25.686900 34600 net.cpp:84] Creating Layer conv12
I1123 19:59:25.686900 34600 net.cpp:406] conv12 <- pool4_2
I1123 19:59:25.686900 34600 net.cpp:380] conv12 -> conv12
I1123 19:59:25.689898 34600 net.cpp:122] Setting up conv12
I1123 19:59:25.689898 34600 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 19:59:25.689898 34600 net.cpp:137] Memory required for data: 386023600
I1123 19:59:25.689898 34600 layer_factory.cpp:58] Creating layer bn_conv12
I1123 19:59:25.689898 34600 net.cpp:84] Creating Layer bn_conv12
I1123 19:59:25.689898 34600 net.cpp:406] bn_conv12 <- conv12
I1123 19:59:25.689898 34600 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 19:59:25.689898 34600 net.cpp:122] Setting up bn_conv12
I1123 19:59:25.689898 34600 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 19:59:25.689898 34600 net.cpp:137] Memory required for data: 388327600
I1123 19:59:25.689898 34600 layer_factory.cpp:58] Creating layer scale_conv12
I1123 19:59:25.689898 34600 net.cpp:84] Creating Layer scale_conv12
I1123 19:59:25.689898 34600 net.cpp:406] scale_conv12 <- conv12
I1123 19:59:25.689898 34600 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 19:59:25.690898 34600 layer_factory.cpp:58] Creating layer scale_conv12
I1123 19:59:25.690898 34600 net.cpp:122] Setting up scale_conv12
I1123 19:59:25.690898 34600 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 19:59:25.690898 34600 net.cpp:137] Memory required for data: 390631600
I1123 19:59:25.690898 34600 layer_factory.cpp:58] Creating layer relu_conv12
I1123 19:59:25.690898 34600 net.cpp:84] Creating Layer relu_conv12
I1123 19:59:25.690898 34600 net.cpp:406] relu_conv12 <- conv12
I1123 19:59:25.690898 34600 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 19:59:25.690898 34600 net.cpp:122] Setting up relu_conv12
I1123 19:59:25.690898 34600 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 19:59:25.690898 34600 net.cpp:137] Memory required for data: 392935600
I1123 19:59:25.690898 34600 layer_factory.cpp:58] Creating layer poolcp6
I1123 19:59:25.690898 34600 net.cpp:84] Creating Layer poolcp6
I1123 19:59:25.690898 34600 net.cpp:406] poolcp6 <- conv12
I1123 19:59:25.690898 34600 net.cpp:380] poolcp6 -> poolcp6
I1123 19:59:25.690898 34600 net.cpp:122] Setting up poolcp6
I1123 19:59:25.690898 34600 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1123 19:59:25.690898 34600 net.cpp:137] Memory required for data: 392971600
I1123 19:59:25.690898 34600 layer_factory.cpp:58] Creating layer ip1
I1123 19:59:25.690898 34600 net.cpp:84] Creating Layer ip1
I1123 19:59:25.690898 34600 net.cpp:406] ip1 <- poolcp6
I1123 19:59:25.690898 34600 net.cpp:380] ip1 -> ip1
I1123 19:59:25.690898 34600 net.cpp:122] Setting up ip1
I1123 19:59:25.690898 34600 net.cpp:129] Top shape: 100 10 (1000)
I1123 19:59:25.690898 34600 net.cpp:137] Memory required for data: 392975600
I1123 19:59:25.690898 34600 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 19:59:25.690898 34600 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 19:59:25.690898 34600 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 19:59:25.690898 34600 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 19:59:25.690898 34600 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 19:59:25.691898 34600 net.cpp:122] Setting up ip1_ip1_0_split
I1123 19:59:25.691898 34600 net.cpp:129] Top shape: 100 10 (1000)
I1123 19:59:25.691898 34600 net.cpp:129] Top shape: 100 10 (1000)
I1123 19:59:25.691898 34600 net.cpp:137] Memory required for data: 392983600
I1123 19:59:25.691898 34600 layer_factory.cpp:58] Creating layer accuracy
I1123 19:59:25.691898 34600 net.cpp:84] Creating Layer accuracy
I1123 19:59:25.691898 34600 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1123 19:59:25.691898 34600 net.cpp:406] accuracy <- label_cifar_1_split_0
I1123 19:59:25.691898 34600 net.cpp:380] accuracy -> accuracy
I1123 19:59:25.691898 34600 net.cpp:122] Setting up accuracy
I1123 19:59:25.691898 34600 net.cpp:129] Top shape: (1)
I1123 19:59:25.691898 34600 net.cpp:137] Memory required for data: 392983604
I1123 19:59:25.691898 34600 layer_factory.cpp:58] Creating layer loss
I1123 19:59:25.691898 34600 net.cpp:84] Creating Layer loss
I1123 19:59:25.691898 34600 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 19:59:25.691898 34600 net.cpp:406] loss <- label_cifar_1_split_1
I1123 19:59:25.691898 34600 net.cpp:380] loss -> loss
I1123 19:59:25.691898 34600 layer_factory.cpp:58] Creating layer loss
I1123 19:59:25.691898 34600 net.cpp:122] Setting up loss
I1123 19:59:25.691898 34600 net.cpp:129] Top shape: (1)
I1123 19:59:25.691898 34600 net.cpp:132]     with loss weight 1
I1123 19:59:25.691898 34600 net.cpp:137] Memory required for data: 392983608
I1123 19:59:25.691898 34600 net.cpp:198] loss needs backward computation.
I1123 19:59:25.691898 34600 net.cpp:200] accuracy does not need backward computation.
I1123 19:59:25.691898 34600 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 19:59:25.691898 34600 net.cpp:198] ip1 needs backward computation.
I1123 19:59:25.691898 34600 net.cpp:198] poolcp6 needs backward computation.
I1123 19:59:25.691898 34600 net.cpp:198] relu_conv12 needs backward computation.
I1123 19:59:25.691898 34600 net.cpp:198] scale_conv12 needs backward computation.
I1123 19:59:25.691898 34600 net.cpp:198] bn_conv12 needs backward computation.
I1123 19:59:25.691898 34600 net.cpp:198] conv12 needs backward computation.
I1123 19:59:25.691898 34600 net.cpp:198] pool4_2 needs backward computation.
I1123 19:59:25.691898 34600 net.cpp:198] relu4_2 needs backward computation.
I1123 19:59:25.691898 34600 net.cpp:198] scale4_2 needs backward computation.
I1123 19:59:25.691898 34600 net.cpp:198] bn4_2 needs backward computation.
I1123 19:59:25.691898 34600 net.cpp:198] conv4_2 needs backward computation.
I1123 19:59:25.691898 34600 net.cpp:198] relu4_1 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] scale4_1 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] bn4_1 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] conv4_1 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] relu4 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] scale4 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] bn4 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] conv4 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] relu3 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] scale3 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] bn3 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] conv3 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] pool2_1 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] relu2_2 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] scale2_2 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] bn2_2 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] conv2_2 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] relu2 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] scale2 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] bn2 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] conv2 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] relu1 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] scale1 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] bn1 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:198] conv1 needs backward computation.
I1123 19:59:25.692884 34600 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 19:59:25.692884 34600 net.cpp:200] cifar does not need backward computation.
I1123 19:59:25.692884 34600 net.cpp:242] This network produces output accuracy
I1123 19:59:25.692884 34600 net.cpp:242] This network produces output loss
I1123 19:59:25.692884 34600 net.cpp:255] Network initialization done.
I1123 19:59:25.692884 34600 solver.cpp:56] Solver scaffolding done.
I1123 19:59:25.694895 34600 caffe.cpp:249] Starting Optimization
I1123 19:59:25.694895 34600 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_7x7_1.6M
I1123 19:59:25.694895 34600 solver.cpp:273] Learning Rate Policy: multistep
I1123 19:59:25.697883 34600 solver.cpp:330] Iteration 0, Testing net (#0)
I1123 19:59:25.699884 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 19:59:29.759336 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:59:29.922371 34600 solver.cpp:397]     Test net output #0: accuracy = 0.1
I1123 19:59:29.922371 34600 solver.cpp:397]     Test net output #1: loss = 78.6029 (* 1 = 78.6029 loss)
I1123 19:59:30.078415 34600 solver.cpp:218] Iteration 0 (0 iter/s, 4.38234s/100 iters), loss = 3.45264
I1123 19:59:30.078415 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.11
I1123 19:59:30.078415 34600 solver.cpp:237]     Train net output #1: loss = 3.45264 (* 1 = 3.45264 loss)
I1123 19:59:30.078415 34600 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1123 19:59:42.117650 34600 solver.cpp:218] Iteration 100 (8.30635 iter/s, 12.039s/100 iters), loss = 1.70281
I1123 19:59:42.117650 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.35
I1123 19:59:42.117650 34600 solver.cpp:237]     Train net output #1: loss = 1.70281 (* 1 = 1.70281 loss)
I1123 19:59:42.117650 34600 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1123 19:59:54.165066 34600 solver.cpp:218] Iteration 200 (8.3011 iter/s, 12.0466s/100 iters), loss = 1.78213
I1123 19:59:54.165066 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.33
I1123 19:59:54.165066 34600 solver.cpp:237]     Train net output #1: loss = 1.78213 (* 1 = 1.78213 loss)
I1123 19:59:54.165565 34600 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1123 20:00:06.219756 34600 solver.cpp:218] Iteration 300 (8.29561 iter/s, 12.0546s/100 iters), loss = 1.55016
I1123 20:00:06.220757 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.5
I1123 20:00:06.220757 34600 solver.cpp:237]     Train net output #1: loss = 1.55016 (* 1 = 1.55016 loss)
I1123 20:00:06.220757 34600 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1123 20:00:18.281352 34600 solver.cpp:218] Iteration 400 (8.29185 iter/s, 12.06s/100 iters), loss = 1.36908
I1123 20:00:18.281352 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.5
I1123 20:00:18.281352 34600 solver.cpp:237]     Train net output #1: loss = 1.36908 (* 1 = 1.36908 loss)
I1123 20:00:18.281352 34600 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1123 20:00:29.740972 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:00:30.220217 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_500.caffemodel
I1123 20:00:30.270242 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_500.solverstate
I1123 20:00:30.291240 34600 solver.cpp:330] Iteration 500, Testing net (#0)
I1123 20:00:30.291240 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:00:34.299476 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:00:34.463114 34600 solver.cpp:397]     Test net output #0: accuracy = 0.4148
I1123 20:00:34.463114 34600 solver.cpp:397]     Test net output #1: loss = 1.58674 (* 1 = 1.58674 loss)
I1123 20:00:34.579661 34600 solver.cpp:218] Iteration 500 (6.13562 iter/s, 16.2983s/100 iters), loss = 1.39873
I1123 20:00:34.579661 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.52
I1123 20:00:34.579661 34600 solver.cpp:237]     Train net output #1: loss = 1.39873 (* 1 = 1.39873 loss)
I1123 20:00:34.579661 34600 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1123 20:00:46.631606 34600 solver.cpp:218] Iteration 600 (8.29799 iter/s, 12.0511s/100 iters), loss = 1.22004
I1123 20:00:46.631606 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.61
I1123 20:00:46.632104 34600 solver.cpp:237]     Train net output #1: loss = 1.22004 (* 1 = 1.22004 loss)
I1123 20:00:46.632104 34600 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1123 20:00:58.689123 34600 solver.cpp:218] Iteration 700 (8.29395 iter/s, 12.057s/100 iters), loss = 1.05886
I1123 20:00:58.689630 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.56
I1123 20:00:58.689630 34600 solver.cpp:237]     Train net output #1: loss = 1.05886 (* 1 = 1.05886 loss)
I1123 20:00:58.689630 34600 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1123 20:01:10.741328 34600 solver.cpp:218] Iteration 800 (8.29797 iter/s, 12.0511s/100 iters), loss = 0.942023
I1123 20:01:10.741328 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1123 20:01:10.741328 34600 solver.cpp:237]     Train net output #1: loss = 0.942023 (* 1 = 0.942023 loss)
I1123 20:01:10.741328 34600 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1123 20:01:22.795574 34600 solver.cpp:218] Iteration 900 (8.29595 iter/s, 12.0541s/100 iters), loss = 0.974585
I1123 20:01:22.795574 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1123 20:01:22.795574 34600 solver.cpp:237]     Train net output #1: loss = 0.974585 (* 1 = 0.974585 loss)
I1123 20:01:22.795574 34600 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1123 20:01:34.249218 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:01:34.727236 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_1000.caffemodel
I1123 20:01:34.768720 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_1000.solverstate
I1123 20:01:34.789238 34600 solver.cpp:330] Iteration 1000, Testing net (#0)
I1123 20:01:34.789238 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:01:38.827605 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:01:38.993753 34600 solver.cpp:397]     Test net output #0: accuracy = 0.4753
I1123 20:01:38.994740 34600 solver.cpp:397]     Test net output #1: loss = 1.43732 (* 1 = 1.43732 loss)
I1123 20:01:39.111789 34600 solver.cpp:218] Iteration 1000 (6.12913 iter/s, 16.3155s/100 iters), loss = 1.06846
I1123 20:01:39.111789 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1123 20:01:39.111789 34600 solver.cpp:237]     Train net output #1: loss = 1.06846 (* 1 = 1.06846 loss)
I1123 20:01:39.111789 34600 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1123 20:01:51.170356 34600 solver.cpp:218] Iteration 1100 (8.29343 iter/s, 12.0577s/100 iters), loss = 0.928709
I1123 20:01:51.170356 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1123 20:01:51.170356 34600 solver.cpp:237]     Train net output #1: loss = 0.928709 (* 1 = 0.928709 loss)
I1123 20:01:51.170856 34600 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1123 20:02:03.224436 34600 solver.cpp:218] Iteration 1200 (8.29653 iter/s, 12.0532s/100 iters), loss = 0.894564
I1123 20:02:03.224436 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1123 20:02:03.224436 34600 solver.cpp:237]     Train net output #1: loss = 0.894564 (* 1 = 0.894564 loss)
I1123 20:02:03.224436 34600 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1123 20:02:15.274065 34600 solver.cpp:218] Iteration 1300 (8.29942 iter/s, 12.049s/100 iters), loss = 0.769007
I1123 20:02:15.274065 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1123 20:02:15.274065 34600 solver.cpp:237]     Train net output #1: loss = 0.769007 (* 1 = 0.769007 loss)
I1123 20:02:15.274065 34600 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1123 20:02:27.324478 34600 solver.cpp:218] Iteration 1400 (8.29909 iter/s, 12.0495s/100 iters), loss = 0.859889
I1123 20:02:27.324478 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1123 20:02:27.324478 34600 solver.cpp:237]     Train net output #1: loss = 0.859889 (* 1 = 0.859889 loss)
I1123 20:02:27.324478 34600 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1123 20:02:38.783414 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:02:39.261930 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_1500.caffemodel
I1123 20:02:39.300415 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_1500.solverstate
I1123 20:02:39.319934 34600 solver.cpp:330] Iteration 1500, Testing net (#0)
I1123 20:02:39.320418 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:02:43.365228 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:02:43.531282 34600 solver.cpp:397]     Test net output #0: accuracy = 0.5277
I1123 20:02:43.531282 34600 solver.cpp:397]     Test net output #1: loss = 1.34845 (* 1 = 1.34845 loss)
I1123 20:02:43.649318 34600 solver.cpp:218] Iteration 1500 (6.12566 iter/s, 16.3248s/100 iters), loss = 0.8316
I1123 20:02:43.649318 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 20:02:43.649318 34600 solver.cpp:237]     Train net output #1: loss = 0.8316 (* 1 = 0.8316 loss)
I1123 20:02:43.649318 34600 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1123 20:02:55.705421 34600 solver.cpp:218] Iteration 1600 (8.2953 iter/s, 12.055s/100 iters), loss = 0.633005
I1123 20:02:55.705421 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 20:02:55.705421 34600 solver.cpp:237]     Train net output #1: loss = 0.633005 (* 1 = 0.633005 loss)
I1123 20:02:55.705421 34600 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1123 20:03:07.755832 34600 solver.cpp:218] Iteration 1700 (8.29906 iter/s, 12.0496s/100 iters), loss = 0.732342
I1123 20:03:07.755832 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 20:03:07.755832 34600 solver.cpp:237]     Train net output #1: loss = 0.732342 (* 1 = 0.732342 loss)
I1123 20:03:07.755832 34600 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1123 20:03:19.802059 34600 solver.cpp:218] Iteration 1800 (8.3015 iter/s, 12.046s/100 iters), loss = 0.726796
I1123 20:03:19.802559 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 20:03:19.802559 34600 solver.cpp:237]     Train net output #1: loss = 0.726796 (* 1 = 0.726796 loss)
I1123 20:03:19.802559 34600 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1123 20:03:31.856407 34600 solver.cpp:218] Iteration 1900 (8.29638 iter/s, 12.0535s/100 iters), loss = 0.654271
I1123 20:03:31.856407 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 20:03:31.856407 34600 solver.cpp:237]     Train net output #1: loss = 0.654271 (* 1 = 0.654271 loss)
I1123 20:03:31.856407 34600 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1123 20:03:43.312392 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:03:43.789875 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_2000.caffemodel
I1123 20:03:43.829896 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_2000.solverstate
I1123 20:03:43.850898 34600 solver.cpp:330] Iteration 2000, Testing net (#0)
I1123 20:03:43.850898 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:03:47.895906 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:03:48.060994 34600 solver.cpp:397]     Test net output #0: accuracy = 0.6637
I1123 20:03:48.060994 34600 solver.cpp:397]     Test net output #1: loss = 0.966489 (* 1 = 0.966489 loss)
I1123 20:03:48.180044 34600 solver.cpp:218] Iteration 2000 (6.12642 iter/s, 16.3227s/100 iters), loss = 0.569644
I1123 20:03:48.180044 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 20:03:48.180044 34600 solver.cpp:237]     Train net output #1: loss = 0.569644 (* 1 = 0.569644 loss)
I1123 20:03:48.180044 34600 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1123 20:04:00.227661 34600 solver.cpp:218] Iteration 2100 (8.30059 iter/s, 12.0473s/100 iters), loss = 0.602767
I1123 20:04:00.228147 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 20:04:00.228147 34600 solver.cpp:237]     Train net output #1: loss = 0.602767 (* 1 = 0.602767 loss)
I1123 20:04:00.228147 34600 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1123 20:04:12.279021 34600 solver.cpp:218] Iteration 2200 (8.29846 iter/s, 12.0504s/100 iters), loss = 0.632737
I1123 20:04:12.279021 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 20:04:12.279021 34600 solver.cpp:237]     Train net output #1: loss = 0.632737 (* 1 = 0.632737 loss)
I1123 20:04:12.279021 34600 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1123 20:04:24.325865 34600 solver.cpp:218] Iteration 2300 (8.30126 iter/s, 12.0464s/100 iters), loss = 0.641973
I1123 20:04:24.325865 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1123 20:04:24.325865 34600 solver.cpp:237]     Train net output #1: loss = 0.641973 (* 1 = 0.641973 loss)
I1123 20:04:24.325865 34600 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1123 20:04:36.378625 34600 solver.cpp:218] Iteration 2400 (8.29723 iter/s, 12.0522s/100 iters), loss = 0.694772
I1123 20:04:36.378625 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 20:04:36.378625 34600 solver.cpp:237]     Train net output #1: loss = 0.694772 (* 1 = 0.694772 loss)
I1123 20:04:36.378625 34600 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1123 20:04:47.828302 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:04:48.307330 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_2500.caffemodel
I1123 20:04:48.348351 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_2500.solverstate
I1123 20:04:48.367869 34600 solver.cpp:330] Iteration 2500, Testing net (#0)
I1123 20:04:48.367869 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:04:52.404745 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:04:52.569802 34600 solver.cpp:397]     Test net output #0: accuracy = 0.582
I1123 20:04:52.569802 34600 solver.cpp:397]     Test net output #1: loss = 1.22275 (* 1 = 1.22275 loss)
I1123 20:04:52.687855 34600 solver.cpp:218] Iteration 2500 (6.13164 iter/s, 16.3089s/100 iters), loss = 0.529076
I1123 20:04:52.687855 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 20:04:52.687855 34600 solver.cpp:237]     Train net output #1: loss = 0.529076 (* 1 = 0.529076 loss)
I1123 20:04:52.687855 34600 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1123 20:05:04.747596 34600 solver.cpp:218] Iteration 2600 (8.29273 iter/s, 12.0588s/100 iters), loss = 0.60144
I1123 20:05:04.747596 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 20:05:04.747596 34600 solver.cpp:237]     Train net output #1: loss = 0.60144 (* 1 = 0.60144 loss)
I1123 20:05:04.747596 34600 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1123 20:05:16.805495 34600 solver.cpp:218] Iteration 2700 (8.29368 iter/s, 12.0574s/100 iters), loss = 0.578183
I1123 20:05:16.805495 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 20:05:16.805495 34600 solver.cpp:237]     Train net output #1: loss = 0.578183 (* 1 = 0.578183 loss)
I1123 20:05:16.805495 34600 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1123 20:05:28.859081 34600 solver.cpp:218] Iteration 2800 (8.29689 iter/s, 12.0527s/100 iters), loss = 0.715058
I1123 20:05:28.859081 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 20:05:28.859081 34600 solver.cpp:237]     Train net output #1: loss = 0.715058 (* 1 = 0.715058 loss)
I1123 20:05:28.859081 34600 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1123 20:05:40.911093 34600 solver.cpp:218] Iteration 2900 (8.29768 iter/s, 12.0516s/100 iters), loss = 0.574316
I1123 20:05:40.911093 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 20:05:40.911093 34600 solver.cpp:237]     Train net output #1: loss = 0.574316 (* 1 = 0.574316 loss)
I1123 20:05:40.911093 34600 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1123 20:05:52.365613 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:05:52.844600 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_3000.caffemodel
I1123 20:05:52.888640 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_3000.solverstate
I1123 20:05:52.909659 34600 solver.cpp:330] Iteration 3000, Testing net (#0)
I1123 20:05:52.909659 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:05:56.945323 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:05:57.109354 34600 solver.cpp:397]     Test net output #0: accuracy = 0.6585
I1123 20:05:57.109354 34600 solver.cpp:397]     Test net output #1: loss = 0.964582 (* 1 = 0.964582 loss)
I1123 20:05:57.228003 34600 solver.cpp:218] Iteration 3000 (6.12896 iter/s, 16.316s/100 iters), loss = 0.536722
I1123 20:05:57.228003 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 20:05:57.228003 34600 solver.cpp:237]     Train net output #1: loss = 0.536722 (* 1 = 0.536722 loss)
I1123 20:05:57.228003 34600 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1123 20:06:09.279626 34600 solver.cpp:218] Iteration 3100 (8.29788 iter/s, 12.0513s/100 iters), loss = 0.571469
I1123 20:06:09.279626 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 20:06:09.279626 34600 solver.cpp:237]     Train net output #1: loss = 0.571469 (* 1 = 0.571469 loss)
I1123 20:06:09.279626 34600 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1123 20:06:21.344758 34600 solver.cpp:218] Iteration 3200 (8.28892 iter/s, 12.0643s/100 iters), loss = 0.581199
I1123 20:06:21.344758 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 20:06:21.344758 34600 solver.cpp:237]     Train net output #1: loss = 0.581199 (* 1 = 0.581199 loss)
I1123 20:06:21.344758 34600 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1123 20:06:33.398685 34600 solver.cpp:218] Iteration 3300 (8.29657 iter/s, 12.0532s/100 iters), loss = 0.598325
I1123 20:06:33.398685 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 20:06:33.398685 34600 solver.cpp:237]     Train net output #1: loss = 0.598325 (* 1 = 0.598325 loss)
I1123 20:06:33.398685 34600 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1123 20:06:45.444535 34600 solver.cpp:218] Iteration 3400 (8.30151 iter/s, 12.046s/100 iters), loss = 0.594728
I1123 20:06:45.444535 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1123 20:06:45.444535 34600 solver.cpp:237]     Train net output #1: loss = 0.594728 (* 1 = 0.594728 loss)
I1123 20:06:45.444535 34600 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1123 20:06:56.899976 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:06:57.378000 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_3500.caffemodel
I1123 20:06:57.418519 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_3500.solverstate
I1123 20:06:57.440017 34600 solver.cpp:330] Iteration 3500, Testing net (#0)
I1123 20:06:57.440017 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:07:01.480132 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:07:01.645205 34600 solver.cpp:397]     Test net output #0: accuracy = 0.4887
I1123 20:07:01.645205 34600 solver.cpp:397]     Test net output #1: loss = 1.5425 (* 1 = 1.5425 loss)
I1123 20:07:01.763224 34600 solver.cpp:218] Iteration 3500 (6.1285 iter/s, 16.3172s/100 iters), loss = 0.536044
I1123 20:07:01.763224 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 20:07:01.763224 34600 solver.cpp:237]     Train net output #1: loss = 0.536044 (* 1 = 0.536044 loss)
I1123 20:07:01.763224 34600 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1123 20:07:13.824146 34600 solver.cpp:218] Iteration 3600 (8.29153 iter/s, 12.0605s/100 iters), loss = 0.535974
I1123 20:07:13.824146 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 20:07:13.824146 34600 solver.cpp:237]     Train net output #1: loss = 0.535974 (* 1 = 0.535974 loss)
I1123 20:07:13.824146 34600 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1123 20:07:25.869206 34600 solver.cpp:218] Iteration 3700 (8.30261 iter/s, 12.0444s/100 iters), loss = 0.553656
I1123 20:07:25.869206 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 20:07:25.869206 34600 solver.cpp:237]     Train net output #1: loss = 0.553656 (* 1 = 0.553656 loss)
I1123 20:07:25.869206 34600 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1123 20:07:37.919230 34600 solver.cpp:218] Iteration 3800 (8.29892 iter/s, 12.0498s/100 iters), loss = 0.54367
I1123 20:07:37.919230 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 20:07:37.919230 34600 solver.cpp:237]     Train net output #1: loss = 0.54367 (* 1 = 0.54367 loss)
I1123 20:07:37.919230 34600 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1123 20:07:49.973170 34600 solver.cpp:218] Iteration 3900 (8.29681 iter/s, 12.0528s/100 iters), loss = 0.623528
I1123 20:07:49.973170 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 20:07:49.973170 34600 solver.cpp:237]     Train net output #1: loss = 0.623528 (* 1 = 0.623528 loss)
I1123 20:07:49.973170 34600 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1123 20:08:01.424401 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:08:01.903434 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_4000.caffemodel
I1123 20:08:01.941933 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_4000.solverstate
I1123 20:08:01.962433 34600 solver.cpp:330] Iteration 4000, Testing net (#0)
I1123 20:08:01.962433 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:08:06.004457 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:08:06.170506 34600 solver.cpp:397]     Test net output #0: accuracy = 0.688
I1123 20:08:06.170506 34600 solver.cpp:397]     Test net output #1: loss = 0.923755 (* 1 = 0.923755 loss)
I1123 20:08:06.288547 34600 solver.cpp:218] Iteration 4000 (6.12917 iter/s, 16.3154s/100 iters), loss = 0.455255
I1123 20:08:06.288547 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 20:08:06.288547 34600 solver.cpp:237]     Train net output #1: loss = 0.455255 (* 1 = 0.455255 loss)
I1123 20:08:06.288547 34600 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1123 20:08:18.341508 34600 solver.cpp:218] Iteration 4100 (8.29738 iter/s, 12.052s/100 iters), loss = 0.438762
I1123 20:08:18.342028 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 20:08:18.342028 34600 solver.cpp:237]     Train net output #1: loss = 0.438762 (* 1 = 0.438762 loss)
I1123 20:08:18.342028 34600 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1123 20:08:30.393468 34600 solver.cpp:218] Iteration 4200 (8.29801 iter/s, 12.0511s/100 iters), loss = 0.534801
I1123 20:08:30.393468 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 20:08:30.393468 34600 solver.cpp:237]     Train net output #1: loss = 0.534801 (* 1 = 0.534801 loss)
I1123 20:08:30.393468 34600 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1123 20:08:42.438303 34600 solver.cpp:218] Iteration 4300 (8.30269 iter/s, 12.0443s/100 iters), loss = 0.547778
I1123 20:08:42.438303 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 20:08:42.438303 34600 solver.cpp:237]     Train net output #1: loss = 0.547778 (* 1 = 0.547778 loss)
I1123 20:08:42.438303 34600 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1123 20:08:54.489136 34600 solver.cpp:218] Iteration 4400 (8.29853 iter/s, 12.0503s/100 iters), loss = 0.530239
I1123 20:08:54.489136 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 20:08:54.489136 34600 solver.cpp:237]     Train net output #1: loss = 0.530239 (* 1 = 0.530239 loss)
I1123 20:08:54.489136 34600 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1123 20:09:05.939950 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:09:06.418113 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_4500.caffemodel
I1123 20:09:06.457614 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_4500.solverstate
I1123 20:09:06.478113 34600 solver.cpp:330] Iteration 4500, Testing net (#0)
I1123 20:09:06.478113 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:09:10.521090 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:09:10.687141 34600 solver.cpp:397]     Test net output #0: accuracy = 0.5584
I1123 20:09:10.688143 34600 solver.cpp:397]     Test net output #1: loss = 1.46289 (* 1 = 1.46289 loss)
I1123 20:09:10.805699 34600 solver.cpp:218] Iteration 4500 (6.12904 iter/s, 16.3158s/100 iters), loss = 0.414791
I1123 20:09:10.805699 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 20:09:10.805699 34600 solver.cpp:237]     Train net output #1: loss = 0.414791 (* 1 = 0.414791 loss)
I1123 20:09:10.805699 34600 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1123 20:09:22.859290 34600 solver.cpp:218] Iteration 4600 (8.29688 iter/s, 12.0527s/100 iters), loss = 0.430434
I1123 20:09:22.859290 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 20:09:22.859290 34600 solver.cpp:237]     Train net output #1: loss = 0.430434 (* 1 = 0.430434 loss)
I1123 20:09:22.859290 34600 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1123 20:09:34.908782 34600 solver.cpp:218] Iteration 4700 (8.2991 iter/s, 12.0495s/100 iters), loss = 0.475315
I1123 20:09:34.908782 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 20:09:34.908782 34600 solver.cpp:237]     Train net output #1: loss = 0.475315 (* 1 = 0.475315 loss)
I1123 20:09:34.908782 34600 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1123 20:09:46.960561 34600 solver.cpp:218] Iteration 4800 (8.29835 iter/s, 12.0506s/100 iters), loss = 0.540784
I1123 20:09:46.960561 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 20:09:46.960561 34600 solver.cpp:237]     Train net output #1: loss = 0.540784 (* 1 = 0.540784 loss)
I1123 20:09:46.960561 34600 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1123 20:09:59.007967 34600 solver.cpp:218] Iteration 4900 (8.3009 iter/s, 12.0469s/100 iters), loss = 0.469891
I1123 20:09:59.007967 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 20:09:59.007967 34600 solver.cpp:237]     Train net output #1: loss = 0.469891 (* 1 = 0.469891 loss)
I1123 20:09:59.007967 34600 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1123 20:10:10.497499 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:10:10.978052 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_5000.caffemodel
I1123 20:10:11.018551 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_5000.solverstate
I1123 20:10:11.039032 34600 solver.cpp:330] Iteration 5000, Testing net (#0)
I1123 20:10:11.039032 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:10:15.083523 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:10:15.248567 34600 solver.cpp:397]     Test net output #0: accuracy = 0.5826
I1123 20:10:15.248567 34600 solver.cpp:397]     Test net output #1: loss = 1.37145 (* 1 = 1.37145 loss)
I1123 20:10:15.366603 34600 solver.cpp:218] Iteration 5000 (6.11326 iter/s, 16.3579s/100 iters), loss = 0.455484
I1123 20:10:15.366603 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 20:10:15.366603 34600 solver.cpp:237]     Train net output #1: loss = 0.455484 (* 1 = 0.455484 loss)
I1123 20:10:15.366603 34600 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1123 20:10:15.366603 34600 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1123 20:10:27.421511 34600 solver.cpp:218] Iteration 5100 (8.29569 iter/s, 12.0545s/100 iters), loss = 0.344313
I1123 20:10:27.422013 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 20:10:27.422013 34600 solver.cpp:237]     Train net output #1: loss = 0.344313 (* 1 = 0.344313 loss)
I1123 20:10:27.422013 34600 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1123 20:10:39.469573 34600 solver.cpp:218] Iteration 5200 (8.30071 iter/s, 12.0472s/100 iters), loss = 0.328698
I1123 20:10:39.469573 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 20:10:39.469573 34600 solver.cpp:237]     Train net output #1: loss = 0.328698 (* 1 = 0.328698 loss)
I1123 20:10:39.469573 34600 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1123 20:10:51.519603 34600 solver.cpp:218] Iteration 5300 (8.29914 iter/s, 12.0494s/100 iters), loss = 0.352141
I1123 20:10:51.519603 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 20:10:51.519603 34600 solver.cpp:237]     Train net output #1: loss = 0.352141 (* 1 = 0.352141 loss)
I1123 20:10:51.519603 34600 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1123 20:11:03.575218 34600 solver.cpp:218] Iteration 5400 (8.29535 iter/s, 12.0549s/100 iters), loss = 0.269251
I1123 20:11:03.575218 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 20:11:03.575218 34600 solver.cpp:237]     Train net output #1: loss = 0.269251 (* 1 = 0.269251 loss)
I1123 20:11:03.575218 34600 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1123 20:11:15.025913 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:11:15.503428 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_5500.caffemodel
I1123 20:11:15.541932 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_5500.solverstate
I1123 20:11:15.561931 34600 solver.cpp:330] Iteration 5500, Testing net (#0)
I1123 20:11:15.562433 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:11:19.605434 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:11:19.770998 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8574
I1123 20:11:19.770998 34600 solver.cpp:397]     Test net output #1: loss = 0.411599 (* 1 = 0.411599 loss)
I1123 20:11:19.889030 34600 solver.cpp:218] Iteration 5500 (6.12995 iter/s, 16.3133s/100 iters), loss = 0.24541
I1123 20:11:19.889030 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:11:19.889030 34600 solver.cpp:237]     Train net output #1: loss = 0.24541 (* 1 = 0.24541 loss)
I1123 20:11:19.889030 34600 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1123 20:11:31.940721 34600 solver.cpp:218] Iteration 5600 (8.2982 iter/s, 12.0508s/100 iters), loss = 0.351192
I1123 20:11:31.940721 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 20:11:31.940721 34600 solver.cpp:237]     Train net output #1: loss = 0.351192 (* 1 = 0.351192 loss)
I1123 20:11:31.940721 34600 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1123 20:11:43.990579 34600 solver.cpp:218] Iteration 5700 (8.29919 iter/s, 12.0494s/100 iters), loss = 0.306461
I1123 20:11:43.990579 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 20:11:43.990579 34600 solver.cpp:237]     Train net output #1: loss = 0.30646 (* 1 = 0.30646 loss)
I1123 20:11:43.990579 34600 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1123 20:11:56.044946 34600 solver.cpp:218] Iteration 5800 (8.2957 iter/s, 12.0544s/100 iters), loss = 0.319034
I1123 20:11:56.045945 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 20:11:56.045945 34600 solver.cpp:237]     Train net output #1: loss = 0.319034 (* 1 = 0.319034 loss)
I1123 20:11:56.045945 34600 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1123 20:12:08.091578 34600 solver.cpp:218] Iteration 5900 (8.30192 iter/s, 12.0454s/100 iters), loss = 0.204356
I1123 20:12:08.091578 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:12:08.091578 34600 solver.cpp:237]     Train net output #1: loss = 0.204356 (* 1 = 0.204356 loss)
I1123 20:12:08.091578 34600 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1123 20:12:19.544003 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:12:20.024709 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_6000.caffemodel
I1123 20:12:20.066196 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_6000.solverstate
I1123 20:12:20.086218 34600 solver.cpp:330] Iteration 6000, Testing net (#0)
I1123 20:12:20.086218 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:12:24.127359 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:12:24.293421 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8673
I1123 20:12:24.293421 34600 solver.cpp:397]     Test net output #1: loss = 0.387262 (* 1 = 0.387262 loss)
I1123 20:12:24.411439 34600 solver.cpp:218] Iteration 6000 (6.12768 iter/s, 16.3194s/100 iters), loss = 0.257083
I1123 20:12:24.411439 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 20:12:24.411439 34600 solver.cpp:237]     Train net output #1: loss = 0.257083 (* 1 = 0.257083 loss)
I1123 20:12:24.411439 34600 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1123 20:12:36.461081 34600 solver.cpp:218] Iteration 6100 (8.29966 iter/s, 12.0487s/100 iters), loss = 0.323431
I1123 20:12:36.461081 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 20:12:36.461081 34600 solver.cpp:237]     Train net output #1: loss = 0.323431 (* 1 = 0.323431 loss)
I1123 20:12:36.461081 34600 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1123 20:12:48.503790 34600 solver.cpp:218] Iteration 6200 (8.30442 iter/s, 12.0418s/100 iters), loss = 0.217743
I1123 20:12:48.503790 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:12:48.503790 34600 solver.cpp:237]     Train net output #1: loss = 0.217743 (* 1 = 0.217743 loss)
I1123 20:12:48.503790 34600 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1123 20:13:00.559882 34600 solver.cpp:218] Iteration 6300 (8.29498 iter/s, 12.0555s/100 iters), loss = 0.280648
I1123 20:13:00.559882 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 20:13:00.559882 34600 solver.cpp:237]     Train net output #1: loss = 0.280648 (* 1 = 0.280648 loss)
I1123 20:13:00.559882 34600 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1123 20:13:12.612144 34600 solver.cpp:218] Iteration 6400 (8.29707 iter/s, 12.0525s/100 iters), loss = 0.191674
I1123 20:13:12.612144 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:13:12.612144 34600 solver.cpp:237]     Train net output #1: loss = 0.191674 (* 1 = 0.191674 loss)
I1123 20:13:12.612144 34600 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1123 20:13:24.066963 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:13:24.545085 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_6500.caffemodel
I1123 20:13:24.584076 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_6500.solverstate
I1123 20:13:24.605077 34600 solver.cpp:330] Iteration 6500, Testing net (#0)
I1123 20:13:24.605077 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:13:28.647389 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:13:28.813429 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8702
I1123 20:13:28.813429 34600 solver.cpp:397]     Test net output #1: loss = 0.377519 (* 1 = 0.377519 loss)
I1123 20:13:28.931917 34600 solver.cpp:218] Iteration 6500 (6.12813 iter/s, 16.3182s/100 iters), loss = 0.189904
I1123 20:13:28.931917 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:13:28.931917 34600 solver.cpp:237]     Train net output #1: loss = 0.189904 (* 1 = 0.189904 loss)
I1123 20:13:28.931917 34600 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1123 20:13:40.983187 34600 solver.cpp:218] Iteration 6600 (8.29809 iter/s, 12.051s/100 iters), loss = 0.239336
I1123 20:13:40.983687 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:13:40.983687 34600 solver.cpp:237]     Train net output #1: loss = 0.239336 (* 1 = 0.239336 loss)
I1123 20:13:40.983687 34600 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1123 20:13:53.035640 34600 solver.cpp:218] Iteration 6700 (8.29761 iter/s, 12.0517s/100 iters), loss = 0.216685
I1123 20:13:53.035640 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:13:53.035640 34600 solver.cpp:237]     Train net output #1: loss = 0.216685 (* 1 = 0.216685 loss)
I1123 20:13:53.035640 34600 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1123 20:14:05.090414 34600 solver.cpp:218] Iteration 6800 (8.29578 iter/s, 12.0543s/100 iters), loss = 0.242152
I1123 20:14:05.090919 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:14:05.090919 34600 solver.cpp:237]     Train net output #1: loss = 0.242152 (* 1 = 0.242152 loss)
I1123 20:14:05.090919 34600 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1123 20:14:17.143920 34600 solver.cpp:218] Iteration 6900 (8.29702 iter/s, 12.0525s/100 iters), loss = 0.155305
I1123 20:14:17.143920 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:14:17.143920 34600 solver.cpp:237]     Train net output #1: loss = 0.155305 (* 1 = 0.155305 loss)
I1123 20:14:17.143920 34600 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1123 20:14:28.589956 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:14:29.069461 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_7000.caffemodel
I1123 20:14:29.107496 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_7000.solverstate
I1123 20:14:29.127514 34600 solver.cpp:330] Iteration 7000, Testing net (#0)
I1123 20:14:29.127514 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:14:33.162951 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:14:33.328364 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8694
I1123 20:14:33.328364 34600 solver.cpp:397]     Test net output #1: loss = 0.380175 (* 1 = 0.380175 loss)
I1123 20:14:33.446894 34600 solver.cpp:218] Iteration 7000 (6.1341 iter/s, 16.3023s/100 iters), loss = 0.187706
I1123 20:14:33.446894 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:14:33.446894 34600 solver.cpp:237]     Train net output #1: loss = 0.187706 (* 1 = 0.187706 loss)
I1123 20:14:33.446894 34600 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1123 20:14:45.506695 34600 solver.cpp:218] Iteration 7100 (8.29236 iter/s, 12.0593s/100 iters), loss = 0.238026
I1123 20:14:45.506695 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:14:45.506695 34600 solver.cpp:237]     Train net output #1: loss = 0.238026 (* 1 = 0.238026 loss)
I1123 20:14:45.506695 34600 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1123 20:14:57.566071 34600 solver.cpp:218] Iteration 7200 (8.2926 iter/s, 12.0589s/100 iters), loss = 0.197546
I1123 20:14:57.566570 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:14:57.566570 34600 solver.cpp:237]     Train net output #1: loss = 0.197546 (* 1 = 0.197546 loss)
I1123 20:14:57.566570 34600 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1123 20:15:09.614534 34600 solver.cpp:218] Iteration 7300 (8.30023 iter/s, 12.0479s/100 iters), loss = 0.200549
I1123 20:15:09.615038 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:15:09.615038 34600 solver.cpp:237]     Train net output #1: loss = 0.200549 (* 1 = 0.200549 loss)
I1123 20:15:09.615038 34600 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1123 20:15:21.669975 34600 solver.cpp:218] Iteration 7400 (8.29542 iter/s, 12.0548s/100 iters), loss = 0.168051
I1123 20:15:21.670476 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:15:21.670476 34600 solver.cpp:237]     Train net output #1: loss = 0.168051 (* 1 = 0.168051 loss)
I1123 20:15:21.670476 34600 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1123 20:15:33.123188 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:15:33.601825 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_7500.caffemodel
I1123 20:15:33.643307 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_7500.solverstate
I1123 20:15:33.663825 34600 solver.cpp:330] Iteration 7500, Testing net (#0)
I1123 20:15:33.664307 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:15:37.704021 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:15:37.870084 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8625
I1123 20:15:37.870084 34600 solver.cpp:397]     Test net output #1: loss = 0.392066 (* 1 = 0.392066 loss)
I1123 20:15:37.988690 34600 solver.cpp:218] Iteration 7500 (6.12829 iter/s, 16.3178s/100 iters), loss = 0.190508
I1123 20:15:37.988690 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:15:37.988690 34600 solver.cpp:237]     Train net output #1: loss = 0.190508 (* 1 = 0.190508 loss)
I1123 20:15:37.988690 34600 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1123 20:15:50.042742 34600 solver.cpp:218] Iteration 7600 (8.29604 iter/s, 12.0539s/100 iters), loss = 0.238308
I1123 20:15:50.042742 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:15:50.042742 34600 solver.cpp:237]     Train net output #1: loss = 0.238308 (* 1 = 0.238308 loss)
I1123 20:15:50.042742 34600 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1123 20:16:02.095273 34600 solver.cpp:218] Iteration 7700 (8.29764 iter/s, 12.0516s/100 iters), loss = 0.175499
I1123 20:16:02.095273 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:16:02.095273 34600 solver.cpp:237]     Train net output #1: loss = 0.175499 (* 1 = 0.175499 loss)
I1123 20:16:02.095273 34600 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1123 20:16:14.149384 34600 solver.cpp:218] Iteration 7800 (8.2963 iter/s, 12.0536s/100 iters), loss = 0.237455
I1123 20:16:14.149883 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 20:16:14.149883 34600 solver.cpp:237]     Train net output #1: loss = 0.237455 (* 1 = 0.237455 loss)
I1123 20:16:14.149883 34600 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1123 20:16:26.196071 34600 solver.cpp:218] Iteration 7900 (8.30156 iter/s, 12.0459s/100 iters), loss = 0.16599
I1123 20:16:26.196071 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 20:16:26.196071 34600 solver.cpp:237]     Train net output #1: loss = 0.16599 (* 1 = 0.16599 loss)
I1123 20:16:26.196071 34600 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1123 20:16:37.646126 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:16:38.125649 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_8000.caffemodel
I1123 20:16:38.165668 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_8000.solverstate
I1123 20:16:38.191167 34600 solver.cpp:330] Iteration 8000, Testing net (#0)
I1123 20:16:38.191167 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:16:42.232074 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:16:42.398620 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8745
I1123 20:16:42.398620 34600 solver.cpp:397]     Test net output #1: loss = 0.371051 (* 1 = 0.371051 loss)
I1123 20:16:42.517154 34600 solver.cpp:218] Iteration 8000 (6.12745 iter/s, 16.32s/100 iters), loss = 0.171384
I1123 20:16:42.517154 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:16:42.517154 34600 solver.cpp:237]     Train net output #1: loss = 0.171384 (* 1 = 0.171384 loss)
I1123 20:16:42.517154 34600 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1123 20:16:54.568111 34600 solver.cpp:218] Iteration 8100 (8.29842 iter/s, 12.0505s/100 iters), loss = 0.286125
I1123 20:16:54.568111 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 20:16:54.568111 34600 solver.cpp:237]     Train net output #1: loss = 0.286125 (* 1 = 0.286125 loss)
I1123 20:16:54.568111 34600 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1123 20:17:06.615162 34600 solver.cpp:218] Iteration 8200 (8.30097 iter/s, 12.0468s/100 iters), loss = 0.17495
I1123 20:17:06.615662 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:17:06.615662 34600 solver.cpp:237]     Train net output #1: loss = 0.17495 (* 1 = 0.17495 loss)
I1123 20:17:06.615662 34600 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1123 20:17:18.661667 34600 solver.cpp:218] Iteration 8300 (8.30166 iter/s, 12.0458s/100 iters), loss = 0.188824
I1123 20:17:18.661667 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:17:18.661667 34600 solver.cpp:237]     Train net output #1: loss = 0.188824 (* 1 = 0.188824 loss)
I1123 20:17:18.661667 34600 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1123 20:17:30.709564 34600 solver.cpp:218] Iteration 8400 (8.30084 iter/s, 12.047s/100 iters), loss = 0.1992
I1123 20:17:30.709564 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:17:30.709564 34600 solver.cpp:237]     Train net output #1: loss = 0.1992 (* 1 = 0.1992 loss)
I1123 20:17:30.709564 34600 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1123 20:17:42.158594 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:17:42.637604 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_8500.caffemodel
I1123 20:17:42.676605 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_8500.solverstate
I1123 20:17:42.697096 34600 solver.cpp:330] Iteration 8500, Testing net (#0)
I1123 20:17:42.697096 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:17:46.736764 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:17:46.902267 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8687
I1123 20:17:46.902267 34600 solver.cpp:397]     Test net output #1: loss = 0.387549 (* 1 = 0.387549 loss)
I1123 20:17:47.019829 34600 solver.cpp:218] Iteration 8500 (6.13134 iter/s, 16.3097s/100 iters), loss = 0.131334
I1123 20:17:47.019829 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:17:47.019829 34600 solver.cpp:237]     Train net output #1: loss = 0.131334 (* 1 = 0.131334 loss)
I1123 20:17:47.019829 34600 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1123 20:17:59.078356 34600 solver.cpp:218] Iteration 8600 (8.29329 iter/s, 12.0579s/100 iters), loss = 0.187422
I1123 20:17:59.078356 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:17:59.078356 34600 solver.cpp:237]     Train net output #1: loss = 0.187422 (* 1 = 0.187422 loss)
I1123 20:17:59.078356 34600 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1123 20:18:11.190760 34600 solver.cpp:218] Iteration 8700 (8.25649 iter/s, 12.1117s/100 iters), loss = 0.242896
I1123 20:18:11.190760 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:18:11.190760 34600 solver.cpp:237]     Train net output #1: loss = 0.242896 (* 1 = 0.242896 loss)
I1123 20:18:11.190760 34600 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1123 20:18:23.247334 34600 solver.cpp:218] Iteration 8800 (8.29436 iter/s, 12.0564s/100 iters), loss = 0.154832
I1123 20:18:23.247334 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:18:23.247334 34600 solver.cpp:237]     Train net output #1: loss = 0.154832 (* 1 = 0.154832 loss)
I1123 20:18:23.247334 34600 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1123 20:18:35.300535 34600 solver.cpp:218] Iteration 8900 (8.29695 iter/s, 12.0526s/100 iters), loss = 0.150613
I1123 20:18:35.300535 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:18:35.300535 34600 solver.cpp:237]     Train net output #1: loss = 0.150613 (* 1 = 0.150613 loss)
I1123 20:18:35.300535 34600 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1123 20:18:46.750450 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:18:47.228941 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_9000.caffemodel
I1123 20:18:47.269954 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_9000.solverstate
I1123 20:18:47.290455 34600 solver.cpp:330] Iteration 9000, Testing net (#0)
I1123 20:18:47.290455 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:18:51.323158 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:18:51.488205 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8664
I1123 20:18:51.488205 34600 solver.cpp:397]     Test net output #1: loss = 0.394309 (* 1 = 0.394309 loss)
I1123 20:18:51.606741 34600 solver.cpp:218] Iteration 9000 (6.13311 iter/s, 16.305s/100 iters), loss = 0.158116
I1123 20:18:51.606741 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:18:51.606741 34600 solver.cpp:237]     Train net output #1: loss = 0.158116 (* 1 = 0.158116 loss)
I1123 20:18:51.606741 34600 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1123 20:19:03.670163 34600 solver.cpp:218] Iteration 9100 (8.28984 iter/s, 12.063s/100 iters), loss = 0.18644
I1123 20:19:03.670163 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:19:03.670163 34600 solver.cpp:237]     Train net output #1: loss = 0.18644 (* 1 = 0.18644 loss)
I1123 20:19:03.670163 34600 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1123 20:19:15.725611 34600 solver.cpp:218] Iteration 9200 (8.29521 iter/s, 12.0552s/100 iters), loss = 0.192125
I1123 20:19:15.725611 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 20:19:15.725611 34600 solver.cpp:237]     Train net output #1: loss = 0.192124 (* 1 = 0.192124 loss)
I1123 20:19:15.725611 34600 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1123 20:19:27.779584 34600 solver.cpp:218] Iteration 9300 (8.29638 iter/s, 12.0535s/100 iters), loss = 0.162593
I1123 20:19:27.780087 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:19:27.780087 34600 solver.cpp:237]     Train net output #1: loss = 0.162593 (* 1 = 0.162593 loss)
I1123 20:19:27.780087 34600 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1123 20:19:39.836457 34600 solver.cpp:218] Iteration 9400 (8.29455 iter/s, 12.0561s/100 iters), loss = 0.142849
I1123 20:19:39.836457 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:19:39.836457 34600 solver.cpp:237]     Train net output #1: loss = 0.142849 (* 1 = 0.142849 loss)
I1123 20:19:39.836457 34600 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1123 20:19:51.297890 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:19:51.775450 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_9500.caffemodel
I1123 20:19:51.817950 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_9500.solverstate
I1123 20:19:51.837929 34600 solver.cpp:330] Iteration 9500, Testing net (#0)
I1123 20:19:51.837929 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:19:55.872323 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:19:56.038393 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8689
I1123 20:19:56.038393 34600 solver.cpp:397]     Test net output #1: loss = 0.386149 (* 1 = 0.386149 loss)
I1123 20:19:56.156424 34600 solver.cpp:218] Iteration 9500 (6.12751 iter/s, 16.3198s/100 iters), loss = 0.166018
I1123 20:19:56.157428 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:19:56.157428 34600 solver.cpp:237]     Train net output #1: loss = 0.166018 (* 1 = 0.166018 loss)
I1123 20:19:56.157428 34600 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1123 20:19:56.157428 34600 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1123 20:20:08.207399 34600 solver.cpp:218] Iteration 9600 (8.29907 iter/s, 12.0495s/100 iters), loss = 0.182101
I1123 20:20:08.207399 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:20:08.207399 34600 solver.cpp:237]     Train net output #1: loss = 0.182101 (* 1 = 0.182101 loss)
I1123 20:20:08.207399 34600 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1123 20:20:20.263276 34600 solver.cpp:218] Iteration 9700 (8.29516 iter/s, 12.0552s/100 iters), loss = 0.129399
I1123 20:20:20.263276 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:20:20.263276 34600 solver.cpp:237]     Train net output #1: loss = 0.129399 (* 1 = 0.129399 loss)
I1123 20:20:20.263276 34600 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1123 20:20:32.317397 34600 solver.cpp:218] Iteration 9800 (8.29638 iter/s, 12.0534s/100 iters), loss = 0.129573
I1123 20:20:32.317397 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:20:32.317397 34600 solver.cpp:237]     Train net output #1: loss = 0.129573 (* 1 = 0.129573 loss)
I1123 20:20:32.317397 34600 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1123 20:20:44.374081 34600 solver.cpp:218] Iteration 9900 (8.29435 iter/s, 12.0564s/100 iters), loss = 0.141212
I1123 20:20:44.374081 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:20:44.374081 34600 solver.cpp:237]     Train net output #1: loss = 0.141212 (* 1 = 0.141212 loss)
I1123 20:20:44.374081 34600 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1123 20:20:55.829304 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:20:56.308550 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_10000.caffemodel
I1123 20:20:56.347038 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_10000.solverstate
I1123 20:20:56.367537 34600 solver.cpp:330] Iteration 10000, Testing net (#0)
I1123 20:20:56.367537 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:21:00.394876 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:21:00.559943 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8853
I1123 20:21:00.559943 34600 solver.cpp:397]     Test net output #1: loss = 0.338501 (* 1 = 0.338501 loss)
I1123 20:21:00.677979 34600 solver.cpp:218] Iteration 10000 (6.13357 iter/s, 16.3037s/100 iters), loss = 0.11582
I1123 20:21:00.677979 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:21:00.677979 34600 solver.cpp:237]     Train net output #1: loss = 0.11582 (* 1 = 0.11582 loss)
I1123 20:21:00.677979 34600 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1123 20:21:12.729406 34600 solver.cpp:218] Iteration 10100 (8.29847 iter/s, 12.0504s/100 iters), loss = 0.185196
I1123 20:21:12.729406 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 20:21:12.729406 34600 solver.cpp:237]     Train net output #1: loss = 0.185195 (* 1 = 0.185195 loss)
I1123 20:21:12.729406 34600 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1123 20:21:24.783305 34600 solver.cpp:218] Iteration 10200 (8.2966 iter/s, 12.0531s/100 iters), loss = 0.13317
I1123 20:21:24.783305 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:21:24.783305 34600 solver.cpp:237]     Train net output #1: loss = 0.13317 (* 1 = 0.13317 loss)
I1123 20:21:24.783305 34600 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1123 20:21:36.834220 34600 solver.cpp:218] Iteration 10300 (8.29849 iter/s, 12.0504s/100 iters), loss = 0.150358
I1123 20:21:36.834220 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 20:21:36.834220 34600 solver.cpp:237]     Train net output #1: loss = 0.150358 (* 1 = 0.150358 loss)
I1123 20:21:36.834220 34600 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1123 20:21:48.883828 34600 solver.cpp:218] Iteration 10400 (8.29942 iter/s, 12.049s/100 iters), loss = 0.0871156
I1123 20:21:48.883828 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:21:48.883828 34600 solver.cpp:237]     Train net output #1: loss = 0.0871155 (* 1 = 0.0871155 loss)
I1123 20:21:48.883828 34600 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1123 20:22:00.339761 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:22:00.818953 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_10500.caffemodel
I1123 20:22:00.858968 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_10500.solverstate
I1123 20:22:00.880987 34600 solver.cpp:330] Iteration 10500, Testing net (#0)
I1123 20:22:00.880987 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:22:04.919104 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:22:05.084671 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8863
I1123 20:22:05.084671 34600 solver.cpp:397]     Test net output #1: loss = 0.337132 (* 1 = 0.337132 loss)
I1123 20:22:05.202705 34600 solver.cpp:218] Iteration 10500 (6.12803 iter/s, 16.3185s/100 iters), loss = 0.114615
I1123 20:22:05.202705 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:22:05.202705 34600 solver.cpp:237]     Train net output #1: loss = 0.114615 (* 1 = 0.114615 loss)
I1123 20:22:05.202705 34600 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1123 20:22:17.255527 34600 solver.cpp:218] Iteration 10600 (8.29755 iter/s, 12.0518s/100 iters), loss = 0.18951
I1123 20:22:17.255527 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:22:17.255527 34600 solver.cpp:237]     Train net output #1: loss = 0.18951 (* 1 = 0.18951 loss)
I1123 20:22:17.255527 34600 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1123 20:22:29.312429 34600 solver.cpp:218] Iteration 10700 (8.2943 iter/s, 12.0565s/100 iters), loss = 0.089287
I1123 20:22:29.312429 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:22:29.312429 34600 solver.cpp:237]     Train net output #1: loss = 0.0892869 (* 1 = 0.0892869 loss)
I1123 20:22:29.312429 34600 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1123 20:22:41.367614 34600 solver.cpp:218] Iteration 10800 (8.29572 iter/s, 12.0544s/100 iters), loss = 0.12599
I1123 20:22:41.367614 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:22:41.367614 34600 solver.cpp:237]     Train net output #1: loss = 0.12599 (* 1 = 0.12599 loss)
I1123 20:22:41.367614 34600 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1123 20:22:53.426241 34600 solver.cpp:218] Iteration 10900 (8.29319 iter/s, 12.0581s/100 iters), loss = 0.101319
I1123 20:22:53.426241 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:22:53.426241 34600 solver.cpp:237]     Train net output #1: loss = 0.101319 (* 1 = 0.101319 loss)
I1123 20:22:53.426241 34600 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1123 20:23:04.881366 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:23:05.362845 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_11000.caffemodel
I1123 20:23:05.401846 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_11000.solverstate
I1123 20:23:05.421847 34600 solver.cpp:330] Iteration 11000, Testing net (#0)
I1123 20:23:05.421847 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:23:09.452458 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:23:09.617519 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8869
I1123 20:23:09.617519 34600 solver.cpp:397]     Test net output #1: loss = 0.338215 (* 1 = 0.338215 loss)
I1123 20:23:09.734572 34600 solver.cpp:218] Iteration 11000 (6.13186 iter/s, 16.3083s/100 iters), loss = 0.0838159
I1123 20:23:09.734572 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:23:09.735571 34600 solver.cpp:237]     Train net output #1: loss = 0.0838159 (* 1 = 0.0838159 loss)
I1123 20:23:09.735571 34600 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1123 20:23:21.792069 34600 solver.cpp:218] Iteration 11100 (8.29459 iter/s, 12.056s/100 iters), loss = 0.220418
I1123 20:23:21.792069 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:23:21.792069 34600 solver.cpp:237]     Train net output #1: loss = 0.220418 (* 1 = 0.220418 loss)
I1123 20:23:21.792069 34600 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1123 20:23:33.846403 34600 solver.cpp:218] Iteration 11200 (8.29602 iter/s, 12.054s/100 iters), loss = 0.124755
I1123 20:23:33.846403 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:23:33.846904 34600 solver.cpp:237]     Train net output #1: loss = 0.124755 (* 1 = 0.124755 loss)
I1123 20:23:33.846904 34600 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1123 20:23:45.898697 34600 solver.cpp:218] Iteration 11300 (8.2978 iter/s, 12.0514s/100 iters), loss = 0.138273
I1123 20:23:45.898697 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:23:45.898697 34600 solver.cpp:237]     Train net output #1: loss = 0.138274 (* 1 = 0.138274 loss)
I1123 20:23:45.898697 34600 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1123 20:23:57.952253 34600 solver.cpp:218] Iteration 11400 (8.29661 iter/s, 12.0531s/100 iters), loss = 0.0888392
I1123 20:23:57.952253 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:23:57.952253 34600 solver.cpp:237]     Train net output #1: loss = 0.0888394 (* 1 = 0.0888394 loss)
I1123 20:23:57.952253 34600 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1123 20:24:09.408428 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:24:09.887492 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_11500.caffemodel
I1123 20:24:09.927474 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_11500.solverstate
I1123 20:24:09.948473 34600 solver.cpp:330] Iteration 11500, Testing net (#0)
I1123 20:24:09.948473 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:24:13.988814 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:24:14.153868 34600 solver.cpp:397]     Test net output #0: accuracy = 0.888
I1123 20:24:14.153868 34600 solver.cpp:397]     Test net output #1: loss = 0.338588 (* 1 = 0.338588 loss)
I1123 20:24:14.272403 34600 solver.cpp:218] Iteration 11500 (6.12765 iter/s, 16.3195s/100 iters), loss = 0.123963
I1123 20:24:14.272403 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:24:14.272403 34600 solver.cpp:237]     Train net output #1: loss = 0.123963 (* 1 = 0.123963 loss)
I1123 20:24:14.272403 34600 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1123 20:24:26.331543 34600 solver.cpp:218] Iteration 11600 (8.29265 iter/s, 12.0589s/100 iters), loss = 0.218953
I1123 20:24:26.331543 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:24:26.331543 34600 solver.cpp:237]     Train net output #1: loss = 0.218954 (* 1 = 0.218954 loss)
I1123 20:24:26.331543 34600 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1123 20:24:38.386147 34600 solver.cpp:218] Iteration 11700 (8.2961 iter/s, 12.0539s/100 iters), loss = 0.152855
I1123 20:24:38.386147 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:24:38.386147 34600 solver.cpp:237]     Train net output #1: loss = 0.152856 (* 1 = 0.152856 loss)
I1123 20:24:38.386147 34600 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1123 20:24:50.440421 34600 solver.cpp:218] Iteration 11800 (8.29642 iter/s, 12.0534s/100 iters), loss = 0.11765
I1123 20:24:50.440421 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:24:50.440421 34600 solver.cpp:237]     Train net output #1: loss = 0.11765 (* 1 = 0.11765 loss)
I1123 20:24:50.440421 34600 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1123 20:25:02.493784 34600 solver.cpp:218] Iteration 11900 (8.29666 iter/s, 12.053s/100 iters), loss = 0.0828145
I1123 20:25:02.494279 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:25:02.494279 34600 solver.cpp:237]     Train net output #1: loss = 0.0828146 (* 1 = 0.0828146 loss)
I1123 20:25:02.494279 34600 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1123 20:25:13.949059 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:25:14.427219 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_12000.caffemodel
I1123 20:25:14.468241 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_12000.solverstate
I1123 20:25:14.488741 34600 solver.cpp:330] Iteration 12000, Testing net (#0)
I1123 20:25:14.488741 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:25:18.526403 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:25:18.691458 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8881
I1123 20:25:18.691458 34600 solver.cpp:397]     Test net output #1: loss = 0.338933 (* 1 = 0.338933 loss)
I1123 20:25:18.809494 34600 solver.cpp:218] Iteration 12000 (6.12928 iter/s, 16.3151s/100 iters), loss = 0.147178
I1123 20:25:18.809494 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:25:18.809494 34600 solver.cpp:237]     Train net output #1: loss = 0.147178 (* 1 = 0.147178 loss)
I1123 20:25:18.809494 34600 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1123 20:25:30.867530 34600 solver.cpp:218] Iteration 12100 (8.29363 iter/s, 12.0575s/100 iters), loss = 0.196177
I1123 20:25:30.867530 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:25:30.867530 34600 solver.cpp:237]     Train net output #1: loss = 0.196177 (* 1 = 0.196177 loss)
I1123 20:25:30.867530 34600 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1123 20:25:42.927479 34600 solver.cpp:218] Iteration 12200 (8.29248 iter/s, 12.0591s/100 iters), loss = 0.100316
I1123 20:25:42.927479 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:25:42.927479 34600 solver.cpp:237]     Train net output #1: loss = 0.100316 (* 1 = 0.100316 loss)
I1123 20:25:42.927479 34600 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1123 20:25:54.983790 34600 solver.cpp:218] Iteration 12300 (8.29499 iter/s, 12.0555s/100 iters), loss = 0.0721268
I1123 20:25:54.983790 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:25:54.983790 34600 solver.cpp:237]     Train net output #1: loss = 0.0721267 (* 1 = 0.0721267 loss)
I1123 20:25:54.983790 34600 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1123 20:26:07.040881 34600 solver.cpp:218] Iteration 12400 (8.29418 iter/s, 12.0566s/100 iters), loss = 0.0795988
I1123 20:26:07.040881 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:26:07.040881 34600 solver.cpp:237]     Train net output #1: loss = 0.0795988 (* 1 = 0.0795988 loss)
I1123 20:26:07.040881 34600 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1123 20:26:18.492698 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:26:18.970686 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_12500.caffemodel
I1123 20:26:19.008685 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_12500.solverstate
I1123 20:26:19.028686 34600 solver.cpp:330] Iteration 12500, Testing net (#0)
I1123 20:26:19.028686 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:26:23.065794 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:26:23.230844 34600 solver.cpp:397]     Test net output #0: accuracy = 0.887
I1123 20:26:23.231840 34600 solver.cpp:397]     Test net output #1: loss = 0.3416 (* 1 = 0.3416 loss)
I1123 20:26:23.348881 34600 solver.cpp:218] Iteration 12500 (6.13218 iter/s, 16.3074s/100 iters), loss = 0.120981
I1123 20:26:23.348881 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:26:23.348881 34600 solver.cpp:237]     Train net output #1: loss = 0.120981 (* 1 = 0.120981 loss)
I1123 20:26:23.348881 34600 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1123 20:26:35.400272 34600 solver.cpp:218] Iteration 12600 (8.29808 iter/s, 12.051s/100 iters), loss = 0.167847
I1123 20:26:35.400758 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:26:35.400758 34600 solver.cpp:237]     Train net output #1: loss = 0.167847 (* 1 = 0.167847 loss)
I1123 20:26:35.400758 34600 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1123 20:26:47.454278 34600 solver.cpp:218] Iteration 12700 (8.2965 iter/s, 12.0533s/100 iters), loss = 0.118095
I1123 20:26:47.454278 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:26:47.454278 34600 solver.cpp:237]     Train net output #1: loss = 0.118095 (* 1 = 0.118095 loss)
I1123 20:26:47.454278 34600 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1123 20:26:59.505486 34600 solver.cpp:218] Iteration 12800 (8.29846 iter/s, 12.0504s/100 iters), loss = 0.11481
I1123 20:26:59.505486 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:26:59.505486 34600 solver.cpp:237]     Train net output #1: loss = 0.11481 (* 1 = 0.11481 loss)
I1123 20:26:59.505486 34600 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1123 20:27:11.555692 34600 solver.cpp:218] Iteration 12900 (8.29897 iter/s, 12.0497s/100 iters), loss = 0.0943117
I1123 20:27:11.555692 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:27:11.555692 34600 solver.cpp:237]     Train net output #1: loss = 0.0943116 (* 1 = 0.0943116 loss)
I1123 20:27:11.555692 34600 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1123 20:27:23.013903 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:27:23.492938 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_13000.caffemodel
I1123 20:27:23.533941 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_13000.solverstate
I1123 20:27:23.554440 34600 solver.cpp:330] Iteration 13000, Testing net (#0)
I1123 20:27:23.554440 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:27:27.589457 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:27:27.754555 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8878
I1123 20:27:27.754555 34600 solver.cpp:397]     Test net output #1: loss = 0.342425 (* 1 = 0.342425 loss)
I1123 20:27:27.872611 34600 solver.cpp:218] Iteration 13000 (6.12882 iter/s, 16.3164s/100 iters), loss = 0.123062
I1123 20:27:27.872611 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:27:27.872611 34600 solver.cpp:237]     Train net output #1: loss = 0.123062 (* 1 = 0.123062 loss)
I1123 20:27:27.872611 34600 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1123 20:27:39.923952 34600 solver.cpp:218] Iteration 13100 (8.29861 iter/s, 12.0502s/100 iters), loss = 0.171716
I1123 20:27:39.923952 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:27:39.923952 34600 solver.cpp:237]     Train net output #1: loss = 0.171716 (* 1 = 0.171716 loss)
I1123 20:27:39.923952 34600 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1123 20:27:51.979733 34600 solver.cpp:218] Iteration 13200 (8.29487 iter/s, 12.0556s/100 iters), loss = 0.131911
I1123 20:27:51.979733 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:27:51.979733 34600 solver.cpp:237]     Train net output #1: loss = 0.131911 (* 1 = 0.131911 loss)
I1123 20:27:51.979733 34600 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1123 20:28:04.033511 34600 solver.cpp:218] Iteration 13300 (8.29663 iter/s, 12.0531s/100 iters), loss = 0.137165
I1123 20:28:04.034013 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:28:04.034013 34600 solver.cpp:237]     Train net output #1: loss = 0.137165 (* 1 = 0.137165 loss)
I1123 20:28:04.034013 34600 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1123 20:28:16.089553 34600 solver.cpp:218] Iteration 13400 (8.29504 iter/s, 12.0554s/100 iters), loss = 0.0863427
I1123 20:28:16.089553 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:28:16.090060 34600 solver.cpp:237]     Train net output #1: loss = 0.0863426 (* 1 = 0.0863426 loss)
I1123 20:28:16.090060 34600 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1123 20:28:27.546906 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:28:28.024919 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_13500.caffemodel
I1123 20:28:28.065438 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_13500.solverstate
I1123 20:28:28.085419 34600 solver.cpp:330] Iteration 13500, Testing net (#0)
I1123 20:28:28.085419 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:28:32.119740 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:28:32.284795 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8868
I1123 20:28:32.284795 34600 solver.cpp:397]     Test net output #1: loss = 0.344771 (* 1 = 0.344771 loss)
I1123 20:28:32.402840 34600 solver.cpp:218] Iteration 13500 (6.13039 iter/s, 16.3122s/100 iters), loss = 0.110112
I1123 20:28:32.402840 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:28:32.402840 34600 solver.cpp:237]     Train net output #1: loss = 0.110112 (* 1 = 0.110112 loss)
I1123 20:28:32.402840 34600 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1123 20:28:44.463795 34600 solver.cpp:218] Iteration 13600 (8.29132 iter/s, 12.0608s/100 iters), loss = 0.15182
I1123 20:28:44.464300 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:28:44.464300 34600 solver.cpp:237]     Train net output #1: loss = 0.15182 (* 1 = 0.15182 loss)
I1123 20:28:44.464300 34600 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1123 20:28:56.519796 34600 solver.cpp:218] Iteration 13700 (8.29531 iter/s, 12.055s/100 iters), loss = 0.10291
I1123 20:28:56.519796 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:28:56.519796 34600 solver.cpp:237]     Train net output #1: loss = 0.10291 (* 1 = 0.10291 loss)
I1123 20:28:56.519796 34600 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1123 20:29:08.573997 34600 solver.cpp:218] Iteration 13800 (8.29625 iter/s, 12.0536s/100 iters), loss = 0.0917326
I1123 20:29:08.573997 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:29:08.573997 34600 solver.cpp:237]     Train net output #1: loss = 0.0917325 (* 1 = 0.0917325 loss)
I1123 20:29:08.573997 34600 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1123 20:29:20.626920 34600 solver.cpp:218] Iteration 13900 (8.29701 iter/s, 12.0525s/100 iters), loss = 0.0962183
I1123 20:29:20.626920 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:29:20.626920 34600 solver.cpp:237]     Train net output #1: loss = 0.0962183 (* 1 = 0.0962183 loss)
I1123 20:29:20.626920 34600 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1123 20:29:32.089417 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:29:32.569527 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_14000.caffemodel
I1123 20:29:32.606029 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_14000.solverstate
I1123 20:29:32.626528 34600 solver.cpp:330] Iteration 14000, Testing net (#0)
I1123 20:29:32.626528 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:29:36.658771 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:29:36.825259 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8877
I1123 20:29:36.825259 34600 solver.cpp:397]     Test net output #1: loss = 0.343175 (* 1 = 0.343175 loss)
I1123 20:29:36.943310 34600 solver.cpp:218] Iteration 14000 (6.12913 iter/s, 16.3155s/100 iters), loss = 0.11828
I1123 20:29:36.943310 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:29:36.943310 34600 solver.cpp:237]     Train net output #1: loss = 0.11828 (* 1 = 0.11828 loss)
I1123 20:29:36.943310 34600 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1123 20:29:49.000653 34600 solver.cpp:218] Iteration 14100 (8.29409 iter/s, 12.0568s/100 iters), loss = 0.201555
I1123 20:29:49.000653 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:29:49.000653 34600 solver.cpp:237]     Train net output #1: loss = 0.201555 (* 1 = 0.201555 loss)
I1123 20:29:49.000653 34600 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1123 20:30:01.051528 34600 solver.cpp:218] Iteration 14200 (8.29841 iter/s, 12.0505s/100 iters), loss = 0.0997551
I1123 20:30:01.051528 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:30:01.052047 34600 solver.cpp:237]     Train net output #1: loss = 0.099755 (* 1 = 0.099755 loss)
I1123 20:30:01.052047 34600 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1123 20:30:13.107743 34600 solver.cpp:218] Iteration 14300 (8.2951 iter/s, 12.0553s/100 iters), loss = 0.0835514
I1123 20:30:13.107743 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:30:13.107743 34600 solver.cpp:237]     Train net output #1: loss = 0.0835513 (* 1 = 0.0835513 loss)
I1123 20:30:13.107743 34600 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1123 20:30:25.163518 34600 solver.cpp:218] Iteration 14400 (8.29508 iter/s, 12.0553s/100 iters), loss = 0.0684602
I1123 20:30:25.163518 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:30:25.163518 34600 solver.cpp:237]     Train net output #1: loss = 0.0684601 (* 1 = 0.0684601 loss)
I1123 20:30:25.163518 34600 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1123 20:30:36.617416 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:30:37.095104 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_14500.caffemodel
I1123 20:30:37.135110 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_14500.solverstate
I1123 20:30:37.155128 34600 solver.cpp:330] Iteration 14500, Testing net (#0)
I1123 20:30:37.155609 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:30:41.181787 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:30:41.347846 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8891
I1123 20:30:41.347846 34600 solver.cpp:397]     Test net output #1: loss = 0.345731 (* 1 = 0.345731 loss)
I1123 20:30:41.465880 34600 solver.cpp:218] Iteration 14500 (6.13436 iter/s, 16.3016s/100 iters), loss = 0.0917781
I1123 20:30:41.465880 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:30:41.465880 34600 solver.cpp:237]     Train net output #1: loss = 0.091778 (* 1 = 0.091778 loss)
I1123 20:30:41.465880 34600 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1123 20:30:53.519652 34600 solver.cpp:218] Iteration 14600 (8.29674 iter/s, 12.0529s/100 iters), loss = 0.125005
I1123 20:30:53.519652 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:30:53.519652 34600 solver.cpp:237]     Train net output #1: loss = 0.125005 (* 1 = 0.125005 loss)
I1123 20:30:53.519652 34600 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1123 20:31:05.892231 34600 solver.cpp:218] Iteration 14700 (8.08276 iter/s, 12.372s/100 iters), loss = 0.109711
I1123 20:31:05.892231 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:31:05.892231 34600 solver.cpp:237]     Train net output #1: loss = 0.109711 (* 1 = 0.109711 loss)
I1123 20:31:05.892231 34600 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1123 20:31:18.053035 34600 solver.cpp:218] Iteration 14800 (8.22309 iter/s, 12.1609s/100 iters), loss = 0.134019
I1123 20:31:18.053035 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:31:18.053035 34600 solver.cpp:237]     Train net output #1: loss = 0.134018 (* 1 = 0.134018 loss)
I1123 20:31:18.053035 34600 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1123 20:31:30.169061 34600 solver.cpp:218] Iteration 14900 (8.25456 iter/s, 12.1145s/100 iters), loss = 0.0750403
I1123 20:31:30.169061 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:31:30.169061 34600 solver.cpp:237]     Train net output #1: loss = 0.0750402 (* 1 = 0.0750402 loss)
I1123 20:31:30.169061 34600 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1123 20:31:41.629591 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:31:42.108669 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_15000.caffemodel
I1123 20:31:42.149657 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_15000.solverstate
I1123 20:31:42.172149 34600 solver.cpp:330] Iteration 15000, Testing net (#0)
I1123 20:31:42.172149 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:31:46.218695 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:31:46.384763 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8879
I1123 20:31:46.384763 34600 solver.cpp:397]     Test net output #1: loss = 0.34609 (* 1 = 0.34609 loss)
I1123 20:31:46.501816 34600 solver.cpp:218] Iteration 15000 (6.12282 iter/s, 16.3323s/100 iters), loss = 0.0738067
I1123 20:31:46.501816 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:31:46.501816 34600 solver.cpp:237]     Train net output #1: loss = 0.0738066 (* 1 = 0.0738066 loss)
I1123 20:31:46.501816 34600 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1123 20:31:58.570236 34600 solver.cpp:218] Iteration 15100 (8.28672 iter/s, 12.0675s/100 iters), loss = 0.139982
I1123 20:31:58.570236 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:31:58.570236 34600 solver.cpp:237]     Train net output #1: loss = 0.139982 (* 1 = 0.139982 loss)
I1123 20:31:58.570236 34600 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1123 20:32:10.637645 34600 solver.cpp:218] Iteration 15200 (8.28734 iter/s, 12.0666s/100 iters), loss = 0.0981777
I1123 20:32:10.637645 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:32:10.637645 34600 solver.cpp:237]     Train net output #1: loss = 0.0981776 (* 1 = 0.0981776 loss)
I1123 20:32:10.637645 34600 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1123 20:32:22.685370 34600 solver.cpp:218] Iteration 15300 (8.30056 iter/s, 12.0474s/100 iters), loss = 0.0710174
I1123 20:32:22.685370 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:32:22.685370 34600 solver.cpp:237]     Train net output #1: loss = 0.0710173 (* 1 = 0.0710173 loss)
I1123 20:32:22.685370 34600 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1123 20:32:22.685370 34600 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1123 20:32:34.735981 34600 solver.cpp:218] Iteration 15400 (8.29864 iter/s, 12.0502s/100 iters), loss = 0.0772115
I1123 20:32:34.736493 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:32:34.736493 34600 solver.cpp:237]     Train net output #1: loss = 0.0772114 (* 1 = 0.0772114 loss)
I1123 20:32:34.736493 34600 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1123 20:32:46.184562 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:32:46.662065 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_15500.caffemodel
I1123 20:32:46.699573 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_15500.solverstate
I1123 20:32:46.720082 34600 solver.cpp:330] Iteration 15500, Testing net (#0)
I1123 20:32:46.720082 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:32:50.763146 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:32:50.929337 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8888
I1123 20:32:50.929337 34600 solver.cpp:397]     Test net output #1: loss = 0.347257 (* 1 = 0.347257 loss)
I1123 20:32:51.047358 34600 solver.cpp:218] Iteration 15500 (6.13091 iter/s, 16.3108s/100 iters), loss = 0.0812189
I1123 20:32:51.047358 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:32:51.047358 34600 solver.cpp:237]     Train net output #1: loss = 0.0812188 (* 1 = 0.0812188 loss)
I1123 20:32:51.047358 34600 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1123 20:33:03.118479 34600 solver.cpp:218] Iteration 15600 (8.28495 iter/s, 12.0701s/100 iters), loss = 0.108422
I1123 20:33:03.118479 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:33:03.118479 34600 solver.cpp:237]     Train net output #1: loss = 0.108422 (* 1 = 0.108422 loss)
I1123 20:33:03.118479 34600 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1123 20:33:15.163992 34600 solver.cpp:218] Iteration 15700 (8.30221 iter/s, 12.045s/100 iters), loss = 0.109083
I1123 20:33:15.163992 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:33:15.163992 34600 solver.cpp:237]     Train net output #1: loss = 0.109083 (* 1 = 0.109083 loss)
I1123 20:33:15.163992 34600 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1123 20:33:27.207592 34600 solver.cpp:218] Iteration 15800 (8.30359 iter/s, 12.043s/100 iters), loss = 0.0828253
I1123 20:33:27.207592 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:33:27.207592 34600 solver.cpp:237]     Train net output #1: loss = 0.0828252 (* 1 = 0.0828252 loss)
I1123 20:33:27.207592 34600 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1123 20:33:39.255295 34600 solver.cpp:218] Iteration 15900 (8.3007 iter/s, 12.0472s/100 iters), loss = 0.0702038
I1123 20:33:39.255795 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:33:39.255795 34600 solver.cpp:237]     Train net output #1: loss = 0.0702037 (* 1 = 0.0702037 loss)
I1123 20:33:39.255795 34600 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1123 20:33:50.707710 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:33:51.187247 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_16000.caffemodel
I1123 20:33:51.229228 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_16000.solverstate
I1123 20:33:51.248245 34600 solver.cpp:330] Iteration 16000, Testing net (#0)
I1123 20:33:51.248744 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:33:55.287011 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:33:55.452545 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8905
I1123 20:33:55.452545 34600 solver.cpp:397]     Test net output #1: loss = 0.34651 (* 1 = 0.34651 loss)
I1123 20:33:55.571197 34600 solver.cpp:218] Iteration 16000 (6.12932 iter/s, 16.315s/100 iters), loss = 0.105633
I1123 20:33:55.571197 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:33:55.571197 34600 solver.cpp:237]     Train net output #1: loss = 0.105633 (* 1 = 0.105633 loss)
I1123 20:33:55.571197 34600 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1123 20:34:07.622555 34600 solver.cpp:218] Iteration 16100 (8.29815 iter/s, 12.0509s/100 iters), loss = 0.144317
I1123 20:34:07.623059 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:34:07.623059 34600 solver.cpp:237]     Train net output #1: loss = 0.144317 (* 1 = 0.144317 loss)
I1123 20:34:07.623059 34600 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1123 20:34:19.670815 34600 solver.cpp:218] Iteration 16200 (8.30069 iter/s, 12.0472s/100 iters), loss = 0.100425
I1123 20:34:19.670815 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:34:19.670815 34600 solver.cpp:237]     Train net output #1: loss = 0.100424 (* 1 = 0.100424 loss)
I1123 20:34:19.670815 34600 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1123 20:34:31.719638 34600 solver.cpp:218] Iteration 16300 (8.29981 iter/s, 12.0485s/100 iters), loss = 0.101989
I1123 20:34:31.719638 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:34:31.719638 34600 solver.cpp:237]     Train net output #1: loss = 0.101989 (* 1 = 0.101989 loss)
I1123 20:34:31.719638 34600 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1123 20:34:43.771873 34600 solver.cpp:218] Iteration 16400 (8.29778 iter/s, 12.0514s/100 iters), loss = 0.0787407
I1123 20:34:43.771873 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:34:43.771873 34600 solver.cpp:237]     Train net output #1: loss = 0.0787406 (* 1 = 0.0787406 loss)
I1123 20:34:43.771873 34600 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1123 20:34:55.220938 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:34:55.700031 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_16500.caffemodel
I1123 20:34:55.739071 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_16500.solverstate
I1123 20:34:55.759048 34600 solver.cpp:330] Iteration 16500, Testing net (#0)
I1123 20:34:55.759548 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:34:59.805037 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:34:59.971575 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8904
I1123 20:34:59.971575 34600 solver.cpp:397]     Test net output #1: loss = 0.346059 (* 1 = 0.346059 loss)
I1123 20:35:00.089624 34600 solver.cpp:218] Iteration 16500 (6.12852 iter/s, 16.3171s/100 iters), loss = 0.0728052
I1123 20:35:00.089624 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:35:00.089624 34600 solver.cpp:237]     Train net output #1: loss = 0.0728051 (* 1 = 0.0728051 loss)
I1123 20:35:00.089624 34600 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1123 20:35:12.149044 34600 solver.cpp:218] Iteration 16600 (8.29259 iter/s, 12.059s/100 iters), loss = 0.14088
I1123 20:35:12.149044 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:35:12.149044 34600 solver.cpp:237]     Train net output #1: loss = 0.14088 (* 1 = 0.14088 loss)
I1123 20:35:12.149044 34600 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1123 20:35:24.193437 34600 solver.cpp:218] Iteration 16700 (8.3029 iter/s, 12.044s/100 iters), loss = 0.102131
I1123 20:35:24.193437 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:35:24.193437 34600 solver.cpp:237]     Train net output #1: loss = 0.102131 (* 1 = 0.102131 loss)
I1123 20:35:24.193437 34600 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1123 20:35:36.247673 34600 solver.cpp:218] Iteration 16800 (8.2965 iter/s, 12.0533s/100 iters), loss = 0.110378
I1123 20:35:36.247673 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:35:36.247673 34600 solver.cpp:237]     Train net output #1: loss = 0.110378 (* 1 = 0.110378 loss)
I1123 20:35:36.247673 34600 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1123 20:35:48.293731 34600 solver.cpp:218] Iteration 16900 (8.30193 iter/s, 12.0454s/100 iters), loss = 0.0998229
I1123 20:35:48.293731 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:35:48.293731 34600 solver.cpp:237]     Train net output #1: loss = 0.0998228 (* 1 = 0.0998228 loss)
I1123 20:35:48.293731 34600 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1123 20:35:59.744729 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:36:00.222529 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_17000.caffemodel
I1123 20:36:00.260517 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_17000.solverstate
I1123 20:36:00.281019 34600 solver.cpp:330] Iteration 17000, Testing net (#0)
I1123 20:36:00.281019 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:36:04.322227 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:36:04.487864 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8898
I1123 20:36:04.487864 34600 solver.cpp:397]     Test net output #1: loss = 0.346258 (* 1 = 0.346258 loss)
I1123 20:36:04.605886 34600 solver.cpp:218] Iteration 17000 (6.13056 iter/s, 16.3117s/100 iters), loss = 0.0693336
I1123 20:36:04.605886 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:36:04.605886 34600 solver.cpp:237]     Train net output #1: loss = 0.0693334 (* 1 = 0.0693334 loss)
I1123 20:36:04.605886 34600 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1123 20:36:16.666307 34600 solver.cpp:218] Iteration 17100 (8.29226 iter/s, 12.0594s/100 iters), loss = 0.151705
I1123 20:36:16.666307 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:36:16.666307 34600 solver.cpp:237]     Train net output #1: loss = 0.151705 (* 1 = 0.151705 loss)
I1123 20:36:16.666307 34600 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1123 20:36:28.717064 34600 solver.cpp:218] Iteration 17200 (8.29846 iter/s, 12.0504s/100 iters), loss = 0.104793
I1123 20:36:28.717568 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:36:28.717568 34600 solver.cpp:237]     Train net output #1: loss = 0.104793 (* 1 = 0.104793 loss)
I1123 20:36:28.717568 34600 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1123 20:36:40.759125 34600 solver.cpp:218] Iteration 17300 (8.30486 iter/s, 12.0411s/100 iters), loss = 0.0832888
I1123 20:36:40.759125 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:36:40.759125 34600 solver.cpp:237]     Train net output #1: loss = 0.0832887 (* 1 = 0.0832887 loss)
I1123 20:36:40.759125 34600 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1123 20:36:52.814190 34600 solver.cpp:218] Iteration 17400 (8.29584 iter/s, 12.0542s/100 iters), loss = 0.0897592
I1123 20:36:52.814190 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:36:52.814190 34600 solver.cpp:237]     Train net output #1: loss = 0.0897591 (* 1 = 0.0897591 loss)
I1123 20:36:52.814190 34600 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1123 20:37:04.262698 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:37:04.740190 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_17500.caffemodel
I1123 20:37:04.779690 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_17500.solverstate
I1123 20:37:04.801188 34600 solver.cpp:330] Iteration 17500, Testing net (#0)
I1123 20:37:04.801688 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:37:08.848003 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:37:09.013998 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8898
I1123 20:37:09.013998 34600 solver.cpp:397]     Test net output #1: loss = 0.346113 (* 1 = 0.346113 loss)
I1123 20:37:09.133000 34600 solver.cpp:218] Iteration 17500 (6.12803 iter/s, 16.3184s/100 iters), loss = 0.0783585
I1123 20:37:09.133000 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:37:09.133000 34600 solver.cpp:237]     Train net output #1: loss = 0.0783584 (* 1 = 0.0783584 loss)
I1123 20:37:09.133000 34600 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1123 20:37:21.198683 34600 solver.cpp:218] Iteration 17600 (8.28847 iter/s, 12.065s/100 iters), loss = 0.1366
I1123 20:37:21.198683 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:37:21.198683 34600 solver.cpp:237]     Train net output #1: loss = 0.136599 (* 1 = 0.136599 loss)
I1123 20:37:21.198683 34600 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1123 20:37:33.239409 34600 solver.cpp:218] Iteration 17700 (8.30552 iter/s, 12.0402s/100 iters), loss = 0.0903699
I1123 20:37:33.239910 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:37:33.239910 34600 solver.cpp:237]     Train net output #1: loss = 0.0903699 (* 1 = 0.0903699 loss)
I1123 20:37:33.239910 34600 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1123 20:37:45.290623 34600 solver.cpp:218] Iteration 17800 (8.29845 iter/s, 12.0504s/100 iters), loss = 0.0665628
I1123 20:37:45.290623 34600 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 20:37:45.290623 34600 solver.cpp:237]     Train net output #1: loss = 0.0665628 (* 1 = 0.0665628 loss)
I1123 20:37:45.290623 34600 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1123 20:37:57.330718 34600 solver.cpp:218] Iteration 17900 (8.30593 iter/s, 12.0396s/100 iters), loss = 0.0725153
I1123 20:37:57.331220 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:37:57.331220 34600 solver.cpp:237]     Train net output #1: loss = 0.0725152 (* 1 = 0.0725152 loss)
I1123 20:37:57.331220 34600 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1123 20:38:08.773769 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:38:09.252770 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_18000.caffemodel
I1123 20:38:09.292770 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_18000.solverstate
I1123 20:38:09.312770 34600 solver.cpp:330] Iteration 18000, Testing net (#0)
I1123 20:38:09.312770 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:38:13.354508 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:38:13.519594 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8895
I1123 20:38:13.519594 34600 solver.cpp:397]     Test net output #1: loss = 0.346268 (* 1 = 0.346268 loss)
I1123 20:38:13.638571 34600 solver.cpp:218] Iteration 18000 (6.13217 iter/s, 16.3074s/100 iters), loss = 0.0770176
I1123 20:38:13.638571 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:38:13.638571 34600 solver.cpp:237]     Train net output #1: loss = 0.0770175 (* 1 = 0.0770175 loss)
I1123 20:38:13.638571 34600 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1123 20:38:25.688829 34600 solver.cpp:218] Iteration 18100 (8.29949 iter/s, 12.0489s/100 iters), loss = 0.163294
I1123 20:38:25.688829 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:38:25.688829 34600 solver.cpp:237]     Train net output #1: loss = 0.163294 (* 1 = 0.163294 loss)
I1123 20:38:25.688829 34600 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1123 20:38:37.794847 34600 solver.cpp:218] Iteration 18200 (8.2607 iter/s, 12.1055s/100 iters), loss = 0.0845315
I1123 20:38:37.794847 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:38:37.794847 34600 solver.cpp:237]     Train net output #1: loss = 0.0845313 (* 1 = 0.0845313 loss)
I1123 20:38:37.794847 34600 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1123 20:38:49.928258 34600 solver.cpp:218] Iteration 18300 (8.24225 iter/s, 12.1326s/100 iters), loss = 0.100323
I1123 20:38:49.928258 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:38:49.928258 34600 solver.cpp:237]     Train net output #1: loss = 0.100323 (* 1 = 0.100323 loss)
I1123 20:38:49.928258 34600 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1123 20:39:02.038004 34600 solver.cpp:218] Iteration 18400 (8.25823 iter/s, 12.1091s/100 iters), loss = 0.0851846
I1123 20:39:02.038004 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:39:02.038004 34600 solver.cpp:237]     Train net output #1: loss = 0.0851845 (* 1 = 0.0851845 loss)
I1123 20:39:02.038004 34600 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1123 20:39:13.629676 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:39:14.110751 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_18500.caffemodel
I1123 20:39:14.149768 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_18500.solverstate
I1123 20:39:14.171252 34600 solver.cpp:330] Iteration 18500, Testing net (#0)
I1123 20:39:14.171252 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:39:18.223595 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:39:18.388262 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8903
I1123 20:39:18.388262 34600 solver.cpp:397]     Test net output #1: loss = 0.34642 (* 1 = 0.34642 loss)
I1123 20:39:18.506798 34600 solver.cpp:218] Iteration 18500 (6.07236 iter/s, 16.4681s/100 iters), loss = 0.0835234
I1123 20:39:18.506798 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:39:18.506798 34600 solver.cpp:237]     Train net output #1: loss = 0.0835233 (* 1 = 0.0835233 loss)
I1123 20:39:18.506798 34600 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1123 20:39:30.557718 34600 solver.cpp:218] Iteration 18600 (8.29825 iter/s, 12.0507s/100 iters), loss = 0.111955
I1123 20:39:30.558218 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:39:30.558218 34600 solver.cpp:237]     Train net output #1: loss = 0.111955 (* 1 = 0.111955 loss)
I1123 20:39:30.558218 34600 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1123 20:39:42.764901 34600 solver.cpp:218] Iteration 18700 (8.19253 iter/s, 12.2062s/100 iters), loss = 0.0809062
I1123 20:39:42.764901 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:39:42.764901 34600 solver.cpp:237]     Train net output #1: loss = 0.0809061 (* 1 = 0.0809061 loss)
I1123 20:39:42.764901 34600 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1123 20:39:55.099464 34600 solver.cpp:218] Iteration 18800 (8.10758 iter/s, 12.3341s/100 iters), loss = 0.0937068
I1123 20:39:55.099464 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:39:55.099464 34600 solver.cpp:237]     Train net output #1: loss = 0.0937067 (* 1 = 0.0937067 loss)
I1123 20:39:55.099464 34600 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1123 20:40:07.457703 34600 solver.cpp:218] Iteration 18900 (8.09238 iter/s, 12.3573s/100 iters), loss = 0.0581535
I1123 20:40:07.457703 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:40:07.457703 34600 solver.cpp:237]     Train net output #1: loss = 0.0581534 (* 1 = 0.0581534 loss)
I1123 20:40:07.457703 34600 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1123 20:40:19.105412 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:40:19.587929 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_19000.caffemodel
I1123 20:40:19.628449 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_19000.solverstate
I1123 20:40:19.649448 34600 solver.cpp:330] Iteration 19000, Testing net (#0)
I1123 20:40:19.649448 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:40:23.689995 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:40:23.855804 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8899
I1123 20:40:23.855804 34600 solver.cpp:397]     Test net output #1: loss = 0.346914 (* 1 = 0.346914 loss)
I1123 20:40:23.974490 34600 solver.cpp:218] Iteration 19000 (6.05458 iter/s, 16.5164s/100 iters), loss = 0.108509
I1123 20:40:23.974490 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:40:23.974490 34600 solver.cpp:237]     Train net output #1: loss = 0.108508 (* 1 = 0.108508 loss)
I1123 20:40:23.974490 34600 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1123 20:40:36.035236 34600 solver.cpp:218] Iteration 19100 (8.29199 iter/s, 12.0598s/100 iters), loss = 0.190781
I1123 20:40:36.035735 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:40:36.035735 34600 solver.cpp:237]     Train net output #1: loss = 0.190781 (* 1 = 0.190781 loss)
I1123 20:40:36.035735 34600 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1123 20:40:48.130118 34600 solver.cpp:218] Iteration 19200 (8.26848 iter/s, 12.0941s/100 iters), loss = 0.0952713
I1123 20:40:48.130118 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:40:48.130118 34600 solver.cpp:237]     Train net output #1: loss = 0.0952712 (* 1 = 0.0952712 loss)
I1123 20:40:48.130118 34600 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1123 20:41:00.188628 34600 solver.cpp:218] Iteration 19300 (8.2935 iter/s, 12.0576s/100 iters), loss = 0.0791655
I1123 20:41:00.188628 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:41:00.188628 34600 solver.cpp:237]     Train net output #1: loss = 0.0791655 (* 1 = 0.0791655 loss)
I1123 20:41:00.188628 34600 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1123 20:41:12.245718 34600 solver.cpp:218] Iteration 19400 (8.29429 iter/s, 12.0565s/100 iters), loss = 0.078726
I1123 20:41:12.245718 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:41:12.245718 34600 solver.cpp:237]     Train net output #1: loss = 0.0787259 (* 1 = 0.0787259 loss)
I1123 20:41:12.245718 34600 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1123 20:41:23.894853 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:41:24.375854 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_19500.caffemodel
I1123 20:41:24.417852 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_19500.solverstate
I1123 20:41:24.438372 34600 solver.cpp:330] Iteration 19500, Testing net (#0)
I1123 20:41:24.438372 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:41:28.526367 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:41:28.692363 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8893
I1123 20:41:28.692363 34600 solver.cpp:397]     Test net output #1: loss = 0.346758 (* 1 = 0.346758 loss)
I1123 20:41:28.810988 34600 solver.cpp:218] Iteration 19500 (6.03688 iter/s, 16.5649s/100 iters), loss = 0.061583
I1123 20:41:28.810988 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:41:28.810988 34600 solver.cpp:237]     Train net output #1: loss = 0.0615829 (* 1 = 0.0615829 loss)
I1123 20:41:28.810988 34600 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1123 20:41:28.810988 34600 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1123 20:41:40.878217 34600 solver.cpp:218] Iteration 19600 (8.28752 iter/s, 12.0663s/100 iters), loss = 0.169675
I1123 20:41:40.878217 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:41:40.878217 34600 solver.cpp:237]     Train net output #1: loss = 0.169675 (* 1 = 0.169675 loss)
I1123 20:41:40.878217 34600 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1123 20:41:52.996083 34600 solver.cpp:218] Iteration 19700 (8.25279 iter/s, 12.1171s/100 iters), loss = 0.0899941
I1123 20:41:52.996083 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:41:52.996083 34600 solver.cpp:237]     Train net output #1: loss = 0.089994 (* 1 = 0.089994 loss)
I1123 20:41:52.996083 34600 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1123 20:42:05.264613 34600 solver.cpp:218] Iteration 19800 (8.15122 iter/s, 12.2681s/100 iters), loss = 0.108543
I1123 20:42:05.264613 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:42:05.264613 34600 solver.cpp:237]     Train net output #1: loss = 0.108543 (* 1 = 0.108543 loss)
I1123 20:42:05.264613 34600 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1123 20:42:17.392133 34600 solver.cpp:218] Iteration 19900 (8.24629 iter/s, 12.1267s/100 iters), loss = 0.0594217
I1123 20:42:17.392133 34600 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 20:42:17.392133 34600 solver.cpp:237]     Train net output #1: loss = 0.0594217 (* 1 = 0.0594217 loss)
I1123 20:42:17.392133 34600 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1123 20:42:28.995600 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:42:29.492641 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_20000.caffemodel
I1123 20:42:29.534641 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_20000.solverstate
I1123 20:42:29.556658 34600 solver.cpp:330] Iteration 20000, Testing net (#0)
I1123 20:42:29.557150 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:42:33.613045 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:42:33.778112 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8894
I1123 20:42:33.778112 34600 solver.cpp:397]     Test net output #1: loss = 0.346798 (* 1 = 0.346798 loss)
I1123 20:42:33.897104 34600 solver.cpp:218] Iteration 20000 (6.05881 iter/s, 16.5049s/100 iters), loss = 0.0818856
I1123 20:42:33.897104 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:42:33.897104 34600 solver.cpp:237]     Train net output #1: loss = 0.0818856 (* 1 = 0.0818856 loss)
I1123 20:42:33.897104 34600 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1123 20:42:46.109043 34600 solver.cpp:218] Iteration 20100 (8.18919 iter/s, 12.2112s/100 iters), loss = 0.128351
I1123 20:42:46.109043 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:42:46.109043 34600 solver.cpp:237]     Train net output #1: loss = 0.128351 (* 1 = 0.128351 loss)
I1123 20:42:46.109043 34600 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1123 20:42:58.402179 34600 solver.cpp:218] Iteration 20200 (8.13534 iter/s, 12.2921s/100 iters), loss = 0.0899107
I1123 20:42:58.402179 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:42:58.402179 34600 solver.cpp:237]     Train net output #1: loss = 0.0899106 (* 1 = 0.0899106 loss)
I1123 20:42:58.402179 34600 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1123 20:43:10.467854 34600 solver.cpp:218] Iteration 20300 (8.28831 iter/s, 12.0652s/100 iters), loss = 0.0892423
I1123 20:43:10.468355 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:43:10.468355 34600 solver.cpp:237]     Train net output #1: loss = 0.0892422 (* 1 = 0.0892422 loss)
I1123 20:43:10.468355 34600 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1123 20:43:22.657752 34600 solver.cpp:218] Iteration 20400 (8.2042 iter/s, 12.1889s/100 iters), loss = 0.075836
I1123 20:43:22.657752 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:43:22.657752 34600 solver.cpp:237]     Train net output #1: loss = 0.0758359 (* 1 = 0.0758359 loss)
I1123 20:43:22.657752 34600 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1123 20:43:34.294391 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:43:34.781957 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_20500.caffemodel
I1123 20:43:34.822945 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_20500.solverstate
I1123 20:43:34.843945 34600 solver.cpp:330] Iteration 20500, Testing net (#0)
I1123 20:43:34.843945 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:43:38.926631 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:43:39.094863 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8892
I1123 20:43:39.094863 34600 solver.cpp:397]     Test net output #1: loss = 0.346753 (* 1 = 0.346753 loss)
I1123 20:43:39.213876 34600 solver.cpp:218] Iteration 20500 (6.04009 iter/s, 16.556s/100 iters), loss = 0.103244
I1123 20:43:39.213876 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:43:39.213876 34600 solver.cpp:237]     Train net output #1: loss = 0.103244 (* 1 = 0.103244 loss)
I1123 20:43:39.213876 34600 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1123 20:43:51.482184 34600 solver.cpp:218] Iteration 20600 (8.15206 iter/s, 12.2668s/100 iters), loss = 0.177419
I1123 20:43:51.482184 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:43:51.482184 34600 solver.cpp:237]     Train net output #1: loss = 0.177419 (* 1 = 0.177419 loss)
I1123 20:43:51.482184 34600 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1123 20:44:03.696367 34600 solver.cpp:218] Iteration 20700 (8.18763 iter/s, 12.2136s/100 iters), loss = 0.0846328
I1123 20:44:03.696367 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:44:03.696367 34600 solver.cpp:237]     Train net output #1: loss = 0.0846327 (* 1 = 0.0846327 loss)
I1123 20:44:03.696367 34600 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1123 20:44:15.753569 34600 solver.cpp:218] Iteration 20800 (8.29425 iter/s, 12.0565s/100 iters), loss = 0.116362
I1123 20:44:15.753569 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:44:15.753569 34600 solver.cpp:237]     Train net output #1: loss = 0.116362 (* 1 = 0.116362 loss)
I1123 20:44:15.753569 34600 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1123 20:44:28.058378 34600 solver.cpp:218] Iteration 20900 (8.12686 iter/s, 12.3049s/100 iters), loss = 0.0722618
I1123 20:44:28.058378 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:44:28.058378 34600 solver.cpp:237]     Train net output #1: loss = 0.0722618 (* 1 = 0.0722618 loss)
I1123 20:44:28.058378 34600 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1123 20:44:39.813979 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:44:40.304771 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_21000.caffemodel
I1123 20:44:40.345777 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_21000.solverstate
I1123 20:44:40.366780 34600 solver.cpp:330] Iteration 21000, Testing net (#0)
I1123 20:44:40.366780 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:44:44.479943 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:44:44.648002 34600 solver.cpp:397]     Test net output #0: accuracy = 0.889
I1123 20:44:44.648002 34600 solver.cpp:397]     Test net output #1: loss = 0.34689 (* 1 = 0.34689 loss)
I1123 20:44:44.767037 34600 solver.cpp:218] Iteration 21000 (5.98521 iter/s, 16.7079s/100 iters), loss = 0.0839242
I1123 20:44:44.767037 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:44:44.767037 34600 solver.cpp:237]     Train net output #1: loss = 0.0839242 (* 1 = 0.0839242 loss)
I1123 20:44:44.767037 34600 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1123 20:44:57.067903 34600 solver.cpp:218] Iteration 21100 (8.13014 iter/s, 12.2999s/100 iters), loss = 0.135258
I1123 20:44:57.067903 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:44:57.067903 34600 solver.cpp:237]     Train net output #1: loss = 0.135258 (* 1 = 0.135258 loss)
I1123 20:44:57.067903 34600 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1123 20:45:09.296349 34600 solver.cpp:218] Iteration 21200 (8.17779 iter/s, 12.2282s/100 iters), loss = 0.102874
I1123 20:45:09.296349 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:45:09.296349 34600 solver.cpp:237]     Train net output #1: loss = 0.102874 (* 1 = 0.102874 loss)
I1123 20:45:09.297348 34600 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1123 20:45:21.358894 34600 solver.cpp:218] Iteration 21300 (8.2911 iter/s, 12.0611s/100 iters), loss = 0.100243
I1123 20:45:21.358894 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:45:21.358894 34600 solver.cpp:237]     Train net output #1: loss = 0.100243 (* 1 = 0.100243 loss)
I1123 20:45:21.358894 34600 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1123 20:45:33.449800 34600 solver.cpp:218] Iteration 21400 (8.27093 iter/s, 12.0905s/100 iters), loss = 0.0658436
I1123 20:45:33.450299 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:45:33.450299 34600 solver.cpp:237]     Train net output #1: loss = 0.0658436 (* 1 = 0.0658436 loss)
I1123 20:45:33.450299 34600 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1123 20:45:45.074923 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:45:45.561453 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_21500.caffemodel
I1123 20:45:45.603471 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_21500.solverstate
I1123 20:45:45.623963 34600 solver.cpp:330] Iteration 21500, Testing net (#0)
I1123 20:45:45.623963 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:45:49.699856 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:45:49.866902 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8891
I1123 20:45:49.866902 34600 solver.cpp:397]     Test net output #1: loss = 0.34688 (* 1 = 0.34688 loss)
I1123 20:45:49.987519 34600 solver.cpp:218] Iteration 21500 (6.04703 iter/s, 16.5371s/100 iters), loss = 0.0734125
I1123 20:45:49.987519 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:45:49.987519 34600 solver.cpp:237]     Train net output #1: loss = 0.0734125 (* 1 = 0.0734125 loss)
I1123 20:45:49.987519 34600 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1123 20:46:02.400471 34600 solver.cpp:218] Iteration 21600 (8.05685 iter/s, 12.4118s/100 iters), loss = 0.166877
I1123 20:46:02.400471 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:46:02.400471 34600 solver.cpp:237]     Train net output #1: loss = 0.166877 (* 1 = 0.166877 loss)
I1123 20:46:02.400471 34600 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1123 20:46:14.632381 34600 solver.cpp:218] Iteration 21700 (8.17537 iter/s, 12.2319s/100 iters), loss = 0.13826
I1123 20:46:14.632381 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:46:14.632381 34600 solver.cpp:237]     Train net output #1: loss = 0.13826 (* 1 = 0.13826 loss)
I1123 20:46:14.632381 34600 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1123 20:46:26.833300 34600 solver.cpp:218] Iteration 21800 (8.19656 iter/s, 12.2002s/100 iters), loss = 0.0831571
I1123 20:46:26.833300 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:46:26.833300 34600 solver.cpp:237]     Train net output #1: loss = 0.0831571 (* 1 = 0.0831571 loss)
I1123 20:46:26.833300 34600 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1123 20:46:39.435097 34600 solver.cpp:218] Iteration 21900 (7.9358 iter/s, 12.6011s/100 iters), loss = 0.0592788
I1123 20:46:39.435097 34600 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 20:46:39.435097 34600 solver.cpp:237]     Train net output #1: loss = 0.0592788 (* 1 = 0.0592788 loss)
I1123 20:46:39.435097 34600 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1123 20:46:51.365792 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:46:51.863808 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_22000.caffemodel
I1123 20:46:51.904819 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_22000.solverstate
I1123 20:46:51.926822 34600 solver.cpp:330] Iteration 22000, Testing net (#0)
I1123 20:46:51.927817 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:46:56.087884 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:46:56.214385 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8892
I1123 20:46:56.214385 34600 solver.cpp:397]     Test net output #1: loss = 0.346931 (* 1 = 0.346931 loss)
I1123 20:46:56.337399 34600 solver.cpp:218] Iteration 22000 (5.91678 iter/s, 16.9011s/100 iters), loss = 0.0765468
I1123 20:46:56.337399 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:46:56.337399 34600 solver.cpp:237]     Train net output #1: loss = 0.0765468 (* 1 = 0.0765468 loss)
I1123 20:46:56.337399 34600 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1123 20:46:56.337399 34600 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1123 20:47:08.823415 34600 solver.cpp:218] Iteration 22100 (8.0096 iter/s, 12.485s/100 iters), loss = 0.13004
I1123 20:47:08.823415 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:47:08.823415 34600 solver.cpp:237]     Train net output #1: loss = 0.13004 (* 1 = 0.13004 loss)
I1123 20:47:08.823415 34600 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1123 20:47:21.324597 34600 solver.cpp:218] Iteration 22200 (7.99935 iter/s, 12.501s/100 iters), loss = 0.128192
I1123 20:47:21.324597 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:47:21.324597 34600 solver.cpp:237]     Train net output #1: loss = 0.128192 (* 1 = 0.128192 loss)
I1123 20:47:21.324597 34600 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1123 20:47:33.813696 34600 solver.cpp:218] Iteration 22300 (8.00763 iter/s, 12.4881s/100 iters), loss = 0.0823821
I1123 20:47:33.813696 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:47:33.813696 34600 solver.cpp:237]     Train net output #1: loss = 0.0823821 (* 1 = 0.0823821 loss)
I1123 20:47:33.813696 34600 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1123 20:47:46.320416 34600 solver.cpp:218] Iteration 22400 (7.99624 iter/s, 12.5059s/100 iters), loss = 0.0509095
I1123 20:47:46.320416 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:47:46.320416 34600 solver.cpp:237]     Train net output #1: loss = 0.0509095 (* 1 = 0.0509095 loss)
I1123 20:47:46.320416 34600 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1123 20:47:58.238517 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:47:58.736574 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_22500.caffemodel
I1123 20:47:58.777451 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_22500.solverstate
I1123 20:47:58.799449 34600 solver.cpp:330] Iteration 22500, Testing net (#0)
I1123 20:47:58.799449 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:48:02.934692 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:48:03.103247 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8895
I1123 20:48:03.103247 34600 solver.cpp:397]     Test net output #1: loss = 0.346866 (* 1 = 0.346866 loss)
I1123 20:48:03.225267 34600 solver.cpp:218] Iteration 22500 (5.91543 iter/s, 16.9049s/100 iters), loss = 0.0962864
I1123 20:48:03.225267 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:48:03.225267 34600 solver.cpp:237]     Train net output #1: loss = 0.0962864 (* 1 = 0.0962864 loss)
I1123 20:48:03.225267 34600 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1123 20:48:15.749665 34600 solver.cpp:218] Iteration 22600 (7.98492 iter/s, 12.5236s/100 iters), loss = 0.101803
I1123 20:48:15.749665 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:48:15.749665 34600 solver.cpp:237]     Train net output #1: loss = 0.101803 (* 1 = 0.101803 loss)
I1123 20:48:15.749665 34600 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1123 20:48:28.252593 34600 solver.cpp:218] Iteration 22700 (7.99871 iter/s, 12.502s/100 iters), loss = 0.128604
I1123 20:48:28.252593 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:48:28.253592 34600 solver.cpp:237]     Train net output #1: loss = 0.128604 (* 1 = 0.128604 loss)
I1123 20:48:28.253592 34600 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1123 20:48:40.753950 34600 solver.cpp:218] Iteration 22800 (8.00009 iter/s, 12.4999s/100 iters), loss = 0.117845
I1123 20:48:40.753950 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:48:40.753950 34600 solver.cpp:237]     Train net output #1: loss = 0.117846 (* 1 = 0.117846 loss)
I1123 20:48:40.753950 34600 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1123 20:48:53.242524 34600 solver.cpp:218] Iteration 22900 (8.00749 iter/s, 12.4883s/100 iters), loss = 0.0746203
I1123 20:48:53.243024 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:48:53.243024 34600 solver.cpp:237]     Train net output #1: loss = 0.0746204 (* 1 = 0.0746204 loss)
I1123 20:48:53.243024 34600 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1123 20:49:05.113340 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:49:05.610564 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_23000.caffemodel
I1123 20:49:05.652067 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_23000.solverstate
I1123 20:49:05.673564 34600 solver.cpp:330] Iteration 23000, Testing net (#0)
I1123 20:49:05.673564 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:49:09.801108 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:49:09.970147 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8893
I1123 20:49:09.970147 34600 solver.cpp:397]     Test net output #1: loss = 0.346889 (* 1 = 0.346889 loss)
I1123 20:49:10.093199 34600 solver.cpp:218] Iteration 23000 (5.93487 iter/s, 16.8496s/100 iters), loss = 0.102476
I1123 20:49:10.093199 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:49:10.093199 34600 solver.cpp:237]     Train net output #1: loss = 0.102476 (* 1 = 0.102476 loss)
I1123 20:49:10.093199 34600 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1123 20:49:22.589108 34600 solver.cpp:218] Iteration 23100 (8.00311 iter/s, 12.4951s/100 iters), loss = 0.110603
I1123 20:49:22.589108 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:49:22.589108 34600 solver.cpp:237]     Train net output #1: loss = 0.110603 (* 1 = 0.110603 loss)
I1123 20:49:22.589108 34600 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1123 20:49:35.086740 34600 solver.cpp:218] Iteration 23200 (8.0019 iter/s, 12.497s/100 iters), loss = 0.105904
I1123 20:49:35.086740 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:49:35.086740 34600 solver.cpp:237]     Train net output #1: loss = 0.105904 (* 1 = 0.105904 loss)
I1123 20:49:35.086740 34600 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1123 20:49:47.493671 34600 solver.cpp:218] Iteration 23300 (8.06024 iter/s, 12.4066s/100 iters), loss = 0.104061
I1123 20:49:47.494171 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:49:47.494171 34600 solver.cpp:237]     Train net output #1: loss = 0.104061 (* 1 = 0.104061 loss)
I1123 20:49:47.494171 34600 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1123 20:49:59.965379 34600 solver.cpp:218] Iteration 23400 (8.01863 iter/s, 12.471s/100 iters), loss = 0.0955155
I1123 20:49:59.965379 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:49:59.965879 34600 solver.cpp:237]     Train net output #1: loss = 0.0955156 (* 1 = 0.0955156 loss)
I1123 20:49:59.965879 34600 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1123 20:50:11.609323 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:50:12.088824 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_23500.caffemodel
I1123 20:50:12.130358 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_23500.solverstate
I1123 20:50:12.150859 34600 solver.cpp:330] Iteration 23500, Testing net (#0)
I1123 20:50:12.151361 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:50:16.210417 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:50:16.376469 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8891
I1123 20:50:16.376469 34600 solver.cpp:397]     Test net output #1: loss = 0.346809 (* 1 = 0.346809 loss)
I1123 20:50:16.494442 34600 solver.cpp:218] Iteration 23500 (6.05032 iter/s, 16.528s/100 iters), loss = 0.112172
I1123 20:50:16.494442 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:50:16.494442 34600 solver.cpp:237]     Train net output #1: loss = 0.112172 (* 1 = 0.112172 loss)
I1123 20:50:16.494442 34600 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1123 20:50:28.590659 34600 solver.cpp:218] Iteration 23600 (8.26745 iter/s, 12.0956s/100 iters), loss = 0.13064
I1123 20:50:28.590659 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:50:28.590659 34600 solver.cpp:237]     Train net output #1: loss = 0.130641 (* 1 = 0.130641 loss)
I1123 20:50:28.590659 34600 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1123 20:50:40.764808 34600 solver.cpp:218] Iteration 23700 (8.2145 iter/s, 12.1736s/100 iters), loss = 0.108492
I1123 20:50:40.764808 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:50:40.764808 34600 solver.cpp:237]     Train net output #1: loss = 0.108492 (* 1 = 0.108492 loss)
I1123 20:50:40.764808 34600 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1123 20:50:52.854583 34600 solver.cpp:218] Iteration 23800 (8.27215 iter/s, 12.0887s/100 iters), loss = 0.105561
I1123 20:50:52.854583 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:50:52.854583 34600 solver.cpp:237]     Train net output #1: loss = 0.105561 (* 1 = 0.105561 loss)
I1123 20:50:52.854583 34600 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1123 20:51:04.920590 34600 solver.cpp:218] Iteration 23900 (8.28793 iter/s, 12.0657s/100 iters), loss = 0.0817829
I1123 20:51:04.921090 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:51:04.921090 34600 solver.cpp:237]     Train net output #1: loss = 0.081783 (* 1 = 0.081783 loss)
I1123 20:51:04.921090 34600 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1123 20:51:16.381049 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:51:16.860086 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_24000.caffemodel
I1123 20:51:16.900135 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_24000.solverstate
I1123 20:51:16.921629 34600 solver.cpp:330] Iteration 24000, Testing net (#0)
I1123 20:51:16.921629 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:51:20.963184 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:51:21.128751 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8896
I1123 20:51:21.128751 34600 solver.cpp:397]     Test net output #1: loss = 0.347117 (* 1 = 0.347117 loss)
I1123 20:51:21.247275 34600 solver.cpp:218] Iteration 24000 (6.1254 iter/s, 16.3255s/100 iters), loss = 0.0966853
I1123 20:51:21.247275 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:51:21.247275 34600 solver.cpp:237]     Train net output #1: loss = 0.0966854 (* 1 = 0.0966854 loss)
I1123 20:51:21.247275 34600 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1123 20:51:33.300426 34600 solver.cpp:218] Iteration 24100 (8.29671 iter/s, 12.053s/100 iters), loss = 0.133246
I1123 20:51:33.300426 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:51:33.300426 34600 solver.cpp:237]     Train net output #1: loss = 0.133247 (* 1 = 0.133247 loss)
I1123 20:51:33.300426 34600 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1123 20:51:45.351140 34600 solver.cpp:218] Iteration 24200 (8.299 iter/s, 12.0497s/100 iters), loss = 0.104973
I1123 20:51:45.351140 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:51:45.351140 34600 solver.cpp:237]     Train net output #1: loss = 0.104973 (* 1 = 0.104973 loss)
I1123 20:51:45.351140 34600 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1123 20:51:57.421763 34600 solver.cpp:218] Iteration 24300 (8.2847 iter/s, 12.0704s/100 iters), loss = 0.0909513
I1123 20:51:57.421763 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:51:57.422267 34600 solver.cpp:237]     Train net output #1: loss = 0.0909514 (* 1 = 0.0909514 loss)
I1123 20:51:57.422267 34600 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1123 20:52:09.467146 34600 solver.cpp:218] Iteration 24400 (8.30209 iter/s, 12.0452s/100 iters), loss = 0.0833588
I1123 20:52:09.467146 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:52:09.467146 34600 solver.cpp:237]     Train net output #1: loss = 0.0833588 (* 1 = 0.0833588 loss)
I1123 20:52:09.467146 34600 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1123 20:52:20.917088 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:52:21.395015 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_24500.caffemodel
I1123 20:52:21.435588 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_24500.solverstate
I1123 20:52:21.455586 34600 solver.cpp:330] Iteration 24500, Testing net (#0)
I1123 20:52:21.455586 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:52:25.499717 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:52:25.665171 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8894
I1123 20:52:25.665171 34600 solver.cpp:397]     Test net output #1: loss = 0.347035 (* 1 = 0.347035 loss)
I1123 20:52:25.783219 34600 solver.cpp:218] Iteration 24500 (6.12937 iter/s, 16.3149s/100 iters), loss = 0.0735022
I1123 20:52:25.783219 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:52:25.783219 34600 solver.cpp:237]     Train net output #1: loss = 0.0735023 (* 1 = 0.0735023 loss)
I1123 20:52:25.783219 34600 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1123 20:52:37.836179 34600 solver.cpp:218] Iteration 24600 (8.29724 iter/s, 12.0522s/100 iters), loss = 0.122467
I1123 20:52:37.836179 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:52:37.836179 34600 solver.cpp:237]     Train net output #1: loss = 0.122467 (* 1 = 0.122467 loss)
I1123 20:52:37.836179 34600 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1123 20:52:49.883327 34600 solver.cpp:218] Iteration 24700 (8.30126 iter/s, 12.0464s/100 iters), loss = 0.0996662
I1123 20:52:49.883327 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:52:49.883327 34600 solver.cpp:237]     Train net output #1: loss = 0.0996663 (* 1 = 0.0996663 loss)
I1123 20:52:49.883327 34600 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1123 20:53:01.924759 34600 solver.cpp:218] Iteration 24800 (8.30489 iter/s, 12.0411s/100 iters), loss = 0.0978626
I1123 20:53:01.924759 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:53:01.924759 34600 solver.cpp:237]     Train net output #1: loss = 0.0978626 (* 1 = 0.0978626 loss)
I1123 20:53:01.924759 34600 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1123 20:53:13.978415 34600 solver.cpp:218] Iteration 24900 (8.29677 iter/s, 12.0529s/100 iters), loss = 0.0799267
I1123 20:53:13.978415 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:53:13.978415 34600 solver.cpp:237]     Train net output #1: loss = 0.0799267 (* 1 = 0.0799267 loss)
I1123 20:53:13.978415 34600 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1123 20:53:25.429487 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:53:25.908485 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_25000.caffemodel
I1123 20:53:25.949484 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_25000.solverstate
I1123 20:53:25.970485 34600 solver.cpp:330] Iteration 25000, Testing net (#0)
I1123 20:53:25.970485 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:53:30.005669 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:53:30.171236 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8893
I1123 20:53:30.171236 34600 solver.cpp:397]     Test net output #1: loss = 0.346846 (* 1 = 0.346846 loss)
I1123 20:53:30.290305 34600 solver.cpp:218] Iteration 25000 (6.13081 iter/s, 16.3111s/100 iters), loss = 0.0950901
I1123 20:53:30.290305 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:53:30.290305 34600 solver.cpp:237]     Train net output #1: loss = 0.0950902 (* 1 = 0.0950902 loss)
I1123 20:53:30.290305 34600 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1123 20:53:42.355103 34600 solver.cpp:218] Iteration 25100 (8.28886 iter/s, 12.0644s/100 iters), loss = 0.130669
I1123 20:53:42.355103 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:53:42.355603 34600 solver.cpp:237]     Train net output #1: loss = 0.130669 (* 1 = 0.130669 loss)
I1123 20:53:42.355603 34600 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1123 20:53:54.409179 34600 solver.cpp:218] Iteration 25200 (8.29662 iter/s, 12.0531s/100 iters), loss = 0.0701243
I1123 20:53:54.409179 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:53:54.409179 34600 solver.cpp:237]     Train net output #1: loss = 0.0701243 (* 1 = 0.0701243 loss)
I1123 20:53:54.409179 34600 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1123 20:54:06.458518 34600 solver.cpp:218] Iteration 25300 (8.29941 iter/s, 12.049s/100 iters), loss = 0.0870121
I1123 20:54:06.458518 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:54:06.458518 34600 solver.cpp:237]     Train net output #1: loss = 0.0870121 (* 1 = 0.0870121 loss)
I1123 20:54:06.458518 34600 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1123 20:54:18.535750 34600 solver.cpp:218] Iteration 25400 (8.28073 iter/s, 12.0762s/100 iters), loss = 0.0966485
I1123 20:54:18.535750 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:54:18.535750 34600 solver.cpp:237]     Train net output #1: loss = 0.0966485 (* 1 = 0.0966485 loss)
I1123 20:54:18.535750 34600 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1123 20:54:29.992305 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:54:30.471844 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_25500.caffemodel
I1123 20:54:30.511826 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_25500.solverstate
I1123 20:54:30.531826 34600 solver.cpp:330] Iteration 25500, Testing net (#0)
I1123 20:54:30.532346 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:54:34.571403 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:54:34.736474 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8892
I1123 20:54:34.736474 34600 solver.cpp:397]     Test net output #1: loss = 0.346819 (* 1 = 0.346819 loss)
I1123 20:54:34.854463 34600 solver.cpp:218] Iteration 25500 (6.12799 iter/s, 16.3186s/100 iters), loss = 0.0689123
I1123 20:54:34.854463 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:54:34.854463 34600 solver.cpp:237]     Train net output #1: loss = 0.0689124 (* 1 = 0.0689124 loss)
I1123 20:54:34.854463 34600 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1123 20:54:46.925001 34600 solver.cpp:218] Iteration 25600 (8.2848 iter/s, 12.0703s/100 iters), loss = 0.181233
I1123 20:54:46.926002 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:54:46.926002 34600 solver.cpp:237]     Train net output #1: loss = 0.181233 (* 1 = 0.181233 loss)
I1123 20:54:46.926002 34600 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1123 20:54:58.976073 34600 solver.cpp:218] Iteration 25700 (8.29906 iter/s, 12.0496s/100 iters), loss = 0.101706
I1123 20:54:58.976073 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:54:58.976073 34600 solver.cpp:237]     Train net output #1: loss = 0.101706 (* 1 = 0.101706 loss)
I1123 20:54:58.976073 34600 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1123 20:55:11.026231 34600 solver.cpp:218] Iteration 25800 (8.29888 iter/s, 12.0498s/100 iters), loss = 0.0966114
I1123 20:55:11.026731 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:55:11.026731 34600 solver.cpp:237]     Train net output #1: loss = 0.0966115 (* 1 = 0.0966115 loss)
I1123 20:55:11.026731 34600 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1123 20:55:23.071854 34600 solver.cpp:218] Iteration 25900 (8.30228 iter/s, 12.0449s/100 iters), loss = 0.0746934
I1123 20:55:23.071854 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:55:23.071854 34600 solver.cpp:237]     Train net output #1: loss = 0.0746935 (* 1 = 0.0746935 loss)
I1123 20:55:23.071854 34600 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1123 20:55:34.631039 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:55:35.113605 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_26000.caffemodel
I1123 20:55:35.154604 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_26000.solverstate
I1123 20:55:35.176642 34600 solver.cpp:330] Iteration 26000, Testing net (#0)
I1123 20:55:35.176642 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:55:39.224215 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:55:39.390267 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8895
I1123 20:55:39.390267 34600 solver.cpp:397]     Test net output #1: loss = 0.346844 (* 1 = 0.346844 loss)
I1123 20:55:39.507897 34600 solver.cpp:218] Iteration 26000 (6.08447 iter/s, 16.4353s/100 iters), loss = 0.0776873
I1123 20:55:39.507897 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:55:39.507897 34600 solver.cpp:237]     Train net output #1: loss = 0.0776874 (* 1 = 0.0776874 loss)
I1123 20:55:39.507897 34600 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1123 20:55:51.585502 34600 solver.cpp:218] Iteration 26100 (8.28045 iter/s, 12.0766s/100 iters), loss = 0.184416
I1123 20:55:51.585502 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 20:55:51.585502 34600 solver.cpp:237]     Train net output #1: loss = 0.184416 (* 1 = 0.184416 loss)
I1123 20:55:51.585502 34600 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1123 20:56:03.734730 34600 solver.cpp:218] Iteration 26200 (8.23125 iter/s, 12.1488s/100 iters), loss = 0.0929876
I1123 20:56:03.734730 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:56:03.734730 34600 solver.cpp:237]     Train net output #1: loss = 0.0929876 (* 1 = 0.0929876 loss)
I1123 20:56:03.734730 34600 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1123 20:56:15.791205 34600 solver.cpp:218] Iteration 26300 (8.2947 iter/s, 12.0559s/100 iters), loss = 0.106711
I1123 20:56:15.791205 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:56:15.791205 34600 solver.cpp:237]     Train net output #1: loss = 0.106711 (* 1 = 0.106711 loss)
I1123 20:56:15.791205 34600 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1123 20:56:28.091951 34600 solver.cpp:218] Iteration 26400 (8.13017 iter/s, 12.2999s/100 iters), loss = 0.0621516
I1123 20:56:28.091951 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:56:28.091951 34600 solver.cpp:237]     Train net output #1: loss = 0.0621517 (* 1 = 0.0621517 loss)
I1123 20:56:28.091951 34600 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1123 20:56:39.678709 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:56:40.165887 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_26500.caffemodel
I1123 20:56:40.208869 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_26500.solverstate
I1123 20:56:40.229888 34600 solver.cpp:330] Iteration 26500, Testing net (#0)
I1123 20:56:40.229888 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:56:44.270990 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:56:44.437527 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8895
I1123 20:56:44.437527 34600 solver.cpp:397]     Test net output #1: loss = 0.3468 (* 1 = 0.3468 loss)
I1123 20:56:44.555068 34600 solver.cpp:218] Iteration 26500 (6.07428 iter/s, 16.4629s/100 iters), loss = 0.0805024
I1123 20:56:44.555068 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:56:44.555068 34600 solver.cpp:237]     Train net output #1: loss = 0.0805024 (* 1 = 0.0805024 loss)
I1123 20:56:44.555068 34600 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1123 20:56:56.606859 34600 solver.cpp:218] Iteration 26600 (8.29804 iter/s, 12.051s/100 iters), loss = 0.108595
I1123 20:56:56.606859 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:56:56.606859 34600 solver.cpp:237]     Train net output #1: loss = 0.108595 (* 1 = 0.108595 loss)
I1123 20:56:56.606859 34600 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1123 20:57:08.656263 34600 solver.cpp:218] Iteration 26700 (8.29956 iter/s, 12.0488s/100 iters), loss = 0.0988833
I1123 20:57:08.656263 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:57:08.656263 34600 solver.cpp:237]     Train net output #1: loss = 0.0988834 (* 1 = 0.0988834 loss)
I1123 20:57:08.656263 34600 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1123 20:57:20.706809 34600 solver.cpp:218] Iteration 26800 (8.29877 iter/s, 12.05s/100 iters), loss = 0.113362
I1123 20:57:20.707314 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:57:20.707314 34600 solver.cpp:237]     Train net output #1: loss = 0.113362 (* 1 = 0.113362 loss)
I1123 20:57:20.707314 34600 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1123 20:57:32.756006 34600 solver.cpp:218] Iteration 26900 (8.29995 iter/s, 12.0483s/100 iters), loss = 0.0655679
I1123 20:57:32.756006 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:57:32.756006 34600 solver.cpp:237]     Train net output #1: loss = 0.0655679 (* 1 = 0.0655679 loss)
I1123 20:57:32.756006 34600 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1123 20:57:44.207180 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:57:44.686223 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_27000.caffemodel
I1123 20:57:44.725271 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_27000.solverstate
I1123 20:57:44.745774 34600 solver.cpp:330] Iteration 27000, Testing net (#0)
I1123 20:57:44.745774 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:57:48.783771 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:57:48.949827 34600 solver.cpp:397]     Test net output #0: accuracy = 0.889
I1123 20:57:48.949827 34600 solver.cpp:397]     Test net output #1: loss = 0.346812 (* 1 = 0.346812 loss)
I1123 20:57:49.068562 34600 solver.cpp:218] Iteration 27000 (6.13059 iter/s, 16.3117s/100 iters), loss = 0.0824278
I1123 20:57:49.068562 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:57:49.068562 34600 solver.cpp:237]     Train net output #1: loss = 0.0824278 (* 1 = 0.0824278 loss)
I1123 20:57:49.068562 34600 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1123 20:57:49.068562 34600 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1123 20:58:01.167068 34600 solver.cpp:218] Iteration 27100 (8.26546 iter/s, 12.0985s/100 iters), loss = 0.143631
I1123 20:58:01.167068 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 20:58:01.167068 34600 solver.cpp:237]     Train net output #1: loss = 0.143631 (* 1 = 0.143631 loss)
I1123 20:58:01.167068 34600 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1123 20:58:13.271811 34600 solver.cpp:218] Iteration 27200 (8.26204 iter/s, 12.1035s/100 iters), loss = 0.0965431
I1123 20:58:13.271811 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:58:13.271811 34600 solver.cpp:237]     Train net output #1: loss = 0.0965431 (* 1 = 0.0965431 loss)
I1123 20:58:13.271811 34600 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1123 20:58:25.401965 34600 solver.cpp:218] Iteration 27300 (8.24385 iter/s, 12.1303s/100 iters), loss = 0.0843198
I1123 20:58:25.401965 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:58:25.401965 34600 solver.cpp:237]     Train net output #1: loss = 0.0843198 (* 1 = 0.0843198 loss)
I1123 20:58:25.401965 34600 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1123 20:58:37.481792 34600 solver.cpp:218] Iteration 27400 (8.27897 iter/s, 12.0788s/100 iters), loss = 0.0741486
I1123 20:58:37.482292 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:58:37.482292 34600 solver.cpp:237]     Train net output #1: loss = 0.0741486 (* 1 = 0.0741486 loss)
I1123 20:58:37.482292 34600 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1123 20:58:48.945646 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:58:49.424130 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_27500.caffemodel
I1123 20:58:49.464133 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_27500.solverstate
I1123 20:58:49.483629 34600 solver.cpp:330] Iteration 27500, Testing net (#0)
I1123 20:58:49.483629 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:58:53.529649 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:58:53.695185 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8892
I1123 20:58:53.695185 34600 solver.cpp:397]     Test net output #1: loss = 0.346866 (* 1 = 0.346866 loss)
I1123 20:58:53.812743 34600 solver.cpp:218] Iteration 27500 (6.12363 iter/s, 16.3302s/100 iters), loss = 0.0724596
I1123 20:58:53.813243 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 20:58:53.813243 34600 solver.cpp:237]     Train net output #1: loss = 0.0724595 (* 1 = 0.0724595 loss)
I1123 20:58:53.813243 34600 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1123 20:59:05.867334 34600 solver.cpp:218] Iteration 27600 (8.29627 iter/s, 12.0536s/100 iters), loss = 0.132626
I1123 20:59:05.867334 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 20:59:05.867334 34600 solver.cpp:237]     Train net output #1: loss = 0.132626 (* 1 = 0.132626 loss)
I1123 20:59:05.867334 34600 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1123 20:59:17.913549 34600 solver.cpp:218] Iteration 27700 (8.30126 iter/s, 12.0464s/100 iters), loss = 0.0769574
I1123 20:59:17.914535 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 20:59:17.914535 34600 solver.cpp:237]     Train net output #1: loss = 0.0769573 (* 1 = 0.0769573 loss)
I1123 20:59:17.914535 34600 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1123 20:59:29.984771 34600 solver.cpp:218] Iteration 27800 (8.2851 iter/s, 12.0699s/100 iters), loss = 0.100797
I1123 20:59:29.984771 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:59:29.984771 34600 solver.cpp:237]     Train net output #1: loss = 0.100797 (* 1 = 0.100797 loss)
I1123 20:59:29.984771 34600 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1123 20:59:42.064437 34600 solver.cpp:218] Iteration 27900 (8.27884 iter/s, 12.079s/100 iters), loss = 0.0715614
I1123 20:59:42.064437 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 20:59:42.064437 34600 solver.cpp:237]     Train net output #1: loss = 0.0715613 (* 1 = 0.0715613 loss)
I1123 20:59:42.064437 34600 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1123 20:59:53.522508 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:59:54.002176 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_28000.caffemodel
I1123 20:59:54.041167 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_28000.solverstate
I1123 20:59:54.061673 34600 solver.cpp:330] Iteration 28000, Testing net (#0)
I1123 20:59:54.061673 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 20:59:58.110038 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:59:58.276095 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8895
I1123 20:59:58.276095 34600 solver.cpp:397]     Test net output #1: loss = 0.347049 (* 1 = 0.347049 loss)
I1123 20:59:58.393138 34600 solver.cpp:218] Iteration 28000 (6.12421 iter/s, 16.3286s/100 iters), loss = 0.105815
I1123 20:59:58.393138 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 20:59:58.393138 34600 solver.cpp:237]     Train net output #1: loss = 0.105815 (* 1 = 0.105815 loss)
I1123 20:59:58.393138 34600 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1123 21:00:10.510998 34600 solver.cpp:218] Iteration 28100 (8.2533 iter/s, 12.1164s/100 iters), loss = 0.146673
I1123 21:00:10.510998 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:00:10.510998 34600 solver.cpp:237]     Train net output #1: loss = 0.146673 (* 1 = 0.146673 loss)
I1123 21:00:10.510998 34600 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1123 21:00:22.559792 34600 solver.cpp:218] Iteration 28200 (8.29998 iter/s, 12.0482s/100 iters), loss = 0.075281
I1123 21:00:22.559792 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:00:22.559792 34600 solver.cpp:237]     Train net output #1: loss = 0.075281 (* 1 = 0.075281 loss)
I1123 21:00:22.559792 34600 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1123 21:00:34.609838 34600 solver.cpp:218] Iteration 28300 (8.29896 iter/s, 12.0497s/100 iters), loss = 0.0976353
I1123 21:00:34.609838 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:00:34.609838 34600 solver.cpp:237]     Train net output #1: loss = 0.0976352 (* 1 = 0.0976352 loss)
I1123 21:00:34.609838 34600 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1123 21:00:46.665693 34600 solver.cpp:218] Iteration 28400 (8.2953 iter/s, 12.055s/100 iters), loss = 0.0591579
I1123 21:00:46.665693 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:00:46.665693 34600 solver.cpp:237]     Train net output #1: loss = 0.0591578 (* 1 = 0.0591578 loss)
I1123 21:00:46.665693 34600 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1123 21:00:58.137993 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:00:58.613997 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_28500.caffemodel
I1123 21:00:58.654487 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_28500.solverstate
I1123 21:00:58.673979 34600 solver.cpp:330] Iteration 28500, Testing net (#0)
I1123 21:00:58.673979 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:01:02.721798 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:01:02.887850 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8895
I1123 21:01:02.888861 34600 solver.cpp:397]     Test net output #1: loss = 0.346826 (* 1 = 0.346826 loss)
I1123 21:01:03.005901 34600 solver.cpp:218] Iteration 28500 (6.11993 iter/s, 16.3401s/100 iters), loss = 0.0960013
I1123 21:01:03.005901 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:01:03.005901 34600 solver.cpp:237]     Train net output #1: loss = 0.0960013 (* 1 = 0.0960013 loss)
I1123 21:01:03.005901 34600 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1123 21:01:15.094148 34600 solver.cpp:218] Iteration 28600 (8.27303 iter/s, 12.0875s/100 iters), loss = 0.1403
I1123 21:01:15.094652 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:01:15.094652 34600 solver.cpp:237]     Train net output #1: loss = 0.1403 (* 1 = 0.1403 loss)
I1123 21:01:15.094652 34600 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1123 21:01:27.165895 34600 solver.cpp:218] Iteration 28700 (8.28454 iter/s, 12.0707s/100 iters), loss = 0.0875585
I1123 21:01:27.165895 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:01:27.165895 34600 solver.cpp:237]     Train net output #1: loss = 0.0875584 (* 1 = 0.0875584 loss)
I1123 21:01:27.165895 34600 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1123 21:01:39.221333 34600 solver.cpp:218] Iteration 28800 (8.29526 iter/s, 12.0551s/100 iters), loss = 0.0930073
I1123 21:01:39.221333 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:01:39.221333 34600 solver.cpp:237]     Train net output #1: loss = 0.0930073 (* 1 = 0.0930073 loss)
I1123 21:01:39.221333 34600 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1123 21:01:51.289866 34600 solver.cpp:218] Iteration 28900 (8.2865 iter/s, 12.0678s/100 iters), loss = 0.0791098
I1123 21:01:51.289866 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:01:51.289866 34600 solver.cpp:237]     Train net output #1: loss = 0.0791097 (* 1 = 0.0791097 loss)
I1123 21:01:51.289866 34600 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1123 21:02:02.758299 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:02:03.236385 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_29000.caffemodel
I1123 21:02:03.276387 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_29000.solverstate
I1123 21:02:03.295889 34600 solver.cpp:330] Iteration 29000, Testing net (#0)
I1123 21:02:03.295889 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:02:07.337888 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:02:07.502506 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8895
I1123 21:02:07.502506 34600 solver.cpp:397]     Test net output #1: loss = 0.346954 (* 1 = 0.346954 loss)
I1123 21:02:07.621052 34600 solver.cpp:218] Iteration 29000 (6.12356 iter/s, 16.3304s/100 iters), loss = 0.0940872
I1123 21:02:07.621052 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:02:07.621052 34600 solver.cpp:237]     Train net output #1: loss = 0.0940872 (* 1 = 0.0940872 loss)
I1123 21:02:07.621052 34600 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1123 21:02:19.699586 34600 solver.cpp:218] Iteration 29100 (8.27955 iter/s, 12.078s/100 iters), loss = 0.138085
I1123 21:02:19.699586 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:02:19.699586 34600 solver.cpp:237]     Train net output #1: loss = 0.138085 (* 1 = 0.138085 loss)
I1123 21:02:19.699586 34600 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1123 21:02:31.751044 34600 solver.cpp:218] Iteration 29200 (8.29805 iter/s, 12.051s/100 iters), loss = 0.0953347
I1123 21:02:31.751044 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:02:31.751044 34600 solver.cpp:237]     Train net output #1: loss = 0.0953346 (* 1 = 0.0953346 loss)
I1123 21:02:31.751044 34600 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1123 21:02:43.804173 34600 solver.cpp:218] Iteration 29300 (8.29668 iter/s, 12.053s/100 iters), loss = 0.07874
I1123 21:02:43.804173 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:02:43.804173 34600 solver.cpp:237]     Train net output #1: loss = 0.0787399 (* 1 = 0.0787399 loss)
I1123 21:02:43.804173 34600 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1123 21:02:55.971731 34600 solver.cpp:218] Iteration 29400 (8.21952 iter/s, 12.1662s/100 iters), loss = 0.0631887
I1123 21:02:55.971731 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:02:55.971731 34600 solver.cpp:237]     Train net output #1: loss = 0.0631887 (* 1 = 0.0631887 loss)
I1123 21:02:55.971731 34600 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1123 21:03:07.561693 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:03:08.062083 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_29500.caffemodel
I1123 21:03:08.101161 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_29500.solverstate
I1123 21:03:08.122181 34600 solver.cpp:330] Iteration 29500, Testing net (#0)
I1123 21:03:08.122181 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:03:12.219355 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:03:12.389375 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8893
I1123 21:03:12.389375 34600 solver.cpp:397]     Test net output #1: loss = 0.346826 (* 1 = 0.346826 loss)
I1123 21:03:12.514377 34600 solver.cpp:218] Iteration 29500 (6.04494 iter/s, 16.5428s/100 iters), loss = 0.0800112
I1123 21:03:12.514377 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:03:12.514377 34600 solver.cpp:237]     Train net output #1: loss = 0.0800111 (* 1 = 0.0800111 loss)
I1123 21:03:12.515378 34600 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1123 21:03:24.815455 34600 solver.cpp:218] Iteration 29600 (8.13021 iter/s, 12.2998s/100 iters), loss = 0.136232
I1123 21:03:24.815455 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:03:24.815455 34600 solver.cpp:237]     Train net output #1: loss = 0.136232 (* 1 = 0.136232 loss)
I1123 21:03:24.815455 34600 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1123 21:03:36.867699 34600 solver.cpp:218] Iteration 29700 (8.29788 iter/s, 12.0513s/100 iters), loss = 0.0734003
I1123 21:03:36.867699 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:03:36.867699 34600 solver.cpp:237]     Train net output #1: loss = 0.0734003 (* 1 = 0.0734003 loss)
I1123 21:03:36.867699 34600 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1123 21:03:48.919986 34600 solver.cpp:218] Iteration 29800 (8.29759 iter/s, 12.0517s/100 iters), loss = 0.0989657
I1123 21:03:48.919986 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:03:48.919986 34600 solver.cpp:237]     Train net output #1: loss = 0.0989657 (* 1 = 0.0989657 loss)
I1123 21:03:48.919986 34600 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1123 21:04:00.974678 34600 solver.cpp:218] Iteration 29900 (8.29566 iter/s, 12.0545s/100 iters), loss = 0.0641126
I1123 21:04:00.975178 34600 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:04:00.975178 34600 solver.cpp:237]     Train net output #1: loss = 0.0641126 (* 1 = 0.0641126 loss)
I1123 21:04:00.975178 34600 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1123 21:04:12.427469 27844 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:04:12.905100 34600 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_30000.caffemodel
I1123 21:04:12.947100 34600 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_30000.solverstate
I1123 21:04:13.010098 34600 solver.cpp:310] Iteration 30000, loss = 0.0948085
I1123 21:04:13.010098 34600 solver.cpp:330] Iteration 30000, Testing net (#0)
I1123 21:04:13.010098 34600 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:04:17.047657 30128 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:04:17.212694 34600 solver.cpp:397]     Test net output #0: accuracy = 0.8891
I1123 21:04:17.212694 34600 solver.cpp:397]     Test net output #1: loss = 0.346858 (* 1 = 0.346858 loss)
I1123 21:04:17.212694 34600 solver.cpp:315] Optimization Done.
I1123 21:04:17.212694 34600 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 
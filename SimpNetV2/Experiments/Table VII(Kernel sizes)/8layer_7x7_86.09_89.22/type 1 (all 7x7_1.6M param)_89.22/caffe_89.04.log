
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1123 21:08:29.093792 16332 caffe.cpp:219] Using GPUs 0
I1123 21:08:29.272296 16332 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1123 21:08:29.571946 16332 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 21:08:29.588019 16332 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_1.6M_8L_7x7"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1123 21:08:29.623044 16332 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 21:08:29.624023 16332 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 21:08:29.624023 16332 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 21:08:29.624023 16332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1123 21:08:29.624023 16332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1123 21:08:29.624023 16332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1123 21:08:29.624023 16332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1123 21:08:29.624023 16332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1123 21:08:29.624023 16332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1123 21:08:29.624023 16332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1123 21:08:29.624023 16332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1123 21:08:29.624023 16332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1123 21:08:29.624023 16332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1123 21:08:29.624023 16332 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_7x7_1.6M"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 41
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 43
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 85
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 3
    kernel_size: 7
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 21:08:29.678110 16332 layer_factory.cpp:58] Creating layer cifar
I1123 21:08:29.685108 16332 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1123 21:08:29.685108 16332 net.cpp:84] Creating Layer cifar
I1123 21:08:29.685108 16332 net.cpp:380] cifar -> data
I1123 21:08:29.685108 16332 net.cpp:380] cifar -> label
I1123 21:08:29.686107 16332 data_layer.cpp:45] output data size: 100,3,32,32
I1123 21:08:29.693109 16332 net.cpp:122] Setting up cifar
I1123 21:08:29.693109 16332 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 21:08:29.693109 16332 net.cpp:129] Top shape: 100 (100)
I1123 21:08:29.693109 16332 net.cpp:137] Memory required for data: 1229200
I1123 21:08:29.693109 16332 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 21:08:29.693109 16332 net.cpp:84] Creating Layer label_cifar_1_split
I1123 21:08:29.693109 16332 net.cpp:406] label_cifar_1_split <- label
I1123 21:08:29.693109 16332 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 21:08:29.693109 16332 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 21:08:29.693109 16332 net.cpp:122] Setting up label_cifar_1_split
I1123 21:08:29.693109 16332 net.cpp:129] Top shape: 100 (100)
I1123 21:08:29.693109 16332 net.cpp:129] Top shape: 100 (100)
I1123 21:08:29.693109 16332 net.cpp:137] Memory required for data: 1230000
I1123 21:08:29.693109 16332 layer_factory.cpp:58] Creating layer conv1
I1123 21:08:29.693109 16332 net.cpp:84] Creating Layer conv1
I1123 21:08:29.693109 16332 net.cpp:406] conv1 <- data
I1123 21:08:29.693109 16332 net.cpp:380] conv1 -> conv1
I1123 21:08:29.694108 27888 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 21:08:29.938261 16332 net.cpp:122] Setting up conv1
I1123 21:08:29.938261 16332 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 21:08:29.938261 16332 net.cpp:137] Memory required for data: 18023600
I1123 21:08:29.938261 16332 layer_factory.cpp:58] Creating layer bn1
I1123 21:08:29.938261 16332 net.cpp:84] Creating Layer bn1
I1123 21:08:29.938261 16332 net.cpp:406] bn1 <- conv1
I1123 21:08:29.938261 16332 net.cpp:367] bn1 -> conv1 (in-place)
I1123 21:08:29.938261 16332 net.cpp:122] Setting up bn1
I1123 21:08:29.938261 16332 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 21:08:29.938261 16332 net.cpp:137] Memory required for data: 34817200
I1123 21:08:29.938261 16332 layer_factory.cpp:58] Creating layer scale1
I1123 21:08:29.938261 16332 net.cpp:84] Creating Layer scale1
I1123 21:08:29.938261 16332 net.cpp:406] scale1 <- conv1
I1123 21:08:29.938261 16332 net.cpp:367] scale1 -> conv1 (in-place)
I1123 21:08:29.938261 16332 layer_factory.cpp:58] Creating layer scale1
I1123 21:08:29.938261 16332 net.cpp:122] Setting up scale1
I1123 21:08:29.938261 16332 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 21:08:29.938261 16332 net.cpp:137] Memory required for data: 51610800
I1123 21:08:29.938261 16332 layer_factory.cpp:58] Creating layer relu1
I1123 21:08:29.938261 16332 net.cpp:84] Creating Layer relu1
I1123 21:08:29.938261 16332 net.cpp:406] relu1 <- conv1
I1123 21:08:29.938261 16332 net.cpp:367] relu1 -> conv1 (in-place)
I1123 21:08:29.939260 16332 net.cpp:122] Setting up relu1
I1123 21:08:29.939260 16332 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 21:08:29.939260 16332 net.cpp:137] Memory required for data: 68404400
I1123 21:08:29.939260 16332 layer_factory.cpp:58] Creating layer conv2
I1123 21:08:29.939260 16332 net.cpp:84] Creating Layer conv2
I1123 21:08:29.939260 16332 net.cpp:406] conv2 <- conv1
I1123 21:08:29.939260 16332 net.cpp:380] conv2 -> conv2
I1123 21:08:29.940259 16332 net.cpp:122] Setting up conv2
I1123 21:08:29.940259 16332 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 21:08:29.940259 16332 net.cpp:137] Memory required for data: 86017200
I1123 21:08:29.940259 16332 layer_factory.cpp:58] Creating layer bn2
I1123 21:08:29.940259 16332 net.cpp:84] Creating Layer bn2
I1123 21:08:29.940259 16332 net.cpp:406] bn2 <- conv2
I1123 21:08:29.940259 16332 net.cpp:367] bn2 -> conv2 (in-place)
I1123 21:08:29.940259 16332 net.cpp:122] Setting up bn2
I1123 21:08:29.940259 16332 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 21:08:29.940259 16332 net.cpp:137] Memory required for data: 103630000
I1123 21:08:29.940259 16332 layer_factory.cpp:58] Creating layer scale2
I1123 21:08:29.940259 16332 net.cpp:84] Creating Layer scale2
I1123 21:08:29.940259 16332 net.cpp:406] scale2 <- conv2
I1123 21:08:29.940259 16332 net.cpp:367] scale2 -> conv2 (in-place)
I1123 21:08:29.940259 16332 layer_factory.cpp:58] Creating layer scale2
I1123 21:08:29.941259 16332 net.cpp:122] Setting up scale2
I1123 21:08:29.941259 16332 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 21:08:29.941259 16332 net.cpp:137] Memory required for data: 121242800
I1123 21:08:29.941259 16332 layer_factory.cpp:58] Creating layer relu2
I1123 21:08:29.941259 16332 net.cpp:84] Creating Layer relu2
I1123 21:08:29.941259 16332 net.cpp:406] relu2 <- conv2
I1123 21:08:29.941259 16332 net.cpp:367] relu2 -> conv2 (in-place)
I1123 21:08:29.941259 16332 net.cpp:122] Setting up relu2
I1123 21:08:29.941259 16332 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 21:08:29.941259 16332 net.cpp:137] Memory required for data: 138855600
I1123 21:08:29.941259 16332 layer_factory.cpp:58] Creating layer conv2_2
I1123 21:08:29.941259 16332 net.cpp:84] Creating Layer conv2_2
I1123 21:08:29.941259 16332 net.cpp:406] conv2_2 <- conv2
I1123 21:08:29.941259 16332 net.cpp:380] conv2_2 -> conv2_2
I1123 21:08:29.942273 16332 net.cpp:122] Setting up conv2_2
I1123 21:08:29.942273 16332 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 21:08:29.942273 16332 net.cpp:137] Memory required for data: 167527600
I1123 21:08:29.943277 16332 layer_factory.cpp:58] Creating layer bn2_2
I1123 21:08:29.943277 16332 net.cpp:84] Creating Layer bn2_2
I1123 21:08:29.943277 16332 net.cpp:406] bn2_2 <- conv2_2
I1123 21:08:29.943277 16332 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 21:08:29.943277 16332 net.cpp:122] Setting up bn2_2
I1123 21:08:29.943277 16332 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 21:08:29.943277 16332 net.cpp:137] Memory required for data: 196199600
I1123 21:08:29.943277 16332 layer_factory.cpp:58] Creating layer scale2_2
I1123 21:08:29.943277 16332 net.cpp:84] Creating Layer scale2_2
I1123 21:08:29.943277 16332 net.cpp:406] scale2_2 <- conv2_2
I1123 21:08:29.943277 16332 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 21:08:29.943277 16332 layer_factory.cpp:58] Creating layer scale2_2
I1123 21:08:29.943277 16332 net.cpp:122] Setting up scale2_2
I1123 21:08:29.943277 16332 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 21:08:29.943277 16332 net.cpp:137] Memory required for data: 224871600
I1123 21:08:29.943277 16332 layer_factory.cpp:58] Creating layer relu2_2
I1123 21:08:29.943277 16332 net.cpp:84] Creating Layer relu2_2
I1123 21:08:29.943277 16332 net.cpp:406] relu2_2 <- conv2_2
I1123 21:08:29.943277 16332 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 21:08:29.943277 16332 net.cpp:122] Setting up relu2_2
I1123 21:08:29.943277 16332 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 21:08:29.943277 16332 net.cpp:137] Memory required for data: 253543600
I1123 21:08:29.943277 16332 layer_factory.cpp:58] Creating layer pool2_1
I1123 21:08:29.943277 16332 net.cpp:84] Creating Layer pool2_1
I1123 21:08:29.943277 16332 net.cpp:406] pool2_1 <- conv2_2
I1123 21:08:29.943277 16332 net.cpp:380] pool2_1 -> pool2_1
I1123 21:08:29.943277 16332 net.cpp:122] Setting up pool2_1
I1123 21:08:29.943277 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:29.943277 16332 net.cpp:137] Memory required for data: 260711600
I1123 21:08:29.943277 16332 layer_factory.cpp:58] Creating layer conv3
I1123 21:08:29.943277 16332 net.cpp:84] Creating Layer conv3
I1123 21:08:29.943277 16332 net.cpp:406] conv3 <- pool2_1
I1123 21:08:29.943277 16332 net.cpp:380] conv3 -> conv3
I1123 21:08:29.947278 16332 net.cpp:122] Setting up conv3
I1123 21:08:29.947278 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:29.947278 16332 net.cpp:137] Memory required for data: 267879600
I1123 21:08:29.947278 16332 layer_factory.cpp:58] Creating layer bn3
I1123 21:08:29.947278 16332 net.cpp:84] Creating Layer bn3
I1123 21:08:29.947278 16332 net.cpp:406] bn3 <- conv3
I1123 21:08:29.947278 16332 net.cpp:367] bn3 -> conv3 (in-place)
I1123 21:08:29.947278 16332 net.cpp:122] Setting up bn3
I1123 21:08:29.947278 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:29.947278 16332 net.cpp:137] Memory required for data: 275047600
I1123 21:08:29.947278 16332 layer_factory.cpp:58] Creating layer scale3
I1123 21:08:29.947278 16332 net.cpp:84] Creating Layer scale3
I1123 21:08:29.947278 16332 net.cpp:406] scale3 <- conv3
I1123 21:08:29.947278 16332 net.cpp:367] scale3 -> conv3 (in-place)
I1123 21:08:29.947278 16332 layer_factory.cpp:58] Creating layer scale3
I1123 21:08:29.947278 16332 net.cpp:122] Setting up scale3
I1123 21:08:29.947278 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:29.947278 16332 net.cpp:137] Memory required for data: 282215600
I1123 21:08:29.947278 16332 layer_factory.cpp:58] Creating layer relu3
I1123 21:08:29.947278 16332 net.cpp:84] Creating Layer relu3
I1123 21:08:29.947278 16332 net.cpp:406] relu3 <- conv3
I1123 21:08:29.947278 16332 net.cpp:367] relu3 -> conv3 (in-place)
I1123 21:08:29.947278 16332 net.cpp:122] Setting up relu3
I1123 21:08:29.947278 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:29.947278 16332 net.cpp:137] Memory required for data: 289383600
I1123 21:08:29.947278 16332 layer_factory.cpp:58] Creating layer conv4
I1123 21:08:29.947278 16332 net.cpp:84] Creating Layer conv4
I1123 21:08:29.947278 16332 net.cpp:406] conv4 <- conv3
I1123 21:08:29.947278 16332 net.cpp:380] conv4 -> conv4
I1123 21:08:29.949277 16332 net.cpp:122] Setting up conv4
I1123 21:08:29.950273 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:29.950273 16332 net.cpp:137] Memory required for data: 296551600
I1123 21:08:29.950273 16332 layer_factory.cpp:58] Creating layer bn4
I1123 21:08:29.950273 16332 net.cpp:84] Creating Layer bn4
I1123 21:08:29.950273 16332 net.cpp:406] bn4 <- conv4
I1123 21:08:29.950273 16332 net.cpp:367] bn4 -> conv4 (in-place)
I1123 21:08:29.950273 16332 net.cpp:122] Setting up bn4
I1123 21:08:29.950273 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:29.950273 16332 net.cpp:137] Memory required for data: 303719600
I1123 21:08:29.950273 16332 layer_factory.cpp:58] Creating layer scale4
I1123 21:08:29.950273 16332 net.cpp:84] Creating Layer scale4
I1123 21:08:29.950273 16332 net.cpp:406] scale4 <- conv4
I1123 21:08:29.950273 16332 net.cpp:367] scale4 -> conv4 (in-place)
I1123 21:08:29.950273 16332 layer_factory.cpp:58] Creating layer scale4
I1123 21:08:29.950273 16332 net.cpp:122] Setting up scale4
I1123 21:08:29.950273 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:29.950273 16332 net.cpp:137] Memory required for data: 310887600
I1123 21:08:29.950273 16332 layer_factory.cpp:58] Creating layer relu4
I1123 21:08:29.950273 16332 net.cpp:84] Creating Layer relu4
I1123 21:08:29.950273 16332 net.cpp:406] relu4 <- conv4
I1123 21:08:29.950273 16332 net.cpp:367] relu4 -> conv4 (in-place)
I1123 21:08:29.951283 16332 net.cpp:122] Setting up relu4
I1123 21:08:29.951283 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:29.951283 16332 net.cpp:137] Memory required for data: 318055600
I1123 21:08:29.951283 16332 layer_factory.cpp:58] Creating layer conv4_1
I1123 21:08:29.951283 16332 net.cpp:84] Creating Layer conv4_1
I1123 21:08:29.951283 16332 net.cpp:406] conv4_1 <- conv4
I1123 21:08:29.951283 16332 net.cpp:380] conv4_1 -> conv4_1
I1123 21:08:29.954270 16332 net.cpp:122] Setting up conv4_1
I1123 21:08:29.954270 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:29.954270 16332 net.cpp:137] Memory required for data: 325223600
I1123 21:08:29.954270 16332 layer_factory.cpp:58] Creating layer bn4_1
I1123 21:08:29.954270 16332 net.cpp:84] Creating Layer bn4_1
I1123 21:08:29.954270 16332 net.cpp:406] bn4_1 <- conv4_1
I1123 21:08:29.954270 16332 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 21:08:29.954270 16332 net.cpp:122] Setting up bn4_1
I1123 21:08:29.954270 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:29.954270 16332 net.cpp:137] Memory required for data: 332391600
I1123 21:08:29.954270 16332 layer_factory.cpp:58] Creating layer scale4_1
I1123 21:08:29.954270 16332 net.cpp:84] Creating Layer scale4_1
I1123 21:08:29.954270 16332 net.cpp:406] scale4_1 <- conv4_1
I1123 21:08:29.954270 16332 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 21:08:29.954270 16332 layer_factory.cpp:58] Creating layer scale4_1
I1123 21:08:29.954270 16332 net.cpp:122] Setting up scale4_1
I1123 21:08:29.954270 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:29.954270 16332 net.cpp:137] Memory required for data: 339559600
I1123 21:08:29.954270 16332 layer_factory.cpp:58] Creating layer relu4_1
I1123 21:08:29.954270 16332 net.cpp:84] Creating Layer relu4_1
I1123 21:08:29.954270 16332 net.cpp:406] relu4_1 <- conv4_1
I1123 21:08:29.954270 16332 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 21:08:29.955261 16332 net.cpp:122] Setting up relu4_1
I1123 21:08:29.955261 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:29.955261 16332 net.cpp:137] Memory required for data: 346727600
I1123 21:08:29.955261 16332 layer_factory.cpp:58] Creating layer conv4_2
I1123 21:08:29.955261 16332 net.cpp:84] Creating Layer conv4_2
I1123 21:08:29.955261 16332 net.cpp:406] conv4_2 <- conv4_1
I1123 21:08:29.955261 16332 net.cpp:380] conv4_2 -> conv4_2
I1123 21:08:29.958258 16332 net.cpp:122] Setting up conv4_2
I1123 21:08:29.958258 16332 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 21:08:29.958258 16332 net.cpp:137] Memory required for data: 355431600
I1123 21:08:29.958258 16332 layer_factory.cpp:58] Creating layer bn4_2
I1123 21:08:29.959259 16332 net.cpp:84] Creating Layer bn4_2
I1123 21:08:29.959259 16332 net.cpp:406] bn4_2 <- conv4_2
I1123 21:08:29.959259 16332 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 21:08:29.959259 16332 net.cpp:122] Setting up bn4_2
I1123 21:08:29.959259 16332 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 21:08:29.959259 16332 net.cpp:137] Memory required for data: 364135600
I1123 21:08:29.959259 16332 layer_factory.cpp:58] Creating layer scale4_2
I1123 21:08:29.959259 16332 net.cpp:84] Creating Layer scale4_2
I1123 21:08:29.959259 16332 net.cpp:406] scale4_2 <- conv4_2
I1123 21:08:29.959259 16332 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 21:08:29.959259 16332 layer_factory.cpp:58] Creating layer scale4_2
I1123 21:08:29.959259 16332 net.cpp:122] Setting up scale4_2
I1123 21:08:29.959259 16332 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 21:08:29.959259 16332 net.cpp:137] Memory required for data: 372839600
I1123 21:08:29.959259 16332 layer_factory.cpp:58] Creating layer relu4_2
I1123 21:08:29.959259 16332 net.cpp:84] Creating Layer relu4_2
I1123 21:08:29.959259 16332 net.cpp:406] relu4_2 <- conv4_2
I1123 21:08:29.959259 16332 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 21:08:29.959259 16332 net.cpp:122] Setting up relu4_2
I1123 21:08:29.959259 16332 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 21:08:29.959259 16332 net.cpp:137] Memory required for data: 381543600
I1123 21:08:29.959259 16332 layer_factory.cpp:58] Creating layer pool4_2
I1123 21:08:29.959259 16332 net.cpp:84] Creating Layer pool4_2
I1123 21:08:29.959259 16332 net.cpp:406] pool4_2 <- conv4_2
I1123 21:08:29.959259 16332 net.cpp:380] pool4_2 -> pool4_2
I1123 21:08:29.959259 16332 net.cpp:122] Setting up pool4_2
I1123 21:08:29.959259 16332 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1123 21:08:29.959259 16332 net.cpp:137] Memory required for data: 383719600
I1123 21:08:29.959259 16332 layer_factory.cpp:58] Creating layer conv12
I1123 21:08:29.959259 16332 net.cpp:84] Creating Layer conv12
I1123 21:08:29.959259 16332 net.cpp:406] conv12 <- pool4_2
I1123 21:08:29.959259 16332 net.cpp:380] conv12 -> conv12
I1123 21:08:29.964267 16332 net.cpp:122] Setting up conv12
I1123 21:08:29.964267 16332 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 21:08:29.964267 16332 net.cpp:137] Memory required for data: 386023600
I1123 21:08:29.964267 16332 layer_factory.cpp:58] Creating layer bn_conv12
I1123 21:08:29.964267 16332 net.cpp:84] Creating Layer bn_conv12
I1123 21:08:29.964267 16332 net.cpp:406] bn_conv12 <- conv12
I1123 21:08:29.964267 16332 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 21:08:29.964267 16332 net.cpp:122] Setting up bn_conv12
I1123 21:08:29.964267 16332 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 21:08:29.964267 16332 net.cpp:137] Memory required for data: 388327600
I1123 21:08:29.964267 16332 layer_factory.cpp:58] Creating layer scale_conv12
I1123 21:08:29.964267 16332 net.cpp:84] Creating Layer scale_conv12
I1123 21:08:29.964267 16332 net.cpp:406] scale_conv12 <- conv12
I1123 21:08:29.964267 16332 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 21:08:29.964764 16332 layer_factory.cpp:58] Creating layer scale_conv12
I1123 21:08:29.964764 16332 net.cpp:122] Setting up scale_conv12
I1123 21:08:29.964764 16332 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 21:08:29.964764 16332 net.cpp:137] Memory required for data: 390631600
I1123 21:08:29.964764 16332 layer_factory.cpp:58] Creating layer relu_conv12
I1123 21:08:29.964764 16332 net.cpp:84] Creating Layer relu_conv12
I1123 21:08:29.964764 16332 net.cpp:406] relu_conv12 <- conv12
I1123 21:08:29.964764 16332 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 21:08:29.964764 16332 net.cpp:122] Setting up relu_conv12
I1123 21:08:29.964764 16332 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 21:08:29.964764 16332 net.cpp:137] Memory required for data: 392935600
I1123 21:08:29.964764 16332 layer_factory.cpp:58] Creating layer poolcp6
I1123 21:08:29.964764 16332 net.cpp:84] Creating Layer poolcp6
I1123 21:08:29.964764 16332 net.cpp:406] poolcp6 <- conv12
I1123 21:08:29.965265 16332 net.cpp:380] poolcp6 -> poolcp6
I1123 21:08:29.965265 16332 net.cpp:122] Setting up poolcp6
I1123 21:08:29.965265 16332 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1123 21:08:29.965265 16332 net.cpp:137] Memory required for data: 392971600
I1123 21:08:29.965265 16332 layer_factory.cpp:58] Creating layer ip1
I1123 21:08:29.965265 16332 net.cpp:84] Creating Layer ip1
I1123 21:08:29.965265 16332 net.cpp:406] ip1 <- poolcp6
I1123 21:08:29.965265 16332 net.cpp:380] ip1 -> ip1
I1123 21:08:29.965265 16332 net.cpp:122] Setting up ip1
I1123 21:08:29.965265 16332 net.cpp:129] Top shape: 100 10 (1000)
I1123 21:08:29.965265 16332 net.cpp:137] Memory required for data: 392975600
I1123 21:08:29.965265 16332 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 21:08:29.965265 16332 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 21:08:29.965265 16332 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 21:08:29.965265 16332 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 21:08:29.965265 16332 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 21:08:29.965265 16332 net.cpp:122] Setting up ip1_ip1_0_split
I1123 21:08:29.965265 16332 net.cpp:129] Top shape: 100 10 (1000)
I1123 21:08:29.965265 16332 net.cpp:129] Top shape: 100 10 (1000)
I1123 21:08:29.965265 16332 net.cpp:137] Memory required for data: 392983600
I1123 21:08:29.965265 16332 layer_factory.cpp:58] Creating layer accuracy_training
I1123 21:08:29.965265 16332 net.cpp:84] Creating Layer accuracy_training
I1123 21:08:29.965265 16332 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1123 21:08:29.965265 16332 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1123 21:08:29.965265 16332 net.cpp:380] accuracy_training -> accuracy_training
I1123 21:08:29.965265 16332 net.cpp:122] Setting up accuracy_training
I1123 21:08:29.965265 16332 net.cpp:129] Top shape: (1)
I1123 21:08:29.965265 16332 net.cpp:137] Memory required for data: 392983604
I1123 21:08:29.965265 16332 layer_factory.cpp:58] Creating layer loss
I1123 21:08:29.965265 16332 net.cpp:84] Creating Layer loss
I1123 21:08:29.965265 16332 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 21:08:29.965265 16332 net.cpp:406] loss <- label_cifar_1_split_1
I1123 21:08:29.965265 16332 net.cpp:380] loss -> loss
I1123 21:08:29.965265 16332 layer_factory.cpp:58] Creating layer loss
I1123 21:08:29.965765 16332 net.cpp:122] Setting up loss
I1123 21:08:29.965765 16332 net.cpp:129] Top shape: (1)
I1123 21:08:29.965765 16332 net.cpp:132]     with loss weight 1
I1123 21:08:29.965765 16332 net.cpp:137] Memory required for data: 392983608
I1123 21:08:29.965765 16332 net.cpp:198] loss needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:200] accuracy_training does not need backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] ip1 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] poolcp6 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] relu_conv12 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] scale_conv12 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] bn_conv12 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] conv12 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] pool4_2 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] relu4_2 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] scale4_2 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] bn4_2 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] conv4_2 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] relu4_1 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] scale4_1 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] bn4_1 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] conv4_1 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] relu4 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] scale4 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] bn4 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] conv4 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] relu3 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] scale3 needs backward computation.
I1123 21:08:29.965765 16332 net.cpp:198] bn3 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] conv3 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] pool2_1 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] relu2_2 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] scale2_2 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] bn2_2 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] conv2_2 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] relu2 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] scale2 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] bn2 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] conv2 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] relu1 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] scale1 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] bn1 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:198] conv1 needs backward computation.
I1123 21:08:29.966279 16332 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 21:08:29.966279 16332 net.cpp:200] cifar does not need backward computation.
I1123 21:08:29.966279 16332 net.cpp:242] This network produces output accuracy_training
I1123 21:08:29.966279 16332 net.cpp:242] This network produces output loss
I1123 21:08:29.966279 16332 net.cpp:255] Network initialization done.
I1123 21:08:29.966778 16332 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 21:08:29.966778 16332 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 21:08:29.966778 16332 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 21:08:29.966778 16332 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1123 21:08:29.966778 16332 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1123 21:08:29.966778 16332 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1123 21:08:29.966778 16332 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1123 21:08:29.966778 16332 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1123 21:08:29.966778 16332 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1123 21:08:29.966778 16332 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1123 21:08:29.966778 16332 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1123 21:08:29.966778 16332 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1123 21:08:29.966778 16332 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1123 21:08:29.967278 16332 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_7x7_1.6M"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 41
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 43
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 85
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 3
    kernel_size: 7
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 21:08:29.967278 16332 layer_factory.cpp:58] Creating layer cifar
I1123 21:08:30.005784 16332 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1123 21:08:30.005784 16332 net.cpp:84] Creating Layer cifar
I1123 21:08:30.005784 16332 net.cpp:380] cifar -> data
I1123 21:08:30.005784 16332 net.cpp:380] cifar -> label
I1123 21:08:30.005784 16332 data_layer.cpp:45] output data size: 100,3,32,32
I1123 21:08:30.010799 16332 net.cpp:122] Setting up cifar
I1123 21:08:30.010799 16332 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 21:08:30.010799 16332 net.cpp:129] Top shape: 100 (100)
I1123 21:08:30.010799 16332 net.cpp:137] Memory required for data: 1229200
I1123 21:08:30.010799 16332 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 21:08:30.010799 16332 net.cpp:84] Creating Layer label_cifar_1_split
I1123 21:08:30.010799 16332 net.cpp:406] label_cifar_1_split <- label
I1123 21:08:30.010799 16332 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 21:08:30.010799 16332 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 21:08:30.010799 16332 net.cpp:122] Setting up label_cifar_1_split
I1123 21:08:30.010799 16332 net.cpp:129] Top shape: 100 (100)
I1123 21:08:30.010799 16332 net.cpp:129] Top shape: 100 (100)
I1123 21:08:30.010799 16332 net.cpp:137] Memory required for data: 1230000
I1123 21:08:30.010799 16332 layer_factory.cpp:58] Creating layer conv1
I1123 21:08:30.010799 16332 net.cpp:84] Creating Layer conv1
I1123 21:08:30.010799 16332 net.cpp:406] conv1 <- data
I1123 21:08:30.010799 16332 net.cpp:380] conv1 -> conv1
I1123 21:08:30.011800  3636 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 21:08:30.012784 16332 net.cpp:122] Setting up conv1
I1123 21:08:30.012784 16332 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 21:08:30.012784 16332 net.cpp:137] Memory required for data: 18023600
I1123 21:08:30.012784 16332 layer_factory.cpp:58] Creating layer bn1
I1123 21:08:30.012784 16332 net.cpp:84] Creating Layer bn1
I1123 21:08:30.012784 16332 net.cpp:406] bn1 <- conv1
I1123 21:08:30.012784 16332 net.cpp:367] bn1 -> conv1 (in-place)
I1123 21:08:30.012784 16332 net.cpp:122] Setting up bn1
I1123 21:08:30.012784 16332 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 21:08:30.012784 16332 net.cpp:137] Memory required for data: 34817200
I1123 21:08:30.012784 16332 layer_factory.cpp:58] Creating layer scale1
I1123 21:08:30.012784 16332 net.cpp:84] Creating Layer scale1
I1123 21:08:30.012784 16332 net.cpp:406] scale1 <- conv1
I1123 21:08:30.012784 16332 net.cpp:367] scale1 -> conv1 (in-place)
I1123 21:08:30.012784 16332 layer_factory.cpp:58] Creating layer scale1
I1123 21:08:30.013799 16332 net.cpp:122] Setting up scale1
I1123 21:08:30.013799 16332 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 21:08:30.013799 16332 net.cpp:137] Memory required for data: 51610800
I1123 21:08:30.013799 16332 layer_factory.cpp:58] Creating layer relu1
I1123 21:08:30.013799 16332 net.cpp:84] Creating Layer relu1
I1123 21:08:30.013799 16332 net.cpp:406] relu1 <- conv1
I1123 21:08:30.013799 16332 net.cpp:367] relu1 -> conv1 (in-place)
I1123 21:08:30.013799 16332 net.cpp:122] Setting up relu1
I1123 21:08:30.013799 16332 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1123 21:08:30.013799 16332 net.cpp:137] Memory required for data: 68404400
I1123 21:08:30.013799 16332 layer_factory.cpp:58] Creating layer conv2
I1123 21:08:30.013799 16332 net.cpp:84] Creating Layer conv2
I1123 21:08:30.013799 16332 net.cpp:406] conv2 <- conv1
I1123 21:08:30.013799 16332 net.cpp:380] conv2 -> conv2
I1123 21:08:30.015800 16332 net.cpp:122] Setting up conv2
I1123 21:08:30.015800 16332 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 21:08:30.015800 16332 net.cpp:137] Memory required for data: 86017200
I1123 21:08:30.015800 16332 layer_factory.cpp:58] Creating layer bn2
I1123 21:08:30.015800 16332 net.cpp:84] Creating Layer bn2
I1123 21:08:30.015800 16332 net.cpp:406] bn2 <- conv2
I1123 21:08:30.015800 16332 net.cpp:367] bn2 -> conv2 (in-place)
I1123 21:08:30.015800 16332 net.cpp:122] Setting up bn2
I1123 21:08:30.015800 16332 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 21:08:30.015800 16332 net.cpp:137] Memory required for data: 103630000
I1123 21:08:30.015800 16332 layer_factory.cpp:58] Creating layer scale2
I1123 21:08:30.015800 16332 net.cpp:84] Creating Layer scale2
I1123 21:08:30.015800 16332 net.cpp:406] scale2 <- conv2
I1123 21:08:30.015800 16332 net.cpp:367] scale2 -> conv2 (in-place)
I1123 21:08:30.015800 16332 layer_factory.cpp:58] Creating layer scale2
I1123 21:08:30.016780 16332 net.cpp:122] Setting up scale2
I1123 21:08:30.016780 16332 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 21:08:30.016780 16332 net.cpp:137] Memory required for data: 121242800
I1123 21:08:30.016780 16332 layer_factory.cpp:58] Creating layer relu2
I1123 21:08:30.016780 16332 net.cpp:84] Creating Layer relu2
I1123 21:08:30.016780 16332 net.cpp:406] relu2 <- conv2
I1123 21:08:30.016780 16332 net.cpp:367] relu2 -> conv2 (in-place)
I1123 21:08:30.016780 16332 net.cpp:122] Setting up relu2
I1123 21:08:30.016780 16332 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1123 21:08:30.016780 16332 net.cpp:137] Memory required for data: 138855600
I1123 21:08:30.016780 16332 layer_factory.cpp:58] Creating layer conv2_2
I1123 21:08:30.016780 16332 net.cpp:84] Creating Layer conv2_2
I1123 21:08:30.016780 16332 net.cpp:406] conv2_2 <- conv2
I1123 21:08:30.016780 16332 net.cpp:380] conv2_2 -> conv2_2
I1123 21:08:30.019805 16332 net.cpp:122] Setting up conv2_2
I1123 21:08:30.019805 16332 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 21:08:30.019805 16332 net.cpp:137] Memory required for data: 167527600
I1123 21:08:30.019805 16332 layer_factory.cpp:58] Creating layer bn2_2
I1123 21:08:30.019805 16332 net.cpp:84] Creating Layer bn2_2
I1123 21:08:30.019805 16332 net.cpp:406] bn2_2 <- conv2_2
I1123 21:08:30.019805 16332 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 21:08:30.019805 16332 net.cpp:122] Setting up bn2_2
I1123 21:08:30.019805 16332 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 21:08:30.019805 16332 net.cpp:137] Memory required for data: 196199600
I1123 21:08:30.019805 16332 layer_factory.cpp:58] Creating layer scale2_2
I1123 21:08:30.019805 16332 net.cpp:84] Creating Layer scale2_2
I1123 21:08:30.019805 16332 net.cpp:406] scale2_2 <- conv2_2
I1123 21:08:30.019805 16332 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 21:08:30.019805 16332 layer_factory.cpp:58] Creating layer scale2_2
I1123 21:08:30.019805 16332 net.cpp:122] Setting up scale2_2
I1123 21:08:30.019805 16332 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 21:08:30.019805 16332 net.cpp:137] Memory required for data: 224871600
I1123 21:08:30.019805 16332 layer_factory.cpp:58] Creating layer relu2_2
I1123 21:08:30.019805 16332 net.cpp:84] Creating Layer relu2_2
I1123 21:08:30.019805 16332 net.cpp:406] relu2_2 <- conv2_2
I1123 21:08:30.019805 16332 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 21:08:30.019805 16332 net.cpp:122] Setting up relu2_2
I1123 21:08:30.019805 16332 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1123 21:08:30.019805 16332 net.cpp:137] Memory required for data: 253543600
I1123 21:08:30.019805 16332 layer_factory.cpp:58] Creating layer pool2_1
I1123 21:08:30.019805 16332 net.cpp:84] Creating Layer pool2_1
I1123 21:08:30.019805 16332 net.cpp:406] pool2_1 <- conv2_2
I1123 21:08:30.019805 16332 net.cpp:380] pool2_1 -> pool2_1
I1123 21:08:30.019805 16332 net.cpp:122] Setting up pool2_1
I1123 21:08:30.019805 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:30.019805 16332 net.cpp:137] Memory required for data: 260711600
I1123 21:08:30.019805 16332 layer_factory.cpp:58] Creating layer conv3
I1123 21:08:30.019805 16332 net.cpp:84] Creating Layer conv3
I1123 21:08:30.019805 16332 net.cpp:406] conv3 <- pool2_1
I1123 21:08:30.019805 16332 net.cpp:380] conv3 -> conv3
I1123 21:08:30.022800 16332 net.cpp:122] Setting up conv3
I1123 21:08:30.022800 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:30.022800 16332 net.cpp:137] Memory required for data: 267879600
I1123 21:08:30.022800 16332 layer_factory.cpp:58] Creating layer bn3
I1123 21:08:30.022800 16332 net.cpp:84] Creating Layer bn3
I1123 21:08:30.022800 16332 net.cpp:406] bn3 <- conv3
I1123 21:08:30.022800 16332 net.cpp:367] bn3 -> conv3 (in-place)
I1123 21:08:30.022800 16332 net.cpp:122] Setting up bn3
I1123 21:08:30.022800 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:30.022800 16332 net.cpp:137] Memory required for data: 275047600
I1123 21:08:30.022800 16332 layer_factory.cpp:58] Creating layer scale3
I1123 21:08:30.022800 16332 net.cpp:84] Creating Layer scale3
I1123 21:08:30.022800 16332 net.cpp:406] scale3 <- conv3
I1123 21:08:30.022800 16332 net.cpp:367] scale3 -> conv3 (in-place)
I1123 21:08:30.023782 16332 layer_factory.cpp:58] Creating layer scale3
I1123 21:08:30.023782 16332 net.cpp:122] Setting up scale3
I1123 21:08:30.023782 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:30.023782 16332 net.cpp:137] Memory required for data: 282215600
I1123 21:08:30.023782 16332 layer_factory.cpp:58] Creating layer relu3
I1123 21:08:30.023782 16332 net.cpp:84] Creating Layer relu3
I1123 21:08:30.023782 16332 net.cpp:406] relu3 <- conv3
I1123 21:08:30.023782 16332 net.cpp:367] relu3 -> conv3 (in-place)
I1123 21:08:30.023782 16332 net.cpp:122] Setting up relu3
I1123 21:08:30.023782 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:30.023782 16332 net.cpp:137] Memory required for data: 289383600
I1123 21:08:30.023782 16332 layer_factory.cpp:58] Creating layer conv4
I1123 21:08:30.023782 16332 net.cpp:84] Creating Layer conv4
I1123 21:08:30.023782 16332 net.cpp:406] conv4 <- conv3
I1123 21:08:30.023782 16332 net.cpp:380] conv4 -> conv4
I1123 21:08:30.026799 16332 net.cpp:122] Setting up conv4
I1123 21:08:30.026799 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:30.026799 16332 net.cpp:137] Memory required for data: 296551600
I1123 21:08:30.026799 16332 layer_factory.cpp:58] Creating layer bn4
I1123 21:08:30.026799 16332 net.cpp:84] Creating Layer bn4
I1123 21:08:30.026799 16332 net.cpp:406] bn4 <- conv4
I1123 21:08:30.026799 16332 net.cpp:367] bn4 -> conv4 (in-place)
I1123 21:08:30.026799 16332 net.cpp:122] Setting up bn4
I1123 21:08:30.026799 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:30.026799 16332 net.cpp:137] Memory required for data: 303719600
I1123 21:08:30.026799 16332 layer_factory.cpp:58] Creating layer scale4
I1123 21:08:30.026799 16332 net.cpp:84] Creating Layer scale4
I1123 21:08:30.026799 16332 net.cpp:406] scale4 <- conv4
I1123 21:08:30.026799 16332 net.cpp:367] scale4 -> conv4 (in-place)
I1123 21:08:30.026799 16332 layer_factory.cpp:58] Creating layer scale4
I1123 21:08:30.027796 16332 net.cpp:122] Setting up scale4
I1123 21:08:30.027796 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:30.027796 16332 net.cpp:137] Memory required for data: 310887600
I1123 21:08:30.027796 16332 layer_factory.cpp:58] Creating layer relu4
I1123 21:08:30.027796 16332 net.cpp:84] Creating Layer relu4
I1123 21:08:30.027796 16332 net.cpp:406] relu4 <- conv4
I1123 21:08:30.027796 16332 net.cpp:367] relu4 -> conv4 (in-place)
I1123 21:08:30.027796 16332 net.cpp:122] Setting up relu4
I1123 21:08:30.027796 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:30.027796 16332 net.cpp:137] Memory required for data: 318055600
I1123 21:08:30.027796 16332 layer_factory.cpp:58] Creating layer conv4_1
I1123 21:08:30.027796 16332 net.cpp:84] Creating Layer conv4_1
I1123 21:08:30.027796 16332 net.cpp:406] conv4_1 <- conv4
I1123 21:08:30.027796 16332 net.cpp:380] conv4_1 -> conv4_1
I1123 21:08:30.029799 16332 net.cpp:122] Setting up conv4_1
I1123 21:08:30.029799 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:30.029799 16332 net.cpp:137] Memory required for data: 325223600
I1123 21:08:30.029799 16332 layer_factory.cpp:58] Creating layer bn4_1
I1123 21:08:30.029799 16332 net.cpp:84] Creating Layer bn4_1
I1123 21:08:30.029799 16332 net.cpp:406] bn4_1 <- conv4_1
I1123 21:08:30.029799 16332 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 21:08:30.030794 16332 net.cpp:122] Setting up bn4_1
I1123 21:08:30.030794 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:30.030794 16332 net.cpp:137] Memory required for data: 332391600
I1123 21:08:30.030794 16332 layer_factory.cpp:58] Creating layer scale4_1
I1123 21:08:30.030794 16332 net.cpp:84] Creating Layer scale4_1
I1123 21:08:30.030794 16332 net.cpp:406] scale4_1 <- conv4_1
I1123 21:08:30.030794 16332 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 21:08:30.030794 16332 layer_factory.cpp:58] Creating layer scale4_1
I1123 21:08:30.030794 16332 net.cpp:122] Setting up scale4_1
I1123 21:08:30.030794 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:30.030794 16332 net.cpp:137] Memory required for data: 339559600
I1123 21:08:30.030794 16332 layer_factory.cpp:58] Creating layer relu4_1
I1123 21:08:30.030794 16332 net.cpp:84] Creating Layer relu4_1
I1123 21:08:30.030794 16332 net.cpp:406] relu4_1 <- conv4_1
I1123 21:08:30.030794 16332 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 21:08:30.030794 16332 net.cpp:122] Setting up relu4_1
I1123 21:08:30.030794 16332 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1123 21:08:30.030794 16332 net.cpp:137] Memory required for data: 346727600
I1123 21:08:30.030794 16332 layer_factory.cpp:58] Creating layer conv4_2
I1123 21:08:30.030794 16332 net.cpp:84] Creating Layer conv4_2
I1123 21:08:30.030794 16332 net.cpp:406] conv4_2 <- conv4_1
I1123 21:08:30.030794 16332 net.cpp:380] conv4_2 -> conv4_2
I1123 21:08:30.034798 16332 net.cpp:122] Setting up conv4_2
I1123 21:08:30.034798 16332 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 21:08:30.034798 16332 net.cpp:137] Memory required for data: 355431600
I1123 21:08:30.034798 16332 layer_factory.cpp:58] Creating layer bn4_2
I1123 21:08:30.034798 16332 net.cpp:84] Creating Layer bn4_2
I1123 21:08:30.034798 16332 net.cpp:406] bn4_2 <- conv4_2
I1123 21:08:30.034798 16332 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 21:08:30.034798 16332 net.cpp:122] Setting up bn4_2
I1123 21:08:30.034798 16332 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 21:08:30.034798 16332 net.cpp:137] Memory required for data: 364135600
I1123 21:08:30.034798 16332 layer_factory.cpp:58] Creating layer scale4_2
I1123 21:08:30.034798 16332 net.cpp:84] Creating Layer scale4_2
I1123 21:08:30.034798 16332 net.cpp:406] scale4_2 <- conv4_2
I1123 21:08:30.034798 16332 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 21:08:30.034798 16332 layer_factory.cpp:58] Creating layer scale4_2
I1123 21:08:30.035796 16332 net.cpp:122] Setting up scale4_2
I1123 21:08:30.035796 16332 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 21:08:30.035796 16332 net.cpp:137] Memory required for data: 372839600
I1123 21:08:30.035796 16332 layer_factory.cpp:58] Creating layer relu4_2
I1123 21:08:30.035796 16332 net.cpp:84] Creating Layer relu4_2
I1123 21:08:30.035796 16332 net.cpp:406] relu4_2 <- conv4_2
I1123 21:08:30.035796 16332 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 21:08:30.035796 16332 net.cpp:122] Setting up relu4_2
I1123 21:08:30.035796 16332 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1123 21:08:30.035796 16332 net.cpp:137] Memory required for data: 381543600
I1123 21:08:30.035796 16332 layer_factory.cpp:58] Creating layer pool4_2
I1123 21:08:30.035796 16332 net.cpp:84] Creating Layer pool4_2
I1123 21:08:30.035796 16332 net.cpp:406] pool4_2 <- conv4_2
I1123 21:08:30.035796 16332 net.cpp:380] pool4_2 -> pool4_2
I1123 21:08:30.035796 16332 net.cpp:122] Setting up pool4_2
I1123 21:08:30.035796 16332 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1123 21:08:30.035796 16332 net.cpp:137] Memory required for data: 383719600
I1123 21:08:30.035796 16332 layer_factory.cpp:58] Creating layer conv12
I1123 21:08:30.035796 16332 net.cpp:84] Creating Layer conv12
I1123 21:08:30.035796 16332 net.cpp:406] conv12 <- pool4_2
I1123 21:08:30.035796 16332 net.cpp:380] conv12 -> conv12
I1123 21:08:30.040781 16332 net.cpp:122] Setting up conv12
I1123 21:08:30.040781 16332 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 21:08:30.040781 16332 net.cpp:137] Memory required for data: 386023600
I1123 21:08:30.040781 16332 layer_factory.cpp:58] Creating layer bn_conv12
I1123 21:08:30.040781 16332 net.cpp:84] Creating Layer bn_conv12
I1123 21:08:30.040781 16332 net.cpp:406] bn_conv12 <- conv12
I1123 21:08:30.040781 16332 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 21:08:30.040781 16332 net.cpp:122] Setting up bn_conv12
I1123 21:08:30.040781 16332 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 21:08:30.040781 16332 net.cpp:137] Memory required for data: 388327600
I1123 21:08:30.040781 16332 layer_factory.cpp:58] Creating layer scale_conv12
I1123 21:08:30.040781 16332 net.cpp:84] Creating Layer scale_conv12
I1123 21:08:30.040781 16332 net.cpp:406] scale_conv12 <- conv12
I1123 21:08:30.040781 16332 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 21:08:30.040781 16332 layer_factory.cpp:58] Creating layer scale_conv12
I1123 21:08:30.040781 16332 net.cpp:122] Setting up scale_conv12
I1123 21:08:30.040781 16332 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 21:08:30.040781 16332 net.cpp:137] Memory required for data: 390631600
I1123 21:08:30.040781 16332 layer_factory.cpp:58] Creating layer relu_conv12
I1123 21:08:30.040781 16332 net.cpp:84] Creating Layer relu_conv12
I1123 21:08:30.040781 16332 net.cpp:406] relu_conv12 <- conv12
I1123 21:08:30.040781 16332 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 21:08:30.041781 16332 net.cpp:122] Setting up relu_conv12
I1123 21:08:30.041781 16332 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1123 21:08:30.041781 16332 net.cpp:137] Memory required for data: 392935600
I1123 21:08:30.041781 16332 layer_factory.cpp:58] Creating layer poolcp6
I1123 21:08:30.041781 16332 net.cpp:84] Creating Layer poolcp6
I1123 21:08:30.041781 16332 net.cpp:406] poolcp6 <- conv12
I1123 21:08:30.041781 16332 net.cpp:380] poolcp6 -> poolcp6
I1123 21:08:30.041781 16332 net.cpp:122] Setting up poolcp6
I1123 21:08:30.041781 16332 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1123 21:08:30.041781 16332 net.cpp:137] Memory required for data: 392971600
I1123 21:08:30.041781 16332 layer_factory.cpp:58] Creating layer ip1
I1123 21:08:30.041781 16332 net.cpp:84] Creating Layer ip1
I1123 21:08:30.041781 16332 net.cpp:406] ip1 <- poolcp6
I1123 21:08:30.041781 16332 net.cpp:380] ip1 -> ip1
I1123 21:08:30.041781 16332 net.cpp:122] Setting up ip1
I1123 21:08:30.041781 16332 net.cpp:129] Top shape: 100 10 (1000)
I1123 21:08:30.041781 16332 net.cpp:137] Memory required for data: 392975600
I1123 21:08:30.041781 16332 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 21:08:30.041781 16332 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 21:08:30.041781 16332 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 21:08:30.041781 16332 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 21:08:30.041781 16332 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 21:08:30.041781 16332 net.cpp:122] Setting up ip1_ip1_0_split
I1123 21:08:30.041781 16332 net.cpp:129] Top shape: 100 10 (1000)
I1123 21:08:30.041781 16332 net.cpp:129] Top shape: 100 10 (1000)
I1123 21:08:30.041781 16332 net.cpp:137] Memory required for data: 392983600
I1123 21:08:30.041781 16332 layer_factory.cpp:58] Creating layer accuracy
I1123 21:08:30.041781 16332 net.cpp:84] Creating Layer accuracy
I1123 21:08:30.041781 16332 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1123 21:08:30.041781 16332 net.cpp:406] accuracy <- label_cifar_1_split_0
I1123 21:08:30.041781 16332 net.cpp:380] accuracy -> accuracy
I1123 21:08:30.041781 16332 net.cpp:122] Setting up accuracy
I1123 21:08:30.041781 16332 net.cpp:129] Top shape: (1)
I1123 21:08:30.041781 16332 net.cpp:137] Memory required for data: 392983604
I1123 21:08:30.041781 16332 layer_factory.cpp:58] Creating layer loss
I1123 21:08:30.041781 16332 net.cpp:84] Creating Layer loss
I1123 21:08:30.041781 16332 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 21:08:30.041781 16332 net.cpp:406] loss <- label_cifar_1_split_1
I1123 21:08:30.041781 16332 net.cpp:380] loss -> loss
I1123 21:08:30.041781 16332 layer_factory.cpp:58] Creating layer loss
I1123 21:08:30.042780 16332 net.cpp:122] Setting up loss
I1123 21:08:30.042780 16332 net.cpp:129] Top shape: (1)
I1123 21:08:30.042780 16332 net.cpp:132]     with loss weight 1
I1123 21:08:30.042780 16332 net.cpp:137] Memory required for data: 392983608
I1123 21:08:30.042780 16332 net.cpp:198] loss needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:200] accuracy does not need backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] ip1 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] poolcp6 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] relu_conv12 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] scale_conv12 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] bn_conv12 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] conv12 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] pool4_2 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] relu4_2 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] scale4_2 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] bn4_2 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] conv4_2 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] relu4_1 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] scale4_1 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] bn4_1 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] conv4_1 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] relu4 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] scale4 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] bn4 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] conv4 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] relu3 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] scale3 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] bn3 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] conv3 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] pool2_1 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] relu2_2 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] scale2_2 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] bn2_2 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] conv2_2 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] relu2 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] scale2 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] bn2 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] conv2 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] relu1 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] scale1 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] bn1 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:198] conv1 needs backward computation.
I1123 21:08:30.042780 16332 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 21:08:30.042780 16332 net.cpp:200] cifar does not need backward computation.
I1123 21:08:30.042780 16332 net.cpp:242] This network produces output accuracy
I1123 21:08:30.042780 16332 net.cpp:242] This network produces output loss
I1123 21:08:30.042780 16332 net.cpp:255] Network initialization done.
I1123 21:08:30.042780 16332 solver.cpp:56] Solver scaffolding done.
I1123 21:08:30.044780 16332 caffe.cpp:249] Starting Optimization
I1123 21:08:30.044780 16332 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_7x7_1.6M
I1123 21:08:30.044780 16332 solver.cpp:273] Learning Rate Policy: multistep
I1123 21:08:30.047782 16332 solver.cpp:330] Iteration 0, Testing net (#0)
I1123 21:08:30.049795 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:08:34.093408  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:08:34.257413 16332 solver.cpp:397]     Test net output #0: accuracy = 0.1
I1123 21:08:34.257413 16332 solver.cpp:397]     Test net output #1: loss = 78.6029 (* 1 = 78.6029 loss)
I1123 21:08:34.411737 16332 solver.cpp:218] Iteration 0 (-nan iter/s, 4.36637s/100 iters), loss = 3.45264
I1123 21:08:34.411737 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.11
I1123 21:08:34.411737 16332 solver.cpp:237]     Train net output #1: loss = 3.45264 (* 1 = 3.45264 loss)
I1123 21:08:34.411737 16332 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1123 21:08:46.551100 16332 solver.cpp:218] Iteration 100 (8.23824 iter/s, 12.1385s/100 iters), loss = 1.75156
I1123 21:08:46.551597 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.37
I1123 21:08:46.551597 16332 solver.cpp:237]     Train net output #1: loss = 1.75156 (* 1 = 1.75156 loss)
I1123 21:08:46.551597 16332 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1123 21:08:58.734733 16332 solver.cpp:218] Iteration 200 (8.20844 iter/s, 12.1826s/100 iters), loss = 1.81052
I1123 21:08:58.734733 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.35
I1123 21:08:58.734733 16332 solver.cpp:237]     Train net output #1: loss = 1.81052 (* 1 = 1.81052 loss)
I1123 21:08:58.734733 16332 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1123 21:09:10.760565 16332 solver.cpp:218] Iteration 300 (8.31582 iter/s, 12.0253s/100 iters), loss = 1.50485
I1123 21:09:10.760565 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.44
I1123 21:09:10.760565 16332 solver.cpp:237]     Train net output #1: loss = 1.50485 (* 1 = 1.50485 loss)
I1123 21:09:10.760565 16332 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1123 21:09:22.943187 16332 solver.cpp:218] Iteration 400 (8.20891 iter/s, 12.1819s/100 iters), loss = 1.44713
I1123 21:09:22.943187 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.46
I1123 21:09:22.943187 16332 solver.cpp:237]     Train net output #1: loss = 1.44713 (* 1 = 1.44713 loss)
I1123 21:09:22.943187 16332 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1123 21:09:34.652591 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:09:35.136576 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_500.caffemodel
I1123 21:09:35.186084 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_500.solverstate
I1123 21:09:35.206084 16332 solver.cpp:330] Iteration 500, Testing net (#0)
I1123 21:09:35.206084 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:09:39.335233  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:09:39.504313 16332 solver.cpp:397]     Test net output #0: accuracy = 0.4801
I1123 21:09:39.504313 16332 solver.cpp:397]     Test net output #1: loss = 1.45786 (* 1 = 1.45786 loss)
I1123 21:09:39.624325 16332 solver.cpp:218] Iteration 500 (5.99476 iter/s, 16.6812s/100 iters), loss = 1.38152
I1123 21:09:39.624325 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.57
I1123 21:09:39.624325 16332 solver.cpp:237]     Train net output #1: loss = 1.38152 (* 1 = 1.38152 loss)
I1123 21:09:39.624325 16332 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1123 21:09:51.870486 16332 solver.cpp:218] Iteration 600 (8.16623 iter/s, 12.2456s/100 iters), loss = 1.23507
I1123 21:09:51.870486 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.55
I1123 21:09:51.871485 16332 solver.cpp:237]     Train net output #1: loss = 1.23507 (* 1 = 1.23507 loss)
I1123 21:09:51.871485 16332 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1123 21:10:03.948298 16332 solver.cpp:218] Iteration 700 (8.28057 iter/s, 12.0765s/100 iters), loss = 1.1387
I1123 21:10:03.948298 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.54
I1123 21:10:03.948298 16332 solver.cpp:237]     Train net output #1: loss = 1.1387 (* 1 = 1.1387 loss)
I1123 21:10:03.948298 16332 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1123 21:10:16.001236 16332 solver.cpp:218] Iteration 800 (8.29709 iter/s, 12.0524s/100 iters), loss = 0.941145
I1123 21:10:16.001236 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1123 21:10:16.001236 16332 solver.cpp:237]     Train net output #1: loss = 0.941145 (* 1 = 0.941145 loss)
I1123 21:10:16.001236 16332 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1123 21:10:28.109622 16332 solver.cpp:218] Iteration 900 (8.25916 iter/s, 12.1078s/100 iters), loss = 1.03307
I1123 21:10:28.110121 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1123 21:10:28.110121 16332 solver.cpp:237]     Train net output #1: loss = 1.03307 (* 1 = 1.03307 loss)
I1123 21:10:28.110121 16332 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1123 21:10:39.778940 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:10:40.266013 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_1000.caffemodel
I1123 21:10:40.308003 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_1000.solverstate
I1123 21:10:40.329504 16332 solver.cpp:330] Iteration 1000, Testing net (#0)
I1123 21:10:40.330003 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:10:44.428740  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:10:44.597761 16332 solver.cpp:397]     Test net output #0: accuracy = 0.6056
I1123 21:10:44.597761 16332 solver.cpp:397]     Test net output #1: loss = 1.11113 (* 1 = 1.11113 loss)
I1123 21:10:44.719477 16332 solver.cpp:218] Iteration 1000 (6.02081 iter/s, 16.6091s/100 iters), loss = 1.1215
I1123 21:10:44.719477 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1123 21:10:44.719477 16332 solver.cpp:237]     Train net output #1: loss = 1.1215 (* 1 = 1.1215 loss)
I1123 21:10:44.719477 16332 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1123 21:10:56.870532 16332 solver.cpp:218] Iteration 1100 (8.22998 iter/s, 12.1507s/100 iters), loss = 0.952019
I1123 21:10:56.870532 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1123 21:10:56.870532 16332 solver.cpp:237]     Train net output #1: loss = 0.952019 (* 1 = 0.952019 loss)
I1123 21:10:56.870532 16332 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1123 21:11:08.928021 16332 solver.cpp:218] Iteration 1200 (8.29435 iter/s, 12.0564s/100 iters), loss = 0.902009
I1123 21:11:08.928021 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.62
I1123 21:11:08.928021 16332 solver.cpp:237]     Train net output #1: loss = 0.902009 (* 1 = 0.902009 loss)
I1123 21:11:08.928021 16332 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1123 21:11:21.082738 16332 solver.cpp:218] Iteration 1300 (8.22789 iter/s, 12.1538s/100 iters), loss = 0.784431
I1123 21:11:21.082738 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 21:11:21.082738 16332 solver.cpp:237]     Train net output #1: loss = 0.784431 (* 1 = 0.784431 loss)
I1123 21:11:21.082738 16332 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1123 21:11:33.148150 16332 solver.cpp:218] Iteration 1400 (8.28846 iter/s, 12.065s/100 iters), loss = 0.873131
I1123 21:11:33.148150 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1123 21:11:33.148150 16332 solver.cpp:237]     Train net output #1: loss = 0.873131 (* 1 = 0.873131 loss)
I1123 21:11:33.148150 16332 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1123 21:11:44.626160 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:11:45.105003 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_1500.caffemodel
I1123 21:11:45.144986 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_1500.solverstate
I1123 21:11:45.166486 16332 solver.cpp:330] Iteration 1500, Testing net (#0)
I1123 21:11:45.166486 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:11:49.220497  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:11:49.386373 16332 solver.cpp:397]     Test net output #0: accuracy = 0.5644
I1123 21:11:49.386373 16332 solver.cpp:397]     Test net output #1: loss = 1.23192 (* 1 = 1.23192 loss)
I1123 21:11:49.504406 16332 solver.cpp:218] Iteration 1500 (6.11403 iter/s, 16.3558s/100 iters), loss = 0.81372
I1123 21:11:49.504406 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1123 21:11:49.504406 16332 solver.cpp:237]     Train net output #1: loss = 0.81372 (* 1 = 0.81372 loss)
I1123 21:11:49.504406 16332 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1123 21:12:01.558291 16332 solver.cpp:218] Iteration 1600 (8.29684 iter/s, 12.0528s/100 iters), loss = 0.644482
I1123 21:12:01.558291 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 21:12:01.558291 16332 solver.cpp:237]     Train net output #1: loss = 0.644482 (* 1 = 0.644482 loss)
I1123 21:12:01.558291 16332 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1123 21:12:13.625000 16332 solver.cpp:218] Iteration 1700 (8.28752 iter/s, 12.0663s/100 iters), loss = 0.711555
I1123 21:12:13.625000 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1123 21:12:13.625000 16332 solver.cpp:237]     Train net output #1: loss = 0.711555 (* 1 = 0.711555 loss)
I1123 21:12:13.625000 16332 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1123 21:12:25.676982 16332 solver.cpp:218] Iteration 1800 (8.29786 iter/s, 12.0513s/100 iters), loss = 0.717737
I1123 21:12:25.676982 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 21:12:25.676982 16332 solver.cpp:237]     Train net output #1: loss = 0.717737 (* 1 = 0.717737 loss)
I1123 21:12:25.676982 16332 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1123 21:12:37.750710 16332 solver.cpp:218] Iteration 1900 (8.28292 iter/s, 12.073s/100 iters), loss = 0.713674
I1123 21:12:37.751195 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 21:12:37.751195 16332 solver.cpp:237]     Train net output #1: loss = 0.713674 (* 1 = 0.713674 loss)
I1123 21:12:37.751195 16332 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1123 21:12:49.263787 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:12:49.744037 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_2000.caffemodel
I1123 21:12:49.784521 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_2000.solverstate
I1123 21:12:49.805050 16332 solver.cpp:330] Iteration 2000, Testing net (#0)
I1123 21:12:49.805050 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:12:53.843910  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:12:54.009963 16332 solver.cpp:397]     Test net output #0: accuracy = 0.6621
I1123 21:12:54.009963 16332 solver.cpp:397]     Test net output #1: loss = 0.958681 (* 1 = 0.958681 loss)
I1123 21:12:54.128000 16332 solver.cpp:218] Iteration 2000 (6.10626 iter/s, 16.3766s/100 iters), loss = 0.61807
I1123 21:12:54.128000 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 21:12:54.128000 16332 solver.cpp:237]     Train net output #1: loss = 0.61807 (* 1 = 0.61807 loss)
I1123 21:12:54.128000 16332 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1123 21:13:06.186285 16332 solver.cpp:218] Iteration 2100 (8.29367 iter/s, 12.0574s/100 iters), loss = 0.62673
I1123 21:13:06.186285 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 21:13:06.186285 16332 solver.cpp:237]     Train net output #1: loss = 0.62673 (* 1 = 0.62673 loss)
I1123 21:13:06.186285 16332 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1123 21:13:18.238423 16332 solver.cpp:218] Iteration 2200 (8.29782 iter/s, 12.0514s/100 iters), loss = 0.575457
I1123 21:13:18.238423 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 21:13:18.238423 16332 solver.cpp:237]     Train net output #1: loss = 0.575457 (* 1 = 0.575457 loss)
I1123 21:13:18.238423 16332 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1123 21:13:30.289490 16332 solver.cpp:218] Iteration 2300 (8.2984 iter/s, 12.0505s/100 iters), loss = 0.651017
I1123 21:13:30.289490 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 21:13:30.289490 16332 solver.cpp:237]     Train net output #1: loss = 0.651017 (* 1 = 0.651017 loss)
I1123 21:13:30.289490 16332 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1123 21:13:42.337246 16332 solver.cpp:218] Iteration 2400 (8.30056 iter/s, 12.0474s/100 iters), loss = 0.693126
I1123 21:13:42.337246 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1123 21:13:42.337246 16332 solver.cpp:237]     Train net output #1: loss = 0.693126 (* 1 = 0.693126 loss)
I1123 21:13:42.337246 16332 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1123 21:13:53.786168 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:13:54.262678 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_2500.caffemodel
I1123 21:13:54.302697 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_2500.solverstate
I1123 21:13:54.323683 16332 solver.cpp:330] Iteration 2500, Testing net (#0)
I1123 21:13:54.323683 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:13:58.364158  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:13:58.530228 16332 solver.cpp:397]     Test net output #0: accuracy = 0.6336
I1123 21:13:58.530228 16332 solver.cpp:397]     Test net output #1: loss = 1.06907 (* 1 = 1.06907 loss)
I1123 21:13:58.648272 16332 solver.cpp:218] Iteration 2500 (6.1311 iter/s, 16.3103s/100 iters), loss = 0.570329
I1123 21:13:58.648272 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 21:13:58.648272 16332 solver.cpp:237]     Train net output #1: loss = 0.570329 (* 1 = 0.570329 loss)
I1123 21:13:58.648272 16332 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1123 21:14:10.705866 16332 solver.cpp:218] Iteration 2600 (8.29413 iter/s, 12.0567s/100 iters), loss = 0.606878
I1123 21:14:10.705866 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 21:14:10.705866 16332 solver.cpp:237]     Train net output #1: loss = 0.606878 (* 1 = 0.606878 loss)
I1123 21:14:10.705866 16332 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1123 21:14:22.758822 16332 solver.cpp:218] Iteration 2700 (8.29706 iter/s, 12.0525s/100 iters), loss = 0.559774
I1123 21:14:22.758822 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 21:14:22.758822 16332 solver.cpp:237]     Train net output #1: loss = 0.559774 (* 1 = 0.559774 loss)
I1123 21:14:22.758822 16332 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1123 21:14:34.807212 16332 solver.cpp:218] Iteration 2800 (8.30029 iter/s, 12.0478s/100 iters), loss = 0.605806
I1123 21:14:34.807704 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 21:14:34.807704 16332 solver.cpp:237]     Train net output #1: loss = 0.605806 (* 1 = 0.605806 loss)
I1123 21:14:34.807704 16332 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1123 21:14:46.858737 16332 solver.cpp:218] Iteration 2900 (8.29827 iter/s, 12.0507s/100 iters), loss = 0.546857
I1123 21:14:46.858737 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 21:14:46.858737 16332 solver.cpp:237]     Train net output #1: loss = 0.546857 (* 1 = 0.546857 loss)
I1123 21:14:46.858737 16332 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1123 21:14:58.314218 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:14:58.791797 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_3000.caffemodel
I1123 21:14:58.833781 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_3000.solverstate
I1123 21:14:58.854796 16332 solver.cpp:330] Iteration 3000, Testing net (#0)
I1123 21:14:58.854796 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:15:02.897099  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:15:03.062113 16332 solver.cpp:397]     Test net output #0: accuracy = 0.6667
I1123 21:15:03.062113 16332 solver.cpp:397]     Test net output #1: loss = 0.968146 (* 1 = 0.968146 loss)
I1123 21:15:03.180692 16332 solver.cpp:218] Iteration 3000 (6.12704 iter/s, 16.3211s/100 iters), loss = 0.619555
I1123 21:15:03.180692 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 21:15:03.180692 16332 solver.cpp:237]     Train net output #1: loss = 0.619555 (* 1 = 0.619555 loss)
I1123 21:15:03.180692 16332 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1123 21:15:15.229905 16332 solver.cpp:218] Iteration 3100 (8.29966 iter/s, 12.0487s/100 iters), loss = 0.521404
I1123 21:15:15.230388 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 21:15:15.230388 16332 solver.cpp:237]     Train net output #1: loss = 0.521404 (* 1 = 0.521404 loss)
I1123 21:15:15.230388 16332 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1123 21:15:27.282331 16332 solver.cpp:218] Iteration 3200 (8.2976 iter/s, 12.0517s/100 iters), loss = 0.582614
I1123 21:15:27.282331 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 21:15:27.282331 16332 solver.cpp:237]     Train net output #1: loss = 0.582614 (* 1 = 0.582614 loss)
I1123 21:15:27.282331 16332 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1123 21:15:39.330559 16332 solver.cpp:218] Iteration 3300 (8.30037 iter/s, 12.0477s/100 iters), loss = 0.607675
I1123 21:15:39.330559 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 21:15:39.330559 16332 solver.cpp:237]     Train net output #1: loss = 0.607675 (* 1 = 0.607675 loss)
I1123 21:15:39.330559 16332 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1123 21:15:51.379029 16332 solver.cpp:218] Iteration 3400 (8.30048 iter/s, 12.0475s/100 iters), loss = 0.480062
I1123 21:15:51.379029 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 21:15:51.379029 16332 solver.cpp:237]     Train net output #1: loss = 0.480062 (* 1 = 0.480062 loss)
I1123 21:15:51.379029 16332 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1123 21:16:02.833691 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:16:03.312707 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_3500.caffemodel
I1123 21:16:03.352707 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_3500.solverstate
I1123 21:16:03.373708 16332 solver.cpp:330] Iteration 3500, Testing net (#0)
I1123 21:16:03.373708 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:16:07.409701  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:16:07.575309 16332 solver.cpp:397]     Test net output #0: accuracy = 0.6367
I1123 21:16:07.575309 16332 solver.cpp:397]     Test net output #1: loss = 1.05091 (* 1 = 1.05091 loss)
I1123 21:16:07.692847 16332 solver.cpp:218] Iteration 3500 (6.12983 iter/s, 16.3137s/100 iters), loss = 0.516297
I1123 21:16:07.692847 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 21:16:07.692847 16332 solver.cpp:237]     Train net output #1: loss = 0.516297 (* 1 = 0.516297 loss)
I1123 21:16:07.692847 16332 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1123 21:16:19.751896 16332 solver.cpp:218] Iteration 3600 (8.29326 iter/s, 12.058s/100 iters), loss = 0.566438
I1123 21:16:19.751896 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 21:16:19.751896 16332 solver.cpp:237]     Train net output #1: loss = 0.566438 (* 1 = 0.566438 loss)
I1123 21:16:19.751896 16332 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1123 21:16:31.810184 16332 solver.cpp:218] Iteration 3700 (8.29319 iter/s, 12.0581s/100 iters), loss = 0.587736
I1123 21:16:31.810665 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 21:16:31.810665 16332 solver.cpp:237]     Train net output #1: loss = 0.587736 (* 1 = 0.587736 loss)
I1123 21:16:31.810665 16332 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1123 21:16:43.865046 16332 solver.cpp:218] Iteration 3800 (8.29619 iter/s, 12.0537s/100 iters), loss = 0.587205
I1123 21:16:43.865046 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 21:16:43.865046 16332 solver.cpp:237]     Train net output #1: loss = 0.587205 (* 1 = 0.587205 loss)
I1123 21:16:43.865046 16332 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1123 21:16:55.911877 16332 solver.cpp:218] Iteration 3900 (8.30105 iter/s, 12.0467s/100 iters), loss = 0.572188
I1123 21:16:55.911877 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 21:16:55.911877 16332 solver.cpp:237]     Train net output #1: loss = 0.572188 (* 1 = 0.572188 loss)
I1123 21:16:55.911877 16332 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1123 21:17:07.365290 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:17:07.843206 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_4000.caffemodel
I1123 21:17:07.881705 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_4000.solverstate
I1123 21:17:07.902704 16332 solver.cpp:330] Iteration 4000, Testing net (#0)
I1123 21:17:07.903204 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:17:11.942903  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:17:12.107971 16332 solver.cpp:397]     Test net output #0: accuracy = 0.6468
I1123 21:17:12.107971 16332 solver.cpp:397]     Test net output #1: loss = 1.0587 (* 1 = 1.0587 loss)
I1123 21:17:12.227011 16332 solver.cpp:218] Iteration 4000 (6.12967 iter/s, 16.3141s/100 iters), loss = 0.530903
I1123 21:17:12.227011 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 21:17:12.227011 16332 solver.cpp:237]     Train net output #1: loss = 0.530903 (* 1 = 0.530903 loss)
I1123 21:17:12.227011 16332 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1123 21:17:24.287787 16332 solver.cpp:218] Iteration 4100 (8.29166 iter/s, 12.0603s/100 iters), loss = 0.501434
I1123 21:17:24.287787 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 21:17:24.287787 16332 solver.cpp:237]     Train net output #1: loss = 0.501434 (* 1 = 0.501434 loss)
I1123 21:17:24.287787 16332 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1123 21:17:36.338889 16332 solver.cpp:218] Iteration 4200 (8.29864 iter/s, 12.0502s/100 iters), loss = 0.571104
I1123 21:17:36.338889 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 21:17:36.338889 16332 solver.cpp:237]     Train net output #1: loss = 0.571104 (* 1 = 0.571104 loss)
I1123 21:17:36.338889 16332 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1123 21:17:48.391151 16332 solver.cpp:218] Iteration 4300 (8.29765 iter/s, 12.0516s/100 iters), loss = 0.639471
I1123 21:17:48.391151 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 21:17:48.391151 16332 solver.cpp:237]     Train net output #1: loss = 0.639471 (* 1 = 0.639471 loss)
I1123 21:17:48.391151 16332 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1123 21:18:00.447311 16332 solver.cpp:218] Iteration 4400 (8.29475 iter/s, 12.0558s/100 iters), loss = 0.47321
I1123 21:18:00.447811 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 21:18:00.447811 16332 solver.cpp:237]     Train net output #1: loss = 0.47321 (* 1 = 0.47321 loss)
I1123 21:18:00.447811 16332 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1123 21:18:11.896226 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:18:12.375830 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_4500.caffemodel
I1123 21:18:12.416316 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_4500.solverstate
I1123 21:18:12.436313 16332 solver.cpp:330] Iteration 4500, Testing net (#0)
I1123 21:18:12.436313 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:18:16.475358  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:18:16.641618 16332 solver.cpp:397]     Test net output #0: accuracy = 0.6426
I1123 21:18:16.641618 16332 solver.cpp:397]     Test net output #1: loss = 1.02302 (* 1 = 1.02302 loss)
I1123 21:18:16.759343 16332 solver.cpp:218] Iteration 4500 (6.13082 iter/s, 16.311s/100 iters), loss = 0.375836
I1123 21:18:16.759343 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 21:18:16.759343 16332 solver.cpp:237]     Train net output #1: loss = 0.375836 (* 1 = 0.375836 loss)
I1123 21:18:16.759343 16332 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1123 21:18:28.814769 16332 solver.cpp:218] Iteration 4600 (8.29526 iter/s, 12.0551s/100 iters), loss = 0.497627
I1123 21:18:28.814769 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 21:18:28.814769 16332 solver.cpp:237]     Train net output #1: loss = 0.497627 (* 1 = 0.497627 loss)
I1123 21:18:28.815258 16332 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1123 21:18:40.872148 16332 solver.cpp:218] Iteration 4700 (8.29424 iter/s, 12.0566s/100 iters), loss = 0.505046
I1123 21:18:40.872148 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 21:18:40.872148 16332 solver.cpp:237]     Train net output #1: loss = 0.505046 (* 1 = 0.505046 loss)
I1123 21:18:40.872148 16332 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1123 21:18:52.970679 16332 solver.cpp:218] Iteration 4800 (8.26591 iter/s, 12.0979s/100 iters), loss = 0.497566
I1123 21:18:52.970679 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 21:18:52.970679 16332 solver.cpp:237]     Train net output #1: loss = 0.497566 (* 1 = 0.497566 loss)
I1123 21:18:52.970679 16332 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1123 21:19:05.082677 16332 solver.cpp:218] Iteration 4900 (8.25652 iter/s, 12.1116s/100 iters), loss = 0.523449
I1123 21:19:05.082677 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 21:19:05.082677 16332 solver.cpp:237]     Train net output #1: loss = 0.523449 (* 1 = 0.523449 loss)
I1123 21:19:05.082677 16332 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1123 21:19:16.640933 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:19:17.129477 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_5000.caffemodel
I1123 21:19:17.172471 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_5000.solverstate
I1123 21:19:17.193984 16332 solver.cpp:330] Iteration 5000, Testing net (#0)
I1123 21:19:17.193984 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:19:21.252357  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:19:21.418459 16332 solver.cpp:397]     Test net output #0: accuracy = 0.5851
I1123 21:19:21.418459 16332 solver.cpp:397]     Test net output #1: loss = 1.34007 (* 1 = 1.34007 loss)
I1123 21:19:21.537485 16332 solver.cpp:218] Iteration 5000 (6.0776 iter/s, 16.4539s/100 iters), loss = 0.463136
I1123 21:19:21.537485 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1123 21:19:21.537485 16332 solver.cpp:237]     Train net output #1: loss = 0.463136 (* 1 = 0.463136 loss)
I1123 21:19:21.537485 16332 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1123 21:19:21.537485 16332 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1123 21:19:33.692824 16332 solver.cpp:218] Iteration 5100 (8.22733 iter/s, 12.1546s/100 iters), loss = 0.359454
I1123 21:19:33.692824 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1123 21:19:33.692824 16332 solver.cpp:237]     Train net output #1: loss = 0.359454 (* 1 = 0.359454 loss)
I1123 21:19:33.692824 16332 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1123 21:19:45.750854 16332 solver.cpp:218] Iteration 5200 (8.29348 iter/s, 12.0577s/100 iters), loss = 0.398376
I1123 21:19:45.750854 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 21:19:45.751344 16332 solver.cpp:237]     Train net output #1: loss = 0.398376 (* 1 = 0.398376 loss)
I1123 21:19:45.751344 16332 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1123 21:19:57.799865 16332 solver.cpp:218] Iteration 5300 (8.30004 iter/s, 12.0481s/100 iters), loss = 0.379365
I1123 21:19:57.799865 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 21:19:57.799865 16332 solver.cpp:237]     Train net output #1: loss = 0.379365 (* 1 = 0.379365 loss)
I1123 21:19:57.799865 16332 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1123 21:20:09.921422 16332 solver.cpp:218] Iteration 5400 (8.25032 iter/s, 12.1207s/100 iters), loss = 0.272001
I1123 21:20:09.921422 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 21:20:09.921422 16332 solver.cpp:237]     Train net output #1: loss = 0.272001 (* 1 = 0.272001 loss)
I1123 21:20:09.921422 16332 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1123 21:20:21.458638 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:20:21.936695 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_5500.caffemodel
I1123 21:20:21.976672 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_5500.solverstate
I1123 21:20:21.996189 16332 solver.cpp:330] Iteration 5500, Testing net (#0)
I1123 21:20:21.996670 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:20:26.041272  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:20:26.208470 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8625
I1123 21:20:26.208470 16332 solver.cpp:397]     Test net output #1: loss = 0.398277 (* 1 = 0.398277 loss)
I1123 21:20:26.328505 16332 solver.cpp:218] Iteration 5500 (6.09501 iter/s, 16.4069s/100 iters), loss = 0.241877
I1123 21:20:26.328505 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 21:20:26.328505 16332 solver.cpp:237]     Train net output #1: loss = 0.241877 (* 1 = 0.241877 loss)
I1123 21:20:26.328505 16332 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1123 21:20:38.486235 16332 solver.cpp:218] Iteration 5600 (8.22579 iter/s, 12.1569s/100 iters), loss = 0.308778
I1123 21:20:38.486235 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 21:20:38.486235 16332 solver.cpp:237]     Train net output #1: loss = 0.308778 (* 1 = 0.308778 loss)
I1123 21:20:38.486235 16332 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1123 21:20:50.536224 16332 solver.cpp:218] Iteration 5700 (8.29926 iter/s, 12.0493s/100 iters), loss = 0.374271
I1123 21:20:50.536224 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 21:20:50.536224 16332 solver.cpp:237]     Train net output #1: loss = 0.374271 (* 1 = 0.374271 loss)
I1123 21:20:50.536224 16332 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1123 21:21:02.611307 16332 solver.cpp:218] Iteration 5800 (8.28199 iter/s, 12.0744s/100 iters), loss = 0.342444
I1123 21:21:02.611307 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 21:21:02.611307 16332 solver.cpp:237]     Train net output #1: loss = 0.342444 (* 1 = 0.342444 loss)
I1123 21:21:02.611307 16332 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1123 21:21:14.704062 16332 solver.cpp:218] Iteration 5900 (8.26991 iter/s, 12.092s/100 iters), loss = 0.230069
I1123 21:21:14.704062 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 21:21:14.704062 16332 solver.cpp:237]     Train net output #1: loss = 0.230069 (* 1 = 0.230069 loss)
I1123 21:21:14.704062 16332 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1123 21:21:26.162976 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:21:26.644476 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_6000.caffemodel
I1123 21:21:26.685475 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_6000.solverstate
I1123 21:21:26.705976 16332 solver.cpp:330] Iteration 6000, Testing net (#0)
I1123 21:21:26.705976 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:21:30.745326  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:21:30.910852 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8674
I1123 21:21:30.910852 16332 solver.cpp:397]     Test net output #1: loss = 0.388003 (* 1 = 0.388003 loss)
I1123 21:21:31.028791 16332 solver.cpp:218] Iteration 6000 (6.12591 iter/s, 16.3241s/100 iters), loss = 0.237462
I1123 21:21:31.028791 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:21:31.028791 16332 solver.cpp:237]     Train net output #1: loss = 0.237462 (* 1 = 0.237462 loss)
I1123 21:21:31.028791 16332 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1123 21:21:43.095940 16332 solver.cpp:218] Iteration 6100 (8.28738 iter/s, 12.0665s/100 iters), loss = 0.301711
I1123 21:21:43.096428 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 21:21:43.096428 16332 solver.cpp:237]     Train net output #1: loss = 0.301711 (* 1 = 0.301711 loss)
I1123 21:21:43.096428 16332 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1123 21:21:55.144799 16332 solver.cpp:218] Iteration 6200 (8.30028 iter/s, 12.0478s/100 iters), loss = 0.294457
I1123 21:21:55.144799 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 21:21:55.144799 16332 solver.cpp:237]     Train net output #1: loss = 0.294457 (* 1 = 0.294457 loss)
I1123 21:21:55.144799 16332 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1123 21:22:07.197734 16332 solver.cpp:218] Iteration 6300 (8.29694 iter/s, 12.0526s/100 iters), loss = 0.284779
I1123 21:22:07.197734 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 21:22:07.197734 16332 solver.cpp:237]     Train net output #1: loss = 0.284779 (* 1 = 0.284779 loss)
I1123 21:22:07.197734 16332 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1123 21:22:19.325405 16332 solver.cpp:218] Iteration 6400 (8.24604 iter/s, 12.127s/100 iters), loss = 0.165283
I1123 21:22:19.325405 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:22:19.325405 16332 solver.cpp:237]     Train net output #1: loss = 0.165283 (* 1 = 0.165283 loss)
I1123 21:22:19.325405 16332 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1123 21:22:30.785604 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:22:31.264672 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_6500.caffemodel
I1123 21:22:31.304152 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_6500.solverstate
I1123 21:22:31.323667 16332 solver.cpp:330] Iteration 6500, Testing net (#0)
I1123 21:22:31.324151 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:22:35.364053  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:22:35.530586 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8699
I1123 21:22:35.530586 16332 solver.cpp:397]     Test net output #1: loss = 0.377726 (* 1 = 0.377726 loss)
I1123 21:22:35.651130 16332 solver.cpp:218] Iteration 6500 (6.12563 iter/s, 16.3249s/100 iters), loss = 0.187338
I1123 21:22:35.651130 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:22:35.651130 16332 solver.cpp:237]     Train net output #1: loss = 0.187338 (* 1 = 0.187338 loss)
I1123 21:22:35.651130 16332 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1123 21:22:47.782613 16332 solver.cpp:218] Iteration 6600 (8.2434 iter/s, 12.1309s/100 iters), loss = 0.228311
I1123 21:22:47.782613 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:22:47.782613 16332 solver.cpp:237]     Train net output #1: loss = 0.228311 (* 1 = 0.228311 loss)
I1123 21:22:47.782613 16332 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1123 21:22:59.908676 16332 solver.cpp:218] Iteration 6700 (8.24725 iter/s, 12.1253s/100 iters), loss = 0.258397
I1123 21:22:59.908676 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 21:22:59.908676 16332 solver.cpp:237]     Train net output #1: loss = 0.258397 (* 1 = 0.258397 loss)
I1123 21:22:59.908676 16332 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1123 21:23:12.008191 16332 solver.cpp:218] Iteration 6800 (8.26509 iter/s, 12.0991s/100 iters), loss = 0.291205
I1123 21:23:12.008191 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 21:23:12.008191 16332 solver.cpp:237]     Train net output #1: loss = 0.291205 (* 1 = 0.291205 loss)
I1123 21:23:12.008191 16332 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1123 21:23:24.217548 16332 solver.cpp:218] Iteration 6900 (8.19094 iter/s, 12.2086s/100 iters), loss = 0.164279
I1123 21:23:24.217548 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 21:23:24.217548 16332 solver.cpp:237]     Train net output #1: loss = 0.164279 (* 1 = 0.164279 loss)
I1123 21:23:24.217548 16332 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1123 21:23:35.750658 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:23:36.233886 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_7000.caffemodel
I1123 21:23:36.272887 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_7000.solverstate
I1123 21:23:36.292886 16332 solver.cpp:330] Iteration 7000, Testing net (#0)
I1123 21:23:36.292886 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:23:40.343284  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:23:40.508342 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8724
I1123 21:23:40.508342 16332 solver.cpp:397]     Test net output #1: loss = 0.374102 (* 1 = 0.374102 loss)
I1123 21:23:40.627373 16332 solver.cpp:218] Iteration 7000 (6.09435 iter/s, 16.4086s/100 iters), loss = 0.17047
I1123 21:23:40.627373 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:23:40.627373 16332 solver.cpp:237]     Train net output #1: loss = 0.17047 (* 1 = 0.17047 loss)
I1123 21:23:40.627373 16332 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1123 21:23:52.793061 16332 solver.cpp:218] Iteration 7100 (8.21982 iter/s, 12.1657s/100 iters), loss = 0.220753
I1123 21:23:52.793061 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:23:52.793061 16332 solver.cpp:237]     Train net output #1: loss = 0.220753 (* 1 = 0.220753 loss)
I1123 21:23:52.793061 16332 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1123 21:24:04.914815 16332 solver.cpp:218] Iteration 7200 (8.25049 iter/s, 12.1205s/100 iters), loss = 0.240357
I1123 21:24:04.914815 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 21:24:04.914815 16332 solver.cpp:237]     Train net output #1: loss = 0.240357 (* 1 = 0.240357 loss)
I1123 21:24:04.914815 16332 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1123 21:24:17.001687 16332 solver.cpp:218] Iteration 7300 (8.27352 iter/s, 12.0868s/100 iters), loss = 0.205954
I1123 21:24:17.001687 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 21:24:17.001687 16332 solver.cpp:237]     Train net output #1: loss = 0.205953 (* 1 = 0.205953 loss)
I1123 21:24:17.001687 16332 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1123 21:24:29.168062 16332 solver.cpp:218] Iteration 7400 (8.22011 iter/s, 12.1653s/100 iters), loss = 0.18308
I1123 21:24:29.168062 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:24:29.168062 16332 solver.cpp:237]     Train net output #1: loss = 0.18308 (* 1 = 0.18308 loss)
I1123 21:24:29.168062 16332 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1123 21:24:40.728633 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:24:41.206634 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_7500.caffemodel
I1123 21:24:41.247134 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_7500.solverstate
I1123 21:24:41.267134 16332 solver.cpp:330] Iteration 7500, Testing net (#0)
I1123 21:24:41.267134 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:24:45.367683  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:24:45.534301 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8696
I1123 21:24:45.534301 16332 solver.cpp:397]     Test net output #1: loss = 0.386869 (* 1 = 0.386869 loss)
I1123 21:24:45.652429 16332 solver.cpp:218] Iteration 7500 (6.06652 iter/s, 16.4839s/100 iters), loss = 0.156699
I1123 21:24:45.652429 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:24:45.652429 16332 solver.cpp:237]     Train net output #1: loss = 0.156699 (* 1 = 0.156699 loss)
I1123 21:24:45.652429 16332 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1123 21:24:57.763794 16332 solver.cpp:218] Iteration 7600 (8.25748 iter/s, 12.1102s/100 iters), loss = 0.255478
I1123 21:24:57.763794 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 21:24:57.763794 16332 solver.cpp:237]     Train net output #1: loss = 0.255477 (* 1 = 0.255477 loss)
I1123 21:24:57.763794 16332 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1123 21:25:09.850621 16332 solver.cpp:218] Iteration 7700 (8.27368 iter/s, 12.0865s/100 iters), loss = 0.183802
I1123 21:25:09.850621 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 21:25:09.850621 16332 solver.cpp:237]     Train net output #1: loss = 0.183802 (* 1 = 0.183802 loss)
I1123 21:25:09.850621 16332 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1123 21:25:22.164155 16332 solver.cpp:218] Iteration 7800 (8.12175 iter/s, 12.3126s/100 iters), loss = 0.280355
I1123 21:25:22.164155 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 21:25:22.164155 16332 solver.cpp:237]     Train net output #1: loss = 0.280355 (* 1 = 0.280355 loss)
I1123 21:25:22.164155 16332 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1123 21:25:34.313765 16332 solver.cpp:218] Iteration 7900 (8.23116 iter/s, 12.149s/100 iters), loss = 0.199665
I1123 21:25:34.313765 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 21:25:34.313765 16332 solver.cpp:237]     Train net output #1: loss = 0.199665 (* 1 = 0.199665 loss)
I1123 21:25:34.313765 16332 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1123 21:25:45.847113 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:25:46.347170 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_8000.caffemodel
I1123 21:25:46.388674 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_8000.solverstate
I1123 21:25:46.410670 16332 solver.cpp:330] Iteration 8000, Testing net (#0)
I1123 21:25:46.410670 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:25:50.543223  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:25:50.711274 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8733
I1123 21:25:50.711274 16332 solver.cpp:397]     Test net output #1: loss = 0.379877 (* 1 = 0.379877 loss)
I1123 21:25:50.831879 16332 solver.cpp:218] Iteration 8000 (6.05406 iter/s, 16.5179s/100 iters), loss = 0.145533
I1123 21:25:50.831879 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:25:50.831879 16332 solver.cpp:237]     Train net output #1: loss = 0.145533 (* 1 = 0.145533 loss)
I1123 21:25:50.831879 16332 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1123 21:26:03.255823 16332 solver.cpp:218] Iteration 8100 (8.04969 iter/s, 12.4228s/100 iters), loss = 0.277605
I1123 21:26:03.255823 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 21:26:03.255823 16332 solver.cpp:237]     Train net output #1: loss = 0.277605 (* 1 = 0.277605 loss)
I1123 21:26:03.255823 16332 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1123 21:26:15.604542 16332 solver.cpp:218] Iteration 8200 (8.09851 iter/s, 12.3479s/100 iters), loss = 0.192051
I1123 21:26:15.604542 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 21:26:15.604542 16332 solver.cpp:237]     Train net output #1: loss = 0.192051 (* 1 = 0.192051 loss)
I1123 21:26:15.604542 16332 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1123 21:26:28.084161 16332 solver.cpp:218] Iteration 8300 (8.01323 iter/s, 12.4794s/100 iters), loss = 0.226968
I1123 21:26:28.084661 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 21:26:28.084661 16332 solver.cpp:237]     Train net output #1: loss = 0.226968 (* 1 = 0.226968 loss)
I1123 21:26:28.084661 16332 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1123 21:26:40.406556 16332 solver.cpp:218] Iteration 8400 (8.1159 iter/s, 12.3215s/100 iters), loss = 0.142625
I1123 21:26:40.406556 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:26:40.406556 16332 solver.cpp:237]     Train net output #1: loss = 0.142625 (* 1 = 0.142625 loss)
I1123 21:26:40.406556 16332 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1123 21:26:52.208514 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:26:52.703002 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_8500.caffemodel
I1123 21:26:52.742493 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_8500.solverstate
I1123 21:26:52.764019 16332 solver.cpp:330] Iteration 8500, Testing net (#0)
I1123 21:26:52.764019 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:26:56.943119  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:26:57.112188 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8731
I1123 21:26:57.112188 16332 solver.cpp:397]     Test net output #1: loss = 0.379326 (* 1 = 0.379326 loss)
I1123 21:26:57.233204 16332 solver.cpp:218] Iteration 8500 (5.94325 iter/s, 16.8258s/100 iters), loss = 0.169557
I1123 21:26:57.233204 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 21:26:57.233204 16332 solver.cpp:237]     Train net output #1: loss = 0.169556 (* 1 = 0.169556 loss)
I1123 21:26:57.233204 16332 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1123 21:27:09.712504 16332 solver.cpp:218] Iteration 8600 (8.01363 iter/s, 12.4787s/100 iters), loss = 0.1771
I1123 21:27:09.712504 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:27:09.712504 16332 solver.cpp:237]     Train net output #1: loss = 0.1771 (* 1 = 0.1771 loss)
I1123 21:27:09.712504 16332 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1123 21:27:22.039934 16332 solver.cpp:218] Iteration 8700 (8.1125 iter/s, 12.3267s/100 iters), loss = 0.217792
I1123 21:27:22.039934 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 21:27:22.039934 16332 solver.cpp:237]     Train net output #1: loss = 0.217792 (* 1 = 0.217792 loss)
I1123 21:27:22.039934 16332 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1123 21:27:34.446660 16332 solver.cpp:218] Iteration 8800 (8.06042 iter/s, 12.4063s/100 iters), loss = 0.165533
I1123 21:27:34.446660 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:27:34.446660 16332 solver.cpp:237]     Train net output #1: loss = 0.165533 (* 1 = 0.165533 loss)
I1123 21:27:34.446660 16332 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1123 21:27:46.783426 16332 solver.cpp:218] Iteration 8900 (8.10635 iter/s, 12.336s/100 iters), loss = 0.127652
I1123 21:27:46.783426 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:27:46.783426 16332 solver.cpp:237]     Train net output #1: loss = 0.127652 (* 1 = 0.127652 loss)
I1123 21:27:46.783426 16332 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1123 21:27:58.583628 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:27:59.104611 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_9000.caffemodel
I1123 21:27:59.149624 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_9000.solverstate
I1123 21:27:59.171609 16332 solver.cpp:330] Iteration 9000, Testing net (#0)
I1123 21:27:59.171609 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:28:03.296092  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:28:03.466120 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8716
I1123 21:28:03.466120 16332 solver.cpp:397]     Test net output #1: loss = 0.388495 (* 1 = 0.388495 loss)
I1123 21:28:03.587170 16332 solver.cpp:218] Iteration 9000 (5.95138 iter/s, 16.8028s/100 iters), loss = 0.194682
I1123 21:28:03.587170 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 21:28:03.587170 16332 solver.cpp:237]     Train net output #1: loss = 0.194681 (* 1 = 0.194681 loss)
I1123 21:28:03.587170 16332 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1123 21:28:15.991436 16332 solver.cpp:218] Iteration 9100 (8.06199 iter/s, 12.4039s/100 iters), loss = 0.231111
I1123 21:28:15.991436 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 21:28:15.991436 16332 solver.cpp:237]     Train net output #1: loss = 0.231111 (* 1 = 0.231111 loss)
I1123 21:28:15.991436 16332 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1123 21:28:28.488368 16332 solver.cpp:218] Iteration 9200 (8.00235 iter/s, 12.4963s/100 iters), loss = 0.192582
I1123 21:28:28.488368 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 21:28:28.488368 16332 solver.cpp:237]     Train net output #1: loss = 0.192582 (* 1 = 0.192582 loss)
I1123 21:28:28.488368 16332 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1123 21:28:40.921008 16332 solver.cpp:218] Iteration 9300 (8.04383 iter/s, 12.4319s/100 iters), loss = 0.123674
I1123 21:28:40.921008 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:28:40.921008 16332 solver.cpp:237]     Train net output #1: loss = 0.123674 (* 1 = 0.123674 loss)
I1123 21:28:40.921008 16332 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1123 21:28:53.394382 16332 solver.cpp:218] Iteration 9400 (8.01762 iter/s, 12.4725s/100 iters), loss = 0.0782294
I1123 21:28:53.394382 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:28:53.394382 16332 solver.cpp:237]     Train net output #1: loss = 0.0782293 (* 1 = 0.0782293 loss)
I1123 21:28:53.394382 16332 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1123 21:29:05.212754 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:29:05.702255 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_9500.caffemodel
I1123 21:29:05.743255 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_9500.solverstate
I1123 21:29:05.763754 16332 solver.cpp:330] Iteration 9500, Testing net (#0)
I1123 21:29:05.763754 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:29:09.881769  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:29:10.049823 16332 solver.cpp:397]     Test net output #0: accuracy = 0.873
I1123 21:29:10.049823 16332 solver.cpp:397]     Test net output #1: loss = 0.378806 (* 1 = 0.378806 loss)
I1123 21:29:10.170858 16332 solver.cpp:218] Iteration 9500 (5.96082 iter/s, 16.7762s/100 iters), loss = 0.144655
I1123 21:29:10.170858 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:29:10.170858 16332 solver.cpp:237]     Train net output #1: loss = 0.144655 (* 1 = 0.144655 loss)
I1123 21:29:10.170858 16332 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1123 21:29:10.170858 16332 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1123 21:29:22.550040 16332 solver.cpp:218] Iteration 9600 (8.07891 iter/s, 12.3779s/100 iters), loss = 0.207375
I1123 21:29:22.550040 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:29:22.550040 16332 solver.cpp:237]     Train net output #1: loss = 0.207375 (* 1 = 0.207375 loss)
I1123 21:29:22.550040 16332 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1123 21:29:34.625728 16332 solver.cpp:218] Iteration 9700 (8.28136 iter/s, 12.0753s/100 iters), loss = 0.139447
I1123 21:29:34.625728 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:29:34.625728 16332 solver.cpp:237]     Train net output #1: loss = 0.139447 (* 1 = 0.139447 loss)
I1123 21:29:34.625728 16332 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1123 21:29:46.680202 16332 solver.cpp:218] Iteration 9800 (8.29603 iter/s, 12.054s/100 iters), loss = 0.138467
I1123 21:29:46.680685 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:29:46.680685 16332 solver.cpp:237]     Train net output #1: loss = 0.138467 (* 1 = 0.138467 loss)
I1123 21:29:46.680685 16332 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1123 21:29:58.732275 16332 solver.cpp:218] Iteration 9900 (8.29799 iter/s, 12.0511s/100 iters), loss = 0.117744
I1123 21:29:58.732275 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:29:58.732275 16332 solver.cpp:237]     Train net output #1: loss = 0.117744 (* 1 = 0.117744 loss)
I1123 21:29:58.732275 16332 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1123 21:30:10.309036 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:30:10.790896 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_10000.caffemodel
I1123 21:30:10.829893 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_10000.solverstate
I1123 21:30:10.850893 16332 solver.cpp:330] Iteration 10000, Testing net (#0)
I1123 21:30:10.850893 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:30:14.901875  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:30:15.066957 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8886
I1123 21:30:15.066957 16332 solver.cpp:397]     Test net output #1: loss = 0.346099 (* 1 = 0.346099 loss)
I1123 21:30:15.184911 16332 solver.cpp:218] Iteration 10000 (6.078 iter/s, 16.4528s/100 iters), loss = 0.0955885
I1123 21:30:15.185909 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:30:15.185909 16332 solver.cpp:237]     Train net output #1: loss = 0.0955885 (* 1 = 0.0955885 loss)
I1123 21:30:15.185909 16332 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1123 21:30:27.252447 16332 solver.cpp:218] Iteration 10100 (8.28755 iter/s, 12.0663s/100 iters), loss = 0.144871
I1123 21:30:27.252447 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:30:27.252447 16332 solver.cpp:237]     Train net output #1: loss = 0.144871 (* 1 = 0.144871 loss)
I1123 21:30:27.252948 16332 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1123 21:30:39.303915 16332 solver.cpp:218] Iteration 10200 (8.29802 iter/s, 12.0511s/100 iters), loss = 0.165204
I1123 21:30:39.303915 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:30:39.303915 16332 solver.cpp:237]     Train net output #1: loss = 0.165204 (* 1 = 0.165204 loss)
I1123 21:30:39.303915 16332 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1123 21:30:51.352527 16332 solver.cpp:218] Iteration 10300 (8.30052 iter/s, 12.0474s/100 iters), loss = 0.118793
I1123 21:30:51.352527 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:30:51.352527 16332 solver.cpp:237]     Train net output #1: loss = 0.118793 (* 1 = 0.118793 loss)
I1123 21:30:51.352527 16332 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1123 21:31:03.394745 16332 solver.cpp:218] Iteration 10400 (8.30432 iter/s, 12.0419s/100 iters), loss = 0.106971
I1123 21:31:03.394745 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:31:03.394745 16332 solver.cpp:237]     Train net output #1: loss = 0.106971 (* 1 = 0.106971 loss)
I1123 21:31:03.395246 16332 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1123 21:31:14.846047 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:31:15.324311 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_10500.caffemodel
I1123 21:31:15.364809 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_10500.solverstate
I1123 21:31:15.384308 16332 solver.cpp:330] Iteration 10500, Testing net (#0)
I1123 21:31:15.384308 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:31:19.429787  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:31:19.595844 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8889
I1123 21:31:19.595844 16332 solver.cpp:397]     Test net output #1: loss = 0.345732 (* 1 = 0.345732 loss)
I1123 21:31:19.713879 16332 solver.cpp:218] Iteration 10500 (6.12801 iter/s, 16.3185s/100 iters), loss = 0.113023
I1123 21:31:19.713879 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:31:19.713879 16332 solver.cpp:237]     Train net output #1: loss = 0.113023 (* 1 = 0.113023 loss)
I1123 21:31:19.713879 16332 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1123 21:31:31.769466 16332 solver.cpp:218] Iteration 10600 (8.29558 iter/s, 12.0546s/100 iters), loss = 0.18091
I1123 21:31:31.769466 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:31:31.769466 16332 solver.cpp:237]     Train net output #1: loss = 0.18091 (* 1 = 0.18091 loss)
I1123 21:31:31.769466 16332 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1123 21:31:43.816558 16332 solver.cpp:218] Iteration 10700 (8.30105 iter/s, 12.0467s/100 iters), loss = 0.072885
I1123 21:31:43.816558 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:31:43.816558 16332 solver.cpp:237]     Train net output #1: loss = 0.072885 (* 1 = 0.072885 loss)
I1123 21:31:43.816558 16332 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1123 21:31:55.870050 16332 solver.cpp:218] Iteration 10800 (8.29679 iter/s, 12.0529s/100 iters), loss = 0.141495
I1123 21:31:55.870050 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:31:55.870050 16332 solver.cpp:237]     Train net output #1: loss = 0.141495 (* 1 = 0.141495 loss)
I1123 21:31:55.870050 16332 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1123 21:32:07.926473 16332 solver.cpp:218] Iteration 10900 (8.29476 iter/s, 12.0558s/100 iters), loss = 0.0935992
I1123 21:32:07.926973 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:32:07.926973 16332 solver.cpp:237]     Train net output #1: loss = 0.0935991 (* 1 = 0.0935991 loss)
I1123 21:32:07.926973 16332 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1123 21:32:19.377399 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:32:19.854980 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_11000.caffemodel
I1123 21:32:19.894971 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_11000.solverstate
I1123 21:32:19.914971 16332 solver.cpp:330] Iteration 11000, Testing net (#0)
I1123 21:32:19.915457 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:32:23.952416  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:32:24.118950 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8885
I1123 21:32:24.118950 16332 solver.cpp:397]     Test net output #1: loss = 0.343552 (* 1 = 0.343552 loss)
I1123 21:32:24.237481 16332 solver.cpp:218] Iteration 11000 (6.13116 iter/s, 16.3101s/100 iters), loss = 0.0972423
I1123 21:32:24.237481 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:32:24.237481 16332 solver.cpp:237]     Train net output #1: loss = 0.0972422 (* 1 = 0.0972422 loss)
I1123 21:32:24.237481 16332 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1123 21:32:36.297022 16332 solver.cpp:218] Iteration 11100 (8.29282 iter/s, 12.0586s/100 iters), loss = 0.184926
I1123 21:32:36.297022 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:32:36.297022 16332 solver.cpp:237]     Train net output #1: loss = 0.184926 (* 1 = 0.184926 loss)
I1123 21:32:36.297022 16332 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1123 21:32:48.347308 16332 solver.cpp:218] Iteration 11200 (8.29874 iter/s, 12.05s/100 iters), loss = 0.147613
I1123 21:32:48.347308 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:32:48.347808 16332 solver.cpp:237]     Train net output #1: loss = 0.147613 (* 1 = 0.147613 loss)
I1123 21:32:48.347808 16332 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1123 21:33:00.394166 16332 solver.cpp:218] Iteration 11300 (8.30135 iter/s, 12.0462s/100 iters), loss = 0.107779
I1123 21:33:00.394657 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:33:00.394657 16332 solver.cpp:237]     Train net output #1: loss = 0.107778 (* 1 = 0.107778 loss)
I1123 21:33:00.394657 16332 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1123 21:33:12.444111 16332 solver.cpp:218] Iteration 11400 (8.29944 iter/s, 12.049s/100 iters), loss = 0.0875
I1123 21:33:12.444111 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:33:12.444111 16332 solver.cpp:237]     Train net output #1: loss = 0.0874999 (* 1 = 0.0874999 loss)
I1123 21:33:12.444111 16332 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1123 21:33:23.899257 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:33:24.376402 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_11500.caffemodel
I1123 21:33:24.414944 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_11500.solverstate
I1123 21:33:24.434439 16332 solver.cpp:330] Iteration 11500, Testing net (#0)
I1123 21:33:24.434439 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:33:28.474536  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:33:28.640563 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8888
I1123 21:33:28.640563 16332 solver.cpp:397]     Test net output #1: loss = 0.344336 (* 1 = 0.344336 loss)
I1123 21:33:28.758589 16332 solver.cpp:218] Iteration 11500 (6.12984 iter/s, 16.3136s/100 iters), loss = 0.147286
I1123 21:33:28.758589 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 21:33:28.758589 16332 solver.cpp:237]     Train net output #1: loss = 0.147286 (* 1 = 0.147286 loss)
I1123 21:33:28.758589 16332 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1123 21:33:40.825497 16332 solver.cpp:218] Iteration 11600 (8.28724 iter/s, 12.0667s/100 iters), loss = 0.165082
I1123 21:33:40.825997 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:33:40.825997 16332 solver.cpp:237]     Train net output #1: loss = 0.165082 (* 1 = 0.165082 loss)
I1123 21:33:40.825997 16332 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1123 21:33:52.873725 16332 solver.cpp:218] Iteration 11700 (8.30066 iter/s, 12.0472s/100 iters), loss = 0.149411
I1123 21:33:52.873725 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:33:52.873725 16332 solver.cpp:237]     Train net output #1: loss = 0.149411 (* 1 = 0.149411 loss)
I1123 21:33:52.873725 16332 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1123 21:34:04.928431 16332 solver.cpp:218] Iteration 11800 (8.29585 iter/s, 12.0542s/100 iters), loss = 0.088074
I1123 21:34:04.928431 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:34:04.928431 16332 solver.cpp:237]     Train net output #1: loss = 0.0880739 (* 1 = 0.0880739 loss)
I1123 21:34:04.928431 16332 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1123 21:34:16.980041 16332 solver.cpp:218] Iteration 11900 (8.29826 iter/s, 12.0507s/100 iters), loss = 0.0996964
I1123 21:34:16.980041 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:34:16.980041 16332 solver.cpp:237]     Train net output #1: loss = 0.0996963 (* 1 = 0.0996963 loss)
I1123 21:34:16.980041 16332 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1123 21:34:28.435526 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:34:28.914511 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_12000.caffemodel
I1123 21:34:28.955512 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_12000.solverstate
I1123 21:34:28.975512 16332 solver.cpp:330] Iteration 12000, Testing net (#0)
I1123 21:34:28.975512 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:34:33.018745  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:34:33.184299 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8898
I1123 21:34:33.184785 16332 solver.cpp:397]     Test net output #1: loss = 0.345275 (* 1 = 0.345275 loss)
I1123 21:34:33.301789 16332 solver.cpp:218] Iteration 12000 (6.12691 iter/s, 16.3214s/100 iters), loss = 0.106405
I1123 21:34:33.301789 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:34:33.301789 16332 solver.cpp:237]     Train net output #1: loss = 0.106405 (* 1 = 0.106405 loss)
I1123 21:34:33.301789 16332 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1123 21:34:45.354142 16332 solver.cpp:218] Iteration 12100 (8.29756 iter/s, 12.0517s/100 iters), loss = 0.182167
I1123 21:34:45.354142 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:34:45.354142 16332 solver.cpp:237]     Train net output #1: loss = 0.182167 (* 1 = 0.182167 loss)
I1123 21:34:45.354142 16332 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1123 21:34:57.406569 16332 solver.cpp:218] Iteration 12200 (8.29761 iter/s, 12.0517s/100 iters), loss = 0.098763
I1123 21:34:57.406569 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:34:57.406569 16332 solver.cpp:237]     Train net output #1: loss = 0.0987628 (* 1 = 0.0987628 loss)
I1123 21:34:57.406569 16332 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1123 21:35:09.454491 16332 solver.cpp:218] Iteration 12300 (8.30054 iter/s, 12.0474s/100 iters), loss = 0.0607177
I1123 21:35:09.454491 16332 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 21:35:09.454491 16332 solver.cpp:237]     Train net output #1: loss = 0.0607176 (* 1 = 0.0607176 loss)
I1123 21:35:09.454491 16332 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1123 21:35:21.503926 16332 solver.cpp:218] Iteration 12400 (8.2995 iter/s, 12.0489s/100 iters), loss = 0.0711463
I1123 21:35:21.504408 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:35:21.504408 16332 solver.cpp:237]     Train net output #1: loss = 0.0711462 (* 1 = 0.0711462 loss)
I1123 21:35:21.504408 16332 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1123 21:35:32.955613 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:35:33.434612 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_12500.caffemodel
I1123 21:35:33.475611 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_12500.solverstate
I1123 21:35:33.494627 16332 solver.cpp:330] Iteration 12500, Testing net (#0)
I1123 21:35:33.495111 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:35:37.535063  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:35:37.700850 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8894
I1123 21:35:37.700850 16332 solver.cpp:397]     Test net output #1: loss = 0.345838 (* 1 = 0.345838 loss)
I1123 21:35:37.819284 16332 solver.cpp:218] Iteration 12500 (6.12952 iter/s, 16.3145s/100 iters), loss = 0.102937
I1123 21:35:37.819284 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:35:37.819284 16332 solver.cpp:237]     Train net output #1: loss = 0.102937 (* 1 = 0.102937 loss)
I1123 21:35:37.819284 16332 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1123 21:35:49.875066 16332 solver.cpp:218] Iteration 12600 (8.29519 iter/s, 12.0552s/100 iters), loss = 0.186128
I1123 21:35:49.875066 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:35:49.875066 16332 solver.cpp:237]     Train net output #1: loss = 0.186128 (* 1 = 0.186128 loss)
I1123 21:35:49.875066 16332 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1123 21:36:01.925880 16332 solver.cpp:218] Iteration 12700 (8.29864 iter/s, 12.0502s/100 iters), loss = 0.103947
I1123 21:36:01.925880 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:36:01.925880 16332 solver.cpp:237]     Train net output #1: loss = 0.103947 (* 1 = 0.103947 loss)
I1123 21:36:01.925880 16332 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1123 21:36:13.976589 16332 solver.cpp:218] Iteration 12800 (8.29846 iter/s, 12.0504s/100 iters), loss = 0.0941392
I1123 21:36:13.976589 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:36:13.976589 16332 solver.cpp:237]     Train net output #1: loss = 0.094139 (* 1 = 0.094139 loss)
I1123 21:36:13.976589 16332 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1123 21:36:26.030588 16332 solver.cpp:218] Iteration 12900 (8.29651 iter/s, 12.0533s/100 iters), loss = 0.0669354
I1123 21:36:26.031088 16332 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 21:36:26.031088 16332 solver.cpp:237]     Train net output #1: loss = 0.0669352 (* 1 = 0.0669352 loss)
I1123 21:36:26.031088 16332 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1123 21:36:37.482602 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:36:37.961534 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_13000.caffemodel
I1123 21:36:37.999534 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_13000.solverstate
I1123 21:36:38.019067 16332 solver.cpp:330] Iteration 13000, Testing net (#0)
I1123 21:36:38.019547 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:36:42.057593  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:36:42.224153 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8886
I1123 21:36:42.224153 16332 solver.cpp:397]     Test net output #1: loss = 0.348487 (* 1 = 0.348487 loss)
I1123 21:36:42.342162 16332 solver.cpp:218] Iteration 13000 (6.13095 iter/s, 16.3107s/100 iters), loss = 0.105397
I1123 21:36:42.342162 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:36:42.342162 16332 solver.cpp:237]     Train net output #1: loss = 0.105397 (* 1 = 0.105397 loss)
I1123 21:36:42.342162 16332 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1123 21:36:54.395493 16332 solver.cpp:218] Iteration 13100 (8.29681 iter/s, 12.0528s/100 iters), loss = 0.150638
I1123 21:36:54.395983 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:36:54.395983 16332 solver.cpp:237]     Train net output #1: loss = 0.150638 (* 1 = 0.150638 loss)
I1123 21:36:54.395983 16332 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1123 21:37:06.444691 16332 solver.cpp:218] Iteration 13200 (8.29994 iter/s, 12.0483s/100 iters), loss = 0.138797
I1123 21:37:06.444691 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:37:06.444691 16332 solver.cpp:237]     Train net output #1: loss = 0.138797 (* 1 = 0.138797 loss)
I1123 21:37:06.444691 16332 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1123 21:37:18.495760 16332 solver.cpp:218] Iteration 13300 (8.29854 iter/s, 12.0503s/100 iters), loss = 0.118454
I1123 21:37:18.495760 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:37:18.495760 16332 solver.cpp:237]     Train net output #1: loss = 0.118454 (* 1 = 0.118454 loss)
I1123 21:37:18.495760 16332 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1123 21:37:30.539547 16332 solver.cpp:218] Iteration 13400 (8.30335 iter/s, 12.0433s/100 iters), loss = 0.058879
I1123 21:37:30.539547 16332 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 21:37:30.539547 16332 solver.cpp:237]     Train net output #1: loss = 0.0588788 (* 1 = 0.0588788 loss)
I1123 21:37:30.539547 16332 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1123 21:37:41.993150 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:37:42.469328 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_13500.caffemodel
I1123 21:37:42.508800 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_13500.solverstate
I1123 21:37:42.527799 16332 solver.cpp:330] Iteration 13500, Testing net (#0)
I1123 21:37:42.527799 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:37:46.571933  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:37:46.737947 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8878
I1123 21:37:46.737947 16332 solver.cpp:397]     Test net output #1: loss = 0.348897 (* 1 = 0.348897 loss)
I1123 21:37:46.856148 16332 solver.cpp:218] Iteration 13500 (6.12895 iter/s, 16.316s/100 iters), loss = 0.110013
I1123 21:37:46.856148 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:37:46.856148 16332 solver.cpp:237]     Train net output #1: loss = 0.110013 (* 1 = 0.110013 loss)
I1123 21:37:46.856148 16332 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1123 21:37:58.926187 16332 solver.cpp:218] Iteration 13600 (8.28537 iter/s, 12.0695s/100 iters), loss = 0.143647
I1123 21:37:58.926187 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:37:58.926187 16332 solver.cpp:237]     Train net output #1: loss = 0.143647 (* 1 = 0.143647 loss)
I1123 21:37:58.926187 16332 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1123 21:38:10.969110 16332 solver.cpp:218] Iteration 13700 (8.30382 iter/s, 12.0427s/100 iters), loss = 0.118265
I1123 21:38:10.969110 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:38:10.969110 16332 solver.cpp:237]     Train net output #1: loss = 0.118264 (* 1 = 0.118264 loss)
I1123 21:38:10.969110 16332 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1123 21:38:23.018401 16332 solver.cpp:218] Iteration 13800 (8.29987 iter/s, 12.0484s/100 iters), loss = 0.144346
I1123 21:38:23.018887 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:38:23.018887 16332 solver.cpp:237]     Train net output #1: loss = 0.144346 (* 1 = 0.144346 loss)
I1123 21:38:23.018887 16332 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1123 21:38:35.064738 16332 solver.cpp:218] Iteration 13900 (8.30165 iter/s, 12.0458s/100 iters), loss = 0.0788747
I1123 21:38:35.064738 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:38:35.064738 16332 solver.cpp:237]     Train net output #1: loss = 0.0788745 (* 1 = 0.0788745 loss)
I1123 21:38:35.064738 16332 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1123 21:38:46.518122 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:38:46.997123 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_14000.caffemodel
I1123 21:38:47.035121 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_14000.solverstate
I1123 21:38:47.054637 16332 solver.cpp:330] Iteration 14000, Testing net (#0)
I1123 21:38:47.054637 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:38:51.096269  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:38:51.261260 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8883
I1123 21:38:51.261260 16332 solver.cpp:397]     Test net output #1: loss = 0.346327 (* 1 = 0.346327 loss)
I1123 21:38:51.380255 16332 solver.cpp:218] Iteration 14000 (6.12957 iter/s, 16.3144s/100 iters), loss = 0.138952
I1123 21:38:51.380255 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 21:38:51.380255 16332 solver.cpp:237]     Train net output #1: loss = 0.138951 (* 1 = 0.138951 loss)
I1123 21:38:51.380255 16332 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1123 21:39:03.435288 16332 solver.cpp:218] Iteration 14100 (8.29575 iter/s, 12.0544s/100 iters), loss = 0.163851
I1123 21:39:03.435288 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1123 21:39:03.435288 16332 solver.cpp:237]     Train net output #1: loss = 0.16385 (* 1 = 0.16385 loss)
I1123 21:39:03.435288 16332 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1123 21:39:15.487845 16332 solver.cpp:218] Iteration 14200 (8.29717 iter/s, 12.0523s/100 iters), loss = 0.121738
I1123 21:39:15.488328 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:39:15.488328 16332 solver.cpp:237]     Train net output #1: loss = 0.121738 (* 1 = 0.121738 loss)
I1123 21:39:15.488328 16332 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1123 21:39:27.539119 16332 solver.cpp:218] Iteration 14300 (8.29842 iter/s, 12.0505s/100 iters), loss = 0.0877159
I1123 21:39:27.539119 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:39:27.539119 16332 solver.cpp:237]     Train net output #1: loss = 0.0877156 (* 1 = 0.0877156 loss)
I1123 21:39:27.539119 16332 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1123 21:39:39.591128 16332 solver.cpp:218] Iteration 14400 (8.29772 iter/s, 12.0515s/100 iters), loss = 0.0675843
I1123 21:39:39.591128 16332 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 21:39:39.591128 16332 solver.cpp:237]     Train net output #1: loss = 0.067584 (* 1 = 0.067584 loss)
I1123 21:39:39.591128 16332 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1123 21:39:51.044324 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:39:51.524422 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_14500.caffemodel
I1123 21:39:51.563421 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_14500.solverstate
I1123 21:39:51.582922 16332 solver.cpp:330] Iteration 14500, Testing net (#0)
I1123 21:39:51.582922 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:39:55.621228  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:39:55.787065 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8903
I1123 21:39:55.787065 16332 solver.cpp:397]     Test net output #1: loss = 0.347203 (* 1 = 0.347203 loss)
I1123 21:39:55.905081 16332 solver.cpp:218] Iteration 14500 (6.13004 iter/s, 16.3131s/100 iters), loss = 0.106488
I1123 21:39:55.905081 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:39:55.905081 16332 solver.cpp:237]     Train net output #1: loss = 0.106487 (* 1 = 0.106487 loss)
I1123 21:39:55.905081 16332 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1123 21:40:07.987546 16332 solver.cpp:218] Iteration 14600 (8.27708 iter/s, 12.0816s/100 iters), loss = 0.104022
I1123 21:40:07.987546 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:40:07.987546 16332 solver.cpp:237]     Train net output #1: loss = 0.104022 (* 1 = 0.104022 loss)
I1123 21:40:07.987546 16332 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1123 21:40:20.043246 16332 solver.cpp:218] Iteration 14700 (8.29495 iter/s, 12.0555s/100 iters), loss = 0.108869
I1123 21:40:20.043736 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:40:20.043736 16332 solver.cpp:237]     Train net output #1: loss = 0.108869 (* 1 = 0.108869 loss)
I1123 21:40:20.043736 16332 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1123 21:40:32.094383 16332 solver.cpp:218] Iteration 14800 (8.29869 iter/s, 12.0501s/100 iters), loss = 0.118547
I1123 21:40:32.094383 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:40:32.094383 16332 solver.cpp:237]     Train net output #1: loss = 0.118547 (* 1 = 0.118547 loss)
I1123 21:40:32.094383 16332 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1123 21:40:44.139346 16332 solver.cpp:218] Iteration 14900 (8.30252 iter/s, 12.0445s/100 iters), loss = 0.0976464
I1123 21:40:44.139346 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 21:40:44.139346 16332 solver.cpp:237]     Train net output #1: loss = 0.097646 (* 1 = 0.097646 loss)
I1123 21:40:44.139346 16332 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1123 21:40:55.595280 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:40:56.073416 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_15000.caffemodel
I1123 21:40:56.113970 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_15000.solverstate
I1123 21:40:56.133471 16332 solver.cpp:330] Iteration 15000, Testing net (#0)
I1123 21:40:56.133471 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:41:00.174643  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:41:00.339352 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8876
I1123 21:41:00.339352 16332 solver.cpp:397]     Test net output #1: loss = 0.349623 (* 1 = 0.349623 loss)
I1123 21:41:00.457901 16332 solver.cpp:218] Iteration 15000 (6.12818 iter/s, 16.3181s/100 iters), loss = 0.0791377
I1123 21:41:00.457901 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:41:00.457901 16332 solver.cpp:237]     Train net output #1: loss = 0.0791374 (* 1 = 0.0791374 loss)
I1123 21:41:00.457901 16332 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1123 21:41:12.528070 16332 solver.cpp:218] Iteration 15100 (8.28557 iter/s, 12.0692s/100 iters), loss = 0.132389
I1123 21:41:12.528070 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:41:12.528070 16332 solver.cpp:237]     Train net output #1: loss = 0.132388 (* 1 = 0.132388 loss)
I1123 21:41:12.528070 16332 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1123 21:41:24.569252 16332 solver.cpp:218] Iteration 15200 (8.30525 iter/s, 12.0406s/100 iters), loss = 0.104146
I1123 21:41:24.569252 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:41:24.569252 16332 solver.cpp:237]     Train net output #1: loss = 0.104145 (* 1 = 0.104145 loss)
I1123 21:41:24.569252 16332 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1123 21:41:36.617468 16332 solver.cpp:218] Iteration 15300 (8.30039 iter/s, 12.0476s/100 iters), loss = 0.070118
I1123 21:41:36.617468 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:41:36.617468 16332 solver.cpp:237]     Train net output #1: loss = 0.0701176 (* 1 = 0.0701176 loss)
I1123 21:41:36.617468 16332 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1123 21:41:36.617468 16332 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1123 21:41:48.666112 16332 solver.cpp:218] Iteration 15400 (8.30013 iter/s, 12.048s/100 iters), loss = 0.0931563
I1123 21:41:48.666112 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:41:48.666112 16332 solver.cpp:237]     Train net output #1: loss = 0.0931559 (* 1 = 0.0931559 loss)
I1123 21:41:48.666112 16332 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1123 21:42:00.118854 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:42:00.596356 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_15500.caffemodel
I1123 21:42:00.634861 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_15500.solverstate
I1123 21:42:00.654372 16332 solver.cpp:330] Iteration 15500, Testing net (#0)
I1123 21:42:00.654372 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:42:04.699546  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:42:04.865607 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8874
I1123 21:42:04.865607 16332 solver.cpp:397]     Test net output #1: loss = 0.350331 (* 1 = 0.350331 loss)
I1123 21:42:04.984676 16332 solver.cpp:218] Iteration 15500 (6.12825 iter/s, 16.3179s/100 iters), loss = 0.103506
I1123 21:42:04.984676 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:42:04.984676 16332 solver.cpp:237]     Train net output #1: loss = 0.103506 (* 1 = 0.103506 loss)
I1123 21:42:04.984676 16332 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1123 21:42:17.041306 16332 solver.cpp:218] Iteration 15600 (8.29457 iter/s, 12.0561s/100 iters), loss = 0.177627
I1123 21:42:17.041306 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:42:17.041306 16332 solver.cpp:237]     Train net output #1: loss = 0.177627 (* 1 = 0.177627 loss)
I1123 21:42:17.041306 16332 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1123 21:42:29.096937 16332 solver.cpp:218] Iteration 15700 (8.29513 iter/s, 12.0553s/100 iters), loss = 0.0918113
I1123 21:42:29.097422 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:42:29.097422 16332 solver.cpp:237]     Train net output #1: loss = 0.0918109 (* 1 = 0.0918109 loss)
I1123 21:42:29.097422 16332 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1123 21:42:41.146991 16332 solver.cpp:218] Iteration 15800 (8.29889 iter/s, 12.0498s/100 iters), loss = 0.101311
I1123 21:42:41.146991 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:42:41.146991 16332 solver.cpp:237]     Train net output #1: loss = 0.101311 (* 1 = 0.101311 loss)
I1123 21:42:41.146991 16332 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1123 21:42:53.201081 16332 solver.cpp:218] Iteration 15900 (8.29677 iter/s, 12.0529s/100 iters), loss = 0.0853023
I1123 21:42:53.201081 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:42:53.201081 16332 solver.cpp:237]     Train net output #1: loss = 0.0853019 (* 1 = 0.0853019 loss)
I1123 21:42:53.201081 16332 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1123 21:43:04.658085 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:43:05.136559 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_16000.caffemodel
I1123 21:43:05.178563 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_16000.solverstate
I1123 21:43:05.199081 16332 solver.cpp:330] Iteration 16000, Testing net (#0)
I1123 21:43:05.199081 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:43:09.237208  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:43:09.402748 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8882
I1123 21:43:09.402748 16332 solver.cpp:397]     Test net output #1: loss = 0.349194 (* 1 = 0.349194 loss)
I1123 21:43:09.520859 16332 solver.cpp:218] Iteration 16000 (6.12778 iter/s, 16.3191s/100 iters), loss = 0.10736
I1123 21:43:09.521350 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:43:09.521350 16332 solver.cpp:237]     Train net output #1: loss = 0.10736 (* 1 = 0.10736 loss)
I1123 21:43:09.521350 16332 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1123 21:43:21.576807 16332 solver.cpp:218] Iteration 16100 (8.29506 iter/s, 12.0554s/100 iters), loss = 0.130589
I1123 21:43:21.577298 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:43:21.577298 16332 solver.cpp:237]     Train net output #1: loss = 0.130588 (* 1 = 0.130588 loss)
I1123 21:43:21.577298 16332 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1123 21:43:33.626088 16332 solver.cpp:218] Iteration 16200 (8.29969 iter/s, 12.0486s/100 iters), loss = 0.0916081
I1123 21:43:33.626571 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:43:33.626571 16332 solver.cpp:237]     Train net output #1: loss = 0.0916077 (* 1 = 0.0916077 loss)
I1123 21:43:33.626571 16332 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1123 21:43:45.671658 16332 solver.cpp:218] Iteration 16300 (8.30205 iter/s, 12.0452s/100 iters), loss = 0.0867609
I1123 21:43:45.671658 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:43:45.671658 16332 solver.cpp:237]     Train net output #1: loss = 0.0867605 (* 1 = 0.0867605 loss)
I1123 21:43:45.671658 16332 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1123 21:43:57.727282 16332 solver.cpp:218] Iteration 16400 (8.29562 iter/s, 12.0546s/100 iters), loss = 0.0543457
I1123 21:43:57.727282 16332 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 21:43:57.727282 16332 solver.cpp:237]     Train net output #1: loss = 0.0543453 (* 1 = 0.0543453 loss)
I1123 21:43:57.727282 16332 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1123 21:44:09.179268 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:44:09.657827 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_16500.caffemodel
I1123 21:44:09.697823 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_16500.solverstate
I1123 21:44:09.717322 16332 solver.cpp:330] Iteration 16500, Testing net (#0)
I1123 21:44:09.717322 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:44:13.758105  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:44:13.924172 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8891
I1123 21:44:13.924172 16332 solver.cpp:397]     Test net output #1: loss = 0.348956 (* 1 = 0.348956 loss)
I1123 21:44:14.042212 16332 solver.cpp:218] Iteration 16500 (6.12967 iter/s, 16.3141s/100 iters), loss = 0.0922122
I1123 21:44:14.042212 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:44:14.042212 16332 solver.cpp:237]     Train net output #1: loss = 0.0922118 (* 1 = 0.0922118 loss)
I1123 21:44:14.042212 16332 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1123 21:44:26.162453 16332 solver.cpp:218] Iteration 16600 (8.25109 iter/s, 12.1196s/100 iters), loss = 0.0971491
I1123 21:44:26.162453 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:44:26.162453 16332 solver.cpp:237]     Train net output #1: loss = 0.0971487 (* 1 = 0.0971487 loss)
I1123 21:44:26.162453 16332 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1123 21:44:38.290139 16332 solver.cpp:218] Iteration 16700 (8.24579 iter/s, 12.1274s/100 iters), loss = 0.0964924
I1123 21:44:38.290630 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:44:38.290630 16332 solver.cpp:237]     Train net output #1: loss = 0.096492 (* 1 = 0.096492 loss)
I1123 21:44:38.290630 16332 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1123 21:44:50.351171 16332 solver.cpp:218] Iteration 16800 (8.29186 iter/s, 12.06s/100 iters), loss = 0.112649
I1123 21:44:50.351171 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:44:50.351171 16332 solver.cpp:237]     Train net output #1: loss = 0.112649 (* 1 = 0.112649 loss)
I1123 21:44:50.351171 16332 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1123 21:45:02.408776 16332 solver.cpp:218] Iteration 16900 (8.29395 iter/s, 12.057s/100 iters), loss = 0.0924333
I1123 21:45:02.408776 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:45:02.408776 16332 solver.cpp:237]     Train net output #1: loss = 0.0924329 (* 1 = 0.0924329 loss)
I1123 21:45:02.408776 16332 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1123 21:45:13.862475 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:45:14.341856 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_17000.caffemodel
I1123 21:45:14.380362 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_17000.solverstate
I1123 21:45:14.399876 16332 solver.cpp:330] Iteration 17000, Testing net (#0)
I1123 21:45:14.399876 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:45:18.437896  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:45:18.603045 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8888
I1123 21:45:18.603045 16332 solver.cpp:397]     Test net output #1: loss = 0.349105 (* 1 = 0.349105 loss)
I1123 21:45:18.721092 16332 solver.cpp:218] Iteration 17000 (6.13049 iter/s, 16.3119s/100 iters), loss = 0.0826205
I1123 21:45:18.721092 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:45:18.721092 16332 solver.cpp:237]     Train net output #1: loss = 0.0826201 (* 1 = 0.0826201 loss)
I1123 21:45:18.721092 16332 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1123 21:45:30.784107 16332 solver.cpp:218] Iteration 17100 (8.29034 iter/s, 12.0622s/100 iters), loss = 0.118695
I1123 21:45:30.784107 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:45:30.784107 16332 solver.cpp:237]     Train net output #1: loss = 0.118695 (* 1 = 0.118695 loss)
I1123 21:45:30.784107 16332 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1123 21:45:42.837277 16332 solver.cpp:218] Iteration 17200 (8.29685 iter/s, 12.0528s/100 iters), loss = 0.0922225
I1123 21:45:42.837277 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:45:42.837277 16332 solver.cpp:237]     Train net output #1: loss = 0.0922221 (* 1 = 0.0922221 loss)
I1123 21:45:42.837277 16332 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1123 21:45:54.894281 16332 solver.cpp:218] Iteration 17300 (8.29436 iter/s, 12.0564s/100 iters), loss = 0.100723
I1123 21:45:54.894281 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:45:54.894281 16332 solver.cpp:237]     Train net output #1: loss = 0.100723 (* 1 = 0.100723 loss)
I1123 21:45:54.894281 16332 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1123 21:46:06.950572 16332 solver.cpp:218] Iteration 17400 (8.29507 iter/s, 12.0554s/100 iters), loss = 0.0645385
I1123 21:46:06.950572 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:46:06.950572 16332 solver.cpp:237]     Train net output #1: loss = 0.0645382 (* 1 = 0.0645382 loss)
I1123 21:46:06.950572 16332 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1123 21:46:18.463887 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:46:18.940049 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_17500.caffemodel
I1123 21:46:18.981050 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_17500.solverstate
I1123 21:46:19.002050 16332 solver.cpp:330] Iteration 17500, Testing net (#0)
I1123 21:46:19.002050 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:46:23.069051  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:46:23.236102 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8893
I1123 21:46:23.236102 16332 solver.cpp:397]     Test net output #1: loss = 0.349267 (* 1 = 0.349267 loss)
I1123 21:46:23.355125 16332 solver.cpp:218] Iteration 17500 (6.09616 iter/s, 16.4038s/100 iters), loss = 0.0956685
I1123 21:46:23.355125 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:46:23.355125 16332 solver.cpp:237]     Train net output #1: loss = 0.0956682 (* 1 = 0.0956682 loss)
I1123 21:46:23.355125 16332 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1123 21:46:35.511675 16332 solver.cpp:218] Iteration 17600 (8.22639 iter/s, 12.156s/100 iters), loss = 0.105614
I1123 21:46:35.511675 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:46:35.511675 16332 solver.cpp:237]     Train net output #1: loss = 0.105613 (* 1 = 0.105613 loss)
I1123 21:46:35.511675 16332 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1123 21:46:47.812556 16332 solver.cpp:218] Iteration 17700 (8.1299 iter/s, 12.3003s/100 iters), loss = 0.125226
I1123 21:46:47.812556 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:46:47.812556 16332 solver.cpp:237]     Train net output #1: loss = 0.125226 (* 1 = 0.125226 loss)
I1123 21:46:47.812556 16332 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1123 21:47:00.225715 16332 solver.cpp:218] Iteration 17800 (8.05633 iter/s, 12.4126s/100 iters), loss = 0.0602105
I1123 21:47:00.225715 16332 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 21:47:00.225715 16332 solver.cpp:237]     Train net output #1: loss = 0.0602101 (* 1 = 0.0602101 loss)
I1123 21:47:00.225715 16332 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1123 21:47:12.493683 16332 solver.cpp:218] Iteration 17900 (8.15176 iter/s, 12.2673s/100 iters), loss = 0.0454139
I1123 21:47:12.493683 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:47:12.493683 16332 solver.cpp:237]     Train net output #1: loss = 0.0454136 (* 1 = 0.0454136 loss)
I1123 21:47:12.493683 16332 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1123 21:47:24.136308 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:47:24.617954 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_18000.caffemodel
I1123 21:47:24.658454 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_18000.solverstate
I1123 21:47:24.678953 16332 solver.cpp:330] Iteration 18000, Testing net (#0)
I1123 21:47:24.678953 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:47:28.765982  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:47:28.934006 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8893
I1123 21:47:28.934006 16332 solver.cpp:397]     Test net output #1: loss = 0.349086 (* 1 = 0.349086 loss)
I1123 21:47:29.055053 16332 solver.cpp:218] Iteration 18000 (6.03853 iter/s, 16.5603s/100 iters), loss = 0.0945586
I1123 21:47:29.055053 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:47:29.055053 16332 solver.cpp:237]     Train net output #1: loss = 0.0945582 (* 1 = 0.0945582 loss)
I1123 21:47:29.055053 16332 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1123 21:47:41.413566 16332 solver.cpp:218] Iteration 18100 (8.09152 iter/s, 12.3586s/100 iters), loss = 0.10961
I1123 21:47:41.413566 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:47:41.413566 16332 solver.cpp:237]     Train net output #1: loss = 0.10961 (* 1 = 0.10961 loss)
I1123 21:47:41.413566 16332 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1123 21:47:53.689481 16332 solver.cpp:218] Iteration 18200 (8.14644 iter/s, 12.2753s/100 iters), loss = 0.113491
I1123 21:47:53.689481 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:47:53.689481 16332 solver.cpp:237]     Train net output #1: loss = 0.113491 (* 1 = 0.113491 loss)
I1123 21:47:53.689481 16332 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1123 21:48:05.774250 16332 solver.cpp:218] Iteration 18300 (8.27588 iter/s, 12.0833s/100 iters), loss = 0.113842
I1123 21:48:05.774250 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:48:05.774250 16332 solver.cpp:237]     Train net output #1: loss = 0.113841 (* 1 = 0.113841 loss)
I1123 21:48:05.774250 16332 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1123 21:48:17.884186 16332 solver.cpp:218] Iteration 18400 (8.25775 iter/s, 12.1098s/100 iters), loss = 0.126696
I1123 21:48:17.884186 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:48:17.884186 16332 solver.cpp:237]     Train net output #1: loss = 0.126696 (* 1 = 0.126696 loss)
I1123 21:48:17.884186 16332 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1123 21:48:29.589239 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:48:30.084555 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_18500.caffemodel
I1123 21:48:30.122057 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_18500.solverstate
I1123 21:48:30.143043 16332 solver.cpp:330] Iteration 18500, Testing net (#0)
I1123 21:48:30.143043 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:48:34.231190  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:48:34.401031 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8893
I1123 21:48:34.401031 16332 solver.cpp:397]     Test net output #1: loss = 0.349528 (* 1 = 0.349528 loss)
I1123 21:48:34.524080 16332 solver.cpp:218] Iteration 18500 (6.0101 iter/s, 16.6387s/100 iters), loss = 0.108899
I1123 21:48:34.524080 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:48:34.524080 16332 solver.cpp:237]     Train net output #1: loss = 0.108898 (* 1 = 0.108898 loss)
I1123 21:48:34.524080 16332 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1123 21:48:46.875715 16332 solver.cpp:218] Iteration 18600 (8.09628 iter/s, 12.3513s/100 iters), loss = 0.104173
I1123 21:48:46.876200 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:48:46.876200 16332 solver.cpp:237]     Train net output #1: loss = 0.104173 (* 1 = 0.104173 loss)
I1123 21:48:46.876200 16332 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1123 21:48:59.268782 16332 solver.cpp:218] Iteration 18700 (8.06987 iter/s, 12.3918s/100 iters), loss = 0.0993379
I1123 21:48:59.268782 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:48:59.268782 16332 solver.cpp:237]     Train net output #1: loss = 0.0993375 (* 1 = 0.0993375 loss)
I1123 21:48:59.268782 16332 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1123 21:49:11.414863 16332 solver.cpp:218] Iteration 18800 (8.23328 iter/s, 12.1458s/100 iters), loss = 0.0849694
I1123 21:49:11.415349 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:49:11.415349 16332 solver.cpp:237]     Train net output #1: loss = 0.084969 (* 1 = 0.084969 loss)
I1123 21:49:11.415349 16332 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1123 21:49:23.495132 16332 solver.cpp:218] Iteration 18900 (8.2786 iter/s, 12.0793s/100 iters), loss = 0.0550141
I1123 21:49:23.495132 16332 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 21:49:23.495132 16332 solver.cpp:237]     Train net output #1: loss = 0.0550137 (* 1 = 0.0550137 loss)
I1123 21:49:23.495132 16332 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1123 21:49:34.981505 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:49:35.461117 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_19000.caffemodel
I1123 21:49:35.501112 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_19000.solverstate
I1123 21:49:35.521112 16332 solver.cpp:330] Iteration 19000, Testing net (#0)
I1123 21:49:35.521112 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:49:39.557776  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:49:39.723309 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8893
I1123 21:49:39.723309 16332 solver.cpp:397]     Test net output #1: loss = 0.349343 (* 1 = 0.349343 loss)
I1123 21:49:39.842352 16332 solver.cpp:218] Iteration 19000 (6.11756 iter/s, 16.3464s/100 iters), loss = 0.111541
I1123 21:49:39.842352 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:49:39.842352 16332 solver.cpp:237]     Train net output #1: loss = 0.11154 (* 1 = 0.11154 loss)
I1123 21:49:39.842352 16332 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1123 21:49:51.987975 16332 solver.cpp:218] Iteration 19100 (8.23393 iter/s, 12.1449s/100 iters), loss = 0.217426
I1123 21:49:51.987975 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:49:51.987975 16332 solver.cpp:237]     Train net output #1: loss = 0.217426 (* 1 = 0.217426 loss)
I1123 21:49:51.987975 16332 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1123 21:50:04.175459 16332 solver.cpp:218] Iteration 19200 (8.20558 iter/s, 12.1868s/100 iters), loss = 0.112065
I1123 21:50:04.175459 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:50:04.175459 16332 solver.cpp:237]     Train net output #1: loss = 0.112065 (* 1 = 0.112065 loss)
I1123 21:50:04.175459 16332 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1123 21:50:16.308339 16332 solver.cpp:218] Iteration 19300 (8.24254 iter/s, 12.1322s/100 iters), loss = 0.0925299
I1123 21:50:16.308339 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:50:16.308339 16332 solver.cpp:237]     Train net output #1: loss = 0.0925296 (* 1 = 0.0925296 loss)
I1123 21:50:16.308339 16332 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1123 21:50:28.355677 16332 solver.cpp:218] Iteration 19400 (8.30094 iter/s, 12.0468s/100 iters), loss = 0.0843924
I1123 21:50:28.355677 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:50:28.355677 16332 solver.cpp:237]     Train net output #1: loss = 0.0843921 (* 1 = 0.0843921 loss)
I1123 21:50:28.355677 16332 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1123 21:50:39.811543 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:50:40.291544 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_19500.caffemodel
I1123 21:50:40.332042 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_19500.solverstate
I1123 21:50:40.351042 16332 solver.cpp:330] Iteration 19500, Testing net (#0)
I1123 21:50:40.351542 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:50:44.394489  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:50:44.560060 16332 solver.cpp:397]     Test net output #0: accuracy = 0.89
I1123 21:50:44.560060 16332 solver.cpp:397]     Test net output #1: loss = 0.348973 (* 1 = 0.348973 loss)
I1123 21:50:44.678071 16332 solver.cpp:218] Iteration 19500 (6.12669 iter/s, 16.322s/100 iters), loss = 0.0852431
I1123 21:50:44.678071 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:50:44.678071 16332 solver.cpp:237]     Train net output #1: loss = 0.0852428 (* 1 = 0.0852428 loss)
I1123 21:50:44.678071 16332 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1123 21:50:44.678071 16332 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1123 21:50:56.733968 16332 solver.cpp:218] Iteration 19600 (8.29548 iter/s, 12.0548s/100 iters), loss = 0.101721
I1123 21:50:56.733968 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:50:56.733968 16332 solver.cpp:237]     Train net output #1: loss = 0.101721 (* 1 = 0.101721 loss)
I1123 21:50:56.733968 16332 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1123 21:51:08.776996 16332 solver.cpp:218] Iteration 19700 (8.30395 iter/s, 12.0425s/100 iters), loss = 0.105168
I1123 21:51:08.776996 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:51:08.776996 16332 solver.cpp:237]     Train net output #1: loss = 0.105167 (* 1 = 0.105167 loss)
I1123 21:51:08.776996 16332 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1123 21:51:20.828469 16332 solver.cpp:218] Iteration 19800 (8.29792 iter/s, 12.0512s/100 iters), loss = 0.0641364
I1123 21:51:20.828469 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:51:20.828469 16332 solver.cpp:237]     Train net output #1: loss = 0.0641361 (* 1 = 0.0641361 loss)
I1123 21:51:20.828469 16332 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1123 21:51:32.877070 16332 solver.cpp:218] Iteration 19900 (8.30011 iter/s, 12.048s/100 iters), loss = 0.0848376
I1123 21:51:32.877557 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:51:32.877557 16332 solver.cpp:237]     Train net output #1: loss = 0.0848373 (* 1 = 0.0848373 loss)
I1123 21:51:32.877557 16332 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1123 21:51:44.333338 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:51:44.810411 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_20000.caffemodel
I1123 21:51:44.848413 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_20000.solverstate
I1123 21:51:44.868412 16332 solver.cpp:330] Iteration 20000, Testing net (#0)
I1123 21:51:44.868412 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:51:48.907425  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:51:49.073642 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8897
I1123 21:51:49.073642 16332 solver.cpp:397]     Test net output #1: loss = 0.349205 (* 1 = 0.349205 loss)
I1123 21:51:49.192152 16332 solver.cpp:218] Iteration 20000 (6.12958 iter/s, 16.3143s/100 iters), loss = 0.0949615
I1123 21:51:49.192152 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:51:49.192152 16332 solver.cpp:237]     Train net output #1: loss = 0.0949612 (* 1 = 0.0949612 loss)
I1123 21:51:49.192152 16332 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1123 21:52:01.249686 16332 solver.cpp:218] Iteration 20100 (8.29425 iter/s, 12.0565s/100 iters), loss = 0.153528
I1123 21:52:01.249686 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:52:01.249686 16332 solver.cpp:237]     Train net output #1: loss = 0.153528 (* 1 = 0.153528 loss)
I1123 21:52:01.249686 16332 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1123 21:52:13.307876 16332 solver.cpp:218] Iteration 20200 (8.2933 iter/s, 12.0579s/100 iters), loss = 0.0963547
I1123 21:52:13.308377 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:52:13.308377 16332 solver.cpp:237]     Train net output #1: loss = 0.0963544 (* 1 = 0.0963544 loss)
I1123 21:52:13.308377 16332 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1123 21:52:25.368507 16332 solver.cpp:218] Iteration 20300 (8.29209 iter/s, 12.0597s/100 iters), loss = 0.0670927
I1123 21:52:25.368507 16332 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 21:52:25.368507 16332 solver.cpp:237]     Train net output #1: loss = 0.0670923 (* 1 = 0.0670923 loss)
I1123 21:52:25.368507 16332 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1123 21:52:37.410707 16332 solver.cpp:218] Iteration 20400 (8.30465 iter/s, 12.0414s/100 iters), loss = 0.082836
I1123 21:52:37.410707 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:52:37.410707 16332 solver.cpp:237]     Train net output #1: loss = 0.0828356 (* 1 = 0.0828356 loss)
I1123 21:52:37.410707 16332 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1123 21:52:48.869696 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:52:49.346491 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_20500.caffemodel
I1123 21:52:49.386528 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_20500.solverstate
I1123 21:52:49.407027 16332 solver.cpp:330] Iteration 20500, Testing net (#0)
I1123 21:52:49.407027 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:52:53.445627  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:52:53.611469 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8899
I1123 21:52:53.611469 16332 solver.cpp:397]     Test net output #1: loss = 0.349027 (* 1 = 0.349027 loss)
I1123 21:52:53.729020 16332 solver.cpp:218] Iteration 20500 (6.12811 iter/s, 16.3183s/100 iters), loss = 0.106121
I1123 21:52:53.729020 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:52:53.729020 16332 solver.cpp:237]     Train net output #1: loss = 0.106121 (* 1 = 0.106121 loss)
I1123 21:52:53.729020 16332 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1123 21:53:05.784596 16332 solver.cpp:218] Iteration 20600 (8.29576 iter/s, 12.0543s/100 iters), loss = 0.158316
I1123 21:53:05.784596 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:53:05.784596 16332 solver.cpp:237]     Train net output #1: loss = 0.158315 (* 1 = 0.158315 loss)
I1123 21:53:05.784596 16332 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1123 21:53:17.836488 16332 solver.cpp:218] Iteration 20700 (8.29769 iter/s, 12.0515s/100 iters), loss = 0.0901478
I1123 21:53:17.836488 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:53:17.836488 16332 solver.cpp:237]     Train net output #1: loss = 0.0901474 (* 1 = 0.0901474 loss)
I1123 21:53:17.836488 16332 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1123 21:53:29.890291 16332 solver.cpp:218] Iteration 20800 (8.29625 iter/s, 12.0536s/100 iters), loss = 0.090137
I1123 21:53:29.890291 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:53:29.890291 16332 solver.cpp:237]     Train net output #1: loss = 0.0901367 (* 1 = 0.0901367 loss)
I1123 21:53:29.890291 16332 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1123 21:53:41.938575 16332 solver.cpp:218] Iteration 20900 (8.30085 iter/s, 12.047s/100 iters), loss = 0.0677644
I1123 21:53:41.938575 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:53:41.938575 16332 solver.cpp:237]     Train net output #1: loss = 0.0677641 (* 1 = 0.0677641 loss)
I1123 21:53:41.938575 16332 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1123 21:53:53.393610 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:53:53.873725 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_21000.caffemodel
I1123 21:53:53.913724 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_21000.solverstate
I1123 21:53:53.933725 16332 solver.cpp:330] Iteration 21000, Testing net (#0)
I1123 21:53:53.933725 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:53:57.975522  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:53:58.141525 16332 solver.cpp:397]     Test net output #0: accuracy = 0.89
I1123 21:53:58.141525 16332 solver.cpp:397]     Test net output #1: loss = 0.349192 (* 1 = 0.349192 loss)
I1123 21:53:58.261086 16332 solver.cpp:218] Iteration 21000 (6.12677 iter/s, 16.3218s/100 iters), loss = 0.0964495
I1123 21:53:58.261086 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:53:58.261086 16332 solver.cpp:237]     Train net output #1: loss = 0.0964492 (* 1 = 0.0964492 loss)
I1123 21:53:58.261086 16332 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1123 21:54:10.318145 16332 solver.cpp:218] Iteration 21100 (8.29415 iter/s, 12.0567s/100 iters), loss = 0.0828213
I1123 21:54:10.318145 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:54:10.318645 16332 solver.cpp:237]     Train net output #1: loss = 0.082821 (* 1 = 0.082821 loss)
I1123 21:54:10.318645 16332 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1123 21:54:22.371367 16332 solver.cpp:218] Iteration 21200 (8.29722 iter/s, 12.0522s/100 iters), loss = 0.0860697
I1123 21:54:22.371367 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:54:22.371367 16332 solver.cpp:237]     Train net output #1: loss = 0.0860694 (* 1 = 0.0860694 loss)
I1123 21:54:22.371367 16332 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1123 21:54:34.421066 16332 solver.cpp:218] Iteration 21300 (8.29881 iter/s, 12.0499s/100 iters), loss = 0.101
I1123 21:54:34.422066 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:54:34.422066 16332 solver.cpp:237]     Train net output #1: loss = 0.101 (* 1 = 0.101 loss)
I1123 21:54:34.422066 16332 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1123 21:54:46.471593 16332 solver.cpp:218] Iteration 21400 (8.29917 iter/s, 12.0494s/100 iters), loss = 0.0662096
I1123 21:54:46.471593 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:54:46.471593 16332 solver.cpp:237]     Train net output #1: loss = 0.0662092 (* 1 = 0.0662092 loss)
I1123 21:54:46.472098 16332 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1123 21:54:57.924563 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:54:58.404633 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_21500.caffemodel
I1123 21:54:58.441121 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_21500.solverstate
I1123 21:54:58.460640 16332 solver.cpp:330] Iteration 21500, Testing net (#0)
I1123 21:54:58.460640 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:55:02.495712  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:55:02.660727 16332 solver.cpp:397]     Test net output #0: accuracy = 0.89
I1123 21:55:02.660727 16332 solver.cpp:397]     Test net output #1: loss = 0.349018 (* 1 = 0.349018 loss)
I1123 21:55:02.778736 16332 solver.cpp:218] Iteration 21500 (6.13265 iter/s, 16.3062s/100 iters), loss = 0.0861109
I1123 21:55:02.778736 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:55:02.778736 16332 solver.cpp:237]     Train net output #1: loss = 0.0861105 (* 1 = 0.0861105 loss)
I1123 21:55:02.778736 16332 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1123 21:55:14.828130 16332 solver.cpp:218] Iteration 21600 (8.29951 iter/s, 12.0489s/100 iters), loss = 0.15429
I1123 21:55:14.828130 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:55:14.828130 16332 solver.cpp:237]     Train net output #1: loss = 0.15429 (* 1 = 0.15429 loss)
I1123 21:55:14.828130 16332 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1123 21:55:26.876359 16332 solver.cpp:218] Iteration 21700 (8.30044 iter/s, 12.0476s/100 iters), loss = 0.114063
I1123 21:55:26.876359 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:55:26.876359 16332 solver.cpp:237]     Train net output #1: loss = 0.114063 (* 1 = 0.114063 loss)
I1123 21:55:26.876359 16332 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1123 21:55:38.924775 16332 solver.cpp:218] Iteration 21800 (8.30017 iter/s, 12.0479s/100 iters), loss = 0.0870253
I1123 21:55:38.924775 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:55:38.924775 16332 solver.cpp:237]     Train net output #1: loss = 0.0870249 (* 1 = 0.0870249 loss)
I1123 21:55:38.924775 16332 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1123 21:55:50.982422 16332 solver.cpp:218] Iteration 21900 (8.29409 iter/s, 12.0568s/100 iters), loss = 0.0646674
I1123 21:55:50.982422 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:55:50.982422 16332 solver.cpp:237]     Train net output #1: loss = 0.0646671 (* 1 = 0.0646671 loss)
I1123 21:55:50.982422 16332 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1123 21:56:02.438809 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:56:02.916297 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_22000.caffemodel
I1123 21:56:02.955795 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_22000.solverstate
I1123 21:56:02.976294 16332 solver.cpp:330] Iteration 22000, Testing net (#0)
I1123 21:56:02.976294 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:56:07.010903  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:56:07.176439 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8902
I1123 21:56:07.176439 16332 solver.cpp:397]     Test net output #1: loss = 0.349024 (* 1 = 0.349024 loss)
I1123 21:56:07.293484 16332 solver.cpp:218] Iteration 22000 (6.13077 iter/s, 16.3112s/100 iters), loss = 0.0912755
I1123 21:56:07.294471 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:56:07.294471 16332 solver.cpp:237]     Train net output #1: loss = 0.0912751 (* 1 = 0.0912751 loss)
I1123 21:56:07.294471 16332 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1123 21:56:07.294471 16332 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1123 21:56:19.351403 16332 solver.cpp:218] Iteration 22100 (8.29398 iter/s, 12.0569s/100 iters), loss = 0.180207
I1123 21:56:19.351403 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:56:19.351403 16332 solver.cpp:237]     Train net output #1: loss = 0.180207 (* 1 = 0.180207 loss)
I1123 21:56:19.351403 16332 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1123 21:56:31.410888 16332 solver.cpp:218] Iteration 22200 (8.29276 iter/s, 12.0587s/100 iters), loss = 0.087476
I1123 21:56:31.410888 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:56:31.411386 16332 solver.cpp:237]     Train net output #1: loss = 0.0874756 (* 1 = 0.0874756 loss)
I1123 21:56:31.411386 16332 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1123 21:56:43.464892 16332 solver.cpp:218] Iteration 22300 (8.29671 iter/s, 12.053s/100 iters), loss = 0.0838994
I1123 21:56:43.464892 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:56:43.464892 16332 solver.cpp:237]     Train net output #1: loss = 0.083899 (* 1 = 0.083899 loss)
I1123 21:56:43.464892 16332 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1123 21:56:55.511909 16332 solver.cpp:218] Iteration 22400 (8.30085 iter/s, 12.047s/100 iters), loss = 0.0534879
I1123 21:56:55.511909 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:56:55.511909 16332 solver.cpp:237]     Train net output #1: loss = 0.0534875 (* 1 = 0.0534875 loss)
I1123 21:56:55.511909 16332 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1123 21:57:06.961405 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:57:07.441478 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_22500.caffemodel
I1123 21:57:07.478996 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_22500.solverstate
I1123 21:57:07.498497 16332 solver.cpp:330] Iteration 22500, Testing net (#0)
I1123 21:57:07.498497 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:57:11.537304  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:57:11.702813 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8901
I1123 21:57:11.702813 16332 solver.cpp:397]     Test net output #1: loss = 0.349085 (* 1 = 0.349085 loss)
I1123 21:57:11.820310 16332 solver.cpp:218] Iteration 22500 (6.13193 iter/s, 16.3081s/100 iters), loss = 0.121289
I1123 21:57:11.820310 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:57:11.821298 16332 solver.cpp:237]     Train net output #1: loss = 0.121288 (* 1 = 0.121288 loss)
I1123 21:57:11.821298 16332 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1123 21:57:23.877025 16332 solver.cpp:218] Iteration 22600 (8.29497 iter/s, 12.0555s/100 iters), loss = 0.0745184
I1123 21:57:23.877025 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:57:23.877025 16332 solver.cpp:237]     Train net output #1: loss = 0.074518 (* 1 = 0.074518 loss)
I1123 21:57:23.877025 16332 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1123 21:57:35.934231 16332 solver.cpp:218] Iteration 22700 (8.29438 iter/s, 12.0564s/100 iters), loss = 0.137745
I1123 21:57:35.934231 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:57:35.934231 16332 solver.cpp:237]     Train net output #1: loss = 0.137745 (* 1 = 0.137745 loss)
I1123 21:57:35.934231 16332 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1123 21:57:47.992110 16332 solver.cpp:218] Iteration 22800 (8.29353 iter/s, 12.0576s/100 iters), loss = 0.108995
I1123 21:57:47.992609 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:57:47.992609 16332 solver.cpp:237]     Train net output #1: loss = 0.108994 (* 1 = 0.108994 loss)
I1123 21:57:47.992609 16332 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1123 21:58:00.044776 16332 solver.cpp:218] Iteration 22900 (8.29754 iter/s, 12.0518s/100 iters), loss = 0.0780669
I1123 21:58:00.044776 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:58:00.044776 16332 solver.cpp:237]     Train net output #1: loss = 0.0780664 (* 1 = 0.0780664 loss)
I1123 21:58:00.044776 16332 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1123 21:58:11.499182 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:58:11.979168 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_23000.caffemodel
I1123 21:58:12.018183 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_23000.solverstate
I1123 21:58:12.037667 16332 solver.cpp:330] Iteration 23000, Testing net (#0)
I1123 21:58:12.037667 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:58:16.072114  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:58:16.238174 16332 solver.cpp:397]     Test net output #0: accuracy = 0.89
I1123 21:58:16.238174 16332 solver.cpp:397]     Test net output #1: loss = 0.349129 (* 1 = 0.349129 loss)
I1123 21:58:16.357220 16332 solver.cpp:218] Iteration 23000 (6.1306 iter/s, 16.3116s/100 iters), loss = 0.103571
I1123 21:58:16.357220 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:58:16.357220 16332 solver.cpp:237]     Train net output #1: loss = 0.103571 (* 1 = 0.103571 loss)
I1123 21:58:16.357220 16332 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1123 21:58:28.413405 16332 solver.cpp:218] Iteration 23100 (8.29474 iter/s, 12.0558s/100 iters), loss = 0.151653
I1123 21:58:28.413405 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:58:28.413405 16332 solver.cpp:237]     Train net output #1: loss = 0.151653 (* 1 = 0.151653 loss)
I1123 21:58:28.413405 16332 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1123 21:58:40.465991 16332 solver.cpp:218] Iteration 23200 (8.2976 iter/s, 12.0517s/100 iters), loss = 0.12282
I1123 21:58:40.465991 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 21:58:40.465991 16332 solver.cpp:237]     Train net output #1: loss = 0.12282 (* 1 = 0.12282 loss)
I1123 21:58:40.465991 16332 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1123 21:58:52.522290 16332 solver.cpp:218] Iteration 23300 (8.29482 iter/s, 12.0557s/100 iters), loss = 0.084979
I1123 21:58:52.522290 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 21:58:52.522290 16332 solver.cpp:237]     Train net output #1: loss = 0.0849786 (* 1 = 0.0849786 loss)
I1123 21:58:52.522290 16332 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1123 21:59:04.581293 16332 solver.cpp:218] Iteration 23400 (8.29295 iter/s, 12.0584s/100 iters), loss = 0.0995864
I1123 21:59:04.581293 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:59:04.581293 16332 solver.cpp:237]     Train net output #1: loss = 0.099586 (* 1 = 0.099586 loss)
I1123 21:59:04.581293 16332 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1123 21:59:16.030308 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:59:16.508309 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_23500.caffemodel
I1123 21:59:16.547809 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_23500.solverstate
I1123 21:59:16.566823 16332 solver.cpp:330] Iteration 23500, Testing net (#0)
I1123 21:59:16.566823 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 21:59:20.610705  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 21:59:20.776726 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8899
I1123 21:59:20.776726 16332 solver.cpp:397]     Test net output #1: loss = 0.349106 (* 1 = 0.349106 loss)
I1123 21:59:20.894778 16332 solver.cpp:218] Iteration 23500 (6.13017 iter/s, 16.3128s/100 iters), loss = 0.122316
I1123 21:59:20.894778 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 21:59:20.894778 16332 solver.cpp:237]     Train net output #1: loss = 0.122316 (* 1 = 0.122316 loss)
I1123 21:59:20.894778 16332 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1123 21:59:32.951221 16332 solver.cpp:218] Iteration 23600 (8.29455 iter/s, 12.0561s/100 iters), loss = 0.160328
I1123 21:59:32.951707 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 21:59:32.951707 16332 solver.cpp:237]     Train net output #1: loss = 0.160327 (* 1 = 0.160327 loss)
I1123 21:59:32.951707 16332 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1123 21:59:45.003103 16332 solver.cpp:218] Iteration 23700 (8.29792 iter/s, 12.0512s/100 iters), loss = 0.131748
I1123 21:59:45.003604 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 21:59:45.003604 16332 solver.cpp:237]     Train net output #1: loss = 0.131747 (* 1 = 0.131747 loss)
I1123 21:59:45.003604 16332 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1123 21:59:57.053000 16332 solver.cpp:218] Iteration 23800 (8.29939 iter/s, 12.0491s/100 iters), loss = 0.0816739
I1123 21:59:57.053000 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 21:59:57.053000 16332 solver.cpp:237]     Train net output #1: loss = 0.0816735 (* 1 = 0.0816735 loss)
I1123 21:59:57.053000 16332 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1123 22:00:09.126214 16332 solver.cpp:218] Iteration 23900 (8.28311 iter/s, 12.0728s/100 iters), loss = 0.093181
I1123 22:00:09.126214 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:00:09.126713 16332 solver.cpp:237]     Train net output #1: loss = 0.0931806 (* 1 = 0.0931806 loss)
I1123 22:00:09.126713 16332 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1123 22:00:20.577260 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:00:21.057263 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_24000.caffemodel
I1123 22:00:21.097270 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_24000.solverstate
I1123 22:00:21.116259 16332 solver.cpp:330] Iteration 24000, Testing net (#0)
I1123 22:00:21.116760 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:00:25.155789  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:00:25.321849 16332 solver.cpp:397]     Test net output #0: accuracy = 0.89
I1123 22:00:25.321849 16332 solver.cpp:397]     Test net output #1: loss = 0.349154 (* 1 = 0.349154 loss)
I1123 22:00:25.439894 16332 solver.cpp:218] Iteration 24000 (6.1301 iter/s, 16.313s/100 iters), loss = 0.107297
I1123 22:00:25.439894 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 22:00:25.439894 16332 solver.cpp:237]     Train net output #1: loss = 0.107297 (* 1 = 0.107297 loss)
I1123 22:00:25.439894 16332 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1123 22:00:37.491602 16332 solver.cpp:218] Iteration 24100 (8.29803 iter/s, 12.0511s/100 iters), loss = 0.111046
I1123 22:00:37.491602 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 22:00:37.491602 16332 solver.cpp:237]     Train net output #1: loss = 0.111045 (* 1 = 0.111045 loss)
I1123 22:00:37.491602 16332 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1123 22:00:49.549239 16332 solver.cpp:218] Iteration 24200 (8.29411 iter/s, 12.0568s/100 iters), loss = 0.113164
I1123 22:00:49.549239 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:00:49.549239 16332 solver.cpp:237]     Train net output #1: loss = 0.113164 (* 1 = 0.113164 loss)
I1123 22:00:49.549239 16332 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1123 22:01:01.612133 16332 solver.cpp:218] Iteration 24300 (8.29016 iter/s, 12.0625s/100 iters), loss = 0.117278
I1123 22:01:01.612133 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:01:01.612133 16332 solver.cpp:237]     Train net output #1: loss = 0.117277 (* 1 = 0.117277 loss)
I1123 22:01:01.612133 16332 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1123 22:01:13.659095 16332 solver.cpp:218] Iteration 24400 (8.30123 iter/s, 12.0464s/100 iters), loss = 0.0949759
I1123 22:01:13.659095 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:01:13.659095 16332 solver.cpp:237]     Train net output #1: loss = 0.0949755 (* 1 = 0.0949755 loss)
I1123 22:01:13.659095 16332 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1123 22:01:25.111526 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:01:25.589119 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_24500.caffemodel
I1123 22:01:25.628103 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_24500.solverstate
I1123 22:01:25.647603 16332 solver.cpp:330] Iteration 24500, Testing net (#0)
I1123 22:01:25.647603 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:01:29.685231  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:01:29.850270 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8902
I1123 22:01:29.850270 16332 solver.cpp:397]     Test net output #1: loss = 0.349174 (* 1 = 0.349174 loss)
I1123 22:01:29.969305 16332 solver.cpp:218] Iteration 24500 (6.13133 iter/s, 16.3097s/100 iters), loss = 0.0878692
I1123 22:01:29.969305 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:01:29.969305 16332 solver.cpp:237]     Train net output #1: loss = 0.0878688 (* 1 = 0.0878688 loss)
I1123 22:01:29.969305 16332 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1123 22:01:42.029072 16332 solver.cpp:218] Iteration 24600 (8.29257 iter/s, 12.059s/100 iters), loss = 0.117236
I1123 22:01:42.029570 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 22:01:42.029570 16332 solver.cpp:237]     Train net output #1: loss = 0.117235 (* 1 = 0.117235 loss)
I1123 22:01:42.029570 16332 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1123 22:01:54.087177 16332 solver.cpp:218] Iteration 24700 (8.29371 iter/s, 12.0573s/100 iters), loss = 0.127077
I1123 22:01:54.087177 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:01:54.087177 16332 solver.cpp:237]     Train net output #1: loss = 0.127076 (* 1 = 0.127076 loss)
I1123 22:01:54.087177 16332 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1123 22:02:06.135689 16332 solver.cpp:218] Iteration 24800 (8.3004 iter/s, 12.0476s/100 iters), loss = 0.0922876
I1123 22:02:06.135689 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:02:06.135689 16332 solver.cpp:237]     Train net output #1: loss = 0.0922872 (* 1 = 0.0922872 loss)
I1123 22:02:06.135689 16332 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1123 22:02:18.186326 16332 solver.cpp:218] Iteration 24900 (8.29855 iter/s, 12.0503s/100 iters), loss = 0.0830138
I1123 22:02:18.186326 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 22:02:18.186326 16332 solver.cpp:237]     Train net output #1: loss = 0.0830134 (* 1 = 0.0830134 loss)
I1123 22:02:18.186326 16332 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1123 22:02:29.639523 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:02:30.118288 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_25000.caffemodel
I1123 22:02:30.158293 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_25000.solverstate
I1123 22:02:30.180786 16332 solver.cpp:330] Iteration 25000, Testing net (#0)
I1123 22:02:30.180786 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:02:34.216492  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:02:34.382513 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8904
I1123 22:02:34.382513 16332 solver.cpp:397]     Test net output #1: loss = 0.349235 (* 1 = 0.349235 loss)
I1123 22:02:34.500150 16332 solver.cpp:218] Iteration 25000 (6.13 iter/s, 16.3132s/100 iters), loss = 0.10544
I1123 22:02:34.500150 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 22:02:34.500150 16332 solver.cpp:237]     Train net output #1: loss = 0.10544 (* 1 = 0.10544 loss)
I1123 22:02:34.500150 16332 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1123 22:02:46.556902 16332 solver.cpp:218] Iteration 25100 (8.29464 iter/s, 12.056s/100 iters), loss = 0.152476
I1123 22:02:46.556902 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:02:46.556902 16332 solver.cpp:237]     Train net output #1: loss = 0.152476 (* 1 = 0.152476 loss)
I1123 22:02:46.556902 16332 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1123 22:02:58.602553 16332 solver.cpp:218] Iteration 25200 (8.30232 iter/s, 12.0448s/100 iters), loss = 0.0755611
I1123 22:02:58.602553 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 22:02:58.602553 16332 solver.cpp:237]     Train net output #1: loss = 0.0755607 (* 1 = 0.0755607 loss)
I1123 22:02:58.602553 16332 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1123 22:03:10.650410 16332 solver.cpp:218] Iteration 25300 (8.30071 iter/s, 12.0472s/100 iters), loss = 0.0864468
I1123 22:03:10.650410 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:03:10.650410 16332 solver.cpp:237]     Train net output #1: loss = 0.0864464 (* 1 = 0.0864464 loss)
I1123 22:03:10.650410 16332 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1123 22:03:22.705588 16332 solver.cpp:218] Iteration 25400 (8.29562 iter/s, 12.0546s/100 iters), loss = 0.108306
I1123 22:03:22.705588 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 22:03:22.705588 16332 solver.cpp:237]     Train net output #1: loss = 0.108305 (* 1 = 0.108305 loss)
I1123 22:03:22.705588 16332 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1123 22:03:34.150993 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:03:34.629990 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_25500.caffemodel
I1123 22:03:34.667990 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_25500.solverstate
I1123 22:03:34.688490 16332 solver.cpp:330] Iteration 25500, Testing net (#0)
I1123 22:03:34.688490 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:03:38.722880  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:03:38.887898 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8901
I1123 22:03:38.888881 16332 solver.cpp:397]     Test net output #1: loss = 0.349182 (* 1 = 0.349182 loss)
I1123 22:03:39.005887 16332 solver.cpp:218] Iteration 25500 (6.13483 iter/s, 16.3004s/100 iters), loss = 0.0808697
I1123 22:03:39.005887 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:03:39.005887 16332 solver.cpp:237]     Train net output #1: loss = 0.0808693 (* 1 = 0.0808693 loss)
I1123 22:03:39.005887 16332 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1123 22:03:51.072634 16332 solver.cpp:218] Iteration 25600 (8.28806 iter/s, 12.0655s/100 iters), loss = 0.130561
I1123 22:03:51.072634 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:03:51.072634 16332 solver.cpp:237]     Train net output #1: loss = 0.130561 (* 1 = 0.130561 loss)
I1123 22:03:51.072634 16332 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1123 22:04:03.130273 16332 solver.cpp:218] Iteration 25700 (8.29402 iter/s, 12.0569s/100 iters), loss = 0.109731
I1123 22:04:03.130273 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:04:03.130273 16332 solver.cpp:237]     Train net output #1: loss = 0.109731 (* 1 = 0.109731 loss)
I1123 22:04:03.130273 16332 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1123 22:04:15.179661 16332 solver.cpp:218] Iteration 25800 (8.29936 iter/s, 12.0491s/100 iters), loss = 0.120833
I1123 22:04:15.180174 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 22:04:15.180174 16332 solver.cpp:237]     Train net output #1: loss = 0.120833 (* 1 = 0.120833 loss)
I1123 22:04:15.180174 16332 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1123 22:04:27.227264 16332 solver.cpp:218] Iteration 25900 (8.30103 iter/s, 12.0467s/100 iters), loss = 0.0706877
I1123 22:04:27.227264 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 22:04:27.227264 16332 solver.cpp:237]     Train net output #1: loss = 0.0706873 (* 1 = 0.0706873 loss)
I1123 22:04:27.227264 16332 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1123 22:04:38.678470 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:04:39.157459 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_26000.caffemodel
I1123 22:04:39.198988 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_26000.solverstate
I1123 22:04:39.219002 16332 solver.cpp:330] Iteration 26000, Testing net (#0)
I1123 22:04:39.219002 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:04:43.260905  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:04:43.427371 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8902
I1123 22:04:43.427371 16332 solver.cpp:397]     Test net output #1: loss = 0.348979 (* 1 = 0.348979 loss)
I1123 22:04:43.544387 16332 solver.cpp:218] Iteration 26000 (6.12861 iter/s, 16.3169s/100 iters), loss = 0.0751501
I1123 22:04:43.544387 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 22:04:43.544387 16332 solver.cpp:237]     Train net output #1: loss = 0.0751497 (* 1 = 0.0751497 loss)
I1123 22:04:43.544387 16332 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1123 22:04:55.597857 16332 solver.cpp:218] Iteration 26100 (8.2971 iter/s, 12.0524s/100 iters), loss = 0.153999
I1123 22:04:55.597857 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 22:04:55.597857 16332 solver.cpp:237]     Train net output #1: loss = 0.153999 (* 1 = 0.153999 loss)
I1123 22:04:55.597857 16332 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1123 22:05:07.645838 16332 solver.cpp:218] Iteration 26200 (8.30072 iter/s, 12.0471s/100 iters), loss = 0.0731389
I1123 22:05:07.645838 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 22:05:07.645838 16332 solver.cpp:237]     Train net output #1: loss = 0.0731385 (* 1 = 0.0731385 loss)
I1123 22:05:07.645838 16332 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1123 22:05:19.701853 16332 solver.cpp:218] Iteration 26300 (8.29484 iter/s, 12.0557s/100 iters), loss = 0.122431
I1123 22:05:19.701853 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 22:05:19.702337 16332 solver.cpp:237]     Train net output #1: loss = 0.122431 (* 1 = 0.122431 loss)
I1123 22:05:19.702337 16332 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1123 22:05:31.748569 16332 solver.cpp:218] Iteration 26400 (8.30144 iter/s, 12.0461s/100 iters), loss = 0.0701419
I1123 22:05:31.749071 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:05:31.749071 16332 solver.cpp:237]     Train net output #1: loss = 0.0701415 (* 1 = 0.0701415 loss)
I1123 22:05:31.749071 16332 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1123 22:05:43.200206 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:05:43.677743 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_26500.caffemodel
I1123 22:05:43.717243 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_26500.solverstate
I1123 22:05:43.737243 16332 solver.cpp:330] Iteration 26500, Testing net (#0)
I1123 22:05:43.737243 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:05:47.777891  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:05:47.943748 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8901
I1123 22:05:47.943748 16332 solver.cpp:397]     Test net output #1: loss = 0.349174 (* 1 = 0.349174 loss)
I1123 22:05:48.061297 16332 solver.cpp:218] Iteration 26500 (6.1303 iter/s, 16.3124s/100 iters), loss = 0.0835966
I1123 22:05:48.061297 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:05:48.061297 16332 solver.cpp:237]     Train net output #1: loss = 0.0835962 (* 1 = 0.0835962 loss)
I1123 22:05:48.061297 16332 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1123 22:06:00.116046 16332 solver.cpp:218] Iteration 26600 (8.29616 iter/s, 12.0538s/100 iters), loss = 0.117429
I1123 22:06:00.116533 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 22:06:00.116533 16332 solver.cpp:237]     Train net output #1: loss = 0.117429 (* 1 = 0.117429 loss)
I1123 22:06:00.116533 16332 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1123 22:06:12.168723 16332 solver.cpp:218] Iteration 26700 (8.2975 iter/s, 12.0518s/100 iters), loss = 0.0691576
I1123 22:06:12.168723 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:06:12.168723 16332 solver.cpp:237]     Train net output #1: loss = 0.0691572 (* 1 = 0.0691572 loss)
I1123 22:06:12.168723 16332 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1123 22:06:24.220415 16332 solver.cpp:218] Iteration 26800 (8.29813 iter/s, 12.0509s/100 iters), loss = 0.122165
I1123 22:06:24.220415 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 22:06:24.220415 16332 solver.cpp:237]     Train net output #1: loss = 0.122165 (* 1 = 0.122165 loss)
I1123 22:06:24.220415 16332 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1123 22:06:36.275061 16332 solver.cpp:218] Iteration 26900 (8.2961 iter/s, 12.0539s/100 iters), loss = 0.0609748
I1123 22:06:36.275061 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 22:06:36.275061 16332 solver.cpp:237]     Train net output #1: loss = 0.0609744 (* 1 = 0.0609744 loss)
I1123 22:06:36.275061 16332 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1123 22:06:47.722196 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:06:48.201274 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_27000.caffemodel
I1123 22:06:48.241274 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_27000.solverstate
I1123 22:06:48.261775 16332 solver.cpp:330] Iteration 27000, Testing net (#0)
I1123 22:06:48.261775 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:06:52.300204  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:06:52.465767 16332 solver.cpp:397]     Test net output #0: accuracy = 0.89
I1123 22:06:52.465767 16332 solver.cpp:397]     Test net output #1: loss = 0.348995 (* 1 = 0.348995 loss)
I1123 22:06:52.583809 16332 solver.cpp:218] Iteration 27000 (6.13176 iter/s, 16.3085s/100 iters), loss = 0.0845723
I1123 22:06:52.583809 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 22:06:52.583809 16332 solver.cpp:237]     Train net output #1: loss = 0.0845719 (* 1 = 0.0845719 loss)
I1123 22:06:52.583809 16332 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1123 22:06:52.583809 16332 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1123 22:07:04.645809 16332 solver.cpp:218] Iteration 27100 (8.29111 iter/s, 12.0611s/100 iters), loss = 0.158668
I1123 22:07:04.645809 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 22:07:04.645809 16332 solver.cpp:237]     Train net output #1: loss = 0.158667 (* 1 = 0.158667 loss)
I1123 22:07:04.645809 16332 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1123 22:07:16.698873 16332 solver.cpp:218] Iteration 27200 (8.29707 iter/s, 12.0524s/100 iters), loss = 0.110764
I1123 22:07:16.698873 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:07:16.698873 16332 solver.cpp:237]     Train net output #1: loss = 0.110764 (* 1 = 0.110764 loss)
I1123 22:07:16.698873 16332 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1123 22:07:28.747333 16332 solver.cpp:218] Iteration 27300 (8.30044 iter/s, 12.0476s/100 iters), loss = 0.0761639
I1123 22:07:28.747333 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:07:28.747333 16332 solver.cpp:237]     Train net output #1: loss = 0.0761635 (* 1 = 0.0761635 loss)
I1123 22:07:28.747333 16332 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1123 22:07:40.802603 16332 solver.cpp:218] Iteration 27400 (8.2955 iter/s, 12.0547s/100 iters), loss = 0.0453396
I1123 22:07:40.802603 16332 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 22:07:40.802603 16332 solver.cpp:237]     Train net output #1: loss = 0.0453392 (* 1 = 0.0453392 loss)
I1123 22:07:40.802603 16332 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1123 22:07:52.254719 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:07:52.734258 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_27500.caffemodel
I1123 22:07:52.773757 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_27500.solverstate
I1123 22:07:52.794242 16332 solver.cpp:330] Iteration 27500, Testing net (#0)
I1123 22:07:52.794242 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:07:56.828745  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:07:56.994824 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8898
I1123 22:07:56.994824 16332 solver.cpp:397]     Test net output #1: loss = 0.349072 (* 1 = 0.349072 loss)
I1123 22:07:57.112393 16332 solver.cpp:218] Iteration 27500 (6.13145 iter/s, 16.3093s/100 iters), loss = 0.075298
I1123 22:07:57.112393 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:07:57.112393 16332 solver.cpp:237]     Train net output #1: loss = 0.0752976 (* 1 = 0.0752976 loss)
I1123 22:07:57.112393 16332 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1123 22:08:09.167160 16332 solver.cpp:218] Iteration 27600 (8.29594 iter/s, 12.0541s/100 iters), loss = 0.184289
I1123 22:08:09.167160 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 22:08:09.167160 16332 solver.cpp:237]     Train net output #1: loss = 0.184289 (* 1 = 0.184289 loss)
I1123 22:08:09.167160 16332 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1123 22:08:21.229248 16332 solver.cpp:218] Iteration 27700 (8.291 iter/s, 12.0613s/100 iters), loss = 0.14283
I1123 22:08:21.229248 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 22:08:21.229248 16332 solver.cpp:237]     Train net output #1: loss = 0.142829 (* 1 = 0.142829 loss)
I1123 22:08:21.229248 16332 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1123 22:08:33.294605 16332 solver.cpp:218] Iteration 27800 (8.28863 iter/s, 12.0647s/100 iters), loss = 0.0807898
I1123 22:08:33.294605 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:08:33.294605 16332 solver.cpp:237]     Train net output #1: loss = 0.0807894 (* 1 = 0.0807894 loss)
I1123 22:08:33.294605 16332 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1123 22:08:45.343158 16332 solver.cpp:218] Iteration 27900 (8.30021 iter/s, 12.0479s/100 iters), loss = 0.0592824
I1123 22:08:45.343158 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 22:08:45.343158 16332 solver.cpp:237]     Train net output #1: loss = 0.059282 (* 1 = 0.059282 loss)
I1123 22:08:45.343158 16332 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1123 22:08:56.796777 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:08:57.275279 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_28000.caffemodel
I1123 22:08:57.315277 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_28000.solverstate
I1123 22:08:57.336277 16332 solver.cpp:330] Iteration 28000, Testing net (#0)
I1123 22:08:57.336277 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:09:01.374652  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:09:01.540215 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8903
I1123 22:09:01.540215 16332 solver.cpp:397]     Test net output #1: loss = 0.349138 (* 1 = 0.349138 loss)
I1123 22:09:01.658763 16332 solver.cpp:218] Iteration 28000 (6.12926 iter/s, 16.3152s/100 iters), loss = 0.0841189
I1123 22:09:01.658763 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:09:01.658763 16332 solver.cpp:237]     Train net output #1: loss = 0.0841186 (* 1 = 0.0841186 loss)
I1123 22:09:01.658763 16332 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1123 22:09:13.718374 16332 solver.cpp:218] Iteration 28100 (8.29256 iter/s, 12.059s/100 iters), loss = 0.118201
I1123 22:09:13.718374 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 22:09:13.718374 16332 solver.cpp:237]     Train net output #1: loss = 0.118201 (* 1 = 0.118201 loss)
I1123 22:09:13.718374 16332 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1123 22:09:25.764784 16332 solver.cpp:218] Iteration 28200 (8.3017 iter/s, 12.0457s/100 iters), loss = 0.119035
I1123 22:09:25.764784 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:09:25.764784 16332 solver.cpp:237]     Train net output #1: loss = 0.119034 (* 1 = 0.119034 loss)
I1123 22:09:25.764784 16332 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1123 22:09:37.870110 16332 solver.cpp:218] Iteration 28300 (8.26129 iter/s, 12.1046s/100 iters), loss = 0.0978825
I1123 22:09:37.870110 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:09:37.870110 16332 solver.cpp:237]     Train net output #1: loss = 0.0978822 (* 1 = 0.0978822 loss)
I1123 22:09:37.870110 16332 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1123 22:09:49.932824 16332 solver.cpp:218] Iteration 28400 (8.2904 iter/s, 12.0621s/100 iters), loss = 0.130874
I1123 22:09:49.932824 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 22:09:49.932824 16332 solver.cpp:237]     Train net output #1: loss = 0.130874 (* 1 = 0.130874 loss)
I1123 22:09:49.932824 16332 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1123 22:10:01.389902 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:10:01.869572 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_28500.caffemodel
I1123 22:10:01.909090 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_28500.solverstate
I1123 22:10:01.930094 16332 solver.cpp:330] Iteration 28500, Testing net (#0)
I1123 22:10:01.930094 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:10:05.968375  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:10:06.133950 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8904
I1123 22:10:06.133950 16332 solver.cpp:397]     Test net output #1: loss = 0.349135 (* 1 = 0.349135 loss)
I1123 22:10:06.251968 16332 solver.cpp:218] Iteration 28500 (6.12814 iter/s, 16.3182s/100 iters), loss = 0.10519
I1123 22:10:06.251968 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:10:06.251968 16332 solver.cpp:237]     Train net output #1: loss = 0.10519 (* 1 = 0.10519 loss)
I1123 22:10:06.251968 16332 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1123 22:10:18.306215 16332 solver.cpp:218] Iteration 28600 (8.29604 iter/s, 12.0539s/100 iters), loss = 0.140924
I1123 22:10:18.306215 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:10:18.306215 16332 solver.cpp:237]     Train net output #1: loss = 0.140923 (* 1 = 0.140923 loss)
I1123 22:10:18.306713 16332 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1123 22:10:30.356739 16332 solver.cpp:218] Iteration 28700 (8.29903 iter/s, 12.0496s/100 iters), loss = 0.120832
I1123 22:10:30.356739 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 22:10:30.356739 16332 solver.cpp:237]     Train net output #1: loss = 0.120832 (* 1 = 0.120832 loss)
I1123 22:10:30.356739 16332 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1123 22:10:42.413971 16332 solver.cpp:218] Iteration 28800 (8.29395 iter/s, 12.057s/100 iters), loss = 0.101929
I1123 22:10:42.414472 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:10:42.414472 16332 solver.cpp:237]     Train net output #1: loss = 0.101928 (* 1 = 0.101928 loss)
I1123 22:10:42.414472 16332 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1123 22:10:54.461637 16332 solver.cpp:218] Iteration 28900 (8.30092 iter/s, 12.0469s/100 iters), loss = 0.0735487
I1123 22:10:54.461637 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 22:10:54.461637 16332 solver.cpp:237]     Train net output #1: loss = 0.0735484 (* 1 = 0.0735484 loss)
I1123 22:10:54.461637 16332 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1123 22:11:05.920543 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:11:06.399067 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_29000.caffemodel
I1123 22:11:06.438552 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_29000.solverstate
I1123 22:11:06.459051 16332 solver.cpp:330] Iteration 29000, Testing net (#0)
I1123 22:11:06.459051 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:11:10.497252  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:11:10.662307 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8902
I1123 22:11:10.662307 16332 solver.cpp:397]     Test net output #1: loss = 0.349172 (* 1 = 0.349172 loss)
I1123 22:11:10.780342 16332 solver.cpp:218] Iteration 29000 (6.12833 iter/s, 16.3177s/100 iters), loss = 0.0853984
I1123 22:11:10.780342 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 22:11:10.780342 16332 solver.cpp:237]     Train net output #1: loss = 0.0853981 (* 1 = 0.0853981 loss)
I1123 22:11:10.780342 16332 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1123 22:11:22.838964 16332 solver.cpp:218] Iteration 29100 (8.29325 iter/s, 12.058s/100 iters), loss = 0.169558
I1123 22:11:22.838964 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 22:11:22.838964 16332 solver.cpp:237]     Train net output #1: loss = 0.169558 (* 1 = 0.169558 loss)
I1123 22:11:22.838964 16332 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1123 22:11:34.890967 16332 solver.cpp:218] Iteration 29200 (8.29759 iter/s, 12.0517s/100 iters), loss = 0.065518
I1123 22:11:34.890967 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:11:34.890967 16332 solver.cpp:237]     Train net output #1: loss = 0.0655177 (* 1 = 0.0655177 loss)
I1123 22:11:34.890967 16332 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1123 22:11:46.945925 16332 solver.cpp:218] Iteration 29300 (8.29568 iter/s, 12.0545s/100 iters), loss = 0.0907124
I1123 22:11:46.946426 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 22:11:46.946426 16332 solver.cpp:237]     Train net output #1: loss = 0.090712 (* 1 = 0.090712 loss)
I1123 22:11:46.946426 16332 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1123 22:11:58.997792 16332 solver.cpp:218] Iteration 29400 (8.29807 iter/s, 12.051s/100 iters), loss = 0.0743722
I1123 22:11:58.997792 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 22:11:58.997792 16332 solver.cpp:237]     Train net output #1: loss = 0.0743719 (* 1 = 0.0743719 loss)
I1123 22:11:58.997792 16332 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1123 22:12:10.455976 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:12:10.932991 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_29500.caffemodel
I1123 22:12:10.973994 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_29500.solverstate
I1123 22:12:10.993993 16332 solver.cpp:330] Iteration 29500, Testing net (#0)
I1123 22:12:10.993993 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:12:15.033367  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:12:15.198423 16332 solver.cpp:397]     Test net output #0: accuracy = 0.89
I1123 22:12:15.198423 16332 solver.cpp:397]     Test net output #1: loss = 0.349 (* 1 = 0.349 loss)
I1123 22:12:15.316478 16332 solver.cpp:218] Iteration 29500 (6.12824 iter/s, 16.3179s/100 iters), loss = 0.0924931
I1123 22:12:15.316478 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:12:15.316478 16332 solver.cpp:237]     Train net output #1: loss = 0.0924927 (* 1 = 0.0924927 loss)
I1123 22:12:15.316478 16332 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1123 22:12:27.379760 16332 solver.cpp:218] Iteration 29600 (8.28995 iter/s, 12.0628s/100 iters), loss = 0.0926439
I1123 22:12:27.379760 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:12:27.379760 16332 solver.cpp:237]     Train net output #1: loss = 0.0926435 (* 1 = 0.0926435 loss)
I1123 22:12:27.379760 16332 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1123 22:12:39.437794 16332 solver.cpp:218] Iteration 29700 (8.29351 iter/s, 12.0576s/100 iters), loss = 0.0717874
I1123 22:12:39.437794 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 22:12:39.437794 16332 solver.cpp:237]     Train net output #1: loss = 0.0717871 (* 1 = 0.0717871 loss)
I1123 22:12:39.437794 16332 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1123 22:12:51.494282 16332 solver.cpp:218] Iteration 29800 (8.29481 iter/s, 12.0557s/100 iters), loss = 0.0822186
I1123 22:12:51.494282 16332 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 22:12:51.494282 16332 solver.cpp:237]     Train net output #1: loss = 0.0822182 (* 1 = 0.0822182 loss)
I1123 22:12:51.494282 16332 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1123 22:13:03.548395 16332 solver.cpp:218] Iteration 29900 (8.2962 iter/s, 12.0537s/100 iters), loss = 0.0462542
I1123 22:13:03.548395 16332 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 22:13:03.548395 16332 solver.cpp:237]     Train net output #1: loss = 0.0462539 (* 1 = 0.0462539 loss)
I1123 22:13:03.548395 16332 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1123 22:13:15.005237 27888 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:13:15.482815 16332 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_30000.caffemodel
I1123 22:13:15.521313 16332 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6M_8L_7x7_iter_30000.solverstate
I1123 22:13:15.584314 16332 solver.cpp:310] Iteration 30000, loss = 0.119174
I1123 22:13:15.584314 16332 solver.cpp:330] Iteration 30000, Testing net (#0)
I1123 22:13:15.584314 16332 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:13:19.619432  3636 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:13:19.784509 16332 solver.cpp:397]     Test net output #0: accuracy = 0.8899
I1123 22:13:19.784509 16332 solver.cpp:397]     Test net output #1: loss = 0.349019 (* 1 = 0.349019 loss)
I1123 22:13:19.784509 16332 solver.cpp:315] Optimization Done.
I1123 22:13:19.784509 16332 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 

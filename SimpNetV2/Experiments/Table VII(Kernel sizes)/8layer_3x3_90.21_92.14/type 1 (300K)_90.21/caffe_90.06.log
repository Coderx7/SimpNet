
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1122 09:28:25.982635  9524 caffe.cpp:219] Using GPUs 0
I1122 09:28:26.157694  9524 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1122 09:28:26.455637  9524 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 09:28:26.473656  9524 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_300k_8L"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1122 09:28:26.474647  9524 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 09:28:26.475637  9524 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 09:28:26.476655  9524 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1122 09:28:26.476655  9524 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1122 09:28:26.476655  9524 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1122 09:28:26.476655  9524 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1122 09:28:26.476655  9524 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1122 09:28:26.476655  9524 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1122 09:28:26.476655  9524 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1122 09:28:26.476655  9524 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1122 09:28:26.476655  9524 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1122 09:28:26.476655  9524 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1122 09:28:26.476655  9524 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1122 09:28:26.476655  9524 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 41
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 43
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 85
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1122 09:28:26.687399  9524 layer_factory.cpp:58] Creating layer cifar
I1122 09:28:26.694452  9524 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1122 09:28:26.694452  9524 net.cpp:84] Creating Layer cifar
I1122 09:28:26.694452  9524 net.cpp:380] cifar -> data
I1122 09:28:26.694452  9524 net.cpp:380] cifar -> label
I1122 09:28:26.695456  9524 data_layer.cpp:45] output data size: 100,3,32,32
I1122 09:28:26.702447  9524 net.cpp:122] Setting up cifar
I1122 09:28:26.702447  9524 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1122 09:28:26.702447  9524 net.cpp:129] Top shape: 100 (100)
I1122 09:28:26.702447  9524 net.cpp:137] Memory required for data: 1229200
I1122 09:28:26.702447  9524 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1122 09:28:26.702447  9524 net.cpp:84] Creating Layer label_cifar_1_split
I1122 09:28:26.702447  9524 net.cpp:406] label_cifar_1_split <- label
I1122 09:28:26.702447  9524 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1122 09:28:26.702447  9524 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1122 09:28:26.702447  9524 net.cpp:122] Setting up label_cifar_1_split
I1122 09:28:26.702447  9524 net.cpp:129] Top shape: 100 (100)
I1122 09:28:26.702447  9524 net.cpp:129] Top shape: 100 (100)
I1122 09:28:26.702447  9524 net.cpp:137] Memory required for data: 1230000
I1122 09:28:26.702447  9524 layer_factory.cpp:58] Creating layer conv1
I1122 09:28:26.702447  9524 net.cpp:84] Creating Layer conv1
I1122 09:28:26.702447  9524 net.cpp:406] conv1 <- data
I1122 09:28:26.702447  9524 net.cpp:380] conv1 -> conv1
I1122 09:28:26.703438 18160 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 09:28:26.954713  9524 net.cpp:122] Setting up conv1
I1122 09:28:26.954713  9524 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:28:26.954713  9524 net.cpp:137] Memory required for data: 18023600
I1122 09:28:26.954713  9524 layer_factory.cpp:58] Creating layer bn1
I1122 09:28:26.954713  9524 net.cpp:84] Creating Layer bn1
I1122 09:28:26.954713  9524 net.cpp:406] bn1 <- conv1
I1122 09:28:26.954713  9524 net.cpp:367] bn1 -> conv1 (in-place)
I1122 09:28:26.954713  9524 net.cpp:122] Setting up bn1
I1122 09:28:26.954713  9524 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:28:26.954713  9524 net.cpp:137] Memory required for data: 34817200
I1122 09:28:26.954713  9524 layer_factory.cpp:58] Creating layer scale1
I1122 09:28:26.954713  9524 net.cpp:84] Creating Layer scale1
I1122 09:28:26.954713  9524 net.cpp:406] scale1 <- conv1
I1122 09:28:26.954713  9524 net.cpp:367] scale1 -> conv1 (in-place)
I1122 09:28:26.954713  9524 layer_factory.cpp:58] Creating layer scale1
I1122 09:28:26.954713  9524 net.cpp:122] Setting up scale1
I1122 09:28:26.954713  9524 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:28:26.954713  9524 net.cpp:137] Memory required for data: 51610800
I1122 09:28:26.954713  9524 layer_factory.cpp:58] Creating layer relu1
I1122 09:28:26.954713  9524 net.cpp:84] Creating Layer relu1
I1122 09:28:26.954713  9524 net.cpp:406] relu1 <- conv1
I1122 09:28:26.954713  9524 net.cpp:367] relu1 -> conv1 (in-place)
I1122 09:28:26.955705  9524 net.cpp:122] Setting up relu1
I1122 09:28:26.955705  9524 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:28:26.955705  9524 net.cpp:137] Memory required for data: 68404400
I1122 09:28:26.955705  9524 layer_factory.cpp:58] Creating layer conv2
I1122 09:28:26.955705  9524 net.cpp:84] Creating Layer conv2
I1122 09:28:26.955705  9524 net.cpp:406] conv2 <- conv1
I1122 09:28:26.955705  9524 net.cpp:380] conv2 -> conv2
I1122 09:28:26.956706  9524 net.cpp:122] Setting up conv2
I1122 09:28:26.956706  9524 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:28:26.956706  9524 net.cpp:137] Memory required for data: 86017200
I1122 09:28:26.956706  9524 layer_factory.cpp:58] Creating layer bn2
I1122 09:28:26.956706  9524 net.cpp:84] Creating Layer bn2
I1122 09:28:26.956706  9524 net.cpp:406] bn2 <- conv2
I1122 09:28:26.957705  9524 net.cpp:367] bn2 -> conv2 (in-place)
I1122 09:28:26.957705  9524 net.cpp:122] Setting up bn2
I1122 09:28:26.957705  9524 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:28:26.957705  9524 net.cpp:137] Memory required for data: 103630000
I1122 09:28:26.957705  9524 layer_factory.cpp:58] Creating layer scale2
I1122 09:28:26.957705  9524 net.cpp:84] Creating Layer scale2
I1122 09:28:26.957705  9524 net.cpp:406] scale2 <- conv2
I1122 09:28:26.957705  9524 net.cpp:367] scale2 -> conv2 (in-place)
I1122 09:28:26.957705  9524 layer_factory.cpp:58] Creating layer scale2
I1122 09:28:26.957705  9524 net.cpp:122] Setting up scale2
I1122 09:28:26.957705  9524 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:28:26.957705  9524 net.cpp:137] Memory required for data: 121242800
I1122 09:28:26.957705  9524 layer_factory.cpp:58] Creating layer relu2
I1122 09:28:26.957705  9524 net.cpp:84] Creating Layer relu2
I1122 09:28:26.957705  9524 net.cpp:406] relu2 <- conv2
I1122 09:28:26.957705  9524 net.cpp:367] relu2 -> conv2 (in-place)
I1122 09:28:26.957705  9524 net.cpp:122] Setting up relu2
I1122 09:28:26.957705  9524 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:28:26.957705  9524 net.cpp:137] Memory required for data: 138855600
I1122 09:28:26.957705  9524 layer_factory.cpp:58] Creating layer conv2_2
I1122 09:28:26.957705  9524 net.cpp:84] Creating Layer conv2_2
I1122 09:28:26.957705  9524 net.cpp:406] conv2_2 <- conv2
I1122 09:28:26.957705  9524 net.cpp:380] conv2_2 -> conv2_2
I1122 09:28:26.959705  9524 net.cpp:122] Setting up conv2_2
I1122 09:28:26.959705  9524 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:28:26.959705  9524 net.cpp:137] Memory required for data: 167527600
I1122 09:28:26.959705  9524 layer_factory.cpp:58] Creating layer bn2_2
I1122 09:28:26.959705  9524 net.cpp:84] Creating Layer bn2_2
I1122 09:28:26.959705  9524 net.cpp:406] bn2_2 <- conv2_2
I1122 09:28:26.959705  9524 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1122 09:28:26.959705  9524 net.cpp:122] Setting up bn2_2
I1122 09:28:26.959705  9524 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:28:26.959705  9524 net.cpp:137] Memory required for data: 196199600
I1122 09:28:26.959705  9524 layer_factory.cpp:58] Creating layer scale2_2
I1122 09:28:26.959705  9524 net.cpp:84] Creating Layer scale2_2
I1122 09:28:26.959705  9524 net.cpp:406] scale2_2 <- conv2_2
I1122 09:28:26.959705  9524 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1122 09:28:26.960705  9524 layer_factory.cpp:58] Creating layer scale2_2
I1122 09:28:26.960705  9524 net.cpp:122] Setting up scale2_2
I1122 09:28:26.960705  9524 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:28:26.960705  9524 net.cpp:137] Memory required for data: 224871600
I1122 09:28:26.960705  9524 layer_factory.cpp:58] Creating layer relu2_2
I1122 09:28:26.960705  9524 net.cpp:84] Creating Layer relu2_2
I1122 09:28:26.960705  9524 net.cpp:406] relu2_2 <- conv2_2
I1122 09:28:26.960705  9524 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1122 09:28:26.960705  9524 net.cpp:122] Setting up relu2_2
I1122 09:28:26.960705  9524 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:28:26.960705  9524 net.cpp:137] Memory required for data: 253543600
I1122 09:28:26.960705  9524 layer_factory.cpp:58] Creating layer pool2_1
I1122 09:28:26.960705  9524 net.cpp:84] Creating Layer pool2_1
I1122 09:28:26.960705  9524 net.cpp:406] pool2_1 <- conv2_2
I1122 09:28:26.960705  9524 net.cpp:380] pool2_1 -> pool2_1
I1122 09:28:26.960705  9524 net.cpp:122] Setting up pool2_1
I1122 09:28:26.960705  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.960705  9524 net.cpp:137] Memory required for data: 260711600
I1122 09:28:26.960705  9524 layer_factory.cpp:58] Creating layer conv3
I1122 09:28:26.960705  9524 net.cpp:84] Creating Layer conv3
I1122 09:28:26.960705  9524 net.cpp:406] conv3 <- pool2_1
I1122 09:28:26.960705  9524 net.cpp:380] conv3 -> conv3
I1122 09:28:26.962712  9524 net.cpp:122] Setting up conv3
I1122 09:28:26.962712  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.962712  9524 net.cpp:137] Memory required for data: 267879600
I1122 09:28:26.962712  9524 layer_factory.cpp:58] Creating layer bn3
I1122 09:28:26.962712  9524 net.cpp:84] Creating Layer bn3
I1122 09:28:26.962712  9524 net.cpp:406] bn3 <- conv3
I1122 09:28:26.962712  9524 net.cpp:367] bn3 -> conv3 (in-place)
I1122 09:28:26.962712  9524 net.cpp:122] Setting up bn3
I1122 09:28:26.962712  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.962712  9524 net.cpp:137] Memory required for data: 275047600
I1122 09:28:26.962712  9524 layer_factory.cpp:58] Creating layer scale3
I1122 09:28:26.962712  9524 net.cpp:84] Creating Layer scale3
I1122 09:28:26.962712  9524 net.cpp:406] scale3 <- conv3
I1122 09:28:26.962712  9524 net.cpp:367] scale3 -> conv3 (in-place)
I1122 09:28:26.962712  9524 layer_factory.cpp:58] Creating layer scale3
I1122 09:28:26.962712  9524 net.cpp:122] Setting up scale3
I1122 09:28:26.962712  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.962712  9524 net.cpp:137] Memory required for data: 282215600
I1122 09:28:26.962712  9524 layer_factory.cpp:58] Creating layer relu3
I1122 09:28:26.962712  9524 net.cpp:84] Creating Layer relu3
I1122 09:28:26.962712  9524 net.cpp:406] relu3 <- conv3
I1122 09:28:26.962712  9524 net.cpp:367] relu3 -> conv3 (in-place)
I1122 09:28:26.962712  9524 net.cpp:122] Setting up relu3
I1122 09:28:26.962712  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.962712  9524 net.cpp:137] Memory required for data: 289383600
I1122 09:28:26.962712  9524 layer_factory.cpp:58] Creating layer conv4
I1122 09:28:26.962712  9524 net.cpp:84] Creating Layer conv4
I1122 09:28:26.962712  9524 net.cpp:406] conv4 <- conv3
I1122 09:28:26.962712  9524 net.cpp:380] conv4 -> conv4
I1122 09:28:26.964715  9524 net.cpp:122] Setting up conv4
I1122 09:28:26.964715  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.964715  9524 net.cpp:137] Memory required for data: 296551600
I1122 09:28:26.964715  9524 layer_factory.cpp:58] Creating layer bn4
I1122 09:28:26.964715  9524 net.cpp:84] Creating Layer bn4
I1122 09:28:26.964715  9524 net.cpp:406] bn4 <- conv4
I1122 09:28:26.964715  9524 net.cpp:367] bn4 -> conv4 (in-place)
I1122 09:28:26.964715  9524 net.cpp:122] Setting up bn4
I1122 09:28:26.964715  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.964715  9524 net.cpp:137] Memory required for data: 303719600
I1122 09:28:26.964715  9524 layer_factory.cpp:58] Creating layer scale4
I1122 09:28:26.964715  9524 net.cpp:84] Creating Layer scale4
I1122 09:28:26.964715  9524 net.cpp:406] scale4 <- conv4
I1122 09:28:26.964715  9524 net.cpp:367] scale4 -> conv4 (in-place)
I1122 09:28:26.964715  9524 layer_factory.cpp:58] Creating layer scale4
I1122 09:28:26.965704  9524 net.cpp:122] Setting up scale4
I1122 09:28:26.965704  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.965704  9524 net.cpp:137] Memory required for data: 310887600
I1122 09:28:26.965704  9524 layer_factory.cpp:58] Creating layer relu4
I1122 09:28:26.965704  9524 net.cpp:84] Creating Layer relu4
I1122 09:28:26.965704  9524 net.cpp:406] relu4 <- conv4
I1122 09:28:26.965704  9524 net.cpp:367] relu4 -> conv4 (in-place)
I1122 09:28:26.965704  9524 net.cpp:122] Setting up relu4
I1122 09:28:26.965704  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.965704  9524 net.cpp:137] Memory required for data: 318055600
I1122 09:28:26.965704  9524 layer_factory.cpp:58] Creating layer conv4_1
I1122 09:28:26.965704  9524 net.cpp:84] Creating Layer conv4_1
I1122 09:28:26.965704  9524 net.cpp:406] conv4_1 <- conv4
I1122 09:28:26.965704  9524 net.cpp:380] conv4_1 -> conv4_1
I1122 09:28:26.967708  9524 net.cpp:122] Setting up conv4_1
I1122 09:28:26.967708  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.967708  9524 net.cpp:137] Memory required for data: 325223600
I1122 09:28:26.967708  9524 layer_factory.cpp:58] Creating layer bn4_1
I1122 09:28:26.967708  9524 net.cpp:84] Creating Layer bn4_1
I1122 09:28:26.967708  9524 net.cpp:406] bn4_1 <- conv4_1
I1122 09:28:26.967708  9524 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1122 09:28:26.967708  9524 net.cpp:122] Setting up bn4_1
I1122 09:28:26.967708  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.967708  9524 net.cpp:137] Memory required for data: 332391600
I1122 09:28:26.967708  9524 layer_factory.cpp:58] Creating layer scale4_1
I1122 09:28:26.967708  9524 net.cpp:84] Creating Layer scale4_1
I1122 09:28:26.967708  9524 net.cpp:406] scale4_1 <- conv4_1
I1122 09:28:26.967708  9524 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1122 09:28:26.967708  9524 layer_factory.cpp:58] Creating layer scale4_1
I1122 09:28:26.967708  9524 net.cpp:122] Setting up scale4_1
I1122 09:28:26.967708  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.967708  9524 net.cpp:137] Memory required for data: 339559600
I1122 09:28:26.967708  9524 layer_factory.cpp:58] Creating layer relu4_1
I1122 09:28:26.967708  9524 net.cpp:84] Creating Layer relu4_1
I1122 09:28:26.968709  9524 net.cpp:406] relu4_1 <- conv4_1
I1122 09:28:26.968709  9524 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1122 09:28:26.968709  9524 net.cpp:122] Setting up relu4_1
I1122 09:28:26.968709  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.968709  9524 net.cpp:137] Memory required for data: 346727600
I1122 09:28:26.968709  9524 layer_factory.cpp:58] Creating layer conv4_2
I1122 09:28:26.968709  9524 net.cpp:84] Creating Layer conv4_2
I1122 09:28:26.968709  9524 net.cpp:406] conv4_2 <- conv4_1
I1122 09:28:26.968709  9524 net.cpp:380] conv4_2 -> conv4_2
I1122 09:28:26.970705  9524 net.cpp:122] Setting up conv4_2
I1122 09:28:26.970705  9524 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:28:26.970705  9524 net.cpp:137] Memory required for data: 355431600
I1122 09:28:26.970705  9524 layer_factory.cpp:58] Creating layer bn4_2
I1122 09:28:26.970705  9524 net.cpp:84] Creating Layer bn4_2
I1122 09:28:26.970705  9524 net.cpp:406] bn4_2 <- conv4_2
I1122 09:28:26.970705  9524 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1122 09:28:26.970705  9524 net.cpp:122] Setting up bn4_2
I1122 09:28:26.970705  9524 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:28:26.970705  9524 net.cpp:137] Memory required for data: 364135600
I1122 09:28:26.970705  9524 layer_factory.cpp:58] Creating layer scale4_2
I1122 09:28:26.970705  9524 net.cpp:84] Creating Layer scale4_2
I1122 09:28:26.970705  9524 net.cpp:406] scale4_2 <- conv4_2
I1122 09:28:26.970705  9524 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1122 09:28:26.970705  9524 layer_factory.cpp:58] Creating layer scale4_2
I1122 09:28:26.970705  9524 net.cpp:122] Setting up scale4_2
I1122 09:28:26.970705  9524 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:28:26.970705  9524 net.cpp:137] Memory required for data: 372839600
I1122 09:28:26.970705  9524 layer_factory.cpp:58] Creating layer relu4_2
I1122 09:28:26.970705  9524 net.cpp:84] Creating Layer relu4_2
I1122 09:28:26.970705  9524 net.cpp:406] relu4_2 <- conv4_2
I1122 09:28:26.970705  9524 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1122 09:28:26.971707  9524 net.cpp:122] Setting up relu4_2
I1122 09:28:26.971707  9524 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:28:26.971707  9524 net.cpp:137] Memory required for data: 381543600
I1122 09:28:26.971707  9524 layer_factory.cpp:58] Creating layer pool4_2
I1122 09:28:26.971707  9524 net.cpp:84] Creating Layer pool4_2
I1122 09:28:26.971707  9524 net.cpp:406] pool4_2 <- conv4_2
I1122 09:28:26.971707  9524 net.cpp:380] pool4_2 -> pool4_2
I1122 09:28:26.971707  9524 net.cpp:122] Setting up pool4_2
I1122 09:28:26.971707  9524 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 09:28:26.971707  9524 net.cpp:137] Memory required for data: 383719600
I1122 09:28:26.971707  9524 layer_factory.cpp:58] Creating layer conv12
I1122 09:28:26.971707  9524 net.cpp:84] Creating Layer conv12
I1122 09:28:26.971707  9524 net.cpp:406] conv12 <- pool4_2
I1122 09:28:26.971707  9524 net.cpp:380] conv12 -> conv12
I1122 09:28:26.973706  9524 net.cpp:122] Setting up conv12
I1122 09:28:26.973706  9524 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:28:26.973706  9524 net.cpp:137] Memory required for data: 386023600
I1122 09:28:26.973706  9524 layer_factory.cpp:58] Creating layer bn_conv12
I1122 09:28:26.973706  9524 net.cpp:84] Creating Layer bn_conv12
I1122 09:28:26.973706  9524 net.cpp:406] bn_conv12 <- conv12
I1122 09:28:26.973706  9524 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1122 09:28:26.973706  9524 net.cpp:122] Setting up bn_conv12
I1122 09:28:26.973706  9524 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:28:26.973706  9524 net.cpp:137] Memory required for data: 388327600
I1122 09:28:26.973706  9524 layer_factory.cpp:58] Creating layer scale_conv12
I1122 09:28:26.973706  9524 net.cpp:84] Creating Layer scale_conv12
I1122 09:28:26.973706  9524 net.cpp:406] scale_conv12 <- conv12
I1122 09:28:26.973706  9524 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1122 09:28:26.973706  9524 layer_factory.cpp:58] Creating layer scale_conv12
I1122 09:28:26.973706  9524 net.cpp:122] Setting up scale_conv12
I1122 09:28:26.973706  9524 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:28:26.973706  9524 net.cpp:137] Memory required for data: 390631600
I1122 09:28:26.973706  9524 layer_factory.cpp:58] Creating layer relu_conv12
I1122 09:28:26.973706  9524 net.cpp:84] Creating Layer relu_conv12
I1122 09:28:26.973706  9524 net.cpp:406] relu_conv12 <- conv12
I1122 09:28:26.973706  9524 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1122 09:28:26.973706  9524 net.cpp:122] Setting up relu_conv12
I1122 09:28:26.973706  9524 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:28:26.973706  9524 net.cpp:137] Memory required for data: 392935600
I1122 09:28:26.973706  9524 layer_factory.cpp:58] Creating layer poolcp6
I1122 09:28:26.973706  9524 net.cpp:84] Creating Layer poolcp6
I1122 09:28:26.973706  9524 net.cpp:406] poolcp6 <- conv12
I1122 09:28:26.973706  9524 net.cpp:380] poolcp6 -> poolcp6
I1122 09:28:26.973706  9524 net.cpp:122] Setting up poolcp6
I1122 09:28:26.973706  9524 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1122 09:28:26.973706  9524 net.cpp:137] Memory required for data: 392971600
I1122 09:28:26.973706  9524 layer_factory.cpp:58] Creating layer ip1
I1122 09:28:26.973706  9524 net.cpp:84] Creating Layer ip1
I1122 09:28:26.973706  9524 net.cpp:406] ip1 <- poolcp6
I1122 09:28:26.973706  9524 net.cpp:380] ip1 -> ip1
I1122 09:28:26.974704  9524 net.cpp:122] Setting up ip1
I1122 09:28:26.974704  9524 net.cpp:129] Top shape: 100 10 (1000)
I1122 09:28:26.974704  9524 net.cpp:137] Memory required for data: 392975600
I1122 09:28:26.974704  9524 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1122 09:28:26.974704  9524 net.cpp:84] Creating Layer ip1_ip1_0_split
I1122 09:28:26.974704  9524 net.cpp:406] ip1_ip1_0_split <- ip1
I1122 09:28:26.974704  9524 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1122 09:28:26.974704  9524 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1122 09:28:26.974704  9524 net.cpp:122] Setting up ip1_ip1_0_split
I1122 09:28:26.974704  9524 net.cpp:129] Top shape: 100 10 (1000)
I1122 09:28:26.974704  9524 net.cpp:129] Top shape: 100 10 (1000)
I1122 09:28:26.974704  9524 net.cpp:137] Memory required for data: 392983600
I1122 09:28:26.974704  9524 layer_factory.cpp:58] Creating layer accuracy_training
I1122 09:28:26.974704  9524 net.cpp:84] Creating Layer accuracy_training
I1122 09:28:26.974704  9524 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1122 09:28:26.974704  9524 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1122 09:28:26.974704  9524 net.cpp:380] accuracy_training -> accuracy_training
I1122 09:28:26.974704  9524 net.cpp:122] Setting up accuracy_training
I1122 09:28:26.974704  9524 net.cpp:129] Top shape: (1)
I1122 09:28:26.974704  9524 net.cpp:137] Memory required for data: 392983604
I1122 09:28:26.974704  9524 layer_factory.cpp:58] Creating layer loss
I1122 09:28:26.974704  9524 net.cpp:84] Creating Layer loss
I1122 09:28:26.974704  9524 net.cpp:406] loss <- ip1_ip1_0_split_1
I1122 09:28:26.974704  9524 net.cpp:406] loss <- label_cifar_1_split_1
I1122 09:28:26.974704  9524 net.cpp:380] loss -> loss
I1122 09:28:26.974704  9524 layer_factory.cpp:58] Creating layer loss
I1122 09:28:26.974704  9524 net.cpp:122] Setting up loss
I1122 09:28:26.974704  9524 net.cpp:129] Top shape: (1)
I1122 09:28:26.974704  9524 net.cpp:132]     with loss weight 1
I1122 09:28:26.974704  9524 net.cpp:137] Memory required for data: 392983608
I1122 09:28:26.974704  9524 net.cpp:198] loss needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:200] accuracy_training does not need backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] ip1 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] poolcp6 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] relu_conv12 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] scale_conv12 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] bn_conv12 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] conv12 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] pool4_2 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] relu4_2 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] scale4_2 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] bn4_2 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] conv4_2 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] relu4_1 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] scale4_1 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] bn4_1 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] conv4_1 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] relu4 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] scale4 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] bn4 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] conv4 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] relu3 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] scale3 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] bn3 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] conv3 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] pool2_1 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] relu2_2 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] scale2_2 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] bn2_2 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] conv2_2 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] relu2 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] scale2 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] bn2 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] conv2 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] relu1 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] scale1 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] bn1 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:198] conv1 needs backward computation.
I1122 09:28:26.974704  9524 net.cpp:200] label_cifar_1_split does not need backward computation.
I1122 09:28:26.974704  9524 net.cpp:200] cifar does not need backward computation.
I1122 09:28:26.974704  9524 net.cpp:242] This network produces output accuracy_training
I1122 09:28:26.974704  9524 net.cpp:242] This network produces output loss
I1122 09:28:26.974704  9524 net.cpp:255] Network initialization done.
I1122 09:28:26.975704  9524 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 09:28:26.975704  9524 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1122 09:28:26.975704  9524 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 09:28:26.975704  9524 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1122 09:28:26.975704  9524 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1122 09:28:26.975704  9524 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1122 09:28:26.975704  9524 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1122 09:28:26.975704  9524 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1122 09:28:26.975704  9524 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1122 09:28:26.975704  9524 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1122 09:28:26.975704  9524 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1122 09:28:26.975704  9524 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1122 09:28:26.975704  9524 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1122 09:28:26.975704  9524 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 41
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 43
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 85
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1122 09:28:26.975704  9524 layer_factory.cpp:58] Creating layer cifar
I1122 09:28:26.983705  9524 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1122 09:28:26.983705  9524 net.cpp:84] Creating Layer cifar
I1122 09:28:26.983705  9524 net.cpp:380] cifar -> data
I1122 09:28:26.983705  9524 net.cpp:380] cifar -> label
I1122 09:28:26.983705  9524 data_layer.cpp:45] output data size: 100,3,32,32
I1122 09:28:26.989707  9524 net.cpp:122] Setting up cifar
I1122 09:28:26.989707  9524 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1122 09:28:26.989707  9524 net.cpp:129] Top shape: 100 (100)
I1122 09:28:26.989707  9524 net.cpp:137] Memory required for data: 1229200
I1122 09:28:26.989707  9524 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1122 09:28:26.989707  9524 net.cpp:84] Creating Layer label_cifar_1_split
I1122 09:28:26.989707  9524 net.cpp:406] label_cifar_1_split <- label
I1122 09:28:26.989707  9524 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1122 09:28:26.989707  9524 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1122 09:28:26.990705  9524 net.cpp:122] Setting up label_cifar_1_split
I1122 09:28:26.990705  9524 net.cpp:129] Top shape: 100 (100)
I1122 09:28:26.990705  9524 net.cpp:129] Top shape: 100 (100)
I1122 09:28:26.990705  9524 net.cpp:137] Memory required for data: 1230000
I1122 09:28:26.990705  9524 layer_factory.cpp:58] Creating layer conv1
I1122 09:28:26.990705  9524 net.cpp:84] Creating Layer conv1
I1122 09:28:26.990705  9524 net.cpp:406] conv1 <- data
I1122 09:28:26.990705  9524 net.cpp:380] conv1 -> conv1
I1122 09:28:26.991708  4976 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 09:28:26.992705  9524 net.cpp:122] Setting up conv1
I1122 09:28:26.992705  9524 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:28:26.992705  9524 net.cpp:137] Memory required for data: 18023600
I1122 09:28:26.992705  9524 layer_factory.cpp:58] Creating layer bn1
I1122 09:28:26.992705  9524 net.cpp:84] Creating Layer bn1
I1122 09:28:26.992705  9524 net.cpp:406] bn1 <- conv1
I1122 09:28:26.992705  9524 net.cpp:367] bn1 -> conv1 (in-place)
I1122 09:28:26.992705  9524 net.cpp:122] Setting up bn1
I1122 09:28:26.992705  9524 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:28:26.992705  9524 net.cpp:137] Memory required for data: 34817200
I1122 09:28:26.992705  9524 layer_factory.cpp:58] Creating layer scale1
I1122 09:28:26.992705  9524 net.cpp:84] Creating Layer scale1
I1122 09:28:26.992705  9524 net.cpp:406] scale1 <- conv1
I1122 09:28:26.992705  9524 net.cpp:367] scale1 -> conv1 (in-place)
I1122 09:28:26.992705  9524 layer_factory.cpp:58] Creating layer scale1
I1122 09:28:26.992705  9524 net.cpp:122] Setting up scale1
I1122 09:28:26.992705  9524 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:28:26.992705  9524 net.cpp:137] Memory required for data: 51610800
I1122 09:28:26.992705  9524 layer_factory.cpp:58] Creating layer relu1
I1122 09:28:26.992705  9524 net.cpp:84] Creating Layer relu1
I1122 09:28:26.992705  9524 net.cpp:406] relu1 <- conv1
I1122 09:28:26.992705  9524 net.cpp:367] relu1 -> conv1 (in-place)
I1122 09:28:26.993706  9524 net.cpp:122] Setting up relu1
I1122 09:28:26.993706  9524 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:28:26.993706  9524 net.cpp:137] Memory required for data: 68404400
I1122 09:28:26.993706  9524 layer_factory.cpp:58] Creating layer conv2
I1122 09:28:26.993706  9524 net.cpp:84] Creating Layer conv2
I1122 09:28:26.993706  9524 net.cpp:406] conv2 <- conv1
I1122 09:28:26.993706  9524 net.cpp:380] conv2 -> conv2
I1122 09:28:26.994704  9524 net.cpp:122] Setting up conv2
I1122 09:28:26.994704  9524 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:28:26.994704  9524 net.cpp:137] Memory required for data: 86017200
I1122 09:28:26.994704  9524 layer_factory.cpp:58] Creating layer bn2
I1122 09:28:26.994704  9524 net.cpp:84] Creating Layer bn2
I1122 09:28:26.994704  9524 net.cpp:406] bn2 <- conv2
I1122 09:28:26.994704  9524 net.cpp:367] bn2 -> conv2 (in-place)
I1122 09:28:26.994704  9524 net.cpp:122] Setting up bn2
I1122 09:28:26.994704  9524 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:28:26.994704  9524 net.cpp:137] Memory required for data: 103630000
I1122 09:28:26.994704  9524 layer_factory.cpp:58] Creating layer scale2
I1122 09:28:26.994704  9524 net.cpp:84] Creating Layer scale2
I1122 09:28:26.994704  9524 net.cpp:406] scale2 <- conv2
I1122 09:28:26.994704  9524 net.cpp:367] scale2 -> conv2 (in-place)
I1122 09:28:26.994704  9524 layer_factory.cpp:58] Creating layer scale2
I1122 09:28:26.994704  9524 net.cpp:122] Setting up scale2
I1122 09:28:26.994704  9524 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:28:26.994704  9524 net.cpp:137] Memory required for data: 121242800
I1122 09:28:26.995705  9524 layer_factory.cpp:58] Creating layer relu2
I1122 09:28:26.995705  9524 net.cpp:84] Creating Layer relu2
I1122 09:28:26.995705  9524 net.cpp:406] relu2 <- conv2
I1122 09:28:26.995705  9524 net.cpp:367] relu2 -> conv2 (in-place)
I1122 09:28:26.995705  9524 net.cpp:122] Setting up relu2
I1122 09:28:26.995705  9524 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:28:26.995705  9524 net.cpp:137] Memory required for data: 138855600
I1122 09:28:26.995705  9524 layer_factory.cpp:58] Creating layer conv2_2
I1122 09:28:26.995705  9524 net.cpp:84] Creating Layer conv2_2
I1122 09:28:26.995705  9524 net.cpp:406] conv2_2 <- conv2
I1122 09:28:26.995705  9524 net.cpp:380] conv2_2 -> conv2_2
I1122 09:28:26.997705  9524 net.cpp:122] Setting up conv2_2
I1122 09:28:26.997705  9524 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:28:26.997705  9524 net.cpp:137] Memory required for data: 167527600
I1122 09:28:26.997705  9524 layer_factory.cpp:58] Creating layer bn2_2
I1122 09:28:26.997705  9524 net.cpp:84] Creating Layer bn2_2
I1122 09:28:26.997705  9524 net.cpp:406] bn2_2 <- conv2_2
I1122 09:28:26.997705  9524 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1122 09:28:26.998704  9524 net.cpp:122] Setting up bn2_2
I1122 09:28:26.998704  9524 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:28:26.998704  9524 net.cpp:137] Memory required for data: 196199600
I1122 09:28:26.998704  9524 layer_factory.cpp:58] Creating layer scale2_2
I1122 09:28:26.998704  9524 net.cpp:84] Creating Layer scale2_2
I1122 09:28:26.998704  9524 net.cpp:406] scale2_2 <- conv2_2
I1122 09:28:26.998704  9524 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1122 09:28:26.998704  9524 layer_factory.cpp:58] Creating layer scale2_2
I1122 09:28:26.998704  9524 net.cpp:122] Setting up scale2_2
I1122 09:28:26.998704  9524 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:28:26.998704  9524 net.cpp:137] Memory required for data: 224871600
I1122 09:28:26.998704  9524 layer_factory.cpp:58] Creating layer relu2_2
I1122 09:28:26.998704  9524 net.cpp:84] Creating Layer relu2_2
I1122 09:28:26.998704  9524 net.cpp:406] relu2_2 <- conv2_2
I1122 09:28:26.998704  9524 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1122 09:28:26.998704  9524 net.cpp:122] Setting up relu2_2
I1122 09:28:26.998704  9524 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:28:26.998704  9524 net.cpp:137] Memory required for data: 253543600
I1122 09:28:26.998704  9524 layer_factory.cpp:58] Creating layer pool2_1
I1122 09:28:26.998704  9524 net.cpp:84] Creating Layer pool2_1
I1122 09:28:26.998704  9524 net.cpp:406] pool2_1 <- conv2_2
I1122 09:28:26.998704  9524 net.cpp:380] pool2_1 -> pool2_1
I1122 09:28:26.998704  9524 net.cpp:122] Setting up pool2_1
I1122 09:28:26.998704  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:26.998704  9524 net.cpp:137] Memory required for data: 260711600
I1122 09:28:26.998704  9524 layer_factory.cpp:58] Creating layer conv3
I1122 09:28:26.998704  9524 net.cpp:84] Creating Layer conv3
I1122 09:28:26.998704  9524 net.cpp:406] conv3 <- pool2_1
I1122 09:28:26.998704  9524 net.cpp:380] conv3 -> conv3
I1122 09:28:27.000705  9524 net.cpp:122] Setting up conv3
I1122 09:28:27.000705  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:27.000705  9524 net.cpp:137] Memory required for data: 267879600
I1122 09:28:27.000705  9524 layer_factory.cpp:58] Creating layer bn3
I1122 09:28:27.000705  9524 net.cpp:84] Creating Layer bn3
I1122 09:28:27.000705  9524 net.cpp:406] bn3 <- conv3
I1122 09:28:27.000705  9524 net.cpp:367] bn3 -> conv3 (in-place)
I1122 09:28:27.001706  9524 net.cpp:122] Setting up bn3
I1122 09:28:27.001706  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:27.001706  9524 net.cpp:137] Memory required for data: 275047600
I1122 09:28:27.001706  9524 layer_factory.cpp:58] Creating layer scale3
I1122 09:28:27.001706  9524 net.cpp:84] Creating Layer scale3
I1122 09:28:27.001706  9524 net.cpp:406] scale3 <- conv3
I1122 09:28:27.001706  9524 net.cpp:367] scale3 -> conv3 (in-place)
I1122 09:28:27.001706  9524 layer_factory.cpp:58] Creating layer scale3
I1122 09:28:27.001706  9524 net.cpp:122] Setting up scale3
I1122 09:28:27.001706  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:27.001706  9524 net.cpp:137] Memory required for data: 282215600
I1122 09:28:27.001706  9524 layer_factory.cpp:58] Creating layer relu3
I1122 09:28:27.001706  9524 net.cpp:84] Creating Layer relu3
I1122 09:28:27.001706  9524 net.cpp:406] relu3 <- conv3
I1122 09:28:27.001706  9524 net.cpp:367] relu3 -> conv3 (in-place)
I1122 09:28:27.001706  9524 net.cpp:122] Setting up relu3
I1122 09:28:27.001706  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:27.001706  9524 net.cpp:137] Memory required for data: 289383600
I1122 09:28:27.001706  9524 layer_factory.cpp:58] Creating layer conv4
I1122 09:28:27.001706  9524 net.cpp:84] Creating Layer conv4
I1122 09:28:27.001706  9524 net.cpp:406] conv4 <- conv3
I1122 09:28:27.001706  9524 net.cpp:380] conv4 -> conv4
I1122 09:28:27.003705  9524 net.cpp:122] Setting up conv4
I1122 09:28:27.003705  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:27.003705  9524 net.cpp:137] Memory required for data: 296551600
I1122 09:28:27.003705  9524 layer_factory.cpp:58] Creating layer bn4
I1122 09:28:27.003705  9524 net.cpp:84] Creating Layer bn4
I1122 09:28:27.003705  9524 net.cpp:406] bn4 <- conv4
I1122 09:28:27.003705  9524 net.cpp:367] bn4 -> conv4 (in-place)
I1122 09:28:27.003705  9524 net.cpp:122] Setting up bn4
I1122 09:28:27.003705  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:27.003705  9524 net.cpp:137] Memory required for data: 303719600
I1122 09:28:27.003705  9524 layer_factory.cpp:58] Creating layer scale4
I1122 09:28:27.003705  9524 net.cpp:84] Creating Layer scale4
I1122 09:28:27.003705  9524 net.cpp:406] scale4 <- conv4
I1122 09:28:27.003705  9524 net.cpp:367] scale4 -> conv4 (in-place)
I1122 09:28:27.003705  9524 layer_factory.cpp:58] Creating layer scale4
I1122 09:28:27.003705  9524 net.cpp:122] Setting up scale4
I1122 09:28:27.003705  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:27.003705  9524 net.cpp:137] Memory required for data: 310887600
I1122 09:28:27.003705  9524 layer_factory.cpp:58] Creating layer relu4
I1122 09:28:27.003705  9524 net.cpp:84] Creating Layer relu4
I1122 09:28:27.003705  9524 net.cpp:406] relu4 <- conv4
I1122 09:28:27.003705  9524 net.cpp:367] relu4 -> conv4 (in-place)
I1122 09:28:27.004705  9524 net.cpp:122] Setting up relu4
I1122 09:28:27.004705  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:27.004705  9524 net.cpp:137] Memory required for data: 318055600
I1122 09:28:27.004705  9524 layer_factory.cpp:58] Creating layer conv4_1
I1122 09:28:27.004705  9524 net.cpp:84] Creating Layer conv4_1
I1122 09:28:27.004705  9524 net.cpp:406] conv4_1 <- conv4
I1122 09:28:27.004705  9524 net.cpp:380] conv4_1 -> conv4_1
I1122 09:28:27.005705  9524 net.cpp:122] Setting up conv4_1
I1122 09:28:27.005705  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:27.006707  9524 net.cpp:137] Memory required for data: 325223600
I1122 09:28:27.006707  9524 layer_factory.cpp:58] Creating layer bn4_1
I1122 09:28:27.006707  9524 net.cpp:84] Creating Layer bn4_1
I1122 09:28:27.006707  9524 net.cpp:406] bn4_1 <- conv4_1
I1122 09:28:27.006707  9524 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1122 09:28:27.006707  9524 net.cpp:122] Setting up bn4_1
I1122 09:28:27.006707  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:27.006707  9524 net.cpp:137] Memory required for data: 332391600
I1122 09:28:27.006707  9524 layer_factory.cpp:58] Creating layer scale4_1
I1122 09:28:27.006707  9524 net.cpp:84] Creating Layer scale4_1
I1122 09:28:27.006707  9524 net.cpp:406] scale4_1 <- conv4_1
I1122 09:28:27.006707  9524 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1122 09:28:27.006707  9524 layer_factory.cpp:58] Creating layer scale4_1
I1122 09:28:27.006707  9524 net.cpp:122] Setting up scale4_1
I1122 09:28:27.006707  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:27.006707  9524 net.cpp:137] Memory required for data: 339559600
I1122 09:28:27.006707  9524 layer_factory.cpp:58] Creating layer relu4_1
I1122 09:28:27.006707  9524 net.cpp:84] Creating Layer relu4_1
I1122 09:28:27.006707  9524 net.cpp:406] relu4_1 <- conv4_1
I1122 09:28:27.006707  9524 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1122 09:28:27.006707  9524 net.cpp:122] Setting up relu4_1
I1122 09:28:27.006707  9524 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:28:27.006707  9524 net.cpp:137] Memory required for data: 346727600
I1122 09:28:27.006707  9524 layer_factory.cpp:58] Creating layer conv4_2
I1122 09:28:27.006707  9524 net.cpp:84] Creating Layer conv4_2
I1122 09:28:27.006707  9524 net.cpp:406] conv4_2 <- conv4_1
I1122 09:28:27.006707  9524 net.cpp:380] conv4_2 -> conv4_2
I1122 09:28:27.008729  9524 net.cpp:122] Setting up conv4_2
I1122 09:28:27.008729  9524 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:28:27.008729  9524 net.cpp:137] Memory required for data: 355431600
I1122 09:28:27.008729  9524 layer_factory.cpp:58] Creating layer bn4_2
I1122 09:28:27.009722  9524 net.cpp:84] Creating Layer bn4_2
I1122 09:28:27.009722  9524 net.cpp:406] bn4_2 <- conv4_2
I1122 09:28:27.009722  9524 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1122 09:28:27.009722  9524 net.cpp:122] Setting up bn4_2
I1122 09:28:27.009722  9524 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:28:27.009722  9524 net.cpp:137] Memory required for data: 364135600
I1122 09:28:27.009722  9524 layer_factory.cpp:58] Creating layer scale4_2
I1122 09:28:27.009722  9524 net.cpp:84] Creating Layer scale4_2
I1122 09:28:27.009722  9524 net.cpp:406] scale4_2 <- conv4_2
I1122 09:28:27.009722  9524 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1122 09:28:27.009722  9524 layer_factory.cpp:58] Creating layer scale4_2
I1122 09:28:27.009722  9524 net.cpp:122] Setting up scale4_2
I1122 09:28:27.009722  9524 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:28:27.009722  9524 net.cpp:137] Memory required for data: 372839600
I1122 09:28:27.009722  9524 layer_factory.cpp:58] Creating layer relu4_2
I1122 09:28:27.009722  9524 net.cpp:84] Creating Layer relu4_2
I1122 09:28:27.009722  9524 net.cpp:406] relu4_2 <- conv4_2
I1122 09:28:27.009722  9524 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1122 09:28:27.010711  9524 net.cpp:122] Setting up relu4_2
I1122 09:28:27.010711  9524 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:28:27.010711  9524 net.cpp:137] Memory required for data: 381543600
I1122 09:28:27.010711  9524 layer_factory.cpp:58] Creating layer pool4_2
I1122 09:28:27.010711  9524 net.cpp:84] Creating Layer pool4_2
I1122 09:28:27.010711  9524 net.cpp:406] pool4_2 <- conv4_2
I1122 09:28:27.010711  9524 net.cpp:380] pool4_2 -> pool4_2
I1122 09:28:27.010711  9524 net.cpp:122] Setting up pool4_2
I1122 09:28:27.010711  9524 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 09:28:27.010711  9524 net.cpp:137] Memory required for data: 383719600
I1122 09:28:27.010711  9524 layer_factory.cpp:58] Creating layer conv12
I1122 09:28:27.010711  9524 net.cpp:84] Creating Layer conv12
I1122 09:28:27.010711  9524 net.cpp:406] conv12 <- pool4_2
I1122 09:28:27.010711  9524 net.cpp:380] conv12 -> conv12
I1122 09:28:27.012709  9524 net.cpp:122] Setting up conv12
I1122 09:28:27.012709  9524 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:28:27.012709  9524 net.cpp:137] Memory required for data: 386023600
I1122 09:28:27.012709  9524 layer_factory.cpp:58] Creating layer bn_conv12
I1122 09:28:27.012709  9524 net.cpp:84] Creating Layer bn_conv12
I1122 09:28:27.012709  9524 net.cpp:406] bn_conv12 <- conv12
I1122 09:28:27.012709  9524 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1122 09:28:27.012709  9524 net.cpp:122] Setting up bn_conv12
I1122 09:28:27.012709  9524 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:28:27.012709  9524 net.cpp:137] Memory required for data: 388327600
I1122 09:28:27.012709  9524 layer_factory.cpp:58] Creating layer scale_conv12
I1122 09:28:27.012709  9524 net.cpp:84] Creating Layer scale_conv12
I1122 09:28:27.012709  9524 net.cpp:406] scale_conv12 <- conv12
I1122 09:28:27.012709  9524 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1122 09:28:27.012709  9524 layer_factory.cpp:58] Creating layer scale_conv12
I1122 09:28:27.012709  9524 net.cpp:122] Setting up scale_conv12
I1122 09:28:27.012709  9524 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:28:27.012709  9524 net.cpp:137] Memory required for data: 390631600
I1122 09:28:27.012709  9524 layer_factory.cpp:58] Creating layer relu_conv12
I1122 09:28:27.012709  9524 net.cpp:84] Creating Layer relu_conv12
I1122 09:28:27.012709  9524 net.cpp:406] relu_conv12 <- conv12
I1122 09:28:27.012709  9524 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1122 09:28:27.013710  9524 net.cpp:122] Setting up relu_conv12
I1122 09:28:27.013710  9524 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:28:27.013710  9524 net.cpp:137] Memory required for data: 392935600
I1122 09:28:27.013710  9524 layer_factory.cpp:58] Creating layer poolcp6
I1122 09:28:27.013710  9524 net.cpp:84] Creating Layer poolcp6
I1122 09:28:27.013710  9524 net.cpp:406] poolcp6 <- conv12
I1122 09:28:27.013710  9524 net.cpp:380] poolcp6 -> poolcp6
I1122 09:28:27.013710  9524 net.cpp:122] Setting up poolcp6
I1122 09:28:27.013710  9524 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1122 09:28:27.013710  9524 net.cpp:137] Memory required for data: 392971600
I1122 09:28:27.013710  9524 layer_factory.cpp:58] Creating layer ip1
I1122 09:28:27.013710  9524 net.cpp:84] Creating Layer ip1
I1122 09:28:27.013710  9524 net.cpp:406] ip1 <- poolcp6
I1122 09:28:27.013710  9524 net.cpp:380] ip1 -> ip1
I1122 09:28:27.013710  9524 net.cpp:122] Setting up ip1
I1122 09:28:27.013710  9524 net.cpp:129] Top shape: 100 10 (1000)
I1122 09:28:27.013710  9524 net.cpp:137] Memory required for data: 392975600
I1122 09:28:27.013710  9524 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1122 09:28:27.013710  9524 net.cpp:84] Creating Layer ip1_ip1_0_split
I1122 09:28:27.013710  9524 net.cpp:406] ip1_ip1_0_split <- ip1
I1122 09:28:27.013710  9524 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1122 09:28:27.013710  9524 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1122 09:28:27.013710  9524 net.cpp:122] Setting up ip1_ip1_0_split
I1122 09:28:27.013710  9524 net.cpp:129] Top shape: 100 10 (1000)
I1122 09:28:27.013710  9524 net.cpp:129] Top shape: 100 10 (1000)
I1122 09:28:27.013710  9524 net.cpp:137] Memory required for data: 392983600
I1122 09:28:27.013710  9524 layer_factory.cpp:58] Creating layer accuracy
I1122 09:28:27.013710  9524 net.cpp:84] Creating Layer accuracy
I1122 09:28:27.013710  9524 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1122 09:28:27.013710  9524 net.cpp:406] accuracy <- label_cifar_1_split_0
I1122 09:28:27.013710  9524 net.cpp:380] accuracy -> accuracy
I1122 09:28:27.013710  9524 net.cpp:122] Setting up accuracy
I1122 09:28:27.013710  9524 net.cpp:129] Top shape: (1)
I1122 09:28:27.013710  9524 net.cpp:137] Memory required for data: 392983604
I1122 09:28:27.013710  9524 layer_factory.cpp:58] Creating layer loss
I1122 09:28:27.013710  9524 net.cpp:84] Creating Layer loss
I1122 09:28:27.013710  9524 net.cpp:406] loss <- ip1_ip1_0_split_1
I1122 09:28:27.013710  9524 net.cpp:406] loss <- label_cifar_1_split_1
I1122 09:28:27.013710  9524 net.cpp:380] loss -> loss
I1122 09:28:27.013710  9524 layer_factory.cpp:58] Creating layer loss
I1122 09:28:27.014710  9524 net.cpp:122] Setting up loss
I1122 09:28:27.014710  9524 net.cpp:129] Top shape: (1)
I1122 09:28:27.014710  9524 net.cpp:132]     with loss weight 1
I1122 09:28:27.014710  9524 net.cpp:137] Memory required for data: 392983608
I1122 09:28:27.014710  9524 net.cpp:198] loss needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:200] accuracy does not need backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] ip1 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] poolcp6 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] relu_conv12 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] scale_conv12 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] bn_conv12 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] conv12 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] pool4_2 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] relu4_2 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] scale4_2 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] bn4_2 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] conv4_2 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] relu4_1 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] scale4_1 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] bn4_1 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] conv4_1 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] relu4 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] scale4 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] bn4 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] conv4 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] relu3 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] scale3 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] bn3 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] conv3 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] pool2_1 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] relu2_2 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] scale2_2 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] bn2_2 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] conv2_2 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] relu2 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] scale2 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] bn2 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] conv2 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] relu1 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] scale1 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] bn1 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:198] conv1 needs backward computation.
I1122 09:28:27.014710  9524 net.cpp:200] label_cifar_1_split does not need backward computation.
I1122 09:28:27.014710  9524 net.cpp:200] cifar does not need backward computation.
I1122 09:28:27.014710  9524 net.cpp:242] This network produces output accuracy
I1122 09:28:27.014710  9524 net.cpp:242] This network produces output loss
I1122 09:28:27.014710  9524 net.cpp:255] Network initialization done.
I1122 09:28:27.014710  9524 solver.cpp:56] Solver scaffolding done.
I1122 09:28:27.017705  9524 caffe.cpp:249] Starting Optimization
I1122 09:28:27.017705  9524 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k
I1122 09:28:27.017705  9524 solver.cpp:273] Learning Rate Policy: multistep
I1122 09:28:27.019711  9524 solver.cpp:330] Iteration 0, Testing net (#0)
I1122 09:28:27.020725  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:28:28.133612  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:28:28.175689  9524 solver.cpp:397]     Test net output #0: accuracy = 0.0889
I1122 09:28:28.175689  9524 solver.cpp:397]     Test net output #1: loss = 79.5723 (* 1 = 79.5723 loss)
I1122 09:28:28.249182  9524 solver.cpp:218] Iteration 0 (0 iter/s, 1.23077s/100 iters), loss = 3.655
I1122 09:28:28.249668  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.12
I1122 09:28:28.249668  9524 solver.cpp:237]     Train net output #1: loss = 3.655 (* 1 = 3.655 loss)
I1122 09:28:28.249668  9524 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1122 09:28:32.526461  9524 solver.cpp:218] Iteration 100 (23.3823 iter/s, 4.27674s/100 iters), loss = 1.61525
I1122 09:28:32.526461  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.43
I1122 09:28:32.526461  9524 solver.cpp:237]     Train net output #1: loss = 1.61525 (* 1 = 1.61525 loss)
I1122 09:28:32.526461  9524 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1122 09:28:36.827435  9524 solver.cpp:218] Iteration 200 (23.2518 iter/s, 4.30074s/100 iters), loss = 1.47309
I1122 09:28:36.827435  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.49
I1122 09:28:36.827435  9524 solver.cpp:237]     Train net output #1: loss = 1.47309 (* 1 = 1.47309 loss)
I1122 09:28:36.827435  9524 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1122 09:28:41.093427  9524 solver.cpp:218] Iteration 300 (23.4421 iter/s, 4.26583s/100 iters), loss = 1.25888
I1122 09:28:41.093427  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.53
I1122 09:28:41.093427  9524 solver.cpp:237]     Train net output #1: loss = 1.25888 (* 1 = 1.25888 loss)
I1122 09:28:41.093427  9524 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1122 09:28:45.377334  9524 solver.cpp:218] Iteration 400 (23.349 iter/s, 4.28284s/100 iters), loss = 1.28306
I1122 09:28:45.377334  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.49
I1122 09:28:45.377334  9524 solver.cpp:237]     Train net output #1: loss = 1.28306 (* 1 = 1.28306 loss)
I1122 09:28:45.377334  9524 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1122 09:28:49.447629 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:28:49.614663  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_500.caffemodel
I1122 09:28:49.633661  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_500.solverstate
I1122 09:28:49.637660  9524 solver.cpp:330] Iteration 500, Testing net (#0)
I1122 09:28:49.637660  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:28:50.712312  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:28:50.754832  9524 solver.cpp:397]     Test net output #0: accuracy = 0.3925
I1122 09:28:50.754832  9524 solver.cpp:397]     Test net output #1: loss = 1.65367 (* 1 = 1.65367 loss)
I1122 09:28:50.795363  9524 solver.cpp:218] Iteration 500 (18.4561 iter/s, 5.41825s/100 iters), loss = 1.13072
I1122 09:28:50.795363  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.61
I1122 09:28:50.795363  9524 solver.cpp:237]     Train net output #1: loss = 1.13072 (* 1 = 1.13072 loss)
I1122 09:28:50.795363  9524 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1122 09:28:55.077313  9524 solver.cpp:218] Iteration 600 (23.3592 iter/s, 4.28096s/100 iters), loss = 1.03638
I1122 09:28:55.077313  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1122 09:28:55.077313  9524 solver.cpp:237]     Train net output #1: loss = 1.03638 (* 1 = 1.03638 loss)
I1122 09:28:55.077313  9524 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1122 09:28:59.348570  9524 solver.cpp:218] Iteration 700 (23.4158 iter/s, 4.27062s/100 iters), loss = 1.0197
I1122 09:28:59.348570  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1122 09:28:59.348570  9524 solver.cpp:237]     Train net output #1: loss = 1.0197 (* 1 = 1.0197 loss)
I1122 09:28:59.348570  9524 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1122 09:29:03.620712  9524 solver.cpp:218] Iteration 800 (23.4085 iter/s, 4.27196s/100 iters), loss = 0.860147
I1122 09:29:03.620712  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1122 09:29:03.620712  9524 solver.cpp:237]     Train net output #1: loss = 0.860147 (* 1 = 0.860147 loss)
I1122 09:29:03.620712  9524 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1122 09:29:07.896541  9524 solver.cpp:218] Iteration 900 (23.3857 iter/s, 4.27612s/100 iters), loss = 0.908561
I1122 09:29:07.897554  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1122 09:29:07.897554  9524 solver.cpp:237]     Train net output #1: loss = 0.908561 (* 1 = 0.908561 loss)
I1122 09:29:07.897554  9524 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1122 09:29:11.959120 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:29:12.128152  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_1000.caffemodel
I1122 09:29:12.138677  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_1000.solverstate
I1122 09:29:12.142675  9524 solver.cpp:330] Iteration 1000, Testing net (#0)
I1122 09:29:12.143177  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:29:13.214527  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:29:13.256544  9524 solver.cpp:397]     Test net output #0: accuracy = 0.6151
I1122 09:29:13.256544  9524 solver.cpp:397]     Test net output #1: loss = 1.12243 (* 1 = 1.12243 loss)
I1122 09:29:13.297557  9524 solver.cpp:218] Iteration 1000 (18.5181 iter/s, 5.40012s/100 iters), loss = 0.854072
I1122 09:29:13.297557  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1122 09:29:13.297557  9524 solver.cpp:237]     Train net output #1: loss = 0.854072 (* 1 = 0.854072 loss)
I1122 09:29:13.297557  9524 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1122 09:29:17.596295  9524 solver.cpp:218] Iteration 1100 (23.2653 iter/s, 4.29826s/100 iters), loss = 0.778476
I1122 09:29:17.596295  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1122 09:29:17.596295  9524 solver.cpp:237]     Train net output #1: loss = 0.778476 (* 1 = 0.778476 loss)
I1122 09:29:17.596295  9524 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1122 09:29:21.901893  9524 solver.cpp:218] Iteration 1200 (23.2267 iter/s, 4.3054s/100 iters), loss = 0.733861
I1122 09:29:21.901893  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1122 09:29:21.901893  9524 solver.cpp:237]     Train net output #1: loss = 0.733861 (* 1 = 0.733861 loss)
I1122 09:29:21.901893  9524 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1122 09:29:26.182471  9524 solver.cpp:218] Iteration 1300 (23.3637 iter/s, 4.28014s/100 iters), loss = 0.710529
I1122 09:29:26.182471  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 09:29:26.182471  9524 solver.cpp:237]     Train net output #1: loss = 0.710529 (* 1 = 0.710529 loss)
I1122 09:29:26.182471  9524 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1122 09:29:30.469851  9524 solver.cpp:218] Iteration 1400 (23.3272 iter/s, 4.28684s/100 iters), loss = 0.708481
I1122 09:29:30.469851  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1122 09:29:30.469851  9524 solver.cpp:237]     Train net output #1: loss = 0.708481 (* 1 = 0.708481 loss)
I1122 09:29:30.469851  9524 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1122 09:29:34.539518 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:29:34.708598  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_1500.caffemodel
I1122 09:29:34.718602  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_1500.solverstate
I1122 09:29:34.722605  9524 solver.cpp:330] Iteration 1500, Testing net (#0)
I1122 09:29:34.722605  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:29:35.794840  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:29:35.836855  9524 solver.cpp:397]     Test net output #0: accuracy = 0.6485
I1122 09:29:35.837857  9524 solver.cpp:397]     Test net output #1: loss = 1.01277 (* 1 = 1.01277 loss)
I1122 09:29:35.878875  9524 solver.cpp:218] Iteration 1500 (18.4895 iter/s, 5.40848s/100 iters), loss = 0.672456
I1122 09:29:35.878875  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1122 09:29:35.878875  9524 solver.cpp:237]     Train net output #1: loss = 0.672456 (* 1 = 0.672456 loss)
I1122 09:29:35.878875  9524 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1122 09:29:40.191287  9524 solver.cpp:218] Iteration 1600 (23.1909 iter/s, 4.31203s/100 iters), loss = 0.666632
I1122 09:29:40.191287  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1122 09:29:40.191287  9524 solver.cpp:237]     Train net output #1: loss = 0.666632 (* 1 = 0.666632 loss)
I1122 09:29:40.191287  9524 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1122 09:29:44.456183  9524 solver.cpp:218] Iteration 1700 (23.4496 iter/s, 4.26446s/100 iters), loss = 0.650738
I1122 09:29:44.456183  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 09:29:44.456183  9524 solver.cpp:237]     Train net output #1: loss = 0.650738 (* 1 = 0.650738 loss)
I1122 09:29:44.456183  9524 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1122 09:29:48.719777  9524 solver.cpp:218] Iteration 1800 (23.4545 iter/s, 4.26357s/100 iters), loss = 0.744369
I1122 09:29:48.719777  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1122 09:29:48.719777  9524 solver.cpp:237]     Train net output #1: loss = 0.744369 (* 1 = 0.744369 loss)
I1122 09:29:48.719777  9524 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1122 09:29:52.983422  9524 solver.cpp:218] Iteration 1900 (23.4571 iter/s, 4.2631s/100 iters), loss = 0.595184
I1122 09:29:52.983422  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 09:29:52.983422  9524 solver.cpp:237]     Train net output #1: loss = 0.595184 (* 1 = 0.595184 loss)
I1122 09:29:52.983422  9524 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1122 09:29:57.040964 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:29:57.208542  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_2000.caffemodel
I1122 09:29:57.218749  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_2000.solverstate
I1122 09:29:57.222748  9524 solver.cpp:330] Iteration 2000, Testing net (#0)
I1122 09:29:57.222748  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:29:58.293553  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:29:58.335618  9524 solver.cpp:397]     Test net output #0: accuracy = 0.5573
I1122 09:29:58.335618  9524 solver.cpp:397]     Test net output #1: loss = 1.24164 (* 1 = 1.24164 loss)
I1122 09:29:58.377622  9524 solver.cpp:218] Iteration 2000 (18.5414 iter/s, 5.39335s/100 iters), loss = 0.595579
I1122 09:29:58.377622  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1122 09:29:58.377622  9524 solver.cpp:237]     Train net output #1: loss = 0.595579 (* 1 = 0.595579 loss)
I1122 09:29:58.377622  9524 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1122 09:30:02.685680  9524 solver.cpp:218] Iteration 2100 (23.2133 iter/s, 4.30788s/100 iters), loss = 0.569633
I1122 09:30:02.685680  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 09:30:02.685680  9524 solver.cpp:237]     Train net output #1: loss = 0.569633 (* 1 = 0.569633 loss)
I1122 09:30:02.685680  9524 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1122 09:30:06.963220  9524 solver.cpp:218] Iteration 2200 (23.3802 iter/s, 4.27713s/100 iters), loss = 0.594459
I1122 09:30:06.963220  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1122 09:30:06.963220  9524 solver.cpp:237]     Train net output #1: loss = 0.594459 (* 1 = 0.594459 loss)
I1122 09:30:06.963220  9524 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1122 09:30:11.239769  9524 solver.cpp:218] Iteration 2300 (23.3818 iter/s, 4.27683s/100 iters), loss = 0.687329
I1122 09:30:11.240770  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1122 09:30:11.240770  9524 solver.cpp:237]     Train net output #1: loss = 0.687329 (* 1 = 0.687329 loss)
I1122 09:30:11.240770  9524 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1122 09:30:15.520256  9524 solver.cpp:218] Iteration 2400 (23.3684 iter/s, 4.27928s/100 iters), loss = 0.643727
I1122 09:30:15.520256  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1122 09:30:15.520256  9524 solver.cpp:237]     Train net output #1: loss = 0.643727 (* 1 = 0.643727 loss)
I1122 09:30:15.520256  9524 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1122 09:30:19.602787 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:30:19.770853  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_2500.caffemodel
I1122 09:30:19.781855  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_2500.solverstate
I1122 09:30:19.785853  9524 solver.cpp:330] Iteration 2500, Testing net (#0)
I1122 09:30:19.785853  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:30:20.874157  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:30:20.916666  9524 solver.cpp:397]     Test net output #0: accuracy = 0.6352
I1122 09:30:20.916666  9524 solver.cpp:397]     Test net output #1: loss = 1.0437 (* 1 = 1.0437 loss)
I1122 09:30:20.957167  9524 solver.cpp:218] Iteration 2500 (18.3931 iter/s, 5.43683s/100 iters), loss = 0.635853
I1122 09:30:20.957167  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1122 09:30:20.957167  9524 solver.cpp:237]     Train net output #1: loss = 0.635853 (* 1 = 0.635853 loss)
I1122 09:30:20.957167  9524 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1122 09:30:25.299991  9524 solver.cpp:218] Iteration 2600 (23.0309 iter/s, 4.34199s/100 iters), loss = 0.549049
I1122 09:30:25.299991  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 09:30:25.299991  9524 solver.cpp:237]     Train net output #1: loss = 0.549049 (* 1 = 0.549049 loss)
I1122 09:30:25.299991  9524 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1122 09:30:29.579044  9524 solver.cpp:218] Iteration 2700 (23.3691 iter/s, 4.27915s/100 iters), loss = 0.63941
I1122 09:30:29.579044  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 09:30:29.579044  9524 solver.cpp:237]     Train net output #1: loss = 0.63941 (* 1 = 0.63941 loss)
I1122 09:30:29.579044  9524 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1122 09:30:33.859727  9524 solver.cpp:218] Iteration 2800 (23.3626 iter/s, 4.28034s/100 iters), loss = 0.509876
I1122 09:30:33.859727  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 09:30:33.859727  9524 solver.cpp:237]     Train net output #1: loss = 0.509876 (* 1 = 0.509876 loss)
I1122 09:30:33.859727  9524 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1122 09:30:38.139384  9524 solver.cpp:218] Iteration 2900 (23.3688 iter/s, 4.27922s/100 iters), loss = 0.534369
I1122 09:30:38.139384  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 09:30:38.139384  9524 solver.cpp:237]     Train net output #1: loss = 0.534369 (* 1 = 0.534369 loss)
I1122 09:30:38.139384  9524 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1122 09:30:42.209815 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:30:42.377871  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_3000.caffemodel
I1122 09:30:42.388878  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_3000.solverstate
I1122 09:30:42.416417  9524 solver.cpp:330] Iteration 3000, Testing net (#0)
I1122 09:30:42.416417  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:30:43.490025  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:30:43.531044  9524 solver.cpp:397]     Test net output #0: accuracy = 0.6275
I1122 09:30:43.531044  9524 solver.cpp:397]     Test net output #1: loss = 1.03997 (* 1 = 1.03997 loss)
I1122 09:30:43.573334  9524 solver.cpp:218] Iteration 3000 (18.4054 iter/s, 5.4332s/100 iters), loss = 0.595502
I1122 09:30:43.573334  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 09:30:43.573334  9524 solver.cpp:237]     Train net output #1: loss = 0.595502 (* 1 = 0.595502 loss)
I1122 09:30:43.573334  9524 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1122 09:30:47.843418  9524 solver.cpp:218] Iteration 3100 (23.4177 iter/s, 4.27027s/100 iters), loss = 0.496277
I1122 09:30:47.843418  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1122 09:30:47.843418  9524 solver.cpp:237]     Train net output #1: loss = 0.496277 (* 1 = 0.496277 loss)
I1122 09:30:47.843418  9524 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1122 09:30:52.123332  9524 solver.cpp:218] Iteration 3200 (23.3689 iter/s, 4.2792s/100 iters), loss = 0.500052
I1122 09:30:52.123332  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 09:30:52.123332  9524 solver.cpp:237]     Train net output #1: loss = 0.500052 (* 1 = 0.500052 loss)
I1122 09:30:52.123332  9524 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1122 09:30:56.401320  9524 solver.cpp:218] Iteration 3300 (23.3761 iter/s, 4.27788s/100 iters), loss = 0.646104
I1122 09:30:56.401320  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1122 09:30:56.401320  9524 solver.cpp:237]     Train net output #1: loss = 0.646104 (* 1 = 0.646104 loss)
I1122 09:30:56.401320  9524 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1122 09:31:00.679018  9524 solver.cpp:218] Iteration 3400 (23.3805 iter/s, 4.27706s/100 iters), loss = 0.565125
I1122 09:31:00.679018  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 09:31:00.679018  9524 solver.cpp:237]     Train net output #1: loss = 0.565125 (* 1 = 0.565125 loss)
I1122 09:31:00.679018  9524 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1122 09:31:04.747707 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:31:04.915745  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_3500.caffemodel
I1122 09:31:04.925770  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_3500.solverstate
I1122 09:31:04.929769  9524 solver.cpp:330] Iteration 3500, Testing net (#0)
I1122 09:31:04.929769  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:31:06.001159  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:31:06.043215  9524 solver.cpp:397]     Test net output #0: accuracy = 0.6953
I1122 09:31:06.043215  9524 solver.cpp:397]     Test net output #1: loss = 0.864564 (* 1 = 0.864564 loss)
I1122 09:31:06.084209  9524 solver.cpp:218] Iteration 3500 (18.5001 iter/s, 5.40538s/100 iters), loss = 0.453562
I1122 09:31:06.085223  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1122 09:31:06.085223  9524 solver.cpp:237]     Train net output #1: loss = 0.453562 (* 1 = 0.453562 loss)
I1122 09:31:06.085223  9524 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1122 09:31:10.347913  9524 solver.cpp:218] Iteration 3600 (23.4601 iter/s, 4.26256s/100 iters), loss = 0.451464
I1122 09:31:10.347913  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1122 09:31:10.347913  9524 solver.cpp:237]     Train net output #1: loss = 0.451464 (* 1 = 0.451464 loss)
I1122 09:31:10.347913  9524 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1122 09:31:14.608611  9524 solver.cpp:218] Iteration 3700 (23.4715 iter/s, 4.26048s/100 iters), loss = 0.556505
I1122 09:31:14.608611  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 09:31:14.608611  9524 solver.cpp:237]     Train net output #1: loss = 0.556505 (* 1 = 0.556505 loss)
I1122 09:31:14.608611  9524 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1122 09:31:18.867256  9524 solver.cpp:218] Iteration 3800 (23.4811 iter/s, 4.25874s/100 iters), loss = 0.524242
I1122 09:31:18.867256  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 09:31:18.867256  9524 solver.cpp:237]     Train net output #1: loss = 0.524242 (* 1 = 0.524242 loss)
I1122 09:31:18.867256  9524 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1122 09:31:23.128986  9524 solver.cpp:218] Iteration 3900 (23.4692 iter/s, 4.26091s/100 iters), loss = 0.531351
I1122 09:31:23.128986  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 09:31:23.128986  9524 solver.cpp:237]     Train net output #1: loss = 0.531351 (* 1 = 0.531351 loss)
I1122 09:31:23.128986  9524 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1122 09:31:27.180564 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:31:27.348635  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_4000.caffemodel
I1122 09:31:27.361655  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_4000.solverstate
I1122 09:31:27.365654  9524 solver.cpp:330] Iteration 4000, Testing net (#0)
I1122 09:31:27.365654  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:31:28.438009  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:31:28.480041  9524 solver.cpp:397]     Test net output #0: accuracy = 0.7239
I1122 09:31:28.480041  9524 solver.cpp:397]     Test net output #1: loss = 0.787783 (* 1 = 0.787783 loss)
I1122 09:31:28.522027  9524 solver.cpp:218] Iteration 4000 (18.5449 iter/s, 5.39232s/100 iters), loss = 0.489774
I1122 09:31:28.522027  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 09:31:28.522027  9524 solver.cpp:237]     Train net output #1: loss = 0.489774 (* 1 = 0.489774 loss)
I1122 09:31:28.522027  9524 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1122 09:31:32.804801  9524 solver.cpp:218] Iteration 4100 (23.3475 iter/s, 4.28312s/100 iters), loss = 0.495486
I1122 09:31:32.804801  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 09:31:32.804801  9524 solver.cpp:237]     Train net output #1: loss = 0.495486 (* 1 = 0.495486 loss)
I1122 09:31:32.804801  9524 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1122 09:31:37.086473  9524 solver.cpp:218] Iteration 4200 (23.3593 iter/s, 4.28095s/100 iters), loss = 0.486574
I1122 09:31:37.086473  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 09:31:37.086473  9524 solver.cpp:237]     Train net output #1: loss = 0.486574 (* 1 = 0.486574 loss)
I1122 09:31:37.086473  9524 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1122 09:31:41.366989  9524 solver.cpp:218] Iteration 4300 (23.3619 iter/s, 4.28048s/100 iters), loss = 0.553409
I1122 09:31:41.366989  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 09:31:41.366989  9524 solver.cpp:237]     Train net output #1: loss = 0.553409 (* 1 = 0.553409 loss)
I1122 09:31:41.366989  9524 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1122 09:31:45.649744  9524 solver.cpp:218] Iteration 4400 (23.355 iter/s, 4.28173s/100 iters), loss = 0.445432
I1122 09:31:45.649744  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 09:31:45.649744  9524 solver.cpp:237]     Train net output #1: loss = 0.445432 (* 1 = 0.445432 loss)
I1122 09:31:45.649744  9524 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1122 09:31:49.718387 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:31:49.887473  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_4500.caffemodel
I1122 09:31:49.897476  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_4500.solverstate
I1122 09:31:49.901473  9524 solver.cpp:330] Iteration 4500, Testing net (#0)
I1122 09:31:49.901473  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:31:50.970957  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:31:51.013972  9524 solver.cpp:397]     Test net output #0: accuracy = 0.6929
I1122 09:31:51.013972  9524 solver.cpp:397]     Test net output #1: loss = 0.908555 (* 1 = 0.908555 loss)
I1122 09:31:51.054989  9524 solver.cpp:218] Iteration 4500 (18.5019 iter/s, 5.40484s/100 iters), loss = 0.45318
I1122 09:31:51.054989  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 09:31:51.054989  9524 solver.cpp:237]     Train net output #1: loss = 0.45318 (* 1 = 0.45318 loss)
I1122 09:31:51.054989  9524 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1122 09:31:55.307461  9524 solver.cpp:218] Iteration 4600 (23.5168 iter/s, 4.25228s/100 iters), loss = 0.41684
I1122 09:31:55.307461  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 09:31:55.307461  9524 solver.cpp:237]     Train net output #1: loss = 0.41684 (* 1 = 0.41684 loss)
I1122 09:31:55.307461  9524 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1122 09:31:59.569939  9524 solver.cpp:218] Iteration 4700 (23.4644 iter/s, 4.26178s/100 iters), loss = 0.482025
I1122 09:31:59.569939  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 09:31:59.569939  9524 solver.cpp:237]     Train net output #1: loss = 0.482025 (* 1 = 0.482025 loss)
I1122 09:31:59.569939  9524 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1122 09:32:03.829581  9524 solver.cpp:218] Iteration 4800 (23.4786 iter/s, 4.25919s/100 iters), loss = 0.553923
I1122 09:32:03.829581  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 09:32:03.829581  9524 solver.cpp:237]     Train net output #1: loss = 0.553923 (* 1 = 0.553923 loss)
I1122 09:32:03.829581  9524 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1122 09:32:08.089206  9524 solver.cpp:218] Iteration 4900 (23.4757 iter/s, 4.25973s/100 iters), loss = 0.45441
I1122 09:32:08.089206  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 09:32:08.089206  9524 solver.cpp:237]     Train net output #1: loss = 0.45441 (* 1 = 0.45441 loss)
I1122 09:32:08.089206  9524 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1122 09:32:12.140903 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:32:12.308004  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_5000.caffemodel
I1122 09:32:12.318996  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_5000.solverstate
I1122 09:32:12.322996  9524 solver.cpp:330] Iteration 5000, Testing net (#0)
I1122 09:32:12.322996  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:32:13.394218  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:32:13.436218  9524 solver.cpp:397]     Test net output #0: accuracy = 0.7056
I1122 09:32:13.436218  9524 solver.cpp:397]     Test net output #1: loss = 0.860967 (* 1 = 0.860967 loss)
I1122 09:32:13.476274  9524 solver.cpp:218] Iteration 5000 (18.5634 iter/s, 5.38694s/100 iters), loss = 0.468145
I1122 09:32:13.476274  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 09:32:13.476274  9524 solver.cpp:237]     Train net output #1: loss = 0.468145 (* 1 = 0.468145 loss)
I1122 09:32:13.476274  9524 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1122 09:32:13.476274  9524 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1122 09:32:17.750980  9524 solver.cpp:218] Iteration 5100 (23.3971 iter/s, 4.27404s/100 iters), loss = 0.305879
I1122 09:32:17.750980  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 09:32:17.750980  9524 solver.cpp:237]     Train net output #1: loss = 0.305879 (* 1 = 0.305879 loss)
I1122 09:32:17.750980  9524 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1122 09:32:22.023794  9524 solver.cpp:218] Iteration 5200 (23.4083 iter/s, 4.27198s/100 iters), loss = 0.331877
I1122 09:32:22.023794  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:32:22.023794  9524 solver.cpp:237]     Train net output #1: loss = 0.331877 (* 1 = 0.331877 loss)
I1122 09:32:22.023794  9524 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1122 09:32:26.297766  9524 solver.cpp:218] Iteration 5300 (23.3965 iter/s, 4.27414s/100 iters), loss = 0.335329
I1122 09:32:26.297766  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 09:32:26.297766  9524 solver.cpp:237]     Train net output #1: loss = 0.335329 (* 1 = 0.335329 loss)
I1122 09:32:26.297766  9524 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1122 09:32:30.569413  9524 solver.cpp:218] Iteration 5400 (23.4131 iter/s, 4.27111s/100 iters), loss = 0.305123
I1122 09:32:30.569914  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 09:32:30.569914  9524 solver.cpp:237]     Train net output #1: loss = 0.305123 (* 1 = 0.305123 loss)
I1122 09:32:30.569914  9524 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1122 09:32:34.631422 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:32:34.799504  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_5500.caffemodel
I1122 09:32:34.809504  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_5500.solverstate
I1122 09:32:34.813504  9524 solver.cpp:330] Iteration 5500, Testing net (#0)
I1122 09:32:34.813504  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:32:35.886795  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:32:35.928797  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8779
I1122 09:32:35.928797  9524 solver.cpp:397]     Test net output #1: loss = 0.369941 (* 1 = 0.369941 loss)
I1122 09:32:35.970335  9524 solver.cpp:218] Iteration 5500 (18.5182 iter/s, 5.4001s/100 iters), loss = 0.293935
I1122 09:32:35.970335  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 09:32:35.970335  9524 solver.cpp:237]     Train net output #1: loss = 0.293935 (* 1 = 0.293935 loss)
I1122 09:32:35.970335  9524 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1122 09:32:40.247660  9524 solver.cpp:218] Iteration 5600 (23.3787 iter/s, 4.2774s/100 iters), loss = 0.249077
I1122 09:32:40.247660  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:32:40.247660  9524 solver.cpp:237]     Train net output #1: loss = 0.249077 (* 1 = 0.249077 loss)
I1122 09:32:40.247660  9524 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1122 09:32:44.528184  9524 solver.cpp:218] Iteration 5700 (23.3649 iter/s, 4.27993s/100 iters), loss = 0.30979
I1122 09:32:44.528184  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 09:32:44.528184  9524 solver.cpp:237]     Train net output #1: loss = 0.30979 (* 1 = 0.30979 loss)
I1122 09:32:44.528184  9524 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1122 09:32:48.804569  9524 solver.cpp:218] Iteration 5800 (23.3851 iter/s, 4.27623s/100 iters), loss = 0.280154
I1122 09:32:48.804569  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:32:48.804569  9524 solver.cpp:237]     Train net output #1: loss = 0.280154 (* 1 = 0.280154 loss)
I1122 09:32:48.804569  9524 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1122 09:32:53.079176  9524 solver.cpp:218] Iteration 5900 (23.397 iter/s, 4.27405s/100 iters), loss = 0.223089
I1122 09:32:53.079176  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 09:32:53.079176  9524 solver.cpp:237]     Train net output #1: loss = 0.223089 (* 1 = 0.223089 loss)
I1122 09:32:53.079176  9524 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1122 09:32:57.151933 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:32:57.319025  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_6000.caffemodel
I1122 09:32:57.330019  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_6000.solverstate
I1122 09:32:57.334018  9524 solver.cpp:330] Iteration 6000, Testing net (#0)
I1122 09:32:57.334018  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:32:58.404304  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:32:58.446322  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8808
I1122 09:32:58.446322  9524 solver.cpp:397]     Test net output #1: loss = 0.355015 (* 1 = 0.355015 loss)
I1122 09:32:58.487359  9524 solver.cpp:218] Iteration 6000 (18.4919 iter/s, 5.40777s/100 iters), loss = 0.227482
I1122 09:32:58.487359  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:32:58.487359  9524 solver.cpp:237]     Train net output #1: loss = 0.227482 (* 1 = 0.227482 loss)
I1122 09:32:58.487359  9524 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1122 09:33:02.761868  9524 solver.cpp:218] Iteration 6100 (23.3965 iter/s, 4.27414s/100 iters), loss = 0.285135
I1122 09:33:02.761868  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 09:33:02.761868  9524 solver.cpp:237]     Train net output #1: loss = 0.285135 (* 1 = 0.285135 loss)
I1122 09:33:02.761868  9524 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1122 09:33:07.040279  9524 solver.cpp:218] Iteration 6200 (23.3741 iter/s, 4.27824s/100 iters), loss = 0.286276
I1122 09:33:07.040279  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:33:07.040279  9524 solver.cpp:237]     Train net output #1: loss = 0.286276 (* 1 = 0.286276 loss)
I1122 09:33:07.040279  9524 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1122 09:33:11.314770  9524 solver.cpp:218] Iteration 6300 (23.3953 iter/s, 4.27436s/100 iters), loss = 0.225103
I1122 09:33:11.314770  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:33:11.314770  9524 solver.cpp:237]     Train net output #1: loss = 0.225103 (* 1 = 0.225103 loss)
I1122 09:33:11.314770  9524 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1122 09:33:15.594362  9524 solver.cpp:218] Iteration 6400 (23.3727 iter/s, 4.27849s/100 iters), loss = 0.223183
I1122 09:33:15.594362  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 09:33:15.594362  9524 solver.cpp:237]     Train net output #1: loss = 0.223183 (* 1 = 0.223183 loss)
I1122 09:33:15.594362  9524 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1122 09:33:19.657959 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:33:19.827042  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_6500.caffemodel
I1122 09:33:19.840039  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_6500.solverstate
I1122 09:33:19.844040  9524 solver.cpp:330] Iteration 6500, Testing net (#0)
I1122 09:33:19.844040  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:33:20.913357  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:33:20.955358  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8846
I1122 09:33:20.955358  9524 solver.cpp:397]     Test net output #1: loss = 0.345146 (* 1 = 0.345146 loss)
I1122 09:33:20.996397  9524 solver.cpp:218] Iteration 6500 (18.5127 iter/s, 5.40169s/100 iters), loss = 0.239631
I1122 09:33:20.996397  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 09:33:20.996397  9524 solver.cpp:237]     Train net output #1: loss = 0.239631 (* 1 = 0.239631 loss)
I1122 09:33:20.996397  9524 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1122 09:33:25.256800  9524 solver.cpp:218] Iteration 6600 (23.471 iter/s, 4.26058s/100 iters), loss = 0.199823
I1122 09:33:25.256800  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:33:25.256800  9524 solver.cpp:237]     Train net output #1: loss = 0.199823 (* 1 = 0.199823 loss)
I1122 09:33:25.256800  9524 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1122 09:33:29.518425  9524 solver.cpp:218] Iteration 6700 (23.4669 iter/s, 4.26133s/100 iters), loss = 0.238022
I1122 09:33:29.518425  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:33:29.518425  9524 solver.cpp:237]     Train net output #1: loss = 0.238022 (* 1 = 0.238022 loss)
I1122 09:33:29.518425  9524 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1122 09:33:33.781965  9524 solver.cpp:218] Iteration 6800 (23.459 iter/s, 4.26275s/100 iters), loss = 0.264475
I1122 09:33:33.782465  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 09:33:33.782465  9524 solver.cpp:237]     Train net output #1: loss = 0.264475 (* 1 = 0.264475 loss)
I1122 09:33:33.782465  9524 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1122 09:33:38.040526  9524 solver.cpp:218] Iteration 6900 (23.4833 iter/s, 4.25834s/100 iters), loss = 0.176154
I1122 09:33:38.040526  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:33:38.040526  9524 solver.cpp:237]     Train net output #1: loss = 0.176154 (* 1 = 0.176154 loss)
I1122 09:33:38.040526  9524 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1122 09:33:42.093137 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:33:42.259196  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_7000.caffemodel
I1122 09:33:42.292745  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_7000.solverstate
I1122 09:33:42.296744  9524 solver.cpp:330] Iteration 7000, Testing net (#0)
I1122 09:33:42.296744  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:33:43.367547  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:33:43.409607  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8868
I1122 09:33:43.409607  9524 solver.cpp:397]     Test net output #1: loss = 0.337505 (* 1 = 0.337505 loss)
I1122 09:33:43.450610  9524 solver.cpp:218] Iteration 7000 (18.4851 iter/s, 5.40975s/100 iters), loss = 0.229195
I1122 09:33:43.450610  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:33:43.450610  9524 solver.cpp:237]     Train net output #1: loss = 0.229195 (* 1 = 0.229195 loss)
I1122 09:33:43.450610  9524 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1122 09:33:47.729272  9524 solver.cpp:218] Iteration 7100 (23.3763 iter/s, 4.27783s/100 iters), loss = 0.208047
I1122 09:33:47.729272  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:33:47.729272  9524 solver.cpp:237]     Train net output #1: loss = 0.208048 (* 1 = 0.208048 loss)
I1122 09:33:47.729272  9524 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1122 09:33:52.000401  9524 solver.cpp:218] Iteration 7200 (23.4145 iter/s, 4.27087s/100 iters), loss = 0.219585
I1122 09:33:52.000401  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:33:52.000401  9524 solver.cpp:237]     Train net output #1: loss = 0.219585 (* 1 = 0.219585 loss)
I1122 09:33:52.000401  9524 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1122 09:33:56.279469  9524 solver.cpp:218] Iteration 7300 (23.3701 iter/s, 4.27898s/100 iters), loss = 0.217063
I1122 09:33:56.279469  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:33:56.279469  9524 solver.cpp:237]     Train net output #1: loss = 0.217063 (* 1 = 0.217063 loss)
I1122 09:33:56.279469  9524 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1122 09:34:00.559010  9524 solver.cpp:218] Iteration 7400 (23.3724 iter/s, 4.27855s/100 iters), loss = 0.211584
I1122 09:34:00.559010  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 09:34:00.559010  9524 solver.cpp:237]     Train net output #1: loss = 0.211584 (* 1 = 0.211584 loss)
I1122 09:34:00.559010  9524 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1122 09:34:04.624699 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:34:04.791733  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_7500.caffemodel
I1122 09:34:04.802234  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_7500.solverstate
I1122 09:34:04.806735  9524 solver.cpp:330] Iteration 7500, Testing net (#0)
I1122 09:34:04.806735  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:34:05.877811  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:34:05.919878  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8854
I1122 09:34:05.919878  9524 solver.cpp:397]     Test net output #1: loss = 0.337613 (* 1 = 0.337613 loss)
I1122 09:34:05.960891  9524 solver.cpp:218] Iteration 7500 (18.5126 iter/s, 5.40173s/100 iters), loss = 0.207319
I1122 09:34:05.960891  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:34:05.960891  9524 solver.cpp:237]     Train net output #1: loss = 0.207319 (* 1 = 0.207319 loss)
I1122 09:34:05.960891  9524 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1122 09:34:10.235330  9524 solver.cpp:218] Iteration 7600 (23.3952 iter/s, 4.27438s/100 iters), loss = 0.264677
I1122 09:34:10.235330  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 09:34:10.235330  9524 solver.cpp:237]     Train net output #1: loss = 0.264677 (* 1 = 0.264677 loss)
I1122 09:34:10.235330  9524 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1122 09:34:14.512807  9524 solver.cpp:218] Iteration 7700 (23.3795 iter/s, 4.27725s/100 iters), loss = 0.206917
I1122 09:34:14.512807  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 09:34:14.512807  9524 solver.cpp:237]     Train net output #1: loss = 0.206917 (* 1 = 0.206917 loss)
I1122 09:34:14.512807  9524 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1122 09:34:18.786377  9524 solver.cpp:218] Iteration 7800 (23.4059 iter/s, 4.27243s/100 iters), loss = 0.251373
I1122 09:34:18.786377  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 09:34:18.786377  9524 solver.cpp:237]     Train net output #1: loss = 0.251373 (* 1 = 0.251373 loss)
I1122 09:34:18.786377  9524 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1122 09:34:23.062829  9524 solver.cpp:218] Iteration 7900 (23.3854 iter/s, 4.27618s/100 iters), loss = 0.165504
I1122 09:34:23.062829  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:34:23.062829  9524 solver.cpp:237]     Train net output #1: loss = 0.165504 (* 1 = 0.165504 loss)
I1122 09:34:23.062829  9524 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1122 09:34:27.134412 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:34:27.302467  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_8000.caffemodel
I1122 09:34:27.341507  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_8000.solverstate
I1122 09:34:27.345506  9524 solver.cpp:330] Iteration 8000, Testing net (#0)
I1122 09:34:27.345506  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:34:28.414788  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:34:28.455852  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8822
I1122 09:34:28.455852  9524 solver.cpp:397]     Test net output #1: loss = 0.345782 (* 1 = 0.345782 loss)
I1122 09:34:28.497853  9524 solver.cpp:218] Iteration 8000 (18.4009 iter/s, 5.43452s/100 iters), loss = 0.215172
I1122 09:34:28.497853  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 09:34:28.497853  9524 solver.cpp:237]     Train net output #1: loss = 0.215172 (* 1 = 0.215172 loss)
I1122 09:34:28.497853  9524 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1122 09:34:32.776278  9524 solver.cpp:218] Iteration 8100 (23.3707 iter/s, 4.27886s/100 iters), loss = 0.217106
I1122 09:34:32.777278  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:34:32.777278  9524 solver.cpp:237]     Train net output #1: loss = 0.217106 (* 1 = 0.217106 loss)
I1122 09:34:32.777278  9524 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1122 09:34:37.054368  9524 solver.cpp:218] Iteration 8200 (23.3782 iter/s, 4.27749s/100 iters), loss = 0.203814
I1122 09:34:37.054368  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:34:37.054368  9524 solver.cpp:237]     Train net output #1: loss = 0.203814 (* 1 = 0.203814 loss)
I1122 09:34:37.054368  9524 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1122 09:34:41.324697  9524 solver.cpp:218] Iteration 8300 (23.4198 iter/s, 4.26989s/100 iters), loss = 0.227465
I1122 09:34:41.324697  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:34:41.324697  9524 solver.cpp:237]     Train net output #1: loss = 0.227465 (* 1 = 0.227465 loss)
I1122 09:34:41.324697  9524 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1122 09:34:45.603348  9524 solver.cpp:218] Iteration 8400 (23.3758 iter/s, 4.27793s/100 iters), loss = 0.188037
I1122 09:34:45.603348  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:34:45.603348  9524 solver.cpp:237]     Train net output #1: loss = 0.188037 (* 1 = 0.188037 loss)
I1122 09:34:45.603348  9524 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1122 09:34:49.665874 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:34:49.833936  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_8500.caffemodel
I1122 09:34:49.843937  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_8500.solverstate
I1122 09:34:49.847937  9524 solver.cpp:330] Iteration 8500, Testing net (#0)
I1122 09:34:49.847937  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:34:50.918987  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:34:50.961000  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8824
I1122 09:34:50.961000  9524 solver.cpp:397]     Test net output #1: loss = 0.340254 (* 1 = 0.340254 loss)
I1122 09:34:51.002493  9524 solver.cpp:218] Iteration 8500 (18.5233 iter/s, 5.39859s/100 iters), loss = 0.243972
I1122 09:34:51.002493  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 09:34:51.002493  9524 solver.cpp:237]     Train net output #1: loss = 0.243972 (* 1 = 0.243972 loss)
I1122 09:34:51.002493  9524 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1122 09:34:55.263178  9524 solver.cpp:218] Iteration 8600 (23.4726 iter/s, 4.26029s/100 iters), loss = 0.231314
I1122 09:34:55.263178  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:34:55.263178  9524 solver.cpp:237]     Train net output #1: loss = 0.231314 (* 1 = 0.231314 loss)
I1122 09:34:55.263178  9524 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1122 09:34:59.524632  9524 solver.cpp:218] Iteration 8700 (23.4697 iter/s, 4.26082s/100 iters), loss = 0.214707
I1122 09:34:59.524632  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 09:34:59.524632  9524 solver.cpp:237]     Train net output #1: loss = 0.214707 (* 1 = 0.214707 loss)
I1122 09:34:59.524632  9524 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1122 09:35:03.784083  9524 solver.cpp:218] Iteration 8800 (23.4774 iter/s, 4.25942s/100 iters), loss = 0.231367
I1122 09:35:03.784083  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 09:35:03.784083  9524 solver.cpp:237]     Train net output #1: loss = 0.231367 (* 1 = 0.231367 loss)
I1122 09:35:03.784083  9524 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1122 09:35:08.044631  9524 solver.cpp:218] Iteration 8900 (23.4741 iter/s, 4.26002s/100 iters), loss = 0.139695
I1122 09:35:08.044631  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:35:08.044631  9524 solver.cpp:237]     Train net output #1: loss = 0.139695 (* 1 = 0.139695 loss)
I1122 09:35:08.044631  9524 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1122 09:35:12.092221 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:35:12.260299  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_9000.caffemodel
I1122 09:35:12.291335  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_9000.solverstate
I1122 09:35:12.295334  9524 solver.cpp:330] Iteration 9000, Testing net (#0)
I1122 09:35:12.295334  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:35:13.366856  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:35:13.408859  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8873
I1122 09:35:13.408859  9524 solver.cpp:397]     Test net output #1: loss = 0.332359 (* 1 = 0.332359 loss)
I1122 09:35:13.449857  9524 solver.cpp:218] Iteration 9000 (18.5016 iter/s, 5.40493s/100 iters), loss = 0.193988
I1122 09:35:13.449857  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:35:13.449857  9524 solver.cpp:237]     Train net output #1: loss = 0.193988 (* 1 = 0.193988 loss)
I1122 09:35:13.449857  9524 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1122 09:35:17.729092  9524 solver.cpp:218] Iteration 9100 (23.3725 iter/s, 4.27853s/100 iters), loss = 0.197714
I1122 09:35:17.729092  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:35:17.729092  9524 solver.cpp:237]     Train net output #1: loss = 0.197714 (* 1 = 0.197714 loss)
I1122 09:35:17.729092  9524 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1122 09:35:22.010159  9524 solver.cpp:218] Iteration 9200 (23.3607 iter/s, 4.2807s/100 iters), loss = 0.199693
I1122 09:35:22.010159  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:35:22.010159  9524 solver.cpp:237]     Train net output #1: loss = 0.199694 (* 1 = 0.199694 loss)
I1122 09:35:22.010159  9524 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1122 09:35:26.290834  9524 solver.cpp:218] Iteration 9300 (23.359 iter/s, 4.281s/100 iters), loss = 0.211459
I1122 09:35:26.290834  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 09:35:26.290834  9524 solver.cpp:237]     Train net output #1: loss = 0.211459 (* 1 = 0.211459 loss)
I1122 09:35:26.290834  9524 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1122 09:35:30.575141  9524 solver.cpp:218] Iteration 9400 (23.3462 iter/s, 4.28335s/100 iters), loss = 0.163519
I1122 09:35:30.575141  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:35:30.575141  9524 solver.cpp:237]     Train net output #1: loss = 0.163519 (* 1 = 0.163519 loss)
I1122 09:35:30.575141  9524 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1122 09:35:34.648212 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:35:34.816294  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_9500.caffemodel
I1122 09:35:34.827270  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_9500.solverstate
I1122 09:35:34.831271  9524 solver.cpp:330] Iteration 9500, Testing net (#0)
I1122 09:35:34.831271  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:35:35.902187  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:35:35.944190  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8891
I1122 09:35:35.944190  9524 solver.cpp:397]     Test net output #1: loss = 0.330861 (* 1 = 0.330861 loss)
I1122 09:35:35.984725  9524 solver.cpp:218] Iteration 9500 (18.4855 iter/s, 5.40965s/100 iters), loss = 0.222647
I1122 09:35:35.984725  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 09:35:35.984725  9524 solver.cpp:237]     Train net output #1: loss = 0.222648 (* 1 = 0.222648 loss)
I1122 09:35:35.984725  9524 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1122 09:35:35.984725  9524 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1122 09:35:40.243459  9524 solver.cpp:218] Iteration 9600 (23.4857 iter/s, 4.25791s/100 iters), loss = 0.1641
I1122 09:35:40.243459  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:35:40.243459  9524 solver.cpp:237]     Train net output #1: loss = 0.164101 (* 1 = 0.164101 loss)
I1122 09:35:40.243459  9524 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1122 09:35:44.501257  9524 solver.cpp:218] Iteration 9700 (23.4896 iter/s, 4.2572s/100 iters), loss = 0.146797
I1122 09:35:44.501257  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:35:44.501257  9524 solver.cpp:237]     Train net output #1: loss = 0.146797 (* 1 = 0.146797 loss)
I1122 09:35:44.501257  9524 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1122 09:35:48.761358  9524 solver.cpp:218] Iteration 9800 (23.4707 iter/s, 4.26064s/100 iters), loss = 0.208339
I1122 09:35:48.762363  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:35:48.762363  9524 solver.cpp:237]     Train net output #1: loss = 0.20834 (* 1 = 0.20834 loss)
I1122 09:35:48.762363  9524 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1122 09:35:53.015940  9524 solver.cpp:218] Iteration 9900 (23.5113 iter/s, 4.25327s/100 iters), loss = 0.141965
I1122 09:35:53.015940  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:35:53.015940  9524 solver.cpp:237]     Train net output #1: loss = 0.141965 (* 1 = 0.141965 loss)
I1122 09:35:53.015940  9524 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1122 09:35:57.061406 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:35:57.229463  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_10000.caffemodel
I1122 09:35:57.266479  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_10000.solverstate
I1122 09:35:57.270478  9524 solver.cpp:330] Iteration 10000, Testing net (#0)
I1122 09:35:57.270478  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:35:58.338201  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:35:58.381750  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8961
I1122 09:35:58.381750  9524 solver.cpp:397]     Test net output #1: loss = 0.300748 (* 1 = 0.300748 loss)
I1122 09:35:58.422752  9524 solver.cpp:218] Iteration 10000 (18.495 iter/s, 5.40686s/100 iters), loss = 0.162571
I1122 09:35:58.422752  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:35:58.422752  9524 solver.cpp:237]     Train net output #1: loss = 0.162571 (* 1 = 0.162571 loss)
I1122 09:35:58.422752  9524 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1122 09:36:02.705101  9524 solver.cpp:218] Iteration 10100 (23.3547 iter/s, 4.2818s/100 iters), loss = 0.148543
I1122 09:36:02.705101  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:36:02.705101  9524 solver.cpp:237]     Train net output #1: loss = 0.148543 (* 1 = 0.148543 loss)
I1122 09:36:02.705101  9524 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1122 09:36:06.981803  9524 solver.cpp:218] Iteration 10200 (23.386 iter/s, 4.27606s/100 iters), loss = 0.183796
I1122 09:36:06.981803  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:36:06.981803  9524 solver.cpp:237]     Train net output #1: loss = 0.183796 (* 1 = 0.183796 loss)
I1122 09:36:06.981803  9524 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1122 09:36:11.256530  9524 solver.cpp:218] Iteration 10300 (23.3908 iter/s, 4.27519s/100 iters), loss = 0.197263
I1122 09:36:11.256530  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:36:11.256530  9524 solver.cpp:237]     Train net output #1: loss = 0.197263 (* 1 = 0.197263 loss)
I1122 09:36:11.256530  9524 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1122 09:36:15.529151  9524 solver.cpp:218] Iteration 10400 (23.4075 iter/s, 4.27214s/100 iters), loss = 0.116475
I1122 09:36:15.529151  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:36:15.529151  9524 solver.cpp:237]     Train net output #1: loss = 0.116475 (* 1 = 0.116475 loss)
I1122 09:36:15.529151  9524 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1122 09:36:19.596623 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:36:19.764714  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_10500.caffemodel
I1122 09:36:19.774718  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_10500.solverstate
I1122 09:36:19.778718  9524 solver.cpp:330] Iteration 10500, Testing net (#0)
I1122 09:36:19.778718  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:36:20.851069  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:36:20.893077  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8993
I1122 09:36:20.893077  9524 solver.cpp:397]     Test net output #1: loss = 0.298683 (* 1 = 0.298683 loss)
I1122 09:36:20.934088  9524 solver.cpp:218] Iteration 10500 (18.5034 iter/s, 5.40442s/100 iters), loss = 0.162829
I1122 09:36:20.934088  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:36:20.934088  9524 solver.cpp:237]     Train net output #1: loss = 0.162829 (* 1 = 0.162829 loss)
I1122 09:36:20.934088  9524 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1122 09:36:25.211812  9524 solver.cpp:218] Iteration 10600 (23.3797 iter/s, 4.27721s/100 iters), loss = 0.19133
I1122 09:36:25.211812  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:36:25.211812  9524 solver.cpp:237]     Train net output #1: loss = 0.19133 (* 1 = 0.19133 loss)
I1122 09:36:25.211812  9524 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1122 09:36:29.496366  9524 solver.cpp:218] Iteration 10700 (23.3438 iter/s, 4.28379s/100 iters), loss = 0.156025
I1122 09:36:29.496366  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:36:29.496366  9524 solver.cpp:237]     Train net output #1: loss = 0.156025 (* 1 = 0.156025 loss)
I1122 09:36:29.496366  9524 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1122 09:36:33.784571  9524 solver.cpp:218] Iteration 10800 (23.3187 iter/s, 4.28841s/100 iters), loss = 0.218227
I1122 09:36:33.784571  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 09:36:33.784571  9524 solver.cpp:237]     Train net output #1: loss = 0.218227 (* 1 = 0.218227 loss)
I1122 09:36:33.784571  9524 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1122 09:36:38.060128  9524 solver.cpp:218] Iteration 10900 (23.3936 iter/s, 4.27468s/100 iters), loss = 0.146407
I1122 09:36:38.060128  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:36:38.060128  9524 solver.cpp:237]     Train net output #1: loss = 0.146407 (* 1 = 0.146407 loss)
I1122 09:36:38.060128  9524 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1122 09:36:42.120199 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:36:42.288329  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_11000.caffemodel
I1122 09:36:42.333302  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_11000.solverstate
I1122 09:36:42.337317  9524 solver.cpp:330] Iteration 11000, Testing net (#0)
I1122 09:36:42.337317  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:36:43.408711  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:36:43.451231  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8991
I1122 09:36:43.451751  9524 solver.cpp:397]     Test net output #1: loss = 0.298091 (* 1 = 0.298091 loss)
I1122 09:36:43.492761  9524 solver.cpp:218] Iteration 11000 (18.4083 iter/s, 5.43234s/100 iters), loss = 0.142238
I1122 09:36:43.492761  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:36:43.492761  9524 solver.cpp:237]     Train net output #1: loss = 0.142238 (* 1 = 0.142238 loss)
I1122 09:36:43.492761  9524 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1122 09:36:47.769551  9524 solver.cpp:218] Iteration 11100 (23.3822 iter/s, 4.27675s/100 iters), loss = 0.171134
I1122 09:36:47.769551  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:36:47.769551  9524 solver.cpp:237]     Train net output #1: loss = 0.171134 (* 1 = 0.171134 loss)
I1122 09:36:47.769551  9524 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1122 09:36:52.047878  9524 solver.cpp:218] Iteration 11200 (23.3763 iter/s, 4.27783s/100 iters), loss = 0.148937
I1122 09:36:52.047878  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:36:52.047878  9524 solver.cpp:237]     Train net output #1: loss = 0.148937 (* 1 = 0.148937 loss)
I1122 09:36:52.047878  9524 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1122 09:36:56.320415  9524 solver.cpp:218] Iteration 11300 (23.4095 iter/s, 4.27176s/100 iters), loss = 0.160457
I1122 09:36:56.320415  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:36:56.320415  9524 solver.cpp:237]     Train net output #1: loss = 0.160457 (* 1 = 0.160457 loss)
I1122 09:36:56.320415  9524 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1122 09:37:00.595947  9524 solver.cpp:218] Iteration 11400 (23.3933 iter/s, 4.27473s/100 iters), loss = 0.125549
I1122 09:37:00.595947  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:37:00.595947  9524 solver.cpp:237]     Train net output #1: loss = 0.125549 (* 1 = 0.125549 loss)
I1122 09:37:00.595947  9524 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1122 09:37:04.659545 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:37:04.827632  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_11500.caffemodel
I1122 09:37:04.838642  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_11500.solverstate
I1122 09:37:04.842641  9524 solver.cpp:330] Iteration 11500, Testing net (#0)
I1122 09:37:04.842641  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:37:05.913914  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:37:05.955931  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8984
I1122 09:37:05.955931  9524 solver.cpp:397]     Test net output #1: loss = 0.298491 (* 1 = 0.298491 loss)
I1122 09:37:05.997932  9524 solver.cpp:218] Iteration 11500 (18.5132 iter/s, 5.40156s/100 iters), loss = 0.217475
I1122 09:37:05.997932  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:37:05.997932  9524 solver.cpp:237]     Train net output #1: loss = 0.217475 (* 1 = 0.217475 loss)
I1122 09:37:05.997932  9524 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1122 09:37:10.260731  9524 solver.cpp:218] Iteration 11600 (23.4566 iter/s, 4.2632s/100 iters), loss = 0.180937
I1122 09:37:10.260731  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 09:37:10.260731  9524 solver.cpp:237]     Train net output #1: loss = 0.180937 (* 1 = 0.180937 loss)
I1122 09:37:10.260731  9524 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1122 09:37:14.520263  9524 solver.cpp:218] Iteration 11700 (23.4823 iter/s, 4.25853s/100 iters), loss = 0.130184
I1122 09:37:14.520263  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:37:14.520263  9524 solver.cpp:237]     Train net output #1: loss = 0.130184 (* 1 = 0.130184 loss)
I1122 09:37:14.520263  9524 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1122 09:37:18.776558  9524 solver.cpp:218] Iteration 11800 (23.4937 iter/s, 4.25645s/100 iters), loss = 0.157404
I1122 09:37:18.776558  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:37:18.776558  9524 solver.cpp:237]     Train net output #1: loss = 0.157404 (* 1 = 0.157404 loss)
I1122 09:37:18.776558  9524 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1122 09:37:23.042079  9524 solver.cpp:218] Iteration 11900 (23.4475 iter/s, 4.26485s/100 iters), loss = 0.123738
I1122 09:37:23.042079  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:37:23.042079  9524 solver.cpp:237]     Train net output #1: loss = 0.123738 (* 1 = 0.123738 loss)
I1122 09:37:23.042079  9524 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1122 09:37:27.098670 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:37:27.265740  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_12000.caffemodel
I1122 09:37:27.307744  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_12000.solverstate
I1122 09:37:27.311743  9524 solver.cpp:330] Iteration 12000, Testing net (#0)
I1122 09:37:27.311743  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:37:28.384317  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:37:28.425827  9524 solver.cpp:397]     Test net output #0: accuracy = 0.9003
I1122 09:37:28.426322  9524 solver.cpp:397]     Test net output #1: loss = 0.297922 (* 1 = 0.297922 loss)
I1122 09:37:28.467340  9524 solver.cpp:218] Iteration 12000 (18.4356 iter/s, 5.42428s/100 iters), loss = 0.195492
I1122 09:37:28.467340  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:37:28.467340  9524 solver.cpp:237]     Train net output #1: loss = 0.195492 (* 1 = 0.195492 loss)
I1122 09:37:28.467340  9524 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1122 09:37:32.744158  9524 solver.cpp:218] Iteration 12100 (23.3797 iter/s, 4.27722s/100 iters), loss = 0.190131
I1122 09:37:32.744158  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:37:32.744158  9524 solver.cpp:237]     Train net output #1: loss = 0.190131 (* 1 = 0.190131 loss)
I1122 09:37:32.744158  9524 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1122 09:37:37.019665  9524 solver.cpp:218] Iteration 12200 (23.3926 iter/s, 4.27485s/100 iters), loss = 0.146501
I1122 09:37:37.019665  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:37:37.019665  9524 solver.cpp:237]     Train net output #1: loss = 0.146501 (* 1 = 0.146501 loss)
I1122 09:37:37.019665  9524 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1122 09:37:41.297519  9524 solver.cpp:218] Iteration 12300 (23.381 iter/s, 4.27698s/100 iters), loss = 0.156204
I1122 09:37:41.297519  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:37:41.297519  9524 solver.cpp:237]     Train net output #1: loss = 0.156204 (* 1 = 0.156204 loss)
I1122 09:37:41.297519  9524 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1122 09:37:45.573217  9524 solver.cpp:218] Iteration 12400 (23.3847 iter/s, 4.27631s/100 iters), loss = 0.114675
I1122 09:37:45.574231  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:37:45.574231  9524 solver.cpp:237]     Train net output #1: loss = 0.114675 (* 1 = 0.114675 loss)
I1122 09:37:45.574231  9524 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1122 09:37:49.643579 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:37:49.812101  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_12500.caffemodel
I1122 09:37:49.821107  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_12500.solverstate
I1122 09:37:49.825106  9524 solver.cpp:330] Iteration 12500, Testing net (#0)
I1122 09:37:49.825106  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:37:50.897363  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:37:50.939376  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8988
I1122 09:37:50.939884  9524 solver.cpp:397]     Test net output #1: loss = 0.298552 (* 1 = 0.298552 loss)
I1122 09:37:50.980382  9524 solver.cpp:218] Iteration 12500 (18.4971 iter/s, 5.40625s/100 iters), loss = 0.145165
I1122 09:37:50.980382  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:37:50.980382  9524 solver.cpp:237]     Train net output #1: loss = 0.145165 (* 1 = 0.145165 loss)
I1122 09:37:50.980382  9524 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1122 09:37:55.258031  9524 solver.cpp:218] Iteration 12600 (23.3776 iter/s, 4.2776s/100 iters), loss = 0.176254
I1122 09:37:55.258031  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:37:55.258031  9524 solver.cpp:237]     Train net output #1: loss = 0.176254 (* 1 = 0.176254 loss)
I1122 09:37:55.258031  9524 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1122 09:37:59.537662  9524 solver.cpp:218] Iteration 12700 (23.3714 iter/s, 4.27873s/100 iters), loss = 0.14292
I1122 09:37:59.537662  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:37:59.538162  9524 solver.cpp:237]     Train net output #1: loss = 0.14292 (* 1 = 0.14292 loss)
I1122 09:37:59.538162  9524 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1122 09:38:03.822221  9524 solver.cpp:218] Iteration 12800 (23.3431 iter/s, 4.28392s/100 iters), loss = 0.198376
I1122 09:38:03.822221  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:38:03.822221  9524 solver.cpp:237]     Train net output #1: loss = 0.198376 (* 1 = 0.198376 loss)
I1122 09:38:03.822221  9524 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1122 09:38:08.104863  9524 solver.cpp:218] Iteration 12900 (23.3492 iter/s, 4.2828s/100 iters), loss = 0.132309
I1122 09:38:08.104863  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:38:08.104863  9524 solver.cpp:237]     Train net output #1: loss = 0.132309 (* 1 = 0.132309 loss)
I1122 09:38:08.104863  9524 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1122 09:38:12.177418 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:38:12.346487  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_13000.caffemodel
I1122 09:38:12.383493  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_13000.solverstate
I1122 09:38:12.387487  9524 solver.cpp:330] Iteration 13000, Testing net (#0)
I1122 09:38:12.387487  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:38:13.456416  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:38:13.499439  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8991
I1122 09:38:13.499439  9524 solver.cpp:397]     Test net output #1: loss = 0.297466 (* 1 = 0.297466 loss)
I1122 09:38:13.540449  9524 solver.cpp:218] Iteration 13000 (18.3995 iter/s, 5.43493s/100 iters), loss = 0.199205
I1122 09:38:13.540449  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:38:13.540951  9524 solver.cpp:237]     Train net output #1: loss = 0.199205 (* 1 = 0.199205 loss)
I1122 09:38:13.540951  9524 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1122 09:38:17.822499  9524 solver.cpp:218] Iteration 13100 (23.353 iter/s, 4.2821s/100 iters), loss = 0.19054
I1122 09:38:17.822499  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:38:17.822499  9524 solver.cpp:237]     Train net output #1: loss = 0.19054 (* 1 = 0.19054 loss)
I1122 09:38:17.822499  9524 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1122 09:38:22.101207  9524 solver.cpp:218] Iteration 13200 (23.376 iter/s, 4.2779s/100 iters), loss = 0.104355
I1122 09:38:22.101207  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 09:38:22.101207  9524 solver.cpp:237]     Train net output #1: loss = 0.104355 (* 1 = 0.104355 loss)
I1122 09:38:22.101207  9524 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1122 09:38:26.373771  9524 solver.cpp:218] Iteration 13300 (23.4084 iter/s, 4.27198s/100 iters), loss = 0.166038
I1122 09:38:26.373771  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:38:26.373771  9524 solver.cpp:237]     Train net output #1: loss = 0.166038 (* 1 = 0.166038 loss)
I1122 09:38:26.373771  9524 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1122 09:38:30.652530  9524 solver.cpp:218] Iteration 13400 (23.375 iter/s, 4.27808s/100 iters), loss = 0.0720518
I1122 09:38:30.652530  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 09:38:30.652530  9524 solver.cpp:237]     Train net output #1: loss = 0.0720517 (* 1 = 0.0720517 loss)
I1122 09:38:30.652530  9524 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1122 09:38:34.722816 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:38:34.890899  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_13500.caffemodel
I1122 09:38:34.901875  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_13500.solverstate
I1122 09:38:34.905875  9524 solver.cpp:330] Iteration 13500, Testing net (#0)
I1122 09:38:34.905875  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:38:35.975813  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:38:36.016875  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8985
I1122 09:38:36.016875  9524 solver.cpp:397]     Test net output #1: loss = 0.297837 (* 1 = 0.297837 loss)
I1122 09:38:36.056888  9524 solver.cpp:218] Iteration 13500 (18.5023 iter/s, 5.40472s/100 iters), loss = 0.148704
I1122 09:38:36.056888  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:38:36.056888  9524 solver.cpp:237]     Train net output #1: loss = 0.148704 (* 1 = 0.148704 loss)
I1122 09:38:36.057888  9524 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1122 09:38:40.318874  9524 solver.cpp:218] Iteration 13600 (23.466 iter/s, 4.26149s/100 iters), loss = 0.151058
I1122 09:38:40.318874  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:38:40.318874  9524 solver.cpp:237]     Train net output #1: loss = 0.151058 (* 1 = 0.151058 loss)
I1122 09:38:40.318874  9524 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1122 09:38:44.577487  9524 solver.cpp:218] Iteration 13700 (23.4874 iter/s, 4.25761s/100 iters), loss = 0.153522
I1122 09:38:44.577487  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:38:44.577487  9524 solver.cpp:237]     Train net output #1: loss = 0.153522 (* 1 = 0.153522 loss)
I1122 09:38:44.577487  9524 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1122 09:38:48.844131  9524 solver.cpp:218] Iteration 13800 (23.4398 iter/s, 4.26626s/100 iters), loss = 0.168042
I1122 09:38:48.844131  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:38:48.844131  9524 solver.cpp:237]     Train net output #1: loss = 0.168042 (* 1 = 0.168042 loss)
I1122 09:38:48.844131  9524 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1122 09:38:53.103749  9524 solver.cpp:218] Iteration 13900 (23.4768 iter/s, 4.25953s/100 iters), loss = 0.118966
I1122 09:38:53.103749  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:38:53.103749  9524 solver.cpp:237]     Train net output #1: loss = 0.118966 (* 1 = 0.118966 loss)
I1122 09:38:53.103749  9524 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1122 09:38:57.157310 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:38:57.325376  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_14000.caffemodel
I1122 09:38:57.360352  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_14000.solverstate
I1122 09:38:57.364359  9524 solver.cpp:330] Iteration 14000, Testing net (#0)
I1122 09:38:57.364359  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:38:58.431689  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:38:58.473717  9524 solver.cpp:397]     Test net output #0: accuracy = 0.9004
I1122 09:38:58.473717  9524 solver.cpp:397]     Test net output #1: loss = 0.297327 (* 1 = 0.297327 loss)
I1122 09:38:58.514741  9524 solver.cpp:218] Iteration 14000 (18.481 iter/s, 5.41097s/100 iters), loss = 0.142806
I1122 09:38:58.514741  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:38:58.514741  9524 solver.cpp:237]     Train net output #1: loss = 0.142806 (* 1 = 0.142806 loss)
I1122 09:38:58.514741  9524 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1122 09:39:02.793390  9524 solver.cpp:218] Iteration 14100 (23.373 iter/s, 4.27843s/100 iters), loss = 0.191999
I1122 09:39:02.794384  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:39:02.794384  9524 solver.cpp:237]     Train net output #1: loss = 0.191999 (* 1 = 0.191999 loss)
I1122 09:39:02.794384  9524 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1122 09:39:07.072957  9524 solver.cpp:218] Iteration 14200 (23.3727 iter/s, 4.2785s/100 iters), loss = 0.142528
I1122 09:39:07.072957  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:39:07.072957  9524 solver.cpp:237]     Train net output #1: loss = 0.142528 (* 1 = 0.142528 loss)
I1122 09:39:07.072957  9524 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1122 09:39:11.349488  9524 solver.cpp:218] Iteration 14300 (23.3869 iter/s, 4.27589s/100 iters), loss = 0.153663
I1122 09:39:11.349488  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:39:11.349488  9524 solver.cpp:237]     Train net output #1: loss = 0.153663 (* 1 = 0.153663 loss)
I1122 09:39:11.349488  9524 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1122 09:39:15.633112  9524 solver.cpp:218] Iteration 14400 (23.3432 iter/s, 4.28391s/100 iters), loss = 0.0825814
I1122 09:39:15.633112  9524 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 09:39:15.633112  9524 solver.cpp:237]     Train net output #1: loss = 0.0825814 (* 1 = 0.0825814 loss)
I1122 09:39:15.633112  9524 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1122 09:39:19.703727 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:39:19.871805  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_14500.caffemodel
I1122 09:39:19.881790  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_14500.solverstate
I1122 09:39:19.885808  9524 solver.cpp:330] Iteration 14500, Testing net (#0)
I1122 09:39:19.885808  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:39:20.958107  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:39:21.000108  9524 solver.cpp:397]     Test net output #0: accuracy = 0.9005
I1122 09:39:21.000108  9524 solver.cpp:397]     Test net output #1: loss = 0.297516 (* 1 = 0.297516 loss)
I1122 09:39:21.041122  9524 solver.cpp:218] Iteration 14500 (18.4923 iter/s, 5.40766s/100 iters), loss = 0.160718
I1122 09:39:21.041122  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:39:21.041122  9524 solver.cpp:237]     Train net output #1: loss = 0.160718 (* 1 = 0.160718 loss)
I1122 09:39:21.041122  9524 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1122 09:39:25.300814  9524 solver.cpp:218] Iteration 14600 (23.4775 iter/s, 4.2594s/100 iters), loss = 0.179697
I1122 09:39:25.300814  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:39:25.300814  9524 solver.cpp:237]     Train net output #1: loss = 0.179697 (* 1 = 0.179697 loss)
I1122 09:39:25.300814  9524 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1122 09:39:29.561559  9524 solver.cpp:218] Iteration 14700 (23.4763 iter/s, 4.25962s/100 iters), loss = 0.158679
I1122 09:39:29.561559  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:39:29.561559  9524 solver.cpp:237]     Train net output #1: loss = 0.158679 (* 1 = 0.158679 loss)
I1122 09:39:29.561559  9524 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1122 09:39:33.826278  9524 solver.cpp:218] Iteration 14800 (23.4481 iter/s, 4.26475s/100 iters), loss = 0.194494
I1122 09:39:33.826278  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 09:39:33.826278  9524 solver.cpp:237]     Train net output #1: loss = 0.194494 (* 1 = 0.194494 loss)
I1122 09:39:33.826278  9524 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1122 09:39:38.088013  9524 solver.cpp:218] Iteration 14900 (23.4642 iter/s, 4.2618s/100 iters), loss = 0.141075
I1122 09:39:38.089013  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:39:38.089013  9524 solver.cpp:237]     Train net output #1: loss = 0.141075 (* 1 = 0.141075 loss)
I1122 09:39:38.089013  9524 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1122 09:39:42.127358 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:39:42.292933  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_15000.caffemodel
I1122 09:39:42.324934  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_15000.solverstate
I1122 09:39:42.328433  9524 solver.cpp:330] Iteration 15000, Testing net (#0)
I1122 09:39:42.328433  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:39:43.399122  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:39:43.441148  9524 solver.cpp:397]     Test net output #0: accuracy = 0.9006
I1122 09:39:43.441148  9524 solver.cpp:397]     Test net output #1: loss = 0.296575 (* 1 = 0.296575 loss)
I1122 09:39:43.482178  9524 solver.cpp:218] Iteration 15000 (18.5423 iter/s, 5.39306s/100 iters), loss = 0.144442
I1122 09:39:43.482178  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:39:43.482178  9524 solver.cpp:237]     Train net output #1: loss = 0.144442 (* 1 = 0.144442 loss)
I1122 09:39:43.482178  9524 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1122 09:39:47.755990  9524 solver.cpp:218] Iteration 15100 (23.4021 iter/s, 4.27313s/100 iters), loss = 0.147297
I1122 09:39:47.755990  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:39:47.755990  9524 solver.cpp:237]     Train net output #1: loss = 0.147297 (* 1 = 0.147297 loss)
I1122 09:39:47.755990  9524 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1122 09:39:52.030596  9524 solver.cpp:218] Iteration 15200 (23.3912 iter/s, 4.27511s/100 iters), loss = 0.141874
I1122 09:39:52.030596  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:39:52.031594  9524 solver.cpp:237]     Train net output #1: loss = 0.141874 (* 1 = 0.141874 loss)
I1122 09:39:52.031594  9524 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1122 09:39:56.306334  9524 solver.cpp:218] Iteration 15300 (23.3947 iter/s, 4.27447s/100 iters), loss = 0.15042
I1122 09:39:56.306334  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:39:56.306334  9524 solver.cpp:237]     Train net output #1: loss = 0.15042 (* 1 = 0.15042 loss)
I1122 09:39:56.306334  9524 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1122 09:39:56.306334  9524 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1122 09:40:00.583904  9524 solver.cpp:218] Iteration 15400 (23.3757 iter/s, 4.27794s/100 iters), loss = 0.124837
I1122 09:40:00.583904  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:40:00.583904  9524 solver.cpp:237]     Train net output #1: loss = 0.124837 (* 1 = 0.124837 loss)
I1122 09:40:00.583904  9524 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1122 09:40:04.650017 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:40:04.818173  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_15500.caffemodel
I1122 09:40:04.828168  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_15500.solverstate
I1122 09:40:04.832149  9524 solver.cpp:330] Iteration 15500, Testing net (#0)
I1122 09:40:04.832149  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:40:05.904413  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:40:05.945427  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8993
I1122 09:40:05.945427  9524 solver.cpp:397]     Test net output #1: loss = 0.297453 (* 1 = 0.297453 loss)
I1122 09:40:05.986451  9524 solver.cpp:218] Iteration 15500 (18.5116 iter/s, 5.40201s/100 iters), loss = 0.123229
I1122 09:40:05.986451  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:40:05.986451  9524 solver.cpp:237]     Train net output #1: loss = 0.123229 (* 1 = 0.123229 loss)
I1122 09:40:05.986451  9524 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1122 09:40:10.269094  9524 solver.cpp:218] Iteration 15600 (23.3559 iter/s, 4.28158s/100 iters), loss = 0.173532
I1122 09:40:10.269094  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:40:10.269094  9524 solver.cpp:237]     Train net output #1: loss = 0.173532 (* 1 = 0.173532 loss)
I1122 09:40:10.269094  9524 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1122 09:40:14.545689  9524 solver.cpp:218] Iteration 15700 (23.3842 iter/s, 4.2764s/100 iters), loss = 0.115383
I1122 09:40:14.545689  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:40:14.545689  9524 solver.cpp:237]     Train net output #1: loss = 0.115383 (* 1 = 0.115383 loss)
I1122 09:40:14.545689  9524 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1122 09:40:18.828261  9524 solver.cpp:218] Iteration 15800 (23.3535 iter/s, 4.28201s/100 iters), loss = 0.145351
I1122 09:40:18.828261  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:40:18.828261  9524 solver.cpp:237]     Train net output #1: loss = 0.145351 (* 1 = 0.145351 loss)
I1122 09:40:18.828261  9524 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1122 09:40:23.110142  9524 solver.cpp:218] Iteration 15900 (23.3541 iter/s, 4.2819s/100 iters), loss = 0.0890839
I1122 09:40:23.110142  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 09:40:23.110142  9524 solver.cpp:237]     Train net output #1: loss = 0.0890839 (* 1 = 0.0890839 loss)
I1122 09:40:23.110142  9524 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1122 09:40:27.180639 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:40:27.347668  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_16000.caffemodel
I1122 09:40:27.384205  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_16000.solverstate
I1122 09:40:27.388206  9524 solver.cpp:330] Iteration 16000, Testing net (#0)
I1122 09:40:27.388206  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:40:28.458978  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:40:28.501014  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8991
I1122 09:40:28.501014  9524 solver.cpp:397]     Test net output #1: loss = 0.297046 (* 1 = 0.297046 loss)
I1122 09:40:28.542013  9524 solver.cpp:218] Iteration 16000 (18.4106 iter/s, 5.43164s/100 iters), loss = 0.153204
I1122 09:40:28.542013  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:40:28.542013  9524 solver.cpp:237]     Train net output #1: loss = 0.153204 (* 1 = 0.153204 loss)
I1122 09:40:28.542013  9524 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1122 09:40:32.823539  9524 solver.cpp:218] Iteration 16100 (23.3587 iter/s, 4.28107s/100 iters), loss = 0.20001
I1122 09:40:32.823539  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:40:32.823539  9524 solver.cpp:237]     Train net output #1: loss = 0.20001 (* 1 = 0.20001 loss)
I1122 09:40:32.823539  9524 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1122 09:40:37.106225  9524 solver.cpp:218] Iteration 16200 (23.3514 iter/s, 4.2824s/100 iters), loss = 0.140798
I1122 09:40:37.106225  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:40:37.106225  9524 solver.cpp:237]     Train net output #1: loss = 0.140798 (* 1 = 0.140798 loss)
I1122 09:40:37.106225  9524 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1122 09:40:41.384701  9524 solver.cpp:218] Iteration 16300 (23.377 iter/s, 4.27772s/100 iters), loss = 0.193448
I1122 09:40:41.384701  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:40:41.384701  9524 solver.cpp:237]     Train net output #1: loss = 0.193448 (* 1 = 0.193448 loss)
I1122 09:40:41.384701  9524 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1122 09:40:45.669884  9524 solver.cpp:218] Iteration 16400 (23.3386 iter/s, 4.28475s/100 iters), loss = 0.0824053
I1122 09:40:45.669884  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 09:40:45.669884  9524 solver.cpp:237]     Train net output #1: loss = 0.0824053 (* 1 = 0.0824053 loss)
I1122 09:40:45.669884  9524 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1122 09:40:49.736541 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:40:49.904623  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_16500.caffemodel
I1122 09:40:49.914610  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_16500.solverstate
I1122 09:40:49.918609  9524 solver.cpp:330] Iteration 16500, Testing net (#0)
I1122 09:40:49.918609  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:40:50.990361  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:40:51.032892  9524 solver.cpp:397]     Test net output #0: accuracy = 0.9002
I1122 09:40:51.032892  9524 solver.cpp:397]     Test net output #1: loss = 0.296721 (* 1 = 0.296721 loss)
I1122 09:40:51.073892  9524 solver.cpp:218] Iteration 16500 (18.5054 iter/s, 5.40382s/100 iters), loss = 0.165684
I1122 09:40:51.073892  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:40:51.073892  9524 solver.cpp:237]     Train net output #1: loss = 0.165684 (* 1 = 0.165684 loss)
I1122 09:40:51.073892  9524 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1122 09:40:55.338562  9524 solver.cpp:218] Iteration 16600 (23.448 iter/s, 4.26475s/100 iters), loss = 0.168267
I1122 09:40:55.338562  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:40:55.338562  9524 solver.cpp:237]     Train net output #1: loss = 0.168267 (* 1 = 0.168267 loss)
I1122 09:40:55.338562  9524 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1122 09:40:59.605720  9524 solver.cpp:218] Iteration 16700 (23.4413 iter/s, 4.26597s/100 iters), loss = 0.136781
I1122 09:40:59.605720  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:40:59.605720  9524 solver.cpp:237]     Train net output #1: loss = 0.136781 (* 1 = 0.136781 loss)
I1122 09:40:59.605720  9524 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1122 09:41:03.866864  9524 solver.cpp:218] Iteration 16800 (23.4669 iter/s, 4.26132s/100 iters), loss = 0.186301
I1122 09:41:03.866864  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:41:03.866864  9524 solver.cpp:237]     Train net output #1: loss = 0.1863 (* 1 = 0.1863 loss)
I1122 09:41:03.866864  9524 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1122 09:41:08.129722  9524 solver.cpp:218] Iteration 16900 (23.4601 iter/s, 4.26255s/100 iters), loss = 0.135054
I1122 09:41:08.129722  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:41:08.129722  9524 solver.cpp:237]     Train net output #1: loss = 0.135053 (* 1 = 0.135053 loss)
I1122 09:41:08.129722  9524 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1122 09:41:12.179306 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:41:12.347373  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_17000.caffemodel
I1122 09:41:12.383379  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_17000.solverstate
I1122 09:41:12.387377  9524 solver.cpp:330] Iteration 17000, Testing net (#0)
I1122 09:41:12.387377  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:41:13.458825  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:41:13.501395  9524 solver.cpp:397]     Test net output #0: accuracy = 0.9001
I1122 09:41:13.501395  9524 solver.cpp:397]     Test net output #1: loss = 0.296545 (* 1 = 0.296545 loss)
I1122 09:41:13.542397  9524 solver.cpp:218] Iteration 17000 (18.4785 iter/s, 5.41168s/100 iters), loss = 0.134065
I1122 09:41:13.542397  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:41:13.542397  9524 solver.cpp:237]     Train net output #1: loss = 0.134065 (* 1 = 0.134065 loss)
I1122 09:41:13.542397  9524 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1122 09:41:17.820003  9524 solver.cpp:218] Iteration 17100 (23.3803 iter/s, 4.27711s/100 iters), loss = 0.157319
I1122 09:41:17.820003  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:41:17.820003  9524 solver.cpp:237]     Train net output #1: loss = 0.157319 (* 1 = 0.157319 loss)
I1122 09:41:17.820003  9524 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1122 09:41:22.100508  9524 solver.cpp:218] Iteration 17200 (23.3622 iter/s, 4.28041s/100 iters), loss = 0.137266
I1122 09:41:22.100508  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:41:22.100508  9524 solver.cpp:237]     Train net output #1: loss = 0.137266 (* 1 = 0.137266 loss)
I1122 09:41:22.100508  9524 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1122 09:41:26.377140  9524 solver.cpp:218] Iteration 17300 (23.3847 iter/s, 4.2763s/100 iters), loss = 0.169273
I1122 09:41:26.377140  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:41:26.377140  9524 solver.cpp:237]     Train net output #1: loss = 0.169273 (* 1 = 0.169273 loss)
I1122 09:41:26.377140  9524 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1122 09:41:30.658887  9524 solver.cpp:218] Iteration 17400 (23.3573 iter/s, 4.28131s/100 iters), loss = 0.124604
I1122 09:41:30.658887  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:41:30.658887  9524 solver.cpp:237]     Train net output #1: loss = 0.124604 (* 1 = 0.124604 loss)
I1122 09:41:30.658887  9524 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1122 09:41:34.732647 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:41:34.900687  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_17500.caffemodel
I1122 09:41:34.911212  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_17500.solverstate
I1122 09:41:34.915207  9524 solver.cpp:330] Iteration 17500, Testing net (#0)
I1122 09:41:34.915207  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:41:35.987067  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:41:36.029104  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8996
I1122 09:41:36.030108  9524 solver.cpp:397]     Test net output #1: loss = 0.296436 (* 1 = 0.296436 loss)
I1122 09:41:36.071120  9524 solver.cpp:218] Iteration 17500 (18.4795 iter/s, 5.4114s/100 iters), loss = 0.149722
I1122 09:41:36.071120  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:41:36.071120  9524 solver.cpp:237]     Train net output #1: loss = 0.149722 (* 1 = 0.149722 loss)
I1122 09:41:36.071120  9524 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1122 09:41:40.346709  9524 solver.cpp:218] Iteration 17600 (23.3898 iter/s, 4.27537s/100 iters), loss = 0.170425
I1122 09:41:40.346709  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:41:40.346709  9524 solver.cpp:237]     Train net output #1: loss = 0.170425 (* 1 = 0.170425 loss)
I1122 09:41:40.346709  9524 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1122 09:41:44.627277  9524 solver.cpp:218] Iteration 17700 (23.3621 iter/s, 4.28043s/100 iters), loss = 0.117672
I1122 09:41:44.627277  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 09:41:44.627277  9524 solver.cpp:237]     Train net output #1: loss = 0.117672 (* 1 = 0.117672 loss)
I1122 09:41:44.627277  9524 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1122 09:41:48.909842  9524 solver.cpp:218] Iteration 17800 (23.3525 iter/s, 4.2822s/100 iters), loss = 0.173823
I1122 09:41:48.909842  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:41:48.909842  9524 solver.cpp:237]     Train net output #1: loss = 0.173823 (* 1 = 0.173823 loss)
I1122 09:41:48.909842  9524 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1122 09:41:53.188400  9524 solver.cpp:218] Iteration 17900 (23.3739 iter/s, 4.27827s/100 iters), loss = 0.0777451
I1122 09:41:53.188400  9524 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 09:41:53.188400  9524 solver.cpp:237]     Train net output #1: loss = 0.0777451 (* 1 = 0.0777451 loss)
I1122 09:41:53.188400  9524 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1122 09:41:57.251899 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:41:57.419966  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_18000.caffemodel
I1122 09:41:57.450991  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_18000.solverstate
I1122 09:41:57.453991  9524 solver.cpp:330] Iteration 18000, Testing net (#0)
I1122 09:41:57.454991  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:41:58.525185  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:41:58.567206  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8994
I1122 09:41:58.567206  9524 solver.cpp:397]     Test net output #1: loss = 0.296297 (* 1 = 0.296297 loss)
I1122 09:41:58.608212  9524 solver.cpp:218] Iteration 18000 (18.4511 iter/s, 5.41974s/100 iters), loss = 0.128212
I1122 09:41:58.608212  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:41:58.608212  9524 solver.cpp:237]     Train net output #1: loss = 0.128212 (* 1 = 0.128212 loss)
I1122 09:41:58.608212  9524 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1122 09:42:02.881803  9524 solver.cpp:218] Iteration 18100 (23.4059 iter/s, 4.27242s/100 iters), loss = 0.131175
I1122 09:42:02.881803  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:42:02.881803  9524 solver.cpp:237]     Train net output #1: loss = 0.131175 (* 1 = 0.131175 loss)
I1122 09:42:02.881803  9524 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1122 09:42:07.159404  9524 solver.cpp:218] Iteration 18200 (23.3771 iter/s, 4.2777s/100 iters), loss = 0.135908
I1122 09:42:07.159404  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:42:07.159404  9524 solver.cpp:237]     Train net output #1: loss = 0.135908 (* 1 = 0.135908 loss)
I1122 09:42:07.159404  9524 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1122 09:42:11.436995  9524 solver.cpp:218] Iteration 18300 (23.3793 iter/s, 4.27729s/100 iters), loss = 0.162058
I1122 09:42:11.436995  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:42:11.436995  9524 solver.cpp:237]     Train net output #1: loss = 0.162058 (* 1 = 0.162058 loss)
I1122 09:42:11.436995  9524 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1122 09:42:15.715539  9524 solver.cpp:218] Iteration 18400 (23.3742 iter/s, 4.27822s/100 iters), loss = 0.13809
I1122 09:42:15.715539  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:42:15.715539  9524 solver.cpp:237]     Train net output #1: loss = 0.13809 (* 1 = 0.13809 loss)
I1122 09:42:15.715539  9524 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1122 09:42:19.788239 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:42:19.957315  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_18500.caffemodel
I1122 09:42:19.967309  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_18500.solverstate
I1122 09:42:19.971319  9524 solver.cpp:330] Iteration 18500, Testing net (#0)
I1122 09:42:19.971319  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:42:21.041719  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:42:21.083734  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8999
I1122 09:42:21.084751  9524 solver.cpp:397]     Test net output #1: loss = 0.296187 (* 1 = 0.296187 loss)
I1122 09:42:21.125753  9524 solver.cpp:218] Iteration 18500 (18.4875 iter/s, 5.40906s/100 iters), loss = 0.152402
I1122 09:42:21.125753  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:42:21.125753  9524 solver.cpp:237]     Train net output #1: loss = 0.152402 (* 1 = 0.152402 loss)
I1122 09:42:21.125753  9524 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1122 09:42:25.386323  9524 solver.cpp:218] Iteration 18600 (23.4703 iter/s, 4.26071s/100 iters), loss = 0.122039
I1122 09:42:25.386323  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:42:25.386323  9524 solver.cpp:237]     Train net output #1: loss = 0.122039 (* 1 = 0.122039 loss)
I1122 09:42:25.386323  9524 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1122 09:42:29.650058  9524 solver.cpp:218] Iteration 18700 (23.4541 iter/s, 4.26365s/100 iters), loss = 0.136655
I1122 09:42:29.650058  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:42:29.650058  9524 solver.cpp:237]     Train net output #1: loss = 0.136655 (* 1 = 0.136655 loss)
I1122 09:42:29.650058  9524 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1122 09:42:33.912582  9524 solver.cpp:218] Iteration 18800 (23.4616 iter/s, 4.26229s/100 iters), loss = 0.154911
I1122 09:42:33.913583  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:42:33.913583  9524 solver.cpp:237]     Train net output #1: loss = 0.154911 (* 1 = 0.154911 loss)
I1122 09:42:33.913583  9524 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1122 09:42:38.178303  9524 solver.cpp:218] Iteration 18900 (23.4496 iter/s, 4.26447s/100 iters), loss = 0.107784
I1122 09:42:38.178303  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:42:38.178303  9524 solver.cpp:237]     Train net output #1: loss = 0.107784 (* 1 = 0.107784 loss)
I1122 09:42:38.178303  9524 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1122 09:42:42.229917 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:42:42.397992  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_19000.caffemodel
I1122 09:42:42.434017  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_19000.solverstate
I1122 09:42:42.438001  9524 solver.cpp:330] Iteration 19000, Testing net (#0)
I1122 09:42:42.438001  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:42:43.508260  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:42:43.551265  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8999
I1122 09:42:43.551265  9524 solver.cpp:397]     Test net output #1: loss = 0.296201 (* 1 = 0.296201 loss)
I1122 09:42:43.592275  9524 solver.cpp:218] Iteration 19000 (18.471 iter/s, 5.41388s/100 iters), loss = 0.147699
I1122 09:42:43.592275  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:42:43.592275  9524 solver.cpp:237]     Train net output #1: loss = 0.147699 (* 1 = 0.147699 loss)
I1122 09:42:43.592275  9524 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1122 09:42:47.874923  9524 solver.cpp:218] Iteration 19100 (23.3522 iter/s, 4.28225s/100 iters), loss = 0.162485
I1122 09:42:47.874923  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:42:47.874923  9524 solver.cpp:237]     Train net output #1: loss = 0.162485 (* 1 = 0.162485 loss)
I1122 09:42:47.874923  9524 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1122 09:42:52.155861  9524 solver.cpp:218] Iteration 19200 (23.3601 iter/s, 4.28081s/100 iters), loss = 0.133415
I1122 09:42:52.155861  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:42:52.155861  9524 solver.cpp:237]     Train net output #1: loss = 0.133415 (* 1 = 0.133415 loss)
I1122 09:42:52.155861  9524 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1122 09:42:56.433833  9524 solver.cpp:218] Iteration 19300 (23.3785 iter/s, 4.27744s/100 iters), loss = 0.169897
I1122 09:42:56.433833  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:42:56.433833  9524 solver.cpp:237]     Train net output #1: loss = 0.169897 (* 1 = 0.169897 loss)
I1122 09:42:56.433833  9524 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1122 09:43:00.709444  9524 solver.cpp:218] Iteration 19400 (23.3878 iter/s, 4.27573s/100 iters), loss = 0.0914283
I1122 09:43:00.710445  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:43:00.710445  9524 solver.cpp:237]     Train net output #1: loss = 0.0914282 (* 1 = 0.0914282 loss)
I1122 09:43:00.710445  9524 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1122 09:43:04.781983 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:43:04.951046  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_19500.caffemodel
I1122 09:43:04.961052  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_19500.solverstate
I1122 09:43:04.964056  9524 solver.cpp:330] Iteration 19500, Testing net (#0)
I1122 09:43:04.964056  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:43:06.036531  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:43:06.079054  9524 solver.cpp:397]     Test net output #0: accuracy = 0.9003
I1122 09:43:06.079054  9524 solver.cpp:397]     Test net output #1: loss = 0.296229 (* 1 = 0.296229 loss)
I1122 09:43:06.120069  9524 solver.cpp:218] Iteration 19500 (18.486 iter/s, 5.40951s/100 iters), loss = 0.124071
I1122 09:43:06.120069  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:43:06.120069  9524 solver.cpp:237]     Train net output #1: loss = 0.124071 (* 1 = 0.124071 loss)
I1122 09:43:06.120069  9524 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1122 09:43:06.120069  9524 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1122 09:43:10.381608  9524 solver.cpp:218] Iteration 19600 (23.466 iter/s, 4.26149s/100 iters), loss = 0.13399
I1122 09:43:10.381608  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:43:10.381608  9524 solver.cpp:237]     Train net output #1: loss = 0.13399 (* 1 = 0.13399 loss)
I1122 09:43:10.381608  9524 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1122 09:43:14.638326  9524 solver.cpp:218] Iteration 19700 (23.4945 iter/s, 4.25632s/100 iters), loss = 0.130087
I1122 09:43:14.638326  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:43:14.638326  9524 solver.cpp:237]     Train net output #1: loss = 0.130086 (* 1 = 0.130086 loss)
I1122 09:43:14.638326  9524 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1122 09:43:18.902868  9524 solver.cpp:218] Iteration 19800 (23.453 iter/s, 4.26385s/100 iters), loss = 0.166492
I1122 09:43:18.902868  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:43:18.902868  9524 solver.cpp:237]     Train net output #1: loss = 0.166492 (* 1 = 0.166492 loss)
I1122 09:43:18.902868  9524 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1122 09:43:23.161825  9524 solver.cpp:218] Iteration 19900 (23.482 iter/s, 4.25857s/100 iters), loss = 0.0905231
I1122 09:43:23.161825  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 09:43:23.161825  9524 solver.cpp:237]     Train net output #1: loss = 0.090523 (* 1 = 0.090523 loss)
I1122 09:43:23.161825  9524 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1122 09:43:27.221109 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:43:27.388181  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_20000.caffemodel
I1122 09:43:27.425182  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_20000.solverstate
I1122 09:43:27.428180  9524 solver.cpp:330] Iteration 20000, Testing net (#0)
I1122 09:43:27.428180  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:43:28.500324  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:43:28.541340  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8995
I1122 09:43:28.541340  9524 solver.cpp:397]     Test net output #1: loss = 0.296304 (* 1 = 0.296304 loss)
I1122 09:43:28.582875  9524 solver.cpp:218] Iteration 20000 (18.4481 iter/s, 5.4206s/100 iters), loss = 0.171574
I1122 09:43:28.582875  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:43:28.582875  9524 solver.cpp:237]     Train net output #1: loss = 0.171574 (* 1 = 0.171574 loss)
I1122 09:43:28.582875  9524 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1122 09:43:32.865154  9524 solver.cpp:218] Iteration 20100 (23.3543 iter/s, 4.28187s/100 iters), loss = 0.154783
I1122 09:43:32.865154  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:43:32.865154  9524 solver.cpp:237]     Train net output #1: loss = 0.154782 (* 1 = 0.154782 loss)
I1122 09:43:32.865154  9524 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1122 09:43:37.145386  9524 solver.cpp:218] Iteration 20200 (23.3633 iter/s, 4.28021s/100 iters), loss = 0.152403
I1122 09:43:37.145386  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:43:37.145386  9524 solver.cpp:237]     Train net output #1: loss = 0.152403 (* 1 = 0.152403 loss)
I1122 09:43:37.145386  9524 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1122 09:43:41.425012  9524 solver.cpp:218] Iteration 20300 (23.3671 iter/s, 4.27952s/100 iters), loss = 0.20809
I1122 09:43:41.425012  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 09:43:41.425012  9524 solver.cpp:237]     Train net output #1: loss = 0.20809 (* 1 = 0.20809 loss)
I1122 09:43:41.425012  9524 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1122 09:43:45.702601  9524 solver.cpp:218] Iteration 20400 (23.3808 iter/s, 4.27701s/100 iters), loss = 0.119606
I1122 09:43:45.702601  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:43:45.702601  9524 solver.cpp:237]     Train net output #1: loss = 0.119606 (* 1 = 0.119606 loss)
I1122 09:43:45.702601  9524 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1122 09:43:49.772773 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:43:49.940333  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_20500.caffemodel
I1122 09:43:49.950335  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_20500.solverstate
I1122 09:43:49.954315  9524 solver.cpp:330] Iteration 20500, Testing net (#0)
I1122 09:43:49.954315  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:43:51.024102  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:43:51.066628  9524 solver.cpp:397]     Test net output #0: accuracy = 0.9
I1122 09:43:51.066628  9524 solver.cpp:397]     Test net output #1: loss = 0.296235 (* 1 = 0.296235 loss)
I1122 09:43:51.107640  9524 solver.cpp:218] Iteration 20500 (18.5031 iter/s, 5.4045s/100 iters), loss = 0.146276
I1122 09:43:51.107640  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:43:51.107640  9524 solver.cpp:237]     Train net output #1: loss = 0.146276 (* 1 = 0.146276 loss)
I1122 09:43:51.107640  9524 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1122 09:43:55.392946  9524 solver.cpp:218] Iteration 20600 (23.3401 iter/s, 4.28447s/100 iters), loss = 0.1869
I1122 09:43:55.392946  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:43:55.392946  9524 solver.cpp:237]     Train net output #1: loss = 0.186899 (* 1 = 0.186899 loss)
I1122 09:43:55.392946  9524 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1122 09:43:59.674098  9524 solver.cpp:218] Iteration 20700 (23.359 iter/s, 4.281s/100 iters), loss = 0.122097
I1122 09:43:59.674098  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 09:43:59.674098  9524 solver.cpp:237]     Train net output #1: loss = 0.122097 (* 1 = 0.122097 loss)
I1122 09:43:59.674098  9524 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1122 09:44:03.954279  9524 solver.cpp:218] Iteration 20800 (23.366 iter/s, 4.27973s/100 iters), loss = 0.149792
I1122 09:44:03.954279  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:44:03.954279  9524 solver.cpp:237]     Train net output #1: loss = 0.149792 (* 1 = 0.149792 loss)
I1122 09:44:03.954279  9524 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1122 09:44:08.233950  9524 solver.cpp:218] Iteration 20900 (23.3655 iter/s, 4.27982s/100 iters), loss = 0.100429
I1122 09:44:08.233950  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 09:44:08.233950  9524 solver.cpp:237]     Train net output #1: loss = 0.100429 (* 1 = 0.100429 loss)
I1122 09:44:08.233950  9524 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1122 09:44:12.302672 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:44:12.472729  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_21000.caffemodel
I1122 09:44:12.491775  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_21000.solverstate
I1122 09:44:12.500771  9524 solver.cpp:330] Iteration 21000, Testing net (#0)
I1122 09:44:12.501787  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:44:13.572793  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:44:13.614871  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8996
I1122 09:44:13.614871  9524 solver.cpp:397]     Test net output #1: loss = 0.296308 (* 1 = 0.296308 loss)
I1122 09:44:13.655884  9524 solver.cpp:218] Iteration 21000 (18.4472 iter/s, 5.42086s/100 iters), loss = 0.126201
I1122 09:44:13.655884  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:44:13.655884  9524 solver.cpp:237]     Train net output #1: loss = 0.1262 (* 1 = 0.1262 loss)
I1122 09:44:13.655884  9524 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1122 09:44:17.935120  9524 solver.cpp:218] Iteration 21100 (23.3694 iter/s, 4.2791s/100 iters), loss = 0.146557
I1122 09:44:17.935120  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:44:17.935120  9524 solver.cpp:237]     Train net output #1: loss = 0.146557 (* 1 = 0.146557 loss)
I1122 09:44:17.935120  9524 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1122 09:44:22.216753  9524 solver.cpp:218] Iteration 21200 (23.3578 iter/s, 4.28122s/100 iters), loss = 0.1204
I1122 09:44:22.216753  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:44:22.216753  9524 solver.cpp:237]     Train net output #1: loss = 0.1204 (* 1 = 0.1204 loss)
I1122 09:44:22.216753  9524 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1122 09:44:26.495230  9524 solver.cpp:218] Iteration 21300 (23.3735 iter/s, 4.27834s/100 iters), loss = 0.179399
I1122 09:44:26.495230  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:44:26.495230  9524 solver.cpp:237]     Train net output #1: loss = 0.179399 (* 1 = 0.179399 loss)
I1122 09:44:26.495230  9524 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1122 09:44:30.779989  9524 solver.cpp:218] Iteration 21400 (23.3419 iter/s, 4.28415s/100 iters), loss = 0.105188
I1122 09:44:30.779989  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 09:44:30.779989  9524 solver.cpp:237]     Train net output #1: loss = 0.105188 (* 1 = 0.105188 loss)
I1122 09:44:30.779989  9524 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1122 09:44:34.849627 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:44:35.018681  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_21500.caffemodel
I1122 09:44:35.028682  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_21500.solverstate
I1122 09:44:35.032666  9524 solver.cpp:330] Iteration 21500, Testing net (#0)
I1122 09:44:35.032666  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:44:36.104156  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:44:36.146149  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8999
I1122 09:44:36.146149  9524 solver.cpp:397]     Test net output #1: loss = 0.296413 (* 1 = 0.296413 loss)
I1122 09:44:36.188163  9524 solver.cpp:218] Iteration 21500 (18.4921 iter/s, 5.40772s/100 iters), loss = 0.146007
I1122 09:44:36.188163  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:44:36.188163  9524 solver.cpp:237]     Train net output #1: loss = 0.146007 (* 1 = 0.146007 loss)
I1122 09:44:36.188163  9524 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1122 09:44:40.452782  9524 solver.cpp:218] Iteration 21600 (23.4468 iter/s, 4.26497s/100 iters), loss = 0.153895
I1122 09:44:40.452782  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:44:40.452782  9524 solver.cpp:237]     Train net output #1: loss = 0.153895 (* 1 = 0.153895 loss)
I1122 09:44:40.452782  9524 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1122 09:44:44.716403  9524 solver.cpp:218] Iteration 21700 (23.4593 iter/s, 4.2627s/100 iters), loss = 0.156252
I1122 09:44:44.716403  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:44:44.716403  9524 solver.cpp:237]     Train net output #1: loss = 0.156251 (* 1 = 0.156251 loss)
I1122 09:44:44.716403  9524 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1122 09:44:48.980779  9524 solver.cpp:218] Iteration 21800 (23.4532 iter/s, 4.26381s/100 iters), loss = 0.174132
I1122 09:44:48.980779  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:44:48.980779  9524 solver.cpp:237]     Train net output #1: loss = 0.174132 (* 1 = 0.174132 loss)
I1122 09:44:48.980779  9524 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1122 09:44:53.240901  9524 solver.cpp:218] Iteration 21900 (23.4743 iter/s, 4.25997s/100 iters), loss = 0.0919785
I1122 09:44:53.240901  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:44:53.240901  9524 solver.cpp:237]     Train net output #1: loss = 0.0919784 (* 1 = 0.0919784 loss)
I1122 09:44:53.240901  9524 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1122 09:44:57.297477 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:44:57.465042  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_22000.caffemodel
I1122 09:44:57.483573  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_22000.solverstate
I1122 09:44:57.487572  9524 solver.cpp:330] Iteration 22000, Testing net (#0)
I1122 09:44:57.487572  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:44:58.558331  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:44:58.601392  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8999
I1122 09:44:58.601392  9524 solver.cpp:397]     Test net output #1: loss = 0.296271 (* 1 = 0.296271 loss)
I1122 09:44:58.642386  9524 solver.cpp:218] Iteration 22000 (18.5146 iter/s, 5.40114s/100 iters), loss = 0.180094
I1122 09:44:58.642386  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:44:58.642386  9524 solver.cpp:237]     Train net output #1: loss = 0.180094 (* 1 = 0.180094 loss)
I1122 09:44:58.642386  9524 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1122 09:44:58.642386  9524 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1122 09:45:02.918895  9524 solver.cpp:218] Iteration 22100 (23.3854 iter/s, 4.27618s/100 iters), loss = 0.158498
I1122 09:45:02.918895  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:45:02.918895  9524 solver.cpp:237]     Train net output #1: loss = 0.158498 (* 1 = 0.158498 loss)
I1122 09:45:02.918895  9524 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1122 09:45:07.196736  9524 solver.cpp:218] Iteration 22200 (23.3756 iter/s, 4.27797s/100 iters), loss = 0.140921
I1122 09:45:07.196736  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:45:07.196736  9524 solver.cpp:237]     Train net output #1: loss = 0.140921 (* 1 = 0.140921 loss)
I1122 09:45:07.196736  9524 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1122 09:45:11.471752  9524 solver.cpp:218] Iteration 22300 (23.3976 iter/s, 4.27395s/100 iters), loss = 0.154197
I1122 09:45:11.471752  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:45:11.471752  9524 solver.cpp:237]     Train net output #1: loss = 0.154197 (* 1 = 0.154197 loss)
I1122 09:45:11.471752  9524 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1122 09:45:15.747933  9524 solver.cpp:218] Iteration 22400 (23.3833 iter/s, 4.27656s/100 iters), loss = 0.0994478
I1122 09:45:15.748939  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 09:45:15.748939  9524 solver.cpp:237]     Train net output #1: loss = 0.0994477 (* 1 = 0.0994477 loss)
I1122 09:45:15.748939  9524 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1122 09:45:19.819675 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:45:19.987226  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_22500.caffemodel
I1122 09:45:20.000746  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_22500.solverstate
I1122 09:45:20.004746  9524 solver.cpp:330] Iteration 22500, Testing net (#0)
I1122 09:45:20.004746  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:45:21.076539  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:45:21.119074  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8996
I1122 09:45:21.119074  9524 solver.cpp:397]     Test net output #1: loss = 0.296293 (* 1 = 0.296293 loss)
I1122 09:45:21.160058  9524 solver.cpp:218] Iteration 22500 (18.4804 iter/s, 5.41115s/100 iters), loss = 0.170483
I1122 09:45:21.160058  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:45:21.160058  9524 solver.cpp:237]     Train net output #1: loss = 0.170483 (* 1 = 0.170483 loss)
I1122 09:45:21.160058  9524 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1122 09:45:25.440933  9524 solver.cpp:218] Iteration 22600 (23.3622 iter/s, 4.28043s/100 iters), loss = 0.172712
I1122 09:45:25.440933  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:45:25.440933  9524 solver.cpp:237]     Train net output #1: loss = 0.172712 (* 1 = 0.172712 loss)
I1122 09:45:25.440933  9524 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1122 09:45:29.719558  9524 solver.cpp:218] Iteration 22700 (23.3724 iter/s, 4.27855s/100 iters), loss = 0.140476
I1122 09:45:29.719558  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:45:29.719558  9524 solver.cpp:237]     Train net output #1: loss = 0.140476 (* 1 = 0.140476 loss)
I1122 09:45:29.719558  9524 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1122 09:45:34.000229  9524 solver.cpp:218] Iteration 22800 (23.3629 iter/s, 4.28029s/100 iters), loss = 0.14506
I1122 09:45:34.000229  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:45:34.000229  9524 solver.cpp:237]     Train net output #1: loss = 0.14506 (* 1 = 0.14506 loss)
I1122 09:45:34.000229  9524 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1122 09:45:38.276814  9524 solver.cpp:218] Iteration 22900 (23.3857 iter/s, 4.27611s/100 iters), loss = 0.144875
I1122 09:45:38.276814  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:45:38.276814  9524 solver.cpp:237]     Train net output #1: loss = 0.144875 (* 1 = 0.144875 loss)
I1122 09:45:38.276814  9524 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1122 09:45:42.345376 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:45:42.513442  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_23000.caffemodel
I1122 09:45:42.550462  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_23000.solverstate
I1122 09:45:42.553462  9524 solver.cpp:330] Iteration 23000, Testing net (#0)
I1122 09:45:42.553462  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:45:43.625756  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:45:43.667763  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8999
I1122 09:45:43.667763  9524 solver.cpp:397]     Test net output #1: loss = 0.296275 (* 1 = 0.296275 loss)
I1122 09:45:43.708807  9524 solver.cpp:218] Iteration 23000 (18.4103 iter/s, 5.43173s/100 iters), loss = 0.153094
I1122 09:45:43.708807  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:45:43.708807  9524 solver.cpp:237]     Train net output #1: loss = 0.153094 (* 1 = 0.153094 loss)
I1122 09:45:43.708807  9524 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1122 09:45:47.988541  9524 solver.cpp:218] Iteration 23100 (23.3706 iter/s, 4.27887s/100 iters), loss = 0.141235
I1122 09:45:47.988541  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:45:47.988541  9524 solver.cpp:237]     Train net output #1: loss = 0.141235 (* 1 = 0.141235 loss)
I1122 09:45:47.989039  9524 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1122 09:45:52.268221  9524 solver.cpp:218] Iteration 23200 (23.3672 iter/s, 4.2795s/100 iters), loss = 0.128477
I1122 09:45:52.268221  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:45:52.268221  9524 solver.cpp:237]     Train net output #1: loss = 0.128477 (* 1 = 0.128477 loss)
I1122 09:45:52.268221  9524 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1122 09:45:56.543077  9524 solver.cpp:218] Iteration 23300 (23.3982 iter/s, 4.27384s/100 iters), loss = 0.219718
I1122 09:45:56.543077  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:45:56.543077  9524 solver.cpp:237]     Train net output #1: loss = 0.219718 (* 1 = 0.219718 loss)
I1122 09:45:56.543077  9524 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1122 09:46:00.819661  9524 solver.cpp:218] Iteration 23400 (23.3837 iter/s, 4.27648s/100 iters), loss = 0.119909
I1122 09:46:00.819661  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:46:00.819661  9524 solver.cpp:237]     Train net output #1: loss = 0.119908 (* 1 = 0.119908 loss)
I1122 09:46:00.819661  9524 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1122 09:46:04.887276 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:46:05.055333  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_23500.caffemodel
I1122 09:46:05.065332  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_23500.solverstate
I1122 09:46:05.069332  9524 solver.cpp:330] Iteration 23500, Testing net (#0)
I1122 09:46:05.069332  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:46:06.140630  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:46:06.182672  9524 solver.cpp:397]     Test net output #0: accuracy = 0.9
I1122 09:46:06.182672  9524 solver.cpp:397]     Test net output #1: loss = 0.296262 (* 1 = 0.296262 loss)
I1122 09:46:06.223688  9524 solver.cpp:218] Iteration 23500 (18.5067 iter/s, 5.40344s/100 iters), loss = 0.148011
I1122 09:46:06.223688  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:46:06.223688  9524 solver.cpp:237]     Train net output #1: loss = 0.148011 (* 1 = 0.148011 loss)
I1122 09:46:06.223688  9524 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1122 09:46:10.487815  9524 solver.cpp:218] Iteration 23600 (23.4538 iter/s, 4.2637s/100 iters), loss = 0.198116
I1122 09:46:10.487815  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:46:10.487815  9524 solver.cpp:237]     Train net output #1: loss = 0.198116 (* 1 = 0.198116 loss)
I1122 09:46:10.487815  9524 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1122 09:46:14.747177  9524 solver.cpp:218] Iteration 23700 (23.4766 iter/s, 4.25956s/100 iters), loss = 0.111071
I1122 09:46:14.747177  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:46:14.747177  9524 solver.cpp:237]     Train net output #1: loss = 0.111071 (* 1 = 0.111071 loss)
I1122 09:46:14.747177  9524 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1122 09:46:19.009901  9524 solver.cpp:218] Iteration 23800 (23.4625 iter/s, 4.26212s/100 iters), loss = 0.169959
I1122 09:46:19.009901  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 09:46:19.009901  9524 solver.cpp:237]     Train net output #1: loss = 0.169959 (* 1 = 0.169959 loss)
I1122 09:46:19.009901  9524 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1122 09:46:23.268484  9524 solver.cpp:218] Iteration 23900 (23.4827 iter/s, 4.25845s/100 iters), loss = 0.118006
I1122 09:46:23.268484  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:46:23.268484  9524 solver.cpp:237]     Train net output #1: loss = 0.118006 (* 1 = 0.118006 loss)
I1122 09:46:23.268484  9524 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1122 09:46:27.315068 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:46:27.484129  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_24000.caffemodel
I1122 09:46:27.517149  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_24000.solverstate
I1122 09:46:27.521169  9524 solver.cpp:330] Iteration 24000, Testing net (#0)
I1122 09:46:27.521169  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:46:28.591431  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:46:28.633476  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8997
I1122 09:46:28.633476  9524 solver.cpp:397]     Test net output #1: loss = 0.296301 (* 1 = 0.296301 loss)
I1122 09:46:28.675482  9524 solver.cpp:218] Iteration 24000 (18.4974 iter/s, 5.40616s/100 iters), loss = 0.1218
I1122 09:46:28.675482  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:46:28.675482  9524 solver.cpp:237]     Train net output #1: loss = 0.1218 (* 1 = 0.1218 loss)
I1122 09:46:28.675482  9524 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1122 09:46:32.957044  9524 solver.cpp:218] Iteration 24100 (23.3594 iter/s, 4.28094s/100 iters), loss = 0.150858
I1122 09:46:32.957044  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:46:32.957044  9524 solver.cpp:237]     Train net output #1: loss = 0.150858 (* 1 = 0.150858 loss)
I1122 09:46:32.957044  9524 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1122 09:46:37.238821  9524 solver.cpp:218] Iteration 24200 (23.3523 iter/s, 4.28224s/100 iters), loss = 0.162568
I1122 09:46:37.238821  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:46:37.238821  9524 solver.cpp:237]     Train net output #1: loss = 0.162568 (* 1 = 0.162568 loss)
I1122 09:46:37.238821  9524 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1122 09:46:41.521356  9524 solver.cpp:218] Iteration 24300 (23.3548 iter/s, 4.28177s/100 iters), loss = 0.176564
I1122 09:46:41.521356  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:46:41.521356  9524 solver.cpp:237]     Train net output #1: loss = 0.176564 (* 1 = 0.176564 loss)
I1122 09:46:41.521356  9524 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1122 09:46:45.807502  9524 solver.cpp:218] Iteration 24400 (23.3353 iter/s, 4.28535s/100 iters), loss = 0.11666
I1122 09:46:45.807502  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 09:46:45.807502  9524 solver.cpp:237]     Train net output #1: loss = 0.11666 (* 1 = 0.11666 loss)
I1122 09:46:45.807502  9524 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1122 09:46:49.874445 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:46:50.041502  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_24500.caffemodel
I1122 09:46:50.051501  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_24500.solverstate
I1122 09:46:50.055500  9524 solver.cpp:330] Iteration 24500, Testing net (#0)
I1122 09:46:50.055500  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:46:51.120115  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:46:51.161655  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8997
I1122 09:46:51.161655  9524 solver.cpp:397]     Test net output #1: loss = 0.296369 (* 1 = 0.296369 loss)
I1122 09:46:51.202669  9524 solver.cpp:218] Iteration 24500 (18.5344 iter/s, 5.39536s/100 iters), loss = 0.149408
I1122 09:46:51.202669  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:46:51.202669  9524 solver.cpp:237]     Train net output #1: loss = 0.149408 (* 1 = 0.149408 loss)
I1122 09:46:51.202669  9524 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1122 09:46:55.463225  9524 solver.cpp:218] Iteration 24600 (23.4757 iter/s, 4.25973s/100 iters), loss = 0.141554
I1122 09:46:55.463225  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:46:55.463225  9524 solver.cpp:237]     Train net output #1: loss = 0.141554 (* 1 = 0.141554 loss)
I1122 09:46:55.463225  9524 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1122 09:46:59.726883  9524 solver.cpp:218] Iteration 24700 (23.4562 iter/s, 4.26327s/100 iters), loss = 0.134717
I1122 09:46:59.726883  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:46:59.726883  9524 solver.cpp:237]     Train net output #1: loss = 0.134717 (* 1 = 0.134717 loss)
I1122 09:46:59.726883  9524 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1122 09:47:03.982558  9524 solver.cpp:218] Iteration 24800 (23.496 iter/s, 4.25604s/100 iters), loss = 0.141937
I1122 09:47:03.982558  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:47:03.982558  9524 solver.cpp:237]     Train net output #1: loss = 0.141937 (* 1 = 0.141937 loss)
I1122 09:47:03.982558  9524 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1122 09:47:08.244369  9524 solver.cpp:218] Iteration 24900 (23.4696 iter/s, 4.26084s/100 iters), loss = 0.134218
I1122 09:47:08.244369  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:47:08.244369  9524 solver.cpp:237]     Train net output #1: loss = 0.134218 (* 1 = 0.134218 loss)
I1122 09:47:08.244369  9524 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1122 09:47:12.293084 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:47:12.460151  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_25000.caffemodel
I1122 09:47:12.484179  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_25000.solverstate
I1122 09:47:12.488173  9524 solver.cpp:330] Iteration 25000, Testing net (#0)
I1122 09:47:12.488173  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:47:13.561007  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:47:13.602505  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8997
I1122 09:47:13.602505  9524 solver.cpp:397]     Test net output #1: loss = 0.296376 (* 1 = 0.296376 loss)
I1122 09:47:13.643518  9524 solver.cpp:218] Iteration 25000 (18.5217 iter/s, 5.39908s/100 iters), loss = 0.200915
I1122 09:47:13.643518  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:47:13.643518  9524 solver.cpp:237]     Train net output #1: loss = 0.200915 (* 1 = 0.200915 loss)
I1122 09:47:13.643518  9524 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1122 09:47:17.925413  9524 solver.cpp:218] Iteration 25100 (23.3574 iter/s, 4.2813s/100 iters), loss = 0.124002
I1122 09:47:17.925413  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:47:17.925413  9524 solver.cpp:237]     Train net output #1: loss = 0.124002 (* 1 = 0.124002 loss)
I1122 09:47:17.925413  9524 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1122 09:47:22.199766  9524 solver.cpp:218] Iteration 25200 (23.3991 iter/s, 4.27366s/100 iters), loss = 0.122355
I1122 09:47:22.199766  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:47:22.199766  9524 solver.cpp:237]     Train net output #1: loss = 0.122355 (* 1 = 0.122355 loss)
I1122 09:47:22.199766  9524 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1122 09:47:26.477757  9524 solver.cpp:218] Iteration 25300 (23.3749 iter/s, 4.27809s/100 iters), loss = 0.182927
I1122 09:47:26.477757  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:47:26.477757  9524 solver.cpp:237]     Train net output #1: loss = 0.182927 (* 1 = 0.182927 loss)
I1122 09:47:26.477757  9524 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1122 09:47:30.756799  9524 solver.cpp:218] Iteration 25400 (23.3744 iter/s, 4.27819s/100 iters), loss = 0.124533
I1122 09:47:30.756799  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:47:30.756799  9524 solver.cpp:237]     Train net output #1: loss = 0.124533 (* 1 = 0.124533 loss)
I1122 09:47:30.756799  9524 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1122 09:47:34.821244 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:47:34.989302  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_25500.caffemodel
I1122 09:47:34.999322  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_25500.solverstate
I1122 09:47:35.002321  9524 solver.cpp:330] Iteration 25500, Testing net (#0)
I1122 09:47:35.003321  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:47:36.076555  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:47:36.118585  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8998
I1122 09:47:36.118585  9524 solver.cpp:397]     Test net output #1: loss = 0.296281 (* 1 = 0.296281 loss)
I1122 09:47:36.159574  9524 solver.cpp:218] Iteration 25500 (18.5094 iter/s, 5.40265s/100 iters), loss = 0.135005
I1122 09:47:36.159574  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:47:36.159574  9524 solver.cpp:237]     Train net output #1: loss = 0.135005 (* 1 = 0.135005 loss)
I1122 09:47:36.159574  9524 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1122 09:47:40.438144  9524 solver.cpp:218] Iteration 25600 (23.3712 iter/s, 4.27878s/100 iters), loss = 0.177547
I1122 09:47:40.438144  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:47:40.439146  9524 solver.cpp:237]     Train net output #1: loss = 0.177547 (* 1 = 0.177547 loss)
I1122 09:47:40.439146  9524 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1122 09:47:44.720760  9524 solver.cpp:218] Iteration 25700 (23.3565 iter/s, 4.28147s/100 iters), loss = 0.143818
I1122 09:47:44.720760  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:47:44.720760  9524 solver.cpp:237]     Train net output #1: loss = 0.143818 (* 1 = 0.143818 loss)
I1122 09:47:44.720760  9524 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1122 09:47:49.000414  9524 solver.cpp:218] Iteration 25800 (23.3694 iter/s, 4.2791s/100 iters), loss = 0.179143
I1122 09:47:49.000414  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:47:49.000414  9524 solver.cpp:237]     Train net output #1: loss = 0.179143 (* 1 = 0.179143 loss)
I1122 09:47:49.000414  9524 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1122 09:47:53.279559  9524 solver.cpp:218] Iteration 25900 (23.3707 iter/s, 4.27886s/100 iters), loss = 0.0889654
I1122 09:47:53.279559  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:47:53.279559  9524 solver.cpp:237]     Train net output #1: loss = 0.0889653 (* 1 = 0.0889653 loss)
I1122 09:47:53.279559  9524 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1122 09:47:57.346634 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:47:57.515719  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_26000.caffemodel
I1122 09:47:57.532740  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_26000.solverstate
I1122 09:47:57.536736  9524 solver.cpp:330] Iteration 26000, Testing net (#0)
I1122 09:47:57.536736  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:47:58.608093  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:47:58.649107  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8998
I1122 09:47:58.650115  9524 solver.cpp:397]     Test net output #1: loss = 0.296305 (* 1 = 0.296305 loss)
I1122 09:47:58.691143  9524 solver.cpp:218] Iteration 26000 (18.4809 iter/s, 5.411s/100 iters), loss = 0.111797
I1122 09:47:58.691143  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:47:58.691143  9524 solver.cpp:237]     Train net output #1: loss = 0.111797 (* 1 = 0.111797 loss)
I1122 09:47:58.691143  9524 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1122 09:48:02.969764  9524 solver.cpp:218] Iteration 26100 (23.3688 iter/s, 4.27921s/100 iters), loss = 0.139848
I1122 09:48:02.970767  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:48:02.970767  9524 solver.cpp:237]     Train net output #1: loss = 0.139848 (* 1 = 0.139848 loss)
I1122 09:48:02.970767  9524 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1122 09:48:07.246382  9524 solver.cpp:218] Iteration 26200 (23.3868 iter/s, 4.27593s/100 iters), loss = 0.149262
I1122 09:48:07.246382  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:48:07.246382  9524 solver.cpp:237]     Train net output #1: loss = 0.149262 (* 1 = 0.149262 loss)
I1122 09:48:07.246382  9524 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1122 09:48:11.525019  9524 solver.cpp:218] Iteration 26300 (23.3756 iter/s, 4.27796s/100 iters), loss = 0.14915
I1122 09:48:11.525019  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:48:11.525019  9524 solver.cpp:237]     Train net output #1: loss = 0.14915 (* 1 = 0.14915 loss)
I1122 09:48:11.525019  9524 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1122 09:48:15.799612  9524 solver.cpp:218] Iteration 26400 (23.3933 iter/s, 4.27473s/100 iters), loss = 0.128162
I1122 09:48:15.799612  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:48:15.799612  9524 solver.cpp:237]     Train net output #1: loss = 0.128162 (* 1 = 0.128162 loss)
I1122 09:48:15.799612  9524 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1122 09:48:19.873070 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:48:20.041126  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_26500.caffemodel
I1122 09:48:20.051126  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_26500.solverstate
I1122 09:48:20.055127  9524 solver.cpp:330] Iteration 26500, Testing net (#0)
I1122 09:48:20.055127  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:48:21.127465  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:48:21.169466  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8999
I1122 09:48:21.169466  9524 solver.cpp:397]     Test net output #1: loss = 0.296281 (* 1 = 0.296281 loss)
I1122 09:48:21.210530  9524 solver.cpp:218] Iteration 26500 (18.4846 iter/s, 5.40991s/100 iters), loss = 0.153869
I1122 09:48:21.210530  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:48:21.210530  9524 solver.cpp:237]     Train net output #1: loss = 0.153869 (* 1 = 0.153869 loss)
I1122 09:48:21.210530  9524 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1122 09:48:25.476230  9524 solver.cpp:218] Iteration 26600 (23.4432 iter/s, 4.26563s/100 iters), loss = 0.160669
I1122 09:48:25.476230  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:48:25.476230  9524 solver.cpp:237]     Train net output #1: loss = 0.160669 (* 1 = 0.160669 loss)
I1122 09:48:25.476230  9524 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1122 09:48:29.740914  9524 solver.cpp:218] Iteration 26700 (23.4516 iter/s, 4.26411s/100 iters), loss = 0.138025
I1122 09:48:29.740914  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:48:29.740914  9524 solver.cpp:237]     Train net output #1: loss = 0.138024 (* 1 = 0.138024 loss)
I1122 09:48:29.740914  9524 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1122 09:48:34.006562  9524 solver.cpp:218] Iteration 26800 (23.4459 iter/s, 4.26513s/100 iters), loss = 0.134405
I1122 09:48:34.006562  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:48:34.006562  9524 solver.cpp:237]     Train net output #1: loss = 0.134405 (* 1 = 0.134405 loss)
I1122 09:48:34.006562  9524 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1122 09:48:38.267191  9524 solver.cpp:218] Iteration 26900 (23.471 iter/s, 4.26057s/100 iters), loss = 0.100599
I1122 09:48:38.267191  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 09:48:38.267191  9524 solver.cpp:237]     Train net output #1: loss = 0.100599 (* 1 = 0.100599 loss)
I1122 09:48:38.267191  9524 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1122 09:48:42.323813 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:48:42.492334  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_27000.caffemodel
I1122 09:48:42.516880  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_27000.solverstate
I1122 09:48:42.520877  9524 solver.cpp:330] Iteration 27000, Testing net (#0)
I1122 09:48:42.520877  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:48:43.593724  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:48:43.635259  9524 solver.cpp:397]     Test net output #0: accuracy = 0.9
I1122 09:48:43.635259  9524 solver.cpp:397]     Test net output #1: loss = 0.296266 (* 1 = 0.296266 loss)
I1122 09:48:43.676241  9524 solver.cpp:218] Iteration 27000 (18.4884 iter/s, 5.40879s/100 iters), loss = 0.159902
I1122 09:48:43.676241  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:48:43.676241  9524 solver.cpp:237]     Train net output #1: loss = 0.159902 (* 1 = 0.159902 loss)
I1122 09:48:43.676241  9524 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1122 09:48:43.676241  9524 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1122 09:48:47.952967  9524 solver.cpp:218] Iteration 27100 (23.3845 iter/s, 4.27634s/100 iters), loss = 0.179517
I1122 09:48:47.952967  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:48:47.952967  9524 solver.cpp:237]     Train net output #1: loss = 0.179517 (* 1 = 0.179517 loss)
I1122 09:48:47.952967  9524 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1122 09:48:52.226637  9524 solver.cpp:218] Iteration 27200 (23.4033 iter/s, 4.27289s/100 iters), loss = 0.13346
I1122 09:48:52.226637  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:48:52.226637  9524 solver.cpp:237]     Train net output #1: loss = 0.13346 (* 1 = 0.13346 loss)
I1122 09:48:52.226637  9524 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1122 09:48:56.502892  9524 solver.cpp:218] Iteration 27300 (23.3872 iter/s, 4.27583s/100 iters), loss = 0.199602
I1122 09:48:56.502892  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 09:48:56.502892  9524 solver.cpp:237]     Train net output #1: loss = 0.199601 (* 1 = 0.199601 loss)
I1122 09:48:56.502892  9524 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1122 09:49:00.781059  9524 solver.cpp:218] Iteration 27400 (23.3756 iter/s, 4.27797s/100 iters), loss = 0.0944279
I1122 09:49:00.781059  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:49:00.781059  9524 solver.cpp:237]     Train net output #1: loss = 0.0944277 (* 1 = 0.0944277 loss)
I1122 09:49:00.781059  9524 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1122 09:49:04.849622 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:49:05.018692  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_27500.caffemodel
I1122 09:49:05.027695  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_27500.solverstate
I1122 09:49:05.031697  9524 solver.cpp:330] Iteration 27500, Testing net (#0)
I1122 09:49:05.031697  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:49:06.104472  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:49:06.146621  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8998
I1122 09:49:06.146621  9524 solver.cpp:397]     Test net output #1: loss = 0.296338 (* 1 = 0.296338 loss)
I1122 09:49:06.187616  9524 solver.cpp:218] Iteration 27500 (18.4978 iter/s, 5.40604s/100 iters), loss = 0.185528
I1122 09:49:06.187616  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 09:49:06.187616  9524 solver.cpp:237]     Train net output #1: loss = 0.185528 (* 1 = 0.185528 loss)
I1122 09:49:06.187616  9524 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1122 09:49:10.468740  9524 solver.cpp:218] Iteration 27600 (23.3571 iter/s, 4.28135s/100 iters), loss = 0.147987
I1122 09:49:10.469743  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:49:10.469743  9524 solver.cpp:237]     Train net output #1: loss = 0.147987 (* 1 = 0.147987 loss)
I1122 09:49:10.469743  9524 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1122 09:49:14.746332  9524 solver.cpp:218] Iteration 27700 (23.3812 iter/s, 4.27694s/100 iters), loss = 0.106838
I1122 09:49:14.746332  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 09:49:14.746332  9524 solver.cpp:237]     Train net output #1: loss = 0.106838 (* 1 = 0.106838 loss)
I1122 09:49:14.746332  9524 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1122 09:49:19.028126  9524 solver.cpp:218] Iteration 27800 (23.3585 iter/s, 4.28109s/100 iters), loss = 0.165118
I1122 09:49:19.028126  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:49:19.028126  9524 solver.cpp:237]     Train net output #1: loss = 0.165118 (* 1 = 0.165118 loss)
I1122 09:49:19.028126  9524 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1122 09:49:23.313280  9524 solver.cpp:218] Iteration 27900 (23.3385 iter/s, 4.28476s/100 iters), loss = 0.0973103
I1122 09:49:23.313280  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:49:23.313781  9524 solver.cpp:237]     Train net output #1: loss = 0.0973101 (* 1 = 0.0973101 loss)
I1122 09:49:23.313781  9524 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1122 09:49:27.386330 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:49:27.554402  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_28000.caffemodel
I1122 09:49:27.633463  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_28000.solverstate
I1122 09:49:27.637464  9524 solver.cpp:330] Iteration 28000, Testing net (#0)
I1122 09:49:27.637464  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:49:28.709833  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:49:28.751852  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8997
I1122 09:49:28.751852  9524 solver.cpp:397]     Test net output #1: loss = 0.296349 (* 1 = 0.296349 loss)
I1122 09:49:28.791867  9524 solver.cpp:218] Iteration 28000 (18.2525 iter/s, 5.47871s/100 iters), loss = 0.149178
I1122 09:49:28.792881  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:49:28.792881  9524 solver.cpp:237]     Train net output #1: loss = 0.149177 (* 1 = 0.149177 loss)
I1122 09:49:28.792881  9524 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1122 09:49:33.066486  9524 solver.cpp:218] Iteration 28100 (23.3984 iter/s, 4.27379s/100 iters), loss = 0.161738
I1122 09:49:33.066486  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:49:33.066486  9524 solver.cpp:237]     Train net output #1: loss = 0.161738 (* 1 = 0.161738 loss)
I1122 09:49:33.066486  9524 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1122 09:49:37.349748  9524 solver.cpp:218] Iteration 28200 (23.3477 iter/s, 4.28308s/100 iters), loss = 0.127387
I1122 09:49:37.349748  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 09:49:37.349748  9524 solver.cpp:237]     Train net output #1: loss = 0.127386 (* 1 = 0.127386 loss)
I1122 09:49:37.349748  9524 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1122 09:49:41.626932  9524 solver.cpp:218] Iteration 28300 (23.3835 iter/s, 4.27652s/100 iters), loss = 0.187289
I1122 09:49:41.626932  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:49:41.627432  9524 solver.cpp:237]     Train net output #1: loss = 0.187289 (* 1 = 0.187289 loss)
I1122 09:49:41.627432  9524 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1122 09:49:45.894968  9524 solver.cpp:218] Iteration 28400 (23.4309 iter/s, 4.26787s/100 iters), loss = 0.12654
I1122 09:49:45.894968  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:49:45.894968  9524 solver.cpp:237]     Train net output #1: loss = 0.12654 (* 1 = 0.12654 loss)
I1122 09:49:45.894968  9524 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1122 09:49:49.959583 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:49:50.126593  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_28500.caffemodel
I1122 09:49:50.135614  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_28500.solverstate
I1122 09:49:50.139613  9524 solver.cpp:330] Iteration 28500, Testing net (#0)
I1122 09:49:50.139613  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:49:51.210947  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:49:51.252990  9524 solver.cpp:397]     Test net output #0: accuracy = 0.9002
I1122 09:49:51.252990  9524 solver.cpp:397]     Test net output #1: loss = 0.296168 (* 1 = 0.296168 loss)
I1122 09:49:51.294008  9524 solver.cpp:218] Iteration 28500 (18.5246 iter/s, 5.39822s/100 iters), loss = 0.139008
I1122 09:49:51.294008  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:49:51.294008  9524 solver.cpp:237]     Train net output #1: loss = 0.139008 (* 1 = 0.139008 loss)
I1122 09:49:51.294008  9524 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1122 09:49:55.554447  9524 solver.cpp:218] Iteration 28600 (23.4743 iter/s, 4.25997s/100 iters), loss = 0.197503
I1122 09:49:55.554447  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:49:55.554447  9524 solver.cpp:237]     Train net output #1: loss = 0.197502 (* 1 = 0.197502 loss)
I1122 09:49:55.554447  9524 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1122 09:49:59.808893  9524 solver.cpp:218] Iteration 28700 (23.504 iter/s, 4.25459s/100 iters), loss = 0.122071
I1122 09:49:59.808893  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:49:59.808893  9524 solver.cpp:237]     Train net output #1: loss = 0.12207 (* 1 = 0.12207 loss)
I1122 09:49:59.808893  9524 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1122 09:50:04.060613  9524 solver.cpp:218] Iteration 28800 (23.5256 iter/s, 4.25069s/100 iters), loss = 0.159784
I1122 09:50:04.060613  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:50:04.060613  9524 solver.cpp:237]     Train net output #1: loss = 0.159783 (* 1 = 0.159783 loss)
I1122 09:50:04.060613  9524 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1122 09:50:08.315465  9524 solver.cpp:218] Iteration 28900 (23.5049 iter/s, 4.25443s/100 iters), loss = 0.0877197
I1122 09:50:08.315465  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 09:50:08.315465  9524 solver.cpp:237]     Train net output #1: loss = 0.0877194 (* 1 = 0.0877194 loss)
I1122 09:50:08.315465  9524 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1122 09:50:12.356159 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:50:12.522655  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_29000.caffemodel
I1122 09:50:12.559156  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_29000.solverstate
I1122 09:50:12.563156  9524 solver.cpp:330] Iteration 29000, Testing net (#0)
I1122 09:50:12.563156  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:50:13.624336  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:50:13.666335  9524 solver.cpp:397]     Test net output #0: accuracy = 0.9
I1122 09:50:13.666335  9524 solver.cpp:397]     Test net output #1: loss = 0.296263 (* 1 = 0.296263 loss)
I1122 09:50:13.707875  9524 solver.cpp:218] Iteration 29000 (18.5461 iter/s, 5.39195s/100 iters), loss = 0.154383
I1122 09:50:13.707875  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:50:13.707875  9524 solver.cpp:237]     Train net output #1: loss = 0.154383 (* 1 = 0.154383 loss)
I1122 09:50:13.707875  9524 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1122 09:50:17.978529  9524 solver.cpp:218] Iteration 29100 (23.4164 iter/s, 4.2705s/100 iters), loss = 0.157542
I1122 09:50:17.978529  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 09:50:17.978529  9524 solver.cpp:237]     Train net output #1: loss = 0.157541 (* 1 = 0.157541 loss)
I1122 09:50:17.978529  9524 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1122 09:50:22.257541  9524 solver.cpp:218] Iteration 29200 (23.369 iter/s, 4.27917s/100 iters), loss = 0.145484
I1122 09:50:22.257541  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:50:22.257541  9524 solver.cpp:237]     Train net output #1: loss = 0.145483 (* 1 = 0.145483 loss)
I1122 09:50:22.257541  9524 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1122 09:50:26.597126  9524 solver.cpp:218] Iteration 29300 (23.0456 iter/s, 4.33923s/100 iters), loss = 0.17035
I1122 09:50:26.597126  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:50:26.597126  9524 solver.cpp:237]     Train net output #1: loss = 0.17035 (* 1 = 0.17035 loss)
I1122 09:50:26.597126  9524 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1122 09:50:30.868762  9524 solver.cpp:218] Iteration 29400 (23.4129 iter/s, 4.27115s/100 iters), loss = 0.0971794
I1122 09:50:30.868762  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 09:50:30.868762  9524 solver.cpp:237]     Train net output #1: loss = 0.0971791 (* 1 = 0.0971791 loss)
I1122 09:50:30.868762  9524 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1122 09:50:34.929181 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:50:35.097218  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_29500.caffemodel
I1122 09:50:35.110733  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_29500.solverstate
I1122 09:50:35.114727  9524 solver.cpp:330] Iteration 29500, Testing net (#0)
I1122 09:50:35.114727  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:50:36.172904  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:50:36.214429  9524 solver.cpp:397]     Test net output #0: accuracy = 0.8998
I1122 09:50:36.214429  9524 solver.cpp:397]     Test net output #1: loss = 0.296312 (* 1 = 0.296312 loss)
I1122 09:50:36.254963  9524 solver.cpp:218] Iteration 29500 (18.5679 iter/s, 5.38564s/100 iters), loss = 0.1502
I1122 09:50:36.254963  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 09:50:36.254963  9524 solver.cpp:237]     Train net output #1: loss = 0.1502 (* 1 = 0.1502 loss)
I1122 09:50:36.254963  9524 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1122 09:50:40.504642  9524 solver.cpp:218] Iteration 29600 (23.5337 iter/s, 4.24923s/100 iters), loss = 0.160943
I1122 09:50:40.504642  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 09:50:40.504642  9524 solver.cpp:237]     Train net output #1: loss = 0.160942 (* 1 = 0.160942 loss)
I1122 09:50:40.504642  9524 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1122 09:50:44.743671  9524 solver.cpp:218] Iteration 29700 (23.5945 iter/s, 4.23828s/100 iters), loss = 0.12376
I1122 09:50:44.743671  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:50:44.743671  9524 solver.cpp:237]     Train net output #1: loss = 0.12376 (* 1 = 0.12376 loss)
I1122 09:50:44.743671  9524 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1122 09:50:48.993906  9524 solver.cpp:218] Iteration 29800 (23.5298 iter/s, 4.24993s/100 iters), loss = 0.14719
I1122 09:50:48.993906  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 09:50:48.993906  9524 solver.cpp:237]     Train net output #1: loss = 0.14719 (* 1 = 0.14719 loss)
I1122 09:50:48.993906  9524 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1122 09:50:53.225883  9524 solver.cpp:218] Iteration 29900 (23.6328 iter/s, 4.23141s/100 iters), loss = 0.0990583
I1122 09:50:53.225883  9524 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 09:50:53.225883  9524 solver.cpp:237]     Train net output #1: loss = 0.099058 (* 1 = 0.099058 loss)
I1122 09:50:53.225883  9524 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1122 09:50:57.257769 18160 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:50:57.423272  9524 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_30000.caffemodel
I1122 09:50:57.449772  9524 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_30000.solverstate
I1122 09:50:57.466272  9524 solver.cpp:310] Iteration 30000, loss = 0.140814
I1122 09:50:57.466272  9524 solver.cpp:330] Iteration 30000, Testing net (#0)
I1122 09:50:57.466272  9524 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:50:58.522429  4976 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:50:58.563429  9524 solver.cpp:397]     Test net output #0: accuracy = 0.9
I1122 09:50:58.563429  9524 solver.cpp:397]     Test net output #1: loss = 0.29632 (* 1 = 0.29632 loss)
I1122 09:50:58.563429  9524 solver.cpp:315] Optimization Done.
I1122 09:50:58.563429  9524 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 

G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1122 09:57:51.413097 19984 caffe.cpp:219] Using GPUs 0
I1122 09:57:51.588217 19984 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1122 09:57:51.891028 19984 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 09:57:51.907043 19984 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_300k_8L"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1122 09:57:51.908030 19984 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 09:57:51.908030 19984 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 09:57:51.908030 19984 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1122 09:57:51.908030 19984 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1122 09:57:51.909047 19984 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1122 09:57:51.909047 19984 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1122 09:57:51.909047 19984 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1122 09:57:51.909047 19984 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1122 09:57:51.909047 19984 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1122 09:57:51.909047 19984 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1122 09:57:51.909047 19984 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1122 09:57:51.909047 19984 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1122 09:57:51.909047 19984 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1122 09:57:51.909047 19984 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 41
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 43
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 85
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1122 09:57:51.912070 19984 layer_factory.cpp:58] Creating layer cifar
I1122 09:57:51.920029 19984 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1122 09:57:51.920029 19984 net.cpp:84] Creating Layer cifar
I1122 09:57:51.920029 19984 net.cpp:380] cifar -> data
I1122 09:57:51.920029 19984 net.cpp:380] cifar -> label
I1122 09:57:51.921048 19984 data_layer.cpp:45] output data size: 100,3,32,32
I1122 09:57:51.926028 19984 net.cpp:122] Setting up cifar
I1122 09:57:51.926028 19984 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1122 09:57:51.926028 19984 net.cpp:129] Top shape: 100 (100)
I1122 09:57:51.926028 19984 net.cpp:137] Memory required for data: 1229200
I1122 09:57:51.926028 19984 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1122 09:57:51.926028 19984 net.cpp:84] Creating Layer label_cifar_1_split
I1122 09:57:51.926028 19984 net.cpp:406] label_cifar_1_split <- label
I1122 09:57:51.926028 19984 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1122 09:57:51.926028 19984 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1122 09:57:51.926028 19984 net.cpp:122] Setting up label_cifar_1_split
I1122 09:57:51.926028 19984 net.cpp:129] Top shape: 100 (100)
I1122 09:57:51.926028 19984 net.cpp:129] Top shape: 100 (100)
I1122 09:57:51.926028 19984 net.cpp:137] Memory required for data: 1230000
I1122 09:57:51.926028 19984 layer_factory.cpp:58] Creating layer conv1
I1122 09:57:51.926028 19984 net.cpp:84] Creating Layer conv1
I1122 09:57:51.926028 19984 net.cpp:406] conv1 <- data
I1122 09:57:51.926028 19984 net.cpp:380] conv1 -> conv1
I1122 09:57:51.927028 10096 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 09:57:52.170099 19984 net.cpp:122] Setting up conv1
I1122 09:57:52.170099 19984 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:57:52.170099 19984 net.cpp:137] Memory required for data: 18023600
I1122 09:57:52.170099 19984 layer_factory.cpp:58] Creating layer bn1
I1122 09:57:52.170099 19984 net.cpp:84] Creating Layer bn1
I1122 09:57:52.170099 19984 net.cpp:406] bn1 <- conv1
I1122 09:57:52.170099 19984 net.cpp:367] bn1 -> conv1 (in-place)
I1122 09:57:52.170598 19984 net.cpp:122] Setting up bn1
I1122 09:57:52.170598 19984 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:57:52.170598 19984 net.cpp:137] Memory required for data: 34817200
I1122 09:57:52.170598 19984 layer_factory.cpp:58] Creating layer scale1
I1122 09:57:52.170598 19984 net.cpp:84] Creating Layer scale1
I1122 09:57:52.170598 19984 net.cpp:406] scale1 <- conv1
I1122 09:57:52.170598 19984 net.cpp:367] scale1 -> conv1 (in-place)
I1122 09:57:52.170598 19984 layer_factory.cpp:58] Creating layer scale1
I1122 09:57:52.170598 19984 net.cpp:122] Setting up scale1
I1122 09:57:52.170598 19984 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:57:52.170598 19984 net.cpp:137] Memory required for data: 51610800
I1122 09:57:52.170598 19984 layer_factory.cpp:58] Creating layer relu1
I1122 09:57:52.170598 19984 net.cpp:84] Creating Layer relu1
I1122 09:57:52.170598 19984 net.cpp:406] relu1 <- conv1
I1122 09:57:52.170598 19984 net.cpp:367] relu1 -> conv1 (in-place)
I1122 09:57:52.171098 19984 net.cpp:122] Setting up relu1
I1122 09:57:52.171098 19984 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:57:52.171098 19984 net.cpp:137] Memory required for data: 68404400
I1122 09:57:52.171098 19984 layer_factory.cpp:58] Creating layer conv2
I1122 09:57:52.171098 19984 net.cpp:84] Creating Layer conv2
I1122 09:57:52.171098 19984 net.cpp:406] conv2 <- conv1
I1122 09:57:52.171098 19984 net.cpp:380] conv2 -> conv2
I1122 09:57:52.172598 19984 net.cpp:122] Setting up conv2
I1122 09:57:52.172598 19984 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:57:52.172598 19984 net.cpp:137] Memory required for data: 86017200
I1122 09:57:52.173105 19984 layer_factory.cpp:58] Creating layer bn2
I1122 09:57:52.173105 19984 net.cpp:84] Creating Layer bn2
I1122 09:57:52.173105 19984 net.cpp:406] bn2 <- conv2
I1122 09:57:52.173105 19984 net.cpp:367] bn2 -> conv2 (in-place)
I1122 09:57:52.173105 19984 net.cpp:122] Setting up bn2
I1122 09:57:52.173105 19984 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:57:52.173105 19984 net.cpp:137] Memory required for data: 103630000
I1122 09:57:52.173105 19984 layer_factory.cpp:58] Creating layer scale2
I1122 09:57:52.173105 19984 net.cpp:84] Creating Layer scale2
I1122 09:57:52.173105 19984 net.cpp:406] scale2 <- conv2
I1122 09:57:52.173105 19984 net.cpp:367] scale2 -> conv2 (in-place)
I1122 09:57:52.173105 19984 layer_factory.cpp:58] Creating layer scale2
I1122 09:57:52.173105 19984 net.cpp:122] Setting up scale2
I1122 09:57:52.173105 19984 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:57:52.173105 19984 net.cpp:137] Memory required for data: 121242800
I1122 09:57:52.173105 19984 layer_factory.cpp:58] Creating layer relu2
I1122 09:57:52.173105 19984 net.cpp:84] Creating Layer relu2
I1122 09:57:52.173105 19984 net.cpp:406] relu2 <- conv2
I1122 09:57:52.173105 19984 net.cpp:367] relu2 -> conv2 (in-place)
I1122 09:57:52.173610 19984 net.cpp:122] Setting up relu2
I1122 09:57:52.173610 19984 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:57:52.173610 19984 net.cpp:137] Memory required for data: 138855600
I1122 09:57:52.173610 19984 layer_factory.cpp:58] Creating layer conv2_2
I1122 09:57:52.173610 19984 net.cpp:84] Creating Layer conv2_2
I1122 09:57:52.173610 19984 net.cpp:406] conv2_2 <- conv2
I1122 09:57:52.173610 19984 net.cpp:380] conv2_2 -> conv2_2
I1122 09:57:52.175611 19984 net.cpp:122] Setting up conv2_2
I1122 09:57:52.175611 19984 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:57:52.175611 19984 net.cpp:137] Memory required for data: 167527600
I1122 09:57:52.175611 19984 layer_factory.cpp:58] Creating layer bn2_2
I1122 09:57:52.175611 19984 net.cpp:84] Creating Layer bn2_2
I1122 09:57:52.175611 19984 net.cpp:406] bn2_2 <- conv2_2
I1122 09:57:52.175611 19984 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1122 09:57:52.175611 19984 net.cpp:122] Setting up bn2_2
I1122 09:57:52.175611 19984 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:57:52.175611 19984 net.cpp:137] Memory required for data: 196199600
I1122 09:57:52.175611 19984 layer_factory.cpp:58] Creating layer scale2_2
I1122 09:57:52.176111 19984 net.cpp:84] Creating Layer scale2_2
I1122 09:57:52.176111 19984 net.cpp:406] scale2_2 <- conv2_2
I1122 09:57:52.176111 19984 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1122 09:57:52.176111 19984 layer_factory.cpp:58] Creating layer scale2_2
I1122 09:57:52.176111 19984 net.cpp:122] Setting up scale2_2
I1122 09:57:52.176111 19984 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:57:52.176111 19984 net.cpp:137] Memory required for data: 224871600
I1122 09:57:52.176111 19984 layer_factory.cpp:58] Creating layer relu2_2
I1122 09:57:52.176111 19984 net.cpp:84] Creating Layer relu2_2
I1122 09:57:52.176111 19984 net.cpp:406] relu2_2 <- conv2_2
I1122 09:57:52.176111 19984 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1122 09:57:52.176111 19984 net.cpp:122] Setting up relu2_2
I1122 09:57:52.176111 19984 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:57:52.176111 19984 net.cpp:137] Memory required for data: 253543600
I1122 09:57:52.176111 19984 layer_factory.cpp:58] Creating layer pool2_1
I1122 09:57:52.176111 19984 net.cpp:84] Creating Layer pool2_1
I1122 09:57:52.176111 19984 net.cpp:406] pool2_1 <- conv2_2
I1122 09:57:52.176111 19984 net.cpp:380] pool2_1 -> pool2_1
I1122 09:57:52.176111 19984 net.cpp:122] Setting up pool2_1
I1122 09:57:52.176111 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.176111 19984 net.cpp:137] Memory required for data: 260711600
I1122 09:57:52.176111 19984 layer_factory.cpp:58] Creating layer conv3
I1122 09:57:52.176111 19984 net.cpp:84] Creating Layer conv3
I1122 09:57:52.176610 19984 net.cpp:406] conv3 <- pool2_1
I1122 09:57:52.176610 19984 net.cpp:380] conv3 -> conv3
I1122 09:57:52.177609 19984 net.cpp:122] Setting up conv3
I1122 09:57:52.177609 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.177609 19984 net.cpp:137] Memory required for data: 267879600
I1122 09:57:52.177609 19984 layer_factory.cpp:58] Creating layer bn3
I1122 09:57:52.177609 19984 net.cpp:84] Creating Layer bn3
I1122 09:57:52.177609 19984 net.cpp:406] bn3 <- conv3
I1122 09:57:52.177609 19984 net.cpp:367] bn3 -> conv3 (in-place)
I1122 09:57:52.177609 19984 net.cpp:122] Setting up bn3
I1122 09:57:52.177609 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.177609 19984 net.cpp:137] Memory required for data: 275047600
I1122 09:57:52.177609 19984 layer_factory.cpp:58] Creating layer scale3
I1122 09:57:52.177609 19984 net.cpp:84] Creating Layer scale3
I1122 09:57:52.177609 19984 net.cpp:406] scale3 <- conv3
I1122 09:57:52.177609 19984 net.cpp:367] scale3 -> conv3 (in-place)
I1122 09:57:52.177609 19984 layer_factory.cpp:58] Creating layer scale3
I1122 09:57:52.177609 19984 net.cpp:122] Setting up scale3
I1122 09:57:52.177609 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.177609 19984 net.cpp:137] Memory required for data: 282215600
I1122 09:57:52.177609 19984 layer_factory.cpp:58] Creating layer relu3
I1122 09:57:52.177609 19984 net.cpp:84] Creating Layer relu3
I1122 09:57:52.177609 19984 net.cpp:406] relu3 <- conv3
I1122 09:57:52.177609 19984 net.cpp:367] relu3 -> conv3 (in-place)
I1122 09:57:52.178619 19984 net.cpp:122] Setting up relu3
I1122 09:57:52.178619 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.178619 19984 net.cpp:137] Memory required for data: 289383600
I1122 09:57:52.178619 19984 layer_factory.cpp:58] Creating layer conv4
I1122 09:57:52.178619 19984 net.cpp:84] Creating Layer conv4
I1122 09:57:52.178619 19984 net.cpp:406] conv4 <- conv3
I1122 09:57:52.178619 19984 net.cpp:380] conv4 -> conv4
I1122 09:57:52.179620 19984 net.cpp:122] Setting up conv4
I1122 09:57:52.179620 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.179620 19984 net.cpp:137] Memory required for data: 296551600
I1122 09:57:52.179620 19984 layer_factory.cpp:58] Creating layer bn4
I1122 09:57:52.179620 19984 net.cpp:84] Creating Layer bn4
I1122 09:57:52.179620 19984 net.cpp:406] bn4 <- conv4
I1122 09:57:52.179620 19984 net.cpp:367] bn4 -> conv4 (in-place)
I1122 09:57:52.179620 19984 net.cpp:122] Setting up bn4
I1122 09:57:52.179620 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.179620 19984 net.cpp:137] Memory required for data: 303719600
I1122 09:57:52.179620 19984 layer_factory.cpp:58] Creating layer scale4
I1122 09:57:52.179620 19984 net.cpp:84] Creating Layer scale4
I1122 09:57:52.179620 19984 net.cpp:406] scale4 <- conv4
I1122 09:57:52.179620 19984 net.cpp:367] scale4 -> conv4 (in-place)
I1122 09:57:52.179620 19984 layer_factory.cpp:58] Creating layer scale4
I1122 09:57:52.180620 19984 net.cpp:122] Setting up scale4
I1122 09:57:52.180620 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.180620 19984 net.cpp:137] Memory required for data: 310887600
I1122 09:57:52.180620 19984 layer_factory.cpp:58] Creating layer relu4
I1122 09:57:52.180620 19984 net.cpp:84] Creating Layer relu4
I1122 09:57:52.180620 19984 net.cpp:406] relu4 <- conv4
I1122 09:57:52.180620 19984 net.cpp:367] relu4 -> conv4 (in-place)
I1122 09:57:52.180620 19984 net.cpp:122] Setting up relu4
I1122 09:57:52.180620 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.180620 19984 net.cpp:137] Memory required for data: 318055600
I1122 09:57:52.180620 19984 layer_factory.cpp:58] Creating layer conv4_1
I1122 09:57:52.180620 19984 net.cpp:84] Creating Layer conv4_1
I1122 09:57:52.180620 19984 net.cpp:406] conv4_1 <- conv4
I1122 09:57:52.180620 19984 net.cpp:380] conv4_1 -> conv4_1
I1122 09:57:52.181619 19984 net.cpp:122] Setting up conv4_1
I1122 09:57:52.181619 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.181619 19984 net.cpp:137] Memory required for data: 325223600
I1122 09:57:52.181619 19984 layer_factory.cpp:58] Creating layer bn4_1
I1122 09:57:52.181619 19984 net.cpp:84] Creating Layer bn4_1
I1122 09:57:52.181619 19984 net.cpp:406] bn4_1 <- conv4_1
I1122 09:57:52.181619 19984 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1122 09:57:52.182620 19984 net.cpp:122] Setting up bn4_1
I1122 09:57:52.182620 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.182620 19984 net.cpp:137] Memory required for data: 332391600
I1122 09:57:52.182620 19984 layer_factory.cpp:58] Creating layer scale4_1
I1122 09:57:52.182620 19984 net.cpp:84] Creating Layer scale4_1
I1122 09:57:52.182620 19984 net.cpp:406] scale4_1 <- conv4_1
I1122 09:57:52.182620 19984 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1122 09:57:52.182620 19984 layer_factory.cpp:58] Creating layer scale4_1
I1122 09:57:52.182620 19984 net.cpp:122] Setting up scale4_1
I1122 09:57:52.182620 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.182620 19984 net.cpp:137] Memory required for data: 339559600
I1122 09:57:52.182620 19984 layer_factory.cpp:58] Creating layer relu4_1
I1122 09:57:52.182620 19984 net.cpp:84] Creating Layer relu4_1
I1122 09:57:52.182620 19984 net.cpp:406] relu4_1 <- conv4_1
I1122 09:57:52.182620 19984 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1122 09:57:52.182620 19984 net.cpp:122] Setting up relu4_1
I1122 09:57:52.182620 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.182620 19984 net.cpp:137] Memory required for data: 346727600
I1122 09:57:52.182620 19984 layer_factory.cpp:58] Creating layer conv4_2
I1122 09:57:52.182620 19984 net.cpp:84] Creating Layer conv4_2
I1122 09:57:52.182620 19984 net.cpp:406] conv4_2 <- conv4_1
I1122 09:57:52.182620 19984 net.cpp:380] conv4_2 -> conv4_2
I1122 09:57:52.184609 19984 net.cpp:122] Setting up conv4_2
I1122 09:57:52.184609 19984 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:57:52.184609 19984 net.cpp:137] Memory required for data: 355431600
I1122 09:57:52.184609 19984 layer_factory.cpp:58] Creating layer bn4_2
I1122 09:57:52.184609 19984 net.cpp:84] Creating Layer bn4_2
I1122 09:57:52.184609 19984 net.cpp:406] bn4_2 <- conv4_2
I1122 09:57:52.184609 19984 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1122 09:57:52.184609 19984 net.cpp:122] Setting up bn4_2
I1122 09:57:52.184609 19984 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:57:52.184609 19984 net.cpp:137] Memory required for data: 364135600
I1122 09:57:52.184609 19984 layer_factory.cpp:58] Creating layer scale4_2
I1122 09:57:52.184609 19984 net.cpp:84] Creating Layer scale4_2
I1122 09:57:52.184609 19984 net.cpp:406] scale4_2 <- conv4_2
I1122 09:57:52.184609 19984 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1122 09:57:52.184609 19984 layer_factory.cpp:58] Creating layer scale4_2
I1122 09:57:52.184609 19984 net.cpp:122] Setting up scale4_2
I1122 09:57:52.184609 19984 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:57:52.184609 19984 net.cpp:137] Memory required for data: 372839600
I1122 09:57:52.184609 19984 layer_factory.cpp:58] Creating layer relu4_2
I1122 09:57:52.184609 19984 net.cpp:84] Creating Layer relu4_2
I1122 09:57:52.184609 19984 net.cpp:406] relu4_2 <- conv4_2
I1122 09:57:52.184609 19984 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1122 09:57:52.184609 19984 net.cpp:122] Setting up relu4_2
I1122 09:57:52.184609 19984 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:57:52.184609 19984 net.cpp:137] Memory required for data: 381543600
I1122 09:57:52.184609 19984 layer_factory.cpp:58] Creating layer pool4_2
I1122 09:57:52.184609 19984 net.cpp:84] Creating Layer pool4_2
I1122 09:57:52.184609 19984 net.cpp:406] pool4_2 <- conv4_2
I1122 09:57:52.184609 19984 net.cpp:380] pool4_2 -> pool4_2
I1122 09:57:52.184609 19984 net.cpp:122] Setting up pool4_2
I1122 09:57:52.184609 19984 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 09:57:52.184609 19984 net.cpp:137] Memory required for data: 383719600
I1122 09:57:52.184609 19984 layer_factory.cpp:58] Creating layer conv12
I1122 09:57:52.184609 19984 net.cpp:84] Creating Layer conv12
I1122 09:57:52.184609 19984 net.cpp:406] conv12 <- pool4_2
I1122 09:57:52.184609 19984 net.cpp:380] conv12 -> conv12
I1122 09:57:52.186620 19984 net.cpp:122] Setting up conv12
I1122 09:57:52.186620 19984 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:57:52.186620 19984 net.cpp:137] Memory required for data: 386023600
I1122 09:57:52.186620 19984 layer_factory.cpp:58] Creating layer bn_conv12
I1122 09:57:52.186620 19984 net.cpp:84] Creating Layer bn_conv12
I1122 09:57:52.186620 19984 net.cpp:406] bn_conv12 <- conv12
I1122 09:57:52.186620 19984 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1122 09:57:52.187624 19984 net.cpp:122] Setting up bn_conv12
I1122 09:57:52.187624 19984 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:57:52.187624 19984 net.cpp:137] Memory required for data: 388327600
I1122 09:57:52.187624 19984 layer_factory.cpp:58] Creating layer scale_conv12
I1122 09:57:52.187624 19984 net.cpp:84] Creating Layer scale_conv12
I1122 09:57:52.187624 19984 net.cpp:406] scale_conv12 <- conv12
I1122 09:57:52.187624 19984 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1122 09:57:52.187624 19984 layer_factory.cpp:58] Creating layer scale_conv12
I1122 09:57:52.187624 19984 net.cpp:122] Setting up scale_conv12
I1122 09:57:52.187624 19984 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:57:52.187624 19984 net.cpp:137] Memory required for data: 390631600
I1122 09:57:52.187624 19984 layer_factory.cpp:58] Creating layer relu_conv12
I1122 09:57:52.187624 19984 net.cpp:84] Creating Layer relu_conv12
I1122 09:57:52.187624 19984 net.cpp:406] relu_conv12 <- conv12
I1122 09:57:52.187624 19984 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1122 09:57:52.187624 19984 net.cpp:122] Setting up relu_conv12
I1122 09:57:52.187624 19984 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:57:52.187624 19984 net.cpp:137] Memory required for data: 392935600
I1122 09:57:52.187624 19984 layer_factory.cpp:58] Creating layer poolcp6
I1122 09:57:52.187624 19984 net.cpp:84] Creating Layer poolcp6
I1122 09:57:52.187624 19984 net.cpp:406] poolcp6 <- conv12
I1122 09:57:52.187624 19984 net.cpp:380] poolcp6 -> poolcp6
I1122 09:57:52.187624 19984 net.cpp:122] Setting up poolcp6
I1122 09:57:52.187624 19984 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1122 09:57:52.187624 19984 net.cpp:137] Memory required for data: 392971600
I1122 09:57:52.187624 19984 layer_factory.cpp:58] Creating layer ip1
I1122 09:57:52.187624 19984 net.cpp:84] Creating Layer ip1
I1122 09:57:52.187624 19984 net.cpp:406] ip1 <- poolcp6
I1122 09:57:52.187624 19984 net.cpp:380] ip1 -> ip1
I1122 09:57:52.187624 19984 net.cpp:122] Setting up ip1
I1122 09:57:52.187624 19984 net.cpp:129] Top shape: 100 10 (1000)
I1122 09:57:52.187624 19984 net.cpp:137] Memory required for data: 392975600
I1122 09:57:52.187624 19984 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1122 09:57:52.187624 19984 net.cpp:84] Creating Layer ip1_ip1_0_split
I1122 09:57:52.187624 19984 net.cpp:406] ip1_ip1_0_split <- ip1
I1122 09:57:52.187624 19984 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1122 09:57:52.187624 19984 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1122 09:57:52.187624 19984 net.cpp:122] Setting up ip1_ip1_0_split
I1122 09:57:52.187624 19984 net.cpp:129] Top shape: 100 10 (1000)
I1122 09:57:52.187624 19984 net.cpp:129] Top shape: 100 10 (1000)
I1122 09:57:52.187624 19984 net.cpp:137] Memory required for data: 392983600
I1122 09:57:52.187624 19984 layer_factory.cpp:58] Creating layer accuracy_training
I1122 09:57:52.187624 19984 net.cpp:84] Creating Layer accuracy_training
I1122 09:57:52.187624 19984 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1122 09:57:52.188623 19984 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1122 09:57:52.188623 19984 net.cpp:380] accuracy_training -> accuracy_training
I1122 09:57:52.188623 19984 net.cpp:122] Setting up accuracy_training
I1122 09:57:52.188623 19984 net.cpp:129] Top shape: (1)
I1122 09:57:52.188623 19984 net.cpp:137] Memory required for data: 392983604
I1122 09:57:52.188623 19984 layer_factory.cpp:58] Creating layer loss
I1122 09:57:52.188623 19984 net.cpp:84] Creating Layer loss
I1122 09:57:52.188623 19984 net.cpp:406] loss <- ip1_ip1_0_split_1
I1122 09:57:52.188623 19984 net.cpp:406] loss <- label_cifar_1_split_1
I1122 09:57:52.188623 19984 net.cpp:380] loss -> loss
I1122 09:57:52.188623 19984 layer_factory.cpp:58] Creating layer loss
I1122 09:57:52.188623 19984 net.cpp:122] Setting up loss
I1122 09:57:52.188623 19984 net.cpp:129] Top shape: (1)
I1122 09:57:52.188623 19984 net.cpp:132]     with loss weight 1
I1122 09:57:52.188623 19984 net.cpp:137] Memory required for data: 392983608
I1122 09:57:52.188623 19984 net.cpp:198] loss needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:200] accuracy_training does not need backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] ip1 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] poolcp6 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] relu_conv12 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] scale_conv12 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] bn_conv12 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] conv12 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] pool4_2 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] relu4_2 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] scale4_2 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] bn4_2 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] conv4_2 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] relu4_1 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] scale4_1 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] bn4_1 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] conv4_1 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] relu4 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] scale4 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] bn4 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] conv4 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] relu3 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] scale3 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] bn3 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] conv3 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] pool2_1 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] relu2_2 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] scale2_2 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] bn2_2 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] conv2_2 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] relu2 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] scale2 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] bn2 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] conv2 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] relu1 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] scale1 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] bn1 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:198] conv1 needs backward computation.
I1122 09:57:52.188623 19984 net.cpp:200] label_cifar_1_split does not need backward computation.
I1122 09:57:52.188623 19984 net.cpp:200] cifar does not need backward computation.
I1122 09:57:52.188623 19984 net.cpp:242] This network produces output accuracy_training
I1122 09:57:52.188623 19984 net.cpp:242] This network produces output loss
I1122 09:57:52.188623 19984 net.cpp:255] Network initialization done.
I1122 09:57:52.189625 19984 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 09:57:52.189625 19984 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1122 09:57:52.189625 19984 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 09:57:52.189625 19984 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1122 09:57:52.189625 19984 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1122 09:57:52.189625 19984 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1122 09:57:52.189625 19984 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1122 09:57:52.189625 19984 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1122 09:57:52.189625 19984 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1122 09:57:52.189625 19984 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1122 09:57:52.189625 19984 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1122 09:57:52.189625 19984 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1122 09:57:52.189625 19984 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1122 09:57:52.189625 19984 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 41
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 43
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 70
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 85
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1122 09:57:52.189625 19984 layer_factory.cpp:58] Creating layer cifar
I1122 09:57:52.196627 19984 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1122 09:57:52.196627 19984 net.cpp:84] Creating Layer cifar
I1122 09:57:52.196627 19984 net.cpp:380] cifar -> data
I1122 09:57:52.196627 19984 net.cpp:380] cifar -> label
I1122 09:57:52.196627 19984 data_layer.cpp:45] output data size: 100,3,32,32
I1122 09:57:52.202610 19984 net.cpp:122] Setting up cifar
I1122 09:57:52.202610 19984 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1122 09:57:52.202610 19984 net.cpp:129] Top shape: 100 (100)
I1122 09:57:52.202610 19984 net.cpp:137] Memory required for data: 1229200
I1122 09:57:52.202610 19984 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1122 09:57:52.202610 19984 net.cpp:84] Creating Layer label_cifar_1_split
I1122 09:57:52.202610 19984 net.cpp:406] label_cifar_1_split <- label
I1122 09:57:52.202610 19984 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1122 09:57:52.202610 19984 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1122 09:57:52.202610 19984 net.cpp:122] Setting up label_cifar_1_split
I1122 09:57:52.202610 19984 net.cpp:129] Top shape: 100 (100)
I1122 09:57:52.202610 19984 net.cpp:129] Top shape: 100 (100)
I1122 09:57:52.202610 19984 net.cpp:137] Memory required for data: 1230000
I1122 09:57:52.202610 19984 layer_factory.cpp:58] Creating layer conv1
I1122 09:57:52.202610 19984 net.cpp:84] Creating Layer conv1
I1122 09:57:52.202610 19984 net.cpp:406] conv1 <- data
I1122 09:57:52.202610 19984 net.cpp:380] conv1 -> conv1
I1122 09:57:52.203629 17228 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 09:57:52.204628 19984 net.cpp:122] Setting up conv1
I1122 09:57:52.204628 19984 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:57:52.204628 19984 net.cpp:137] Memory required for data: 18023600
I1122 09:57:52.204628 19984 layer_factory.cpp:58] Creating layer bn1
I1122 09:57:52.204628 19984 net.cpp:84] Creating Layer bn1
I1122 09:57:52.204628 19984 net.cpp:406] bn1 <- conv1
I1122 09:57:52.204628 19984 net.cpp:367] bn1 -> conv1 (in-place)
I1122 09:57:52.204628 19984 net.cpp:122] Setting up bn1
I1122 09:57:52.204628 19984 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:57:52.204628 19984 net.cpp:137] Memory required for data: 34817200
I1122 09:57:52.204628 19984 layer_factory.cpp:58] Creating layer scale1
I1122 09:57:52.204628 19984 net.cpp:84] Creating Layer scale1
I1122 09:57:52.204628 19984 net.cpp:406] scale1 <- conv1
I1122 09:57:52.204628 19984 net.cpp:367] scale1 -> conv1 (in-place)
I1122 09:57:52.204628 19984 layer_factory.cpp:58] Creating layer scale1
I1122 09:57:52.204628 19984 net.cpp:122] Setting up scale1
I1122 09:57:52.204628 19984 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:57:52.204628 19984 net.cpp:137] Memory required for data: 51610800
I1122 09:57:52.204628 19984 layer_factory.cpp:58] Creating layer relu1
I1122 09:57:52.204628 19984 net.cpp:84] Creating Layer relu1
I1122 09:57:52.204628 19984 net.cpp:406] relu1 <- conv1
I1122 09:57:52.204628 19984 net.cpp:367] relu1 -> conv1 (in-place)
I1122 09:57:52.205610 19984 net.cpp:122] Setting up relu1
I1122 09:57:52.205610 19984 net.cpp:129] Top shape: 100 41 32 32 (4198400)
I1122 09:57:52.205610 19984 net.cpp:137] Memory required for data: 68404400
I1122 09:57:52.205610 19984 layer_factory.cpp:58] Creating layer conv2
I1122 09:57:52.205610 19984 net.cpp:84] Creating Layer conv2
I1122 09:57:52.205610 19984 net.cpp:406] conv2 <- conv1
I1122 09:57:52.205610 19984 net.cpp:380] conv2 -> conv2
I1122 09:57:52.206629 19984 net.cpp:122] Setting up conv2
I1122 09:57:52.206629 19984 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:57:52.206629 19984 net.cpp:137] Memory required for data: 86017200
I1122 09:57:52.206629 19984 layer_factory.cpp:58] Creating layer bn2
I1122 09:57:52.206629 19984 net.cpp:84] Creating Layer bn2
I1122 09:57:52.206629 19984 net.cpp:406] bn2 <- conv2
I1122 09:57:52.206629 19984 net.cpp:367] bn2 -> conv2 (in-place)
I1122 09:57:52.206629 19984 net.cpp:122] Setting up bn2
I1122 09:57:52.206629 19984 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:57:52.206629 19984 net.cpp:137] Memory required for data: 103630000
I1122 09:57:52.206629 19984 layer_factory.cpp:58] Creating layer scale2
I1122 09:57:52.206629 19984 net.cpp:84] Creating Layer scale2
I1122 09:57:52.206629 19984 net.cpp:406] scale2 <- conv2
I1122 09:57:52.206629 19984 net.cpp:367] scale2 -> conv2 (in-place)
I1122 09:57:52.206629 19984 layer_factory.cpp:58] Creating layer scale2
I1122 09:57:52.206629 19984 net.cpp:122] Setting up scale2
I1122 09:57:52.206629 19984 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:57:52.206629 19984 net.cpp:137] Memory required for data: 121242800
I1122 09:57:52.206629 19984 layer_factory.cpp:58] Creating layer relu2
I1122 09:57:52.206629 19984 net.cpp:84] Creating Layer relu2
I1122 09:57:52.206629 19984 net.cpp:406] relu2 <- conv2
I1122 09:57:52.206629 19984 net.cpp:367] relu2 -> conv2 (in-place)
I1122 09:57:52.207628 19984 net.cpp:122] Setting up relu2
I1122 09:57:52.207628 19984 net.cpp:129] Top shape: 100 43 32 32 (4403200)
I1122 09:57:52.207628 19984 net.cpp:137] Memory required for data: 138855600
I1122 09:57:52.207628 19984 layer_factory.cpp:58] Creating layer conv2_2
I1122 09:57:52.207628 19984 net.cpp:84] Creating Layer conv2_2
I1122 09:57:52.207628 19984 net.cpp:406] conv2_2 <- conv2
I1122 09:57:52.207628 19984 net.cpp:380] conv2_2 -> conv2_2
I1122 09:57:52.208626 19984 net.cpp:122] Setting up conv2_2
I1122 09:57:52.208626 19984 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:57:52.208626 19984 net.cpp:137] Memory required for data: 167527600
I1122 09:57:52.208626 19984 layer_factory.cpp:58] Creating layer bn2_2
I1122 09:57:52.208626 19984 net.cpp:84] Creating Layer bn2_2
I1122 09:57:52.208626 19984 net.cpp:406] bn2_2 <- conv2_2
I1122 09:57:52.208626 19984 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1122 09:57:52.209628 19984 net.cpp:122] Setting up bn2_2
I1122 09:57:52.209628 19984 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:57:52.209628 19984 net.cpp:137] Memory required for data: 196199600
I1122 09:57:52.209628 19984 layer_factory.cpp:58] Creating layer scale2_2
I1122 09:57:52.209628 19984 net.cpp:84] Creating Layer scale2_2
I1122 09:57:52.209628 19984 net.cpp:406] scale2_2 <- conv2_2
I1122 09:57:52.209628 19984 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1122 09:57:52.209628 19984 layer_factory.cpp:58] Creating layer scale2_2
I1122 09:57:52.209628 19984 net.cpp:122] Setting up scale2_2
I1122 09:57:52.209628 19984 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:57:52.209628 19984 net.cpp:137] Memory required for data: 224871600
I1122 09:57:52.209628 19984 layer_factory.cpp:58] Creating layer relu2_2
I1122 09:57:52.209628 19984 net.cpp:84] Creating Layer relu2_2
I1122 09:57:52.209628 19984 net.cpp:406] relu2_2 <- conv2_2
I1122 09:57:52.209628 19984 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1122 09:57:52.209628 19984 net.cpp:122] Setting up relu2_2
I1122 09:57:52.209628 19984 net.cpp:129] Top shape: 100 70 32 32 (7168000)
I1122 09:57:52.209628 19984 net.cpp:137] Memory required for data: 253543600
I1122 09:57:52.209628 19984 layer_factory.cpp:58] Creating layer pool2_1
I1122 09:57:52.209628 19984 net.cpp:84] Creating Layer pool2_1
I1122 09:57:52.209628 19984 net.cpp:406] pool2_1 <- conv2_2
I1122 09:57:52.209628 19984 net.cpp:380] pool2_1 -> pool2_1
I1122 09:57:52.209628 19984 net.cpp:122] Setting up pool2_1
I1122 09:57:52.209628 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.209628 19984 net.cpp:137] Memory required for data: 260711600
I1122 09:57:52.209628 19984 layer_factory.cpp:58] Creating layer conv3
I1122 09:57:52.209628 19984 net.cpp:84] Creating Layer conv3
I1122 09:57:52.209628 19984 net.cpp:406] conv3 <- pool2_1
I1122 09:57:52.209628 19984 net.cpp:380] conv3 -> conv3
I1122 09:57:52.211630 19984 net.cpp:122] Setting up conv3
I1122 09:57:52.211630 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.211630 19984 net.cpp:137] Memory required for data: 267879600
I1122 09:57:52.211630 19984 layer_factory.cpp:58] Creating layer bn3
I1122 09:57:52.211630 19984 net.cpp:84] Creating Layer bn3
I1122 09:57:52.211630 19984 net.cpp:406] bn3 <- conv3
I1122 09:57:52.211630 19984 net.cpp:367] bn3 -> conv3 (in-place)
I1122 09:57:52.211630 19984 net.cpp:122] Setting up bn3
I1122 09:57:52.211630 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.211630 19984 net.cpp:137] Memory required for data: 275047600
I1122 09:57:52.211630 19984 layer_factory.cpp:58] Creating layer scale3
I1122 09:57:52.211630 19984 net.cpp:84] Creating Layer scale3
I1122 09:57:52.211630 19984 net.cpp:406] scale3 <- conv3
I1122 09:57:52.211630 19984 net.cpp:367] scale3 -> conv3 (in-place)
I1122 09:57:52.212628 19984 layer_factory.cpp:58] Creating layer scale3
I1122 09:57:52.212628 19984 net.cpp:122] Setting up scale3
I1122 09:57:52.212628 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.212628 19984 net.cpp:137] Memory required for data: 282215600
I1122 09:57:52.212628 19984 layer_factory.cpp:58] Creating layer relu3
I1122 09:57:52.212628 19984 net.cpp:84] Creating Layer relu3
I1122 09:57:52.212628 19984 net.cpp:406] relu3 <- conv3
I1122 09:57:52.212628 19984 net.cpp:367] relu3 -> conv3 (in-place)
I1122 09:57:52.212628 19984 net.cpp:122] Setting up relu3
I1122 09:57:52.212628 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.212628 19984 net.cpp:137] Memory required for data: 289383600
I1122 09:57:52.212628 19984 layer_factory.cpp:58] Creating layer conv4
I1122 09:57:52.212628 19984 net.cpp:84] Creating Layer conv4
I1122 09:57:52.212628 19984 net.cpp:406] conv4 <- conv3
I1122 09:57:52.212628 19984 net.cpp:380] conv4 -> conv4
I1122 09:57:52.213629 19984 net.cpp:122] Setting up conv4
I1122 09:57:52.213629 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.213629 19984 net.cpp:137] Memory required for data: 296551600
I1122 09:57:52.213629 19984 layer_factory.cpp:58] Creating layer bn4
I1122 09:57:52.213629 19984 net.cpp:84] Creating Layer bn4
I1122 09:57:52.213629 19984 net.cpp:406] bn4 <- conv4
I1122 09:57:52.213629 19984 net.cpp:367] bn4 -> conv4 (in-place)
I1122 09:57:52.214628 19984 net.cpp:122] Setting up bn4
I1122 09:57:52.214628 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.214628 19984 net.cpp:137] Memory required for data: 303719600
I1122 09:57:52.214628 19984 layer_factory.cpp:58] Creating layer scale4
I1122 09:57:52.214628 19984 net.cpp:84] Creating Layer scale4
I1122 09:57:52.214628 19984 net.cpp:406] scale4 <- conv4
I1122 09:57:52.214628 19984 net.cpp:367] scale4 -> conv4 (in-place)
I1122 09:57:52.214628 19984 layer_factory.cpp:58] Creating layer scale4
I1122 09:57:52.214628 19984 net.cpp:122] Setting up scale4
I1122 09:57:52.214628 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.214628 19984 net.cpp:137] Memory required for data: 310887600
I1122 09:57:52.214628 19984 layer_factory.cpp:58] Creating layer relu4
I1122 09:57:52.214628 19984 net.cpp:84] Creating Layer relu4
I1122 09:57:52.214628 19984 net.cpp:406] relu4 <- conv4
I1122 09:57:52.214628 19984 net.cpp:367] relu4 -> conv4 (in-place)
I1122 09:57:52.214628 19984 net.cpp:122] Setting up relu4
I1122 09:57:52.214628 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.214628 19984 net.cpp:137] Memory required for data: 318055600
I1122 09:57:52.214628 19984 layer_factory.cpp:58] Creating layer conv4_1
I1122 09:57:52.214628 19984 net.cpp:84] Creating Layer conv4_1
I1122 09:57:52.214628 19984 net.cpp:406] conv4_1 <- conv4
I1122 09:57:52.214628 19984 net.cpp:380] conv4_1 -> conv4_1
I1122 09:57:52.216624 19984 net.cpp:122] Setting up conv4_1
I1122 09:57:52.216624 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.216624 19984 net.cpp:137] Memory required for data: 325223600
I1122 09:57:52.216624 19984 layer_factory.cpp:58] Creating layer bn4_1
I1122 09:57:52.216624 19984 net.cpp:84] Creating Layer bn4_1
I1122 09:57:52.216624 19984 net.cpp:406] bn4_1 <- conv4_1
I1122 09:57:52.216624 19984 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1122 09:57:52.216624 19984 net.cpp:122] Setting up bn4_1
I1122 09:57:52.216624 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.216624 19984 net.cpp:137] Memory required for data: 332391600
I1122 09:57:52.216624 19984 layer_factory.cpp:58] Creating layer scale4_1
I1122 09:57:52.216624 19984 net.cpp:84] Creating Layer scale4_1
I1122 09:57:52.216624 19984 net.cpp:406] scale4_1 <- conv4_1
I1122 09:57:52.216624 19984 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1122 09:57:52.216624 19984 layer_factory.cpp:58] Creating layer scale4_1
I1122 09:57:52.216624 19984 net.cpp:122] Setting up scale4_1
I1122 09:57:52.216624 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.216624 19984 net.cpp:137] Memory required for data: 339559600
I1122 09:57:52.216624 19984 layer_factory.cpp:58] Creating layer relu4_1
I1122 09:57:52.216624 19984 net.cpp:84] Creating Layer relu4_1
I1122 09:57:52.216624 19984 net.cpp:406] relu4_1 <- conv4_1
I1122 09:57:52.216624 19984 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1122 09:57:52.216624 19984 net.cpp:122] Setting up relu4_1
I1122 09:57:52.216624 19984 net.cpp:129] Top shape: 100 70 16 16 (1792000)
I1122 09:57:52.216624 19984 net.cpp:137] Memory required for data: 346727600
I1122 09:57:52.216624 19984 layer_factory.cpp:58] Creating layer conv4_2
I1122 09:57:52.216624 19984 net.cpp:84] Creating Layer conv4_2
I1122 09:57:52.216624 19984 net.cpp:406] conv4_2 <- conv4_1
I1122 09:57:52.216624 19984 net.cpp:380] conv4_2 -> conv4_2
I1122 09:57:52.218611 19984 net.cpp:122] Setting up conv4_2
I1122 09:57:52.218611 19984 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:57:52.218611 19984 net.cpp:137] Memory required for data: 355431600
I1122 09:57:52.218611 19984 layer_factory.cpp:58] Creating layer bn4_2
I1122 09:57:52.218611 19984 net.cpp:84] Creating Layer bn4_2
I1122 09:57:52.218611 19984 net.cpp:406] bn4_2 <- conv4_2
I1122 09:57:52.218611 19984 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1122 09:57:52.218611 19984 net.cpp:122] Setting up bn4_2
I1122 09:57:52.218611 19984 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:57:52.218611 19984 net.cpp:137] Memory required for data: 364135600
I1122 09:57:52.218611 19984 layer_factory.cpp:58] Creating layer scale4_2
I1122 09:57:52.218611 19984 net.cpp:84] Creating Layer scale4_2
I1122 09:57:52.218611 19984 net.cpp:406] scale4_2 <- conv4_2
I1122 09:57:52.218611 19984 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1122 09:57:52.218611 19984 layer_factory.cpp:58] Creating layer scale4_2
I1122 09:57:52.219629 19984 net.cpp:122] Setting up scale4_2
I1122 09:57:52.219629 19984 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:57:52.219629 19984 net.cpp:137] Memory required for data: 372839600
I1122 09:57:52.219629 19984 layer_factory.cpp:58] Creating layer relu4_2
I1122 09:57:52.219629 19984 net.cpp:84] Creating Layer relu4_2
I1122 09:57:52.219629 19984 net.cpp:406] relu4_2 <- conv4_2
I1122 09:57:52.219629 19984 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1122 09:57:52.219629 19984 net.cpp:122] Setting up relu4_2
I1122 09:57:52.219629 19984 net.cpp:129] Top shape: 100 85 16 16 (2176000)
I1122 09:57:52.219629 19984 net.cpp:137] Memory required for data: 381543600
I1122 09:57:52.219629 19984 layer_factory.cpp:58] Creating layer pool4_2
I1122 09:57:52.219629 19984 net.cpp:84] Creating Layer pool4_2
I1122 09:57:52.219629 19984 net.cpp:406] pool4_2 <- conv4_2
I1122 09:57:52.219629 19984 net.cpp:380] pool4_2 -> pool4_2
I1122 09:57:52.219629 19984 net.cpp:122] Setting up pool4_2
I1122 09:57:52.219629 19984 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1122 09:57:52.219629 19984 net.cpp:137] Memory required for data: 383719600
I1122 09:57:52.219629 19984 layer_factory.cpp:58] Creating layer conv12
I1122 09:57:52.219629 19984 net.cpp:84] Creating Layer conv12
I1122 09:57:52.219629 19984 net.cpp:406] conv12 <- pool4_2
I1122 09:57:52.219629 19984 net.cpp:380] conv12 -> conv12
I1122 09:57:52.221627 19984 net.cpp:122] Setting up conv12
I1122 09:57:52.221627 19984 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:57:52.221627 19984 net.cpp:137] Memory required for data: 386023600
I1122 09:57:52.221627 19984 layer_factory.cpp:58] Creating layer bn_conv12
I1122 09:57:52.221627 19984 net.cpp:84] Creating Layer bn_conv12
I1122 09:57:52.221627 19984 net.cpp:406] bn_conv12 <- conv12
I1122 09:57:52.221627 19984 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1122 09:57:52.221627 19984 net.cpp:122] Setting up bn_conv12
I1122 09:57:52.221627 19984 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:57:52.221627 19984 net.cpp:137] Memory required for data: 388327600
I1122 09:57:52.221627 19984 layer_factory.cpp:58] Creating layer scale_conv12
I1122 09:57:52.221627 19984 net.cpp:84] Creating Layer scale_conv12
I1122 09:57:52.221627 19984 net.cpp:406] scale_conv12 <- conv12
I1122 09:57:52.221627 19984 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1122 09:57:52.221627 19984 layer_factory.cpp:58] Creating layer scale_conv12
I1122 09:57:52.221627 19984 net.cpp:122] Setting up scale_conv12
I1122 09:57:52.221627 19984 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:57:52.221627 19984 net.cpp:137] Memory required for data: 390631600
I1122 09:57:52.221627 19984 layer_factory.cpp:58] Creating layer relu_conv12
I1122 09:57:52.221627 19984 net.cpp:84] Creating Layer relu_conv12
I1122 09:57:52.221627 19984 net.cpp:406] relu_conv12 <- conv12
I1122 09:57:52.221627 19984 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1122 09:57:52.222630 19984 net.cpp:122] Setting up relu_conv12
I1122 09:57:52.222630 19984 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 09:57:52.222630 19984 net.cpp:137] Memory required for data: 392935600
I1122 09:57:52.222630 19984 layer_factory.cpp:58] Creating layer poolcp6
I1122 09:57:52.222630 19984 net.cpp:84] Creating Layer poolcp6
I1122 09:57:52.222630 19984 net.cpp:406] poolcp6 <- conv12
I1122 09:57:52.222630 19984 net.cpp:380] poolcp6 -> poolcp6
I1122 09:57:52.222630 19984 net.cpp:122] Setting up poolcp6
I1122 09:57:52.222630 19984 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1122 09:57:52.222630 19984 net.cpp:137] Memory required for data: 392971600
I1122 09:57:52.222630 19984 layer_factory.cpp:58] Creating layer ip1
I1122 09:57:52.222630 19984 net.cpp:84] Creating Layer ip1
I1122 09:57:52.222630 19984 net.cpp:406] ip1 <- poolcp6
I1122 09:57:52.222630 19984 net.cpp:380] ip1 -> ip1
I1122 09:57:52.222630 19984 net.cpp:122] Setting up ip1
I1122 09:57:52.222630 19984 net.cpp:129] Top shape: 100 10 (1000)
I1122 09:57:52.222630 19984 net.cpp:137] Memory required for data: 392975600
I1122 09:57:52.222630 19984 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1122 09:57:52.222630 19984 net.cpp:84] Creating Layer ip1_ip1_0_split
I1122 09:57:52.222630 19984 net.cpp:406] ip1_ip1_0_split <- ip1
I1122 09:57:52.222630 19984 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1122 09:57:52.222630 19984 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1122 09:57:52.222630 19984 net.cpp:122] Setting up ip1_ip1_0_split
I1122 09:57:52.222630 19984 net.cpp:129] Top shape: 100 10 (1000)
I1122 09:57:52.222630 19984 net.cpp:129] Top shape: 100 10 (1000)
I1122 09:57:52.222630 19984 net.cpp:137] Memory required for data: 392983600
I1122 09:57:52.222630 19984 layer_factory.cpp:58] Creating layer accuracy
I1122 09:57:52.222630 19984 net.cpp:84] Creating Layer accuracy
I1122 09:57:52.222630 19984 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1122 09:57:52.222630 19984 net.cpp:406] accuracy <- label_cifar_1_split_0
I1122 09:57:52.222630 19984 net.cpp:380] accuracy -> accuracy
I1122 09:57:52.222630 19984 net.cpp:122] Setting up accuracy
I1122 09:57:52.222630 19984 net.cpp:129] Top shape: (1)
I1122 09:57:52.222630 19984 net.cpp:137] Memory required for data: 392983604
I1122 09:57:52.222630 19984 layer_factory.cpp:58] Creating layer loss
I1122 09:57:52.222630 19984 net.cpp:84] Creating Layer loss
I1122 09:57:52.222630 19984 net.cpp:406] loss <- ip1_ip1_0_split_1
I1122 09:57:52.222630 19984 net.cpp:406] loss <- label_cifar_1_split_1
I1122 09:57:52.222630 19984 net.cpp:380] loss -> loss
I1122 09:57:52.222630 19984 layer_factory.cpp:58] Creating layer loss
I1122 09:57:52.222630 19984 net.cpp:122] Setting up loss
I1122 09:57:52.222630 19984 net.cpp:129] Top shape: (1)
I1122 09:57:52.222630 19984 net.cpp:132]     with loss weight 1
I1122 09:57:52.222630 19984 net.cpp:137] Memory required for data: 392983608
I1122 09:57:52.222630 19984 net.cpp:198] loss needs backward computation.
I1122 09:57:52.222630 19984 net.cpp:200] accuracy does not need backward computation.
I1122 09:57:52.222630 19984 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1122 09:57:52.222630 19984 net.cpp:198] ip1 needs backward computation.
I1122 09:57:52.222630 19984 net.cpp:198] poolcp6 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] relu_conv12 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] scale_conv12 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] bn_conv12 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] conv12 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] pool4_2 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] relu4_2 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] scale4_2 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] bn4_2 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] conv4_2 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] relu4_1 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] scale4_1 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] bn4_1 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] conv4_1 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] relu4 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] scale4 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] bn4 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] conv4 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] relu3 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] scale3 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] bn3 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] conv3 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] pool2_1 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] relu2_2 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] scale2_2 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] bn2_2 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] conv2_2 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] relu2 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] scale2 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] bn2 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] conv2 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] relu1 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] scale1 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] bn1 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:198] conv1 needs backward computation.
I1122 09:57:52.223628 19984 net.cpp:200] label_cifar_1_split does not need backward computation.
I1122 09:57:52.223628 19984 net.cpp:200] cifar does not need backward computation.
I1122 09:57:52.223628 19984 net.cpp:242] This network produces output accuracy
I1122 09:57:52.223628 19984 net.cpp:242] This network produces output loss
I1122 09:57:52.223628 19984 net.cpp:255] Network initialization done.
I1122 09:57:52.223628 19984 solver.cpp:56] Solver scaffolding done.
I1122 09:57:52.225627 19984 caffe.cpp:249] Starting Optimization
I1122 09:57:52.225627 19984 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_300k
I1122 09:57:52.225627 19984 solver.cpp:273] Learning Rate Policy: multistep
I1122 09:57:52.227627 19984 solver.cpp:330] Iteration 0, Testing net (#0)
I1122 09:57:52.228623 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:57:53.340144 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:57:53.381784 19984 solver.cpp:397]     Test net output #0: accuracy = 0.0889
I1122 09:57:53.381784 19984 solver.cpp:397]     Test net output #1: loss = 79.5723 (* 1 = 79.5723 loss)
I1122 09:57:53.455798 19984 solver.cpp:218] Iteration 0 (0 iter/s, 1.22859s/100 iters), loss = 3.655
I1122 09:57:53.455798 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.12
I1122 09:57:53.455798 19984 solver.cpp:237]     Train net output #1: loss = 3.655 (* 1 = 3.655 loss)
I1122 09:57:53.455798 19984 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1122 09:57:57.720719 19984 solver.cpp:218] Iteration 100 (23.4475 iter/s, 4.26484s/100 iters), loss = 1.64429
I1122 09:57:57.720719 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.41
I1122 09:57:57.720719 19984 solver.cpp:237]     Train net output #1: loss = 1.64429 (* 1 = 1.64429 loss)
I1122 09:57:57.720719 19984 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1122 09:58:01.965951 19984 solver.cpp:218] Iteration 200 (23.556 iter/s, 4.2452s/100 iters), loss = 1.56873
I1122 09:58:01.965951 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.43
I1122 09:58:01.965951 19984 solver.cpp:237]     Train net output #1: loss = 1.56873 (* 1 = 1.56873 loss)
I1122 09:58:01.965951 19984 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1122 09:58:06.206782 19984 solver.cpp:218] Iteration 300 (23.5847 iter/s, 4.24004s/100 iters), loss = 1.28879
I1122 09:58:06.206782 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.54
I1122 09:58:06.206782 19984 solver.cpp:237]     Train net output #1: loss = 1.28879 (* 1 = 1.28879 loss)
I1122 09:58:06.206782 19984 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1122 09:58:10.451371 19984 solver.cpp:218] Iteration 400 (23.559 iter/s, 4.24466s/100 iters), loss = 1.2147
I1122 09:58:10.451371 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.48
I1122 09:58:10.451371 19984 solver.cpp:237]     Train net output #1: loss = 1.2147 (* 1 = 1.2147 loss)
I1122 09:58:10.451371 19984 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1122 09:58:14.486315 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:58:14.653403 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_500.caffemodel
I1122 09:58:14.668362 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_500.solverstate
I1122 09:58:14.672361 19984 solver.cpp:330] Iteration 500, Testing net (#0)
I1122 09:58:14.672361 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:58:15.733242 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:58:15.775225 19984 solver.cpp:397]     Test net output #0: accuracy = 0.4919
I1122 09:58:15.775225 19984 solver.cpp:397]     Test net output #1: loss = 1.40232 (* 1 = 1.40232 loss)
I1122 09:58:15.816263 19984 solver.cpp:218] Iteration 500 (18.6423 iter/s, 5.36413s/100 iters), loss = 1.1431
I1122 09:58:15.816263 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.57
I1122 09:58:15.816263 19984 solver.cpp:237]     Train net output #1: loss = 1.1431 (* 1 = 1.1431 loss)
I1122 09:58:15.816263 19984 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1122 09:58:20.067948 19984 solver.cpp:218] Iteration 600 (23.5199 iter/s, 4.25172s/100 iters), loss = 1.00878
I1122 09:58:20.067948 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1122 09:58:20.067948 19984 solver.cpp:237]     Train net output #1: loss = 1.00878 (* 1 = 1.00878 loss)
I1122 09:58:20.067948 19984 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1122 09:58:24.321363 19984 solver.cpp:218] Iteration 700 (23.5112 iter/s, 4.2533s/100 iters), loss = 1.03792
I1122 09:58:24.321363 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1122 09:58:24.322365 19984 solver.cpp:237]     Train net output #1: loss = 1.03792 (* 1 = 1.03792 loss)
I1122 09:58:24.322365 19984 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1122 09:58:28.580201 19984 solver.cpp:218] Iteration 800 (23.4835 iter/s, 4.25831s/100 iters), loss = 0.808356
I1122 09:58:28.580201 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1122 09:58:28.580201 19984 solver.cpp:237]     Train net output #1: loss = 0.808356 (* 1 = 0.808356 loss)
I1122 09:58:28.580201 19984 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1122 09:58:32.841274 19984 solver.cpp:218] Iteration 900 (23.4717 iter/s, 4.26044s/100 iters), loss = 0.848461
I1122 09:58:32.841274 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1122 09:58:32.841274 19984 solver.cpp:237]     Train net output #1: loss = 0.848461 (* 1 = 0.848461 loss)
I1122 09:58:32.841274 19984 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1122 09:58:36.900169 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:58:37.067925 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_1000.caffemodel
I1122 09:58:37.078907 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_1000.solverstate
I1122 09:58:37.082907 19984 solver.cpp:330] Iteration 1000, Testing net (#0)
I1122 09:58:37.082907 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:58:38.145493 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:58:38.187479 19984 solver.cpp:397]     Test net output #0: accuracy = 0.5092
I1122 09:58:38.187479 19984 solver.cpp:397]     Test net output #1: loss = 1.37547 (* 1 = 1.37547 loss)
I1122 09:58:38.228101 19984 solver.cpp:218] Iteration 1000 (18.5642 iter/s, 5.38671s/100 iters), loss = 0.769088
I1122 09:58:38.228101 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1122 09:58:38.228101 19984 solver.cpp:237]     Train net output #1: loss = 0.769088 (* 1 = 0.769088 loss)
I1122 09:58:38.228101 19984 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1122 09:58:42.491523 19984 solver.cpp:218] Iteration 1100 (23.4578 iter/s, 4.26297s/100 iters), loss = 0.773812
I1122 09:58:42.491523 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1122 09:58:42.491523 19984 solver.cpp:237]     Train net output #1: loss = 0.773812 (* 1 = 0.773812 loss)
I1122 09:58:42.491523 19984 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1122 09:58:46.747722 19984 solver.cpp:218] Iteration 1200 (23.4992 iter/s, 4.25547s/100 iters), loss = 0.765972
I1122 09:58:46.747722 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1122 09:58:46.747722 19984 solver.cpp:237]     Train net output #1: loss = 0.765972 (* 1 = 0.765972 loss)
I1122 09:58:46.747722 19984 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1122 09:58:51.005116 19984 solver.cpp:218] Iteration 1300 (23.4887 iter/s, 4.25737s/100 iters), loss = 0.725475
I1122 09:58:51.005116 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1122 09:58:51.005116 19984 solver.cpp:237]     Train net output #1: loss = 0.725475 (* 1 = 0.725475 loss)
I1122 09:58:51.005116 19984 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1122 09:58:55.267897 19984 solver.cpp:218] Iteration 1400 (23.4624 iter/s, 4.26213s/100 iters), loss = 0.721878
I1122 09:58:55.267897 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1122 09:58:55.267897 19984 solver.cpp:237]     Train net output #1: loss = 0.721878 (* 1 = 0.721878 loss)
I1122 09:58:55.267897 19984 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1122 09:58:59.327002 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:58:59.495441 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_1500.caffemodel
I1122 09:58:59.505429 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_1500.solverstate
I1122 09:58:59.509430 19984 solver.cpp:330] Iteration 1500, Testing net (#0)
I1122 09:58:59.509430 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:59:00.576968 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:59:00.618988 19984 solver.cpp:397]     Test net output #0: accuracy = 0.491
I1122 09:59:00.618988 19984 solver.cpp:397]     Test net output #1: loss = 1.38782 (* 1 = 1.38782 loss)
I1122 09:59:00.659513 19984 solver.cpp:218] Iteration 1500 (18.5478 iter/s, 5.39146s/100 iters), loss = 0.784671
I1122 09:59:00.659513 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1122 09:59:00.659513 19984 solver.cpp:237]     Train net output #1: loss = 0.784671 (* 1 = 0.784671 loss)
I1122 09:59:00.659513 19984 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1122 09:59:04.912884 19984 solver.cpp:218] Iteration 1600 (23.5112 iter/s, 4.25329s/100 iters), loss = 0.64748
I1122 09:59:04.912884 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 09:59:04.912884 19984 solver.cpp:237]     Train net output #1: loss = 0.64748 (* 1 = 0.64748 loss)
I1122 09:59:04.912884 19984 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1122 09:59:09.170344 19984 solver.cpp:218] Iteration 1700 (23.495 iter/s, 4.25622s/100 iters), loss = 0.568156
I1122 09:59:09.170344 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 09:59:09.170344 19984 solver.cpp:237]     Train net output #1: loss = 0.568156 (* 1 = 0.568156 loss)
I1122 09:59:09.170344 19984 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1122 09:59:13.417470 19984 solver.cpp:218] Iteration 1800 (23.5444 iter/s, 4.24729s/100 iters), loss = 0.706277
I1122 09:59:13.417470 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1122 09:59:13.417470 19984 solver.cpp:237]     Train net output #1: loss = 0.706277 (* 1 = 0.706277 loss)
I1122 09:59:13.417470 19984 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1122 09:59:17.668567 19984 solver.cpp:218] Iteration 1900 (23.525 iter/s, 4.25079s/100 iters), loss = 0.566317
I1122 09:59:17.668567 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1122 09:59:17.668567 19984 solver.cpp:237]     Train net output #1: loss = 0.566317 (* 1 = 0.566317 loss)
I1122 09:59:17.668567 19984 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1122 09:59:21.711364 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:59:21.879467 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_2000.caffemodel
I1122 09:59:21.889461 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_2000.solverstate
I1122 09:59:21.893461 19984 solver.cpp:330] Iteration 2000, Testing net (#0)
I1122 09:59:21.893461 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:59:22.959659 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:59:23.001657 19984 solver.cpp:397]     Test net output #0: accuracy = 0.6748
I1122 09:59:23.001657 19984 solver.cpp:397]     Test net output #1: loss = 0.969406 (* 1 = 0.969406 loss)
I1122 09:59:23.043175 19984 solver.cpp:218] Iteration 2000 (18.6078 iter/s, 5.37409s/100 iters), loss = 0.590263
I1122 09:59:23.043175 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 09:59:23.043175 19984 solver.cpp:237]     Train net output #1: loss = 0.590263 (* 1 = 0.590263 loss)
I1122 09:59:23.043676 19984 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1122 09:59:27.314685 19984 solver.cpp:218] Iteration 2100 (23.4154 iter/s, 4.27069s/100 iters), loss = 0.472945
I1122 09:59:27.314685 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 09:59:27.314685 19984 solver.cpp:237]     Train net output #1: loss = 0.472945 (* 1 = 0.472945 loss)
I1122 09:59:27.314685 19984 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1122 09:59:31.584347 19984 solver.cpp:218] Iteration 2200 (23.4224 iter/s, 4.26941s/100 iters), loss = 0.536013
I1122 09:59:31.584347 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 09:59:31.584347 19984 solver.cpp:237]     Train net output #1: loss = 0.536013 (* 1 = 0.536013 loss)
I1122 09:59:31.584347 19984 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1122 09:59:35.855482 19984 solver.cpp:218] Iteration 2300 (23.4144 iter/s, 4.27088s/100 iters), loss = 0.703107
I1122 09:59:35.855482 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1122 09:59:35.855482 19984 solver.cpp:237]     Train net output #1: loss = 0.703107 (* 1 = 0.703107 loss)
I1122 09:59:35.855482 19984 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1122 09:59:40.123975 19984 solver.cpp:218] Iteration 2400 (23.4274 iter/s, 4.26851s/100 iters), loss = 0.556538
I1122 09:59:40.123975 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 09:59:40.123975 19984 solver.cpp:237]     Train net output #1: loss = 0.556538 (* 1 = 0.556538 loss)
I1122 09:59:40.123975 19984 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1122 09:59:44.185384 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:59:44.353006 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_2500.caffemodel
I1122 09:59:44.367002 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_2500.solverstate
I1122 09:59:44.371018 19984 solver.cpp:330] Iteration 2500, Testing net (#0)
I1122 09:59:44.371018 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 09:59:45.439352 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 09:59:45.480366 19984 solver.cpp:397]     Test net output #0: accuracy = 0.5653
I1122 09:59:45.480366 19984 solver.cpp:397]     Test net output #1: loss = 1.24178 (* 1 = 1.24178 loss)
I1122 09:59:45.521369 19984 solver.cpp:218] Iteration 2500 (18.5285 iter/s, 5.3971s/100 iters), loss = 0.54917
I1122 09:59:45.521369 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 09:59:45.521369 19984 solver.cpp:237]     Train net output #1: loss = 0.54917 (* 1 = 0.54917 loss)
I1122 09:59:45.521369 19984 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1122 09:59:49.790426 19984 solver.cpp:218] Iteration 2600 (23.4283 iter/s, 4.26835s/100 iters), loss = 0.454859
I1122 09:59:49.790426 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 09:59:49.790426 19984 solver.cpp:237]     Train net output #1: loss = 0.454859 (* 1 = 0.454859 loss)
I1122 09:59:49.790426 19984 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1122 09:59:54.064587 19984 solver.cpp:218] Iteration 2700 (23.3968 iter/s, 4.27409s/100 iters), loss = 0.495266
I1122 09:59:54.064587 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 09:59:54.064587 19984 solver.cpp:237]     Train net output #1: loss = 0.495266 (* 1 = 0.495266 loss)
I1122 09:59:54.064587 19984 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1122 09:59:58.333407 19984 solver.cpp:218] Iteration 2800 (23.4263 iter/s, 4.2687s/100 iters), loss = 0.612619
I1122 09:59:58.334411 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1122 09:59:58.334411 19984 solver.cpp:237]     Train net output #1: loss = 0.612619 (* 1 = 0.612619 loss)
I1122 09:59:58.334411 19984 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1122 10:00:02.639777 19984 solver.cpp:218] Iteration 2900 (23.2276 iter/s, 4.30521s/100 iters), loss = 0.523683
I1122 10:00:02.639777 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 10:00:02.639777 19984 solver.cpp:237]     Train net output #1: loss = 0.523683 (* 1 = 0.523683 loss)
I1122 10:00:02.639777 19984 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1122 10:00:06.704717 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:00:06.872267 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_3000.caffemodel
I1122 10:00:06.883286 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_3000.solverstate
I1122 10:00:06.887285 19984 solver.cpp:330] Iteration 3000, Testing net (#0)
I1122 10:00:06.887285 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:00:07.954881 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:00:07.996892 19984 solver.cpp:397]     Test net output #0: accuracy = 0.6675
I1122 10:00:07.996892 19984 solver.cpp:397]     Test net output #1: loss = 0.925361 (* 1 = 0.925361 loss)
I1122 10:00:08.037891 19984 solver.cpp:218] Iteration 3000 (18.526 iter/s, 5.39782s/100 iters), loss = 0.483232
I1122 10:00:08.037891 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 10:00:08.037891 19984 solver.cpp:237]     Train net output #1: loss = 0.483232 (* 1 = 0.483232 loss)
I1122 10:00:08.037891 19984 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1122 10:00:12.311753 19984 solver.cpp:218] Iteration 3100 (23.3974 iter/s, 4.27398s/100 iters), loss = 0.50637
I1122 10:00:12.311753 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 10:00:12.312754 19984 solver.cpp:237]     Train net output #1: loss = 0.50637 (* 1 = 0.50637 loss)
I1122 10:00:12.312754 19984 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1122 10:00:16.574506 19984 solver.cpp:218] Iteration 3200 (23.4653 iter/s, 4.26162s/100 iters), loss = 0.535562
I1122 10:00:16.574506 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 10:00:16.574506 19984 solver.cpp:237]     Train net output #1: loss = 0.535562 (* 1 = 0.535562 loss)
I1122 10:00:16.574506 19984 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1122 10:00:20.841943 19984 solver.cpp:218] Iteration 3300 (23.4332 iter/s, 4.26745s/100 iters), loss = 0.583952
I1122 10:00:20.841943 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 10:00:20.841943 19984 solver.cpp:237]     Train net output #1: loss = 0.583952 (* 1 = 0.583952 loss)
I1122 10:00:20.841943 19984 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1122 10:00:25.108332 19984 solver.cpp:218] Iteration 3400 (23.4417 iter/s, 4.2659s/100 iters), loss = 0.479294
I1122 10:00:25.108332 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 10:00:25.108332 19984 solver.cpp:237]     Train net output #1: loss = 0.479294 (* 1 = 0.479294 loss)
I1122 10:00:25.108332 19984 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1122 10:00:29.171629 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:00:29.338558 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_3500.caffemodel
I1122 10:00:29.349544 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_3500.solverstate
I1122 10:00:29.355051 19984 solver.cpp:330] Iteration 3500, Testing net (#0)
I1122 10:00:29.355051 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:00:30.420629 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:00:30.463127 19984 solver.cpp:397]     Test net output #0: accuracy = 0.5896
I1122 10:00:30.463127 19984 solver.cpp:397]     Test net output #1: loss = 1.23893 (* 1 = 1.23893 loss)
I1122 10:00:30.503659 19984 solver.cpp:218] Iteration 3500 (18.5368 iter/s, 5.39469s/100 iters), loss = 0.56219
I1122 10:00:30.503659 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 10:00:30.503659 19984 solver.cpp:237]     Train net output #1: loss = 0.56219 (* 1 = 0.56219 loss)
I1122 10:00:30.503659 19984 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1122 10:00:34.756666 19984 solver.cpp:218] Iteration 3600 (23.5135 iter/s, 4.25287s/100 iters), loss = 0.469201
I1122 10:00:34.756666 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 10:00:34.756666 19984 solver.cpp:237]     Train net output #1: loss = 0.469201 (* 1 = 0.469201 loss)
I1122 10:00:34.756666 19984 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1122 10:00:39.005995 19984 solver.cpp:218] Iteration 3700 (23.5327 iter/s, 4.2494s/100 iters), loss = 0.512274
I1122 10:00:39.005995 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 10:00:39.005995 19984 solver.cpp:237]     Train net output #1: loss = 0.512274 (* 1 = 0.512274 loss)
I1122 10:00:39.005995 19984 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1122 10:00:43.258766 19984 solver.cpp:218] Iteration 3800 (23.5188 iter/s, 4.25192s/100 iters), loss = 0.615776
I1122 10:00:43.258766 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 10:00:43.258766 19984 solver.cpp:237]     Train net output #1: loss = 0.615776 (* 1 = 0.615776 loss)
I1122 10:00:43.258766 19984 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1122 10:00:47.514369 19984 solver.cpp:218] Iteration 3900 (23.497 iter/s, 4.25585s/100 iters), loss = 0.519688
I1122 10:00:47.515373 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 10:00:47.515373 19984 solver.cpp:237]     Train net output #1: loss = 0.519688 (* 1 = 0.519688 loss)
I1122 10:00:47.515373 19984 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1122 10:00:51.555125 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:00:51.722350 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_4000.caffemodel
I1122 10:00:51.733352 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_4000.solverstate
I1122 10:00:51.737351 19984 solver.cpp:330] Iteration 4000, Testing net (#0)
I1122 10:00:51.737351 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:00:52.801985 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:00:52.843974 19984 solver.cpp:397]     Test net output #0: accuracy = 0.7015
I1122 10:00:52.843974 19984 solver.cpp:397]     Test net output #1: loss = 0.869952 (* 1 = 0.869952 loss)
I1122 10:00:52.885071 19984 solver.cpp:218] Iteration 4000 (18.6224 iter/s, 5.36988s/100 iters), loss = 0.460437
I1122 10:00:52.885071 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 10:00:52.885071 19984 solver.cpp:237]     Train net output #1: loss = 0.460437 (* 1 = 0.460437 loss)
I1122 10:00:52.885071 19984 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1122 10:00:57.159062 19984 solver.cpp:218] Iteration 4100 (23.4004 iter/s, 4.27343s/100 iters), loss = 0.500182
I1122 10:00:57.159062 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 10:00:57.159062 19984 solver.cpp:237]     Train net output #1: loss = 0.500182 (* 1 = 0.500182 loss)
I1122 10:00:57.159062 19984 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1122 10:01:01.432049 19984 solver.cpp:218] Iteration 4200 (23.4016 iter/s, 4.27321s/100 iters), loss = 0.476071
I1122 10:01:01.433048 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 10:01:01.433048 19984 solver.cpp:237]     Train net output #1: loss = 0.476071 (* 1 = 0.476071 loss)
I1122 10:01:01.433048 19984 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1122 10:01:05.703498 19984 solver.cpp:218] Iteration 4300 (23.417 iter/s, 4.27041s/100 iters), loss = 0.440604
I1122 10:01:05.703498 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1122 10:01:05.703498 19984 solver.cpp:237]     Train net output #1: loss = 0.440604 (* 1 = 0.440604 loss)
I1122 10:01:05.703498 19984 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1122 10:01:09.978623 19984 solver.cpp:218] Iteration 4400 (23.3925 iter/s, 4.27487s/100 iters), loss = 0.42075
I1122 10:01:09.978623 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 10:01:09.978623 19984 solver.cpp:237]     Train net output #1: loss = 0.42075 (* 1 = 0.42075 loss)
I1122 10:01:09.979123 19984 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1122 10:01:14.040925 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:01:14.208094 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_4500.caffemodel
I1122 10:01:14.218086 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_4500.solverstate
I1122 10:01:14.223085 19984 solver.cpp:330] Iteration 4500, Testing net (#0)
I1122 10:01:14.223085 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:01:15.289880 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:01:15.331866 19984 solver.cpp:397]     Test net output #0: accuracy = 0.7242
I1122 10:01:15.331866 19984 solver.cpp:397]     Test net output #1: loss = 0.80428 (* 1 = 0.80428 loss)
I1122 10:01:15.372398 19984 solver.cpp:218] Iteration 4500 (18.5417 iter/s, 5.39324s/100 iters), loss = 0.539445
I1122 10:01:15.372398 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1122 10:01:15.372398 19984 solver.cpp:237]     Train net output #1: loss = 0.539445 (* 1 = 0.539445 loss)
I1122 10:01:15.372398 19984 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1122 10:01:19.620615 19984 solver.cpp:218] Iteration 4600 (23.5429 iter/s, 4.24757s/100 iters), loss = 0.459763
I1122 10:01:19.620615 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 10:01:19.620615 19984 solver.cpp:237]     Train net output #1: loss = 0.459763 (* 1 = 0.459763 loss)
I1122 10:01:19.620615 19984 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1122 10:01:23.876520 19984 solver.cpp:218] Iteration 4700 (23.4993 iter/s, 4.25544s/100 iters), loss = 0.56148
I1122 10:01:23.876520 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1122 10:01:23.876520 19984 solver.cpp:237]     Train net output #1: loss = 0.56148 (* 1 = 0.56148 loss)
I1122 10:01:23.876520 19984 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1122 10:01:28.126396 19984 solver.cpp:218] Iteration 4800 (23.5273 iter/s, 4.25038s/100 iters), loss = 0.572346
I1122 10:01:28.127404 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 10:01:28.127404 19984 solver.cpp:237]     Train net output #1: loss = 0.572346 (* 1 = 0.572346 loss)
I1122 10:01:28.127404 19984 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1122 10:01:32.376196 19984 solver.cpp:218] Iteration 4900 (23.5366 iter/s, 4.24869s/100 iters), loss = 0.405531
I1122 10:01:32.376196 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 10:01:32.376196 19984 solver.cpp:237]     Train net output #1: loss = 0.405531 (* 1 = 0.405531 loss)
I1122 10:01:32.376196 19984 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1122 10:01:36.417115 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:01:36.583763 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_5000.caffemodel
I1122 10:01:36.595269 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_5000.solverstate
I1122 10:01:36.599284 19984 solver.cpp:330] Iteration 5000, Testing net (#0)
I1122 10:01:36.599284 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:01:37.665783 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:01:37.707834 19984 solver.cpp:397]     Test net output #0: accuracy = 0.7286
I1122 10:01:37.707834 19984 solver.cpp:397]     Test net output #1: loss = 0.785914 (* 1 = 0.785914 loss)
I1122 10:01:37.748823 19984 solver.cpp:218] Iteration 5000 (18.6139 iter/s, 5.37232s/100 iters), loss = 0.391921
I1122 10:01:37.748823 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1122 10:01:37.748823 19984 solver.cpp:237]     Train net output #1: loss = 0.391921 (* 1 = 0.391921 loss)
I1122 10:01:37.748823 19984 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1122 10:01:37.748823 19984 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1122 10:01:42.021647 19984 solver.cpp:218] Iteration 5100 (23.4065 iter/s, 4.27232s/100 iters), loss = 0.304381
I1122 10:01:42.021647 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 10:01:42.021647 19984 solver.cpp:237]     Train net output #1: loss = 0.304381 (* 1 = 0.304381 loss)
I1122 10:01:42.021647 19984 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1122 10:01:46.287108 19984 solver.cpp:218] Iteration 5200 (23.4452 iter/s, 4.26527s/100 iters), loss = 0.350026
I1122 10:01:46.287108 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 10:01:46.287108 19984 solver.cpp:237]     Train net output #1: loss = 0.350026 (* 1 = 0.350026 loss)
I1122 10:01:46.287108 19984 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1122 10:01:50.551311 19984 solver.cpp:218] Iteration 5300 (23.4504 iter/s, 4.26432s/100 iters), loss = 0.327865
I1122 10:01:50.551311 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 10:01:50.551311 19984 solver.cpp:237]     Train net output #1: loss = 0.327865 (* 1 = 0.327865 loss)
I1122 10:01:50.551311 19984 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1122 10:01:54.816200 19984 solver.cpp:218] Iteration 5400 (23.4509 iter/s, 4.26422s/100 iters), loss = 0.251117
I1122 10:01:54.816200 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:01:54.816200 19984 solver.cpp:237]     Train net output #1: loss = 0.251117 (* 1 = 0.251117 loss)
I1122 10:01:54.816200 19984 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1122 10:01:58.876958 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:01:59.045104 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_5500.caffemodel
I1122 10:01:59.055105 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_5500.solverstate
I1122 10:01:59.060087 19984 solver.cpp:330] Iteration 5500, Testing net (#0)
I1122 10:01:59.060087 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:02:00.128422 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:02:00.170406 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8684
I1122 10:02:00.170406 19984 solver.cpp:397]     Test net output #1: loss = 0.383479 (* 1 = 0.383479 loss)
I1122 10:02:00.211433 19984 solver.cpp:218] Iteration 5500 (18.5365 iter/s, 5.39476s/100 iters), loss = 0.215742
I1122 10:02:00.211433 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:02:00.211433 19984 solver.cpp:237]     Train net output #1: loss = 0.215742 (* 1 = 0.215742 loss)
I1122 10:02:00.211433 19984 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1122 10:02:04.496026 19984 solver.cpp:218] Iteration 5600 (23.3411 iter/s, 4.2843s/100 iters), loss = 0.271824
I1122 10:02:04.496026 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:02:04.496026 19984 solver.cpp:237]     Train net output #1: loss = 0.271824 (* 1 = 0.271824 loss)
I1122 10:02:04.496026 19984 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1122 10:02:08.768293 19984 solver.cpp:218] Iteration 5700 (23.4113 iter/s, 4.27144s/100 iters), loss = 0.264118
I1122 10:02:08.768293 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 10:02:08.768293 19984 solver.cpp:237]     Train net output #1: loss = 0.264118 (* 1 = 0.264118 loss)
I1122 10:02:08.768293 19984 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1122 10:02:13.035753 19984 solver.cpp:218] Iteration 5800 (23.436 iter/s, 4.26694s/100 iters), loss = 0.30205
I1122 10:02:13.035753 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 10:02:13.035753 19984 solver.cpp:237]     Train net output #1: loss = 0.30205 (* 1 = 0.30205 loss)
I1122 10:02:13.035753 19984 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1122 10:02:17.304200 19984 solver.cpp:218] Iteration 5900 (23.4268 iter/s, 4.26861s/100 iters), loss = 0.193007
I1122 10:02:17.304200 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:02:17.304200 19984 solver.cpp:237]     Train net output #1: loss = 0.193007 (* 1 = 0.193007 loss)
I1122 10:02:17.304200 19984 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1122 10:02:21.374332 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:02:21.545384 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_6000.caffemodel
I1122 10:02:21.557888 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_6000.solverstate
I1122 10:02:21.562392 19984 solver.cpp:330] Iteration 6000, Testing net (#0)
I1122 10:02:21.562392 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:02:22.632346 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:02:22.674883 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8766
I1122 10:02:22.674883 19984 solver.cpp:397]     Test net output #1: loss = 0.362247 (* 1 = 0.362247 loss)
I1122 10:02:22.716876 19984 solver.cpp:218] Iteration 6000 (18.4776 iter/s, 5.41196s/100 iters), loss = 0.331403
I1122 10:02:22.716876 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:02:22.716876 19984 solver.cpp:237]     Train net output #1: loss = 0.331403 (* 1 = 0.331403 loss)
I1122 10:02:22.716876 19984 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1122 10:02:27.021937 19984 solver.cpp:218] Iteration 6100 (23.229 iter/s, 4.30496s/100 iters), loss = 0.238679
I1122 10:02:27.021937 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:02:27.021937 19984 solver.cpp:237]     Train net output #1: loss = 0.238679 (* 1 = 0.238679 loss)
I1122 10:02:27.021937 19984 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1122 10:02:31.341771 19984 solver.cpp:218] Iteration 6200 (23.1492 iter/s, 4.3198s/100 iters), loss = 0.243291
I1122 10:02:31.342754 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 10:02:31.342754 19984 solver.cpp:237]     Train net output #1: loss = 0.243291 (* 1 = 0.243291 loss)
I1122 10:02:31.342754 19984 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1122 10:02:35.628362 19984 solver.cpp:218] Iteration 6300 (23.3351 iter/s, 4.28539s/100 iters), loss = 0.3095
I1122 10:02:35.628362 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 10:02:35.628362 19984 solver.cpp:237]     Train net output #1: loss = 0.3095 (* 1 = 0.3095 loss)
I1122 10:02:35.628362 19984 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1122 10:02:39.903947 19984 solver.cpp:218] Iteration 6400 (23.3892 iter/s, 4.27548s/100 iters), loss = 0.184062
I1122 10:02:39.903947 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:02:39.903947 19984 solver.cpp:237]     Train net output #1: loss = 0.184062 (* 1 = 0.184062 loss)
I1122 10:02:39.903947 19984 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1122 10:02:43.991092 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:02:44.158701 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_6500.caffemodel
I1122 10:02:44.170227 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_6500.solverstate
I1122 10:02:44.174226 19984 solver.cpp:330] Iteration 6500, Testing net (#0)
I1122 10:02:44.174226 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:02:45.237504 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:02:45.280027 19984 solver.cpp:397]     Test net output #0: accuracy = 0.872
I1122 10:02:45.280027 19984 solver.cpp:397]     Test net output #1: loss = 0.367071 (* 1 = 0.367071 loss)
I1122 10:02:45.320101 19984 solver.cpp:218] Iteration 6500 (18.4632 iter/s, 5.41617s/100 iters), loss = 0.227785
I1122 10:02:45.320101 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:02:45.321101 19984 solver.cpp:237]     Train net output #1: loss = 0.227785 (* 1 = 0.227785 loss)
I1122 10:02:45.321101 19984 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1122 10:02:49.616700 19984 solver.cpp:218] Iteration 6600 (23.2783 iter/s, 4.29585s/100 iters), loss = 0.264794
I1122 10:02:49.616700 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:02:49.616700 19984 solver.cpp:237]     Train net output #1: loss = 0.264794 (* 1 = 0.264794 loss)
I1122 10:02:49.616700 19984 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1122 10:02:53.953290 19984 solver.cpp:218] Iteration 6700 (23.0605 iter/s, 4.33642s/100 iters), loss = 0.246348
I1122 10:02:53.953290 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:02:53.953290 19984 solver.cpp:237]     Train net output #1: loss = 0.246348 (* 1 = 0.246348 loss)
I1122 10:02:53.953290 19984 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1122 10:02:58.254192 19984 solver.cpp:218] Iteration 6800 (23.2573 iter/s, 4.29972s/100 iters), loss = 0.253814
I1122 10:02:58.254192 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:02:58.254192 19984 solver.cpp:237]     Train net output #1: loss = 0.253814 (* 1 = 0.253814 loss)
I1122 10:02:58.254192 19984 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1122 10:03:02.528686 19984 solver.cpp:218] Iteration 6900 (23.3942 iter/s, 4.27457s/100 iters), loss = 0.154745
I1122 10:03:02.528686 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:03:02.528686 19984 solver.cpp:237]     Train net output #1: loss = 0.154745 (* 1 = 0.154745 loss)
I1122 10:03:02.528686 19984 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1122 10:03:06.578239 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:03:06.744374 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_7000.caffemodel
I1122 10:03:06.755372 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_7000.solverstate
I1122 10:03:06.759373 19984 solver.cpp:330] Iteration 7000, Testing net (#0)
I1122 10:03:06.759373 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:03:07.823959 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:03:07.864984 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8799
I1122 10:03:07.864984 19984 solver.cpp:397]     Test net output #1: loss = 0.350155 (* 1 = 0.350155 loss)
I1122 10:03:07.905793 19984 solver.cpp:218] Iteration 7000 (18.5977 iter/s, 5.377s/100 iters), loss = 0.261757
I1122 10:03:07.905793 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 10:03:07.905793 19984 solver.cpp:237]     Train net output #1: loss = 0.261757 (* 1 = 0.261757 loss)
I1122 10:03:07.905793 19984 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1122 10:03:12.204084 19984 solver.cpp:218] Iteration 7100 (23.2669 iter/s, 4.29795s/100 iters), loss = 0.192586
I1122 10:03:12.204084 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:03:12.205088 19984 solver.cpp:237]     Train net output #1: loss = 0.192586 (* 1 = 0.192586 loss)
I1122 10:03:12.205088 19984 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1122 10:03:16.469527 19984 solver.cpp:218] Iteration 7200 (23.4488 iter/s, 4.26461s/100 iters), loss = 0.219789
I1122 10:03:16.469527 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:03:16.469527 19984 solver.cpp:237]     Train net output #1: loss = 0.219789 (* 1 = 0.219789 loss)
I1122 10:03:16.469527 19984 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1122 10:03:20.738279 19984 solver.cpp:218] Iteration 7300 (23.4271 iter/s, 4.26856s/100 iters), loss = 0.239781
I1122 10:03:20.738279 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 10:03:20.738279 19984 solver.cpp:237]     Train net output #1: loss = 0.239781 (* 1 = 0.239781 loss)
I1122 10:03:20.738279 19984 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1122 10:03:25.040135 19984 solver.cpp:218] Iteration 7400 (23.2492 iter/s, 4.30122s/100 iters), loss = 0.16754
I1122 10:03:25.040135 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:03:25.040135 19984 solver.cpp:237]     Train net output #1: loss = 0.16754 (* 1 = 0.16754 loss)
I1122 10:03:25.040135 19984 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1122 10:03:29.099941 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:03:29.265988 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_7500.caffemodel
I1122 10:03:29.276988 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_7500.solverstate
I1122 10:03:29.280992 19984 solver.cpp:330] Iteration 7500, Testing net (#0)
I1122 10:03:29.280992 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:03:30.346446 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:03:30.389935 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8737
I1122 10:03:30.389935 19984 solver.cpp:397]     Test net output #1: loss = 0.360044 (* 1 = 0.360044 loss)
I1122 10:03:30.431021 19984 solver.cpp:218] Iteration 7500 (18.5521 iter/s, 5.39021s/100 iters), loss = 0.253049
I1122 10:03:30.431021 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:03:30.431021 19984 solver.cpp:237]     Train net output #1: loss = 0.253049 (* 1 = 0.253049 loss)
I1122 10:03:30.431021 19984 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1122 10:03:34.731443 19984 solver.cpp:218] Iteration 7600 (23.2536 iter/s, 4.30041s/100 iters), loss = 0.22331
I1122 10:03:34.731443 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:03:34.731443 19984 solver.cpp:237]     Train net output #1: loss = 0.22331 (* 1 = 0.22331 loss)
I1122 10:03:34.731443 19984 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1122 10:03:39.004678 19984 solver.cpp:218] Iteration 7700 (23.4049 iter/s, 4.27261s/100 iters), loss = 0.191211
I1122 10:03:39.004678 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:03:39.004678 19984 solver.cpp:237]     Train net output #1: loss = 0.191211 (* 1 = 0.191211 loss)
I1122 10:03:39.004678 19984 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1122 10:03:43.329313 19984 solver.cpp:218] Iteration 7800 (23.1264 iter/s, 4.32406s/100 iters), loss = 0.28484
I1122 10:03:43.329313 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 10:03:43.329313 19984 solver.cpp:237]     Train net output #1: loss = 0.28484 (* 1 = 0.28484 loss)
I1122 10:03:43.329313 19984 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1122 10:03:47.659572 19984 solver.cpp:218] Iteration 7900 (23.0932 iter/s, 4.33029s/100 iters), loss = 0.179392
I1122 10:03:47.659572 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:03:47.659572 19984 solver.cpp:237]     Train net output #1: loss = 0.179392 (* 1 = 0.179392 loss)
I1122 10:03:47.659572 19984 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1122 10:03:51.767132 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:03:51.935196 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_8000.caffemodel
I1122 10:03:51.945196 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_8000.solverstate
I1122 10:03:51.949198 19984 solver.cpp:330] Iteration 8000, Testing net (#0)
I1122 10:03:51.949198 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:03:53.028908 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:03:53.070919 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8746
I1122 10:03:53.070919 19984 solver.cpp:397]     Test net output #1: loss = 0.363196 (* 1 = 0.363196 loss)
I1122 10:03:53.112926 19984 solver.cpp:218] Iteration 8000 (18.3403 iter/s, 5.45246s/100 iters), loss = 0.176154
I1122 10:03:53.112926 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:03:53.112926 19984 solver.cpp:237]     Train net output #1: loss = 0.176154 (* 1 = 0.176154 loss)
I1122 10:03:53.112926 19984 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1122 10:03:57.418442 19984 solver.cpp:218] Iteration 8100 (23.2269 iter/s, 4.30535s/100 iters), loss = 0.226338
I1122 10:03:57.418442 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:03:57.418442 19984 solver.cpp:237]     Train net output #1: loss = 0.226338 (* 1 = 0.226338 loss)
I1122 10:03:57.418442 19984 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1122 10:04:01.737473 19984 solver.cpp:218] Iteration 8200 (23.1555 iter/s, 4.31863s/100 iters), loss = 0.193821
I1122 10:04:01.737473 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:04:01.737473 19984 solver.cpp:237]     Train net output #1: loss = 0.193821 (* 1 = 0.193821 loss)
I1122 10:04:01.737473 19984 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1122 10:04:06.059231 19984 solver.cpp:218] Iteration 8300 (23.1379 iter/s, 4.32192s/100 iters), loss = 0.30502
I1122 10:04:06.059231 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 10:04:06.059231 19984 solver.cpp:237]     Train net output #1: loss = 0.30502 (* 1 = 0.30502 loss)
I1122 10:04:06.059231 19984 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1122 10:04:10.375476 19984 solver.cpp:218] Iteration 8400 (23.1731 iter/s, 4.31535s/100 iters), loss = 0.149864
I1122 10:04:10.375476 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:04:10.375476 19984 solver.cpp:237]     Train net output #1: loss = 0.149864 (* 1 = 0.149864 loss)
I1122 10:04:10.375476 19984 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1122 10:04:14.430014 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:04:14.597055 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_8500.caffemodel
I1122 10:04:14.607055 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_8500.solverstate
I1122 10:04:14.611052 19984 solver.cpp:330] Iteration 8500, Testing net (#0)
I1122 10:04:14.611577 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:04:15.676491 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:04:15.718993 19984 solver.cpp:397]     Test net output #0: accuracy = 0.877
I1122 10:04:15.718993 19984 solver.cpp:397]     Test net output #1: loss = 0.356037 (* 1 = 0.356037 loss)
I1122 10:04:15.759482 19984 solver.cpp:218] Iteration 8500 (18.5742 iter/s, 5.38383s/100 iters), loss = 0.169885
I1122 10:04:15.759482 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:04:15.759482 19984 solver.cpp:237]     Train net output #1: loss = 0.169885 (* 1 = 0.169885 loss)
I1122 10:04:15.759482 19984 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1122 10:04:20.047082 19984 solver.cpp:218] Iteration 8600 (23.3272 iter/s, 4.28685s/100 iters), loss = 0.166991
I1122 10:04:20.047082 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:04:20.047082 19984 solver.cpp:237]     Train net output #1: loss = 0.166991 (* 1 = 0.166991 loss)
I1122 10:04:20.047082 19984 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1122 10:04:24.351938 19984 solver.cpp:218] Iteration 8700 (23.2312 iter/s, 4.30455s/100 iters), loss = 0.185409
I1122 10:04:24.351938 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:04:24.351938 19984 solver.cpp:237]     Train net output #1: loss = 0.185409 (* 1 = 0.185409 loss)
I1122 10:04:24.351938 19984 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1122 10:04:28.608500 19984 solver.cpp:218] Iteration 8800 (23.4959 iter/s, 4.25605s/100 iters), loss = 0.194891
I1122 10:04:28.608500 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:04:28.608500 19984 solver.cpp:237]     Train net output #1: loss = 0.194891 (* 1 = 0.194891 loss)
I1122 10:04:28.608500 19984 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1122 10:04:32.858295 19984 solver.cpp:218] Iteration 8900 (23.5337 iter/s, 4.24922s/100 iters), loss = 0.15947
I1122 10:04:32.858295 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:04:32.858295 19984 solver.cpp:237]     Train net output #1: loss = 0.15947 (* 1 = 0.15947 loss)
I1122 10:04:32.858295 19984 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1122 10:04:36.909494 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:04:37.076561 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_9000.caffemodel
I1122 10:04:37.086560 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_9000.solverstate
I1122 10:04:37.090564 19984 solver.cpp:330] Iteration 9000, Testing net (#0)
I1122 10:04:37.090564 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:04:38.155604 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:04:38.197628 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8683
I1122 10:04:38.197628 19984 solver.cpp:397]     Test net output #1: loss = 0.387658 (* 1 = 0.387658 loss)
I1122 10:04:38.239132 19984 solver.cpp:218] Iteration 9000 (18.5853 iter/s, 5.38059s/100 iters), loss = 0.17315
I1122 10:04:38.239132 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:04:38.239132 19984 solver.cpp:237]     Train net output #1: loss = 0.17315 (* 1 = 0.17315 loss)
I1122 10:04:38.239132 19984 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1122 10:04:42.515151 19984 solver.cpp:218] Iteration 9100 (23.3847 iter/s, 4.2763s/100 iters), loss = 0.207081
I1122 10:04:42.515151 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:04:42.515151 19984 solver.cpp:237]     Train net output #1: loss = 0.207081 (* 1 = 0.207081 loss)
I1122 10:04:42.515151 19984 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1122 10:04:46.782847 19984 solver.cpp:218] Iteration 9200 (23.4361 iter/s, 4.26692s/100 iters), loss = 0.209909
I1122 10:04:46.782847 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:04:46.782847 19984 solver.cpp:237]     Train net output #1: loss = 0.209909 (* 1 = 0.209909 loss)
I1122 10:04:46.782847 19984 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1122 10:04:51.055443 19984 solver.cpp:218] Iteration 9300 (23.4051 iter/s, 4.27258s/100 iters), loss = 0.164215
I1122 10:04:51.055443 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:04:51.055443 19984 solver.cpp:237]     Train net output #1: loss = 0.164215 (* 1 = 0.164215 loss)
I1122 10:04:51.055443 19984 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1122 10:04:55.331105 19984 solver.cpp:218] Iteration 9400 (23.3919 iter/s, 4.27498s/100 iters), loss = 0.144585
I1122 10:04:55.331105 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:04:55.331105 19984 solver.cpp:237]     Train net output #1: loss = 0.144585 (* 1 = 0.144585 loss)
I1122 10:04:55.331105 19984 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1122 10:04:59.393920 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:04:59.561986 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_9500.caffemodel
I1122 10:04:59.573467 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_9500.solverstate
I1122 10:04:59.577466 19984 solver.cpp:330] Iteration 9500, Testing net (#0)
I1122 10:04:59.577466 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:05:00.642429 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:05:00.684945 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8766
I1122 10:05:00.684945 19984 solver.cpp:397]     Test net output #1: loss = 0.360158 (* 1 = 0.360158 loss)
I1122 10:05:00.725461 19984 solver.cpp:218] Iteration 9500 (18.5392 iter/s, 5.39398s/100 iters), loss = 0.241336
I1122 10:05:00.725461 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 10:05:00.725461 19984 solver.cpp:237]     Train net output #1: loss = 0.241336 (* 1 = 0.241336 loss)
I1122 10:05:00.725461 19984 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1122 10:05:00.725461 19984 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1122 10:05:04.975224 19984 solver.cpp:218] Iteration 9600 (23.5343 iter/s, 4.24912s/100 iters), loss = 0.169103
I1122 10:05:04.975224 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:05:04.975224 19984 solver.cpp:237]     Train net output #1: loss = 0.169103 (* 1 = 0.169103 loss)
I1122 10:05:04.975224 19984 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1122 10:05:09.228112 19984 solver.cpp:218] Iteration 9700 (23.5129 iter/s, 4.25299s/100 iters), loss = 0.169408
I1122 10:05:09.228112 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:05:09.228112 19984 solver.cpp:237]     Train net output #1: loss = 0.169408 (* 1 = 0.169408 loss)
I1122 10:05:09.228112 19984 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1122 10:05:13.487532 19984 solver.cpp:218] Iteration 9800 (23.4839 iter/s, 4.25824s/100 iters), loss = 0.206968
I1122 10:05:13.487532 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:05:13.487532 19984 solver.cpp:237]     Train net output #1: loss = 0.206968 (* 1 = 0.206968 loss)
I1122 10:05:13.487532 19984 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1122 10:05:17.734676 19984 solver.cpp:218] Iteration 9900 (23.5445 iter/s, 4.24728s/100 iters), loss = 0.126673
I1122 10:05:17.734676 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:05:17.734676 19984 solver.cpp:237]     Train net output #1: loss = 0.126673 (* 1 = 0.126673 loss)
I1122 10:05:17.734676 19984 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1122 10:05:21.775729 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:05:21.942324 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_10000.caffemodel
I1122 10:05:21.953325 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_10000.solverstate
I1122 10:05:21.957324 19984 solver.cpp:330] Iteration 10000, Testing net (#0)
I1122 10:05:21.957324 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:05:23.024812 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:05:23.065809 19984 solver.cpp:397]     Test net output #0: accuracy = 0.895
I1122 10:05:23.065809 19984 solver.cpp:397]     Test net output #1: loss = 0.310808 (* 1 = 0.310808 loss)
I1122 10:05:23.106523 19984 solver.cpp:218] Iteration 10000 (18.6158 iter/s, 5.37179s/100 iters), loss = 0.178973
I1122 10:05:23.107542 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:05:23.107542 19984 solver.cpp:237]     Train net output #1: loss = 0.178973 (* 1 = 0.178973 loss)
I1122 10:05:23.107542 19984 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1122 10:05:27.378329 19984 solver.cpp:218] Iteration 10100 (23.4165 iter/s, 4.27049s/100 iters), loss = 0.159174
I1122 10:05:27.378329 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:05:27.378329 19984 solver.cpp:237]     Train net output #1: loss = 0.159174 (* 1 = 0.159174 loss)
I1122 10:05:27.378329 19984 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1122 10:05:31.684082 19984 solver.cpp:218] Iteration 10200 (23.2227 iter/s, 4.30613s/100 iters), loss = 0.225492
I1122 10:05:31.684082 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:05:31.684082 19984 solver.cpp:237]     Train net output #1: loss = 0.225492 (* 1 = 0.225492 loss)
I1122 10:05:31.685081 19984 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1122 10:05:35.952546 19984 solver.cpp:218] Iteration 10300 (23.4323 iter/s, 4.26761s/100 iters), loss = 0.174794
I1122 10:05:35.952546 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:05:35.952546 19984 solver.cpp:237]     Train net output #1: loss = 0.174794 (* 1 = 0.174794 loss)
I1122 10:05:35.952546 19984 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1122 10:05:40.216701 19984 solver.cpp:218] Iteration 10400 (23.4505 iter/s, 4.26431s/100 iters), loss = 0.105219
I1122 10:05:40.217708 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:05:40.217708 19984 solver.cpp:237]     Train net output #1: loss = 0.105219 (* 1 = 0.105219 loss)
I1122 10:05:40.217708 19984 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1122 10:05:44.275710 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:05:44.442440 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_10500.caffemodel
I1122 10:05:44.452421 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_10500.solverstate
I1122 10:05:44.456421 19984 solver.cpp:330] Iteration 10500, Testing net (#0)
I1122 10:05:44.456421 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:05:45.520565 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:05:45.562544 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8941
I1122 10:05:45.562544 19984 solver.cpp:397]     Test net output #1: loss = 0.308542 (* 1 = 0.308542 loss)
I1122 10:05:45.603569 19984 solver.cpp:218] Iteration 10500 (18.567 iter/s, 5.38591s/100 iters), loss = 0.201667
I1122 10:05:45.603569 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:05:45.603569 19984 solver.cpp:237]     Train net output #1: loss = 0.201667 (* 1 = 0.201667 loss)
I1122 10:05:45.603569 19984 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1122 10:05:49.875445 19984 solver.cpp:218] Iteration 10600 (23.4118 iter/s, 4.27135s/100 iters), loss = 0.187873
I1122 10:05:49.875445 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:05:49.875445 19984 solver.cpp:237]     Train net output #1: loss = 0.187873 (* 1 = 0.187873 loss)
I1122 10:05:49.875445 19984 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1122 10:05:54.143434 19984 solver.cpp:218] Iteration 10700 (23.4325 iter/s, 4.26758s/100 iters), loss = 0.141649
I1122 10:05:54.143434 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:05:54.143434 19984 solver.cpp:237]     Train net output #1: loss = 0.141649 (* 1 = 0.141649 loss)
I1122 10:05:54.143434 19984 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1122 10:05:58.413125 19984 solver.cpp:218] Iteration 10800 (23.4231 iter/s, 4.26928s/100 iters), loss = 0.203678
I1122 10:05:58.413125 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:05:58.413125 19984 solver.cpp:237]     Train net output #1: loss = 0.203678 (* 1 = 0.203678 loss)
I1122 10:05:58.413125 19984 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1122 10:06:02.692276 19984 solver.cpp:218] Iteration 10900 (23.3682 iter/s, 4.27932s/100 iters), loss = 0.0993551
I1122 10:06:02.692276 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:06:02.692276 19984 solver.cpp:237]     Train net output #1: loss = 0.0993551 (* 1 = 0.0993551 loss)
I1122 10:06:02.692276 19984 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1122 10:06:06.757905 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:06:06.925967 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_11000.caffemodel
I1122 10:06:06.935967 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_11000.solverstate
I1122 10:06:06.940989 19984 solver.cpp:330] Iteration 11000, Testing net (#0)
I1122 10:06:06.940989 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:06:08.006366 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:06:08.048208 19984 solver.cpp:397]     Test net output #0: accuracy = 0.895
I1122 10:06:08.048208 19984 solver.cpp:397]     Test net output #1: loss = 0.306853 (* 1 = 0.306853 loss)
I1122 10:06:08.089220 19984 solver.cpp:218] Iteration 11000 (18.5317 iter/s, 5.39615s/100 iters), loss = 0.208701
I1122 10:06:08.089220 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:06:08.089220 19984 solver.cpp:237]     Train net output #1: loss = 0.208701 (* 1 = 0.208701 loss)
I1122 10:06:08.089220 19984 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1122 10:06:12.360504 19984 solver.cpp:218] Iteration 11100 (23.4152 iter/s, 4.27072s/100 iters), loss = 0.204768
I1122 10:06:12.360504 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:06:12.360504 19984 solver.cpp:237]     Train net output #1: loss = 0.204768 (* 1 = 0.204768 loss)
I1122 10:06:12.360504 19984 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1122 10:06:16.630488 19984 solver.cpp:218] Iteration 11200 (23.4196 iter/s, 4.26993s/100 iters), loss = 0.153072
I1122 10:06:16.630488 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:06:16.630488 19984 solver.cpp:237]     Train net output #1: loss = 0.153072 (* 1 = 0.153072 loss)
I1122 10:06:16.630488 19984 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1122 10:06:20.917021 19984 solver.cpp:218] Iteration 11300 (23.3332 iter/s, 4.28574s/100 iters), loss = 0.128789
I1122 10:06:20.917021 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:06:20.917021 19984 solver.cpp:237]     Train net output #1: loss = 0.128789 (* 1 = 0.128789 loss)
I1122 10:06:20.917021 19984 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1122 10:06:25.191155 19984 solver.cpp:218] Iteration 11400 (23.3953 iter/s, 4.27436s/100 iters), loss = 0.108088
I1122 10:06:25.191155 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:06:25.191155 19984 solver.cpp:237]     Train net output #1: loss = 0.108088 (* 1 = 0.108088 loss)
I1122 10:06:25.191155 19984 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1122 10:06:29.286530 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:06:29.454644 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_11500.caffemodel
I1122 10:06:29.465646 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_11500.solverstate
I1122 10:06:29.469645 19984 solver.cpp:330] Iteration 11500, Testing net (#0)
I1122 10:06:29.469645 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:06:30.536978 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:06:30.578320 19984 solver.cpp:397]     Test net output #0: accuracy = 0.893
I1122 10:06:30.578320 19984 solver.cpp:397]     Test net output #1: loss = 0.306319 (* 1 = 0.306319 loss)
I1122 10:06:30.619314 19984 solver.cpp:218] Iteration 11500 (18.4254 iter/s, 5.4273s/100 iters), loss = 0.169217
I1122 10:06:30.619314 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:06:30.619314 19984 solver.cpp:237]     Train net output #1: loss = 0.169217 (* 1 = 0.169217 loss)
I1122 10:06:30.619314 19984 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1122 10:06:34.894135 19984 solver.cpp:218] Iteration 11600 (23.3967 iter/s, 4.2741s/100 iters), loss = 0.156007
I1122 10:06:34.894135 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:06:34.894135 19984 solver.cpp:237]     Train net output #1: loss = 0.156007 (* 1 = 0.156007 loss)
I1122 10:06:34.894135 19984 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1122 10:06:39.169476 19984 solver.cpp:218] Iteration 11700 (23.3915 iter/s, 4.27505s/100 iters), loss = 0.175073
I1122 10:06:39.169476 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:06:39.169476 19984 solver.cpp:237]     Train net output #1: loss = 0.175073 (* 1 = 0.175073 loss)
I1122 10:06:39.169476 19984 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1122 10:06:43.430260 19984 solver.cpp:218] Iteration 11800 (23.4723 iter/s, 4.26034s/100 iters), loss = 0.171359
I1122 10:06:43.430260 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 10:06:43.430260 19984 solver.cpp:237]     Train net output #1: loss = 0.171359 (* 1 = 0.171359 loss)
I1122 10:06:43.430260 19984 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1122 10:06:47.700655 19984 solver.cpp:218] Iteration 11900 (23.417 iter/s, 4.2704s/100 iters), loss = 0.0846966
I1122 10:06:47.700655 19984 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 10:06:47.700655 19984 solver.cpp:237]     Train net output #1: loss = 0.0846966 (* 1 = 0.0846966 loss)
I1122 10:06:47.700655 19984 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1122 10:06:51.849859 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:06:52.019939 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_12000.caffemodel
I1122 10:06:52.030431 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_12000.solverstate
I1122 10:06:52.034934 19984 solver.cpp:330] Iteration 12000, Testing net (#0)
I1122 10:06:52.034934 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:06:53.104404 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:06:53.146411 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8938
I1122 10:06:53.146411 19984 solver.cpp:397]     Test net output #1: loss = 0.306922 (* 1 = 0.306922 loss)
I1122 10:06:53.187516 19984 solver.cpp:218] Iteration 12000 (18.2289 iter/s, 5.48579s/100 iters), loss = 0.144478
I1122 10:06:53.187516 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:06:53.187516 19984 solver.cpp:237]     Train net output #1: loss = 0.144478 (* 1 = 0.144478 loss)
I1122 10:06:53.187516 19984 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1122 10:06:57.521453 19984 solver.cpp:218] Iteration 12100 (23.0748 iter/s, 4.33373s/100 iters), loss = 0.132046
I1122 10:06:57.521453 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:06:57.521453 19984 solver.cpp:237]     Train net output #1: loss = 0.132046 (* 1 = 0.132046 loss)
I1122 10:06:57.521453 19984 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1122 10:07:01.822037 19984 solver.cpp:218] Iteration 12200 (23.2552 iter/s, 4.30012s/100 iters), loss = 0.152852
I1122 10:07:01.822037 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:07:01.822037 19984 solver.cpp:237]     Train net output #1: loss = 0.152852 (* 1 = 0.152852 loss)
I1122 10:07:01.822037 19984 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1122 10:07:06.133253 19984 solver.cpp:218] Iteration 12300 (23.1964 iter/s, 4.31102s/100 iters), loss = 0.134974
I1122 10:07:06.133253 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:07:06.133253 19984 solver.cpp:237]     Train net output #1: loss = 0.134974 (* 1 = 0.134974 loss)
I1122 10:07:06.133253 19984 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1122 10:07:10.447831 19984 solver.cpp:218] Iteration 12400 (23.1783 iter/s, 4.31438s/100 iters), loss = 0.0938488
I1122 10:07:10.447831 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:07:10.447831 19984 solver.cpp:237]     Train net output #1: loss = 0.0938488 (* 1 = 0.0938488 loss)
I1122 10:07:10.447831 19984 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1122 10:07:14.525820 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:07:14.693673 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_12500.caffemodel
I1122 10:07:14.703675 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_12500.solverstate
I1122 10:07:14.707675 19984 solver.cpp:330] Iteration 12500, Testing net (#0)
I1122 10:07:14.707675 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:07:15.774147 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:07:15.816149 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8937
I1122 10:07:15.816149 19984 solver.cpp:397]     Test net output #1: loss = 0.307687 (* 1 = 0.307687 loss)
I1122 10:07:15.857672 19984 solver.cpp:218] Iteration 12500 (18.4883 iter/s, 5.40884s/100 iters), loss = 0.186915
I1122 10:07:15.857672 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:07:15.857672 19984 solver.cpp:237]     Train net output #1: loss = 0.186915 (* 1 = 0.186915 loss)
I1122 10:07:15.857672 19984 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1122 10:07:20.143754 19984 solver.cpp:218] Iteration 12600 (23.3301 iter/s, 4.28631s/100 iters), loss = 0.156133
I1122 10:07:20.143754 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:07:20.143754 19984 solver.cpp:237]     Train net output #1: loss = 0.156133 (* 1 = 0.156133 loss)
I1122 10:07:20.143754 19984 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1122 10:07:24.450525 19984 solver.cpp:218] Iteration 12700 (23.2215 iter/s, 4.30636s/100 iters), loss = 0.163764
I1122 10:07:24.450525 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:07:24.450525 19984 solver.cpp:237]     Train net output #1: loss = 0.163764 (* 1 = 0.163764 loss)
I1122 10:07:24.450525 19984 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1122 10:07:28.746855 19984 solver.cpp:218] Iteration 12800 (23.2808 iter/s, 4.29538s/100 iters), loss = 0.175618
I1122 10:07:28.746855 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:07:28.746855 19984 solver.cpp:237]     Train net output #1: loss = 0.175618 (* 1 = 0.175618 loss)
I1122 10:07:28.746855 19984 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1122 10:07:33.027796 19984 solver.cpp:218] Iteration 12900 (23.3597 iter/s, 4.28088s/100 iters), loss = 0.0776508
I1122 10:07:33.027796 19984 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 10:07:33.027796 19984 solver.cpp:237]     Train net output #1: loss = 0.0776508 (* 1 = 0.0776508 loss)
I1122 10:07:33.027796 19984 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1122 10:07:37.118407 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:07:37.286430 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_13000.caffemodel
I1122 10:07:37.296429 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_13000.solverstate
I1122 10:07:37.300431 19984 solver.cpp:330] Iteration 13000, Testing net (#0)
I1122 10:07:37.300431 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:07:38.367712 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:07:38.409739 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8949
I1122 10:07:38.409739 19984 solver.cpp:397]     Test net output #1: loss = 0.308202 (* 1 = 0.308202 loss)
I1122 10:07:38.450742 19984 solver.cpp:218] Iteration 13000 (18.4416 iter/s, 5.42253s/100 iters), loss = 0.203726
I1122 10:07:38.450742 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:07:38.450742 19984 solver.cpp:237]     Train net output #1: loss = 0.203726 (* 1 = 0.203726 loss)
I1122 10:07:38.450742 19984 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1122 10:07:42.730733 19984 solver.cpp:218] Iteration 13100 (23.3659 iter/s, 4.27973s/100 iters), loss = 0.166987
I1122 10:07:42.730733 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:07:42.730733 19984 solver.cpp:237]     Train net output #1: loss = 0.166987 (* 1 = 0.166987 loss)
I1122 10:07:42.730733 19984 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1122 10:07:47.034966 19984 solver.cpp:218] Iteration 13200 (23.2361 iter/s, 4.30366s/100 iters), loss = 0.142199
I1122 10:07:47.034966 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:07:47.034966 19984 solver.cpp:237]     Train net output #1: loss = 0.142199 (* 1 = 0.142199 loss)
I1122 10:07:47.034966 19984 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1122 10:07:51.325557 19984 solver.cpp:218] Iteration 13300 (23.3097 iter/s, 4.29006s/100 iters), loss = 0.176357
I1122 10:07:51.325557 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:07:51.325557 19984 solver.cpp:237]     Train net output #1: loss = 0.176357 (* 1 = 0.176357 loss)
I1122 10:07:51.325557 19984 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1122 10:07:55.609338 19984 solver.cpp:218] Iteration 13400 (23.349 iter/s, 4.28283s/100 iters), loss = 0.0840036
I1122 10:07:55.609338 19984 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 10:07:55.609338 19984 solver.cpp:237]     Train net output #1: loss = 0.0840037 (* 1 = 0.0840037 loss)
I1122 10:07:55.609338 19984 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1122 10:07:59.676339 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:07:59.844456 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_13500.caffemodel
I1122 10:07:59.855445 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_13500.solverstate
I1122 10:07:59.859446 19984 solver.cpp:330] Iteration 13500, Testing net (#0)
I1122 10:07:59.859446 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:08:00.924922 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:08:00.966915 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8942
I1122 10:08:00.966915 19984 solver.cpp:397]     Test net output #1: loss = 0.30669 (* 1 = 0.30669 loss)
I1122 10:08:01.007925 19984 solver.cpp:218] Iteration 13500 (18.5229 iter/s, 5.39873s/100 iters), loss = 0.146608
I1122 10:08:01.007925 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:08:01.007925 19984 solver.cpp:237]     Train net output #1: loss = 0.146608 (* 1 = 0.146608 loss)
I1122 10:08:01.007925 19984 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1122 10:08:05.268244 19984 solver.cpp:218] Iteration 13600 (23.4782 iter/s, 4.25927s/100 iters), loss = 0.163876
I1122 10:08:05.268244 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:08:05.268244 19984 solver.cpp:237]     Train net output #1: loss = 0.163876 (* 1 = 0.163876 loss)
I1122 10:08:05.268244 19984 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1122 10:08:09.541514 19984 solver.cpp:218] Iteration 13700 (23.3984 iter/s, 4.2738s/100 iters), loss = 0.128031
I1122 10:08:09.542515 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:08:09.542515 19984 solver.cpp:237]     Train net output #1: loss = 0.128031 (* 1 = 0.128031 loss)
I1122 10:08:09.542515 19984 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1122 10:08:13.814541 19984 solver.cpp:218] Iteration 13800 (23.4071 iter/s, 4.27221s/100 iters), loss = 0.14536
I1122 10:08:13.814541 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:08:13.814541 19984 solver.cpp:237]     Train net output #1: loss = 0.145361 (* 1 = 0.145361 loss)
I1122 10:08:13.814541 19984 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1122 10:08:18.075695 19984 solver.cpp:218] Iteration 13900 (23.4709 iter/s, 4.2606s/100 iters), loss = 0.0831821
I1122 10:08:18.075695 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:08:18.075695 19984 solver.cpp:237]     Train net output #1: loss = 0.0831822 (* 1 = 0.0831822 loss)
I1122 10:08:18.075695 19984 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1122 10:08:22.144217 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:08:22.311825 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_14000.caffemodel
I1122 10:08:22.321799 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_14000.solverstate
I1122 10:08:22.326797 19984 solver.cpp:330] Iteration 14000, Testing net (#0)
I1122 10:08:22.326797 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:08:23.392204 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:08:23.433727 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8946
I1122 10:08:23.433727 19984 solver.cpp:397]     Test net output #1: loss = 0.306328 (* 1 = 0.306328 loss)
I1122 10:08:23.474721 19984 solver.cpp:218] Iteration 14000 (18.5228 iter/s, 5.39876s/100 iters), loss = 0.1454
I1122 10:08:23.474721 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:08:23.474721 19984 solver.cpp:237]     Train net output #1: loss = 0.1454 (* 1 = 0.1454 loss)
I1122 10:08:23.474721 19984 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1122 10:08:27.762392 19984 solver.cpp:218] Iteration 14100 (23.3261 iter/s, 4.28704s/100 iters), loss = 0.160137
I1122 10:08:27.762392 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:08:27.762392 19984 solver.cpp:237]     Train net output #1: loss = 0.160137 (* 1 = 0.160137 loss)
I1122 10:08:27.762392 19984 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1122 10:08:32.058146 19984 solver.cpp:218] Iteration 14200 (23.2776 iter/s, 4.29598s/100 iters), loss = 0.13799
I1122 10:08:32.058146 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:08:32.058146 19984 solver.cpp:237]     Train net output #1: loss = 0.13799 (* 1 = 0.13799 loss)
I1122 10:08:32.058146 19984 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1122 10:08:36.359681 19984 solver.cpp:218] Iteration 14300 (23.2548 iter/s, 4.30018s/100 iters), loss = 0.103034
I1122 10:08:36.359681 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:08:36.359681 19984 solver.cpp:237]     Train net output #1: loss = 0.103034 (* 1 = 0.103034 loss)
I1122 10:08:36.359681 19984 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1122 10:08:40.646543 19984 solver.cpp:218] Iteration 14400 (23.3275 iter/s, 4.28678s/100 iters), loss = 0.087786
I1122 10:08:40.646543 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:08:40.646543 19984 solver.cpp:237]     Train net output #1: loss = 0.087786 (* 1 = 0.087786 loss)
I1122 10:08:40.646543 19984 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1122 10:08:44.721845 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:08:44.894402 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_14500.caffemodel
I1122 10:08:44.905913 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_14500.solverstate
I1122 10:08:44.910423 19984 solver.cpp:330] Iteration 14500, Testing net (#0)
I1122 10:08:44.910423 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:08:45.989358 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:08:46.031394 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8955
I1122 10:08:46.031394 19984 solver.cpp:397]     Test net output #1: loss = 0.307028 (* 1 = 0.307028 loss)
I1122 10:08:46.072392 19984 solver.cpp:218] Iteration 14500 (18.4305 iter/s, 5.4258s/100 iters), loss = 0.126714
I1122 10:08:46.072392 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:08:46.072392 19984 solver.cpp:237]     Train net output #1: loss = 0.126714 (* 1 = 0.126714 loss)
I1122 10:08:46.072392 19984 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1122 10:08:50.340106 19984 solver.cpp:218] Iteration 14600 (23.4329 iter/s, 4.2675s/100 iters), loss = 0.130849
I1122 10:08:50.340106 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:08:50.340106 19984 solver.cpp:237]     Train net output #1: loss = 0.130849 (* 1 = 0.130849 loss)
I1122 10:08:50.340106 19984 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1122 10:08:54.647809 19984 solver.cpp:218] Iteration 14700 (23.2185 iter/s, 4.30691s/100 iters), loss = 0.146733
I1122 10:08:54.647809 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:08:54.647809 19984 solver.cpp:237]     Train net output #1: loss = 0.146733 (* 1 = 0.146733 loss)
I1122 10:08:54.647809 19984 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1122 10:08:58.987244 19984 solver.cpp:218] Iteration 14800 (23.0455 iter/s, 4.33925s/100 iters), loss = 0.221385
I1122 10:08:58.987244 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 10:08:58.987244 19984 solver.cpp:237]     Train net output #1: loss = 0.221385 (* 1 = 0.221385 loss)
I1122 10:08:58.987244 19984 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1122 10:09:03.275362 19984 solver.cpp:218] Iteration 14900 (23.323 iter/s, 4.28761s/100 iters), loss = 0.0895145
I1122 10:09:03.275362 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:09:03.275362 19984 solver.cpp:237]     Train net output #1: loss = 0.0895144 (* 1 = 0.0895144 loss)
I1122 10:09:03.275362 19984 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1122 10:09:07.358978 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:09:07.529436 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_15000.caffemodel
I1122 10:09:07.543424 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_15000.solverstate
I1122 10:09:07.571427 19984 solver.cpp:330] Iteration 15000, Testing net (#0)
I1122 10:09:07.571427 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:09:08.650669 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:09:08.693651 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8965
I1122 10:09:08.693651 19984 solver.cpp:397]     Test net output #1: loss = 0.306158 (* 1 = 0.306158 loss)
I1122 10:09:08.736158 19984 solver.cpp:218] Iteration 15000 (18.3168 iter/s, 5.45946s/100 iters), loss = 0.162612
I1122 10:09:08.736158 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:09:08.736158 19984 solver.cpp:237]     Train net output #1: loss = 0.162612 (* 1 = 0.162612 loss)
I1122 10:09:08.736158 19984 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1122 10:09:13.060783 19984 solver.cpp:218] Iteration 15100 (23.1242 iter/s, 4.32447s/100 iters), loss = 0.175847
I1122 10:09:13.060783 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:09:13.060783 19984 solver.cpp:237]     Train net output #1: loss = 0.175847 (* 1 = 0.175847 loss)
I1122 10:09:13.060783 19984 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1122 10:09:17.388372 19984 solver.cpp:218] Iteration 15200 (23.1112 iter/s, 4.32691s/100 iters), loss = 0.128749
I1122 10:09:17.388372 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:09:17.388372 19984 solver.cpp:237]     Train net output #1: loss = 0.128749 (* 1 = 0.128749 loss)
I1122 10:09:17.388372 19984 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1122 10:09:21.671010 19984 solver.cpp:218] Iteration 15300 (23.3496 iter/s, 4.28273s/100 iters), loss = 0.144375
I1122 10:09:21.671010 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:09:21.671010 19984 solver.cpp:237]     Train net output #1: loss = 0.144375 (* 1 = 0.144375 loss)
I1122 10:09:21.671010 19984 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1122 10:09:21.671010 19984 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1122 10:09:25.954020 19984 solver.cpp:218] Iteration 15400 (23.348 iter/s, 4.28303s/100 iters), loss = 0.108759
I1122 10:09:25.954020 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:09:25.954020 19984 solver.cpp:237]     Train net output #1: loss = 0.108759 (* 1 = 0.108759 loss)
I1122 10:09:25.954020 19984 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1122 10:09:30.024195 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:09:30.191395 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_15500.caffemodel
I1122 10:09:30.201400 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_15500.solverstate
I1122 10:09:30.205399 19984 solver.cpp:330] Iteration 15500, Testing net (#0)
I1122 10:09:30.205399 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:09:31.271430 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:09:31.315428 19984 solver.cpp:397]     Test net output #0: accuracy = 0.896
I1122 10:09:31.315428 19984 solver.cpp:397]     Test net output #1: loss = 0.307043 (* 1 = 0.307043 loss)
I1122 10:09:31.357460 19984 solver.cpp:218] Iteration 15500 (18.5111 iter/s, 5.40216s/100 iters), loss = 0.14588
I1122 10:09:31.357460 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:09:31.357460 19984 solver.cpp:237]     Train net output #1: loss = 0.14588 (* 1 = 0.14588 loss)
I1122 10:09:31.357460 19984 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1122 10:09:35.639878 19984 solver.cpp:218] Iteration 15600 (23.3515 iter/s, 4.28238s/100 iters), loss = 0.155876
I1122 10:09:35.639878 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:09:35.639878 19984 solver.cpp:237]     Train net output #1: loss = 0.155876 (* 1 = 0.155876 loss)
I1122 10:09:35.639878 19984 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1122 10:09:39.923642 19984 solver.cpp:218] Iteration 15700 (23.3463 iter/s, 4.28333s/100 iters), loss = 0.114808
I1122 10:09:39.923642 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:09:39.923642 19984 solver.cpp:237]     Train net output #1: loss = 0.114808 (* 1 = 0.114808 loss)
I1122 10:09:39.923642 19984 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1122 10:09:44.204640 19984 solver.cpp:218] Iteration 15800 (23.3632 iter/s, 4.28023s/100 iters), loss = 0.140753
I1122 10:09:44.204640 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:09:44.204640 19984 solver.cpp:237]     Train net output #1: loss = 0.140753 (* 1 = 0.140753 loss)
I1122 10:09:44.204640 19984 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1122 10:09:48.482591 19984 solver.cpp:218] Iteration 15900 (23.3743 iter/s, 4.27821s/100 iters), loss = 0.0923913
I1122 10:09:48.482591 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:09:48.482591 19984 solver.cpp:237]     Train net output #1: loss = 0.0923913 (* 1 = 0.0923913 loss)
I1122 10:09:48.482591 19984 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1122 10:09:52.552410 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:09:52.720741 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_16000.caffemodel
I1122 10:09:52.732738 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_16000.solverstate
I1122 10:09:52.736740 19984 solver.cpp:330] Iteration 16000, Testing net (#0)
I1122 10:09:52.736740 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:09:53.805438 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:09:53.847452 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8952
I1122 10:09:53.848440 19984 solver.cpp:397]     Test net output #1: loss = 0.306489 (* 1 = 0.306489 loss)
I1122 10:09:53.889467 19984 solver.cpp:218] Iteration 16000 (18.4961 iter/s, 5.40654s/100 iters), loss = 0.186155
I1122 10:09:53.889467 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:09:53.889467 19984 solver.cpp:237]     Train net output #1: loss = 0.186155 (* 1 = 0.186155 loss)
I1122 10:09:53.889467 19984 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1122 10:09:58.204612 19984 solver.cpp:218] Iteration 16100 (23.1766 iter/s, 4.3147s/100 iters), loss = 0.163694
I1122 10:09:58.205612 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:09:58.205612 19984 solver.cpp:237]     Train net output #1: loss = 0.163694 (* 1 = 0.163694 loss)
I1122 10:09:58.205612 19984 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1122 10:10:02.542081 19984 solver.cpp:218] Iteration 16200 (23.0587 iter/s, 4.33676s/100 iters), loss = 0.13592
I1122 10:10:02.542081 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:10:02.542081 19984 solver.cpp:237]     Train net output #1: loss = 0.13592 (* 1 = 0.13592 loss)
I1122 10:10:02.542081 19984 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1122 10:10:06.864487 19984 solver.cpp:218] Iteration 16300 (23.1373 iter/s, 4.32202s/100 iters), loss = 0.149622
I1122 10:10:06.864487 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:10:06.864487 19984 solver.cpp:237]     Train net output #1: loss = 0.149622 (* 1 = 0.149622 loss)
I1122 10:10:06.864487 19984 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1122 10:10:11.167254 19984 solver.cpp:218] Iteration 16400 (23.2454 iter/s, 4.30192s/100 iters), loss = 0.0826297
I1122 10:10:11.167254 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:10:11.167254 19984 solver.cpp:237]     Train net output #1: loss = 0.0826298 (* 1 = 0.0826298 loss)
I1122 10:10:11.167254 19984 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1122 10:10:15.233077 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:10:15.401712 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_16500.caffemodel
I1122 10:10:15.411713 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_16500.solverstate
I1122 10:10:15.416713 19984 solver.cpp:330] Iteration 16500, Testing net (#0)
I1122 10:10:15.416713 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:10:16.483165 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:10:16.525164 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8957
I1122 10:10:16.525164 19984 solver.cpp:397]     Test net output #1: loss = 0.306057 (* 1 = 0.306057 loss)
I1122 10:10:16.565673 19984 solver.cpp:218] Iteration 16500 (18.5263 iter/s, 5.39774s/100 iters), loss = 0.170709
I1122 10:10:16.565673 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:10:16.565673 19984 solver.cpp:237]     Train net output #1: loss = 0.170709 (* 1 = 0.170709 loss)
I1122 10:10:16.565673 19984 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1122 10:10:20.819046 19984 solver.cpp:218] Iteration 16600 (23.5105 iter/s, 4.25342s/100 iters), loss = 0.184236
I1122 10:10:20.819046 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:10:20.819046 19984 solver.cpp:237]     Train net output #1: loss = 0.184236 (* 1 = 0.184236 loss)
I1122 10:10:20.819046 19984 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1122 10:10:25.072558 19984 solver.cpp:218] Iteration 16700 (23.5124 iter/s, 4.25307s/100 iters), loss = 0.148736
I1122 10:10:25.072558 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:10:25.072558 19984 solver.cpp:237]     Train net output #1: loss = 0.148736 (* 1 = 0.148736 loss)
I1122 10:10:25.072558 19984 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1122 10:10:29.324007 19984 solver.cpp:218] Iteration 16800 (23.5255 iter/s, 4.25071s/100 iters), loss = 0.154849
I1122 10:10:29.324007 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:10:29.324007 19984 solver.cpp:237]     Train net output #1: loss = 0.154849 (* 1 = 0.154849 loss)
I1122 10:10:29.324007 19984 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1122 10:10:33.577010 19984 solver.cpp:218] Iteration 16900 (23.5155 iter/s, 4.25251s/100 iters), loss = 0.0679336
I1122 10:10:33.577010 19984 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 10:10:33.577010 19984 solver.cpp:237]     Train net output #1: loss = 0.0679337 (* 1 = 0.0679337 loss)
I1122 10:10:33.577010 19984 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1122 10:10:37.621868 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:10:37.788718 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_17000.caffemodel
I1122 10:10:37.799721 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_17000.solverstate
I1122 10:10:37.803736 19984 solver.cpp:330] Iteration 17000, Testing net (#0)
I1122 10:10:37.803736 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:10:38.871248 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:10:38.912879 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8959
I1122 10:10:38.912879 19984 solver.cpp:397]     Test net output #1: loss = 0.306283 (* 1 = 0.306283 loss)
I1122 10:10:38.953896 19984 solver.cpp:218] Iteration 17000 (18.5973 iter/s, 5.37713s/100 iters), loss = 0.144401
I1122 10:10:38.953896 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:10:38.953896 19984 solver.cpp:237]     Train net output #1: loss = 0.144401 (* 1 = 0.144401 loss)
I1122 10:10:38.953896 19984 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1122 10:10:43.220365 19984 solver.cpp:218] Iteration 17100 (23.4438 iter/s, 4.26552s/100 iters), loss = 0.174082
I1122 10:10:43.220365 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:10:43.220365 19984 solver.cpp:237]     Train net output #1: loss = 0.174082 (* 1 = 0.174082 loss)
I1122 10:10:43.220365 19984 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1122 10:10:47.497318 19984 solver.cpp:218] Iteration 17200 (23.3824 iter/s, 4.27671s/100 iters), loss = 0.16405
I1122 10:10:47.497318 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:10:47.497318 19984 solver.cpp:237]     Train net output #1: loss = 0.16405 (* 1 = 0.16405 loss)
I1122 10:10:47.497318 19984 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1122 10:10:51.780618 19984 solver.cpp:218] Iteration 17300 (23.3509 iter/s, 4.28249s/100 iters), loss = 0.140524
I1122 10:10:51.780618 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:10:51.780618 19984 solver.cpp:237]     Train net output #1: loss = 0.140524 (* 1 = 0.140524 loss)
I1122 10:10:51.780618 19984 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1122 10:10:56.058113 19984 solver.cpp:218] Iteration 17400 (23.3801 iter/s, 4.27715s/100 iters), loss = 0.115044
I1122 10:10:56.058113 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:10:56.058113 19984 solver.cpp:237]     Train net output #1: loss = 0.115044 (* 1 = 0.115044 loss)
I1122 10:10:56.058113 19984 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1122 10:11:00.125470 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:11:00.292657 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_17500.caffemodel
I1122 10:11:00.302654 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_17500.solverstate
I1122 10:11:00.307657 19984 solver.cpp:330] Iteration 17500, Testing net (#0)
I1122 10:11:00.307657 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:11:01.374851 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:11:01.416867 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8957
I1122 10:11:01.416867 19984 solver.cpp:397]     Test net output #1: loss = 0.306363 (* 1 = 0.306363 loss)
I1122 10:11:01.457877 19984 solver.cpp:218] Iteration 17500 (18.519 iter/s, 5.39985s/100 iters), loss = 0.165318
I1122 10:11:01.457877 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:11:01.457877 19984 solver.cpp:237]     Train net output #1: loss = 0.165318 (* 1 = 0.165318 loss)
I1122 10:11:01.457877 19984 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1122 10:11:05.737431 19984 solver.cpp:218] Iteration 17600 (23.3698 iter/s, 4.27903s/100 iters), loss = 0.123125
I1122 10:11:05.737431 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:11:05.737942 19984 solver.cpp:237]     Train net output #1: loss = 0.123125 (* 1 = 0.123125 loss)
I1122 10:11:05.737942 19984 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1122 10:11:10.015944 19984 solver.cpp:218] Iteration 17700 (23.3755 iter/s, 4.27799s/100 iters), loss = 0.135244
I1122 10:11:10.015944 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:11:10.015944 19984 solver.cpp:237]     Train net output #1: loss = 0.135244 (* 1 = 0.135244 loss)
I1122 10:11:10.015944 19984 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1122 10:11:14.289693 19984 solver.cpp:218] Iteration 17800 (23.4007 iter/s, 4.27339s/100 iters), loss = 0.165873
I1122 10:11:14.289693 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:11:14.289693 19984 solver.cpp:237]     Train net output #1: loss = 0.165873 (* 1 = 0.165873 loss)
I1122 10:11:14.289693 19984 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1122 10:11:18.555421 19984 solver.cpp:218] Iteration 17900 (23.4429 iter/s, 4.26568s/100 iters), loss = 0.0744689
I1122 10:11:18.555421 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:11:18.555421 19984 solver.cpp:237]     Train net output #1: loss = 0.0744688 (* 1 = 0.0744688 loss)
I1122 10:11:18.555421 19984 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1122 10:11:22.613512 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:11:22.781605 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_18000.caffemodel
I1122 10:11:22.793100 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_18000.solverstate
I1122 10:11:22.797103 19984 solver.cpp:330] Iteration 18000, Testing net (#0)
I1122 10:11:22.797103 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:11:23.863538 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:11:23.906087 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8955
I1122 10:11:23.906087 19984 solver.cpp:397]     Test net output #1: loss = 0.306322 (* 1 = 0.306322 loss)
I1122 10:11:23.947083 19984 solver.cpp:218] Iteration 18000 (18.5495 iter/s, 5.39099s/100 iters), loss = 0.170218
I1122 10:11:23.947083 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:11:23.947083 19984 solver.cpp:237]     Train net output #1: loss = 0.170218 (* 1 = 0.170218 loss)
I1122 10:11:23.947083 19984 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1122 10:11:28.217262 19984 solver.cpp:218] Iteration 18100 (23.4174 iter/s, 4.27034s/100 iters), loss = 0.154853
I1122 10:11:28.218267 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:11:28.218267 19984 solver.cpp:237]     Train net output #1: loss = 0.154853 (* 1 = 0.154853 loss)
I1122 10:11:28.218267 19984 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1122 10:11:32.547683 19984 solver.cpp:218] Iteration 18200 (23.1 iter/s, 4.329s/100 iters), loss = 0.13709
I1122 10:11:32.547683 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:11:32.547683 19984 solver.cpp:237]     Train net output #1: loss = 0.13709 (* 1 = 0.13709 loss)
I1122 10:11:32.547683 19984 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1122 10:11:36.870841 19984 solver.cpp:218] Iteration 18300 (23.1306 iter/s, 4.32328s/100 iters), loss = 0.155546
I1122 10:11:36.870841 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:11:36.870841 19984 solver.cpp:237]     Train net output #1: loss = 0.155546 (* 1 = 0.155546 loss)
I1122 10:11:36.870841 19984 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1122 10:11:41.194222 19984 solver.cpp:218] Iteration 18400 (23.1321 iter/s, 4.32299s/100 iters), loss = 0.081097
I1122 10:11:41.194739 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:11:41.194739 19984 solver.cpp:237]     Train net output #1: loss = 0.0810969 (* 1 = 0.0810969 loss)
I1122 10:11:41.194739 19984 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1122 10:11:45.297833 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:11:45.466275 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_18500.caffemodel
I1122 10:11:45.476256 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_18500.solverstate
I1122 10:11:45.480257 19984 solver.cpp:330] Iteration 18500, Testing net (#0)
I1122 10:11:45.480257 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:11:46.549437 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:11:46.590960 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8952
I1122 10:11:46.590960 19984 solver.cpp:397]     Test net output #1: loss = 0.306246 (* 1 = 0.306246 loss)
I1122 10:11:46.631474 19984 solver.cpp:218] Iteration 18500 (18.392 iter/s, 5.43716s/100 iters), loss = 0.13729
I1122 10:11:46.631474 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:11:46.631474 19984 solver.cpp:237]     Train net output #1: loss = 0.13729 (* 1 = 0.13729 loss)
I1122 10:11:46.631474 19984 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1122 10:11:50.918653 19984 solver.cpp:218] Iteration 18600 (23.3307 iter/s, 4.2862s/100 iters), loss = 0.155233
I1122 10:11:50.918653 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:11:50.918653 19984 solver.cpp:237]     Train net output #1: loss = 0.155233 (* 1 = 0.155233 loss)
I1122 10:11:50.918653 19984 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1122 10:11:55.181154 19984 solver.cpp:218] Iteration 18700 (23.4634 iter/s, 4.26195s/100 iters), loss = 0.118926
I1122 10:11:55.181154 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:11:55.181154 19984 solver.cpp:237]     Train net output #1: loss = 0.118926 (* 1 = 0.118926 loss)
I1122 10:11:55.181154 19984 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1122 10:11:59.443974 19984 solver.cpp:218] Iteration 18800 (23.4557 iter/s, 4.26336s/100 iters), loss = 0.137836
I1122 10:11:59.443974 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:11:59.443974 19984 solver.cpp:237]     Train net output #1: loss = 0.137836 (* 1 = 0.137836 loss)
I1122 10:11:59.444974 19984 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1122 10:12:03.709187 19984 solver.cpp:218] Iteration 18900 (23.4512 iter/s, 4.26418s/100 iters), loss = 0.115507
I1122 10:12:03.709187 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:12:03.709187 19984 solver.cpp:237]     Train net output #1: loss = 0.115507 (* 1 = 0.115507 loss)
I1122 10:12:03.709187 19984 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1122 10:12:07.764902 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:12:07.934798 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_19000.caffemodel
I1122 10:12:07.945798 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_19000.solverstate
I1122 10:12:07.949798 19984 solver.cpp:330] Iteration 19000, Testing net (#0)
I1122 10:12:07.949798 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:12:09.016130 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:12:09.057653 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8956
I1122 10:12:09.057653 19984 solver.cpp:397]     Test net output #1: loss = 0.306244 (* 1 = 0.306244 loss)
I1122 10:12:09.098632 19984 solver.cpp:218] Iteration 19000 (18.5558 iter/s, 5.38915s/100 iters), loss = 0.170407
I1122 10:12:09.098632 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:12:09.098632 19984 solver.cpp:237]     Train net output #1: loss = 0.170407 (* 1 = 0.170407 loss)
I1122 10:12:09.098632 19984 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1122 10:12:13.386621 19984 solver.cpp:218] Iteration 19100 (23.32 iter/s, 4.28816s/100 iters), loss = 0.160658
I1122 10:12:13.387630 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:12:13.387630 19984 solver.cpp:237]     Train net output #1: loss = 0.160658 (* 1 = 0.160658 loss)
I1122 10:12:13.387630 19984 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1122 10:12:17.673696 19984 solver.cpp:218] Iteration 19200 (23.3333 iter/s, 4.28572s/100 iters), loss = 0.154813
I1122 10:12:17.673696 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:12:17.673696 19984 solver.cpp:237]     Train net output #1: loss = 0.154812 (* 1 = 0.154812 loss)
I1122 10:12:17.673696 19984 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1122 10:12:21.972257 19984 solver.cpp:218] Iteration 19300 (23.2658 iter/s, 4.29815s/100 iters), loss = 0.135892
I1122 10:12:21.972257 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:12:21.972257 19984 solver.cpp:237]     Train net output #1: loss = 0.135892 (* 1 = 0.135892 loss)
I1122 10:12:21.972257 19984 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1122 10:12:26.257784 19984 solver.cpp:218] Iteration 19400 (23.336 iter/s, 4.28522s/100 iters), loss = 0.0728238
I1122 10:12:26.257784 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:12:26.257784 19984 solver.cpp:237]     Train net output #1: loss = 0.0728238 (* 1 = 0.0728238 loss)
I1122 10:12:26.257784 19984 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1122 10:12:30.329365 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:12:30.496407 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_19500.caffemodel
I1122 10:12:30.506408 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_19500.solverstate
I1122 10:12:30.511406 19984 solver.cpp:330] Iteration 19500, Testing net (#0)
I1122 10:12:30.511406 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:12:31.577612 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:12:31.620594 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8961
I1122 10:12:31.620594 19984 solver.cpp:397]     Test net output #1: loss = 0.30592 (* 1 = 0.30592 loss)
I1122 10:12:31.661123 19984 solver.cpp:218] Iteration 19500 (18.5081 iter/s, 5.40304s/100 iters), loss = 0.13917
I1122 10:12:31.661123 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:12:31.661123 19984 solver.cpp:237]     Train net output #1: loss = 0.13917 (* 1 = 0.13917 loss)
I1122 10:12:31.661123 19984 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1122 10:12:31.661123 19984 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1122 10:12:35.933571 19984 solver.cpp:218] Iteration 19600 (23.4053 iter/s, 4.27254s/100 iters), loss = 0.139351
I1122 10:12:35.933571 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:12:35.933571 19984 solver.cpp:237]     Train net output #1: loss = 0.139351 (* 1 = 0.139351 loss)
I1122 10:12:35.933571 19984 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1122 10:12:40.198159 19984 solver.cpp:218] Iteration 19700 (23.4502 iter/s, 4.26436s/100 iters), loss = 0.16424
I1122 10:12:40.199151 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:12:40.199151 19984 solver.cpp:237]     Train net output #1: loss = 0.16424 (* 1 = 0.16424 loss)
I1122 10:12:40.199151 19984 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1122 10:12:44.454694 19984 solver.cpp:218] Iteration 19800 (23.4974 iter/s, 4.2558s/100 iters), loss = 0.173239
I1122 10:12:44.454694 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:12:44.454694 19984 solver.cpp:237]     Train net output #1: loss = 0.173239 (* 1 = 0.173239 loss)
I1122 10:12:44.454694 19984 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1122 10:12:48.705365 19984 solver.cpp:218] Iteration 19900 (23.5272 iter/s, 4.25039s/100 iters), loss = 0.085186
I1122 10:12:48.705365 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:12:48.705365 19984 solver.cpp:237]     Train net output #1: loss = 0.085186 (* 1 = 0.085186 loss)
I1122 10:12:48.705365 19984 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1122 10:12:52.750388 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:12:52.918452 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_20000.caffemodel
I1122 10:12:52.928433 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_20000.solverstate
I1122 10:12:52.932433 19984 solver.cpp:330] Iteration 20000, Testing net (#0)
I1122 10:12:52.933432 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:12:54.001114 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:12:54.043131 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8963
I1122 10:12:54.043131 19984 solver.cpp:397]     Test net output #1: loss = 0.305875 (* 1 = 0.305875 loss)
I1122 10:12:54.084142 19984 solver.cpp:218] Iteration 20000 (18.5943 iter/s, 5.37801s/100 iters), loss = 0.154781
I1122 10:12:54.084142 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:12:54.084142 19984 solver.cpp:237]     Train net output #1: loss = 0.154781 (* 1 = 0.154781 loss)
I1122 10:12:54.084142 19984 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1122 10:12:58.378904 19984 solver.cpp:218] Iteration 20100 (23.2871 iter/s, 4.29423s/100 iters), loss = 0.142195
I1122 10:12:58.378904 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:12:58.378904 19984 solver.cpp:237]     Train net output #1: loss = 0.142195 (* 1 = 0.142195 loss)
I1122 10:12:58.378904 19984 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1122 10:13:02.647379 19984 solver.cpp:218] Iteration 20200 (23.4292 iter/s, 4.26817s/100 iters), loss = 0.135514
I1122 10:13:02.647379 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:13:02.647379 19984 solver.cpp:237]     Train net output #1: loss = 0.135514 (* 1 = 0.135514 loss)
I1122 10:13:02.647379 19984 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1122 10:13:06.919705 19984 solver.cpp:218] Iteration 20300 (23.4091 iter/s, 4.27185s/100 iters), loss = 0.17798
I1122 10:13:06.919705 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:13:06.919705 19984 solver.cpp:237]     Train net output #1: loss = 0.17798 (* 1 = 0.17798 loss)
I1122 10:13:06.919705 19984 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1122 10:13:11.188446 19984 solver.cpp:218] Iteration 20400 (23.4284 iter/s, 4.26832s/100 iters), loss = 0.0688413
I1122 10:13:11.188446 19984 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 10:13:11.188446 19984 solver.cpp:237]     Train net output #1: loss = 0.0688412 (* 1 = 0.0688412 loss)
I1122 10:13:11.188446 19984 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1122 10:13:15.269358 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:13:15.437155 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_20500.caffemodel
I1122 10:13:15.449142 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_20500.solverstate
I1122 10:13:15.453158 19984 solver.cpp:330] Iteration 20500, Testing net (#0)
I1122 10:13:15.453158 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:13:16.522245 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:13:16.564234 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8963
I1122 10:13:16.564234 19984 solver.cpp:397]     Test net output #1: loss = 0.305811 (* 1 = 0.305811 loss)
I1122 10:13:16.605206 19984 solver.cpp:218] Iteration 20500 (18.4602 iter/s, 5.41707s/100 iters), loss = 0.169348
I1122 10:13:16.605206 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:13:16.605206 19984 solver.cpp:237]     Train net output #1: loss = 0.169348 (* 1 = 0.169348 loss)
I1122 10:13:16.605206 19984 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1122 10:13:20.932143 19984 solver.cpp:218] Iteration 20600 (23.1123 iter/s, 4.3267s/100 iters), loss = 0.14787
I1122 10:13:20.933153 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:13:20.933153 19984 solver.cpp:237]     Train net output #1: loss = 0.14787 (* 1 = 0.14787 loss)
I1122 10:13:20.933153 19984 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1122 10:13:25.241238 19984 solver.cpp:218] Iteration 20700 (23.2142 iter/s, 4.3077s/100 iters), loss = 0.128207
I1122 10:13:25.241238 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:13:25.241238 19984 solver.cpp:237]     Train net output #1: loss = 0.128206 (* 1 = 0.128206 loss)
I1122 10:13:25.241238 19984 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1122 10:13:29.604951 19984 solver.cpp:218] Iteration 20800 (22.9184 iter/s, 4.3633s/100 iters), loss = 0.127346
I1122 10:13:29.604951 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:13:29.604951 19984 solver.cpp:237]     Train net output #1: loss = 0.127346 (* 1 = 0.127346 loss)
I1122 10:13:29.604951 19984 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1122 10:13:33.921108 19984 solver.cpp:218] Iteration 20900 (23.169 iter/s, 4.31612s/100 iters), loss = 0.0721232
I1122 10:13:33.921108 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:13:33.921108 19984 solver.cpp:237]     Train net output #1: loss = 0.0721232 (* 1 = 0.0721232 loss)
I1122 10:13:33.921108 19984 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1122 10:13:38.007963 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:13:38.176288 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_21000.caffemodel
I1122 10:13:38.186282 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_21000.solverstate
I1122 10:13:38.190289 19984 solver.cpp:330] Iteration 21000, Testing net (#0)
I1122 10:13:38.190289 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:13:39.257172 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:13:39.299695 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8963
I1122 10:13:39.299695 19984 solver.cpp:397]     Test net output #1: loss = 0.305878 (* 1 = 0.305878 loss)
I1122 10:13:39.340221 19984 solver.cpp:218] Iteration 21000 (18.4548 iter/s, 5.41865s/100 iters), loss = 0.169344
I1122 10:13:39.340221 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:13:39.340221 19984 solver.cpp:237]     Train net output #1: loss = 0.169344 (* 1 = 0.169344 loss)
I1122 10:13:39.340221 19984 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1122 10:13:43.630656 19984 solver.cpp:218] Iteration 21100 (23.3082 iter/s, 4.29033s/100 iters), loss = 0.142107
I1122 10:13:43.630656 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:13:43.630656 19984 solver.cpp:237]     Train net output #1: loss = 0.142107 (* 1 = 0.142107 loss)
I1122 10:13:43.630656 19984 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1122 10:13:47.914144 19984 solver.cpp:218] Iteration 21200 (23.3494 iter/s, 4.28277s/100 iters), loss = 0.129106
I1122 10:13:47.914144 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:13:47.914144 19984 solver.cpp:237]     Train net output #1: loss = 0.129106 (* 1 = 0.129106 loss)
I1122 10:13:47.914144 19984 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1122 10:13:52.196822 19984 solver.cpp:218] Iteration 21300 (23.3529 iter/s, 4.28212s/100 iters), loss = 0.191948
I1122 10:13:52.196822 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:13:52.196822 19984 solver.cpp:237]     Train net output #1: loss = 0.191948 (* 1 = 0.191948 loss)
I1122 10:13:52.196822 19984 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1122 10:13:56.475900 19984 solver.cpp:218] Iteration 21400 (23.3713 iter/s, 4.27876s/100 iters), loss = 0.0737319
I1122 10:13:56.475900 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:13:56.475900 19984 solver.cpp:237]     Train net output #1: loss = 0.0737318 (* 1 = 0.0737318 loss)
I1122 10:13:56.475900 19984 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1122 10:14:00.552314 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:14:00.719355 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_21500.caffemodel
I1122 10:14:00.730353 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_21500.solverstate
I1122 10:14:00.735352 19984 solver.cpp:330] Iteration 21500, Testing net (#0)
I1122 10:14:00.735352 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:14:01.808451 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:14:01.851047 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8962
I1122 10:14:01.851047 19984 solver.cpp:397]     Test net output #1: loss = 0.305977 (* 1 = 0.305977 loss)
I1122 10:14:01.893057 19984 solver.cpp:218] Iteration 21500 (18.4599 iter/s, 5.41716s/100 iters), loss = 0.128954
I1122 10:14:01.893057 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:14:01.893057 19984 solver.cpp:237]     Train net output #1: loss = 0.128954 (* 1 = 0.128954 loss)
I1122 10:14:01.893057 19984 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1122 10:14:06.166538 19984 solver.cpp:218] Iteration 21600 (23.4038 iter/s, 4.2728s/100 iters), loss = 0.189262
I1122 10:14:06.166538 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:14:06.166538 19984 solver.cpp:237]     Train net output #1: loss = 0.189262 (* 1 = 0.189262 loss)
I1122 10:14:06.166538 19984 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1122 10:14:10.443132 19984 solver.cpp:218] Iteration 21700 (23.3882 iter/s, 4.27566s/100 iters), loss = 0.164169
I1122 10:14:10.443132 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:14:10.443132 19984 solver.cpp:237]     Train net output #1: loss = 0.164168 (* 1 = 0.164168 loss)
I1122 10:14:10.443132 19984 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1122 10:14:14.714785 19984 solver.cpp:218] Iteration 21800 (23.4114 iter/s, 4.27143s/100 iters), loss = 0.166193
I1122 10:14:14.714785 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:14:14.714785 19984 solver.cpp:237]     Train net output #1: loss = 0.166193 (* 1 = 0.166193 loss)
I1122 10:14:14.714785 19984 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1122 10:14:18.977288 19984 solver.cpp:218] Iteration 21900 (23.4608 iter/s, 4.26243s/100 iters), loss = 0.0743509
I1122 10:14:18.977288 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:14:18.977288 19984 solver.cpp:237]     Train net output #1: loss = 0.0743508 (* 1 = 0.0743508 loss)
I1122 10:14:18.977288 19984 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1122 10:14:23.035460 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:14:23.203274 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_22000.caffemodel
I1122 10:14:23.213263 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_22000.solverstate
I1122 10:14:23.217763 19984 solver.cpp:330] Iteration 22000, Testing net (#0)
I1122 10:14:23.217763 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:14:24.283504 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:14:24.325513 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8963
I1122 10:14:24.325513 19984 solver.cpp:397]     Test net output #1: loss = 0.305988 (* 1 = 0.305988 loss)
I1122 10:14:24.368530 19984 solver.cpp:218] Iteration 22000 (18.5499 iter/s, 5.39087s/100 iters), loss = 0.152594
I1122 10:14:24.368530 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:14:24.368530 19984 solver.cpp:237]     Train net output #1: loss = 0.152594 (* 1 = 0.152594 loss)
I1122 10:14:24.368530 19984 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1122 10:14:24.368530 19984 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1122 10:14:28.670600 19984 solver.cpp:218] Iteration 22100 (23.2475 iter/s, 4.30154s/100 iters), loss = 0.141953
I1122 10:14:28.670600 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:14:28.670600 19984 solver.cpp:237]     Train net output #1: loss = 0.141953 (* 1 = 0.141953 loss)
I1122 10:14:28.670600 19984 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1122 10:14:32.957747 19984 solver.cpp:218] Iteration 22200 (23.3266 iter/s, 4.28695s/100 iters), loss = 0.142852
I1122 10:14:32.957747 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:14:32.957747 19984 solver.cpp:237]     Train net output #1: loss = 0.142852 (* 1 = 0.142852 loss)
I1122 10:14:32.957747 19984 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1122 10:14:37.248576 19984 solver.cpp:218] Iteration 22300 (23.3109 iter/s, 4.28984s/100 iters), loss = 0.110063
I1122 10:14:37.248576 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:14:37.248576 19984 solver.cpp:237]     Train net output #1: loss = 0.110063 (* 1 = 0.110063 loss)
I1122 10:14:37.248576 19984 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1122 10:14:41.531137 19984 solver.cpp:218] Iteration 22400 (23.3543 iter/s, 4.28187s/100 iters), loss = 0.0703693
I1122 10:14:41.531137 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:14:41.531137 19984 solver.cpp:237]     Train net output #1: loss = 0.0703692 (* 1 = 0.0703692 loss)
I1122 10:14:41.531137 19984 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1122 10:14:45.602218 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:14:45.770867 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_22500.caffemodel
I1122 10:14:45.783849 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_22500.solverstate
I1122 10:14:45.787863 19984 solver.cpp:330] Iteration 22500, Testing net (#0)
I1122 10:14:45.787863 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:14:46.847726 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:14:46.889726 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8962
I1122 10:14:46.889726 19984 solver.cpp:397]     Test net output #1: loss = 0.305929 (* 1 = 0.305929 loss)
I1122 10:14:46.931232 19984 solver.cpp:218] Iteration 22500 (18.5184 iter/s, 5.40003s/100 iters), loss = 0.161176
I1122 10:14:46.931232 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:14:46.931232 19984 solver.cpp:237]     Train net output #1: loss = 0.161176 (* 1 = 0.161176 loss)
I1122 10:14:46.931232 19984 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1122 10:14:51.228030 19984 solver.cpp:218] Iteration 22600 (23.2765 iter/s, 4.29618s/100 iters), loss = 0.141598
I1122 10:14:51.228030 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:14:51.228030 19984 solver.cpp:237]     Train net output #1: loss = 0.141598 (* 1 = 0.141598 loss)
I1122 10:14:51.228030 19984 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1122 10:14:55.509404 19984 solver.cpp:218] Iteration 22700 (23.3547 iter/s, 4.28179s/100 iters), loss = 0.1173
I1122 10:14:55.509404 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:14:55.509404 19984 solver.cpp:237]     Train net output #1: loss = 0.1173 (* 1 = 0.1173 loss)
I1122 10:14:55.510408 19984 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1122 10:14:59.794454 19984 solver.cpp:218] Iteration 22800 (23.3391 iter/s, 4.28465s/100 iters), loss = 0.153899
I1122 10:14:59.794454 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:14:59.794454 19984 solver.cpp:237]     Train net output #1: loss = 0.153899 (* 1 = 0.153899 loss)
I1122 10:14:59.795455 19984 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1122 10:15:04.080379 19984 solver.cpp:218] Iteration 22900 (23.3359 iter/s, 4.28524s/100 iters), loss = 0.100805
I1122 10:15:04.080379 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:15:04.080379 19984 solver.cpp:237]     Train net output #1: loss = 0.100805 (* 1 = 0.100805 loss)
I1122 10:15:04.080379 19984 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1122 10:15:08.155815 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:15:08.323850 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_23000.caffemodel
I1122 10:15:08.334355 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_23000.solverstate
I1122 10:15:08.338362 19984 solver.cpp:330] Iteration 23000, Testing net (#0)
I1122 10:15:08.338362 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:15:09.404677 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:15:09.447197 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8964
I1122 10:15:09.447197 19984 solver.cpp:397]     Test net output #1: loss = 0.305887 (* 1 = 0.305887 loss)
I1122 10:15:09.487715 19984 solver.cpp:218] Iteration 23000 (18.4949 iter/s, 5.4069s/100 iters), loss = 0.13163
I1122 10:15:09.487715 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:15:09.487715 19984 solver.cpp:237]     Train net output #1: loss = 0.13163 (* 1 = 0.13163 loss)
I1122 10:15:09.487715 19984 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1122 10:15:13.771821 19984 solver.cpp:218] Iteration 23100 (23.3456 iter/s, 4.28346s/100 iters), loss = 0.160368
I1122 10:15:13.771821 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:15:13.771821 19984 solver.cpp:237]     Train net output #1: loss = 0.160368 (* 1 = 0.160368 loss)
I1122 10:15:13.771821 19984 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1122 10:15:18.052609 19984 solver.cpp:218] Iteration 23200 (23.3621 iter/s, 4.28044s/100 iters), loss = 0.162711
I1122 10:15:18.052609 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:15:18.052609 19984 solver.cpp:237]     Train net output #1: loss = 0.162711 (* 1 = 0.162711 loss)
I1122 10:15:18.052609 19984 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1122 10:15:22.334125 19984 solver.cpp:218] Iteration 23300 (23.3569 iter/s, 4.28139s/100 iters), loss = 0.132726
I1122 10:15:22.334125 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:15:22.334125 19984 solver.cpp:237]     Train net output #1: loss = 0.132726 (* 1 = 0.132726 loss)
I1122 10:15:22.334125 19984 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1122 10:15:26.614579 19984 solver.cpp:218] Iteration 23400 (23.3625 iter/s, 4.28037s/100 iters), loss = 0.0794836
I1122 10:15:26.614579 19984 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 10:15:26.615579 19984 solver.cpp:237]     Train net output #1: loss = 0.0794836 (* 1 = 0.0794836 loss)
I1122 10:15:26.615579 19984 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1122 10:15:30.683615 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:15:30.851680 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_23500.caffemodel
I1122 10:15:30.861224 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_23500.solverstate
I1122 10:15:30.865226 19984 solver.cpp:330] Iteration 23500, Testing net (#0)
I1122 10:15:30.865226 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:15:31.931057 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:15:31.974072 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8964
I1122 10:15:31.974072 19984 solver.cpp:397]     Test net output #1: loss = 0.305854 (* 1 = 0.305854 loss)
I1122 10:15:32.015074 19984 solver.cpp:218] Iteration 23500 (18.5193 iter/s, 5.39976s/100 iters), loss = 0.143167
I1122 10:15:32.015074 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:15:32.015074 19984 solver.cpp:237]     Train net output #1: loss = 0.143167 (* 1 = 0.143167 loss)
I1122 10:15:32.015074 19984 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1122 10:15:36.287432 19984 solver.cpp:218] Iteration 23600 (23.4071 iter/s, 4.27221s/100 iters), loss = 0.139247
I1122 10:15:36.287432 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:15:36.287432 19984 solver.cpp:237]     Train net output #1: loss = 0.139247 (* 1 = 0.139247 loss)
I1122 10:15:36.287432 19984 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1122 10:15:40.554133 19984 solver.cpp:218] Iteration 23700 (23.4433 iter/s, 4.26561s/100 iters), loss = 0.117025
I1122 10:15:40.554133 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:15:40.554133 19984 solver.cpp:237]     Train net output #1: loss = 0.117024 (* 1 = 0.117024 loss)
I1122 10:15:40.554133 19984 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1122 10:15:44.821256 19984 solver.cpp:218] Iteration 23800 (23.4338 iter/s, 4.26734s/100 iters), loss = 0.153433
I1122 10:15:44.821256 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:15:44.821256 19984 solver.cpp:237]     Train net output #1: loss = 0.153433 (* 1 = 0.153433 loss)
I1122 10:15:44.821256 19984 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1122 10:15:49.091411 19984 solver.cpp:218] Iteration 23900 (23.4227 iter/s, 4.26936s/100 iters), loss = 0.0910894
I1122 10:15:49.091411 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:15:49.091411 19984 solver.cpp:237]     Train net output #1: loss = 0.0910893 (* 1 = 0.0910893 loss)
I1122 10:15:49.091411 19984 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1122 10:15:53.145939 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:15:53.312991 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_24000.caffemodel
I1122 10:15:53.322976 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_24000.solverstate
I1122 10:15:53.326979 19984 solver.cpp:330] Iteration 24000, Testing net (#0)
I1122 10:15:53.326979 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:15:54.393695 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:15:54.435690 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8966
I1122 10:15:54.435690 19984 solver.cpp:397]     Test net output #1: loss = 0.305858 (* 1 = 0.305858 loss)
I1122 10:15:54.477715 19984 solver.cpp:218] Iteration 24000 (18.568 iter/s, 5.3856s/100 iters), loss = 0.147369
I1122 10:15:54.477715 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:15:54.477715 19984 solver.cpp:237]     Train net output #1: loss = 0.147369 (* 1 = 0.147369 loss)
I1122 10:15:54.477715 19984 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1122 10:15:58.765295 19984 solver.cpp:218] Iteration 24100 (23.3254 iter/s, 4.28717s/100 iters), loss = 0.130037
I1122 10:15:58.765295 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:15:58.765295 19984 solver.cpp:237]     Train net output #1: loss = 0.130037 (* 1 = 0.130037 loss)
I1122 10:15:58.765295 19984 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1122 10:16:03.064563 19984 solver.cpp:218] Iteration 24200 (23.2605 iter/s, 4.29914s/100 iters), loss = 0.154222
I1122 10:16:03.064563 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:16:03.064563 19984 solver.cpp:237]     Train net output #1: loss = 0.154222 (* 1 = 0.154222 loss)
I1122 10:16:03.065058 19984 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1122 10:16:07.350128 19984 solver.cpp:218] Iteration 24300 (23.3368 iter/s, 4.28508s/100 iters), loss = 0.211582
I1122 10:16:07.350128 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:16:07.350128 19984 solver.cpp:237]     Train net output #1: loss = 0.211582 (* 1 = 0.211582 loss)
I1122 10:16:07.350128 19984 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1122 10:16:11.644109 19984 solver.cpp:218] Iteration 24400 (23.2869 iter/s, 4.29427s/100 iters), loss = 0.0740183
I1122 10:16:11.644109 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:16:11.645108 19984 solver.cpp:237]     Train net output #1: loss = 0.0740182 (* 1 = 0.0740182 loss)
I1122 10:16:11.645108 19984 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1122 10:16:15.732509 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:16:15.899585 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_24500.caffemodel
I1122 10:16:15.910584 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_24500.solverstate
I1122 10:16:15.914598 19984 solver.cpp:330] Iteration 24500, Testing net (#0)
I1122 10:16:15.914598 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:16:16.978989 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:16:17.020993 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8962
I1122 10:16:17.020993 19984 solver.cpp:397]     Test net output #1: loss = 0.305926 (* 1 = 0.305926 loss)
I1122 10:16:17.062006 19984 solver.cpp:218] Iteration 24500 (18.4615 iter/s, 5.41668s/100 iters), loss = 0.132633
I1122 10:16:17.062006 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:16:17.062006 19984 solver.cpp:237]     Train net output #1: loss = 0.132633 (* 1 = 0.132633 loss)
I1122 10:16:17.062006 19984 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1122 10:16:21.354195 19984 solver.cpp:218] Iteration 24600 (23.2972 iter/s, 4.29236s/100 iters), loss = 0.121491
I1122 10:16:21.354195 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:16:21.354195 19984 solver.cpp:237]     Train net output #1: loss = 0.121491 (* 1 = 0.121491 loss)
I1122 10:16:21.354195 19984 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1122 10:16:25.622002 19984 solver.cpp:218] Iteration 24700 (23.4352 iter/s, 4.26708s/100 iters), loss = 0.139424
I1122 10:16:25.622002 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:16:25.622002 19984 solver.cpp:237]     Train net output #1: loss = 0.139424 (* 1 = 0.139424 loss)
I1122 10:16:25.622002 19984 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1122 10:16:29.888612 19984 solver.cpp:218] Iteration 24800 (23.4389 iter/s, 4.26642s/100 iters), loss = 0.126948
I1122 10:16:29.888612 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:16:29.888612 19984 solver.cpp:237]     Train net output #1: loss = 0.126948 (* 1 = 0.126948 loss)
I1122 10:16:29.888612 19984 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1122 10:16:34.151386 19984 solver.cpp:218] Iteration 24900 (23.4604 iter/s, 4.2625s/100 iters), loss = 0.0860845
I1122 10:16:34.151386 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:16:34.151386 19984 solver.cpp:237]     Train net output #1: loss = 0.0860845 (* 1 = 0.0860845 loss)
I1122 10:16:34.151386 19984 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1122 10:16:38.223814 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:16:38.390851 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_25000.caffemodel
I1122 10:16:38.403456 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_25000.solverstate
I1122 10:16:38.408462 19984 solver.cpp:330] Iteration 25000, Testing net (#0)
I1122 10:16:38.408462 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:16:39.473419 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:16:39.515471 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8965
I1122 10:16:39.515471 19984 solver.cpp:397]     Test net output #1: loss = 0.30588 (* 1 = 0.30588 loss)
I1122 10:16:39.558466 19984 solver.cpp:218] Iteration 25000 (18.4962 iter/s, 5.40653s/100 iters), loss = 0.180022
I1122 10:16:39.558466 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:16:39.558466 19984 solver.cpp:237]     Train net output #1: loss = 0.180022 (* 1 = 0.180022 loss)
I1122 10:16:39.558466 19984 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1122 10:16:43.845201 19984 solver.cpp:218] Iteration 25100 (23.3314 iter/s, 4.28608s/100 iters), loss = 0.15679
I1122 10:16:43.845201 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:16:43.845201 19984 solver.cpp:237]     Train net output #1: loss = 0.15679 (* 1 = 0.15679 loss)
I1122 10:16:43.845201 19984 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1122 10:16:48.150038 19984 solver.cpp:218] Iteration 25200 (23.2308 iter/s, 4.30463s/100 iters), loss = 0.137934
I1122 10:16:48.150038 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:16:48.150038 19984 solver.cpp:237]     Train net output #1: loss = 0.137934 (* 1 = 0.137934 loss)
I1122 10:16:48.150038 19984 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1122 10:16:52.434856 19984 solver.cpp:218] Iteration 25300 (23.3427 iter/s, 4.28399s/100 iters), loss = 0.199763
I1122 10:16:52.434856 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:16:52.434856 19984 solver.cpp:237]     Train net output #1: loss = 0.199763 (* 1 = 0.199763 loss)
I1122 10:16:52.434856 19984 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1122 10:16:56.711526 19984 solver.cpp:218] Iteration 25400 (23.3848 iter/s, 4.27628s/100 iters), loss = 0.0839267
I1122 10:16:56.711526 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:16:56.711526 19984 solver.cpp:237]     Train net output #1: loss = 0.0839267 (* 1 = 0.0839267 loss)
I1122 10:16:56.711526 19984 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1122 10:17:00.782660 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:17:00.950744 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_25500.caffemodel
I1122 10:17:00.961725 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_25500.solverstate
I1122 10:17:00.965744 19984 solver.cpp:330] Iteration 25500, Testing net (#0)
I1122 10:17:00.965744 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:17:02.031411 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:17:02.072427 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8962
I1122 10:17:02.072427 19984 solver.cpp:397]     Test net output #1: loss = 0.305913 (* 1 = 0.305913 loss)
I1122 10:17:02.113466 19984 solver.cpp:218] Iteration 25500 (18.5107 iter/s, 5.40229s/100 iters), loss = 0.142748
I1122 10:17:02.114470 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:17:02.114470 19984 solver.cpp:237]     Train net output #1: loss = 0.142748 (* 1 = 0.142748 loss)
I1122 10:17:02.114470 19984 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1122 10:17:06.399196 19984 solver.cpp:218] Iteration 25600 (23.3393 iter/s, 4.28462s/100 iters), loss = 0.167839
I1122 10:17:06.399196 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:17:06.399196 19984 solver.cpp:237]     Train net output #1: loss = 0.167839 (* 1 = 0.167839 loss)
I1122 10:17:06.399196 19984 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1122 10:17:10.703677 19984 solver.cpp:218] Iteration 25700 (23.233 iter/s, 4.30421s/100 iters), loss = 0.111657
I1122 10:17:10.704177 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:17:10.704177 19984 solver.cpp:237]     Train net output #1: loss = 0.111657 (* 1 = 0.111657 loss)
I1122 10:17:10.704177 19984 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1122 10:17:15.004957 19984 solver.cpp:218] Iteration 25800 (23.2533 iter/s, 4.30046s/100 iters), loss = 0.179597
I1122 10:17:15.004957 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:17:15.004957 19984 solver.cpp:237]     Train net output #1: loss = 0.179597 (* 1 = 0.179597 loss)
I1122 10:17:15.004957 19984 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1122 10:17:19.310983 19984 solver.cpp:218] Iteration 25900 (23.224 iter/s, 4.30589s/100 iters), loss = 0.073698
I1122 10:17:19.310983 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:17:19.310983 19984 solver.cpp:237]     Train net output #1: loss = 0.073698 (* 1 = 0.073698 loss)
I1122 10:17:19.310983 19984 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1122 10:17:23.394804 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:17:23.562058 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_26000.caffemodel
I1122 10:17:23.573051 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_26000.solverstate
I1122 10:17:23.577054 19984 solver.cpp:330] Iteration 26000, Testing net (#0)
I1122 10:17:23.577054 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:17:24.642213 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:17:24.684195 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8965
I1122 10:17:24.684195 19984 solver.cpp:397]     Test net output #1: loss = 0.305908 (* 1 = 0.305908 loss)
I1122 10:17:24.725224 19984 solver.cpp:218] Iteration 26000 (18.4713 iter/s, 5.41379s/100 iters), loss = 0.178019
I1122 10:17:24.725224 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 10:17:24.725224 19984 solver.cpp:237]     Train net output #1: loss = 0.178019 (* 1 = 0.178019 loss)
I1122 10:17:24.725224 19984 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1122 10:17:29.021260 19984 solver.cpp:218] Iteration 26100 (23.279 iter/s, 4.29572s/100 iters), loss = 0.148809
I1122 10:17:29.021260 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:17:29.021260 19984 solver.cpp:237]     Train net output #1: loss = 0.148809 (* 1 = 0.148809 loss)
I1122 10:17:29.021746 19984 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1122 10:17:33.309285 19984 solver.cpp:218] Iteration 26200 (23.3225 iter/s, 4.28771s/100 iters), loss = 0.128136
I1122 10:17:33.309285 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:17:33.309285 19984 solver.cpp:237]     Train net output #1: loss = 0.128136 (* 1 = 0.128136 loss)
I1122 10:17:33.309285 19984 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1122 10:17:37.622272 19984 solver.cpp:218] Iteration 26300 (23.1885 iter/s, 4.31248s/100 iters), loss = 0.144081
I1122 10:17:37.622272 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:17:37.622272 19984 solver.cpp:237]     Train net output #1: loss = 0.144081 (* 1 = 0.144081 loss)
I1122 10:17:37.622272 19984 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1122 10:17:41.918367 19984 solver.cpp:218] Iteration 26400 (23.2797 iter/s, 4.29559s/100 iters), loss = 0.0747142
I1122 10:17:41.918367 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:17:41.918367 19984 solver.cpp:237]     Train net output #1: loss = 0.0747142 (* 1 = 0.0747142 loss)
I1122 10:17:41.918367 19984 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1122 10:17:46.015138 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:17:46.189891 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_26500.caffemodel
I1122 10:17:46.200893 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_26500.solverstate
I1122 10:17:46.204893 19984 solver.cpp:330] Iteration 26500, Testing net (#0)
I1122 10:17:46.205893 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:17:47.280145 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:17:47.322125 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8964
I1122 10:17:47.322125 19984 solver.cpp:397]     Test net output #1: loss = 0.305918 (* 1 = 0.305918 loss)
I1122 10:17:47.364148 19984 solver.cpp:218] Iteration 26500 (18.3628 iter/s, 5.4458s/100 iters), loss = 0.192324
I1122 10:17:47.364148 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 10:17:47.364148 19984 solver.cpp:237]     Train net output #1: loss = 0.192324 (* 1 = 0.192324 loss)
I1122 10:17:47.364148 19984 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1122 10:17:51.655434 19984 solver.cpp:218] Iteration 26600 (23.3082 iter/s, 4.29034s/100 iters), loss = 0.123718
I1122 10:17:51.655434 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:17:51.655434 19984 solver.cpp:237]     Train net output #1: loss = 0.123718 (* 1 = 0.123718 loss)
I1122 10:17:51.655434 19984 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1122 10:17:55.956104 19984 solver.cpp:218] Iteration 26700 (23.2548 iter/s, 4.30018s/100 iters), loss = 0.146886
I1122 10:17:55.956104 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:17:55.956104 19984 solver.cpp:237]     Train net output #1: loss = 0.146886 (* 1 = 0.146886 loss)
I1122 10:17:55.956104 19984 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1122 10:18:00.241308 19984 solver.cpp:218] Iteration 26800 (23.3395 iter/s, 4.28459s/100 iters), loss = 0.129815
I1122 10:18:00.241308 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:18:00.241308 19984 solver.cpp:237]     Train net output #1: loss = 0.129815 (* 1 = 0.129815 loss)
I1122 10:18:00.241308 19984 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1122 10:18:04.494659 19984 solver.cpp:218] Iteration 26900 (23.5085 iter/s, 4.25378s/100 iters), loss = 0.0856444
I1122 10:18:04.495664 19984 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 10:18:04.495664 19984 solver.cpp:237]     Train net output #1: loss = 0.0856444 (* 1 = 0.0856444 loss)
I1122 10:18:04.495664 19984 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1122 10:18:08.538995 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:18:08.705090 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_27000.caffemodel
I1122 10:18:08.716068 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_27000.solverstate
I1122 10:18:08.721069 19984 solver.cpp:330] Iteration 27000, Testing net (#0)
I1122 10:18:08.721069 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:18:09.784255 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:18:09.826243 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8963
I1122 10:18:09.826243 19984 solver.cpp:397]     Test net output #1: loss = 0.305783 (* 1 = 0.305783 loss)
I1122 10:18:09.867764 19984 solver.cpp:218] Iteration 27000 (18.6147 iter/s, 5.37211s/100 iters), loss = 0.160762
I1122 10:18:09.867764 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:18:09.867764 19984 solver.cpp:237]     Train net output #1: loss = 0.160762 (* 1 = 0.160762 loss)
I1122 10:18:09.867764 19984 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1122 10:18:09.867764 19984 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1122 10:18:14.135387 19984 solver.cpp:218] Iteration 27100 (23.436 iter/s, 4.26695s/100 iters), loss = 0.169078
I1122 10:18:14.135387 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:18:14.135387 19984 solver.cpp:237]     Train net output #1: loss = 0.169078 (* 1 = 0.169078 loss)
I1122 10:18:14.135387 19984 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1122 10:18:18.406335 19984 solver.cpp:218] Iteration 27200 (23.4156 iter/s, 4.27067s/100 iters), loss = 0.158206
I1122 10:18:18.406335 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:18:18.406335 19984 solver.cpp:237]     Train net output #1: loss = 0.158206 (* 1 = 0.158206 loss)
I1122 10:18:18.406335 19984 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1122 10:18:22.671279 19984 solver.cpp:218] Iteration 27300 (23.4459 iter/s, 4.26514s/100 iters), loss = 0.170361
I1122 10:18:22.671279 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:18:22.671279 19984 solver.cpp:237]     Train net output #1: loss = 0.170361 (* 1 = 0.170361 loss)
I1122 10:18:22.671279 19984 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1122 10:18:26.954006 19984 solver.cpp:218] Iteration 27400 (23.3552 iter/s, 4.28169s/100 iters), loss = 0.0903032
I1122 10:18:26.954006 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:18:26.954006 19984 solver.cpp:237]     Train net output #1: loss = 0.0903032 (* 1 = 0.0903032 loss)
I1122 10:18:26.954006 19984 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1122 10:18:31.029521 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:18:31.199667 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_27500.caffemodel
I1122 10:18:31.209666 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_27500.solverstate
I1122 10:18:31.213667 19984 solver.cpp:330] Iteration 27500, Testing net (#0)
I1122 10:18:31.213667 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:18:32.279742 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:18:32.321745 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8962
I1122 10:18:32.321745 19984 solver.cpp:397]     Test net output #1: loss = 0.305852 (* 1 = 0.305852 loss)
I1122 10:18:32.364300 19984 solver.cpp:218] Iteration 27500 (18.4847 iter/s, 5.40988s/100 iters), loss = 0.161441
I1122 10:18:32.364300 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:18:32.364300 19984 solver.cpp:237]     Train net output #1: loss = 0.161441 (* 1 = 0.161441 loss)
I1122 10:18:32.364300 19984 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1122 10:18:36.673213 19984 solver.cpp:218] Iteration 27600 (23.2079 iter/s, 4.30887s/100 iters), loss = 0.144653
I1122 10:18:36.673213 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:18:36.673213 19984 solver.cpp:237]     Train net output #1: loss = 0.144653 (* 1 = 0.144653 loss)
I1122 10:18:36.673213 19984 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1122 10:18:40.975491 19984 solver.cpp:218] Iteration 27700 (23.2451 iter/s, 4.30198s/100 iters), loss = 0.137935
I1122 10:18:40.975491 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:18:40.975491 19984 solver.cpp:237]     Train net output #1: loss = 0.137935 (* 1 = 0.137935 loss)
I1122 10:18:40.975491 19984 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1122 10:18:45.268821 19984 solver.cpp:218] Iteration 27800 (23.2968 iter/s, 4.29243s/100 iters), loss = 0.133123
I1122 10:18:45.268821 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:18:45.268821 19984 solver.cpp:237]     Train net output #1: loss = 0.133123 (* 1 = 0.133123 loss)
I1122 10:18:45.268821 19984 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1122 10:18:49.565057 19984 solver.cpp:218] Iteration 27900 (23.2758 iter/s, 4.29631s/100 iters), loss = 0.07116
I1122 10:18:49.565057 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:18:49.565057 19984 solver.cpp:237]     Train net output #1: loss = 0.0711599 (* 1 = 0.0711599 loss)
I1122 10:18:49.565057 19984 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1122 10:18:53.641993 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:18:53.809633 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_28000.caffemodel
I1122 10:18:53.819613 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_28000.solverstate
I1122 10:18:53.824635 19984 solver.cpp:330] Iteration 28000, Testing net (#0)
I1122 10:18:53.824635 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:18:54.894438 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:18:54.935438 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8964
I1122 10:18:54.935438 19984 solver.cpp:397]     Test net output #1: loss = 0.305969 (* 1 = 0.305969 loss)
I1122 10:18:54.977454 19984 solver.cpp:218] Iteration 28000 (18.4789 iter/s, 5.41157s/100 iters), loss = 0.122919
I1122 10:18:54.977454 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:18:54.977454 19984 solver.cpp:237]     Train net output #1: loss = 0.122919 (* 1 = 0.122919 loss)
I1122 10:18:54.977454 19984 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1122 10:18:59.254920 19984 solver.cpp:218] Iteration 28100 (23.379 iter/s, 4.27734s/100 iters), loss = 0.108111
I1122 10:18:59.255424 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:18:59.255424 19984 solver.cpp:237]     Train net output #1: loss = 0.108111 (* 1 = 0.108111 loss)
I1122 10:18:59.255424 19984 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1122 10:19:03.521564 19984 solver.cpp:218] Iteration 28200 (23.4379 iter/s, 4.2666s/100 iters), loss = 0.136328
I1122 10:19:03.521564 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:19:03.521564 19984 solver.cpp:237]     Train net output #1: loss = 0.136328 (* 1 = 0.136328 loss)
I1122 10:19:03.521564 19984 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1122 10:19:07.781672 19984 solver.cpp:218] Iteration 28300 (23.4779 iter/s, 4.25933s/100 iters), loss = 0.138496
I1122 10:19:07.781672 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:19:07.781672 19984 solver.cpp:237]     Train net output #1: loss = 0.138496 (* 1 = 0.138496 loss)
I1122 10:19:07.781672 19984 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1122 10:19:12.053467 19984 solver.cpp:218] Iteration 28400 (23.4129 iter/s, 4.27114s/100 iters), loss = 0.0578561
I1122 10:19:12.053467 19984 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 10:19:12.053467 19984 solver.cpp:237]     Train net output #1: loss = 0.0578561 (* 1 = 0.0578561 loss)
I1122 10:19:12.053467 19984 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1122 10:19:16.111595 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:19:16.278595 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_28500.caffemodel
I1122 10:19:16.289584 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_28500.solverstate
I1122 10:19:16.293586 19984 solver.cpp:330] Iteration 28500, Testing net (#0)
I1122 10:19:16.293586 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:19:17.359380 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:19:17.401403 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8963
I1122 10:19:17.401403 19984 solver.cpp:397]     Test net output #1: loss = 0.305814 (* 1 = 0.305814 loss)
I1122 10:19:17.442382 19984 solver.cpp:218] Iteration 28500 (18.5582 iter/s, 5.38844s/100 iters), loss = 0.13697
I1122 10:19:17.442382 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:19:17.442382 19984 solver.cpp:237]     Train net output #1: loss = 0.13697 (* 1 = 0.13697 loss)
I1122 10:19:17.442382 19984 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1122 10:19:21.692219 19984 solver.cpp:218] Iteration 28600 (23.5285 iter/s, 4.25016s/100 iters), loss = 0.147867
I1122 10:19:21.692219 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:19:21.692219 19984 solver.cpp:237]     Train net output #1: loss = 0.147867 (* 1 = 0.147867 loss)
I1122 10:19:21.692219 19984 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1122 10:19:25.971045 19984 solver.cpp:218] Iteration 28700 (23.3758 iter/s, 4.27793s/100 iters), loss = 0.145634
I1122 10:19:25.971045 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:19:25.971045 19984 solver.cpp:237]     Train net output #1: loss = 0.145634 (* 1 = 0.145634 loss)
I1122 10:19:25.971045 19984 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1122 10:19:30.263268 19984 solver.cpp:218] Iteration 28800 (23.3014 iter/s, 4.29158s/100 iters), loss = 0.178033
I1122 10:19:30.263268 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:19:30.263268 19984 solver.cpp:237]     Train net output #1: loss = 0.178033 (* 1 = 0.178033 loss)
I1122 10:19:30.263268 19984 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1122 10:19:34.548493 19984 solver.cpp:218] Iteration 28900 (23.3352 iter/s, 4.28537s/100 iters), loss = 0.0918418
I1122 10:19:34.548493 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 10:19:34.548493 19984 solver.cpp:237]     Train net output #1: loss = 0.0918419 (* 1 = 0.0918419 loss)
I1122 10:19:34.548493 19984 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1122 10:19:38.655460 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:19:38.828732 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_29000.caffemodel
I1122 10:19:38.840713 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_29000.solverstate
I1122 10:19:38.845711 19984 solver.cpp:330] Iteration 29000, Testing net (#0)
I1122 10:19:38.845711 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:19:39.934623 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:19:39.977133 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8963
I1122 10:19:39.977133 19984 solver.cpp:397]     Test net output #1: loss = 0.305921 (* 1 = 0.305921 loss)
I1122 10:19:40.019145 19984 solver.cpp:218] Iteration 29000 (18.2816 iter/s, 5.46999s/100 iters), loss = 0.143871
I1122 10:19:40.019145 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 10:19:40.019145 19984 solver.cpp:237]     Train net output #1: loss = 0.143871 (* 1 = 0.143871 loss)
I1122 10:19:40.019145 19984 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1122 10:19:44.355195 19984 solver.cpp:218] Iteration 29100 (23.068 iter/s, 4.33502s/100 iters), loss = 0.143895
I1122 10:19:44.355195 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:19:44.355195 19984 solver.cpp:237]     Train net output #1: loss = 0.143895 (* 1 = 0.143895 loss)
I1122 10:19:44.355195 19984 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1122 10:19:48.644227 19984 solver.cpp:218] Iteration 29200 (23.3157 iter/s, 4.28896s/100 iters), loss = 0.136767
I1122 10:19:48.644227 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 10:19:48.644227 19984 solver.cpp:237]     Train net output #1: loss = 0.136767 (* 1 = 0.136767 loss)
I1122 10:19:48.644227 19984 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1122 10:19:52.986555 19984 solver.cpp:218] Iteration 29300 (23.0312 iter/s, 4.34194s/100 iters), loss = 0.157564
I1122 10:19:52.986555 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:19:52.986555 19984 solver.cpp:237]     Train net output #1: loss = 0.157564 (* 1 = 0.157564 loss)
I1122 10:19:52.986555 19984 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1122 10:19:57.291791 19984 solver.cpp:218] Iteration 29400 (23.2318 iter/s, 4.30444s/100 iters), loss = 0.0935649
I1122 10:19:57.291791 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:19:57.291791 19984 solver.cpp:237]     Train net output #1: loss = 0.093565 (* 1 = 0.093565 loss)
I1122 10:19:57.291791 19984 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1122 10:20:01.385344 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:20:01.555366 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_29500.caffemodel
I1122 10:20:01.568366 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_29500.solverstate
I1122 10:20:01.573367 19984 solver.cpp:330] Iteration 29500, Testing net (#0)
I1122 10:20:01.573367 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:20:02.635383 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:20:02.677371 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8964
I1122 10:20:02.677371 19984 solver.cpp:397]     Test net output #1: loss = 0.305872 (* 1 = 0.305872 loss)
I1122 10:20:02.718878 19984 solver.cpp:218] Iteration 29500 (18.4283 iter/s, 5.42645s/100 iters), loss = 0.165468
I1122 10:20:02.718878 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:20:02.718878 19984 solver.cpp:237]     Train net output #1: loss = 0.165468 (* 1 = 0.165468 loss)
I1122 10:20:02.718878 19984 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1122 10:20:06.972723 19984 solver.cpp:218] Iteration 29600 (23.5115 iter/s, 4.25323s/100 iters), loss = 0.197081
I1122 10:20:06.972723 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 10:20:06.972723 19984 solver.cpp:237]     Train net output #1: loss = 0.197081 (* 1 = 0.197081 loss)
I1122 10:20:06.972723 19984 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1122 10:20:11.228164 19984 solver.cpp:218] Iteration 29700 (23.5008 iter/s, 4.25517s/100 iters), loss = 0.116265
I1122 10:20:11.228164 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:20:11.228164 19984 solver.cpp:237]     Train net output #1: loss = 0.116265 (* 1 = 0.116265 loss)
I1122 10:20:11.228164 19984 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1122 10:20:15.489646 19984 solver.cpp:218] Iteration 29800 (23.4653 iter/s, 4.26161s/100 iters), loss = 0.103811
I1122 10:20:15.489646 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 10:20:15.489646 19984 solver.cpp:237]     Train net output #1: loss = 0.103811 (* 1 = 0.103811 loss)
I1122 10:20:15.489646 19984 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1122 10:20:19.779112 19984 solver.cpp:218] Iteration 29900 (23.3183 iter/s, 4.28848s/100 iters), loss = 0.0603392
I1122 10:20:19.779112 19984 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 10:20:19.779112 19984 solver.cpp:237]     Train net output #1: loss = 0.0603393 (* 1 = 0.0603393 loss)
I1122 10:20:19.779112 19984 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1122 10:20:23.817379 10096 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:20:23.984441 19984 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_30000.caffemodel
I1122 10:20:23.994439 19984 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_8L_iter_30000.solverstate
I1122 10:20:24.011449 19984 solver.cpp:310] Iteration 30000, loss = 0.196095
I1122 10:20:24.011449 19984 solver.cpp:330] Iteration 30000, Testing net (#0)
I1122 10:20:24.011449 19984 net.cpp:676] Ignoring source layer accuracy_training
I1122 10:20:25.078027 17228 data_layer.cpp:73] Restarting data prefetching from start.
I1122 10:20:25.120028 19984 solver.cpp:397]     Test net output #0: accuracy = 0.8962
I1122 10:20:25.120028 19984 solver.cpp:397]     Test net output #1: loss = 0.305969 (* 1 = 0.305969 loss)
I1122 10:20:25.120028 19984 solver.cpp:315] Optimization Done.
I1122 10:20:25.120028 19984 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 

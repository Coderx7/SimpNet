
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1124 05:32:57.045014 15792 caffe.cpp:219] Using GPUs 0
I1124 05:32:57.215813 15792 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1124 05:32:57.517302 15792 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1124 05:32:57.532841 15792 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_1.6k_8L_3x3"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1124 05:32:57.533841 15792 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1124 05:32:57.534842 15792 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1124 05:32:57.534842 15792 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1124 05:32:57.534842 15792 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1124 05:32:57.534842 15792 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1124 05:32:57.534842 15792 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1124 05:32:57.534842 15792 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1124 05:32:57.534842 15792 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1124 05:32:57.534842 15792 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1124 05:32:57.534842 15792 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1124 05:32:57.534842 15792 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1124 05:32:57.534842 15792 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1124 05:32:57.534842 15792 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1124 05:32:57.534842 15792 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_1.6M"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 66
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 215
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1124 05:32:57.547823 15792 layer_factory.cpp:58] Creating layer cifar
I1124 05:32:57.560842 15792 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1124 05:32:57.560842 15792 net.cpp:84] Creating Layer cifar
I1124 05:32:57.560842 15792 net.cpp:380] cifar -> data
I1124 05:32:57.560842 15792 net.cpp:380] cifar -> label
I1124 05:32:57.561842 15792 data_layer.cpp:45] output data size: 100,3,32,32
I1124 05:32:57.567842 15792 net.cpp:122] Setting up cifar
I1124 05:32:57.567842 15792 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1124 05:32:57.567842 15792 net.cpp:129] Top shape: 100 (100)
I1124 05:32:57.567842 15792 net.cpp:137] Memory required for data: 1229200
I1124 05:32:57.567842 15792 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1124 05:32:57.567842 15792 net.cpp:84] Creating Layer label_cifar_1_split
I1124 05:32:57.567842 15792 net.cpp:406] label_cifar_1_split <- label
I1124 05:32:57.567842 15792 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1124 05:32:57.567842 15792 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1124 05:32:57.567842 15792 net.cpp:122] Setting up label_cifar_1_split
I1124 05:32:57.568841 15792 net.cpp:129] Top shape: 100 (100)
I1124 05:32:57.568841 15792 net.cpp:129] Top shape: 100 (100)
I1124 05:32:57.568841 15792 net.cpp:137] Memory required for data: 1230000
I1124 05:32:57.568841 15792 layer_factory.cpp:58] Creating layer conv1
I1124 05:32:57.568841 15792 net.cpp:84] Creating Layer conv1
I1124 05:32:57.568841 15792 net.cpp:406] conv1 <- data
I1124 05:32:57.568841 15792 net.cpp:380] conv1 -> conv1
I1124 05:32:57.570837 23768 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1124 05:32:57.813438 15792 net.cpp:122] Setting up conv1
I1124 05:32:57.813438 15792 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 05:32:57.813438 15792 net.cpp:137] Memory required for data: 28263600
I1124 05:32:57.813438 15792 layer_factory.cpp:58] Creating layer bn1
I1124 05:32:57.813438 15792 net.cpp:84] Creating Layer bn1
I1124 05:32:57.813438 15792 net.cpp:406] bn1 <- conv1
I1124 05:32:57.813438 15792 net.cpp:367] bn1 -> conv1 (in-place)
I1124 05:32:57.814438 15792 net.cpp:122] Setting up bn1
I1124 05:32:57.814438 15792 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 05:32:57.814438 15792 net.cpp:137] Memory required for data: 55297200
I1124 05:32:57.814438 15792 layer_factory.cpp:58] Creating layer scale1
I1124 05:32:57.814438 15792 net.cpp:84] Creating Layer scale1
I1124 05:32:57.814438 15792 net.cpp:406] scale1 <- conv1
I1124 05:32:57.814438 15792 net.cpp:367] scale1 -> conv1 (in-place)
I1124 05:32:57.814438 15792 layer_factory.cpp:58] Creating layer scale1
I1124 05:32:57.814438 15792 net.cpp:122] Setting up scale1
I1124 05:32:57.814438 15792 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 05:32:57.814438 15792 net.cpp:137] Memory required for data: 82330800
I1124 05:32:57.814438 15792 layer_factory.cpp:58] Creating layer relu1
I1124 05:32:57.814438 15792 net.cpp:84] Creating Layer relu1
I1124 05:32:57.814438 15792 net.cpp:406] relu1 <- conv1
I1124 05:32:57.814438 15792 net.cpp:367] relu1 -> conv1 (in-place)
I1124 05:32:57.814438 15792 net.cpp:122] Setting up relu1
I1124 05:32:57.814438 15792 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 05:32:57.814438 15792 net.cpp:137] Memory required for data: 109364400
I1124 05:32:57.814438 15792 layer_factory.cpp:58] Creating layer conv2
I1124 05:32:57.814438 15792 net.cpp:84] Creating Layer conv2
I1124 05:32:57.814438 15792 net.cpp:406] conv2 <- conv1
I1124 05:32:57.814438 15792 net.cpp:380] conv2 -> conv2
I1124 05:32:57.816943 15792 net.cpp:122] Setting up conv2
I1124 05:32:57.816943 15792 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 05:32:57.816943 15792 net.cpp:137] Memory required for data: 148686000
I1124 05:32:57.816943 15792 layer_factory.cpp:58] Creating layer bn2
I1124 05:32:57.816943 15792 net.cpp:84] Creating Layer bn2
I1124 05:32:57.816943 15792 net.cpp:406] bn2 <- conv2
I1124 05:32:57.816943 15792 net.cpp:367] bn2 -> conv2 (in-place)
I1124 05:32:57.816943 15792 net.cpp:122] Setting up bn2
I1124 05:32:57.816943 15792 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 05:32:57.816943 15792 net.cpp:137] Memory required for data: 188007600
I1124 05:32:57.816943 15792 layer_factory.cpp:58] Creating layer scale2
I1124 05:32:57.816943 15792 net.cpp:84] Creating Layer scale2
I1124 05:32:57.816943 15792 net.cpp:406] scale2 <- conv2
I1124 05:32:57.816943 15792 net.cpp:367] scale2 -> conv2 (in-place)
I1124 05:32:57.817443 15792 layer_factory.cpp:58] Creating layer scale2
I1124 05:32:57.817443 15792 net.cpp:122] Setting up scale2
I1124 05:32:57.817443 15792 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 05:32:57.817443 15792 net.cpp:137] Memory required for data: 227329200
I1124 05:32:57.817443 15792 layer_factory.cpp:58] Creating layer relu2
I1124 05:32:57.817443 15792 net.cpp:84] Creating Layer relu2
I1124 05:32:57.817443 15792 net.cpp:406] relu2 <- conv2
I1124 05:32:57.817443 15792 net.cpp:367] relu2 -> conv2 (in-place)
I1124 05:32:57.817443 15792 net.cpp:122] Setting up relu2
I1124 05:32:57.817443 15792 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 05:32:57.817443 15792 net.cpp:137] Memory required for data: 266650800
I1124 05:32:57.817443 15792 layer_factory.cpp:58] Creating layer conv2_2
I1124 05:32:57.817443 15792 net.cpp:84] Creating Layer conv2_2
I1124 05:32:57.817443 15792 net.cpp:406] conv2_2 <- conv2
I1124 05:32:57.817443 15792 net.cpp:380] conv2_2 -> conv2_2
I1124 05:32:57.819458 15792 net.cpp:122] Setting up conv2_2
I1124 05:32:57.819458 15792 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 05:32:57.819458 15792 net.cpp:137] Memory required for data: 319079600
I1124 05:32:57.819458 15792 layer_factory.cpp:58] Creating layer bn2_2
I1124 05:32:57.819458 15792 net.cpp:84] Creating Layer bn2_2
I1124 05:32:57.819458 15792 net.cpp:406] bn2_2 <- conv2_2
I1124 05:32:57.819458 15792 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1124 05:32:57.819957 15792 net.cpp:122] Setting up bn2_2
I1124 05:32:57.819957 15792 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 05:32:57.819957 15792 net.cpp:137] Memory required for data: 371508400
I1124 05:32:57.819957 15792 layer_factory.cpp:58] Creating layer scale2_2
I1124 05:32:57.819957 15792 net.cpp:84] Creating Layer scale2_2
I1124 05:32:57.819957 15792 net.cpp:406] scale2_2 <- conv2_2
I1124 05:32:57.819957 15792 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1124 05:32:57.819957 15792 layer_factory.cpp:58] Creating layer scale2_2
I1124 05:32:57.819957 15792 net.cpp:122] Setting up scale2_2
I1124 05:32:57.819957 15792 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 05:32:57.819957 15792 net.cpp:137] Memory required for data: 423937200
I1124 05:32:57.819957 15792 layer_factory.cpp:58] Creating layer relu2_2
I1124 05:32:57.819957 15792 net.cpp:84] Creating Layer relu2_2
I1124 05:32:57.819957 15792 net.cpp:406] relu2_2 <- conv2_2
I1124 05:32:57.819957 15792 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1124 05:32:57.819957 15792 net.cpp:122] Setting up relu2_2
I1124 05:32:57.819957 15792 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 05:32:57.819957 15792 net.cpp:137] Memory required for data: 476366000
I1124 05:32:57.819957 15792 layer_factory.cpp:58] Creating layer pool2_1
I1124 05:32:57.819957 15792 net.cpp:84] Creating Layer pool2_1
I1124 05:32:57.819957 15792 net.cpp:406] pool2_1 <- conv2_2
I1124 05:32:57.819957 15792 net.cpp:380] pool2_1 -> pool2_1
I1124 05:32:57.820457 15792 net.cpp:122] Setting up pool2_1
I1124 05:32:57.820457 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.820457 15792 net.cpp:137] Memory required for data: 489473200
I1124 05:32:57.820457 15792 layer_factory.cpp:58] Creating layer conv3
I1124 05:32:57.820457 15792 net.cpp:84] Creating Layer conv3
I1124 05:32:57.820457 15792 net.cpp:406] conv3 <- pool2_1
I1124 05:32:57.820457 15792 net.cpp:380] conv3 -> conv3
I1124 05:32:57.822458 15792 net.cpp:122] Setting up conv3
I1124 05:32:57.822458 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.822458 15792 net.cpp:137] Memory required for data: 502580400
I1124 05:32:57.822458 15792 layer_factory.cpp:58] Creating layer bn3
I1124 05:32:57.822458 15792 net.cpp:84] Creating Layer bn3
I1124 05:32:57.822458 15792 net.cpp:406] bn3 <- conv3
I1124 05:32:57.822458 15792 net.cpp:367] bn3 -> conv3 (in-place)
I1124 05:32:57.822957 15792 net.cpp:122] Setting up bn3
I1124 05:32:57.822957 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.822957 15792 net.cpp:137] Memory required for data: 515687600
I1124 05:32:57.822957 15792 layer_factory.cpp:58] Creating layer scale3
I1124 05:32:57.822957 15792 net.cpp:84] Creating Layer scale3
I1124 05:32:57.822957 15792 net.cpp:406] scale3 <- conv3
I1124 05:32:57.822957 15792 net.cpp:367] scale3 -> conv3 (in-place)
I1124 05:32:57.822957 15792 layer_factory.cpp:58] Creating layer scale3
I1124 05:32:57.822957 15792 net.cpp:122] Setting up scale3
I1124 05:32:57.822957 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.822957 15792 net.cpp:137] Memory required for data: 528794800
I1124 05:32:57.822957 15792 layer_factory.cpp:58] Creating layer relu3
I1124 05:32:57.822957 15792 net.cpp:84] Creating Layer relu3
I1124 05:32:57.822957 15792 net.cpp:406] relu3 <- conv3
I1124 05:32:57.822957 15792 net.cpp:367] relu3 -> conv3 (in-place)
I1124 05:32:57.822957 15792 net.cpp:122] Setting up relu3
I1124 05:32:57.822957 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.822957 15792 net.cpp:137] Memory required for data: 541902000
I1124 05:32:57.822957 15792 layer_factory.cpp:58] Creating layer conv4
I1124 05:32:57.822957 15792 net.cpp:84] Creating Layer conv4
I1124 05:32:57.822957 15792 net.cpp:406] conv4 <- conv3
I1124 05:32:57.822957 15792 net.cpp:380] conv4 -> conv4
I1124 05:32:57.825461 15792 net.cpp:122] Setting up conv4
I1124 05:32:57.825461 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.825461 15792 net.cpp:137] Memory required for data: 555009200
I1124 05:32:57.825461 15792 layer_factory.cpp:58] Creating layer bn4
I1124 05:32:57.825461 15792 net.cpp:84] Creating Layer bn4
I1124 05:32:57.825461 15792 net.cpp:406] bn4 <- conv4
I1124 05:32:57.825461 15792 net.cpp:367] bn4 -> conv4 (in-place)
I1124 05:32:57.825461 15792 net.cpp:122] Setting up bn4
I1124 05:32:57.825461 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.825461 15792 net.cpp:137] Memory required for data: 568116400
I1124 05:32:57.825461 15792 layer_factory.cpp:58] Creating layer scale4
I1124 05:32:57.825461 15792 net.cpp:84] Creating Layer scale4
I1124 05:32:57.825461 15792 net.cpp:406] scale4 <- conv4
I1124 05:32:57.825461 15792 net.cpp:367] scale4 -> conv4 (in-place)
I1124 05:32:57.825461 15792 layer_factory.cpp:58] Creating layer scale4
I1124 05:32:57.825461 15792 net.cpp:122] Setting up scale4
I1124 05:32:57.825461 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.825461 15792 net.cpp:137] Memory required for data: 581223600
I1124 05:32:57.825461 15792 layer_factory.cpp:58] Creating layer relu4
I1124 05:32:57.825461 15792 net.cpp:84] Creating Layer relu4
I1124 05:32:57.825461 15792 net.cpp:406] relu4 <- conv4
I1124 05:32:57.825461 15792 net.cpp:367] relu4 -> conv4 (in-place)
I1124 05:32:57.825958 15792 net.cpp:122] Setting up relu4
I1124 05:32:57.825958 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.825958 15792 net.cpp:137] Memory required for data: 594330800
I1124 05:32:57.825958 15792 layer_factory.cpp:58] Creating layer conv4_1
I1124 05:32:57.825958 15792 net.cpp:84] Creating Layer conv4_1
I1124 05:32:57.825958 15792 net.cpp:406] conv4_1 <- conv4
I1124 05:32:57.825958 15792 net.cpp:380] conv4_1 -> conv4_1
I1124 05:32:57.827958 15792 net.cpp:122] Setting up conv4_1
I1124 05:32:57.827958 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.827958 15792 net.cpp:137] Memory required for data: 607438000
I1124 05:32:57.827958 15792 layer_factory.cpp:58] Creating layer bn4_1
I1124 05:32:57.827958 15792 net.cpp:84] Creating Layer bn4_1
I1124 05:32:57.827958 15792 net.cpp:406] bn4_1 <- conv4_1
I1124 05:32:57.827958 15792 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1124 05:32:57.828459 15792 net.cpp:122] Setting up bn4_1
I1124 05:32:57.828459 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.828459 15792 net.cpp:137] Memory required for data: 620545200
I1124 05:32:57.828459 15792 layer_factory.cpp:58] Creating layer scale4_1
I1124 05:32:57.828459 15792 net.cpp:84] Creating Layer scale4_1
I1124 05:32:57.828459 15792 net.cpp:406] scale4_1 <- conv4_1
I1124 05:32:57.828459 15792 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1124 05:32:57.828459 15792 layer_factory.cpp:58] Creating layer scale4_1
I1124 05:32:57.828459 15792 net.cpp:122] Setting up scale4_1
I1124 05:32:57.828459 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.828459 15792 net.cpp:137] Memory required for data: 633652400
I1124 05:32:57.828459 15792 layer_factory.cpp:58] Creating layer relu4_1
I1124 05:32:57.828459 15792 net.cpp:84] Creating Layer relu4_1
I1124 05:32:57.828459 15792 net.cpp:406] relu4_1 <- conv4_1
I1124 05:32:57.828459 15792 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1124 05:32:57.828958 15792 net.cpp:122] Setting up relu4_1
I1124 05:32:57.828958 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.828958 15792 net.cpp:137] Memory required for data: 646759600
I1124 05:32:57.828958 15792 layer_factory.cpp:58] Creating layer conv4_2
I1124 05:32:57.828958 15792 net.cpp:84] Creating Layer conv4_2
I1124 05:32:57.828958 15792 net.cpp:406] conv4_2 <- conv4_1
I1124 05:32:57.828958 15792 net.cpp:380] conv4_2 -> conv4_2
I1124 05:32:57.831974 15792 net.cpp:122] Setting up conv4_2
I1124 05:32:57.831974 15792 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 05:32:57.831974 15792 net.cpp:137] Memory required for data: 668775600
I1124 05:32:57.831974 15792 layer_factory.cpp:58] Creating layer bn4_2
I1124 05:32:57.831974 15792 net.cpp:84] Creating Layer bn4_2
I1124 05:32:57.831974 15792 net.cpp:406] bn4_2 <- conv4_2
I1124 05:32:57.831974 15792 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1124 05:32:57.831974 15792 net.cpp:122] Setting up bn4_2
I1124 05:32:57.831974 15792 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 05:32:57.831974 15792 net.cpp:137] Memory required for data: 690791600
I1124 05:32:57.831974 15792 layer_factory.cpp:58] Creating layer scale4_2
I1124 05:32:57.831974 15792 net.cpp:84] Creating Layer scale4_2
I1124 05:32:57.831974 15792 net.cpp:406] scale4_2 <- conv4_2
I1124 05:32:57.831974 15792 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1124 05:32:57.831974 15792 layer_factory.cpp:58] Creating layer scale4_2
I1124 05:32:57.831974 15792 net.cpp:122] Setting up scale4_2
I1124 05:32:57.831974 15792 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 05:32:57.831974 15792 net.cpp:137] Memory required for data: 712807600
I1124 05:32:57.832974 15792 layer_factory.cpp:58] Creating layer relu4_2
I1124 05:32:57.832974 15792 net.cpp:84] Creating Layer relu4_2
I1124 05:32:57.832974 15792 net.cpp:406] relu4_2 <- conv4_2
I1124 05:32:57.832974 15792 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1124 05:32:57.832974 15792 net.cpp:122] Setting up relu4_2
I1124 05:32:57.832974 15792 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 05:32:57.832974 15792 net.cpp:137] Memory required for data: 734823600
I1124 05:32:57.832974 15792 layer_factory.cpp:58] Creating layer pool4_2
I1124 05:32:57.832974 15792 net.cpp:84] Creating Layer pool4_2
I1124 05:32:57.832974 15792 net.cpp:406] pool4_2 <- conv4_2
I1124 05:32:57.832974 15792 net.cpp:380] pool4_2 -> pool4_2
I1124 05:32:57.832974 15792 net.cpp:122] Setting up pool4_2
I1124 05:32:57.832974 15792 net.cpp:129] Top shape: 100 215 8 8 (1376000)
I1124 05:32:57.832974 15792 net.cpp:137] Memory required for data: 740327600
I1124 05:32:57.832974 15792 layer_factory.cpp:58] Creating layer conv12
I1124 05:32:57.832974 15792 net.cpp:84] Creating Layer conv12
I1124 05:32:57.832974 15792 net.cpp:406] conv12 <- pool4_2
I1124 05:32:57.832974 15792 net.cpp:380] conv12 -> conv12
I1124 05:32:57.839974 15792 net.cpp:122] Setting up conv12
I1124 05:32:57.839974 15792 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 05:32:57.839974 15792 net.cpp:137] Memory required for data: 750158000
I1124 05:32:57.839974 15792 layer_factory.cpp:58] Creating layer bn_conv12
I1124 05:32:57.839974 15792 net.cpp:84] Creating Layer bn_conv12
I1124 05:32:57.839974 15792 net.cpp:406] bn_conv12 <- conv12
I1124 05:32:57.839974 15792 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1124 05:32:57.839974 15792 net.cpp:122] Setting up bn_conv12
I1124 05:32:57.839974 15792 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 05:32:57.839974 15792 net.cpp:137] Memory required for data: 759988400
I1124 05:32:57.839974 15792 layer_factory.cpp:58] Creating layer scale_conv12
I1124 05:32:57.839974 15792 net.cpp:84] Creating Layer scale_conv12
I1124 05:32:57.839974 15792 net.cpp:406] scale_conv12 <- conv12
I1124 05:32:57.839974 15792 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1124 05:32:57.839974 15792 layer_factory.cpp:58] Creating layer scale_conv12
I1124 05:32:57.839974 15792 net.cpp:122] Setting up scale_conv12
I1124 05:32:57.839974 15792 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 05:32:57.839974 15792 net.cpp:137] Memory required for data: 769818800
I1124 05:32:57.839974 15792 layer_factory.cpp:58] Creating layer relu_conv12
I1124 05:32:57.839974 15792 net.cpp:84] Creating Layer relu_conv12
I1124 05:32:57.839974 15792 net.cpp:406] relu_conv12 <- conv12
I1124 05:32:57.839974 15792 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1124 05:32:57.840981 15792 net.cpp:122] Setting up relu_conv12
I1124 05:32:57.840981 15792 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 05:32:57.840981 15792 net.cpp:137] Memory required for data: 779649200
I1124 05:32:57.840981 15792 layer_factory.cpp:58] Creating layer poolcp6
I1124 05:32:57.840981 15792 net.cpp:84] Creating Layer poolcp6
I1124 05:32:57.840981 15792 net.cpp:406] poolcp6 <- conv12
I1124 05:32:57.840981 15792 net.cpp:380] poolcp6 -> poolcp6
I1124 05:32:57.840981 15792 net.cpp:122] Setting up poolcp6
I1124 05:32:57.840981 15792 net.cpp:129] Top shape: 100 384 1 1 (38400)
I1124 05:32:57.840981 15792 net.cpp:137] Memory required for data: 779802800
I1124 05:32:57.840981 15792 layer_factory.cpp:58] Creating layer ip1
I1124 05:32:57.840981 15792 net.cpp:84] Creating Layer ip1
I1124 05:32:57.840981 15792 net.cpp:406] ip1 <- poolcp6
I1124 05:32:57.840981 15792 net.cpp:380] ip1 -> ip1
I1124 05:32:57.840981 15792 net.cpp:122] Setting up ip1
I1124 05:32:57.840981 15792 net.cpp:129] Top shape: 100 10 (1000)
I1124 05:32:57.840981 15792 net.cpp:137] Memory required for data: 779806800
I1124 05:32:57.840981 15792 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1124 05:32:57.840981 15792 net.cpp:84] Creating Layer ip1_ip1_0_split
I1124 05:32:57.840981 15792 net.cpp:406] ip1_ip1_0_split <- ip1
I1124 05:32:57.840981 15792 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1124 05:32:57.840981 15792 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1124 05:32:57.840981 15792 net.cpp:122] Setting up ip1_ip1_0_split
I1124 05:32:57.840981 15792 net.cpp:129] Top shape: 100 10 (1000)
I1124 05:32:57.840981 15792 net.cpp:129] Top shape: 100 10 (1000)
I1124 05:32:57.840981 15792 net.cpp:137] Memory required for data: 779814800
I1124 05:32:57.840981 15792 layer_factory.cpp:58] Creating layer accuracy_training
I1124 05:32:57.840981 15792 net.cpp:84] Creating Layer accuracy_training
I1124 05:32:57.840981 15792 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1124 05:32:57.840981 15792 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1124 05:32:57.840981 15792 net.cpp:380] accuracy_training -> accuracy_training
I1124 05:32:57.840981 15792 net.cpp:122] Setting up accuracy_training
I1124 05:32:57.840981 15792 net.cpp:129] Top shape: (1)
I1124 05:32:57.840981 15792 net.cpp:137] Memory required for data: 779814804
I1124 05:32:57.840981 15792 layer_factory.cpp:58] Creating layer loss
I1124 05:32:57.840981 15792 net.cpp:84] Creating Layer loss
I1124 05:32:57.840981 15792 net.cpp:406] loss <- ip1_ip1_0_split_1
I1124 05:32:57.840981 15792 net.cpp:406] loss <- label_cifar_1_split_1
I1124 05:32:57.840981 15792 net.cpp:380] loss -> loss
I1124 05:32:57.840981 15792 layer_factory.cpp:58] Creating layer loss
I1124 05:32:57.840981 15792 net.cpp:122] Setting up loss
I1124 05:32:57.840981 15792 net.cpp:129] Top shape: (1)
I1124 05:32:57.840981 15792 net.cpp:132]     with loss weight 1
I1124 05:32:57.840981 15792 net.cpp:137] Memory required for data: 779814808
I1124 05:32:57.840981 15792 net.cpp:198] loss needs backward computation.
I1124 05:32:57.840981 15792 net.cpp:200] accuracy_training does not need backward computation.
I1124 05:32:57.840981 15792 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1124 05:32:57.840981 15792 net.cpp:198] ip1 needs backward computation.
I1124 05:32:57.840981 15792 net.cpp:198] poolcp6 needs backward computation.
I1124 05:32:57.840981 15792 net.cpp:198] relu_conv12 needs backward computation.
I1124 05:32:57.840981 15792 net.cpp:198] scale_conv12 needs backward computation.
I1124 05:32:57.840981 15792 net.cpp:198] bn_conv12 needs backward computation.
I1124 05:32:57.840981 15792 net.cpp:198] conv12 needs backward computation.
I1124 05:32:57.840981 15792 net.cpp:198] pool4_2 needs backward computation.
I1124 05:32:57.840981 15792 net.cpp:198] relu4_2 needs backward computation.
I1124 05:32:57.840981 15792 net.cpp:198] scale4_2 needs backward computation.
I1124 05:32:57.840981 15792 net.cpp:198] bn4_2 needs backward computation.
I1124 05:32:57.840981 15792 net.cpp:198] conv4_2 needs backward computation.
I1124 05:32:57.840981 15792 net.cpp:198] relu4_1 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] scale4_1 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] bn4_1 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] conv4_1 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] relu4 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] scale4 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] bn4 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] conv4 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] relu3 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] scale3 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] bn3 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] conv3 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] pool2_1 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] relu2_2 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] scale2_2 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] bn2_2 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] conv2_2 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] relu2 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] scale2 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] bn2 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] conv2 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] relu1 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] scale1 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] bn1 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:198] conv1 needs backward computation.
I1124 05:32:57.841974 15792 net.cpp:200] label_cifar_1_split does not need backward computation.
I1124 05:32:57.841974 15792 net.cpp:200] cifar does not need backward computation.
I1124 05:32:57.841974 15792 net.cpp:242] This network produces output accuracy_training
I1124 05:32:57.841974 15792 net.cpp:242] This network produces output loss
I1124 05:32:57.841974 15792 net.cpp:255] Network initialization done.
I1124 05:32:57.841974 15792 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1124 05:32:57.841974 15792 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1124 05:32:57.841974 15792 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1124 05:32:57.841974 15792 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1124 05:32:57.841974 15792 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1124 05:32:57.841974 15792 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1124 05:32:57.841974 15792 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1124 05:32:57.841974 15792 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1124 05:32:57.841974 15792 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1124 05:32:57.841974 15792 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1124 05:32:57.841974 15792 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1124 05:32:57.841974 15792 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1124 05:32:57.841974 15792 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1124 05:32:57.842973 15792 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_1.6M"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 66
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 215
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1124 05:32:57.842973 15792 layer_factory.cpp:58] Creating layer cifar
I1124 05:32:57.894987 15792 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1124 05:32:57.894987 15792 net.cpp:84] Creating Layer cifar
I1124 05:32:57.894987 15792 net.cpp:380] cifar -> data
I1124 05:32:57.894987 15792 net.cpp:380] cifar -> label
I1124 05:32:57.894987 15792 data_layer.cpp:45] output data size: 100,3,32,32
I1124 05:32:57.900960 15792 net.cpp:122] Setting up cifar
I1124 05:32:57.900960 15792 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1124 05:32:57.900960 15792 net.cpp:129] Top shape: 100 (100)
I1124 05:32:57.900960 15792 net.cpp:137] Memory required for data: 1229200
I1124 05:32:57.900960 15792 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1124 05:32:57.900960 15792 net.cpp:84] Creating Layer label_cifar_1_split
I1124 05:32:57.900960 15792 net.cpp:406] label_cifar_1_split <- label
I1124 05:32:57.900960 15792 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1124 05:32:57.900960 15792 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1124 05:32:57.900960 15792 net.cpp:122] Setting up label_cifar_1_split
I1124 05:32:57.900960 15792 net.cpp:129] Top shape: 100 (100)
I1124 05:32:57.900960 15792 net.cpp:129] Top shape: 100 (100)
I1124 05:32:57.900960 15792 net.cpp:137] Memory required for data: 1230000
I1124 05:32:57.900960 15792 layer_factory.cpp:58] Creating layer conv1
I1124 05:32:57.900960 15792 net.cpp:84] Creating Layer conv1
I1124 05:32:57.901962 15792 net.cpp:406] conv1 <- data
I1124 05:32:57.901962 15792 net.cpp:380] conv1 -> conv1
I1124 05:32:57.902961 35900 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1124 05:32:57.902961 15792 net.cpp:122] Setting up conv1
I1124 05:32:57.902961 15792 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 05:32:57.902961 15792 net.cpp:137] Memory required for data: 28263600
I1124 05:32:57.902961 15792 layer_factory.cpp:58] Creating layer bn1
I1124 05:32:57.902961 15792 net.cpp:84] Creating Layer bn1
I1124 05:32:57.902961 15792 net.cpp:406] bn1 <- conv1
I1124 05:32:57.902961 15792 net.cpp:367] bn1 -> conv1 (in-place)
I1124 05:32:57.902961 15792 net.cpp:122] Setting up bn1
I1124 05:32:57.902961 15792 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 05:32:57.902961 15792 net.cpp:137] Memory required for data: 55297200
I1124 05:32:57.903957 15792 layer_factory.cpp:58] Creating layer scale1
I1124 05:32:57.903957 15792 net.cpp:84] Creating Layer scale1
I1124 05:32:57.903957 15792 net.cpp:406] scale1 <- conv1
I1124 05:32:57.903957 15792 net.cpp:367] scale1 -> conv1 (in-place)
I1124 05:32:57.903957 15792 layer_factory.cpp:58] Creating layer scale1
I1124 05:32:57.903957 15792 net.cpp:122] Setting up scale1
I1124 05:32:57.903957 15792 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 05:32:57.903957 15792 net.cpp:137] Memory required for data: 82330800
I1124 05:32:57.903957 15792 layer_factory.cpp:58] Creating layer relu1
I1124 05:32:57.903957 15792 net.cpp:84] Creating Layer relu1
I1124 05:32:57.903957 15792 net.cpp:406] relu1 <- conv1
I1124 05:32:57.903957 15792 net.cpp:367] relu1 -> conv1 (in-place)
I1124 05:32:57.903957 15792 net.cpp:122] Setting up relu1
I1124 05:32:57.903957 15792 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 05:32:57.903957 15792 net.cpp:137] Memory required for data: 109364400
I1124 05:32:57.903957 15792 layer_factory.cpp:58] Creating layer conv2
I1124 05:32:57.903957 15792 net.cpp:84] Creating Layer conv2
I1124 05:32:57.903957 15792 net.cpp:406] conv2 <- conv1
I1124 05:32:57.903957 15792 net.cpp:380] conv2 -> conv2
I1124 05:32:57.905957 15792 net.cpp:122] Setting up conv2
I1124 05:32:57.905957 15792 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 05:32:57.905957 15792 net.cpp:137] Memory required for data: 148686000
I1124 05:32:57.905957 15792 layer_factory.cpp:58] Creating layer bn2
I1124 05:32:57.905957 15792 net.cpp:84] Creating Layer bn2
I1124 05:32:57.905957 15792 net.cpp:406] bn2 <- conv2
I1124 05:32:57.905957 15792 net.cpp:367] bn2 -> conv2 (in-place)
I1124 05:32:57.905957 15792 net.cpp:122] Setting up bn2
I1124 05:32:57.905957 15792 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 05:32:57.905957 15792 net.cpp:137] Memory required for data: 188007600
I1124 05:32:57.905957 15792 layer_factory.cpp:58] Creating layer scale2
I1124 05:32:57.905957 15792 net.cpp:84] Creating Layer scale2
I1124 05:32:57.905957 15792 net.cpp:406] scale2 <- conv2
I1124 05:32:57.905957 15792 net.cpp:367] scale2 -> conv2 (in-place)
I1124 05:32:57.905957 15792 layer_factory.cpp:58] Creating layer scale2
I1124 05:32:57.905957 15792 net.cpp:122] Setting up scale2
I1124 05:32:57.906957 15792 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 05:32:57.906957 15792 net.cpp:137] Memory required for data: 227329200
I1124 05:32:57.906957 15792 layer_factory.cpp:58] Creating layer relu2
I1124 05:32:57.906957 15792 net.cpp:84] Creating Layer relu2
I1124 05:32:57.906957 15792 net.cpp:406] relu2 <- conv2
I1124 05:32:57.906957 15792 net.cpp:367] relu2 -> conv2 (in-place)
I1124 05:32:57.906957 15792 net.cpp:122] Setting up relu2
I1124 05:32:57.906957 15792 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 05:32:57.906957 15792 net.cpp:137] Memory required for data: 266650800
I1124 05:32:57.906957 15792 layer_factory.cpp:58] Creating layer conv2_2
I1124 05:32:57.906957 15792 net.cpp:84] Creating Layer conv2_2
I1124 05:32:57.906957 15792 net.cpp:406] conv2_2 <- conv2
I1124 05:32:57.906957 15792 net.cpp:380] conv2_2 -> conv2_2
I1124 05:32:57.908957 15792 net.cpp:122] Setting up conv2_2
I1124 05:32:57.908957 15792 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 05:32:57.908957 15792 net.cpp:137] Memory required for data: 319079600
I1124 05:32:57.908957 15792 layer_factory.cpp:58] Creating layer bn2_2
I1124 05:32:57.908957 15792 net.cpp:84] Creating Layer bn2_2
I1124 05:32:57.908957 15792 net.cpp:406] bn2_2 <- conv2_2
I1124 05:32:57.908957 15792 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1124 05:32:57.908957 15792 net.cpp:122] Setting up bn2_2
I1124 05:32:57.908957 15792 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 05:32:57.908957 15792 net.cpp:137] Memory required for data: 371508400
I1124 05:32:57.908957 15792 layer_factory.cpp:58] Creating layer scale2_2
I1124 05:32:57.908957 15792 net.cpp:84] Creating Layer scale2_2
I1124 05:32:57.908957 15792 net.cpp:406] scale2_2 <- conv2_2
I1124 05:32:57.909958 15792 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1124 05:32:57.909958 15792 layer_factory.cpp:58] Creating layer scale2_2
I1124 05:32:57.909958 15792 net.cpp:122] Setting up scale2_2
I1124 05:32:57.909958 15792 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 05:32:57.909958 15792 net.cpp:137] Memory required for data: 423937200
I1124 05:32:57.909958 15792 layer_factory.cpp:58] Creating layer relu2_2
I1124 05:32:57.909958 15792 net.cpp:84] Creating Layer relu2_2
I1124 05:32:57.909958 15792 net.cpp:406] relu2_2 <- conv2_2
I1124 05:32:57.909958 15792 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1124 05:32:57.909958 15792 net.cpp:122] Setting up relu2_2
I1124 05:32:57.909958 15792 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 05:32:57.909958 15792 net.cpp:137] Memory required for data: 476366000
I1124 05:32:57.909958 15792 layer_factory.cpp:58] Creating layer pool2_1
I1124 05:32:57.909958 15792 net.cpp:84] Creating Layer pool2_1
I1124 05:32:57.909958 15792 net.cpp:406] pool2_1 <- conv2_2
I1124 05:32:57.909958 15792 net.cpp:380] pool2_1 -> pool2_1
I1124 05:32:57.909958 15792 net.cpp:122] Setting up pool2_1
I1124 05:32:57.909958 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.909958 15792 net.cpp:137] Memory required for data: 489473200
I1124 05:32:57.909958 15792 layer_factory.cpp:58] Creating layer conv3
I1124 05:32:57.909958 15792 net.cpp:84] Creating Layer conv3
I1124 05:32:57.909958 15792 net.cpp:406] conv3 <- pool2_1
I1124 05:32:57.909958 15792 net.cpp:380] conv3 -> conv3
I1124 05:32:57.911958 15792 net.cpp:122] Setting up conv3
I1124 05:32:57.911958 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.911958 15792 net.cpp:137] Memory required for data: 502580400
I1124 05:32:57.911958 15792 layer_factory.cpp:58] Creating layer bn3
I1124 05:32:57.911958 15792 net.cpp:84] Creating Layer bn3
I1124 05:32:57.911958 15792 net.cpp:406] bn3 <- conv3
I1124 05:32:57.911958 15792 net.cpp:367] bn3 -> conv3 (in-place)
I1124 05:32:57.912958 15792 net.cpp:122] Setting up bn3
I1124 05:32:57.912958 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.912958 15792 net.cpp:137] Memory required for data: 515687600
I1124 05:32:57.912958 15792 layer_factory.cpp:58] Creating layer scale3
I1124 05:32:57.912958 15792 net.cpp:84] Creating Layer scale3
I1124 05:32:57.912958 15792 net.cpp:406] scale3 <- conv3
I1124 05:32:57.912958 15792 net.cpp:367] scale3 -> conv3 (in-place)
I1124 05:32:57.912958 15792 layer_factory.cpp:58] Creating layer scale3
I1124 05:32:57.912958 15792 net.cpp:122] Setting up scale3
I1124 05:32:57.912958 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.912958 15792 net.cpp:137] Memory required for data: 528794800
I1124 05:32:57.912958 15792 layer_factory.cpp:58] Creating layer relu3
I1124 05:32:57.912958 15792 net.cpp:84] Creating Layer relu3
I1124 05:32:57.912958 15792 net.cpp:406] relu3 <- conv3
I1124 05:32:57.912958 15792 net.cpp:367] relu3 -> conv3 (in-place)
I1124 05:32:57.912958 15792 net.cpp:122] Setting up relu3
I1124 05:32:57.912958 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.912958 15792 net.cpp:137] Memory required for data: 541902000
I1124 05:32:57.912958 15792 layer_factory.cpp:58] Creating layer conv4
I1124 05:32:57.912958 15792 net.cpp:84] Creating Layer conv4
I1124 05:32:57.912958 15792 net.cpp:406] conv4 <- conv3
I1124 05:32:57.912958 15792 net.cpp:380] conv4 -> conv4
I1124 05:32:57.915503 15792 net.cpp:122] Setting up conv4
I1124 05:32:57.915503 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.915503 15792 net.cpp:137] Memory required for data: 555009200
I1124 05:32:57.915503 15792 layer_factory.cpp:58] Creating layer bn4
I1124 05:32:57.915503 15792 net.cpp:84] Creating Layer bn4
I1124 05:32:57.915503 15792 net.cpp:406] bn4 <- conv4
I1124 05:32:57.915503 15792 net.cpp:367] bn4 -> conv4 (in-place)
I1124 05:32:57.915503 15792 net.cpp:122] Setting up bn4
I1124 05:32:57.915503 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.915503 15792 net.cpp:137] Memory required for data: 568116400
I1124 05:32:57.915503 15792 layer_factory.cpp:58] Creating layer scale4
I1124 05:32:57.915503 15792 net.cpp:84] Creating Layer scale4
I1124 05:32:57.915503 15792 net.cpp:406] scale4 <- conv4
I1124 05:32:57.915503 15792 net.cpp:367] scale4 -> conv4 (in-place)
I1124 05:32:57.915503 15792 layer_factory.cpp:58] Creating layer scale4
I1124 05:32:57.916002 15792 net.cpp:122] Setting up scale4
I1124 05:32:57.916002 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.916002 15792 net.cpp:137] Memory required for data: 581223600
I1124 05:32:57.916002 15792 layer_factory.cpp:58] Creating layer relu4
I1124 05:32:57.916002 15792 net.cpp:84] Creating Layer relu4
I1124 05:32:57.916002 15792 net.cpp:406] relu4 <- conv4
I1124 05:32:57.916002 15792 net.cpp:367] relu4 -> conv4 (in-place)
I1124 05:32:57.916002 15792 net.cpp:122] Setting up relu4
I1124 05:32:57.916002 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.916002 15792 net.cpp:137] Memory required for data: 594330800
I1124 05:32:57.916002 15792 layer_factory.cpp:58] Creating layer conv4_1
I1124 05:32:57.916002 15792 net.cpp:84] Creating Layer conv4_1
I1124 05:32:57.916002 15792 net.cpp:406] conv4_1 <- conv4
I1124 05:32:57.916002 15792 net.cpp:380] conv4_1 -> conv4_1
I1124 05:32:57.918503 15792 net.cpp:122] Setting up conv4_1
I1124 05:32:57.918503 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.918503 15792 net.cpp:137] Memory required for data: 607438000
I1124 05:32:57.918503 15792 layer_factory.cpp:58] Creating layer bn4_1
I1124 05:32:57.918503 15792 net.cpp:84] Creating Layer bn4_1
I1124 05:32:57.918503 15792 net.cpp:406] bn4_1 <- conv4_1
I1124 05:32:57.918503 15792 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1124 05:32:57.918503 15792 net.cpp:122] Setting up bn4_1
I1124 05:32:57.918503 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.918503 15792 net.cpp:137] Memory required for data: 620545200
I1124 05:32:57.918503 15792 layer_factory.cpp:58] Creating layer scale4_1
I1124 05:32:57.918503 15792 net.cpp:84] Creating Layer scale4_1
I1124 05:32:57.918503 15792 net.cpp:406] scale4_1 <- conv4_1
I1124 05:32:57.918503 15792 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1124 05:32:57.918503 15792 layer_factory.cpp:58] Creating layer scale4_1
I1124 05:32:57.919003 15792 net.cpp:122] Setting up scale4_1
I1124 05:32:57.919003 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.919003 15792 net.cpp:137] Memory required for data: 633652400
I1124 05:32:57.919003 15792 layer_factory.cpp:58] Creating layer relu4_1
I1124 05:32:57.919003 15792 net.cpp:84] Creating Layer relu4_1
I1124 05:32:57.919003 15792 net.cpp:406] relu4_1 <- conv4_1
I1124 05:32:57.919003 15792 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1124 05:32:57.919003 15792 net.cpp:122] Setting up relu4_1
I1124 05:32:57.919003 15792 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 05:32:57.919003 15792 net.cpp:137] Memory required for data: 646759600
I1124 05:32:57.919502 15792 layer_factory.cpp:58] Creating layer conv4_2
I1124 05:32:57.919502 15792 net.cpp:84] Creating Layer conv4_2
I1124 05:32:57.919502 15792 net.cpp:406] conv4_2 <- conv4_1
I1124 05:32:57.919502 15792 net.cpp:380] conv4_2 -> conv4_2
I1124 05:32:57.922504 15792 net.cpp:122] Setting up conv4_2
I1124 05:32:57.922504 15792 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 05:32:57.922504 15792 net.cpp:137] Memory required for data: 668775600
I1124 05:32:57.922504 15792 layer_factory.cpp:58] Creating layer bn4_2
I1124 05:32:57.922504 15792 net.cpp:84] Creating Layer bn4_2
I1124 05:32:57.922504 15792 net.cpp:406] bn4_2 <- conv4_2
I1124 05:32:57.922504 15792 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1124 05:32:57.922504 15792 net.cpp:122] Setting up bn4_2
I1124 05:32:57.922504 15792 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 05:32:57.922504 15792 net.cpp:137] Memory required for data: 690791600
I1124 05:32:57.922504 15792 layer_factory.cpp:58] Creating layer scale4_2
I1124 05:32:57.922504 15792 net.cpp:84] Creating Layer scale4_2
I1124 05:32:57.922504 15792 net.cpp:406] scale4_2 <- conv4_2
I1124 05:32:57.922504 15792 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1124 05:32:57.922504 15792 layer_factory.cpp:58] Creating layer scale4_2
I1124 05:32:57.922504 15792 net.cpp:122] Setting up scale4_2
I1124 05:32:57.922504 15792 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 05:32:57.922504 15792 net.cpp:137] Memory required for data: 712807600
I1124 05:32:57.922504 15792 layer_factory.cpp:58] Creating layer relu4_2
I1124 05:32:57.922504 15792 net.cpp:84] Creating Layer relu4_2
I1124 05:32:57.922504 15792 net.cpp:406] relu4_2 <- conv4_2
I1124 05:32:57.922504 15792 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1124 05:32:57.923502 15792 net.cpp:122] Setting up relu4_2
I1124 05:32:57.923502 15792 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 05:32:57.923502 15792 net.cpp:137] Memory required for data: 734823600
I1124 05:32:57.923502 15792 layer_factory.cpp:58] Creating layer pool4_2
I1124 05:32:57.923502 15792 net.cpp:84] Creating Layer pool4_2
I1124 05:32:57.923502 15792 net.cpp:406] pool4_2 <- conv4_2
I1124 05:32:57.923502 15792 net.cpp:380] pool4_2 -> pool4_2
I1124 05:32:57.923502 15792 net.cpp:122] Setting up pool4_2
I1124 05:32:57.923502 15792 net.cpp:129] Top shape: 100 215 8 8 (1376000)
I1124 05:32:57.923502 15792 net.cpp:137] Memory required for data: 740327600
I1124 05:32:57.923502 15792 layer_factory.cpp:58] Creating layer conv12
I1124 05:32:57.923502 15792 net.cpp:84] Creating Layer conv12
I1124 05:32:57.923502 15792 net.cpp:406] conv12 <- pool4_2
I1124 05:32:57.923502 15792 net.cpp:380] conv12 -> conv12
I1124 05:32:57.929986 15792 net.cpp:122] Setting up conv12
I1124 05:32:57.929986 15792 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 05:32:57.929986 15792 net.cpp:137] Memory required for data: 750158000
I1124 05:32:57.929986 15792 layer_factory.cpp:58] Creating layer bn_conv12
I1124 05:32:57.929986 15792 net.cpp:84] Creating Layer bn_conv12
I1124 05:32:57.929986 15792 net.cpp:406] bn_conv12 <- conv12
I1124 05:32:57.929986 15792 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1124 05:32:57.930490 15792 net.cpp:122] Setting up bn_conv12
I1124 05:32:57.930490 15792 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 05:32:57.930490 15792 net.cpp:137] Memory required for data: 759988400
I1124 05:32:57.930490 15792 layer_factory.cpp:58] Creating layer scale_conv12
I1124 05:32:57.930490 15792 net.cpp:84] Creating Layer scale_conv12
I1124 05:32:57.930490 15792 net.cpp:406] scale_conv12 <- conv12
I1124 05:32:57.930490 15792 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1124 05:32:57.930490 15792 layer_factory.cpp:58] Creating layer scale_conv12
I1124 05:32:57.930490 15792 net.cpp:122] Setting up scale_conv12
I1124 05:32:57.930490 15792 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 05:32:57.930490 15792 net.cpp:137] Memory required for data: 769818800
I1124 05:32:57.930490 15792 layer_factory.cpp:58] Creating layer relu_conv12
I1124 05:32:57.930490 15792 net.cpp:84] Creating Layer relu_conv12
I1124 05:32:57.930490 15792 net.cpp:406] relu_conv12 <- conv12
I1124 05:32:57.930490 15792 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1124 05:32:57.931814 15792 net.cpp:122] Setting up relu_conv12
I1124 05:32:57.931814 15792 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 05:32:57.931814 15792 net.cpp:137] Memory required for data: 779649200
I1124 05:32:57.931814 15792 layer_factory.cpp:58] Creating layer poolcp6
I1124 05:32:57.931814 15792 net.cpp:84] Creating Layer poolcp6
I1124 05:32:57.931814 15792 net.cpp:406] poolcp6 <- conv12
I1124 05:32:57.931814 15792 net.cpp:380] poolcp6 -> poolcp6
I1124 05:32:57.931814 15792 net.cpp:122] Setting up poolcp6
I1124 05:32:57.931814 15792 net.cpp:129] Top shape: 100 384 1 1 (38400)
I1124 05:32:57.931814 15792 net.cpp:137] Memory required for data: 779802800
I1124 05:32:57.931814 15792 layer_factory.cpp:58] Creating layer ip1
I1124 05:32:57.931814 15792 net.cpp:84] Creating Layer ip1
I1124 05:32:57.931814 15792 net.cpp:406] ip1 <- poolcp6
I1124 05:32:57.931814 15792 net.cpp:380] ip1 -> ip1
I1124 05:32:57.931814 15792 net.cpp:122] Setting up ip1
I1124 05:32:57.931814 15792 net.cpp:129] Top shape: 100 10 (1000)
I1124 05:32:57.931814 15792 net.cpp:137] Memory required for data: 779806800
I1124 05:32:57.931814 15792 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1124 05:32:57.931814 15792 net.cpp:84] Creating Layer ip1_ip1_0_split
I1124 05:32:57.931814 15792 net.cpp:406] ip1_ip1_0_split <- ip1
I1124 05:32:57.931814 15792 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1124 05:32:57.931814 15792 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1124 05:32:57.931814 15792 net.cpp:122] Setting up ip1_ip1_0_split
I1124 05:32:57.931814 15792 net.cpp:129] Top shape: 100 10 (1000)
I1124 05:32:57.931814 15792 net.cpp:129] Top shape: 100 10 (1000)
I1124 05:32:57.931814 15792 net.cpp:137] Memory required for data: 779814800
I1124 05:32:57.931814 15792 layer_factory.cpp:58] Creating layer accuracy
I1124 05:32:57.931814 15792 net.cpp:84] Creating Layer accuracy
I1124 05:32:57.931814 15792 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1124 05:32:57.932806 15792 net.cpp:406] accuracy <- label_cifar_1_split_0
I1124 05:32:57.932806 15792 net.cpp:380] accuracy -> accuracy
I1124 05:32:57.932806 15792 net.cpp:122] Setting up accuracy
I1124 05:32:57.932806 15792 net.cpp:129] Top shape: (1)
I1124 05:32:57.932806 15792 net.cpp:137] Memory required for data: 779814804
I1124 05:32:57.932806 15792 layer_factory.cpp:58] Creating layer loss
I1124 05:32:57.932806 15792 net.cpp:84] Creating Layer loss
I1124 05:32:57.932806 15792 net.cpp:406] loss <- ip1_ip1_0_split_1
I1124 05:32:57.932806 15792 net.cpp:406] loss <- label_cifar_1_split_1
I1124 05:32:57.932806 15792 net.cpp:380] loss -> loss
I1124 05:32:57.932806 15792 layer_factory.cpp:58] Creating layer loss
I1124 05:32:57.932806 15792 net.cpp:122] Setting up loss
I1124 05:32:57.932806 15792 net.cpp:129] Top shape: (1)
I1124 05:32:57.932806 15792 net.cpp:132]     with loss weight 1
I1124 05:32:57.932806 15792 net.cpp:137] Memory required for data: 779814808
I1124 05:32:57.932806 15792 net.cpp:198] loss needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:200] accuracy does not need backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] ip1 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] poolcp6 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] relu_conv12 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] scale_conv12 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] bn_conv12 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] conv12 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] pool4_2 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] relu4_2 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] scale4_2 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] bn4_2 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] conv4_2 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] relu4_1 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] scale4_1 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] bn4_1 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] conv4_1 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] relu4 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] scale4 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] bn4 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] conv4 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] relu3 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] scale3 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] bn3 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] conv3 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] pool2_1 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] relu2_2 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] scale2_2 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] bn2_2 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] conv2_2 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] relu2 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] scale2 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] bn2 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] conv2 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] relu1 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] scale1 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] bn1 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:198] conv1 needs backward computation.
I1124 05:32:57.932806 15792 net.cpp:200] label_cifar_1_split does not need backward computation.
I1124 05:32:57.932806 15792 net.cpp:200] cifar does not need backward computation.
I1124 05:32:57.932806 15792 net.cpp:242] This network produces output accuracy
I1124 05:32:57.933806 15792 net.cpp:242] This network produces output loss
I1124 05:32:57.933806 15792 net.cpp:255] Network initialization done.
I1124 05:32:57.933806 15792 solver.cpp:56] Solver scaffolding done.
I1124 05:32:57.936817 15792 caffe.cpp:249] Starting Optimization
I1124 05:32:57.936817 15792 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_1.6M
I1124 05:32:57.936817 15792 solver.cpp:273] Learning Rate Policy: multistep
I1124 05:32:57.940806 15792 solver.cpp:330] Iteration 0, Testing net (#0)
I1124 05:32:57.941808 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:33:00.084549 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:33:00.167618 15792 solver.cpp:397]     Test net output #0: accuracy = 0.1009
I1124 05:33:00.167618 15792 solver.cpp:397]     Test net output #1: loss = 78.5243 (* 1 = 78.5243 loss)
I1124 05:33:00.293642 15792 solver.cpp:218] Iteration 0 (0 iter/s, 2.3566s/100 iters), loss = 4.0367
I1124 05:33:00.294646 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.12
I1124 05:33:00.294646 15792 solver.cpp:237]     Train net output #1: loss = 4.0367 (* 1 = 4.0367 loss)
I1124 05:33:00.294646 15792 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1124 05:33:08.238191 15792 solver.cpp:218] Iteration 100 (12.7008 iter/s, 7.87352s/100 iters), loss = 2.36348
I1124 05:33:08.238191 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.36
I1124 05:33:08.238191 15792 solver.cpp:237]     Train net output #1: loss = 2.36348 (* 1 = 2.36348 loss)
I1124 05:33:08.238191 15792 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1124 05:33:16.094275 15792 solver.cpp:218] Iteration 200 (12.7304 iter/s, 7.85523s/100 iters), loss = 1.92714
I1124 05:33:16.094275 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.34
I1124 05:33:16.094275 15792 solver.cpp:237]     Train net output #1: loss = 1.92714 (* 1 = 1.92714 loss)
I1124 05:33:16.094275 15792 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1124 05:33:23.934780 15792 solver.cpp:218] Iteration 300 (12.7547 iter/s, 7.84025s/100 iters), loss = 1.90238
I1124 05:33:23.934780 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.48
I1124 05:33:23.934780 15792 solver.cpp:237]     Train net output #1: loss = 1.90238 (* 1 = 1.90238 loss)
I1124 05:33:23.934780 15792 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1124 05:33:31.735543 15792 solver.cpp:218] Iteration 400 (12.82 iter/s, 7.80033s/100 iters), loss = 1.80765
I1124 05:33:31.735543 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.48
I1124 05:33:31.735543 15792 solver.cpp:237]     Train net output #1: loss = 1.80765 (* 1 = 1.80765 loss)
I1124 05:33:31.735543 15792 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1124 05:33:39.167775 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:33:39.476848 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_500.caffemodel
I1124 05:33:39.522866 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_500.solverstate
I1124 05:33:39.543865 15792 solver.cpp:330] Iteration 500, Testing net (#0)
I1124 05:33:39.543865 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:33:41.624570 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:33:41.708101 15792 solver.cpp:397]     Test net output #0: accuracy = 0.4325
I1124 05:33:41.708101 15792 solver.cpp:397]     Test net output #1: loss = 1.52988 (* 1 = 1.52988 loss)
I1124 05:33:41.784196 15792 solver.cpp:218] Iteration 500 (9.95196 iter/s, 10.0483s/100 iters), loss = 1.36528
I1124 05:33:41.784196 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.58
I1124 05:33:41.784196 15792 solver.cpp:237]     Train net output #1: loss = 1.36528 (* 1 = 1.36528 loss)
I1124 05:33:41.784196 15792 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1124 05:33:49.606994 15792 solver.cpp:218] Iteration 600 (12.784 iter/s, 7.82227s/100 iters), loss = 1.39856
I1124 05:33:49.606994 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.56
I1124 05:33:49.606994 15792 solver.cpp:237]     Train net output #1: loss = 1.39856 (* 1 = 1.39856 loss)
I1124 05:33:49.606994 15792 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1124 05:33:57.431967 15792 solver.cpp:218] Iteration 700 (12.7798 iter/s, 7.82484s/100 iters), loss = 1.11537
I1124 05:33:57.432955 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.6
I1124 05:33:57.432955 15792 solver.cpp:237]     Train net output #1: loss = 1.11537 (* 1 = 1.11537 loss)
I1124 05:33:57.432955 15792 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1124 05:34:05.254233 15792 solver.cpp:218] Iteration 800 (12.7863 iter/s, 7.82085s/100 iters), loss = 0.985155
I1124 05:34:05.254233 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1124 05:34:05.254233 15792 solver.cpp:237]     Train net output #1: loss = 0.985155 (* 1 = 0.985155 loss)
I1124 05:34:05.254233 15792 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1124 05:34:13.085060 15792 solver.cpp:218] Iteration 900 (12.7701 iter/s, 7.83077s/100 iters), loss = 0.982435
I1124 05:34:13.085060 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1124 05:34:13.085060 15792 solver.cpp:237]     Train net output #1: loss = 0.982435 (* 1 = 0.982435 loss)
I1124 05:34:13.085060 15792 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1124 05:34:20.531296 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:34:20.842381 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_1000.caffemodel
I1124 05:34:20.883365 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_1000.solverstate
I1124 05:34:20.904358 15792 solver.cpp:330] Iteration 1000, Testing net (#0)
I1124 05:34:20.904855 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:34:22.981468 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:34:23.066504 15792 solver.cpp:397]     Test net output #0: accuracy = 0.5285
I1124 05:34:23.066504 15792 solver.cpp:397]     Test net output #1: loss = 1.27915 (* 1 = 1.27915 loss)
I1124 05:34:23.143028 15792 solver.cpp:218] Iteration 1000 (9.94324 iter/s, 10.0571s/100 iters), loss = 0.853085
I1124 05:34:23.143028 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1124 05:34:23.143028 15792 solver.cpp:237]     Train net output #1: loss = 0.853085 (* 1 = 0.853085 loss)
I1124 05:34:23.143028 15792 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1124 05:34:30.974956 15792 solver.cpp:218] Iteration 1100 (12.7685 iter/s, 7.83174s/100 iters), loss = 0.84794
I1124 05:34:30.974956 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1124 05:34:30.974956 15792 solver.cpp:237]     Train net output #1: loss = 0.84794 (* 1 = 0.84794 loss)
I1124 05:34:30.974956 15792 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1124 05:34:38.854619 15792 solver.cpp:218] Iteration 1200 (12.6923 iter/s, 7.87881s/100 iters), loss = 0.757857
I1124 05:34:38.854619 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1124 05:34:38.854619 15792 solver.cpp:237]     Train net output #1: loss = 0.757857 (* 1 = 0.757857 loss)
I1124 05:34:38.854619 15792 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1124 05:34:46.710549 15792 solver.cpp:218] Iteration 1300 (12.7289 iter/s, 7.85615s/100 iters), loss = 0.702641
I1124 05:34:46.710549 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1124 05:34:46.710549 15792 solver.cpp:237]     Train net output #1: loss = 0.702641 (* 1 = 0.702641 loss)
I1124 05:34:46.710549 15792 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1124 05:34:54.576666 15792 solver.cpp:218] Iteration 1400 (12.7147 iter/s, 7.86491s/100 iters), loss = 0.804432
I1124 05:34:54.576666 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1124 05:34:54.576666 15792 solver.cpp:237]     Train net output #1: loss = 0.804432 (* 1 = 0.804432 loss)
I1124 05:34:54.576666 15792 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1124 05:35:02.024209 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:35:02.334228 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_1500.caffemodel
I1124 05:35:02.375232 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_1500.solverstate
I1124 05:35:02.395232 15792 solver.cpp:330] Iteration 1500, Testing net (#0)
I1124 05:35:02.395232 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:35:04.477362 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:35:04.561367 15792 solver.cpp:397]     Test net output #0: accuracy = 0.6333
I1124 05:35:04.561367 15792 solver.cpp:397]     Test net output #1: loss = 1.01795 (* 1 = 1.01795 loss)
I1124 05:35:04.638368 15792 solver.cpp:218] Iteration 1500 (9.93922 iter/s, 10.0611s/100 iters), loss = 0.640939
I1124 05:35:04.638368 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1124 05:35:04.638368 15792 solver.cpp:237]     Train net output #1: loss = 0.640939 (* 1 = 0.640939 loss)
I1124 05:35:04.638368 15792 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1124 05:35:12.470787 15792 solver.cpp:218] Iteration 1600 (12.7682 iter/s, 7.83193s/100 iters), loss = 0.597657
I1124 05:35:12.470787 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1124 05:35:12.470787 15792 solver.cpp:237]     Train net output #1: loss = 0.597657 (* 1 = 0.597657 loss)
I1124 05:35:12.470787 15792 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1124 05:35:20.301396 15792 solver.cpp:218] Iteration 1700 (12.7704 iter/s, 7.83063s/100 iters), loss = 0.579936
I1124 05:35:20.301396 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1124 05:35:20.301396 15792 solver.cpp:237]     Train net output #1: loss = 0.579936 (* 1 = 0.579936 loss)
I1124 05:35:20.301396 15792 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1124 05:35:28.131693 15792 solver.cpp:218] Iteration 1800 (12.7722 iter/s, 7.82949s/100 iters), loss = 0.543962
I1124 05:35:28.132194 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1124 05:35:28.132194 15792 solver.cpp:237]     Train net output #1: loss = 0.543962 (* 1 = 0.543962 loss)
I1124 05:35:28.132194 15792 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1124 05:35:35.962716 15792 solver.cpp:218] Iteration 1900 (12.7698 iter/s, 7.83095s/100 iters), loss = 0.593169
I1124 05:35:35.963716 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1124 05:35:35.963716 15792 solver.cpp:237]     Train net output #1: loss = 0.593169 (* 1 = 0.593169 loss)
I1124 05:35:35.963716 15792 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1124 05:35:43.426296 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:35:43.736848 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_2000.caffemodel
I1124 05:35:43.775382 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_2000.solverstate
I1124 05:35:43.795390 15792 solver.cpp:330] Iteration 2000, Testing net (#0)
I1124 05:35:43.795390 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:35:45.881539 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:35:45.965548 15792 solver.cpp:397]     Test net output #0: accuracy = 0.7249
I1124 05:35:45.965548 15792 solver.cpp:397]     Test net output #1: loss = 0.818478 (* 1 = 0.818478 loss)
I1124 05:35:46.043050 15792 solver.cpp:218] Iteration 2000 (9.9216 iter/s, 10.079s/100 iters), loss = 0.462297
I1124 05:35:46.043050 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1124 05:35:46.043050 15792 solver.cpp:237]     Train net output #1: loss = 0.462297 (* 1 = 0.462297 loss)
I1124 05:35:46.043050 15792 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1124 05:35:53.895778 15792 solver.cpp:218] Iteration 2100 (12.7344 iter/s, 7.85273s/100 iters), loss = 0.511897
I1124 05:35:53.895778 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1124 05:35:53.895778 15792 solver.cpp:237]     Train net output #1: loss = 0.511897 (* 1 = 0.511897 loss)
I1124 05:35:53.895778 15792 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1124 05:36:01.750577 15792 solver.cpp:218] Iteration 2200 (12.7322 iter/s, 7.85411s/100 iters), loss = 0.558421
I1124 05:36:01.750577 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1124 05:36:01.750577 15792 solver.cpp:237]     Train net output #1: loss = 0.558421 (* 1 = 0.558421 loss)
I1124 05:36:01.750577 15792 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1124 05:36:09.606252 15792 solver.cpp:218] Iteration 2300 (12.7302 iter/s, 7.85535s/100 iters), loss = 0.576498
I1124 05:36:09.606252 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1124 05:36:09.606252 15792 solver.cpp:237]     Train net output #1: loss = 0.576498 (* 1 = 0.576498 loss)
I1124 05:36:09.606252 15792 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1124 05:36:17.462007 15792 solver.cpp:218] Iteration 2400 (12.7312 iter/s, 7.85472s/100 iters), loss = 0.517235
I1124 05:36:17.462007 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1124 05:36:17.462007 15792 solver.cpp:237]     Train net output #1: loss = 0.517235 (* 1 = 0.517235 loss)
I1124 05:36:17.462007 15792 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1124 05:36:24.969727 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:36:25.281793 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_2500.caffemodel
I1124 05:36:25.323802 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_2500.solverstate
I1124 05:36:25.345297 15792 solver.cpp:330] Iteration 2500, Testing net (#0)
I1124 05:36:25.345297 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:36:27.455976 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:36:27.541494 15792 solver.cpp:397]     Test net output #0: accuracy = 0.7772
I1124 05:36:27.541494 15792 solver.cpp:397]     Test net output #1: loss = 0.684933 (* 1 = 0.684933 loss)
I1124 05:36:27.618994 15792 solver.cpp:218] Iteration 2500 (9.84591 iter/s, 10.1565s/100 iters), loss = 0.445504
I1124 05:36:27.618994 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1124 05:36:27.618994 15792 solver.cpp:237]     Train net output #1: loss = 0.445504 (* 1 = 0.445504 loss)
I1124 05:36:27.618994 15792 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1124 05:36:35.573909 15792 solver.cpp:218] Iteration 2600 (12.5708 iter/s, 7.95497s/100 iters), loss = 0.410956
I1124 05:36:35.573909 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1124 05:36:35.573909 15792 solver.cpp:237]     Train net output #1: loss = 0.410955 (* 1 = 0.410955 loss)
I1124 05:36:35.573909 15792 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1124 05:36:43.438503 15792 solver.cpp:218] Iteration 2700 (12.7157 iter/s, 7.86427s/100 iters), loss = 0.46469
I1124 05:36:43.438503 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1124 05:36:43.438503 15792 solver.cpp:237]     Train net output #1: loss = 0.46469 (* 1 = 0.46469 loss)
I1124 05:36:43.438503 15792 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1124 05:36:51.300990 15792 solver.cpp:218] Iteration 2800 (12.7207 iter/s, 7.86118s/100 iters), loss = 0.45641
I1124 05:36:51.300990 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1124 05:36:51.300990 15792 solver.cpp:237]     Train net output #1: loss = 0.45641 (* 1 = 0.45641 loss)
I1124 05:36:51.300990 15792 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1124 05:36:59.152103 15792 solver.cpp:218] Iteration 2900 (12.7368 iter/s, 7.85128s/100 iters), loss = 0.419681
I1124 05:36:59.152103 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1124 05:36:59.152103 15792 solver.cpp:237]     Train net output #1: loss = 0.419681 (* 1 = 0.419681 loss)
I1124 05:36:59.152103 15792 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1124 05:37:06.626685 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:37:06.936702 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_3000.caffemodel
I1124 05:37:06.979702 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_3000.solverstate
I1124 05:37:06.999702 15792 solver.cpp:330] Iteration 3000, Testing net (#0)
I1124 05:37:06.999702 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:37:09.085831 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:37:09.169842 15792 solver.cpp:397]     Test net output #0: accuracy = 0.7389
I1124 05:37:09.169842 15792 solver.cpp:397]     Test net output #1: loss = 0.751696 (* 1 = 0.751696 loss)
I1124 05:37:09.246840 15792 solver.cpp:218] Iteration 3000 (9.90704 iter/s, 10.0938s/100 iters), loss = 0.358884
I1124 05:37:09.246840 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1124 05:37:09.246840 15792 solver.cpp:237]     Train net output #1: loss = 0.358884 (* 1 = 0.358884 loss)
I1124 05:37:09.246840 15792 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1124 05:37:17.108383 15792 solver.cpp:218] Iteration 3100 (12.7211 iter/s, 7.86097s/100 iters), loss = 0.48216
I1124 05:37:17.108383 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1124 05:37:17.108383 15792 solver.cpp:237]     Train net output #1: loss = 0.48216 (* 1 = 0.48216 loss)
I1124 05:37:17.108383 15792 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1124 05:37:24.964254 15792 solver.cpp:218] Iteration 3200 (12.7306 iter/s, 7.85509s/100 iters), loss = 0.497381
I1124 05:37:24.964254 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1124 05:37:24.964254 15792 solver.cpp:237]     Train net output #1: loss = 0.497381 (* 1 = 0.497381 loss)
I1124 05:37:24.964254 15792 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1124 05:37:32.822726 15792 solver.cpp:218] Iteration 3300 (12.7256 iter/s, 7.85819s/100 iters), loss = 0.43366
I1124 05:37:32.822726 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1124 05:37:32.823226 15792 solver.cpp:237]     Train net output #1: loss = 0.43366 (* 1 = 0.43366 loss)
I1124 05:37:32.823226 15792 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1124 05:37:40.680308 15792 solver.cpp:218] Iteration 3400 (12.7267 iter/s, 7.85749s/100 iters), loss = 0.415541
I1124 05:37:40.680308 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1124 05:37:40.680308 15792 solver.cpp:237]     Train net output #1: loss = 0.415541 (* 1 = 0.415541 loss)
I1124 05:37:40.680308 15792 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1124 05:37:48.148967 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:37:48.458983 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_3500.caffemodel
I1124 05:37:48.498984 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_3500.solverstate
I1124 05:37:48.518002 15792 solver.cpp:330] Iteration 3500, Testing net (#0)
I1124 05:37:48.518002 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:37:50.615150 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:37:50.701160 15792 solver.cpp:397]     Test net output #0: accuracy = 0.6955
I1124 05:37:50.701160 15792 solver.cpp:397]     Test net output #1: loss = 0.870963 (* 1 = 0.870963 loss)
I1124 05:37:50.779161 15792 solver.cpp:218] Iteration 3500 (9.90312 iter/s, 10.0978s/100 iters), loss = 0.386347
I1124 05:37:50.779161 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1124 05:37:50.779161 15792 solver.cpp:237]     Train net output #1: loss = 0.386347 (* 1 = 0.386347 loss)
I1124 05:37:50.779161 15792 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1124 05:37:58.758936 15792 solver.cpp:218] Iteration 3600 (12.5325 iter/s, 7.97927s/100 iters), loss = 0.407039
I1124 05:37:58.758936 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1124 05:37:58.758936 15792 solver.cpp:237]     Train net output #1: loss = 0.407039 (* 1 = 0.407039 loss)
I1124 05:37:58.758936 15792 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1124 05:38:06.656527 15792 solver.cpp:218] Iteration 3700 (12.6632 iter/s, 7.89689s/100 iters), loss = 0.447227
I1124 05:38:06.656527 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1124 05:38:06.656527 15792 solver.cpp:237]     Train net output #1: loss = 0.447227 (* 1 = 0.447227 loss)
I1124 05:38:06.656527 15792 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1124 05:38:14.557193 15792 solver.cpp:218] Iteration 3800 (12.658 iter/s, 7.90014s/100 iters), loss = 0.456018
I1124 05:38:14.557193 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1124 05:38:14.557193 15792 solver.cpp:237]     Train net output #1: loss = 0.456018 (* 1 = 0.456018 loss)
I1124 05:38:14.557193 15792 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1124 05:38:22.453811 15792 solver.cpp:218] Iteration 3900 (12.6635 iter/s, 7.89672s/100 iters), loss = 0.480574
I1124 05:38:22.454812 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1124 05:38:22.454812 15792 solver.cpp:237]     Train net output #1: loss = 0.480574 (* 1 = 0.480574 loss)
I1124 05:38:22.454812 15792 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1124 05:38:29.977485 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:38:30.287529 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_4000.caffemodel
I1124 05:38:30.328532 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_4000.solverstate
I1124 05:38:30.348534 15792 solver.cpp:330] Iteration 4000, Testing net (#0)
I1124 05:38:30.348534 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:38:32.443665 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:38:32.528681 15792 solver.cpp:397]     Test net output #0: accuracy = 0.8028
I1124 05:38:32.528681 15792 solver.cpp:397]     Test net output #1: loss = 0.612404 (* 1 = 0.612404 loss)
I1124 05:38:32.604677 15792 solver.cpp:218] Iteration 4000 (9.852 iter/s, 10.1502s/100 iters), loss = 0.368946
I1124 05:38:32.604677 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1124 05:38:32.604677 15792 solver.cpp:237]     Train net output #1: loss = 0.368946 (* 1 = 0.368946 loss)
I1124 05:38:32.604677 15792 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1124 05:38:40.502426 15792 solver.cpp:218] Iteration 4100 (12.663 iter/s, 7.89699s/100 iters), loss = 0.385882
I1124 05:38:40.502426 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1124 05:38:40.502426 15792 solver.cpp:237]     Train net output #1: loss = 0.385882 (* 1 = 0.385882 loss)
I1124 05:38:40.502426 15792 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1124 05:38:48.398459 15792 solver.cpp:218] Iteration 4200 (12.6656 iter/s, 7.89541s/100 iters), loss = 0.456986
I1124 05:38:48.398459 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1124 05:38:48.398459 15792 solver.cpp:237]     Train net output #1: loss = 0.456986 (* 1 = 0.456986 loss)
I1124 05:38:48.398459 15792 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1124 05:38:56.301108 15792 solver.cpp:218] Iteration 4300 (12.6546 iter/s, 7.90224s/100 iters), loss = 0.424383
I1124 05:38:56.301108 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1124 05:38:56.301108 15792 solver.cpp:237]     Train net output #1: loss = 0.424383 (* 1 = 0.424383 loss)
I1124 05:38:56.301108 15792 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1124 05:39:04.203685 15792 solver.cpp:218] Iteration 4400 (12.656 iter/s, 7.90139s/100 iters), loss = 0.36568
I1124 05:39:04.203685 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1124 05:39:04.203685 15792 solver.cpp:237]     Train net output #1: loss = 0.36568 (* 1 = 0.36568 loss)
I1124 05:39:04.203685 15792 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1124 05:39:11.719329 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:39:12.039356 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_4500.caffemodel
I1124 05:39:12.080399 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_4500.solverstate
I1124 05:39:12.099397 15792 solver.cpp:330] Iteration 4500, Testing net (#0)
I1124 05:39:12.100383 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:39:14.216536 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:39:14.300541 15792 solver.cpp:397]     Test net output #0: accuracy = 0.7448
I1124 05:39:14.300541 15792 solver.cpp:397]     Test net output #1: loss = 0.740879 (* 1 = 0.740879 loss)
I1124 05:39:14.379546 15792 solver.cpp:218] Iteration 4500 (9.82699 iter/s, 10.1761s/100 iters), loss = 0.349357
I1124 05:39:14.380548 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1124 05:39:14.380548 15792 solver.cpp:237]     Train net output #1: loss = 0.349357 (* 1 = 0.349357 loss)
I1124 05:39:14.380548 15792 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1124 05:39:22.367274 15792 solver.cpp:218] Iteration 4600 (12.5209 iter/s, 7.98663s/100 iters), loss = 0.302452
I1124 05:39:22.367274 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1124 05:39:22.367274 15792 solver.cpp:237]     Train net output #1: loss = 0.302452 (* 1 = 0.302452 loss)
I1124 05:39:22.367274 15792 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1124 05:39:30.419911 15792 solver.cpp:218] Iteration 4700 (12.4189 iter/s, 8.05225s/100 iters), loss = 0.497597
I1124 05:39:30.419911 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1124 05:39:30.419911 15792 solver.cpp:237]     Train net output #1: loss = 0.497597 (* 1 = 0.497597 loss)
I1124 05:39:30.419911 15792 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1124 05:39:38.317128 15792 solver.cpp:218] Iteration 4800 (12.6641 iter/s, 7.89635s/100 iters), loss = 0.373756
I1124 05:39:38.317128 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1124 05:39:38.317128 15792 solver.cpp:237]     Train net output #1: loss = 0.373756 (* 1 = 0.373756 loss)
I1124 05:39:38.317128 15792 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1124 05:39:46.209770 15792 solver.cpp:218] Iteration 4900 (12.6706 iter/s, 7.89229s/100 iters), loss = 0.36186
I1124 05:39:46.209770 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1124 05:39:46.209770 15792 solver.cpp:237]     Train net output #1: loss = 0.36186 (* 1 = 0.36186 loss)
I1124 05:39:46.209770 15792 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1124 05:39:53.713428 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:39:54.025442 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_5000.caffemodel
I1124 05:39:54.067443 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_5000.solverstate
I1124 05:39:54.086442 15792 solver.cpp:330] Iteration 5000, Testing net (#0)
I1124 05:39:54.086442 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:39:56.183609 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:39:56.267616 15792 solver.cpp:397]     Test net output #0: accuracy = 0.782
I1124 05:39:56.267616 15792 solver.cpp:397]     Test net output #1: loss = 0.662468 (* 1 = 0.662468 loss)
I1124 05:39:56.344620 15792 solver.cpp:218] Iteration 5000 (9.86727 iter/s, 10.1345s/100 iters), loss = 0.282675
I1124 05:39:56.344620 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1124 05:39:56.344620 15792 solver.cpp:237]     Train net output #1: loss = 0.282675 (* 1 = 0.282675 loss)
I1124 05:39:56.344620 15792 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1124 05:39:56.344620 15792 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1124 05:40:04.293792 15792 solver.cpp:218] Iteration 5100 (12.5813 iter/s, 7.94833s/100 iters), loss = 0.231869
I1124 05:40:04.293792 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1124 05:40:04.293792 15792 solver.cpp:237]     Train net output #1: loss = 0.231869 (* 1 = 0.231869 loss)
I1124 05:40:04.293792 15792 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1124 05:40:12.432883 15792 solver.cpp:218] Iteration 5200 (12.2874 iter/s, 8.13845s/100 iters), loss = 0.27135
I1124 05:40:12.432883 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1124 05:40:12.432883 15792 solver.cpp:237]     Train net output #1: loss = 0.271349 (* 1 = 0.271349 loss)
I1124 05:40:12.432883 15792 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1124 05:40:20.480882 15792 solver.cpp:218] Iteration 5300 (12.4259 iter/s, 8.04774s/100 iters), loss = 0.222928
I1124 05:40:20.480882 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 05:40:20.480882 15792 solver.cpp:237]     Train net output #1: loss = 0.222928 (* 1 = 0.222928 loss)
I1124 05:40:20.481382 15792 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1124 05:40:28.523881 15792 solver.cpp:218] Iteration 5400 (12.434 iter/s, 8.04247s/100 iters), loss = 0.208951
I1124 05:40:28.523881 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1124 05:40:28.523881 15792 solver.cpp:237]     Train net output #1: loss = 0.208951 (* 1 = 0.208951 loss)
I1124 05:40:28.523881 15792 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1124 05:40:36.171885 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:40:36.488883 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_5500.caffemodel
I1124 05:40:36.530383 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_5500.solverstate
I1124 05:40:36.549902 15792 solver.cpp:330] Iteration 5500, Testing net (#0)
I1124 05:40:36.550386 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:40:38.686383 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:40:38.771883 15792 solver.cpp:397]     Test net output #0: accuracy = 0.8971
I1124 05:40:38.771883 15792 solver.cpp:397]     Test net output #1: loss = 0.304298 (* 1 = 0.304298 loss)
I1124 05:40:38.849881 15792 solver.cpp:218] Iteration 5500 (9.6848 iter/s, 10.3255s/100 iters), loss = 0.195651
I1124 05:40:38.850383 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1124 05:40:38.850383 15792 solver.cpp:237]     Train net output #1: loss = 0.195651 (* 1 = 0.195651 loss)
I1124 05:40:38.850383 15792 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1124 05:40:46.897435 15792 solver.cpp:218] Iteration 5600 (12.4272 iter/s, 8.04683s/100 iters), loss = 0.16154
I1124 05:40:46.897435 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:40:46.897435 15792 solver.cpp:237]     Train net output #1: loss = 0.16154 (* 1 = 0.16154 loss)
I1124 05:40:46.897435 15792 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1124 05:40:54.944973 15792 solver.cpp:218] Iteration 5700 (12.4268 iter/s, 8.0471s/100 iters), loss = 0.192018
I1124 05:40:54.944973 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 05:40:54.944973 15792 solver.cpp:237]     Train net output #1: loss = 0.192018 (* 1 = 0.192018 loss)
I1124 05:40:54.944973 15792 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1124 05:41:03.060544 15792 solver.cpp:218] Iteration 5800 (12.3231 iter/s, 8.11485s/100 iters), loss = 0.190209
I1124 05:41:03.060544 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 05:41:03.060544 15792 solver.cpp:237]     Train net output #1: loss = 0.190209 (* 1 = 0.190209 loss)
I1124 05:41:03.060544 15792 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1124 05:41:11.080114 15792 solver.cpp:218] Iteration 5900 (12.4701 iter/s, 8.01919s/100 iters), loss = 0.196795
I1124 05:41:11.080114 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 05:41:11.080114 15792 solver.cpp:237]     Train net output #1: loss = 0.196795 (* 1 = 0.196795 loss)
I1124 05:41:11.080114 15792 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1124 05:41:18.656066 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:41:18.978096 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_6000.caffemodel
I1124 05:41:19.024087 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_6000.solverstate
I1124 05:41:19.044590 15792 solver.cpp:330] Iteration 6000, Testing net (#0)
I1124 05:41:19.044590 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:41:21.152604 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:41:21.237607 15792 solver.cpp:397]     Test net output #0: accuracy = 0.8995
I1124 05:41:21.237607 15792 solver.cpp:397]     Test net output #1: loss = 0.28834 (* 1 = 0.28834 loss)
I1124 05:41:21.313609 15792 solver.cpp:218] Iteration 6000 (9.77204 iter/s, 10.2333s/100 iters), loss = 0.210169
I1124 05:41:21.313609 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 05:41:21.313609 15792 solver.cpp:237]     Train net output #1: loss = 0.210169 (* 1 = 0.210169 loss)
I1124 05:41:21.313609 15792 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1124 05:41:29.217285 15792 solver.cpp:218] Iteration 6100 (12.653 iter/s, 7.90325s/100 iters), loss = 0.20367
I1124 05:41:29.217285 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 05:41:29.217285 15792 solver.cpp:237]     Train net output #1: loss = 0.20367 (* 1 = 0.20367 loss)
I1124 05:41:29.217285 15792 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1124 05:41:37.112174 15792 solver.cpp:218] Iteration 6200 (12.6682 iter/s, 7.89378s/100 iters), loss = 0.177913
I1124 05:41:37.112174 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:41:37.112174 15792 solver.cpp:237]     Train net output #1: loss = 0.177912 (* 1 = 0.177912 loss)
I1124 05:41:37.112174 15792 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1124 05:41:44.981729 15792 solver.cpp:218] Iteration 6300 (12.7083 iter/s, 7.86889s/100 iters), loss = 0.140472
I1124 05:41:44.981729 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:41:44.981729 15792 solver.cpp:237]     Train net output #1: loss = 0.140472 (* 1 = 0.140472 loss)
I1124 05:41:44.981729 15792 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1124 05:41:52.931529 15792 solver.cpp:218] Iteration 6400 (12.5798 iter/s, 7.94924s/100 iters), loss = 0.132041
I1124 05:41:52.931529 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:41:52.931529 15792 solver.cpp:237]     Train net output #1: loss = 0.132041 (* 1 = 0.132041 loss)
I1124 05:41:52.931529 15792 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1124 05:42:00.475939 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:42:00.806460 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_6500.caffemodel
I1124 05:42:00.847461 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_6500.solverstate
I1124 05:42:00.867461 15792 solver.cpp:330] Iteration 6500, Testing net (#0)
I1124 05:42:00.867461 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:42:02.972116 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:42:03.055618 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9009
I1124 05:42:03.056618 15792 solver.cpp:397]     Test net output #1: loss = 0.285781 (* 1 = 0.285781 loss)
I1124 05:42:03.132627 15792 solver.cpp:218] Iteration 6500 (9.80274 iter/s, 10.2012s/100 iters), loss = 0.135484
I1124 05:42:03.132627 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:42:03.132627 15792 solver.cpp:237]     Train net output #1: loss = 0.135484 (* 1 = 0.135484 loss)
I1124 05:42:03.133625 15792 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1124 05:42:11.052459 15792 solver.cpp:218] Iteration 6600 (12.6281 iter/s, 7.91883s/100 iters), loss = 0.127464
I1124 05:42:11.052459 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:42:11.052459 15792 solver.cpp:237]     Train net output #1: loss = 0.127464 (* 1 = 0.127464 loss)
I1124 05:42:11.052459 15792 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1124 05:42:18.994083 15792 solver.cpp:218] Iteration 6700 (12.5931 iter/s, 7.94086s/100 iters), loss = 0.147046
I1124 05:42:18.994083 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 05:42:18.994083 15792 solver.cpp:237]     Train net output #1: loss = 0.147046 (* 1 = 0.147046 loss)
I1124 05:42:18.994083 15792 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1124 05:42:26.955327 15792 solver.cpp:218] Iteration 6800 (12.5614 iter/s, 7.96091s/100 iters), loss = 0.152958
I1124 05:42:26.955827 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 05:42:26.955827 15792 solver.cpp:237]     Train net output #1: loss = 0.152958 (* 1 = 0.152958 loss)
I1124 05:42:26.955827 15792 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1124 05:42:34.878610 15792 solver.cpp:218] Iteration 6900 (12.6219 iter/s, 7.92276s/100 iters), loss = 0.12667
I1124 05:42:34.878610 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:42:34.878610 15792 solver.cpp:237]     Train net output #1: loss = 0.126669 (* 1 = 0.126669 loss)
I1124 05:42:34.878610 15792 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1124 05:42:42.353175 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:42:42.668203 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_7000.caffemodel
I1124 05:42:42.708204 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_7000.solverstate
I1124 05:42:42.728207 15792 solver.cpp:330] Iteration 7000, Testing net (#0)
I1124 05:42:42.728708 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:42:44.821300 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:42:44.906306 15792 solver.cpp:397]     Test net output #0: accuracy = 0.898
I1124 05:42:44.906306 15792 solver.cpp:397]     Test net output #1: loss = 0.291806 (* 1 = 0.291806 loss)
I1124 05:42:44.983310 15792 solver.cpp:218] Iteration 7000 (9.89711 iter/s, 10.104s/100 iters), loss = 0.149524
I1124 05:42:44.983310 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 05:42:44.983310 15792 solver.cpp:237]     Train net output #1: loss = 0.149524 (* 1 = 0.149524 loss)
I1124 05:42:44.983310 15792 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1124 05:42:52.989742 15792 solver.cpp:218] Iteration 7100 (12.4909 iter/s, 8.00583s/100 iters), loss = 0.13063
I1124 05:42:52.989742 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:42:52.989742 15792 solver.cpp:237]     Train net output #1: loss = 0.13063 (* 1 = 0.13063 loss)
I1124 05:42:52.989742 15792 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1124 05:43:00.873425 15792 solver.cpp:218] Iteration 7200 (12.6853 iter/s, 7.88314s/100 iters), loss = 0.136158
I1124 05:43:00.873425 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 05:43:00.873425 15792 solver.cpp:237]     Train net output #1: loss = 0.136158 (* 1 = 0.136158 loss)
I1124 05:43:00.873425 15792 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1124 05:43:08.752002 15792 solver.cpp:218] Iteration 7300 (12.6922 iter/s, 7.87882s/100 iters), loss = 0.0805626
I1124 05:43:08.753002 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:43:08.753002 15792 solver.cpp:237]     Train net output #1: loss = 0.0805625 (* 1 = 0.0805625 loss)
I1124 05:43:08.753002 15792 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1124 05:43:16.681706 15792 solver.cpp:218] Iteration 7400 (12.612 iter/s, 7.92898s/100 iters), loss = 0.129003
I1124 05:43:16.681706 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 05:43:16.681706 15792 solver.cpp:237]     Train net output #1: loss = 0.129003 (* 1 = 0.129003 loss)
I1124 05:43:16.681706 15792 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1124 05:43:24.210453 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:43:24.525470 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_7500.caffemodel
I1124 05:43:24.566515 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_7500.solverstate
I1124 05:43:24.586513 15792 solver.cpp:330] Iteration 7500, Testing net (#0)
I1124 05:43:24.586513 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:43:26.678694 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:43:26.762699 15792 solver.cpp:397]     Test net output #0: accuracy = 0.8993
I1124 05:43:26.762699 15792 solver.cpp:397]     Test net output #1: loss = 0.294665 (* 1 = 0.294665 loss)
I1124 05:43:26.839699 15792 solver.cpp:218] Iteration 7500 (9.8454 iter/s, 10.157s/100 iters), loss = 0.113949
I1124 05:43:26.839699 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:43:26.839699 15792 solver.cpp:237]     Train net output #1: loss = 0.113949 (* 1 = 0.113949 loss)
I1124 05:43:26.839699 15792 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1124 05:43:34.734350 15792 solver.cpp:218] Iteration 7600 (12.6674 iter/s, 7.8943s/100 iters), loss = 0.168906
I1124 05:43:34.734350 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1124 05:43:34.734350 15792 solver.cpp:237]     Train net output #1: loss = 0.168906 (* 1 = 0.168906 loss)
I1124 05:43:34.734350 15792 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1124 05:43:42.595974 15792 solver.cpp:218] Iteration 7700 (12.7201 iter/s, 7.86158s/100 iters), loss = 0.118239
I1124 05:43:42.595974 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 05:43:42.595974 15792 solver.cpp:237]     Train net output #1: loss = 0.118238 (* 1 = 0.118238 loss)
I1124 05:43:42.596976 15792 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1124 05:43:50.455617 15792 solver.cpp:218] Iteration 7800 (12.725 iter/s, 7.85856s/100 iters), loss = 0.111959
I1124 05:43:50.455617 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:43:50.455617 15792 solver.cpp:237]     Train net output #1: loss = 0.111959 (* 1 = 0.111959 loss)
I1124 05:43:50.455617 15792 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1124 05:43:58.308203 15792 solver.cpp:218] Iteration 7900 (12.7348 iter/s, 7.85248s/100 iters), loss = 0.107622
I1124 05:43:58.308203 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:43:58.308203 15792 solver.cpp:237]     Train net output #1: loss = 0.107622 (* 1 = 0.107622 loss)
I1124 05:43:58.308203 15792 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1124 05:44:05.771803 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:44:06.081822 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_8000.caffemodel
I1124 05:44:06.119823 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_8000.solverstate
I1124 05:44:06.138823 15792 solver.cpp:330] Iteration 8000, Testing net (#0)
I1124 05:44:06.138823 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:44:08.223968 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:44:08.307991 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9006
I1124 05:44:08.307991 15792 solver.cpp:397]     Test net output #1: loss = 0.28533 (* 1 = 0.28533 loss)
I1124 05:44:08.384986 15792 solver.cpp:218] Iteration 8000 (9.92484 iter/s, 10.0757s/100 iters), loss = 0.143471
I1124 05:44:08.384986 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 05:44:08.384986 15792 solver.cpp:237]     Train net output #1: loss = 0.143471 (* 1 = 0.143471 loss)
I1124 05:44:08.384986 15792 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1124 05:44:16.237591 15792 solver.cpp:218] Iteration 8100 (12.7344 iter/s, 7.85272s/100 iters), loss = 0.158002
I1124 05:44:16.237591 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:44:16.237591 15792 solver.cpp:237]     Train net output #1: loss = 0.158002 (* 1 = 0.158002 loss)
I1124 05:44:16.237591 15792 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1124 05:44:24.092339 15792 solver.cpp:218] Iteration 8200 (12.7323 iter/s, 7.85405s/100 iters), loss = 0.112642
I1124 05:44:24.092839 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:44:24.092839 15792 solver.cpp:237]     Train net output #1: loss = 0.112642 (* 1 = 0.112642 loss)
I1124 05:44:24.092839 15792 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1124 05:44:31.938467 15792 solver.cpp:218] Iteration 8300 (12.7467 iter/s, 7.84517s/100 iters), loss = 0.107573
I1124 05:44:31.938467 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:44:31.938467 15792 solver.cpp:237]     Train net output #1: loss = 0.107573 (* 1 = 0.107573 loss)
I1124 05:44:31.938467 15792 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1124 05:44:39.784440 15792 solver.cpp:218] Iteration 8400 (12.7458 iter/s, 7.8457s/100 iters), loss = 0.10958
I1124 05:44:39.784440 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:44:39.784440 15792 solver.cpp:237]     Train net output #1: loss = 0.10958 (* 1 = 0.10958 loss)
I1124 05:44:39.784440 15792 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1124 05:44:47.245949 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:44:47.554966 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_8500.caffemodel
I1124 05:44:47.594485 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_8500.solverstate
I1124 05:44:47.613503 15792 solver.cpp:330] Iteration 8500, Testing net (#0)
I1124 05:44:47.613986 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:44:49.700165 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:44:49.784173 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9016
I1124 05:44:49.784173 15792 solver.cpp:397]     Test net output #1: loss = 0.286879 (* 1 = 0.286879 loss)
I1124 05:44:49.861171 15792 solver.cpp:218] Iteration 8500 (9.92457 iter/s, 10.076s/100 iters), loss = 0.114742
I1124 05:44:49.861171 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:44:49.861171 15792 solver.cpp:237]     Train net output #1: loss = 0.114742 (* 1 = 0.114742 loss)
I1124 05:44:49.861171 15792 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1124 05:44:57.715688 15792 solver.cpp:218] Iteration 8600 (12.7321 iter/s, 7.85413s/100 iters), loss = 0.127922
I1124 05:44:57.715688 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:44:57.715688 15792 solver.cpp:237]     Train net output #1: loss = 0.127922 (* 1 = 0.127922 loss)
I1124 05:44:57.715688 15792 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1124 05:45:05.571389 15792 solver.cpp:218] Iteration 8700 (12.7304 iter/s, 7.85522s/100 iters), loss = 0.112858
I1124 05:45:05.571389 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 05:45:05.571389 15792 solver.cpp:237]     Train net output #1: loss = 0.112858 (* 1 = 0.112858 loss)
I1124 05:45:05.571389 15792 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1124 05:45:13.426581 15792 solver.cpp:218] Iteration 8800 (12.7308 iter/s, 7.85498s/100 iters), loss = 0.0952431
I1124 05:45:13.427083 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:45:13.427083 15792 solver.cpp:237]     Train net output #1: loss = 0.095243 (* 1 = 0.095243 loss)
I1124 05:45:13.427083 15792 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1124 05:45:21.280709 15792 solver.cpp:218] Iteration 8900 (12.7321 iter/s, 7.85413s/100 iters), loss = 0.109141
I1124 05:45:21.281715 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:45:21.281715 15792 solver.cpp:237]     Train net output #1: loss = 0.109141 (* 1 = 0.109141 loss)
I1124 05:45:21.281715 15792 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1124 05:45:28.755131 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:45:29.072031 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_9000.caffemodel
I1124 05:45:29.115031 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_9000.solverstate
I1124 05:45:29.134541 15792 solver.cpp:330] Iteration 9000, Testing net (#0)
I1124 05:45:29.134541 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:45:31.245195 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:45:31.329197 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9009
I1124 05:45:31.329197 15792 solver.cpp:397]     Test net output #1: loss = 0.287208 (* 1 = 0.287208 loss)
I1124 05:45:31.406198 15792 solver.cpp:218] Iteration 9000 (9.87756 iter/s, 10.124s/100 iters), loss = 0.125234
I1124 05:45:31.406198 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:45:31.406198 15792 solver.cpp:237]     Train net output #1: loss = 0.125234 (* 1 = 0.125234 loss)
I1124 05:45:31.406198 15792 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1124 05:45:39.333585 15792 solver.cpp:218] Iteration 9100 (12.615 iter/s, 7.92705s/100 iters), loss = 0.0911264
I1124 05:45:39.333585 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:45:39.333585 15792 solver.cpp:237]     Train net output #1: loss = 0.0911263 (* 1 = 0.0911263 loss)
I1124 05:45:39.333585 15792 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1124 05:45:47.331321 15792 solver.cpp:218] Iteration 9200 (12.504 iter/s, 7.99743s/100 iters), loss = 0.0825948
I1124 05:45:47.331321 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:45:47.331321 15792 solver.cpp:237]     Train net output #1: loss = 0.0825947 (* 1 = 0.0825947 loss)
I1124 05:45:47.331321 15792 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1124 05:45:55.253204 15792 solver.cpp:218] Iteration 9300 (12.6245 iter/s, 7.92111s/100 iters), loss = 0.0592739
I1124 05:45:55.253204 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:45:55.253204 15792 solver.cpp:237]     Train net output #1: loss = 0.0592738 (* 1 = 0.0592738 loss)
I1124 05:45:55.253204 15792 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1124 05:46:03.131940 15792 solver.cpp:218] Iteration 9400 (12.6926 iter/s, 7.87859s/100 iters), loss = 0.0629346
I1124 05:46:03.132441 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:46:03.132441 15792 solver.cpp:237]     Train net output #1: loss = 0.0629345 (* 1 = 0.0629345 loss)
I1124 05:46:03.132441 15792 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1124 05:46:10.597471 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:46:10.905503 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_9500.caffemodel
I1124 05:46:10.948529 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_9500.solverstate
I1124 05:46:10.967535 15792 solver.cpp:330] Iteration 9500, Testing net (#0)
I1124 05:46:10.967535 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:46:13.062675 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:46:13.147687 15792 solver.cpp:397]     Test net output #0: accuracy = 0.8987
I1124 05:46:13.147687 15792 solver.cpp:397]     Test net output #1: loss = 0.299028 (* 1 = 0.299028 loss)
I1124 05:46:13.224685 15792 solver.cpp:218] Iteration 9500 (9.9084 iter/s, 10.0924s/100 iters), loss = 0.120211
I1124 05:46:13.224685 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:46:13.224685 15792 solver.cpp:237]     Train net output #1: loss = 0.120211 (* 1 = 0.120211 loss)
I1124 05:46:13.224685 15792 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1124 05:46:13.224685 15792 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1124 05:46:21.087090 15792 solver.cpp:218] Iteration 9600 (12.7202 iter/s, 7.86153s/100 iters), loss = 0.121801
I1124 05:46:21.087090 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:46:21.087090 15792 solver.cpp:237]     Train net output #1: loss = 0.121801 (* 1 = 0.121801 loss)
I1124 05:46:21.087090 15792 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1124 05:46:28.943866 15792 solver.cpp:218] Iteration 9700 (12.729 iter/s, 7.85609s/100 iters), loss = 0.0785377
I1124 05:46:28.943866 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:46:28.943866 15792 solver.cpp:237]     Train net output #1: loss = 0.0785377 (* 1 = 0.0785377 loss)
I1124 05:46:28.943866 15792 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1124 05:46:36.803958 15792 solver.cpp:218] Iteration 9800 (12.7232 iter/s, 7.85965s/100 iters), loss = 0.0948884
I1124 05:46:36.804460 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:46:36.804460 15792 solver.cpp:237]     Train net output #1: loss = 0.0948883 (* 1 = 0.0948883 loss)
I1124 05:46:36.804460 15792 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1124 05:46:44.704630 15792 solver.cpp:218] Iteration 9900 (12.6584 iter/s, 7.89992s/100 iters), loss = 0.0573094
I1124 05:46:44.704630 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:46:44.704630 15792 solver.cpp:237]     Train net output #1: loss = 0.0573093 (* 1 = 0.0573093 loss)
I1124 05:46:44.704630 15792 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1124 05:46:52.189767 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:46:52.499779 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_10000.caffemodel
I1124 05:46:52.539788 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_10000.solverstate
I1124 05:46:52.559788 15792 solver.cpp:330] Iteration 10000, Testing net (#0)
I1124 05:46:52.559788 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:46:54.646910 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:46:54.730916 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9155
I1124 05:46:54.730916 15792 solver.cpp:397]     Test net output #1: loss = 0.25134 (* 1 = 0.25134 loss)
I1124 05:46:54.807416 15792 solver.cpp:218] Iteration 10000 (9.89865 iter/s, 10.1024s/100 iters), loss = 0.0982299
I1124 05:46:54.807416 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:46:54.807416 15792 solver.cpp:237]     Train net output #1: loss = 0.0982298 (* 1 = 0.0982298 loss)
I1124 05:46:54.807416 15792 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1124 05:47:02.659377 15792 solver.cpp:218] Iteration 10100 (12.7356 iter/s, 7.85199s/100 iters), loss = 0.0883075
I1124 05:47:02.659377 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:47:02.659377 15792 solver.cpp:237]     Train net output #1: loss = 0.0883074 (* 1 = 0.0883074 loss)
I1124 05:47:02.659377 15792 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1124 05:47:10.510027 15792 solver.cpp:218] Iteration 10200 (12.7391 iter/s, 7.84985s/100 iters), loss = 0.0995359
I1124 05:47:10.510027 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:47:10.510027 15792 solver.cpp:237]     Train net output #1: loss = 0.0995359 (* 1 = 0.0995359 loss)
I1124 05:47:10.510027 15792 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1124 05:47:18.362732 15792 solver.cpp:218] Iteration 10300 (12.7354 iter/s, 7.85212s/100 iters), loss = 0.072991
I1124 05:47:18.362732 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:47:18.362732 15792 solver.cpp:237]     Train net output #1: loss = 0.0729909 (* 1 = 0.0729909 loss)
I1124 05:47:18.362732 15792 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1124 05:47:26.217355 15792 solver.cpp:218] Iteration 10400 (12.7325 iter/s, 7.85389s/100 iters), loss = 0.0496106
I1124 05:47:26.217355 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:47:26.217355 15792 solver.cpp:237]     Train net output #1: loss = 0.0496106 (* 1 = 0.0496106 loss)
I1124 05:47:26.217355 15792 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1124 05:47:33.679903 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:47:33.988916 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_10500.caffemodel
I1124 05:47:34.027922 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_10500.solverstate
I1124 05:47:34.047929 15792 solver.cpp:330] Iteration 10500, Testing net (#0)
I1124 05:47:34.047929 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:47:36.133064 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:47:36.217066 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9178
I1124 05:47:36.217066 15792 solver.cpp:397]     Test net output #1: loss = 0.251037 (* 1 = 0.251037 loss)
I1124 05:47:36.294071 15792 solver.cpp:218] Iteration 10500 (9.92429 iter/s, 10.0763s/100 iters), loss = 0.0776667
I1124 05:47:36.294071 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:47:36.294071 15792 solver.cpp:237]     Train net output #1: loss = 0.0776666 (* 1 = 0.0776666 loss)
I1124 05:47:36.294071 15792 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1124 05:47:44.149629 15792 solver.cpp:218] Iteration 10600 (12.7308 iter/s, 7.85499s/100 iters), loss = 0.108485
I1124 05:47:44.149629 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:47:44.149629 15792 solver.cpp:237]     Train net output #1: loss = 0.108485 (* 1 = 0.108485 loss)
I1124 05:47:44.149629 15792 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1124 05:47:52.001304 15792 solver.cpp:218] Iteration 10700 (12.7375 iter/s, 7.85084s/100 iters), loss = 0.063852
I1124 05:47:52.001304 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:47:52.001304 15792 solver.cpp:237]     Train net output #1: loss = 0.0638519 (* 1 = 0.0638519 loss)
I1124 05:47:52.001304 15792 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1124 05:47:59.859882 15792 solver.cpp:218] Iteration 10800 (12.7243 iter/s, 7.85896s/100 iters), loss = 0.091128
I1124 05:47:59.860883 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:47:59.860883 15792 solver.cpp:237]     Train net output #1: loss = 0.091128 (* 1 = 0.091128 loss)
I1124 05:47:59.860883 15792 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1124 05:48:07.720547 15792 solver.cpp:218] Iteration 10900 (12.7237 iter/s, 7.85933s/100 iters), loss = 0.0418425
I1124 05:48:07.720547 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:48:07.720547 15792 solver.cpp:237]     Train net output #1: loss = 0.0418424 (* 1 = 0.0418424 loss)
I1124 05:48:07.720547 15792 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1124 05:48:15.185107 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:48:15.497141 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_11000.caffemodel
I1124 05:48:15.538141 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_11000.solverstate
I1124 05:48:15.557644 15792 solver.cpp:330] Iteration 11000, Testing net (#0)
I1124 05:48:15.557644 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:48:17.643275 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:48:17.727277 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9167
I1124 05:48:17.728278 15792 solver.cpp:397]     Test net output #1: loss = 0.250245 (* 1 = 0.250245 loss)
I1124 05:48:17.804281 15792 solver.cpp:218] Iteration 11000 (9.91694 iter/s, 10.0838s/100 iters), loss = 0.0578029
I1124 05:48:17.804281 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:48:17.804281 15792 solver.cpp:237]     Train net output #1: loss = 0.0578029 (* 1 = 0.0578029 loss)
I1124 05:48:17.804281 15792 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1124 05:48:25.663779 15792 solver.cpp:218] Iteration 11100 (12.7248 iter/s, 7.85865s/100 iters), loss = 0.117629
I1124 05:48:25.663779 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:48:25.663779 15792 solver.cpp:237]     Train net output #1: loss = 0.117629 (* 1 = 0.117629 loss)
I1124 05:48:25.663779 15792 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1124 05:48:33.523898 15792 solver.cpp:218] Iteration 11200 (12.7234 iter/s, 7.85951s/100 iters), loss = 0.0733897
I1124 05:48:33.523898 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:48:33.523898 15792 solver.cpp:237]     Train net output #1: loss = 0.0733897 (* 1 = 0.0733897 loss)
I1124 05:48:33.523898 15792 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1124 05:48:41.465510 15792 solver.cpp:218] Iteration 11300 (12.5921 iter/s, 7.94151s/100 iters), loss = 0.0763931
I1124 05:48:41.465510 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:48:41.465510 15792 solver.cpp:237]     Train net output #1: loss = 0.076393 (* 1 = 0.076393 loss)
I1124 05:48:41.465510 15792 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1124 05:48:49.444363 15792 solver.cpp:218] Iteration 11400 (12.534 iter/s, 7.97829s/100 iters), loss = 0.0507041
I1124 05:48:49.444363 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:48:49.444363 15792 solver.cpp:237]     Train net output #1: loss = 0.0507041 (* 1 = 0.0507041 loss)
I1124 05:48:49.444363 15792 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1124 05:48:57.016501 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:48:57.335518 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_11500.caffemodel
I1124 05:48:57.374521 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_11500.solverstate
I1124 05:48:57.393534 15792 solver.cpp:330] Iteration 11500, Testing net (#0)
I1124 05:48:57.393534 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:48:59.574712 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:48:59.658718 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9173
I1124 05:48:59.658718 15792 solver.cpp:397]     Test net output #1: loss = 0.250253 (* 1 = 0.250253 loss)
I1124 05:48:59.735723 15792 solver.cpp:218] Iteration 11500 (9.7178 iter/s, 10.2904s/100 iters), loss = 0.102719
I1124 05:48:59.735723 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:48:59.735723 15792 solver.cpp:237]     Train net output #1: loss = 0.102719 (* 1 = 0.102719 loss)
I1124 05:48:59.735723 15792 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1124 05:49:07.606734 15792 solver.cpp:218] Iteration 11600 (12.706 iter/s, 7.87031s/100 iters), loss = 0.0979799
I1124 05:49:07.606734 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:49:07.606734 15792 solver.cpp:237]     Train net output #1: loss = 0.0979798 (* 1 = 0.0979798 loss)
I1124 05:49:07.606734 15792 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1124 05:49:15.564034 15792 solver.cpp:218] Iteration 11700 (12.5676 iter/s, 7.95696s/100 iters), loss = 0.105451
I1124 05:49:15.564034 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:49:15.564034 15792 solver.cpp:237]     Train net output #1: loss = 0.105451 (* 1 = 0.105451 loss)
I1124 05:49:15.564034 15792 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1124 05:49:23.444707 15792 solver.cpp:218] Iteration 11800 (12.6893 iter/s, 7.88066s/100 iters), loss = 0.0715053
I1124 05:49:23.444707 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:49:23.444707 15792 solver.cpp:237]     Train net output #1: loss = 0.0715052 (* 1 = 0.0715052 loss)
I1124 05:49:23.444707 15792 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1124 05:49:31.417013 15792 solver.cpp:218] Iteration 11900 (12.5441 iter/s, 7.97189s/100 iters), loss = 0.0607622
I1124 05:49:31.417013 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:49:31.417013 15792 solver.cpp:237]     Train net output #1: loss = 0.0607621 (* 1 = 0.0607621 loss)
I1124 05:49:31.417013 15792 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1124 05:49:38.975561 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:49:39.288609 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_12000.caffemodel
I1124 05:49:39.328614 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_12000.solverstate
I1124 05:49:39.349617 15792 solver.cpp:330] Iteration 12000, Testing net (#0)
I1124 05:49:39.349617 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:49:41.482955 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:49:41.568960 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1124 05:49:41.568960 15792 solver.cpp:397]     Test net output #1: loss = 0.249925 (* 1 = 0.249925 loss)
I1124 05:49:41.648465 15792 solver.cpp:218] Iteration 12000 (9.7748 iter/s, 10.2304s/100 iters), loss = 0.111827
I1124 05:49:41.648465 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 05:49:41.648465 15792 solver.cpp:237]     Train net output #1: loss = 0.111827 (* 1 = 0.111827 loss)
I1124 05:49:41.648465 15792 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1124 05:49:49.750784 15792 solver.cpp:218] Iteration 12100 (12.343 iter/s, 8.10174s/100 iters), loss = 0.0857918
I1124 05:49:49.750784 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:49:49.750784 15792 solver.cpp:237]     Train net output #1: loss = 0.0857917 (* 1 = 0.0857917 loss)
I1124 05:49:49.750784 15792 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1124 05:49:57.727429 15792 solver.cpp:218] Iteration 12200 (12.5373 iter/s, 7.97623s/100 iters), loss = 0.0731808
I1124 05:49:57.727429 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:49:57.727429 15792 solver.cpp:237]     Train net output #1: loss = 0.0731807 (* 1 = 0.0731807 loss)
I1124 05:49:57.727429 15792 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1124 05:50:05.668345 15792 solver.cpp:218] Iteration 12300 (12.5932 iter/s, 7.94082s/100 iters), loss = 0.0391513
I1124 05:50:05.669353 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:50:05.669353 15792 solver.cpp:237]     Train net output #1: loss = 0.0391512 (* 1 = 0.0391512 loss)
I1124 05:50:05.669353 15792 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1124 05:50:13.581712 15792 solver.cpp:218] Iteration 12400 (12.6392 iter/s, 7.91187s/100 iters), loss = 0.027836
I1124 05:50:13.581712 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:50:13.581712 15792 solver.cpp:237]     Train net output #1: loss = 0.0278359 (* 1 = 0.0278359 loss)
I1124 05:50:13.581712 15792 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1124 05:50:21.083814 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:50:21.394913 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_12500.caffemodel
I1124 05:50:21.437911 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_12500.solverstate
I1124 05:50:21.457940 15792 solver.cpp:330] Iteration 12500, Testing net (#0)
I1124 05:50:21.457940 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:50:23.542129 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:50:23.627132 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1124 05:50:23.627132 15792 solver.cpp:397]     Test net output #1: loss = 0.250179 (* 1 = 0.250179 loss)
I1124 05:50:23.703136 15792 solver.cpp:218] Iteration 12500 (9.87985 iter/s, 10.1216s/100 iters), loss = 0.0647627
I1124 05:50:23.703136 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:50:23.703136 15792 solver.cpp:237]     Train net output #1: loss = 0.0647626 (* 1 = 0.0647626 loss)
I1124 05:50:23.703136 15792 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1124 05:50:31.572896 15792 solver.cpp:218] Iteration 12600 (12.7083 iter/s, 7.86887s/100 iters), loss = 0.111545
I1124 05:50:31.572896 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:50:31.572896 15792 solver.cpp:237]     Train net output #1: loss = 0.111545 (* 1 = 0.111545 loss)
I1124 05:50:31.572896 15792 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1124 05:50:39.541769 15792 solver.cpp:218] Iteration 12700 (12.5497 iter/s, 7.96833s/100 iters), loss = 0.0604648
I1124 05:50:39.541769 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:50:39.541769 15792 solver.cpp:237]     Train net output #1: loss = 0.0604647 (* 1 = 0.0604647 loss)
I1124 05:50:39.541769 15792 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1124 05:50:47.406595 15792 solver.cpp:218] Iteration 12800 (12.7149 iter/s, 7.8648s/100 iters), loss = 0.0634173
I1124 05:50:47.407596 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:50:47.407596 15792 solver.cpp:237]     Train net output #1: loss = 0.0634172 (* 1 = 0.0634172 loss)
I1124 05:50:47.407596 15792 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1124 05:50:55.270264 15792 solver.cpp:218] Iteration 12900 (12.7189 iter/s, 7.86229s/100 iters), loss = 0.0502163
I1124 05:50:55.270264 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:50:55.270264 15792 solver.cpp:237]     Train net output #1: loss = 0.0502162 (* 1 = 0.0502162 loss)
I1124 05:50:55.270264 15792 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1124 05:51:02.743907 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:51:03.053947 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_13000.caffemodel
I1124 05:51:03.094475 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_13000.solverstate
I1124 05:51:03.113976 15792 solver.cpp:330] Iteration 13000, Testing net (#0)
I1124 05:51:03.114476 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:51:05.201596 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:51:05.286101 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1124 05:51:05.286101 15792 solver.cpp:397]     Test net output #1: loss = 0.249778 (* 1 = 0.249778 loss)
I1124 05:51:05.362105 15792 solver.cpp:218] Iteration 13000 (9.90897 iter/s, 10.0919s/100 iters), loss = 0.0827969
I1124 05:51:05.362105 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:51:05.362105 15792 solver.cpp:237]     Train net output #1: loss = 0.0827969 (* 1 = 0.0827969 loss)
I1124 05:51:05.362105 15792 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1124 05:51:13.221180 15792 solver.cpp:218] Iteration 13100 (12.7255 iter/s, 7.85821s/100 iters), loss = 0.0864034
I1124 05:51:13.221180 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:51:13.221180 15792 solver.cpp:237]     Train net output #1: loss = 0.0864033 (* 1 = 0.0864033 loss)
I1124 05:51:13.221180 15792 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1124 05:51:21.084035 15792 solver.cpp:218] Iteration 13200 (12.7178 iter/s, 7.86299s/100 iters), loss = 0.0592085
I1124 05:51:21.084035 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:51:21.084035 15792 solver.cpp:237]     Train net output #1: loss = 0.0592084 (* 1 = 0.0592084 loss)
I1124 05:51:21.085036 15792 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1124 05:51:28.942418 15792 solver.cpp:218] Iteration 13300 (12.7275 iter/s, 7.85703s/100 iters), loss = 0.0617042
I1124 05:51:28.942418 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:51:28.942418 15792 solver.cpp:237]     Train net output #1: loss = 0.0617042 (* 1 = 0.0617042 loss)
I1124 05:51:28.942418 15792 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1124 05:51:36.798606 15792 solver.cpp:218] Iteration 13400 (12.729 iter/s, 7.85611s/100 iters), loss = 0.0295188
I1124 05:51:36.798606 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:51:36.798606 15792 solver.cpp:237]     Train net output #1: loss = 0.0295187 (* 1 = 0.0295187 loss)
I1124 05:51:36.798606 15792 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1124 05:51:44.265655 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:51:44.574668 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_13500.caffemodel
I1124 05:51:44.615674 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_13500.solverstate
I1124 05:51:44.635174 15792 solver.cpp:330] Iteration 13500, Testing net (#0)
I1124 05:51:44.635174 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:51:46.722795 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:51:46.806797 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9175
I1124 05:51:46.806797 15792 solver.cpp:397]     Test net output #1: loss = 0.250828 (* 1 = 0.250828 loss)
I1124 05:51:46.883797 15792 solver.cpp:218] Iteration 13500 (9.91597 iter/s, 10.0847s/100 iters), loss = 0.0710471
I1124 05:51:46.883797 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:51:46.883797 15792 solver.cpp:237]     Train net output #1: loss = 0.071047 (* 1 = 0.071047 loss)
I1124 05:51:46.883797 15792 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1124 05:51:54.742377 15792 solver.cpp:218] Iteration 13600 (12.7254 iter/s, 7.85827s/100 iters), loss = 0.104804
I1124 05:51:54.742377 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:51:54.742377 15792 solver.cpp:237]     Train net output #1: loss = 0.104804 (* 1 = 0.104804 loss)
I1124 05:51:54.742377 15792 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1124 05:52:02.600667 15792 solver.cpp:218] Iteration 13700 (12.7274 iter/s, 7.85703s/100 iters), loss = 0.0633524
I1124 05:52:02.600667 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:52:02.600667 15792 solver.cpp:237]     Train net output #1: loss = 0.0633523 (* 1 = 0.0633523 loss)
I1124 05:52:02.600667 15792 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1124 05:52:10.466745 15792 solver.cpp:218] Iteration 13800 (12.7125 iter/s, 7.86629s/100 iters), loss = 0.0701808
I1124 05:52:10.466745 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:52:10.466745 15792 solver.cpp:237]     Train net output #1: loss = 0.0701806 (* 1 = 0.0701806 loss)
I1124 05:52:10.466745 15792 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1124 05:52:18.370487 15792 solver.cpp:218] Iteration 13900 (12.653 iter/s, 7.90323s/100 iters), loss = 0.0311546
I1124 05:52:18.370487 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:52:18.370487 15792 solver.cpp:237]     Train net output #1: loss = 0.0311544 (* 1 = 0.0311544 loss)
I1124 05:52:18.370487 15792 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1124 05:52:25.906618 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:52:26.225656 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_14000.caffemodel
I1124 05:52:26.265164 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_14000.solverstate
I1124 05:52:26.285665 15792 solver.cpp:330] Iteration 14000, Testing net (#0)
I1124 05:52:26.285665 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:52:28.401319 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:52:28.485327 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9175
I1124 05:52:28.485327 15792 solver.cpp:397]     Test net output #1: loss = 0.250666 (* 1 = 0.250666 loss)
I1124 05:52:28.562326 15792 solver.cpp:218] Iteration 14000 (9.81279 iter/s, 10.1908s/100 iters), loss = 0.0589855
I1124 05:52:28.562326 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:52:28.562326 15792 solver.cpp:237]     Train net output #1: loss = 0.0589853 (* 1 = 0.0589853 loss)
I1124 05:52:28.562326 15792 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1124 05:52:36.449971 15792 solver.cpp:218] Iteration 14100 (12.6782 iter/s, 7.88753s/100 iters), loss = 0.101015
I1124 05:52:36.449971 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 05:52:36.449971 15792 solver.cpp:237]     Train net output #1: loss = 0.101015 (* 1 = 0.101015 loss)
I1124 05:52:36.449971 15792 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1124 05:52:44.454707 15792 solver.cpp:218] Iteration 14200 (12.494 iter/s, 8.00384s/100 iters), loss = 0.0658918
I1124 05:52:44.454707 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:52:44.454707 15792 solver.cpp:237]     Train net output #1: loss = 0.0658917 (* 1 = 0.0658917 loss)
I1124 05:52:44.454707 15792 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1124 05:52:52.310294 15792 solver.cpp:218] Iteration 14300 (12.7301 iter/s, 7.85537s/100 iters), loss = 0.045723
I1124 05:52:52.310294 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:52:52.310294 15792 solver.cpp:237]     Train net output #1: loss = 0.0457228 (* 1 = 0.0457228 loss)
I1124 05:52:52.310294 15792 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1124 05:53:00.165874 15792 solver.cpp:218] Iteration 14400 (12.7303 iter/s, 7.85526s/100 iters), loss = 0.0312647
I1124 05:53:00.165874 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:53:00.165874 15792 solver.cpp:237]     Train net output #1: loss = 0.0312646 (* 1 = 0.0312646 loss)
I1124 05:53:00.165874 15792 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1124 05:53:07.633568 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:53:07.941583 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_14500.caffemodel
I1124 05:53:07.982583 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_14500.solverstate
I1124 05:53:08.003628 15792 solver.cpp:330] Iteration 14500, Testing net (#0)
I1124 05:53:08.003628 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:53:10.106758 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:53:10.192266 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1124 05:53:10.192266 15792 solver.cpp:397]     Test net output #1: loss = 0.250189 (* 1 = 0.250189 loss)
I1124 05:53:10.269785 15792 solver.cpp:218] Iteration 14500 (9.89841 iter/s, 10.1026s/100 iters), loss = 0.0717406
I1124 05:53:10.269785 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:53:10.269785 15792 solver.cpp:237]     Train net output #1: loss = 0.0717404 (* 1 = 0.0717404 loss)
I1124 05:53:10.269785 15792 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1124 05:53:18.259341 15792 solver.cpp:218] Iteration 14600 (12.5159 iter/s, 7.98981s/100 iters), loss = 0.0775585
I1124 05:53:18.259341 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:53:18.259341 15792 solver.cpp:237]     Train net output #1: loss = 0.0775583 (* 1 = 0.0775583 loss)
I1124 05:53:18.259341 15792 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1124 05:53:26.320205 15792 solver.cpp:218] Iteration 14700 (12.4063 iter/s, 8.06042s/100 iters), loss = 0.0968369
I1124 05:53:26.320205 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:53:26.320205 15792 solver.cpp:237]     Train net output #1: loss = 0.0968368 (* 1 = 0.0968368 loss)
I1124 05:53:26.321207 15792 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1124 05:53:34.177914 15792 solver.cpp:218] Iteration 14800 (12.7276 iter/s, 7.85692s/100 iters), loss = 0.0571118
I1124 05:53:34.177914 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:53:34.177914 15792 solver.cpp:237]     Train net output #1: loss = 0.0571116 (* 1 = 0.0571116 loss)
I1124 05:53:34.177914 15792 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1124 05:53:42.032927 15792 solver.cpp:218] Iteration 14900 (12.732 iter/s, 7.85421s/100 iters), loss = 0.0466628
I1124 05:53:42.032927 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:53:42.032927 15792 solver.cpp:237]     Train net output #1: loss = 0.0466626 (* 1 = 0.0466626 loss)
I1124 05:53:42.033427 15792 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1124 05:53:49.612871 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:53:49.924896 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_15000.caffemodel
I1124 05:53:49.965906 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_15000.solverstate
I1124 05:53:49.986902 15792 solver.cpp:330] Iteration 15000, Testing net (#0)
I1124 05:53:49.986902 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:53:52.076031 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:53:52.161038 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9175
I1124 05:53:52.161038 15792 solver.cpp:397]     Test net output #1: loss = 0.248734 (* 1 = 0.248734 loss)
I1124 05:53:52.237041 15792 solver.cpp:218] Iteration 15000 (9.80024 iter/s, 10.2038s/100 iters), loss = 0.0664176
I1124 05:53:52.237041 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:53:52.237041 15792 solver.cpp:237]     Train net output #1: loss = 0.0664174 (* 1 = 0.0664174 loss)
I1124 05:53:52.237041 15792 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1124 05:54:00.104771 15792 solver.cpp:218] Iteration 15100 (12.712 iter/s, 7.86656s/100 iters), loss = 0.0640735
I1124 05:54:00.104771 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:54:00.104771 15792 solver.cpp:237]     Train net output #1: loss = 0.0640734 (* 1 = 0.0640734 loss)
I1124 05:54:00.104771 15792 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1124 05:54:07.968500 15792 solver.cpp:218] Iteration 15200 (12.7176 iter/s, 7.86313s/100 iters), loss = 0.0626557
I1124 05:54:07.968500 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:54:07.968500 15792 solver.cpp:237]     Train net output #1: loss = 0.0626555 (* 1 = 0.0626555 loss)
I1124 05:54:07.968500 15792 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1124 05:54:15.828073 15792 solver.cpp:218] Iteration 15300 (12.7238 iter/s, 7.85926s/100 iters), loss = 0.0479764
I1124 05:54:15.828073 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:54:15.828073 15792 solver.cpp:237]     Train net output #1: loss = 0.0479763 (* 1 = 0.0479763 loss)
I1124 05:54:15.828073 15792 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1124 05:54:15.828073 15792 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1124 05:54:23.790647 15792 solver.cpp:218] Iteration 15400 (12.5594 iter/s, 7.96216s/100 iters), loss = 0.0520726
I1124 05:54:23.790647 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:54:23.790647 15792 solver.cpp:237]     Train net output #1: loss = 0.0520725 (* 1 = 0.0520725 loss)
I1124 05:54:23.790647 15792 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1124 05:54:31.428220 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:54:31.736812 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_15500.caffemodel
I1124 05:54:31.776834 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_15500.solverstate
I1124 05:54:31.796850 15792 solver.cpp:330] Iteration 15500, Testing net (#0)
I1124 05:54:31.796850 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:54:33.902487 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:54:33.986490 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1124 05:54:33.986490 15792 solver.cpp:397]     Test net output #1: loss = 0.249197 (* 1 = 0.249197 loss)
I1124 05:54:34.063493 15792 solver.cpp:218] Iteration 15500 (9.73451 iter/s, 10.2727s/100 iters), loss = 0.053478
I1124 05:54:34.063493 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:54:34.063493 15792 solver.cpp:237]     Train net output #1: loss = 0.0534779 (* 1 = 0.0534779 loss)
I1124 05:54:34.063493 15792 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1124 05:54:42.001875 15792 solver.cpp:218] Iteration 15600 (12.5989 iter/s, 7.93721s/100 iters), loss = 0.0762642
I1124 05:54:42.001875 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:54:42.001875 15792 solver.cpp:237]     Train net output #1: loss = 0.076264 (* 1 = 0.076264 loss)
I1124 05:54:42.001875 15792 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1124 05:54:49.963263 15792 solver.cpp:218] Iteration 15700 (12.5617 iter/s, 7.96073s/100 iters), loss = 0.0508309
I1124 05:54:49.963263 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:54:49.963263 15792 solver.cpp:237]     Train net output #1: loss = 0.0508308 (* 1 = 0.0508308 loss)
I1124 05:54:49.963263 15792 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1124 05:54:57.989991 15792 solver.cpp:218] Iteration 15800 (12.4588 iter/s, 8.02648s/100 iters), loss = 0.0559666
I1124 05:54:57.989991 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:54:57.989991 15792 solver.cpp:237]     Train net output #1: loss = 0.0559665 (* 1 = 0.0559665 loss)
I1124 05:54:57.989991 15792 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1124 05:55:05.963783 15792 solver.cpp:218] Iteration 15900 (12.5422 iter/s, 7.97306s/100 iters), loss = 0.041769
I1124 05:55:05.963783 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:55:05.963783 15792 solver.cpp:237]     Train net output #1: loss = 0.0417689 (* 1 = 0.0417689 loss)
I1124 05:55:05.963783 15792 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1124 05:55:13.432606 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:55:13.741631 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_16000.caffemodel
I1124 05:55:13.782621 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_16000.solverstate
I1124 05:55:13.802124 15792 solver.cpp:330] Iteration 16000, Testing net (#0)
I1124 05:55:13.802124 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:55:15.888741 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:55:15.972745 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9188
I1124 05:55:15.972745 15792 solver.cpp:397]     Test net output #1: loss = 0.249302 (* 1 = 0.249302 loss)
I1124 05:55:16.048749 15792 solver.cpp:218] Iteration 16000 (9.91579 iter/s, 10.0849s/100 iters), loss = 0.0590668
I1124 05:55:16.048749 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:55:16.048749 15792 solver.cpp:237]     Train net output #1: loss = 0.0590667 (* 1 = 0.0590667 loss)
I1124 05:55:16.048749 15792 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1124 05:55:23.904356 15792 solver.cpp:218] Iteration 16100 (12.7316 iter/s, 7.85448s/100 iters), loss = 0.0731176
I1124 05:55:23.904356 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:55:23.904356 15792 solver.cpp:237]     Train net output #1: loss = 0.0731175 (* 1 = 0.0731175 loss)
I1124 05:55:23.904356 15792 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1124 05:55:31.859988 15792 solver.cpp:218] Iteration 16200 (12.5704 iter/s, 7.9552s/100 iters), loss = 0.0537489
I1124 05:55:31.859988 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:55:31.859988 15792 solver.cpp:237]     Train net output #1: loss = 0.0537488 (* 1 = 0.0537488 loss)
I1124 05:55:31.859988 15792 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1124 05:55:39.711733 15792 solver.cpp:218] Iteration 16300 (12.7362 iter/s, 7.85161s/100 iters), loss = 0.0822215
I1124 05:55:39.712234 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:55:39.712234 15792 solver.cpp:237]     Train net output #1: loss = 0.0822214 (* 1 = 0.0822214 loss)
I1124 05:55:39.712234 15792 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1124 05:55:47.617336 15792 solver.cpp:218] Iteration 16400 (12.6507 iter/s, 7.90469s/100 iters), loss = 0.0271721
I1124 05:55:47.617336 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:55:47.617336 15792 solver.cpp:237]     Train net output #1: loss = 0.0271719 (* 1 = 0.0271719 loss)
I1124 05:55:47.617336 15792 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1124 05:55:55.132081 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:55:55.442113 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_16500.caffemodel
I1124 05:55:55.482115 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_16500.solverstate
I1124 05:55:55.501622 15792 solver.cpp:330] Iteration 16500, Testing net (#0)
I1124 05:55:55.501622 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:55:57.607261 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:55:57.694268 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9186
I1124 05:55:57.694268 15792 solver.cpp:397]     Test net output #1: loss = 0.249356 (* 1 = 0.249356 loss)
I1124 05:55:57.770272 15792 solver.cpp:218] Iteration 16500 (9.84921 iter/s, 10.1531s/100 iters), loss = 0.0652913
I1124 05:55:57.771272 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:55:57.771272 15792 solver.cpp:237]     Train net output #1: loss = 0.0652911 (* 1 = 0.0652911 loss)
I1124 05:55:57.771272 15792 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1124 05:56:05.743878 15792 solver.cpp:218] Iteration 16600 (12.5427 iter/s, 7.97276s/100 iters), loss = 0.0674888
I1124 05:56:05.743878 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:56:05.743878 15792 solver.cpp:237]     Train net output #1: loss = 0.0674887 (* 1 = 0.0674887 loss)
I1124 05:56:05.743878 15792 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1124 05:56:13.783875 15792 solver.cpp:218] Iteration 16700 (12.4386 iter/s, 8.03949s/100 iters), loss = 0.0775775
I1124 05:56:13.783875 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:56:13.783875 15792 solver.cpp:237]     Train net output #1: loss = 0.0775774 (* 1 = 0.0775774 loss)
I1124 05:56:13.783875 15792 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1124 05:56:21.729779 15792 solver.cpp:218] Iteration 16800 (12.5855 iter/s, 7.94564s/100 iters), loss = 0.0522688
I1124 05:56:21.729779 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:56:21.729779 15792 solver.cpp:237]     Train net output #1: loss = 0.0522687 (* 1 = 0.0522687 loss)
I1124 05:56:21.729779 15792 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1124 05:56:29.678385 15792 solver.cpp:218] Iteration 16900 (12.5823 iter/s, 7.94764s/100 iters), loss = 0.0377874
I1124 05:56:29.678385 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:56:29.678385 15792 solver.cpp:237]     Train net output #1: loss = 0.0377872 (* 1 = 0.0377872 loss)
I1124 05:56:29.678385 15792 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1124 05:56:37.144002 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:56:37.454519 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_17000.caffemodel
I1124 05:56:37.496518 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_17000.solverstate
I1124 05:56:37.516516 15792 solver.cpp:330] Iteration 17000, Testing net (#0)
I1124 05:56:37.516516 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:56:39.601234 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:56:39.685240 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1124 05:56:39.685240 15792 solver.cpp:397]     Test net output #1: loss = 0.249509 (* 1 = 0.249509 loss)
I1124 05:56:39.762240 15792 solver.cpp:218] Iteration 17000 (9.9172 iter/s, 10.0835s/100 iters), loss = 0.0523542
I1124 05:56:39.762240 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:56:39.762240 15792 solver.cpp:237]     Train net output #1: loss = 0.052354 (* 1 = 0.052354 loss)
I1124 05:56:39.762240 15792 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1124 05:56:47.722750 15792 solver.cpp:218] Iteration 17100 (12.5626 iter/s, 7.96013s/100 iters), loss = 0.0758467
I1124 05:56:47.722750 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:56:47.722750 15792 solver.cpp:237]     Train net output #1: loss = 0.0758465 (* 1 = 0.0758465 loss)
I1124 05:56:47.722750 15792 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1124 05:56:55.655297 15792 solver.cpp:218] Iteration 17200 (12.6066 iter/s, 7.93236s/100 iters), loss = 0.0513356
I1124 05:56:55.655297 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:56:55.655297 15792 solver.cpp:237]     Train net output #1: loss = 0.0513355 (* 1 = 0.0513355 loss)
I1124 05:56:55.655297 15792 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1124 05:57:03.582440 15792 solver.cpp:218] Iteration 17300 (12.617 iter/s, 7.92585s/100 iters), loss = 0.0606251
I1124 05:57:03.582440 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:57:03.582440 15792 solver.cpp:237]     Train net output #1: loss = 0.0606249 (* 1 = 0.0606249 loss)
I1124 05:57:03.582440 15792 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1124 05:57:11.477710 15792 solver.cpp:218] Iteration 17400 (12.6655 iter/s, 7.89544s/100 iters), loss = 0.0796687
I1124 05:57:11.477710 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:57:11.477710 15792 solver.cpp:237]     Train net output #1: loss = 0.0796685 (* 1 = 0.0796685 loss)
I1124 05:57:11.477710 15792 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1124 05:57:18.943171 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:57:19.256194 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_17500.caffemodel
I1124 05:57:19.296195 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_17500.solverstate
I1124 05:57:19.316700 15792 solver.cpp:330] Iteration 17500, Testing net (#0)
I1124 05:57:19.316700 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:57:21.403291 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:57:21.487300 15792 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1124 05:57:21.487300 15792 solver.cpp:397]     Test net output #1: loss = 0.249514 (* 1 = 0.249514 loss)
I1124 05:57:21.563304 15792 solver.cpp:218] Iteration 17500 (9.91544 iter/s, 10.0853s/100 iters), loss = 0.0618296
I1124 05:57:21.563304 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:57:21.563304 15792 solver.cpp:237]     Train net output #1: loss = 0.0618294 (* 1 = 0.0618294 loss)
I1124 05:57:21.563304 15792 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1124 05:57:29.410974 15792 solver.cpp:218] Iteration 17600 (12.7436 iter/s, 7.84711s/100 iters), loss = 0.047402
I1124 05:57:29.410974 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:57:29.410974 15792 solver.cpp:237]     Train net output #1: loss = 0.0474018 (* 1 = 0.0474018 loss)
I1124 05:57:29.410974 15792 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1124 05:57:37.261613 15792 solver.cpp:218] Iteration 17700 (12.7389 iter/s, 7.85s/100 iters), loss = 0.0744152
I1124 05:57:37.261613 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 05:57:37.261613 15792 solver.cpp:237]     Train net output #1: loss = 0.0744151 (* 1 = 0.0744151 loss)
I1124 05:57:37.261613 15792 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1124 05:57:45.115237 15792 solver.cpp:218] Iteration 17800 (12.7335 iter/s, 7.85331s/100 iters), loss = 0.0455062
I1124 05:57:45.115237 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:57:45.115237 15792 solver.cpp:237]     Train net output #1: loss = 0.0455061 (* 1 = 0.0455061 loss)
I1124 05:57:45.115237 15792 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1124 05:57:52.966955 15792 solver.cpp:218] Iteration 17900 (12.7374 iter/s, 7.85089s/100 iters), loss = 0.022105
I1124 05:57:52.966955 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:57:52.966955 15792 solver.cpp:237]     Train net output #1: loss = 0.0221048 (* 1 = 0.0221048 loss)
I1124 05:57:52.966955 15792 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1124 05:58:00.435551 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:58:00.744570 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_18000.caffemodel
I1124 05:58:00.783569 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_18000.solverstate
I1124 05:58:00.803072 15792 solver.cpp:330] Iteration 18000, Testing net (#0)
I1124 05:58:00.803072 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:58:02.888697 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:58:02.972707 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9185
I1124 05:58:02.972707 15792 solver.cpp:397]     Test net output #1: loss = 0.249905 (* 1 = 0.249905 loss)
I1124 05:58:03.049706 15792 solver.cpp:218] Iteration 18000 (9.91879 iter/s, 10.0819s/100 iters), loss = 0.0490733
I1124 05:58:03.049706 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:58:03.049706 15792 solver.cpp:237]     Train net output #1: loss = 0.0490732 (* 1 = 0.0490732 loss)
I1124 05:58:03.049706 15792 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1124 05:58:10.899420 15792 solver.cpp:218] Iteration 18100 (12.7391 iter/s, 7.84987s/100 iters), loss = 0.0667109
I1124 05:58:10.900419 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:58:10.900419 15792 solver.cpp:237]     Train net output #1: loss = 0.0667107 (* 1 = 0.0667107 loss)
I1124 05:58:10.900419 15792 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1124 05:58:18.748911 15792 solver.cpp:218] Iteration 18200 (12.7405 iter/s, 7.84897s/100 iters), loss = 0.0491076
I1124 05:58:18.748911 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:58:18.748911 15792 solver.cpp:237]     Train net output #1: loss = 0.0491075 (* 1 = 0.0491075 loss)
I1124 05:58:18.748911 15792 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1124 05:58:26.600029 15792 solver.cpp:218] Iteration 18300 (12.7392 iter/s, 7.84976s/100 iters), loss = 0.0582905
I1124 05:58:26.600029 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:58:26.600029 15792 solver.cpp:237]     Train net output #1: loss = 0.0582903 (* 1 = 0.0582903 loss)
I1124 05:58:26.600029 15792 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1124 05:58:34.449049 15792 solver.cpp:218] Iteration 18400 (12.7404 iter/s, 7.84907s/100 iters), loss = 0.0388082
I1124 05:58:34.449049 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:58:34.449049 15792 solver.cpp:237]     Train net output #1: loss = 0.0388081 (* 1 = 0.0388081 loss)
I1124 05:58:34.449049 15792 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1124 05:58:41.909667 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:58:42.219686 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_18500.caffemodel
I1124 05:58:42.261695 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_18500.solverstate
I1124 05:58:42.280685 15792 solver.cpp:330] Iteration 18500, Testing net (#0)
I1124 05:58:42.280685 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:58:44.366860 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:58:44.450865 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1124 05:58:44.450865 15792 solver.cpp:397]     Test net output #1: loss = 0.2498 (* 1 = 0.2498 loss)
I1124 05:58:44.527871 15792 solver.cpp:218] Iteration 18500 (9.92264 iter/s, 10.078s/100 iters), loss = 0.04442
I1124 05:58:44.527871 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:58:44.527871 15792 solver.cpp:237]     Train net output #1: loss = 0.0444199 (* 1 = 0.0444199 loss)
I1124 05:58:44.527871 15792 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1124 05:58:52.384347 15792 solver.cpp:218] Iteration 18600 (12.729 iter/s, 7.8561s/100 iters), loss = 0.0695301
I1124 05:58:52.384347 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:58:52.384347 15792 solver.cpp:237]     Train net output #1: loss = 0.0695299 (* 1 = 0.0695299 loss)
I1124 05:58:52.384347 15792 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1124 05:59:00.238961 15792 solver.cpp:218] Iteration 18700 (12.7317 iter/s, 7.85439s/100 iters), loss = 0.0509147
I1124 05:59:00.238961 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:59:00.238961 15792 solver.cpp:237]     Train net output #1: loss = 0.0509145 (* 1 = 0.0509145 loss)
I1124 05:59:00.238961 15792 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1124 05:59:08.097564 15792 solver.cpp:218] Iteration 18800 (12.7264 iter/s, 7.85767s/100 iters), loss = 0.0506079
I1124 05:59:08.097564 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:59:08.097564 15792 solver.cpp:237]     Train net output #1: loss = 0.0506077 (* 1 = 0.0506077 loss)
I1124 05:59:08.097564 15792 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1124 05:59:15.953255 15792 solver.cpp:218] Iteration 18900 (12.7295 iter/s, 7.85574s/100 iters), loss = 0.0440409
I1124 05:59:15.953255 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:59:15.953255 15792 solver.cpp:237]     Train net output #1: loss = 0.0440408 (* 1 = 0.0440408 loss)
I1124 05:59:15.953255 15792 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1124 05:59:23.418711 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:59:23.729734 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_19000.caffemodel
I1124 05:59:23.769238 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_19000.solverstate
I1124 05:59:23.789238 15792 solver.cpp:330] Iteration 19000, Testing net (#0)
I1124 05:59:23.789238 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 05:59:25.875869 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 05:59:25.959874 15792 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1124 05:59:25.959874 15792 solver.cpp:397]     Test net output #1: loss = 0.249959 (* 1 = 0.249959 loss)
I1124 05:59:26.036877 15792 solver.cpp:218] Iteration 19000 (9.91796 iter/s, 10.0827s/100 iters), loss = 0.071886
I1124 05:59:26.036877 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:59:26.036877 15792 solver.cpp:237]     Train net output #1: loss = 0.0718858 (* 1 = 0.0718858 loss)
I1124 05:59:26.037379 15792 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1124 05:59:33.876209 15792 solver.cpp:218] Iteration 19100 (12.7572 iter/s, 7.8387s/100 iters), loss = 0.0684507
I1124 05:59:33.876209 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 05:59:33.876209 15792 solver.cpp:237]     Train net output #1: loss = 0.0684506 (* 1 = 0.0684506 loss)
I1124 05:59:33.876209 15792 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1124 05:59:41.732791 15792 solver.cpp:218] Iteration 19200 (12.7287 iter/s, 7.85624s/100 iters), loss = 0.0534188
I1124 05:59:41.732791 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 05:59:41.732791 15792 solver.cpp:237]     Train net output #1: loss = 0.0534186 (* 1 = 0.0534186 loss)
I1124 05:59:41.732791 15792 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1124 05:59:49.592151 15792 solver.cpp:218] Iteration 19300 (12.7248 iter/s, 7.85868s/100 iters), loss = 0.0459595
I1124 05:59:49.592151 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:59:49.592151 15792 solver.cpp:237]     Train net output #1: loss = 0.0459594 (* 1 = 0.0459594 loss)
I1124 05:59:49.592151 15792 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1124 05:59:57.450502 15792 solver.cpp:218] Iteration 19400 (12.7259 iter/s, 7.85798s/100 iters), loss = 0.030324
I1124 05:59:57.450502 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 05:59:57.450502 15792 solver.cpp:237]     Train net output #1: loss = 0.0303238 (* 1 = 0.0303238 loss)
I1124 05:59:57.450502 15792 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1124 06:00:04.947971 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:00:05.258988 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_19500.caffemodel
I1124 06:00:05.298987 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_19500.solverstate
I1124 06:00:05.319492 15792 solver.cpp:330] Iteration 19500, Testing net (#0)
I1124 06:00:05.319492 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:00:07.405102 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:00:07.489116 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1124 06:00:07.489116 15792 solver.cpp:397]     Test net output #1: loss = 0.249786 (* 1 = 0.249786 loss)
I1124 06:00:07.565105 15792 solver.cpp:218] Iteration 19500 (9.88676 iter/s, 10.1145s/100 iters), loss = 0.0650888
I1124 06:00:07.565105 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:00:07.565105 15792 solver.cpp:237]     Train net output #1: loss = 0.0650886 (* 1 = 0.0650886 loss)
I1124 06:00:07.565105 15792 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1124 06:00:07.566107 15792 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1124 06:00:15.424796 15792 solver.cpp:218] Iteration 19600 (12.7253 iter/s, 7.85838s/100 iters), loss = 0.0689553
I1124 06:00:15.424796 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:00:15.424796 15792 solver.cpp:237]     Train net output #1: loss = 0.0689551 (* 1 = 0.0689551 loss)
I1124 06:00:15.424796 15792 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1124 06:00:23.278471 15792 solver.cpp:218] Iteration 19700 (12.7335 iter/s, 7.85331s/100 iters), loss = 0.0597117
I1124 06:00:23.278471 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:00:23.278471 15792 solver.cpp:237]     Train net output #1: loss = 0.0597115 (* 1 = 0.0597115 loss)
I1124 06:00:23.278471 15792 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1124 06:00:31.131069 15792 solver.cpp:218] Iteration 19800 (12.7354 iter/s, 7.85214s/100 iters), loss = 0.0549537
I1124 06:00:31.131069 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:00:31.131069 15792 solver.cpp:237]     Train net output #1: loss = 0.0549535 (* 1 = 0.0549535 loss)
I1124 06:00:31.131069 15792 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1124 06:00:38.982777 15792 solver.cpp:218] Iteration 19900 (12.7363 iter/s, 7.85159s/100 iters), loss = 0.0294723
I1124 06:00:38.982777 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:00:38.982777 15792 solver.cpp:237]     Train net output #1: loss = 0.0294721 (* 1 = 0.0294721 loss)
I1124 06:00:38.982777 15792 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1124 06:00:46.449255 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:00:46.760311 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_20000.caffemodel
I1124 06:00:46.799311 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_20000.solverstate
I1124 06:00:46.819310 15792 solver.cpp:330] Iteration 20000, Testing net (#0)
I1124 06:00:46.819310 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:00:48.905618 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:00:48.990622 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1124 06:00:48.990622 15792 solver.cpp:397]     Test net output #1: loss = 0.249789 (* 1 = 0.249789 loss)
I1124 06:00:49.067626 15792 solver.cpp:218] Iteration 20000 (9.91693 iter/s, 10.0838s/100 iters), loss = 0.0627858
I1124 06:00:49.067626 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:00:49.067626 15792 solver.cpp:237]     Train net output #1: loss = 0.0627856 (* 1 = 0.0627856 loss)
I1124 06:00:49.067626 15792 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1124 06:00:56.918745 15792 solver.cpp:218] Iteration 20100 (12.738 iter/s, 7.85054s/100 iters), loss = 0.0426149
I1124 06:00:56.918745 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:00:56.918745 15792 solver.cpp:237]     Train net output #1: loss = 0.0426147 (* 1 = 0.0426147 loss)
I1124 06:00:56.918745 15792 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1124 06:01:04.768862 15792 solver.cpp:218] Iteration 20200 (12.7392 iter/s, 7.84978s/100 iters), loss = 0.0506752
I1124 06:01:04.768862 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:01:04.768862 15792 solver.cpp:237]     Train net output #1: loss = 0.050675 (* 1 = 0.050675 loss)
I1124 06:01:04.768862 15792 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1124 06:01:12.619796 15792 solver.cpp:218] Iteration 20300 (12.7371 iter/s, 7.85108s/100 iters), loss = 0.0460637
I1124 06:01:12.619796 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:01:12.619796 15792 solver.cpp:237]     Train net output #1: loss = 0.0460635 (* 1 = 0.0460635 loss)
I1124 06:01:12.619796 15792 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1124 06:01:20.475497 15792 solver.cpp:218] Iteration 20400 (12.7313 iter/s, 7.85467s/100 iters), loss = 0.0283183
I1124 06:01:20.475497 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:01:20.475497 15792 solver.cpp:237]     Train net output #1: loss = 0.0283181 (* 1 = 0.0283181 loss)
I1124 06:01:20.475497 15792 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1124 06:01:27.948160 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:01:28.260175 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_20500.caffemodel
I1124 06:01:28.298179 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_20500.solverstate
I1124 06:01:28.319183 15792 solver.cpp:330] Iteration 20500, Testing net (#0)
I1124 06:01:28.319183 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:01:30.408318 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:01:30.492321 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9192
I1124 06:01:30.492321 15792 solver.cpp:397]     Test net output #1: loss = 0.249652 (* 1 = 0.249652 loss)
I1124 06:01:30.568330 15792 solver.cpp:218] Iteration 20500 (9.90682 iter/s, 10.0941s/100 iters), loss = 0.0805053
I1124 06:01:30.568330 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:01:30.568330 15792 solver.cpp:237]     Train net output #1: loss = 0.0805051 (* 1 = 0.0805051 loss)
I1124 06:01:30.568330 15792 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1124 06:01:38.424345 15792 solver.cpp:218] Iteration 20600 (12.7312 iter/s, 7.85475s/100 iters), loss = 0.0961188
I1124 06:01:38.424345 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:01:38.424345 15792 solver.cpp:237]     Train net output #1: loss = 0.0961186 (* 1 = 0.0961186 loss)
I1124 06:01:38.424345 15792 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1124 06:01:46.285687 15792 solver.cpp:218] Iteration 20700 (12.7203 iter/s, 7.86143s/100 iters), loss = 0.0751424
I1124 06:01:46.285687 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:01:46.285687 15792 solver.cpp:237]     Train net output #1: loss = 0.0751423 (* 1 = 0.0751423 loss)
I1124 06:01:46.285687 15792 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1124 06:01:54.143895 15792 solver.cpp:218] Iteration 20800 (12.7276 iter/s, 7.85697s/100 iters), loss = 0.0643237
I1124 06:01:54.143895 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:01:54.143895 15792 solver.cpp:237]     Train net output #1: loss = 0.0643236 (* 1 = 0.0643236 loss)
I1124 06:01:54.143895 15792 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1124 06:02:02.004523 15792 solver.cpp:218] Iteration 20900 (12.7221 iter/s, 7.86034s/100 iters), loss = 0.0194045
I1124 06:02:02.004523 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:02:02.004523 15792 solver.cpp:237]     Train net output #1: loss = 0.0194043 (* 1 = 0.0194043 loss)
I1124 06:02:02.004523 15792 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1124 06:02:09.480036 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:02:09.792071 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_21000.caffemodel
I1124 06:02:09.833083 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_21000.solverstate
I1124 06:02:09.854087 15792 solver.cpp:330] Iteration 21000, Testing net (#0)
I1124 06:02:09.854087 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:02:11.941222 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:02:12.025228 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1124 06:02:12.026228 15792 solver.cpp:397]     Test net output #1: loss = 0.24973 (* 1 = 0.24973 loss)
I1124 06:02:12.102730 15792 solver.cpp:218] Iteration 21000 (9.90317 iter/s, 10.0978s/100 iters), loss = 0.0679672
I1124 06:02:12.103230 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:02:12.103230 15792 solver.cpp:237]     Train net output #1: loss = 0.067967 (* 1 = 0.067967 loss)
I1124 06:02:12.103230 15792 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1124 06:02:19.964366 15792 solver.cpp:218] Iteration 21100 (12.7205 iter/s, 7.86134s/100 iters), loss = 0.0445301
I1124 06:02:19.964366 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:02:19.964366 15792 solver.cpp:237]     Train net output #1: loss = 0.0445299 (* 1 = 0.0445299 loss)
I1124 06:02:19.964366 15792 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1124 06:02:27.828748 15792 solver.cpp:218] Iteration 21200 (12.7174 iter/s, 7.86322s/100 iters), loss = 0.0416723
I1124 06:02:27.828748 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:02:27.828748 15792 solver.cpp:237]     Train net output #1: loss = 0.0416721 (* 1 = 0.0416721 loss)
I1124 06:02:27.828748 15792 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1124 06:02:35.695803 15792 solver.cpp:218] Iteration 21300 (12.7117 iter/s, 7.86678s/100 iters), loss = 0.0685486
I1124 06:02:35.695803 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:02:35.695803 15792 solver.cpp:237]     Train net output #1: loss = 0.0685484 (* 1 = 0.0685484 loss)
I1124 06:02:35.695803 15792 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1124 06:02:43.566021 15792 solver.cpp:218] Iteration 21400 (12.7074 iter/s, 7.86944s/100 iters), loss = 0.0240838
I1124 06:02:43.566021 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:02:43.566021 15792 solver.cpp:237]     Train net output #1: loss = 0.0240836 (* 1 = 0.0240836 loss)
I1124 06:02:43.566021 15792 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1124 06:02:51.045833 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:02:51.356854 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_21500.caffemodel
I1124 06:02:51.398859 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_21500.solverstate
I1124 06:02:51.419862 15792 solver.cpp:330] Iteration 21500, Testing net (#0)
I1124 06:02:51.419862 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:02:53.507477 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:02:53.590979 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1124 06:02:53.590979 15792 solver.cpp:397]     Test net output #1: loss = 0.249794 (* 1 = 0.249794 loss)
I1124 06:02:53.667984 15792 solver.cpp:218] Iteration 21500 (9.89907 iter/s, 10.102s/100 iters), loss = 0.060584
I1124 06:02:53.667984 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:02:53.667984 15792 solver.cpp:237]     Train net output #1: loss = 0.0605838 (* 1 = 0.0605838 loss)
I1124 06:02:53.667984 15792 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1124 06:03:01.536072 15792 solver.cpp:218] Iteration 21600 (12.7098 iter/s, 7.86792s/100 iters), loss = 0.0905788
I1124 06:03:01.536072 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:03:01.536072 15792 solver.cpp:237]     Train net output #1: loss = 0.0905787 (* 1 = 0.0905787 loss)
I1124 06:03:01.537071 15792 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1124 06:03:09.403363 15792 solver.cpp:218] Iteration 21700 (12.7125 iter/s, 7.86625s/100 iters), loss = 0.0493668
I1124 06:03:09.403363 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:03:09.403363 15792 solver.cpp:237]     Train net output #1: loss = 0.0493667 (* 1 = 0.0493667 loss)
I1124 06:03:09.403363 15792 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1124 06:03:17.265878 15792 solver.cpp:218] Iteration 21800 (12.7184 iter/s, 7.8626s/100 iters), loss = 0.0490542
I1124 06:03:17.266891 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:03:17.266891 15792 solver.cpp:237]     Train net output #1: loss = 0.0490541 (* 1 = 0.0490541 loss)
I1124 06:03:17.266891 15792 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1124 06:03:25.127424 15792 solver.cpp:218] Iteration 21900 (12.7216 iter/s, 7.86063s/100 iters), loss = 0.0204892
I1124 06:03:25.127424 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:03:25.127424 15792 solver.cpp:237]     Train net output #1: loss = 0.0204891 (* 1 = 0.0204891 loss)
I1124 06:03:25.127424 15792 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1124 06:03:32.604578 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:03:32.916626 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_22000.caffemodel
I1124 06:03:32.960611 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_22000.solverstate
I1124 06:03:32.982424 15792 solver.cpp:330] Iteration 22000, Testing net (#0)
I1124 06:03:32.982424 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:03:35.066567 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:03:35.150074 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9192
I1124 06:03:35.150074 15792 solver.cpp:397]     Test net output #1: loss = 0.249695 (* 1 = 0.249695 loss)
I1124 06:03:35.227102 15792 solver.cpp:218] Iteration 22000 (9.90203 iter/s, 10.0989s/100 iters), loss = 0.0535748
I1124 06:03:35.227102 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:03:35.227102 15792 solver.cpp:237]     Train net output #1: loss = 0.0535746 (* 1 = 0.0535746 loss)
I1124 06:03:35.227102 15792 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1124 06:03:35.227102 15792 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1124 06:03:43.090183 15792 solver.cpp:218] Iteration 22100 (12.7192 iter/s, 7.86215s/100 iters), loss = 0.0865475
I1124 06:03:43.090183 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:03:43.090183 15792 solver.cpp:237]     Train net output #1: loss = 0.0865474 (* 1 = 0.0865474 loss)
I1124 06:03:43.090183 15792 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1124 06:03:50.949960 15792 solver.cpp:218] Iteration 22200 (12.7231 iter/s, 7.85975s/100 iters), loss = 0.0426031
I1124 06:03:50.949960 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:03:50.949960 15792 solver.cpp:237]     Train net output #1: loss = 0.042603 (* 1 = 0.042603 loss)
I1124 06:03:50.949960 15792 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1124 06:03:58.811262 15792 solver.cpp:218] Iteration 22300 (12.7215 iter/s, 7.86068s/100 iters), loss = 0.0373996
I1124 06:03:58.811262 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:03:58.811262 15792 solver.cpp:237]     Train net output #1: loss = 0.0373994 (* 1 = 0.0373994 loss)
I1124 06:03:58.811262 15792 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1124 06:04:06.667260 15792 solver.cpp:218] Iteration 22400 (12.7303 iter/s, 7.85524s/100 iters), loss = 0.0234789
I1124 06:04:06.667260 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:04:06.667260 15792 solver.cpp:237]     Train net output #1: loss = 0.0234788 (* 1 = 0.0234788 loss)
I1124 06:04:06.667260 15792 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1124 06:04:14.136212 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:04:14.448256 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_22500.caffemodel
I1124 06:04:14.487252 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_22500.solverstate
I1124 06:04:14.507751 15792 solver.cpp:330] Iteration 22500, Testing net (#0)
I1124 06:04:14.507751 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:04:16.593433 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:04:16.677435 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1124 06:04:16.677935 15792 solver.cpp:397]     Test net output #1: loss = 0.249738 (* 1 = 0.249738 loss)
I1124 06:04:16.753437 15792 solver.cpp:218] Iteration 22500 (9.91442 iter/s, 10.0863s/100 iters), loss = 0.0798628
I1124 06:04:16.753437 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:04:16.754438 15792 solver.cpp:237]     Train net output #1: loss = 0.0798626 (* 1 = 0.0798626 loss)
I1124 06:04:16.754438 15792 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1124 06:04:24.614137 15792 solver.cpp:218] Iteration 22600 (12.7228 iter/s, 7.85991s/100 iters), loss = 0.0593954
I1124 06:04:24.614137 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:04:24.614137 15792 solver.cpp:237]     Train net output #1: loss = 0.0593952 (* 1 = 0.0593952 loss)
I1124 06:04:24.614137 15792 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1124 06:04:32.456971 15792 solver.cpp:218] Iteration 22700 (12.7521 iter/s, 7.84187s/100 iters), loss = 0.0710016
I1124 06:04:32.456971 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:04:32.456971 15792 solver.cpp:237]     Train net output #1: loss = 0.0710014 (* 1 = 0.0710014 loss)
I1124 06:04:32.456971 15792 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1124 06:04:40.317934 15792 solver.cpp:218] Iteration 22800 (12.721 iter/s, 7.86099s/100 iters), loss = 0.0629266
I1124 06:04:40.317934 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:04:40.317934 15792 solver.cpp:237]     Train net output #1: loss = 0.0629265 (* 1 = 0.0629265 loss)
I1124 06:04:40.317934 15792 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1124 06:04:48.180655 15792 solver.cpp:218] Iteration 22900 (12.7202 iter/s, 7.86153s/100 iters), loss = 0.0401942
I1124 06:04:48.180655 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:04:48.180655 15792 solver.cpp:237]     Train net output #1: loss = 0.0401941 (* 1 = 0.0401941 loss)
I1124 06:04:48.180655 15792 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1124 06:04:55.657439 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:04:55.967456 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_23000.caffemodel
I1124 06:04:56.008502 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_23000.solverstate
I1124 06:04:56.028501 15792 solver.cpp:330] Iteration 23000, Testing net (#0)
I1124 06:04:56.029501 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:04:58.116612 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:04:58.201619 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9189
I1124 06:04:58.201619 15792 solver.cpp:397]     Test net output #1: loss = 0.249746 (* 1 = 0.249746 loss)
I1124 06:04:58.277618 15792 solver.cpp:218] Iteration 23000 (9.90417 iter/s, 10.0968s/100 iters), loss = 0.0637975
I1124 06:04:58.277618 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:04:58.277618 15792 solver.cpp:237]     Train net output #1: loss = 0.0637973 (* 1 = 0.0637973 loss)
I1124 06:04:58.277618 15792 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1124 06:05:06.138365 15792 solver.cpp:218] Iteration 23100 (12.7227 iter/s, 7.85995s/100 iters), loss = 0.0677627
I1124 06:05:06.138365 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:05:06.138365 15792 solver.cpp:237]     Train net output #1: loss = 0.0677625 (* 1 = 0.0677625 loss)
I1124 06:05:06.138365 15792 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1124 06:05:13.998287 15792 solver.cpp:218] Iteration 23200 (12.7239 iter/s, 7.85921s/100 iters), loss = 0.0635256
I1124 06:05:13.998287 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:05:13.998287 15792 solver.cpp:237]     Train net output #1: loss = 0.0635255 (* 1 = 0.0635255 loss)
I1124 06:05:13.998287 15792 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1124 06:05:21.857148 15792 solver.cpp:218] Iteration 23300 (12.7253 iter/s, 7.85836s/100 iters), loss = 0.0664109
I1124 06:05:21.857148 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:05:21.857148 15792 solver.cpp:237]     Train net output #1: loss = 0.0664107 (* 1 = 0.0664107 loss)
I1124 06:05:21.857148 15792 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1124 06:05:29.721313 15792 solver.cpp:218] Iteration 23400 (12.7168 iter/s, 7.86361s/100 iters), loss = 0.0336299
I1124 06:05:29.721313 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:05:29.721313 15792 solver.cpp:237]     Train net output #1: loss = 0.0336297 (* 1 = 0.0336297 loss)
I1124 06:05:29.721313 15792 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1124 06:05:37.199112 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:05:37.509140 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_23500.caffemodel
I1124 06:05:37.549163 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_23500.solverstate
I1124 06:05:37.570163 15792 solver.cpp:330] Iteration 23500, Testing net (#0)
I1124 06:05:37.570163 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:05:39.656312 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:05:39.741312 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1124 06:05:39.741312 15792 solver.cpp:397]     Test net output #1: loss = 0.249685 (* 1 = 0.249685 loss)
I1124 06:05:39.817317 15792 solver.cpp:218] Iteration 23500 (9.90489 iter/s, 10.096s/100 iters), loss = 0.054494
I1124 06:05:39.817317 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:05:39.817317 15792 solver.cpp:237]     Train net output #1: loss = 0.0544938 (* 1 = 0.0544938 loss)
I1124 06:05:39.817317 15792 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1124 06:05:47.683929 15792 solver.cpp:218] Iteration 23600 (12.7136 iter/s, 7.86557s/100 iters), loss = 0.0794623
I1124 06:05:47.683929 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:05:47.683929 15792 solver.cpp:237]     Train net output #1: loss = 0.0794621 (* 1 = 0.0794621 loss)
I1124 06:05:47.683929 15792 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1124 06:05:55.546636 15792 solver.cpp:218] Iteration 23700 (12.7179 iter/s, 7.86293s/100 iters), loss = 0.0661271
I1124 06:05:55.546636 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:05:55.546636 15792 solver.cpp:237]     Train net output #1: loss = 0.066127 (* 1 = 0.066127 loss)
I1124 06:05:55.546636 15792 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1124 06:06:03.414516 15792 solver.cpp:218] Iteration 23800 (12.7107 iter/s, 7.8674s/100 iters), loss = 0.0418133
I1124 06:06:03.414516 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:06:03.414516 15792 solver.cpp:237]     Train net output #1: loss = 0.0418131 (* 1 = 0.0418131 loss)
I1124 06:06:03.414516 15792 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1124 06:06:11.280325 15792 solver.cpp:218] Iteration 23900 (12.7153 iter/s, 7.86456s/100 iters), loss = 0.0420206
I1124 06:06:11.280325 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:06:11.280325 15792 solver.cpp:237]     Train net output #1: loss = 0.0420204 (* 1 = 0.0420204 loss)
I1124 06:06:11.280325 15792 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1124 06:06:18.762445 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:06:19.073473 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_24000.caffemodel
I1124 06:06:19.113482 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_24000.solverstate
I1124 06:06:19.135468 15792 solver.cpp:330] Iteration 24000, Testing net (#0)
I1124 06:06:19.135468 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:06:21.221601 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:06:21.306604 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9189
I1124 06:06:21.306604 15792 solver.cpp:397]     Test net output #1: loss = 0.249773 (* 1 = 0.249773 loss)
I1124 06:06:21.383610 15792 solver.cpp:218] Iteration 24000 (9.89802 iter/s, 10.103s/100 iters), loss = 0.0684217
I1124 06:06:21.383610 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:06:21.383610 15792 solver.cpp:237]     Train net output #1: loss = 0.0684215 (* 1 = 0.0684215 loss)
I1124 06:06:21.383610 15792 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1124 06:06:29.245115 15792 solver.cpp:218] Iteration 24100 (12.7215 iter/s, 7.86073s/100 iters), loss = 0.072553
I1124 06:06:29.245115 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:06:29.245115 15792 solver.cpp:237]     Train net output #1: loss = 0.0725528 (* 1 = 0.0725528 loss)
I1124 06:06:29.245115 15792 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1124 06:06:37.111995 15792 solver.cpp:218] Iteration 24200 (12.7125 iter/s, 7.86626s/100 iters), loss = 0.0678287
I1124 06:06:37.111995 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:06:37.111995 15792 solver.cpp:237]     Train net output #1: loss = 0.0678285 (* 1 = 0.0678285 loss)
I1124 06:06:37.111995 15792 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1124 06:06:44.977500 15792 solver.cpp:218] Iteration 24300 (12.7147 iter/s, 7.86493s/100 iters), loss = 0.0915792
I1124 06:06:44.977500 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:06:44.977500 15792 solver.cpp:237]     Train net output #1: loss = 0.091579 (* 1 = 0.091579 loss)
I1124 06:06:44.977500 15792 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1124 06:06:52.844069 15792 solver.cpp:218] Iteration 24400 (12.7119 iter/s, 7.86665s/100 iters), loss = 0.0323741
I1124 06:06:52.844069 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:06:52.844069 15792 solver.cpp:237]     Train net output #1: loss = 0.0323739 (* 1 = 0.0323739 loss)
I1124 06:06:52.844069 15792 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1124 06:07:00.327915 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:07:00.638931 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_24500.caffemodel
I1124 06:07:00.680945 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_24500.solverstate
I1124 06:07:00.700940 15792 solver.cpp:330] Iteration 24500, Testing net (#0)
I1124 06:07:00.700940 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:07:02.788058 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:07:02.872064 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9196
I1124 06:07:02.872064 15792 solver.cpp:397]     Test net output #1: loss = 0.249815 (* 1 = 0.249815 loss)
I1124 06:07:02.949067 15792 solver.cpp:218] Iteration 24500 (9.89675 iter/s, 10.1043s/100 iters), loss = 0.051332
I1124 06:07:02.949067 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:07:02.949067 15792 solver.cpp:237]     Train net output #1: loss = 0.0513318 (* 1 = 0.0513318 loss)
I1124 06:07:02.949067 15792 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1124 06:07:10.818646 15792 solver.cpp:218] Iteration 24600 (12.7083 iter/s, 7.86886s/100 iters), loss = 0.0701799
I1124 06:07:10.818646 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:07:10.818646 15792 solver.cpp:237]     Train net output #1: loss = 0.0701797 (* 1 = 0.0701797 loss)
I1124 06:07:10.818646 15792 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1124 06:07:18.684624 15792 solver.cpp:218] Iteration 24700 (12.7143 iter/s, 7.86514s/100 iters), loss = 0.0741626
I1124 06:07:18.684624 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:07:18.684624 15792 solver.cpp:237]     Train net output #1: loss = 0.0741624 (* 1 = 0.0741624 loss)
I1124 06:07:18.684624 15792 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1124 06:07:26.557189 15792 solver.cpp:218] Iteration 24800 (12.7031 iter/s, 7.87207s/100 iters), loss = 0.0298502
I1124 06:07:26.557189 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:07:26.557189 15792 solver.cpp:237]     Train net output #1: loss = 0.02985 (* 1 = 0.02985 loss)
I1124 06:07:26.557189 15792 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1124 06:07:34.431046 15792 solver.cpp:218] Iteration 24900 (12.7012 iter/s, 7.8733s/100 iters), loss = 0.0374745
I1124 06:07:34.431046 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:07:34.431046 15792 solver.cpp:237]     Train net output #1: loss = 0.0374743 (* 1 = 0.0374743 loss)
I1124 06:07:34.431046 15792 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1124 06:07:41.916409 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:07:42.228452 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_25000.caffemodel
I1124 06:07:42.268957 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_25000.solverstate
I1124 06:07:42.289458 15792 solver.cpp:330] Iteration 25000, Testing net (#0)
I1124 06:07:42.289458 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:07:44.377594 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:07:44.461601 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1124 06:07:44.461601 15792 solver.cpp:397]     Test net output #1: loss = 0.249775 (* 1 = 0.249775 loss)
I1124 06:07:44.537608 15792 solver.cpp:218] Iteration 25000 (9.89428 iter/s, 10.1068s/100 iters), loss = 0.106216
I1124 06:07:44.538609 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:07:44.538609 15792 solver.cpp:237]     Train net output #1: loss = 0.106216 (* 1 = 0.106216 loss)
I1124 06:07:44.538609 15792 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1124 06:07:52.405468 15792 solver.cpp:218] Iteration 25100 (12.7116 iter/s, 7.86682s/100 iters), loss = 0.0688196
I1124 06:07:52.405969 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:07:52.405969 15792 solver.cpp:237]     Train net output #1: loss = 0.0688194 (* 1 = 0.0688194 loss)
I1124 06:07:52.405969 15792 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1124 06:08:00.269861 15792 solver.cpp:218] Iteration 25200 (12.717 iter/s, 7.8635s/100 iters), loss = 0.0755496
I1124 06:08:00.269861 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:08:00.269861 15792 solver.cpp:237]     Train net output #1: loss = 0.0755494 (* 1 = 0.0755494 loss)
I1124 06:08:00.269861 15792 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1124 06:08:08.131407 15792 solver.cpp:218] Iteration 25300 (12.7211 iter/s, 7.86096s/100 iters), loss = 0.0419913
I1124 06:08:08.131407 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:08:08.131407 15792 solver.cpp:237]     Train net output #1: loss = 0.0419911 (* 1 = 0.0419911 loss)
I1124 06:08:08.131407 15792 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1124 06:08:15.994022 15792 solver.cpp:218] Iteration 25400 (12.7194 iter/s, 7.86204s/100 iters), loss = 0.0305031
I1124 06:08:15.994022 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:08:15.994022 15792 solver.cpp:237]     Train net output #1: loss = 0.030503 (* 1 = 0.030503 loss)
I1124 06:08:15.994022 15792 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1124 06:08:23.464217 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:08:23.774233 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_25500.caffemodel
I1124 06:08:23.816232 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_25500.solverstate
I1124 06:08:23.836737 15792 solver.cpp:330] Iteration 25500, Testing net (#0)
I1124 06:08:23.836737 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:08:25.923360 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:08:26.007365 15792 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1124 06:08:26.007365 15792 solver.cpp:397]     Test net output #1: loss = 0.249738 (* 1 = 0.249738 loss)
I1124 06:08:26.084372 15792 solver.cpp:218] Iteration 25500 (9.91028 iter/s, 10.0905s/100 iters), loss = 0.0768249
I1124 06:08:26.084372 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:08:26.084372 15792 solver.cpp:237]     Train net output #1: loss = 0.0768247 (* 1 = 0.0768247 loss)
I1124 06:08:26.084372 15792 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1124 06:08:33.943519 15792 solver.cpp:218] Iteration 25600 (12.7256 iter/s, 7.8582s/100 iters), loss = 0.0822004
I1124 06:08:33.943519 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:08:33.943519 15792 solver.cpp:237]     Train net output #1: loss = 0.0822002 (* 1 = 0.0822002 loss)
I1124 06:08:33.943519 15792 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1124 06:08:41.807080 15792 solver.cpp:218] Iteration 25700 (12.7177 iter/s, 7.86306s/100 iters), loss = 0.0664624
I1124 06:08:41.807080 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:08:41.807080 15792 solver.cpp:237]     Train net output #1: loss = 0.0664622 (* 1 = 0.0664622 loss)
I1124 06:08:41.807080 15792 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1124 06:08:49.671854 15792 solver.cpp:218] Iteration 25800 (12.7153 iter/s, 7.86455s/100 iters), loss = 0.0710735
I1124 06:08:49.671854 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:08:49.671854 15792 solver.cpp:237]     Train net output #1: loss = 0.0710733 (* 1 = 0.0710733 loss)
I1124 06:08:49.671854 15792 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1124 06:08:57.538524 15792 solver.cpp:218] Iteration 25900 (12.7139 iter/s, 7.86543s/100 iters), loss = 0.0375115
I1124 06:08:57.538524 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:08:57.538524 15792 solver.cpp:237]     Train net output #1: loss = 0.0375113 (* 1 = 0.0375113 loss)
I1124 06:08:57.538524 15792 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1124 06:09:05.022792 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:09:05.332806 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_26000.caffemodel
I1124 06:09:05.372815 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_26000.solverstate
I1124 06:09:05.394815 15792 solver.cpp:330] Iteration 26000, Testing net (#0)
I1124 06:09:05.394815 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:09:07.480942 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:09:07.564947 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1124 06:09:07.564947 15792 solver.cpp:397]     Test net output #1: loss = 0.249731 (* 1 = 0.249731 loss)
I1124 06:09:07.642451 15792 solver.cpp:218] Iteration 26000 (9.89765 iter/s, 10.1034s/100 iters), loss = 0.0592162
I1124 06:09:07.642451 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:09:07.642451 15792 solver.cpp:237]     Train net output #1: loss = 0.059216 (* 1 = 0.059216 loss)
I1124 06:09:07.642451 15792 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1124 06:09:15.514905 15792 solver.cpp:218] Iteration 26100 (12.7033 iter/s, 7.87199s/100 iters), loss = 0.0599632
I1124 06:09:15.514905 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:09:15.514905 15792 solver.cpp:237]     Train net output #1: loss = 0.059963 (* 1 = 0.059963 loss)
I1124 06:09:15.514905 15792 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1124 06:09:23.380991 15792 solver.cpp:218] Iteration 26200 (12.7128 iter/s, 7.86607s/100 iters), loss = 0.0696626
I1124 06:09:23.380991 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:09:23.380991 15792 solver.cpp:237]     Train net output #1: loss = 0.0696624 (* 1 = 0.0696624 loss)
I1124 06:09:23.380991 15792 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1124 06:09:31.246490 15792 solver.cpp:218] Iteration 26300 (12.7155 iter/s, 7.86442s/100 iters), loss = 0.0344574
I1124 06:09:31.246490 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:09:31.246490 15792 solver.cpp:237]     Train net output #1: loss = 0.0344572 (* 1 = 0.0344572 loss)
I1124 06:09:31.246490 15792 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1124 06:09:39.113917 15792 solver.cpp:218] Iteration 26400 (12.7108 iter/s, 7.86733s/100 iters), loss = 0.047979
I1124 06:09:39.113917 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:09:39.113917 15792 solver.cpp:237]     Train net output #1: loss = 0.0479788 (* 1 = 0.0479788 loss)
I1124 06:09:39.113917 15792 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1124 06:09:46.596611 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:09:46.906628 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_26500.caffemodel
I1124 06:09:46.947628 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_26500.solverstate
I1124 06:09:46.968629 15792 solver.cpp:330] Iteration 26500, Testing net (#0)
I1124 06:09:46.968629 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:09:49.054819 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:09:49.138845 15792 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1124 06:09:49.138845 15792 solver.cpp:397]     Test net output #1: loss = 0.249808 (* 1 = 0.249808 loss)
I1124 06:09:49.215881 15792 solver.cpp:218] Iteration 26500 (9.89996 iter/s, 10.101s/100 iters), loss = 0.0570151
I1124 06:09:49.215881 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:09:49.215881 15792 solver.cpp:237]     Train net output #1: loss = 0.0570149 (* 1 = 0.0570149 loss)
I1124 06:09:49.215881 15792 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1124 06:09:57.086719 15792 solver.cpp:218] Iteration 26600 (12.7057 iter/s, 7.87048s/100 iters), loss = 0.0641115
I1124 06:09:57.086719 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:09:57.086719 15792 solver.cpp:237]     Train net output #1: loss = 0.0641113 (* 1 = 0.0641113 loss)
I1124 06:09:57.086719 15792 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1124 06:10:04.973270 15792 solver.cpp:218] Iteration 26700 (12.6808 iter/s, 7.88592s/100 iters), loss = 0.0656486
I1124 06:10:04.973270 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:10:04.973270 15792 solver.cpp:237]     Train net output #1: loss = 0.0656484 (* 1 = 0.0656484 loss)
I1124 06:10:04.973270 15792 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1124 06:10:12.841137 15792 solver.cpp:218] Iteration 26800 (12.7108 iter/s, 7.86734s/100 iters), loss = 0.0495524
I1124 06:10:12.841137 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:10:12.841137 15792 solver.cpp:237]     Train net output #1: loss = 0.0495522 (* 1 = 0.0495522 loss)
I1124 06:10:12.841137 15792 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1124 06:10:20.700778 15792 solver.cpp:218] Iteration 26900 (12.7235 iter/s, 7.85948s/100 iters), loss = 0.0263
I1124 06:10:20.700778 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:10:20.700778 15792 solver.cpp:237]     Train net output #1: loss = 0.0262998 (* 1 = 0.0262998 loss)
I1124 06:10:20.700778 15792 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1124 06:10:28.179386 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:10:28.491426 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_27000.caffemodel
I1124 06:10:28.530431 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_27000.solverstate
I1124 06:10:28.548426 15792 solver.cpp:330] Iteration 27000, Testing net (#0)
I1124 06:10:28.548426 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:10:30.636598 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:10:30.720614 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9195
I1124 06:10:30.720614 15792 solver.cpp:397]     Test net output #1: loss = 0.249655 (* 1 = 0.249655 loss)
I1124 06:10:30.797610 15792 solver.cpp:218] Iteration 27000 (9.90544 iter/s, 10.0955s/100 iters), loss = 0.0683559
I1124 06:10:30.797610 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:10:30.797610 15792 solver.cpp:237]     Train net output #1: loss = 0.0683557 (* 1 = 0.0683557 loss)
I1124 06:10:30.797610 15792 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1124 06:10:30.797610 15792 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1124 06:10:38.656355 15792 solver.cpp:218] Iteration 27100 (12.7251 iter/s, 7.85846s/100 iters), loss = 0.063249
I1124 06:10:38.656355 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:10:38.656355 15792 solver.cpp:237]     Train net output #1: loss = 0.0632489 (* 1 = 0.0632489 loss)
I1124 06:10:38.656355 15792 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1124 06:10:46.511024 15792 solver.cpp:218] Iteration 27200 (12.7309 iter/s, 7.8549s/100 iters), loss = 0.0599771
I1124 06:10:46.512025 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:10:46.512025 15792 solver.cpp:237]     Train net output #1: loss = 0.0599769 (* 1 = 0.0599769 loss)
I1124 06:10:46.512025 15792 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1124 06:10:54.366919 15792 solver.cpp:218] Iteration 27300 (12.7317 iter/s, 7.85442s/100 iters), loss = 0.0420969
I1124 06:10:54.366919 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:10:54.366919 15792 solver.cpp:237]     Train net output #1: loss = 0.0420968 (* 1 = 0.0420968 loss)
I1124 06:10:54.366919 15792 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1124 06:11:02.224561 15792 solver.cpp:218] Iteration 27400 (12.7269 iter/s, 7.85738s/100 iters), loss = 0.033846
I1124 06:11:02.224561 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:11:02.224561 15792 solver.cpp:237]     Train net output #1: loss = 0.0338459 (* 1 = 0.0338459 loss)
I1124 06:11:02.224561 15792 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1124 06:11:09.691150 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:11:10.001191 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_27500.caffemodel
I1124 06:11:10.040190 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_27500.solverstate
I1124 06:11:10.059209 15792 solver.cpp:330] Iteration 27500, Testing net (#0)
I1124 06:11:10.059209 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:11:12.144421 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:11:12.229434 15792 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1124 06:11:12.229434 15792 solver.cpp:397]     Test net output #1: loss = 0.249694 (* 1 = 0.249694 loss)
I1124 06:11:12.306437 15792 solver.cpp:218] Iteration 27500 (9.91942 iter/s, 10.0812s/100 iters), loss = 0.0624914
I1124 06:11:12.306437 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:11:12.306437 15792 solver.cpp:237]     Train net output #1: loss = 0.0624913 (* 1 = 0.0624913 loss)
I1124 06:11:12.306437 15792 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1124 06:11:20.160269 15792 solver.cpp:218] Iteration 27600 (12.7325 iter/s, 7.85395s/100 iters), loss = 0.0676042
I1124 06:11:20.160269 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:11:20.160269 15792 solver.cpp:237]     Train net output #1: loss = 0.067604 (* 1 = 0.067604 loss)
I1124 06:11:20.160269 15792 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1124 06:11:28.019757 15792 solver.cpp:218] Iteration 27700 (12.725 iter/s, 7.85852s/100 iters), loss = 0.0702171
I1124 06:11:28.019757 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:11:28.019757 15792 solver.cpp:237]     Train net output #1: loss = 0.070217 (* 1 = 0.070217 loss)
I1124 06:11:28.019757 15792 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1124 06:11:35.878435 15792 solver.cpp:218] Iteration 27800 (12.7257 iter/s, 7.85814s/100 iters), loss = 0.0689918
I1124 06:11:35.878435 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:11:35.878435 15792 solver.cpp:237]     Train net output #1: loss = 0.0689916 (* 1 = 0.0689916 loss)
I1124 06:11:35.878435 15792 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1124 06:11:43.736960 15792 solver.cpp:218] Iteration 27900 (12.7248 iter/s, 7.85868s/100 iters), loss = 0.0378134
I1124 06:11:43.736960 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:11:43.736960 15792 solver.cpp:237]     Train net output #1: loss = 0.0378132 (* 1 = 0.0378132 loss)
I1124 06:11:43.736960 15792 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1124 06:11:51.212726 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:11:51.520748 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_28000.caffemodel
I1124 06:11:51.560750 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_28000.solverstate
I1124 06:11:51.579753 15792 solver.cpp:330] Iteration 28000, Testing net (#0)
I1124 06:11:51.579753 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:11:53.666872 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:11:53.750883 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9192
I1124 06:11:53.750883 15792 solver.cpp:397]     Test net output #1: loss = 0.2497 (* 1 = 0.2497 loss)
I1124 06:11:53.827879 15792 solver.cpp:218] Iteration 28000 (9.91082 iter/s, 10.09s/100 iters), loss = 0.0679732
I1124 06:11:53.827879 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:11:53.827879 15792 solver.cpp:237]     Train net output #1: loss = 0.0679731 (* 1 = 0.0679731 loss)
I1124 06:11:53.827879 15792 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1124 06:12:01.681493 15792 solver.cpp:218] Iteration 28100 (12.7335 iter/s, 7.85333s/100 iters), loss = 0.0636505
I1124 06:12:01.681493 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:12:01.681493 15792 solver.cpp:237]     Train net output #1: loss = 0.0636503 (* 1 = 0.0636503 loss)
I1124 06:12:01.681493 15792 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1124 06:12:09.532445 15792 solver.cpp:218] Iteration 28200 (12.7389 iter/s, 7.84997s/100 iters), loss = 0.0551429
I1124 06:12:09.532445 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:12:09.532445 15792 solver.cpp:237]     Train net output #1: loss = 0.0551427 (* 1 = 0.0551427 loss)
I1124 06:12:09.532445 15792 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1124 06:12:17.386806 15792 solver.cpp:218] Iteration 28300 (12.7324 iter/s, 7.854s/100 iters), loss = 0.0437425
I1124 06:12:17.386806 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:12:17.386806 15792 solver.cpp:237]     Train net output #1: loss = 0.0437423 (* 1 = 0.0437423 loss)
I1124 06:12:17.386806 15792 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1124 06:12:25.236531 15792 solver.cpp:218] Iteration 28400 (12.7391 iter/s, 7.84984s/100 iters), loss = 0.0348783
I1124 06:12:25.236531 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:12:25.236531 15792 solver.cpp:237]     Train net output #1: loss = 0.0348781 (* 1 = 0.0348781 loss)
I1124 06:12:25.236531 15792 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1124 06:12:32.701333 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:12:33.012373 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_28500.caffemodel
I1124 06:12:33.052388 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_28500.solverstate
I1124 06:12:33.070408 15792 solver.cpp:330] Iteration 28500, Testing net (#0)
I1124 06:12:33.070408 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:12:35.157506 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:12:35.241509 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1124 06:12:35.241509 15792 solver.cpp:397]     Test net output #1: loss = 0.249738 (* 1 = 0.249738 loss)
I1124 06:12:35.318512 15792 solver.cpp:218] Iteration 28500 (9.91995 iter/s, 10.0807s/100 iters), loss = 0.0682729
I1124 06:12:35.318512 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:12:35.318512 15792 solver.cpp:237]     Train net output #1: loss = 0.0682727 (* 1 = 0.0682727 loss)
I1124 06:12:35.318512 15792 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1124 06:12:43.173523 15792 solver.cpp:218] Iteration 28600 (12.7314 iter/s, 7.85459s/100 iters), loss = 0.0703074
I1124 06:12:43.173523 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:12:43.173523 15792 solver.cpp:237]     Train net output #1: loss = 0.0703072 (* 1 = 0.0703072 loss)
I1124 06:12:43.173523 15792 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1124 06:12:51.026603 15792 solver.cpp:218] Iteration 28700 (12.734 iter/s, 7.85298s/100 iters), loss = 0.0618972
I1124 06:12:51.026603 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:12:51.026603 15792 solver.cpp:237]     Train net output #1: loss = 0.061897 (* 1 = 0.061897 loss)
I1124 06:12:51.026603 15792 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1124 06:12:58.880412 15792 solver.cpp:218] Iteration 28800 (12.7333 iter/s, 7.85343s/100 iters), loss = 0.0551746
I1124 06:12:58.880412 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:12:58.880412 15792 solver.cpp:237]     Train net output #1: loss = 0.0551744 (* 1 = 0.0551744 loss)
I1124 06:12:58.880412 15792 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1124 06:13:06.736033 15792 solver.cpp:218] Iteration 28900 (12.7305 iter/s, 7.85514s/100 iters), loss = 0.0258464
I1124 06:13:06.736033 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:13:06.736033 15792 solver.cpp:237]     Train net output #1: loss = 0.0258463 (* 1 = 0.0258463 loss)
I1124 06:13:06.736033 15792 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1124 06:13:14.199537 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:13:14.509552 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_29000.caffemodel
I1124 06:13:14.550557 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_29000.solverstate
I1124 06:13:14.568555 15792 solver.cpp:330] Iteration 29000, Testing net (#0)
I1124 06:13:14.568555 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:13:16.652688 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:13:16.737692 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9191
I1124 06:13:16.737692 15792 solver.cpp:397]     Test net output #1: loss = 0.24975 (* 1 = 0.24975 loss)
I1124 06:13:16.813695 15792 solver.cpp:218] Iteration 29000 (9.92356 iter/s, 10.077s/100 iters), loss = 0.0663274
I1124 06:13:16.813695 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:13:16.813695 15792 solver.cpp:237]     Train net output #1: loss = 0.0663273 (* 1 = 0.0663273 loss)
I1124 06:13:16.813695 15792 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1124 06:13:24.666016 15792 solver.cpp:218] Iteration 29100 (12.7364 iter/s, 7.8515s/100 iters), loss = 0.0591531
I1124 06:13:24.666016 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:13:24.666016 15792 solver.cpp:237]     Train net output #1: loss = 0.0591529 (* 1 = 0.0591529 loss)
I1124 06:13:24.666517 15792 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1124 06:13:32.520074 15792 solver.cpp:218] Iteration 29200 (12.7333 iter/s, 7.85342s/100 iters), loss = 0.0660887
I1124 06:13:32.520074 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:13:32.520074 15792 solver.cpp:237]     Train net output #1: loss = 0.0660886 (* 1 = 0.0660886 loss)
I1124 06:13:32.520074 15792 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1124 06:13:40.373900 15792 solver.cpp:218] Iteration 29300 (12.7333 iter/s, 7.8534s/100 iters), loss = 0.0416003
I1124 06:13:40.373900 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:13:40.373900 15792 solver.cpp:237]     Train net output #1: loss = 0.0416002 (* 1 = 0.0416002 loss)
I1124 06:13:40.373900 15792 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1124 06:13:48.225500 15792 solver.cpp:218] Iteration 29400 (12.7369 iter/s, 7.8512s/100 iters), loss = 0.0327764
I1124 06:13:48.225500 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:13:48.225500 15792 solver.cpp:237]     Train net output #1: loss = 0.0327762 (* 1 = 0.0327762 loss)
I1124 06:13:48.225500 15792 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1124 06:13:55.689908 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:13:55.999922 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_29500.caffemodel
I1124 06:13:56.037923 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_29500.solverstate
I1124 06:13:56.056922 15792 solver.cpp:330] Iteration 29500, Testing net (#0)
I1124 06:13:56.056922 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:13:58.142087 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:13:58.226094 15792 solver.cpp:397]     Test net output #0: accuracy = 0.919
I1124 06:13:58.226094 15792 solver.cpp:397]     Test net output #1: loss = 0.249742 (* 1 = 0.249742 loss)
I1124 06:13:58.303097 15792 solver.cpp:218] Iteration 29500 (9.92393 iter/s, 10.0767s/100 iters), loss = 0.061024
I1124 06:13:58.303097 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:13:58.303097 15792 solver.cpp:237]     Train net output #1: loss = 0.0610238 (* 1 = 0.0610238 loss)
I1124 06:13:58.303097 15792 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1124 06:14:06.156615 15792 solver.cpp:218] Iteration 29600 (12.7339 iter/s, 7.85304s/100 iters), loss = 0.0715053
I1124 06:14:06.156615 15792 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:14:06.156615 15792 solver.cpp:237]     Train net output #1: loss = 0.0715052 (* 1 = 0.0715052 loss)
I1124 06:14:06.156615 15792 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1124 06:14:14.006165 15792 solver.cpp:218] Iteration 29700 (12.7397 iter/s, 7.84947s/100 iters), loss = 0.0532156
I1124 06:14:14.006165 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:14:14.006165 15792 solver.cpp:237]     Train net output #1: loss = 0.0532155 (* 1 = 0.0532155 loss)
I1124 06:14:14.006165 15792 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1124 06:14:21.857673 15792 solver.cpp:218] Iteration 29800 (12.7375 iter/s, 7.85083s/100 iters), loss = 0.0312151
I1124 06:14:21.857673 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:14:21.857673 15792 solver.cpp:237]     Train net output #1: loss = 0.0312149 (* 1 = 0.0312149 loss)
I1124 06:14:21.857673 15792 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1124 06:14:29.715338 15792 solver.cpp:218] Iteration 29900 (12.7272 iter/s, 7.85721s/100 iters), loss = 0.033645
I1124 06:14:29.715338 15792 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:14:29.715338 15792 solver.cpp:237]     Train net output #1: loss = 0.0336449 (* 1 = 0.0336449 loss)
I1124 06:14:29.715338 15792 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1124 06:14:37.182543 23768 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:14:37.492058 15792 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_30000.caffemodel
I1124 06:14:37.530071 15792 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_30000.solverstate
I1124 06:14:37.573068 15792 solver.cpp:310] Iteration 30000, loss = 0.0732924
I1124 06:14:37.573068 15792 solver.cpp:330] Iteration 30000, Testing net (#0)
I1124 06:14:37.573068 15792 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:14:39.658195 35900 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:14:39.742194 15792 solver.cpp:397]     Test net output #0: accuracy = 0.9193
I1124 06:14:39.742194 15792 solver.cpp:397]     Test net output #1: loss = 0.249696 (* 1 = 0.249696 loss)
I1124 06:14:39.742194 15792 solver.cpp:315] Optimization Done.
I1124 06:14:39.742194 15792 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 

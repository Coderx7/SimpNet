
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1123 22:52:50.617782 22756 caffe.cpp:219] Using GPUs 0
I1123 22:52:50.780288 22756 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1123 22:52:51.077394 22756 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 22:52:51.093899 22756 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_1.6k_8L_3x3"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1123 22:52:51.094399 22756 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 22:52:51.095898 22756 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 22:52:51.095898 22756 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 22:52:51.095898 22756 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1123 22:52:51.095898 22756 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1123 22:52:51.095898 22756 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1123 22:52:51.095898 22756 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1123 22:52:51.095898 22756 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1123 22:52:51.096398 22756 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1123 22:52:51.096398 22756 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1123 22:52:51.096398 22756 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1123 22:52:51.096398 22756 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1123 22:52:51.096398 22756 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1123 22:52:51.096398 22756 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_1.6M"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 66
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 215
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 22:52:51.131425 22756 layer_factory.cpp:58] Creating layer cifar
I1123 22:52:51.138424 22756 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1123 22:52:51.138424 22756 net.cpp:84] Creating Layer cifar
I1123 22:52:51.138424 22756 net.cpp:380] cifar -> data
I1123 22:52:51.138424 22756 net.cpp:380] cifar -> label
I1123 22:52:51.139425 22756 data_layer.cpp:45] output data size: 100,3,32,32
I1123 22:52:51.145424 22756 net.cpp:122] Setting up cifar
I1123 22:52:51.145424 22756 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 22:52:51.145424 22756 net.cpp:129] Top shape: 100 (100)
I1123 22:52:51.145424 22756 net.cpp:137] Memory required for data: 1229200
I1123 22:52:51.145424 22756 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 22:52:51.145424 22756 net.cpp:84] Creating Layer label_cifar_1_split
I1123 22:52:51.145424 22756 net.cpp:406] label_cifar_1_split <- label
I1123 22:52:51.145424 22756 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 22:52:51.145424 22756 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 22:52:51.145424 22756 net.cpp:122] Setting up label_cifar_1_split
I1123 22:52:51.145424 22756 net.cpp:129] Top shape: 100 (100)
I1123 22:52:51.145424 22756 net.cpp:129] Top shape: 100 (100)
I1123 22:52:51.145424 22756 net.cpp:137] Memory required for data: 1230000
I1123 22:52:51.145424 22756 layer_factory.cpp:58] Creating layer conv1
I1123 22:52:51.145424 22756 net.cpp:84] Creating Layer conv1
I1123 22:52:51.145424 22756 net.cpp:406] conv1 <- data
I1123 22:52:51.145424 22756 net.cpp:380] conv1 -> conv1
I1123 22:52:51.146425 24544 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 22:52:51.392953 22756 net.cpp:122] Setting up conv1
I1123 22:52:51.392953 22756 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1123 22:52:51.392953 22756 net.cpp:137] Memory required for data: 28263600
I1123 22:52:51.392953 22756 layer_factory.cpp:58] Creating layer bn1
I1123 22:52:51.392953 22756 net.cpp:84] Creating Layer bn1
I1123 22:52:51.392953 22756 net.cpp:406] bn1 <- conv1
I1123 22:52:51.392953 22756 net.cpp:367] bn1 -> conv1 (in-place)
I1123 22:52:51.393453 22756 net.cpp:122] Setting up bn1
I1123 22:52:51.393453 22756 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1123 22:52:51.393453 22756 net.cpp:137] Memory required for data: 55297200
I1123 22:52:51.393453 22756 layer_factory.cpp:58] Creating layer scale1
I1123 22:52:51.393453 22756 net.cpp:84] Creating Layer scale1
I1123 22:52:51.393453 22756 net.cpp:406] scale1 <- conv1
I1123 22:52:51.393453 22756 net.cpp:367] scale1 -> conv1 (in-place)
I1123 22:52:51.393453 22756 layer_factory.cpp:58] Creating layer scale1
I1123 22:52:51.393453 22756 net.cpp:122] Setting up scale1
I1123 22:52:51.393453 22756 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1123 22:52:51.393453 22756 net.cpp:137] Memory required for data: 82330800
I1123 22:52:51.393453 22756 layer_factory.cpp:58] Creating layer relu1
I1123 22:52:51.393453 22756 net.cpp:84] Creating Layer relu1
I1123 22:52:51.393453 22756 net.cpp:406] relu1 <- conv1
I1123 22:52:51.393453 22756 net.cpp:367] relu1 -> conv1 (in-place)
I1123 22:52:51.393453 22756 net.cpp:122] Setting up relu1
I1123 22:52:51.393453 22756 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1123 22:52:51.393453 22756 net.cpp:137] Memory required for data: 109364400
I1123 22:52:51.393453 22756 layer_factory.cpp:58] Creating layer conv2
I1123 22:52:51.393952 22756 net.cpp:84] Creating Layer conv2
I1123 22:52:51.393952 22756 net.cpp:406] conv2 <- conv1
I1123 22:52:51.393952 22756 net.cpp:380] conv2 -> conv2
I1123 22:52:51.395952 22756 net.cpp:122] Setting up conv2
I1123 22:52:51.395952 22756 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1123 22:52:51.395952 22756 net.cpp:137] Memory required for data: 148686000
I1123 22:52:51.395952 22756 layer_factory.cpp:58] Creating layer bn2
I1123 22:52:51.395952 22756 net.cpp:84] Creating Layer bn2
I1123 22:52:51.395952 22756 net.cpp:406] bn2 <- conv2
I1123 22:52:51.395952 22756 net.cpp:367] bn2 -> conv2 (in-place)
I1123 22:52:51.395952 22756 net.cpp:122] Setting up bn2
I1123 22:52:51.395952 22756 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1123 22:52:51.395952 22756 net.cpp:137] Memory required for data: 188007600
I1123 22:52:51.395952 22756 layer_factory.cpp:58] Creating layer scale2
I1123 22:52:51.395952 22756 net.cpp:84] Creating Layer scale2
I1123 22:52:51.395952 22756 net.cpp:406] scale2 <- conv2
I1123 22:52:51.395952 22756 net.cpp:367] scale2 -> conv2 (in-place)
I1123 22:52:51.395952 22756 layer_factory.cpp:58] Creating layer scale2
I1123 22:52:51.395952 22756 net.cpp:122] Setting up scale2
I1123 22:52:51.395952 22756 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1123 22:52:51.395952 22756 net.cpp:137] Memory required for data: 227329200
I1123 22:52:51.396452 22756 layer_factory.cpp:58] Creating layer relu2
I1123 22:52:51.396452 22756 net.cpp:84] Creating Layer relu2
I1123 22:52:51.396452 22756 net.cpp:406] relu2 <- conv2
I1123 22:52:51.396452 22756 net.cpp:367] relu2 -> conv2 (in-place)
I1123 22:52:51.396452 22756 net.cpp:122] Setting up relu2
I1123 22:52:51.396452 22756 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1123 22:52:51.396452 22756 net.cpp:137] Memory required for data: 266650800
I1123 22:52:51.396452 22756 layer_factory.cpp:58] Creating layer conv2_2
I1123 22:52:51.396452 22756 net.cpp:84] Creating Layer conv2_2
I1123 22:52:51.396452 22756 net.cpp:406] conv2_2 <- conv2
I1123 22:52:51.396452 22756 net.cpp:380] conv2_2 -> conv2_2
I1123 22:52:51.398452 22756 net.cpp:122] Setting up conv2_2
I1123 22:52:51.398452 22756 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1123 22:52:51.398452 22756 net.cpp:137] Memory required for data: 319079600
I1123 22:52:51.398452 22756 layer_factory.cpp:58] Creating layer bn2_2
I1123 22:52:51.398452 22756 net.cpp:84] Creating Layer bn2_2
I1123 22:52:51.398452 22756 net.cpp:406] bn2_2 <- conv2_2
I1123 22:52:51.398452 22756 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 22:52:51.398452 22756 net.cpp:122] Setting up bn2_2
I1123 22:52:51.398452 22756 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1123 22:52:51.398452 22756 net.cpp:137] Memory required for data: 371508400
I1123 22:52:51.398452 22756 layer_factory.cpp:58] Creating layer scale2_2
I1123 22:52:51.398452 22756 net.cpp:84] Creating Layer scale2_2
I1123 22:52:51.398452 22756 net.cpp:406] scale2_2 <- conv2_2
I1123 22:52:51.398452 22756 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 22:52:51.398452 22756 layer_factory.cpp:58] Creating layer scale2_2
I1123 22:52:51.398953 22756 net.cpp:122] Setting up scale2_2
I1123 22:52:51.398953 22756 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1123 22:52:51.398953 22756 net.cpp:137] Memory required for data: 423937200
I1123 22:52:51.398953 22756 layer_factory.cpp:58] Creating layer relu2_2
I1123 22:52:51.398953 22756 net.cpp:84] Creating Layer relu2_2
I1123 22:52:51.398953 22756 net.cpp:406] relu2_2 <- conv2_2
I1123 22:52:51.398953 22756 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 22:52:51.398953 22756 net.cpp:122] Setting up relu2_2
I1123 22:52:51.398953 22756 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1123 22:52:51.398953 22756 net.cpp:137] Memory required for data: 476366000
I1123 22:52:51.398953 22756 layer_factory.cpp:58] Creating layer pool2_1
I1123 22:52:51.398953 22756 net.cpp:84] Creating Layer pool2_1
I1123 22:52:51.398953 22756 net.cpp:406] pool2_1 <- conv2_2
I1123 22:52:51.398953 22756 net.cpp:380] pool2_1 -> pool2_1
I1123 22:52:51.398953 22756 net.cpp:122] Setting up pool2_1
I1123 22:52:51.398953 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.398953 22756 net.cpp:137] Memory required for data: 489473200
I1123 22:52:51.398953 22756 layer_factory.cpp:58] Creating layer conv3
I1123 22:52:51.398953 22756 net.cpp:84] Creating Layer conv3
I1123 22:52:51.398953 22756 net.cpp:406] conv3 <- pool2_1
I1123 22:52:51.398953 22756 net.cpp:380] conv3 -> conv3
I1123 22:52:51.400456 22756 net.cpp:122] Setting up conv3
I1123 22:52:51.400456 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.400456 22756 net.cpp:137] Memory required for data: 502580400
I1123 22:52:51.401456 22756 layer_factory.cpp:58] Creating layer bn3
I1123 22:52:51.401456 22756 net.cpp:84] Creating Layer bn3
I1123 22:52:51.401456 22756 net.cpp:406] bn3 <- conv3
I1123 22:52:51.401456 22756 net.cpp:367] bn3 -> conv3 (in-place)
I1123 22:52:51.401456 22756 net.cpp:122] Setting up bn3
I1123 22:52:51.401456 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.401456 22756 net.cpp:137] Memory required for data: 515687600
I1123 22:52:51.401456 22756 layer_factory.cpp:58] Creating layer scale3
I1123 22:52:51.401456 22756 net.cpp:84] Creating Layer scale3
I1123 22:52:51.401456 22756 net.cpp:406] scale3 <- conv3
I1123 22:52:51.401456 22756 net.cpp:367] scale3 -> conv3 (in-place)
I1123 22:52:51.401456 22756 layer_factory.cpp:58] Creating layer scale3
I1123 22:52:51.401456 22756 net.cpp:122] Setting up scale3
I1123 22:52:51.401456 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.401456 22756 net.cpp:137] Memory required for data: 528794800
I1123 22:52:51.401456 22756 layer_factory.cpp:58] Creating layer relu3
I1123 22:52:51.401456 22756 net.cpp:84] Creating Layer relu3
I1123 22:52:51.401456 22756 net.cpp:406] relu3 <- conv3
I1123 22:52:51.401456 22756 net.cpp:367] relu3 -> conv3 (in-place)
I1123 22:52:51.401456 22756 net.cpp:122] Setting up relu3
I1123 22:52:51.401456 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.401456 22756 net.cpp:137] Memory required for data: 541902000
I1123 22:52:51.401456 22756 layer_factory.cpp:58] Creating layer conv4
I1123 22:52:51.401456 22756 net.cpp:84] Creating Layer conv4
I1123 22:52:51.401456 22756 net.cpp:406] conv4 <- conv3
I1123 22:52:51.401456 22756 net.cpp:380] conv4 -> conv4
I1123 22:52:51.404458 22756 net.cpp:122] Setting up conv4
I1123 22:52:51.404458 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.404458 22756 net.cpp:137] Memory required for data: 555009200
I1123 22:52:51.404458 22756 layer_factory.cpp:58] Creating layer bn4
I1123 22:52:51.404458 22756 net.cpp:84] Creating Layer bn4
I1123 22:52:51.404458 22756 net.cpp:406] bn4 <- conv4
I1123 22:52:51.404458 22756 net.cpp:367] bn4 -> conv4 (in-place)
I1123 22:52:51.404458 22756 net.cpp:122] Setting up bn4
I1123 22:52:51.404458 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.404458 22756 net.cpp:137] Memory required for data: 568116400
I1123 22:52:51.404458 22756 layer_factory.cpp:58] Creating layer scale4
I1123 22:52:51.404458 22756 net.cpp:84] Creating Layer scale4
I1123 22:52:51.404458 22756 net.cpp:406] scale4 <- conv4
I1123 22:52:51.404458 22756 net.cpp:367] scale4 -> conv4 (in-place)
I1123 22:52:51.404458 22756 layer_factory.cpp:58] Creating layer scale4
I1123 22:52:51.404458 22756 net.cpp:122] Setting up scale4
I1123 22:52:51.404458 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.404458 22756 net.cpp:137] Memory required for data: 581223600
I1123 22:52:51.404458 22756 layer_factory.cpp:58] Creating layer relu4
I1123 22:52:51.404458 22756 net.cpp:84] Creating Layer relu4
I1123 22:52:51.404458 22756 net.cpp:406] relu4 <- conv4
I1123 22:52:51.404458 22756 net.cpp:367] relu4 -> conv4 (in-place)
I1123 22:52:51.404458 22756 net.cpp:122] Setting up relu4
I1123 22:52:51.404458 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.404458 22756 net.cpp:137] Memory required for data: 594330800
I1123 22:52:51.404458 22756 layer_factory.cpp:58] Creating layer conv4_1
I1123 22:52:51.404458 22756 net.cpp:84] Creating Layer conv4_1
I1123 22:52:51.405457 22756 net.cpp:406] conv4_1 <- conv4
I1123 22:52:51.405457 22756 net.cpp:380] conv4_1 -> conv4_1
I1123 22:52:51.406456 22756 net.cpp:122] Setting up conv4_1
I1123 22:52:51.406456 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.406456 22756 net.cpp:137] Memory required for data: 607438000
I1123 22:52:51.406456 22756 layer_factory.cpp:58] Creating layer bn4_1
I1123 22:52:51.406456 22756 net.cpp:84] Creating Layer bn4_1
I1123 22:52:51.406456 22756 net.cpp:406] bn4_1 <- conv4_1
I1123 22:52:51.406456 22756 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 22:52:51.407457 22756 net.cpp:122] Setting up bn4_1
I1123 22:52:51.407457 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.407457 22756 net.cpp:137] Memory required for data: 620545200
I1123 22:52:51.407457 22756 layer_factory.cpp:58] Creating layer scale4_1
I1123 22:52:51.407457 22756 net.cpp:84] Creating Layer scale4_1
I1123 22:52:51.407457 22756 net.cpp:406] scale4_1 <- conv4_1
I1123 22:52:51.407457 22756 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 22:52:51.407457 22756 layer_factory.cpp:58] Creating layer scale4_1
I1123 22:52:51.407457 22756 net.cpp:122] Setting up scale4_1
I1123 22:52:51.407457 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.407457 22756 net.cpp:137] Memory required for data: 633652400
I1123 22:52:51.407457 22756 layer_factory.cpp:58] Creating layer relu4_1
I1123 22:52:51.407457 22756 net.cpp:84] Creating Layer relu4_1
I1123 22:52:51.407457 22756 net.cpp:406] relu4_1 <- conv4_1
I1123 22:52:51.407457 22756 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 22:52:51.407457 22756 net.cpp:122] Setting up relu4_1
I1123 22:52:51.407457 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.407457 22756 net.cpp:137] Memory required for data: 646759600
I1123 22:52:51.407457 22756 layer_factory.cpp:58] Creating layer conv4_2
I1123 22:52:51.407457 22756 net.cpp:84] Creating Layer conv4_2
I1123 22:52:51.407457 22756 net.cpp:406] conv4_2 <- conv4_1
I1123 22:52:51.407457 22756 net.cpp:380] conv4_2 -> conv4_2
I1123 22:52:51.411458 22756 net.cpp:122] Setting up conv4_2
I1123 22:52:51.411458 22756 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1123 22:52:51.411458 22756 net.cpp:137] Memory required for data: 668775600
I1123 22:52:51.411458 22756 layer_factory.cpp:58] Creating layer bn4_2
I1123 22:52:51.411458 22756 net.cpp:84] Creating Layer bn4_2
I1123 22:52:51.411458 22756 net.cpp:406] bn4_2 <- conv4_2
I1123 22:52:51.411458 22756 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 22:52:51.411458 22756 net.cpp:122] Setting up bn4_2
I1123 22:52:51.411458 22756 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1123 22:52:51.411458 22756 net.cpp:137] Memory required for data: 690791600
I1123 22:52:51.411458 22756 layer_factory.cpp:58] Creating layer scale4_2
I1123 22:52:51.411458 22756 net.cpp:84] Creating Layer scale4_2
I1123 22:52:51.411458 22756 net.cpp:406] scale4_2 <- conv4_2
I1123 22:52:51.411458 22756 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 22:52:51.411458 22756 layer_factory.cpp:58] Creating layer scale4_2
I1123 22:52:51.411458 22756 net.cpp:122] Setting up scale4_2
I1123 22:52:51.411458 22756 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1123 22:52:51.411458 22756 net.cpp:137] Memory required for data: 712807600
I1123 22:52:51.411458 22756 layer_factory.cpp:58] Creating layer relu4_2
I1123 22:52:51.411458 22756 net.cpp:84] Creating Layer relu4_2
I1123 22:52:51.411458 22756 net.cpp:406] relu4_2 <- conv4_2
I1123 22:52:51.411458 22756 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 22:52:51.411458 22756 net.cpp:122] Setting up relu4_2
I1123 22:52:51.411458 22756 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1123 22:52:51.411458 22756 net.cpp:137] Memory required for data: 734823600
I1123 22:52:51.411458 22756 layer_factory.cpp:58] Creating layer pool4_2
I1123 22:52:51.411458 22756 net.cpp:84] Creating Layer pool4_2
I1123 22:52:51.411458 22756 net.cpp:406] pool4_2 <- conv4_2
I1123 22:52:51.411458 22756 net.cpp:380] pool4_2 -> pool4_2
I1123 22:52:51.411458 22756 net.cpp:122] Setting up pool4_2
I1123 22:52:51.411458 22756 net.cpp:129] Top shape: 100 215 8 8 (1376000)
I1123 22:52:51.411458 22756 net.cpp:137] Memory required for data: 740327600
I1123 22:52:51.411458 22756 layer_factory.cpp:58] Creating layer conv12
I1123 22:52:51.411458 22756 net.cpp:84] Creating Layer conv12
I1123 22:52:51.411458 22756 net.cpp:406] conv12 <- pool4_2
I1123 22:52:51.411458 22756 net.cpp:380] conv12 -> conv12
I1123 22:52:51.418457 22756 net.cpp:122] Setting up conv12
I1123 22:52:51.418457 22756 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1123 22:52:51.418457 22756 net.cpp:137] Memory required for data: 750158000
I1123 22:52:51.418457 22756 layer_factory.cpp:58] Creating layer bn_conv12
I1123 22:52:51.418457 22756 net.cpp:84] Creating Layer bn_conv12
I1123 22:52:51.418457 22756 net.cpp:406] bn_conv12 <- conv12
I1123 22:52:51.418457 22756 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 22:52:51.418457 22756 net.cpp:122] Setting up bn_conv12
I1123 22:52:51.418457 22756 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1123 22:52:51.418457 22756 net.cpp:137] Memory required for data: 759988400
I1123 22:52:51.418457 22756 layer_factory.cpp:58] Creating layer scale_conv12
I1123 22:52:51.418457 22756 net.cpp:84] Creating Layer scale_conv12
I1123 22:52:51.418457 22756 net.cpp:406] scale_conv12 <- conv12
I1123 22:52:51.418457 22756 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 22:52:51.418457 22756 layer_factory.cpp:58] Creating layer scale_conv12
I1123 22:52:51.418457 22756 net.cpp:122] Setting up scale_conv12
I1123 22:52:51.418457 22756 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1123 22:52:51.418457 22756 net.cpp:137] Memory required for data: 769818800
I1123 22:52:51.418457 22756 layer_factory.cpp:58] Creating layer relu_conv12
I1123 22:52:51.418457 22756 net.cpp:84] Creating Layer relu_conv12
I1123 22:52:51.418457 22756 net.cpp:406] relu_conv12 <- conv12
I1123 22:52:51.418457 22756 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 22:52:51.418457 22756 net.cpp:122] Setting up relu_conv12
I1123 22:52:51.418457 22756 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1123 22:52:51.418457 22756 net.cpp:137] Memory required for data: 779649200
I1123 22:52:51.418457 22756 layer_factory.cpp:58] Creating layer poolcp6
I1123 22:52:51.418457 22756 net.cpp:84] Creating Layer poolcp6
I1123 22:52:51.418457 22756 net.cpp:406] poolcp6 <- conv12
I1123 22:52:51.418457 22756 net.cpp:380] poolcp6 -> poolcp6
I1123 22:52:51.418457 22756 net.cpp:122] Setting up poolcp6
I1123 22:52:51.418457 22756 net.cpp:129] Top shape: 100 384 1 1 (38400)
I1123 22:52:51.418457 22756 net.cpp:137] Memory required for data: 779802800
I1123 22:52:51.418457 22756 layer_factory.cpp:58] Creating layer ip1
I1123 22:52:51.419456 22756 net.cpp:84] Creating Layer ip1
I1123 22:52:51.419456 22756 net.cpp:406] ip1 <- poolcp6
I1123 22:52:51.419456 22756 net.cpp:380] ip1 -> ip1
I1123 22:52:51.419456 22756 net.cpp:122] Setting up ip1
I1123 22:52:51.419456 22756 net.cpp:129] Top shape: 100 10 (1000)
I1123 22:52:51.419456 22756 net.cpp:137] Memory required for data: 779806800
I1123 22:52:51.419456 22756 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 22:52:51.419456 22756 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 22:52:51.419456 22756 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 22:52:51.419456 22756 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 22:52:51.419456 22756 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 22:52:51.419456 22756 net.cpp:122] Setting up ip1_ip1_0_split
I1123 22:52:51.419456 22756 net.cpp:129] Top shape: 100 10 (1000)
I1123 22:52:51.419456 22756 net.cpp:129] Top shape: 100 10 (1000)
I1123 22:52:51.419456 22756 net.cpp:137] Memory required for data: 779814800
I1123 22:52:51.419456 22756 layer_factory.cpp:58] Creating layer accuracy_training
I1123 22:52:51.419456 22756 net.cpp:84] Creating Layer accuracy_training
I1123 22:52:51.419456 22756 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1123 22:52:51.419456 22756 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1123 22:52:51.419456 22756 net.cpp:380] accuracy_training -> accuracy_training
I1123 22:52:51.419456 22756 net.cpp:122] Setting up accuracy_training
I1123 22:52:51.419456 22756 net.cpp:129] Top shape: (1)
I1123 22:52:51.419456 22756 net.cpp:137] Memory required for data: 779814804
I1123 22:52:51.419456 22756 layer_factory.cpp:58] Creating layer loss
I1123 22:52:51.419456 22756 net.cpp:84] Creating Layer loss
I1123 22:52:51.419456 22756 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 22:52:51.419456 22756 net.cpp:406] loss <- label_cifar_1_split_1
I1123 22:52:51.419456 22756 net.cpp:380] loss -> loss
I1123 22:52:51.419456 22756 layer_factory.cpp:58] Creating layer loss
I1123 22:52:51.419456 22756 net.cpp:122] Setting up loss
I1123 22:52:51.419456 22756 net.cpp:129] Top shape: (1)
I1123 22:52:51.419456 22756 net.cpp:132]     with loss weight 1
I1123 22:52:51.419456 22756 net.cpp:137] Memory required for data: 779814808
I1123 22:52:51.419456 22756 net.cpp:198] loss needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:200] accuracy_training does not need backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] ip1 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] poolcp6 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] relu_conv12 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] scale_conv12 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] bn_conv12 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] conv12 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] pool4_2 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] relu4_2 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] scale4_2 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] bn4_2 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] conv4_2 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] relu4_1 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] scale4_1 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] bn4_1 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] conv4_1 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] relu4 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] scale4 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] bn4 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] conv4 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] relu3 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] scale3 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] bn3 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] conv3 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] pool2_1 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] relu2_2 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] scale2_2 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] bn2_2 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] conv2_2 needs backward computation.
I1123 22:52:51.419456 22756 net.cpp:198] relu2 needs backward computation.
I1123 22:52:51.420456 22756 net.cpp:198] scale2 needs backward computation.
I1123 22:52:51.420456 22756 net.cpp:198] bn2 needs backward computation.
I1123 22:52:51.420456 22756 net.cpp:198] conv2 needs backward computation.
I1123 22:52:51.420456 22756 net.cpp:198] relu1 needs backward computation.
I1123 22:52:51.420456 22756 net.cpp:198] scale1 needs backward computation.
I1123 22:52:51.420456 22756 net.cpp:198] bn1 needs backward computation.
I1123 22:52:51.420456 22756 net.cpp:198] conv1 needs backward computation.
I1123 22:52:51.420456 22756 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 22:52:51.420456 22756 net.cpp:200] cifar does not need backward computation.
I1123 22:52:51.420456 22756 net.cpp:242] This network produces output accuracy_training
I1123 22:52:51.420456 22756 net.cpp:242] This network produces output loss
I1123 22:52:51.420456 22756 net.cpp:255] Network initialization done.
I1123 22:52:51.420456 22756 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 22:52:51.420456 22756 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1123 22:52:51.420456 22756 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1123 22:52:51.420456 22756 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1123 22:52:51.420456 22756 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1123 22:52:51.420456 22756 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1123 22:52:51.420456 22756 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1123 22:52:51.420456 22756 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1123 22:52:51.420456 22756 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1123 22:52:51.420456 22756 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1123 22:52:51.420456 22756 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1123 22:52:51.420456 22756 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1123 22:52:51.420456 22756 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1123 22:52:51.420456 22756 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_1.6M"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 66
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 215
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1123 22:52:51.421456 22756 layer_factory.cpp:58] Creating layer cifar
I1123 22:52:51.451455 22756 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1123 22:52:51.451455 22756 net.cpp:84] Creating Layer cifar
I1123 22:52:51.451455 22756 net.cpp:380] cifar -> data
I1123 22:52:51.451455 22756 net.cpp:380] cifar -> label
I1123 22:52:51.451455 22756 data_layer.cpp:45] output data size: 100,3,32,32
I1123 22:52:51.457473 22756 net.cpp:122] Setting up cifar
I1123 22:52:51.457473 22756 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1123 22:52:51.457473 22756 net.cpp:129] Top shape: 100 (100)
I1123 22:52:51.457473 22756 net.cpp:137] Memory required for data: 1229200
I1123 22:52:51.457473 22756 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1123 22:52:51.457473 22756 net.cpp:84] Creating Layer label_cifar_1_split
I1123 22:52:51.457473 22756 net.cpp:406] label_cifar_1_split <- label
I1123 22:52:51.457473 22756 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1123 22:52:51.457473 22756 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1123 22:52:51.457473 22756 net.cpp:122] Setting up label_cifar_1_split
I1123 22:52:51.457473 22756 net.cpp:129] Top shape: 100 (100)
I1123 22:52:51.457473 22756 net.cpp:129] Top shape: 100 (100)
I1123 22:52:51.457473 22756 net.cpp:137] Memory required for data: 1230000
I1123 22:52:51.457473 22756 layer_factory.cpp:58] Creating layer conv1
I1123 22:52:51.457473 22756 net.cpp:84] Creating Layer conv1
I1123 22:52:51.457473 22756 net.cpp:406] conv1 <- data
I1123 22:52:51.457473 22756 net.cpp:380] conv1 -> conv1
I1123 22:52:51.458458 24248 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1123 22:52:51.459456 22756 net.cpp:122] Setting up conv1
I1123 22:52:51.459456 22756 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1123 22:52:51.459456 22756 net.cpp:137] Memory required for data: 28263600
I1123 22:52:51.459456 22756 layer_factory.cpp:58] Creating layer bn1
I1123 22:52:51.459456 22756 net.cpp:84] Creating Layer bn1
I1123 22:52:51.459456 22756 net.cpp:406] bn1 <- conv1
I1123 22:52:51.459456 22756 net.cpp:367] bn1 -> conv1 (in-place)
I1123 22:52:51.459456 22756 net.cpp:122] Setting up bn1
I1123 22:52:51.459456 22756 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1123 22:52:51.459456 22756 net.cpp:137] Memory required for data: 55297200
I1123 22:52:51.459456 22756 layer_factory.cpp:58] Creating layer scale1
I1123 22:52:51.459456 22756 net.cpp:84] Creating Layer scale1
I1123 22:52:51.459456 22756 net.cpp:406] scale1 <- conv1
I1123 22:52:51.459456 22756 net.cpp:367] scale1 -> conv1 (in-place)
I1123 22:52:51.459456 22756 layer_factory.cpp:58] Creating layer scale1
I1123 22:52:51.459456 22756 net.cpp:122] Setting up scale1
I1123 22:52:51.459456 22756 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1123 22:52:51.459456 22756 net.cpp:137] Memory required for data: 82330800
I1123 22:52:51.459456 22756 layer_factory.cpp:58] Creating layer relu1
I1123 22:52:51.459456 22756 net.cpp:84] Creating Layer relu1
I1123 22:52:51.459456 22756 net.cpp:406] relu1 <- conv1
I1123 22:52:51.459456 22756 net.cpp:367] relu1 -> conv1 (in-place)
I1123 22:52:51.460458 22756 net.cpp:122] Setting up relu1
I1123 22:52:51.460458 22756 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1123 22:52:51.460458 22756 net.cpp:137] Memory required for data: 109364400
I1123 22:52:51.460458 22756 layer_factory.cpp:58] Creating layer conv2
I1123 22:52:51.460458 22756 net.cpp:84] Creating Layer conv2
I1123 22:52:51.460458 22756 net.cpp:406] conv2 <- conv1
I1123 22:52:51.460458 22756 net.cpp:380] conv2 -> conv2
I1123 22:52:51.462474 22756 net.cpp:122] Setting up conv2
I1123 22:52:51.462474 22756 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1123 22:52:51.462474 22756 net.cpp:137] Memory required for data: 148686000
I1123 22:52:51.462474 22756 layer_factory.cpp:58] Creating layer bn2
I1123 22:52:51.462474 22756 net.cpp:84] Creating Layer bn2
I1123 22:52:51.462474 22756 net.cpp:406] bn2 <- conv2
I1123 22:52:51.462474 22756 net.cpp:367] bn2 -> conv2 (in-place)
I1123 22:52:51.462474 22756 net.cpp:122] Setting up bn2
I1123 22:52:51.462474 22756 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1123 22:52:51.462474 22756 net.cpp:137] Memory required for data: 188007600
I1123 22:52:51.462474 22756 layer_factory.cpp:58] Creating layer scale2
I1123 22:52:51.462474 22756 net.cpp:84] Creating Layer scale2
I1123 22:52:51.462474 22756 net.cpp:406] scale2 <- conv2
I1123 22:52:51.462474 22756 net.cpp:367] scale2 -> conv2 (in-place)
I1123 22:52:51.462474 22756 layer_factory.cpp:58] Creating layer scale2
I1123 22:52:51.462474 22756 net.cpp:122] Setting up scale2
I1123 22:52:51.462474 22756 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1123 22:52:51.462474 22756 net.cpp:137] Memory required for data: 227329200
I1123 22:52:51.462474 22756 layer_factory.cpp:58] Creating layer relu2
I1123 22:52:51.462474 22756 net.cpp:84] Creating Layer relu2
I1123 22:52:51.462474 22756 net.cpp:406] relu2 <- conv2
I1123 22:52:51.462474 22756 net.cpp:367] relu2 -> conv2 (in-place)
I1123 22:52:51.463474 22756 net.cpp:122] Setting up relu2
I1123 22:52:51.463474 22756 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1123 22:52:51.463474 22756 net.cpp:137] Memory required for data: 266650800
I1123 22:52:51.463474 22756 layer_factory.cpp:58] Creating layer conv2_2
I1123 22:52:51.463474 22756 net.cpp:84] Creating Layer conv2_2
I1123 22:52:51.463474 22756 net.cpp:406] conv2_2 <- conv2
I1123 22:52:51.463474 22756 net.cpp:380] conv2_2 -> conv2_2
I1123 22:52:51.465456 22756 net.cpp:122] Setting up conv2_2
I1123 22:52:51.465456 22756 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1123 22:52:51.465456 22756 net.cpp:137] Memory required for data: 319079600
I1123 22:52:51.465456 22756 layer_factory.cpp:58] Creating layer bn2_2
I1123 22:52:51.465456 22756 net.cpp:84] Creating Layer bn2_2
I1123 22:52:51.465456 22756 net.cpp:406] bn2_2 <- conv2_2
I1123 22:52:51.465456 22756 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1123 22:52:51.465456 22756 net.cpp:122] Setting up bn2_2
I1123 22:52:51.465456 22756 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1123 22:52:51.465456 22756 net.cpp:137] Memory required for data: 371508400
I1123 22:52:51.465456 22756 layer_factory.cpp:58] Creating layer scale2_2
I1123 22:52:51.465456 22756 net.cpp:84] Creating Layer scale2_2
I1123 22:52:51.465456 22756 net.cpp:406] scale2_2 <- conv2_2
I1123 22:52:51.465456 22756 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1123 22:52:51.465456 22756 layer_factory.cpp:58] Creating layer scale2_2
I1123 22:52:51.465456 22756 net.cpp:122] Setting up scale2_2
I1123 22:52:51.465456 22756 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1123 22:52:51.465456 22756 net.cpp:137] Memory required for data: 423937200
I1123 22:52:51.465456 22756 layer_factory.cpp:58] Creating layer relu2_2
I1123 22:52:51.465456 22756 net.cpp:84] Creating Layer relu2_2
I1123 22:52:51.465456 22756 net.cpp:406] relu2_2 <- conv2_2
I1123 22:52:51.465456 22756 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1123 22:52:51.465456 22756 net.cpp:122] Setting up relu2_2
I1123 22:52:51.465456 22756 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1123 22:52:51.465456 22756 net.cpp:137] Memory required for data: 476366000
I1123 22:52:51.465456 22756 layer_factory.cpp:58] Creating layer pool2_1
I1123 22:52:51.465456 22756 net.cpp:84] Creating Layer pool2_1
I1123 22:52:51.465456 22756 net.cpp:406] pool2_1 <- conv2_2
I1123 22:52:51.465456 22756 net.cpp:380] pool2_1 -> pool2_1
I1123 22:52:51.465456 22756 net.cpp:122] Setting up pool2_1
I1123 22:52:51.465456 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.465456 22756 net.cpp:137] Memory required for data: 489473200
I1123 22:52:51.466456 22756 layer_factory.cpp:58] Creating layer conv3
I1123 22:52:51.466456 22756 net.cpp:84] Creating Layer conv3
I1123 22:52:51.466456 22756 net.cpp:406] conv3 <- pool2_1
I1123 22:52:51.466456 22756 net.cpp:380] conv3 -> conv3
I1123 22:52:51.468457 22756 net.cpp:122] Setting up conv3
I1123 22:52:51.468457 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.468457 22756 net.cpp:137] Memory required for data: 502580400
I1123 22:52:51.468457 22756 layer_factory.cpp:58] Creating layer bn3
I1123 22:52:51.468457 22756 net.cpp:84] Creating Layer bn3
I1123 22:52:51.468457 22756 net.cpp:406] bn3 <- conv3
I1123 22:52:51.468457 22756 net.cpp:367] bn3 -> conv3 (in-place)
I1123 22:52:51.469456 22756 net.cpp:122] Setting up bn3
I1123 22:52:51.469456 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.469456 22756 net.cpp:137] Memory required for data: 515687600
I1123 22:52:51.469456 22756 layer_factory.cpp:58] Creating layer scale3
I1123 22:52:51.469456 22756 net.cpp:84] Creating Layer scale3
I1123 22:52:51.469456 22756 net.cpp:406] scale3 <- conv3
I1123 22:52:51.469456 22756 net.cpp:367] scale3 -> conv3 (in-place)
I1123 22:52:51.469456 22756 layer_factory.cpp:58] Creating layer scale3
I1123 22:52:51.469456 22756 net.cpp:122] Setting up scale3
I1123 22:52:51.469456 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.469456 22756 net.cpp:137] Memory required for data: 528794800
I1123 22:52:51.469456 22756 layer_factory.cpp:58] Creating layer relu3
I1123 22:52:51.469456 22756 net.cpp:84] Creating Layer relu3
I1123 22:52:51.469456 22756 net.cpp:406] relu3 <- conv3
I1123 22:52:51.469456 22756 net.cpp:367] relu3 -> conv3 (in-place)
I1123 22:52:51.469456 22756 net.cpp:122] Setting up relu3
I1123 22:52:51.469456 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.469456 22756 net.cpp:137] Memory required for data: 541902000
I1123 22:52:51.469456 22756 layer_factory.cpp:58] Creating layer conv4
I1123 22:52:51.469456 22756 net.cpp:84] Creating Layer conv4
I1123 22:52:51.469456 22756 net.cpp:406] conv4 <- conv3
I1123 22:52:51.469456 22756 net.cpp:380] conv4 -> conv4
I1123 22:52:51.472468 22756 net.cpp:122] Setting up conv4
I1123 22:52:51.472468 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.472468 22756 net.cpp:137] Memory required for data: 555009200
I1123 22:52:51.472468 22756 layer_factory.cpp:58] Creating layer bn4
I1123 22:52:51.472468 22756 net.cpp:84] Creating Layer bn4
I1123 22:52:51.472468 22756 net.cpp:406] bn4 <- conv4
I1123 22:52:51.472468 22756 net.cpp:367] bn4 -> conv4 (in-place)
I1123 22:52:51.472468 22756 net.cpp:122] Setting up bn4
I1123 22:52:51.472468 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.472468 22756 net.cpp:137] Memory required for data: 568116400
I1123 22:52:51.472468 22756 layer_factory.cpp:58] Creating layer scale4
I1123 22:52:51.472468 22756 net.cpp:84] Creating Layer scale4
I1123 22:52:51.472468 22756 net.cpp:406] scale4 <- conv4
I1123 22:52:51.472468 22756 net.cpp:367] scale4 -> conv4 (in-place)
I1123 22:52:51.472468 22756 layer_factory.cpp:58] Creating layer scale4
I1123 22:52:51.472468 22756 net.cpp:122] Setting up scale4
I1123 22:52:51.472468 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.472468 22756 net.cpp:137] Memory required for data: 581223600
I1123 22:52:51.472468 22756 layer_factory.cpp:58] Creating layer relu4
I1123 22:52:51.472468 22756 net.cpp:84] Creating Layer relu4
I1123 22:52:51.472468 22756 net.cpp:406] relu4 <- conv4
I1123 22:52:51.472468 22756 net.cpp:367] relu4 -> conv4 (in-place)
I1123 22:52:51.472468 22756 net.cpp:122] Setting up relu4
I1123 22:52:51.472468 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.472468 22756 net.cpp:137] Memory required for data: 594330800
I1123 22:52:51.472468 22756 layer_factory.cpp:58] Creating layer conv4_1
I1123 22:52:51.472468 22756 net.cpp:84] Creating Layer conv4_1
I1123 22:52:51.472468 22756 net.cpp:406] conv4_1 <- conv4
I1123 22:52:51.472468 22756 net.cpp:380] conv4_1 -> conv4_1
I1123 22:52:51.474470 22756 net.cpp:122] Setting up conv4_1
I1123 22:52:51.474470 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.474470 22756 net.cpp:137] Memory required for data: 607438000
I1123 22:52:51.475456 22756 layer_factory.cpp:58] Creating layer bn4_1
I1123 22:52:51.475456 22756 net.cpp:84] Creating Layer bn4_1
I1123 22:52:51.475456 22756 net.cpp:406] bn4_1 <- conv4_1
I1123 22:52:51.475456 22756 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1123 22:52:51.475456 22756 net.cpp:122] Setting up bn4_1
I1123 22:52:51.475456 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.475456 22756 net.cpp:137] Memory required for data: 620545200
I1123 22:52:51.475456 22756 layer_factory.cpp:58] Creating layer scale4_1
I1123 22:52:51.475456 22756 net.cpp:84] Creating Layer scale4_1
I1123 22:52:51.475456 22756 net.cpp:406] scale4_1 <- conv4_1
I1123 22:52:51.475456 22756 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1123 22:52:51.475456 22756 layer_factory.cpp:58] Creating layer scale4_1
I1123 22:52:51.475456 22756 net.cpp:122] Setting up scale4_1
I1123 22:52:51.475456 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.475456 22756 net.cpp:137] Memory required for data: 633652400
I1123 22:52:51.475456 22756 layer_factory.cpp:58] Creating layer relu4_1
I1123 22:52:51.475456 22756 net.cpp:84] Creating Layer relu4_1
I1123 22:52:51.475456 22756 net.cpp:406] relu4_1 <- conv4_1
I1123 22:52:51.475456 22756 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1123 22:52:51.475456 22756 net.cpp:122] Setting up relu4_1
I1123 22:52:51.475456 22756 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1123 22:52:51.475456 22756 net.cpp:137] Memory required for data: 646759600
I1123 22:52:51.475456 22756 layer_factory.cpp:58] Creating layer conv4_2
I1123 22:52:51.475456 22756 net.cpp:84] Creating Layer conv4_2
I1123 22:52:51.475456 22756 net.cpp:406] conv4_2 <- conv4_1
I1123 22:52:51.475456 22756 net.cpp:380] conv4_2 -> conv4_2
I1123 22:52:51.478457 22756 net.cpp:122] Setting up conv4_2
I1123 22:52:51.478457 22756 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1123 22:52:51.478457 22756 net.cpp:137] Memory required for data: 668775600
I1123 22:52:51.478457 22756 layer_factory.cpp:58] Creating layer bn4_2
I1123 22:52:51.478457 22756 net.cpp:84] Creating Layer bn4_2
I1123 22:52:51.478457 22756 net.cpp:406] bn4_2 <- conv4_2
I1123 22:52:51.478457 22756 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1123 22:52:51.478457 22756 net.cpp:122] Setting up bn4_2
I1123 22:52:51.478457 22756 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1123 22:52:51.478457 22756 net.cpp:137] Memory required for data: 690791600
I1123 22:52:51.478457 22756 layer_factory.cpp:58] Creating layer scale4_2
I1123 22:52:51.478457 22756 net.cpp:84] Creating Layer scale4_2
I1123 22:52:51.478457 22756 net.cpp:406] scale4_2 <- conv4_2
I1123 22:52:51.478457 22756 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1123 22:52:51.478457 22756 layer_factory.cpp:58] Creating layer scale4_2
I1123 22:52:51.479457 22756 net.cpp:122] Setting up scale4_2
I1123 22:52:51.479457 22756 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1123 22:52:51.479457 22756 net.cpp:137] Memory required for data: 712807600
I1123 22:52:51.479457 22756 layer_factory.cpp:58] Creating layer relu4_2
I1123 22:52:51.479457 22756 net.cpp:84] Creating Layer relu4_2
I1123 22:52:51.479457 22756 net.cpp:406] relu4_2 <- conv4_2
I1123 22:52:51.479457 22756 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1123 22:52:51.479457 22756 net.cpp:122] Setting up relu4_2
I1123 22:52:51.479457 22756 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1123 22:52:51.479457 22756 net.cpp:137] Memory required for data: 734823600
I1123 22:52:51.479457 22756 layer_factory.cpp:58] Creating layer pool4_2
I1123 22:52:51.479457 22756 net.cpp:84] Creating Layer pool4_2
I1123 22:52:51.479457 22756 net.cpp:406] pool4_2 <- conv4_2
I1123 22:52:51.479457 22756 net.cpp:380] pool4_2 -> pool4_2
I1123 22:52:51.479457 22756 net.cpp:122] Setting up pool4_2
I1123 22:52:51.479457 22756 net.cpp:129] Top shape: 100 215 8 8 (1376000)
I1123 22:52:51.479457 22756 net.cpp:137] Memory required for data: 740327600
I1123 22:52:51.479457 22756 layer_factory.cpp:58] Creating layer conv12
I1123 22:52:51.479457 22756 net.cpp:84] Creating Layer conv12
I1123 22:52:51.479457 22756 net.cpp:406] conv12 <- pool4_2
I1123 22:52:51.479457 22756 net.cpp:380] conv12 -> conv12
I1123 22:52:51.486475 22756 net.cpp:122] Setting up conv12
I1123 22:52:51.486475 22756 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1123 22:52:51.486475 22756 net.cpp:137] Memory required for data: 750158000
I1123 22:52:51.486475 22756 layer_factory.cpp:58] Creating layer bn_conv12
I1123 22:52:51.486475 22756 net.cpp:84] Creating Layer bn_conv12
I1123 22:52:51.486475 22756 net.cpp:406] bn_conv12 <- conv12
I1123 22:52:51.486987 22756 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1123 22:52:51.486987 22756 net.cpp:122] Setting up bn_conv12
I1123 22:52:51.486987 22756 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1123 22:52:51.486987 22756 net.cpp:137] Memory required for data: 759988400
I1123 22:52:51.486987 22756 layer_factory.cpp:58] Creating layer scale_conv12
I1123 22:52:51.486987 22756 net.cpp:84] Creating Layer scale_conv12
I1123 22:52:51.486987 22756 net.cpp:406] scale_conv12 <- conv12
I1123 22:52:51.486987 22756 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1123 22:52:51.486987 22756 layer_factory.cpp:58] Creating layer scale_conv12
I1123 22:52:51.486987 22756 net.cpp:122] Setting up scale_conv12
I1123 22:52:51.486987 22756 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1123 22:52:51.486987 22756 net.cpp:137] Memory required for data: 769818800
I1123 22:52:51.486987 22756 layer_factory.cpp:58] Creating layer relu_conv12
I1123 22:52:51.486987 22756 net.cpp:84] Creating Layer relu_conv12
I1123 22:52:51.486987 22756 net.cpp:406] relu_conv12 <- conv12
I1123 22:52:51.486987 22756 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1123 22:52:51.487985 22756 net.cpp:122] Setting up relu_conv12
I1123 22:52:51.487985 22756 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1123 22:52:51.487985 22756 net.cpp:137] Memory required for data: 779649200
I1123 22:52:51.487985 22756 layer_factory.cpp:58] Creating layer poolcp6
I1123 22:52:51.487985 22756 net.cpp:84] Creating Layer poolcp6
I1123 22:52:51.487985 22756 net.cpp:406] poolcp6 <- conv12
I1123 22:52:51.487985 22756 net.cpp:380] poolcp6 -> poolcp6
I1123 22:52:51.487985 22756 net.cpp:122] Setting up poolcp6
I1123 22:52:51.487985 22756 net.cpp:129] Top shape: 100 384 1 1 (38400)
I1123 22:52:51.487985 22756 net.cpp:137] Memory required for data: 779802800
I1123 22:52:51.487985 22756 layer_factory.cpp:58] Creating layer ip1
I1123 22:52:51.487985 22756 net.cpp:84] Creating Layer ip1
I1123 22:52:51.487985 22756 net.cpp:406] ip1 <- poolcp6
I1123 22:52:51.487985 22756 net.cpp:380] ip1 -> ip1
I1123 22:52:51.487985 22756 net.cpp:122] Setting up ip1
I1123 22:52:51.487985 22756 net.cpp:129] Top shape: 100 10 (1000)
I1123 22:52:51.487985 22756 net.cpp:137] Memory required for data: 779806800
I1123 22:52:51.487985 22756 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1123 22:52:51.487985 22756 net.cpp:84] Creating Layer ip1_ip1_0_split
I1123 22:52:51.487985 22756 net.cpp:406] ip1_ip1_0_split <- ip1
I1123 22:52:51.487985 22756 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1123 22:52:51.487985 22756 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1123 22:52:51.487985 22756 net.cpp:122] Setting up ip1_ip1_0_split
I1123 22:52:51.487985 22756 net.cpp:129] Top shape: 100 10 (1000)
I1123 22:52:51.487985 22756 net.cpp:129] Top shape: 100 10 (1000)
I1123 22:52:51.487985 22756 net.cpp:137] Memory required for data: 779814800
I1123 22:52:51.487985 22756 layer_factory.cpp:58] Creating layer accuracy
I1123 22:52:51.487985 22756 net.cpp:84] Creating Layer accuracy
I1123 22:52:51.487985 22756 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1123 22:52:51.487985 22756 net.cpp:406] accuracy <- label_cifar_1_split_0
I1123 22:52:51.487985 22756 net.cpp:380] accuracy -> accuracy
I1123 22:52:51.487985 22756 net.cpp:122] Setting up accuracy
I1123 22:52:51.488484 22756 net.cpp:129] Top shape: (1)
I1123 22:52:51.488484 22756 net.cpp:137] Memory required for data: 779814804
I1123 22:52:51.488484 22756 layer_factory.cpp:58] Creating layer loss
I1123 22:52:51.488484 22756 net.cpp:84] Creating Layer loss
I1123 22:52:51.488484 22756 net.cpp:406] loss <- ip1_ip1_0_split_1
I1123 22:52:51.488484 22756 net.cpp:406] loss <- label_cifar_1_split_1
I1123 22:52:51.488484 22756 net.cpp:380] loss -> loss
I1123 22:52:51.488484 22756 layer_factory.cpp:58] Creating layer loss
I1123 22:52:51.488484 22756 net.cpp:122] Setting up loss
I1123 22:52:51.488484 22756 net.cpp:129] Top shape: (1)
I1123 22:52:51.488484 22756 net.cpp:132]     with loss weight 1
I1123 22:52:51.488484 22756 net.cpp:137] Memory required for data: 779814808
I1123 22:52:51.488484 22756 net.cpp:198] loss needs backward computation.
I1123 22:52:51.488484 22756 net.cpp:200] accuracy does not need backward computation.
I1123 22:52:51.488484 22756 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1123 22:52:51.488484 22756 net.cpp:198] ip1 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] poolcp6 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] relu_conv12 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] scale_conv12 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] bn_conv12 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] conv12 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] pool4_2 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] relu4_2 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] scale4_2 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] bn4_2 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] conv4_2 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] relu4_1 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] scale4_1 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] bn4_1 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] conv4_1 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] relu4 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] scale4 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] bn4 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] conv4 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] relu3 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] scale3 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] bn3 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] conv3 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] pool2_1 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] relu2_2 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] scale2_2 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] bn2_2 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] conv2_2 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] relu2 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] scale2 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] bn2 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] conv2 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] relu1 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] scale1 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] bn1 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:198] conv1 needs backward computation.
I1123 22:52:51.488984 22756 net.cpp:200] label_cifar_1_split does not need backward computation.
I1123 22:52:51.488984 22756 net.cpp:200] cifar does not need backward computation.
I1123 22:52:51.488984 22756 net.cpp:242] This network produces output accuracy
I1123 22:52:51.488984 22756 net.cpp:242] This network produces output loss
I1123 22:52:51.488984 22756 net.cpp:255] Network initialization done.
I1123 22:52:51.488984 22756 solver.cpp:56] Solver scaffolding done.
I1123 22:52:51.491484 22756 caffe.cpp:249] Starting Optimization
I1123 22:52:51.491484 22756 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_1.6M
I1123 22:52:51.491484 22756 solver.cpp:273] Learning Rate Policy: multistep
I1123 22:52:51.493975 22756 solver.cpp:330] Iteration 0, Testing net (#0)
I1123 22:52:51.495484 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:52:53.632705 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:52:53.715723 22756 solver.cpp:397]     Test net output #0: accuracy = 0.1009
I1123 22:52:53.715723 22756 solver.cpp:397]     Test net output #1: loss = 78.5243 (* 1 = 78.5243 loss)
I1123 22:52:53.837750 22756 solver.cpp:218] Iteration 0 (0 iter/s, 2.3462s/100 iters), loss = 4.0367
I1123 22:52:53.838752 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.12
I1123 22:52:53.838752 22756 solver.cpp:237]     Train net output #1: loss = 4.0367 (* 1 = 4.0367 loss)
I1123 22:52:53.838752 22756 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1123 22:53:01.699659 22756 solver.cpp:218] Iteration 100 (12.7219 iter/s, 7.86047s/100 iters), loss = 1.7741
I1123 22:53:01.699659 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.35
I1123 22:53:01.699659 22756 solver.cpp:237]     Train net output #1: loss = 1.7741 (* 1 = 1.7741 loss)
I1123 22:53:01.699659 22756 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1123 22:53:09.667611 22756 solver.cpp:218] Iteration 200 (12.5507 iter/s, 7.96766s/100 iters), loss = 1.81625
I1123 22:53:09.667611 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.33
I1123 22:53:09.667611 22756 solver.cpp:237]     Train net output #1: loss = 1.81625 (* 1 = 1.81625 loss)
I1123 22:53:09.667611 22756 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1123 22:53:17.551933 22756 solver.cpp:218] Iteration 300 (12.6844 iter/s, 7.88372s/100 iters), loss = 1.73655
I1123 22:53:17.551933 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.52
I1123 22:53:17.551933 22756 solver.cpp:237]     Train net output #1: loss = 1.73655 (* 1 = 1.73655 loss)
I1123 22:53:17.551933 22756 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1123 22:53:25.479629 22756 solver.cpp:218] Iteration 400 (12.6149 iter/s, 7.9271s/100 iters), loss = 1.5704
I1123 22:53:25.479629 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.5
I1123 22:53:25.479629 22756 solver.cpp:237]     Train net output #1: loss = 1.5704 (* 1 = 1.5704 loss)
I1123 22:53:25.479629 22756 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1123 22:53:33.045564 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:53:33.364624 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_500.caffemodel
I1123 22:53:33.411608 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_500.solverstate
I1123 22:53:33.432608 22756 solver.cpp:330] Iteration 500, Testing net (#0)
I1123 22:53:33.432608 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:53:35.529801 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:53:35.613808 22756 solver.cpp:397]     Test net output #0: accuracy = 0.4118
I1123 22:53:35.613808 22756 solver.cpp:397]     Test net output #1: loss = 1.57562 (* 1 = 1.57562 loss)
I1123 22:53:35.690814 22756 solver.cpp:218] Iteration 500 (9.79363 iter/s, 10.2107s/100 iters), loss = 1.40511
I1123 22:53:35.690814 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.55
I1123 22:53:35.690814 22756 solver.cpp:237]     Train net output #1: loss = 1.40511 (* 1 = 1.40511 loss)
I1123 22:53:35.690814 22756 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1123 22:53:43.593072 22756 solver.cpp:218] Iteration 600 (12.6553 iter/s, 7.90183s/100 iters), loss = 1.28563
I1123 22:53:43.593072 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.57
I1123 22:53:43.593072 22756 solver.cpp:237]     Train net output #1: loss = 1.28563 (* 1 = 1.28563 loss)
I1123 22:53:43.593072 22756 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1123 22:53:51.572412 22756 solver.cpp:218] Iteration 700 (12.5325 iter/s, 7.97927s/100 iters), loss = 1.05968
I1123 22:53:51.572412 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.61
I1123 22:53:51.572412 22756 solver.cpp:237]     Train net output #1: loss = 1.05968 (* 1 = 1.05968 loss)
I1123 22:53:51.572412 22756 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1123 22:53:59.583921 22756 solver.cpp:218] Iteration 800 (12.4833 iter/s, 8.01072s/100 iters), loss = 0.940554
I1123 22:53:59.583921 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1123 22:53:59.583921 22756 solver.cpp:237]     Train net output #1: loss = 0.940554 (* 1 = 0.940554 loss)
I1123 22:53:59.583921 22756 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1123 22:54:07.545023 22756 solver.cpp:218] Iteration 900 (12.5619 iter/s, 7.9606s/100 iters), loss = 0.963145
I1123 22:54:07.545023 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.6
I1123 22:54:07.545023 22756 solver.cpp:237]     Train net output #1: loss = 0.963145 (* 1 = 0.963145 loss)
I1123 22:54:07.545023 22756 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1123 22:54:15.018934 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:54:15.329957 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_1000.caffemodel
I1123 22:54:15.369963 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_1000.solverstate
I1123 22:54:15.387953 22756 solver.cpp:330] Iteration 1000, Testing net (#0)
I1123 22:54:15.387953 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:54:17.466979 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:54:17.550995 22756 solver.cpp:397]     Test net output #0: accuracy = 0.5256
I1123 22:54:17.550995 22756 solver.cpp:397]     Test net output #1: loss = 1.24201 (* 1 = 1.24201 loss)
I1123 22:54:17.626998 22756 solver.cpp:218] Iteration 1000 (9.91873 iter/s, 10.0819s/100 iters), loss = 0.854904
I1123 22:54:17.626998 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1123 22:54:17.626998 22756 solver.cpp:237]     Train net output #1: loss = 0.854904 (* 1 = 0.854904 loss)
I1123 22:54:17.626998 22756 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1123 22:54:25.464584 22756 solver.cpp:218] Iteration 1100 (12.7601 iter/s, 7.8369s/100 iters), loss = 0.89569
I1123 22:54:25.464584 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1123 22:54:25.464584 22756 solver.cpp:237]     Train net output #1: loss = 0.89569 (* 1 = 0.89569 loss)
I1123 22:54:25.464584 22756 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1123 22:54:33.301120 22756 solver.cpp:218] Iteration 1200 (12.7623 iter/s, 7.83556s/100 iters), loss = 0.834997
I1123 22:54:33.301120 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1123 22:54:33.301120 22756 solver.cpp:237]     Train net output #1: loss = 0.834997 (* 1 = 0.834997 loss)
I1123 22:54:33.301120 22756 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1123 22:54:41.141638 22756 solver.cpp:218] Iteration 1300 (12.7557 iter/s, 7.83964s/100 iters), loss = 0.648265
I1123 22:54:41.141638 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 22:54:41.141638 22756 solver.cpp:237]     Train net output #1: loss = 0.648265 (* 1 = 0.648265 loss)
I1123 22:54:41.141638 22756 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1123 22:54:49.069476 22756 solver.cpp:218] Iteration 1400 (12.6137 iter/s, 7.92786s/100 iters), loss = 0.760136
I1123 22:54:49.069476 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1123 22:54:49.069476 22756 solver.cpp:237]     Train net output #1: loss = 0.760136 (* 1 = 0.760136 loss)
I1123 22:54:49.069476 22756 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1123 22:54:56.757818 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:54:57.071338 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_1500.caffemodel
I1123 22:54:57.111338 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_1500.solverstate
I1123 22:54:57.130339 22756 solver.cpp:330] Iteration 1500, Testing net (#0)
I1123 22:54:57.130339 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:54:59.218468 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:54:59.302515 22756 solver.cpp:397]     Test net output #0: accuracy = 0.6089
I1123 22:54:59.302515 22756 solver.cpp:397]     Test net output #1: loss = 1.0176 (* 1 = 1.0176 loss)
I1123 22:54:59.379513 22756 solver.cpp:218] Iteration 1500 (9.69976 iter/s, 10.3095s/100 iters), loss = 0.634057
I1123 22:54:59.379513 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 22:54:59.379513 22756 solver.cpp:237]     Train net output #1: loss = 0.634057 (* 1 = 0.634057 loss)
I1123 22:54:59.379513 22756 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1123 22:55:07.267165 22756 solver.cpp:218] Iteration 1600 (12.6799 iter/s, 7.88651s/100 iters), loss = 0.588019
I1123 22:55:07.267165 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1123 22:55:07.267165 22756 solver.cpp:237]     Train net output #1: loss = 0.588019 (* 1 = 0.588019 loss)
I1123 22:55:07.267165 22756 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1123 22:55:15.176350 22756 solver.cpp:218] Iteration 1700 (12.6437 iter/s, 7.9091s/100 iters), loss = 0.67792
I1123 22:55:15.176350 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1123 22:55:15.176350 22756 solver.cpp:237]     Train net output #1: loss = 0.67792 (* 1 = 0.67792 loss)
I1123 22:55:15.176350 22756 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1123 22:55:23.061336 22756 solver.cpp:218] Iteration 1800 (12.6827 iter/s, 7.88475s/100 iters), loss = 0.58784
I1123 22:55:23.061336 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1123 22:55:23.061336 22756 solver.cpp:237]     Train net output #1: loss = 0.58784 (* 1 = 0.58784 loss)
I1123 22:55:23.061336 22756 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1123 22:55:30.943614 22756 solver.cpp:218] Iteration 1900 (12.6882 iter/s, 7.88131s/100 iters), loss = 0.563724
I1123 22:55:30.943614 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1123 22:55:30.943614 22756 solver.cpp:237]     Train net output #1: loss = 0.563724 (* 1 = 0.563724 loss)
I1123 22:55:30.943614 22756 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1123 22:55:38.434756 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:55:38.746273 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_2000.caffemodel
I1123 22:55:38.783777 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_2000.solverstate
I1123 22:55:38.802778 22756 solver.cpp:330] Iteration 2000, Testing net (#0)
I1123 22:55:38.802778 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:55:40.919917 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:55:41.003921 22756 solver.cpp:397]     Test net output #0: accuracy = 0.6603
I1123 22:55:41.003921 22756 solver.cpp:397]     Test net output #1: loss = 0.97281 (* 1 = 0.97281 loss)
I1123 22:55:41.080943 22756 solver.cpp:218] Iteration 2000 (9.86485 iter/s, 10.137s/100 iters), loss = 0.493279
I1123 22:55:41.080943 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 22:55:41.080943 22756 solver.cpp:237]     Train net output #1: loss = 0.493279 (* 1 = 0.493279 loss)
I1123 22:55:41.080943 22756 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1123 22:55:48.931614 22756 solver.cpp:218] Iteration 2100 (12.7383 iter/s, 7.85036s/100 iters), loss = 0.484767
I1123 22:55:48.931614 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 22:55:48.931614 22756 solver.cpp:237]     Train net output #1: loss = 0.484767 (* 1 = 0.484767 loss)
I1123 22:55:48.931614 22756 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1123 22:55:56.822163 22756 solver.cpp:218] Iteration 2200 (12.6744 iter/s, 7.88991s/100 iters), loss = 0.578856
I1123 22:55:56.822163 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1123 22:55:56.822163 22756 solver.cpp:237]     Train net output #1: loss = 0.578856 (* 1 = 0.578856 loss)
I1123 22:55:56.822163 22756 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1123 22:56:04.680806 22756 solver.cpp:218] Iteration 2300 (12.7251 iter/s, 7.85847s/100 iters), loss = 0.665259
I1123 22:56:04.680806 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1123 22:56:04.680806 22756 solver.cpp:237]     Train net output #1: loss = 0.665259 (* 1 = 0.665259 loss)
I1123 22:56:04.681807 22756 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1123 22:56:12.590668 22756 solver.cpp:218] Iteration 2400 (12.6444 iter/s, 7.90862s/100 iters), loss = 0.4844
I1123 22:56:12.590668 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 22:56:12.590668 22756 solver.cpp:237]     Train net output #1: loss = 0.4844 (* 1 = 0.4844 loss)
I1123 22:56:12.590668 22756 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1123 22:56:20.074290 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:56:20.384307 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_2500.caffemodel
I1123 22:56:20.422307 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_2500.solverstate
I1123 22:56:20.441319 22756 solver.cpp:330] Iteration 2500, Testing net (#0)
I1123 22:56:20.441812 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:56:22.527374 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:56:22.611407 22756 solver.cpp:397]     Test net output #0: accuracy = 0.6771
I1123 22:56:22.611407 22756 solver.cpp:397]     Test net output #1: loss = 0.921133 (* 1 = 0.921133 loss)
I1123 22:56:22.688398 22756 solver.cpp:218] Iteration 2500 (9.90388 iter/s, 10.097s/100 iters), loss = 0.419674
I1123 22:56:22.688398 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 22:56:22.688398 22756 solver.cpp:237]     Train net output #1: loss = 0.419674 (* 1 = 0.419674 loss)
I1123 22:56:22.688398 22756 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1123 22:56:30.605163 22756 solver.cpp:218] Iteration 2600 (12.6317 iter/s, 7.91658s/100 iters), loss = 0.516634
I1123 22:56:30.605163 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 22:56:30.605163 22756 solver.cpp:237]     Train net output #1: loss = 0.516634 (* 1 = 0.516634 loss)
I1123 22:56:30.605163 22756 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1123 22:56:38.463023 22756 solver.cpp:218] Iteration 2700 (12.7271 iter/s, 7.85723s/100 iters), loss = 0.478178
I1123 22:56:38.463023 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 22:56:38.463023 22756 solver.cpp:237]     Train net output #1: loss = 0.478178 (* 1 = 0.478178 loss)
I1123 22:56:38.463023 22756 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1123 22:56:46.323695 22756 solver.cpp:218] Iteration 2800 (12.7222 iter/s, 7.86025s/100 iters), loss = 0.485904
I1123 22:56:46.323695 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 22:56:46.323695 22756 solver.cpp:237]     Train net output #1: loss = 0.485904 (* 1 = 0.485904 loss)
I1123 22:56:46.323695 22756 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1123 22:56:54.182284 22756 solver.cpp:218] Iteration 2900 (12.7261 iter/s, 7.8579s/100 iters), loss = 0.389818
I1123 22:56:54.182284 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 22:56:54.182284 22756 solver.cpp:237]     Train net output #1: loss = 0.389818 (* 1 = 0.389818 loss)
I1123 22:56:54.182284 22756 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1123 22:57:01.655028 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:57:01.965075 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_3000.caffemodel
I1123 22:57:02.005061 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_3000.solverstate
I1123 22:57:02.023075 22756 solver.cpp:330] Iteration 3000, Testing net (#0)
I1123 22:57:02.023075 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:57:04.104400 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:57:04.189415 22756 solver.cpp:397]     Test net output #0: accuracy = 0.7794
I1123 22:57:04.189415 22756 solver.cpp:397]     Test net output #1: loss = 0.672579 (* 1 = 0.672579 loss)
I1123 22:57:04.266417 22756 solver.cpp:218] Iteration 3000 (9.91717 iter/s, 10.0835s/100 iters), loss = 0.360918
I1123 22:57:04.266417 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 22:57:04.266417 22756 solver.cpp:237]     Train net output #1: loss = 0.360918 (* 1 = 0.360918 loss)
I1123 22:57:04.266417 22756 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1123 22:57:12.119385 22756 solver.cpp:218] Iteration 3100 (12.7342 iter/s, 7.85285s/100 iters), loss = 0.467011
I1123 22:57:12.119385 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1123 22:57:12.119385 22756 solver.cpp:237]     Train net output #1: loss = 0.467011 (* 1 = 0.467011 loss)
I1123 22:57:12.119385 22756 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1123 22:57:19.973922 22756 solver.cpp:218] Iteration 3200 (12.7323 iter/s, 7.85401s/100 iters), loss = 0.471096
I1123 22:57:19.973922 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 22:57:19.973922 22756 solver.cpp:237]     Train net output #1: loss = 0.471096 (* 1 = 0.471096 loss)
I1123 22:57:19.973922 22756 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1123 22:57:27.825585 22756 solver.cpp:218] Iteration 3300 (12.7368 iter/s, 7.85124s/100 iters), loss = 0.449602
I1123 22:57:27.825585 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 22:57:27.825585 22756 solver.cpp:237]     Train net output #1: loss = 0.449602 (* 1 = 0.449602 loss)
I1123 22:57:27.825585 22756 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1123 22:57:35.677217 22756 solver.cpp:218] Iteration 3400 (12.7363 iter/s, 7.85157s/100 iters), loss = 0.347089
I1123 22:57:35.677217 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1123 22:57:35.677217 22756 solver.cpp:237]     Train net output #1: loss = 0.347089 (* 1 = 0.347089 loss)
I1123 22:57:35.677217 22756 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1123 22:57:43.143939 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:57:43.452955 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_3500.caffemodel
I1123 22:57:43.490959 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_3500.solverstate
I1123 22:57:43.508960 22756 solver.cpp:330] Iteration 3500, Testing net (#0)
I1123 22:57:43.508960 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:57:45.590320 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:57:45.675324 22756 solver.cpp:397]     Test net output #0: accuracy = 0.6793
I1123 22:57:45.675324 22756 solver.cpp:397]     Test net output #1: loss = 0.937061 (* 1 = 0.937061 loss)
I1123 22:57:45.751834 22756 solver.cpp:218] Iteration 3500 (9.92696 iter/s, 10.0736s/100 iters), loss = 0.353725
I1123 22:57:45.751834 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 22:57:45.751834 22756 solver.cpp:237]     Train net output #1: loss = 0.353725 (* 1 = 0.353725 loss)
I1123 22:57:45.751834 22756 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1123 22:57:53.607236 22756 solver.cpp:218] Iteration 3600 (12.7304 iter/s, 7.85522s/100 iters), loss = 0.364199
I1123 22:57:53.607236 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 22:57:53.607236 22756 solver.cpp:237]     Train net output #1: loss = 0.364199 (* 1 = 0.364199 loss)
I1123 22:57:53.607236 22756 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1123 22:58:01.463290 22756 solver.cpp:218] Iteration 3700 (12.7304 iter/s, 7.85519s/100 iters), loss = 0.495034
I1123 22:58:01.463290 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 22:58:01.463290 22756 solver.cpp:237]     Train net output #1: loss = 0.495034 (* 1 = 0.495034 loss)
I1123 22:58:01.463290 22756 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1123 22:58:09.323370 22756 solver.cpp:218] Iteration 3800 (12.7222 iter/s, 7.86025s/100 iters), loss = 0.449492
I1123 22:58:09.324370 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 22:58:09.324370 22756 solver.cpp:237]     Train net output #1: loss = 0.449492 (* 1 = 0.449492 loss)
I1123 22:58:09.324370 22756 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1123 22:58:17.181021 22756 solver.cpp:218] Iteration 3900 (12.7282 iter/s, 7.8566s/100 iters), loss = 0.443314
I1123 22:58:17.181021 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 22:58:17.181021 22756 solver.cpp:237]     Train net output #1: loss = 0.443314 (* 1 = 0.443314 loss)
I1123 22:58:17.181021 22756 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1123 22:58:24.649713 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:58:24.961238 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_4000.caffemodel
I1123 22:58:25.000741 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_4000.solverstate
I1123 22:58:25.019742 22756 solver.cpp:330] Iteration 4000, Testing net (#0)
I1123 22:58:25.019742 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:58:27.101126 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:58:27.185133 22756 solver.cpp:397]     Test net output #0: accuracy = 0.7458
I1123 22:58:27.185133 22756 solver.cpp:397]     Test net output #1: loss = 0.758125 (* 1 = 0.758125 loss)
I1123 22:58:27.262133 22756 solver.cpp:218] Iteration 4000 (9.92052 iter/s, 10.0801s/100 iters), loss = 0.340375
I1123 22:58:27.262133 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 22:58:27.262133 22756 solver.cpp:237]     Train net output #1: loss = 0.340375 (* 1 = 0.340375 loss)
I1123 22:58:27.262133 22756 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1123 22:58:35.120975 22756 solver.cpp:218] Iteration 4100 (12.7252 iter/s, 7.85842s/100 iters), loss = 0.433085
I1123 22:58:35.120975 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 22:58:35.120975 22756 solver.cpp:237]     Train net output #1: loss = 0.433085 (* 1 = 0.433085 loss)
I1123 22:58:35.120975 22756 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1123 22:58:42.975675 22756 solver.cpp:218] Iteration 4200 (12.7321 iter/s, 7.85419s/100 iters), loss = 0.410545
I1123 22:58:42.975675 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 22:58:42.975675 22756 solver.cpp:237]     Train net output #1: loss = 0.410545 (* 1 = 0.410545 loss)
I1123 22:58:42.975675 22756 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1123 22:58:50.827291 22756 solver.cpp:218] Iteration 4300 (12.7356 iter/s, 7.852s/100 iters), loss = 0.415986
I1123 22:58:50.827291 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 22:58:50.827291 22756 solver.cpp:237]     Train net output #1: loss = 0.415986 (* 1 = 0.415986 loss)
I1123 22:58:50.827291 22756 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1123 22:58:58.684814 22756 solver.cpp:218] Iteration 4400 (12.7283 iter/s, 7.85654s/100 iters), loss = 0.337991
I1123 22:58:58.684814 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1123 22:58:58.684814 22756 solver.cpp:237]     Train net output #1: loss = 0.337991 (* 1 = 0.337991 loss)
I1123 22:58:58.684814 22756 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1123 22:59:06.154680 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:59:06.464725 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_4500.caffemodel
I1123 22:59:06.502733 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_4500.solverstate
I1123 22:59:06.521725 22756 solver.cpp:330] Iteration 4500, Testing net (#0)
I1123 22:59:06.521725 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:59:08.603960 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:59:08.687969 22756 solver.cpp:397]     Test net output #0: accuracy = 0.7621
I1123 22:59:08.687969 22756 solver.cpp:397]     Test net output #1: loss = 0.696895 (* 1 = 0.696895 loss)
I1123 22:59:08.764472 22756 solver.cpp:218] Iteration 4500 (9.92197 iter/s, 10.0786s/100 iters), loss = 0.427928
I1123 22:59:08.764472 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1123 22:59:08.764472 22756 solver.cpp:237]     Train net output #1: loss = 0.427928 (* 1 = 0.427928 loss)
I1123 22:59:08.764472 22756 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1123 22:59:16.619174 22756 solver.cpp:218] Iteration 4600 (12.7318 iter/s, 7.85432s/100 iters), loss = 0.347326
I1123 22:59:16.619174 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 22:59:16.619174 22756 solver.cpp:237]     Train net output #1: loss = 0.347326 (* 1 = 0.347326 loss)
I1123 22:59:16.619174 22756 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1123 22:59:24.473470 22756 solver.cpp:218] Iteration 4700 (12.7326 iter/s, 7.85383s/100 iters), loss = 0.475363
I1123 22:59:24.473470 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1123 22:59:24.473470 22756 solver.cpp:237]     Train net output #1: loss = 0.475363 (* 1 = 0.475363 loss)
I1123 22:59:24.473470 22756 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1123 22:59:32.328203 22756 solver.cpp:218] Iteration 4800 (12.7313 iter/s, 7.85463s/100 iters), loss = 0.41486
I1123 22:59:32.328203 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 22:59:32.328203 22756 solver.cpp:237]     Train net output #1: loss = 0.41486 (* 1 = 0.41486 loss)
I1123 22:59:32.328203 22756 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1123 22:59:40.183082 22756 solver.cpp:218] Iteration 4900 (12.7315 iter/s, 7.85451s/100 iters), loss = 0.341075
I1123 22:59:40.183082 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 22:59:40.183082 22756 solver.cpp:237]     Train net output #1: loss = 0.341075 (* 1 = 0.341075 loss)
I1123 22:59:40.183082 22756 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1123 22:59:47.653918 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:59:47.964946 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_5000.caffemodel
I1123 22:59:48.004959 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_5000.solverstate
I1123 22:59:48.022958 22756 solver.cpp:330] Iteration 5000, Testing net (#0)
I1123 22:59:48.022958 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 22:59:50.105275 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 22:59:50.189291 22756 solver.cpp:397]     Test net output #0: accuracy = 0.7592
I1123 22:59:50.189291 22756 solver.cpp:397]     Test net output #1: loss = 0.711309 (* 1 = 0.711309 loss)
I1123 22:59:50.266788 22756 solver.cpp:218] Iteration 5000 (9.91814 iter/s, 10.0825s/100 iters), loss = 0.406483
I1123 22:59:50.266788 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1123 22:59:50.266788 22756 solver.cpp:237]     Train net output #1: loss = 0.406483 (* 1 = 0.406483 loss)
I1123 22:59:50.266788 22756 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1123 22:59:50.266788 22756 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1123 22:59:58.121207 22756 solver.cpp:218] Iteration 5100 (12.7319 iter/s, 7.85426s/100 iters), loss = 0.24823
I1123 22:59:58.121207 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 22:59:58.121207 22756 solver.cpp:237]     Train net output #1: loss = 0.24823 (* 1 = 0.24823 loss)
I1123 22:59:58.121207 22756 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1123 23:00:06.003098 22756 solver.cpp:218] Iteration 5200 (12.6886 iter/s, 7.8811s/100 iters), loss = 0.276168
I1123 23:00:06.003098 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1123 23:00:06.003098 22756 solver.cpp:237]     Train net output #1: loss = 0.276168 (* 1 = 0.276168 loss)
I1123 23:00:06.003098 22756 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1123 23:00:13.857986 22756 solver.cpp:218] Iteration 5300 (12.7307 iter/s, 7.85505s/100 iters), loss = 0.208682
I1123 23:00:13.857986 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:00:13.857986 22756 solver.cpp:237]     Train net output #1: loss = 0.208682 (* 1 = 0.208682 loss)
I1123 23:00:13.857986 22756 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1123 23:00:21.713063 22756 solver.cpp:218] Iteration 5400 (12.7328 iter/s, 7.85374s/100 iters), loss = 0.205539
I1123 23:00:21.713063 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:00:21.713063 22756 solver.cpp:237]     Train net output #1: loss = 0.205539 (* 1 = 0.205539 loss)
I1123 23:00:21.713063 22756 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1123 23:00:29.180876 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:00:29.491415 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_5500.caffemodel
I1123 23:00:29.529927 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_5500.solverstate
I1123 23:00:29.547920 22756 solver.cpp:330] Iteration 5500, Testing net (#0)
I1123 23:00:29.547920 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:00:31.629142 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:00:31.713153 22756 solver.cpp:397]     Test net output #0: accuracy = 0.8968
I1123 23:00:31.713153 22756 solver.cpp:397]     Test net output #1: loss = 0.31007 (* 1 = 0.31007 loss)
I1123 23:00:31.789151 22756 solver.cpp:218] Iteration 5500 (9.92433 iter/s, 10.0763s/100 iters), loss = 0.181767
I1123 23:00:31.789151 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:00:31.789151 22756 solver.cpp:237]     Train net output #1: loss = 0.181767 (* 1 = 0.181767 loss)
I1123 23:00:31.789151 22756 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1123 23:00:39.646237 22756 solver.cpp:218] Iteration 5600 (12.7292 iter/s, 7.85595s/100 iters), loss = 0.178234
I1123 23:00:39.646237 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 23:00:39.646237 22756 solver.cpp:237]     Train net output #1: loss = 0.178234 (* 1 = 0.178234 loss)
I1123 23:00:39.646237 22756 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1123 23:00:47.500181 22756 solver.cpp:218] Iteration 5700 (12.7333 iter/s, 7.85344s/100 iters), loss = 0.22979
I1123 23:00:47.500181 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 23:00:47.500181 22756 solver.cpp:237]     Train net output #1: loss = 0.22979 (* 1 = 0.22979 loss)
I1123 23:00:47.500181 22756 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1123 23:00:55.354431 22756 solver.cpp:218] Iteration 5800 (12.7316 iter/s, 7.85447s/100 iters), loss = 0.188092
I1123 23:00:55.354431 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:00:55.354431 22756 solver.cpp:237]     Train net output #1: loss = 0.188092 (* 1 = 0.188092 loss)
I1123 23:00:55.354431 22756 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1123 23:01:03.216327 22756 solver.cpp:218] Iteration 5900 (12.7206 iter/s, 7.86124s/100 iters), loss = 0.170347
I1123 23:01:03.216327 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:01:03.216327 22756 solver.cpp:237]     Train net output #1: loss = 0.170347 (* 1 = 0.170347 loss)
I1123 23:01:03.216327 22756 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1123 23:01:10.683364 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:01:10.994413 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_6000.caffemodel
I1123 23:01:11.033428 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_6000.solverstate
I1123 23:01:11.051427 22756 solver.cpp:330] Iteration 6000, Testing net (#0)
I1123 23:01:11.051427 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:01:13.131814 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:01:13.215849 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9023
I1123 23:01:13.215849 22756 solver.cpp:397]     Test net output #1: loss = 0.288366 (* 1 = 0.288366 loss)
I1123 23:01:13.292835 22756 solver.cpp:218] Iteration 6000 (9.92424 iter/s, 10.0763s/100 iters), loss = 0.181787
I1123 23:01:13.293836 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:01:13.293836 22756 solver.cpp:237]     Train net output #1: loss = 0.181787 (* 1 = 0.181787 loss)
I1123 23:01:13.293836 22756 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1123 23:01:21.152842 22756 solver.cpp:218] Iteration 6100 (12.7245 iter/s, 7.85885s/100 iters), loss = 0.1905
I1123 23:01:21.152842 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 23:01:21.152842 22756 solver.cpp:237]     Train net output #1: loss = 0.1905 (* 1 = 0.1905 loss)
I1123 23:01:21.152842 22756 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1123 23:01:29.007284 22756 solver.cpp:218] Iteration 6200 (12.7326 iter/s, 7.85384s/100 iters), loss = 0.207365
I1123 23:01:29.007284 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1123 23:01:29.007284 22756 solver.cpp:237]     Train net output #1: loss = 0.207365 (* 1 = 0.207365 loss)
I1123 23:01:29.007284 22756 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1123 23:01:36.862121 22756 solver.cpp:218] Iteration 6300 (12.732 iter/s, 7.85424s/100 iters), loss = 0.157916
I1123 23:01:36.862121 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:01:36.862121 22756 solver.cpp:237]     Train net output #1: loss = 0.157916 (* 1 = 0.157916 loss)
I1123 23:01:36.862121 22756 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1123 23:01:44.716006 22756 solver.cpp:218] Iteration 6400 (12.7334 iter/s, 7.85336s/100 iters), loss = 0.136598
I1123 23:01:44.716006 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:01:44.716006 22756 solver.cpp:237]     Train net output #1: loss = 0.136598 (* 1 = 0.136598 loss)
I1123 23:01:44.716006 22756 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1123 23:01:52.183810 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:01:52.492846 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_6500.caffemodel
I1123 23:01:52.532851 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_6500.solverstate
I1123 23:01:52.550850 22756 solver.cpp:330] Iteration 6500, Testing net (#0)
I1123 23:01:52.550850 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:01:54.632179 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:01:54.716188 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9061
I1123 23:01:54.716188 22756 solver.cpp:397]     Test net output #1: loss = 0.280422 (* 1 = 0.280422 loss)
I1123 23:01:54.793191 22756 solver.cpp:218] Iteration 6500 (9.924 iter/s, 10.0766s/100 iters), loss = 0.137828
I1123 23:01:54.793191 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:01:54.793191 22756 solver.cpp:237]     Train net output #1: loss = 0.137828 (* 1 = 0.137828 loss)
I1123 23:01:54.793191 22756 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1123 23:02:02.646067 22756 solver.cpp:218] Iteration 6600 (12.7337 iter/s, 7.85319s/100 iters), loss = 0.145762
I1123 23:02:02.646067 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:02:02.646067 22756 solver.cpp:237]     Train net output #1: loss = 0.145762 (* 1 = 0.145762 loss)
I1123 23:02:02.646067 22756 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1123 23:02:10.501523 22756 solver.cpp:218] Iteration 6700 (12.7313 iter/s, 7.85463s/100 iters), loss = 0.180994
I1123 23:02:10.502024 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:02:10.502024 22756 solver.cpp:237]     Train net output #1: loss = 0.180994 (* 1 = 0.180994 loss)
I1123 23:02:10.502024 22756 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1123 23:02:18.356940 22756 solver.cpp:218] Iteration 6800 (12.7315 iter/s, 7.85452s/100 iters), loss = 0.146483
I1123 23:02:18.356940 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:02:18.356940 22756 solver.cpp:237]     Train net output #1: loss = 0.146483 (* 1 = 0.146483 loss)
I1123 23:02:18.356940 22756 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1123 23:02:26.213199 22756 solver.cpp:218] Iteration 6900 (12.7288 iter/s, 7.85617s/100 iters), loss = 0.0942484
I1123 23:02:26.213199 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:02:26.213199 22756 solver.cpp:237]     Train net output #1: loss = 0.0942483 (* 1 = 0.0942483 loss)
I1123 23:02:26.213199 22756 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1123 23:02:33.678002 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:02:33.989050 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_7000.caffemodel
I1123 23:02:34.029055 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_7000.solverstate
I1123 23:02:34.048055 22756 solver.cpp:330] Iteration 7000, Testing net (#0)
I1123 23:02:34.048055 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:02:36.129392 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:02:36.213400 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9053
I1123 23:02:36.213400 22756 solver.cpp:397]     Test net output #1: loss = 0.279525 (* 1 = 0.279525 loss)
I1123 23:02:36.289398 22756 solver.cpp:218] Iteration 7000 (9.92443 iter/s, 10.0761s/100 iters), loss = 0.128474
I1123 23:02:36.290400 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:02:36.290400 22756 solver.cpp:237]     Train net output #1: loss = 0.128474 (* 1 = 0.128474 loss)
I1123 23:02:36.290400 22756 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1123 23:02:44.152734 22756 solver.cpp:218] Iteration 7100 (12.719 iter/s, 7.86223s/100 iters), loss = 0.102108
I1123 23:02:44.152734 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:02:44.152734 22756 solver.cpp:237]     Train net output #1: loss = 0.102108 (* 1 = 0.102108 loss)
I1123 23:02:44.152734 22756 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1123 23:02:52.012791 22756 solver.cpp:218] Iteration 7200 (12.723 iter/s, 7.85979s/100 iters), loss = 0.164045
I1123 23:02:52.012791 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:02:52.012791 22756 solver.cpp:237]     Train net output #1: loss = 0.164045 (* 1 = 0.164045 loss)
I1123 23:02:52.012791 22756 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1123 23:02:59.872156 22756 solver.cpp:218] Iteration 7300 (12.7255 iter/s, 7.85824s/100 iters), loss = 0.114684
I1123 23:02:59.872156 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:02:59.872156 22756 solver.cpp:237]     Train net output #1: loss = 0.114684 (* 1 = 0.114684 loss)
I1123 23:02:59.872156 22756 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1123 23:03:07.731268 22756 solver.cpp:218] Iteration 7400 (12.7234 iter/s, 7.85956s/100 iters), loss = 0.0993055
I1123 23:03:07.732270 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:03:07.732270 22756 solver.cpp:237]     Train net output #1: loss = 0.0993055 (* 1 = 0.0993055 loss)
I1123 23:03:07.732270 22756 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1123 23:03:15.204255 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:03:15.515287 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_7500.caffemodel
I1123 23:03:15.555805 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_7500.solverstate
I1123 23:03:15.574295 22756 solver.cpp:330] Iteration 7500, Testing net (#0)
I1123 23:03:15.574796 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:03:17.655704 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:03:17.739713 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9089
I1123 23:03:17.739713 22756 solver.cpp:397]     Test net output #1: loss = 0.272165 (* 1 = 0.272165 loss)
I1123 23:03:17.815721 22756 solver.cpp:218] Iteration 7500 (9.91689 iter/s, 10.0838s/100 iters), loss = 0.110392
I1123 23:03:17.815721 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:03:17.815721 22756 solver.cpp:237]     Train net output #1: loss = 0.110392 (* 1 = 0.110392 loss)
I1123 23:03:17.815721 22756 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1123 23:03:25.672528 22756 solver.cpp:218] Iteration 7600 (12.7289 iter/s, 7.85613s/100 iters), loss = 0.168107
I1123 23:03:25.672528 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:03:25.672528 22756 solver.cpp:237]     Train net output #1: loss = 0.168107 (* 1 = 0.168107 loss)
I1123 23:03:25.672528 22756 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1123 23:03:33.526540 22756 solver.cpp:218] Iteration 7700 (12.7326 iter/s, 7.85386s/100 iters), loss = 0.16388
I1123 23:03:33.527542 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1123 23:03:33.527542 22756 solver.cpp:237]     Train net output #1: loss = 0.16388 (* 1 = 0.16388 loss)
I1123 23:03:33.527542 22756 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1123 23:03:41.382666 22756 solver.cpp:218] Iteration 7800 (12.7313 iter/s, 7.85465s/100 iters), loss = 0.158638
I1123 23:03:41.382666 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:03:41.382666 22756 solver.cpp:237]     Train net output #1: loss = 0.158638 (* 1 = 0.158638 loss)
I1123 23:03:41.382666 22756 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1123 23:03:49.238557 22756 solver.cpp:218] Iteration 7900 (12.7291 iter/s, 7.85603s/100 iters), loss = 0.116498
I1123 23:03:49.238557 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:03:49.238557 22756 solver.cpp:237]     Train net output #1: loss = 0.116498 (* 1 = 0.116498 loss)
I1123 23:03:49.238557 22756 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1123 23:03:56.708436 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:03:57.018478 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_8000.caffemodel
I1123 23:03:57.057494 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_8000.solverstate
I1123 23:03:57.075485 22756 solver.cpp:330] Iteration 8000, Testing net (#0)
I1123 23:03:57.075485 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:03:59.157563 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:03:59.241598 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9031
I1123 23:03:59.241598 22756 solver.cpp:397]     Test net output #1: loss = 0.29366 (* 1 = 0.29366 loss)
I1123 23:03:59.318614 22756 solver.cpp:218] Iteration 8000 (9.92146 iter/s, 10.0792s/100 iters), loss = 0.116278
I1123 23:03:59.318614 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:03:59.318614 22756 solver.cpp:237]     Train net output #1: loss = 0.116278 (* 1 = 0.116278 loss)
I1123 23:03:59.318614 22756 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1123 23:04:07.176514 22756 solver.cpp:218] Iteration 8100 (12.7272 iter/s, 7.85716s/100 iters), loss = 0.169219
I1123 23:04:07.176514 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:04:07.176514 22756 solver.cpp:237]     Train net output #1: loss = 0.169219 (* 1 = 0.169219 loss)
I1123 23:04:07.176514 22756 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1123 23:04:15.035847 22756 solver.cpp:218] Iteration 8200 (12.724 iter/s, 7.85915s/100 iters), loss = 0.153456
I1123 23:04:15.035847 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:04:15.035847 22756 solver.cpp:237]     Train net output #1: loss = 0.153456 (* 1 = 0.153456 loss)
I1123 23:04:15.035847 22756 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1123 23:04:22.895865 22756 solver.cpp:218] Iteration 8300 (12.7233 iter/s, 7.85959s/100 iters), loss = 0.124989
I1123 23:04:22.895865 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:04:22.895865 22756 solver.cpp:237]     Train net output #1: loss = 0.124989 (* 1 = 0.124989 loss)
I1123 23:04:22.895865 22756 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1123 23:04:30.754827 22756 solver.cpp:218] Iteration 8400 (12.725 iter/s, 7.85856s/100 iters), loss = 0.0997149
I1123 23:04:30.754827 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:04:30.754827 22756 solver.cpp:237]     Train net output #1: loss = 0.0997149 (* 1 = 0.0997149 loss)
I1123 23:04:30.754827 22756 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1123 23:04:38.223773 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:04:38.535799 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_8500.caffemodel
I1123 23:04:38.574810 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_8500.solverstate
I1123 23:04:38.592808 22756 solver.cpp:330] Iteration 8500, Testing net (#0)
I1123 23:04:38.592808 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:04:40.674232 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:04:40.758736 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9128
I1123 23:04:40.758736 22756 solver.cpp:397]     Test net output #1: loss = 0.271872 (* 1 = 0.271872 loss)
I1123 23:04:40.835238 22756 solver.cpp:218] Iteration 8500 (9.92133 iter/s, 10.0793s/100 iters), loss = 0.105236
I1123 23:04:40.835238 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:04:40.835238 22756 solver.cpp:237]     Train net output #1: loss = 0.105236 (* 1 = 0.105236 loss)
I1123 23:04:40.835238 22756 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1123 23:04:48.688130 22756 solver.cpp:218] Iteration 8600 (12.7349 iter/s, 7.85245s/100 iters), loss = 0.165723
I1123 23:04:48.688130 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:04:48.688130 22756 solver.cpp:237]     Train net output #1: loss = 0.165723 (* 1 = 0.165723 loss)
I1123 23:04:48.688130 22756 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1123 23:04:56.541010 22756 solver.cpp:218] Iteration 8700 (12.7351 iter/s, 7.8523s/100 iters), loss = 0.143063
I1123 23:04:56.541010 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:04:56.541010 22756 solver.cpp:237]     Train net output #1: loss = 0.143063 (* 1 = 0.143063 loss)
I1123 23:04:56.541010 22756 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1123 23:05:04.394093 22756 solver.cpp:218] Iteration 8800 (12.7348 iter/s, 7.8525s/100 iters), loss = 0.0944759
I1123 23:05:04.394093 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:05:04.394093 22756 solver.cpp:237]     Train net output #1: loss = 0.0944759 (* 1 = 0.0944759 loss)
I1123 23:05:04.394093 22756 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1123 23:05:12.248630 22756 solver.cpp:218] Iteration 8900 (12.7323 iter/s, 7.85403s/100 iters), loss = 0.0925041
I1123 23:05:12.248630 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:05:12.248630 22756 solver.cpp:237]     Train net output #1: loss = 0.092504 (* 1 = 0.092504 loss)
I1123 23:05:12.248630 22756 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1123 23:05:19.712440 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:05:20.023468 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_9000.caffemodel
I1123 23:05:20.063475 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_9000.solverstate
I1123 23:05:20.081475 22756 solver.cpp:330] Iteration 9000, Testing net (#0)
I1123 23:05:20.081976 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:05:22.162705 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:05:22.246716 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9056
I1123 23:05:22.246716 22756 solver.cpp:397]     Test net output #1: loss = 0.280622 (* 1 = 0.280622 loss)
I1123 23:05:22.322713 22756 solver.cpp:218] Iteration 9000 (9.92614 iter/s, 10.0744s/100 iters), loss = 0.131723
I1123 23:05:22.322713 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:05:22.322713 22756 solver.cpp:237]     Train net output #1: loss = 0.131723 (* 1 = 0.131723 loss)
I1123 23:05:22.322713 22756 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1123 23:05:30.176653 22756 solver.cpp:218] Iteration 9100 (12.7341 iter/s, 7.85291s/100 iters), loss = 0.0847825
I1123 23:05:30.176653 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:05:30.176653 22756 solver.cpp:237]     Train net output #1: loss = 0.0847825 (* 1 = 0.0847825 loss)
I1123 23:05:30.176653 22756 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1123 23:05:38.030519 22756 solver.cpp:218] Iteration 9200 (12.7325 iter/s, 7.85391s/100 iters), loss = 0.0916888
I1123 23:05:38.030519 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:05:38.030519 22756 solver.cpp:237]     Train net output #1: loss = 0.0916887 (* 1 = 0.0916887 loss)
I1123 23:05:38.030519 22756 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1123 23:05:45.882400 22756 solver.cpp:218] Iteration 9300 (12.7378 iter/s, 7.85066s/100 iters), loss = 0.0771482
I1123 23:05:45.882400 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:05:45.882400 22756 solver.cpp:237]     Train net output #1: loss = 0.0771481 (* 1 = 0.0771481 loss)
I1123 23:05:45.882400 22756 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1123 23:05:53.733319 22756 solver.cpp:218] Iteration 9400 (12.7376 iter/s, 7.85076s/100 iters), loss = 0.0441005
I1123 23:05:53.733319 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:05:53.733319 22756 solver.cpp:237]     Train net output #1: loss = 0.0441004 (* 1 = 0.0441004 loss)
I1123 23:05:53.733319 22756 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1123 23:06:01.201231 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:06:01.512254 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_9500.caffemodel
I1123 23:06:01.550774 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_9500.solverstate
I1123 23:06:01.569916 22756 solver.cpp:330] Iteration 9500, Testing net (#0)
I1123 23:06:01.569916 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:06:03.651649 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:06:03.735653 22756 solver.cpp:397]     Test net output #0: accuracy = 0.906
I1123 23:06:03.735653 22756 solver.cpp:397]     Test net output #1: loss = 0.283061 (* 1 = 0.283061 loss)
I1123 23:06:03.812661 22756 solver.cpp:218] Iteration 9500 (9.92215 iter/s, 10.0785s/100 iters), loss = 0.125928
I1123 23:06:03.812661 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:06:03.812661 22756 solver.cpp:237]     Train net output #1: loss = 0.125928 (* 1 = 0.125928 loss)
I1123 23:06:03.812661 22756 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1123 23:06:03.812661 22756 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1123 23:06:11.671542 22756 solver.cpp:218] Iteration 9600 (12.725 iter/s, 7.85853s/100 iters), loss = 0.178215
I1123 23:06:11.671542 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:06:11.671542 22756 solver.cpp:237]     Train net output #1: loss = 0.178215 (* 1 = 0.178215 loss)
I1123 23:06:11.671542 22756 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1123 23:06:19.531329 22756 solver.cpp:218] Iteration 9700 (12.723 iter/s, 7.85976s/100 iters), loss = 0.0916613
I1123 23:06:19.531329 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:06:19.531329 22756 solver.cpp:237]     Train net output #1: loss = 0.0916612 (* 1 = 0.0916612 loss)
I1123 23:06:19.531329 22756 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1123 23:06:27.386544 22756 solver.cpp:218] Iteration 9800 (12.7311 iter/s, 7.85479s/100 iters), loss = 0.0618388
I1123 23:06:27.386544 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:06:27.386544 22756 solver.cpp:237]     Train net output #1: loss = 0.0618387 (* 1 = 0.0618387 loss)
I1123 23:06:27.386544 22756 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1123 23:06:35.249951 22756 solver.cpp:218] Iteration 9900 (12.719 iter/s, 7.86223s/100 iters), loss = 0.0520559
I1123 23:06:35.249951 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:06:35.249951 22756 solver.cpp:237]     Train net output #1: loss = 0.0520558 (* 1 = 0.0520558 loss)
I1123 23:06:35.249951 22756 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1123 23:06:42.719332 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:06:43.031391 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_10000.caffemodel
I1123 23:06:43.072196 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_10000.solverstate
I1123 23:06:43.091207 22756 solver.cpp:330] Iteration 10000, Testing net (#0)
I1123 23:06:43.091207 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:06:45.174942 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:06:45.257972 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9174
I1123 23:06:45.258991 22756 solver.cpp:397]     Test net output #1: loss = 0.249649 (* 1 = 0.249649 loss)
I1123 23:06:45.334987 22756 solver.cpp:218] Iteration 10000 (9.91576 iter/s, 10.085s/100 iters), loss = 0.080979
I1123 23:06:45.334987 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:06:45.334987 22756 solver.cpp:237]     Train net output #1: loss = 0.0809789 (* 1 = 0.0809789 loss)
I1123 23:06:45.334987 22756 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1123 23:06:53.192903 22756 solver.cpp:218] Iteration 10100 (12.7274 iter/s, 7.85707s/100 iters), loss = 0.093381
I1123 23:06:53.192903 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:06:53.192903 22756 solver.cpp:237]     Train net output #1: loss = 0.0933809 (* 1 = 0.0933809 loss)
I1123 23:06:53.192903 22756 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1123 23:07:01.047389 22756 solver.cpp:218] Iteration 10200 (12.7326 iter/s, 7.85386s/100 iters), loss = 0.146852
I1123 23:07:01.047389 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:07:01.047389 22756 solver.cpp:237]     Train net output #1: loss = 0.146852 (* 1 = 0.146852 loss)
I1123 23:07:01.047389 22756 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1123 23:07:08.903782 22756 solver.cpp:218] Iteration 10300 (12.7285 iter/s, 7.85636s/100 iters), loss = 0.0730397
I1123 23:07:08.903782 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:07:08.903782 22756 solver.cpp:237]     Train net output #1: loss = 0.0730396 (* 1 = 0.0730396 loss)
I1123 23:07:08.903782 22756 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1123 23:07:16.761792 22756 solver.cpp:218] Iteration 10400 (12.7265 iter/s, 7.85762s/100 iters), loss = 0.0493744
I1123 23:07:16.761792 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:07:16.761792 22756 solver.cpp:237]     Train net output #1: loss = 0.0493744 (* 1 = 0.0493744 loss)
I1123 23:07:16.761792 22756 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1123 23:07:24.227641 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:07:24.538677 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_10500.caffemodel
I1123 23:07:24.579233 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_10500.solverstate
I1123 23:07:24.598232 22756 solver.cpp:330] Iteration 10500, Testing net (#0)
I1123 23:07:24.598232 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:07:26.679947 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:07:26.763957 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1123 23:07:26.763957 22756 solver.cpp:397]     Test net output #1: loss = 0.247993 (* 1 = 0.247993 loss)
I1123 23:07:26.840955 22756 solver.cpp:218] Iteration 10500 (9.92248 iter/s, 10.0781s/100 iters), loss = 0.0647592
I1123 23:07:26.840955 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:07:26.840955 22756 solver.cpp:237]     Train net output #1: loss = 0.0647591 (* 1 = 0.0647591 loss)
I1123 23:07:26.840955 22756 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1123 23:07:34.697710 22756 solver.cpp:218] Iteration 10600 (12.7271 iter/s, 7.85722s/100 iters), loss = 0.123443
I1123 23:07:34.698711 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:07:34.698711 22756 solver.cpp:237]     Train net output #1: loss = 0.123443 (* 1 = 0.123443 loss)
I1123 23:07:34.698711 22756 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1123 23:07:42.555806 22756 solver.cpp:218] Iteration 10700 (12.7279 iter/s, 7.85677s/100 iters), loss = 0.0816274
I1123 23:07:42.555806 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:07:42.555806 22756 solver.cpp:237]     Train net output #1: loss = 0.0816274 (* 1 = 0.0816274 loss)
I1123 23:07:42.555806 22756 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1123 23:07:50.408624 22756 solver.cpp:218] Iteration 10800 (12.7345 iter/s, 7.85268s/100 iters), loss = 0.0821807
I1123 23:07:50.408624 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:07:50.408624 22756 solver.cpp:237]     Train net output #1: loss = 0.0821806 (* 1 = 0.0821806 loss)
I1123 23:07:50.408624 22756 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1123 23:07:58.264423 22756 solver.cpp:218] Iteration 10900 (12.7311 iter/s, 7.85481s/100 iters), loss = 0.0382751
I1123 23:07:58.264423 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:07:58.264423 22756 solver.cpp:237]     Train net output #1: loss = 0.038275 (* 1 = 0.038275 loss)
I1123 23:07:58.264423 22756 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1123 23:08:05.731570 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:08:06.044608 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_11000.caffemodel
I1123 23:08:06.084614 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_11000.solverstate
I1123 23:08:06.102614 22756 solver.cpp:330] Iteration 11000, Testing net (#0)
I1123 23:08:06.103613 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:08:08.185859 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:08:08.269870 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1123 23:08:08.269870 22756 solver.cpp:397]     Test net output #1: loss = 0.247468 (* 1 = 0.247468 loss)
I1123 23:08:08.346374 22756 solver.cpp:218] Iteration 11000 (9.91901 iter/s, 10.0816s/100 iters), loss = 0.0549453
I1123 23:08:08.346374 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:08:08.346374 22756 solver.cpp:237]     Train net output #1: loss = 0.0549452 (* 1 = 0.0549452 loss)
I1123 23:08:08.346374 22756 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1123 23:08:16.206781 22756 solver.cpp:218] Iteration 11100 (12.7228 iter/s, 7.85989s/100 iters), loss = 0.131824
I1123 23:08:16.206781 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:08:16.206781 22756 solver.cpp:237]     Train net output #1: loss = 0.131824 (* 1 = 0.131824 loss)
I1123 23:08:16.206781 22756 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1123 23:08:24.070715 22756 solver.cpp:218] Iteration 11200 (12.717 iter/s, 7.86346s/100 iters), loss = 0.103141
I1123 23:08:24.070715 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:08:24.070715 22756 solver.cpp:237]     Train net output #1: loss = 0.103141 (* 1 = 0.103141 loss)
I1123 23:08:24.070715 22756 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1123 23:08:31.935629 22756 solver.cpp:218] Iteration 11300 (12.716 iter/s, 7.86411s/100 iters), loss = 0.0545364
I1123 23:08:31.935629 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:08:31.935629 22756 solver.cpp:237]     Train net output #1: loss = 0.0545363 (* 1 = 0.0545363 loss)
I1123 23:08:31.935629 22756 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1123 23:08:39.794602 22756 solver.cpp:218] Iteration 11400 (12.724 iter/s, 7.85919s/100 iters), loss = 0.0712433
I1123 23:08:39.794602 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:08:39.794602 22756 solver.cpp:237]     Train net output #1: loss = 0.0712433 (* 1 = 0.0712433 loss)
I1123 23:08:39.794602 22756 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1123 23:08:47.269444 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:08:47.579506 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_11500.caffemodel
I1123 23:08:47.619493 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_11500.solverstate
I1123 23:08:47.639492 22756 solver.cpp:330] Iteration 11500, Testing net (#0)
I1123 23:08:47.639492 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:08:49.718711 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:08:49.803724 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9188
I1123 23:08:49.803724 22756 solver.cpp:397]     Test net output #1: loss = 0.247091 (* 1 = 0.247091 loss)
I1123 23:08:49.880731 22756 solver.cpp:218] Iteration 11500 (9.91576 iter/s, 10.085s/100 iters), loss = 0.0681912
I1123 23:08:49.880731 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:08:49.880731 22756 solver.cpp:237]     Train net output #1: loss = 0.0681912 (* 1 = 0.0681912 loss)
I1123 23:08:49.880731 22756 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1123 23:08:57.738549 22756 solver.cpp:218] Iteration 11600 (12.7256 iter/s, 7.85818s/100 iters), loss = 0.109198
I1123 23:08:57.739550 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:08:57.739550 22756 solver.cpp:237]     Train net output #1: loss = 0.109198 (* 1 = 0.109198 loss)
I1123 23:08:57.739550 22756 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1123 23:09:05.592332 22756 solver.cpp:218] Iteration 11700 (12.734 iter/s, 7.85301s/100 iters), loss = 0.0969005
I1123 23:09:05.592332 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:09:05.592332 22756 solver.cpp:237]     Train net output #1: loss = 0.0969005 (* 1 = 0.0969005 loss)
I1123 23:09:05.592332 22756 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1123 23:09:13.449580 22756 solver.cpp:218] Iteration 11800 (12.7288 iter/s, 7.85618s/100 iters), loss = 0.0670801
I1123 23:09:13.449580 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:09:13.449580 22756 solver.cpp:237]     Train net output #1: loss = 0.0670801 (* 1 = 0.0670801 loss)
I1123 23:09:13.449580 22756 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1123 23:09:21.304481 22756 solver.cpp:218] Iteration 11900 (12.7308 iter/s, 7.855s/100 iters), loss = 0.0563922
I1123 23:09:21.304481 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:09:21.304481 22756 solver.cpp:237]     Train net output #1: loss = 0.0563922 (* 1 = 0.0563922 loss)
I1123 23:09:21.304481 22756 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1123 23:09:28.772346 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:09:29.083375 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_12000.caffemodel
I1123 23:09:29.121373 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_12000.solverstate
I1123 23:09:29.140372 22756 solver.cpp:330] Iteration 12000, Testing net (#0)
I1123 23:09:29.140372 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:09:31.220644 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:09:31.304651 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1123 23:09:31.305660 22756 solver.cpp:397]     Test net output #1: loss = 0.247153 (* 1 = 0.247153 loss)
I1123 23:09:31.381660 22756 solver.cpp:218] Iteration 12000 (9.92402 iter/s, 10.0766s/100 iters), loss = 0.0818984
I1123 23:09:31.381660 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:09:31.381660 22756 solver.cpp:237]     Train net output #1: loss = 0.0818984 (* 1 = 0.0818984 loss)
I1123 23:09:31.381660 22756 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1123 23:09:39.240785 22756 solver.cpp:218] Iteration 12100 (12.7243 iter/s, 7.85896s/100 iters), loss = 0.0869889
I1123 23:09:39.240785 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:09:39.240785 22756 solver.cpp:237]     Train net output #1: loss = 0.0869889 (* 1 = 0.0869889 loss)
I1123 23:09:39.240785 22756 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1123 23:09:47.099597 22756 solver.cpp:218] Iteration 12200 (12.7256 iter/s, 7.85818s/100 iters), loss = 0.0873661
I1123 23:09:47.099597 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:09:47.099597 22756 solver.cpp:237]     Train net output #1: loss = 0.0873661 (* 1 = 0.0873661 loss)
I1123 23:09:47.099597 22756 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1123 23:09:54.958544 22756 solver.cpp:218] Iteration 12300 (12.7255 iter/s, 7.85826s/100 iters), loss = 0.0372362
I1123 23:09:54.959043 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:09:54.959043 22756 solver.cpp:237]     Train net output #1: loss = 0.0372362 (* 1 = 0.0372362 loss)
I1123 23:09:54.959043 22756 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1123 23:10:02.840670 22756 solver.cpp:218] Iteration 12400 (12.688 iter/s, 7.88143s/100 iters), loss = 0.0341567
I1123 23:10:02.840670 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:10:02.840670 22756 solver.cpp:237]     Train net output #1: loss = 0.0341568 (* 1 = 0.0341568 loss)
I1123 23:10:02.840670 22756 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1123 23:10:10.312553 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:10:10.622586 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_12500.caffemodel
I1123 23:10:10.661586 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_12500.solverstate
I1123 23:10:10.680589 22756 solver.cpp:330] Iteration 12500, Testing net (#0)
I1123 23:10:10.680589 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:10:12.760882 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:10:12.845901 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1123 23:10:12.845901 22756 solver.cpp:397]     Test net output #1: loss = 0.247587 (* 1 = 0.247587 loss)
I1123 23:10:12.922900 22756 solver.cpp:218] Iteration 12500 (9.9192 iter/s, 10.0815s/100 iters), loss = 0.0643581
I1123 23:10:12.922900 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:10:12.922900 22756 solver.cpp:237]     Train net output #1: loss = 0.0643581 (* 1 = 0.0643581 loss)
I1123 23:10:12.922900 22756 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1123 23:10:20.778364 22756 solver.cpp:218] Iteration 12600 (12.7309 iter/s, 7.85493s/100 iters), loss = 0.125977
I1123 23:10:20.778364 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1123 23:10:20.778364 22756 solver.cpp:237]     Train net output #1: loss = 0.125977 (* 1 = 0.125977 loss)
I1123 23:10:20.778364 22756 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1123 23:10:28.635798 22756 solver.cpp:218] Iteration 12700 (12.7269 iter/s, 7.85739s/100 iters), loss = 0.0633201
I1123 23:10:28.635798 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:10:28.635798 22756 solver.cpp:237]     Train net output #1: loss = 0.0633201 (* 1 = 0.0633201 loss)
I1123 23:10:28.635798 22756 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1123 23:10:36.492117 22756 solver.cpp:218] Iteration 12800 (12.7285 iter/s, 7.85638s/100 iters), loss = 0.0541237
I1123 23:10:36.493119 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:10:36.493119 22756 solver.cpp:237]     Train net output #1: loss = 0.0541237 (* 1 = 0.0541237 loss)
I1123 23:10:36.493119 22756 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1123 23:10:44.348942 22756 solver.cpp:218] Iteration 12900 (12.7296 iter/s, 7.85571s/100 iters), loss = 0.0506204
I1123 23:10:44.348942 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:10:44.348942 22756 solver.cpp:237]     Train net output #1: loss = 0.0506204 (* 1 = 0.0506204 loss)
I1123 23:10:44.348942 22756 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1123 23:10:51.813824 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:10:52.124866 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_13000.caffemodel
I1123 23:10:52.166867 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_13000.solverstate
I1123 23:10:52.190381 22756 solver.cpp:330] Iteration 13000, Testing net (#0)
I1123 23:10:52.190381 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:10:54.271575 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:10:54.356108 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9187
I1123 23:10:54.356108 22756 solver.cpp:397]     Test net output #1: loss = 0.247075 (* 1 = 0.247075 loss)
I1123 23:10:54.433100 22756 solver.cpp:218] Iteration 13000 (9.91737 iter/s, 10.0833s/100 iters), loss = 0.0553679
I1123 23:10:54.433100 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:10:54.433100 22756 solver.cpp:237]     Train net output #1: loss = 0.0553679 (* 1 = 0.0553679 loss)
I1123 23:10:54.433100 22756 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1123 23:11:02.292340 22756 solver.cpp:218] Iteration 13100 (12.7235 iter/s, 7.85947s/100 iters), loss = 0.124312
I1123 23:11:02.292340 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:11:02.292340 22756 solver.cpp:237]     Train net output #1: loss = 0.124312 (* 1 = 0.124312 loss)
I1123 23:11:02.292340 22756 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1123 23:11:10.149308 22756 solver.cpp:218] Iteration 13200 (12.728 iter/s, 7.85667s/100 iters), loss = 0.0759517
I1123 23:11:10.149308 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:11:10.150310 22756 solver.cpp:237]     Train net output #1: loss = 0.0759517 (* 1 = 0.0759517 loss)
I1123 23:11:10.150310 22756 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1123 23:11:18.005082 22756 solver.cpp:218] Iteration 13300 (12.7313 iter/s, 7.85464s/100 iters), loss = 0.0673552
I1123 23:11:18.005082 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:11:18.005082 22756 solver.cpp:237]     Train net output #1: loss = 0.0673552 (* 1 = 0.0673552 loss)
I1123 23:11:18.005082 22756 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1123 23:11:25.861217 22756 solver.cpp:218] Iteration 13400 (12.7298 iter/s, 7.85558s/100 iters), loss = 0.0352835
I1123 23:11:25.861217 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:11:25.861217 22756 solver.cpp:237]     Train net output #1: loss = 0.0352835 (* 1 = 0.0352835 loss)
I1123 23:11:25.861217 22756 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1123 23:11:33.329223 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:11:33.640244 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_13500.caffemodel
I1123 23:11:33.681751 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_13500.solverstate
I1123 23:11:33.701251 22756 solver.cpp:330] Iteration 13500, Testing net (#0)
I1123 23:11:33.701251 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:11:35.782624 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:11:35.866645 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1123 23:11:35.866645 22756 solver.cpp:397]     Test net output #1: loss = 0.247957 (* 1 = 0.247957 loss)
I1123 23:11:35.942658 22756 solver.cpp:218] Iteration 13500 (9.91921 iter/s, 10.0815s/100 iters), loss = 0.0656971
I1123 23:11:35.942658 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:11:35.942658 22756 solver.cpp:237]     Train net output #1: loss = 0.0656971 (* 1 = 0.0656971 loss)
I1123 23:11:35.942658 22756 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1123 23:11:43.799494 22756 solver.cpp:218] Iteration 13600 (12.73 iter/s, 7.85548s/100 iters), loss = 0.13377
I1123 23:11:43.799494 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:11:43.799494 22756 solver.cpp:237]     Train net output #1: loss = 0.13377 (* 1 = 0.13377 loss)
I1123 23:11:43.799494 22756 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1123 23:11:51.654891 22756 solver.cpp:218] Iteration 13700 (12.7307 iter/s, 7.85502s/100 iters), loss = 0.069582
I1123 23:11:51.654891 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:11:51.654891 22756 solver.cpp:237]     Train net output #1: loss = 0.069582 (* 1 = 0.069582 loss)
I1123 23:11:51.654891 22756 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1123 23:11:59.507699 22756 solver.cpp:218] Iteration 13800 (12.7347 iter/s, 7.85255s/100 iters), loss = 0.0783193
I1123 23:11:59.507699 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:11:59.507699 22756 solver.cpp:237]     Train net output #1: loss = 0.0783193 (* 1 = 0.0783193 loss)
I1123 23:11:59.507699 22756 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1123 23:12:07.362854 22756 solver.cpp:218] Iteration 13900 (12.7308 iter/s, 7.85495s/100 iters), loss = 0.0379048
I1123 23:12:07.362854 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:12:07.362854 22756 solver.cpp:237]     Train net output #1: loss = 0.0379048 (* 1 = 0.0379048 loss)
I1123 23:12:07.362854 22756 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1123 23:12:14.828722 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:12:15.139780 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_14000.caffemodel
I1123 23:12:15.181782 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_14000.solverstate
I1123 23:12:15.200783 22756 solver.cpp:330] Iteration 14000, Testing net (#0)
I1123 23:12:15.200783 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:12:17.283998 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:12:17.368006 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9192
I1123 23:12:17.368006 22756 solver.cpp:397]     Test net output #1: loss = 0.246059 (* 1 = 0.246059 loss)
I1123 23:12:17.445017 22756 solver.cpp:218] Iteration 14000 (9.91962 iter/s, 10.081s/100 iters), loss = 0.034459
I1123 23:12:17.445017 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:12:17.445017 22756 solver.cpp:237]     Train net output #1: loss = 0.034459 (* 1 = 0.034459 loss)
I1123 23:12:17.445017 22756 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1123 23:12:25.304463 22756 solver.cpp:218] Iteration 14100 (12.7243 iter/s, 7.85899s/100 iters), loss = 0.111038
I1123 23:12:25.304463 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:12:25.304463 22756 solver.cpp:237]     Train net output #1: loss = 0.111038 (* 1 = 0.111038 loss)
I1123 23:12:25.304463 22756 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1123 23:12:33.165868 22756 solver.cpp:218] Iteration 14200 (12.7202 iter/s, 7.86153s/100 iters), loss = 0.0693128
I1123 23:12:33.165868 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:12:33.165868 22756 solver.cpp:237]     Train net output #1: loss = 0.0693128 (* 1 = 0.0693128 loss)
I1123 23:12:33.165868 22756 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1123 23:12:41.025770 22756 solver.cpp:218] Iteration 14300 (12.7234 iter/s, 7.85952s/100 iters), loss = 0.0449824
I1123 23:12:41.025770 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:12:41.025770 22756 solver.cpp:237]     Train net output #1: loss = 0.0449825 (* 1 = 0.0449825 loss)
I1123 23:12:41.025770 22756 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1123 23:12:48.888620 22756 solver.cpp:218] Iteration 14400 (12.7194 iter/s, 7.862s/100 iters), loss = 0.0272578
I1123 23:12:48.888620 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:12:48.888620 22756 solver.cpp:237]     Train net output #1: loss = 0.0272578 (* 1 = 0.0272578 loss)
I1123 23:12:48.888620 22756 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1123 23:12:56.363565 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:12:56.673588 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_14500.caffemodel
I1123 23:12:56.713099 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_14500.solverstate
I1123 23:12:56.731595 22756 solver.cpp:330] Iteration 14500, Testing net (#0)
I1123 23:12:56.731595 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:12:58.812876 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:12:58.896881 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1123 23:12:58.896881 22756 solver.cpp:397]     Test net output #1: loss = 0.249105 (* 1 = 0.249105 loss)
I1123 23:12:58.973903 22756 solver.cpp:218] Iteration 14500 (9.91622 iter/s, 10.0845s/100 iters), loss = 0.0445792
I1123 23:12:58.973903 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:12:58.973903 22756 solver.cpp:237]     Train net output #1: loss = 0.0445792 (* 1 = 0.0445792 loss)
I1123 23:12:58.973903 22756 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1123 23:13:06.827919 22756 solver.cpp:218] Iteration 14600 (12.7333 iter/s, 7.85344s/100 iters), loss = 0.0647714
I1123 23:13:06.827919 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:13:06.827919 22756 solver.cpp:237]     Train net output #1: loss = 0.0647715 (* 1 = 0.0647715 loss)
I1123 23:13:06.827919 22756 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1123 23:13:14.680124 22756 solver.cpp:218] Iteration 14700 (12.7363 iter/s, 7.85159s/100 iters), loss = 0.0734084
I1123 23:13:14.680124 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:13:14.680124 22756 solver.cpp:237]     Train net output #1: loss = 0.0734084 (* 1 = 0.0734084 loss)
I1123 23:13:14.680124 22756 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1123 23:13:22.529363 22756 solver.cpp:218] Iteration 14800 (12.7394 iter/s, 7.84964s/100 iters), loss = 0.0568621
I1123 23:13:22.529363 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:13:22.530364 22756 solver.cpp:237]     Train net output #1: loss = 0.0568621 (* 1 = 0.0568621 loss)
I1123 23:13:22.530364 22756 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1123 23:13:30.382246 22756 solver.cpp:218] Iteration 14900 (12.7349 iter/s, 7.85242s/100 iters), loss = 0.053006
I1123 23:13:30.383246 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:13:30.383246 22756 solver.cpp:237]     Train net output #1: loss = 0.0530061 (* 1 = 0.0530061 loss)
I1123 23:13:30.383246 22756 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1123 23:13:37.853106 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:13:38.164149 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_15000.caffemodel
I1123 23:13:38.209134 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_15000.solverstate
I1123 23:13:38.227640 22756 solver.cpp:330] Iteration 15000, Testing net (#0)
I1123 23:13:38.227640 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:13:40.307358 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:13:40.391366 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9202
I1123 23:13:40.391366 22756 solver.cpp:397]     Test net output #1: loss = 0.248215 (* 1 = 0.248215 loss)
I1123 23:13:40.468375 22756 solver.cpp:218] Iteration 15000 (9.91559 iter/s, 10.0851s/100 iters), loss = 0.0380247
I1123 23:13:40.468375 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:13:40.468375 22756 solver.cpp:237]     Train net output #1: loss = 0.0380247 (* 1 = 0.0380247 loss)
I1123 23:13:40.468375 22756 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1123 23:13:48.328222 22756 solver.cpp:218] Iteration 15100 (12.7243 iter/s, 7.85896s/100 iters), loss = 0.0644809
I1123 23:13:48.328222 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:13:48.328222 22756 solver.cpp:237]     Train net output #1: loss = 0.0644809 (* 1 = 0.0644809 loss)
I1123 23:13:48.328222 22756 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1123 23:13:56.183069 22756 solver.cpp:218] Iteration 15200 (12.7316 iter/s, 7.85445s/100 iters), loss = 0.0708626
I1123 23:13:56.183069 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:13:56.183069 22756 solver.cpp:237]     Train net output #1: loss = 0.0708627 (* 1 = 0.0708627 loss)
I1123 23:13:56.183069 22756 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1123 23:14:04.041028 22756 solver.cpp:218] Iteration 15300 (12.7269 iter/s, 7.85737s/100 iters), loss = 0.0516949
I1123 23:14:04.041028 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:14:04.041028 22756 solver.cpp:237]     Train net output #1: loss = 0.051695 (* 1 = 0.051695 loss)
I1123 23:14:04.041028 22756 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1123 23:14:04.041028 22756 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1123 23:14:11.892973 22756 solver.cpp:218] Iteration 15400 (12.7352 iter/s, 7.85222s/100 iters), loss = 0.042873
I1123 23:14:11.892973 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:14:11.892973 22756 solver.cpp:237]     Train net output #1: loss = 0.0428731 (* 1 = 0.0428731 loss)
I1123 23:14:11.892973 22756 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1123 23:14:19.359932 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:14:19.669965 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_15500.caffemodel
I1123 23:14:19.706965 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_15500.solverstate
I1123 23:14:19.726474 22756 solver.cpp:330] Iteration 15500, Testing net (#0)
I1123 23:14:19.726474 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:14:21.806206 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:14:21.890215 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1123 23:14:21.890215 22756 solver.cpp:397]     Test net output #1: loss = 0.246755 (* 1 = 0.246755 loss)
I1123 23:14:21.967247 22756 solver.cpp:218] Iteration 15500 (9.92719 iter/s, 10.0733s/100 iters), loss = 0.03843
I1123 23:14:21.967247 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:14:21.967247 22756 solver.cpp:237]     Train net output #1: loss = 0.0384301 (* 1 = 0.0384301 loss)
I1123 23:14:21.967247 22756 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1123 23:14:29.821646 22756 solver.cpp:218] Iteration 15600 (12.7332 iter/s, 7.85347s/100 iters), loss = 0.0983134
I1123 23:14:29.821646 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:14:29.821646 22756 solver.cpp:237]     Train net output #1: loss = 0.0983135 (* 1 = 0.0983135 loss)
I1123 23:14:29.821646 22756 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1123 23:14:37.680157 22756 solver.cpp:218] Iteration 15700 (12.7253 iter/s, 7.85836s/100 iters), loss = 0.0514639
I1123 23:14:37.680157 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:14:37.680157 22756 solver.cpp:237]     Train net output #1: loss = 0.051464 (* 1 = 0.051464 loss)
I1123 23:14:37.680157 22756 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1123 23:14:45.535039 22756 solver.cpp:218] Iteration 15800 (12.7317 iter/s, 7.85444s/100 iters), loss = 0.0645622
I1123 23:14:45.535039 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:14:45.535039 22756 solver.cpp:237]     Train net output #1: loss = 0.0645623 (* 1 = 0.0645623 loss)
I1123 23:14:45.535039 22756 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1123 23:14:53.389052 22756 solver.cpp:218] Iteration 15900 (12.7336 iter/s, 7.85321s/100 iters), loss = 0.0308579
I1123 23:14:53.389052 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:14:53.389052 22756 solver.cpp:237]     Train net output #1: loss = 0.030858 (* 1 = 0.030858 loss)
I1123 23:14:53.389052 22756 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1123 23:15:00.859027 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:15:01.170058 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_16000.caffemodel
I1123 23:15:01.211058 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_16000.solverstate
I1123 23:15:01.230057 22756 solver.cpp:330] Iteration 16000, Testing net (#0)
I1123 23:15:01.230057 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:15:03.311267 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:15:03.395278 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1123 23:15:03.395278 22756 solver.cpp:397]     Test net output #1: loss = 0.246961 (* 1 = 0.246961 loss)
I1123 23:15:03.472286 22756 solver.cpp:218] Iteration 16000 (9.91794 iter/s, 10.0827s/100 iters), loss = 0.0435179
I1123 23:15:03.472286 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:15:03.472286 22756 solver.cpp:237]     Train net output #1: loss = 0.043518 (* 1 = 0.043518 loss)
I1123 23:15:03.472286 22756 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1123 23:15:11.330646 22756 solver.cpp:218] Iteration 16100 (12.726 iter/s, 7.85794s/100 iters), loss = 0.10737
I1123 23:15:11.330646 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:15:11.330646 22756 solver.cpp:237]     Train net output #1: loss = 0.10737 (* 1 = 0.10737 loss)
I1123 23:15:11.330646 22756 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1123 23:15:19.185503 22756 solver.cpp:218] Iteration 16200 (12.7318 iter/s, 7.85433s/100 iters), loss = 0.0547651
I1123 23:15:19.185503 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:15:19.185503 22756 solver.cpp:237]     Train net output #1: loss = 0.0547652 (* 1 = 0.0547652 loss)
I1123 23:15:19.185503 22756 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1123 23:15:27.041893 22756 solver.cpp:218] Iteration 16300 (12.7284 iter/s, 7.85647s/100 iters), loss = 0.0644575
I1123 23:15:27.041893 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:15:27.041893 22756 solver.cpp:237]     Train net output #1: loss = 0.0644576 (* 1 = 0.0644576 loss)
I1123 23:15:27.041893 22756 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1123 23:15:34.899842 22756 solver.cpp:218] Iteration 16400 (12.7277 iter/s, 7.85688s/100 iters), loss = 0.0251183
I1123 23:15:34.899842 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:15:34.899842 22756 solver.cpp:237]     Train net output #1: loss = 0.0251184 (* 1 = 0.0251184 loss)
I1123 23:15:34.899842 22756 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1123 23:15:42.368691 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:15:42.678735 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_16500.caffemodel
I1123 23:15:42.718745 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_16500.solverstate
I1123 23:15:42.737746 22756 solver.cpp:330] Iteration 16500, Testing net (#0)
I1123 23:15:42.737746 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:15:44.818974 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:15:44.903602 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9185
I1123 23:15:44.903602 22756 solver.cpp:397]     Test net output #1: loss = 0.246901 (* 1 = 0.246901 loss)
I1123 23:15:44.980604 22756 solver.cpp:218] Iteration 16500 (9.92041 iter/s, 10.0802s/100 iters), loss = 0.0520628
I1123 23:15:44.980604 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:15:44.980604 22756 solver.cpp:237]     Train net output #1: loss = 0.0520629 (* 1 = 0.0520629 loss)
I1123 23:15:44.980604 22756 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1123 23:15:52.840586 22756 solver.cpp:218] Iteration 16600 (12.7224 iter/s, 7.86014s/100 iters), loss = 0.0842557
I1123 23:15:52.840586 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:15:52.840586 22756 solver.cpp:237]     Train net output #1: loss = 0.0842558 (* 1 = 0.0842558 loss)
I1123 23:15:52.840586 22756 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1123 23:16:00.699615 22756 solver.cpp:218] Iteration 16700 (12.7257 iter/s, 7.85809s/100 iters), loss = 0.0778337
I1123 23:16:00.699615 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:16:00.699615 22756 solver.cpp:237]     Train net output #1: loss = 0.0778338 (* 1 = 0.0778338 loss)
I1123 23:16:00.699615 22756 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1123 23:16:08.555519 22756 solver.cpp:218] Iteration 16800 (12.7295 iter/s, 7.85577s/100 iters), loss = 0.0518209
I1123 23:16:08.555519 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:16:08.555519 22756 solver.cpp:237]     Train net output #1: loss = 0.051821 (* 1 = 0.051821 loss)
I1123 23:16:08.555519 22756 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1123 23:16:16.471431 22756 solver.cpp:218] Iteration 16900 (12.6346 iter/s, 7.91479s/100 iters), loss = 0.0244259
I1123 23:16:16.471431 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:16:16.471431 22756 solver.cpp:237]     Train net output #1: loss = 0.024426 (* 1 = 0.024426 loss)
I1123 23:16:16.471431 22756 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1123 23:16:23.923960 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:16:24.234594 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_17000.caffemodel
I1123 23:16:24.275588 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_17000.solverstate
I1123 23:16:24.294085 22756 solver.cpp:330] Iteration 17000, Testing net (#0)
I1123 23:16:24.294605 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:16:26.368917 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:16:26.452953 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1123 23:16:26.452953 22756 solver.cpp:397]     Test net output #1: loss = 0.247105 (* 1 = 0.247105 loss)
I1123 23:16:26.529961 22756 solver.cpp:218] Iteration 17000 (9.94213 iter/s, 10.0582s/100 iters), loss = 0.0510856
I1123 23:16:26.529961 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:16:26.529961 22756 solver.cpp:237]     Train net output #1: loss = 0.0510857 (* 1 = 0.0510857 loss)
I1123 23:16:26.529961 22756 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1123 23:16:34.363613 22756 solver.cpp:218] Iteration 17100 (12.7652 iter/s, 7.83383s/100 iters), loss = 0.0985328
I1123 23:16:34.363613 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:16:34.363613 22756 solver.cpp:237]     Train net output #1: loss = 0.0985329 (* 1 = 0.0985329 loss)
I1123 23:16:34.363613 22756 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1123 23:16:42.190979 22756 solver.cpp:218] Iteration 17200 (12.7774 iter/s, 7.82633s/100 iters), loss = 0.0620041
I1123 23:16:42.190979 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:16:42.190979 22756 solver.cpp:237]     Train net output #1: loss = 0.0620042 (* 1 = 0.0620042 loss)
I1123 23:16:42.190979 22756 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1123 23:16:50.025676 22756 solver.cpp:218] Iteration 17300 (12.7648 iter/s, 7.83403s/100 iters), loss = 0.0476099
I1123 23:16:50.025676 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:16:50.025676 22756 solver.cpp:237]     Train net output #1: loss = 0.04761 (* 1 = 0.04761 loss)
I1123 23:16:50.025676 22756 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1123 23:16:57.853541 22756 solver.cpp:218] Iteration 17400 (12.775 iter/s, 7.82776s/100 iters), loss = 0.0413203
I1123 23:16:57.853541 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:16:57.853541 22756 solver.cpp:237]     Train net output #1: loss = 0.0413204 (* 1 = 0.0413204 loss)
I1123 23:16:57.853541 22756 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1123 23:17:05.297461 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:17:05.607487 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_17500.caffemodel
I1123 23:17:05.644489 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_17500.solverstate
I1123 23:17:05.662499 22756 solver.cpp:330] Iteration 17500, Testing net (#0)
I1123 23:17:05.663503 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:17:07.736726 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:17:07.819733 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9177
I1123 23:17:07.819733 22756 solver.cpp:397]     Test net output #1: loss = 0.247318 (* 1 = 0.247318 loss)
I1123 23:17:07.896739 22756 solver.cpp:218] Iteration 17500 (9.95782 iter/s, 10.0424s/100 iters), loss = 0.0389839
I1123 23:17:07.896739 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:17:07.896739 22756 solver.cpp:237]     Train net output #1: loss = 0.038984 (* 1 = 0.038984 loss)
I1123 23:17:07.896739 22756 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1123 23:17:15.728569 22756 solver.cpp:218] Iteration 17600 (12.7694 iter/s, 7.8312s/100 iters), loss = 0.0713852
I1123 23:17:15.728569 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:17:15.728569 22756 solver.cpp:237]     Train net output #1: loss = 0.0713853 (* 1 = 0.0713853 loss)
I1123 23:17:15.728569 22756 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1123 23:17:23.554524 22756 solver.cpp:218] Iteration 17700 (12.7779 iter/s, 7.82601s/100 iters), loss = 0.0747607
I1123 23:17:23.554524 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:17:23.554524 22756 solver.cpp:237]     Train net output #1: loss = 0.0747608 (* 1 = 0.0747608 loss)
I1123 23:17:23.554524 22756 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1123 23:17:31.383430 22756 solver.cpp:218] Iteration 17800 (12.7741 iter/s, 7.82835s/100 iters), loss = 0.0488142
I1123 23:17:31.383430 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:17:31.383430 22756 solver.cpp:237]     Train net output #1: loss = 0.0488143 (* 1 = 0.0488143 loss)
I1123 23:17:31.383430 22756 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1123 23:17:39.210290 22756 solver.cpp:218] Iteration 17900 (12.777 iter/s, 7.82658s/100 iters), loss = 0.0257007
I1123 23:17:39.210290 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:17:39.210290 22756 solver.cpp:237]     Train net output #1: loss = 0.0257007 (* 1 = 0.0257007 loss)
I1123 23:17:39.210290 22756 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1123 23:17:46.656091 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:17:46.966126 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_18000.caffemodel
I1123 23:17:47.005633 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_18000.solverstate
I1123 23:17:47.024135 22756 solver.cpp:330] Iteration 18000, Testing net (#0)
I1123 23:17:47.024135 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:17:49.098839 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:17:49.182343 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1123 23:17:49.182343 22756 solver.cpp:397]     Test net output #1: loss = 0.247146 (* 1 = 0.247146 loss)
I1123 23:17:49.258350 22756 solver.cpp:218] Iteration 18000 (9.95281 iter/s, 10.0474s/100 iters), loss = 0.0584763
I1123 23:17:49.258350 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:17:49.258350 22756 solver.cpp:237]     Train net output #1: loss = 0.0584764 (* 1 = 0.0584764 loss)
I1123 23:17:49.258350 22756 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1123 23:17:57.087244 22756 solver.cpp:218] Iteration 18100 (12.7751 iter/s, 7.82773s/100 iters), loss = 0.0684317
I1123 23:17:57.087244 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:17:57.087244 22756 solver.cpp:237]     Train net output #1: loss = 0.0684317 (* 1 = 0.0684317 loss)
I1123 23:17:57.087244 22756 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1123 23:18:04.915189 22756 solver.cpp:218] Iteration 18200 (12.7744 iter/s, 7.82818s/100 iters), loss = 0.0817162
I1123 23:18:04.915189 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:18:04.915189 22756 solver.cpp:237]     Train net output #1: loss = 0.0817163 (* 1 = 0.0817163 loss)
I1123 23:18:04.915189 22756 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1123 23:18:12.747045 22756 solver.cpp:218] Iteration 18300 (12.7702 iter/s, 7.83073s/100 iters), loss = 0.0686201
I1123 23:18:12.747045 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:18:12.747045 22756 solver.cpp:237]     Train net output #1: loss = 0.0686202 (* 1 = 0.0686202 loss)
I1123 23:18:12.747045 22756 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1123 23:18:20.578054 22756 solver.cpp:218] Iteration 18400 (12.7697 iter/s, 7.83102s/100 iters), loss = 0.0419929
I1123 23:18:20.578054 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:18:20.578054 22756 solver.cpp:237]     Train net output #1: loss = 0.041993 (* 1 = 0.041993 loss)
I1123 23:18:20.578054 22756 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1123 23:18:28.021946 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:18:28.331974 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_18500.caffemodel
I1123 23:18:28.371974 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_18500.solverstate
I1123 23:18:28.390974 22756 solver.cpp:330] Iteration 18500, Testing net (#0)
I1123 23:18:28.390974 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:18:30.465325 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:18:30.548333 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1123 23:18:30.548333 22756 solver.cpp:397]     Test net output #1: loss = 0.24726 (* 1 = 0.24726 loss)
I1123 23:18:30.625344 22756 solver.cpp:218] Iteration 18500 (9.95391 iter/s, 10.0463s/100 iters), loss = 0.0590761
I1123 23:18:30.625344 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:18:30.625344 22756 solver.cpp:237]     Train net output #1: loss = 0.0590762 (* 1 = 0.0590762 loss)
I1123 23:18:30.625344 22756 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1123 23:18:38.457229 22756 solver.cpp:218] Iteration 18600 (12.7689 iter/s, 7.83153s/100 iters), loss = 0.076185
I1123 23:18:38.457229 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:18:38.457229 22756 solver.cpp:237]     Train net output #1: loss = 0.0761852 (* 1 = 0.0761852 loss)
I1123 23:18:38.457229 22756 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1123 23:18:46.283998 22756 solver.cpp:218] Iteration 18700 (12.7768 iter/s, 7.82666s/100 iters), loss = 0.0670321
I1123 23:18:46.283998 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:18:46.283998 22756 solver.cpp:237]     Train net output #1: loss = 0.0670322 (* 1 = 0.0670322 loss)
I1123 23:18:46.283998 22756 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1123 23:18:54.113762 22756 solver.cpp:218] Iteration 18800 (12.7736 iter/s, 7.82864s/100 iters), loss = 0.0697579
I1123 23:18:54.113762 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:18:54.113762 22756 solver.cpp:237]     Train net output #1: loss = 0.069758 (* 1 = 0.069758 loss)
I1123 23:18:54.113762 22756 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1123 23:19:01.943681 22756 solver.cpp:218] Iteration 18900 (12.7722 iter/s, 7.8295s/100 iters), loss = 0.0402579
I1123 23:19:01.943681 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:19:01.943681 22756 solver.cpp:237]     Train net output #1: loss = 0.0402581 (* 1 = 0.0402581 loss)
I1123 23:19:01.943681 22756 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1123 23:19:09.384618 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:19:09.693645 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_19000.caffemodel
I1123 23:19:09.731655 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_19000.solverstate
I1123 23:19:09.750155 22756 solver.cpp:330] Iteration 19000, Testing net (#0)
I1123 23:19:09.750155 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:19:11.823940 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:19:11.907948 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1123 23:19:11.907948 22756 solver.cpp:397]     Test net output #1: loss = 0.247571 (* 1 = 0.247571 loss)
I1123 23:19:11.984971 22756 solver.cpp:218] Iteration 19000 (9.95968 iter/s, 10.0405s/100 iters), loss = 0.0494329
I1123 23:19:11.984971 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:19:11.984971 22756 solver.cpp:237]     Train net output #1: loss = 0.049433 (* 1 = 0.049433 loss)
I1123 23:19:11.984971 22756 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1123 23:19:19.813813 22756 solver.cpp:218] Iteration 19100 (12.7726 iter/s, 7.82923s/100 iters), loss = 0.0692299
I1123 23:19:19.814815 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:19:19.814815 22756 solver.cpp:237]     Train net output #1: loss = 0.0692301 (* 1 = 0.0692301 loss)
I1123 23:19:19.814815 22756 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1123 23:19:27.644693 22756 solver.cpp:218] Iteration 19200 (12.772 iter/s, 7.82963s/100 iters), loss = 0.0777768
I1123 23:19:27.644693 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:19:27.644693 22756 solver.cpp:237]     Train net output #1: loss = 0.0777769 (* 1 = 0.0777769 loss)
I1123 23:19:27.644693 22756 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1123 23:19:35.477576 22756 solver.cpp:218] Iteration 19300 (12.7673 iter/s, 7.83253s/100 iters), loss = 0.0368967
I1123 23:19:35.477576 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:19:35.477576 22756 solver.cpp:237]     Train net output #1: loss = 0.0368968 (* 1 = 0.0368968 loss)
I1123 23:19:35.477576 22756 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1123 23:19:43.309918 22756 solver.cpp:218] Iteration 19400 (12.7684 iter/s, 7.83181s/100 iters), loss = 0.0339022
I1123 23:19:43.309918 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:19:43.309918 22756 solver.cpp:237]     Train net output #1: loss = 0.0339024 (* 1 = 0.0339024 loss)
I1123 23:19:43.309918 22756 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1123 23:19:50.752404 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:19:51.061935 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_19500.caffemodel
I1123 23:19:51.100939 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_19500.solverstate
I1123 23:19:51.119936 22756 solver.cpp:330] Iteration 19500, Testing net (#0)
I1123 23:19:51.119936 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:19:53.193404 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:19:53.276458 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9184
I1123 23:19:53.276458 22756 solver.cpp:397]     Test net output #1: loss = 0.24738 (* 1 = 0.24738 loss)
I1123 23:19:53.353974 22756 solver.cpp:218] Iteration 19500 (9.95659 iter/s, 10.0436s/100 iters), loss = 0.0342401
I1123 23:19:53.353974 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:19:53.353974 22756 solver.cpp:237]     Train net output #1: loss = 0.0342403 (* 1 = 0.0342403 loss)
I1123 23:19:53.353974 22756 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1123 23:19:53.353974 22756 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1123 23:20:01.185267 22756 solver.cpp:218] Iteration 19600 (12.7695 iter/s, 7.83115s/100 iters), loss = 0.0959171
I1123 23:20:01.185267 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:20:01.185267 22756 solver.cpp:237]     Train net output #1: loss = 0.0959173 (* 1 = 0.0959173 loss)
I1123 23:20:01.185267 22756 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1123 23:20:09.014683 22756 solver.cpp:218] Iteration 19700 (12.7735 iter/s, 7.82872s/100 iters), loss = 0.0755923
I1123 23:20:09.014683 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:20:09.014683 22756 solver.cpp:237]     Train net output #1: loss = 0.0755924 (* 1 = 0.0755924 loss)
I1123 23:20:09.014683 22756 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1123 23:20:16.848603 22756 solver.cpp:218] Iteration 19800 (12.7659 iter/s, 7.83336s/100 iters), loss = 0.0523108
I1123 23:20:16.848603 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:20:16.848603 22756 solver.cpp:237]     Train net output #1: loss = 0.052311 (* 1 = 0.052311 loss)
I1123 23:20:16.849103 22756 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1123 23:20:24.680997 22756 solver.cpp:218] Iteration 19900 (12.7688 iter/s, 7.8316s/100 iters), loss = 0.0329153
I1123 23:20:24.680997 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:20:24.680997 22756 solver.cpp:237]     Train net output #1: loss = 0.0329155 (* 1 = 0.0329155 loss)
I1123 23:20:24.680997 22756 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1123 23:20:32.124904 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:20:32.434952 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_20000.caffemodel
I1123 23:20:32.472944 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_20000.solverstate
I1123 23:20:32.491945 22756 solver.cpp:330] Iteration 20000, Testing net (#0)
I1123 23:20:32.491945 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:20:34.567216 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:20:34.651228 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1123 23:20:34.651228 22756 solver.cpp:397]     Test net output #1: loss = 0.247458 (* 1 = 0.247458 loss)
I1123 23:20:34.727226 22756 solver.cpp:218] Iteration 20000 (9.95373 iter/s, 10.0465s/100 iters), loss = 0.0616663
I1123 23:20:34.727226 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:20:34.727226 22756 solver.cpp:237]     Train net output #1: loss = 0.0616665 (* 1 = 0.0616665 loss)
I1123 23:20:34.727226 22756 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1123 23:20:42.557968 22756 solver.cpp:218] Iteration 20100 (12.7724 iter/s, 7.82938s/100 iters), loss = 0.0654701
I1123 23:20:42.557968 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:20:42.557968 22756 solver.cpp:237]     Train net output #1: loss = 0.0654703 (* 1 = 0.0654703 loss)
I1123 23:20:42.557968 22756 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1123 23:20:50.386904 22756 solver.cpp:218] Iteration 20200 (12.7733 iter/s, 7.82885s/100 iters), loss = 0.0677873
I1123 23:20:50.386904 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:20:50.386904 22756 solver.cpp:237]     Train net output #1: loss = 0.0677876 (* 1 = 0.0677876 loss)
I1123 23:20:50.386904 22756 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1123 23:20:58.216794 22756 solver.cpp:218] Iteration 20300 (12.7726 iter/s, 7.82927s/100 iters), loss = 0.0658619
I1123 23:20:58.216794 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:20:58.216794 22756 solver.cpp:237]     Train net output #1: loss = 0.0658621 (* 1 = 0.0658621 loss)
I1123 23:20:58.216794 22756 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1123 23:21:06.043663 22756 solver.cpp:218] Iteration 20400 (12.7773 iter/s, 7.82637s/100 iters), loss = 0.0312714
I1123 23:21:06.043663 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:21:06.043663 22756 solver.cpp:237]     Train net output #1: loss = 0.0312716 (* 1 = 0.0312716 loss)
I1123 23:21:06.043663 22756 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1123 23:21:13.486639 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:21:13.795676 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_20500.caffemodel
I1123 23:21:13.836675 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_20500.solverstate
I1123 23:21:13.856672 22756 solver.cpp:330] Iteration 20500, Testing net (#0)
I1123 23:21:13.856672 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:21:15.930939 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:21:16.014945 22756 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1123 23:21:16.014945 22756 solver.cpp:397]     Test net output #1: loss = 0.24733 (* 1 = 0.24733 loss)
I1123 23:21:16.090955 22756 solver.cpp:218] Iteration 20500 (9.95326 iter/s, 10.047s/100 iters), loss = 0.0510085
I1123 23:21:16.090955 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:21:16.090955 22756 solver.cpp:237]     Train net output #1: loss = 0.0510087 (* 1 = 0.0510087 loss)
I1123 23:21:16.090955 22756 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1123 23:21:23.917655 22756 solver.cpp:218] Iteration 20600 (12.7767 iter/s, 7.82673s/100 iters), loss = 0.0838749
I1123 23:21:23.918653 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:21:23.918653 22756 solver.cpp:237]     Train net output #1: loss = 0.0838751 (* 1 = 0.0838751 loss)
I1123 23:21:23.918653 22756 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1123 23:21:31.746397 22756 solver.cpp:218] Iteration 20700 (12.7756 iter/s, 7.82742s/100 iters), loss = 0.0713379
I1123 23:21:31.746397 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:21:31.746397 22756 solver.cpp:237]     Train net output #1: loss = 0.0713381 (* 1 = 0.0713381 loss)
I1123 23:21:31.746397 22756 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1123 23:21:39.573331 22756 solver.cpp:218] Iteration 20800 (12.7775 iter/s, 7.82627s/100 iters), loss = 0.0550346
I1123 23:21:39.573331 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:21:39.573331 22756 solver.cpp:237]     Train net output #1: loss = 0.0550348 (* 1 = 0.0550348 loss)
I1123 23:21:39.573331 22756 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1123 23:21:47.403234 22756 solver.cpp:218] Iteration 20900 (12.7718 iter/s, 7.82974s/100 iters), loss = 0.0211183
I1123 23:21:47.403234 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:21:47.403234 22756 solver.cpp:237]     Train net output #1: loss = 0.0211185 (* 1 = 0.0211185 loss)
I1123 23:21:47.403234 22756 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1123 23:21:54.845065 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:21:55.155094 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_21000.caffemodel
I1123 23:21:55.202095 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_21000.solverstate
I1123 23:21:55.222100 22756 solver.cpp:330] Iteration 21000, Testing net (#0)
I1123 23:21:55.222100 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:21:57.296699 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:21:57.380708 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1123 23:21:57.380708 22756 solver.cpp:397]     Test net output #1: loss = 0.247414 (* 1 = 0.247414 loss)
I1123 23:21:57.456717 22756 solver.cpp:218] Iteration 21000 (9.94716 iter/s, 10.0531s/100 iters), loss = 0.0507066
I1123 23:21:57.456717 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:21:57.456717 22756 solver.cpp:237]     Train net output #1: loss = 0.0507069 (* 1 = 0.0507069 loss)
I1123 23:21:57.456717 22756 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1123 23:22:05.288583 22756 solver.cpp:218] Iteration 21100 (12.7695 iter/s, 7.83117s/100 iters), loss = 0.0662477
I1123 23:22:05.288583 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:22:05.288583 22756 solver.cpp:237]     Train net output #1: loss = 0.0662479 (* 1 = 0.0662479 loss)
I1123 23:22:05.288583 22756 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1123 23:22:13.118450 22756 solver.cpp:218] Iteration 21200 (12.7728 iter/s, 7.82912s/100 iters), loss = 0.0564677
I1123 23:22:13.118450 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:22:13.118450 22756 solver.cpp:237]     Train net output #1: loss = 0.0564679 (* 1 = 0.0564679 loss)
I1123 23:22:13.118450 22756 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1123 23:22:20.946451 22756 solver.cpp:218] Iteration 21300 (12.7747 iter/s, 7.82795s/100 iters), loss = 0.0563908
I1123 23:22:20.946451 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:22:20.946451 22756 solver.cpp:237]     Train net output #1: loss = 0.056391 (* 1 = 0.056391 loss)
I1123 23:22:20.946451 22756 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1123 23:22:28.776412 22756 solver.cpp:218] Iteration 21400 (12.7717 iter/s, 7.8298s/100 iters), loss = 0.0197433
I1123 23:22:28.776412 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:22:28.776412 22756 solver.cpp:237]     Train net output #1: loss = 0.0197435 (* 1 = 0.0197435 loss)
I1123 23:22:28.776412 22756 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1123 23:22:36.215381 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:22:36.524407 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_21500.caffemodel
I1123 23:22:36.563431 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_21500.solverstate
I1123 23:22:36.581420 22756 solver.cpp:330] Iteration 21500, Testing net (#0)
I1123 23:22:36.581420 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:22:38.655753 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:22:38.739259 22756 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1123 23:22:38.739259 22756 solver.cpp:397]     Test net output #1: loss = 0.247534 (* 1 = 0.247534 loss)
I1123 23:22:38.814760 22756 solver.cpp:218] Iteration 21500 (9.96208 iter/s, 10.0381s/100 iters), loss = 0.0295841
I1123 23:22:38.815762 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:22:38.815762 22756 solver.cpp:237]     Train net output #1: loss = 0.0295843 (* 1 = 0.0295843 loss)
I1123 23:22:38.815762 22756 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1123 23:22:46.649642 22756 solver.cpp:218] Iteration 21600 (12.7656 iter/s, 7.83355s/100 iters), loss = 0.0770314
I1123 23:22:46.649642 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:22:46.649642 22756 solver.cpp:237]     Train net output #1: loss = 0.0770317 (* 1 = 0.0770317 loss)
I1123 23:22:46.649642 22756 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1123 23:22:54.482751 22756 solver.cpp:218] Iteration 21700 (12.7666 iter/s, 7.83294s/100 iters), loss = 0.0647497
I1123 23:22:54.483253 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:22:54.483253 22756 solver.cpp:237]     Train net output #1: loss = 0.0647499 (* 1 = 0.0647499 loss)
I1123 23:22:54.483253 22756 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1123 23:23:02.313628 22756 solver.cpp:218] Iteration 21800 (12.7713 iter/s, 7.83008s/100 iters), loss = 0.0622357
I1123 23:23:02.313628 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:23:02.313628 22756 solver.cpp:237]     Train net output #1: loss = 0.0622359 (* 1 = 0.0622359 loss)
I1123 23:23:02.313628 22756 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1123 23:23:10.145534 22756 solver.cpp:218] Iteration 21900 (12.7678 iter/s, 7.83223s/100 iters), loss = 0.0238895
I1123 23:23:10.146536 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:23:10.146536 22756 solver.cpp:237]     Train net output #1: loss = 0.0238897 (* 1 = 0.0238897 loss)
I1123 23:23:10.146536 22756 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1123 23:23:17.595510 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:23:17.905539 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_22000.caffemodel
I1123 23:23:17.946539 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_22000.solverstate
I1123 23:23:17.965061 22756 solver.cpp:330] Iteration 22000, Testing net (#0)
I1123 23:23:17.965061 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:23:20.039919 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:23:20.123929 22756 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1123 23:23:20.123929 22756 solver.cpp:397]     Test net output #1: loss = 0.24741 (* 1 = 0.24741 loss)
I1123 23:23:20.199937 22756 solver.cpp:218] Iteration 22000 (9.94679 iter/s, 10.0535s/100 iters), loss = 0.0506399
I1123 23:23:20.199937 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:23:20.199937 22756 solver.cpp:237]     Train net output #1: loss = 0.0506401 (* 1 = 0.0506401 loss)
I1123 23:23:20.199937 22756 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1123 23:23:20.199937 22756 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1123 23:23:28.032929 22756 solver.cpp:218] Iteration 22100 (12.7675 iter/s, 7.83238s/100 iters), loss = 0.117766
I1123 23:23:28.032929 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:23:28.032929 22756 solver.cpp:237]     Train net output #1: loss = 0.117766 (* 1 = 0.117766 loss)
I1123 23:23:28.032929 22756 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1123 23:23:35.865784 22756 solver.cpp:218] Iteration 22200 (12.7679 iter/s, 7.83211s/100 iters), loss = 0.0380789
I1123 23:23:35.865784 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:23:35.865784 22756 solver.cpp:237]     Train net output #1: loss = 0.0380791 (* 1 = 0.0380791 loss)
I1123 23:23:35.865784 22756 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1123 23:23:43.696660 22756 solver.cpp:218] Iteration 22300 (12.7698 iter/s, 7.83098s/100 iters), loss = 0.0436087
I1123 23:23:43.696660 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:23:43.696660 22756 solver.cpp:237]     Train net output #1: loss = 0.0436089 (* 1 = 0.0436089 loss)
I1123 23:23:43.696660 22756 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1123 23:23:51.536638 22756 solver.cpp:218] Iteration 22400 (12.7572 iter/s, 7.8387s/100 iters), loss = 0.0288506
I1123 23:23:51.536638 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:23:51.536638 22756 solver.cpp:237]     Train net output #1: loss = 0.0288509 (* 1 = 0.0288509 loss)
I1123 23:23:51.536638 22756 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1123 23:23:58.982456 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:23:59.293504 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_22500.caffemodel
I1123 23:23:59.331501 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_22500.solverstate
I1123 23:23:59.350500 22756 solver.cpp:330] Iteration 22500, Testing net (#0)
I1123 23:23:59.350500 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:24:01.425878 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:24:01.509886 22756 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1123 23:24:01.509886 22756 solver.cpp:397]     Test net output #1: loss = 0.247385 (* 1 = 0.247385 loss)
I1123 23:24:01.586891 22756 solver.cpp:218] Iteration 22500 (9.95049 iter/s, 10.0498s/100 iters), loss = 0.0566029
I1123 23:24:01.586891 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:24:01.586891 22756 solver.cpp:237]     Train net output #1: loss = 0.0566032 (* 1 = 0.0566032 loss)
I1123 23:24:01.586891 22756 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1123 23:24:09.418767 22756 solver.cpp:218] Iteration 22600 (12.7692 iter/s, 7.83133s/100 iters), loss = 0.0969877
I1123 23:24:09.418767 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:24:09.418767 22756 solver.cpp:237]     Train net output #1: loss = 0.096988 (* 1 = 0.096988 loss)
I1123 23:24:09.418767 22756 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1123 23:24:17.249289 22756 solver.cpp:218] Iteration 22700 (12.7711 iter/s, 7.83021s/100 iters), loss = 0.0729972
I1123 23:24:17.249289 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:24:17.249289 22756 solver.cpp:237]     Train net output #1: loss = 0.0729975 (* 1 = 0.0729975 loss)
I1123 23:24:17.249289 22756 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1123 23:24:25.081593 22756 solver.cpp:218] Iteration 22800 (12.7687 iter/s, 7.83168s/100 iters), loss = 0.0692519
I1123 23:24:25.081593 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:24:25.081593 22756 solver.cpp:237]     Train net output #1: loss = 0.0692522 (* 1 = 0.0692522 loss)
I1123 23:24:25.081593 22756 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1123 23:24:32.911402 22756 solver.cpp:218] Iteration 22900 (12.772 iter/s, 7.82965s/100 iters), loss = 0.0478964
I1123 23:24:32.911402 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:24:32.911402 22756 solver.cpp:237]     Train net output #1: loss = 0.0478967 (* 1 = 0.0478967 loss)
I1123 23:24:32.911402 22756 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1123 23:24:40.357172 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:24:40.666708 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_23000.caffemodel
I1123 23:24:40.706204 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_23000.solverstate
I1123 23:24:40.725205 22756 solver.cpp:330] Iteration 23000, Testing net (#0)
I1123 23:24:40.725205 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:24:42.800488 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:24:42.884497 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1123 23:24:42.884497 22756 solver.cpp:397]     Test net output #1: loss = 0.247509 (* 1 = 0.247509 loss)
I1123 23:24:42.960999 22756 solver.cpp:218] Iteration 23000 (9.95154 iter/s, 10.0487s/100 iters), loss = 0.0342383
I1123 23:24:42.960999 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:24:42.960999 22756 solver.cpp:237]     Train net output #1: loss = 0.0342386 (* 1 = 0.0342386 loss)
I1123 23:24:42.960999 22756 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1123 23:24:50.793344 22756 solver.cpp:218] Iteration 23100 (12.7684 iter/s, 7.83183s/100 iters), loss = 0.0773329
I1123 23:24:50.793344 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:24:50.793344 22756 solver.cpp:237]     Train net output #1: loss = 0.0773332 (* 1 = 0.0773332 loss)
I1123 23:24:50.793344 22756 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1123 23:24:58.625197 22756 solver.cpp:218] Iteration 23200 (12.7683 iter/s, 7.8319s/100 iters), loss = 0.0727712
I1123 23:24:58.625197 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:24:58.625197 22756 solver.cpp:237]     Train net output #1: loss = 0.0727715 (* 1 = 0.0727715 loss)
I1123 23:24:58.625197 22756 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1123 23:25:06.458204 22756 solver.cpp:218] Iteration 23300 (12.7684 iter/s, 7.83186s/100 iters), loss = 0.0651241
I1123 23:25:06.458204 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:25:06.458204 22756 solver.cpp:237]     Train net output #1: loss = 0.0651244 (* 1 = 0.0651244 loss)
I1123 23:25:06.458204 22756 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1123 23:25:14.289139 22756 solver.cpp:218] Iteration 23400 (12.7696 iter/s, 7.83108s/100 iters), loss = 0.037764
I1123 23:25:14.289139 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:25:14.289139 22756 solver.cpp:237]     Train net output #1: loss = 0.0377643 (* 1 = 0.0377643 loss)
I1123 23:25:14.289139 22756 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1123 23:25:21.737025 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:25:22.046064 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_23500.caffemodel
I1123 23:25:22.087061 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_23500.solverstate
I1123 23:25:22.105052 22756 solver.cpp:330] Iteration 23500, Testing net (#0)
I1123 23:25:22.105052 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:25:24.179396 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:25:24.263401 22756 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1123 23:25:24.263401 22756 solver.cpp:397]     Test net output #1: loss = 0.247382 (* 1 = 0.247382 loss)
I1123 23:25:24.339402 22756 solver.cpp:218] Iteration 23500 (9.95088 iter/s, 10.0494s/100 iters), loss = 0.0416548
I1123 23:25:24.339402 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:25:24.339402 22756 solver.cpp:237]     Train net output #1: loss = 0.0416551 (* 1 = 0.0416551 loss)
I1123 23:25:24.339402 22756 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1123 23:25:32.165822 22756 solver.cpp:218] Iteration 23600 (12.7777 iter/s, 7.82611s/100 iters), loss = 0.0771516
I1123 23:25:32.166323 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:25:32.166323 22756 solver.cpp:237]     Train net output #1: loss = 0.0771519 (* 1 = 0.0771519 loss)
I1123 23:25:32.166323 22756 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1123 23:25:39.991494 22756 solver.cpp:218] Iteration 23700 (12.7791 iter/s, 7.82525s/100 iters), loss = 0.0807293
I1123 23:25:39.991494 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:25:39.991494 22756 solver.cpp:237]     Train net output #1: loss = 0.0807296 (* 1 = 0.0807296 loss)
I1123 23:25:39.991494 22756 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1123 23:25:47.817600 22756 solver.cpp:218] Iteration 23800 (12.7791 iter/s, 7.82528s/100 iters), loss = 0.0663393
I1123 23:25:47.817600 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:25:47.817600 22756 solver.cpp:237]     Train net output #1: loss = 0.0663396 (* 1 = 0.0663396 loss)
I1123 23:25:47.817600 22756 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1123 23:25:55.641459 22756 solver.cpp:218] Iteration 23900 (12.782 iter/s, 7.82348s/100 iters), loss = 0.0483326
I1123 23:25:55.641459 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:25:55.641459 22756 solver.cpp:237]     Train net output #1: loss = 0.0483329 (* 1 = 0.0483329 loss)
I1123 23:25:55.641459 22756 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1123 23:26:03.087388 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:26:03.397428 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_24000.caffemodel
I1123 23:26:03.434427 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_24000.solverstate
I1123 23:26:03.453434 22756 solver.cpp:330] Iteration 24000, Testing net (#0)
I1123 23:26:03.453434 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:26:05.526660 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:26:05.610669 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9182
I1123 23:26:05.610669 22756 solver.cpp:397]     Test net output #1: loss = 0.247494 (* 1 = 0.247494 loss)
I1123 23:26:05.686678 22756 solver.cpp:218] Iteration 24000 (9.95498 iter/s, 10.0452s/100 iters), loss = 0.040091
I1123 23:26:05.686678 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:26:05.686678 22756 solver.cpp:237]     Train net output #1: loss = 0.0400913 (* 1 = 0.0400913 loss)
I1123 23:26:05.686678 22756 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1123 23:26:13.519418 22756 solver.cpp:218] Iteration 24100 (12.7681 iter/s, 7.83201s/100 iters), loss = 0.0971388
I1123 23:26:13.519418 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:26:13.519418 22756 solver.cpp:237]     Train net output #1: loss = 0.0971391 (* 1 = 0.0971391 loss)
I1123 23:26:13.519418 22756 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1123 23:26:21.350375 22756 solver.cpp:218] Iteration 24200 (12.7698 iter/s, 7.83097s/100 iters), loss = 0.0660805
I1123 23:26:21.351377 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:26:21.351377 22756 solver.cpp:237]     Train net output #1: loss = 0.0660808 (* 1 = 0.0660808 loss)
I1123 23:26:21.351377 22756 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1123 23:26:29.182348 22756 solver.cpp:218] Iteration 24300 (12.7702 iter/s, 7.83075s/100 iters), loss = 0.067705
I1123 23:26:29.182348 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:26:29.182348 22756 solver.cpp:237]     Train net output #1: loss = 0.0677053 (* 1 = 0.0677053 loss)
I1123 23:26:29.182348 22756 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1123 23:26:37.018159 22756 solver.cpp:218] Iteration 24400 (12.7631 iter/s, 7.83507s/100 iters), loss = 0.035886
I1123 23:26:37.018159 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:26:37.018159 22756 solver.cpp:237]     Train net output #1: loss = 0.0358863 (* 1 = 0.0358863 loss)
I1123 23:26:37.018159 22756 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1123 23:26:44.466121 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:26:44.776149 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_24500.caffemodel
I1123 23:26:44.816157 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_24500.solverstate
I1123 23:26:44.835160 22756 solver.cpp:330] Iteration 24500, Testing net (#0)
I1123 23:26:44.835160 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:26:46.909369 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:26:46.992377 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1123 23:26:46.992377 22756 solver.cpp:397]     Test net output #1: loss = 0.247426 (* 1 = 0.247426 loss)
I1123 23:26:47.069386 22756 solver.cpp:218] Iteration 24500 (9.94922 iter/s, 10.051s/100 iters), loss = 0.0394483
I1123 23:26:47.069386 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:26:47.069386 22756 solver.cpp:237]     Train net output #1: loss = 0.0394486 (* 1 = 0.0394486 loss)
I1123 23:26:47.069386 22756 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1123 23:26:54.901414 22756 solver.cpp:218] Iteration 24600 (12.769 iter/s, 7.83145s/100 iters), loss = 0.0902874
I1123 23:26:54.901414 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:26:54.901414 22756 solver.cpp:237]     Train net output #1: loss = 0.0902877 (* 1 = 0.0902877 loss)
I1123 23:26:54.901414 22756 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1123 23:27:02.734254 22756 solver.cpp:218] Iteration 24700 (12.7679 iter/s, 7.83217s/100 iters), loss = 0.0766205
I1123 23:27:02.734254 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:27:02.734254 22756 solver.cpp:237]     Train net output #1: loss = 0.0766208 (* 1 = 0.0766208 loss)
I1123 23:27:02.734254 22756 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1123 23:27:10.568141 22756 solver.cpp:218] Iteration 24800 (12.7647 iter/s, 7.83411s/100 iters), loss = 0.0503799
I1123 23:27:10.568141 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:27:10.568141 22756 solver.cpp:237]     Train net output #1: loss = 0.0503802 (* 1 = 0.0503802 loss)
I1123 23:27:10.568141 22756 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1123 23:27:18.401127 22756 solver.cpp:218] Iteration 24900 (12.7672 iter/s, 7.83254s/100 iters), loss = 0.0388719
I1123 23:27:18.402128 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:27:18.402128 22756 solver.cpp:237]     Train net output #1: loss = 0.0388722 (* 1 = 0.0388722 loss)
I1123 23:27:18.402128 22756 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1123 23:27:25.846998 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:27:26.157063 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_25000.caffemodel
I1123 23:27:26.198060 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_25000.solverstate
I1123 23:27:26.217056 22756 solver.cpp:330] Iteration 25000, Testing net (#0)
I1123 23:27:26.217056 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:27:28.291375 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:27:28.374380 22756 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1123 23:27:28.374380 22756 solver.cpp:397]     Test net output #1: loss = 0.247473 (* 1 = 0.247473 loss)
I1123 23:27:28.451390 22756 solver.cpp:218] Iteration 25000 (9.9511 iter/s, 10.0491s/100 iters), loss = 0.0588064
I1123 23:27:28.451390 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:27:28.451390 22756 solver.cpp:237]     Train net output #1: loss = 0.0588067 (* 1 = 0.0588067 loss)
I1123 23:27:28.451390 22756 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1123 23:27:36.281368 22756 solver.cpp:218] Iteration 25100 (12.7714 iter/s, 7.82997s/100 iters), loss = 0.0627804
I1123 23:27:36.281368 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:27:36.281368 22756 solver.cpp:237]     Train net output #1: loss = 0.0627807 (* 1 = 0.0627807 loss)
I1123 23:27:36.281368 22756 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1123 23:27:44.117305 22756 solver.cpp:218] Iteration 25200 (12.7638 iter/s, 7.83465s/100 iters), loss = 0.064262
I1123 23:27:44.117305 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:27:44.117305 22756 solver.cpp:237]     Train net output #1: loss = 0.0642623 (* 1 = 0.0642623 loss)
I1123 23:27:44.117305 22756 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1123 23:27:51.951634 22756 solver.cpp:218] Iteration 25300 (12.7645 iter/s, 7.83421s/100 iters), loss = 0.0447669
I1123 23:27:51.952134 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:27:51.952134 22756 solver.cpp:237]     Train net output #1: loss = 0.0447672 (* 1 = 0.0447672 loss)
I1123 23:27:51.952134 22756 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1123 23:27:59.790089 22756 solver.cpp:218] Iteration 25400 (12.7591 iter/s, 7.83754s/100 iters), loss = 0.0342903
I1123 23:27:59.790089 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:27:59.790089 22756 solver.cpp:237]     Train net output #1: loss = 0.0342907 (* 1 = 0.0342907 loss)
I1123 23:27:59.790089 22756 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1123 23:28:07.239025 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:28:07.549561 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_25500.caffemodel
I1123 23:28:07.589066 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_25500.solverstate
I1123 23:28:07.608063 22756 solver.cpp:330] Iteration 25500, Testing net (#0)
I1123 23:28:07.608063 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:28:09.682544 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:28:09.766556 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1123 23:28:09.766556 22756 solver.cpp:397]     Test net output #1: loss = 0.247491 (* 1 = 0.247491 loss)
I1123 23:28:09.842555 22756 solver.cpp:218] Iteration 25500 (9.9475 iter/s, 10.0528s/100 iters), loss = 0.0439864
I1123 23:28:09.842555 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:28:09.842555 22756 solver.cpp:237]     Train net output #1: loss = 0.0439867 (* 1 = 0.0439867 loss)
I1123 23:28:09.842555 22756 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1123 23:28:17.675503 22756 solver.cpp:218] Iteration 25600 (12.7686 iter/s, 7.83172s/100 iters), loss = 0.071444
I1123 23:28:17.675503 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:28:17.675503 22756 solver.cpp:237]     Train net output #1: loss = 0.0714444 (* 1 = 0.0714444 loss)
I1123 23:28:17.675503 22756 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1123 23:28:25.506472 22756 solver.cpp:218] Iteration 25700 (12.7702 iter/s, 7.83074s/100 iters), loss = 0.0550047
I1123 23:28:25.506472 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:28:25.506472 22756 solver.cpp:237]     Train net output #1: loss = 0.055005 (* 1 = 0.055005 loss)
I1123 23:28:25.506472 22756 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1123 23:28:33.335644 22756 solver.cpp:218] Iteration 25800 (12.7729 iter/s, 7.82909s/100 iters), loss = 0.0689484
I1123 23:28:33.335644 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:28:33.335644 22756 solver.cpp:237]     Train net output #1: loss = 0.0689487 (* 1 = 0.0689487 loss)
I1123 23:28:33.335644 22756 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1123 23:28:41.163589 22756 solver.cpp:218] Iteration 25900 (12.7766 iter/s, 7.82683s/100 iters), loss = 0.0374963
I1123 23:28:41.163589 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:28:41.163589 22756 solver.cpp:237]     Train net output #1: loss = 0.0374966 (* 1 = 0.0374966 loss)
I1123 23:28:41.163589 22756 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1123 23:28:48.606484 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:28:48.915513 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_26000.caffemodel
I1123 23:28:48.954519 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_26000.solverstate
I1123 23:28:48.973521 22756 solver.cpp:330] Iteration 26000, Testing net (#0)
I1123 23:28:48.973521 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:28:51.047796 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:28:51.131819 22756 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1123 23:28:51.131819 22756 solver.cpp:397]     Test net output #1: loss = 0.247396 (* 1 = 0.247396 loss)
I1123 23:28:51.207831 22756 solver.cpp:218] Iteration 26000 (9.95602 iter/s, 10.0442s/100 iters), loss = 0.0360224
I1123 23:28:51.207831 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:28:51.207831 22756 solver.cpp:237]     Train net output #1: loss = 0.0360227 (* 1 = 0.0360227 loss)
I1123 23:28:51.207831 22756 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1123 23:28:59.038714 22756 solver.cpp:218] Iteration 26100 (12.7707 iter/s, 7.83043s/100 iters), loss = 0.0718635
I1123 23:28:59.038714 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:28:59.038714 22756 solver.cpp:237]     Train net output #1: loss = 0.0718638 (* 1 = 0.0718638 loss)
I1123 23:28:59.038714 22756 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1123 23:29:06.868836 22756 solver.cpp:218] Iteration 26200 (12.7728 iter/s, 7.82913s/100 iters), loss = 0.0616333
I1123 23:29:06.868836 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:29:06.868836 22756 solver.cpp:237]     Train net output #1: loss = 0.0616336 (* 1 = 0.0616336 loss)
I1123 23:29:06.868836 22756 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1123 23:29:14.702807 22756 solver.cpp:218] Iteration 26300 (12.7658 iter/s, 7.83341s/100 iters), loss = 0.0376272
I1123 23:29:14.702807 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:29:14.702807 22756 solver.cpp:237]     Train net output #1: loss = 0.0376275 (* 1 = 0.0376275 loss)
I1123 23:29:14.702807 22756 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1123 23:29:22.535743 22756 solver.cpp:218] Iteration 26400 (12.7674 iter/s, 7.83247s/100 iters), loss = 0.034194
I1123 23:29:22.535743 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:29:22.535743 22756 solver.cpp:237]     Train net output #1: loss = 0.0341943 (* 1 = 0.0341943 loss)
I1123 23:29:22.535743 22756 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1123 23:29:29.978664 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:29:30.289712 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_26500.caffemodel
I1123 23:29:30.327711 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_26500.solverstate
I1123 23:29:30.345711 22756 solver.cpp:330] Iteration 26500, Testing net (#0)
I1123 23:29:30.345711 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:29:32.420092 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:29:32.504102 22756 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1123 23:29:32.504102 22756 solver.cpp:397]     Test net output #1: loss = 0.247422 (* 1 = 0.247422 loss)
I1123 23:29:32.580108 22756 solver.cpp:218] Iteration 26500 (9.95583 iter/s, 10.0444s/100 iters), loss = 0.0423346
I1123 23:29:32.580108 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:29:32.580108 22756 solver.cpp:237]     Train net output #1: loss = 0.0423348 (* 1 = 0.0423348 loss)
I1123 23:29:32.580108 22756 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1123 23:29:40.405791 22756 solver.cpp:218] Iteration 26600 (12.7794 iter/s, 7.82507s/100 iters), loss = 0.104867
I1123 23:29:40.405791 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:29:40.405791 22756 solver.cpp:237]     Train net output #1: loss = 0.104867 (* 1 = 0.104867 loss)
I1123 23:29:40.405791 22756 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1123 23:29:48.232926 22756 solver.cpp:218] Iteration 26700 (12.7767 iter/s, 7.82673s/100 iters), loss = 0.0595619
I1123 23:29:48.233927 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:29:48.233927 22756 solver.cpp:237]     Train net output #1: loss = 0.0595622 (* 1 = 0.0595622 loss)
I1123 23:29:48.233927 22756 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1123 23:29:56.065815 22756 solver.cpp:218] Iteration 26800 (12.7686 iter/s, 7.83169s/100 iters), loss = 0.0542647
I1123 23:29:56.065815 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:29:56.065815 22756 solver.cpp:237]     Train net output #1: loss = 0.054265 (* 1 = 0.054265 loss)
I1123 23:29:56.065815 22756 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1123 23:30:03.896651 22756 solver.cpp:218] Iteration 26900 (12.7702 iter/s, 7.83072s/100 iters), loss = 0.0224944
I1123 23:30:03.896651 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:30:03.896651 22756 solver.cpp:237]     Train net output #1: loss = 0.0224946 (* 1 = 0.0224946 loss)
I1123 23:30:03.896651 22756 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1123 23:30:11.341503 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:30:11.650516 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_27000.caffemodel
I1123 23:30:11.691526 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_27000.solverstate
I1123 23:30:11.710036 22756 solver.cpp:330] Iteration 27000, Testing net (#0)
I1123 23:30:11.710036 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:30:13.784816 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:30:13.868818 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9178
I1123 23:30:13.868818 22756 solver.cpp:397]     Test net output #1: loss = 0.247405 (* 1 = 0.247405 loss)
I1123 23:30:13.945827 22756 solver.cpp:218] Iteration 27000 (9.95212 iter/s, 10.0481s/100 iters), loss = 0.0551976
I1123 23:30:13.945827 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:30:13.945827 22756 solver.cpp:237]     Train net output #1: loss = 0.0551978 (* 1 = 0.0551978 loss)
I1123 23:30:13.945827 22756 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1123 23:30:13.945827 22756 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1123 23:30:21.778198 22756 solver.cpp:218] Iteration 27100 (12.7678 iter/s, 7.83217s/100 iters), loss = 0.0856215
I1123 23:30:21.778700 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:30:21.778700 22756 solver.cpp:237]     Train net output #1: loss = 0.0856218 (* 1 = 0.0856218 loss)
I1123 23:30:21.778700 22756 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1123 23:30:29.608526 22756 solver.cpp:218] Iteration 27200 (12.7708 iter/s, 7.83038s/100 iters), loss = 0.0741834
I1123 23:30:29.609527 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:30:29.609527 22756 solver.cpp:237]     Train net output #1: loss = 0.0741837 (* 1 = 0.0741837 loss)
I1123 23:30:29.609527 22756 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1123 23:30:37.440440 22756 solver.cpp:218] Iteration 27300 (12.7704 iter/s, 7.83061s/100 iters), loss = 0.0450387
I1123 23:30:37.440440 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:30:37.440440 22756 solver.cpp:237]     Train net output #1: loss = 0.0450389 (* 1 = 0.0450389 loss)
I1123 23:30:37.440440 22756 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1123 23:30:45.272380 22756 solver.cpp:218] Iteration 27400 (12.768 iter/s, 7.8321s/100 iters), loss = 0.0292047
I1123 23:30:45.272380 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:30:45.272380 22756 solver.cpp:237]     Train net output #1: loss = 0.0292049 (* 1 = 0.0292049 loss)
I1123 23:30:45.272380 22756 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1123 23:30:52.713165 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:30:53.022194 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_27500.caffemodel
I1123 23:30:53.061203 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_27500.solverstate
I1123 23:30:53.079210 22756 solver.cpp:330] Iteration 27500, Testing net (#0)
I1123 23:30:53.079210 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:30:55.155580 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:30:55.238582 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9183
I1123 23:30:55.238582 22756 solver.cpp:397]     Test net output #1: loss = 0.247436 (* 1 = 0.247436 loss)
I1123 23:30:55.315593 22756 solver.cpp:218] Iteration 27500 (9.95741 iter/s, 10.0428s/100 iters), loss = 0.0510008
I1123 23:30:55.315593 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:30:55.315593 22756 solver.cpp:237]     Train net output #1: loss = 0.051001 (* 1 = 0.051001 loss)
I1123 23:30:55.315593 22756 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1123 23:31:03.147495 22756 solver.cpp:218] Iteration 27600 (12.7699 iter/s, 7.83092s/100 iters), loss = 0.0660918
I1123 23:31:03.147495 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:31:03.147495 22756 solver.cpp:237]     Train net output #1: loss = 0.066092 (* 1 = 0.066092 loss)
I1123 23:31:03.147495 22756 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1123 23:31:10.980439 22756 solver.cpp:218] Iteration 27700 (12.7678 iter/s, 7.83219s/100 iters), loss = 0.055745
I1123 23:31:10.980439 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:31:10.980439 22756 solver.cpp:237]     Train net output #1: loss = 0.0557452 (* 1 = 0.0557452 loss)
I1123 23:31:10.980439 22756 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1123 23:31:18.815268 22756 solver.cpp:218] Iteration 27800 (12.7632 iter/s, 7.83503s/100 iters), loss = 0.0554316
I1123 23:31:18.815268 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:31:18.815268 22756 solver.cpp:237]     Train net output #1: loss = 0.0554318 (* 1 = 0.0554318 loss)
I1123 23:31:18.815268 22756 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1123 23:31:26.648371 22756 solver.cpp:218] Iteration 27900 (12.7677 iter/s, 7.83226s/100 iters), loss = 0.0315364
I1123 23:31:26.648371 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:31:26.648371 22756 solver.cpp:237]     Train net output #1: loss = 0.0315367 (* 1 = 0.0315367 loss)
I1123 23:31:26.648371 22756 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1123 23:31:34.096271 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:31:34.406299 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_28000.caffemodel
I1123 23:31:34.446307 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_28000.solverstate
I1123 23:31:34.464807 22756 solver.cpp:330] Iteration 28000, Testing net (#0)
I1123 23:31:34.464807 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:31:36.539880 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:31:36.623905 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1123 23:31:36.623905 22756 solver.cpp:397]     Test net output #1: loss = 0.247443 (* 1 = 0.247443 loss)
I1123 23:31:36.700911 22756 solver.cpp:218] Iteration 28000 (9.94854 iter/s, 10.0517s/100 iters), loss = 0.0585165
I1123 23:31:36.700911 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:31:36.700911 22756 solver.cpp:237]     Train net output #1: loss = 0.0585167 (* 1 = 0.0585167 loss)
I1123 23:31:36.700911 22756 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1123 23:31:44.535809 22756 solver.cpp:218] Iteration 28100 (12.763 iter/s, 7.83515s/100 iters), loss = 0.10017
I1123 23:31:44.535809 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1123 23:31:44.535809 22756 solver.cpp:237]     Train net output #1: loss = 0.10017 (* 1 = 0.10017 loss)
I1123 23:31:44.535809 22756 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1123 23:31:52.371804 22756 solver.cpp:218] Iteration 28200 (12.7639 iter/s, 7.83462s/100 iters), loss = 0.0514921
I1123 23:31:52.371804 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:31:52.371804 22756 solver.cpp:237]     Train net output #1: loss = 0.0514923 (* 1 = 0.0514923 loss)
I1123 23:31:52.371804 22756 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1123 23:32:00.208688 22756 solver.cpp:218] Iteration 28300 (12.7608 iter/s, 7.83651s/100 iters), loss = 0.0464385
I1123 23:32:00.208688 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:32:00.208688 22756 solver.cpp:237]     Train net output #1: loss = 0.0464387 (* 1 = 0.0464387 loss)
I1123 23:32:00.208688 22756 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1123 23:32:08.039646 22756 solver.cpp:218] Iteration 28400 (12.7702 iter/s, 7.83072s/100 iters), loss = 0.0273301
I1123 23:32:08.039646 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:32:08.039646 22756 solver.cpp:237]     Train net output #1: loss = 0.0273304 (* 1 = 0.0273304 loss)
I1123 23:32:08.039646 22756 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1123 23:32:15.483490 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:32:15.793035 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_28500.caffemodel
I1123 23:32:15.831540 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_28500.solverstate
I1123 23:32:15.850539 22756 solver.cpp:330] Iteration 28500, Testing net (#0)
I1123 23:32:15.850539 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:32:17.924801 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:32:18.007812 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9181
I1123 23:32:18.007812 22756 solver.cpp:397]     Test net output #1: loss = 0.247456 (* 1 = 0.247456 loss)
I1123 23:32:18.084811 22756 solver.cpp:218] Iteration 28500 (9.95584 iter/s, 10.0444s/100 iters), loss = 0.0378611
I1123 23:32:18.084811 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:32:18.084811 22756 solver.cpp:237]     Train net output #1: loss = 0.0378613 (* 1 = 0.0378613 loss)
I1123 23:32:18.084811 22756 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1123 23:32:25.913940 22756 solver.cpp:218] Iteration 28600 (12.7727 iter/s, 7.82923s/100 iters), loss = 0.0927922
I1123 23:32:25.913940 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1123 23:32:25.913940 22756 solver.cpp:237]     Train net output #1: loss = 0.0927923 (* 1 = 0.0927923 loss)
I1123 23:32:25.913940 22756 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1123 23:32:33.745888 22756 solver.cpp:218] Iteration 28700 (12.7697 iter/s, 7.83102s/100 iters), loss = 0.0896358
I1123 23:32:33.745888 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:32:33.745888 22756 solver.cpp:237]     Train net output #1: loss = 0.0896359 (* 1 = 0.0896359 loss)
I1123 23:32:33.745888 22756 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1123 23:32:41.576315 22756 solver.cpp:218] Iteration 28800 (12.7719 iter/s, 7.82968s/100 iters), loss = 0.0771331
I1123 23:32:41.576315 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:32:41.576315 22756 solver.cpp:237]     Train net output #1: loss = 0.0771333 (* 1 = 0.0771333 loss)
I1123 23:32:41.576315 22756 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1123 23:32:49.404783 22756 solver.cpp:218] Iteration 28900 (12.7746 iter/s, 7.82804s/100 iters), loss = 0.0408127
I1123 23:32:49.404783 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:32:49.404783 22756 solver.cpp:237]     Train net output #1: loss = 0.0408128 (* 1 = 0.0408128 loss)
I1123 23:32:49.404783 22756 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1123 23:32:56.846330 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:32:57.155370 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_29000.caffemodel
I1123 23:32:57.197890 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_29000.solverstate
I1123 23:32:57.216881 22756 solver.cpp:330] Iteration 29000, Testing net (#0)
I1123 23:32:57.216881 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:32:59.290573 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:32:59.374577 22756 solver.cpp:397]     Test net output #0: accuracy = 0.9179
I1123 23:32:59.374577 22756 solver.cpp:397]     Test net output #1: loss = 0.2475 (* 1 = 0.2475 loss)
I1123 23:32:59.450580 22756 solver.cpp:218] Iteration 29000 (9.95447 iter/s, 10.0457s/100 iters), loss = 0.057574
I1123 23:32:59.450580 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:32:59.450580 22756 solver.cpp:237]     Train net output #1: loss = 0.0575742 (* 1 = 0.0575742 loss)
I1123 23:32:59.450580 22756 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1123 23:33:07.278873 22756 solver.cpp:218] Iteration 29100 (12.7758 iter/s, 7.8273s/100 iters), loss = 0.0780494
I1123 23:33:07.278873 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1123 23:33:07.278873 22756 solver.cpp:237]     Train net output #1: loss = 0.0780495 (* 1 = 0.0780495 loss)
I1123 23:33:07.278873 22756 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1123 23:33:15.112200 22756 solver.cpp:218] Iteration 29200 (12.7655 iter/s, 7.8336s/100 iters), loss = 0.0533111
I1123 23:33:15.112200 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:33:15.112200 22756 solver.cpp:237]     Train net output #1: loss = 0.0533113 (* 1 = 0.0533113 loss)
I1123 23:33:15.112200 22756 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1123 23:33:22.944103 22756 solver.cpp:218] Iteration 29300 (12.77 iter/s, 7.83085s/100 iters), loss = 0.056658
I1123 23:33:22.944103 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:33:22.944103 22756 solver.cpp:237]     Train net output #1: loss = 0.0566581 (* 1 = 0.0566581 loss)
I1123 23:33:22.944103 22756 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1123 23:33:30.776849 22756 solver.cpp:218] Iteration 29400 (12.768 iter/s, 7.83207s/100 iters), loss = 0.0338798
I1123 23:33:30.776849 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:33:30.776849 22756 solver.cpp:237]     Train net output #1: loss = 0.03388 (* 1 = 0.03388 loss)
I1123 23:33:30.776849 22756 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1123 23:33:38.221709 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:33:38.531738 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_29500.caffemodel
I1123 23:33:38.572247 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_29500.solverstate
I1123 23:33:38.591246 22756 solver.cpp:330] Iteration 29500, Testing net (#0)
I1123 23:33:38.591246 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:33:40.667105 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:33:40.751113 22756 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1123 23:33:40.751113 22756 solver.cpp:397]     Test net output #1: loss = 0.247388 (* 1 = 0.247388 loss)
I1123 23:33:40.827128 22756 solver.cpp:218] Iteration 29500 (9.95016 iter/s, 10.0501s/100 iters), loss = 0.043163
I1123 23:33:40.827128 22756 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1123 23:33:40.827128 22756 solver.cpp:237]     Train net output #1: loss = 0.0431631 (* 1 = 0.0431631 loss)
I1123 23:33:40.827128 22756 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1123 23:33:48.661888 22756 solver.cpp:218] Iteration 29600 (12.7638 iter/s, 7.83466s/100 iters), loss = 0.0672318
I1123 23:33:48.661888 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:33:48.661888 22756 solver.cpp:237]     Train net output #1: loss = 0.0672319 (* 1 = 0.0672319 loss)
I1123 23:33:48.661888 22756 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1123 23:33:56.493871 22756 solver.cpp:218] Iteration 29700 (12.7695 iter/s, 7.83114s/100 iters), loss = 0.0706943
I1123 23:33:56.493871 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1123 23:33:56.493871 22756 solver.cpp:237]     Train net output #1: loss = 0.0706945 (* 1 = 0.0706945 loss)
I1123 23:33:56.493871 22756 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1123 23:34:04.325767 22756 solver.cpp:218] Iteration 29800 (12.7698 iter/s, 7.83097s/100 iters), loss = 0.0473349
I1123 23:34:04.325767 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:34:04.325767 22756 solver.cpp:237]     Train net output #1: loss = 0.047335 (* 1 = 0.047335 loss)
I1123 23:34:04.325767 22756 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1123 23:34:12.152716 22756 solver.cpp:218] Iteration 29900 (12.7759 iter/s, 7.82724s/100 iters), loss = 0.0441587
I1123 23:34:12.152716 22756 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1123 23:34:12.152716 22756 solver.cpp:237]     Train net output #1: loss = 0.0441589 (* 1 = 0.0441589 loss)
I1123 23:34:12.152716 22756 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1123 23:34:19.596608 24544 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:34:19.906635 22756 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_30000.caffemodel
I1123 23:34:19.947149 22756 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_30000.solverstate
I1123 23:34:19.991652 22756 solver.cpp:310] Iteration 30000, loss = 0.0577618
I1123 23:34:19.991652 22756 solver.cpp:330] Iteration 30000, Testing net (#0)
I1123 23:34:19.991652 22756 net.cpp:676] Ignoring source layer accuracy_training
I1123 23:34:22.065903 24248 data_layer.cpp:73] Restarting data prefetching from start.
I1123 23:34:22.149912 22756 solver.cpp:397]     Test net output #0: accuracy = 0.918
I1123 23:34:22.149912 22756 solver.cpp:397]     Test net output #1: loss = 0.247423 (* 1 = 0.247423 loss)
I1123 23:34:22.149912 22756 solver.cpp:315] Optimization Done.
I1123 23:34:22.149912 22756 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 
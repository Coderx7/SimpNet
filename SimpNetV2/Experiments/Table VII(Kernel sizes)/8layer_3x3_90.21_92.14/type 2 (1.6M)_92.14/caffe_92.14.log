
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1124 06:17:35.964728 35012 caffe.cpp:219] Using GPUs 0
I1124 06:17:36.135308 35012 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1124 06:17:36.438318 35012 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1124 06:17:36.455318 35012 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_1.6k_8L_3x3"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1124 06:17:36.455318 35012 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1124 06:17:36.456318 35012 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1124 06:17:36.456318 35012 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1124 06:17:36.456318 35012 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1124 06:17:36.456318 35012 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1124 06:17:36.456318 35012 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1124 06:17:36.456318 35012 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1124 06:17:36.456318 35012 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1124 06:17:36.456318 35012 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1124 06:17:36.456318 35012 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1124 06:17:36.456318 35012 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1124 06:17:36.456318 35012 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1124 06:17:36.456318 35012 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1124 06:17:36.456318 35012 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_1.6M"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 66
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 215
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1124 06:17:36.493319 35012 layer_factory.cpp:58] Creating layer cifar
I1124 06:17:36.501325 35012 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1124 06:17:36.501325 35012 net.cpp:84] Creating Layer cifar
I1124 06:17:36.501325 35012 net.cpp:380] cifar -> data
I1124 06:17:36.501325 35012 net.cpp:380] cifar -> label
I1124 06:17:36.502324 35012 data_layer.cpp:45] output data size: 100,3,32,32
I1124 06:17:36.509325 35012 net.cpp:122] Setting up cifar
I1124 06:17:36.509325 35012 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1124 06:17:36.509325 35012 net.cpp:129] Top shape: 100 (100)
I1124 06:17:36.509325 35012 net.cpp:137] Memory required for data: 1229200
I1124 06:17:36.509325 35012 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1124 06:17:36.509325 35012 net.cpp:84] Creating Layer label_cifar_1_split
I1124 06:17:36.509325 35012 net.cpp:406] label_cifar_1_split <- label
I1124 06:17:36.509325 35012 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1124 06:17:36.509325 35012 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1124 06:17:36.509325 35012 net.cpp:122] Setting up label_cifar_1_split
I1124 06:17:36.509824 35012 net.cpp:129] Top shape: 100 (100)
I1124 06:17:36.509824 35012 net.cpp:129] Top shape: 100 (100)
I1124 06:17:36.509824 35012 net.cpp:137] Memory required for data: 1230000
I1124 06:17:36.509824 35012 layer_factory.cpp:58] Creating layer conv1
I1124 06:17:36.509824 35012 net.cpp:84] Creating Layer conv1
I1124 06:17:36.509824 35012 net.cpp:406] conv1 <- data
I1124 06:17:36.509824 35012 net.cpp:380] conv1 -> conv1
I1124 06:17:36.510324 24484 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1124 06:17:36.779356 35012 net.cpp:122] Setting up conv1
I1124 06:17:36.780355 35012 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 06:17:36.780355 35012 net.cpp:137] Memory required for data: 28263600
I1124 06:17:36.780355 35012 layer_factory.cpp:58] Creating layer bn1
I1124 06:17:36.780355 35012 net.cpp:84] Creating Layer bn1
I1124 06:17:36.780355 35012 net.cpp:406] bn1 <- conv1
I1124 06:17:36.780355 35012 net.cpp:367] bn1 -> conv1 (in-place)
I1124 06:17:36.780355 35012 net.cpp:122] Setting up bn1
I1124 06:17:36.780355 35012 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 06:17:36.780355 35012 net.cpp:137] Memory required for data: 55297200
I1124 06:17:36.780355 35012 layer_factory.cpp:58] Creating layer scale1
I1124 06:17:36.780355 35012 net.cpp:84] Creating Layer scale1
I1124 06:17:36.780355 35012 net.cpp:406] scale1 <- conv1
I1124 06:17:36.780355 35012 net.cpp:367] scale1 -> conv1 (in-place)
I1124 06:17:36.780355 35012 layer_factory.cpp:58] Creating layer scale1
I1124 06:17:36.780355 35012 net.cpp:122] Setting up scale1
I1124 06:17:36.780355 35012 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 06:17:36.780355 35012 net.cpp:137] Memory required for data: 82330800
I1124 06:17:36.780355 35012 layer_factory.cpp:58] Creating layer relu1
I1124 06:17:36.780355 35012 net.cpp:84] Creating Layer relu1
I1124 06:17:36.780355 35012 net.cpp:406] relu1 <- conv1
I1124 06:17:36.780355 35012 net.cpp:367] relu1 -> conv1 (in-place)
I1124 06:17:36.780355 35012 net.cpp:122] Setting up relu1
I1124 06:17:36.780355 35012 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 06:17:36.780355 35012 net.cpp:137] Memory required for data: 109364400
I1124 06:17:36.780355 35012 layer_factory.cpp:58] Creating layer conv2
I1124 06:17:36.780355 35012 net.cpp:84] Creating Layer conv2
I1124 06:17:36.780355 35012 net.cpp:406] conv2 <- conv1
I1124 06:17:36.780355 35012 net.cpp:380] conv2 -> conv2
I1124 06:17:36.783355 35012 net.cpp:122] Setting up conv2
I1124 06:17:36.783355 35012 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 06:17:36.783355 35012 net.cpp:137] Memory required for data: 148686000
I1124 06:17:36.783355 35012 layer_factory.cpp:58] Creating layer bn2
I1124 06:17:36.783355 35012 net.cpp:84] Creating Layer bn2
I1124 06:17:36.783355 35012 net.cpp:406] bn2 <- conv2
I1124 06:17:36.783355 35012 net.cpp:367] bn2 -> conv2 (in-place)
I1124 06:17:36.783355 35012 net.cpp:122] Setting up bn2
I1124 06:17:36.783355 35012 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 06:17:36.783355 35012 net.cpp:137] Memory required for data: 188007600
I1124 06:17:36.783355 35012 layer_factory.cpp:58] Creating layer scale2
I1124 06:17:36.783355 35012 net.cpp:84] Creating Layer scale2
I1124 06:17:36.783355 35012 net.cpp:406] scale2 <- conv2
I1124 06:17:36.783355 35012 net.cpp:367] scale2 -> conv2 (in-place)
I1124 06:17:36.783355 35012 layer_factory.cpp:58] Creating layer scale2
I1124 06:17:36.783355 35012 net.cpp:122] Setting up scale2
I1124 06:17:36.783355 35012 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 06:17:36.783355 35012 net.cpp:137] Memory required for data: 227329200
I1124 06:17:36.783355 35012 layer_factory.cpp:58] Creating layer relu2
I1124 06:17:36.783355 35012 net.cpp:84] Creating Layer relu2
I1124 06:17:36.783355 35012 net.cpp:406] relu2 <- conv2
I1124 06:17:36.783355 35012 net.cpp:367] relu2 -> conv2 (in-place)
I1124 06:17:36.783355 35012 net.cpp:122] Setting up relu2
I1124 06:17:36.783355 35012 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 06:17:36.783355 35012 net.cpp:137] Memory required for data: 266650800
I1124 06:17:36.783355 35012 layer_factory.cpp:58] Creating layer conv2_2
I1124 06:17:36.783355 35012 net.cpp:84] Creating Layer conv2_2
I1124 06:17:36.783355 35012 net.cpp:406] conv2_2 <- conv2
I1124 06:17:36.783355 35012 net.cpp:380] conv2_2 -> conv2_2
I1124 06:17:36.785354 35012 net.cpp:122] Setting up conv2_2
I1124 06:17:36.785354 35012 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 06:17:36.785354 35012 net.cpp:137] Memory required for data: 319079600
I1124 06:17:36.785354 35012 layer_factory.cpp:58] Creating layer bn2_2
I1124 06:17:36.785354 35012 net.cpp:84] Creating Layer bn2_2
I1124 06:17:36.785354 35012 net.cpp:406] bn2_2 <- conv2_2
I1124 06:17:36.785354 35012 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1124 06:17:36.785354 35012 net.cpp:122] Setting up bn2_2
I1124 06:17:36.785354 35012 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 06:17:36.785354 35012 net.cpp:137] Memory required for data: 371508400
I1124 06:17:36.785354 35012 layer_factory.cpp:58] Creating layer scale2_2
I1124 06:17:36.785354 35012 net.cpp:84] Creating Layer scale2_2
I1124 06:17:36.785354 35012 net.cpp:406] scale2_2 <- conv2_2
I1124 06:17:36.785354 35012 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1124 06:17:36.785354 35012 layer_factory.cpp:58] Creating layer scale2_2
I1124 06:17:36.786355 35012 net.cpp:122] Setting up scale2_2
I1124 06:17:36.786355 35012 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 06:17:36.786355 35012 net.cpp:137] Memory required for data: 423937200
I1124 06:17:36.786355 35012 layer_factory.cpp:58] Creating layer relu2_2
I1124 06:17:36.786355 35012 net.cpp:84] Creating Layer relu2_2
I1124 06:17:36.786355 35012 net.cpp:406] relu2_2 <- conv2_2
I1124 06:17:36.786355 35012 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1124 06:17:36.786355 35012 net.cpp:122] Setting up relu2_2
I1124 06:17:36.786355 35012 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 06:17:36.786355 35012 net.cpp:137] Memory required for data: 476366000
I1124 06:17:36.786355 35012 layer_factory.cpp:58] Creating layer pool2_1
I1124 06:17:36.786355 35012 net.cpp:84] Creating Layer pool2_1
I1124 06:17:36.786355 35012 net.cpp:406] pool2_1 <- conv2_2
I1124 06:17:36.786355 35012 net.cpp:380] pool2_1 -> pool2_1
I1124 06:17:36.786355 35012 net.cpp:122] Setting up pool2_1
I1124 06:17:36.786355 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.786355 35012 net.cpp:137] Memory required for data: 489473200
I1124 06:17:36.786355 35012 layer_factory.cpp:58] Creating layer conv3
I1124 06:17:36.786355 35012 net.cpp:84] Creating Layer conv3
I1124 06:17:36.786355 35012 net.cpp:406] conv3 <- pool2_1
I1124 06:17:36.786355 35012 net.cpp:380] conv3 -> conv3
I1124 06:17:36.788355 35012 net.cpp:122] Setting up conv3
I1124 06:17:36.788355 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.788355 35012 net.cpp:137] Memory required for data: 502580400
I1124 06:17:36.788355 35012 layer_factory.cpp:58] Creating layer bn3
I1124 06:17:36.788355 35012 net.cpp:84] Creating Layer bn3
I1124 06:17:36.788355 35012 net.cpp:406] bn3 <- conv3
I1124 06:17:36.788355 35012 net.cpp:367] bn3 -> conv3 (in-place)
I1124 06:17:36.788355 35012 net.cpp:122] Setting up bn3
I1124 06:17:36.788355 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.788355 35012 net.cpp:137] Memory required for data: 515687600
I1124 06:17:36.788355 35012 layer_factory.cpp:58] Creating layer scale3
I1124 06:17:36.788355 35012 net.cpp:84] Creating Layer scale3
I1124 06:17:36.788355 35012 net.cpp:406] scale3 <- conv3
I1124 06:17:36.788355 35012 net.cpp:367] scale3 -> conv3 (in-place)
I1124 06:17:36.788355 35012 layer_factory.cpp:58] Creating layer scale3
I1124 06:17:36.788355 35012 net.cpp:122] Setting up scale3
I1124 06:17:36.788355 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.788355 35012 net.cpp:137] Memory required for data: 528794800
I1124 06:17:36.788355 35012 layer_factory.cpp:58] Creating layer relu3
I1124 06:17:36.788355 35012 net.cpp:84] Creating Layer relu3
I1124 06:17:36.788355 35012 net.cpp:406] relu3 <- conv3
I1124 06:17:36.788355 35012 net.cpp:367] relu3 -> conv3 (in-place)
I1124 06:17:36.789355 35012 net.cpp:122] Setting up relu3
I1124 06:17:36.789355 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.789355 35012 net.cpp:137] Memory required for data: 541902000
I1124 06:17:36.789355 35012 layer_factory.cpp:58] Creating layer conv4
I1124 06:17:36.789355 35012 net.cpp:84] Creating Layer conv4
I1124 06:17:36.789355 35012 net.cpp:406] conv4 <- conv3
I1124 06:17:36.789355 35012 net.cpp:380] conv4 -> conv4
I1124 06:17:36.791355 35012 net.cpp:122] Setting up conv4
I1124 06:17:36.791355 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.791355 35012 net.cpp:137] Memory required for data: 555009200
I1124 06:17:36.791355 35012 layer_factory.cpp:58] Creating layer bn4
I1124 06:17:36.791355 35012 net.cpp:84] Creating Layer bn4
I1124 06:17:36.791355 35012 net.cpp:406] bn4 <- conv4
I1124 06:17:36.791355 35012 net.cpp:367] bn4 -> conv4 (in-place)
I1124 06:17:36.791355 35012 net.cpp:122] Setting up bn4
I1124 06:17:36.791355 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.791355 35012 net.cpp:137] Memory required for data: 568116400
I1124 06:17:36.791355 35012 layer_factory.cpp:58] Creating layer scale4
I1124 06:17:36.791355 35012 net.cpp:84] Creating Layer scale4
I1124 06:17:36.791355 35012 net.cpp:406] scale4 <- conv4
I1124 06:17:36.791355 35012 net.cpp:367] scale4 -> conv4 (in-place)
I1124 06:17:36.791355 35012 layer_factory.cpp:58] Creating layer scale4
I1124 06:17:36.791355 35012 net.cpp:122] Setting up scale4
I1124 06:17:36.791355 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.791355 35012 net.cpp:137] Memory required for data: 581223600
I1124 06:17:36.791355 35012 layer_factory.cpp:58] Creating layer relu4
I1124 06:17:36.791355 35012 net.cpp:84] Creating Layer relu4
I1124 06:17:36.791355 35012 net.cpp:406] relu4 <- conv4
I1124 06:17:36.791355 35012 net.cpp:367] relu4 -> conv4 (in-place)
I1124 06:17:36.792356 35012 net.cpp:122] Setting up relu4
I1124 06:17:36.792356 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.792356 35012 net.cpp:137] Memory required for data: 594330800
I1124 06:17:36.792356 35012 layer_factory.cpp:58] Creating layer conv4_1
I1124 06:17:36.792356 35012 net.cpp:84] Creating Layer conv4_1
I1124 06:17:36.792356 35012 net.cpp:406] conv4_1 <- conv4
I1124 06:17:36.792356 35012 net.cpp:380] conv4_1 -> conv4_1
I1124 06:17:36.794872 35012 net.cpp:122] Setting up conv4_1
I1124 06:17:36.794872 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.794872 35012 net.cpp:137] Memory required for data: 607438000
I1124 06:17:36.794872 35012 layer_factory.cpp:58] Creating layer bn4_1
I1124 06:17:36.794872 35012 net.cpp:84] Creating Layer bn4_1
I1124 06:17:36.794872 35012 net.cpp:406] bn4_1 <- conv4_1
I1124 06:17:36.794872 35012 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1124 06:17:36.794872 35012 net.cpp:122] Setting up bn4_1
I1124 06:17:36.794872 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.794872 35012 net.cpp:137] Memory required for data: 620545200
I1124 06:17:36.794872 35012 layer_factory.cpp:58] Creating layer scale4_1
I1124 06:17:36.794872 35012 net.cpp:84] Creating Layer scale4_1
I1124 06:17:36.794872 35012 net.cpp:406] scale4_1 <- conv4_1
I1124 06:17:36.794872 35012 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1124 06:17:36.794872 35012 layer_factory.cpp:58] Creating layer scale4_1
I1124 06:17:36.794872 35012 net.cpp:122] Setting up scale4_1
I1124 06:17:36.794872 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.794872 35012 net.cpp:137] Memory required for data: 633652400
I1124 06:17:36.794872 35012 layer_factory.cpp:58] Creating layer relu4_1
I1124 06:17:36.794872 35012 net.cpp:84] Creating Layer relu4_1
I1124 06:17:36.794872 35012 net.cpp:406] relu4_1 <- conv4_1
I1124 06:17:36.794872 35012 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1124 06:17:36.795879 35012 net.cpp:122] Setting up relu4_1
I1124 06:17:36.795879 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.795879 35012 net.cpp:137] Memory required for data: 646759600
I1124 06:17:36.795879 35012 layer_factory.cpp:58] Creating layer conv4_2
I1124 06:17:36.795879 35012 net.cpp:84] Creating Layer conv4_2
I1124 06:17:36.795879 35012 net.cpp:406] conv4_2 <- conv4_1
I1124 06:17:36.795879 35012 net.cpp:380] conv4_2 -> conv4_2
I1124 06:17:36.799871 35012 net.cpp:122] Setting up conv4_2
I1124 06:17:36.799871 35012 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 06:17:36.799871 35012 net.cpp:137] Memory required for data: 668775600
I1124 06:17:36.799871 35012 layer_factory.cpp:58] Creating layer bn4_2
I1124 06:17:36.799871 35012 net.cpp:84] Creating Layer bn4_2
I1124 06:17:36.799871 35012 net.cpp:406] bn4_2 <- conv4_2
I1124 06:17:36.799871 35012 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1124 06:17:36.799871 35012 net.cpp:122] Setting up bn4_2
I1124 06:17:36.799871 35012 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 06:17:36.799871 35012 net.cpp:137] Memory required for data: 690791600
I1124 06:17:36.799871 35012 layer_factory.cpp:58] Creating layer scale4_2
I1124 06:17:36.799871 35012 net.cpp:84] Creating Layer scale4_2
I1124 06:17:36.799871 35012 net.cpp:406] scale4_2 <- conv4_2
I1124 06:17:36.799871 35012 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1124 06:17:36.799871 35012 layer_factory.cpp:58] Creating layer scale4_2
I1124 06:17:36.800370 35012 net.cpp:122] Setting up scale4_2
I1124 06:17:36.800370 35012 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 06:17:36.800370 35012 net.cpp:137] Memory required for data: 712807600
I1124 06:17:36.800370 35012 layer_factory.cpp:58] Creating layer relu4_2
I1124 06:17:36.800370 35012 net.cpp:84] Creating Layer relu4_2
I1124 06:17:36.800370 35012 net.cpp:406] relu4_2 <- conv4_2
I1124 06:17:36.800370 35012 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1124 06:17:36.800370 35012 net.cpp:122] Setting up relu4_2
I1124 06:17:36.800370 35012 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 06:17:36.800370 35012 net.cpp:137] Memory required for data: 734823600
I1124 06:17:36.800370 35012 layer_factory.cpp:58] Creating layer pool4_2
I1124 06:17:36.800370 35012 net.cpp:84] Creating Layer pool4_2
I1124 06:17:36.800370 35012 net.cpp:406] pool4_2 <- conv4_2
I1124 06:17:36.800370 35012 net.cpp:380] pool4_2 -> pool4_2
I1124 06:17:36.800370 35012 net.cpp:122] Setting up pool4_2
I1124 06:17:36.800370 35012 net.cpp:129] Top shape: 100 215 8 8 (1376000)
I1124 06:17:36.800370 35012 net.cpp:137] Memory required for data: 740327600
I1124 06:17:36.800370 35012 layer_factory.cpp:58] Creating layer conv12
I1124 06:17:36.800370 35012 net.cpp:84] Creating Layer conv12
I1124 06:17:36.800370 35012 net.cpp:406] conv12 <- pool4_2
I1124 06:17:36.800370 35012 net.cpp:380] conv12 -> conv12
I1124 06:17:36.806870 35012 net.cpp:122] Setting up conv12
I1124 06:17:36.806870 35012 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 06:17:36.806870 35012 net.cpp:137] Memory required for data: 750158000
I1124 06:17:36.806870 35012 layer_factory.cpp:58] Creating layer bn_conv12
I1124 06:17:36.806870 35012 net.cpp:84] Creating Layer bn_conv12
I1124 06:17:36.806870 35012 net.cpp:406] bn_conv12 <- conv12
I1124 06:17:36.806870 35012 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1124 06:17:36.807370 35012 net.cpp:122] Setting up bn_conv12
I1124 06:17:36.807370 35012 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 06:17:36.807370 35012 net.cpp:137] Memory required for data: 759988400
I1124 06:17:36.807370 35012 layer_factory.cpp:58] Creating layer scale_conv12
I1124 06:17:36.807370 35012 net.cpp:84] Creating Layer scale_conv12
I1124 06:17:36.807370 35012 net.cpp:406] scale_conv12 <- conv12
I1124 06:17:36.807370 35012 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1124 06:17:36.807370 35012 layer_factory.cpp:58] Creating layer scale_conv12
I1124 06:17:36.807370 35012 net.cpp:122] Setting up scale_conv12
I1124 06:17:36.807370 35012 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 06:17:36.807370 35012 net.cpp:137] Memory required for data: 769818800
I1124 06:17:36.807370 35012 layer_factory.cpp:58] Creating layer relu_conv12
I1124 06:17:36.807370 35012 net.cpp:84] Creating Layer relu_conv12
I1124 06:17:36.807370 35012 net.cpp:406] relu_conv12 <- conv12
I1124 06:17:36.807370 35012 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1124 06:17:36.807370 35012 net.cpp:122] Setting up relu_conv12
I1124 06:17:36.807370 35012 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 06:17:36.807370 35012 net.cpp:137] Memory required for data: 779649200
I1124 06:17:36.807370 35012 layer_factory.cpp:58] Creating layer poolcp6
I1124 06:17:36.807870 35012 net.cpp:84] Creating Layer poolcp6
I1124 06:17:36.807870 35012 net.cpp:406] poolcp6 <- conv12
I1124 06:17:36.807870 35012 net.cpp:380] poolcp6 -> poolcp6
I1124 06:17:36.807870 35012 net.cpp:122] Setting up poolcp6
I1124 06:17:36.807870 35012 net.cpp:129] Top shape: 100 384 1 1 (38400)
I1124 06:17:36.807870 35012 net.cpp:137] Memory required for data: 779802800
I1124 06:17:36.807870 35012 layer_factory.cpp:58] Creating layer ip1
I1124 06:17:36.807870 35012 net.cpp:84] Creating Layer ip1
I1124 06:17:36.807870 35012 net.cpp:406] ip1 <- poolcp6
I1124 06:17:36.807870 35012 net.cpp:380] ip1 -> ip1
I1124 06:17:36.807870 35012 net.cpp:122] Setting up ip1
I1124 06:17:36.807870 35012 net.cpp:129] Top shape: 100 10 (1000)
I1124 06:17:36.807870 35012 net.cpp:137] Memory required for data: 779806800
I1124 06:17:36.807870 35012 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1124 06:17:36.807870 35012 net.cpp:84] Creating Layer ip1_ip1_0_split
I1124 06:17:36.807870 35012 net.cpp:406] ip1_ip1_0_split <- ip1
I1124 06:17:36.807870 35012 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1124 06:17:36.807870 35012 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1124 06:17:36.807870 35012 net.cpp:122] Setting up ip1_ip1_0_split
I1124 06:17:36.807870 35012 net.cpp:129] Top shape: 100 10 (1000)
I1124 06:17:36.807870 35012 net.cpp:129] Top shape: 100 10 (1000)
I1124 06:17:36.807870 35012 net.cpp:137] Memory required for data: 779814800
I1124 06:17:36.807870 35012 layer_factory.cpp:58] Creating layer accuracy_training
I1124 06:17:36.807870 35012 net.cpp:84] Creating Layer accuracy_training
I1124 06:17:36.807870 35012 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1124 06:17:36.807870 35012 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1124 06:17:36.807870 35012 net.cpp:380] accuracy_training -> accuracy_training
I1124 06:17:36.807870 35012 net.cpp:122] Setting up accuracy_training
I1124 06:17:36.807870 35012 net.cpp:129] Top shape: (1)
I1124 06:17:36.807870 35012 net.cpp:137] Memory required for data: 779814804
I1124 06:17:36.807870 35012 layer_factory.cpp:58] Creating layer loss
I1124 06:17:36.807870 35012 net.cpp:84] Creating Layer loss
I1124 06:17:36.807870 35012 net.cpp:406] loss <- ip1_ip1_0_split_1
I1124 06:17:36.807870 35012 net.cpp:406] loss <- label_cifar_1_split_1
I1124 06:17:36.808370 35012 net.cpp:380] loss -> loss
I1124 06:17:36.808370 35012 layer_factory.cpp:58] Creating layer loss
I1124 06:17:36.808370 35012 net.cpp:122] Setting up loss
I1124 06:17:36.808370 35012 net.cpp:129] Top shape: (1)
I1124 06:17:36.808370 35012 net.cpp:132]     with loss weight 1
I1124 06:17:36.808370 35012 net.cpp:137] Memory required for data: 779814808
I1124 06:17:36.808370 35012 net.cpp:198] loss needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:200] accuracy_training does not need backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] ip1 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] poolcp6 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] relu_conv12 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] scale_conv12 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] bn_conv12 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] conv12 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] pool4_2 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] relu4_2 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] scale4_2 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] bn4_2 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] conv4_2 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] relu4_1 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] scale4_1 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] bn4_1 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] conv4_1 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] relu4 needs backward computation.
I1124 06:17:36.808370 35012 net.cpp:198] scale4 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] bn4 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] conv4 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] relu3 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] scale3 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] bn3 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] conv3 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] pool2_1 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] relu2_2 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] scale2_2 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] bn2_2 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] conv2_2 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] relu2 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] scale2 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] bn2 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] conv2 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] relu1 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] scale1 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] bn1 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:198] conv1 needs backward computation.
I1124 06:17:36.808871 35012 net.cpp:200] label_cifar_1_split does not need backward computation.
I1124 06:17:36.808871 35012 net.cpp:200] cifar does not need backward computation.
I1124 06:17:36.808871 35012 net.cpp:242] This network produces output accuracy_training
I1124 06:17:36.808871 35012 net.cpp:242] This network produces output loss
I1124 06:17:36.808871 35012 net.cpp:255] Network initialization done.
I1124 06:17:36.809370 35012 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1124 06:17:36.809370 35012 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1124 06:17:36.809370 35012 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1124 06:17:36.809370 35012 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1124 06:17:36.809370 35012 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1124 06:17:36.809370 35012 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1124 06:17:36.809370 35012 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1124 06:17:36.809370 35012 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1124 06:17:36.809370 35012 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1124 06:17:36.809370 35012 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1124 06:17:36.809370 35012 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1124 06:17:36.809370 35012 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1124 06:17:36.809370 35012 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1124 06:17:36.809870 35012 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_1.6M"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 66
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 215
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1124 06:17:36.809870 35012 layer_factory.cpp:58] Creating layer cifar
I1124 06:17:36.873387 35012 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1124 06:17:36.873387 35012 net.cpp:84] Creating Layer cifar
I1124 06:17:36.873387 35012 net.cpp:380] cifar -> data
I1124 06:17:36.873387 35012 net.cpp:380] cifar -> label
I1124 06:17:36.873387 35012 data_layer.cpp:45] output data size: 100,3,32,32
I1124 06:17:36.880374 35012 net.cpp:122] Setting up cifar
I1124 06:17:36.880374 35012 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1124 06:17:36.880374 35012 net.cpp:129] Top shape: 100 (100)
I1124 06:17:36.880374 35012 net.cpp:137] Memory required for data: 1229200
I1124 06:17:36.880374 35012 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1124 06:17:36.880374 35012 net.cpp:84] Creating Layer label_cifar_1_split
I1124 06:17:36.880374 35012 net.cpp:406] label_cifar_1_split <- label
I1124 06:17:36.880374 35012 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1124 06:17:36.880374 35012 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1124 06:17:36.880374 35012 net.cpp:122] Setting up label_cifar_1_split
I1124 06:17:36.880374 35012 net.cpp:129] Top shape: 100 (100)
I1124 06:17:36.880374 35012 net.cpp:129] Top shape: 100 (100)
I1124 06:17:36.880374 35012 net.cpp:137] Memory required for data: 1230000
I1124 06:17:36.880374 35012 layer_factory.cpp:58] Creating layer conv1
I1124 06:17:36.880374 35012 net.cpp:84] Creating Layer conv1
I1124 06:17:36.880374 35012 net.cpp:406] conv1 <- data
I1124 06:17:36.880374 35012 net.cpp:380] conv1 -> conv1
I1124 06:17:36.881384 15732 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1124 06:17:36.881384 35012 net.cpp:122] Setting up conv1
I1124 06:17:36.881384 35012 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 06:17:36.881384 35012 net.cpp:137] Memory required for data: 28263600
I1124 06:17:36.881384 35012 layer_factory.cpp:58] Creating layer bn1
I1124 06:17:36.881384 35012 net.cpp:84] Creating Layer bn1
I1124 06:17:36.881384 35012 net.cpp:406] bn1 <- conv1
I1124 06:17:36.881384 35012 net.cpp:367] bn1 -> conv1 (in-place)
I1124 06:17:36.882374 35012 net.cpp:122] Setting up bn1
I1124 06:17:36.882374 35012 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 06:17:36.882374 35012 net.cpp:137] Memory required for data: 55297200
I1124 06:17:36.882374 35012 layer_factory.cpp:58] Creating layer scale1
I1124 06:17:36.882374 35012 net.cpp:84] Creating Layer scale1
I1124 06:17:36.882374 35012 net.cpp:406] scale1 <- conv1
I1124 06:17:36.882374 35012 net.cpp:367] scale1 -> conv1 (in-place)
I1124 06:17:36.882374 35012 layer_factory.cpp:58] Creating layer scale1
I1124 06:17:36.882374 35012 net.cpp:122] Setting up scale1
I1124 06:17:36.882374 35012 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 06:17:36.882374 35012 net.cpp:137] Memory required for data: 82330800
I1124 06:17:36.882374 35012 layer_factory.cpp:58] Creating layer relu1
I1124 06:17:36.882374 35012 net.cpp:84] Creating Layer relu1
I1124 06:17:36.882374 35012 net.cpp:406] relu1 <- conv1
I1124 06:17:36.882374 35012 net.cpp:367] relu1 -> conv1 (in-place)
I1124 06:17:36.882374 35012 net.cpp:122] Setting up relu1
I1124 06:17:36.882374 35012 net.cpp:129] Top shape: 100 66 32 32 (6758400)
I1124 06:17:36.882374 35012 net.cpp:137] Memory required for data: 109364400
I1124 06:17:36.882374 35012 layer_factory.cpp:58] Creating layer conv2
I1124 06:17:36.882374 35012 net.cpp:84] Creating Layer conv2
I1124 06:17:36.882374 35012 net.cpp:406] conv2 <- conv1
I1124 06:17:36.882374 35012 net.cpp:380] conv2 -> conv2
I1124 06:17:36.884373 35012 net.cpp:122] Setting up conv2
I1124 06:17:36.884373 35012 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 06:17:36.884373 35012 net.cpp:137] Memory required for data: 148686000
I1124 06:17:36.884373 35012 layer_factory.cpp:58] Creating layer bn2
I1124 06:17:36.884373 35012 net.cpp:84] Creating Layer bn2
I1124 06:17:36.884373 35012 net.cpp:406] bn2 <- conv2
I1124 06:17:36.884373 35012 net.cpp:367] bn2 -> conv2 (in-place)
I1124 06:17:36.884373 35012 net.cpp:122] Setting up bn2
I1124 06:17:36.884373 35012 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 06:17:36.884373 35012 net.cpp:137] Memory required for data: 188007600
I1124 06:17:36.884373 35012 layer_factory.cpp:58] Creating layer scale2
I1124 06:17:36.884373 35012 net.cpp:84] Creating Layer scale2
I1124 06:17:36.884373 35012 net.cpp:406] scale2 <- conv2
I1124 06:17:36.884373 35012 net.cpp:367] scale2 -> conv2 (in-place)
I1124 06:17:36.885373 35012 layer_factory.cpp:58] Creating layer scale2
I1124 06:17:36.885373 35012 net.cpp:122] Setting up scale2
I1124 06:17:36.885373 35012 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 06:17:36.885373 35012 net.cpp:137] Memory required for data: 227329200
I1124 06:17:36.885373 35012 layer_factory.cpp:58] Creating layer relu2
I1124 06:17:36.885373 35012 net.cpp:84] Creating Layer relu2
I1124 06:17:36.885373 35012 net.cpp:406] relu2 <- conv2
I1124 06:17:36.885373 35012 net.cpp:367] relu2 -> conv2 (in-place)
I1124 06:17:36.885373 35012 net.cpp:122] Setting up relu2
I1124 06:17:36.885373 35012 net.cpp:129] Top shape: 100 96 32 32 (9830400)
I1124 06:17:36.885373 35012 net.cpp:137] Memory required for data: 266650800
I1124 06:17:36.885373 35012 layer_factory.cpp:58] Creating layer conv2_2
I1124 06:17:36.885373 35012 net.cpp:84] Creating Layer conv2_2
I1124 06:17:36.885373 35012 net.cpp:406] conv2_2 <- conv2
I1124 06:17:36.885373 35012 net.cpp:380] conv2_2 -> conv2_2
I1124 06:17:36.887373 35012 net.cpp:122] Setting up conv2_2
I1124 06:17:36.887373 35012 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 06:17:36.887373 35012 net.cpp:137] Memory required for data: 319079600
I1124 06:17:36.887373 35012 layer_factory.cpp:58] Creating layer bn2_2
I1124 06:17:36.887373 35012 net.cpp:84] Creating Layer bn2_2
I1124 06:17:36.887373 35012 net.cpp:406] bn2_2 <- conv2_2
I1124 06:17:36.888373 35012 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1124 06:17:36.888373 35012 net.cpp:122] Setting up bn2_2
I1124 06:17:36.888373 35012 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 06:17:36.888373 35012 net.cpp:137] Memory required for data: 371508400
I1124 06:17:36.888373 35012 layer_factory.cpp:58] Creating layer scale2_2
I1124 06:17:36.888373 35012 net.cpp:84] Creating Layer scale2_2
I1124 06:17:36.888373 35012 net.cpp:406] scale2_2 <- conv2_2
I1124 06:17:36.888373 35012 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1124 06:17:36.888373 35012 layer_factory.cpp:58] Creating layer scale2_2
I1124 06:17:36.888373 35012 net.cpp:122] Setting up scale2_2
I1124 06:17:36.888373 35012 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 06:17:36.888373 35012 net.cpp:137] Memory required for data: 423937200
I1124 06:17:36.888373 35012 layer_factory.cpp:58] Creating layer relu2_2
I1124 06:17:36.888373 35012 net.cpp:84] Creating Layer relu2_2
I1124 06:17:36.888373 35012 net.cpp:406] relu2_2 <- conv2_2
I1124 06:17:36.888373 35012 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1124 06:17:36.888373 35012 net.cpp:122] Setting up relu2_2
I1124 06:17:36.888373 35012 net.cpp:129] Top shape: 100 128 32 32 (13107200)
I1124 06:17:36.888373 35012 net.cpp:137] Memory required for data: 476366000
I1124 06:17:36.888373 35012 layer_factory.cpp:58] Creating layer pool2_1
I1124 06:17:36.888373 35012 net.cpp:84] Creating Layer pool2_1
I1124 06:17:36.888373 35012 net.cpp:406] pool2_1 <- conv2_2
I1124 06:17:36.888373 35012 net.cpp:380] pool2_1 -> pool2_1
I1124 06:17:36.888373 35012 net.cpp:122] Setting up pool2_1
I1124 06:17:36.888373 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.888373 35012 net.cpp:137] Memory required for data: 489473200
I1124 06:17:36.888373 35012 layer_factory.cpp:58] Creating layer conv3
I1124 06:17:36.888373 35012 net.cpp:84] Creating Layer conv3
I1124 06:17:36.888373 35012 net.cpp:406] conv3 <- pool2_1
I1124 06:17:36.888373 35012 net.cpp:380] conv3 -> conv3
I1124 06:17:36.891373 35012 net.cpp:122] Setting up conv3
I1124 06:17:36.891373 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.891373 35012 net.cpp:137] Memory required for data: 502580400
I1124 06:17:36.891373 35012 layer_factory.cpp:58] Creating layer bn3
I1124 06:17:36.891373 35012 net.cpp:84] Creating Layer bn3
I1124 06:17:36.891373 35012 net.cpp:406] bn3 <- conv3
I1124 06:17:36.891373 35012 net.cpp:367] bn3 -> conv3 (in-place)
I1124 06:17:36.891373 35012 net.cpp:122] Setting up bn3
I1124 06:17:36.892374 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.892374 35012 net.cpp:137] Memory required for data: 515687600
I1124 06:17:36.892374 35012 layer_factory.cpp:58] Creating layer scale3
I1124 06:17:36.892374 35012 net.cpp:84] Creating Layer scale3
I1124 06:17:36.892374 35012 net.cpp:406] scale3 <- conv3
I1124 06:17:36.892374 35012 net.cpp:367] scale3 -> conv3 (in-place)
I1124 06:17:36.892374 35012 layer_factory.cpp:58] Creating layer scale3
I1124 06:17:36.892374 35012 net.cpp:122] Setting up scale3
I1124 06:17:36.892374 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.892374 35012 net.cpp:137] Memory required for data: 528794800
I1124 06:17:36.892374 35012 layer_factory.cpp:58] Creating layer relu3
I1124 06:17:36.892374 35012 net.cpp:84] Creating Layer relu3
I1124 06:17:36.892374 35012 net.cpp:406] relu3 <- conv3
I1124 06:17:36.892374 35012 net.cpp:367] relu3 -> conv3 (in-place)
I1124 06:17:36.892374 35012 net.cpp:122] Setting up relu3
I1124 06:17:36.892374 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.892374 35012 net.cpp:137] Memory required for data: 541902000
I1124 06:17:36.892374 35012 layer_factory.cpp:58] Creating layer conv4
I1124 06:17:36.892374 35012 net.cpp:84] Creating Layer conv4
I1124 06:17:36.892374 35012 net.cpp:406] conv4 <- conv3
I1124 06:17:36.892374 35012 net.cpp:380] conv4 -> conv4
I1124 06:17:36.894886 35012 net.cpp:122] Setting up conv4
I1124 06:17:36.894886 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.894886 35012 net.cpp:137] Memory required for data: 555009200
I1124 06:17:36.894886 35012 layer_factory.cpp:58] Creating layer bn4
I1124 06:17:36.894886 35012 net.cpp:84] Creating Layer bn4
I1124 06:17:36.894886 35012 net.cpp:406] bn4 <- conv4
I1124 06:17:36.894886 35012 net.cpp:367] bn4 -> conv4 (in-place)
I1124 06:17:36.895386 35012 net.cpp:122] Setting up bn4
I1124 06:17:36.895386 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.895386 35012 net.cpp:137] Memory required for data: 568116400
I1124 06:17:36.895386 35012 layer_factory.cpp:58] Creating layer scale4
I1124 06:17:36.895386 35012 net.cpp:84] Creating Layer scale4
I1124 06:17:36.895386 35012 net.cpp:406] scale4 <- conv4
I1124 06:17:36.895386 35012 net.cpp:367] scale4 -> conv4 (in-place)
I1124 06:17:36.895386 35012 layer_factory.cpp:58] Creating layer scale4
I1124 06:17:36.895386 35012 net.cpp:122] Setting up scale4
I1124 06:17:36.895386 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.895386 35012 net.cpp:137] Memory required for data: 581223600
I1124 06:17:36.895386 35012 layer_factory.cpp:58] Creating layer relu4
I1124 06:17:36.895386 35012 net.cpp:84] Creating Layer relu4
I1124 06:17:36.895386 35012 net.cpp:406] relu4 <- conv4
I1124 06:17:36.895386 35012 net.cpp:367] relu4 -> conv4 (in-place)
I1124 06:17:36.895886 35012 net.cpp:122] Setting up relu4
I1124 06:17:36.895886 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.895886 35012 net.cpp:137] Memory required for data: 594330800
I1124 06:17:36.895886 35012 layer_factory.cpp:58] Creating layer conv4_1
I1124 06:17:36.895886 35012 net.cpp:84] Creating Layer conv4_1
I1124 06:17:36.895886 35012 net.cpp:406] conv4_1 <- conv4
I1124 06:17:36.895886 35012 net.cpp:380] conv4_1 -> conv4_1
I1124 06:17:36.897886 35012 net.cpp:122] Setting up conv4_1
I1124 06:17:36.897886 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.897886 35012 net.cpp:137] Memory required for data: 607438000
I1124 06:17:36.897886 35012 layer_factory.cpp:58] Creating layer bn4_1
I1124 06:17:36.897886 35012 net.cpp:84] Creating Layer bn4_1
I1124 06:17:36.897886 35012 net.cpp:406] bn4_1 <- conv4_1
I1124 06:17:36.897886 35012 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1124 06:17:36.898386 35012 net.cpp:122] Setting up bn4_1
I1124 06:17:36.898386 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.898386 35012 net.cpp:137] Memory required for data: 620545200
I1124 06:17:36.898386 35012 layer_factory.cpp:58] Creating layer scale4_1
I1124 06:17:36.898386 35012 net.cpp:84] Creating Layer scale4_1
I1124 06:17:36.898386 35012 net.cpp:406] scale4_1 <- conv4_1
I1124 06:17:36.898386 35012 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1124 06:17:36.898386 35012 layer_factory.cpp:58] Creating layer scale4_1
I1124 06:17:36.898386 35012 net.cpp:122] Setting up scale4_1
I1124 06:17:36.898386 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.898386 35012 net.cpp:137] Memory required for data: 633652400
I1124 06:17:36.898386 35012 layer_factory.cpp:58] Creating layer relu4_1
I1124 06:17:36.898386 35012 net.cpp:84] Creating Layer relu4_1
I1124 06:17:36.898386 35012 net.cpp:406] relu4_1 <- conv4_1
I1124 06:17:36.898386 35012 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1124 06:17:36.898886 35012 net.cpp:122] Setting up relu4_1
I1124 06:17:36.898886 35012 net.cpp:129] Top shape: 100 128 16 16 (3276800)
I1124 06:17:36.898886 35012 net.cpp:137] Memory required for data: 646759600
I1124 06:17:36.898886 35012 layer_factory.cpp:58] Creating layer conv4_2
I1124 06:17:36.898886 35012 net.cpp:84] Creating Layer conv4_2
I1124 06:17:36.898886 35012 net.cpp:406] conv4_2 <- conv4_1
I1124 06:17:36.898886 35012 net.cpp:380] conv4_2 -> conv4_2
I1124 06:17:36.901387 35012 net.cpp:122] Setting up conv4_2
I1124 06:17:36.901387 35012 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 06:17:36.901387 35012 net.cpp:137] Memory required for data: 668775600
I1124 06:17:36.901387 35012 layer_factory.cpp:58] Creating layer bn4_2
I1124 06:17:36.901387 35012 net.cpp:84] Creating Layer bn4_2
I1124 06:17:36.901387 35012 net.cpp:406] bn4_2 <- conv4_2
I1124 06:17:36.901387 35012 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1124 06:17:36.901886 35012 net.cpp:122] Setting up bn4_2
I1124 06:17:36.901886 35012 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 06:17:36.901886 35012 net.cpp:137] Memory required for data: 690791600
I1124 06:17:36.901886 35012 layer_factory.cpp:58] Creating layer scale4_2
I1124 06:17:36.901886 35012 net.cpp:84] Creating Layer scale4_2
I1124 06:17:36.901886 35012 net.cpp:406] scale4_2 <- conv4_2
I1124 06:17:36.901886 35012 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1124 06:17:36.901886 35012 layer_factory.cpp:58] Creating layer scale4_2
I1124 06:17:36.901886 35012 net.cpp:122] Setting up scale4_2
I1124 06:17:36.901886 35012 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 06:17:36.901886 35012 net.cpp:137] Memory required for data: 712807600
I1124 06:17:36.901886 35012 layer_factory.cpp:58] Creating layer relu4_2
I1124 06:17:36.901886 35012 net.cpp:84] Creating Layer relu4_2
I1124 06:17:36.901886 35012 net.cpp:406] relu4_2 <- conv4_2
I1124 06:17:36.901886 35012 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1124 06:17:36.902389 35012 net.cpp:122] Setting up relu4_2
I1124 06:17:36.902389 35012 net.cpp:129] Top shape: 100 215 16 16 (5504000)
I1124 06:17:36.902389 35012 net.cpp:137] Memory required for data: 734823600
I1124 06:17:36.902389 35012 layer_factory.cpp:58] Creating layer pool4_2
I1124 06:17:36.902389 35012 net.cpp:84] Creating Layer pool4_2
I1124 06:17:36.902389 35012 net.cpp:406] pool4_2 <- conv4_2
I1124 06:17:36.902389 35012 net.cpp:380] pool4_2 -> pool4_2
I1124 06:17:36.902389 35012 net.cpp:122] Setting up pool4_2
I1124 06:17:36.902389 35012 net.cpp:129] Top shape: 100 215 8 8 (1376000)
I1124 06:17:36.902389 35012 net.cpp:137] Memory required for data: 740327600
I1124 06:17:36.902389 35012 layer_factory.cpp:58] Creating layer conv12
I1124 06:17:36.902886 35012 net.cpp:84] Creating Layer conv12
I1124 06:17:36.902886 35012 net.cpp:406] conv12 <- pool4_2
I1124 06:17:36.902886 35012 net.cpp:380] conv12 -> conv12
I1124 06:17:36.909489 35012 net.cpp:122] Setting up conv12
I1124 06:17:36.909489 35012 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 06:17:36.909489 35012 net.cpp:137] Memory required for data: 750158000
I1124 06:17:36.909489 35012 layer_factory.cpp:58] Creating layer bn_conv12
I1124 06:17:36.909489 35012 net.cpp:84] Creating Layer bn_conv12
I1124 06:17:36.909489 35012 net.cpp:406] bn_conv12 <- conv12
I1124 06:17:36.909489 35012 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1124 06:17:36.909489 35012 net.cpp:122] Setting up bn_conv12
I1124 06:17:36.909489 35012 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 06:17:36.909489 35012 net.cpp:137] Memory required for data: 759988400
I1124 06:17:36.909489 35012 layer_factory.cpp:58] Creating layer scale_conv12
I1124 06:17:36.909489 35012 net.cpp:84] Creating Layer scale_conv12
I1124 06:17:36.909489 35012 net.cpp:406] scale_conv12 <- conv12
I1124 06:17:36.909489 35012 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1124 06:17:36.909489 35012 layer_factory.cpp:58] Creating layer scale_conv12
I1124 06:17:36.909996 35012 net.cpp:122] Setting up scale_conv12
I1124 06:17:36.909996 35012 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 06:17:36.909996 35012 net.cpp:137] Memory required for data: 769818800
I1124 06:17:36.909996 35012 layer_factory.cpp:58] Creating layer relu_conv12
I1124 06:17:36.909996 35012 net.cpp:84] Creating Layer relu_conv12
I1124 06:17:36.909996 35012 net.cpp:406] relu_conv12 <- conv12
I1124 06:17:36.909996 35012 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1124 06:17:36.909996 35012 net.cpp:122] Setting up relu_conv12
I1124 06:17:36.909996 35012 net.cpp:129] Top shape: 100 384 8 8 (2457600)
I1124 06:17:36.909996 35012 net.cpp:137] Memory required for data: 779649200
I1124 06:17:36.909996 35012 layer_factory.cpp:58] Creating layer poolcp6
I1124 06:17:36.909996 35012 net.cpp:84] Creating Layer poolcp6
I1124 06:17:36.909996 35012 net.cpp:406] poolcp6 <- conv12
I1124 06:17:36.909996 35012 net.cpp:380] poolcp6 -> poolcp6
I1124 06:17:36.909996 35012 net.cpp:122] Setting up poolcp6
I1124 06:17:36.909996 35012 net.cpp:129] Top shape: 100 384 1 1 (38400)
I1124 06:17:36.909996 35012 net.cpp:137] Memory required for data: 779802800
I1124 06:17:36.909996 35012 layer_factory.cpp:58] Creating layer ip1
I1124 06:17:36.909996 35012 net.cpp:84] Creating Layer ip1
I1124 06:17:36.909996 35012 net.cpp:406] ip1 <- poolcp6
I1124 06:17:36.909996 35012 net.cpp:380] ip1 -> ip1
I1124 06:17:36.911059 35012 net.cpp:122] Setting up ip1
I1124 06:17:36.911059 35012 net.cpp:129] Top shape: 100 10 (1000)
I1124 06:17:36.911059 35012 net.cpp:137] Memory required for data: 779806800
I1124 06:17:36.911059 35012 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1124 06:17:36.911059 35012 net.cpp:84] Creating Layer ip1_ip1_0_split
I1124 06:17:36.911059 35012 net.cpp:406] ip1_ip1_0_split <- ip1
I1124 06:17:36.911059 35012 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1124 06:17:36.911059 35012 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1124 06:17:36.911059 35012 net.cpp:122] Setting up ip1_ip1_0_split
I1124 06:17:36.911059 35012 net.cpp:129] Top shape: 100 10 (1000)
I1124 06:17:36.911059 35012 net.cpp:129] Top shape: 100 10 (1000)
I1124 06:17:36.911059 35012 net.cpp:137] Memory required for data: 779814800
I1124 06:17:36.911059 35012 layer_factory.cpp:58] Creating layer accuracy
I1124 06:17:36.911059 35012 net.cpp:84] Creating Layer accuracy
I1124 06:17:36.911059 35012 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1124 06:17:36.911059 35012 net.cpp:406] accuracy <- label_cifar_1_split_0
I1124 06:17:36.911059 35012 net.cpp:380] accuracy -> accuracy
I1124 06:17:36.911059 35012 net.cpp:122] Setting up accuracy
I1124 06:17:36.911059 35012 net.cpp:129] Top shape: (1)
I1124 06:17:36.911059 35012 net.cpp:137] Memory required for data: 779814804
I1124 06:17:36.911059 35012 layer_factory.cpp:58] Creating layer loss
I1124 06:17:36.911059 35012 net.cpp:84] Creating Layer loss
I1124 06:17:36.911059 35012 net.cpp:406] loss <- ip1_ip1_0_split_1
I1124 06:17:36.911059 35012 net.cpp:406] loss <- label_cifar_1_split_1
I1124 06:17:36.911059 35012 net.cpp:380] loss -> loss
I1124 06:17:36.911059 35012 layer_factory.cpp:58] Creating layer loss
I1124 06:17:36.911059 35012 net.cpp:122] Setting up loss
I1124 06:17:36.911059 35012 net.cpp:129] Top shape: (1)
I1124 06:17:36.911059 35012 net.cpp:132]     with loss weight 1
I1124 06:17:36.911059 35012 net.cpp:137] Memory required for data: 779814808
I1124 06:17:36.911059 35012 net.cpp:198] loss needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:200] accuracy does not need backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] ip1 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] poolcp6 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] relu_conv12 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] scale_conv12 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] bn_conv12 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] conv12 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] pool4_2 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] relu4_2 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] scale4_2 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] bn4_2 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] conv4_2 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] relu4_1 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] scale4_1 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] bn4_1 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] conv4_1 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] relu4 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] scale4 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] bn4 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] conv4 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] relu3 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] scale3 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] bn3 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] conv3 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] pool2_1 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] relu2_2 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] scale2_2 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] bn2_2 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] conv2_2 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] relu2 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] scale2 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] bn2 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] conv2 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] relu1 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] scale1 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] bn1 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:198] conv1 needs backward computation.
I1124 06:17:36.911059 35012 net.cpp:200] label_cifar_1_split does not need backward computation.
I1124 06:17:36.911059 35012 net.cpp:200] cifar does not need backward computation.
I1124 06:17:36.911059 35012 net.cpp:242] This network produces output accuracy
I1124 06:17:36.911059 35012 net.cpp:242] This network produces output loss
I1124 06:17:36.911059 35012 net.cpp:255] Network initialization done.
I1124 06:17:36.912045 35012 solver.cpp:56] Solver scaffolding done.
I1124 06:17:36.914059 35012 caffe.cpp:249] Starting Optimization
I1124 06:17:36.914059 35012 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_1.6M
I1124 06:17:36.914059 35012 solver.cpp:273] Learning Rate Policy: multistep
I1124 06:17:36.916060 35012 solver.cpp:330] Iteration 0, Testing net (#0)
I1124 06:17:36.918050 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:17:39.048568 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:17:39.132220 35012 solver.cpp:397]     Test net output #0: accuracy = 0.1009
I1124 06:17:39.132220 35012 solver.cpp:397]     Test net output #1: loss = 78.5243 (* 1 = 78.5243 loss)
I1124 06:17:39.259766 35012 solver.cpp:218] Iteration 0 (0 iter/s, 2.34514s/100 iters), loss = 4.0367
I1124 06:17:39.259766 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.12
I1124 06:17:39.259766 35012 solver.cpp:237]     Train net output #1: loss = 4.0367 (* 1 = 4.0367 loss)
I1124 06:17:39.259766 35012 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1124 06:17:47.132863 35012 solver.cpp:218] Iteration 100 (12.7019 iter/s, 7.87283s/100 iters), loss = 2.62131
I1124 06:17:47.132863 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.36
I1124 06:17:47.132863 35012 solver.cpp:237]     Train net output #1: loss = 2.62131 (* 1 = 2.62131 loss)
I1124 06:17:47.132863 35012 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1124 06:17:55.003769 35012 solver.cpp:218] Iteration 200 (12.7069 iter/s, 7.86975s/100 iters), loss = 1.94535
I1124 06:17:55.003769 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.31
I1124 06:17:55.003769 35012 solver.cpp:237]     Train net output #1: loss = 1.94535 (* 1 = 1.94535 loss)
I1124 06:17:55.003769 35012 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1124 06:18:02.826268 35012 solver.cpp:218] Iteration 300 (12.7848 iter/s, 7.8218s/100 iters), loss = 1.60929
I1124 06:18:02.826268 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.49
I1124 06:18:02.826268 35012 solver.cpp:237]     Train net output #1: loss = 1.60929 (* 1 = 1.60929 loss)
I1124 06:18:02.826268 35012 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1124 06:18:10.683712 35012 solver.cpp:218] Iteration 400 (12.7268 iter/s, 7.85742s/100 iters), loss = 1.68198
I1124 06:18:10.683712 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.4
I1124 06:18:10.683712 35012 solver.cpp:237]     Train net output #1: loss = 1.68198 (* 1 = 1.68198 loss)
I1124 06:18:10.683712 35012 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1124 06:18:18.200114 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:18:18.510223 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_500.caffemodel
I1124 06:18:18.555290 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_500.solverstate
I1124 06:18:18.574288 35012 solver.cpp:330] Iteration 500, Testing net (#0)
I1124 06:18:18.574288 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:18:20.648077 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:18:20.732121 35012 solver.cpp:397]     Test net output #0: accuracy = 0.4328
I1124 06:18:20.732121 35012 solver.cpp:397]     Test net output #1: loss = 1.52431 (* 1 = 1.52431 loss)
I1124 06:18:20.808135 35012 solver.cpp:218] Iteration 500 (9.87733 iter/s, 10.1242s/100 iters), loss = 1.32821
I1124 06:18:20.808135 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.59
I1124 06:18:20.808135 35012 solver.cpp:237]     Train net output #1: loss = 1.32821 (* 1 = 1.32821 loss)
I1124 06:18:20.808135 35012 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1124 06:18:28.662050 35012 solver.cpp:218] Iteration 600 (12.7338 iter/s, 7.85315s/100 iters), loss = 1.26203
I1124 06:18:28.662050 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.57
I1124 06:18:28.662050 35012 solver.cpp:237]     Train net output #1: loss = 1.26203 (* 1 = 1.26203 loss)
I1124 06:18:28.662050 35012 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1124 06:18:36.519906 35012 solver.cpp:218] Iteration 700 (12.7276 iter/s, 7.85692s/100 iters), loss = 1.29852
I1124 06:18:36.519906 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.56
I1124 06:18:36.519906 35012 solver.cpp:237]     Train net output #1: loss = 1.29852 (* 1 = 1.29852 loss)
I1124 06:18:36.519906 35012 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1124 06:18:44.385432 35012 solver.cpp:218] Iteration 800 (12.7146 iter/s, 7.86499s/100 iters), loss = 0.973149
I1124 06:18:44.385432 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1124 06:18:44.385432 35012 solver.cpp:237]     Train net output #1: loss = 0.973149 (* 1 = 0.973149 loss)
I1124 06:18:44.385432 35012 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1124 06:18:52.291142 35012 solver.cpp:218] Iteration 900 (12.65 iter/s, 7.90514s/100 iters), loss = 1.15666
I1124 06:18:52.291142 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.62
I1124 06:18:52.291142 35012 solver.cpp:237]     Train net output #1: loss = 1.15666 (* 1 = 1.15666 loss)
I1124 06:18:52.291142 35012 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1124 06:18:59.790416 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:19:00.102552 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_1000.caffemodel
I1124 06:19:00.143592 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_1000.solverstate
I1124 06:19:00.163597 35012 solver.cpp:330] Iteration 1000, Testing net (#0)
I1124 06:19:00.163597 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:19:02.244410 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:19:02.328450 35012 solver.cpp:397]     Test net output #0: accuracy = 0.5493
I1124 06:19:02.328450 35012 solver.cpp:397]     Test net output #1: loss = 1.27361 (* 1 = 1.27361 loss)
I1124 06:19:02.405448 35012 solver.cpp:218] Iteration 1000 (9.8875 iter/s, 10.1138s/100 iters), loss = 0.819445
I1124 06:19:02.405448 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1124 06:19:02.405448 35012 solver.cpp:237]     Train net output #1: loss = 0.819445 (* 1 = 0.819445 loss)
I1124 06:19:02.405448 35012 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1124 06:19:10.278100 35012 solver.cpp:218] Iteration 1100 (12.703 iter/s, 7.87216s/100 iters), loss = 0.837336
I1124 06:19:10.278100 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1124 06:19:10.278100 35012 solver.cpp:237]     Train net output #1: loss = 0.837336 (* 1 = 0.837336 loss)
I1124 06:19:10.278100 35012 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1124 06:19:18.139880 35012 solver.cpp:218] Iteration 1200 (12.7195 iter/s, 7.86197s/100 iters), loss = 0.864595
I1124 06:19:18.139880 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1124 06:19:18.139880 35012 solver.cpp:237]     Train net output #1: loss = 0.864595 (* 1 = 0.864595 loss)
I1124 06:19:18.139880 35012 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1124 06:19:26.005791 35012 solver.cpp:218] Iteration 1300 (12.7145 iter/s, 7.86506s/100 iters), loss = 0.771347
I1124 06:19:26.005791 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1124 06:19:26.005791 35012 solver.cpp:237]     Train net output #1: loss = 0.771347 (* 1 = 0.771347 loss)
I1124 06:19:26.005791 35012 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1124 06:19:33.849139 35012 solver.cpp:218] Iteration 1400 (12.7509 iter/s, 7.84257s/100 iters), loss = 0.868738
I1124 06:19:33.849139 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1124 06:19:33.849139 35012 solver.cpp:237]     Train net output #1: loss = 0.868738 (* 1 = 0.868738 loss)
I1124 06:19:33.849139 35012 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1124 06:19:41.307036 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:19:41.615798 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_1500.caffemodel
I1124 06:19:41.657810 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_1500.solverstate
I1124 06:19:41.677315 35012 solver.cpp:330] Iteration 1500, Testing net (#0)
I1124 06:19:41.677814 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:19:43.752599 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:19:43.836130 35012 solver.cpp:397]     Test net output #0: accuracy = 0.6019
I1124 06:19:43.836130 35012 solver.cpp:397]     Test net output #1: loss = 1.0954 (* 1 = 1.0954 loss)
I1124 06:19:43.913158 35012 solver.cpp:218] Iteration 1500 (9.93684 iter/s, 10.0636s/100 iters), loss = 0.633055
I1124 06:19:43.913158 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1124 06:19:43.913158 35012 solver.cpp:237]     Train net output #1: loss = 0.633055 (* 1 = 0.633055 loss)
I1124 06:19:43.913158 35012 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1124 06:19:51.769935 35012 solver.cpp:218] Iteration 1600 (12.728 iter/s, 7.85668s/100 iters), loss = 0.606326
I1124 06:19:51.769935 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1124 06:19:51.769935 35012 solver.cpp:237]     Train net output #1: loss = 0.606326 (* 1 = 0.606326 loss)
I1124 06:19:51.769935 35012 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1124 06:19:59.628093 35012 solver.cpp:218] Iteration 1700 (12.7269 iter/s, 7.85735s/100 iters), loss = 0.621083
I1124 06:19:59.628093 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1124 06:19:59.628093 35012 solver.cpp:237]     Train net output #1: loss = 0.621083 (* 1 = 0.621083 loss)
I1124 06:19:59.628093 35012 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1124 06:20:07.518265 35012 solver.cpp:218] Iteration 1800 (12.6751 iter/s, 7.88945s/100 iters), loss = 0.583977
I1124 06:20:07.518265 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1124 06:20:07.518265 35012 solver.cpp:237]     Train net output #1: loss = 0.583977 (* 1 = 0.583977 loss)
I1124 06:20:07.518265 35012 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1124 06:20:15.378875 35012 solver.cpp:218] Iteration 1900 (12.7224 iter/s, 7.86016s/100 iters), loss = 0.60542
I1124 06:20:15.378875 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1124 06:20:15.378875 35012 solver.cpp:237]     Train net output #1: loss = 0.60542 (* 1 = 0.60542 loss)
I1124 06:20:15.378875 35012 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1124 06:20:22.849902 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:20:23.161913 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_2000.caffemodel
I1124 06:20:23.202932 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_2000.solverstate
I1124 06:20:23.222437 35012 solver.cpp:330] Iteration 2000, Testing net (#0)
I1124 06:20:23.222936 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:20:25.301654 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:20:25.386165 35012 solver.cpp:397]     Test net output #0: accuracy = 0.741
I1124 06:20:25.386165 35012 solver.cpp:397]     Test net output #1: loss = 0.780398 (* 1 = 0.780398 loss)
I1124 06:20:25.463176 35012 solver.cpp:218] Iteration 2000 (9.91677 iter/s, 10.0839s/100 iters), loss = 0.448912
I1124 06:20:25.463176 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1124 06:20:25.463176 35012 solver.cpp:237]     Train net output #1: loss = 0.448912 (* 1 = 0.448912 loss)
I1124 06:20:25.463176 35012 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1124 06:20:33.327637 35012 solver.cpp:218] Iteration 2100 (12.7164 iter/s, 7.86385s/100 iters), loss = 0.519808
I1124 06:20:33.327637 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1124 06:20:33.327637 35012 solver.cpp:237]     Train net output #1: loss = 0.519808 (* 1 = 0.519808 loss)
I1124 06:20:33.327637 35012 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1124 06:20:41.196146 35012 solver.cpp:218] Iteration 2200 (12.7085 iter/s, 7.86875s/100 iters), loss = 0.57013
I1124 06:20:41.196146 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1124 06:20:41.196146 35012 solver.cpp:237]     Train net output #1: loss = 0.57013 (* 1 = 0.57013 loss)
I1124 06:20:41.196146 35012 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1124 06:20:49.057832 35012 solver.cpp:218] Iteration 2300 (12.7218 iter/s, 7.86053s/100 iters), loss = 0.62865
I1124 06:20:49.057832 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1124 06:20:49.057832 35012 solver.cpp:237]     Train net output #1: loss = 0.62865 (* 1 = 0.62865 loss)
I1124 06:20:49.057832 35012 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1124 06:20:56.922200 35012 solver.cpp:218] Iteration 2400 (12.7164 iter/s, 7.86385s/100 iters), loss = 0.513029
I1124 06:20:56.922200 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1124 06:20:56.922200 35012 solver.cpp:237]     Train net output #1: loss = 0.513029 (* 1 = 0.513029 loss)
I1124 06:20:56.922200 35012 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1124 06:21:04.396277 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:21:04.707367 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_2500.caffemodel
I1124 06:21:04.746356 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_2500.solverstate
I1124 06:21:04.765352 35012 solver.cpp:330] Iteration 2500, Testing net (#0)
I1124 06:21:04.765352 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:21:06.845242 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:21:06.928273 35012 solver.cpp:397]     Test net output #0: accuracy = 0.7423
I1124 06:21:06.928273 35012 solver.cpp:397]     Test net output #1: loss = 0.758942 (* 1 = 0.758942 loss)
I1124 06:21:07.006279 35012 solver.cpp:218] Iteration 2500 (9.91698 iter/s, 10.0837s/100 iters), loss = 0.426158
I1124 06:21:07.006279 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1124 06:21:07.006279 35012 solver.cpp:237]     Train net output #1: loss = 0.426158 (* 1 = 0.426158 loss)
I1124 06:21:07.006279 35012 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1124 06:21:14.863051 35012 solver.cpp:218] Iteration 2600 (12.7289 iter/s, 7.85615s/100 iters), loss = 0.451582
I1124 06:21:14.863051 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1124 06:21:14.863051 35012 solver.cpp:237]     Train net output #1: loss = 0.451582 (* 1 = 0.451582 loss)
I1124 06:21:14.863051 35012 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1124 06:21:22.723632 35012 solver.cpp:218] Iteration 2700 (12.722 iter/s, 7.86041s/100 iters), loss = 0.473983
I1124 06:21:22.723632 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1124 06:21:22.723632 35012 solver.cpp:237]     Train net output #1: loss = 0.473983 (* 1 = 0.473983 loss)
I1124 06:21:22.723632 35012 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1124 06:21:30.579442 35012 solver.cpp:218] Iteration 2800 (12.7311 iter/s, 7.85478s/100 iters), loss = 0.492773
I1124 06:21:30.579442 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1124 06:21:30.579442 35012 solver.cpp:237]     Train net output #1: loss = 0.492773 (* 1 = 0.492773 loss)
I1124 06:21:30.579442 35012 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1124 06:21:38.439985 35012 solver.cpp:218] Iteration 2900 (12.7218 iter/s, 7.86051s/100 iters), loss = 0.456773
I1124 06:21:38.439985 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1124 06:21:38.439985 35012 solver.cpp:237]     Train net output #1: loss = 0.456773 (* 1 = 0.456773 loss)
I1124 06:21:38.439985 35012 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1124 06:21:45.909862 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:21:46.219988 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_3000.caffemodel
I1124 06:21:46.262528 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_3000.solverstate
I1124 06:21:46.282505 35012 solver.cpp:330] Iteration 3000, Testing net (#0)
I1124 06:21:46.282505 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:21:48.362051 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:21:48.445087 35012 solver.cpp:397]     Test net output #0: accuracy = 0.7263
I1124 06:21:48.446094 35012 solver.cpp:397]     Test net output #1: loss = 0.849171 (* 1 = 0.849171 loss)
I1124 06:21:48.522054 35012 solver.cpp:218] Iteration 3000 (9.919 iter/s, 10.0817s/100 iters), loss = 0.461393
I1124 06:21:48.522054 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1124 06:21:48.522054 35012 solver.cpp:237]     Train net output #1: loss = 0.461393 (* 1 = 0.461393 loss)
I1124 06:21:48.522054 35012 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1124 06:21:56.382526 35012 solver.cpp:218] Iteration 3100 (12.7229 iter/s, 7.85985s/100 iters), loss = 0.430113
I1124 06:21:56.382526 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1124 06:21:56.382526 35012 solver.cpp:237]     Train net output #1: loss = 0.430113 (* 1 = 0.430113 loss)
I1124 06:21:56.382526 35012 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1124 06:22:04.247159 35012 solver.cpp:218] Iteration 3200 (12.716 iter/s, 7.8641s/100 iters), loss = 0.492728
I1124 06:22:04.247159 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1124 06:22:04.247159 35012 solver.cpp:237]     Train net output #1: loss = 0.492728 (* 1 = 0.492728 loss)
I1124 06:22:04.247159 35012 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1124 06:22:12.105837 35012 solver.cpp:218] Iteration 3300 (12.7263 iter/s, 7.85777s/100 iters), loss = 0.541706
I1124 06:22:12.105837 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1124 06:22:12.105837 35012 solver.cpp:237]     Train net output #1: loss = 0.541706 (* 1 = 0.541706 loss)
I1124 06:22:12.105837 35012 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1124 06:22:19.961789 35012 solver.cpp:218] Iteration 3400 (12.7296 iter/s, 7.85571s/100 iters), loss = 0.472737
I1124 06:22:19.961789 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1124 06:22:19.961789 35012 solver.cpp:237]     Train net output #1: loss = 0.472737 (* 1 = 0.472737 loss)
I1124 06:22:19.961789 35012 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1124 06:22:27.434693 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:22:27.745816 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_3500.caffemodel
I1124 06:22:27.785802 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_3500.solverstate
I1124 06:22:27.806288 35012 solver.cpp:330] Iteration 3500, Testing net (#0)
I1124 06:22:27.806288 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:22:29.885620 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:22:29.968662 35012 solver.cpp:397]     Test net output #0: accuracy = 0.7902
I1124 06:22:29.968662 35012 solver.cpp:397]     Test net output #1: loss = 0.625808 (* 1 = 0.625808 loss)
I1124 06:22:30.045696 35012 solver.cpp:218] Iteration 3500 (9.91701 iter/s, 10.0837s/100 iters), loss = 0.362155
I1124 06:22:30.045696 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1124 06:22:30.045696 35012 solver.cpp:237]     Train net output #1: loss = 0.362155 (* 1 = 0.362155 loss)
I1124 06:22:30.045696 35012 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1124 06:22:37.904345 35012 solver.cpp:218] Iteration 3600 (12.7263 iter/s, 7.85775s/100 iters), loss = 0.429541
I1124 06:22:37.904345 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1124 06:22:37.904345 35012 solver.cpp:237]     Train net output #1: loss = 0.429541 (* 1 = 0.429541 loss)
I1124 06:22:37.904345 35012 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1124 06:22:45.763608 35012 solver.cpp:218] Iteration 3700 (12.7241 iter/s, 7.8591s/100 iters), loss = 0.475823
I1124 06:22:45.763608 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1124 06:22:45.763608 35012 solver.cpp:237]     Train net output #1: loss = 0.475823 (* 1 = 0.475823 loss)
I1124 06:22:45.763608 35012 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1124 06:22:53.624888 35012 solver.cpp:218] Iteration 3800 (12.721 iter/s, 7.86105s/100 iters), loss = 0.483354
I1124 06:22:53.624888 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1124 06:22:53.624888 35012 solver.cpp:237]     Train net output #1: loss = 0.483354 (* 1 = 0.483354 loss)
I1124 06:22:53.624888 35012 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1124 06:23:01.485074 35012 solver.cpp:218] Iteration 3900 (12.7239 iter/s, 7.8592s/100 iters), loss = 0.569578
I1124 06:23:01.485074 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1124 06:23:01.485074 35012 solver.cpp:237]     Train net output #1: loss = 0.569578 (* 1 = 0.569578 loss)
I1124 06:23:01.485074 35012 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1124 06:23:08.952304 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:23:09.263046 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_4000.caffemodel
I1124 06:23:09.305037 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_4000.solverstate
I1124 06:23:09.326055 35012 solver.cpp:330] Iteration 4000, Testing net (#0)
I1124 06:23:09.326055 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:23:11.405304 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:23:11.489341 35012 solver.cpp:397]     Test net output #0: accuracy = 0.7681
I1124 06:23:11.489341 35012 solver.cpp:397]     Test net output #1: loss = 0.666897 (* 1 = 0.666897 loss)
I1124 06:23:11.565946 35012 solver.cpp:218] Iteration 4000 (9.91982 iter/s, 10.0808s/100 iters), loss = 0.365757
I1124 06:23:11.565946 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1124 06:23:11.565946 35012 solver.cpp:237]     Train net output #1: loss = 0.365757 (* 1 = 0.365757 loss)
I1124 06:23:11.565946 35012 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1124 06:23:19.427106 35012 solver.cpp:218] Iteration 4100 (12.7225 iter/s, 7.8601s/100 iters), loss = 0.461914
I1124 06:23:19.427106 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1124 06:23:19.427106 35012 solver.cpp:237]     Train net output #1: loss = 0.461914 (* 1 = 0.461914 loss)
I1124 06:23:19.427106 35012 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1124 06:23:27.293102 35012 solver.cpp:218] Iteration 4200 (12.7137 iter/s, 7.86555s/100 iters), loss = 0.536927
I1124 06:23:27.293102 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1124 06:23:27.293102 35012 solver.cpp:237]     Train net output #1: loss = 0.536927 (* 1 = 0.536927 loss)
I1124 06:23:27.293102 35012 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1124 06:23:35.152854 35012 solver.cpp:218] Iteration 4300 (12.7234 iter/s, 7.85956s/100 iters), loss = 0.381893
I1124 06:23:35.152854 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1124 06:23:35.152854 35012 solver.cpp:237]     Train net output #1: loss = 0.381893 (* 1 = 0.381893 loss)
I1124 06:23:35.152854 35012 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1124 06:23:43.015825 35012 solver.cpp:218] Iteration 4400 (12.719 iter/s, 7.86223s/100 iters), loss = 0.359149
I1124 06:23:43.015825 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1124 06:23:43.015825 35012 solver.cpp:237]     Train net output #1: loss = 0.359149 (* 1 = 0.359149 loss)
I1124 06:23:43.015825 35012 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1124 06:23:50.491777 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:23:50.802884 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_4500.caffemodel
I1124 06:23:50.841886 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_4500.solverstate
I1124 06:23:50.861883 35012 solver.cpp:330] Iteration 4500, Testing net (#0)
I1124 06:23:50.861883 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:23:52.941645 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:23:53.025671 35012 solver.cpp:397]     Test net output #0: accuracy = 0.7581
I1124 06:23:53.025671 35012 solver.cpp:397]     Test net output #1: loss = 0.729256 (* 1 = 0.729256 loss)
I1124 06:23:53.102722 35012 solver.cpp:218] Iteration 4500 (9.91417 iter/s, 10.0866s/100 iters), loss = 0.392725
I1124 06:23:53.102722 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1124 06:23:53.102722 35012 solver.cpp:237]     Train net output #1: loss = 0.392725 (* 1 = 0.392725 loss)
I1124 06:23:53.102722 35012 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1124 06:24:00.961016 35012 solver.cpp:218] Iteration 4600 (12.7269 iter/s, 7.85739s/100 iters), loss = 0.372009
I1124 06:24:00.961016 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1124 06:24:00.961016 35012 solver.cpp:237]     Train net output #1: loss = 0.372009 (* 1 = 0.372009 loss)
I1124 06:24:00.961016 35012 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1124 06:24:08.822474 35012 solver.cpp:218] Iteration 4700 (12.7207 iter/s, 7.8612s/100 iters), loss = 0.56564
I1124 06:24:08.822474 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1124 06:24:08.822474 35012 solver.cpp:237]     Train net output #1: loss = 0.56564 (* 1 = 0.56564 loss)
I1124 06:24:08.822474 35012 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1124 06:24:16.678225 35012 solver.cpp:218] Iteration 4800 (12.7306 iter/s, 7.8551s/100 iters), loss = 0.400997
I1124 06:24:16.678225 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1124 06:24:16.678225 35012 solver.cpp:237]     Train net output #1: loss = 0.400997 (* 1 = 0.400997 loss)
I1124 06:24:16.678225 35012 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1124 06:24:24.539541 35012 solver.cpp:218] Iteration 4900 (12.7211 iter/s, 7.86094s/100 iters), loss = 0.331846
I1124 06:24:24.539541 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1124 06:24:24.539541 35012 solver.cpp:237]     Train net output #1: loss = 0.331846 (* 1 = 0.331846 loss)
I1124 06:24:24.539541 35012 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1124 06:24:32.006855 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:24:32.317381 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_5000.caffemodel
I1124 06:24:32.363374 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_5000.solverstate
I1124 06:24:32.383875 35012 solver.cpp:330] Iteration 5000, Testing net (#0)
I1124 06:24:32.383875 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:24:34.461942 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:24:34.545969 35012 solver.cpp:397]     Test net output #0: accuracy = 0.7163
I1124 06:24:34.545969 35012 solver.cpp:397]     Test net output #1: loss = 0.792391 (* 1 = 0.792391 loss)
I1124 06:24:34.621985 35012 solver.cpp:218] Iteration 5000 (9.9183 iter/s, 10.0824s/100 iters), loss = 0.414613
I1124 06:24:34.621985 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1124 06:24:34.621985 35012 solver.cpp:237]     Train net output #1: loss = 0.414613 (* 1 = 0.414613 loss)
I1124 06:24:34.621985 35012 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1124 06:24:34.621985 35012 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1124 06:24:42.483304 35012 solver.cpp:218] Iteration 5100 (12.7225 iter/s, 7.86012s/100 iters), loss = 0.273997
I1124 06:24:42.483304 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1124 06:24:42.483304 35012 solver.cpp:237]     Train net output #1: loss = 0.273997 (* 1 = 0.273997 loss)
I1124 06:24:42.483304 35012 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1124 06:24:50.344569 35012 solver.cpp:218] Iteration 5200 (12.7205 iter/s, 7.86131s/100 iters), loss = 0.245709
I1124 06:24:50.344569 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1124 06:24:50.344569 35012 solver.cpp:237]     Train net output #1: loss = 0.245709 (* 1 = 0.245709 loss)
I1124 06:24:50.344569 35012 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1124 06:24:58.202191 35012 solver.cpp:218] Iteration 5300 (12.7275 iter/s, 7.85702s/100 iters), loss = 0.248011
I1124 06:24:58.202191 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1124 06:24:58.202191 35012 solver.cpp:237]     Train net output #1: loss = 0.248011 (* 1 = 0.248011 loss)
I1124 06:24:58.202191 35012 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1124 06:25:06.067176 35012 solver.cpp:218] Iteration 5400 (12.7151 iter/s, 7.86465s/100 iters), loss = 0.19759
I1124 06:25:06.067176 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 06:25:06.067176 35012 solver.cpp:237]     Train net output #1: loss = 0.19759 (* 1 = 0.19759 loss)
I1124 06:25:06.067176 35012 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1124 06:25:13.546610 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:25:13.856734 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_5500.caffemodel
I1124 06:25:13.898262 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_5500.solverstate
I1124 06:25:13.918262 35012 solver.cpp:330] Iteration 5500, Testing net (#0)
I1124 06:25:13.918262 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:25:15.994599 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:25:16.079619 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9018
I1124 06:25:16.079619 35012 solver.cpp:397]     Test net output #1: loss = 0.291851 (* 1 = 0.291851 loss)
I1124 06:25:16.155634 35012 solver.cpp:218] Iteration 5500 (9.91279 iter/s, 10.088s/100 iters), loss = 0.193306
I1124 06:25:16.155634 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1124 06:25:16.155634 35012 solver.cpp:237]     Train net output #1: loss = 0.193306 (* 1 = 0.193306 loss)
I1124 06:25:16.155634 35012 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1124 06:25:24.012964 35012 solver.cpp:218] Iteration 5600 (12.7288 iter/s, 7.85619s/100 iters), loss = 0.203728
I1124 06:25:24.012964 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 06:25:24.012964 35012 solver.cpp:237]     Train net output #1: loss = 0.203728 (* 1 = 0.203728 loss)
I1124 06:25:24.012964 35012 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1124 06:25:31.871495 35012 solver.cpp:218] Iteration 5700 (12.7253 iter/s, 7.85835s/100 iters), loss = 0.211447
I1124 06:25:31.871495 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 06:25:31.871495 35012 solver.cpp:237]     Train net output #1: loss = 0.211447 (* 1 = 0.211447 loss)
I1124 06:25:31.871495 35012 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1124 06:25:39.729840 35012 solver.cpp:218] Iteration 5800 (12.7266 iter/s, 7.85754s/100 iters), loss = 0.255119
I1124 06:25:39.729840 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1124 06:25:39.729840 35012 solver.cpp:237]     Train net output #1: loss = 0.255119 (* 1 = 0.255119 loss)
I1124 06:25:39.729840 35012 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1124 06:25:47.588593 35012 solver.cpp:218] Iteration 5900 (12.7252 iter/s, 7.85841s/100 iters), loss = 0.158487
I1124 06:25:47.588593 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 06:25:47.588593 35012 solver.cpp:237]     Train net output #1: loss = 0.158487 (* 1 = 0.158487 loss)
I1124 06:25:47.588593 35012 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1124 06:25:55.062996 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:25:55.373610 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_6000.caffemodel
I1124 06:25:55.413146 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_6000.solverstate
I1124 06:25:55.433130 35012 solver.cpp:330] Iteration 6000, Testing net (#0)
I1124 06:25:55.433130 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:25:57.511203 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:25:57.595172 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9057
I1124 06:25:57.595172 35012 solver.cpp:397]     Test net output #1: loss = 0.28082 (* 1 = 0.28082 loss)
I1124 06:25:57.672168 35012 solver.cpp:218] Iteration 6000 (9.91777 iter/s, 10.0829s/100 iters), loss = 0.203798
I1124 06:25:57.672168 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 06:25:57.672168 35012 solver.cpp:237]     Train net output #1: loss = 0.203798 (* 1 = 0.203798 loss)
I1124 06:25:57.672168 35012 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1124 06:26:05.539510 35012 solver.cpp:218] Iteration 6100 (12.7117 iter/s, 7.86675s/100 iters), loss = 0.171084
I1124 06:26:05.539510 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 06:26:05.539510 35012 solver.cpp:237]     Train net output #1: loss = 0.171084 (* 1 = 0.171084 loss)
I1124 06:26:05.539510 35012 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1124 06:26:13.404822 35012 solver.cpp:218] Iteration 6200 (12.7145 iter/s, 7.86501s/100 iters), loss = 0.19625
I1124 06:26:13.404822 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:26:13.404822 35012 solver.cpp:237]     Train net output #1: loss = 0.19625 (* 1 = 0.19625 loss)
I1124 06:26:13.404822 35012 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1124 06:26:21.268925 35012 solver.cpp:218] Iteration 6300 (12.7172 iter/s, 7.86339s/100 iters), loss = 0.15918
I1124 06:26:21.268925 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:26:21.268925 35012 solver.cpp:237]     Train net output #1: loss = 0.15918 (* 1 = 0.15918 loss)
I1124 06:26:21.268925 35012 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1124 06:26:29.131161 35012 solver.cpp:218] Iteration 6400 (12.7185 iter/s, 7.86258s/100 iters), loss = 0.124138
I1124 06:26:29.131161 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:26:29.131161 35012 solver.cpp:237]     Train net output #1: loss = 0.124138 (* 1 = 0.124138 loss)
I1124 06:26:29.131161 35012 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1124 06:26:36.605610 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:26:36.915735 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_6500.caffemodel
I1124 06:26:36.958241 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_6500.solverstate
I1124 06:26:36.979760 35012 solver.cpp:330] Iteration 6500, Testing net (#0)
I1124 06:26:36.980762 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:26:39.059617 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:26:39.143632 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9093
I1124 06:26:39.143632 35012 solver.cpp:397]     Test net output #1: loss = 0.268603 (* 1 = 0.268603 loss)
I1124 06:26:39.220669 35012 solver.cpp:218] Iteration 6500 (9.91271 iter/s, 10.0881s/100 iters), loss = 0.152571
I1124 06:26:39.220669 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 06:26:39.220669 35012 solver.cpp:237]     Train net output #1: loss = 0.152571 (* 1 = 0.152571 loss)
I1124 06:26:39.220669 35012 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1124 06:26:47.079008 35012 solver.cpp:218] Iteration 6600 (12.725 iter/s, 7.85857s/100 iters), loss = 0.148538
I1124 06:26:47.079008 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:26:47.079008 35012 solver.cpp:237]     Train net output #1: loss = 0.148538 (* 1 = 0.148538 loss)
I1124 06:26:47.079008 35012 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1124 06:26:54.945473 35012 solver.cpp:218] Iteration 6700 (12.7135 iter/s, 7.86567s/100 iters), loss = 0.156298
I1124 06:26:54.945473 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 06:26:54.945974 35012 solver.cpp:237]     Train net output #1: loss = 0.156298 (* 1 = 0.156298 loss)
I1124 06:26:54.945974 35012 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1124 06:27:02.806445 35012 solver.cpp:218] Iteration 6800 (12.7215 iter/s, 7.86071s/100 iters), loss = 0.145776
I1124 06:27:02.806445 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 06:27:02.806445 35012 solver.cpp:237]     Train net output #1: loss = 0.145776 (* 1 = 0.145776 loss)
I1124 06:27:02.806445 35012 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1124 06:27:10.666146 35012 solver.cpp:218] Iteration 6900 (12.7245 iter/s, 7.85886s/100 iters), loss = 0.104066
I1124 06:27:10.666146 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:27:10.666146 35012 solver.cpp:237]     Train net output #1: loss = 0.104066 (* 1 = 0.104066 loss)
I1124 06:27:10.666146 35012 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1124 06:27:18.137832 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:27:18.450619 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_7000.caffemodel
I1124 06:27:18.490636 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_7000.solverstate
I1124 06:27:18.511119 35012 solver.cpp:330] Iteration 7000, Testing net (#0)
I1124 06:27:18.511119 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:27:20.594123 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:27:20.678138 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9069
I1124 06:27:20.678138 35012 solver.cpp:397]     Test net output #1: loss = 0.274266 (* 1 = 0.274266 loss)
I1124 06:27:20.755118 35012 solver.cpp:218] Iteration 7000 (9.91268 iter/s, 10.0881s/100 iters), loss = 0.151816
I1124 06:27:20.755118 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:27:20.755118 35012 solver.cpp:237]     Train net output #1: loss = 0.151816 (* 1 = 0.151816 loss)
I1124 06:27:20.755118 35012 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1124 06:27:28.612944 35012 solver.cpp:218] Iteration 7100 (12.7257 iter/s, 7.8581s/100 iters), loss = 0.113526
I1124 06:27:28.612944 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:27:28.612944 35012 solver.cpp:237]     Train net output #1: loss = 0.113526 (* 1 = 0.113526 loss)
I1124 06:27:28.612944 35012 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1124 06:27:36.470176 35012 solver.cpp:218] Iteration 7200 (12.7288 iter/s, 7.85622s/100 iters), loss = 0.189977
I1124 06:27:36.470176 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 06:27:36.470176 35012 solver.cpp:237]     Train net output #1: loss = 0.189977 (* 1 = 0.189977 loss)
I1124 06:27:36.470176 35012 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1124 06:27:44.330140 35012 solver.cpp:218] Iteration 7300 (12.7238 iter/s, 7.8593s/100 iters), loss = 0.127997
I1124 06:27:44.330140 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:27:44.330140 35012 solver.cpp:237]     Train net output #1: loss = 0.127997 (* 1 = 0.127997 loss)
I1124 06:27:44.330140 35012 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1124 06:27:52.188211 35012 solver.cpp:218] Iteration 7400 (12.7259 iter/s, 7.85798s/100 iters), loss = 0.122233
I1124 06:27:52.188211 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:27:52.188211 35012 solver.cpp:237]     Train net output #1: loss = 0.122233 (* 1 = 0.122233 loss)
I1124 06:27:52.188211 35012 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1124 06:27:59.661417 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:27:59.972453 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_7500.caffemodel
I1124 06:28:00.013468 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_7500.solverstate
I1124 06:28:00.034466 35012 solver.cpp:330] Iteration 7500, Testing net (#0)
I1124 06:28:00.034466 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:28:02.115520 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:28:02.199527 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9074
I1124 06:28:02.199527 35012 solver.cpp:397]     Test net output #1: loss = 0.27513 (* 1 = 0.27513 loss)
I1124 06:28:02.276041 35012 solver.cpp:218] Iteration 7500 (9.91357 iter/s, 10.0872s/100 iters), loss = 0.134183
I1124 06:28:02.276041 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 06:28:02.276041 35012 solver.cpp:237]     Train net output #1: loss = 0.134183 (* 1 = 0.134183 loss)
I1124 06:28:02.276041 35012 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1124 06:28:10.138381 35012 solver.cpp:218] Iteration 7600 (12.7193 iter/s, 7.86208s/100 iters), loss = 0.127951
I1124 06:28:10.138381 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 06:28:10.138381 35012 solver.cpp:237]     Train net output #1: loss = 0.127951 (* 1 = 0.127951 loss)
I1124 06:28:10.138381 35012 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1124 06:28:17.998237 35012 solver.cpp:218] Iteration 7700 (12.7242 iter/s, 7.85906s/100 iters), loss = 0.12534
I1124 06:28:17.998237 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:28:17.998237 35012 solver.cpp:237]     Train net output #1: loss = 0.12534 (* 1 = 0.12534 loss)
I1124 06:28:17.998237 35012 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1124 06:28:25.859848 35012 solver.cpp:218] Iteration 7800 (12.72 iter/s, 7.86165s/100 iters), loss = 0.167541
I1124 06:28:25.859848 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 06:28:25.859848 35012 solver.cpp:237]     Train net output #1: loss = 0.167541 (* 1 = 0.167541 loss)
I1124 06:28:25.859848 35012 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1124 06:28:33.720604 35012 solver.cpp:218] Iteration 7900 (12.7231 iter/s, 7.85972s/100 iters), loss = 0.0819155
I1124 06:28:33.720604 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:28:33.720604 35012 solver.cpp:237]     Train net output #1: loss = 0.0819155 (* 1 = 0.0819155 loss)
I1124 06:28:33.720604 35012 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1124 06:28:41.192718 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:28:41.502895 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_8000.caffemodel
I1124 06:28:41.542934 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_8000.solverstate
I1124 06:28:41.562934 35012 solver.cpp:330] Iteration 8000, Testing net (#0)
I1124 06:28:41.562934 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:28:43.641808 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:28:43.725844 35012 solver.cpp:397]     Test net output #0: accuracy = 0.905
I1124 06:28:43.725844 35012 solver.cpp:397]     Test net output #1: loss = 0.275462 (* 1 = 0.275462 loss)
I1124 06:28:43.802359 35012 solver.cpp:218] Iteration 8000 (9.91939 iter/s, 10.0813s/100 iters), loss = 0.127935
I1124 06:28:43.802359 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 06:28:43.802359 35012 solver.cpp:237]     Train net output #1: loss = 0.127935 (* 1 = 0.127935 loss)
I1124 06:28:43.802359 35012 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1124 06:28:51.660526 35012 solver.cpp:218] Iteration 8100 (12.7263 iter/s, 7.85772s/100 iters), loss = 0.113167
I1124 06:28:51.660526 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:28:51.660526 35012 solver.cpp:237]     Train net output #1: loss = 0.113166 (* 1 = 0.113166 loss)
I1124 06:28:51.660526 35012 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1124 06:28:59.524345 35012 solver.cpp:218] Iteration 8200 (12.7162 iter/s, 7.864s/100 iters), loss = 0.141957
I1124 06:28:59.525347 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 06:28:59.525347 35012 solver.cpp:237]     Train net output #1: loss = 0.141957 (* 1 = 0.141957 loss)
I1124 06:28:59.525347 35012 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1124 06:29:07.387974 35012 solver.cpp:218] Iteration 8300 (12.7178 iter/s, 7.86299s/100 iters), loss = 0.146886
I1124 06:29:07.387974 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 06:29:07.387974 35012 solver.cpp:237]     Train net output #1: loss = 0.146886 (* 1 = 0.146886 loss)
I1124 06:29:07.387974 35012 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1124 06:29:15.245759 35012 solver.cpp:218] Iteration 8400 (12.7275 iter/s, 7.85698s/100 iters), loss = 0.0929773
I1124 06:29:15.245759 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:29:15.245759 35012 solver.cpp:237]     Train net output #1: loss = 0.0929772 (* 1 = 0.0929772 loss)
I1124 06:29:15.245759 35012 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1124 06:29:22.721001 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:29:23.030036 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_8500.caffemodel
I1124 06:29:23.070061 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_8500.solverstate
I1124 06:29:23.090191 35012 solver.cpp:330] Iteration 8500, Testing net (#0)
I1124 06:29:23.090191 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:29:25.169785 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:29:25.253821 35012 solver.cpp:397]     Test net output #0: accuracy = 0.91
I1124 06:29:25.253821 35012 solver.cpp:397]     Test net output #1: loss = 0.263497 (* 1 = 0.263497 loss)
I1124 06:29:25.329447 35012 solver.cpp:218] Iteration 8500 (9.91715 iter/s, 10.0835s/100 iters), loss = 0.0966235
I1124 06:29:25.330451 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:29:25.330451 35012 solver.cpp:237]     Train net output #1: loss = 0.0966235 (* 1 = 0.0966235 loss)
I1124 06:29:25.330451 35012 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1124 06:29:33.192075 35012 solver.cpp:218] Iteration 8600 (12.7204 iter/s, 7.86141s/100 iters), loss = 0.122061
I1124 06:29:33.192075 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:29:33.192075 35012 solver.cpp:237]     Train net output #1: loss = 0.122061 (* 1 = 0.122061 loss)
I1124 06:29:33.192075 35012 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1124 06:29:41.052582 35012 solver.cpp:218] Iteration 8700 (12.7223 iter/s, 7.86024s/100 iters), loss = 0.129929
I1124 06:29:41.052582 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 06:29:41.052582 35012 solver.cpp:237]     Train net output #1: loss = 0.129929 (* 1 = 0.129929 loss)
I1124 06:29:41.052582 35012 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1124 06:29:48.914937 35012 solver.cpp:218] Iteration 8800 (12.7193 iter/s, 7.86206s/100 iters), loss = 0.0921289
I1124 06:29:48.914937 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:29:48.914937 35012 solver.cpp:237]     Train net output #1: loss = 0.0921288 (* 1 = 0.0921288 loss)
I1124 06:29:48.914937 35012 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1124 06:29:56.770815 35012 solver.cpp:218] Iteration 8900 (12.7305 iter/s, 7.85517s/100 iters), loss = 0.0911133
I1124 06:29:56.771317 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:29:56.771317 35012 solver.cpp:237]     Train net output #1: loss = 0.0911133 (* 1 = 0.0911133 loss)
I1124 06:29:56.771317 35012 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1124 06:30:04.266858 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:30:04.577992 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_9000.caffemodel
I1124 06:30:04.620016 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_9000.solverstate
I1124 06:30:04.640015 35012 solver.cpp:330] Iteration 9000, Testing net (#0)
I1124 06:30:04.640514 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:30:06.720130 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:30:06.803151 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9067
I1124 06:30:06.803151 35012 solver.cpp:397]     Test net output #1: loss = 0.270475 (* 1 = 0.270475 loss)
I1124 06:30:06.879158 35012 solver.cpp:218] Iteration 9000 (9.89285 iter/s, 10.1083s/100 iters), loss = 0.12499
I1124 06:30:06.879158 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 06:30:06.879158 35012 solver.cpp:237]     Train net output #1: loss = 0.12499 (* 1 = 0.12499 loss)
I1124 06:30:06.879158 35012 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1124 06:30:14.739044 35012 solver.cpp:218] Iteration 9100 (12.7249 iter/s, 7.85863s/100 iters), loss = 0.112002
I1124 06:30:14.739044 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 06:30:14.739044 35012 solver.cpp:237]     Train net output #1: loss = 0.112001 (* 1 = 0.112001 loss)
I1124 06:30:14.739044 35012 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1124 06:30:22.602447 35012 solver.cpp:218] Iteration 9200 (12.7181 iter/s, 7.86279s/100 iters), loss = 0.0941205
I1124 06:30:22.602447 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:30:22.602447 35012 solver.cpp:237]     Train net output #1: loss = 0.0941205 (* 1 = 0.0941205 loss)
I1124 06:30:22.602447 35012 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1124 06:30:30.462947 35012 solver.cpp:218] Iteration 9300 (12.7224 iter/s, 7.86016s/100 iters), loss = 0.0861168
I1124 06:30:30.462947 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:30:30.462947 35012 solver.cpp:237]     Train net output #1: loss = 0.0861168 (* 1 = 0.0861168 loss)
I1124 06:30:30.462947 35012 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1124 06:30:38.324801 35012 solver.cpp:218] Iteration 9400 (12.7193 iter/s, 7.86207s/100 iters), loss = 0.072584
I1124 06:30:38.324801 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:30:38.324801 35012 solver.cpp:237]     Train net output #1: loss = 0.0725839 (* 1 = 0.0725839 loss)
I1124 06:30:38.324801 35012 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1124 06:30:45.796355 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:30:46.106559 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_9500.caffemodel
I1124 06:30:46.145577 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_9500.solverstate
I1124 06:30:46.165577 35012 solver.cpp:330] Iteration 9500, Testing net (#0)
I1124 06:30:46.165577 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:30:48.244544 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:30:48.328562 35012 solver.cpp:397]     Test net output #0: accuracy = 0.904
I1124 06:30:48.328562 35012 solver.cpp:397]     Test net output #1: loss = 0.280034 (* 1 = 0.280034 loss)
I1124 06:30:48.405581 35012 solver.cpp:218] Iteration 9500 (9.9209 iter/s, 10.0797s/100 iters), loss = 0.148524
I1124 06:30:48.405581 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 06:30:48.405581 35012 solver.cpp:237]     Train net output #1: loss = 0.148524 (* 1 = 0.148524 loss)
I1124 06:30:48.405581 35012 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1124 06:30:48.405581 35012 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1124 06:30:56.269119 35012 solver.cpp:218] Iteration 9600 (12.7175 iter/s, 7.86319s/100 iters), loss = 0.0982659
I1124 06:30:56.269619 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:30:56.269619 35012 solver.cpp:237]     Train net output #1: loss = 0.0982658 (* 1 = 0.0982658 loss)
I1124 06:30:56.269619 35012 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1124 06:31:04.130658 35012 solver.cpp:218] Iteration 9700 (12.7205 iter/s, 7.86131s/100 iters), loss = 0.0782599
I1124 06:31:04.130658 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:31:04.130658 35012 solver.cpp:237]     Train net output #1: loss = 0.0782599 (* 1 = 0.0782599 loss)
I1124 06:31:04.130658 35012 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1124 06:31:11.988937 35012 solver.cpp:218] Iteration 9800 (12.7262 iter/s, 7.8578s/100 iters), loss = 0.081635
I1124 06:31:11.988937 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:31:11.988937 35012 solver.cpp:237]     Train net output #1: loss = 0.081635 (* 1 = 0.081635 loss)
I1124 06:31:11.988937 35012 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1124 06:31:19.851269 35012 solver.cpp:218] Iteration 9900 (12.7208 iter/s, 7.86117s/100 iters), loss = 0.0940544
I1124 06:31:19.851269 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:31:19.851269 35012 solver.cpp:237]     Train net output #1: loss = 0.0940543 (* 1 = 0.0940543 loss)
I1124 06:31:19.851269 35012 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1124 06:31:27.322584 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:31:27.633685 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_10000.caffemodel
I1124 06:31:27.677228 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_10000.solverstate
I1124 06:31:27.697746 35012 solver.cpp:330] Iteration 10000, Testing net (#0)
I1124 06:31:27.697746 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:31:29.778851 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:31:29.862884 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9194
I1124 06:31:29.862884 35012 solver.cpp:397]     Test net output #1: loss = 0.239065 (* 1 = 0.239065 loss)
I1124 06:31:29.939918 35012 solver.cpp:218] Iteration 10000 (9.91266 iter/s, 10.0881s/100 iters), loss = 0.10246
I1124 06:31:29.939918 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:31:29.939918 35012 solver.cpp:237]     Train net output #1: loss = 0.10246 (* 1 = 0.10246 loss)
I1124 06:31:29.939918 35012 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1124 06:31:37.802620 35012 solver.cpp:218] Iteration 10100 (12.7186 iter/s, 7.86252s/100 iters), loss = 0.0694618
I1124 06:31:37.802620 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:31:37.802620 35012 solver.cpp:237]     Train net output #1: loss = 0.0694618 (* 1 = 0.0694618 loss)
I1124 06:31:37.802620 35012 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1124 06:31:45.666363 35012 solver.cpp:218] Iteration 10200 (12.7181 iter/s, 7.86283s/100 iters), loss = 0.120908
I1124 06:31:45.666363 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:31:45.666363 35012 solver.cpp:237]     Train net output #1: loss = 0.120908 (* 1 = 0.120908 loss)
I1124 06:31:45.666363 35012 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1124 06:31:53.531255 35012 solver.cpp:218] Iteration 10300 (12.715 iter/s, 7.86474s/100 iters), loss = 0.0650327
I1124 06:31:53.531255 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:31:53.531255 35012 solver.cpp:237]     Train net output #1: loss = 0.0650326 (* 1 = 0.0650326 loss)
I1124 06:31:53.531255 35012 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1124 06:32:01.391070 35012 solver.cpp:218] Iteration 10400 (12.7239 iter/s, 7.85922s/100 iters), loss = 0.0608684
I1124 06:32:01.391070 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:32:01.391070 35012 solver.cpp:237]     Train net output #1: loss = 0.0608684 (* 1 = 0.0608684 loss)
I1124 06:32:01.391070 35012 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1124 06:32:08.870431 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:32:09.182502 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_10500.caffemodel
I1124 06:32:09.221493 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_10500.solverstate
I1124 06:32:09.242027 35012 solver.cpp:330] Iteration 10500, Testing net (#0)
I1124 06:32:09.242027 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:32:11.319602 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:32:11.403640 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9198
I1124 06:32:11.403640 35012 solver.cpp:397]     Test net output #1: loss = 0.237598 (* 1 = 0.237598 loss)
I1124 06:32:11.480679 35012 solver.cpp:218] Iteration 10500 (9.91183 iter/s, 10.089s/100 iters), loss = 0.0922582
I1124 06:32:11.480679 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:32:11.480679 35012 solver.cpp:237]     Train net output #1: loss = 0.0922582 (* 1 = 0.0922582 loss)
I1124 06:32:11.480679 35012 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1124 06:32:19.350231 35012 solver.cpp:218] Iteration 10600 (12.708 iter/s, 7.86903s/100 iters), loss = 0.108253
I1124 06:32:19.350231 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:32:19.350231 35012 solver.cpp:237]     Train net output #1: loss = 0.108253 (* 1 = 0.108253 loss)
I1124 06:32:19.350231 35012 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1124 06:32:27.214905 35012 solver.cpp:218] Iteration 10700 (12.7154 iter/s, 7.86446s/100 iters), loss = 0.0690084
I1124 06:32:27.214905 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:32:27.214905 35012 solver.cpp:237]     Train net output #1: loss = 0.0690083 (* 1 = 0.0690083 loss)
I1124 06:32:27.214905 35012 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1124 06:32:35.078310 35012 solver.cpp:218] Iteration 10800 (12.717 iter/s, 7.86346s/100 iters), loss = 0.0948194
I1124 06:32:35.079315 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:32:35.079315 35012 solver.cpp:237]     Train net output #1: loss = 0.0948194 (* 1 = 0.0948194 loss)
I1124 06:32:35.079315 35012 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1124 06:32:42.944876 35012 solver.cpp:218] Iteration 10900 (12.7139 iter/s, 7.8654s/100 iters), loss = 0.0598117
I1124 06:32:42.944876 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:32:42.944876 35012 solver.cpp:237]     Train net output #1: loss = 0.0598117 (* 1 = 0.0598117 loss)
I1124 06:32:42.944876 35012 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1124 06:32:50.418627 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:32:50.729251 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_11000.caffemodel
I1124 06:32:50.770763 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_11000.solverstate
I1124 06:32:50.790760 35012 solver.cpp:330] Iteration 11000, Testing net (#0)
I1124 06:32:50.790760 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:32:52.867509 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:32:52.951555 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9202
I1124 06:32:52.951555 35012 solver.cpp:397]     Test net output #1: loss = 0.238955 (* 1 = 0.238955 loss)
I1124 06:32:53.028563 35012 solver.cpp:218] Iteration 11000 (9.91764 iter/s, 10.083s/100 iters), loss = 0.0886504
I1124 06:32:53.028563 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:32:53.028563 35012 solver.cpp:237]     Train net output #1: loss = 0.0886503 (* 1 = 0.0886503 loss)
I1124 06:32:53.028563 35012 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1124 06:33:00.895858 35012 solver.cpp:218] Iteration 11100 (12.7111 iter/s, 7.86715s/100 iters), loss = 0.0929353
I1124 06:33:00.895858 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 06:33:00.895858 35012 solver.cpp:237]     Train net output #1: loss = 0.0929353 (* 1 = 0.0929353 loss)
I1124 06:33:00.895858 35012 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1124 06:33:08.757009 35012 solver.cpp:218] Iteration 11200 (12.7212 iter/s, 7.8609s/100 iters), loss = 0.0820562
I1124 06:33:08.757009 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:33:08.757009 35012 solver.cpp:237]     Train net output #1: loss = 0.0820561 (* 1 = 0.0820561 loss)
I1124 06:33:08.757009 35012 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1124 06:33:16.617874 35012 solver.cpp:218] Iteration 11300 (12.7236 iter/s, 7.85944s/100 iters), loss = 0.0663442
I1124 06:33:16.617874 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:33:16.617874 35012 solver.cpp:237]     Train net output #1: loss = 0.0663441 (* 1 = 0.0663441 loss)
I1124 06:33:16.617874 35012 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1124 06:33:24.482892 35012 solver.cpp:218] Iteration 11400 (12.7147 iter/s, 7.86494s/100 iters), loss = 0.0664514
I1124 06:33:24.482892 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:33:24.482892 35012 solver.cpp:237]     Train net output #1: loss = 0.0664514 (* 1 = 0.0664514 loss)
I1124 06:33:24.482892 35012 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1124 06:33:31.957063 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:33:32.268079 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_11500.caffemodel
I1124 06:33:32.307083 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_11500.solverstate
I1124 06:33:32.327584 35012 solver.cpp:330] Iteration 11500, Testing net (#0)
I1124 06:33:32.327584 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:33:34.406611 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:33:34.490118 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9202
I1124 06:33:34.490118 35012 solver.cpp:397]     Test net output #1: loss = 0.239314 (* 1 = 0.239314 loss)
I1124 06:33:34.566628 35012 solver.cpp:218] Iteration 11500 (9.9176 iter/s, 10.0831s/100 iters), loss = 0.121482
I1124 06:33:34.566628 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 06:33:34.566628 35012 solver.cpp:237]     Train net output #1: loss = 0.121481 (* 1 = 0.121481 loss)
I1124 06:33:34.566628 35012 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1124 06:33:42.426806 35012 solver.cpp:218] Iteration 11600 (12.7228 iter/s, 7.85988s/100 iters), loss = 0.100755
I1124 06:33:42.426806 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:33:42.426806 35012 solver.cpp:237]     Train net output #1: loss = 0.100755 (* 1 = 0.100755 loss)
I1124 06:33:42.426806 35012 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1124 06:33:50.291271 35012 solver.cpp:218] Iteration 11700 (12.7167 iter/s, 7.86366s/100 iters), loss = 0.0949858
I1124 06:33:50.291271 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:33:50.291271 35012 solver.cpp:237]     Train net output #1: loss = 0.0949857 (* 1 = 0.0949857 loss)
I1124 06:33:50.291271 35012 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1124 06:33:58.156435 35012 solver.cpp:218] Iteration 11800 (12.7149 iter/s, 7.86479s/100 iters), loss = 0.0875483
I1124 06:33:58.156435 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:33:58.156435 35012 solver.cpp:237]     Train net output #1: loss = 0.0875483 (* 1 = 0.0875483 loss)
I1124 06:33:58.156435 35012 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1124 06:34:06.026446 35012 solver.cpp:218] Iteration 11900 (12.7073 iter/s, 7.86951s/100 iters), loss = 0.0579908
I1124 06:34:06.026446 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:34:06.026446 35012 solver.cpp:237]     Train net output #1: loss = 0.0579907 (* 1 = 0.0579907 loss)
I1124 06:34:06.026446 35012 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1124 06:34:13.500965 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:34:13.811074 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_12000.caffemodel
I1124 06:34:13.850116 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_12000.solverstate
I1124 06:34:13.868114 35012 solver.cpp:330] Iteration 12000, Testing net (#0)
I1124 06:34:13.868114 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:34:15.947018 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:34:16.031575 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9205
I1124 06:34:16.031575 35012 solver.cpp:397]     Test net output #1: loss = 0.23863 (* 1 = 0.23863 loss)
I1124 06:34:16.108078 35012 solver.cpp:218] Iteration 12000 (9.91948 iter/s, 10.0812s/100 iters), loss = 0.0865798
I1124 06:34:16.108078 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:34:16.108078 35012 solver.cpp:237]     Train net output #1: loss = 0.0865797 (* 1 = 0.0865797 loss)
I1124 06:34:16.108078 35012 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1124 06:34:23.971377 35012 solver.cpp:218] Iteration 12100 (12.7184 iter/s, 7.86263s/100 iters), loss = 0.0771229
I1124 06:34:23.971377 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:34:23.971377 35012 solver.cpp:237]     Train net output #1: loss = 0.0771228 (* 1 = 0.0771228 loss)
I1124 06:34:23.971377 35012 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1124 06:34:31.836251 35012 solver.cpp:218] Iteration 12200 (12.7158 iter/s, 7.86423s/100 iters), loss = 0.0987701
I1124 06:34:31.836251 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:34:31.836251 35012 solver.cpp:237]     Train net output #1: loss = 0.09877 (* 1 = 0.09877 loss)
I1124 06:34:31.836251 35012 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1124 06:34:39.700577 35012 solver.cpp:218] Iteration 12300 (12.7166 iter/s, 7.86372s/100 iters), loss = 0.0472022
I1124 06:34:39.700577 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:34:39.700577 35012 solver.cpp:237]     Train net output #1: loss = 0.0472021 (* 1 = 0.0472021 loss)
I1124 06:34:39.700577 35012 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1124 06:34:47.566347 35012 solver.cpp:218] Iteration 12400 (12.713 iter/s, 7.86594s/100 iters), loss = 0.0499225
I1124 06:34:47.566347 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:34:47.566347 35012 solver.cpp:237]     Train net output #1: loss = 0.0499225 (* 1 = 0.0499225 loss)
I1124 06:34:47.566347 35012 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1124 06:34:55.042011 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:34:55.354050 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_12500.caffemodel
I1124 06:34:55.395051 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_12500.solverstate
I1124 06:34:55.414054 35012 solver.cpp:330] Iteration 12500, Testing net (#0)
I1124 06:34:55.414054 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:34:57.493952 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:34:57.576983 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9207
I1124 06:34:57.576983 35012 solver.cpp:397]     Test net output #1: loss = 0.237719 (* 1 = 0.237719 loss)
I1124 06:34:57.654500 35012 solver.cpp:218] Iteration 12500 (9.91376 iter/s, 10.087s/100 iters), loss = 0.073112
I1124 06:34:57.654500 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:34:57.654500 35012 solver.cpp:237]     Train net output #1: loss = 0.0731119 (* 1 = 0.0731119 loss)
I1124 06:34:57.654500 35012 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1124 06:35:05.514001 35012 solver.cpp:218] Iteration 12600 (12.7241 iter/s, 7.85912s/100 iters), loss = 0.0759114
I1124 06:35:05.514001 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:35:05.514001 35012 solver.cpp:237]     Train net output #1: loss = 0.0759113 (* 1 = 0.0759113 loss)
I1124 06:35:05.514001 35012 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1124 06:35:13.379566 35012 solver.cpp:218] Iteration 12700 (12.7146 iter/s, 7.865s/100 iters), loss = 0.0761818
I1124 06:35:13.379566 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:35:13.379566 35012 solver.cpp:237]     Train net output #1: loss = 0.0761817 (* 1 = 0.0761817 loss)
I1124 06:35:13.379566 35012 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1124 06:35:21.241269 35012 solver.cpp:218] Iteration 12800 (12.7205 iter/s, 7.86135s/100 iters), loss = 0.0669083
I1124 06:35:21.241269 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:35:21.241269 35012 solver.cpp:237]     Train net output #1: loss = 0.0669082 (* 1 = 0.0669082 loss)
I1124 06:35:21.241269 35012 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1124 06:35:29.102274 35012 solver.cpp:218] Iteration 12900 (12.722 iter/s, 7.8604s/100 iters), loss = 0.0493993
I1124 06:35:29.102274 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:35:29.102274 35012 solver.cpp:237]     Train net output #1: loss = 0.0493992 (* 1 = 0.0493992 loss)
I1124 06:35:29.102274 35012 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1124 06:35:36.576117 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:35:36.886725 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_13000.caffemodel
I1124 06:35:36.925524 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_13000.solverstate
I1124 06:35:36.945505 35012 solver.cpp:330] Iteration 13000, Testing net (#0)
I1124 06:35:36.945505 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:35:39.023898 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:35:39.107939 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9199
I1124 06:35:39.107939 35012 solver.cpp:397]     Test net output #1: loss = 0.239053 (* 1 = 0.239053 loss)
I1124 06:35:39.183934 35012 solver.cpp:218] Iteration 13000 (9.91894 iter/s, 10.0817s/100 iters), loss = 0.0799483
I1124 06:35:39.183934 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:35:39.183934 35012 solver.cpp:237]     Train net output #1: loss = 0.0799482 (* 1 = 0.0799482 loss)
I1124 06:35:39.183934 35012 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1124 06:35:47.047199 35012 solver.cpp:218] Iteration 13100 (12.7186 iter/s, 7.86252s/100 iters), loss = 0.0984373
I1124 06:35:47.047199 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:35:47.047199 35012 solver.cpp:237]     Train net output #1: loss = 0.0984372 (* 1 = 0.0984372 loss)
I1124 06:35:47.047199 35012 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1124 06:35:54.909260 35012 solver.cpp:218] Iteration 13200 (12.7197 iter/s, 7.8618s/100 iters), loss = 0.110609
I1124 06:35:54.909260 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:35:54.909260 35012 solver.cpp:237]     Train net output #1: loss = 0.110608 (* 1 = 0.110608 loss)
I1124 06:35:54.909260 35012 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1124 06:36:02.770125 35012 solver.cpp:218] Iteration 13300 (12.7231 iter/s, 7.8597s/100 iters), loss = 0.0466363
I1124 06:36:02.770125 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:36:02.770125 35012 solver.cpp:237]     Train net output #1: loss = 0.0466362 (* 1 = 0.0466362 loss)
I1124 06:36:02.770125 35012 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1124 06:36:10.630107 35012 solver.cpp:218] Iteration 13400 (12.7225 iter/s, 7.86006s/100 iters), loss = 0.0420679
I1124 06:36:10.630107 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:36:10.630107 35012 solver.cpp:237]     Train net output #1: loss = 0.0420678 (* 1 = 0.0420678 loss)
I1124 06:36:10.630107 35012 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1124 06:36:18.105278 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:36:18.416378 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_13500.caffemodel
I1124 06:36:18.458379 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_13500.solverstate
I1124 06:36:18.476897 35012 solver.cpp:330] Iteration 13500, Testing net (#0)
I1124 06:36:18.477398 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:36:20.557658 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:36:20.640189 35012 solver.cpp:397]     Test net output #0: accuracy = 0.92
I1124 06:36:20.640189 35012 solver.cpp:397]     Test net output #1: loss = 0.240497 (* 1 = 0.240497 loss)
I1124 06:36:20.717224 35012 solver.cpp:218] Iteration 13500 (9.91442 iter/s, 10.0863s/100 iters), loss = 0.0701619
I1124 06:36:20.717224 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:36:20.717224 35012 solver.cpp:237]     Train net output #1: loss = 0.0701618 (* 1 = 0.0701618 loss)
I1124 06:36:20.717224 35012 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1124 06:36:28.575094 35012 solver.cpp:218] Iteration 13600 (12.7271 iter/s, 7.85725s/100 iters), loss = 0.0783551
I1124 06:36:28.575094 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:36:28.575094 35012 solver.cpp:237]     Train net output #1: loss = 0.078355 (* 1 = 0.078355 loss)
I1124 06:36:28.575094 35012 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1124 06:36:36.436776 35012 solver.cpp:218] Iteration 13700 (12.7207 iter/s, 7.86119s/100 iters), loss = 0.0705215
I1124 06:36:36.436776 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:36:36.436776 35012 solver.cpp:237]     Train net output #1: loss = 0.0705214 (* 1 = 0.0705214 loss)
I1124 06:36:36.436776 35012 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1124 06:36:44.297796 35012 solver.cpp:218] Iteration 13800 (12.721 iter/s, 7.86101s/100 iters), loss = 0.0500895
I1124 06:36:44.297796 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:36:44.297796 35012 solver.cpp:237]     Train net output #1: loss = 0.0500894 (* 1 = 0.0500894 loss)
I1124 06:36:44.297796 35012 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1124 06:36:52.158682 35012 solver.cpp:218] Iteration 13900 (12.7228 iter/s, 7.85989s/100 iters), loss = 0.0538113
I1124 06:36:52.158682 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:36:52.158682 35012 solver.cpp:237]     Train net output #1: loss = 0.0538112 (* 1 = 0.0538112 loss)
I1124 06:36:52.158682 35012 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1124 06:36:59.631209 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:36:59.940241 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_14000.caffemodel
I1124 06:36:59.980762 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_14000.solverstate
I1124 06:37:00.002262 35012 solver.cpp:330] Iteration 14000, Testing net (#0)
I1124 06:37:00.002262 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:37:02.081588 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:37:02.165103 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9188
I1124 06:37:02.165103 35012 solver.cpp:397]     Test net output #1: loss = 0.24052 (* 1 = 0.24052 loss)
I1124 06:37:02.242147 35012 solver.cpp:218] Iteration 14000 (9.91776 iter/s, 10.0829s/100 iters), loss = 0.0748963
I1124 06:37:02.242147 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:37:02.242147 35012 solver.cpp:237]     Train net output #1: loss = 0.0748962 (* 1 = 0.0748962 loss)
I1124 06:37:02.242147 35012 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1124 06:37:10.101371 35012 solver.cpp:218] Iteration 14100 (12.7247 iter/s, 7.85873s/100 iters), loss = 0.0750071
I1124 06:37:10.101371 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:37:10.101371 35012 solver.cpp:237]     Train net output #1: loss = 0.075007 (* 1 = 0.075007 loss)
I1124 06:37:10.101371 35012 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1124 06:37:17.965034 35012 solver.cpp:218] Iteration 14200 (12.7173 iter/s, 7.86332s/100 iters), loss = 0.0646728
I1124 06:37:17.965034 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:37:17.965034 35012 solver.cpp:237]     Train net output #1: loss = 0.0646727 (* 1 = 0.0646727 loss)
I1124 06:37:17.965034 35012 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1124 06:37:25.828192 35012 solver.cpp:218] Iteration 14300 (12.7189 iter/s, 7.86233s/100 iters), loss = 0.0461018
I1124 06:37:25.828192 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:37:25.828192 35012 solver.cpp:237]     Train net output #1: loss = 0.0461016 (* 1 = 0.0461016 loss)
I1124 06:37:25.828192 35012 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1124 06:37:33.688761 35012 solver.cpp:218] Iteration 14400 (12.7209 iter/s, 7.86107s/100 iters), loss = 0.033723
I1124 06:37:33.689774 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:37:33.689774 35012 solver.cpp:237]     Train net output #1: loss = 0.0337228 (* 1 = 0.0337228 loss)
I1124 06:37:33.689774 35012 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1124 06:37:41.160161 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:37:41.470273 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_14500.caffemodel
I1124 06:37:41.510816 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_14500.solverstate
I1124 06:37:41.530800 35012 solver.cpp:330] Iteration 14500, Testing net (#0)
I1124 06:37:41.530800 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:37:43.609176 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:37:43.693222 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9198
I1124 06:37:43.693222 35012 solver.cpp:397]     Test net output #1: loss = 0.239385 (* 1 = 0.239385 loss)
I1124 06:37:43.769784 35012 solver.cpp:218] Iteration 14500 (9.92067 iter/s, 10.08s/100 iters), loss = 0.0585806
I1124 06:37:43.769784 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:37:43.769784 35012 solver.cpp:237]     Train net output #1: loss = 0.0585805 (* 1 = 0.0585805 loss)
I1124 06:37:43.769784 35012 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1124 06:37:51.634572 35012 solver.cpp:218] Iteration 14600 (12.7164 iter/s, 7.86383s/100 iters), loss = 0.0873583
I1124 06:37:51.634572 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:37:51.634572 35012 solver.cpp:237]     Train net output #1: loss = 0.0873581 (* 1 = 0.0873581 loss)
I1124 06:37:51.634572 35012 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1124 06:37:59.492244 35012 solver.cpp:218] Iteration 14700 (12.7268 iter/s, 7.85741s/100 iters), loss = 0.101894
I1124 06:37:59.492244 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:37:59.492244 35012 solver.cpp:237]     Train net output #1: loss = 0.101894 (* 1 = 0.101894 loss)
I1124 06:37:59.492244 35012 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1124 06:38:07.352366 35012 solver.cpp:218] Iteration 14800 (12.7232 iter/s, 7.85967s/100 iters), loss = 0.0762364
I1124 06:38:07.352366 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:38:07.352366 35012 solver.cpp:237]     Train net output #1: loss = 0.0762363 (* 1 = 0.0762363 loss)
I1124 06:38:07.352366 35012 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1124 06:38:15.212604 35012 solver.cpp:218] Iteration 14900 (12.7229 iter/s, 7.85985s/100 iters), loss = 0.0834849
I1124 06:38:15.212604 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:38:15.212604 35012 solver.cpp:237]     Train net output #1: loss = 0.0834848 (* 1 = 0.0834848 loss)
I1124 06:38:15.212604 35012 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1124 06:38:22.686378 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:38:22.995585 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_15000.caffemodel
I1124 06:38:23.037598 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_15000.solverstate
I1124 06:38:23.056586 35012 solver.cpp:330] Iteration 15000, Testing net (#0)
I1124 06:38:23.056586 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:38:25.136813 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:38:25.218829 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9206
I1124 06:38:25.218829 35012 solver.cpp:397]     Test net output #1: loss = 0.239465 (* 1 = 0.239465 loss)
I1124 06:38:25.295866 35012 solver.cpp:218] Iteration 15000 (9.91813 iter/s, 10.0825s/100 iters), loss = 0.0730778
I1124 06:38:25.295866 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:38:25.295866 35012 solver.cpp:237]     Train net output #1: loss = 0.0730776 (* 1 = 0.0730776 loss)
I1124 06:38:25.295866 35012 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1124 06:38:33.156186 35012 solver.cpp:218] Iteration 15100 (12.7228 iter/s, 7.85991s/100 iters), loss = 0.050935
I1124 06:38:33.156186 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:38:33.156186 35012 solver.cpp:237]     Train net output #1: loss = 0.0509349 (* 1 = 0.0509349 loss)
I1124 06:38:33.156186 35012 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1124 06:38:41.014475 35012 solver.cpp:218] Iteration 15200 (12.7263 iter/s, 7.85774s/100 iters), loss = 0.0673704
I1124 06:38:41.014475 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:38:41.014475 35012 solver.cpp:237]     Train net output #1: loss = 0.0673703 (* 1 = 0.0673703 loss)
I1124 06:38:41.014475 35012 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1124 06:38:48.871539 35012 solver.cpp:218] Iteration 15300 (12.7276 iter/s, 7.85693s/100 iters), loss = 0.0374802
I1124 06:38:48.871539 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:38:48.871539 35012 solver.cpp:237]     Train net output #1: loss = 0.03748 (* 1 = 0.03748 loss)
I1124 06:38:48.871539 35012 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1124 06:38:48.871539 35012 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1124 06:38:56.730576 35012 solver.cpp:218] Iteration 15400 (12.7247 iter/s, 7.85876s/100 iters), loss = 0.0395237
I1124 06:38:56.730576 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:38:56.730576 35012 solver.cpp:237]     Train net output #1: loss = 0.0395236 (* 1 = 0.0395236 loss)
I1124 06:38:56.730576 35012 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1124 06:39:04.204551 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:39:04.516633 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_15500.caffemodel
I1124 06:39:04.556675 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_15500.solverstate
I1124 06:39:04.576655 35012 solver.cpp:330] Iteration 15500, Testing net (#0)
I1124 06:39:04.576655 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:39:06.655501 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:39:06.739548 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9201
I1124 06:39:06.739548 35012 solver.cpp:397]     Test net output #1: loss = 0.23962 (* 1 = 0.23962 loss)
I1124 06:39:06.816542 35012 solver.cpp:218] Iteration 15500 (9.9156 iter/s, 10.0851s/100 iters), loss = 0.0675909
I1124 06:39:06.816542 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:39:06.816542 35012 solver.cpp:237]     Train net output #1: loss = 0.0675908 (* 1 = 0.0675908 loss)
I1124 06:39:06.816542 35012 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1124 06:39:14.681849 35012 solver.cpp:218] Iteration 15600 (12.7152 iter/s, 7.86462s/100 iters), loss = 0.0723545
I1124 06:39:14.681849 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:39:14.681849 35012 solver.cpp:237]     Train net output #1: loss = 0.0723544 (* 1 = 0.0723544 loss)
I1124 06:39:14.681849 35012 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1124 06:39:22.547210 35012 solver.cpp:218] Iteration 15700 (12.7147 iter/s, 7.86489s/100 iters), loss = 0.045005
I1124 06:39:22.547210 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:39:22.547210 35012 solver.cpp:237]     Train net output #1: loss = 0.0450049 (* 1 = 0.0450049 loss)
I1124 06:39:22.547210 35012 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1124 06:39:30.407243 35012 solver.cpp:218] Iteration 15800 (12.7237 iter/s, 7.85936s/100 iters), loss = 0.0629153
I1124 06:39:30.407243 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:39:30.407243 35012 solver.cpp:237]     Train net output #1: loss = 0.0629151 (* 1 = 0.0629151 loss)
I1124 06:39:30.407243 35012 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1124 06:39:38.270957 35012 solver.cpp:218] Iteration 15900 (12.7174 iter/s, 7.86323s/100 iters), loss = 0.0575228
I1124 06:39:38.270957 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:39:38.270957 35012 solver.cpp:237]     Train net output #1: loss = 0.0575227 (* 1 = 0.0575227 loss)
I1124 06:39:38.270957 35012 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1124 06:39:45.742475 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:39:46.053602 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_16000.caffemodel
I1124 06:39:46.095623 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_16000.solverstate
I1124 06:39:46.116623 35012 solver.cpp:330] Iteration 16000, Testing net (#0)
I1124 06:39:46.116623 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:39:48.196408 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:39:48.280452 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9204
I1124 06:39:48.280452 35012 solver.cpp:397]     Test net output #1: loss = 0.239177 (* 1 = 0.239177 loss)
I1124 06:39:48.357458 35012 solver.cpp:218] Iteration 16000 (9.91457 iter/s, 10.0862s/100 iters), loss = 0.0565909
I1124 06:39:48.357458 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:39:48.357458 35012 solver.cpp:237]     Train net output #1: loss = 0.0565907 (* 1 = 0.0565907 loss)
I1124 06:39:48.357458 35012 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1124 06:39:56.223168 35012 solver.cpp:218] Iteration 16100 (12.7146 iter/s, 7.865s/100 iters), loss = 0.0835307
I1124 06:39:56.223168 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:39:56.223168 35012 solver.cpp:237]     Train net output #1: loss = 0.0835306 (* 1 = 0.0835306 loss)
I1124 06:39:56.223168 35012 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1124 06:40:04.112296 35012 solver.cpp:218] Iteration 16200 (12.6762 iter/s, 7.88878s/100 iters), loss = 0.0619188
I1124 06:40:04.112296 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:40:04.112296 35012 solver.cpp:237]     Train net output #1: loss = 0.0619187 (* 1 = 0.0619187 loss)
I1124 06:40:04.112296 35012 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1124 06:40:11.975302 35012 solver.cpp:218] Iteration 16300 (12.7177 iter/s, 7.86303s/100 iters), loss = 0.0671367
I1124 06:40:11.975302 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:40:11.975302 35012 solver.cpp:237]     Train net output #1: loss = 0.0671366 (* 1 = 0.0671366 loss)
I1124 06:40:11.975302 35012 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1124 06:40:19.839355 35012 solver.cpp:218] Iteration 16400 (12.7179 iter/s, 7.86295s/100 iters), loss = 0.0244897
I1124 06:40:19.839355 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:40:19.839355 35012 solver.cpp:237]     Train net output #1: loss = 0.0244896 (* 1 = 0.0244896 loss)
I1124 06:40:19.839355 35012 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1124 06:40:27.316339 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:40:27.627478 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_16500.caffemodel
I1124 06:40:27.668480 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_16500.solverstate
I1124 06:40:27.689502 35012 solver.cpp:330] Iteration 16500, Testing net (#0)
I1124 06:40:27.689502 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:40:29.768330 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:40:29.851382 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9205
I1124 06:40:29.851382 35012 solver.cpp:397]     Test net output #1: loss = 0.239154 (* 1 = 0.239154 loss)
I1124 06:40:29.928385 35012 solver.cpp:218] Iteration 16500 (9.91217 iter/s, 10.0886s/100 iters), loss = 0.0568754
I1124 06:40:29.928385 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:40:29.928385 35012 solver.cpp:237]     Train net output #1: loss = 0.0568753 (* 1 = 0.0568753 loss)
I1124 06:40:29.928385 35012 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1124 06:40:37.787701 35012 solver.cpp:218] Iteration 16600 (12.7245 iter/s, 7.85886s/100 iters), loss = 0.0809014
I1124 06:40:37.787701 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:40:37.787701 35012 solver.cpp:237]     Train net output #1: loss = 0.0809013 (* 1 = 0.0809013 loss)
I1124 06:40:37.787701 35012 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1124 06:40:45.648906 35012 solver.cpp:218] Iteration 16700 (12.721 iter/s, 7.86099s/100 iters), loss = 0.0669352
I1124 06:40:45.648906 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:40:45.648906 35012 solver.cpp:237]     Train net output #1: loss = 0.0669351 (* 1 = 0.0669351 loss)
I1124 06:40:45.648906 35012 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1124 06:40:53.505439 35012 solver.cpp:218] Iteration 16800 (12.7283 iter/s, 7.85654s/100 iters), loss = 0.0562994
I1124 06:40:53.506438 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:40:53.506438 35012 solver.cpp:237]     Train net output #1: loss = 0.0562993 (* 1 = 0.0562993 loss)
I1124 06:40:53.506438 35012 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1124 06:41:01.366224 35012 solver.cpp:218] Iteration 16900 (12.7234 iter/s, 7.85953s/100 iters), loss = 0.0515376
I1124 06:41:01.366224 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:41:01.366224 35012 solver.cpp:237]     Train net output #1: loss = 0.0515375 (* 1 = 0.0515375 loss)
I1124 06:41:01.366224 35012 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1124 06:41:08.838608 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:41:09.149737 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_17000.caffemodel
I1124 06:41:09.193753 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_17000.solverstate
I1124 06:41:09.213752 35012 solver.cpp:330] Iteration 17000, Testing net (#0)
I1124 06:41:09.213752 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:41:11.292603 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:41:11.376597 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9206
I1124 06:41:11.376597 35012 solver.cpp:397]     Test net output #1: loss = 0.239246 (* 1 = 0.239246 loss)
I1124 06:41:11.453626 35012 solver.cpp:218] Iteration 17000 (9.91371 iter/s, 10.087s/100 iters), loss = 0.067661
I1124 06:41:11.454125 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:41:11.454125 35012 solver.cpp:237]     Train net output #1: loss = 0.0676608 (* 1 = 0.0676608 loss)
I1124 06:41:11.454125 35012 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1124 06:41:19.313591 35012 solver.cpp:218] Iteration 17100 (12.7239 iter/s, 7.85925s/100 iters), loss = 0.0674308
I1124 06:41:19.313591 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:41:19.313591 35012 solver.cpp:237]     Train net output #1: loss = 0.0674307 (* 1 = 0.0674307 loss)
I1124 06:41:19.313591 35012 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1124 06:41:27.175112 35012 solver.cpp:218] Iteration 17200 (12.7205 iter/s, 7.86132s/100 iters), loss = 0.0430976
I1124 06:41:27.175112 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:41:27.175112 35012 solver.cpp:237]     Train net output #1: loss = 0.0430975 (* 1 = 0.0430975 loss)
I1124 06:41:27.175112 35012 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1124 06:41:35.035284 35012 solver.cpp:218] Iteration 17300 (12.724 iter/s, 7.85917s/100 iters), loss = 0.0516779
I1124 06:41:35.035284 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:41:35.035284 35012 solver.cpp:237]     Train net output #1: loss = 0.0516778 (* 1 = 0.0516778 loss)
I1124 06:41:35.035284 35012 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1124 06:41:42.890442 35012 solver.cpp:218] Iteration 17400 (12.7301 iter/s, 7.85539s/100 iters), loss = 0.0634874
I1124 06:41:42.890442 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:41:42.890442 35012 solver.cpp:237]     Train net output #1: loss = 0.0634873 (* 1 = 0.0634873 loss)
I1124 06:41:42.890442 35012 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1124 06:41:50.361979 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:41:50.672076 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_17500.caffemodel
I1124 06:41:50.713078 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_17500.solverstate
I1124 06:41:50.733621 35012 solver.cpp:330] Iteration 17500, Testing net (#0)
I1124 06:41:50.733621 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:41:52.813848 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:41:52.897920 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9207
I1124 06:41:52.897920 35012 solver.cpp:397]     Test net output #1: loss = 0.239173 (* 1 = 0.239173 loss)
I1124 06:41:52.974944 35012 solver.cpp:218] Iteration 17500 (9.91746 iter/s, 10.0832s/100 iters), loss = 0.0589665
I1124 06:41:52.974944 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:41:52.974944 35012 solver.cpp:237]     Train net output #1: loss = 0.0589664 (* 1 = 0.0589664 loss)
I1124 06:41:52.974944 35012 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1124 06:42:00.839860 35012 solver.cpp:218] Iteration 17600 (12.715 iter/s, 7.86472s/100 iters), loss = 0.0717659
I1124 06:42:00.839860 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:42:00.839860 35012 solver.cpp:237]     Train net output #1: loss = 0.0717658 (* 1 = 0.0717658 loss)
I1124 06:42:00.839860 35012 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1124 06:42:08.704666 35012 solver.cpp:218] Iteration 17700 (12.7163 iter/s, 7.86392s/100 iters), loss = 0.069202
I1124 06:42:08.704666 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:42:08.704666 35012 solver.cpp:237]     Train net output #1: loss = 0.0692019 (* 1 = 0.0692019 loss)
I1124 06:42:08.704666 35012 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1124 06:42:16.612805 35012 solver.cpp:218] Iteration 17800 (12.6457 iter/s, 7.90782s/100 iters), loss = 0.0377396
I1124 06:42:16.612805 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:42:16.612805 35012 solver.cpp:237]     Train net output #1: loss = 0.0377395 (* 1 = 0.0377395 loss)
I1124 06:42:16.612805 35012 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1124 06:42:24.456998 35012 solver.cpp:218] Iteration 17900 (12.7492 iter/s, 7.84365s/100 iters), loss = 0.0334277
I1124 06:42:24.456998 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:42:24.456998 35012 solver.cpp:237]     Train net output #1: loss = 0.0334276 (* 1 = 0.0334276 loss)
I1124 06:42:24.456998 35012 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1124 06:42:31.907613 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:42:32.215827 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_18000.caffemodel
I1124 06:42:32.255342 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_18000.solverstate
I1124 06:42:32.275331 35012 solver.cpp:330] Iteration 18000, Testing net (#0)
I1124 06:42:32.275331 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:42:34.344779 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:42:34.427801 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9204
I1124 06:42:34.427801 35012 solver.cpp:397]     Test net output #1: loss = 0.239384 (* 1 = 0.239384 loss)
I1124 06:42:34.503849 35012 solver.cpp:218] Iteration 18000 (9.95316 iter/s, 10.0471s/100 iters), loss = 0.0779229
I1124 06:42:34.503849 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:42:34.503849 35012 solver.cpp:237]     Train net output #1: loss = 0.0779228 (* 1 = 0.0779228 loss)
I1124 06:42:34.503849 35012 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1124 06:42:42.336086 35012 solver.cpp:218] Iteration 18100 (12.7686 iter/s, 7.83171s/100 iters), loss = 0.0546263
I1124 06:42:42.336086 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:42:42.336086 35012 solver.cpp:237]     Train net output #1: loss = 0.0546262 (* 1 = 0.0546262 loss)
I1124 06:42:42.336086 35012 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1124 06:42:50.166424 35012 solver.cpp:218] Iteration 18200 (12.7716 iter/s, 7.82987s/100 iters), loss = 0.0451538
I1124 06:42:50.166424 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:42:50.166424 35012 solver.cpp:237]     Train net output #1: loss = 0.0451537 (* 1 = 0.0451537 loss)
I1124 06:42:50.166424 35012 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1124 06:42:57.994891 35012 solver.cpp:218] Iteration 18300 (12.7747 iter/s, 7.82797s/100 iters), loss = 0.0559775
I1124 06:42:57.994891 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:42:57.994891 35012 solver.cpp:237]     Train net output #1: loss = 0.0559774 (* 1 = 0.0559774 loss)
I1124 06:42:57.994891 35012 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1124 06:43:05.829671 35012 solver.cpp:218] Iteration 18400 (12.7656 iter/s, 7.83353s/100 iters), loss = 0.0825875
I1124 06:43:05.829671 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:43:05.829671 35012 solver.cpp:237]     Train net output #1: loss = 0.0825874 (* 1 = 0.0825874 loss)
I1124 06:43:05.829671 35012 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1124 06:43:13.275457 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:43:13.583657 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_18500.caffemodel
I1124 06:43:13.623661 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_18500.solverstate
I1124 06:43:13.643681 35012 solver.cpp:330] Iteration 18500, Testing net (#0)
I1124 06:43:13.644161 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:43:15.712456 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:43:15.795491 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9209
I1124 06:43:15.795491 35012 solver.cpp:397]     Test net output #1: loss = 0.239422 (* 1 = 0.239422 loss)
I1124 06:43:15.872542 35012 solver.cpp:218] Iteration 18500 (9.95774 iter/s, 10.0424s/100 iters), loss = 0.057007
I1124 06:43:15.872542 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:43:15.872542 35012 solver.cpp:237]     Train net output #1: loss = 0.0570069 (* 1 = 0.0570069 loss)
I1124 06:43:15.872542 35012 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1124 06:43:23.705056 35012 solver.cpp:218] Iteration 18600 (12.768 iter/s, 7.83205s/100 iters), loss = 0.052236
I1124 06:43:23.705056 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:43:23.705056 35012 solver.cpp:237]     Train net output #1: loss = 0.0522359 (* 1 = 0.0522359 loss)
I1124 06:43:23.705056 35012 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1124 06:43:31.535948 35012 solver.cpp:218] Iteration 18700 (12.771 iter/s, 7.83024s/100 iters), loss = 0.0670679
I1124 06:43:31.535948 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:43:31.535948 35012 solver.cpp:237]     Train net output #1: loss = 0.0670678 (* 1 = 0.0670678 loss)
I1124 06:43:31.535948 35012 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1124 06:43:39.367765 35012 solver.cpp:218] Iteration 18800 (12.7681 iter/s, 7.83201s/100 iters), loss = 0.0442919
I1124 06:43:39.367765 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:43:39.367765 35012 solver.cpp:237]     Train net output #1: loss = 0.0442918 (* 1 = 0.0442918 loss)
I1124 06:43:39.367765 35012 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1124 06:43:47.195917 35012 solver.cpp:218] Iteration 18900 (12.7764 iter/s, 7.82695s/100 iters), loss = 0.0527875
I1124 06:43:47.195917 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:43:47.195917 35012 solver.cpp:237]     Train net output #1: loss = 0.0527874 (* 1 = 0.0527874 loss)
I1124 06:43:47.195917 35012 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1124 06:43:54.638449 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:43:54.946533 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_19000.caffemodel
I1124 06:43:54.985575 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_19000.solverstate
I1124 06:43:55.004576 35012 solver.cpp:330] Iteration 19000, Testing net (#0)
I1124 06:43:55.004576 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:43:57.075554 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:43:57.159075 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9214
I1124 06:43:57.159075 35012 solver.cpp:397]     Test net output #1: loss = 0.239572 (* 1 = 0.239572 loss)
I1124 06:43:57.235595 35012 solver.cpp:218] Iteration 19000 (9.96089 iter/s, 10.0393s/100 iters), loss = 0.0684833
I1124 06:43:57.235595 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:43:57.235595 35012 solver.cpp:237]     Train net output #1: loss = 0.0684832 (* 1 = 0.0684832 loss)
I1124 06:43:57.235595 35012 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1124 06:44:05.070001 35012 solver.cpp:218] Iteration 19100 (12.7647 iter/s, 7.83411s/100 iters), loss = 0.0761408
I1124 06:44:05.070001 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:44:05.070001 35012 solver.cpp:237]     Train net output #1: loss = 0.0761407 (* 1 = 0.0761407 loss)
I1124 06:44:05.070001 35012 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1124 06:44:12.904893 35012 solver.cpp:218] Iteration 19200 (12.7647 iter/s, 7.83412s/100 iters), loss = 0.0535402
I1124 06:44:12.904893 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:44:12.904893 35012 solver.cpp:237]     Train net output #1: loss = 0.0535401 (* 1 = 0.0535401 loss)
I1124 06:44:12.904893 35012 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1124 06:44:20.738337 35012 solver.cpp:218] Iteration 19300 (12.7653 iter/s, 7.83372s/100 iters), loss = 0.050656
I1124 06:44:20.738337 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:44:20.738337 35012 solver.cpp:237]     Train net output #1: loss = 0.0506559 (* 1 = 0.0506559 loss)
I1124 06:44:20.738337 35012 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1124 06:44:28.573879 35012 solver.cpp:218] Iteration 19400 (12.7636 iter/s, 7.83477s/100 iters), loss = 0.0428841
I1124 06:44:28.573879 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:44:28.573879 35012 solver.cpp:237]     Train net output #1: loss = 0.042884 (* 1 = 0.042884 loss)
I1124 06:44:28.573879 35012 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1124 06:44:36.021942 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:44:36.332103 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_19500.caffemodel
I1124 06:44:36.374095 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_19500.solverstate
I1124 06:44:36.393604 35012 solver.cpp:330] Iteration 19500, Testing net (#0)
I1124 06:44:36.393604 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:44:38.463632 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:44:38.546799 35012 solver.cpp:397]     Test net output #0: accuracy = 0.921
I1124 06:44:38.546799 35012 solver.cpp:397]     Test net output #1: loss = 0.239511 (* 1 = 0.239511 loss)
I1124 06:44:38.623195 35012 solver.cpp:218] Iteration 19500 (9.95117 iter/s, 10.0491s/100 iters), loss = 0.0705301
I1124 06:44:38.623195 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:44:38.623195 35012 solver.cpp:237]     Train net output #1: loss = 0.07053 (* 1 = 0.07053 loss)
I1124 06:44:38.623195 35012 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1124 06:44:38.623195 35012 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1124 06:44:46.459321 35012 solver.cpp:218] Iteration 19600 (12.763 iter/s, 7.83514s/100 iters), loss = 0.0499674
I1124 06:44:46.459321 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:44:46.459321 35012 solver.cpp:237]     Train net output #1: loss = 0.0499674 (* 1 = 0.0499674 loss)
I1124 06:44:46.459321 35012 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1124 06:44:54.292402 35012 solver.cpp:218] Iteration 19700 (12.7661 iter/s, 7.83322s/100 iters), loss = 0.0640461
I1124 06:44:54.292402 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:44:54.292402 35012 solver.cpp:237]     Train net output #1: loss = 0.064046 (* 1 = 0.064046 loss)
I1124 06:44:54.292402 35012 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1124 06:45:02.128434 35012 solver.cpp:218] Iteration 19800 (12.7633 iter/s, 7.83496s/100 iters), loss = 0.0409012
I1124 06:45:02.128434 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:45:02.128434 35012 solver.cpp:237]     Train net output #1: loss = 0.0409012 (* 1 = 0.0409012 loss)
I1124 06:45:02.128434 35012 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1124 06:45:09.963217 35012 solver.cpp:218] Iteration 19900 (12.7644 iter/s, 7.83429s/100 iters), loss = 0.0437774
I1124 06:45:09.963217 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:45:09.963217 35012 solver.cpp:237]     Train net output #1: loss = 0.0437773 (* 1 = 0.0437773 loss)
I1124 06:45:09.963217 35012 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1124 06:45:17.412669 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:45:17.720767 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_20000.caffemodel
I1124 06:45:17.759321 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_20000.solverstate
I1124 06:45:17.778841 35012 solver.cpp:330] Iteration 20000, Testing net (#0)
I1124 06:45:17.779320 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:45:19.848819 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:45:19.931839 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9212
I1124 06:45:19.931839 35012 solver.cpp:397]     Test net output #1: loss = 0.239474 (* 1 = 0.239474 loss)
I1124 06:45:20.008469 35012 solver.cpp:218] Iteration 20000 (9.9553 iter/s, 10.0449s/100 iters), loss = 0.0665079
I1124 06:45:20.008469 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:45:20.008469 35012 solver.cpp:237]     Train net output #1: loss = 0.0665079 (* 1 = 0.0665079 loss)
I1124 06:45:20.008469 35012 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1124 06:45:27.840639 35012 solver.cpp:218] Iteration 20100 (12.7687 iter/s, 7.83167s/100 iters), loss = 0.074061
I1124 06:45:27.840639 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:45:27.840639 35012 solver.cpp:237]     Train net output #1: loss = 0.074061 (* 1 = 0.074061 loss)
I1124 06:45:27.840639 35012 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1124 06:45:35.675474 35012 solver.cpp:218] Iteration 20200 (12.7642 iter/s, 7.83443s/100 iters), loss = 0.0480006
I1124 06:45:35.675474 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:45:35.675474 35012 solver.cpp:237]     Train net output #1: loss = 0.0480006 (* 1 = 0.0480006 loss)
I1124 06:45:35.675474 35012 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1124 06:45:43.509189 35012 solver.cpp:218] Iteration 20300 (12.7665 iter/s, 7.83301s/100 iters), loss = 0.0672287
I1124 06:45:43.509189 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:45:43.509189 35012 solver.cpp:237]     Train net output #1: loss = 0.0672287 (* 1 = 0.0672287 loss)
I1124 06:45:43.509189 35012 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1124 06:45:51.345706 35012 solver.cpp:218] Iteration 20400 (12.7616 iter/s, 7.83601s/100 iters), loss = 0.0511383
I1124 06:45:51.345706 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:45:51.345706 35012 solver.cpp:237]     Train net output #1: loss = 0.0511383 (* 1 = 0.0511383 loss)
I1124 06:45:51.345706 35012 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1124 06:45:58.793376 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:45:59.102991 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_20500.caffemodel
I1124 06:45:59.142050 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_20500.solverstate
I1124 06:45:59.162047 35012 solver.cpp:330] Iteration 20500, Testing net (#0)
I1124 06:45:59.162047 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:46:01.233121 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:46:01.316671 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9212
I1124 06:46:01.316671 35012 solver.cpp:397]     Test net output #1: loss = 0.23936 (* 1 = 0.23936 loss)
I1124 06:46:01.393671 35012 solver.cpp:218] Iteration 20500 (9.95256 iter/s, 10.0477s/100 iters), loss = 0.0618993
I1124 06:46:01.393671 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:46:01.393671 35012 solver.cpp:237]     Train net output #1: loss = 0.0618993 (* 1 = 0.0618993 loss)
I1124 06:46:01.393671 35012 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1124 06:46:09.224375 35012 solver.cpp:218] Iteration 20600 (12.7709 iter/s, 7.83028s/100 iters), loss = 0.0758749
I1124 06:46:09.224375 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:46:09.224375 35012 solver.cpp:237]     Train net output #1: loss = 0.0758749 (* 1 = 0.0758749 loss)
I1124 06:46:09.224375 35012 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1124 06:46:17.055660 35012 solver.cpp:218] Iteration 20700 (12.7708 iter/s, 7.83036s/100 iters), loss = 0.0669788
I1124 06:46:17.055660 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:46:17.055660 35012 solver.cpp:237]     Train net output #1: loss = 0.0669788 (* 1 = 0.0669788 loss)
I1124 06:46:17.055660 35012 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1124 06:46:24.882395 35012 solver.cpp:218] Iteration 20800 (12.777 iter/s, 7.82657s/100 iters), loss = 0.0624167
I1124 06:46:24.882395 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:46:24.882395 35012 solver.cpp:237]     Train net output #1: loss = 0.0624167 (* 1 = 0.0624167 loss)
I1124 06:46:24.882395 35012 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1124 06:46:32.707773 35012 solver.cpp:218] Iteration 20900 (12.7795 iter/s, 7.82501s/100 iters), loss = 0.0388214
I1124 06:46:32.707773 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:46:32.707773 35012 solver.cpp:237]     Train net output #1: loss = 0.0388213 (* 1 = 0.0388213 loss)
I1124 06:46:32.707773 35012 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1124 06:46:40.151224 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:46:40.460809 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_21000.caffemodel
I1124 06:46:40.501335 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_21000.solverstate
I1124 06:46:40.520347 35012 solver.cpp:330] Iteration 21000, Testing net (#0)
I1124 06:46:40.521340 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:46:42.590200 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:46:42.673738 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9211
I1124 06:46:42.673738 35012 solver.cpp:397]     Test net output #1: loss = 0.239471 (* 1 = 0.239471 loss)
I1124 06:46:42.749778 35012 solver.cpp:218] Iteration 21000 (9.95884 iter/s, 10.0413s/100 iters), loss = 0.0499173
I1124 06:46:42.749778 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:46:42.749778 35012 solver.cpp:237]     Train net output #1: loss = 0.0499173 (* 1 = 0.0499173 loss)
I1124 06:46:42.749778 35012 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1124 06:46:50.582190 35012 solver.cpp:218] Iteration 21100 (12.7679 iter/s, 7.83215s/100 iters), loss = 0.0531093
I1124 06:46:50.582190 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:46:50.582190 35012 solver.cpp:237]     Train net output #1: loss = 0.0531093 (* 1 = 0.0531093 loss)
I1124 06:46:50.582190 35012 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1124 06:46:58.412130 35012 solver.cpp:218] Iteration 21200 (12.7726 iter/s, 7.82929s/100 iters), loss = 0.0472335
I1124 06:46:58.412130 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:46:58.412130 35012 solver.cpp:237]     Train net output #1: loss = 0.0472335 (* 1 = 0.0472335 loss)
I1124 06:46:58.412130 35012 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1124 06:47:06.246055 35012 solver.cpp:218] Iteration 21300 (12.766 iter/s, 7.83334s/100 iters), loss = 0.0804115
I1124 06:47:06.246055 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:47:06.246055 35012 solver.cpp:237]     Train net output #1: loss = 0.0804115 (* 1 = 0.0804115 loss)
I1124 06:47:06.246055 35012 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1124 06:47:14.074539 35012 solver.cpp:218] Iteration 21400 (12.7746 iter/s, 7.82805s/100 iters), loss = 0.0533567
I1124 06:47:14.074539 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:47:14.074539 35012 solver.cpp:237]     Train net output #1: loss = 0.0533567 (* 1 = 0.0533567 loss)
I1124 06:47:14.074539 35012 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1124 06:47:21.519004 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:47:21.826122 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_21500.caffemodel
I1124 06:47:21.866114 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_21500.solverstate
I1124 06:47:21.886116 35012 solver.cpp:330] Iteration 21500, Testing net (#0)
I1124 06:47:21.887102 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:47:23.955709 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:47:24.039746 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9211
I1124 06:47:24.039746 35012 solver.cpp:397]     Test net output #1: loss = 0.239436 (* 1 = 0.239436 loss)
I1124 06:47:24.115581 35012 solver.cpp:218] Iteration 21500 (9.95948 iter/s, 10.0407s/100 iters), loss = 0.0702147
I1124 06:47:24.115581 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:47:24.115581 35012 solver.cpp:237]     Train net output #1: loss = 0.0702146 (* 1 = 0.0702146 loss)
I1124 06:47:24.115581 35012 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1124 06:47:31.952302 35012 solver.cpp:218] Iteration 21600 (12.7617 iter/s, 7.83592s/100 iters), loss = 0.0737882
I1124 06:47:31.952302 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:47:31.952302 35012 solver.cpp:237]     Train net output #1: loss = 0.0737882 (* 1 = 0.0737882 loss)
I1124 06:47:31.952302 35012 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1124 06:47:39.780930 35012 solver.cpp:218] Iteration 21700 (12.7737 iter/s, 7.82857s/100 iters), loss = 0.0647178
I1124 06:47:39.780930 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:47:39.780930 35012 solver.cpp:237]     Train net output #1: loss = 0.0647178 (* 1 = 0.0647178 loss)
I1124 06:47:39.780930 35012 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1124 06:47:47.615084 35012 solver.cpp:218] Iteration 21800 (12.7665 iter/s, 7.83302s/100 iters), loss = 0.0656456
I1124 06:47:47.615084 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:47:47.615084 35012 solver.cpp:237]     Train net output #1: loss = 0.0656455 (* 1 = 0.0656455 loss)
I1124 06:47:47.615084 35012 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1124 06:47:55.445824 35012 solver.cpp:218] Iteration 21900 (12.7701 iter/s, 7.83078s/100 iters), loss = 0.0253726
I1124 06:47:55.445824 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:47:55.445824 35012 solver.cpp:237]     Train net output #1: loss = 0.0253726 (* 1 = 0.0253726 loss)
I1124 06:47:55.445824 35012 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1124 06:48:02.893942 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:48:03.203316 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_22000.caffemodel
I1124 06:48:03.244840 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_22000.solverstate
I1124 06:48:03.264839 35012 solver.cpp:330] Iteration 22000, Testing net (#0)
I1124 06:48:03.264839 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:48:05.334347 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:48:05.416877 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9212
I1124 06:48:05.417865 35012 solver.cpp:397]     Test net output #1: loss = 0.239441 (* 1 = 0.239441 loss)
I1124 06:48:05.493885 35012 solver.cpp:218] Iteration 22000 (9.95244 iter/s, 10.0478s/100 iters), loss = 0.0638184
I1124 06:48:05.493885 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:48:05.493885 35012 solver.cpp:237]     Train net output #1: loss = 0.0638184 (* 1 = 0.0638184 loss)
I1124 06:48:05.493885 35012 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1124 06:48:05.493885 35012 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1124 06:48:13.321698 35012 solver.cpp:218] Iteration 22100 (12.777 iter/s, 7.82658s/100 iters), loss = 0.0766338
I1124 06:48:13.321698 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:48:13.321698 35012 solver.cpp:237]     Train net output #1: loss = 0.0766337 (* 1 = 0.0766337 loss)
I1124 06:48:13.321698 35012 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1124 06:48:21.149740 35012 solver.cpp:218] Iteration 22200 (12.7748 iter/s, 7.82793s/100 iters), loss = 0.0562324
I1124 06:48:21.149740 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:48:21.149740 35012 solver.cpp:237]     Train net output #1: loss = 0.0562323 (* 1 = 0.0562323 loss)
I1124 06:48:21.149740 35012 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1124 06:48:28.985028 35012 solver.cpp:218] Iteration 22300 (12.764 iter/s, 7.83455s/100 iters), loss = 0.0432032
I1124 06:48:28.985028 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:48:28.985028 35012 solver.cpp:237]     Train net output #1: loss = 0.0432031 (* 1 = 0.0432031 loss)
I1124 06:48:28.985028 35012 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1124 06:48:36.812327 35012 solver.cpp:218] Iteration 22400 (12.7761 iter/s, 7.82712s/100 iters), loss = 0.0373931
I1124 06:48:36.812327 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:48:36.812327 35012 solver.cpp:237]     Train net output #1: loss = 0.0373931 (* 1 = 0.0373931 loss)
I1124 06:48:36.812327 35012 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1124 06:48:44.253542 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:48:44.562754 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_22500.caffemodel
I1124 06:48:44.601778 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_22500.solverstate
I1124 06:48:44.621778 35012 solver.cpp:330] Iteration 22500, Testing net (#0)
I1124 06:48:44.621778 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:48:46.691355 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:48:46.774879 35012 solver.cpp:397]     Test net output #0: accuracy = 0.921
I1124 06:48:46.774879 35012 solver.cpp:397]     Test net output #1: loss = 0.239489 (* 1 = 0.239489 loss)
I1124 06:48:46.850642 35012 solver.cpp:218] Iteration 22500 (9.96188 iter/s, 10.0383s/100 iters), loss = 0.0839074
I1124 06:48:46.851629 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:48:46.851629 35012 solver.cpp:237]     Train net output #1: loss = 0.0839074 (* 1 = 0.0839074 loss)
I1124 06:48:46.851629 35012 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1124 06:48:54.678481 35012 solver.cpp:218] Iteration 22600 (12.7772 iter/s, 7.82644s/100 iters), loss = 0.0689812
I1124 06:48:54.678481 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:48:54.678481 35012 solver.cpp:237]     Train net output #1: loss = 0.0689812 (* 1 = 0.0689812 loss)
I1124 06:48:54.678481 35012 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1124 06:49:02.506748 35012 solver.cpp:218] Iteration 22700 (12.7735 iter/s, 7.82873s/100 iters), loss = 0.0649922
I1124 06:49:02.507750 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:49:02.507750 35012 solver.cpp:237]     Train net output #1: loss = 0.0649922 (* 1 = 0.0649922 loss)
I1124 06:49:02.507750 35012 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1124 06:49:10.338914 35012 solver.cpp:218] Iteration 22800 (12.7695 iter/s, 7.83116s/100 iters), loss = 0.0680713
I1124 06:49:10.338914 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:49:10.338914 35012 solver.cpp:237]     Train net output #1: loss = 0.0680712 (* 1 = 0.0680712 loss)
I1124 06:49:10.338914 35012 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1124 06:49:18.167861 35012 solver.cpp:218] Iteration 22900 (12.7732 iter/s, 7.82886s/100 iters), loss = 0.0531038
I1124 06:49:18.167861 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:49:18.168850 35012 solver.cpp:237]     Train net output #1: loss = 0.0531037 (* 1 = 0.0531037 loss)
I1124 06:49:18.168850 35012 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1124 06:49:25.612464 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:49:25.922571 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_23000.caffemodel
I1124 06:49:25.960558 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_23000.solverstate
I1124 06:49:25.980557 35012 solver.cpp:330] Iteration 23000, Testing net (#0)
I1124 06:49:25.980557 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:49:28.049474 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:49:28.132479 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9211
I1124 06:49:28.132479 35012 solver.cpp:397]     Test net output #1: loss = 0.239436 (* 1 = 0.239436 loss)
I1124 06:49:28.209533 35012 solver.cpp:218] Iteration 23000 (9.96005 iter/s, 10.0401s/100 iters), loss = 0.0553063
I1124 06:49:28.209533 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:49:28.209533 35012 solver.cpp:237]     Train net output #1: loss = 0.0553062 (* 1 = 0.0553062 loss)
I1124 06:49:28.209533 35012 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1124 06:49:36.039108 35012 solver.cpp:218] Iteration 23100 (12.7731 iter/s, 7.82898s/100 iters), loss = 0.0644871
I1124 06:49:36.039108 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:49:36.039108 35012 solver.cpp:237]     Train net output #1: loss = 0.064487 (* 1 = 0.064487 loss)
I1124 06:49:36.039108 35012 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1124 06:49:43.869945 35012 solver.cpp:218] Iteration 23200 (12.7709 iter/s, 7.83031s/100 iters), loss = 0.0735951
I1124 06:49:43.869945 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:49:43.869945 35012 solver.cpp:237]     Train net output #1: loss = 0.073595 (* 1 = 0.073595 loss)
I1124 06:49:43.869945 35012 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1124 06:49:51.700677 35012 solver.cpp:218] Iteration 23300 (12.7713 iter/s, 7.83003s/100 iters), loss = 0.075495
I1124 06:49:51.700677 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:49:51.700677 35012 solver.cpp:237]     Train net output #1: loss = 0.075495 (* 1 = 0.075495 loss)
I1124 06:49:51.700677 35012 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1124 06:49:59.530720 35012 solver.cpp:218] Iteration 23400 (12.771 iter/s, 7.83023s/100 iters), loss = 0.0617827
I1124 06:49:59.530720 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:49:59.530720 35012 solver.cpp:237]     Train net output #1: loss = 0.0617826 (* 1 = 0.0617826 loss)
I1124 06:49:59.530720 35012 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1124 06:50:06.974473 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:50:07.284055 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_23500.caffemodel
I1124 06:50:07.324615 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_23500.solverstate
I1124 06:50:07.343624 35012 solver.cpp:330] Iteration 23500, Testing net (#0)
I1124 06:50:07.344610 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:50:09.413532 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:50:09.496084 35012 solver.cpp:397]     Test net output #0: accuracy = 0.921
I1124 06:50:09.496084 35012 solver.cpp:397]     Test net output #1: loss = 0.239442 (* 1 = 0.239442 loss)
I1124 06:50:09.573084 35012 solver.cpp:218] Iteration 23500 (9.95864 iter/s, 10.0415s/100 iters), loss = 0.0531099
I1124 06:50:09.573084 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:50:09.573084 35012 solver.cpp:237]     Train net output #1: loss = 0.0531099 (* 1 = 0.0531099 loss)
I1124 06:50:09.573084 35012 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1124 06:50:17.406013 35012 solver.cpp:218] Iteration 23600 (12.7678 iter/s, 7.83222s/100 iters), loss = 0.0591698
I1124 06:50:17.406013 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:50:17.406013 35012 solver.cpp:237]     Train net output #1: loss = 0.0591697 (* 1 = 0.0591697 loss)
I1124 06:50:17.406013 35012 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1124 06:50:25.239302 35012 solver.cpp:218] Iteration 23700 (12.766 iter/s, 7.83329s/100 iters), loss = 0.0705426
I1124 06:50:25.239302 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:50:25.239302 35012 solver.cpp:237]     Train net output #1: loss = 0.0705425 (* 1 = 0.0705425 loss)
I1124 06:50:25.239302 35012 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1124 06:50:33.072259 35012 solver.cpp:218] Iteration 23800 (12.7668 iter/s, 7.83281s/100 iters), loss = 0.0476822
I1124 06:50:33.072259 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:50:33.072259 35012 solver.cpp:237]     Train net output #1: loss = 0.0476822 (* 1 = 0.0476822 loss)
I1124 06:50:33.072259 35012 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1124 06:50:40.909396 35012 solver.cpp:218] Iteration 23900 (12.7617 iter/s, 7.83595s/100 iters), loss = 0.0418907
I1124 06:50:40.909396 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:50:40.909396 35012 solver.cpp:237]     Train net output #1: loss = 0.0418906 (* 1 = 0.0418906 loss)
I1124 06:50:40.909396 35012 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1124 06:50:48.358680 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:50:48.668740 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_24000.caffemodel
I1124 06:50:48.707772 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_24000.solverstate
I1124 06:50:48.727279 35012 solver.cpp:330] Iteration 24000, Testing net (#0)
I1124 06:50:48.727761 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:50:50.796447 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:50:50.879462 35012 solver.cpp:397]     Test net output #0: accuracy = 0.921
I1124 06:50:50.879462 35012 solver.cpp:397]     Test net output #1: loss = 0.239516 (* 1 = 0.239516 loss)
I1124 06:50:50.956076 35012 solver.cpp:218] Iteration 24000 (9.95386 iter/s, 10.0464s/100 iters), loss = 0.0667364
I1124 06:50:50.956076 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:50:50.956076 35012 solver.cpp:237]     Train net output #1: loss = 0.0667364 (* 1 = 0.0667364 loss)
I1124 06:50:50.956076 35012 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1124 06:50:58.790166 35012 solver.cpp:218] Iteration 24100 (12.7653 iter/s, 7.83373s/100 iters), loss = 0.0828647
I1124 06:50:58.790166 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:50:58.790166 35012 solver.cpp:237]     Train net output #1: loss = 0.0828647 (* 1 = 0.0828647 loss)
I1124 06:50:58.790166 35012 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1124 06:51:06.621351 35012 solver.cpp:218] Iteration 24200 (12.7697 iter/s, 7.83106s/100 iters), loss = 0.0655766
I1124 06:51:06.621351 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:51:06.621351 35012 solver.cpp:237]     Train net output #1: loss = 0.0655765 (* 1 = 0.0655765 loss)
I1124 06:51:06.621351 35012 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1124 06:51:14.452196 35012 solver.cpp:218] Iteration 24300 (12.7712 iter/s, 7.83015s/100 iters), loss = 0.0634682
I1124 06:51:14.452196 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:51:14.452196 35012 solver.cpp:237]     Train net output #1: loss = 0.0634682 (* 1 = 0.0634682 loss)
I1124 06:51:14.452196 35012 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1124 06:51:22.284055 35012 solver.cpp:218] Iteration 24400 (12.7694 iter/s, 7.83122s/100 iters), loss = 0.0684552
I1124 06:51:22.284543 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:51:22.284543 35012 solver.cpp:237]     Train net output #1: loss = 0.0684552 (* 1 = 0.0684552 loss)
I1124 06:51:22.284543 35012 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1124 06:51:29.725625 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:51:30.034819 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_24500.caffemodel
I1124 06:51:30.076827 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_24500.solverstate
I1124 06:51:30.096846 35012 solver.cpp:330] Iteration 24500, Testing net (#0)
I1124 06:51:30.096846 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:51:32.166828 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:51:32.249339 35012 solver.cpp:397]     Test net output #0: accuracy = 0.921
I1124 06:51:32.249339 35012 solver.cpp:397]     Test net output #1: loss = 0.239481 (* 1 = 0.239481 loss)
I1124 06:51:32.326382 35012 solver.cpp:218] Iteration 24500 (9.95884 iter/s, 10.0413s/100 iters), loss = 0.0445299
I1124 06:51:32.326382 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:51:32.326382 35012 solver.cpp:237]     Train net output #1: loss = 0.0445299 (* 1 = 0.0445299 loss)
I1124 06:51:32.326382 35012 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1124 06:51:40.158911 35012 solver.cpp:218] Iteration 24600 (12.7669 iter/s, 7.83274s/100 iters), loss = 0.0695171
I1124 06:51:40.158911 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:51:40.158911 35012 solver.cpp:237]     Train net output #1: loss = 0.069517 (* 1 = 0.069517 loss)
I1124 06:51:40.158911 35012 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1124 06:51:47.989918 35012 solver.cpp:218] Iteration 24700 (12.7708 iter/s, 7.83036s/100 iters), loss = 0.0733436
I1124 06:51:47.989918 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:51:47.989918 35012 solver.cpp:237]     Train net output #1: loss = 0.0733435 (* 1 = 0.0733435 loss)
I1124 06:51:47.989918 35012 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1124 06:51:55.828007 35012 solver.cpp:218] Iteration 24800 (12.7588 iter/s, 7.83773s/100 iters), loss = 0.0375523
I1124 06:51:55.828007 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:51:55.828007 35012 solver.cpp:237]     Train net output #1: loss = 0.0375522 (* 1 = 0.0375522 loss)
I1124 06:51:55.828007 35012 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1124 06:52:03.659323 35012 solver.cpp:218] Iteration 24900 (12.7702 iter/s, 7.83073s/100 iters), loss = 0.0499917
I1124 06:52:03.659323 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:52:03.659323 35012 solver.cpp:237]     Train net output #1: loss = 0.0499916 (* 1 = 0.0499916 loss)
I1124 06:52:03.659323 35012 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1124 06:52:11.106340 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:52:11.416033 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_25000.caffemodel
I1124 06:52:11.455014 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_25000.solverstate
I1124 06:52:11.474557 35012 solver.cpp:330] Iteration 25000, Testing net (#0)
I1124 06:52:11.475039 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:52:13.544950 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:52:13.627995 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9212
I1124 06:52:13.627995 35012 solver.cpp:397]     Test net output #1: loss = 0.23959 (* 1 = 0.23959 loss)
I1124 06:52:13.705029 35012 solver.cpp:218] Iteration 25000 (9.95544 iter/s, 10.0448s/100 iters), loss = 0.0705101
I1124 06:52:13.705029 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:52:13.705029 35012 solver.cpp:237]     Train net output #1: loss = 0.07051 (* 1 = 0.07051 loss)
I1124 06:52:13.705029 35012 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1124 06:52:21.536443 35012 solver.cpp:218] Iteration 25100 (12.7695 iter/s, 7.83114s/100 iters), loss = 0.0754825
I1124 06:52:21.536443 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:52:21.536443 35012 solver.cpp:237]     Train net output #1: loss = 0.0754824 (* 1 = 0.0754824 loss)
I1124 06:52:21.536443 35012 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1124 06:52:29.365244 35012 solver.cpp:218] Iteration 25200 (12.7743 iter/s, 7.82825s/100 iters), loss = 0.0655462
I1124 06:52:29.365244 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:52:29.365244 35012 solver.cpp:237]     Train net output #1: loss = 0.0655461 (* 1 = 0.0655461 loss)
I1124 06:52:29.365244 35012 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1124 06:52:37.191799 35012 solver.cpp:218] Iteration 25300 (12.7768 iter/s, 7.82669s/100 iters), loss = 0.0417994
I1124 06:52:37.192806 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:52:37.192806 35012 solver.cpp:237]     Train net output #1: loss = 0.0417993 (* 1 = 0.0417993 loss)
I1124 06:52:37.192806 35012 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1124 06:52:45.017799 35012 solver.cpp:218] Iteration 25400 (12.7803 iter/s, 7.82457s/100 iters), loss = 0.0642143
I1124 06:52:45.017799 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:52:45.017799 35012 solver.cpp:237]     Train net output #1: loss = 0.0642142 (* 1 = 0.0642142 loss)
I1124 06:52:45.017799 35012 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1124 06:52:52.461982 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:52:52.770088 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_25500.caffemodel
I1124 06:52:52.811591 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_25500.solverstate
I1124 06:52:52.831591 35012 solver.cpp:330] Iteration 25500, Testing net (#0)
I1124 06:52:52.831591 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:52:54.901497 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:52:54.984519 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9211
I1124 06:52:54.984519 35012 solver.cpp:397]     Test net output #1: loss = 0.239523 (* 1 = 0.239523 loss)
I1124 06:52:55.061533 35012 solver.cpp:218] Iteration 25500 (9.95684 iter/s, 10.0433s/100 iters), loss = 0.0387089
I1124 06:52:55.061533 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:52:55.061533 35012 solver.cpp:237]     Train net output #1: loss = 0.0387088 (* 1 = 0.0387088 loss)
I1124 06:52:55.061533 35012 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1124 06:53:02.885246 35012 solver.cpp:218] Iteration 25600 (12.7814 iter/s, 7.82389s/100 iters), loss = 0.102408
I1124 06:53:02.885246 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:53:02.886250 35012 solver.cpp:237]     Train net output #1: loss = 0.102408 (* 1 = 0.102408 loss)
I1124 06:53:02.886250 35012 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1124 06:53:10.711355 35012 solver.cpp:218] Iteration 25700 (12.7797 iter/s, 7.82491s/100 iters), loss = 0.0737303
I1124 06:53:10.711355 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:53:10.711355 35012 solver.cpp:237]     Train net output #1: loss = 0.0737302 (* 1 = 0.0737302 loss)
I1124 06:53:10.711355 35012 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1124 06:53:18.542681 35012 solver.cpp:218] Iteration 25800 (12.7696 iter/s, 7.83111s/100 iters), loss = 0.0662171
I1124 06:53:18.542681 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:53:18.542681 35012 solver.cpp:237]     Train net output #1: loss = 0.066217 (* 1 = 0.066217 loss)
I1124 06:53:18.542681 35012 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1124 06:53:26.369551 35012 solver.cpp:218] Iteration 25900 (12.7779 iter/s, 7.82599s/100 iters), loss = 0.0366142
I1124 06:53:26.369551 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:53:26.369551 35012 solver.cpp:237]     Train net output #1: loss = 0.0366141 (* 1 = 0.0366141 loss)
I1124 06:53:26.369551 35012 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1124 06:53:33.811508 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:53:34.119616 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_26000.caffemodel
I1124 06:53:34.159137 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_26000.solverstate
I1124 06:53:34.180637 35012 solver.cpp:330] Iteration 26000, Testing net (#0)
I1124 06:53:34.180637 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:53:36.249078 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:53:36.332125 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9213
I1124 06:53:36.332125 35012 solver.cpp:397]     Test net output #1: loss = 0.23947 (* 1 = 0.23947 loss)
I1124 06:53:36.409144 35012 solver.cpp:218] Iteration 26000 (9.96067 iter/s, 10.0395s/100 iters), loss = 0.0670887
I1124 06:53:36.409144 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:53:36.409632 35012 solver.cpp:237]     Train net output #1: loss = 0.0670886 (* 1 = 0.0670886 loss)
I1124 06:53:36.409632 35012 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1124 06:53:44.236027 35012 solver.cpp:218] Iteration 26100 (12.7764 iter/s, 7.82693s/100 iters), loss = 0.0593054
I1124 06:53:44.236027 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:53:44.237015 35012 solver.cpp:237]     Train net output #1: loss = 0.0593053 (* 1 = 0.0593053 loss)
I1124 06:53:44.237015 35012 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1124 06:53:52.063997 35012 solver.cpp:218] Iteration 26200 (12.776 iter/s, 7.82716s/100 iters), loss = 0.0576471
I1124 06:53:52.063997 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:53:52.063997 35012 solver.cpp:237]     Train net output #1: loss = 0.057647 (* 1 = 0.057647 loss)
I1124 06:53:52.063997 35012 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1124 06:53:59.892029 35012 solver.cpp:218] Iteration 26300 (12.7755 iter/s, 7.82748s/100 iters), loss = 0.0403365
I1124 06:53:59.892029 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:53:59.892029 35012 solver.cpp:237]     Train net output #1: loss = 0.0403364 (* 1 = 0.0403364 loss)
I1124 06:53:59.892029 35012 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1124 06:54:07.717428 35012 solver.cpp:218] Iteration 26400 (12.7804 iter/s, 7.82447s/100 iters), loss = 0.067435
I1124 06:54:07.717428 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:54:07.717428 35012 solver.cpp:237]     Train net output #1: loss = 0.067435 (* 1 = 0.067435 loss)
I1124 06:54:07.717428 35012 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1124 06:54:15.161639 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:54:15.470319 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_26500.caffemodel
I1124 06:54:15.509507 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_26500.solverstate
I1124 06:54:15.528517 35012 solver.cpp:330] Iteration 26500, Testing net (#0)
I1124 06:54:15.528517 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:54:17.598373 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:54:17.681393 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9211
I1124 06:54:17.681393 35012 solver.cpp:397]     Test net output #1: loss = 0.239485 (* 1 = 0.239485 loss)
I1124 06:54:17.758424 35012 solver.cpp:218] Iteration 26500 (9.9597 iter/s, 10.0405s/100 iters), loss = 0.0887335
I1124 06:54:17.758424 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 06:54:17.758424 35012 solver.cpp:237]     Train net output #1: loss = 0.0887334 (* 1 = 0.0887334 loss)
I1124 06:54:17.758424 35012 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1124 06:54:25.589391 35012 solver.cpp:218] Iteration 26600 (12.77 iter/s, 7.83088s/100 iters), loss = 0.0760391
I1124 06:54:25.589391 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:54:25.589879 35012 solver.cpp:237]     Train net output #1: loss = 0.076039 (* 1 = 0.076039 loss)
I1124 06:54:25.589879 35012 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1124 06:54:33.421761 35012 solver.cpp:218] Iteration 26700 (12.7689 iter/s, 7.83153s/100 iters), loss = 0.0520464
I1124 06:54:33.421761 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:54:33.421761 35012 solver.cpp:237]     Train net output #1: loss = 0.0520463 (* 1 = 0.0520463 loss)
I1124 06:54:33.421761 35012 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1124 06:54:41.251418 35012 solver.cpp:218] Iteration 26800 (12.7714 iter/s, 7.83s/100 iters), loss = 0.0346733
I1124 06:54:41.252405 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:54:41.252405 35012 solver.cpp:237]     Train net output #1: loss = 0.0346732 (* 1 = 0.0346732 loss)
I1124 06:54:41.252405 35012 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1124 06:54:49.081487 35012 solver.cpp:218] Iteration 26900 (12.7724 iter/s, 7.82937s/100 iters), loss = 0.0441639
I1124 06:54:49.081487 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:54:49.081487 35012 solver.cpp:237]     Train net output #1: loss = 0.0441637 (* 1 = 0.0441637 loss)
I1124 06:54:49.081487 35012 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1124 06:54:56.527904 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:54:56.837671 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_27000.caffemodel
I1124 06:54:56.878669 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_27000.solverstate
I1124 06:54:56.898206 35012 solver.cpp:330] Iteration 27000, Testing net (#0)
I1124 06:54:56.898686 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:54:58.968117 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:54:59.050896 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9212
I1124 06:54:59.050896 35012 solver.cpp:397]     Test net output #1: loss = 0.239498 (* 1 = 0.239498 loss)
I1124 06:54:59.128016 35012 solver.cpp:218] Iteration 27000 (9.95464 iter/s, 10.0456s/100 iters), loss = 0.0583449
I1124 06:54:59.128016 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:54:59.128016 35012 solver.cpp:237]     Train net output #1: loss = 0.0583447 (* 1 = 0.0583447 loss)
I1124 06:54:59.128016 35012 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1124 06:54:59.128016 35012 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1124 06:55:06.955471 35012 solver.cpp:218] Iteration 27100 (12.7762 iter/s, 7.82707s/100 iters), loss = 0.0986344
I1124 06:55:06.955471 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:55:06.955471 35012 solver.cpp:237]     Train net output #1: loss = 0.0986342 (* 1 = 0.0986342 loss)
I1124 06:55:06.955471 35012 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1124 06:55:14.780133 35012 solver.cpp:218] Iteration 27200 (12.78 iter/s, 7.8247s/100 iters), loss = 0.0649018
I1124 06:55:14.781119 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:55:14.781119 35012 solver.cpp:237]     Train net output #1: loss = 0.0649017 (* 1 = 0.0649017 loss)
I1124 06:55:14.781119 35012 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1124 06:55:22.609622 35012 solver.cpp:218] Iteration 27300 (12.7742 iter/s, 7.82826s/100 iters), loss = 0.0651296
I1124 06:55:22.609622 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:55:22.609622 35012 solver.cpp:237]     Train net output #1: loss = 0.0651295 (* 1 = 0.0651295 loss)
I1124 06:55:22.609622 35012 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1124 06:55:30.439786 35012 solver.cpp:218] Iteration 27400 (12.7722 iter/s, 7.82949s/100 iters), loss = 0.0355494
I1124 06:55:30.439786 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:55:30.439786 35012 solver.cpp:237]     Train net output #1: loss = 0.0355492 (* 1 = 0.0355492 loss)
I1124 06:55:30.439786 35012 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1124 06:55:37.887789 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:55:38.197825 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_27500.caffemodel
I1124 06:55:38.240836 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_27500.solverstate
I1124 06:55:38.259850 35012 solver.cpp:330] Iteration 27500, Testing net (#0)
I1124 06:55:38.259850 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:55:40.328478 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:55:40.412508 35012 solver.cpp:397]     Test net output #0: accuracy = 0.921
I1124 06:55:40.412508 35012 solver.cpp:397]     Test net output #1: loss = 0.239484 (* 1 = 0.239484 loss)
I1124 06:55:40.489024 35012 solver.cpp:218] Iteration 27500 (9.9515 iter/s, 10.0487s/100 iters), loss = 0.0705252
I1124 06:55:40.489024 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:55:40.489024 35012 solver.cpp:237]     Train net output #1: loss = 0.070525 (* 1 = 0.070525 loss)
I1124 06:55:40.489024 35012 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1124 06:55:48.313223 35012 solver.cpp:218] Iteration 27600 (12.7804 iter/s, 7.82449s/100 iters), loss = 0.0488217
I1124 06:55:48.313223 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:55:48.313223 35012 solver.cpp:237]     Train net output #1: loss = 0.0488216 (* 1 = 0.0488216 loss)
I1124 06:55:48.313223 35012 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1124 06:55:56.138937 35012 solver.cpp:218] Iteration 27700 (12.7799 iter/s, 7.82481s/100 iters), loss = 0.0578028
I1124 06:55:56.138937 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:55:56.138937 35012 solver.cpp:237]     Train net output #1: loss = 0.0578026 (* 1 = 0.0578026 loss)
I1124 06:55:56.138937 35012 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1124 06:56:03.968106 35012 solver.cpp:218] Iteration 27800 (12.7737 iter/s, 7.82861s/100 iters), loss = 0.0740385
I1124 06:56:03.968106 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:56:03.968106 35012 solver.cpp:237]     Train net output #1: loss = 0.0740383 (* 1 = 0.0740383 loss)
I1124 06:56:03.968106 35012 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1124 06:56:11.791122 35012 solver.cpp:218] Iteration 27900 (12.7837 iter/s, 7.82245s/100 iters), loss = 0.0383819
I1124 06:56:11.791122 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:56:11.791122 35012 solver.cpp:237]     Train net output #1: loss = 0.0383817 (* 1 = 0.0383817 loss)
I1124 06:56:11.791122 35012 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1124 06:56:19.234395 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:56:19.543988 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_28000.caffemodel
I1124 06:56:19.584404 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_28000.solverstate
I1124 06:56:19.603404 35012 solver.cpp:330] Iteration 28000, Testing net (#0)
I1124 06:56:19.603404 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:56:21.672763 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:56:21.756292 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9211
I1124 06:56:21.756292 35012 solver.cpp:397]     Test net output #1: loss = 0.239524 (* 1 = 0.239524 loss)
I1124 06:56:21.832818 35012 solver.cpp:218] Iteration 28000 (9.95873 iter/s, 10.0414s/100 iters), loss = 0.0488897
I1124 06:56:21.832818 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:56:21.832818 35012 solver.cpp:237]     Train net output #1: loss = 0.0488895 (* 1 = 0.0488895 loss)
I1124 06:56:21.833304 35012 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1124 06:56:29.666045 35012 solver.cpp:218] Iteration 28100 (12.7668 iter/s, 7.83281s/100 iters), loss = 0.0680665
I1124 06:56:29.666045 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:56:29.666045 35012 solver.cpp:237]     Train net output #1: loss = 0.0680664 (* 1 = 0.0680664 loss)
I1124 06:56:29.666045 35012 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1124 06:56:37.500090 35012 solver.cpp:218] Iteration 28200 (12.7652 iter/s, 7.83378s/100 iters), loss = 0.0415674
I1124 06:56:37.500090 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:56:37.500090 35012 solver.cpp:237]     Train net output #1: loss = 0.0415673 (* 1 = 0.0415673 loss)
I1124 06:56:37.500090 35012 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1124 06:56:45.332200 35012 solver.cpp:218] Iteration 28300 (12.7686 iter/s, 7.8317s/100 iters), loss = 0.05391
I1124 06:56:45.332200 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:56:45.332200 35012 solver.cpp:237]     Train net output #1: loss = 0.0539099 (* 1 = 0.0539099 loss)
I1124 06:56:45.332200 35012 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1124 06:56:53.170768 35012 solver.cpp:218] Iteration 28400 (12.7588 iter/s, 7.83773s/100 iters), loss = 0.0430552
I1124 06:56:53.170768 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:56:53.170768 35012 solver.cpp:237]     Train net output #1: loss = 0.043055 (* 1 = 0.043055 loss)
I1124 06:56:53.170768 35012 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1124 06:57:00.617643 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:57:00.923840 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_28500.caffemodel
I1124 06:57:00.962823 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_28500.solverstate
I1124 06:57:00.981823 35012 solver.cpp:330] Iteration 28500, Testing net (#0)
I1124 06:57:00.981823 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:57:03.050295 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:57:03.133345 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9212
I1124 06:57:03.133345 35012 solver.cpp:397]     Test net output #1: loss = 0.239499 (* 1 = 0.239499 loss)
I1124 06:57:03.210366 35012 solver.cpp:218] Iteration 28500 (9.96116 iter/s, 10.039s/100 iters), loss = 0.0633645
I1124 06:57:03.210366 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 06:57:03.210366 35012 solver.cpp:237]     Train net output #1: loss = 0.0633643 (* 1 = 0.0633643 loss)
I1124 06:57:03.210366 35012 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1124 06:57:11.035746 35012 solver.cpp:218] Iteration 28600 (12.7792 iter/s, 7.82524s/100 iters), loss = 0.0729831
I1124 06:57:11.035746 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:57:11.035746 35012 solver.cpp:237]     Train net output #1: loss = 0.0729829 (* 1 = 0.0729829 loss)
I1124 06:57:11.035746 35012 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1124 06:57:18.866616 35012 solver.cpp:218] Iteration 28700 (12.7707 iter/s, 7.83042s/100 iters), loss = 0.0680792
I1124 06:57:18.866616 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:57:18.866616 35012 solver.cpp:237]     Train net output #1: loss = 0.068079 (* 1 = 0.068079 loss)
I1124 06:57:18.866616 35012 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1124 06:57:26.698081 35012 solver.cpp:218] Iteration 28800 (12.7703 iter/s, 7.83067s/100 iters), loss = 0.0599412
I1124 06:57:26.698567 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:57:26.698567 35012 solver.cpp:237]     Train net output #1: loss = 0.059941 (* 1 = 0.059941 loss)
I1124 06:57:26.698567 35012 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1124 06:57:34.530201 35012 solver.cpp:218] Iteration 28900 (12.7688 iter/s, 7.83159s/100 iters), loss = 0.0402465
I1124 06:57:34.530201 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:57:34.530692 35012 solver.cpp:237]     Train net output #1: loss = 0.0402463 (* 1 = 0.0402463 loss)
I1124 06:57:34.530692 35012 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1124 06:57:41.971405 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:57:42.280838 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_29000.caffemodel
I1124 06:57:42.318850 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_29000.solverstate
I1124 06:57:42.337867 35012 solver.cpp:330] Iteration 29000, Testing net (#0)
I1124 06:57:42.338347 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:57:44.406448 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:57:44.489488 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9211
I1124 06:57:44.489488 35012 solver.cpp:397]     Test net output #1: loss = 0.239522 (* 1 = 0.239522 loss)
I1124 06:57:44.566520 35012 solver.cpp:218] Iteration 29000 (9.96458 iter/s, 10.0355s/100 iters), loss = 0.0492081
I1124 06:57:44.566520 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:57:44.566520 35012 solver.cpp:237]     Train net output #1: loss = 0.0492079 (* 1 = 0.0492079 loss)
I1124 06:57:44.566520 35012 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1124 06:57:52.396216 35012 solver.cpp:218] Iteration 29100 (12.7714 iter/s, 7.83s/100 iters), loss = 0.0601577
I1124 06:57:52.397204 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:57:52.397204 35012 solver.cpp:237]     Train net output #1: loss = 0.0601575 (* 1 = 0.0601575 loss)
I1124 06:57:52.397204 35012 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1124 06:58:00.229264 35012 solver.cpp:218] Iteration 29200 (12.7674 iter/s, 7.83248s/100 iters), loss = 0.0463386
I1124 06:58:00.229264 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:58:00.229264 35012 solver.cpp:237]     Train net output #1: loss = 0.0463384 (* 1 = 0.0463384 loss)
I1124 06:58:00.229264 35012 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1124 06:58:08.061172 35012 solver.cpp:218] Iteration 29300 (12.7702 iter/s, 7.83071s/100 iters), loss = 0.0386242
I1124 06:58:08.061172 35012 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 06:58:08.061172 35012 solver.cpp:237]     Train net output #1: loss = 0.038624 (* 1 = 0.038624 loss)
I1124 06:58:08.061172 35012 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1124 06:58:15.891223 35012 solver.cpp:218] Iteration 29400 (12.7718 iter/s, 7.82976s/100 iters), loss = 0.047978
I1124 06:58:15.891223 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:58:15.891223 35012 solver.cpp:237]     Train net output #1: loss = 0.0479778 (* 1 = 0.0479778 loss)
I1124 06:58:15.891223 35012 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1124 06:58:23.340167 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:58:23.648993 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_29500.caffemodel
I1124 06:58:23.690052 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_29500.solverstate
I1124 06:58:23.709054 35012 solver.cpp:330] Iteration 29500, Testing net (#0)
I1124 06:58:23.709054 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:58:25.778367 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:58:25.861887 35012 solver.cpp:397]     Test net output #0: accuracy = 0.9212
I1124 06:58:25.861887 35012 solver.cpp:397]     Test net output #1: loss = 0.239433 (* 1 = 0.239433 loss)
I1124 06:58:25.938416 35012 solver.cpp:218] Iteration 29500 (9.9537 iter/s, 10.0465s/100 iters), loss = 0.0687431
I1124 06:58:25.938416 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:58:25.938416 35012 solver.cpp:237]     Train net output #1: loss = 0.0687429 (* 1 = 0.0687429 loss)
I1124 06:58:25.938416 35012 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1124 06:58:33.771561 35012 solver.cpp:218] Iteration 29600 (12.7671 iter/s, 7.83262s/100 iters), loss = 0.0647411
I1124 06:58:33.771561 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:58:33.771561 35012 solver.cpp:237]     Train net output #1: loss = 0.0647409 (* 1 = 0.0647409 loss)
I1124 06:58:33.771561 35012 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1124 06:58:41.600576 35012 solver.cpp:218] Iteration 29700 (12.7725 iter/s, 7.82933s/100 iters), loss = 0.0685666
I1124 06:58:41.600576 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:58:41.600576 35012 solver.cpp:237]     Train net output #1: loss = 0.0685664 (* 1 = 0.0685664 loss)
I1124 06:58:41.601563 35012 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1124 06:58:49.429872 35012 solver.cpp:218] Iteration 29800 (12.7747 iter/s, 7.82798s/100 iters), loss = 0.0403181
I1124 06:58:49.429872 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 06:58:49.429872 35012 solver.cpp:237]     Train net output #1: loss = 0.0403179 (* 1 = 0.0403179 loss)
I1124 06:58:49.429872 35012 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1124 06:58:57.255537 35012 solver.cpp:218] Iteration 29900 (12.779 iter/s, 7.82535s/100 iters), loss = 0.0474651
I1124 06:58:57.255537 35012 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 06:58:57.255537 35012 solver.cpp:237]     Train net output #1: loss = 0.0474649 (* 1 = 0.0474649 loss)
I1124 06:58:57.255537 35012 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1124 06:59:04.696753 24484 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:59:05.006775 35012 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_30000.caffemodel
I1124 06:59:05.045797 35012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_3x3_iter_30000.solverstate
I1124 06:59:05.089781 35012 solver.cpp:310] Iteration 30000, loss = 0.085171
I1124 06:59:05.089781 35012 solver.cpp:330] Iteration 30000, Testing net (#0)
I1124 06:59:05.089781 35012 net.cpp:676] Ignoring source layer accuracy_training
I1124 06:59:07.159901 15732 data_layer.cpp:73] Restarting data prefetching from start.
I1124 06:59:07.242924 35012 solver.cpp:397]     Test net output #0: accuracy = 0.921
I1124 06:59:07.242924 35012 solver.cpp:397]     Test net output #1: loss = 0.239497 (* 1 = 0.239497 loss)
I1124 06:59:07.242924 35012 solver.cpp:315] Optimization Done.
I1124 06:59:07.242924 35012 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 

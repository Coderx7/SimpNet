
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1124 08:58:22.004160 32468 caffe.cpp:219] Using GPUs 0
I1124 08:58:22.176213 32468 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1124 08:58:22.479991 32468 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1124 08:58:22.495995 32468 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_1.6k_8L_5x5"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1124 08:58:22.495995 32468 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1124 08:58:22.496995 32468 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1124 08:58:22.496995 32468 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1124 08:58:22.496995 32468 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1124 08:58:22.496995 32468 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1124 08:58:22.496995 32468 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1124 08:58:22.496995 32468 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1124 08:58:22.496995 32468 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1124 08:58:22.496995 32468 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1124 08:58:22.496995 32468 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1124 08:58:22.496995 32468 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1124 08:58:22.496995 32468 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1124 08:58:22.496995 32468 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1124 08:58:22.496995 32468 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_5x5_1.6k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 53
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 90
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 90
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 90
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 90
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 110
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 200
    pad: 2
    kernel_size: 5
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1124 08:58:22.508021 32468 layer_factory.cpp:58] Creating layer cifar
I1124 08:58:22.517007 32468 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1124 08:58:22.517007 32468 net.cpp:84] Creating Layer cifar
I1124 08:58:22.517007 32468 net.cpp:380] cifar -> data
I1124 08:58:22.517007 32468 net.cpp:380] cifar -> label
I1124 08:58:22.517997 32468 data_layer.cpp:45] output data size: 100,3,32,32
I1124 08:58:22.523998 32468 net.cpp:122] Setting up cifar
I1124 08:58:22.523998 32468 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1124 08:58:22.523998 32468 net.cpp:129] Top shape: 100 (100)
I1124 08:58:22.523998 32468 net.cpp:137] Memory required for data: 1229200
I1124 08:58:22.523998 32468 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1124 08:58:22.523998 32468 net.cpp:84] Creating Layer label_cifar_1_split
I1124 08:58:22.524996 32468 net.cpp:406] label_cifar_1_split <- label
I1124 08:58:22.524996 32468 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1124 08:58:22.524996 32468 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1124 08:58:22.524996 32468 net.cpp:122] Setting up label_cifar_1_split
I1124 08:58:22.524996 32468 net.cpp:129] Top shape: 100 (100)
I1124 08:58:22.524996 32468 net.cpp:129] Top shape: 100 (100)
I1124 08:58:22.524996 32468 net.cpp:137] Memory required for data: 1230000
I1124 08:58:22.524996 32468 layer_factory.cpp:58] Creating layer conv1
I1124 08:58:22.524996 32468 net.cpp:84] Creating Layer conv1
I1124 08:58:22.524996 32468 net.cpp:406] conv1 <- data
I1124 08:58:22.524996 32468 net.cpp:380] conv1 -> conv1
I1124 08:58:22.525996 26912 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1124 08:58:22.772464 32468 net.cpp:122] Setting up conv1
I1124 08:58:22.772464 32468 net.cpp:129] Top shape: 100 53 32 32 (5427200)
I1124 08:58:22.772464 32468 net.cpp:137] Memory required for data: 22938800
I1124 08:58:22.772464 32468 layer_factory.cpp:58] Creating layer bn1
I1124 08:58:22.772464 32468 net.cpp:84] Creating Layer bn1
I1124 08:58:22.772464 32468 net.cpp:406] bn1 <- conv1
I1124 08:58:22.772464 32468 net.cpp:367] bn1 -> conv1 (in-place)
I1124 08:58:22.772464 32468 net.cpp:122] Setting up bn1
I1124 08:58:22.772464 32468 net.cpp:129] Top shape: 100 53 32 32 (5427200)
I1124 08:58:22.772464 32468 net.cpp:137] Memory required for data: 44647600
I1124 08:58:22.772464 32468 layer_factory.cpp:58] Creating layer scale1
I1124 08:58:22.772464 32468 net.cpp:84] Creating Layer scale1
I1124 08:58:22.772464 32468 net.cpp:406] scale1 <- conv1
I1124 08:58:22.772464 32468 net.cpp:367] scale1 -> conv1 (in-place)
I1124 08:58:22.772464 32468 layer_factory.cpp:58] Creating layer scale1
I1124 08:58:22.772984 32468 net.cpp:122] Setting up scale1
I1124 08:58:22.772984 32468 net.cpp:129] Top shape: 100 53 32 32 (5427200)
I1124 08:58:22.772984 32468 net.cpp:137] Memory required for data: 66356400
I1124 08:58:22.772984 32468 layer_factory.cpp:58] Creating layer relu1
I1124 08:58:22.772984 32468 net.cpp:84] Creating Layer relu1
I1124 08:58:22.772984 32468 net.cpp:406] relu1 <- conv1
I1124 08:58:22.772984 32468 net.cpp:367] relu1 -> conv1 (in-place)
I1124 08:58:22.772984 32468 net.cpp:122] Setting up relu1
I1124 08:58:22.772984 32468 net.cpp:129] Top shape: 100 53 32 32 (5427200)
I1124 08:58:22.772984 32468 net.cpp:137] Memory required for data: 88065200
I1124 08:58:22.772984 32468 layer_factory.cpp:58] Creating layer conv2
I1124 08:58:22.772984 32468 net.cpp:84] Creating Layer conv2
I1124 08:58:22.772984 32468 net.cpp:406] conv2 <- conv1
I1124 08:58:22.772984 32468 net.cpp:380] conv2 -> conv2
I1124 08:58:22.774966 32468 net.cpp:122] Setting up conv2
I1124 08:58:22.774966 32468 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1124 08:58:22.774966 32468 net.cpp:137] Memory required for data: 110593200
I1124 08:58:22.774966 32468 layer_factory.cpp:58] Creating layer bn2
I1124 08:58:22.774966 32468 net.cpp:84] Creating Layer bn2
I1124 08:58:22.774966 32468 net.cpp:406] bn2 <- conv2
I1124 08:58:22.774966 32468 net.cpp:367] bn2 -> conv2 (in-place)
I1124 08:58:22.774966 32468 net.cpp:122] Setting up bn2
I1124 08:58:22.774966 32468 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1124 08:58:22.774966 32468 net.cpp:137] Memory required for data: 133121200
I1124 08:58:22.774966 32468 layer_factory.cpp:58] Creating layer scale2
I1124 08:58:22.774966 32468 net.cpp:84] Creating Layer scale2
I1124 08:58:22.774966 32468 net.cpp:406] scale2 <- conv2
I1124 08:58:22.774966 32468 net.cpp:367] scale2 -> conv2 (in-place)
I1124 08:58:22.774966 32468 layer_factory.cpp:58] Creating layer scale2
I1124 08:58:22.775465 32468 net.cpp:122] Setting up scale2
I1124 08:58:22.775465 32468 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1124 08:58:22.775465 32468 net.cpp:137] Memory required for data: 155649200
I1124 08:58:22.775465 32468 layer_factory.cpp:58] Creating layer relu2
I1124 08:58:22.775465 32468 net.cpp:84] Creating Layer relu2
I1124 08:58:22.775465 32468 net.cpp:406] relu2 <- conv2
I1124 08:58:22.775465 32468 net.cpp:367] relu2 -> conv2 (in-place)
I1124 08:58:22.775465 32468 net.cpp:122] Setting up relu2
I1124 08:58:22.775465 32468 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1124 08:58:22.775465 32468 net.cpp:137] Memory required for data: 178177200
I1124 08:58:22.775465 32468 layer_factory.cpp:58] Creating layer conv2_2
I1124 08:58:22.775465 32468 net.cpp:84] Creating Layer conv2_2
I1124 08:58:22.775465 32468 net.cpp:406] conv2_2 <- conv2
I1124 08:58:22.775465 32468 net.cpp:380] conv2_2 -> conv2_2
I1124 08:58:22.777467 32468 net.cpp:122] Setting up conv2_2
I1124 08:58:22.777467 32468 net.cpp:129] Top shape: 100 90 32 32 (9216000)
I1124 08:58:22.777467 32468 net.cpp:137] Memory required for data: 215041200
I1124 08:58:22.777467 32468 layer_factory.cpp:58] Creating layer bn2_2
I1124 08:58:22.777467 32468 net.cpp:84] Creating Layer bn2_2
I1124 08:58:22.777467 32468 net.cpp:406] bn2_2 <- conv2_2
I1124 08:58:22.777467 32468 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1124 08:58:22.777976 32468 net.cpp:122] Setting up bn2_2
I1124 08:58:22.777976 32468 net.cpp:129] Top shape: 100 90 32 32 (9216000)
I1124 08:58:22.777976 32468 net.cpp:137] Memory required for data: 251905200
I1124 08:58:22.777976 32468 layer_factory.cpp:58] Creating layer scale2_2
I1124 08:58:22.777976 32468 net.cpp:84] Creating Layer scale2_2
I1124 08:58:22.777976 32468 net.cpp:406] scale2_2 <- conv2_2
I1124 08:58:22.777976 32468 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1124 08:58:22.777976 32468 layer_factory.cpp:58] Creating layer scale2_2
I1124 08:58:22.777976 32468 net.cpp:122] Setting up scale2_2
I1124 08:58:22.777976 32468 net.cpp:129] Top shape: 100 90 32 32 (9216000)
I1124 08:58:22.777976 32468 net.cpp:137] Memory required for data: 288769200
I1124 08:58:22.777976 32468 layer_factory.cpp:58] Creating layer relu2_2
I1124 08:58:22.777976 32468 net.cpp:84] Creating Layer relu2_2
I1124 08:58:22.777976 32468 net.cpp:406] relu2_2 <- conv2_2
I1124 08:58:22.777976 32468 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1124 08:58:22.777976 32468 net.cpp:122] Setting up relu2_2
I1124 08:58:22.778465 32468 net.cpp:129] Top shape: 100 90 32 32 (9216000)
I1124 08:58:22.778465 32468 net.cpp:137] Memory required for data: 325633200
I1124 08:58:22.778465 32468 layer_factory.cpp:58] Creating layer pool2_1
I1124 08:58:22.778465 32468 net.cpp:84] Creating Layer pool2_1
I1124 08:58:22.778465 32468 net.cpp:406] pool2_1 <- conv2_2
I1124 08:58:22.778465 32468 net.cpp:380] pool2_1 -> pool2_1
I1124 08:58:22.778465 32468 net.cpp:122] Setting up pool2_1
I1124 08:58:22.778465 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.778465 32468 net.cpp:137] Memory required for data: 334849200
I1124 08:58:22.778465 32468 layer_factory.cpp:58] Creating layer conv3
I1124 08:58:22.778465 32468 net.cpp:84] Creating Layer conv3
I1124 08:58:22.778465 32468 net.cpp:406] conv3 <- pool2_1
I1124 08:58:22.778465 32468 net.cpp:380] conv3 -> conv3
I1124 08:58:22.781464 32468 net.cpp:122] Setting up conv3
I1124 08:58:22.781464 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.781464 32468 net.cpp:137] Memory required for data: 344065200
I1124 08:58:22.781464 32468 layer_factory.cpp:58] Creating layer bn3
I1124 08:58:22.781464 32468 net.cpp:84] Creating Layer bn3
I1124 08:58:22.781464 32468 net.cpp:406] bn3 <- conv3
I1124 08:58:22.781464 32468 net.cpp:367] bn3 -> conv3 (in-place)
I1124 08:58:22.781464 32468 net.cpp:122] Setting up bn3
I1124 08:58:22.781464 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.781464 32468 net.cpp:137] Memory required for data: 353281200
I1124 08:58:22.781965 32468 layer_factory.cpp:58] Creating layer scale3
I1124 08:58:22.781965 32468 net.cpp:84] Creating Layer scale3
I1124 08:58:22.781965 32468 net.cpp:406] scale3 <- conv3
I1124 08:58:22.781965 32468 net.cpp:367] scale3 -> conv3 (in-place)
I1124 08:58:22.781965 32468 layer_factory.cpp:58] Creating layer scale3
I1124 08:58:22.781965 32468 net.cpp:122] Setting up scale3
I1124 08:58:22.781965 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.781965 32468 net.cpp:137] Memory required for data: 362497200
I1124 08:58:22.781965 32468 layer_factory.cpp:58] Creating layer relu3
I1124 08:58:22.781965 32468 net.cpp:84] Creating Layer relu3
I1124 08:58:22.781965 32468 net.cpp:406] relu3 <- conv3
I1124 08:58:22.781965 32468 net.cpp:367] relu3 -> conv3 (in-place)
I1124 08:58:22.782465 32468 net.cpp:122] Setting up relu3
I1124 08:58:22.782465 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.782465 32468 net.cpp:137] Memory required for data: 371713200
I1124 08:58:22.782465 32468 layer_factory.cpp:58] Creating layer conv4
I1124 08:58:22.782465 32468 net.cpp:84] Creating Layer conv4
I1124 08:58:22.782465 32468 net.cpp:406] conv4 <- conv3
I1124 08:58:22.782465 32468 net.cpp:380] conv4 -> conv4
I1124 08:58:22.785465 32468 net.cpp:122] Setting up conv4
I1124 08:58:22.785964 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.785964 32468 net.cpp:137] Memory required for data: 380929200
I1124 08:58:22.785964 32468 layer_factory.cpp:58] Creating layer bn4
I1124 08:58:22.785964 32468 net.cpp:84] Creating Layer bn4
I1124 08:58:22.785964 32468 net.cpp:406] bn4 <- conv4
I1124 08:58:22.785964 32468 net.cpp:367] bn4 -> conv4 (in-place)
I1124 08:58:22.785964 32468 net.cpp:122] Setting up bn4
I1124 08:58:22.785964 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.785964 32468 net.cpp:137] Memory required for data: 390145200
I1124 08:58:22.785964 32468 layer_factory.cpp:58] Creating layer scale4
I1124 08:58:22.785964 32468 net.cpp:84] Creating Layer scale4
I1124 08:58:22.785964 32468 net.cpp:406] scale4 <- conv4
I1124 08:58:22.785964 32468 net.cpp:367] scale4 -> conv4 (in-place)
I1124 08:58:22.785964 32468 layer_factory.cpp:58] Creating layer scale4
I1124 08:58:22.786464 32468 net.cpp:122] Setting up scale4
I1124 08:58:22.786464 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.786464 32468 net.cpp:137] Memory required for data: 399361200
I1124 08:58:22.786464 32468 layer_factory.cpp:58] Creating layer relu4
I1124 08:58:22.786464 32468 net.cpp:84] Creating Layer relu4
I1124 08:58:22.786464 32468 net.cpp:406] relu4 <- conv4
I1124 08:58:22.786464 32468 net.cpp:367] relu4 -> conv4 (in-place)
I1124 08:58:22.786964 32468 net.cpp:122] Setting up relu4
I1124 08:58:22.786964 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.786964 32468 net.cpp:137] Memory required for data: 408577200
I1124 08:58:22.786964 32468 layer_factory.cpp:58] Creating layer conv4_1
I1124 08:58:22.786964 32468 net.cpp:84] Creating Layer conv4_1
I1124 08:58:22.786964 32468 net.cpp:406] conv4_1 <- conv4
I1124 08:58:22.786964 32468 net.cpp:380] conv4_1 -> conv4_1
I1124 08:58:22.788985 32468 net.cpp:122] Setting up conv4_1
I1124 08:58:22.789484 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.789484 32468 net.cpp:137] Memory required for data: 417793200
I1124 08:58:22.789484 32468 layer_factory.cpp:58] Creating layer bn4_1
I1124 08:58:22.789484 32468 net.cpp:84] Creating Layer bn4_1
I1124 08:58:22.789484 32468 net.cpp:406] bn4_1 <- conv4_1
I1124 08:58:22.789484 32468 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1124 08:58:22.789484 32468 net.cpp:122] Setting up bn4_1
I1124 08:58:22.789484 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.789484 32468 net.cpp:137] Memory required for data: 427009200
I1124 08:58:22.789484 32468 layer_factory.cpp:58] Creating layer scale4_1
I1124 08:58:22.789484 32468 net.cpp:84] Creating Layer scale4_1
I1124 08:58:22.789484 32468 net.cpp:406] scale4_1 <- conv4_1
I1124 08:58:22.789484 32468 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1124 08:58:22.789484 32468 layer_factory.cpp:58] Creating layer scale4_1
I1124 08:58:22.789484 32468 net.cpp:122] Setting up scale4_1
I1124 08:58:22.789484 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.789484 32468 net.cpp:137] Memory required for data: 436225200
I1124 08:58:22.789484 32468 layer_factory.cpp:58] Creating layer relu4_1
I1124 08:58:22.789484 32468 net.cpp:84] Creating Layer relu4_1
I1124 08:58:22.789484 32468 net.cpp:406] relu4_1 <- conv4_1
I1124 08:58:22.789484 32468 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1124 08:58:22.790467 32468 net.cpp:122] Setting up relu4_1
I1124 08:58:22.790467 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.790467 32468 net.cpp:137] Memory required for data: 445441200
I1124 08:58:22.790467 32468 layer_factory.cpp:58] Creating layer conv4_2
I1124 08:58:22.790467 32468 net.cpp:84] Creating Layer conv4_2
I1124 08:58:22.790467 32468 net.cpp:406] conv4_2 <- conv4_1
I1124 08:58:22.790467 32468 net.cpp:380] conv4_2 -> conv4_2
I1124 08:58:22.793964 32468 net.cpp:122] Setting up conv4_2
I1124 08:58:22.793964 32468 net.cpp:129] Top shape: 100 110 16 16 (2816000)
I1124 08:58:22.793964 32468 net.cpp:137] Memory required for data: 456705200
I1124 08:58:22.793964 32468 layer_factory.cpp:58] Creating layer bn4_2
I1124 08:58:22.793964 32468 net.cpp:84] Creating Layer bn4_2
I1124 08:58:22.793964 32468 net.cpp:406] bn4_2 <- conv4_2
I1124 08:58:22.793964 32468 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1124 08:58:22.793964 32468 net.cpp:122] Setting up bn4_2
I1124 08:58:22.793964 32468 net.cpp:129] Top shape: 100 110 16 16 (2816000)
I1124 08:58:22.793964 32468 net.cpp:137] Memory required for data: 467969200
I1124 08:58:22.793964 32468 layer_factory.cpp:58] Creating layer scale4_2
I1124 08:58:22.793964 32468 net.cpp:84] Creating Layer scale4_2
I1124 08:58:22.793964 32468 net.cpp:406] scale4_2 <- conv4_2
I1124 08:58:22.793964 32468 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1124 08:58:22.794464 32468 layer_factory.cpp:58] Creating layer scale4_2
I1124 08:58:22.794464 32468 net.cpp:122] Setting up scale4_2
I1124 08:58:22.794464 32468 net.cpp:129] Top shape: 100 110 16 16 (2816000)
I1124 08:58:22.794464 32468 net.cpp:137] Memory required for data: 479233200
I1124 08:58:22.794464 32468 layer_factory.cpp:58] Creating layer relu4_2
I1124 08:58:22.794464 32468 net.cpp:84] Creating Layer relu4_2
I1124 08:58:22.794464 32468 net.cpp:406] relu4_2 <- conv4_2
I1124 08:58:22.794464 32468 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1124 08:58:22.794965 32468 net.cpp:122] Setting up relu4_2
I1124 08:58:22.794965 32468 net.cpp:129] Top shape: 100 110 16 16 (2816000)
I1124 08:58:22.794965 32468 net.cpp:137] Memory required for data: 490497200
I1124 08:58:22.794965 32468 layer_factory.cpp:58] Creating layer pool4_2
I1124 08:58:22.794965 32468 net.cpp:84] Creating Layer pool4_2
I1124 08:58:22.794965 32468 net.cpp:406] pool4_2 <- conv4_2
I1124 08:58:22.794965 32468 net.cpp:380] pool4_2 -> pool4_2
I1124 08:58:22.794965 32468 net.cpp:122] Setting up pool4_2
I1124 08:58:22.794965 32468 net.cpp:129] Top shape: 100 110 8 8 (704000)
I1124 08:58:22.794965 32468 net.cpp:137] Memory required for data: 493313200
I1124 08:58:22.794965 32468 layer_factory.cpp:58] Creating layer conv12
I1124 08:58:22.794965 32468 net.cpp:84] Creating Layer conv12
I1124 08:58:22.794965 32468 net.cpp:406] conv12 <- pool4_2
I1124 08:58:22.794965 32468 net.cpp:380] conv12 -> conv12
I1124 08:58:22.799964 32468 net.cpp:122] Setting up conv12
I1124 08:58:22.799964 32468 net.cpp:129] Top shape: 100 200 8 8 (1280000)
I1124 08:58:22.799964 32468 net.cpp:137] Memory required for data: 498433200
I1124 08:58:22.800475 32468 layer_factory.cpp:58] Creating layer bn_conv12
I1124 08:58:22.800475 32468 net.cpp:84] Creating Layer bn_conv12
I1124 08:58:22.800475 32468 net.cpp:406] bn_conv12 <- conv12
I1124 08:58:22.800475 32468 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1124 08:58:22.800475 32468 net.cpp:122] Setting up bn_conv12
I1124 08:58:22.800475 32468 net.cpp:129] Top shape: 100 200 8 8 (1280000)
I1124 08:58:22.800475 32468 net.cpp:137] Memory required for data: 503553200
I1124 08:58:22.800475 32468 layer_factory.cpp:58] Creating layer scale_conv12
I1124 08:58:22.800475 32468 net.cpp:84] Creating Layer scale_conv12
I1124 08:58:22.800475 32468 net.cpp:406] scale_conv12 <- conv12
I1124 08:58:22.800475 32468 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1124 08:58:22.800475 32468 layer_factory.cpp:58] Creating layer scale_conv12
I1124 08:58:22.800475 32468 net.cpp:122] Setting up scale_conv12
I1124 08:58:22.800475 32468 net.cpp:129] Top shape: 100 200 8 8 (1280000)
I1124 08:58:22.800475 32468 net.cpp:137] Memory required for data: 508673200
I1124 08:58:22.800475 32468 layer_factory.cpp:58] Creating layer relu_conv12
I1124 08:58:22.800974 32468 net.cpp:84] Creating Layer relu_conv12
I1124 08:58:22.800974 32468 net.cpp:406] relu_conv12 <- conv12
I1124 08:58:22.800974 32468 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1124 08:58:22.800974 32468 net.cpp:122] Setting up relu_conv12
I1124 08:58:22.800974 32468 net.cpp:129] Top shape: 100 200 8 8 (1280000)
I1124 08:58:22.800974 32468 net.cpp:137] Memory required for data: 513793200
I1124 08:58:22.800974 32468 layer_factory.cpp:58] Creating layer poolcp6
I1124 08:58:22.800974 32468 net.cpp:84] Creating Layer poolcp6
I1124 08:58:22.800974 32468 net.cpp:406] poolcp6 <- conv12
I1124 08:58:22.800974 32468 net.cpp:380] poolcp6 -> poolcp6
I1124 08:58:22.800974 32468 net.cpp:122] Setting up poolcp6
I1124 08:58:22.800974 32468 net.cpp:129] Top shape: 100 200 1 1 (20000)
I1124 08:58:22.800974 32468 net.cpp:137] Memory required for data: 513873200
I1124 08:58:22.800974 32468 layer_factory.cpp:58] Creating layer ip1
I1124 08:58:22.800974 32468 net.cpp:84] Creating Layer ip1
I1124 08:58:22.800974 32468 net.cpp:406] ip1 <- poolcp6
I1124 08:58:22.800974 32468 net.cpp:380] ip1 -> ip1
I1124 08:58:22.801475 32468 net.cpp:122] Setting up ip1
I1124 08:58:22.801475 32468 net.cpp:129] Top shape: 100 10 (1000)
I1124 08:58:22.801475 32468 net.cpp:137] Memory required for data: 513877200
I1124 08:58:22.801475 32468 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1124 08:58:22.801475 32468 net.cpp:84] Creating Layer ip1_ip1_0_split
I1124 08:58:22.801475 32468 net.cpp:406] ip1_ip1_0_split <- ip1
I1124 08:58:22.801475 32468 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1124 08:58:22.801475 32468 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1124 08:58:22.801475 32468 net.cpp:122] Setting up ip1_ip1_0_split
I1124 08:58:22.801475 32468 net.cpp:129] Top shape: 100 10 (1000)
I1124 08:58:22.801475 32468 net.cpp:129] Top shape: 100 10 (1000)
I1124 08:58:22.801475 32468 net.cpp:137] Memory required for data: 513885200
I1124 08:58:22.801475 32468 layer_factory.cpp:58] Creating layer accuracy_training
I1124 08:58:22.801475 32468 net.cpp:84] Creating Layer accuracy_training
I1124 08:58:22.801475 32468 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1124 08:58:22.801475 32468 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1124 08:58:22.801475 32468 net.cpp:380] accuracy_training -> accuracy_training
I1124 08:58:22.801475 32468 net.cpp:122] Setting up accuracy_training
I1124 08:58:22.801475 32468 net.cpp:129] Top shape: (1)
I1124 08:58:22.801475 32468 net.cpp:137] Memory required for data: 513885204
I1124 08:58:22.801475 32468 layer_factory.cpp:58] Creating layer loss
I1124 08:58:22.801475 32468 net.cpp:84] Creating Layer loss
I1124 08:58:22.801475 32468 net.cpp:406] loss <- ip1_ip1_0_split_1
I1124 08:58:22.801475 32468 net.cpp:406] loss <- label_cifar_1_split_1
I1124 08:58:22.801475 32468 net.cpp:380] loss -> loss
I1124 08:58:22.801475 32468 layer_factory.cpp:58] Creating layer loss
I1124 08:58:22.801965 32468 net.cpp:122] Setting up loss
I1124 08:58:22.801965 32468 net.cpp:129] Top shape: (1)
I1124 08:58:22.801965 32468 net.cpp:132]     with loss weight 1
I1124 08:58:22.801965 32468 net.cpp:137] Memory required for data: 513885208
I1124 08:58:22.801965 32468 net.cpp:198] loss needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:200] accuracy_training does not need backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] ip1 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] poolcp6 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] relu_conv12 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] scale_conv12 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] bn_conv12 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] conv12 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] pool4_2 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] relu4_2 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] scale4_2 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] bn4_2 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] conv4_2 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] relu4_1 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] scale4_1 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] bn4_1 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] conv4_1 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] relu4 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] scale4 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] bn4 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] conv4 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] relu3 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] scale3 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] bn3 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] conv3 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] pool2_1 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] relu2_2 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] scale2_2 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] bn2_2 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] conv2_2 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] relu2 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] scale2 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] bn2 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] conv2 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] relu1 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] scale1 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] bn1 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:198] conv1 needs backward computation.
I1124 08:58:22.801965 32468 net.cpp:200] label_cifar_1_split does not need backward computation.
I1124 08:58:22.801965 32468 net.cpp:200] cifar does not need backward computation.
I1124 08:58:22.801965 32468 net.cpp:242] This network produces output accuracy_training
I1124 08:58:22.801965 32468 net.cpp:242] This network produces output loss
I1124 08:58:22.801965 32468 net.cpp:255] Network initialization done.
I1124 08:58:22.802475 32468 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1124 08:58:22.802475 32468 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1124 08:58:22.802475 32468 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1124 08:58:22.802975 32468 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1124 08:58:22.802975 32468 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1124 08:58:22.802975 32468 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1124 08:58:22.802975 32468 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1124 08:58:22.802975 32468 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1124 08:58:22.802975 32468 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1124 08:58:22.802975 32468 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1124 08:58:22.802975 32468 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1124 08:58:22.802975 32468 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1124 08:58:22.802975 32468 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1124 08:58:22.802975 32468 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_5x5_1.6k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 53
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 90
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 90
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 90
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 90
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 110
    bias_term: true
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 200
    pad: 2
    kernel_size: 5
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1124 08:58:22.803467 32468 layer_factory.cpp:58] Creating layer cifar
I1124 08:58:22.883464 32468 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1124 08:58:22.883464 32468 net.cpp:84] Creating Layer cifar
I1124 08:58:22.883464 32468 net.cpp:380] cifar -> data
I1124 08:58:22.883464 32468 net.cpp:380] cifar -> label
I1124 08:58:22.883464 32468 data_layer.cpp:45] output data size: 100,3,32,32
I1124 08:58:22.888464 32468 net.cpp:122] Setting up cifar
I1124 08:58:22.888464 32468 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1124 08:58:22.888964 32468 net.cpp:129] Top shape: 100 (100)
I1124 08:58:22.888964 32468 net.cpp:137] Memory required for data: 1229200
I1124 08:58:22.888964 32468 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1124 08:58:22.888964 32468 net.cpp:84] Creating Layer label_cifar_1_split
I1124 08:58:22.888964 32468 net.cpp:406] label_cifar_1_split <- label
I1124 08:58:22.888964 32468 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1124 08:58:22.888964 32468 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1124 08:58:22.888964 32468 net.cpp:122] Setting up label_cifar_1_split
I1124 08:58:22.888964 32468 net.cpp:129] Top shape: 100 (100)
I1124 08:58:22.888964 32468 net.cpp:129] Top shape: 100 (100)
I1124 08:58:22.888964 32468 net.cpp:137] Memory required for data: 1230000
I1124 08:58:22.888964 32468 layer_factory.cpp:58] Creating layer conv1
I1124 08:58:22.888964 32468 net.cpp:84] Creating Layer conv1
I1124 08:58:22.888964 32468 net.cpp:406] conv1 <- data
I1124 08:58:22.888964 32468 net.cpp:380] conv1 -> conv1
I1124 08:58:22.889963 29896 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1124 08:58:22.890465 32468 net.cpp:122] Setting up conv1
I1124 08:58:22.890465 32468 net.cpp:129] Top shape: 100 53 32 32 (5427200)
I1124 08:58:22.890465 32468 net.cpp:137] Memory required for data: 22938800
I1124 08:58:22.890465 32468 layer_factory.cpp:58] Creating layer bn1
I1124 08:58:22.890465 32468 net.cpp:84] Creating Layer bn1
I1124 08:58:22.890465 32468 net.cpp:406] bn1 <- conv1
I1124 08:58:22.890465 32468 net.cpp:367] bn1 -> conv1 (in-place)
I1124 08:58:22.890964 32468 net.cpp:122] Setting up bn1
I1124 08:58:22.890964 32468 net.cpp:129] Top shape: 100 53 32 32 (5427200)
I1124 08:58:22.890964 32468 net.cpp:137] Memory required for data: 44647600
I1124 08:58:22.890964 32468 layer_factory.cpp:58] Creating layer scale1
I1124 08:58:22.890964 32468 net.cpp:84] Creating Layer scale1
I1124 08:58:22.890964 32468 net.cpp:406] scale1 <- conv1
I1124 08:58:22.890964 32468 net.cpp:367] scale1 -> conv1 (in-place)
I1124 08:58:22.890964 32468 layer_factory.cpp:58] Creating layer scale1
I1124 08:58:22.890964 32468 net.cpp:122] Setting up scale1
I1124 08:58:22.890964 32468 net.cpp:129] Top shape: 100 53 32 32 (5427200)
I1124 08:58:22.890964 32468 net.cpp:137] Memory required for data: 66356400
I1124 08:58:22.890964 32468 layer_factory.cpp:58] Creating layer relu1
I1124 08:58:22.890964 32468 net.cpp:84] Creating Layer relu1
I1124 08:58:22.890964 32468 net.cpp:406] relu1 <- conv1
I1124 08:58:22.890964 32468 net.cpp:367] relu1 -> conv1 (in-place)
I1124 08:58:22.891463 32468 net.cpp:122] Setting up relu1
I1124 08:58:22.891463 32468 net.cpp:129] Top shape: 100 53 32 32 (5427200)
I1124 08:58:22.891463 32468 net.cpp:137] Memory required for data: 88065200
I1124 08:58:22.891463 32468 layer_factory.cpp:58] Creating layer conv2
I1124 08:58:22.891463 32468 net.cpp:84] Creating Layer conv2
I1124 08:58:22.891463 32468 net.cpp:406] conv2 <- conv1
I1124 08:58:22.891463 32468 net.cpp:380] conv2 -> conv2
I1124 08:58:22.892963 32468 net.cpp:122] Setting up conv2
I1124 08:58:22.892963 32468 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1124 08:58:22.892963 32468 net.cpp:137] Memory required for data: 110593200
I1124 08:58:22.892963 32468 layer_factory.cpp:58] Creating layer bn2
I1124 08:58:22.892963 32468 net.cpp:84] Creating Layer bn2
I1124 08:58:22.892963 32468 net.cpp:406] bn2 <- conv2
I1124 08:58:22.892963 32468 net.cpp:367] bn2 -> conv2 (in-place)
I1124 08:58:22.893465 32468 net.cpp:122] Setting up bn2
I1124 08:58:22.893465 32468 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1124 08:58:22.893465 32468 net.cpp:137] Memory required for data: 133121200
I1124 08:58:22.893465 32468 layer_factory.cpp:58] Creating layer scale2
I1124 08:58:22.893465 32468 net.cpp:84] Creating Layer scale2
I1124 08:58:22.893465 32468 net.cpp:406] scale2 <- conv2
I1124 08:58:22.893465 32468 net.cpp:367] scale2 -> conv2 (in-place)
I1124 08:58:22.893465 32468 layer_factory.cpp:58] Creating layer scale2
I1124 08:58:22.893465 32468 net.cpp:122] Setting up scale2
I1124 08:58:22.893465 32468 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1124 08:58:22.893465 32468 net.cpp:137] Memory required for data: 155649200
I1124 08:58:22.893465 32468 layer_factory.cpp:58] Creating layer relu2
I1124 08:58:22.893465 32468 net.cpp:84] Creating Layer relu2
I1124 08:58:22.893465 32468 net.cpp:406] relu2 <- conv2
I1124 08:58:22.893465 32468 net.cpp:367] relu2 -> conv2 (in-place)
I1124 08:58:22.894464 32468 net.cpp:122] Setting up relu2
I1124 08:58:22.894464 32468 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1124 08:58:22.894464 32468 net.cpp:137] Memory required for data: 178177200
I1124 08:58:22.894464 32468 layer_factory.cpp:58] Creating layer conv2_2
I1124 08:58:22.894464 32468 net.cpp:84] Creating Layer conv2_2
I1124 08:58:22.894464 32468 net.cpp:406] conv2_2 <- conv2
I1124 08:58:22.894464 32468 net.cpp:380] conv2_2 -> conv2_2
I1124 08:58:22.896464 32468 net.cpp:122] Setting up conv2_2
I1124 08:58:22.896464 32468 net.cpp:129] Top shape: 100 90 32 32 (9216000)
I1124 08:58:22.896464 32468 net.cpp:137] Memory required for data: 215041200
I1124 08:58:22.896464 32468 layer_factory.cpp:58] Creating layer bn2_2
I1124 08:58:22.896464 32468 net.cpp:84] Creating Layer bn2_2
I1124 08:58:22.896464 32468 net.cpp:406] bn2_2 <- conv2_2
I1124 08:58:22.896464 32468 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1124 08:58:22.896965 32468 net.cpp:122] Setting up bn2_2
I1124 08:58:22.896965 32468 net.cpp:129] Top shape: 100 90 32 32 (9216000)
I1124 08:58:22.896965 32468 net.cpp:137] Memory required for data: 251905200
I1124 08:58:22.896965 32468 layer_factory.cpp:58] Creating layer scale2_2
I1124 08:58:22.896965 32468 net.cpp:84] Creating Layer scale2_2
I1124 08:58:22.896965 32468 net.cpp:406] scale2_2 <- conv2_2
I1124 08:58:22.896965 32468 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1124 08:58:22.896965 32468 layer_factory.cpp:58] Creating layer scale2_2
I1124 08:58:22.896965 32468 net.cpp:122] Setting up scale2_2
I1124 08:58:22.896965 32468 net.cpp:129] Top shape: 100 90 32 32 (9216000)
I1124 08:58:22.896965 32468 net.cpp:137] Memory required for data: 288769200
I1124 08:58:22.896965 32468 layer_factory.cpp:58] Creating layer relu2_2
I1124 08:58:22.896965 32468 net.cpp:84] Creating Layer relu2_2
I1124 08:58:22.896965 32468 net.cpp:406] relu2_2 <- conv2_2
I1124 08:58:22.896965 32468 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1124 08:58:22.897464 32468 net.cpp:122] Setting up relu2_2
I1124 08:58:22.897464 32468 net.cpp:129] Top shape: 100 90 32 32 (9216000)
I1124 08:58:22.897464 32468 net.cpp:137] Memory required for data: 325633200
I1124 08:58:22.897464 32468 layer_factory.cpp:58] Creating layer pool2_1
I1124 08:58:22.897464 32468 net.cpp:84] Creating Layer pool2_1
I1124 08:58:22.897464 32468 net.cpp:406] pool2_1 <- conv2_2
I1124 08:58:22.897464 32468 net.cpp:380] pool2_1 -> pool2_1
I1124 08:58:22.897464 32468 net.cpp:122] Setting up pool2_1
I1124 08:58:22.897464 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.897464 32468 net.cpp:137] Memory required for data: 334849200
I1124 08:58:22.897464 32468 layer_factory.cpp:58] Creating layer conv3
I1124 08:58:22.897464 32468 net.cpp:84] Creating Layer conv3
I1124 08:58:22.897464 32468 net.cpp:406] conv3 <- pool2_1
I1124 08:58:22.897464 32468 net.cpp:380] conv3 -> conv3
I1124 08:58:22.900465 32468 net.cpp:122] Setting up conv3
I1124 08:58:22.900465 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.900465 32468 net.cpp:137] Memory required for data: 344065200
I1124 08:58:22.900465 32468 layer_factory.cpp:58] Creating layer bn3
I1124 08:58:22.900465 32468 net.cpp:84] Creating Layer bn3
I1124 08:58:22.900465 32468 net.cpp:406] bn3 <- conv3
I1124 08:58:22.900465 32468 net.cpp:367] bn3 -> conv3 (in-place)
I1124 08:58:22.900965 32468 net.cpp:122] Setting up bn3
I1124 08:58:22.900965 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.900965 32468 net.cpp:137] Memory required for data: 353281200
I1124 08:58:22.900965 32468 layer_factory.cpp:58] Creating layer scale3
I1124 08:58:22.900965 32468 net.cpp:84] Creating Layer scale3
I1124 08:58:22.900965 32468 net.cpp:406] scale3 <- conv3
I1124 08:58:22.900965 32468 net.cpp:367] scale3 -> conv3 (in-place)
I1124 08:58:22.900965 32468 layer_factory.cpp:58] Creating layer scale3
I1124 08:58:22.900965 32468 net.cpp:122] Setting up scale3
I1124 08:58:22.900965 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.900965 32468 net.cpp:137] Memory required for data: 362497200
I1124 08:58:22.900965 32468 layer_factory.cpp:58] Creating layer relu3
I1124 08:58:22.900965 32468 net.cpp:84] Creating Layer relu3
I1124 08:58:22.900965 32468 net.cpp:406] relu3 <- conv3
I1124 08:58:22.900965 32468 net.cpp:367] relu3 -> conv3 (in-place)
I1124 08:58:22.901465 32468 net.cpp:122] Setting up relu3
I1124 08:58:22.901465 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.901465 32468 net.cpp:137] Memory required for data: 371713200
I1124 08:58:22.901465 32468 layer_factory.cpp:58] Creating layer conv4
I1124 08:58:22.901465 32468 net.cpp:84] Creating Layer conv4
I1124 08:58:22.901465 32468 net.cpp:406] conv4 <- conv3
I1124 08:58:22.901465 32468 net.cpp:380] conv4 -> conv4
I1124 08:58:22.905467 32468 net.cpp:122] Setting up conv4
I1124 08:58:22.905967 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.905967 32468 net.cpp:137] Memory required for data: 380929200
I1124 08:58:22.905967 32468 layer_factory.cpp:58] Creating layer bn4
I1124 08:58:22.905967 32468 net.cpp:84] Creating Layer bn4
I1124 08:58:22.905967 32468 net.cpp:406] bn4 <- conv4
I1124 08:58:22.905967 32468 net.cpp:367] bn4 -> conv4 (in-place)
I1124 08:58:22.905967 32468 net.cpp:122] Setting up bn4
I1124 08:58:22.905967 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.905967 32468 net.cpp:137] Memory required for data: 390145200
I1124 08:58:22.905967 32468 layer_factory.cpp:58] Creating layer scale4
I1124 08:58:22.906469 32468 net.cpp:84] Creating Layer scale4
I1124 08:58:22.906469 32468 net.cpp:406] scale4 <- conv4
I1124 08:58:22.906469 32468 net.cpp:367] scale4 -> conv4 (in-place)
I1124 08:58:22.906469 32468 layer_factory.cpp:58] Creating layer scale4
I1124 08:58:22.906469 32468 net.cpp:122] Setting up scale4
I1124 08:58:22.906469 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.906469 32468 net.cpp:137] Memory required for data: 399361200
I1124 08:58:22.906469 32468 layer_factory.cpp:58] Creating layer relu4
I1124 08:58:22.906469 32468 net.cpp:84] Creating Layer relu4
I1124 08:58:22.906469 32468 net.cpp:406] relu4 <- conv4
I1124 08:58:22.906469 32468 net.cpp:367] relu4 -> conv4 (in-place)
I1124 08:58:22.906968 32468 net.cpp:122] Setting up relu4
I1124 08:58:22.906968 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.906968 32468 net.cpp:137] Memory required for data: 408577200
I1124 08:58:22.906968 32468 layer_factory.cpp:58] Creating layer conv4_1
I1124 08:58:22.906968 32468 net.cpp:84] Creating Layer conv4_1
I1124 08:58:22.906968 32468 net.cpp:406] conv4_1 <- conv4
I1124 08:58:22.906968 32468 net.cpp:380] conv4_1 -> conv4_1
I1124 08:58:22.910526 32468 net.cpp:122] Setting up conv4_1
I1124 08:58:22.910526 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.910526 32468 net.cpp:137] Memory required for data: 417793200
I1124 08:58:22.910526 32468 layer_factory.cpp:58] Creating layer bn4_1
I1124 08:58:22.910526 32468 net.cpp:84] Creating Layer bn4_1
I1124 08:58:22.910526 32468 net.cpp:406] bn4_1 <- conv4_1
I1124 08:58:22.910526 32468 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1124 08:58:22.911525 32468 net.cpp:122] Setting up bn4_1
I1124 08:58:22.911525 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.911525 32468 net.cpp:137] Memory required for data: 427009200
I1124 08:58:22.911525 32468 layer_factory.cpp:58] Creating layer scale4_1
I1124 08:58:22.911525 32468 net.cpp:84] Creating Layer scale4_1
I1124 08:58:22.911525 32468 net.cpp:406] scale4_1 <- conv4_1
I1124 08:58:22.911525 32468 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1124 08:58:22.911525 32468 layer_factory.cpp:58] Creating layer scale4_1
I1124 08:58:22.911525 32468 net.cpp:122] Setting up scale4_1
I1124 08:58:22.911525 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.911525 32468 net.cpp:137] Memory required for data: 436225200
I1124 08:58:22.911525 32468 layer_factory.cpp:58] Creating layer relu4_1
I1124 08:58:22.911525 32468 net.cpp:84] Creating Layer relu4_1
I1124 08:58:22.911525 32468 net.cpp:406] relu4_1 <- conv4_1
I1124 08:58:22.911525 32468 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1124 08:58:22.912526 32468 net.cpp:122] Setting up relu4_1
I1124 08:58:22.912526 32468 net.cpp:129] Top shape: 100 90 16 16 (2304000)
I1124 08:58:22.912526 32468 net.cpp:137] Memory required for data: 445441200
I1124 08:58:22.912526 32468 layer_factory.cpp:58] Creating layer conv4_2
I1124 08:58:22.912526 32468 net.cpp:84] Creating Layer conv4_2
I1124 08:58:22.912526 32468 net.cpp:406] conv4_2 <- conv4_1
I1124 08:58:22.912526 32468 net.cpp:380] conv4_2 -> conv4_2
I1124 08:58:22.917521 32468 net.cpp:122] Setting up conv4_2
I1124 08:58:22.917521 32468 net.cpp:129] Top shape: 100 110 16 16 (2816000)
I1124 08:58:22.917521 32468 net.cpp:137] Memory required for data: 456705200
I1124 08:58:22.917521 32468 layer_factory.cpp:58] Creating layer bn4_2
I1124 08:58:22.917521 32468 net.cpp:84] Creating Layer bn4_2
I1124 08:58:22.917521 32468 net.cpp:406] bn4_2 <- conv4_2
I1124 08:58:22.917521 32468 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1124 08:58:22.917521 32468 net.cpp:122] Setting up bn4_2
I1124 08:58:22.917521 32468 net.cpp:129] Top shape: 100 110 16 16 (2816000)
I1124 08:58:22.917521 32468 net.cpp:137] Memory required for data: 467969200
I1124 08:58:22.917521 32468 layer_factory.cpp:58] Creating layer scale4_2
I1124 08:58:22.917521 32468 net.cpp:84] Creating Layer scale4_2
I1124 08:58:22.917521 32468 net.cpp:406] scale4_2 <- conv4_2
I1124 08:58:22.918524 32468 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1124 08:58:22.918524 32468 layer_factory.cpp:58] Creating layer scale4_2
I1124 08:58:22.918524 32468 net.cpp:122] Setting up scale4_2
I1124 08:58:22.918524 32468 net.cpp:129] Top shape: 100 110 16 16 (2816000)
I1124 08:58:22.918524 32468 net.cpp:137] Memory required for data: 479233200
I1124 08:58:22.918524 32468 layer_factory.cpp:58] Creating layer relu4_2
I1124 08:58:22.918524 32468 net.cpp:84] Creating Layer relu4_2
I1124 08:58:22.918524 32468 net.cpp:406] relu4_2 <- conv4_2
I1124 08:58:22.918524 32468 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1124 08:58:22.919523 32468 net.cpp:122] Setting up relu4_2
I1124 08:58:22.919523 32468 net.cpp:129] Top shape: 100 110 16 16 (2816000)
I1124 08:58:22.919523 32468 net.cpp:137] Memory required for data: 490497200
I1124 08:58:22.919523 32468 layer_factory.cpp:58] Creating layer pool4_2
I1124 08:58:22.919523 32468 net.cpp:84] Creating Layer pool4_2
I1124 08:58:22.919523 32468 net.cpp:406] pool4_2 <- conv4_2
I1124 08:58:22.919523 32468 net.cpp:380] pool4_2 -> pool4_2
I1124 08:58:22.919523 32468 net.cpp:122] Setting up pool4_2
I1124 08:58:22.919523 32468 net.cpp:129] Top shape: 100 110 8 8 (704000)
I1124 08:58:22.919523 32468 net.cpp:137] Memory required for data: 493313200
I1124 08:58:22.919523 32468 layer_factory.cpp:58] Creating layer conv12
I1124 08:58:22.919523 32468 net.cpp:84] Creating Layer conv12
I1124 08:58:22.919523 32468 net.cpp:406] conv12 <- pool4_2
I1124 08:58:22.919523 32468 net.cpp:380] conv12 -> conv12
I1124 08:58:22.924520 32468 net.cpp:122] Setting up conv12
I1124 08:58:22.924520 32468 net.cpp:129] Top shape: 100 200 8 8 (1280000)
I1124 08:58:22.924520 32468 net.cpp:137] Memory required for data: 498433200
I1124 08:58:22.924520 32468 layer_factory.cpp:58] Creating layer bn_conv12
I1124 08:58:22.924520 32468 net.cpp:84] Creating Layer bn_conv12
I1124 08:58:22.924520 32468 net.cpp:406] bn_conv12 <- conv12
I1124 08:58:22.924520 32468 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1124 08:58:22.924520 32468 net.cpp:122] Setting up bn_conv12
I1124 08:58:22.924520 32468 net.cpp:129] Top shape: 100 200 8 8 (1280000)
I1124 08:58:22.924520 32468 net.cpp:137] Memory required for data: 503553200
I1124 08:58:22.924520 32468 layer_factory.cpp:58] Creating layer scale_conv12
I1124 08:58:22.924520 32468 net.cpp:84] Creating Layer scale_conv12
I1124 08:58:22.924520 32468 net.cpp:406] scale_conv12 <- conv12
I1124 08:58:22.924520 32468 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1124 08:58:22.924520 32468 layer_factory.cpp:58] Creating layer scale_conv12
I1124 08:58:22.924520 32468 net.cpp:122] Setting up scale_conv12
I1124 08:58:22.924520 32468 net.cpp:129] Top shape: 100 200 8 8 (1280000)
I1124 08:58:22.924520 32468 net.cpp:137] Memory required for data: 508673200
I1124 08:58:22.924520 32468 layer_factory.cpp:58] Creating layer relu_conv12
I1124 08:58:22.924520 32468 net.cpp:84] Creating Layer relu_conv12
I1124 08:58:22.924520 32468 net.cpp:406] relu_conv12 <- conv12
I1124 08:58:22.924520 32468 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1124 08:58:22.925520 32468 net.cpp:122] Setting up relu_conv12
I1124 08:58:22.925520 32468 net.cpp:129] Top shape: 100 200 8 8 (1280000)
I1124 08:58:22.925520 32468 net.cpp:137] Memory required for data: 513793200
I1124 08:58:22.925520 32468 layer_factory.cpp:58] Creating layer poolcp6
I1124 08:58:22.925520 32468 net.cpp:84] Creating Layer poolcp6
I1124 08:58:22.925520 32468 net.cpp:406] poolcp6 <- conv12
I1124 08:58:22.925520 32468 net.cpp:380] poolcp6 -> poolcp6
I1124 08:58:22.925520 32468 net.cpp:122] Setting up poolcp6
I1124 08:58:22.925520 32468 net.cpp:129] Top shape: 100 200 1 1 (20000)
I1124 08:58:22.925520 32468 net.cpp:137] Memory required for data: 513873200
I1124 08:58:22.925520 32468 layer_factory.cpp:58] Creating layer ip1
I1124 08:58:22.925520 32468 net.cpp:84] Creating Layer ip1
I1124 08:58:22.925520 32468 net.cpp:406] ip1 <- poolcp6
I1124 08:58:22.925520 32468 net.cpp:380] ip1 -> ip1
I1124 08:58:22.925520 32468 net.cpp:122] Setting up ip1
I1124 08:58:22.925520 32468 net.cpp:129] Top shape: 100 10 (1000)
I1124 08:58:22.925520 32468 net.cpp:137] Memory required for data: 513877200
I1124 08:58:22.925520 32468 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1124 08:58:22.925520 32468 net.cpp:84] Creating Layer ip1_ip1_0_split
I1124 08:58:22.925520 32468 net.cpp:406] ip1_ip1_0_split <- ip1
I1124 08:58:22.925520 32468 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1124 08:58:22.925520 32468 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1124 08:58:22.925520 32468 net.cpp:122] Setting up ip1_ip1_0_split
I1124 08:58:22.925520 32468 net.cpp:129] Top shape: 100 10 (1000)
I1124 08:58:22.925520 32468 net.cpp:129] Top shape: 100 10 (1000)
I1124 08:58:22.925520 32468 net.cpp:137] Memory required for data: 513885200
I1124 08:58:22.925520 32468 layer_factory.cpp:58] Creating layer accuracy
I1124 08:58:22.925520 32468 net.cpp:84] Creating Layer accuracy
I1124 08:58:22.925520 32468 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1124 08:58:22.925520 32468 net.cpp:406] accuracy <- label_cifar_1_split_0
I1124 08:58:22.925520 32468 net.cpp:380] accuracy -> accuracy
I1124 08:58:22.925520 32468 net.cpp:122] Setting up accuracy
I1124 08:58:22.925520 32468 net.cpp:129] Top shape: (1)
I1124 08:58:22.925520 32468 net.cpp:137] Memory required for data: 513885204
I1124 08:58:22.925520 32468 layer_factory.cpp:58] Creating layer loss
I1124 08:58:22.925520 32468 net.cpp:84] Creating Layer loss
I1124 08:58:22.925520 32468 net.cpp:406] loss <- ip1_ip1_0_split_1
I1124 08:58:22.925520 32468 net.cpp:406] loss <- label_cifar_1_split_1
I1124 08:58:22.925520 32468 net.cpp:380] loss -> loss
I1124 08:58:22.925520 32468 layer_factory.cpp:58] Creating layer loss
I1124 08:58:22.926520 32468 net.cpp:122] Setting up loss
I1124 08:58:22.926520 32468 net.cpp:129] Top shape: (1)
I1124 08:58:22.926520 32468 net.cpp:132]     with loss weight 1
I1124 08:58:22.926520 32468 net.cpp:137] Memory required for data: 513885208
I1124 08:58:22.926520 32468 net.cpp:198] loss needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:200] accuracy does not need backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] ip1 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] poolcp6 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] relu_conv12 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] scale_conv12 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] bn_conv12 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] conv12 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] pool4_2 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] relu4_2 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] scale4_2 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] bn4_2 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] conv4_2 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] relu4_1 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] scale4_1 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] bn4_1 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] conv4_1 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] relu4 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] scale4 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] bn4 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] conv4 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] relu3 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] scale3 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] bn3 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] conv3 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] pool2_1 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] relu2_2 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] scale2_2 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] bn2_2 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] conv2_2 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] relu2 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] scale2 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] bn2 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] conv2 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] relu1 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] scale1 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] bn1 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:198] conv1 needs backward computation.
I1124 08:58:22.926520 32468 net.cpp:200] label_cifar_1_split does not need backward computation.
I1124 08:58:22.926520 32468 net.cpp:200] cifar does not need backward computation.
I1124 08:58:22.926520 32468 net.cpp:242] This network produces output accuracy
I1124 08:58:22.926520 32468 net.cpp:242] This network produces output loss
I1124 08:58:22.926520 32468 net.cpp:255] Network initialization done.
I1124 08:58:22.926520 32468 solver.cpp:56] Solver scaffolding done.
I1124 08:58:22.929522 32468 caffe.cpp:249] Starting Optimization
I1124 08:58:22.929522 32468 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_8L_Simple_NoGrpCon_NoDrp_5x5_1.6k
I1124 08:58:22.929522 32468 solver.cpp:273] Learning Rate Policy: multistep
I1124 08:58:22.931520 32468 solver.cpp:330] Iteration 0, Testing net (#0)
I1124 08:58:22.932533 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 08:58:25.398571 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 08:58:25.496110 32468 solver.cpp:397]     Test net output #0: accuracy = 0.1004
I1124 08:58:25.496110 32468 solver.cpp:397]     Test net output #1: loss = 78.5679 (* 1 = 78.5679 loss)
I1124 08:58:25.615694 32468 solver.cpp:218] Iteration 0 (-nan iter/s, 2.68568s/100 iters), loss = 3.63442
I1124 08:58:25.615694 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.12
I1124 08:58:25.615694 32468 solver.cpp:237]     Train net output #1: loss = 3.63442 (* 1 = 3.63442 loss)
I1124 08:58:25.615694 32468 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1124 08:58:34.099140 32468 solver.cpp:218] Iteration 100 (11.7882 iter/s, 8.48308s/100 iters), loss = 1.68566
I1124 08:58:34.099140 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.38
I1124 08:58:34.099140 32468 solver.cpp:237]     Train net output #1: loss = 1.68566 (* 1 = 1.68566 loss)
I1124 08:58:34.099140 32468 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1124 08:58:42.550403 32468 solver.cpp:218] Iteration 200 (11.8337 iter/s, 8.45044s/100 iters), loss = 1.98625
I1124 08:58:42.550403 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.35
I1124 08:58:42.550403 32468 solver.cpp:237]     Train net output #1: loss = 1.98625 (* 1 = 1.98625 loss)
I1124 08:58:42.550403 32468 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1124 08:58:51.043781 32468 solver.cpp:218] Iteration 300 (11.7747 iter/s, 8.49276s/100 iters), loss = 1.46703
I1124 08:58:51.043781 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.47
I1124 08:58:51.043781 32468 solver.cpp:237]     Train net output #1: loss = 1.46703 (* 1 = 1.46703 loss)
I1124 08:58:51.043781 32468 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1124 08:58:59.534715 32468 solver.cpp:218] Iteration 400 (11.778 iter/s, 8.49042s/100 iters), loss = 1.58222
I1124 08:58:59.534715 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.49
I1124 08:58:59.534715 32468 solver.cpp:237]     Train net output #1: loss = 1.58222 (* 1 = 1.58222 loss)
I1124 08:58:59.534715 32468 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1124 08:59:07.619707 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 08:59:07.954789 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_500.caffemodel
I1124 08:59:08.002584 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_500.solverstate
I1124 08:59:08.023587 32468 solver.cpp:330] Iteration 500, Testing net (#0)
I1124 08:59:08.023587 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 08:59:10.461637 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 08:59:10.560675 32468 solver.cpp:397]     Test net output #0: accuracy = 0.4348
I1124 08:59:10.560675 32468 solver.cpp:397]     Test net output #1: loss = 1.58398 (* 1 = 1.58398 loss)
I1124 08:59:10.643707 32468 solver.cpp:218] Iteration 500 (9.00178 iter/s, 11.1089s/100 iters), loss = 1.26953
I1124 08:59:10.643707 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.55
I1124 08:59:10.643707 32468 solver.cpp:237]     Train net output #1: loss = 1.26953 (* 1 = 1.26953 loss)
I1124 08:59:10.643707 32468 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1124 08:59:19.157270 32468 solver.cpp:218] Iteration 600 (11.7464 iter/s, 8.51328s/100 iters), loss = 1.23061
I1124 08:59:19.157270 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.59
I1124 08:59:19.157270 32468 solver.cpp:237]     Train net output #1: loss = 1.23061 (* 1 = 1.23061 loss)
I1124 08:59:19.157270 32468 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1124 08:59:27.691041 32468 solver.cpp:218] Iteration 700 (11.7199 iter/s, 8.53247s/100 iters), loss = 1.22422
I1124 08:59:27.691041 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.54
I1124 08:59:27.691041 32468 solver.cpp:237]     Train net output #1: loss = 1.22422 (* 1 = 1.22422 loss)
I1124 08:59:27.691041 32468 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1124 08:59:36.215667 32468 solver.cpp:218] Iteration 800 (11.7305 iter/s, 8.52481s/100 iters), loss = 0.858363
I1124 08:59:36.215667 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1124 08:59:36.215667 32468 solver.cpp:237]     Train net output #1: loss = 0.858363 (* 1 = 0.858363 loss)
I1124 08:59:36.215667 32468 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1124 08:59:44.739737 32468 solver.cpp:218] Iteration 900 (11.733 iter/s, 8.52298s/100 iters), loss = 0.888688
I1124 08:59:44.739737 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1124 08:59:44.739737 32468 solver.cpp:237]     Train net output #1: loss = 0.888688 (* 1 = 0.888688 loss)
I1124 08:59:44.739737 32468 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1124 08:59:52.870050 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 08:59:53.209059 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_1000.caffemodel
I1124 08:59:53.248049 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_1000.solverstate
I1124 08:59:53.267550 32468 solver.cpp:330] Iteration 1000, Testing net (#0)
I1124 08:59:53.267550 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 08:59:55.706600 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 08:59:55.804968 32468 solver.cpp:397]     Test net output #0: accuracy = 0.5554
I1124 08:59:55.804968 32468 solver.cpp:397]     Test net output #1: loss = 1.24169 (* 1 = 1.24169 loss)
I1124 08:59:55.887984 32468 solver.cpp:218] Iteration 1000 (8.97024 iter/s, 11.148s/100 iters), loss = 0.924423
I1124 08:59:55.888487 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1124 08:59:55.888487 32468 solver.cpp:237]     Train net output #1: loss = 0.924423 (* 1 = 0.924423 loss)
I1124 08:59:55.888487 32468 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1124 09:00:04.435158 32468 solver.cpp:218] Iteration 1100 (11.7006 iter/s, 8.54655s/100 iters), loss = 0.799939
I1124 09:00:04.435158 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1124 09:00:04.435158 32468 solver.cpp:237]     Train net output #1: loss = 0.799939 (* 1 = 0.799939 loss)
I1124 09:00:04.435158 32468 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1124 09:00:12.946521 32468 solver.cpp:218] Iteration 1200 (11.7497 iter/s, 8.51086s/100 iters), loss = 0.795341
I1124 09:00:12.946521 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1124 09:00:12.946521 32468 solver.cpp:237]     Train net output #1: loss = 0.795341 (* 1 = 0.795341 loss)
I1124 09:00:12.946521 32468 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1124 09:00:21.470321 32468 solver.cpp:218] Iteration 1300 (11.7328 iter/s, 8.52311s/100 iters), loss = 0.743754
I1124 09:00:21.470321 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1124 09:00:21.470321 32468 solver.cpp:237]     Train net output #1: loss = 0.743754 (* 1 = 0.743754 loss)
I1124 09:00:21.470321 32468 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1124 09:00:30.008977 32468 solver.cpp:218] Iteration 1400 (11.7122 iter/s, 8.53813s/100 iters), loss = 0.854827
I1124 09:00:30.008977 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1124 09:00:30.008977 32468 solver.cpp:237]     Train net output #1: loss = 0.854827 (* 1 = 0.854827 loss)
I1124 09:00:30.008977 32468 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1124 09:00:38.123945 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:00:38.462601 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_1500.caffemodel
I1124 09:00:38.502806 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_1500.solverstate
I1124 09:00:38.521806 32468 solver.cpp:330] Iteration 1500, Testing net (#0)
I1124 09:00:38.521806 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:00:40.966881 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:00:41.066370 32468 solver.cpp:397]     Test net output #0: accuracy = 0.6492
I1124 09:00:41.066370 32468 solver.cpp:397]     Test net output #1: loss = 0.99669 (* 1 = 0.99669 loss)
I1124 09:00:41.150388 32468 solver.cpp:218] Iteration 1500 (8.97615 iter/s, 11.1406s/100 iters), loss = 0.726252
I1124 09:00:41.150388 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1124 09:00:41.150388 32468 solver.cpp:237]     Train net output #1: loss = 0.726252 (* 1 = 0.726252 loss)
I1124 09:00:41.150388 32468 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1124 09:00:49.685662 32468 solver.cpp:218] Iteration 1600 (11.7157 iter/s, 8.53553s/100 iters), loss = 0.643576
I1124 09:00:49.685662 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1124 09:00:49.685662 32468 solver.cpp:237]     Train net output #1: loss = 0.643576 (* 1 = 0.643576 loss)
I1124 09:00:49.685662 32468 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1124 09:00:58.235177 32468 solver.cpp:218] Iteration 1700 (11.6983 iter/s, 8.54822s/100 iters), loss = 0.563083
I1124 09:00:58.235177 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1124 09:00:58.235177 32468 solver.cpp:237]     Train net output #1: loss = 0.563083 (* 1 = 0.563083 loss)
I1124 09:00:58.235177 32468 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1124 09:01:06.800477 32468 solver.cpp:218] Iteration 1800 (11.6757 iter/s, 8.56482s/100 iters), loss = 0.594151
I1124 09:01:06.800477 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1124 09:01:06.800477 32468 solver.cpp:237]     Train net output #1: loss = 0.594151 (* 1 = 0.594151 loss)
I1124 09:01:06.800477 32468 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1124 09:01:15.564052 32468 solver.cpp:218] Iteration 1900 (11.4109 iter/s, 8.76351s/100 iters), loss = 0.623899
I1124 09:01:15.564052 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1124 09:01:15.564052 32468 solver.cpp:237]     Train net output #1: loss = 0.623899 (* 1 = 0.623899 loss)
I1124 09:01:15.564052 32468 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1124 09:01:23.690138 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:01:24.027309 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_2000.caffemodel
I1124 09:01:24.069299 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_2000.solverstate
I1124 09:01:24.089299 32468 solver.cpp:330] Iteration 2000, Testing net (#0)
I1124 09:01:24.089299 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:01:26.542332 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:01:26.642376 32468 solver.cpp:397]     Test net output #0: accuracy = 0.6497
I1124 09:01:26.642376 32468 solver.cpp:397]     Test net output #1: loss = 0.996338 (* 1 = 0.996338 loss)
I1124 09:01:26.725409 32468 solver.cpp:218] Iteration 2000 (8.9601 iter/s, 11.1606s/100 iters), loss = 0.496683
I1124 09:01:26.725409 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1124 09:01:26.725409 32468 solver.cpp:237]     Train net output #1: loss = 0.496683 (* 1 = 0.496683 loss)
I1124 09:01:26.725409 32468 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1124 09:01:35.272161 32468 solver.cpp:218] Iteration 2100 (11.7006 iter/s, 8.54657s/100 iters), loss = 0.514385
I1124 09:01:35.272161 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1124 09:01:35.272161 32468 solver.cpp:237]     Train net output #1: loss = 0.514385 (* 1 = 0.514385 loss)
I1124 09:01:35.272161 32468 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1124 09:01:43.840248 32468 solver.cpp:218] Iteration 2200 (11.6727 iter/s, 8.56698s/100 iters), loss = 0.596236
I1124 09:01:43.840248 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1124 09:01:43.840248 32468 solver.cpp:237]     Train net output #1: loss = 0.596236 (* 1 = 0.596236 loss)
I1124 09:01:43.840248 32468 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1124 09:01:52.410006 32468 solver.cpp:218] Iteration 2300 (11.6691 iter/s, 8.56966s/100 iters), loss = 0.512305
I1124 09:01:52.410006 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1124 09:01:52.410006 32468 solver.cpp:237]     Train net output #1: loss = 0.512305 (* 1 = 0.512305 loss)
I1124 09:01:52.410006 32468 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1124 09:02:01.014320 32468 solver.cpp:218] Iteration 2400 (11.6229 iter/s, 8.6037s/100 iters), loss = 0.546674
I1124 09:02:01.014320 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1124 09:02:01.014320 32468 solver.cpp:237]     Train net output #1: loss = 0.546674 (* 1 = 0.546674 loss)
I1124 09:02:01.014320 32468 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1124 09:02:09.141360 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:02:09.480461 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_2500.caffemodel
I1124 09:02:09.521970 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_2500.solverstate
I1124 09:02:09.539974 32468 solver.cpp:330] Iteration 2500, Testing net (#0)
I1124 09:02:09.539974 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:02:11.983844 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:02:12.083849 32468 solver.cpp:397]     Test net output #0: accuracy = 0.7308
I1124 09:02:12.083849 32468 solver.cpp:397]     Test net output #1: loss = 0.791553 (* 1 = 0.791553 loss)
I1124 09:02:12.166574 32468 solver.cpp:218] Iteration 2500 (8.9673 iter/s, 11.1516s/100 iters), loss = 0.452473
I1124 09:02:12.166574 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1124 09:02:12.166574 32468 solver.cpp:237]     Train net output #1: loss = 0.452473 (* 1 = 0.452473 loss)
I1124 09:02:12.166574 32468 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1124 09:02:20.711694 32468 solver.cpp:218] Iteration 2600 (11.7028 iter/s, 8.54494s/100 iters), loss = 0.525621
I1124 09:02:20.711694 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1124 09:02:20.711694 32468 solver.cpp:237]     Train net output #1: loss = 0.525621 (* 1 = 0.525621 loss)
I1124 09:02:20.711694 32468 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1124 09:02:29.257186 32468 solver.cpp:218] Iteration 2700 (11.7037 iter/s, 8.54434s/100 iters), loss = 0.510718
I1124 09:02:29.257186 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1124 09:02:29.257186 32468 solver.cpp:237]     Train net output #1: loss = 0.510718 (* 1 = 0.510718 loss)
I1124 09:02:29.257186 32468 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1124 09:02:37.798491 32468 solver.cpp:218] Iteration 2800 (11.7083 iter/s, 8.54095s/100 iters), loss = 0.468557
I1124 09:02:37.798491 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1124 09:02:37.798491 32468 solver.cpp:237]     Train net output #1: loss = 0.468557 (* 1 = 0.468557 loss)
I1124 09:02:37.798491 32468 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1124 09:02:46.340875 32468 solver.cpp:218] Iteration 2900 (11.7076 iter/s, 8.54149s/100 iters), loss = 0.464061
I1124 09:02:46.340875 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1124 09:02:46.340875 32468 solver.cpp:237]     Train net output #1: loss = 0.464061 (* 1 = 0.464061 loss)
I1124 09:02:46.340875 32468 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1124 09:02:54.463120 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:02:54.802191 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_3000.caffemodel
I1124 09:02:54.841189 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_3000.solverstate
I1124 09:02:54.859688 32468 solver.cpp:330] Iteration 3000, Testing net (#0)
I1124 09:02:54.859688 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:02:57.303318 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:02:57.402083 32468 solver.cpp:397]     Test net output #0: accuracy = 0.7017
I1124 09:02:57.402083 32468 solver.cpp:397]     Test net output #1: loss = 0.877557 (* 1 = 0.877557 loss)
I1124 09:02:57.486608 32468 solver.cpp:218] Iteration 3000 (8.97252 iter/s, 11.1451s/100 iters), loss = 0.419507
I1124 09:02:57.486608 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1124 09:02:57.486608 32468 solver.cpp:237]     Train net output #1: loss = 0.419507 (* 1 = 0.419507 loss)
I1124 09:02:57.486608 32468 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1124 09:03:06.021749 32468 solver.cpp:218] Iteration 3100 (11.7159 iter/s, 8.53539s/100 iters), loss = 0.478824
I1124 09:03:06.021749 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1124 09:03:06.021749 32468 solver.cpp:237]     Train net output #1: loss = 0.478824 (* 1 = 0.478824 loss)
I1124 09:03:06.021749 32468 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1124 09:03:14.564227 32468 solver.cpp:218] Iteration 3200 (11.7067 iter/s, 8.54209s/100 iters), loss = 0.430493
I1124 09:03:14.565212 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1124 09:03:14.565212 32468 solver.cpp:237]     Train net output #1: loss = 0.430493 (* 1 = 0.430493 loss)
I1124 09:03:14.565212 32468 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1124 09:03:23.102458 32468 solver.cpp:218] Iteration 3300 (11.7138 iter/s, 8.53691s/100 iters), loss = 0.504062
I1124 09:03:23.102458 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1124 09:03:23.102458 32468 solver.cpp:237]     Train net output #1: loss = 0.504062 (* 1 = 0.504062 loss)
I1124 09:03:23.102458 32468 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1124 09:03:31.644358 32468 solver.cpp:218] Iteration 3400 (11.7075 iter/s, 8.54156s/100 iters), loss = 0.464334
I1124 09:03:31.644358 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1124 09:03:31.644358 32468 solver.cpp:237]     Train net output #1: loss = 0.464334 (* 1 = 0.464334 loss)
I1124 09:03:31.644358 32468 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1124 09:03:39.770584 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:03:40.109269 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_3500.caffemodel
I1124 09:03:40.149286 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_3500.solverstate
I1124 09:03:40.171296 32468 solver.cpp:330] Iteration 3500, Testing net (#0)
I1124 09:03:40.171296 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:03:42.614619 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:03:42.714625 32468 solver.cpp:397]     Test net output #0: accuracy = 0.7581
I1124 09:03:42.714625 32468 solver.cpp:397]     Test net output #1: loss = 0.738089 (* 1 = 0.738089 loss)
I1124 09:03:42.797669 32468 solver.cpp:218] Iteration 3500 (8.96643 iter/s, 11.1527s/100 iters), loss = 0.437764
I1124 09:03:42.797669 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1124 09:03:42.797669 32468 solver.cpp:237]     Train net output #1: loss = 0.437764 (* 1 = 0.437764 loss)
I1124 09:03:42.797669 32468 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1124 09:03:51.332391 32468 solver.cpp:218] Iteration 3600 (11.7168 iter/s, 8.53477s/100 iters), loss = 0.509766
I1124 09:03:51.332391 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1124 09:03:51.332391 32468 solver.cpp:237]     Train net output #1: loss = 0.509766 (* 1 = 0.509766 loss)
I1124 09:03:51.332391 32468 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1124 09:03:59.878675 32468 solver.cpp:218] Iteration 3700 (11.7029 iter/s, 8.54489s/100 iters), loss = 0.420622
I1124 09:03:59.878675 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1124 09:03:59.878675 32468 solver.cpp:237]     Train net output #1: loss = 0.420622 (* 1 = 0.420622 loss)
I1124 09:03:59.878675 32468 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1124 09:04:08.414719 32468 solver.cpp:218] Iteration 3800 (11.7152 iter/s, 8.53595s/100 iters), loss = 0.451007
I1124 09:04:08.414719 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1124 09:04:08.414719 32468 solver.cpp:237]     Train net output #1: loss = 0.451007 (* 1 = 0.451007 loss)
I1124 09:04:08.414719 32468 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1124 09:04:16.951541 32468 solver.cpp:218] Iteration 3900 (11.7142 iter/s, 8.53662s/100 iters), loss = 0.486305
I1124 09:04:16.951541 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1124 09:04:16.951541 32468 solver.cpp:237]     Train net output #1: loss = 0.486305 (* 1 = 0.486305 loss)
I1124 09:04:16.951541 32468 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1124 09:04:25.069524 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:04:25.407779 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_4000.caffemodel
I1124 09:04:25.448786 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_4000.solverstate
I1124 09:04:25.466823 32468 solver.cpp:330] Iteration 4000, Testing net (#0)
I1124 09:04:25.466823 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:04:27.910509 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:04:28.009963 32468 solver.cpp:397]     Test net output #0: accuracy = 0.7394
I1124 09:04:28.009963 32468 solver.cpp:397]     Test net output #1: loss = 0.738459 (* 1 = 0.738459 loss)
I1124 09:04:28.092988 32468 solver.cpp:218] Iteration 4000 (8.97652 iter/s, 11.1402s/100 iters), loss = 0.394188
I1124 09:04:28.092988 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1124 09:04:28.092988 32468 solver.cpp:237]     Train net output #1: loss = 0.394188 (* 1 = 0.394188 loss)
I1124 09:04:28.092988 32468 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1124 09:04:36.621412 32468 solver.cpp:218] Iteration 4100 (11.7255 iter/s, 8.52845s/100 iters), loss = 0.390202
I1124 09:04:36.621412 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1124 09:04:36.621412 32468 solver.cpp:237]     Train net output #1: loss = 0.390202 (* 1 = 0.390202 loss)
I1124 09:04:36.621412 32468 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1124 09:04:45.163233 32468 solver.cpp:218] Iteration 4200 (11.7081 iter/s, 8.54112s/100 iters), loss = 0.513813
I1124 09:04:45.163720 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1124 09:04:45.163720 32468 solver.cpp:237]     Train net output #1: loss = 0.513813 (* 1 = 0.513813 loss)
I1124 09:04:45.163720 32468 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1124 09:04:53.701104 32468 solver.cpp:218] Iteration 4300 (11.7129 iter/s, 8.53759s/100 iters), loss = 0.527507
I1124 09:04:53.701104 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1124 09:04:53.701104 32468 solver.cpp:237]     Train net output #1: loss = 0.527507 (* 1 = 0.527507 loss)
I1124 09:04:53.701104 32468 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1124 09:05:02.263078 32468 solver.cpp:218] Iteration 4400 (11.6806 iter/s, 8.56117s/100 iters), loss = 0.390919
I1124 09:05:02.263078 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1124 09:05:02.263078 32468 solver.cpp:237]     Train net output #1: loss = 0.390919 (* 1 = 0.390919 loss)
I1124 09:05:02.263581 32468 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1124 09:05:10.425845 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:05:10.763936 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_4500.caffemodel
I1124 09:05:10.801939 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_4500.solverstate
I1124 09:05:10.820938 32468 solver.cpp:330] Iteration 4500, Testing net (#0)
I1124 09:05:10.820938 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:05:13.264667 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:05:13.363662 32468 solver.cpp:397]     Test net output #0: accuracy = 0.7879
I1124 09:05:13.363662 32468 solver.cpp:397]     Test net output #1: loss = 0.626899 (* 1 = 0.626899 loss)
I1124 09:05:13.446655 32468 solver.cpp:218] Iteration 4500 (8.94228 iter/s, 11.1828s/100 iters), loss = 0.375924
I1124 09:05:13.446655 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1124 09:05:13.446655 32468 solver.cpp:237]     Train net output #1: loss = 0.375924 (* 1 = 0.375924 loss)
I1124 09:05:13.446655 32468 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1124 09:05:21.999701 32468 solver.cpp:218] Iteration 4600 (11.6926 iter/s, 8.55242s/100 iters), loss = 0.383676
I1124 09:05:21.999701 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1124 09:05:21.999701 32468 solver.cpp:237]     Train net output #1: loss = 0.383676 (* 1 = 0.383676 loss)
I1124 09:05:21.999701 32468 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1124 09:05:30.775918 32468 solver.cpp:218] Iteration 4700 (11.3942 iter/s, 8.77636s/100 iters), loss = 0.418654
I1124 09:05:30.776901 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1124 09:05:30.776901 32468 solver.cpp:237]     Train net output #1: loss = 0.418654 (* 1 = 0.418654 loss)
I1124 09:05:30.776901 32468 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1124 09:05:39.524329 32468 solver.cpp:218] Iteration 4800 (11.4314 iter/s, 8.74787s/100 iters), loss = 0.418258
I1124 09:05:39.525316 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1124 09:05:39.525316 32468 solver.cpp:237]     Train net output #1: loss = 0.418258 (* 1 = 0.418258 loss)
I1124 09:05:39.525316 32468 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1124 09:05:48.079103 32468 solver.cpp:218] Iteration 4900 (11.6911 iter/s, 8.55351s/100 iters), loss = 0.471063
I1124 09:05:48.079103 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1124 09:05:48.079103 32468 solver.cpp:237]     Train net output #1: loss = 0.471063 (* 1 = 0.471063 loss)
I1124 09:05:48.079103 32468 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1124 09:05:56.244602 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:05:56.585410 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_5000.caffemodel
I1124 09:05:56.629416 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_5000.solverstate
I1124 09:05:56.648921 32468 solver.cpp:330] Iteration 5000, Testing net (#0)
I1124 09:05:56.648921 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:05:59.119338 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:05:59.221349 32468 solver.cpp:397]     Test net output #0: accuracy = 0.7695
I1124 09:05:59.221349 32468 solver.cpp:397]     Test net output #1: loss = 0.680776 (* 1 = 0.680776 loss)
I1124 09:05:59.311854 32468 solver.cpp:218] Iteration 5000 (8.90303 iter/s, 11.2321s/100 iters), loss = 0.387562
I1124 09:05:59.311854 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1124 09:05:59.311854 32468 solver.cpp:237]     Train net output #1: loss = 0.387562 (* 1 = 0.387562 loss)
I1124 09:05:59.311854 32468 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1124 09:05:59.311854 32468 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1124 09:06:08.033396 32468 solver.cpp:218] Iteration 5100 (11.4659 iter/s, 8.72154s/100 iters), loss = 0.297824
I1124 09:06:08.033396 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1124 09:06:08.033396 32468 solver.cpp:237]     Train net output #1: loss = 0.297824 (* 1 = 0.297824 loss)
I1124 09:06:08.034396 32468 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1124 09:06:16.704506 32468 solver.cpp:218] Iteration 5200 (11.5339 iter/s, 8.67007s/100 iters), loss = 0.290248
I1124 09:06:16.704993 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1124 09:06:16.704993 32468 solver.cpp:237]     Train net output #1: loss = 0.290248 (* 1 = 0.290248 loss)
I1124 09:06:16.704993 32468 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1124 09:06:25.312736 32468 solver.cpp:218] Iteration 5300 (11.617 iter/s, 8.60805s/100 iters), loss = 0.239669
I1124 09:06:25.312736 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1124 09:06:25.312736 32468 solver.cpp:237]     Train net output #1: loss = 0.239669 (* 1 = 0.239669 loss)
I1124 09:06:25.312736 32468 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1124 09:06:33.893944 32468 solver.cpp:218] Iteration 5400 (11.6551 iter/s, 8.57992s/100 iters), loss = 0.247352
I1124 09:06:33.893944 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1124 09:06:33.893944 32468 solver.cpp:237]     Train net output #1: loss = 0.247352 (* 1 = 0.247352 loss)
I1124 09:06:33.893944 32468 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1124 09:06:42.241932 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:06:42.593024 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_5500.caffemodel
I1124 09:06:42.632529 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_5500.solverstate
I1124 09:06:42.651528 32468 solver.cpp:330] Iteration 5500, Testing net (#0)
I1124 09:06:42.651528 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:06:45.127825 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:06:45.227833 32468 solver.cpp:397]     Test net output #0: accuracy = 0.8871
I1124 09:06:45.227833 32468 solver.cpp:397]     Test net output #1: loss = 0.331578 (* 1 = 0.331578 loss)
I1124 09:06:45.310881 32468 solver.cpp:218] Iteration 5500 (8.75923 iter/s, 11.4165s/100 iters), loss = 0.160121
I1124 09:06:45.310881 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:06:45.310881 32468 solver.cpp:237]     Train net output #1: loss = 0.160121 (* 1 = 0.160121 loss)
I1124 09:06:45.310881 32468 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1124 09:06:53.912235 32468 solver.cpp:218] Iteration 5600 (11.6264 iter/s, 8.6011s/100 iters), loss = 0.211155
I1124 09:06:53.912235 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 09:06:53.912235 32468 solver.cpp:237]     Train net output #1: loss = 0.211155 (* 1 = 0.211155 loss)
I1124 09:06:53.912235 32468 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1124 09:07:02.458428 32468 solver.cpp:218] Iteration 5700 (11.7021 iter/s, 8.54547s/100 iters), loss = 0.270839
I1124 09:07:02.458428 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1124 09:07:02.458428 32468 solver.cpp:237]     Train net output #1: loss = 0.270839 (* 1 = 0.270839 loss)
I1124 09:07:02.458428 32468 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1124 09:07:11.057721 32468 solver.cpp:218] Iteration 5800 (11.6297 iter/s, 8.5987s/100 iters), loss = 0.27366
I1124 09:07:11.057721 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1124 09:07:11.057721 32468 solver.cpp:237]     Train net output #1: loss = 0.27366 (* 1 = 0.27366 loss)
I1124 09:07:11.057721 32468 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1124 09:07:19.667199 32468 solver.cpp:218] Iteration 5900 (11.6152 iter/s, 8.60939s/100 iters), loss = 0.164345
I1124 09:07:19.667199 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 09:07:19.667199 32468 solver.cpp:237]     Train net output #1: loss = 0.164345 (* 1 = 0.164345 loss)
I1124 09:07:19.667199 32468 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1124 09:07:27.961526 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:07:28.335527 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_6000.caffemodel
I1124 09:07:28.377527 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_6000.solverstate
I1124 09:07:28.397526 32468 solver.cpp:330] Iteration 6000, Testing net (#0)
I1124 09:07:28.397526 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:07:30.873817 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:07:30.972563 32468 solver.cpp:397]     Test net output #0: accuracy = 0.8889
I1124 09:07:30.972563 32468 solver.cpp:397]     Test net output #1: loss = 0.325964 (* 1 = 0.325964 loss)
I1124 09:07:31.057050 32468 solver.cpp:218] Iteration 6000 (8.78045 iter/s, 11.3889s/100 iters), loss = 0.172114
I1124 09:07:31.057551 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 09:07:31.057551 32468 solver.cpp:237]     Train net output #1: loss = 0.172114 (* 1 = 0.172114 loss)
I1124 09:07:31.057551 32468 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1124 09:07:39.601202 32468 solver.cpp:218] Iteration 6100 (11.7043 iter/s, 8.54388s/100 iters), loss = 0.237638
I1124 09:07:39.601202 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1124 09:07:39.601202 32468 solver.cpp:237]     Train net output #1: loss = 0.237638 (* 1 = 0.237638 loss)
I1124 09:07:39.601202 32468 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1124 09:07:48.170964 32468 solver.cpp:218] Iteration 6200 (11.6704 iter/s, 8.5687s/100 iters), loss = 0.163621
I1124 09:07:48.170964 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 09:07:48.170964 32468 solver.cpp:237]     Train net output #1: loss = 0.163621 (* 1 = 0.163621 loss)
I1124 09:07:48.170964 32468 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1124 09:07:56.913313 32468 solver.cpp:218] Iteration 6300 (11.4386 iter/s, 8.74232s/100 iters), loss = 0.224044
I1124 09:07:56.913313 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 09:07:56.913313 32468 solver.cpp:237]     Train net output #1: loss = 0.224044 (* 1 = 0.224044 loss)
I1124 09:07:56.913313 32468 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1124 09:08:05.649494 32468 solver.cpp:218] Iteration 6400 (11.4472 iter/s, 8.73576s/100 iters), loss = 0.17851
I1124 09:08:05.650487 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 09:08:05.650487 32468 solver.cpp:237]     Train net output #1: loss = 0.17851 (* 1 = 0.17851 loss)
I1124 09:08:05.650487 32468 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1124 09:08:13.825742 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:08:14.184693 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_6500.caffemodel
I1124 09:08:14.224695 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_6500.solverstate
I1124 09:08:14.243695 32468 solver.cpp:330] Iteration 6500, Testing net (#0)
I1124 09:08:14.243695 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:08:16.717814 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:08:16.817829 32468 solver.cpp:397]     Test net output #0: accuracy = 0.8943
I1124 09:08:16.817829 32468 solver.cpp:397]     Test net output #1: loss = 0.306497 (* 1 = 0.306497 loss)
I1124 09:08:16.905853 32468 solver.cpp:218] Iteration 6500 (8.88496 iter/s, 11.255s/100 iters), loss = 0.111109
I1124 09:08:16.905853 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:08:16.905853 32468 solver.cpp:237]     Train net output #1: loss = 0.111109 (* 1 = 0.111109 loss)
I1124 09:08:16.905853 32468 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1124 09:08:25.643399 32468 solver.cpp:218] Iteration 6600 (11.4457 iter/s, 8.73693s/100 iters), loss = 0.14796
I1124 09:08:25.643399 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:08:25.643399 32468 solver.cpp:237]     Train net output #1: loss = 0.14796 (* 1 = 0.14796 loss)
I1124 09:08:25.643399 32468 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1124 09:08:34.351418 32468 solver.cpp:218] Iteration 6700 (11.4847 iter/s, 8.70726s/100 iters), loss = 0.175954
I1124 09:08:34.351418 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 09:08:34.351418 32468 solver.cpp:237]     Train net output #1: loss = 0.175954 (* 1 = 0.175954 loss)
I1124 09:08:34.351418 32468 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1124 09:08:43.005101 32468 solver.cpp:218] Iteration 6800 (11.5576 iter/s, 8.65234s/100 iters), loss = 0.192567
I1124 09:08:43.005101 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 09:08:43.005101 32468 solver.cpp:237]     Train net output #1: loss = 0.192567 (* 1 = 0.192567 loss)
I1124 09:08:43.005101 32468 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1124 09:08:51.753410 32468 solver.cpp:218] Iteration 6900 (11.4314 iter/s, 8.7478s/100 iters), loss = 0.112106
I1124 09:08:51.753410 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:08:51.753410 32468 solver.cpp:237]     Train net output #1: loss = 0.112106 (* 1 = 0.112106 loss)
I1124 09:08:51.753410 32468 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1124 09:08:59.998572 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:09:00.346029 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_7000.caffemodel
I1124 09:09:00.386523 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_7000.solverstate
I1124 09:09:00.404541 32468 solver.cpp:330] Iteration 7000, Testing net (#0)
I1124 09:09:00.405022 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:09:02.851599 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:09:02.950603 32468 solver.cpp:397]     Test net output #0: accuracy = 0.8925
I1124 09:09:02.950603 32468 solver.cpp:397]     Test net output #1: loss = 0.312876 (* 1 = 0.312876 loss)
I1124 09:09:03.035234 32468 solver.cpp:218] Iteration 7000 (8.86428 iter/s, 11.2812s/100 iters), loss = 0.130821
I1124 09:09:03.035234 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:09:03.035234 32468 solver.cpp:237]     Train net output #1: loss = 0.130821 (* 1 = 0.130821 loss)
I1124 09:09:03.035234 32468 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1124 09:09:11.803288 32468 solver.cpp:218] Iteration 7100 (11.4057 iter/s, 8.76752s/100 iters), loss = 0.157696
I1124 09:09:11.803288 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:09:11.803288 32468 solver.cpp:237]     Train net output #1: loss = 0.157696 (* 1 = 0.157696 loss)
I1124 09:09:11.803288 32468 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1124 09:09:20.379075 32468 solver.cpp:218] Iteration 7200 (11.6606 iter/s, 8.57585s/100 iters), loss = 0.134852
I1124 09:09:20.379075 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 09:09:20.380079 32468 solver.cpp:237]     Train net output #1: loss = 0.134852 (* 1 = 0.134852 loss)
I1124 09:09:20.380079 32468 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1124 09:09:29.144548 32468 solver.cpp:218] Iteration 7300 (11.41 iter/s, 8.76427s/100 iters), loss = 0.129563
I1124 09:09:29.144548 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:09:29.144548 32468 solver.cpp:237]     Train net output #1: loss = 0.129563 (* 1 = 0.129563 loss)
I1124 09:09:29.144548 32468 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1124 09:09:37.778853 32468 solver.cpp:218] Iteration 7400 (11.5824 iter/s, 8.63375s/100 iters), loss = 0.166699
I1124 09:09:37.778853 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 09:09:37.778853 32468 solver.cpp:237]     Train net output #1: loss = 0.166699 (* 1 = 0.166699 loss)
I1124 09:09:37.778853 32468 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1124 09:09:46.025189 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:09:46.364518 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_7500.caffemodel
I1124 09:09:46.404510 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_7500.solverstate
I1124 09:09:46.422524 32468 solver.cpp:330] Iteration 7500, Testing net (#0)
I1124 09:09:46.422524 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:09:48.895478 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:09:48.998991 32468 solver.cpp:397]     Test net output #0: accuracy = 0.8904
I1124 09:09:48.998991 32468 solver.cpp:397]     Test net output #1: loss = 0.320655 (* 1 = 0.320655 loss)
I1124 09:09:49.082026 32468 solver.cpp:218] Iteration 7500 (8.84784 iter/s, 11.3022s/100 iters), loss = 0.12185
I1124 09:09:49.082026 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:09:49.082026 32468 solver.cpp:237]     Train net output #1: loss = 0.12185 (* 1 = 0.12185 loss)
I1124 09:09:49.082026 32468 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1124 09:09:57.703017 32468 solver.cpp:218] Iteration 7600 (11.5991 iter/s, 8.62135s/100 iters), loss = 0.16616
I1124 09:09:57.704002 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 09:09:57.704002 32468 solver.cpp:237]     Train net output #1: loss = 0.16616 (* 1 = 0.16616 loss)
I1124 09:09:57.704002 32468 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1124 09:10:06.488834 32468 solver.cpp:218] Iteration 7700 (11.3835 iter/s, 8.78466s/100 iters), loss = 0.105509
I1124 09:10:06.488834 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:10:06.488834 32468 solver.cpp:237]     Train net output #1: loss = 0.105509 (* 1 = 0.105509 loss)
I1124 09:10:06.488834 32468 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1124 09:10:15.023100 32468 solver.cpp:218] Iteration 7800 (11.7173 iter/s, 8.53441s/100 iters), loss = 0.200886
I1124 09:10:15.024101 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 09:10:15.024101 32468 solver.cpp:237]     Train net output #1: loss = 0.200886 (* 1 = 0.200886 loss)
I1124 09:10:15.024101 32468 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1124 09:10:23.621114 32468 solver.cpp:218] Iteration 7900 (11.6323 iter/s, 8.59676s/100 iters), loss = 0.0894907
I1124 09:10:23.621114 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:10:23.621114 32468 solver.cpp:237]     Train net output #1: loss = 0.0894907 (* 1 = 0.0894907 loss)
I1124 09:10:23.621114 32468 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1124 09:10:31.782459 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:10:32.120745 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_8000.caffemodel
I1124 09:10:32.161273 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_8000.solverstate
I1124 09:10:32.184777 32468 solver.cpp:330] Iteration 8000, Testing net (#0)
I1124 09:10:32.184777 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:10:34.639578 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:10:34.737617 32468 solver.cpp:397]     Test net output #0: accuracy = 0.893
I1124 09:10:34.738603 32468 solver.cpp:397]     Test net output #1: loss = 0.320523 (* 1 = 0.320523 loss)
I1124 09:10:34.820649 32468 solver.cpp:218] Iteration 8000 (8.92918 iter/s, 11.1992s/100 iters), loss = 0.176383
I1124 09:10:34.820649 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 09:10:34.820649 32468 solver.cpp:237]     Train net output #1: loss = 0.176383 (* 1 = 0.176383 loss)
I1124 09:10:34.820649 32468 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1124 09:10:43.630388 32468 solver.cpp:218] Iteration 8100 (11.3517 iter/s, 8.80927s/100 iters), loss = 0.204931
I1124 09:10:43.630388 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1124 09:10:43.630388 32468 solver.cpp:237]     Train net output #1: loss = 0.204931 (* 1 = 0.204931 loss)
I1124 09:10:43.630388 32468 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1124 09:10:52.387476 32468 solver.cpp:218] Iteration 8200 (11.4205 iter/s, 8.75615s/100 iters), loss = 0.124138
I1124 09:10:52.387476 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:10:52.387476 32468 solver.cpp:237]     Train net output #1: loss = 0.124138 (* 1 = 0.124138 loss)
I1124 09:10:52.387476 32468 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1124 09:11:01.192915 32468 solver.cpp:218] Iteration 8300 (11.357 iter/s, 8.80517s/100 iters), loss = 0.148055
I1124 09:11:01.193915 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:11:01.193915 32468 solver.cpp:237]     Train net output #1: loss = 0.148055 (* 1 = 0.148055 loss)
I1124 09:11:01.193915 32468 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1124 09:11:10.044492 32468 solver.cpp:218] Iteration 8400 (11.2988 iter/s, 8.85048s/100 iters), loss = 0.168151
I1124 09:11:10.044492 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1124 09:11:10.044492 32468 solver.cpp:237]     Train net output #1: loss = 0.168152 (* 1 = 0.168152 loss)
I1124 09:11:10.044492 32468 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1124 09:11:18.222416 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:11:18.560853 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_8500.caffemodel
I1124 09:11:18.598848 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_8500.solverstate
I1124 09:11:18.617858 32468 solver.cpp:330] Iteration 8500, Testing net (#0)
I1124 09:11:18.617858 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:11:21.063231 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:11:21.162266 32468 solver.cpp:397]     Test net output #0: accuracy = 0.8985
I1124 09:11:21.163247 32468 solver.cpp:397]     Test net output #1: loss = 0.312072 (* 1 = 0.312072 loss)
I1124 09:11:21.246276 32468 solver.cpp:218] Iteration 8500 (8.92777 iter/s, 11.201s/100 iters), loss = 0.107408
I1124 09:11:21.246276 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:11:21.246276 32468 solver.cpp:237]     Train net output #1: loss = 0.107408 (* 1 = 0.107408 loss)
I1124 09:11:21.246276 32468 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1124 09:11:29.807283 32468 solver.cpp:218] Iteration 8600 (11.681 iter/s, 8.56094s/100 iters), loss = 0.144269
I1124 09:11:29.808279 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:11:29.808279 32468 solver.cpp:237]     Train net output #1: loss = 0.144269 (* 1 = 0.144269 loss)
I1124 09:11:29.808279 32468 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1124 09:11:38.372500 32468 solver.cpp:218] Iteration 8700 (11.6765 iter/s, 8.56421s/100 iters), loss = 0.115777
I1124 09:11:38.372500 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:11:38.372500 32468 solver.cpp:237]     Train net output #1: loss = 0.115777 (* 1 = 0.115777 loss)
I1124 09:11:38.372500 32468 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1124 09:11:46.962611 32468 solver.cpp:218] Iteration 8800 (11.6422 iter/s, 8.58947s/100 iters), loss = 0.119571
I1124 09:11:46.962611 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 09:11:46.962611 32468 solver.cpp:237]     Train net output #1: loss = 0.119571 (* 1 = 0.119571 loss)
I1124 09:11:46.962611 32468 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1124 09:11:55.527057 32468 solver.cpp:218] Iteration 8900 (11.6768 iter/s, 8.56398s/100 iters), loss = 0.123184
I1124 09:11:55.527557 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:11:55.527557 32468 solver.cpp:237]     Train net output #1: loss = 0.123184 (* 1 = 0.123184 loss)
I1124 09:11:55.527557 32468 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1124 09:12:03.694725 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:12:04.033836 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_9000.caffemodel
I1124 09:12:04.074853 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_9000.solverstate
I1124 09:12:04.094853 32468 solver.cpp:330] Iteration 9000, Testing net (#0)
I1124 09:12:04.094853 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:12:06.551249 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:12:06.650854 32468 solver.cpp:397]     Test net output #0: accuracy = 0.8935
I1124 09:12:06.650854 32468 solver.cpp:397]     Test net output #1: loss = 0.321606 (* 1 = 0.321606 loss)
I1124 09:12:06.733216 32468 solver.cpp:218] Iteration 9000 (8.92398 iter/s, 11.2058s/100 iters), loss = 0.103434
I1124 09:12:06.733216 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:12:06.733216 32468 solver.cpp:237]     Train net output #1: loss = 0.103434 (* 1 = 0.103434 loss)
I1124 09:12:06.733216 32468 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1124 09:12:15.309798 32468 solver.cpp:218] Iteration 9100 (11.6607 iter/s, 8.57584s/100 iters), loss = 0.101
I1124 09:12:15.309798 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:12:15.309798 32468 solver.cpp:237]     Train net output #1: loss = 0.101 (* 1 = 0.101 loss)
I1124 09:12:15.309798 32468 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1124 09:12:23.871421 32468 solver.cpp:218] Iteration 9200 (11.6801 iter/s, 8.5616s/100 iters), loss = 0.118967
I1124 09:12:23.871421 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:12:23.872421 32468 solver.cpp:237]     Train net output #1: loss = 0.118967 (* 1 = 0.118967 loss)
I1124 09:12:23.872421 32468 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1124 09:12:32.435022 32468 solver.cpp:218] Iteration 9300 (11.6783 iter/s, 8.56291s/100 iters), loss = 0.0702558
I1124 09:12:32.435022 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:12:32.435022 32468 solver.cpp:237]     Train net output #1: loss = 0.0702558 (* 1 = 0.0702558 loss)
I1124 09:12:32.435022 32468 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1124 09:12:40.997993 32468 solver.cpp:218] Iteration 9400 (11.6796 iter/s, 8.5619s/100 iters), loss = 0.0851935
I1124 09:12:40.997993 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:12:40.997993 32468 solver.cpp:237]     Train net output #1: loss = 0.0851935 (* 1 = 0.0851935 loss)
I1124 09:12:40.997993 32468 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1124 09:12:49.159803 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:12:49.501281 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_9500.caffemodel
I1124 09:12:49.542281 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_9500.solverstate
I1124 09:12:49.564287 32468 solver.cpp:330] Iteration 9500, Testing net (#0)
I1124 09:12:49.564287 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:12:52.097477 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:12:52.211510 32468 solver.cpp:397]     Test net output #0: accuracy = 0.8863
I1124 09:12:52.211510 32468 solver.cpp:397]     Test net output #1: loss = 0.345234 (* 1 = 0.345234 loss)
I1124 09:12:52.304234 32468 solver.cpp:218] Iteration 9500 (8.84472 iter/s, 11.3062s/100 iters), loss = 0.113065
I1124 09:12:52.304234 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:12:52.304234 32468 solver.cpp:237]     Train net output #1: loss = 0.113065 (* 1 = 0.113065 loss)
I1124 09:12:52.304234 32468 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1124 09:12:52.304234 32468 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1124 09:13:01.066651 32468 solver.cpp:218] Iteration 9600 (11.4129 iter/s, 8.76202s/100 iters), loss = 0.114372
I1124 09:13:01.066651 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:13:01.066651 32468 solver.cpp:237]     Train net output #1: loss = 0.114372 (* 1 = 0.114372 loss)
I1124 09:13:01.066651 32468 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1124 09:13:09.722216 32468 solver.cpp:218] Iteration 9700 (11.5539 iter/s, 8.65505s/100 iters), loss = 0.0907664
I1124 09:13:09.722216 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:13:09.722216 32468 solver.cpp:237]     Train net output #1: loss = 0.0907664 (* 1 = 0.0907664 loss)
I1124 09:13:09.722216 32468 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1124 09:13:18.398046 32468 solver.cpp:218] Iteration 9800 (11.5279 iter/s, 8.67461s/100 iters), loss = 0.108043
I1124 09:13:18.398046 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:13:18.398046 32468 solver.cpp:237]     Train net output #1: loss = 0.108043 (* 1 = 0.108043 loss)
I1124 09:13:18.398046 32468 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1124 09:13:27.058153 32468 solver.cpp:218] Iteration 9900 (11.5479 iter/s, 8.65955s/100 iters), loss = 0.100334
I1124 09:13:27.058153 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:13:27.058153 32468 solver.cpp:237]     Train net output #1: loss = 0.100334 (* 1 = 0.100334 loss)
I1124 09:13:27.058153 32468 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1124 09:13:35.313998 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:13:35.654002 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_10000.caffemodel
I1124 09:13:35.695003 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_10000.solverstate
I1124 09:13:35.713002 32468 solver.cpp:330] Iteration 10000, Testing net (#0)
I1124 09:13:35.713002 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:13:38.165531 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:13:38.265547 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9077
I1124 09:13:38.266535 32468 solver.cpp:397]     Test net output #1: loss = 0.284822 (* 1 = 0.284822 loss)
I1124 09:13:38.349570 32468 solver.cpp:218] Iteration 10000 (8.85643 iter/s, 11.2912s/100 iters), loss = 0.110193
I1124 09:13:38.349570 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:13:38.349570 32468 solver.cpp:237]     Train net output #1: loss = 0.110193 (* 1 = 0.110193 loss)
I1124 09:13:38.349570 32468 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1124 09:13:47.116703 32468 solver.cpp:218] Iteration 10100 (11.4067 iter/s, 8.76679s/100 iters), loss = 0.100783
I1124 09:13:47.116703 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:13:47.116703 32468 solver.cpp:237]     Train net output #1: loss = 0.100783 (* 1 = 0.100783 loss)
I1124 09:13:47.116703 32468 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1124 09:13:55.830178 32468 solver.cpp:218] Iteration 10200 (11.4779 iter/s, 8.71239s/100 iters), loss = 0.0745065
I1124 09:13:55.830178 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:13:55.830178 32468 solver.cpp:237]     Train net output #1: loss = 0.0745065 (* 1 = 0.0745065 loss)
I1124 09:13:55.830178 32468 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1124 09:14:04.467614 32468 solver.cpp:218] Iteration 10300 (11.5796 iter/s, 8.63591s/100 iters), loss = 0.0957328
I1124 09:14:04.467614 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:14:04.467614 32468 solver.cpp:237]     Train net output #1: loss = 0.0957329 (* 1 = 0.0957329 loss)
I1124 09:14:04.467614 32468 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1124 09:14:13.041091 32468 solver.cpp:218] Iteration 10400 (11.6645 iter/s, 8.57298s/100 iters), loss = 0.0770365
I1124 09:14:13.041091 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:14:13.041091 32468 solver.cpp:237]     Train net output #1: loss = 0.0770366 (* 1 = 0.0770366 loss)
I1124 09:14:13.041091 32468 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1124 09:14:21.189476 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:14:21.527719 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_10500.caffemodel
I1124 09:14:21.568724 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_10500.solverstate
I1124 09:14:21.587218 32468 solver.cpp:330] Iteration 10500, Testing net (#0)
I1124 09:14:21.587218 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:14:24.039355 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:14:24.138782 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9076
I1124 09:14:24.138782 32468 solver.cpp:397]     Test net output #1: loss = 0.285659 (* 1 = 0.285659 loss)
I1124 09:14:24.222821 32468 solver.cpp:218] Iteration 10500 (8.94381 iter/s, 11.1809s/100 iters), loss = 0.0864234
I1124 09:14:24.222821 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:14:24.222821 32468 solver.cpp:237]     Train net output #1: loss = 0.0864234 (* 1 = 0.0864234 loss)
I1124 09:14:24.222821 32468 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1124 09:14:32.789718 32468 solver.cpp:218] Iteration 10600 (11.6736 iter/s, 8.56632s/100 iters), loss = 0.132406
I1124 09:14:32.789718 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:14:32.789718 32468 solver.cpp:237]     Train net output #1: loss = 0.132406 (* 1 = 0.132406 loss)
I1124 09:14:32.789718 32468 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1124 09:14:41.327177 32468 solver.cpp:218] Iteration 10700 (11.7134 iter/s, 8.5372s/100 iters), loss = 0.0671732
I1124 09:14:41.327177 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:14:41.327177 32468 solver.cpp:237]     Train net output #1: loss = 0.0671732 (* 1 = 0.0671732 loss)
I1124 09:14:41.327177 32468 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1124 09:14:49.864395 32468 solver.cpp:218] Iteration 10800 (11.7136 iter/s, 8.53711s/100 iters), loss = 0.0947914
I1124 09:14:49.864395 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:14:49.864395 32468 solver.cpp:237]     Train net output #1: loss = 0.0947914 (* 1 = 0.0947914 loss)
I1124 09:14:49.864395 32468 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1124 09:14:58.409222 32468 solver.cpp:218] Iteration 10900 (11.7044 iter/s, 8.54377s/100 iters), loss = 0.0566189
I1124 09:14:58.409222 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:14:58.409222 32468 solver.cpp:237]     Train net output #1: loss = 0.0566189 (* 1 = 0.0566189 loss)
I1124 09:14:58.409222 32468 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1124 09:15:06.546787 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:15:06.884608 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_11000.caffemodel
I1124 09:15:06.923593 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_11000.solverstate
I1124 09:15:06.941609 32468 solver.cpp:330] Iteration 11000, Testing net (#0)
I1124 09:15:06.942093 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:15:09.388602 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:15:09.488638 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9079
I1124 09:15:09.488638 32468 solver.cpp:397]     Test net output #1: loss = 0.285857 (* 1 = 0.285857 loss)
I1124 09:15:09.572655 32468 solver.cpp:218] Iteration 11000 (8.95845 iter/s, 11.1626s/100 iters), loss = 0.0718299
I1124 09:15:09.572655 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:15:09.572655 32468 solver.cpp:237]     Train net output #1: loss = 0.07183 (* 1 = 0.07183 loss)
I1124 09:15:09.572655 32468 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1124 09:15:18.140866 32468 solver.cpp:218] Iteration 11100 (11.6715 iter/s, 8.56791s/100 iters), loss = 0.102143
I1124 09:15:18.140866 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:15:18.140866 32468 solver.cpp:237]     Train net output #1: loss = 0.102143 (* 1 = 0.102143 loss)
I1124 09:15:18.140866 32468 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1124 09:15:26.715389 32468 solver.cpp:218] Iteration 11200 (11.6632 iter/s, 8.57397s/100 iters), loss = 0.0950224
I1124 09:15:26.715389 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:15:26.715389 32468 solver.cpp:237]     Train net output #1: loss = 0.0950224 (* 1 = 0.0950224 loss)
I1124 09:15:26.715389 32468 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1124 09:15:35.304157 32468 solver.cpp:218] Iteration 11300 (11.6442 iter/s, 8.588s/100 iters), loss = 0.0737068
I1124 09:15:35.304157 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:15:35.304157 32468 solver.cpp:237]     Train net output #1: loss = 0.0737068 (* 1 = 0.0737068 loss)
I1124 09:15:35.304157 32468 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1124 09:15:43.870791 32468 solver.cpp:218] Iteration 11400 (11.6739 iter/s, 8.56609s/100 iters), loss = 0.0634922
I1124 09:15:43.870791 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:15:43.870791 32468 solver.cpp:237]     Train net output #1: loss = 0.0634922 (* 1 = 0.0634922 loss)
I1124 09:15:43.870791 32468 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1124 09:15:52.004643 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:15:52.342803 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_11500.caffemodel
I1124 09:15:52.383802 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_11500.solverstate
I1124 09:15:52.401800 32468 solver.cpp:330] Iteration 11500, Testing net (#0)
I1124 09:15:52.401800 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:15:54.857761 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:15:54.956784 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9087
I1124 09:15:54.957769 32468 solver.cpp:397]     Test net output #1: loss = 0.28571 (* 1 = 0.28571 loss)
I1124 09:15:55.040786 32468 solver.cpp:218] Iteration 11500 (8.95272 iter/s, 11.1698s/100 iters), loss = 0.0886516
I1124 09:15:55.040786 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:15:55.040786 32468 solver.cpp:237]     Train net output #1: loss = 0.0886515 (* 1 = 0.0886515 loss)
I1124 09:15:55.040786 32468 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1124 09:16:03.679958 32468 solver.cpp:218] Iteration 11600 (11.576 iter/s, 8.63855s/100 iters), loss = 0.108551
I1124 09:16:03.679958 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 09:16:03.679958 32468 solver.cpp:237]     Train net output #1: loss = 0.108551 (* 1 = 0.108551 loss)
I1124 09:16:03.679958 32468 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1124 09:16:12.359575 32468 solver.cpp:218] Iteration 11700 (11.5212 iter/s, 8.67965s/100 iters), loss = 0.116447
I1124 09:16:12.359575 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:16:12.360575 32468 solver.cpp:237]     Train net output #1: loss = 0.116447 (* 1 = 0.116447 loss)
I1124 09:16:12.360575 32468 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1124 09:16:21.028343 32468 solver.cpp:218] Iteration 11800 (11.5372 iter/s, 8.6676s/100 iters), loss = 0.0882942
I1124 09:16:21.028343 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:16:21.028343 32468 solver.cpp:237]     Train net output #1: loss = 0.0882942 (* 1 = 0.0882942 loss)
I1124 09:16:21.028343 32468 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1124 09:16:29.672749 32468 solver.cpp:218] Iteration 11900 (11.5688 iter/s, 8.64396s/100 iters), loss = 0.0472709
I1124 09:16:29.672749 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:16:29.672749 32468 solver.cpp:237]     Train net output #1: loss = 0.0472708 (* 1 = 0.0472708 loss)
I1124 09:16:29.672749 32468 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1124 09:16:37.877071 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:16:38.225047 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_12000.caffemodel
I1124 09:16:38.264631 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_12000.solverstate
I1124 09:16:38.282655 32468 solver.cpp:330] Iteration 12000, Testing net (#0)
I1124 09:16:38.282655 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:16:40.773106 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:16:40.874258 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9086
I1124 09:16:40.874258 32468 solver.cpp:397]     Test net output #1: loss = 0.286938 (* 1 = 0.286938 loss)
I1124 09:16:40.959794 32468 solver.cpp:218] Iteration 12000 (8.8602 iter/s, 11.2864s/100 iters), loss = 0.0764037
I1124 09:16:40.959794 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:16:40.959794 32468 solver.cpp:237]     Train net output #1: loss = 0.0764037 (* 1 = 0.0764037 loss)
I1124 09:16:40.959794 32468 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1124 09:16:49.525732 32468 solver.cpp:218] Iteration 12100 (11.6748 iter/s, 8.56542s/100 iters), loss = 0.121078
I1124 09:16:49.525732 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 09:16:49.525732 32468 solver.cpp:237]     Train net output #1: loss = 0.121078 (* 1 = 0.121078 loss)
I1124 09:16:49.525732 32468 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1124 09:16:58.140038 32468 solver.cpp:218] Iteration 12200 (11.6091 iter/s, 8.6139s/100 iters), loss = 0.082146
I1124 09:16:58.140038 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:16:58.140038 32468 solver.cpp:237]     Train net output #1: loss = 0.0821459 (* 1 = 0.0821459 loss)
I1124 09:16:58.140038 32468 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1124 09:17:06.756137 32468 solver.cpp:218] Iteration 12300 (11.6075 iter/s, 8.61514s/100 iters), loss = 0.0389723
I1124 09:17:06.756137 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:17:06.756137 32468 solver.cpp:237]     Train net output #1: loss = 0.0389722 (* 1 = 0.0389722 loss)
I1124 09:17:06.756137 32468 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1124 09:17:15.304476 32468 solver.cpp:218] Iteration 12400 (11.6983 iter/s, 8.54825s/100 iters), loss = 0.0624652
I1124 09:17:15.304476 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:17:15.304476 32468 solver.cpp:237]     Train net output #1: loss = 0.0624651 (* 1 = 0.0624651 loss)
I1124 09:17:15.304476 32468 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1124 09:17:23.451967 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:17:23.790107 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_12500.caffemodel
I1124 09:17:23.830082 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_12500.solverstate
I1124 09:17:23.849102 32468 solver.cpp:330] Iteration 12500, Testing net (#0)
I1124 09:17:23.849102 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:17:26.293547 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:17:26.392593 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9093
I1124 09:17:26.392593 32468 solver.cpp:397]     Test net output #1: loss = 0.286289 (* 1 = 0.286289 loss)
I1124 09:17:26.475111 32468 solver.cpp:218] Iteration 12500 (8.95266 iter/s, 11.1699s/100 iters), loss = 0.0758568
I1124 09:17:26.475111 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:17:26.475111 32468 solver.cpp:237]     Train net output #1: loss = 0.0758567 (* 1 = 0.0758567 loss)
I1124 09:17:26.475111 32468 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1124 09:17:35.010998 32468 solver.cpp:218] Iteration 12600 (11.7169 iter/s, 8.53472s/100 iters), loss = 0.114454
I1124 09:17:35.010998 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:17:35.010998 32468 solver.cpp:237]     Train net output #1: loss = 0.114454 (* 1 = 0.114454 loss)
I1124 09:17:35.010998 32468 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1124 09:17:43.546695 32468 solver.cpp:218] Iteration 12700 (11.716 iter/s, 8.53531s/100 iters), loss = 0.0682882
I1124 09:17:43.546695 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:17:43.546695 32468 solver.cpp:237]     Train net output #1: loss = 0.068288 (* 1 = 0.068288 loss)
I1124 09:17:43.546695 32468 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1124 09:17:52.081676 32468 solver.cpp:218] Iteration 12800 (11.717 iter/s, 8.53462s/100 iters), loss = 0.0713522
I1124 09:17:52.081676 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:17:52.081676 32468 solver.cpp:237]     Train net output #1: loss = 0.0713521 (* 1 = 0.0713521 loss)
I1124 09:17:52.081676 32468 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1124 09:18:00.621492 32468 solver.cpp:218] Iteration 12900 (11.7108 iter/s, 8.53913s/100 iters), loss = 0.0866955
I1124 09:18:00.621492 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:18:00.621492 32468 solver.cpp:237]     Train net output #1: loss = 0.0866953 (* 1 = 0.0866953 loss)
I1124 09:18:00.621492 32468 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1124 09:18:08.738152 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:18:09.076903 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_13000.caffemodel
I1124 09:18:09.114887 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_13000.solverstate
I1124 09:18:09.134387 32468 solver.cpp:330] Iteration 13000, Testing net (#0)
I1124 09:18:09.134387 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:18:11.576360 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:18:11.675458 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9084
I1124 09:18:11.675458 32468 solver.cpp:397]     Test net output #1: loss = 0.288686 (* 1 = 0.288686 loss)
I1124 09:18:11.758996 32468 solver.cpp:218] Iteration 13000 (8.97864 iter/s, 11.1375s/100 iters), loss = 0.0678273
I1124 09:18:11.758996 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:18:11.758996 32468 solver.cpp:237]     Train net output #1: loss = 0.0678272 (* 1 = 0.0678272 loss)
I1124 09:18:11.758996 32468 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1124 09:18:20.300357 32468 solver.cpp:218] Iteration 13100 (11.7088 iter/s, 8.54058s/100 iters), loss = 0.0839633
I1124 09:18:20.300357 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:18:20.300357 32468 solver.cpp:237]     Train net output #1: loss = 0.0839631 (* 1 = 0.0839631 loss)
I1124 09:18:20.300357 32468 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1124 09:18:28.834326 32468 solver.cpp:218] Iteration 13200 (11.7191 iter/s, 8.53309s/100 iters), loss = 0.0615931
I1124 09:18:28.834326 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:18:28.834326 32468 solver.cpp:237]     Train net output #1: loss = 0.061593 (* 1 = 0.061593 loss)
I1124 09:18:28.834326 32468 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1124 09:18:37.370070 32468 solver.cpp:218] Iteration 13300 (11.7154 iter/s, 8.53578s/100 iters), loss = 0.0384173
I1124 09:18:37.370070 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:18:37.370070 32468 solver.cpp:237]     Train net output #1: loss = 0.0384171 (* 1 = 0.0384171 loss)
I1124 09:18:37.370070 32468 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1124 09:18:45.910362 32468 solver.cpp:218] Iteration 13400 (11.7099 iter/s, 8.53981s/100 iters), loss = 0.0344654
I1124 09:18:45.910362 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:18:45.910362 32468 solver.cpp:237]     Train net output #1: loss = 0.0344652 (* 1 = 0.0344652 loss)
I1124 09:18:45.910362 32468 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1124 09:18:54.028863 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:18:54.367591 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_13500.caffemodel
I1124 09:18:54.408596 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_13500.solverstate
I1124 09:18:54.426595 32468 solver.cpp:330] Iteration 13500, Testing net (#0)
I1124 09:18:54.426595 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:18:56.867020 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:18:56.966037 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9089
I1124 09:18:56.966037 32468 solver.cpp:397]     Test net output #1: loss = 0.287752 (* 1 = 0.287752 loss)
I1124 09:18:57.049069 32468 solver.cpp:218] Iteration 13500 (8.97857 iter/s, 11.1376s/100 iters), loss = 0.0678635
I1124 09:18:57.049069 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:18:57.049069 32468 solver.cpp:237]     Train net output #1: loss = 0.0678633 (* 1 = 0.0678633 loss)
I1124 09:18:57.049069 32468 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1124 09:19:05.582593 32468 solver.cpp:218] Iteration 13600 (11.7185 iter/s, 8.53354s/100 iters), loss = 0.0943637
I1124 09:19:05.582593 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:19:05.582593 32468 solver.cpp:237]     Train net output #1: loss = 0.0943635 (* 1 = 0.0943635 loss)
I1124 09:19:05.582593 32468 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1124 09:19:14.125073 32468 solver.cpp:218] Iteration 13700 (11.7068 iter/s, 8.54201s/100 iters), loss = 0.0432087
I1124 09:19:14.125073 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:19:14.125073 32468 solver.cpp:237]     Train net output #1: loss = 0.0432085 (* 1 = 0.0432085 loss)
I1124 09:19:14.125073 32468 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1124 09:19:22.655498 32468 solver.cpp:218] Iteration 13800 (11.7235 iter/s, 8.52985s/100 iters), loss = 0.0776354
I1124 09:19:22.655498 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:19:22.655498 32468 solver.cpp:237]     Train net output #1: loss = 0.0776352 (* 1 = 0.0776352 loss)
I1124 09:19:22.655498 32468 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1124 09:19:31.189338 32468 solver.cpp:218] Iteration 13900 (11.7198 iter/s, 8.53254s/100 iters), loss = 0.0558766
I1124 09:19:31.189338 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:19:31.189338 32468 solver.cpp:237]     Train net output #1: loss = 0.0558764 (* 1 = 0.0558764 loss)
I1124 09:19:31.189338 32468 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1124 09:19:39.295856 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:19:39.633570 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_14000.caffemodel
I1124 09:19:39.672564 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_14000.solverstate
I1124 09:19:39.691063 32468 solver.cpp:330] Iteration 14000, Testing net (#0)
I1124 09:19:39.691063 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:19:42.130841 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:19:42.230518 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9097
I1124 09:19:42.230518 32468 solver.cpp:397]     Test net output #1: loss = 0.289036 (* 1 = 0.289036 loss)
I1124 09:19:42.313516 32468 solver.cpp:218] Iteration 14000 (8.98987 iter/s, 11.1236s/100 iters), loss = 0.0717181
I1124 09:19:42.313516 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:19:42.313516 32468 solver.cpp:237]     Train net output #1: loss = 0.0717179 (* 1 = 0.0717179 loss)
I1124 09:19:42.313516 32468 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1124 09:19:50.855734 32468 solver.cpp:218] Iteration 14100 (11.7068 iter/s, 8.54208s/100 iters), loss = 0.0980952
I1124 09:19:50.855734 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:19:50.855734 32468 solver.cpp:237]     Train net output #1: loss = 0.098095 (* 1 = 0.098095 loss)
I1124 09:19:50.855734 32468 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1124 09:19:59.390229 32468 solver.cpp:218] Iteration 14200 (11.7176 iter/s, 8.53419s/100 iters), loss = 0.0632701
I1124 09:19:59.390229 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:19:59.390229 32468 solver.cpp:237]     Train net output #1: loss = 0.0632699 (* 1 = 0.0632699 loss)
I1124 09:19:59.390229 32468 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1124 09:20:07.951756 32468 solver.cpp:218] Iteration 14300 (11.6808 iter/s, 8.56104s/100 iters), loss = 0.0733116
I1124 09:20:07.951756 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:20:07.951756 32468 solver.cpp:237]     Train net output #1: loss = 0.0733115 (* 1 = 0.0733115 loss)
I1124 09:20:07.951756 32468 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1124 09:20:16.488633 32468 solver.cpp:218] Iteration 14400 (11.7143 iter/s, 8.53659s/100 iters), loss = 0.0418753
I1124 09:20:16.489634 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:20:16.489634 32468 solver.cpp:237]     Train net output #1: loss = 0.0418751 (* 1 = 0.0418751 loss)
I1124 09:20:16.489634 32468 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1124 09:20:24.601680 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:20:24.938802 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_14500.caffemodel
I1124 09:20:24.978819 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_14500.solverstate
I1124 09:20:24.997303 32468 solver.cpp:330] Iteration 14500, Testing net (#0)
I1124 09:20:24.997303 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:20:27.441236 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:20:27.539249 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9099
I1124 09:20:27.539249 32468 solver.cpp:397]     Test net output #1: loss = 0.291726 (* 1 = 0.291726 loss)
I1124 09:20:27.622252 32468 solver.cpp:218] Iteration 14500 (8.98244 iter/s, 11.1328s/100 iters), loss = 0.0849238
I1124 09:20:27.622252 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:20:27.622252 32468 solver.cpp:237]     Train net output #1: loss = 0.0849236 (* 1 = 0.0849236 loss)
I1124 09:20:27.622252 32468 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1124 09:20:36.155800 32468 solver.cpp:218] Iteration 14600 (11.72 iter/s, 8.5324s/100 iters), loss = 0.0756716
I1124 09:20:36.155800 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:20:36.155800 32468 solver.cpp:237]     Train net output #1: loss = 0.0756714 (* 1 = 0.0756714 loss)
I1124 09:20:36.155800 32468 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1124 09:20:44.691617 32468 solver.cpp:218] Iteration 14700 (11.7155 iter/s, 8.53568s/100 iters), loss = 0.10109
I1124 09:20:44.691617 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:20:44.691617 32468 solver.cpp:237]     Train net output #1: loss = 0.10109 (* 1 = 0.10109 loss)
I1124 09:20:44.691617 32468 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1124 09:20:53.227113 32468 solver.cpp:218] Iteration 14800 (11.7168 iter/s, 8.53477s/100 iters), loss = 0.0866278
I1124 09:20:53.227113 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:20:53.227113 32468 solver.cpp:237]     Train net output #1: loss = 0.0866276 (* 1 = 0.0866276 loss)
I1124 09:20:53.227113 32468 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1124 09:21:01.761221 32468 solver.cpp:218] Iteration 14900 (11.7185 iter/s, 8.5335s/100 iters), loss = 0.0748671
I1124 09:21:01.761221 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:21:01.761221 32468 solver.cpp:237]     Train net output #1: loss = 0.074867 (* 1 = 0.074867 loss)
I1124 09:21:01.761221 32468 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1124 09:21:09.870280 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:21:10.207615 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_15000.caffemodel
I1124 09:21:10.247254 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_15000.solverstate
I1124 09:21:10.265265 32468 solver.cpp:330] Iteration 15000, Testing net (#0)
I1124 09:21:10.265265 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:21:12.708364 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:21:12.807396 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9089
I1124 09:21:12.807396 32468 solver.cpp:397]     Test net output #1: loss = 0.290514 (* 1 = 0.290514 loss)
I1124 09:21:12.890446 32468 solver.cpp:218] Iteration 15000 (8.9854 iter/s, 11.1292s/100 iters), loss = 0.0579343
I1124 09:21:12.890446 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:21:12.890446 32468 solver.cpp:237]     Train net output #1: loss = 0.0579342 (* 1 = 0.0579342 loss)
I1124 09:21:12.890446 32468 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1124 09:21:21.422799 32468 solver.cpp:218] Iteration 15100 (11.7218 iter/s, 8.53111s/100 iters), loss = 0.0587117
I1124 09:21:21.422799 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:21:21.422799 32468 solver.cpp:237]     Train net output #1: loss = 0.0587115 (* 1 = 0.0587115 loss)
I1124 09:21:21.422799 32468 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1124 09:21:29.957492 32468 solver.cpp:218] Iteration 15200 (11.7172 iter/s, 8.53445s/100 iters), loss = 0.0377765
I1124 09:21:29.957492 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:21:29.957492 32468 solver.cpp:237]     Train net output #1: loss = 0.0377764 (* 1 = 0.0377764 loss)
I1124 09:21:29.957492 32468 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1124 09:21:38.478930 32468 solver.cpp:218] Iteration 15300 (11.7361 iter/s, 8.5207s/100 iters), loss = 0.0518884
I1124 09:21:38.478930 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:21:38.478930 32468 solver.cpp:237]     Train net output #1: loss = 0.0518882 (* 1 = 0.0518882 loss)
I1124 09:21:38.478930 32468 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1124 09:21:38.478930 32468 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1124 09:21:47.012284 32468 solver.cpp:218] Iteration 15400 (11.7193 iter/s, 8.53293s/100 iters), loss = 0.061531
I1124 09:21:47.012284 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:21:47.012284 32468 solver.cpp:237]     Train net output #1: loss = 0.0615308 (* 1 = 0.0615308 loss)
I1124 09:21:47.012284 32468 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1124 09:21:55.126648 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:21:55.464284 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_15500.caffemodel
I1124 09:21:55.502348 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_15500.solverstate
I1124 09:21:55.521348 32468 solver.cpp:330] Iteration 15500, Testing net (#0)
I1124 09:21:55.521348 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:21:57.963163 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:21:58.062515 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9081
I1124 09:21:58.062515 32468 solver.cpp:397]     Test net output #1: loss = 0.293276 (* 1 = 0.293276 loss)
I1124 09:21:58.145534 32468 solver.cpp:218] Iteration 15500 (8.98189 iter/s, 11.1335s/100 iters), loss = 0.0763999
I1124 09:21:58.146522 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:21:58.146522 32468 solver.cpp:237]     Train net output #1: loss = 0.0763997 (* 1 = 0.0763997 loss)
I1124 09:21:58.146522 32468 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1124 09:22:06.681188 32468 solver.cpp:218] Iteration 15600 (11.7175 iter/s, 8.53422s/100 iters), loss = 0.103701
I1124 09:22:06.681188 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:22:06.681188 32468 solver.cpp:237]     Train net output #1: loss = 0.103701 (* 1 = 0.103701 loss)
I1124 09:22:06.681188 32468 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1124 09:22:15.222180 32468 solver.cpp:218] Iteration 15700 (11.7082 iter/s, 8.54105s/100 iters), loss = 0.038833
I1124 09:22:15.222180 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:22:15.222180 32468 solver.cpp:237]     Train net output #1: loss = 0.0388328 (* 1 = 0.0388328 loss)
I1124 09:22:15.222180 32468 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1124 09:22:23.756135 32468 solver.cpp:218] Iteration 15800 (11.7186 iter/s, 8.53344s/100 iters), loss = 0.0686985
I1124 09:22:23.756135 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:22:23.756135 32468 solver.cpp:237]     Train net output #1: loss = 0.0686983 (* 1 = 0.0686983 loss)
I1124 09:22:23.756135 32468 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1124 09:22:32.294934 32468 solver.cpp:218] Iteration 15900 (11.7127 iter/s, 8.53777s/100 iters), loss = 0.0625731
I1124 09:22:32.294934 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:22:32.294934 32468 solver.cpp:237]     Train net output #1: loss = 0.0625729 (* 1 = 0.0625729 loss)
I1124 09:22:32.294934 32468 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1124 09:22:40.419001 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:22:40.755427 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_16000.caffemodel
I1124 09:22:40.794426 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_16000.solverstate
I1124 09:22:40.812445 32468 solver.cpp:330] Iteration 16000, Testing net (#0)
I1124 09:22:40.812445 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:22:43.255399 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:22:43.354436 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9086
I1124 09:22:43.354436 32468 solver.cpp:397]     Test net output #1: loss = 0.293101 (* 1 = 0.293101 loss)
I1124 09:22:43.437489 32468 solver.cpp:218] Iteration 16000 (8.97451 iter/s, 11.1427s/100 iters), loss = 0.048498
I1124 09:22:43.437489 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:22:43.437489 32468 solver.cpp:237]     Train net output #1: loss = 0.0484978 (* 1 = 0.0484978 loss)
I1124 09:22:43.437489 32468 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1124 09:22:51.971784 32468 solver.cpp:218] Iteration 16100 (11.7182 iter/s, 8.53371s/100 iters), loss = 0.0689387
I1124 09:22:51.971784 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:22:51.971784 32468 solver.cpp:237]     Train net output #1: loss = 0.0689385 (* 1 = 0.0689385 loss)
I1124 09:22:51.971784 32468 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1124 09:23:00.506196 32468 solver.cpp:218] Iteration 16200 (11.7184 iter/s, 8.53358s/100 iters), loss = 0.0782926
I1124 09:23:00.506196 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:23:00.506196 32468 solver.cpp:237]     Train net output #1: loss = 0.0782924 (* 1 = 0.0782924 loss)
I1124 09:23:00.506196 32468 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1124 09:23:09.041755 32468 solver.cpp:218] Iteration 16300 (11.7159 iter/s, 8.53541s/100 iters), loss = 0.0583167
I1124 09:23:09.041755 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:23:09.041755 32468 solver.cpp:237]     Train net output #1: loss = 0.0583165 (* 1 = 0.0583165 loss)
I1124 09:23:09.041755 32468 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1124 09:23:17.573942 32468 solver.cpp:218] Iteration 16400 (11.7217 iter/s, 8.53121s/100 iters), loss = 0.029287
I1124 09:23:17.573942 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:23:17.573942 32468 solver.cpp:237]     Train net output #1: loss = 0.0292868 (* 1 = 0.0292868 loss)
I1124 09:23:17.573942 32468 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1124 09:23:25.686369 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:23:26.024740 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_16500.caffemodel
I1124 09:23:26.064798 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_16500.solverstate
I1124 09:23:26.084801 32468 solver.cpp:330] Iteration 16500, Testing net (#0)
I1124 09:23:26.084801 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:23:28.525326 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:23:28.624346 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9087
I1124 09:23:28.624346 32468 solver.cpp:397]     Test net output #1: loss = 0.292656 (* 1 = 0.292656 loss)
I1124 09:23:28.708211 32468 solver.cpp:218] Iteration 16500 (8.98156 iter/s, 11.1339s/100 iters), loss = 0.071385
I1124 09:23:28.708211 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:23:28.708211 32468 solver.cpp:237]     Train net output #1: loss = 0.0713848 (* 1 = 0.0713848 loss)
I1124 09:23:28.708211 32468 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1124 09:23:37.241384 32468 solver.cpp:218] Iteration 16600 (11.7204 iter/s, 8.53211s/100 iters), loss = 0.0805592
I1124 09:23:37.241384 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:23:37.241384 32468 solver.cpp:237]     Train net output #1: loss = 0.080559 (* 1 = 0.080559 loss)
I1124 09:23:37.241384 32468 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1124 09:23:45.778987 32468 solver.cpp:218] Iteration 16700 (11.7129 iter/s, 8.53763s/100 iters), loss = 0.0554537
I1124 09:23:45.778987 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:23:45.778987 32468 solver.cpp:237]     Train net output #1: loss = 0.0554536 (* 1 = 0.0554536 loss)
I1124 09:23:45.778987 32468 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1124 09:23:54.315371 32468 solver.cpp:218] Iteration 16800 (11.7154 iter/s, 8.53579s/100 iters), loss = 0.0951571
I1124 09:23:54.315371 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:23:54.315371 32468 solver.cpp:237]     Train net output #1: loss = 0.0951569 (* 1 = 0.0951569 loss)
I1124 09:23:54.315371 32468 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1124 09:24:02.856379 32468 solver.cpp:218] Iteration 16900 (11.7095 iter/s, 8.54009s/100 iters), loss = 0.033427
I1124 09:24:02.856379 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:24:02.856379 32468 solver.cpp:237]     Train net output #1: loss = 0.0334268 (* 1 = 0.0334268 loss)
I1124 09:24:02.856379 32468 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1124 09:24:10.968335 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:24:11.306253 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_17000.caffemodel
I1124 09:24:11.347738 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_17000.solverstate
I1124 09:24:11.366237 32468 solver.cpp:330] Iteration 17000, Testing net (#0)
I1124 09:24:11.366237 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:24:13.806617 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:24:13.905817 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9079
I1124 09:24:13.905817 32468 solver.cpp:397]     Test net output #1: loss = 0.292924 (* 1 = 0.292924 loss)
I1124 09:24:13.988366 32468 solver.cpp:218] Iteration 17000 (8.983 iter/s, 11.1321s/100 iters), loss = 0.0536442
I1124 09:24:13.988366 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:24:13.988366 32468 solver.cpp:237]     Train net output #1: loss = 0.053644 (* 1 = 0.053644 loss)
I1124 09:24:13.988366 32468 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1124 09:24:22.521970 32468 solver.cpp:218] Iteration 17100 (11.719 iter/s, 8.53314s/100 iters), loss = 0.0792612
I1124 09:24:22.521970 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:24:22.522956 32468 solver.cpp:237]     Train net output #1: loss = 0.079261 (* 1 = 0.079261 loss)
I1124 09:24:22.522956 32468 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1124 09:24:31.055667 32468 solver.cpp:218] Iteration 17200 (11.719 iter/s, 8.53315s/100 iters), loss = 0.0888506
I1124 09:24:31.055667 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:24:31.055667 32468 solver.cpp:237]     Train net output #1: loss = 0.0888504 (* 1 = 0.0888504 loss)
I1124 09:24:31.055667 32468 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1124 09:24:39.592638 32468 solver.cpp:218] Iteration 17300 (11.7155 iter/s, 8.53568s/100 iters), loss = 0.0646114
I1124 09:24:39.592638 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:24:39.592638 32468 solver.cpp:237]     Train net output #1: loss = 0.0646112 (* 1 = 0.0646112 loss)
I1124 09:24:39.592638 32468 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1124 09:24:48.124338 32468 solver.cpp:218] Iteration 17400 (11.7205 iter/s, 8.53202s/100 iters), loss = 0.0558709
I1124 09:24:48.125339 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:24:48.125339 32468 solver.cpp:237]     Train net output #1: loss = 0.0558707 (* 1 = 0.0558707 loss)
I1124 09:24:48.125339 32468 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1124 09:24:56.239414 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:24:56.577859 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_17500.caffemodel
I1124 09:24:56.617375 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_17500.solverstate
I1124 09:24:56.635367 32468 solver.cpp:330] Iteration 17500, Testing net (#0)
I1124 09:24:56.635367 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:24:59.077181 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:24:59.177194 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9084
I1124 09:24:59.177194 32468 solver.cpp:397]     Test net output #1: loss = 0.292857 (* 1 = 0.292857 loss)
I1124 09:24:59.260242 32468 solver.cpp:218] Iteration 17500 (8.98078 iter/s, 11.1349s/100 iters), loss = 0.0631738
I1124 09:24:59.260242 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:24:59.260242 32468 solver.cpp:237]     Train net output #1: loss = 0.0631736 (* 1 = 0.0631736 loss)
I1124 09:24:59.260242 32468 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1124 09:25:07.793316 32468 solver.cpp:218] Iteration 17600 (11.7195 iter/s, 8.53279s/100 iters), loss = 0.0674815
I1124 09:25:07.793316 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:25:07.793316 32468 solver.cpp:237]     Train net output #1: loss = 0.0674813 (* 1 = 0.0674813 loss)
I1124 09:25:07.793316 32468 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1124 09:25:16.325613 32468 solver.cpp:218] Iteration 17700 (11.7217 iter/s, 8.53122s/100 iters), loss = 0.0825494
I1124 09:25:16.325613 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:25:16.325613 32468 solver.cpp:237]     Train net output #1: loss = 0.0825492 (* 1 = 0.0825492 loss)
I1124 09:25:16.325613 32468 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1124 09:25:24.859728 32468 solver.cpp:218] Iteration 17800 (11.7175 iter/s, 8.53427s/100 iters), loss = 0.0766852
I1124 09:25:24.860728 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:25:24.860728 32468 solver.cpp:237]     Train net output #1: loss = 0.076685 (* 1 = 0.076685 loss)
I1124 09:25:24.860728 32468 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1124 09:25:33.397320 32468 solver.cpp:218] Iteration 17900 (11.7146 iter/s, 8.53636s/100 iters), loss = 0.0288702
I1124 09:25:33.397320 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:25:33.397320 32468 solver.cpp:237]     Train net output #1: loss = 0.0288699 (* 1 = 0.0288699 loss)
I1124 09:25:33.397320 32468 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1124 09:25:41.508292 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:25:41.847349 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_18000.caffemodel
I1124 09:25:41.887859 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_18000.solverstate
I1124 09:25:41.905856 32468 solver.cpp:330] Iteration 18000, Testing net (#0)
I1124 09:25:41.905856 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:25:44.346499 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:25:44.445531 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9087
I1124 09:25:44.445531 32468 solver.cpp:397]     Test net output #1: loss = 0.292942 (* 1 = 0.292942 loss)
I1124 09:25:44.528565 32468 solver.cpp:218] Iteration 18000 (8.98367 iter/s, 11.1313s/100 iters), loss = 0.0540986
I1124 09:25:44.528565 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:25:44.528565 32468 solver.cpp:237]     Train net output #1: loss = 0.0540984 (* 1 = 0.0540984 loss)
I1124 09:25:44.528565 32468 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1124 09:25:53.064483 32468 solver.cpp:218] Iteration 18100 (11.7159 iter/s, 8.53538s/100 iters), loss = 0.0771922
I1124 09:25:53.064483 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:25:53.064483 32468 solver.cpp:237]     Train net output #1: loss = 0.0771919 (* 1 = 0.0771919 loss)
I1124 09:25:53.064483 32468 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1124 09:26:01.599351 32468 solver.cpp:218] Iteration 18200 (11.7175 iter/s, 8.53423s/100 iters), loss = 0.0487739
I1124 09:26:01.599351 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:26:01.599351 32468 solver.cpp:237]     Train net output #1: loss = 0.0487737 (* 1 = 0.0487737 loss)
I1124 09:26:01.599351 32468 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1124 09:26:10.135144 32468 solver.cpp:218] Iteration 18300 (11.7171 iter/s, 8.53453s/100 iters), loss = 0.0604944
I1124 09:26:10.135144 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:26:10.135144 32468 solver.cpp:237]     Train net output #1: loss = 0.0604941 (* 1 = 0.0604941 loss)
I1124 09:26:10.135144 32468 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1124 09:26:18.670264 32468 solver.cpp:218] Iteration 18400 (11.7166 iter/s, 8.53491s/100 iters), loss = 0.0690379
I1124 09:26:18.670264 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:26:18.670264 32468 solver.cpp:237]     Train net output #1: loss = 0.0690377 (* 1 = 0.0690377 loss)
I1124 09:26:18.670264 32468 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1124 09:26:26.790297 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:26:27.129483 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_18500.caffemodel
I1124 09:26:27.168491 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_18500.solverstate
I1124 09:26:27.193511 32468 solver.cpp:330] Iteration 18500, Testing net (#0)
I1124 09:26:27.194511 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:26:29.636288 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:26:29.735306 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9079
I1124 09:26:29.735306 32468 solver.cpp:397]     Test net output #1: loss = 0.292518 (* 1 = 0.292518 loss)
I1124 09:26:29.818351 32468 solver.cpp:218] Iteration 18500 (8.97048 iter/s, 11.1477s/100 iters), loss = 0.0628395
I1124 09:26:29.818351 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:26:29.818351 32468 solver.cpp:237]     Train net output #1: loss = 0.0628393 (* 1 = 0.0628393 loss)
I1124 09:26:29.818351 32468 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1124 09:26:38.350486 32468 solver.cpp:218] Iteration 18600 (11.7213 iter/s, 8.53148s/100 iters), loss = 0.0615256
I1124 09:26:38.350486 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:26:38.350486 32468 solver.cpp:237]     Train net output #1: loss = 0.0615254 (* 1 = 0.0615254 loss)
I1124 09:26:38.350486 32468 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1124 09:26:46.889775 32468 solver.cpp:218] Iteration 18700 (11.7115 iter/s, 8.5386s/100 iters), loss = 0.0353549
I1124 09:26:46.889775 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:26:46.889775 32468 solver.cpp:237]     Train net output #1: loss = 0.0353547 (* 1 = 0.0353547 loss)
I1124 09:26:46.889775 32468 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1124 09:26:55.423583 32468 solver.cpp:218] Iteration 18800 (11.7181 iter/s, 8.53378s/100 iters), loss = 0.0662664
I1124 09:26:55.423583 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:26:55.423583 32468 solver.cpp:237]     Train net output #1: loss = 0.0662662 (* 1 = 0.0662662 loss)
I1124 09:26:55.423583 32468 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1124 09:27:03.953896 32468 solver.cpp:218] Iteration 18900 (11.7241 iter/s, 8.52948s/100 iters), loss = 0.0421809
I1124 09:27:03.953896 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:27:03.953896 32468 solver.cpp:237]     Train net output #1: loss = 0.0421808 (* 1 = 0.0421808 loss)
I1124 09:27:03.953896 32468 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1124 09:27:12.063940 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:27:12.398247 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_19000.caffemodel
I1124 09:27:12.437116 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_19000.solverstate
I1124 09:27:12.456102 32468 solver.cpp:330] Iteration 19000, Testing net (#0)
I1124 09:27:12.456102 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:27:14.898768 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:27:14.997807 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9081
I1124 09:27:14.997807 32468 solver.cpp:397]     Test net output #1: loss = 0.292862 (* 1 = 0.292862 loss)
I1124 09:27:15.080850 32468 solver.cpp:218] Iteration 19000 (8.98734 iter/s, 11.1268s/100 iters), loss = 0.0628276
I1124 09:27:15.080850 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:27:15.080850 32468 solver.cpp:237]     Train net output #1: loss = 0.0628275 (* 1 = 0.0628275 loss)
I1124 09:27:15.080850 32468 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1124 09:27:23.621912 32468 solver.cpp:218] Iteration 19100 (11.7093 iter/s, 8.54023s/100 iters), loss = 0.0891457
I1124 09:27:23.621912 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:27:23.621912 32468 solver.cpp:237]     Train net output #1: loss = 0.0891456 (* 1 = 0.0891456 loss)
I1124 09:27:23.621912 32468 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1124 09:27:32.161113 32468 solver.cpp:218] Iteration 19200 (11.7115 iter/s, 8.53864s/100 iters), loss = 0.0530465
I1124 09:27:32.161113 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:27:32.161113 32468 solver.cpp:237]     Train net output #1: loss = 0.0530463 (* 1 = 0.0530463 loss)
I1124 09:27:32.161113 32468 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1124 09:27:40.695565 32468 solver.cpp:218] Iteration 19300 (11.7172 iter/s, 8.53446s/100 iters), loss = 0.0756427
I1124 09:27:40.695565 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:27:40.695565 32468 solver.cpp:237]     Train net output #1: loss = 0.0756425 (* 1 = 0.0756425 loss)
I1124 09:27:40.695565 32468 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1124 09:27:49.240900 32468 solver.cpp:218] Iteration 19400 (11.7042 iter/s, 8.54397s/100 iters), loss = 0.0695615
I1124 09:27:49.240900 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:27:49.240900 32468 solver.cpp:237]     Train net output #1: loss = 0.0695613 (* 1 = 0.0695613 loss)
I1124 09:27:49.240900 32468 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1124 09:27:57.358557 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:27:57.696969 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_19500.caffemodel
I1124 09:27:57.735486 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_19500.solverstate
I1124 09:27:57.753486 32468 solver.cpp:330] Iteration 19500, Testing net (#0)
I1124 09:27:57.753486 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:28:00.194917 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:28:00.293936 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9086
I1124 09:28:00.294934 32468 solver.cpp:397]     Test net output #1: loss = 0.292684 (* 1 = 0.292684 loss)
I1124 09:28:00.377985 32468 solver.cpp:218] Iteration 19500 (8.97939 iter/s, 11.1366s/100 iters), loss = 0.0469837
I1124 09:28:00.377985 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:28:00.377985 32468 solver.cpp:237]     Train net output #1: loss = 0.0469835 (* 1 = 0.0469835 loss)
I1124 09:28:00.377985 32468 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1124 09:28:00.377985 32468 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1124 09:28:08.916543 32468 solver.cpp:218] Iteration 19600 (11.7118 iter/s, 8.53837s/100 iters), loss = 0.0659516
I1124 09:28:08.916543 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:28:08.916543 32468 solver.cpp:237]     Train net output #1: loss = 0.0659515 (* 1 = 0.0659515 loss)
I1124 09:28:08.916543 32468 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1124 09:28:17.450122 32468 solver.cpp:218] Iteration 19700 (11.7186 iter/s, 8.53343s/100 iters), loss = 0.0663993
I1124 09:28:17.450122 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:28:17.450122 32468 solver.cpp:237]     Train net output #1: loss = 0.0663991 (* 1 = 0.0663991 loss)
I1124 09:28:17.450122 32468 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1124 09:28:25.984431 32468 solver.cpp:218] Iteration 19800 (11.7189 iter/s, 8.5332s/100 iters), loss = 0.0888199
I1124 09:28:25.984431 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:28:25.984431 32468 solver.cpp:237]     Train net output #1: loss = 0.0888197 (* 1 = 0.0888197 loss)
I1124 09:28:25.984431 32468 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1124 09:28:34.538949 32468 solver.cpp:218] Iteration 19900 (11.6904 iter/s, 8.55401s/100 iters), loss = 0.0423866
I1124 09:28:34.538949 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:28:34.538949 32468 solver.cpp:237]     Train net output #1: loss = 0.0423864 (* 1 = 0.0423864 loss)
I1124 09:28:34.538949 32468 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1124 09:28:42.647723 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:28:42.984769 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_20000.caffemodel
I1124 09:28:43.028772 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_20000.solverstate
I1124 09:28:43.048272 32468 solver.cpp:330] Iteration 20000, Testing net (#0)
I1124 09:28:43.048272 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:28:45.491772 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:28:45.590806 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9087
I1124 09:28:45.590806 32468 solver.cpp:397]     Test net output #1: loss = 0.292785 (* 1 = 0.292785 loss)
I1124 09:28:45.674821 32468 solver.cpp:218] Iteration 20000 (8.98059 iter/s, 11.1351s/100 iters), loss = 0.0835684
I1124 09:28:45.674821 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1124 09:28:45.674821 32468 solver.cpp:237]     Train net output #1: loss = 0.0835682 (* 1 = 0.0835682 loss)
I1124 09:28:45.674821 32468 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1124 09:28:54.207589 32468 solver.cpp:218] Iteration 20100 (11.7197 iter/s, 8.53267s/100 iters), loss = 0.0605535
I1124 09:28:54.208089 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:28:54.208089 32468 solver.cpp:237]     Train net output #1: loss = 0.0605533 (* 1 = 0.0605533 loss)
I1124 09:28:54.208089 32468 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1124 09:29:02.739365 32468 solver.cpp:218] Iteration 20200 (11.7217 iter/s, 8.5312s/100 iters), loss = 0.0718202
I1124 09:29:02.739365 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:29:02.739365 32468 solver.cpp:237]     Train net output #1: loss = 0.0718201 (* 1 = 0.0718201 loss)
I1124 09:29:02.739365 32468 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1124 09:29:11.276525 32468 solver.cpp:218] Iteration 20300 (11.7136 iter/s, 8.53709s/100 iters), loss = 0.0706699
I1124 09:29:11.276525 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:29:11.276525 32468 solver.cpp:237]     Train net output #1: loss = 0.0706697 (* 1 = 0.0706697 loss)
I1124 09:29:11.276525 32468 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1124 09:29:19.802197 32468 solver.cpp:218] Iteration 20400 (11.7307 iter/s, 8.52464s/100 iters), loss = 0.0752156
I1124 09:29:19.802197 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:29:19.802197 32468 solver.cpp:237]     Train net output #1: loss = 0.0752154 (* 1 = 0.0752154 loss)
I1124 09:29:19.802197 32468 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1124 09:29:27.915571 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:29:28.254127 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_20500.caffemodel
I1124 09:29:28.295114 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_20500.solverstate
I1124 09:29:28.313110 32468 solver.cpp:330] Iteration 20500, Testing net (#0)
I1124 09:29:28.313110 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:29:30.751622 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:29:30.850615 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9081
I1124 09:29:30.850615 32468 solver.cpp:397]     Test net output #1: loss = 0.292555 (* 1 = 0.292555 loss)
I1124 09:29:30.933650 32468 solver.cpp:218] Iteration 20500 (8.98399 iter/s, 11.1309s/100 iters), loss = 0.0784826
I1124 09:29:30.933650 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:29:30.933650 32468 solver.cpp:237]     Train net output #1: loss = 0.0784824 (* 1 = 0.0784824 loss)
I1124 09:29:30.933650 32468 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1124 09:29:39.475945 32468 solver.cpp:218] Iteration 20600 (11.7064 iter/s, 8.54235s/100 iters), loss = 0.0779927
I1124 09:29:39.475945 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:29:39.475945 32468 solver.cpp:237]     Train net output #1: loss = 0.0779925 (* 1 = 0.0779925 loss)
I1124 09:29:39.475945 32468 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1124 09:29:48.015664 32468 solver.cpp:218] Iteration 20700 (11.7114 iter/s, 8.53871s/100 iters), loss = 0.04195
I1124 09:29:48.015664 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:29:48.016165 32468 solver.cpp:237]     Train net output #1: loss = 0.0419498 (* 1 = 0.0419498 loss)
I1124 09:29:48.016165 32468 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1124 09:29:56.554649 32468 solver.cpp:218] Iteration 20800 (11.7123 iter/s, 8.53805s/100 iters), loss = 0.0813553
I1124 09:29:56.554649 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:29:56.554649 32468 solver.cpp:237]     Train net output #1: loss = 0.081355 (* 1 = 0.081355 loss)
I1124 09:29:56.554649 32468 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1124 09:30:05.124738 32468 solver.cpp:218] Iteration 20900 (11.6687 iter/s, 8.56992s/100 iters), loss = 0.0278927
I1124 09:30:05.124738 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:30:05.124738 32468 solver.cpp:237]     Train net output #1: loss = 0.0278925 (* 1 = 0.0278925 loss)
I1124 09:30:05.124738 32468 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1124 09:30:13.238225 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:30:13.574332 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_21000.caffemodel
I1124 09:30:13.612870 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_21000.solverstate
I1124 09:30:13.631379 32468 solver.cpp:330] Iteration 21000, Testing net (#0)
I1124 09:30:13.631379 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:30:16.072325 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:30:16.170850 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9085
I1124 09:30:16.170850 32468 solver.cpp:397]     Test net output #1: loss = 0.292645 (* 1 = 0.292645 loss)
I1124 09:30:16.254901 32468 solver.cpp:218] Iteration 21000 (8.98484 iter/s, 11.1299s/100 iters), loss = 0.0621698
I1124 09:30:16.254901 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:30:16.254901 32468 solver.cpp:237]     Train net output #1: loss = 0.0621695 (* 1 = 0.0621695 loss)
I1124 09:30:16.254901 32468 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1124 09:30:24.787298 32468 solver.cpp:218] Iteration 21100 (11.7205 iter/s, 8.53203s/100 iters), loss = 0.0654289
I1124 09:30:24.787298 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:30:24.787298 32468 solver.cpp:237]     Train net output #1: loss = 0.0654287 (* 1 = 0.0654287 loss)
I1124 09:30:24.787298 32468 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1124 09:30:33.316004 32468 solver.cpp:218] Iteration 21200 (11.7255 iter/s, 8.5284s/100 iters), loss = 0.0447252
I1124 09:30:33.316992 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:30:33.316992 32468 solver.cpp:237]     Train net output #1: loss = 0.0447249 (* 1 = 0.0447249 loss)
I1124 09:30:33.316992 32468 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1124 09:30:41.853588 32468 solver.cpp:218] Iteration 21300 (11.7139 iter/s, 8.53688s/100 iters), loss = 0.0931218
I1124 09:30:41.853588 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:30:41.853588 32468 solver.cpp:237]     Train net output #1: loss = 0.0931215 (* 1 = 0.0931215 loss)
I1124 09:30:41.853588 32468 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1124 09:30:50.390048 32468 solver.cpp:218] Iteration 21400 (11.7152 iter/s, 8.53594s/100 iters), loss = 0.0569333
I1124 09:30:50.390048 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:30:50.390048 32468 solver.cpp:237]     Train net output #1: loss = 0.0569331 (* 1 = 0.0569331 loss)
I1124 09:30:50.390048 32468 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1124 09:30:58.501600 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:30:58.840330 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_21500.caffemodel
I1124 09:30:58.879822 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_21500.solverstate
I1124 09:30:58.898322 32468 solver.cpp:330] Iteration 21500, Testing net (#0)
I1124 09:30:58.898322 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:31:01.336681 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:31:01.435706 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9081
I1124 09:31:01.436204 32468 solver.cpp:397]     Test net output #1: loss = 0.2928 (* 1 = 0.2928 loss)
I1124 09:31:01.518867 32468 solver.cpp:218] Iteration 21500 (8.98626 iter/s, 11.1281s/100 iters), loss = 0.0590423
I1124 09:31:01.518867 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:31:01.518867 32468 solver.cpp:237]     Train net output #1: loss = 0.0590421 (* 1 = 0.0590421 loss)
I1124 09:31:01.518867 32468 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1124 09:31:10.049939 32468 solver.cpp:218] Iteration 21600 (11.723 iter/s, 8.53027s/100 iters), loss = 0.0946414
I1124 09:31:10.049939 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:31:10.049939 32468 solver.cpp:237]     Train net output #1: loss = 0.0946411 (* 1 = 0.0946411 loss)
I1124 09:31:10.049939 32468 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1124 09:31:18.581544 32468 solver.cpp:218] Iteration 21700 (11.7216 iter/s, 8.53125s/100 iters), loss = 0.066834
I1124 09:31:18.581544 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:31:18.581544 32468 solver.cpp:237]     Train net output #1: loss = 0.0668337 (* 1 = 0.0668337 loss)
I1124 09:31:18.581544 32468 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1124 09:31:27.117005 32468 solver.cpp:218] Iteration 21800 (11.7162 iter/s, 8.53519s/100 iters), loss = 0.0861058
I1124 09:31:27.117005 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:31:27.117005 32468 solver.cpp:237]     Train net output #1: loss = 0.0861055 (* 1 = 0.0861055 loss)
I1124 09:31:27.117005 32468 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1124 09:31:35.654855 32468 solver.cpp:218] Iteration 21900 (11.7137 iter/s, 8.53705s/100 iters), loss = 0.0368113
I1124 09:31:35.654855 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:31:35.654855 32468 solver.cpp:237]     Train net output #1: loss = 0.036811 (* 1 = 0.036811 loss)
I1124 09:31:35.654855 32468 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1124 09:31:43.769522 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:31:44.107844 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_22000.caffemodel
I1124 09:31:44.146826 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_22000.solverstate
I1124 09:31:44.165825 32468 solver.cpp:330] Iteration 22000, Testing net (#0)
I1124 09:31:44.165825 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:31:46.607322 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:31:46.705853 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9081
I1124 09:31:46.705853 32468 solver.cpp:397]     Test net output #1: loss = 0.292744 (* 1 = 0.292744 loss)
I1124 09:31:46.788890 32468 solver.cpp:218] Iteration 22000 (8.98207 iter/s, 11.1333s/100 iters), loss = 0.0812884
I1124 09:31:46.788890 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:31:46.788890 32468 solver.cpp:237]     Train net output #1: loss = 0.0812881 (* 1 = 0.0812881 loss)
I1124 09:31:46.788890 32468 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1124 09:31:46.788890 32468 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1124 09:31:55.324077 32468 solver.cpp:218] Iteration 22100 (11.7172 iter/s, 8.5345s/100 iters), loss = 0.108732
I1124 09:31:55.324077 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:31:55.324077 32468 solver.cpp:237]     Train net output #1: loss = 0.108732 (* 1 = 0.108732 loss)
I1124 09:31:55.324077 32468 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1124 09:32:03.853842 32468 solver.cpp:218] Iteration 22200 (11.7238 iter/s, 8.52966s/100 iters), loss = 0.070664
I1124 09:32:03.853842 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:32:03.853842 32468 solver.cpp:237]     Train net output #1: loss = 0.0706637 (* 1 = 0.0706637 loss)
I1124 09:32:03.853842 32468 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1124 09:32:12.397474 32468 solver.cpp:218] Iteration 22300 (11.7048 iter/s, 8.54354s/100 iters), loss = 0.0507211
I1124 09:32:12.398460 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:32:12.398460 32468 solver.cpp:237]     Train net output #1: loss = 0.0507208 (* 1 = 0.0507208 loss)
I1124 09:32:12.398460 32468 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1124 09:32:20.935314 32468 solver.cpp:218] Iteration 22400 (11.7142 iter/s, 8.53662s/100 iters), loss = 0.0314217
I1124 09:32:20.935314 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:32:20.935314 32468 solver.cpp:237]     Train net output #1: loss = 0.0314214 (* 1 = 0.0314214 loss)
I1124 09:32:20.935314 32468 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1124 09:32:29.038736 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:32:29.377842 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_22500.caffemodel
I1124 09:32:29.417845 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_22500.solverstate
I1124 09:32:29.435860 32468 solver.cpp:330] Iteration 22500, Testing net (#0)
I1124 09:32:29.435860 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:32:31.876024 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:32:31.974056 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9084
I1124 09:32:31.974056 32468 solver.cpp:397]     Test net output #1: loss = 0.292671 (* 1 = 0.292671 loss)
I1124 09:32:32.058723 32468 solver.cpp:218] Iteration 22500 (8.99064 iter/s, 11.1227s/100 iters), loss = 0.0677971
I1124 09:32:32.058723 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:32:32.058723 32468 solver.cpp:237]     Train net output #1: loss = 0.0677968 (* 1 = 0.0677968 loss)
I1124 09:32:32.058723 32468 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1124 09:32:40.592921 32468 solver.cpp:218] Iteration 22600 (11.7176 iter/s, 8.53419s/100 iters), loss = 0.0591603
I1124 09:32:40.592921 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:32:40.592921 32468 solver.cpp:237]     Train net output #1: loss = 0.05916 (* 1 = 0.05916 loss)
I1124 09:32:40.592921 32468 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1124 09:32:49.135196 32468 solver.cpp:218] Iteration 22700 (11.7072 iter/s, 8.54178s/100 iters), loss = 0.0440896
I1124 09:32:49.135196 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:32:49.135196 32468 solver.cpp:237]     Train net output #1: loss = 0.0440893 (* 1 = 0.0440893 loss)
I1124 09:32:49.135196 32468 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1124 09:32:57.680044 32468 solver.cpp:218] Iteration 22800 (11.7035 iter/s, 8.54448s/100 iters), loss = 0.0888148
I1124 09:32:57.680044 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:32:57.680044 32468 solver.cpp:237]     Train net output #1: loss = 0.0888145 (* 1 = 0.0888145 loss)
I1124 09:32:57.680044 32468 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1124 09:33:06.214285 32468 solver.cpp:218] Iteration 22900 (11.718 iter/s, 8.53389s/100 iters), loss = 0.0485867
I1124 09:33:06.215273 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:33:06.215273 32468 solver.cpp:237]     Train net output #1: loss = 0.0485864 (* 1 = 0.0485864 loss)
I1124 09:33:06.215273 32468 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1124 09:33:14.324630 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:33:14.663244 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_23000.caffemodel
I1124 09:33:14.702754 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_23000.solverstate
I1124 09:33:14.720762 32468 solver.cpp:330] Iteration 23000, Testing net (#0)
I1124 09:33:14.720762 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:33:17.164515 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:33:17.263568 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9084
I1124 09:33:17.263568 32468 solver.cpp:397]     Test net output #1: loss = 0.292683 (* 1 = 0.292683 loss)
I1124 09:33:17.346578 32468 solver.cpp:218] Iteration 23000 (8.98378 iter/s, 11.1312s/100 iters), loss = 0.0773011
I1124 09:33:17.346578 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:33:17.346578 32468 solver.cpp:237]     Train net output #1: loss = 0.0773008 (* 1 = 0.0773008 loss)
I1124 09:33:17.346578 32468 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1124 09:33:25.878140 32468 solver.cpp:218] Iteration 23100 (11.722 iter/s, 8.53094s/100 iters), loss = 0.0684739
I1124 09:33:25.878140 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:33:25.878140 32468 solver.cpp:237]     Train net output #1: loss = 0.0684736 (* 1 = 0.0684736 loss)
I1124 09:33:25.878140 32468 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1124 09:33:34.414090 32468 solver.cpp:218] Iteration 23200 (11.7151 iter/s, 8.53597s/100 iters), loss = 0.0922976
I1124 09:33:34.415086 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:33:34.415086 32468 solver.cpp:237]     Train net output #1: loss = 0.0922973 (* 1 = 0.0922973 loss)
I1124 09:33:34.415086 32468 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1124 09:33:42.952059 32468 solver.cpp:218] Iteration 23300 (11.714 iter/s, 8.5368s/100 iters), loss = 0.0847586
I1124 09:33:42.952059 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:33:42.952059 32468 solver.cpp:237]     Train net output #1: loss = 0.0847583 (* 1 = 0.0847583 loss)
I1124 09:33:42.952059 32468 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1124 09:33:51.486627 32468 solver.cpp:218] Iteration 23400 (11.7171 iter/s, 8.53453s/100 iters), loss = 0.0583041
I1124 09:33:51.486627 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:33:51.486627 32468 solver.cpp:237]     Train net output #1: loss = 0.0583038 (* 1 = 0.0583038 loss)
I1124 09:33:51.486627 32468 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1124 09:33:59.596186 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:33:59.933909 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_23500.caffemodel
I1124 09:33:59.973440 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_23500.solverstate
I1124 09:33:59.992440 32468 solver.cpp:330] Iteration 23500, Testing net (#0)
I1124 09:33:59.992440 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:34:02.437050 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:34:02.536067 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9084
I1124 09:34:02.536067 32468 solver.cpp:397]     Test net output #1: loss = 0.292589 (* 1 = 0.292589 loss)
I1124 09:34:02.619093 32468 solver.cpp:218] Iteration 23500 (8.98346 iter/s, 11.1316s/100 iters), loss = 0.0708083
I1124 09:34:02.619093 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:34:02.619093 32468 solver.cpp:237]     Train net output #1: loss = 0.070808 (* 1 = 0.070808 loss)
I1124 09:34:02.619093 32468 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1124 09:34:11.154268 32468 solver.cpp:218] Iteration 23600 (11.7175 iter/s, 8.53425s/100 iters), loss = 0.0613954
I1124 09:34:11.154268 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:34:11.154268 32468 solver.cpp:237]     Train net output #1: loss = 0.0613951 (* 1 = 0.0613951 loss)
I1124 09:34:11.154268 32468 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1124 09:34:19.688133 32468 solver.cpp:218] Iteration 23700 (11.7183 iter/s, 8.53364s/100 iters), loss = 0.0501486
I1124 09:34:19.688133 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:34:19.688133 32468 solver.cpp:237]     Train net output #1: loss = 0.0501483 (* 1 = 0.0501483 loss)
I1124 09:34:19.688133 32468 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1124 09:34:28.231451 32468 solver.cpp:218] Iteration 23800 (11.7056 iter/s, 8.54293s/100 iters), loss = 0.0720985
I1124 09:34:28.231451 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:34:28.231451 32468 solver.cpp:237]     Train net output #1: loss = 0.0720982 (* 1 = 0.0720982 loss)
I1124 09:34:28.231451 32468 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1124 09:34:36.772641 32468 solver.cpp:218] Iteration 23900 (11.7089 iter/s, 8.5405s/100 iters), loss = 0.036173
I1124 09:34:36.772641 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:34:36.772641 32468 solver.cpp:237]     Train net output #1: loss = 0.0361727 (* 1 = 0.0361727 loss)
I1124 09:34:36.772641 32468 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1124 09:34:44.888653 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:34:45.227936 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_24000.caffemodel
I1124 09:34:45.267441 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_24000.solverstate
I1124 09:34:45.286938 32468 solver.cpp:330] Iteration 24000, Testing net (#0)
I1124 09:34:45.286938 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:34:47.728482 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:34:47.826577 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9085
I1124 09:34:47.826577 32468 solver.cpp:397]     Test net output #1: loss = 0.292556 (* 1 = 0.292556 loss)
I1124 09:34:47.909739 32468 solver.cpp:218] Iteration 24000 (8.97887 iter/s, 11.1373s/100 iters), loss = 0.0683626
I1124 09:34:47.910725 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:34:47.910725 32468 solver.cpp:237]     Train net output #1: loss = 0.0683623 (* 1 = 0.0683623 loss)
I1124 09:34:47.910725 32468 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1124 09:34:56.452601 32468 solver.cpp:218] Iteration 24100 (11.7072 iter/s, 8.54173s/100 iters), loss = 0.0820284
I1124 09:34:56.452601 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:34:56.452601 32468 solver.cpp:237]     Train net output #1: loss = 0.0820281 (* 1 = 0.0820281 loss)
I1124 09:34:56.452601 32468 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1124 09:35:04.985386 32468 solver.cpp:218] Iteration 24200 (11.72 iter/s, 8.5324s/100 iters), loss = 0.101104
I1124 09:35:04.985888 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:35:04.985888 32468 solver.cpp:237]     Train net output #1: loss = 0.101103 (* 1 = 0.101103 loss)
I1124 09:35:04.985888 32468 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1124 09:35:13.525738 32468 solver.cpp:218] Iteration 24300 (11.7098 iter/s, 8.53986s/100 iters), loss = 0.1147
I1124 09:35:13.525738 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:35:13.525738 32468 solver.cpp:237]     Train net output #1: loss = 0.1147 (* 1 = 0.1147 loss)
I1124 09:35:13.525738 32468 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1124 09:35:22.073328 32468 solver.cpp:218] Iteration 24400 (11.6995 iter/s, 8.54739s/100 iters), loss = 0.0603387
I1124 09:35:22.073328 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:35:22.073328 32468 solver.cpp:237]     Train net output #1: loss = 0.0603384 (* 1 = 0.0603384 loss)
I1124 09:35:22.073328 32468 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1124 09:35:30.205271 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:35:30.544452 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_24500.caffemodel
I1124 09:35:30.585441 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_24500.solverstate
I1124 09:35:30.604439 32468 solver.cpp:330] Iteration 24500, Testing net (#0)
I1124 09:35:30.604439 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:35:33.046592 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:35:33.146113 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9082
I1124 09:35:33.146113 32468 solver.cpp:397]     Test net output #1: loss = 0.292741 (* 1 = 0.292741 loss)
I1124 09:35:33.229140 32468 solver.cpp:218] Iteration 24500 (8.9645 iter/s, 11.1551s/100 iters), loss = 0.062411
I1124 09:35:33.229140 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:35:33.229140 32468 solver.cpp:237]     Train net output #1: loss = 0.0624107 (* 1 = 0.0624107 loss)
I1124 09:35:33.229140 32468 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1124 09:35:41.778646 32468 solver.cpp:218] Iteration 24600 (11.6977 iter/s, 8.54871s/100 iters), loss = 0.0827745
I1124 09:35:41.778646 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:35:41.778646 32468 solver.cpp:237]     Train net output #1: loss = 0.0827742 (* 1 = 0.0827742 loss)
I1124 09:35:41.778646 32468 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1124 09:35:50.319885 32468 solver.cpp:218] Iteration 24700 (11.7083 iter/s, 8.54097s/100 iters), loss = 0.0565232
I1124 09:35:50.319885 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:35:50.319885 32468 solver.cpp:237]     Train net output #1: loss = 0.0565229 (* 1 = 0.0565229 loss)
I1124 09:35:50.319885 32468 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1124 09:35:58.867722 32468 solver.cpp:218] Iteration 24800 (11.7001 iter/s, 8.54692s/100 iters), loss = 0.0440192
I1124 09:35:58.867722 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:35:58.867722 32468 solver.cpp:237]     Train net output #1: loss = 0.0440189 (* 1 = 0.0440189 loss)
I1124 09:35:58.867722 32468 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1124 09:36:07.410657 32468 solver.cpp:218] Iteration 24900 (11.7067 iter/s, 8.54209s/100 iters), loss = 0.0659663
I1124 09:36:07.410657 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:36:07.410657 32468 solver.cpp:237]     Train net output #1: loss = 0.065966 (* 1 = 0.065966 loss)
I1124 09:36:07.410657 32468 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1124 09:36:15.523452 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:36:15.861564 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_25000.caffemodel
I1124 09:36:15.900569 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_25000.solverstate
I1124 09:36:15.919567 32468 solver.cpp:330] Iteration 25000, Testing net (#0)
I1124 09:36:15.920066 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:36:18.362454 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:36:18.461973 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9086
I1124 09:36:18.461973 32468 solver.cpp:397]     Test net output #1: loss = 0.292704 (* 1 = 0.292704 loss)
I1124 09:36:18.545023 32468 solver.cpp:218] Iteration 25000 (8.98155 iter/s, 11.1339s/100 iters), loss = 0.0861356
I1124 09:36:18.545023 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:36:18.545023 32468 solver.cpp:237]     Train net output #1: loss = 0.0861353 (* 1 = 0.0861353 loss)
I1124 09:36:18.545023 32468 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1124 09:36:27.085096 32468 solver.cpp:218] Iteration 25100 (11.7102 iter/s, 8.53959s/100 iters), loss = 0.0936782
I1124 09:36:27.085096 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1124 09:36:27.085096 32468 solver.cpp:237]     Train net output #1: loss = 0.0936779 (* 1 = 0.0936779 loss)
I1124 09:36:27.085096 32468 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1124 09:36:35.626294 32468 solver.cpp:218] Iteration 25200 (11.7081 iter/s, 8.54111s/100 iters), loss = 0.0537901
I1124 09:36:35.626294 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:36:35.626294 32468 solver.cpp:237]     Train net output #1: loss = 0.0537898 (* 1 = 0.0537898 loss)
I1124 09:36:35.626294 32468 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1124 09:36:44.166577 32468 solver.cpp:218] Iteration 25300 (11.7105 iter/s, 8.53934s/100 iters), loss = 0.0747309
I1124 09:36:44.166577 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:36:44.166577 32468 solver.cpp:237]     Train net output #1: loss = 0.0747306 (* 1 = 0.0747306 loss)
I1124 09:36:44.166577 32468 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1124 09:36:52.716120 32468 solver.cpp:218] Iteration 25400 (11.6966 iter/s, 8.54951s/100 iters), loss = 0.066663
I1124 09:36:52.716120 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:36:52.716120 32468 solver.cpp:237]     Train net output #1: loss = 0.0666627 (* 1 = 0.0666627 loss)
I1124 09:36:52.716120 32468 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1124 09:37:00.835806 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:37:01.172778 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_25500.caffemodel
I1124 09:37:01.215775 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_25500.solverstate
I1124 09:37:01.234803 32468 solver.cpp:330] Iteration 25500, Testing net (#0)
I1124 09:37:01.235285 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:37:03.679744 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:37:03.779265 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9084
I1124 09:37:03.779265 32468 solver.cpp:397]     Test net output #1: loss = 0.292647 (* 1 = 0.292647 loss)
I1124 09:37:03.862315 32468 solver.cpp:218] Iteration 25500 (8.97251 iter/s, 11.1452s/100 iters), loss = 0.0483019
I1124 09:37:03.862315 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:37:03.862315 32468 solver.cpp:237]     Train net output #1: loss = 0.0483016 (* 1 = 0.0483016 loss)
I1124 09:37:03.862315 32468 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1124 09:37:12.400861 32468 solver.cpp:218] Iteration 25600 (11.7123 iter/s, 8.53806s/100 iters), loss = 0.0935328
I1124 09:37:12.400861 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:37:12.400861 32468 solver.cpp:237]     Train net output #1: loss = 0.0935325 (* 1 = 0.0935325 loss)
I1124 09:37:12.400861 32468 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1124 09:37:20.948918 32468 solver.cpp:218] Iteration 25700 (11.6994 iter/s, 8.54746s/100 iters), loss = 0.0758181
I1124 09:37:20.948918 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:37:20.948918 32468 solver.cpp:237]     Train net output #1: loss = 0.0758177 (* 1 = 0.0758177 loss)
I1124 09:37:20.948918 32468 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1124 09:37:29.493127 32468 solver.cpp:218] Iteration 25800 (11.7038 iter/s, 8.54427s/100 iters), loss = 0.0461246
I1124 09:37:29.493127 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:37:29.493127 32468 solver.cpp:237]     Train net output #1: loss = 0.0461242 (* 1 = 0.0461242 loss)
I1124 09:37:29.493127 32468 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1124 09:37:38.030882 32468 solver.cpp:218] Iteration 25900 (11.7131 iter/s, 8.53747s/100 iters), loss = 0.035829
I1124 09:37:38.030882 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:37:38.030882 32468 solver.cpp:237]     Train net output #1: loss = 0.0358287 (* 1 = 0.0358287 loss)
I1124 09:37:38.030882 32468 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1124 09:37:46.151787 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:37:46.490079 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_26000.caffemodel
I1124 09:37:46.530064 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_26000.solverstate
I1124 09:37:46.549064 32468 solver.cpp:330] Iteration 26000, Testing net (#0)
I1124 09:37:46.549064 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:37:48.990664 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:37:49.089689 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9082
I1124 09:37:49.089689 32468 solver.cpp:397]     Test net output #1: loss = 0.29265 (* 1 = 0.29265 loss)
I1124 09:37:49.172897 32468 solver.cpp:218] Iteration 26000 (8.97554 iter/s, 11.1414s/100 iters), loss = 0.0537784
I1124 09:37:49.172897 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:37:49.172897 32468 solver.cpp:237]     Train net output #1: loss = 0.0537781 (* 1 = 0.0537781 loss)
I1124 09:37:49.172897 32468 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1124 09:37:57.708058 32468 solver.cpp:218] Iteration 26100 (11.7181 iter/s, 8.53382s/100 iters), loss = 0.0782837
I1124 09:37:57.708058 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:37:57.708058 32468 solver.cpp:237]     Train net output #1: loss = 0.0782834 (* 1 = 0.0782834 loss)
I1124 09:37:57.708058 32468 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1124 09:38:06.251101 32468 solver.cpp:218] Iteration 26200 (11.7054 iter/s, 8.54309s/100 iters), loss = 0.0583892
I1124 09:38:06.251101 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:38:06.251101 32468 solver.cpp:237]     Train net output #1: loss = 0.0583889 (* 1 = 0.0583889 loss)
I1124 09:38:06.251101 32468 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1124 09:38:14.787585 32468 solver.cpp:218] Iteration 26300 (11.7156 iter/s, 8.53559s/100 iters), loss = 0.0566603
I1124 09:38:14.787585 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:38:14.787585 32468 solver.cpp:237]     Train net output #1: loss = 0.05666 (* 1 = 0.05666 loss)
I1124 09:38:14.787585 32468 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1124 09:38:23.324695 32468 solver.cpp:218] Iteration 26400 (11.7142 iter/s, 8.53667s/100 iters), loss = 0.0570834
I1124 09:38:23.324695 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:38:23.324695 32468 solver.cpp:237]     Train net output #1: loss = 0.057083 (* 1 = 0.057083 loss)
I1124 09:38:23.324695 32468 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1124 09:38:31.440522 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:38:31.779124 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_26500.caffemodel
I1124 09:38:31.817127 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_26500.solverstate
I1124 09:38:31.836613 32468 solver.cpp:330] Iteration 26500, Testing net (#0)
I1124 09:38:31.836613 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:38:34.277104 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:38:34.376694 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9085
I1124 09:38:34.376694 32468 solver.cpp:397]     Test net output #1: loss = 0.292691 (* 1 = 0.292691 loss)
I1124 09:38:34.459250 32468 solver.cpp:218] Iteration 26500 (8.98114 iter/s, 11.1344s/100 iters), loss = 0.0706877
I1124 09:38:34.459250 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:38:34.459250 32468 solver.cpp:237]     Train net output #1: loss = 0.0706873 (* 1 = 0.0706873 loss)
I1124 09:38:34.459250 32468 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1124 09:38:43.001700 32468 solver.cpp:218] Iteration 26600 (11.7072 iter/s, 8.54173s/100 iters), loss = 0.0844654
I1124 09:38:43.001700 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:38:43.001700 32468 solver.cpp:237]     Train net output #1: loss = 0.084465 (* 1 = 0.084465 loss)
I1124 09:38:43.001700 32468 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1124 09:38:51.550207 32468 solver.cpp:218] Iteration 26700 (11.6995 iter/s, 8.5474s/100 iters), loss = 0.0696967
I1124 09:38:51.550207 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:38:51.550207 32468 solver.cpp:237]     Train net output #1: loss = 0.0696963 (* 1 = 0.0696963 loss)
I1124 09:38:51.550207 32468 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1124 09:39:00.097198 32468 solver.cpp:218] Iteration 26800 (11.7001 iter/s, 8.54694s/100 iters), loss = 0.0473143
I1124 09:39:00.097198 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:39:00.097198 32468 solver.cpp:237]     Train net output #1: loss = 0.0473139 (* 1 = 0.0473139 loss)
I1124 09:39:00.097198 32468 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1124 09:39:08.646919 32468 solver.cpp:218] Iteration 26900 (11.6975 iter/s, 8.54884s/100 iters), loss = 0.061146
I1124 09:39:08.646919 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:39:08.646919 32468 solver.cpp:237]     Train net output #1: loss = 0.0611457 (* 1 = 0.0611457 loss)
I1124 09:39:08.646919 32468 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1124 09:39:16.778169 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:39:17.116283 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_27000.caffemodel
I1124 09:39:17.158308 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_27000.solverstate
I1124 09:39:17.180310 32468 solver.cpp:330] Iteration 27000, Testing net (#0)
I1124 09:39:17.180310 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:39:19.624896 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:39:19.723901 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9084
I1124 09:39:19.724900 32468 solver.cpp:397]     Test net output #1: loss = 0.292639 (* 1 = 0.292639 loss)
I1124 09:39:19.807632 32468 solver.cpp:218] Iteration 27000 (8.95993 iter/s, 11.1608s/100 iters), loss = 0.0884283
I1124 09:39:19.807632 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:39:19.807632 32468 solver.cpp:237]     Train net output #1: loss = 0.088428 (* 1 = 0.088428 loss)
I1124 09:39:19.807632 32468 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1124 09:39:19.807632 32468 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1124 09:39:28.345719 32468 solver.cpp:218] Iteration 27100 (11.7129 iter/s, 8.53756s/100 iters), loss = 0.100171
I1124 09:39:28.345719 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:39:28.345719 32468 solver.cpp:237]     Train net output #1: loss = 0.100171 (* 1 = 0.100171 loss)
I1124 09:39:28.345719 32468 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1124 09:39:36.886638 32468 solver.cpp:218] Iteration 27200 (11.7088 iter/s, 8.54056s/100 iters), loss = 0.0471436
I1124 09:39:36.887624 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:39:36.887624 32468 solver.cpp:237]     Train net output #1: loss = 0.0471433 (* 1 = 0.0471433 loss)
I1124 09:39:36.887624 32468 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1124 09:39:45.437047 32468 solver.cpp:218] Iteration 27300 (11.6969 iter/s, 8.54927s/100 iters), loss = 0.0802763
I1124 09:39:45.437047 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:39:45.437047 32468 solver.cpp:237]     Train net output #1: loss = 0.080276 (* 1 = 0.080276 loss)
I1124 09:39:45.437047 32468 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1124 09:39:53.979372 32468 solver.cpp:218] Iteration 27400 (11.7068 iter/s, 8.54207s/100 iters), loss = 0.0323316
I1124 09:39:53.979372 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:39:53.979372 32468 solver.cpp:237]     Train net output #1: loss = 0.0323313 (* 1 = 0.0323313 loss)
I1124 09:39:53.979372 32468 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1124 09:40:02.130486 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:40:02.469612 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_27500.caffemodel
I1124 09:40:02.509119 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_27500.solverstate
I1124 09:40:02.528134 32468 solver.cpp:330] Iteration 27500, Testing net (#0)
I1124 09:40:02.528134 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:40:04.978530 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:40:05.078073 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9081
I1124 09:40:05.078073 32468 solver.cpp:397]     Test net output #1: loss = 0.292603 (* 1 = 0.292603 loss)
I1124 09:40:05.161754 32468 solver.cpp:218] Iteration 27500 (8.94314 iter/s, 11.1818s/100 iters), loss = 0.0572849
I1124 09:40:05.161754 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:40:05.161754 32468 solver.cpp:237]     Train net output #1: loss = 0.0572845 (* 1 = 0.0572845 loss)
I1124 09:40:05.161754 32468 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1124 09:40:13.709960 32468 solver.cpp:218] Iteration 27600 (11.6988 iter/s, 8.54791s/100 iters), loss = 0.0835696
I1124 09:40:13.710943 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:40:13.710943 32468 solver.cpp:237]     Train net output #1: loss = 0.0835693 (* 1 = 0.0835693 loss)
I1124 09:40:13.710943 32468 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1124 09:40:22.248132 32468 solver.cpp:218] Iteration 27700 (11.7129 iter/s, 8.53757s/100 iters), loss = 0.0653021
I1124 09:40:22.248132 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:40:22.248132 32468 solver.cpp:237]     Train net output #1: loss = 0.0653018 (* 1 = 0.0653018 loss)
I1124 09:40:22.248132 32468 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1124 09:40:30.783004 32468 solver.cpp:218] Iteration 27800 (11.7174 iter/s, 8.53434s/100 iters), loss = 0.0541753
I1124 09:40:30.783004 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:40:30.783004 32468 solver.cpp:237]     Train net output #1: loss = 0.0541749 (* 1 = 0.0541749 loss)
I1124 09:40:30.783990 32468 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1124 09:40:39.321120 32468 solver.cpp:218] Iteration 27900 (11.7133 iter/s, 8.5373s/100 iters), loss = 0.0335357
I1124 09:40:39.321120 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:40:39.321120 32468 solver.cpp:237]     Train net output #1: loss = 0.0335353 (* 1 = 0.0335353 loss)
I1124 09:40:39.321120 32468 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1124 09:40:47.433758 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:40:47.770733 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_28000.caffemodel
I1124 09:40:47.851727 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_28000.solverstate
I1124 09:40:47.872232 32468 solver.cpp:330] Iteration 28000, Testing net (#0)
I1124 09:40:47.872232 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:40:50.318020 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:40:50.417050 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9084
I1124 09:40:50.417050 32468 solver.cpp:397]     Test net output #1: loss = 0.292712 (* 1 = 0.292712 loss)
I1124 09:40:50.500087 32468 solver.cpp:218] Iteration 28000 (8.94564 iter/s, 11.1786s/100 iters), loss = 0.0693868
I1124 09:40:50.500087 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:40:50.500087 32468 solver.cpp:237]     Train net output #1: loss = 0.0693865 (* 1 = 0.0693865 loss)
I1124 09:40:50.500087 32468 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1124 09:40:59.056139 32468 solver.cpp:218] Iteration 28100 (11.6885 iter/s, 8.55545s/100 iters), loss = 0.0839444
I1124 09:40:59.056139 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:40:59.056139 32468 solver.cpp:237]     Train net output #1: loss = 0.0839441 (* 1 = 0.0839441 loss)
I1124 09:40:59.056139 32468 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1124 09:41:07.603713 32468 solver.cpp:218] Iteration 28200 (11.701 iter/s, 8.54627s/100 iters), loss = 0.0576458
I1124 09:41:07.603713 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:41:07.603713 32468 solver.cpp:237]     Train net output #1: loss = 0.0576455 (* 1 = 0.0576455 loss)
I1124 09:41:07.603713 32468 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1124 09:41:16.157127 32468 solver.cpp:218] Iteration 28300 (11.6918 iter/s, 8.55302s/100 iters), loss = 0.0539504
I1124 09:41:16.157127 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:41:16.157127 32468 solver.cpp:237]     Train net output #1: loss = 0.0539501 (* 1 = 0.0539501 loss)
I1124 09:41:16.157127 32468 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1124 09:41:24.706225 32468 solver.cpp:218] Iteration 28400 (11.6973 iter/s, 8.54897s/100 iters), loss = 0.0410622
I1124 09:41:24.706225 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:41:24.706225 32468 solver.cpp:237]     Train net output #1: loss = 0.0410618 (* 1 = 0.0410618 loss)
I1124 09:41:24.706225 32468 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1124 09:41:32.903820 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:41:33.241879 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_28500.caffemodel
I1124 09:41:33.280863 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_28500.solverstate
I1124 09:41:33.299885 32468 solver.cpp:330] Iteration 28500, Testing net (#0)
I1124 09:41:33.299885 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:41:35.737610 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:41:35.836613 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9082
I1124 09:41:35.836613 32468 solver.cpp:397]     Test net output #1: loss = 0.292639 (* 1 = 0.292639 loss)
I1124 09:41:35.919632 32468 solver.cpp:218] Iteration 28500 (8.91878 iter/s, 11.2123s/100 iters), loss = 0.0766093
I1124 09:41:35.919632 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:41:35.919632 32468 solver.cpp:237]     Train net output #1: loss = 0.0766089 (* 1 = 0.0766089 loss)
I1124 09:41:35.919632 32468 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1124 09:41:44.454438 32468 solver.cpp:218] Iteration 28600 (11.7173 iter/s, 8.5344s/100 iters), loss = 0.0886964
I1124 09:41:44.454438 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:41:44.454438 32468 solver.cpp:237]     Train net output #1: loss = 0.088696 (* 1 = 0.088696 loss)
I1124 09:41:44.454438 32468 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1124 09:41:52.986800 32468 solver.cpp:218] Iteration 28700 (11.7199 iter/s, 8.53252s/100 iters), loss = 0.0742144
I1124 09:41:52.986800 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:41:52.986800 32468 solver.cpp:237]     Train net output #1: loss = 0.0742141 (* 1 = 0.0742141 loss)
I1124 09:41:52.986800 32468 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1124 09:42:01.522150 32468 solver.cpp:218] Iteration 28800 (11.7172 iter/s, 8.53443s/100 iters), loss = 0.0563237
I1124 09:42:01.522150 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:42:01.522150 32468 solver.cpp:237]     Train net output #1: loss = 0.0563234 (* 1 = 0.0563234 loss)
I1124 09:42:01.522150 32468 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1124 09:42:10.053887 32468 solver.cpp:218] Iteration 28900 (11.7211 iter/s, 8.5316s/100 iters), loss = 0.0327096
I1124 09:42:10.053887 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:42:10.053887 32468 solver.cpp:237]     Train net output #1: loss = 0.0327092 (* 1 = 0.0327092 loss)
I1124 09:42:10.053887 32468 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1124 09:42:18.162449 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:42:18.501536 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_29000.caffemodel
I1124 09:42:18.605602 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_29000.solverstate
I1124 09:42:18.623621 32468 solver.cpp:330] Iteration 29000, Testing net (#0)
I1124 09:42:18.623621 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:42:21.065635 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:42:21.164659 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9084
I1124 09:42:21.164659 32468 solver.cpp:397]     Test net output #1: loss = 0.292775 (* 1 = 0.292775 loss)
I1124 09:42:21.247691 32468 solver.cpp:218] Iteration 29000 (8.93447 iter/s, 11.1926s/100 iters), loss = 0.0505986
I1124 09:42:21.247691 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:42:21.247691 32468 solver.cpp:237]     Train net output #1: loss = 0.0505982 (* 1 = 0.0505982 loss)
I1124 09:42:21.247691 32468 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1124 09:42:29.779040 32468 solver.cpp:218] Iteration 29100 (11.7216 iter/s, 8.53124s/100 iters), loss = 0.0925409
I1124 09:42:29.779040 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1124 09:42:29.779040 32468 solver.cpp:237]     Train net output #1: loss = 0.0925406 (* 1 = 0.0925406 loss)
I1124 09:42:29.779040 32468 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1124 09:42:38.312726 32468 solver.cpp:218] Iteration 29200 (11.7196 iter/s, 8.5327s/100 iters), loss = 0.0608281
I1124 09:42:38.312726 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:42:38.312726 32468 solver.cpp:237]     Train net output #1: loss = 0.0608278 (* 1 = 0.0608278 loss)
I1124 09:42:38.312726 32468 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1124 09:42:46.860460 32468 solver.cpp:218] Iteration 29300 (11.6996 iter/s, 8.54729s/100 iters), loss = 0.0811725
I1124 09:42:46.860460 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:42:46.860460 32468 solver.cpp:237]     Train net output #1: loss = 0.0811722 (* 1 = 0.0811722 loss)
I1124 09:42:46.860460 32468 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1124 09:42:55.392194 32468 solver.cpp:218] Iteration 29400 (11.7207 iter/s, 8.5319s/100 iters), loss = 0.0469475
I1124 09:42:55.392194 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:42:55.392194 32468 solver.cpp:237]     Train net output #1: loss = 0.0469472 (* 1 = 0.0469472 loss)
I1124 09:42:55.392194 32468 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1124 09:43:03.504407 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:43:03.841524 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_29500.caffemodel
I1124 09:43:03.881511 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_29500.solverstate
I1124 09:43:03.900511 32468 solver.cpp:330] Iteration 29500, Testing net (#0)
I1124 09:43:03.900511 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:43:06.341688 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:43:06.440702 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9082
I1124 09:43:06.440702 32468 solver.cpp:397]     Test net output #1: loss = 0.292607 (* 1 = 0.292607 loss)
I1124 09:43:06.524235 32468 solver.cpp:218] Iteration 29500 (8.98403 iter/s, 11.1309s/100 iters), loss = 0.068435
I1124 09:43:06.524235 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1124 09:43:06.524235 32468 solver.cpp:237]     Train net output #1: loss = 0.0684347 (* 1 = 0.0684347 loss)
I1124 09:43:06.524235 32468 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1124 09:43:15.066352 32468 solver.cpp:218] Iteration 29600 (11.7073 iter/s, 8.54169s/100 iters), loss = 0.0511592
I1124 09:43:15.066352 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1124 09:43:15.066352 32468 solver.cpp:237]     Train net output #1: loss = 0.0511589 (* 1 = 0.0511589 loss)
I1124 09:43:15.066352 32468 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1124 09:43:23.599544 32468 solver.cpp:218] Iteration 29700 (11.7192 iter/s, 8.53301s/100 iters), loss = 0.0788579
I1124 09:43:23.599544 32468 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1124 09:43:23.599544 32468 solver.cpp:237]     Train net output #1: loss = 0.0788576 (* 1 = 0.0788576 loss)
I1124 09:43:23.599544 32468 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1124 09:43:32.133829 32468 solver.cpp:218] Iteration 29800 (11.7178 iter/s, 8.53406s/100 iters), loss = 0.0471532
I1124 09:43:32.134833 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:43:32.134833 32468 solver.cpp:237]     Train net output #1: loss = 0.0471528 (* 1 = 0.0471528 loss)
I1124 09:43:32.134833 32468 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1124 09:43:40.670557 32468 solver.cpp:218] Iteration 29900 (11.7158 iter/s, 8.53547s/100 iters), loss = 0.0354133
I1124 09:43:40.670557 32468 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1124 09:43:40.670557 32468 solver.cpp:237]     Train net output #1: loss = 0.035413 (* 1 = 0.035413 loss)
I1124 09:43:40.670557 32468 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1124 09:43:48.783843 26912 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:43:49.121229 32468 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_30000.caffemodel
I1124 09:43:49.191222 32468 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_1.6k_8L_5x5_iter_30000.solverstate
I1124 09:43:49.237251 32468 solver.cpp:310] Iteration 30000, loss = 0.0863012
I1124 09:43:49.237251 32468 solver.cpp:330] Iteration 30000, Testing net (#0)
I1124 09:43:49.237251 32468 net.cpp:676] Ignoring source layer accuracy_training
I1124 09:43:51.676962 29896 data_layer.cpp:73] Restarting data prefetching from start.
I1124 09:43:51.775609 32468 solver.cpp:397]     Test net output #0: accuracy = 0.9084
I1124 09:43:51.775609 32468 solver.cpp:397]     Test net output #1: loss = 0.292614 (* 1 = 0.292614 loss)
I1124 09:43:51.775609 32468 solver.cpp:315] Optimization Done.
I1124 09:43:51.775609 32468 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 
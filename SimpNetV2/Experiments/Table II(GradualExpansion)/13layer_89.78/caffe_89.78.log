
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1122 04:23:07.998929  5428 caffe.cpp:219] Using GPUs 0
I1122 04:23:08.369297  5428 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1122 04:23:08.691612  5428 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 04:23:08.708617  5428 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_300k_13L"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1122 04:23:08.709610  5428 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 04:23:08.710620  5428 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 04:23:08.710620  5428 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1122 04:23:08.710620  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1122 04:23:08.710620  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1122 04:23:08.710620  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1122 04:23:08.710620  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1122 04:23:08.710620  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1122 04:23:08.710620  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1122 04:23:08.710620  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1122 04:23:08.711612  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1122 04:23:08.711612  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1122 04:23:08.711612  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1122 04:23:08.711612  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1122 04:23:08.711612  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1122 04:23:08.711612  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1122 04:23:08.711612  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1122 04:23:08.711612  5428 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1122 04:23:08.711612  5428 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_13L_Simple_NoGrpCon_NoDrp_300k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1122 04:23:08.731623  5428 layer_factory.cpp:58] Creating layer cifar
I1122 04:23:08.743613  5428 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1122 04:23:08.743613  5428 net.cpp:84] Creating Layer cifar
I1122 04:23:08.743613  5428 net.cpp:380] cifar -> data
I1122 04:23:08.743613  5428 net.cpp:380] cifar -> label
I1122 04:23:08.744617  5428 data_layer.cpp:45] output data size: 100,3,32,32
I1122 04:23:08.749632  5428 net.cpp:122] Setting up cifar
I1122 04:23:08.749632  5428 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1122 04:23:08.749632  5428 net.cpp:129] Top shape: 100 (100)
I1122 04:23:08.749632  5428 net.cpp:137] Memory required for data: 1229200
I1122 04:23:08.749632  5428 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1122 04:23:08.749632  5428 net.cpp:84] Creating Layer label_cifar_1_split
I1122 04:23:08.749632  5428 net.cpp:406] label_cifar_1_split <- label
I1122 04:23:08.749632  5428 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1122 04:23:08.749632  5428 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1122 04:23:08.749632  5428 net.cpp:122] Setting up label_cifar_1_split
I1122 04:23:08.749632  5428 net.cpp:129] Top shape: 100 (100)
I1122 04:23:08.749632  5428 net.cpp:129] Top shape: 100 (100)
I1122 04:23:08.749632  5428 net.cpp:137] Memory required for data: 1230000
I1122 04:23:08.749632  5428 layer_factory.cpp:58] Creating layer conv1
I1122 04:23:08.749632  5428 net.cpp:84] Creating Layer conv1
I1122 04:23:08.749632  5428 net.cpp:406] conv1 <- data
I1122 04:23:08.749632  5428 net.cpp:380] conv1 -> conv1
I1122 04:23:08.750613  2416 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 04:23:08.996057  5428 net.cpp:122] Setting up conv1
I1122 04:23:08.996057  5428 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1122 04:23:08.996057  5428 net.cpp:137] Memory required for data: 13518000
I1122 04:23:08.996057  5428 layer_factory.cpp:58] Creating layer bn1
I1122 04:23:08.996057  5428 net.cpp:84] Creating Layer bn1
I1122 04:23:08.996057  5428 net.cpp:406] bn1 <- conv1
I1122 04:23:08.996057  5428 net.cpp:367] bn1 -> conv1 (in-place)
I1122 04:23:08.996057  5428 net.cpp:122] Setting up bn1
I1122 04:23:08.996057  5428 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1122 04:23:08.996057  5428 net.cpp:137] Memory required for data: 25806000
I1122 04:23:08.996057  5428 layer_factory.cpp:58] Creating layer scale1
I1122 04:23:08.996057  5428 net.cpp:84] Creating Layer scale1
I1122 04:23:08.996057  5428 net.cpp:406] scale1 <- conv1
I1122 04:23:08.996057  5428 net.cpp:367] scale1 -> conv1 (in-place)
I1122 04:23:08.996057  5428 layer_factory.cpp:58] Creating layer scale1
I1122 04:23:08.996057  5428 net.cpp:122] Setting up scale1
I1122 04:23:08.996057  5428 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1122 04:23:08.996057  5428 net.cpp:137] Memory required for data: 38094000
I1122 04:23:08.996057  5428 layer_factory.cpp:58] Creating layer relu1
I1122 04:23:08.996057  5428 net.cpp:84] Creating Layer relu1
I1122 04:23:08.996057  5428 net.cpp:406] relu1 <- conv1
I1122 04:23:08.996057  5428 net.cpp:367] relu1 -> conv1 (in-place)
I1122 04:23:08.997056  5428 net.cpp:122] Setting up relu1
I1122 04:23:08.997056  5428 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1122 04:23:08.997056  5428 net.cpp:137] Memory required for data: 50382000
I1122 04:23:08.997056  5428 layer_factory.cpp:58] Creating layer conv1_0
I1122 04:23:08.997056  5428 net.cpp:84] Creating Layer conv1_0
I1122 04:23:08.997056  5428 net.cpp:406] conv1_0 <- conv1
I1122 04:23:08.997056  5428 net.cpp:380] conv1_0 -> conv1_0
I1122 04:23:08.998052  5428 net.cpp:122] Setting up conv1_0
I1122 04:23:08.998052  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:08.998052  5428 net.cpp:137] Memory required for data: 66766000
I1122 04:23:08.998052  5428 layer_factory.cpp:58] Creating layer bn1_0
I1122 04:23:08.998052  5428 net.cpp:84] Creating Layer bn1_0
I1122 04:23:08.998052  5428 net.cpp:406] bn1_0 <- conv1_0
I1122 04:23:08.998052  5428 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1122 04:23:08.999053  5428 net.cpp:122] Setting up bn1_0
I1122 04:23:08.999053  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:08.999053  5428 net.cpp:137] Memory required for data: 83150000
I1122 04:23:08.999053  5428 layer_factory.cpp:58] Creating layer scale1_0
I1122 04:23:08.999053  5428 net.cpp:84] Creating Layer scale1_0
I1122 04:23:08.999053  5428 net.cpp:406] scale1_0 <- conv1_0
I1122 04:23:08.999053  5428 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1122 04:23:08.999053  5428 layer_factory.cpp:58] Creating layer scale1_0
I1122 04:23:08.999053  5428 net.cpp:122] Setting up scale1_0
I1122 04:23:08.999053  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:08.999053  5428 net.cpp:137] Memory required for data: 99534000
I1122 04:23:08.999053  5428 layer_factory.cpp:58] Creating layer relu1_0
I1122 04:23:08.999053  5428 net.cpp:84] Creating Layer relu1_0
I1122 04:23:08.999053  5428 net.cpp:406] relu1_0 <- conv1_0
I1122 04:23:08.999053  5428 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1122 04:23:08.999053  5428 net.cpp:122] Setting up relu1_0
I1122 04:23:08.999053  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:08.999053  5428 net.cpp:137] Memory required for data: 115918000
I1122 04:23:08.999053  5428 layer_factory.cpp:58] Creating layer conv2
I1122 04:23:08.999053  5428 net.cpp:84] Creating Layer conv2
I1122 04:23:08.999053  5428 net.cpp:406] conv2 <- conv1_0
I1122 04:23:08.999053  5428 net.cpp:380] conv2 -> conv2
I1122 04:23:09.000052  5428 net.cpp:122] Setting up conv2
I1122 04:23:09.000052  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.000052  5428 net.cpp:137] Memory required for data: 132302000
I1122 04:23:09.000052  5428 layer_factory.cpp:58] Creating layer bn2
I1122 04:23:09.000052  5428 net.cpp:84] Creating Layer bn2
I1122 04:23:09.000052  5428 net.cpp:406] bn2 <- conv2
I1122 04:23:09.000052  5428 net.cpp:367] bn2 -> conv2 (in-place)
I1122 04:23:09.001054  5428 net.cpp:122] Setting up bn2
I1122 04:23:09.001054  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.001054  5428 net.cpp:137] Memory required for data: 148686000
I1122 04:23:09.001054  5428 layer_factory.cpp:58] Creating layer scale2
I1122 04:23:09.001054  5428 net.cpp:84] Creating Layer scale2
I1122 04:23:09.001054  5428 net.cpp:406] scale2 <- conv2
I1122 04:23:09.001054  5428 net.cpp:367] scale2 -> conv2 (in-place)
I1122 04:23:09.001054  5428 layer_factory.cpp:58] Creating layer scale2
I1122 04:23:09.001054  5428 net.cpp:122] Setting up scale2
I1122 04:23:09.001054  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.001054  5428 net.cpp:137] Memory required for data: 165070000
I1122 04:23:09.001054  5428 layer_factory.cpp:58] Creating layer relu2
I1122 04:23:09.001054  5428 net.cpp:84] Creating Layer relu2
I1122 04:23:09.001054  5428 net.cpp:406] relu2 <- conv2
I1122 04:23:09.001054  5428 net.cpp:367] relu2 -> conv2 (in-place)
I1122 04:23:09.001054  5428 net.cpp:122] Setting up relu2
I1122 04:23:09.001054  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.001054  5428 net.cpp:137] Memory required for data: 181454000
I1122 04:23:09.001054  5428 layer_factory.cpp:58] Creating layer conv2_1
I1122 04:23:09.001054  5428 net.cpp:84] Creating Layer conv2_1
I1122 04:23:09.001054  5428 net.cpp:406] conv2_1 <- conv2
I1122 04:23:09.001054  5428 net.cpp:380] conv2_1 -> conv2_1
I1122 04:23:09.002053  5428 net.cpp:122] Setting up conv2_1
I1122 04:23:09.002053  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.002053  5428 net.cpp:137] Memory required for data: 197838000
I1122 04:23:09.002053  5428 layer_factory.cpp:58] Creating layer bn2_1
I1122 04:23:09.002053  5428 net.cpp:84] Creating Layer bn2_1
I1122 04:23:09.002053  5428 net.cpp:406] bn2_1 <- conv2_1
I1122 04:23:09.002053  5428 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1122 04:23:09.002053  5428 net.cpp:122] Setting up bn2_1
I1122 04:23:09.002053  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.002053  5428 net.cpp:137] Memory required for data: 214222000
I1122 04:23:09.002053  5428 layer_factory.cpp:58] Creating layer scale2_1
I1122 04:23:09.002053  5428 net.cpp:84] Creating Layer scale2_1
I1122 04:23:09.002053  5428 net.cpp:406] scale2_1 <- conv2_1
I1122 04:23:09.002053  5428 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1122 04:23:09.002053  5428 layer_factory.cpp:58] Creating layer scale2_1
I1122 04:23:09.002053  5428 net.cpp:122] Setting up scale2_1
I1122 04:23:09.002053  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.002053  5428 net.cpp:137] Memory required for data: 230606000
I1122 04:23:09.002053  5428 layer_factory.cpp:58] Creating layer relu2_1
I1122 04:23:09.002053  5428 net.cpp:84] Creating Layer relu2_1
I1122 04:23:09.002053  5428 net.cpp:406] relu2_1 <- conv2_1
I1122 04:23:09.002053  5428 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1122 04:23:09.003052  5428 net.cpp:122] Setting up relu2_1
I1122 04:23:09.003052  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.003052  5428 net.cpp:137] Memory required for data: 246990000
I1122 04:23:09.003052  5428 layer_factory.cpp:58] Creating layer conv2_2
I1122 04:23:09.003052  5428 net.cpp:84] Creating Layer conv2_2
I1122 04:23:09.003052  5428 net.cpp:406] conv2_2 <- conv2_1
I1122 04:23:09.003052  5428 net.cpp:380] conv2_2 -> conv2_2
I1122 04:23:09.005038  5428 net.cpp:122] Setting up conv2_2
I1122 04:23:09.005038  5428 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1122 04:23:09.005038  5428 net.cpp:137] Memory required for data: 267470000
I1122 04:23:09.005038  5428 layer_factory.cpp:58] Creating layer bn2_2
I1122 04:23:09.005038  5428 net.cpp:84] Creating Layer bn2_2
I1122 04:23:09.005038  5428 net.cpp:406] bn2_2 <- conv2_2
I1122 04:23:09.005038  5428 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1122 04:23:09.005038  5428 net.cpp:122] Setting up bn2_2
I1122 04:23:09.005038  5428 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1122 04:23:09.005038  5428 net.cpp:137] Memory required for data: 287950000
I1122 04:23:09.005038  5428 layer_factory.cpp:58] Creating layer scale2_2
I1122 04:23:09.005038  5428 net.cpp:84] Creating Layer scale2_2
I1122 04:23:09.005038  5428 net.cpp:406] scale2_2 <- conv2_2
I1122 04:23:09.005038  5428 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1122 04:23:09.005038  5428 layer_factory.cpp:58] Creating layer scale2_2
I1122 04:23:09.005038  5428 net.cpp:122] Setting up scale2_2
I1122 04:23:09.005038  5428 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1122 04:23:09.005038  5428 net.cpp:137] Memory required for data: 308430000
I1122 04:23:09.005038  5428 layer_factory.cpp:58] Creating layer relu2_2
I1122 04:23:09.005038  5428 net.cpp:84] Creating Layer relu2_2
I1122 04:23:09.005038  5428 net.cpp:406] relu2_2 <- conv2_2
I1122 04:23:09.005038  5428 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1122 04:23:09.006053  5428 net.cpp:122] Setting up relu2_2
I1122 04:23:09.006053  5428 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1122 04:23:09.006053  5428 net.cpp:137] Memory required for data: 328910000
I1122 04:23:09.006053  5428 layer_factory.cpp:58] Creating layer pool2_1
I1122 04:23:09.006053  5428 net.cpp:84] Creating Layer pool2_1
I1122 04:23:09.006053  5428 net.cpp:406] pool2_1 <- conv2_2
I1122 04:23:09.006053  5428 net.cpp:380] pool2_1 -> pool2_1
I1122 04:23:09.006053  5428 net.cpp:122] Setting up pool2_1
I1122 04:23:09.006053  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.006053  5428 net.cpp:137] Memory required for data: 334030000
I1122 04:23:09.006053  5428 layer_factory.cpp:58] Creating layer conv3
I1122 04:23:09.006053  5428 net.cpp:84] Creating Layer conv3
I1122 04:23:09.006053  5428 net.cpp:406] conv3 <- pool2_1
I1122 04:23:09.006053  5428 net.cpp:380] conv3 -> conv3
I1122 04:23:09.007053  5428 net.cpp:122] Setting up conv3
I1122 04:23:09.007053  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.007053  5428 net.cpp:137] Memory required for data: 339150000
I1122 04:23:09.007053  5428 layer_factory.cpp:58] Creating layer bn3
I1122 04:23:09.007053  5428 net.cpp:84] Creating Layer bn3
I1122 04:23:09.007053  5428 net.cpp:406] bn3 <- conv3
I1122 04:23:09.007053  5428 net.cpp:367] bn3 -> conv3 (in-place)
I1122 04:23:09.007053  5428 net.cpp:122] Setting up bn3
I1122 04:23:09.007053  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.007053  5428 net.cpp:137] Memory required for data: 344270000
I1122 04:23:09.007053  5428 layer_factory.cpp:58] Creating layer scale3
I1122 04:23:09.007053  5428 net.cpp:84] Creating Layer scale3
I1122 04:23:09.007053  5428 net.cpp:406] scale3 <- conv3
I1122 04:23:09.007053  5428 net.cpp:367] scale3 -> conv3 (in-place)
I1122 04:23:09.007053  5428 layer_factory.cpp:58] Creating layer scale3
I1122 04:23:09.007053  5428 net.cpp:122] Setting up scale3
I1122 04:23:09.007053  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.007053  5428 net.cpp:137] Memory required for data: 349390000
I1122 04:23:09.007053  5428 layer_factory.cpp:58] Creating layer relu3
I1122 04:23:09.007053  5428 net.cpp:84] Creating Layer relu3
I1122 04:23:09.007053  5428 net.cpp:406] relu3 <- conv3
I1122 04:23:09.007053  5428 net.cpp:367] relu3 -> conv3 (in-place)
I1122 04:23:09.008054  5428 net.cpp:122] Setting up relu3
I1122 04:23:09.008054  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.008054  5428 net.cpp:137] Memory required for data: 354510000
I1122 04:23:09.008054  5428 layer_factory.cpp:58] Creating layer conv3_1
I1122 04:23:09.008054  5428 net.cpp:84] Creating Layer conv3_1
I1122 04:23:09.008054  5428 net.cpp:406] conv3_1 <- conv3
I1122 04:23:09.008054  5428 net.cpp:380] conv3_1 -> conv3_1
I1122 04:23:09.009073  5428 net.cpp:122] Setting up conv3_1
I1122 04:23:09.009073  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.009073  5428 net.cpp:137] Memory required for data: 359630000
I1122 04:23:09.009073  5428 layer_factory.cpp:58] Creating layer bn3_1
I1122 04:23:09.009073  5428 net.cpp:84] Creating Layer bn3_1
I1122 04:23:09.009073  5428 net.cpp:406] bn3_1 <- conv3_1
I1122 04:23:09.009073  5428 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1122 04:23:09.009073  5428 net.cpp:122] Setting up bn3_1
I1122 04:23:09.009073  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.009073  5428 net.cpp:137] Memory required for data: 364750000
I1122 04:23:09.009073  5428 layer_factory.cpp:58] Creating layer scale3_1
I1122 04:23:09.009073  5428 net.cpp:84] Creating Layer scale3_1
I1122 04:23:09.009073  5428 net.cpp:406] scale3_1 <- conv3_1
I1122 04:23:09.009073  5428 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1122 04:23:09.009073  5428 layer_factory.cpp:58] Creating layer scale3_1
I1122 04:23:09.010038  5428 net.cpp:122] Setting up scale3_1
I1122 04:23:09.010038  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.010038  5428 net.cpp:137] Memory required for data: 369870000
I1122 04:23:09.010038  5428 layer_factory.cpp:58] Creating layer relu3_1
I1122 04:23:09.010038  5428 net.cpp:84] Creating Layer relu3_1
I1122 04:23:09.010038  5428 net.cpp:406] relu3_1 <- conv3_1
I1122 04:23:09.010038  5428 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1122 04:23:09.010038  5428 net.cpp:122] Setting up relu3_1
I1122 04:23:09.010038  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.010038  5428 net.cpp:137] Memory required for data: 374990000
I1122 04:23:09.010038  5428 layer_factory.cpp:58] Creating layer conv4
I1122 04:23:09.010038  5428 net.cpp:84] Creating Layer conv4
I1122 04:23:09.010038  5428 net.cpp:406] conv4 <- conv3_1
I1122 04:23:09.010038  5428 net.cpp:380] conv4 -> conv4
I1122 04:23:09.011057  5428 net.cpp:122] Setting up conv4
I1122 04:23:09.011057  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.011057  5428 net.cpp:137] Memory required for data: 380110000
I1122 04:23:09.011057  5428 layer_factory.cpp:58] Creating layer bn4
I1122 04:23:09.011057  5428 net.cpp:84] Creating Layer bn4
I1122 04:23:09.011057  5428 net.cpp:406] bn4 <- conv4
I1122 04:23:09.011057  5428 net.cpp:367] bn4 -> conv4 (in-place)
I1122 04:23:09.011057  5428 net.cpp:122] Setting up bn4
I1122 04:23:09.011057  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.011057  5428 net.cpp:137] Memory required for data: 385230000
I1122 04:23:09.011057  5428 layer_factory.cpp:58] Creating layer scale4
I1122 04:23:09.011057  5428 net.cpp:84] Creating Layer scale4
I1122 04:23:09.011057  5428 net.cpp:406] scale4 <- conv4
I1122 04:23:09.011057  5428 net.cpp:367] scale4 -> conv4 (in-place)
I1122 04:23:09.011057  5428 layer_factory.cpp:58] Creating layer scale4
I1122 04:23:09.011057  5428 net.cpp:122] Setting up scale4
I1122 04:23:09.011057  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.011057  5428 net.cpp:137] Memory required for data: 390350000
I1122 04:23:09.011057  5428 layer_factory.cpp:58] Creating layer relu4
I1122 04:23:09.011057  5428 net.cpp:84] Creating Layer relu4
I1122 04:23:09.011057  5428 net.cpp:406] relu4 <- conv4
I1122 04:23:09.011057  5428 net.cpp:367] relu4 -> conv4 (in-place)
I1122 04:23:09.012053  5428 net.cpp:122] Setting up relu4
I1122 04:23:09.012053  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.012053  5428 net.cpp:137] Memory required for data: 395470000
I1122 04:23:09.012053  5428 layer_factory.cpp:58] Creating layer conv4_1
I1122 04:23:09.012053  5428 net.cpp:84] Creating Layer conv4_1
I1122 04:23:09.012053  5428 net.cpp:406] conv4_1 <- conv4
I1122 04:23:09.012053  5428 net.cpp:380] conv4_1 -> conv4_1
I1122 04:23:09.013067  5428 net.cpp:122] Setting up conv4_1
I1122 04:23:09.013067  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.013067  5428 net.cpp:137] Memory required for data: 400590000
I1122 04:23:09.013067  5428 layer_factory.cpp:58] Creating layer bn4_1
I1122 04:23:09.013067  5428 net.cpp:84] Creating Layer bn4_1
I1122 04:23:09.013067  5428 net.cpp:406] bn4_1 <- conv4_1
I1122 04:23:09.013067  5428 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1122 04:23:09.013067  5428 net.cpp:122] Setting up bn4_1
I1122 04:23:09.013067  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.013067  5428 net.cpp:137] Memory required for data: 405710000
I1122 04:23:09.013067  5428 layer_factory.cpp:58] Creating layer scale4_1
I1122 04:23:09.013067  5428 net.cpp:84] Creating Layer scale4_1
I1122 04:23:09.013067  5428 net.cpp:406] scale4_1 <- conv4_1
I1122 04:23:09.013067  5428 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1122 04:23:09.013067  5428 layer_factory.cpp:58] Creating layer scale4_1
I1122 04:23:09.013067  5428 net.cpp:122] Setting up scale4_1
I1122 04:23:09.013067  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.013067  5428 net.cpp:137] Memory required for data: 410830000
I1122 04:23:09.013067  5428 layer_factory.cpp:58] Creating layer relu4_1
I1122 04:23:09.013067  5428 net.cpp:84] Creating Layer relu4_1
I1122 04:23:09.013067  5428 net.cpp:406] relu4_1 <- conv4_1
I1122 04:23:09.013067  5428 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1122 04:23:09.014052  5428 net.cpp:122] Setting up relu4_1
I1122 04:23:09.014052  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.014052  5428 net.cpp:137] Memory required for data: 415950000
I1122 04:23:09.014052  5428 layer_factory.cpp:58] Creating layer conv4_2
I1122 04:23:09.014052  5428 net.cpp:84] Creating Layer conv4_2
I1122 04:23:09.014052  5428 net.cpp:406] conv4_2 <- conv4_1
I1122 04:23:09.014052  5428 net.cpp:380] conv4_2 -> conv4_2
I1122 04:23:09.015053  5428 net.cpp:122] Setting up conv4_2
I1122 04:23:09.015053  5428 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1122 04:23:09.015053  5428 net.cpp:137] Memory required for data: 421889200
I1122 04:23:09.015053  5428 layer_factory.cpp:58] Creating layer bn4_2
I1122 04:23:09.015053  5428 net.cpp:84] Creating Layer bn4_2
I1122 04:23:09.015053  5428 net.cpp:406] bn4_2 <- conv4_2
I1122 04:23:09.015053  5428 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1122 04:23:09.015053  5428 net.cpp:122] Setting up bn4_2
I1122 04:23:09.015053  5428 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1122 04:23:09.015053  5428 net.cpp:137] Memory required for data: 427828400
I1122 04:23:09.015053  5428 layer_factory.cpp:58] Creating layer scale4_2
I1122 04:23:09.015053  5428 net.cpp:84] Creating Layer scale4_2
I1122 04:23:09.015053  5428 net.cpp:406] scale4_2 <- conv4_2
I1122 04:23:09.015053  5428 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1122 04:23:09.015053  5428 layer_factory.cpp:58] Creating layer scale4_2
I1122 04:23:09.016053  5428 net.cpp:122] Setting up scale4_2
I1122 04:23:09.016053  5428 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1122 04:23:09.016053  5428 net.cpp:137] Memory required for data: 433767600
I1122 04:23:09.016053  5428 layer_factory.cpp:58] Creating layer relu4_2
I1122 04:23:09.016053  5428 net.cpp:84] Creating Layer relu4_2
I1122 04:23:09.016053  5428 net.cpp:406] relu4_2 <- conv4_2
I1122 04:23:09.016053  5428 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1122 04:23:09.016053  5428 net.cpp:122] Setting up relu4_2
I1122 04:23:09.016053  5428 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1122 04:23:09.016053  5428 net.cpp:137] Memory required for data: 439706800
I1122 04:23:09.016053  5428 layer_factory.cpp:58] Creating layer pool4_2
I1122 04:23:09.016053  5428 net.cpp:84] Creating Layer pool4_2
I1122 04:23:09.016053  5428 net.cpp:406] pool4_2 <- conv4_2
I1122 04:23:09.016053  5428 net.cpp:380] pool4_2 -> pool4_2
I1122 04:23:09.016053  5428 net.cpp:122] Setting up pool4_2
I1122 04:23:09.016053  5428 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1122 04:23:09.016053  5428 net.cpp:137] Memory required for data: 441191600
I1122 04:23:09.016053  5428 layer_factory.cpp:58] Creating layer conv4_0
I1122 04:23:09.016053  5428 net.cpp:84] Creating Layer conv4_0
I1122 04:23:09.016053  5428 net.cpp:406] conv4_0 <- pool4_2
I1122 04:23:09.016053  5428 net.cpp:380] conv4_0 -> conv4_0
I1122 04:23:09.017057  5428 net.cpp:122] Setting up conv4_0
I1122 04:23:09.017057  5428 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1122 04:23:09.017057  5428 net.cpp:137] Memory required for data: 442676400
I1122 04:23:09.017057  5428 layer_factory.cpp:58] Creating layer bn4_0
I1122 04:23:09.017057  5428 net.cpp:84] Creating Layer bn4_0
I1122 04:23:09.017057  5428 net.cpp:406] bn4_0 <- conv4_0
I1122 04:23:09.017057  5428 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1122 04:23:09.017057  5428 net.cpp:122] Setting up bn4_0
I1122 04:23:09.017057  5428 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1122 04:23:09.017057  5428 net.cpp:137] Memory required for data: 444161200
I1122 04:23:09.017057  5428 layer_factory.cpp:58] Creating layer scale4_0
I1122 04:23:09.017057  5428 net.cpp:84] Creating Layer scale4_0
I1122 04:23:09.017057  5428 net.cpp:406] scale4_0 <- conv4_0
I1122 04:23:09.017057  5428 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1122 04:23:09.018052  5428 layer_factory.cpp:58] Creating layer scale4_0
I1122 04:23:09.018052  5428 net.cpp:122] Setting up scale4_0
I1122 04:23:09.018052  5428 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1122 04:23:09.018052  5428 net.cpp:137] Memory required for data: 445646000
I1122 04:23:09.018052  5428 layer_factory.cpp:58] Creating layer relu4_0
I1122 04:23:09.018052  5428 net.cpp:84] Creating Layer relu4_0
I1122 04:23:09.018052  5428 net.cpp:406] relu4_0 <- conv4_0
I1122 04:23:09.018052  5428 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1122 04:23:09.018052  5428 net.cpp:122] Setting up relu4_0
I1122 04:23:09.018052  5428 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1122 04:23:09.018052  5428 net.cpp:137] Memory required for data: 447130800
I1122 04:23:09.018052  5428 layer_factory.cpp:58] Creating layer conv11
I1122 04:23:09.018052  5428 net.cpp:84] Creating Layer conv11
I1122 04:23:09.018052  5428 net.cpp:406] conv11 <- conv4_0
I1122 04:23:09.018052  5428 net.cpp:380] conv11 -> conv11
I1122 04:23:09.020038  5428 net.cpp:122] Setting up conv11
I1122 04:23:09.020038  5428 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1122 04:23:09.020038  5428 net.cpp:137] Memory required for data: 448922800
I1122 04:23:09.020038  5428 layer_factory.cpp:58] Creating layer bn_conv11
I1122 04:23:09.020038  5428 net.cpp:84] Creating Layer bn_conv11
I1122 04:23:09.020038  5428 net.cpp:406] bn_conv11 <- conv11
I1122 04:23:09.020038  5428 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1122 04:23:09.020038  5428 net.cpp:122] Setting up bn_conv11
I1122 04:23:09.020038  5428 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1122 04:23:09.020038  5428 net.cpp:137] Memory required for data: 450714800
I1122 04:23:09.020038  5428 layer_factory.cpp:58] Creating layer scale_conv11
I1122 04:23:09.020038  5428 net.cpp:84] Creating Layer scale_conv11
I1122 04:23:09.020038  5428 net.cpp:406] scale_conv11 <- conv11
I1122 04:23:09.020038  5428 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1122 04:23:09.020038  5428 layer_factory.cpp:58] Creating layer scale_conv11
I1122 04:23:09.020038  5428 net.cpp:122] Setting up scale_conv11
I1122 04:23:09.020038  5428 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1122 04:23:09.020038  5428 net.cpp:137] Memory required for data: 452506800
I1122 04:23:09.020038  5428 layer_factory.cpp:58] Creating layer relu_conv11
I1122 04:23:09.020038  5428 net.cpp:84] Creating Layer relu_conv11
I1122 04:23:09.020038  5428 net.cpp:406] relu_conv11 <- conv11
I1122 04:23:09.020038  5428 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1122 04:23:09.021052  5428 net.cpp:122] Setting up relu_conv11
I1122 04:23:09.021052  5428 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1122 04:23:09.021052  5428 net.cpp:137] Memory required for data: 454298800
I1122 04:23:09.021052  5428 layer_factory.cpp:58] Creating layer conv12
I1122 04:23:09.021052  5428 net.cpp:84] Creating Layer conv12
I1122 04:23:09.021052  5428 net.cpp:406] conv12 <- conv11
I1122 04:23:09.021052  5428 net.cpp:380] conv12 -> conv12
I1122 04:23:09.022053  5428 net.cpp:122] Setting up conv12
I1122 04:23:09.022053  5428 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 04:23:09.022053  5428 net.cpp:137] Memory required for data: 456602800
I1122 04:23:09.022053  5428 layer_factory.cpp:58] Creating layer bn_conv12
I1122 04:23:09.022053  5428 net.cpp:84] Creating Layer bn_conv12
I1122 04:23:09.022053  5428 net.cpp:406] bn_conv12 <- conv12
I1122 04:23:09.022053  5428 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1122 04:23:09.023053  5428 net.cpp:122] Setting up bn_conv12
I1122 04:23:09.023053  5428 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 04:23:09.023053  5428 net.cpp:137] Memory required for data: 458906800
I1122 04:23:09.023053  5428 layer_factory.cpp:58] Creating layer scale_conv12
I1122 04:23:09.023053  5428 net.cpp:84] Creating Layer scale_conv12
I1122 04:23:09.023053  5428 net.cpp:406] scale_conv12 <- conv12
I1122 04:23:09.023053  5428 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1122 04:23:09.023053  5428 layer_factory.cpp:58] Creating layer scale_conv12
I1122 04:23:09.023053  5428 net.cpp:122] Setting up scale_conv12
I1122 04:23:09.023053  5428 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 04:23:09.023053  5428 net.cpp:137] Memory required for data: 461210800
I1122 04:23:09.023053  5428 layer_factory.cpp:58] Creating layer relu_conv12
I1122 04:23:09.023053  5428 net.cpp:84] Creating Layer relu_conv12
I1122 04:23:09.023053  5428 net.cpp:406] relu_conv12 <- conv12
I1122 04:23:09.023053  5428 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1122 04:23:09.023053  5428 net.cpp:122] Setting up relu_conv12
I1122 04:23:09.023053  5428 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 04:23:09.023053  5428 net.cpp:137] Memory required for data: 463514800
I1122 04:23:09.023053  5428 layer_factory.cpp:58] Creating layer poolcp6
I1122 04:23:09.023053  5428 net.cpp:84] Creating Layer poolcp6
I1122 04:23:09.023053  5428 net.cpp:406] poolcp6 <- conv12
I1122 04:23:09.023053  5428 net.cpp:380] poolcp6 -> poolcp6
I1122 04:23:09.023053  5428 net.cpp:122] Setting up poolcp6
I1122 04:23:09.023053  5428 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1122 04:23:09.023053  5428 net.cpp:137] Memory required for data: 463550800
I1122 04:23:09.023053  5428 layer_factory.cpp:58] Creating layer ip1
I1122 04:23:09.023053  5428 net.cpp:84] Creating Layer ip1
I1122 04:23:09.023053  5428 net.cpp:406] ip1 <- poolcp6
I1122 04:23:09.023053  5428 net.cpp:380] ip1 -> ip1
I1122 04:23:09.023053  5428 net.cpp:122] Setting up ip1
I1122 04:23:09.023053  5428 net.cpp:129] Top shape: 100 10 (1000)
I1122 04:23:09.023053  5428 net.cpp:137] Memory required for data: 463554800
I1122 04:23:09.023053  5428 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1122 04:23:09.023053  5428 net.cpp:84] Creating Layer ip1_ip1_0_split
I1122 04:23:09.023053  5428 net.cpp:406] ip1_ip1_0_split <- ip1
I1122 04:23:09.023053  5428 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1122 04:23:09.023053  5428 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1122 04:23:09.023053  5428 net.cpp:122] Setting up ip1_ip1_0_split
I1122 04:23:09.023053  5428 net.cpp:129] Top shape: 100 10 (1000)
I1122 04:23:09.023053  5428 net.cpp:129] Top shape: 100 10 (1000)
I1122 04:23:09.023053  5428 net.cpp:137] Memory required for data: 463562800
I1122 04:23:09.023053  5428 layer_factory.cpp:58] Creating layer accuracy_training
I1122 04:23:09.023053  5428 net.cpp:84] Creating Layer accuracy_training
I1122 04:23:09.023053  5428 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1122 04:23:09.023053  5428 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1122 04:23:09.023053  5428 net.cpp:380] accuracy_training -> accuracy_training
I1122 04:23:09.023053  5428 net.cpp:122] Setting up accuracy_training
I1122 04:23:09.023053  5428 net.cpp:129] Top shape: (1)
I1122 04:23:09.023053  5428 net.cpp:137] Memory required for data: 463562804
I1122 04:23:09.023053  5428 layer_factory.cpp:58] Creating layer loss
I1122 04:23:09.023053  5428 net.cpp:84] Creating Layer loss
I1122 04:23:09.023053  5428 net.cpp:406] loss <- ip1_ip1_0_split_1
I1122 04:23:09.023053  5428 net.cpp:406] loss <- label_cifar_1_split_1
I1122 04:23:09.023053  5428 net.cpp:380] loss -> loss
I1122 04:23:09.023053  5428 layer_factory.cpp:58] Creating layer loss
I1122 04:23:09.024054  5428 net.cpp:122] Setting up loss
I1122 04:23:09.024054  5428 net.cpp:129] Top shape: (1)
I1122 04:23:09.024054  5428 net.cpp:132]     with loss weight 1
I1122 04:23:09.024054  5428 net.cpp:137] Memory required for data: 463562808
I1122 04:23:09.024054  5428 net.cpp:198] loss needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:200] accuracy_training does not need backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] ip1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] poolcp6 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] relu_conv12 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] scale_conv12 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] bn_conv12 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] conv12 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] relu_conv11 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] scale_conv11 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] bn_conv11 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] conv11 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] relu4_0 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] scale4_0 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] bn4_0 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] conv4_0 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] pool4_2 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] relu4_2 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] scale4_2 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] bn4_2 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] conv4_2 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] relu4_1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] scale4_1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] bn4_1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] conv4_1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] relu4 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] scale4 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] bn4 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] conv4 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] relu3_1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] scale3_1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] bn3_1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] conv3_1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] relu3 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] scale3 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] bn3 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] conv3 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] pool2_1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] relu2_2 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] scale2_2 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] bn2_2 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] conv2_2 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] relu2_1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] scale2_1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] bn2_1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] conv2_1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] relu2 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] scale2 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] bn2 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] conv2 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] relu1_0 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] scale1_0 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] bn1_0 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] conv1_0 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] relu1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] scale1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] bn1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:198] conv1 needs backward computation.
I1122 04:23:09.024054  5428 net.cpp:200] label_cifar_1_split does not need backward computation.
I1122 04:23:09.024054  5428 net.cpp:200] cifar does not need backward computation.
I1122 04:23:09.024054  5428 net.cpp:242] This network produces output accuracy_training
I1122 04:23:09.024054  5428 net.cpp:242] This network produces output loss
I1122 04:23:09.024054  5428 net.cpp:255] Network initialization done.
I1122 04:23:09.025053  5428 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 04:23:09.025053  5428 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1122 04:23:09.025053  5428 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1122 04:23:09.025053  5428 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1122 04:23:09.025053  5428 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_13L_Simple_NoGrpCon_NoDrp_300k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1122 04:23:09.026052  5428 layer_factory.cpp:58] Creating layer cifar
I1122 04:23:09.088922  5428 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1122 04:23:09.088922  5428 net.cpp:84] Creating Layer cifar
I1122 04:23:09.088922  5428 net.cpp:380] cifar -> data
I1122 04:23:09.088922  5428 net.cpp:380] cifar -> label
I1122 04:23:09.088922  5428 data_layer.cpp:45] output data size: 100,3,32,32
I1122 04:23:09.094921  5428 net.cpp:122] Setting up cifar
I1122 04:23:09.094921  5428 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1122 04:23:09.094921  5428 net.cpp:129] Top shape: 100 (100)
I1122 04:23:09.094921  5428 net.cpp:137] Memory required for data: 1229200
I1122 04:23:09.094921  5428 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1122 04:23:09.094921  5428 net.cpp:84] Creating Layer label_cifar_1_split
I1122 04:23:09.094921  5428 net.cpp:406] label_cifar_1_split <- label
I1122 04:23:09.094921  5428 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1122 04:23:09.094921  5428 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1122 04:23:09.094921  5428 net.cpp:122] Setting up label_cifar_1_split
I1122 04:23:09.094921  5428 net.cpp:129] Top shape: 100 (100)
I1122 04:23:09.094921  5428 net.cpp:129] Top shape: 100 (100)
I1122 04:23:09.094921  5428 net.cpp:137] Memory required for data: 1230000
I1122 04:23:09.094921  5428 layer_factory.cpp:58] Creating layer conv1
I1122 04:23:09.094921  5428 net.cpp:84] Creating Layer conv1
I1122 04:23:09.094921  5428 net.cpp:406] conv1 <- data
I1122 04:23:09.094921  5428 net.cpp:380] conv1 -> conv1
I1122 04:23:09.095927 17944 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1122 04:23:09.095927  5428 net.cpp:122] Setting up conv1
I1122 04:23:09.096922  5428 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1122 04:23:09.096922  5428 net.cpp:137] Memory required for data: 13518000
I1122 04:23:09.096922  5428 layer_factory.cpp:58] Creating layer bn1
I1122 04:23:09.096922  5428 net.cpp:84] Creating Layer bn1
I1122 04:23:09.096922  5428 net.cpp:406] bn1 <- conv1
I1122 04:23:09.096922  5428 net.cpp:367] bn1 -> conv1 (in-place)
I1122 04:23:09.096922  5428 net.cpp:122] Setting up bn1
I1122 04:23:09.096922  5428 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1122 04:23:09.096922  5428 net.cpp:137] Memory required for data: 25806000
I1122 04:23:09.096922  5428 layer_factory.cpp:58] Creating layer scale1
I1122 04:23:09.096922  5428 net.cpp:84] Creating Layer scale1
I1122 04:23:09.096922  5428 net.cpp:406] scale1 <- conv1
I1122 04:23:09.096922  5428 net.cpp:367] scale1 -> conv1 (in-place)
I1122 04:23:09.096922  5428 layer_factory.cpp:58] Creating layer scale1
I1122 04:23:09.096922  5428 net.cpp:122] Setting up scale1
I1122 04:23:09.096922  5428 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1122 04:23:09.096922  5428 net.cpp:137] Memory required for data: 38094000
I1122 04:23:09.096922  5428 layer_factory.cpp:58] Creating layer relu1
I1122 04:23:09.096922  5428 net.cpp:84] Creating Layer relu1
I1122 04:23:09.096922  5428 net.cpp:406] relu1 <- conv1
I1122 04:23:09.096922  5428 net.cpp:367] relu1 -> conv1 (in-place)
I1122 04:23:09.096922  5428 net.cpp:122] Setting up relu1
I1122 04:23:09.096922  5428 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1122 04:23:09.096922  5428 net.cpp:137] Memory required for data: 50382000
I1122 04:23:09.096922  5428 layer_factory.cpp:58] Creating layer conv1_0
I1122 04:23:09.096922  5428 net.cpp:84] Creating Layer conv1_0
I1122 04:23:09.096922  5428 net.cpp:406] conv1_0 <- conv1
I1122 04:23:09.096922  5428 net.cpp:380] conv1_0 -> conv1_0
I1122 04:23:09.098919  5428 net.cpp:122] Setting up conv1_0
I1122 04:23:09.098919  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.098919  5428 net.cpp:137] Memory required for data: 66766000
I1122 04:23:09.098919  5428 layer_factory.cpp:58] Creating layer bn1_0
I1122 04:23:09.098919  5428 net.cpp:84] Creating Layer bn1_0
I1122 04:23:09.098919  5428 net.cpp:406] bn1_0 <- conv1_0
I1122 04:23:09.098919  5428 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1122 04:23:09.098919  5428 net.cpp:122] Setting up bn1_0
I1122 04:23:09.098919  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.098919  5428 net.cpp:137] Memory required for data: 83150000
I1122 04:23:09.098919  5428 layer_factory.cpp:58] Creating layer scale1_0
I1122 04:23:09.098919  5428 net.cpp:84] Creating Layer scale1_0
I1122 04:23:09.098919  5428 net.cpp:406] scale1_0 <- conv1_0
I1122 04:23:09.098919  5428 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1122 04:23:09.098919  5428 layer_factory.cpp:58] Creating layer scale1_0
I1122 04:23:09.099922  5428 net.cpp:122] Setting up scale1_0
I1122 04:23:09.099922  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.099922  5428 net.cpp:137] Memory required for data: 99534000
I1122 04:23:09.099922  5428 layer_factory.cpp:58] Creating layer relu1_0
I1122 04:23:09.099922  5428 net.cpp:84] Creating Layer relu1_0
I1122 04:23:09.099922  5428 net.cpp:406] relu1_0 <- conv1_0
I1122 04:23:09.099922  5428 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1122 04:23:09.099922  5428 net.cpp:122] Setting up relu1_0
I1122 04:23:09.099922  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.099922  5428 net.cpp:137] Memory required for data: 115918000
I1122 04:23:09.099922  5428 layer_factory.cpp:58] Creating layer conv2
I1122 04:23:09.099922  5428 net.cpp:84] Creating Layer conv2
I1122 04:23:09.099922  5428 net.cpp:406] conv2 <- conv1_0
I1122 04:23:09.099922  5428 net.cpp:380] conv2 -> conv2
I1122 04:23:09.101930  5428 net.cpp:122] Setting up conv2
I1122 04:23:09.101930  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.101930  5428 net.cpp:137] Memory required for data: 132302000
I1122 04:23:09.101930  5428 layer_factory.cpp:58] Creating layer bn2
I1122 04:23:09.101930  5428 net.cpp:84] Creating Layer bn2
I1122 04:23:09.101930  5428 net.cpp:406] bn2 <- conv2
I1122 04:23:09.101930  5428 net.cpp:367] bn2 -> conv2 (in-place)
I1122 04:23:09.101930  5428 net.cpp:122] Setting up bn2
I1122 04:23:09.101930  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.101930  5428 net.cpp:137] Memory required for data: 148686000
I1122 04:23:09.101930  5428 layer_factory.cpp:58] Creating layer scale2
I1122 04:23:09.101930  5428 net.cpp:84] Creating Layer scale2
I1122 04:23:09.101930  5428 net.cpp:406] scale2 <- conv2
I1122 04:23:09.101930  5428 net.cpp:367] scale2 -> conv2 (in-place)
I1122 04:23:09.102921  5428 layer_factory.cpp:58] Creating layer scale2
I1122 04:23:09.102921  5428 net.cpp:122] Setting up scale2
I1122 04:23:09.102921  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.102921  5428 net.cpp:137] Memory required for data: 165070000
I1122 04:23:09.102921  5428 layer_factory.cpp:58] Creating layer relu2
I1122 04:23:09.102921  5428 net.cpp:84] Creating Layer relu2
I1122 04:23:09.102921  5428 net.cpp:406] relu2 <- conv2
I1122 04:23:09.102921  5428 net.cpp:367] relu2 -> conv2 (in-place)
I1122 04:23:09.102921  5428 net.cpp:122] Setting up relu2
I1122 04:23:09.102921  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.102921  5428 net.cpp:137] Memory required for data: 181454000
I1122 04:23:09.102921  5428 layer_factory.cpp:58] Creating layer conv2_1
I1122 04:23:09.102921  5428 net.cpp:84] Creating Layer conv2_1
I1122 04:23:09.102921  5428 net.cpp:406] conv2_1 <- conv2
I1122 04:23:09.102921  5428 net.cpp:380] conv2_1 -> conv2_1
I1122 04:23:09.104919  5428 net.cpp:122] Setting up conv2_1
I1122 04:23:09.104919  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.104919  5428 net.cpp:137] Memory required for data: 197838000
I1122 04:23:09.104919  5428 layer_factory.cpp:58] Creating layer bn2_1
I1122 04:23:09.104919  5428 net.cpp:84] Creating Layer bn2_1
I1122 04:23:09.104919  5428 net.cpp:406] bn2_1 <- conv2_1
I1122 04:23:09.104919  5428 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1122 04:23:09.105919  5428 net.cpp:122] Setting up bn2_1
I1122 04:23:09.105919  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.105919  5428 net.cpp:137] Memory required for data: 214222000
I1122 04:23:09.105919  5428 layer_factory.cpp:58] Creating layer scale2_1
I1122 04:23:09.105919  5428 net.cpp:84] Creating Layer scale2_1
I1122 04:23:09.105919  5428 net.cpp:406] scale2_1 <- conv2_1
I1122 04:23:09.105919  5428 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1122 04:23:09.105919  5428 layer_factory.cpp:58] Creating layer scale2_1
I1122 04:23:09.105919  5428 net.cpp:122] Setting up scale2_1
I1122 04:23:09.105919  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.105919  5428 net.cpp:137] Memory required for data: 230606000
I1122 04:23:09.105919  5428 layer_factory.cpp:58] Creating layer relu2_1
I1122 04:23:09.105919  5428 net.cpp:84] Creating Layer relu2_1
I1122 04:23:09.105919  5428 net.cpp:406] relu2_1 <- conv2_1
I1122 04:23:09.105919  5428 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1122 04:23:09.105919  5428 net.cpp:122] Setting up relu2_1
I1122 04:23:09.105919  5428 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1122 04:23:09.105919  5428 net.cpp:137] Memory required for data: 246990000
I1122 04:23:09.105919  5428 layer_factory.cpp:58] Creating layer conv2_2
I1122 04:23:09.105919  5428 net.cpp:84] Creating Layer conv2_2
I1122 04:23:09.105919  5428 net.cpp:406] conv2_2 <- conv2_1
I1122 04:23:09.105919  5428 net.cpp:380] conv2_2 -> conv2_2
I1122 04:23:09.107919  5428 net.cpp:122] Setting up conv2_2
I1122 04:23:09.107919  5428 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1122 04:23:09.107919  5428 net.cpp:137] Memory required for data: 267470000
I1122 04:23:09.107919  5428 layer_factory.cpp:58] Creating layer bn2_2
I1122 04:23:09.107919  5428 net.cpp:84] Creating Layer bn2_2
I1122 04:23:09.107919  5428 net.cpp:406] bn2_2 <- conv2_2
I1122 04:23:09.107919  5428 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1122 04:23:09.108923  5428 net.cpp:122] Setting up bn2_2
I1122 04:23:09.108923  5428 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1122 04:23:09.108923  5428 net.cpp:137] Memory required for data: 287950000
I1122 04:23:09.108923  5428 layer_factory.cpp:58] Creating layer scale2_2
I1122 04:23:09.108923  5428 net.cpp:84] Creating Layer scale2_2
I1122 04:23:09.108923  5428 net.cpp:406] scale2_2 <- conv2_2
I1122 04:23:09.108923  5428 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1122 04:23:09.108923  5428 layer_factory.cpp:58] Creating layer scale2_2
I1122 04:23:09.108923  5428 net.cpp:122] Setting up scale2_2
I1122 04:23:09.108923  5428 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1122 04:23:09.108923  5428 net.cpp:137] Memory required for data: 308430000
I1122 04:23:09.108923  5428 layer_factory.cpp:58] Creating layer relu2_2
I1122 04:23:09.108923  5428 net.cpp:84] Creating Layer relu2_2
I1122 04:23:09.108923  5428 net.cpp:406] relu2_2 <- conv2_2
I1122 04:23:09.108923  5428 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1122 04:23:09.108923  5428 net.cpp:122] Setting up relu2_2
I1122 04:23:09.108923  5428 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1122 04:23:09.108923  5428 net.cpp:137] Memory required for data: 328910000
I1122 04:23:09.108923  5428 layer_factory.cpp:58] Creating layer pool2_1
I1122 04:23:09.108923  5428 net.cpp:84] Creating Layer pool2_1
I1122 04:23:09.108923  5428 net.cpp:406] pool2_1 <- conv2_2
I1122 04:23:09.108923  5428 net.cpp:380] pool2_1 -> pool2_1
I1122 04:23:09.108923  5428 net.cpp:122] Setting up pool2_1
I1122 04:23:09.108923  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.108923  5428 net.cpp:137] Memory required for data: 334030000
I1122 04:23:09.108923  5428 layer_factory.cpp:58] Creating layer conv3
I1122 04:23:09.108923  5428 net.cpp:84] Creating Layer conv3
I1122 04:23:09.108923  5428 net.cpp:406] conv3 <- pool2_1
I1122 04:23:09.108923  5428 net.cpp:380] conv3 -> conv3
I1122 04:23:09.110922  5428 net.cpp:122] Setting up conv3
I1122 04:23:09.110922  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.110922  5428 net.cpp:137] Memory required for data: 339150000
I1122 04:23:09.110922  5428 layer_factory.cpp:58] Creating layer bn3
I1122 04:23:09.110922  5428 net.cpp:84] Creating Layer bn3
I1122 04:23:09.110922  5428 net.cpp:406] bn3 <- conv3
I1122 04:23:09.110922  5428 net.cpp:367] bn3 -> conv3 (in-place)
I1122 04:23:09.110922  5428 net.cpp:122] Setting up bn3
I1122 04:23:09.110922  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.110922  5428 net.cpp:137] Memory required for data: 344270000
I1122 04:23:09.110922  5428 layer_factory.cpp:58] Creating layer scale3
I1122 04:23:09.110922  5428 net.cpp:84] Creating Layer scale3
I1122 04:23:09.110922  5428 net.cpp:406] scale3 <- conv3
I1122 04:23:09.110922  5428 net.cpp:367] scale3 -> conv3 (in-place)
I1122 04:23:09.110922  5428 layer_factory.cpp:58] Creating layer scale3
I1122 04:23:09.110922  5428 net.cpp:122] Setting up scale3
I1122 04:23:09.110922  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.110922  5428 net.cpp:137] Memory required for data: 349390000
I1122 04:23:09.110922  5428 layer_factory.cpp:58] Creating layer relu3
I1122 04:23:09.110922  5428 net.cpp:84] Creating Layer relu3
I1122 04:23:09.110922  5428 net.cpp:406] relu3 <- conv3
I1122 04:23:09.110922  5428 net.cpp:367] relu3 -> conv3 (in-place)
I1122 04:23:09.110922  5428 net.cpp:122] Setting up relu3
I1122 04:23:09.110922  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.110922  5428 net.cpp:137] Memory required for data: 354510000
I1122 04:23:09.110922  5428 layer_factory.cpp:58] Creating layer conv3_1
I1122 04:23:09.110922  5428 net.cpp:84] Creating Layer conv3_1
I1122 04:23:09.110922  5428 net.cpp:406] conv3_1 <- conv3
I1122 04:23:09.110922  5428 net.cpp:380] conv3_1 -> conv3_1
I1122 04:23:09.112926  5428 net.cpp:122] Setting up conv3_1
I1122 04:23:09.112926  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.112926  5428 net.cpp:137] Memory required for data: 359630000
I1122 04:23:09.112926  5428 layer_factory.cpp:58] Creating layer bn3_1
I1122 04:23:09.112926  5428 net.cpp:84] Creating Layer bn3_1
I1122 04:23:09.112926  5428 net.cpp:406] bn3_1 <- conv3_1
I1122 04:23:09.112926  5428 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1122 04:23:09.112926  5428 net.cpp:122] Setting up bn3_1
I1122 04:23:09.112926  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.112926  5428 net.cpp:137] Memory required for data: 364750000
I1122 04:23:09.112926  5428 layer_factory.cpp:58] Creating layer scale3_1
I1122 04:23:09.112926  5428 net.cpp:84] Creating Layer scale3_1
I1122 04:23:09.112926  5428 net.cpp:406] scale3_1 <- conv3_1
I1122 04:23:09.112926  5428 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1122 04:23:09.112926  5428 layer_factory.cpp:58] Creating layer scale3_1
I1122 04:23:09.112926  5428 net.cpp:122] Setting up scale3_1
I1122 04:23:09.112926  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.112926  5428 net.cpp:137] Memory required for data: 369870000
I1122 04:23:09.112926  5428 layer_factory.cpp:58] Creating layer relu3_1
I1122 04:23:09.112926  5428 net.cpp:84] Creating Layer relu3_1
I1122 04:23:09.112926  5428 net.cpp:406] relu3_1 <- conv3_1
I1122 04:23:09.112926  5428 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1122 04:23:09.112926  5428 net.cpp:122] Setting up relu3_1
I1122 04:23:09.112926  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.112926  5428 net.cpp:137] Memory required for data: 374990000
I1122 04:23:09.112926  5428 layer_factory.cpp:58] Creating layer conv4
I1122 04:23:09.112926  5428 net.cpp:84] Creating Layer conv4
I1122 04:23:09.112926  5428 net.cpp:406] conv4 <- conv3_1
I1122 04:23:09.112926  5428 net.cpp:380] conv4 -> conv4
I1122 04:23:09.114922  5428 net.cpp:122] Setting up conv4
I1122 04:23:09.114922  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.114922  5428 net.cpp:137] Memory required for data: 380110000
I1122 04:23:09.114922  5428 layer_factory.cpp:58] Creating layer bn4
I1122 04:23:09.114922  5428 net.cpp:84] Creating Layer bn4
I1122 04:23:09.114922  5428 net.cpp:406] bn4 <- conv4
I1122 04:23:09.114922  5428 net.cpp:367] bn4 -> conv4 (in-place)
I1122 04:23:09.114922  5428 net.cpp:122] Setting up bn4
I1122 04:23:09.114922  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.114922  5428 net.cpp:137] Memory required for data: 385230000
I1122 04:23:09.114922  5428 layer_factory.cpp:58] Creating layer scale4
I1122 04:23:09.114922  5428 net.cpp:84] Creating Layer scale4
I1122 04:23:09.114922  5428 net.cpp:406] scale4 <- conv4
I1122 04:23:09.114922  5428 net.cpp:367] scale4 -> conv4 (in-place)
I1122 04:23:09.114922  5428 layer_factory.cpp:58] Creating layer scale4
I1122 04:23:09.115922  5428 net.cpp:122] Setting up scale4
I1122 04:23:09.115922  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.115922  5428 net.cpp:137] Memory required for data: 390350000
I1122 04:23:09.115922  5428 layer_factory.cpp:58] Creating layer relu4
I1122 04:23:09.115922  5428 net.cpp:84] Creating Layer relu4
I1122 04:23:09.115922  5428 net.cpp:406] relu4 <- conv4
I1122 04:23:09.115922  5428 net.cpp:367] relu4 -> conv4 (in-place)
I1122 04:23:09.115922  5428 net.cpp:122] Setting up relu4
I1122 04:23:09.115922  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.115922  5428 net.cpp:137] Memory required for data: 395470000
I1122 04:23:09.115922  5428 layer_factory.cpp:58] Creating layer conv4_1
I1122 04:23:09.115922  5428 net.cpp:84] Creating Layer conv4_1
I1122 04:23:09.115922  5428 net.cpp:406] conv4_1 <- conv4
I1122 04:23:09.115922  5428 net.cpp:380] conv4_1 -> conv4_1
I1122 04:23:09.116922  5428 net.cpp:122] Setting up conv4_1
I1122 04:23:09.116922  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.116922  5428 net.cpp:137] Memory required for data: 400590000
I1122 04:23:09.116922  5428 layer_factory.cpp:58] Creating layer bn4_1
I1122 04:23:09.116922  5428 net.cpp:84] Creating Layer bn4_1
I1122 04:23:09.116922  5428 net.cpp:406] bn4_1 <- conv4_1
I1122 04:23:09.116922  5428 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1122 04:23:09.116922  5428 net.cpp:122] Setting up bn4_1
I1122 04:23:09.116922  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.116922  5428 net.cpp:137] Memory required for data: 405710000
I1122 04:23:09.116922  5428 layer_factory.cpp:58] Creating layer scale4_1
I1122 04:23:09.116922  5428 net.cpp:84] Creating Layer scale4_1
I1122 04:23:09.116922  5428 net.cpp:406] scale4_1 <- conv4_1
I1122 04:23:09.116922  5428 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1122 04:23:09.116922  5428 layer_factory.cpp:58] Creating layer scale4_1
I1122 04:23:09.117923  5428 net.cpp:122] Setting up scale4_1
I1122 04:23:09.117923  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.117923  5428 net.cpp:137] Memory required for data: 410830000
I1122 04:23:09.117923  5428 layer_factory.cpp:58] Creating layer relu4_1
I1122 04:23:09.117923  5428 net.cpp:84] Creating Layer relu4_1
I1122 04:23:09.117923  5428 net.cpp:406] relu4_1 <- conv4_1
I1122 04:23:09.117923  5428 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1122 04:23:09.117923  5428 net.cpp:122] Setting up relu4_1
I1122 04:23:09.117923  5428 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1122 04:23:09.117923  5428 net.cpp:137] Memory required for data: 415950000
I1122 04:23:09.117923  5428 layer_factory.cpp:58] Creating layer conv4_2
I1122 04:23:09.117923  5428 net.cpp:84] Creating Layer conv4_2
I1122 04:23:09.117923  5428 net.cpp:406] conv4_2 <- conv4_1
I1122 04:23:09.117923  5428 net.cpp:380] conv4_2 -> conv4_2
I1122 04:23:09.119920  5428 net.cpp:122] Setting up conv4_2
I1122 04:23:09.119920  5428 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1122 04:23:09.119920  5428 net.cpp:137] Memory required for data: 421889200
I1122 04:23:09.119920  5428 layer_factory.cpp:58] Creating layer bn4_2
I1122 04:23:09.119920  5428 net.cpp:84] Creating Layer bn4_2
I1122 04:23:09.119920  5428 net.cpp:406] bn4_2 <- conv4_2
I1122 04:23:09.119920  5428 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1122 04:23:09.119920  5428 net.cpp:122] Setting up bn4_2
I1122 04:23:09.119920  5428 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1122 04:23:09.119920  5428 net.cpp:137] Memory required for data: 427828400
I1122 04:23:09.119920  5428 layer_factory.cpp:58] Creating layer scale4_2
I1122 04:23:09.119920  5428 net.cpp:84] Creating Layer scale4_2
I1122 04:23:09.119920  5428 net.cpp:406] scale4_2 <- conv4_2
I1122 04:23:09.119920  5428 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1122 04:23:09.119920  5428 layer_factory.cpp:58] Creating layer scale4_2
I1122 04:23:09.119920  5428 net.cpp:122] Setting up scale4_2
I1122 04:23:09.119920  5428 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1122 04:23:09.119920  5428 net.cpp:137] Memory required for data: 433767600
I1122 04:23:09.119920  5428 layer_factory.cpp:58] Creating layer relu4_2
I1122 04:23:09.119920  5428 net.cpp:84] Creating Layer relu4_2
I1122 04:23:09.119920  5428 net.cpp:406] relu4_2 <- conv4_2
I1122 04:23:09.119920  5428 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1122 04:23:09.120923  5428 net.cpp:122] Setting up relu4_2
I1122 04:23:09.120923  5428 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1122 04:23:09.120923  5428 net.cpp:137] Memory required for data: 439706800
I1122 04:23:09.120923  5428 layer_factory.cpp:58] Creating layer pool4_2
I1122 04:23:09.120923  5428 net.cpp:84] Creating Layer pool4_2
I1122 04:23:09.120923  5428 net.cpp:406] pool4_2 <- conv4_2
I1122 04:23:09.120923  5428 net.cpp:380] pool4_2 -> pool4_2
I1122 04:23:09.120923  5428 net.cpp:122] Setting up pool4_2
I1122 04:23:09.120923  5428 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1122 04:23:09.120923  5428 net.cpp:137] Memory required for data: 441191600
I1122 04:23:09.120923  5428 layer_factory.cpp:58] Creating layer conv4_0
I1122 04:23:09.120923  5428 net.cpp:84] Creating Layer conv4_0
I1122 04:23:09.120923  5428 net.cpp:406] conv4_0 <- pool4_2
I1122 04:23:09.120923  5428 net.cpp:380] conv4_0 -> conv4_0
I1122 04:23:09.121922  5428 net.cpp:122] Setting up conv4_0
I1122 04:23:09.121922  5428 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1122 04:23:09.121922  5428 net.cpp:137] Memory required for data: 442676400
I1122 04:23:09.121922  5428 layer_factory.cpp:58] Creating layer bn4_0
I1122 04:23:09.121922  5428 net.cpp:84] Creating Layer bn4_0
I1122 04:23:09.121922  5428 net.cpp:406] bn4_0 <- conv4_0
I1122 04:23:09.121922  5428 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1122 04:23:09.122920  5428 net.cpp:122] Setting up bn4_0
I1122 04:23:09.122920  5428 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1122 04:23:09.122920  5428 net.cpp:137] Memory required for data: 444161200
I1122 04:23:09.122920  5428 layer_factory.cpp:58] Creating layer scale4_0
I1122 04:23:09.122920  5428 net.cpp:84] Creating Layer scale4_0
I1122 04:23:09.122920  5428 net.cpp:406] scale4_0 <- conv4_0
I1122 04:23:09.122920  5428 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1122 04:23:09.122920  5428 layer_factory.cpp:58] Creating layer scale4_0
I1122 04:23:09.122920  5428 net.cpp:122] Setting up scale4_0
I1122 04:23:09.122920  5428 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1122 04:23:09.122920  5428 net.cpp:137] Memory required for data: 445646000
I1122 04:23:09.122920  5428 layer_factory.cpp:58] Creating layer relu4_0
I1122 04:23:09.122920  5428 net.cpp:84] Creating Layer relu4_0
I1122 04:23:09.122920  5428 net.cpp:406] relu4_0 <- conv4_0
I1122 04:23:09.122920  5428 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1122 04:23:09.122920  5428 net.cpp:122] Setting up relu4_0
I1122 04:23:09.122920  5428 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1122 04:23:09.122920  5428 net.cpp:137] Memory required for data: 447130800
I1122 04:23:09.122920  5428 layer_factory.cpp:58] Creating layer conv11
I1122 04:23:09.122920  5428 net.cpp:84] Creating Layer conv11
I1122 04:23:09.122920  5428 net.cpp:406] conv11 <- conv4_0
I1122 04:23:09.122920  5428 net.cpp:380] conv11 -> conv11
I1122 04:23:09.124933  5428 net.cpp:122] Setting up conv11
I1122 04:23:09.124933  5428 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1122 04:23:09.124933  5428 net.cpp:137] Memory required for data: 448922800
I1122 04:23:09.124933  5428 layer_factory.cpp:58] Creating layer bn_conv11
I1122 04:23:09.124933  5428 net.cpp:84] Creating Layer bn_conv11
I1122 04:23:09.124933  5428 net.cpp:406] bn_conv11 <- conv11
I1122 04:23:09.124933  5428 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1122 04:23:09.124933  5428 net.cpp:122] Setting up bn_conv11
I1122 04:23:09.124933  5428 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1122 04:23:09.124933  5428 net.cpp:137] Memory required for data: 450714800
I1122 04:23:09.124933  5428 layer_factory.cpp:58] Creating layer scale_conv11
I1122 04:23:09.124933  5428 net.cpp:84] Creating Layer scale_conv11
I1122 04:23:09.125922  5428 net.cpp:406] scale_conv11 <- conv11
I1122 04:23:09.125922  5428 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1122 04:23:09.125922  5428 layer_factory.cpp:58] Creating layer scale_conv11
I1122 04:23:09.125922  5428 net.cpp:122] Setting up scale_conv11
I1122 04:23:09.125922  5428 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1122 04:23:09.125922  5428 net.cpp:137] Memory required for data: 452506800
I1122 04:23:09.125922  5428 layer_factory.cpp:58] Creating layer relu_conv11
I1122 04:23:09.125922  5428 net.cpp:84] Creating Layer relu_conv11
I1122 04:23:09.125922  5428 net.cpp:406] relu_conv11 <- conv11
I1122 04:23:09.125922  5428 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1122 04:23:09.125922  5428 net.cpp:122] Setting up relu_conv11
I1122 04:23:09.125922  5428 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1122 04:23:09.125922  5428 net.cpp:137] Memory required for data: 454298800
I1122 04:23:09.125922  5428 layer_factory.cpp:58] Creating layer conv12
I1122 04:23:09.125922  5428 net.cpp:84] Creating Layer conv12
I1122 04:23:09.125922  5428 net.cpp:406] conv12 <- conv11
I1122 04:23:09.125922  5428 net.cpp:380] conv12 -> conv12
I1122 04:23:09.127923  5428 net.cpp:122] Setting up conv12
I1122 04:23:09.127923  5428 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 04:23:09.127923  5428 net.cpp:137] Memory required for data: 456602800
I1122 04:23:09.127923  5428 layer_factory.cpp:58] Creating layer bn_conv12
I1122 04:23:09.127923  5428 net.cpp:84] Creating Layer bn_conv12
I1122 04:23:09.127923  5428 net.cpp:406] bn_conv12 <- conv12
I1122 04:23:09.127923  5428 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1122 04:23:09.128922  5428 net.cpp:122] Setting up bn_conv12
I1122 04:23:09.128922  5428 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 04:23:09.128922  5428 net.cpp:137] Memory required for data: 458906800
I1122 04:23:09.128922  5428 layer_factory.cpp:58] Creating layer scale_conv12
I1122 04:23:09.128922  5428 net.cpp:84] Creating Layer scale_conv12
I1122 04:23:09.128922  5428 net.cpp:406] scale_conv12 <- conv12
I1122 04:23:09.128922  5428 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1122 04:23:09.128922  5428 layer_factory.cpp:58] Creating layer scale_conv12
I1122 04:23:09.128922  5428 net.cpp:122] Setting up scale_conv12
I1122 04:23:09.128922  5428 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 04:23:09.128922  5428 net.cpp:137] Memory required for data: 461210800
I1122 04:23:09.128922  5428 layer_factory.cpp:58] Creating layer relu_conv12
I1122 04:23:09.128922  5428 net.cpp:84] Creating Layer relu_conv12
I1122 04:23:09.128922  5428 net.cpp:406] relu_conv12 <- conv12
I1122 04:23:09.128922  5428 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1122 04:23:09.128922  5428 net.cpp:122] Setting up relu_conv12
I1122 04:23:09.128922  5428 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1122 04:23:09.128922  5428 net.cpp:137] Memory required for data: 463514800
I1122 04:23:09.128922  5428 layer_factory.cpp:58] Creating layer poolcp6
I1122 04:23:09.128922  5428 net.cpp:84] Creating Layer poolcp6
I1122 04:23:09.128922  5428 net.cpp:406] poolcp6 <- conv12
I1122 04:23:09.128922  5428 net.cpp:380] poolcp6 -> poolcp6
I1122 04:23:09.128922  5428 net.cpp:122] Setting up poolcp6
I1122 04:23:09.128922  5428 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1122 04:23:09.128922  5428 net.cpp:137] Memory required for data: 463550800
I1122 04:23:09.128922  5428 layer_factory.cpp:58] Creating layer ip1
I1122 04:23:09.128922  5428 net.cpp:84] Creating Layer ip1
I1122 04:23:09.128922  5428 net.cpp:406] ip1 <- poolcp6
I1122 04:23:09.128922  5428 net.cpp:380] ip1 -> ip1
I1122 04:23:09.129922  5428 net.cpp:122] Setting up ip1
I1122 04:23:09.129922  5428 net.cpp:129] Top shape: 100 10 (1000)
I1122 04:23:09.129922  5428 net.cpp:137] Memory required for data: 463554800
I1122 04:23:09.129922  5428 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1122 04:23:09.129922  5428 net.cpp:84] Creating Layer ip1_ip1_0_split
I1122 04:23:09.129922  5428 net.cpp:406] ip1_ip1_0_split <- ip1
I1122 04:23:09.129922  5428 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1122 04:23:09.129922  5428 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1122 04:23:09.129922  5428 net.cpp:122] Setting up ip1_ip1_0_split
I1122 04:23:09.129922  5428 net.cpp:129] Top shape: 100 10 (1000)
I1122 04:23:09.129922  5428 net.cpp:129] Top shape: 100 10 (1000)
I1122 04:23:09.129922  5428 net.cpp:137] Memory required for data: 463562800
I1122 04:23:09.129922  5428 layer_factory.cpp:58] Creating layer accuracy
I1122 04:23:09.129922  5428 net.cpp:84] Creating Layer accuracy
I1122 04:23:09.129922  5428 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1122 04:23:09.129922  5428 net.cpp:406] accuracy <- label_cifar_1_split_0
I1122 04:23:09.129922  5428 net.cpp:380] accuracy -> accuracy
I1122 04:23:09.129922  5428 net.cpp:122] Setting up accuracy
I1122 04:23:09.129922  5428 net.cpp:129] Top shape: (1)
I1122 04:23:09.129922  5428 net.cpp:137] Memory required for data: 463562804
I1122 04:23:09.129922  5428 layer_factory.cpp:58] Creating layer loss
I1122 04:23:09.129922  5428 net.cpp:84] Creating Layer loss
I1122 04:23:09.129922  5428 net.cpp:406] loss <- ip1_ip1_0_split_1
I1122 04:23:09.129922  5428 net.cpp:406] loss <- label_cifar_1_split_1
I1122 04:23:09.129922  5428 net.cpp:380] loss -> loss
I1122 04:23:09.129922  5428 layer_factory.cpp:58] Creating layer loss
I1122 04:23:09.129922  5428 net.cpp:122] Setting up loss
I1122 04:23:09.129922  5428 net.cpp:129] Top shape: (1)
I1122 04:23:09.129922  5428 net.cpp:132]     with loss weight 1
I1122 04:23:09.129922  5428 net.cpp:137] Memory required for data: 463562808
I1122 04:23:09.129922  5428 net.cpp:198] loss needs backward computation.
I1122 04:23:09.129922  5428 net.cpp:200] accuracy does not need backward computation.
I1122 04:23:09.129922  5428 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1122 04:23:09.129922  5428 net.cpp:198] ip1 needs backward computation.
I1122 04:23:09.129922  5428 net.cpp:198] poolcp6 needs backward computation.
I1122 04:23:09.129922  5428 net.cpp:198] relu_conv12 needs backward computation.
I1122 04:23:09.129922  5428 net.cpp:198] scale_conv12 needs backward computation.
I1122 04:23:09.129922  5428 net.cpp:198] bn_conv12 needs backward computation.
I1122 04:23:09.129922  5428 net.cpp:198] conv12 needs backward computation.
I1122 04:23:09.129922  5428 net.cpp:198] relu_conv11 needs backward computation.
I1122 04:23:09.129922  5428 net.cpp:198] scale_conv11 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] bn_conv11 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] conv11 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] relu4_0 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] scale4_0 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] bn4_0 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] conv4_0 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] pool4_2 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] relu4_2 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] scale4_2 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] bn4_2 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] conv4_2 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] relu4_1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] scale4_1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] bn4_1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] conv4_1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] relu4 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] scale4 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] bn4 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] conv4 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] relu3_1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] scale3_1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] bn3_1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] conv3_1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] relu3 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] scale3 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] bn3 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] conv3 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] pool2_1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] relu2_2 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] scale2_2 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] bn2_2 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] conv2_2 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] relu2_1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] scale2_1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] bn2_1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] conv2_1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] relu2 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] scale2 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] bn2 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] conv2 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] relu1_0 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] scale1_0 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] bn1_0 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] conv1_0 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] relu1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] scale1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] bn1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:198] conv1 needs backward computation.
I1122 04:23:09.130923  5428 net.cpp:200] label_cifar_1_split does not need backward computation.
I1122 04:23:09.130923  5428 net.cpp:200] cifar does not need backward computation.
I1122 04:23:09.130923  5428 net.cpp:242] This network produces output accuracy
I1122 04:23:09.130923  5428 net.cpp:242] This network produces output loss
I1122 04:23:09.130923  5428 net.cpp:255] Network initialization done.
I1122 04:23:09.130923  5428 solver.cpp:56] Solver scaffolding done.
I1122 04:23:09.134922  5428 caffe.cpp:249] Starting Optimization
I1122 04:23:09.135922  5428 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_13L_Simple_NoGrpCon_NoDrp_300k
I1122 04:23:09.135922  5428 solver.cpp:273] Learning Rate Policy: multistep
I1122 04:23:09.137922  5428 solver.cpp:330] Iteration 0, Testing net (#0)
I1122 04:23:09.139921  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:23:10.461022 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:23:10.511579  5428 solver.cpp:397]     Test net output #0: accuracy = 0.1
I1122 04:23:10.511579  5428 solver.cpp:397]     Test net output #1: loss = 78.6029 (* 1 = 78.6029 loss)
I1122 04:23:10.621891  5428 solver.cpp:218] Iteration 0 (-1.67576e-40 iter/s, 1.48512s/100 iters), loss = 5.30198
I1122 04:23:10.621891  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.04
I1122 04:23:10.621891  5428 solver.cpp:237]     Train net output #1: loss = 5.30198 (* 1 = 5.30198 loss)
I1122 04:23:10.621891  5428 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1122 04:23:15.886314  5428 solver.cpp:218] Iteration 100 (18.9969 iter/s, 5.26402s/100 iters), loss = 1.57484
I1122 04:23:15.886314  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.4
I1122 04:23:15.886314  5428 solver.cpp:237]     Train net output #1: loss = 1.57484 (* 1 = 1.57484 loss)
I1122 04:23:15.886314  5428 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1122 04:23:21.084928  5428 solver.cpp:218] Iteration 200 (19.237 iter/s, 5.19831s/100 iters), loss = 1.6997
I1122 04:23:21.085428  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.41
I1122 04:23:21.085428  5428 solver.cpp:237]     Train net output #1: loss = 1.6997 (* 1 = 1.6997 loss)
I1122 04:23:21.085428  5428 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1122 04:23:26.292971  5428 solver.cpp:218] Iteration 300 (19.2028 iter/s, 5.20756s/100 iters), loss = 1.37749
I1122 04:23:26.292971  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.5
I1122 04:23:26.292971  5428 solver.cpp:237]     Train net output #1: loss = 1.37749 (* 1 = 1.37749 loss)
I1122 04:23:26.292971  5428 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1122 04:23:31.494495  5428 solver.cpp:218] Iteration 400 (19.2248 iter/s, 5.2016s/100 iters), loss = 1.23272
I1122 04:23:31.494495  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.57
I1122 04:23:31.495497  5428 solver.cpp:237]     Train net output #1: loss = 1.23272 (* 1 = 1.23272 loss)
I1122 04:23:31.495497  5428 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1122 04:23:36.442595  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:23:36.647399  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_500.caffemodel
I1122 04:23:36.668965  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_500.solverstate
I1122 04:23:36.672966  5428 solver.cpp:330] Iteration 500, Testing net (#0)
I1122 04:23:36.672966  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:23:37.925029 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:23:37.973811  5428 solver.cpp:397]     Test net output #0: accuracy = 0.2536
I1122 04:23:37.973811  5428 solver.cpp:397]     Test net output #1: loss = 2.27466 (* 1 = 2.27466 loss)
I1122 04:23:38.023811  5428 solver.cpp:218] Iteration 500 (15.3181 iter/s, 6.52824s/100 iters), loss = 1.15423
I1122 04:23:38.023811  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.6
I1122 04:23:38.023811  5428 solver.cpp:237]     Train net output #1: loss = 1.15423 (* 1 = 1.15423 loss)
I1122 04:23:38.023811  5428 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1122 04:23:43.232991  5428 solver.cpp:218] Iteration 600 (19.1962 iter/s, 5.20937s/100 iters), loss = 1.04568
I1122 04:23:43.233992  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1122 04:23:43.233992  5428 solver.cpp:237]     Train net output #1: loss = 1.04568 (* 1 = 1.04568 loss)
I1122 04:23:43.233992  5428 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1122 04:23:48.442281  5428 solver.cpp:218] Iteration 700 (19.1989 iter/s, 5.20863s/100 iters), loss = 0.980206
I1122 04:23:48.442281  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1122 04:23:48.442281  5428 solver.cpp:237]     Train net output #1: loss = 0.980206 (* 1 = 0.980206 loss)
I1122 04:23:48.442281  5428 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1122 04:23:53.661273  5428 solver.cpp:218] Iteration 800 (19.1649 iter/s, 5.21787s/100 iters), loss = 0.913123
I1122 04:23:53.661273  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1122 04:23:53.661273  5428 solver.cpp:237]     Train net output #1: loss = 0.913123 (* 1 = 0.913123 loss)
I1122 04:23:53.661273  5428 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1122 04:23:58.889394  5428 solver.cpp:218] Iteration 900 (19.1288 iter/s, 5.22771s/100 iters), loss = 0.764497
I1122 04:23:58.889394  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1122 04:23:58.889394  5428 solver.cpp:237]     Train net output #1: loss = 0.764497 (* 1 = 0.764497 loss)
I1122 04:23:58.889394  5428 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1122 04:24:03.851723  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:24:04.057739  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_1000.caffemodel
I1122 04:24:04.071744  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_1000.solverstate
I1122 04:24:04.076745  5428 solver.cpp:330] Iteration 1000, Testing net (#0)
I1122 04:24:04.076745  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:24:05.329836 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:24:05.379842  5428 solver.cpp:397]     Test net output #0: accuracy = 0.3428
I1122 04:24:05.379842  5428 solver.cpp:397]     Test net output #1: loss = 1.89636 (* 1 = 1.89636 loss)
I1122 04:24:05.429841  5428 solver.cpp:218] Iteration 1000 (15.2893 iter/s, 6.54052s/100 iters), loss = 0.792624
I1122 04:24:05.429841  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1122 04:24:05.429841  5428 solver.cpp:237]     Train net output #1: loss = 0.792624 (* 1 = 0.792624 loss)
I1122 04:24:05.429841  5428 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1122 04:24:10.644172  5428 solver.cpp:218] Iteration 1100 (19.1789 iter/s, 5.21406s/100 iters), loss = 0.856642
I1122 04:24:10.644172  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1122 04:24:10.644172  5428 solver.cpp:237]     Train net output #1: loss = 0.856642 (* 1 = 0.856642 loss)
I1122 04:24:10.644172  5428 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1122 04:24:15.862499  5428 solver.cpp:218] Iteration 1200 (19.167 iter/s, 5.21731s/100 iters), loss = 0.810086
I1122 04:24:15.862499  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1122 04:24:15.862499  5428 solver.cpp:237]     Train net output #1: loss = 0.810086 (* 1 = 0.810086 loss)
I1122 04:24:15.862499  5428 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1122 04:24:21.092919  5428 solver.cpp:218] Iteration 1300 (19.1191 iter/s, 5.23038s/100 iters), loss = 0.821765
I1122 04:24:21.092919  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1122 04:24:21.092919  5428 solver.cpp:237]     Train net output #1: loss = 0.821765 (* 1 = 0.821765 loss)
I1122 04:24:21.092919  5428 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1122 04:24:26.396333  5428 solver.cpp:218] Iteration 1400 (18.8565 iter/s, 5.30322s/100 iters), loss = 0.75778
I1122 04:24:26.396333  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1122 04:24:26.396333  5428 solver.cpp:237]     Train net output #1: loss = 0.75778 (* 1 = 0.75778 loss)
I1122 04:24:26.396333  5428 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1122 04:24:31.376575  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:24:31.580586  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_1500.caffemodel
I1122 04:24:31.595093  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_1500.solverstate
I1122 04:24:31.600091  5428 solver.cpp:330] Iteration 1500, Testing net (#0)
I1122 04:24:31.600091  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:24:32.862742 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:24:32.913244  5428 solver.cpp:397]     Test net output #0: accuracy = 0.4087
I1122 04:24:32.913244  5428 solver.cpp:397]     Test net output #1: loss = 1.79961 (* 1 = 1.79961 loss)
I1122 04:24:32.962741  5428 solver.cpp:218] Iteration 1500 (15.2309 iter/s, 6.56559s/100 iters), loss = 0.808491
I1122 04:24:32.962741  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1122 04:24:32.962741  5428 solver.cpp:237]     Train net output #1: loss = 0.808491 (* 1 = 0.808491 loss)
I1122 04:24:32.962741  5428 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1122 04:24:38.197650  5428 solver.cpp:218] Iteration 1600 (19.1031 iter/s, 5.23476s/100 iters), loss = 0.77901
I1122 04:24:38.197650  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1122 04:24:38.197650  5428 solver.cpp:237]     Train net output #1: loss = 0.77901 (* 1 = 0.77901 loss)
I1122 04:24:38.197650  5428 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1122 04:24:43.476027  5428 solver.cpp:218] Iteration 1700 (18.9458 iter/s, 5.2782s/100 iters), loss = 0.715565
I1122 04:24:43.476027  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1122 04:24:43.476027  5428 solver.cpp:237]     Train net output #1: loss = 0.715565 (* 1 = 0.715565 loss)
I1122 04:24:43.476027  5428 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1122 04:24:48.734380  5428 solver.cpp:218] Iteration 1800 (19.0213 iter/s, 5.25726s/100 iters), loss = 0.74301
I1122 04:24:48.734380  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1122 04:24:48.734380  5428 solver.cpp:237]     Train net output #1: loss = 0.74301 (* 1 = 0.74301 loss)
I1122 04:24:48.734380  5428 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1122 04:24:54.048432  5428 solver.cpp:218] Iteration 1900 (18.8188 iter/s, 5.31385s/100 iters), loss = 0.593306
I1122 04:24:54.048432  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 04:24:54.048432  5428 solver.cpp:237]     Train net output #1: loss = 0.593306 (* 1 = 0.593306 loss)
I1122 04:24:54.048432  5428 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1122 04:24:59.037286  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:24:59.244312  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_2000.caffemodel
I1122 04:24:59.258316  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_2000.solverstate
I1122 04:24:59.263317  5428 solver.cpp:330] Iteration 2000, Testing net (#0)
I1122 04:24:59.263317  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:25:00.543419 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:25:00.592422  5428 solver.cpp:397]     Test net output #0: accuracy = 0.4986
I1122 04:25:00.592422  5428 solver.cpp:397]     Test net output #1: loss = 1.43894 (* 1 = 1.43894 loss)
I1122 04:25:00.643424  5428 solver.cpp:218] Iteration 2000 (15.1646 iter/s, 6.59429s/100 iters), loss = 0.625233
I1122 04:25:00.643424  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1122 04:25:00.643424  5428 solver.cpp:237]     Train net output #1: loss = 0.625233 (* 1 = 0.625233 loss)
I1122 04:25:00.643424  5428 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1122 04:25:05.924831  5428 solver.cpp:218] Iteration 2100 (18.934 iter/s, 5.28151s/100 iters), loss = 0.597807
I1122 04:25:05.924831  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 04:25:05.924831  5428 solver.cpp:237]     Train net output #1: loss = 0.597807 (* 1 = 0.597807 loss)
I1122 04:25:05.924831  5428 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1122 04:25:11.230305  5428 solver.cpp:218] Iteration 2200 (18.8508 iter/s, 5.30482s/100 iters), loss = 0.583815
I1122 04:25:11.230305  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1122 04:25:11.230305  5428 solver.cpp:237]     Train net output #1: loss = 0.583815 (* 1 = 0.583815 loss)
I1122 04:25:11.230305  5428 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1122 04:25:16.468727  5428 solver.cpp:218] Iteration 2300 (19.0915 iter/s, 5.23793s/100 iters), loss = 0.638879
I1122 04:25:16.468727  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 04:25:16.468727  5428 solver.cpp:237]     Train net output #1: loss = 0.638879 (* 1 = 0.638879 loss)
I1122 04:25:16.468727  5428 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1122 04:25:21.750217  5428 solver.cpp:218] Iteration 2400 (18.9367 iter/s, 5.28074s/100 iters), loss = 0.601956
I1122 04:25:21.750217  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1122 04:25:21.750217  5428 solver.cpp:237]     Train net output #1: loss = 0.601956 (* 1 = 0.601956 loss)
I1122 04:25:21.750217  5428 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1122 04:25:26.828732  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:25:27.034747  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_2500.caffemodel
I1122 04:25:27.049748  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_2500.solverstate
I1122 04:25:27.054749  5428 solver.cpp:330] Iteration 2500, Testing net (#0)
I1122 04:25:27.054749  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:25:28.338889 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:25:28.389889  5428 solver.cpp:397]     Test net output #0: accuracy = 0.6249
I1122 04:25:28.389889  5428 solver.cpp:397]     Test net output #1: loss = 1.05862 (* 1 = 1.05862 loss)
I1122 04:25:28.441920  5428 solver.cpp:218] Iteration 2500 (14.944 iter/s, 6.69165s/100 iters), loss = 0.647272
I1122 04:25:28.441920  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1122 04:25:28.441920  5428 solver.cpp:237]     Train net output #1: loss = 0.647272 (* 1 = 0.647272 loss)
I1122 04:25:28.441920  5428 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1122 04:25:33.714843  5428 solver.cpp:218] Iteration 2600 (18.9688 iter/s, 5.27181s/100 iters), loss = 0.472876
I1122 04:25:33.714843  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 04:25:33.714843  5428 solver.cpp:237]     Train net output #1: loss = 0.472876 (* 1 = 0.472876 loss)
I1122 04:25:33.714843  5428 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1122 04:25:39.014662  5428 solver.cpp:218] Iteration 2700 (18.8701 iter/s, 5.29939s/100 iters), loss = 0.578058
I1122 04:25:39.014662  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 04:25:39.014662  5428 solver.cpp:237]     Train net output #1: loss = 0.578058 (* 1 = 0.578058 loss)
I1122 04:25:39.014662  5428 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1122 04:25:44.319057  5428 solver.cpp:218] Iteration 2800 (18.8524 iter/s, 5.30436s/100 iters), loss = 0.506885
I1122 04:25:44.319557  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 04:25:44.319557  5428 solver.cpp:237]     Train net output #1: loss = 0.506885 (* 1 = 0.506885 loss)
I1122 04:25:44.319557  5428 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1122 04:25:49.578999  5428 solver.cpp:218] Iteration 2900 (19.0138 iter/s, 5.25934s/100 iters), loss = 0.580873
I1122 04:25:49.578999  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1122 04:25:49.578999  5428 solver.cpp:237]     Train net output #1: loss = 0.580873 (* 1 = 0.580873 loss)
I1122 04:25:49.578999  5428 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1122 04:25:54.711009  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:25:54.925617  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_3000.caffemodel
I1122 04:25:54.940134  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_3000.solverstate
I1122 04:25:54.944133  5428 solver.cpp:330] Iteration 3000, Testing net (#0)
I1122 04:25:54.944133  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:25:56.221702 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:25:56.271745  5428 solver.cpp:397]     Test net output #0: accuracy = 0.656
I1122 04:25:56.271745  5428 solver.cpp:397]     Test net output #1: loss = 1.03173 (* 1 = 1.03173 loss)
I1122 04:25:56.322264  5428 solver.cpp:218] Iteration 3000 (14.8309 iter/s, 6.74268s/100 iters), loss = 0.569462
I1122 04:25:56.322264  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 04:25:56.322264  5428 solver.cpp:237]     Train net output #1: loss = 0.569462 (* 1 = 0.569462 loss)
I1122 04:25:56.322264  5428 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1122 04:26:01.696350  5428 solver.cpp:218] Iteration 3100 (18.6095 iter/s, 5.3736s/100 iters), loss = 0.501142
I1122 04:26:01.696350  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 04:26:01.696350  5428 solver.cpp:237]     Train net output #1: loss = 0.501142 (* 1 = 0.501142 loss)
I1122 04:26:01.696350  5428 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1122 04:26:07.052788  5428 solver.cpp:218] Iteration 3200 (18.6705 iter/s, 5.35606s/100 iters), loss = 0.648073
I1122 04:26:07.052788  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1122 04:26:07.052788  5428 solver.cpp:237]     Train net output #1: loss = 0.648073 (* 1 = 0.648073 loss)
I1122 04:26:07.052788  5428 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1122 04:26:12.405215  5428 solver.cpp:218] Iteration 3300 (18.6856 iter/s, 5.3517s/100 iters), loss = 0.569095
I1122 04:26:12.405215  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 04:26:12.405215  5428 solver.cpp:237]     Train net output #1: loss = 0.569095 (* 1 = 0.569095 loss)
I1122 04:26:12.405215  5428 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1122 04:26:17.764631  5428 solver.cpp:218] Iteration 3400 (18.6574 iter/s, 5.3598s/100 iters), loss = 0.525633
I1122 04:26:17.764631  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1122 04:26:17.764631  5428 solver.cpp:237]     Train net output #1: loss = 0.525633 (* 1 = 0.525633 loss)
I1122 04:26:17.764631  5428 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1122 04:26:22.790202  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:26:22.996214  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_3500.caffemodel
I1122 04:26:23.011214  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_3500.solverstate
I1122 04:26:23.015214  5428 solver.cpp:330] Iteration 3500, Testing net (#0)
I1122 04:26:23.015214  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:26:24.275326 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:26:24.326833  5428 solver.cpp:397]     Test net output #0: accuracy = 0.6036
I1122 04:26:24.326833  5428 solver.cpp:397]     Test net output #1: loss = 1.16178 (* 1 = 1.16178 loss)
I1122 04:26:24.377333  5428 solver.cpp:218] Iteration 3500 (15.1254 iter/s, 6.61138s/100 iters), loss = 0.630384
I1122 04:26:24.377333  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1122 04:26:24.377333  5428 solver.cpp:237]     Train net output #1: loss = 0.630384 (* 1 = 0.630384 loss)
I1122 04:26:24.377333  5428 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1122 04:26:29.659790  5428 solver.cpp:218] Iteration 3600 (18.9304 iter/s, 5.2825s/100 iters), loss = 0.509287
I1122 04:26:29.659790  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 04:26:29.659790  5428 solver.cpp:237]     Train net output #1: loss = 0.509287 (* 1 = 0.509287 loss)
I1122 04:26:29.659790  5428 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1122 04:26:34.916152  5428 solver.cpp:218] Iteration 3700 (19.0271 iter/s, 5.25566s/100 iters), loss = 0.665954
I1122 04:26:34.916152  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1122 04:26:34.916152  5428 solver.cpp:237]     Train net output #1: loss = 0.665954 (* 1 = 0.665954 loss)
I1122 04:26:34.916152  5428 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1122 04:26:40.152536  5428 solver.cpp:218] Iteration 3800 (19.098 iter/s, 5.23616s/100 iters), loss = 0.526463
I1122 04:26:40.152536  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 04:26:40.152536  5428 solver.cpp:237]     Train net output #1: loss = 0.526463 (* 1 = 0.526463 loss)
I1122 04:26:40.152536  5428 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1122 04:26:45.391896  5428 solver.cpp:218] Iteration 3900 (19.086 iter/s, 5.23946s/100 iters), loss = 0.52152
I1122 04:26:45.391896  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 04:26:45.391896  5428 solver.cpp:237]     Train net output #1: loss = 0.52152 (* 1 = 0.52152 loss)
I1122 04:26:45.392896  5428 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1122 04:26:50.372079  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:26:50.579157  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_4000.caffemodel
I1122 04:26:50.595139  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_4000.solverstate
I1122 04:26:50.599139  5428 solver.cpp:330] Iteration 4000, Testing net (#0)
I1122 04:26:50.599139  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:26:51.865514 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:26:51.914513  5428 solver.cpp:397]     Test net output #0: accuracy = 0.6925
I1122 04:26:51.914513  5428 solver.cpp:397]     Test net output #1: loss = 0.884669 (* 1 = 0.884669 loss)
I1122 04:26:51.964556  5428 solver.cpp:218] Iteration 4000 (15.2178 iter/s, 6.57125s/100 iters), loss = 0.617114
I1122 04:26:51.964556  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1122 04:26:51.964556  5428 solver.cpp:237]     Train net output #1: loss = 0.617114 (* 1 = 0.617114 loss)
I1122 04:26:51.964556  5428 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1122 04:26:57.243777  5428 solver.cpp:218] Iteration 4100 (18.9426 iter/s, 5.2791s/100 iters), loss = 0.477342
I1122 04:26:57.243777  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1122 04:26:57.243777  5428 solver.cpp:237]     Train net output #1: loss = 0.477342 (* 1 = 0.477342 loss)
I1122 04:26:57.243777  5428 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1122 04:27:02.494127  5428 solver.cpp:218] Iteration 4200 (19.0464 iter/s, 5.25033s/100 iters), loss = 0.521967
I1122 04:27:02.494127  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 04:27:02.494127  5428 solver.cpp:237]     Train net output #1: loss = 0.521967 (* 1 = 0.521967 loss)
I1122 04:27:02.494127  5428 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1122 04:27:07.743657  5428 solver.cpp:218] Iteration 4300 (19.0529 iter/s, 5.24854s/100 iters), loss = 0.466593
I1122 04:27:07.743657  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1122 04:27:07.743657  5428 solver.cpp:237]     Train net output #1: loss = 0.466593 (* 1 = 0.466593 loss)
I1122 04:27:07.743657  5428 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1122 04:27:12.996212  5428 solver.cpp:218] Iteration 4400 (19.0378 iter/s, 5.2527s/100 iters), loss = 0.506071
I1122 04:27:12.996212  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1122 04:27:12.996212  5428 solver.cpp:237]     Train net output #1: loss = 0.506071 (* 1 = 0.506071 loss)
I1122 04:27:12.996212  5428 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1122 04:27:17.987565  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:27:18.194579  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_4500.caffemodel
I1122 04:27:18.208580  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_4500.solverstate
I1122 04:27:18.212579  5428 solver.cpp:330] Iteration 4500, Testing net (#0)
I1122 04:27:18.212579  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:27:19.472666 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:27:19.522665  5428 solver.cpp:397]     Test net output #0: accuracy = 0.6542
I1122 04:27:19.522665  5428 solver.cpp:397]     Test net output #1: loss = 1.01566 (* 1 = 1.01566 loss)
I1122 04:27:19.571671  5428 solver.cpp:218] Iteration 4500 (15.2091 iter/s, 6.57502s/100 iters), loss = 0.517468
I1122 04:27:19.571671  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 04:27:19.571671  5428 solver.cpp:237]     Train net output #1: loss = 0.517468 (* 1 = 0.517468 loss)
I1122 04:27:19.571671  5428 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1122 04:27:24.795039  5428 solver.cpp:218] Iteration 4600 (19.1476 iter/s, 5.22259s/100 iters), loss = 0.387666
I1122 04:27:24.795039  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 04:27:24.795039  5428 solver.cpp:237]     Train net output #1: loss = 0.387666 (* 1 = 0.387666 loss)
I1122 04:27:24.795039  5428 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1122 04:27:30.020401  5428 solver.cpp:218] Iteration 4700 (19.1386 iter/s, 5.22504s/100 iters), loss = 0.493835
I1122 04:27:30.020401  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1122 04:27:30.020401  5428 solver.cpp:237]     Train net output #1: loss = 0.493835 (* 1 = 0.493835 loss)
I1122 04:27:30.020401  5428 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1122 04:27:35.245357  5428 solver.cpp:218] Iteration 4800 (19.1411 iter/s, 5.22436s/100 iters), loss = 0.525917
I1122 04:27:35.245357  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1122 04:27:35.245357  5428 solver.cpp:237]     Train net output #1: loss = 0.525917 (* 1 = 0.525917 loss)
I1122 04:27:35.245357  5428 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1122 04:27:40.472491  5428 solver.cpp:218] Iteration 4900 (19.1332 iter/s, 5.22653s/100 iters), loss = 0.558423
I1122 04:27:40.472491  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1122 04:27:40.472491  5428 solver.cpp:237]     Train net output #1: loss = 0.558423 (* 1 = 0.558423 loss)
I1122 04:27:40.472491  5428 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1122 04:27:45.443819  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:27:45.649832  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_5000.caffemodel
I1122 04:27:45.663836  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_5000.solverstate
I1122 04:27:45.667835  5428 solver.cpp:330] Iteration 5000, Testing net (#0)
I1122 04:27:45.667835  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:27:46.925917 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:27:46.975922  5428 solver.cpp:397]     Test net output #0: accuracy = 0.7344
I1122 04:27:46.975922  5428 solver.cpp:397]     Test net output #1: loss = 0.785949 (* 1 = 0.785949 loss)
I1122 04:27:47.025921  5428 solver.cpp:218] Iteration 5000 (15.2598 iter/s, 6.55317s/100 iters), loss = 0.516423
I1122 04:27:47.025921  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1122 04:27:47.025921  5428 solver.cpp:237]     Train net output #1: loss = 0.516423 (* 1 = 0.516423 loss)
I1122 04:27:47.025921  5428 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1122 04:27:47.025921  5428 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1122 04:27:52.259282  5428 solver.cpp:218] Iteration 5100 (19.1088 iter/s, 5.23319s/100 iters), loss = 0.30475
I1122 04:27:52.259282  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 04:27:52.259282  5428 solver.cpp:237]     Train net output #1: loss = 0.30475 (* 1 = 0.30475 loss)
I1122 04:27:52.259282  5428 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1122 04:27:57.492658  5428 solver.cpp:218] Iteration 5200 (19.1101 iter/s, 5.23282s/100 iters), loss = 0.351211
I1122 04:27:57.492658  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1122 04:27:57.492658  5428 solver.cpp:237]     Train net output #1: loss = 0.351211 (* 1 = 0.351211 loss)
I1122 04:27:57.492658  5428 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1122 04:28:02.720981  5428 solver.cpp:218] Iteration 5300 (19.1279 iter/s, 5.22796s/100 iters), loss = 0.313032
I1122 04:28:02.720981  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1122 04:28:02.720981  5428 solver.cpp:237]     Train net output #1: loss = 0.313032 (* 1 = 0.313032 loss)
I1122 04:28:02.720981  5428 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1122 04:28:07.953330  5428 solver.cpp:218] Iteration 5400 (19.1135 iter/s, 5.2319s/100 iters), loss = 0.295541
I1122 04:28:07.953830  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1122 04:28:07.953830  5428 solver.cpp:237]     Train net output #1: loss = 0.295541 (* 1 = 0.295541 loss)
I1122 04:28:07.953830  5428 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1122 04:28:12.929656  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:28:13.136667  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_5500.caffemodel
I1122 04:28:13.151170  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_5500.solverstate
I1122 04:28:13.155671  5428 solver.cpp:330] Iteration 5500, Testing net (#0)
I1122 04:28:13.155671  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:28:14.417768 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:28:14.466771  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8623
I1122 04:28:14.466771  5428 solver.cpp:397]     Test net output #1: loss = 0.403828 (* 1 = 0.403828 loss)
I1122 04:28:14.516772  5428 solver.cpp:218] Iteration 5500 (15.2368 iter/s, 6.56305s/100 iters), loss = 0.257002
I1122 04:28:14.516772  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:28:14.516772  5428 solver.cpp:237]     Train net output #1: loss = 0.257002 (* 1 = 0.257002 loss)
I1122 04:28:14.516772  5428 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1122 04:28:19.743775  5428 solver.cpp:218] Iteration 5600 (19.1339 iter/s, 5.22633s/100 iters), loss = 0.294993
I1122 04:28:19.743775  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1122 04:28:19.743775  5428 solver.cpp:237]     Train net output #1: loss = 0.294993 (* 1 = 0.294993 loss)
I1122 04:28:19.743775  5428 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1122 04:28:24.972638  5428 solver.cpp:218] Iteration 5700 (19.1259 iter/s, 5.2285s/100 iters), loss = 0.342127
I1122 04:28:24.972638  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1122 04:28:24.972638  5428 solver.cpp:237]     Train net output #1: loss = 0.342127 (* 1 = 0.342127 loss)
I1122 04:28:24.972638  5428 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1122 04:28:30.200145  5428 solver.cpp:218] Iteration 5800 (19.1318 iter/s, 5.2269s/100 iters), loss = 0.308485
I1122 04:28:30.200145  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 04:28:30.200145  5428 solver.cpp:237]     Train net output #1: loss = 0.308485 (* 1 = 0.308485 loss)
I1122 04:28:30.200145  5428 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1122 04:28:35.421501  5428 solver.cpp:218] Iteration 5900 (19.1509 iter/s, 5.22168s/100 iters), loss = 0.214979
I1122 04:28:35.421501  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:28:35.421501  5428 solver.cpp:237]     Train net output #1: loss = 0.214979 (* 1 = 0.214979 loss)
I1122 04:28:35.421501  5428 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1122 04:28:40.382822  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:28:40.587834  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_6000.caffemodel
I1122 04:28:40.602834  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_6000.solverstate
I1122 04:28:40.606834  5428 solver.cpp:330] Iteration 6000, Testing net (#0)
I1122 04:28:40.606834  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:28:41.865923 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:28:41.914922  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8601
I1122 04:28:41.914922  5428 solver.cpp:397]     Test net output #1: loss = 0.400048 (* 1 = 0.400048 loss)
I1122 04:28:41.964927  5428 solver.cpp:218] Iteration 6000 (15.2837 iter/s, 6.54293s/100 iters), loss = 0.2698
I1122 04:28:41.964927  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 04:28:41.964927  5428 solver.cpp:237]     Train net output #1: loss = 0.2698 (* 1 = 0.2698 loss)
I1122 04:28:41.964927  5428 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1122 04:28:47.200299  5428 solver.cpp:218] Iteration 6100 (19.1026 iter/s, 5.2349s/100 iters), loss = 0.346815
I1122 04:28:47.200299  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1122 04:28:47.200299  5428 solver.cpp:237]     Train net output #1: loss = 0.346815 (* 1 = 0.346815 loss)
I1122 04:28:47.200299  5428 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1122 04:28:52.438684  5428 solver.cpp:218] Iteration 6200 (19.0929 iter/s, 5.23755s/100 iters), loss = 0.268157
I1122 04:28:52.438684  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 04:28:52.438684  5428 solver.cpp:237]     Train net output #1: loss = 0.268157 (* 1 = 0.268157 loss)
I1122 04:28:52.438684  5428 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1122 04:28:57.680204  5428 solver.cpp:218] Iteration 6300 (19.0808 iter/s, 5.24086s/100 iters), loss = 0.314628
I1122 04:28:57.680204  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1122 04:28:57.680204  5428 solver.cpp:237]     Train net output #1: loss = 0.314627 (* 1 = 0.314627 loss)
I1122 04:28:57.680204  5428 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1122 04:29:02.929198  5428 solver.cpp:218] Iteration 6400 (19.0516 iter/s, 5.2489s/100 iters), loss = 0.234589
I1122 04:29:02.929198  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:29:02.929198  5428 solver.cpp:237]     Train net output #1: loss = 0.234589 (* 1 = 0.234589 loss)
I1122 04:29:02.929198  5428 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1122 04:29:07.923586  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:29:08.130077  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_6500.caffemodel
I1122 04:29:08.145081  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_6500.solverstate
I1122 04:29:08.149569  5428 solver.cpp:330] Iteration 6500, Testing net (#0)
I1122 04:29:08.149569  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:29:09.409226 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:29:09.459748  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8686
I1122 04:29:09.459748  5428 solver.cpp:397]     Test net output #1: loss = 0.38345 (* 1 = 0.38345 loss)
I1122 04:29:09.509776  5428 solver.cpp:218] Iteration 6500 (15.1976 iter/s, 6.58s/100 iters), loss = 0.201191
I1122 04:29:09.509776  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:29:09.509776  5428 solver.cpp:237]     Train net output #1: loss = 0.201191 (* 1 = 0.201191 loss)
I1122 04:29:09.509776  5428 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1122 04:29:14.762048  5428 solver.cpp:218] Iteration 6600 (19.0412 iter/s, 5.25178s/100 iters), loss = 0.258097
I1122 04:29:14.762048  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:29:14.762048  5428 solver.cpp:237]     Train net output #1: loss = 0.258097 (* 1 = 0.258097 loss)
I1122 04:29:14.762048  5428 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1122 04:29:20.003020  5428 solver.cpp:218] Iteration 6700 (19.0797 iter/s, 5.24118s/100 iters), loss = 0.248338
I1122 04:29:20.003020  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:29:20.003020  5428 solver.cpp:237]     Train net output #1: loss = 0.248338 (* 1 = 0.248338 loss)
I1122 04:29:20.003020  5428 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1122 04:29:25.241067  5428 solver.cpp:218] Iteration 6800 (19.0929 iter/s, 5.23755s/100 iters), loss = 0.274133
I1122 04:29:25.241067  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 04:29:25.241067  5428 solver.cpp:237]     Train net output #1: loss = 0.274133 (* 1 = 0.274133 loss)
I1122 04:29:25.241067  5428 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1122 04:29:30.464571  5428 solver.cpp:218] Iteration 6900 (19.1483 iter/s, 5.2224s/100 iters), loss = 0.204482
I1122 04:29:30.464571  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:29:30.464571  5428 solver.cpp:237]     Train net output #1: loss = 0.204482 (* 1 = 0.204482 loss)
I1122 04:29:30.464571  5428 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1122 04:29:35.434296  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:29:35.638813  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_7000.caffemodel
I1122 04:29:35.653815  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_7000.solverstate
I1122 04:29:35.657816  5428 solver.cpp:330] Iteration 7000, Testing net (#0)
I1122 04:29:35.657816  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:29:36.916314 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:29:36.966313  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8757
I1122 04:29:36.966313  5428 solver.cpp:397]     Test net output #1: loss = 0.365586 (* 1 = 0.365586 loss)
I1122 04:29:37.015849  5428 solver.cpp:218] Iteration 7000 (15.2638 iter/s, 6.55147s/100 iters), loss = 0.210081
I1122 04:29:37.015849  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:29:37.015849  5428 solver.cpp:237]     Train net output #1: loss = 0.210081 (* 1 = 0.210081 loss)
I1122 04:29:37.015849  5428 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1122 04:29:42.253648  5428 solver.cpp:218] Iteration 7100 (19.0953 iter/s, 5.2369s/100 iters), loss = 0.243709
I1122 04:29:42.253648  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 04:29:42.253648  5428 solver.cpp:237]     Train net output #1: loss = 0.243709 (* 1 = 0.243709 loss)
I1122 04:29:42.253648  5428 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1122 04:29:47.488822  5428 solver.cpp:218] Iteration 7200 (19.1035 iter/s, 5.23464s/100 iters), loss = 0.251764
I1122 04:29:47.488822  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 04:29:47.488822  5428 solver.cpp:237]     Train net output #1: loss = 0.251764 (* 1 = 0.251764 loss)
I1122 04:29:47.488822  5428 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1122 04:29:52.728554  5428 solver.cpp:218] Iteration 7300 (19.0866 iter/s, 5.23927s/100 iters), loss = 0.195322
I1122 04:29:52.728554  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:29:52.728554  5428 solver.cpp:237]     Train net output #1: loss = 0.195322 (* 1 = 0.195322 loss)
I1122 04:29:52.728554  5428 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1122 04:29:57.966137  5428 solver.cpp:218] Iteration 7400 (19.094 iter/s, 5.23725s/100 iters), loss = 0.20345
I1122 04:29:57.966137  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:29:57.966137  5428 solver.cpp:237]     Train net output #1: loss = 0.20345 (* 1 = 0.20345 loss)
I1122 04:29:57.966137  5428 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1122 04:30:02.981554  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:30:03.188091  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_7500.caffemodel
I1122 04:30:03.203092  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_7500.solverstate
I1122 04:30:03.207092  5428 solver.cpp:330] Iteration 7500, Testing net (#0)
I1122 04:30:03.207092  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:30:04.467316 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:30:04.517315  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8743
I1122 04:30:04.517315  5428 solver.cpp:397]     Test net output #1: loss = 0.361318 (* 1 = 0.361318 loss)
I1122 04:30:04.566823  5428 solver.cpp:218] Iteration 7500 (15.149 iter/s, 6.60109s/100 iters), loss = 0.2317
I1122 04:30:04.567869  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:30:04.567869  5428 solver.cpp:237]     Train net output #1: loss = 0.2317 (* 1 = 0.2317 loss)
I1122 04:30:04.567869  5428 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1122 04:30:09.806648  5428 solver.cpp:218] Iteration 7600 (19.0885 iter/s, 5.23875s/100 iters), loss = 0.234021
I1122 04:30:09.806648  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:30:09.806648  5428 solver.cpp:237]     Train net output #1: loss = 0.234021 (* 1 = 0.234021 loss)
I1122 04:30:09.806648  5428 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1122 04:30:15.039520  5428 solver.cpp:218] Iteration 7700 (19.1099 iter/s, 5.2329s/100 iters), loss = 0.279162
I1122 04:30:15.039520  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 04:30:15.039520  5428 solver.cpp:237]     Train net output #1: loss = 0.279162 (* 1 = 0.279162 loss)
I1122 04:30:15.039520  5428 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1122 04:30:20.269448  5428 solver.cpp:218] Iteration 7800 (19.1246 iter/s, 5.22887s/100 iters), loss = 0.277172
I1122 04:30:20.269448  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:30:20.269448  5428 solver.cpp:237]     Train net output #1: loss = 0.277172 (* 1 = 0.277172 loss)
I1122 04:30:20.269448  5428 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1122 04:30:25.502854  5428 solver.cpp:218] Iteration 7900 (19.1097 iter/s, 5.23295s/100 iters), loss = 0.15401
I1122 04:30:25.502854  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:30:25.502854  5428 solver.cpp:237]     Train net output #1: loss = 0.154009 (* 1 = 0.154009 loss)
I1122 04:30:25.502854  5428 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1122 04:30:30.476673  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:30:30.682188  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_8000.caffemodel
I1122 04:30:30.696187  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_8000.solverstate
I1122 04:30:30.700187  5428 solver.cpp:330] Iteration 8000, Testing net (#0)
I1122 04:30:30.700187  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:30:31.958292 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:30:32.008298  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8761
I1122 04:30:32.008298  5428 solver.cpp:397]     Test net output #1: loss = 0.358714 (* 1 = 0.358714 loss)
I1122 04:30:32.057298  5428 solver.cpp:218] Iteration 8000 (15.2559 iter/s, 6.55485s/100 iters), loss = 0.211547
I1122 04:30:32.058300  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:30:32.058300  5428 solver.cpp:237]     Train net output #1: loss = 0.211547 (* 1 = 0.211547 loss)
I1122 04:30:32.058300  5428 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1122 04:30:37.304786  5428 solver.cpp:218] Iteration 8100 (19.0616 iter/s, 5.24614s/100 iters), loss = 0.278077
I1122 04:30:37.304786  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:30:37.304786  5428 solver.cpp:237]     Train net output #1: loss = 0.278077 (* 1 = 0.278077 loss)
I1122 04:30:37.304786  5428 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1122 04:30:42.565448  5428 solver.cpp:218] Iteration 8200 (19.0107 iter/s, 5.26019s/100 iters), loss = 0.232611
I1122 04:30:42.565448  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:30:42.565448  5428 solver.cpp:237]     Train net output #1: loss = 0.232611 (* 1 = 0.232611 loss)
I1122 04:30:42.565448  5428 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1122 04:30:47.824895  5428 solver.cpp:218] Iteration 8300 (19.0137 iter/s, 5.25937s/100 iters), loss = 0.220624
I1122 04:30:47.824895  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:30:47.824895  5428 solver.cpp:237]     Train net output #1: loss = 0.220624 (* 1 = 0.220624 loss)
I1122 04:30:47.824895  5428 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1122 04:30:50.137568  5428 solver.cpp:218] Iteration 8400 (18.9751 iter/s, 5.27005s/100 iters), loss = 0.204611
I1122 04:30:50.137568  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:30:50.137568  5428 solver.cpp:237]     Train net output #1: loss = 0.204611 (* 1 = 0.204611 loss)
I1122 04:30:50.137568  5428 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1122 04:30:55.150012  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:30:55.358041  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_8500.caffemodel
I1122 04:30:55.379036  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_8500.solverstate
I1122 04:30:55.384037  5428 solver.cpp:330] Iteration 8500, Testing net (#0)
I1122 04:30:55.384037  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:30:56.646155 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:30:56.696154  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8791
I1122 04:30:56.696154  5428 solver.cpp:397]     Test net output #1: loss = 0.348216 (* 1 = 0.348216 loss)
I1122 04:30:56.746165  5428 solver.cpp:218] Iteration 8500 (15.1325 iter/s, 6.60828s/100 iters), loss = 0.159603
I1122 04:30:56.746165  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:30:56.746165  5428 solver.cpp:237]     Train net output #1: loss = 0.159603 (* 1 = 0.159603 loss)
I1122 04:30:56.746165  5428 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1122 04:31:02.002764  5428 solver.cpp:218] Iteration 8600 (19.0273 iter/s, 5.2556s/100 iters), loss = 0.181002
I1122 04:31:02.002764  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:31:02.002764  5428 solver.cpp:237]     Train net output #1: loss = 0.181002 (* 1 = 0.181002 loss)
I1122 04:31:02.002764  5428 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1122 04:31:07.257393  5428 solver.cpp:218] Iteration 8700 (19.0324 iter/s, 5.25419s/100 iters), loss = 0.262588
I1122 04:31:07.257393  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 04:31:07.257393  5428 solver.cpp:237]     Train net output #1: loss = 0.262588 (* 1 = 0.262588 loss)
I1122 04:31:07.257393  5428 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1122 04:31:12.514027  5428 solver.cpp:218] Iteration 8800 (19.0249 iter/s, 5.25628s/100 iters), loss = 0.210838
I1122 04:31:12.514027  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:31:12.514027  5428 solver.cpp:237]     Train net output #1: loss = 0.210838 (* 1 = 0.210838 loss)
I1122 04:31:12.514027  5428 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1122 04:31:17.784790  5428 solver.cpp:218] Iteration 8900 (18.9728 iter/s, 5.27069s/100 iters), loss = 0.151513
I1122 04:31:17.784790  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:31:17.784790  5428 solver.cpp:237]     Train net output #1: loss = 0.151513 (* 1 = 0.151513 loss)
I1122 04:31:17.784790  5428 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1122 04:31:22.793500  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:31:23.000530  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_9000.caffemodel
I1122 04:31:23.016536  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_9000.solverstate
I1122 04:31:23.021037  5428 solver.cpp:330] Iteration 9000, Testing net (#0)
I1122 04:31:23.021037  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:31:24.283723 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:31:24.332741  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8691
I1122 04:31:24.332741  5428 solver.cpp:397]     Test net output #1: loss = 0.37629 (* 1 = 0.37629 loss)
I1122 04:31:24.381742  5428 solver.cpp:218] Iteration 9000 (15.1587 iter/s, 6.59686s/100 iters), loss = 0.222573
I1122 04:31:24.382741  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:31:24.382741  5428 solver.cpp:237]     Train net output #1: loss = 0.222573 (* 1 = 0.222573 loss)
I1122 04:31:24.382741  5428 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1122 04:31:29.638239  5428 solver.cpp:218] Iteration 9100 (19.0283 iter/s, 5.25534s/100 iters), loss = 0.168826
I1122 04:31:29.638239  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:31:29.638239  5428 solver.cpp:237]     Train net output #1: loss = 0.168826 (* 1 = 0.168826 loss)
I1122 04:31:29.638239  5428 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1122 04:31:34.897604  5428 solver.cpp:218] Iteration 9200 (19.0157 iter/s, 5.25881s/100 iters), loss = 0.228242
I1122 04:31:34.897604  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 04:31:34.897604  5428 solver.cpp:237]     Train net output #1: loss = 0.228242 (* 1 = 0.228242 loss)
I1122 04:31:34.897604  5428 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1122 04:31:40.169117  5428 solver.cpp:218] Iteration 9300 (18.9716 iter/s, 5.27103s/100 iters), loss = 0.206931
I1122 04:31:40.169117  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 04:31:40.169117  5428 solver.cpp:237]     Train net output #1: loss = 0.206931 (* 1 = 0.206931 loss)
I1122 04:31:40.169117  5428 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1122 04:31:45.426542  5428 solver.cpp:218] Iteration 9400 (19.0212 iter/s, 5.25729s/100 iters), loss = 0.149143
I1122 04:31:45.427043  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:31:45.427043  5428 solver.cpp:237]     Train net output #1: loss = 0.149143 (* 1 = 0.149143 loss)
I1122 04:31:45.427043  5428 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1122 04:31:50.429939  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:31:50.636979  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_9500.caffemodel
I1122 04:31:50.651969  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_9500.solverstate
I1122 04:31:50.656970  5428 solver.cpp:330] Iteration 9500, Testing net (#0)
I1122 04:31:50.656970  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:31:51.922096 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:31:51.972103  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8737
I1122 04:31:51.972103  5428 solver.cpp:397]     Test net output #1: loss = 0.35747 (* 1 = 0.35747 loss)
I1122 04:31:52.022099  5428 solver.cpp:218] Iteration 9500 (15.1638 iter/s, 6.59464s/100 iters), loss = 0.206728
I1122 04:31:52.022099  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:31:52.022099  5428 solver.cpp:237]     Train net output #1: loss = 0.206728 (* 1 = 0.206728 loss)
I1122 04:31:52.022099  5428 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1122 04:31:52.022099  5428 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1122 04:31:57.286638  5428 solver.cpp:218] Iteration 9600 (18.9965 iter/s, 5.26413s/100 iters), loss = 0.240872
I1122 04:31:57.286638  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:31:57.286638  5428 solver.cpp:237]     Train net output #1: loss = 0.240872 (* 1 = 0.240872 loss)
I1122 04:31:57.286638  5428 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1122 04:32:02.553380  5428 solver.cpp:218] Iteration 9700 (18.9871 iter/s, 5.26674s/100 iters), loss = 0.180003
I1122 04:32:02.553380  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:32:02.553380  5428 solver.cpp:237]     Train net output #1: loss = 0.180003 (* 1 = 0.180003 loss)
I1122 04:32:02.553380  5428 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1122 04:32:07.822191  5428 solver.cpp:218] Iteration 9800 (18.9819 iter/s, 5.26818s/100 iters), loss = 0.186194
I1122 04:32:07.822191  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:32:07.822191  5428 solver.cpp:237]     Train net output #1: loss = 0.186194 (* 1 = 0.186194 loss)
I1122 04:32:07.822191  5428 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1122 04:32:13.085731  5428 solver.cpp:218] Iteration 9900 (18.9996 iter/s, 5.26328s/100 iters), loss = 0.172771
I1122 04:32:13.085731  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:32:13.085731  5428 solver.cpp:237]     Train net output #1: loss = 0.172771 (* 1 = 0.172771 loss)
I1122 04:32:13.085731  5428 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1122 04:32:18.100917  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:32:18.308955  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_10000.caffemodel
I1122 04:32:18.325469  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_10000.solverstate
I1122 04:32:18.329484  5428 solver.cpp:330] Iteration 10000, Testing net (#0)
I1122 04:32:18.329484  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:32:19.596091 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:32:19.646090  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8921
I1122 04:32:19.646090  5428 solver.cpp:397]     Test net output #1: loss = 0.314893 (* 1 = 0.314893 loss)
I1122 04:32:19.696089  5428 solver.cpp:218] Iteration 10000 (15.13 iter/s, 6.60937s/100 iters), loss = 0.153541
I1122 04:32:19.696089  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:32:19.696089  5428 solver.cpp:237]     Train net output #1: loss = 0.15354 (* 1 = 0.15354 loss)
I1122 04:32:19.696089  5428 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1122 04:32:24.953244  5428 solver.cpp:218] Iteration 10100 (19.0204 iter/s, 5.25751s/100 iters), loss = 0.160718
I1122 04:32:24.953244  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:32:24.953244  5428 solver.cpp:237]     Train net output #1: loss = 0.160718 (* 1 = 0.160718 loss)
I1122 04:32:24.953244  5428 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1122 04:32:30.206760  5428 solver.cpp:218] Iteration 10200 (19.0389 iter/s, 5.25239s/100 iters), loss = 0.198526
I1122 04:32:30.206760  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:32:30.206760  5428 solver.cpp:237]     Train net output #1: loss = 0.198526 (* 1 = 0.198526 loss)
I1122 04:32:30.206760  5428 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1122 04:32:35.462177  5428 solver.cpp:218] Iteration 10300 (19.027 iter/s, 5.25569s/100 iters), loss = 0.153918
I1122 04:32:35.463176  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:32:35.463176  5428 solver.cpp:237]     Train net output #1: loss = 0.153918 (* 1 = 0.153918 loss)
I1122 04:32:35.463176  5428 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1122 04:32:40.731060  5428 solver.cpp:218] Iteration 10400 (18.9834 iter/s, 5.26775s/100 iters), loss = 0.122378
I1122 04:32:40.731060  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:32:40.731060  5428 solver.cpp:237]     Train net output #1: loss = 0.122378 (* 1 = 0.122378 loss)
I1122 04:32:40.731060  5428 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1122 04:32:45.737870  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:32:45.946887  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_10500.caffemodel
I1122 04:32:45.958886  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_10500.solverstate
I1122 04:32:45.962885  5428 solver.cpp:330] Iteration 10500, Testing net (#0)
I1122 04:32:45.962885  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:32:47.229010 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:32:47.279515  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8926
I1122 04:32:47.279515  5428 solver.cpp:397]     Test net output #1: loss = 0.310645 (* 1 = 0.310645 loss)
I1122 04:32:47.329016  5428 solver.cpp:218] Iteration 10500 (15.1561 iter/s, 6.59802s/100 iters), loss = 0.188795
I1122 04:32:47.329016  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:32:47.329016  5428 solver.cpp:237]     Train net output #1: loss = 0.188795 (* 1 = 0.188795 loss)
I1122 04:32:47.329016  5428 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1122 04:32:52.592468  5428 solver.cpp:218] Iteration 10600 (19.0023 iter/s, 5.26253s/100 iters), loss = 0.232209
I1122 04:32:52.592468  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:32:52.592468  5428 solver.cpp:237]     Train net output #1: loss = 0.232209 (* 1 = 0.232209 loss)
I1122 04:32:52.592468  5428 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1122 04:32:57.851332  5428 solver.cpp:218] Iteration 10700 (19.0171 iter/s, 5.25842s/100 iters), loss = 0.150815
I1122 04:32:57.851332  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:32:57.851332  5428 solver.cpp:237]     Train net output #1: loss = 0.150815 (* 1 = 0.150815 loss)
I1122 04:32:57.851332  5428 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1122 04:33:03.089156  5428 solver.cpp:218] Iteration 10800 (19.091 iter/s, 5.23807s/100 iters), loss = 0.153594
I1122 04:33:03.090157  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:33:03.090157  5428 solver.cpp:237]     Train net output #1: loss = 0.153593 (* 1 = 0.153593 loss)
I1122 04:33:03.090157  5428 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1122 04:33:08.314324  5428 solver.cpp:218] Iteration 10900 (19.143 iter/s, 5.22385s/100 iters), loss = 0.191159
I1122 04:33:08.314324  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 04:33:08.314324  5428 solver.cpp:237]     Train net output #1: loss = 0.191159 (* 1 = 0.191159 loss)
I1122 04:33:08.314324  5428 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1122 04:33:13.291550  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:33:13.500182  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_11000.caffemodel
I1122 04:33:13.515185  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_11000.solverstate
I1122 04:33:13.519685  5428 solver.cpp:330] Iteration 11000, Testing net (#0)
I1122 04:33:13.519685  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:33:14.784111 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:33:14.834584  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8939
I1122 04:33:14.834584  5428 solver.cpp:397]     Test net output #1: loss = 0.311831 (* 1 = 0.311831 loss)
I1122 04:33:14.884598  5428 solver.cpp:218] Iteration 11000 (15.2193 iter/s, 6.57058s/100 iters), loss = 0.138097
I1122 04:33:14.884598  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:33:14.884598  5428 solver.cpp:237]     Train net output #1: loss = 0.138097 (* 1 = 0.138097 loss)
I1122 04:33:14.884598  5428 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1122 04:33:20.129110  5428 solver.cpp:218] Iteration 11100 (19.0703 iter/s, 5.24376s/100 iters), loss = 0.179127
I1122 04:33:20.129110  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:33:20.129110  5428 solver.cpp:237]     Train net output #1: loss = 0.179127 (* 1 = 0.179127 loss)
I1122 04:33:20.129110  5428 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1122 04:33:25.375679  5428 solver.cpp:218] Iteration 11200 (19.0602 iter/s, 5.24653s/100 iters), loss = 0.154847
I1122 04:33:25.375679  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:33:25.376678  5428 solver.cpp:237]     Train net output #1: loss = 0.154847 (* 1 = 0.154847 loss)
I1122 04:33:25.376678  5428 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1122 04:33:30.625103  5428 solver.cpp:218] Iteration 11300 (19.0529 iter/s, 5.24855s/100 iters), loss = 0.192767
I1122 04:33:30.625103  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:33:30.625103  5428 solver.cpp:237]     Train net output #1: loss = 0.192767 (* 1 = 0.192767 loss)
I1122 04:33:30.625103  5428 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1122 04:33:35.868476  5428 solver.cpp:218] Iteration 11400 (19.0743 iter/s, 5.24265s/100 iters), loss = 0.170166
I1122 04:33:35.868476  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:33:35.868476  5428 solver.cpp:237]     Train net output #1: loss = 0.170166 (* 1 = 0.170166 loss)
I1122 04:33:35.868476  5428 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1122 04:33:40.872808  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:33:41.080821  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_11500.caffemodel
I1122 04:33:41.094820  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_11500.solverstate
I1122 04:33:41.099822  5428 solver.cpp:330] Iteration 11500, Testing net (#0)
I1122 04:33:41.099822  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:33:42.360424 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:33:42.410940  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8937
I1122 04:33:42.410940  5428 solver.cpp:397]     Test net output #1: loss = 0.309002 (* 1 = 0.309002 loss)
I1122 04:33:42.460930  5428 solver.cpp:218] Iteration 11500 (15.1694 iter/s, 6.59223s/100 iters), loss = 0.190243
I1122 04:33:42.460930  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:33:42.460930  5428 solver.cpp:237]     Train net output #1: loss = 0.190243 (* 1 = 0.190243 loss)
I1122 04:33:42.460930  5428 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1122 04:33:47.733280  5428 solver.cpp:218] Iteration 11600 (18.9694 iter/s, 5.27166s/100 iters), loss = 0.223352
I1122 04:33:47.733280  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:33:47.733280  5428 solver.cpp:237]     Train net output #1: loss = 0.223352 (* 1 = 0.223352 loss)
I1122 04:33:47.733280  5428 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1122 04:33:53.010900  5428 solver.cpp:218] Iteration 11700 (18.9491 iter/s, 5.2773s/100 iters), loss = 0.189534
I1122 04:33:53.010900  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:33:53.010900  5428 solver.cpp:237]     Train net output #1: loss = 0.189534 (* 1 = 0.189534 loss)
I1122 04:33:53.010900  5428 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1122 04:33:58.277300  5428 solver.cpp:218] Iteration 11800 (18.9885 iter/s, 5.26636s/100 iters), loss = 0.191118
I1122 04:33:58.277300  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:33:58.277300  5428 solver.cpp:237]     Train net output #1: loss = 0.191118 (* 1 = 0.191118 loss)
I1122 04:33:58.277300  5428 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1122 04:34:03.530685  5428 solver.cpp:218] Iteration 11900 (19.0382 iter/s, 5.25259s/100 iters), loss = 0.103995
I1122 04:34:03.530685  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:34:03.530685  5428 solver.cpp:237]     Train net output #1: loss = 0.103994 (* 1 = 0.103994 loss)
I1122 04:34:03.530685  5428 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1122 04:34:08.527046  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:34:08.734057  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_12000.caffemodel
I1122 04:34:08.749058  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_12000.solverstate
I1122 04:34:08.753057  5428 solver.cpp:330] Iteration 12000, Testing net (#0)
I1122 04:34:08.753057  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:34:10.015218 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:34:10.065217  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8955
I1122 04:34:10.065217  5428 solver.cpp:397]     Test net output #1: loss = 0.308362 (* 1 = 0.308362 loss)
I1122 04:34:10.115222  5428 solver.cpp:218] Iteration 12000 (15.1881 iter/s, 6.58409s/100 iters), loss = 0.190209
I1122 04:34:10.115222  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:34:10.115222  5428 solver.cpp:237]     Train net output #1: loss = 0.190209 (* 1 = 0.190209 loss)
I1122 04:34:10.115222  5428 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1122 04:34:15.366657  5428 solver.cpp:218] Iteration 12100 (19.0446 iter/s, 5.25084s/100 iters), loss = 0.272923
I1122 04:34:15.366657  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1122 04:34:15.366657  5428 solver.cpp:237]     Train net output #1: loss = 0.272923 (* 1 = 0.272923 loss)
I1122 04:34:15.366657  5428 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1122 04:34:20.619037  5428 solver.cpp:218] Iteration 12200 (19.04 iter/s, 5.25211s/100 iters), loss = 0.147814
I1122 04:34:20.619037  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:34:20.619037  5428 solver.cpp:237]     Train net output #1: loss = 0.147814 (* 1 = 0.147814 loss)
I1122 04:34:20.619037  5428 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1122 04:34:25.881604  5428 solver.cpp:218] Iteration 12300 (19.0045 iter/s, 5.26191s/100 iters), loss = 0.146595
I1122 04:34:25.881604  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:34:25.881604  5428 solver.cpp:237]     Train net output #1: loss = 0.146595 (* 1 = 0.146595 loss)
I1122 04:34:25.881604  5428 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1122 04:34:31.137984  5428 solver.cpp:218] Iteration 12400 (19.0248 iter/s, 5.25629s/100 iters), loss = 0.115254
I1122 04:34:31.137984  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:34:31.137984  5428 solver.cpp:237]     Train net output #1: loss = 0.115254 (* 1 = 0.115254 loss)
I1122 04:34:31.137984  5428 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1122 04:34:36.133388  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:34:36.340404  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_12500.caffemodel
I1122 04:34:36.355407  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_12500.solverstate
I1122 04:34:36.359403  5428 solver.cpp:330] Iteration 12500, Testing net (#0)
I1122 04:34:36.359403  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:34:37.621556 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:34:37.671556  5428 solver.cpp:397]     Test net output #0: accuracy = 0.895
I1122 04:34:37.671556  5428 solver.cpp:397]     Test net output #1: loss = 0.308769 (* 1 = 0.308769 loss)
I1122 04:34:37.721561  5428 solver.cpp:218] Iteration 12500 (15.1903 iter/s, 6.58315s/100 iters), loss = 0.153192
I1122 04:34:37.721561  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:34:37.721561  5428 solver.cpp:237]     Train net output #1: loss = 0.153192 (* 1 = 0.153192 loss)
I1122 04:34:37.721561  5428 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1122 04:34:42.975296  5428 solver.cpp:218] Iteration 12600 (19.0363 iter/s, 5.25312s/100 iters), loss = 0.222078
I1122 04:34:42.975296  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:34:42.975296  5428 solver.cpp:237]     Train net output #1: loss = 0.222078 (* 1 = 0.222078 loss)
I1122 04:34:42.975296  5428 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1122 04:34:48.222673  5428 solver.cpp:218] Iteration 12700 (19.0589 iter/s, 5.2469s/100 iters), loss = 0.18648
I1122 04:34:48.222673  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:34:48.222673  5428 solver.cpp:237]     Train net output #1: loss = 0.18648 (* 1 = 0.18648 loss)
I1122 04:34:48.222673  5428 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1122 04:34:53.470049  5428 solver.cpp:218] Iteration 12800 (19.0585 iter/s, 5.24702s/100 iters), loss = 0.134039
I1122 04:34:53.470049  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:34:53.470049  5428 solver.cpp:237]     Train net output #1: loss = 0.134039 (* 1 = 0.134039 loss)
I1122 04:34:53.470049  5428 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1122 04:34:58.721415  5428 solver.cpp:218] Iteration 12900 (19.0434 iter/s, 5.25116s/100 iters), loss = 0.105824
I1122 04:34:58.721415  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 04:34:58.721415  5428 solver.cpp:237]     Train net output #1: loss = 0.105824 (* 1 = 0.105824 loss)
I1122 04:34:58.721415  5428 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1122 04:35:03.714047  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:35:03.922062  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_13000.caffemodel
I1122 04:35:03.937063  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_13000.solverstate
I1122 04:35:03.941062  5428 solver.cpp:330] Iteration 13000, Testing net (#0)
I1122 04:35:03.942065  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:35:05.208164 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:35:05.258162  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8934
I1122 04:35:05.258162  5428 solver.cpp:397]     Test net output #1: loss = 0.310454 (* 1 = 0.310454 loss)
I1122 04:35:05.308166  5428 solver.cpp:218] Iteration 13000 (15.1845 iter/s, 6.58564s/100 iters), loss = 0.155595
I1122 04:35:05.308166  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:35:05.308166  5428 solver.cpp:237]     Train net output #1: loss = 0.155595 (* 1 = 0.155595 loss)
I1122 04:35:05.308166  5428 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1122 04:35:10.561488  5428 solver.cpp:218] Iteration 13100 (19.035 iter/s, 5.25349s/100 iters), loss = 0.216458
I1122 04:35:10.561488  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:35:10.561488  5428 solver.cpp:237]     Train net output #1: loss = 0.216458 (* 1 = 0.216458 loss)
I1122 04:35:10.561488  5428 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1122 04:35:15.813546  5428 solver.cpp:218] Iteration 13200 (19.0414 iter/s, 5.25171s/100 iters), loss = 0.183629
I1122 04:35:15.813546  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:35:15.813546  5428 solver.cpp:237]     Train net output #1: loss = 0.183629 (* 1 = 0.183629 loss)
I1122 04:35:15.813546  5428 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1122 04:35:21.076400  5428 solver.cpp:218] Iteration 13300 (19.004 iter/s, 5.26206s/100 iters), loss = 0.177984
I1122 04:35:21.076400  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:35:21.076900  5428 solver.cpp:237]     Train net output #1: loss = 0.177984 (* 1 = 0.177984 loss)
I1122 04:35:21.076900  5428 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1122 04:35:26.329365  5428 solver.cpp:218] Iteration 13400 (19.0385 iter/s, 5.2525s/100 iters), loss = 0.109096
I1122 04:35:26.329365  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:35:26.329365  5428 solver.cpp:237]     Train net output #1: loss = 0.109095 (* 1 = 0.109095 loss)
I1122 04:35:26.329365  5428 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1122 04:35:31.328409  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:35:31.536330  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_13500.caffemodel
I1122 04:35:31.550329  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_13500.solverstate
I1122 04:35:31.555328  5428 solver.cpp:330] Iteration 13500, Testing net (#0)
I1122 04:35:31.555328  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:35:32.815114 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:35:32.866118  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8959
I1122 04:35:32.866118  5428 solver.cpp:397]     Test net output #1: loss = 0.306856 (* 1 = 0.306856 loss)
I1122 04:35:32.915746  5428 solver.cpp:218] Iteration 13500 (15.1828 iter/s, 6.58641s/100 iters), loss = 0.129109
I1122 04:35:32.915746  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:35:32.915746  5428 solver.cpp:237]     Train net output #1: loss = 0.129109 (* 1 = 0.129109 loss)
I1122 04:35:32.915746  5428 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1122 04:35:38.173127  5428 solver.cpp:218] Iteration 13600 (19.0234 iter/s, 5.25667s/100 iters), loss = 0.154315
I1122 04:35:38.173127  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:35:38.173127  5428 solver.cpp:237]     Train net output #1: loss = 0.154315 (* 1 = 0.154315 loss)
I1122 04:35:38.173127  5428 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1122 04:35:43.432929  5428 solver.cpp:218] Iteration 13700 (19.0153 iter/s, 5.25892s/100 iters), loss = 0.174991
I1122 04:35:43.432929  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:35:43.432929  5428 solver.cpp:237]     Train net output #1: loss = 0.174991 (* 1 = 0.174991 loss)
I1122 04:35:43.432929  5428 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1122 04:35:48.682307  5428 solver.cpp:218] Iteration 13800 (19.0494 iter/s, 5.24951s/100 iters), loss = 0.175108
I1122 04:35:48.682307  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:35:48.682307  5428 solver.cpp:237]     Train net output #1: loss = 0.175108 (* 1 = 0.175108 loss)
I1122 04:35:48.682307  5428 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1122 04:35:53.941830  5428 solver.cpp:218] Iteration 13900 (19.0158 iter/s, 5.25878s/100 iters), loss = 0.10924
I1122 04:35:53.941830  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:35:53.941830  5428 solver.cpp:237]     Train net output #1: loss = 0.10924 (* 1 = 0.10924 loss)
I1122 04:35:53.941830  5428 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1122 04:35:58.916122  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:35:59.122136  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_14000.caffemodel
I1122 04:35:59.136137  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_14000.solverstate
I1122 04:35:59.140136  5428 solver.cpp:330] Iteration 14000, Testing net (#0)
I1122 04:35:59.140136  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:36:00.398234 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:36:00.448253  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8952
I1122 04:36:00.449242  5428 solver.cpp:397]     Test net output #1: loss = 0.308411 (* 1 = 0.308411 loss)
I1122 04:36:00.498239  5428 solver.cpp:218] Iteration 14000 (15.2535 iter/s, 6.55587s/100 iters), loss = 0.139805
I1122 04:36:00.498239  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:36:00.498239  5428 solver.cpp:237]     Train net output #1: loss = 0.139805 (* 1 = 0.139805 loss)
I1122 04:36:00.498239  5428 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1122 04:36:05.731550  5428 solver.cpp:218] Iteration 14100 (19.1105 iter/s, 5.23271s/100 iters), loss = 0.180981
I1122 04:36:05.731550  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:36:05.731550  5428 solver.cpp:237]     Train net output #1: loss = 0.18098 (* 1 = 0.18098 loss)
I1122 04:36:05.731550  5428 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1122 04:36:10.972913  5428 solver.cpp:218] Iteration 14200 (19.078 iter/s, 5.24165s/100 iters), loss = 0.229959
I1122 04:36:10.972913  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:36:10.972913  5428 solver.cpp:237]     Train net output #1: loss = 0.229958 (* 1 = 0.229958 loss)
I1122 04:36:10.972913  5428 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1122 04:36:16.201772  5428 solver.cpp:218] Iteration 14300 (19.1285 iter/s, 5.2278s/100 iters), loss = 0.176696
I1122 04:36:16.201772  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:36:16.201772  5428 solver.cpp:237]     Train net output #1: loss = 0.176696 (* 1 = 0.176696 loss)
I1122 04:36:16.201772  5428 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1122 04:36:21.438683  5428 solver.cpp:218] Iteration 14400 (19.0941 iter/s, 5.23722s/100 iters), loss = 0.124248
I1122 04:36:21.439671  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:36:21.439671  5428 solver.cpp:237]     Train net output #1: loss = 0.124248 (* 1 = 0.124248 loss)
I1122 04:36:21.439671  5428 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1122 04:36:26.415482  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:36:26.620995  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_14500.caffemodel
I1122 04:36:26.634994  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_14500.solverstate
I1122 04:36:26.638994  5428 solver.cpp:330] Iteration 14500, Testing net (#0)
I1122 04:36:26.638994  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:36:27.900082 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:36:27.950088  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8967
I1122 04:36:27.950088  5428 solver.cpp:397]     Test net output #1: loss = 0.306601 (* 1 = 0.306601 loss)
I1122 04:36:28.000087  5428 solver.cpp:218] Iteration 14500 (15.2422 iter/s, 6.56073s/100 iters), loss = 0.172281
I1122 04:36:28.000087  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:36:28.000087  5428 solver.cpp:237]     Train net output #1: loss = 0.172281 (* 1 = 0.172281 loss)
I1122 04:36:28.000087  5428 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1122 04:36:33.232472  5428 solver.cpp:218] Iteration 14600 (19.1148 iter/s, 5.23155s/100 iters), loss = 0.19081
I1122 04:36:33.232472  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:36:33.232472  5428 solver.cpp:237]     Train net output #1: loss = 0.19081 (* 1 = 0.19081 loss)
I1122 04:36:33.232472  5428 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1122 04:36:38.475904  5428 solver.cpp:218] Iteration 14700 (19.0737 iter/s, 5.24283s/100 iters), loss = 0.153585
I1122 04:36:38.475904  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:36:38.475904  5428 solver.cpp:237]     Train net output #1: loss = 0.153585 (* 1 = 0.153585 loss)
I1122 04:36:38.475904  5428 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1122 04:36:43.722313  5428 solver.cpp:218] Iteration 14800 (19.0617 iter/s, 5.24613s/100 iters), loss = 0.159736
I1122 04:36:43.722313  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:36:43.722313  5428 solver.cpp:237]     Train net output #1: loss = 0.159736 (* 1 = 0.159736 loss)
I1122 04:36:43.722313  5428 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1122 04:36:48.967681  5428 solver.cpp:218] Iteration 14900 (19.0652 iter/s, 5.24515s/100 iters), loss = 0.122006
I1122 04:36:48.967681  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:36:48.967681  5428 solver.cpp:237]     Train net output #1: loss = 0.122006 (* 1 = 0.122006 loss)
I1122 04:36:48.967681  5428 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1122 04:36:53.951303  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:36:54.157258  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_15000.caffemodel
I1122 04:36:54.172260  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_15000.solverstate
I1122 04:36:54.176260  5428 solver.cpp:330] Iteration 15000, Testing net (#0)
I1122 04:36:54.176260  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:36:55.440496 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:36:55.490491  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8969
I1122 04:36:55.490491  5428 solver.cpp:397]     Test net output #1: loss = 0.308594 (* 1 = 0.308594 loss)
I1122 04:36:55.540499  5428 solver.cpp:218] Iteration 15000 (15.215 iter/s, 6.57245s/100 iters), loss = 0.12493
I1122 04:36:55.540499  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:36:55.540499  5428 solver.cpp:237]     Train net output #1: loss = 0.12493 (* 1 = 0.12493 loss)
I1122 04:36:55.540499  5428 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1122 04:37:00.785326  5428 solver.cpp:218] Iteration 15100 (19.0695 iter/s, 5.24398s/100 iters), loss = 0.143411
I1122 04:37:00.785326  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:37:00.785326  5428 solver.cpp:237]     Train net output #1: loss = 0.14341 (* 1 = 0.14341 loss)
I1122 04:37:00.785326  5428 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1122 04:37:06.016273  5428 solver.cpp:218] Iteration 15200 (19.1184 iter/s, 5.23056s/100 iters), loss = 0.16111
I1122 04:37:06.016273  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:37:06.016273  5428 solver.cpp:237]     Train net output #1: loss = 0.16111 (* 1 = 0.16111 loss)
I1122 04:37:06.016273  5428 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1122 04:37:11.244161  5428 solver.cpp:218] Iteration 15300 (19.129 iter/s, 5.22767s/100 iters), loss = 0.180875
I1122 04:37:11.244161  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:37:11.244161  5428 solver.cpp:237]     Train net output #1: loss = 0.180875 (* 1 = 0.180875 loss)
I1122 04:37:11.244161  5428 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1122 04:37:11.244161  5428 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1122 04:37:16.471567  5428 solver.cpp:218] Iteration 15400 (19.1301 iter/s, 5.22737s/100 iters), loss = 0.0931398
I1122 04:37:16.471567  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:37:16.471567  5428 solver.cpp:237]     Train net output #1: loss = 0.0931396 (* 1 = 0.0931396 loss)
I1122 04:37:16.471567  5428 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1122 04:37:21.452944  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:37:21.657958  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_15500.caffemodel
I1122 04:37:21.671957  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_15500.solverstate
I1122 04:37:21.675956  5428 solver.cpp:330] Iteration 15500, Testing net (#0)
I1122 04:37:21.675956  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:37:22.934167 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:37:22.985148  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8961
I1122 04:37:22.985148  5428 solver.cpp:397]     Test net output #1: loss = 0.308073 (* 1 = 0.308073 loss)
I1122 04:37:23.035152  5428 solver.cpp:218] Iteration 15500 (15.238 iter/s, 6.56252s/100 iters), loss = 0.149386
I1122 04:37:23.035152  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:37:23.035152  5428 solver.cpp:237]     Train net output #1: loss = 0.149386 (* 1 = 0.149386 loss)
I1122 04:37:23.035152  5428 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1122 04:37:28.246491  5428 solver.cpp:218] Iteration 15600 (19.1894 iter/s, 5.21121s/100 iters), loss = 0.232959
I1122 04:37:28.246491  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:37:28.246491  5428 solver.cpp:237]     Train net output #1: loss = 0.232959 (* 1 = 0.232959 loss)
I1122 04:37:28.246491  5428 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1122 04:37:33.466840  5428 solver.cpp:218] Iteration 15700 (19.158 iter/s, 5.21974s/100 iters), loss = 0.170495
I1122 04:37:33.466840  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:37:33.466840  5428 solver.cpp:237]     Train net output #1: loss = 0.170495 (* 1 = 0.170495 loss)
I1122 04:37:33.466840  5428 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1122 04:37:38.695196  5428 solver.cpp:218] Iteration 15800 (19.1255 iter/s, 5.22864s/100 iters), loss = 0.176319
I1122 04:37:38.695196  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:37:38.695196  5428 solver.cpp:237]     Train net output #1: loss = 0.176319 (* 1 = 0.176319 loss)
I1122 04:37:38.695196  5428 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1122 04:37:43.918579  5428 solver.cpp:218] Iteration 15900 (19.148 iter/s, 5.22246s/100 iters), loss = 0.11615
I1122 04:37:43.918579  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:37:43.918579  5428 solver.cpp:237]     Train net output #1: loss = 0.116149 (* 1 = 0.116149 loss)
I1122 04:37:43.918579  5428 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1122 04:37:48.879912  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:37:49.084924  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_16000.caffemodel
I1122 04:37:49.099925  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_16000.solverstate
I1122 04:37:49.103925  5428 solver.cpp:330] Iteration 16000, Testing net (#0)
I1122 04:37:49.103925  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:37:50.361013 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:37:50.411006  5428 solver.cpp:397]     Test net output #0: accuracy = 0.897
I1122 04:37:50.411006  5428 solver.cpp:397]     Test net output #1: loss = 0.307045 (* 1 = 0.307045 loss)
I1122 04:37:50.461015  5428 solver.cpp:218] Iteration 16000 (15.2863 iter/s, 6.54179s/100 iters), loss = 0.132025
I1122 04:37:50.461015  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:37:50.461015  5428 solver.cpp:237]     Train net output #1: loss = 0.132025 (* 1 = 0.132025 loss)
I1122 04:37:50.461015  5428 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1122 04:37:55.689358  5428 solver.cpp:218] Iteration 16100 (19.1262 iter/s, 5.22842s/100 iters), loss = 0.161722
I1122 04:37:55.689358  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:37:55.689358  5428 solver.cpp:237]     Train net output #1: loss = 0.161721 (* 1 = 0.161721 loss)
I1122 04:37:55.689358  5428 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1122 04:38:00.919268  5428 solver.cpp:218] Iteration 16200 (19.124 iter/s, 5.22903s/100 iters), loss = 0.161491
I1122 04:38:00.919268  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:38:00.919268  5428 solver.cpp:237]     Train net output #1: loss = 0.161491 (* 1 = 0.161491 loss)
I1122 04:38:00.919268  5428 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1122 04:38:06.142261  5428 solver.cpp:218] Iteration 16300 (19.1442 iter/s, 5.22352s/100 iters), loss = 0.144873
I1122 04:38:06.143260  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:38:06.143260  5428 solver.cpp:237]     Train net output #1: loss = 0.144873 (* 1 = 0.144873 loss)
I1122 04:38:06.143260  5428 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1122 04:38:11.367161  5428 solver.cpp:218] Iteration 16400 (19.1415 iter/s, 5.22426s/100 iters), loss = 0.120783
I1122 04:38:11.367161  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:38:11.367161  5428 solver.cpp:237]     Train net output #1: loss = 0.120783 (* 1 = 0.120783 loss)
I1122 04:38:11.367161  5428 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1122 04:38:16.333844  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:38:16.539062  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_16500.caffemodel
I1122 04:38:16.553059  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_16500.solverstate
I1122 04:38:16.557063  5428 solver.cpp:330] Iteration 16500, Testing net (#0)
I1122 04:38:16.557063  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:38:17.816226 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:38:17.866029  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8968
I1122 04:38:17.866029  5428 solver.cpp:397]     Test net output #1: loss = 0.306649 (* 1 = 0.306649 loss)
I1122 04:38:17.916029  5428 solver.cpp:218] Iteration 16500 (15.2723 iter/s, 6.54781s/100 iters), loss = 0.109093
I1122 04:38:17.916029  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:38:17.916029  5428 solver.cpp:237]     Train net output #1: loss = 0.109093 (* 1 = 0.109093 loss)
I1122 04:38:17.916029  5428 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1122 04:38:23.156246  5428 solver.cpp:218] Iteration 16600 (19.0831 iter/s, 5.24024s/100 iters), loss = 0.178704
I1122 04:38:23.156246  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:38:23.156246  5428 solver.cpp:237]     Train net output #1: loss = 0.178704 (* 1 = 0.178704 loss)
I1122 04:38:23.156246  5428 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1122 04:38:28.386034  5428 solver.cpp:218] Iteration 16700 (19.1231 iter/s, 5.22928s/100 iters), loss = 0.177226
I1122 04:38:28.386034  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:38:28.386034  5428 solver.cpp:237]     Train net output #1: loss = 0.177226 (* 1 = 0.177226 loss)
I1122 04:38:28.386034  5428 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1122 04:38:33.612442  5428 solver.cpp:218] Iteration 16800 (19.1344 iter/s, 5.2262s/100 iters), loss = 0.167699
I1122 04:38:33.612442  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:38:33.612442  5428 solver.cpp:237]     Train net output #1: loss = 0.167699 (* 1 = 0.167699 loss)
I1122 04:38:33.612442  5428 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1122 04:38:38.844794  5428 solver.cpp:218] Iteration 16900 (19.1164 iter/s, 5.2311s/100 iters), loss = 0.0974853
I1122 04:38:38.844794  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:38:38.844794  5428 solver.cpp:237]     Train net output #1: loss = 0.097485 (* 1 = 0.097485 loss)
I1122 04:38:38.844794  5428 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1122 04:38:43.817644  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:38:44.024664  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_17000.caffemodel
I1122 04:38:44.039669  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_17000.solverstate
I1122 04:38:44.043669  5428 solver.cpp:330] Iteration 17000, Testing net (#0)
I1122 04:38:44.043669  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:38:45.299764 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:38:45.349767  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8978
I1122 04:38:45.349767  5428 solver.cpp:397]     Test net output #1: loss = 0.306399 (* 1 = 0.306399 loss)
I1122 04:38:45.400768  5428 solver.cpp:218] Iteration 17000 (15.2544 iter/s, 6.55547s/100 iters), loss = 0.145858
I1122 04:38:45.400768  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:38:45.400768  5428 solver.cpp:237]     Train net output #1: loss = 0.145858 (* 1 = 0.145858 loss)
I1122 04:38:45.400768  5428 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1122 04:38:50.630193  5428 solver.cpp:218] Iteration 17100 (19.1211 iter/s, 5.22984s/100 iters), loss = 0.250311
I1122 04:38:50.631182  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:38:50.631182  5428 solver.cpp:237]     Train net output #1: loss = 0.250311 (* 1 = 0.250311 loss)
I1122 04:38:50.631182  5428 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1122 04:38:55.858676  5428 solver.cpp:218] Iteration 17200 (19.1314 iter/s, 5.22701s/100 iters), loss = 0.163243
I1122 04:38:55.858676  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:38:55.858676  5428 solver.cpp:237]     Train net output #1: loss = 0.163243 (* 1 = 0.163243 loss)
I1122 04:38:55.858676  5428 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1122 04:39:01.087033  5428 solver.cpp:218] Iteration 17300 (19.1248 iter/s, 5.22881s/100 iters), loss = 0.190366
I1122 04:39:01.087033  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:39:01.088033  5428 solver.cpp:237]     Train net output #1: loss = 0.190366 (* 1 = 0.190366 loss)
I1122 04:39:01.088033  5428 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1122 04:39:06.329309  5428 solver.cpp:218] Iteration 17400 (19.0775 iter/s, 5.24179s/100 iters), loss = 0.12644
I1122 04:39:06.329309  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:39:06.329309  5428 solver.cpp:237]     Train net output #1: loss = 0.12644 (* 1 = 0.12644 loss)
I1122 04:39:06.329309  5428 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1122 04:39:11.300690  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:39:11.506705  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_17500.caffemodel
I1122 04:39:11.520704  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_17500.solverstate
I1122 04:39:11.524703  5428 solver.cpp:330] Iteration 17500, Testing net (#0)
I1122 04:39:11.524703  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:39:12.782786 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:39:12.832790  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8977
I1122 04:39:12.832790  5428 solver.cpp:397]     Test net output #1: loss = 0.306435 (* 1 = 0.306435 loss)
I1122 04:39:12.882789  5428 solver.cpp:218] Iteration 17500 (15.2616 iter/s, 6.55238s/100 iters), loss = 0.14479
I1122 04:39:12.882789  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:39:12.882789  5428 solver.cpp:237]     Train net output #1: loss = 0.14479 (* 1 = 0.14479 loss)
I1122 04:39:12.882789  5428 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1122 04:39:18.117151  5428 solver.cpp:218] Iteration 17600 (19.1064 iter/s, 5.23385s/100 iters), loss = 0.188389
I1122 04:39:18.117151  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:39:18.117151  5428 solver.cpp:237]     Train net output #1: loss = 0.188389 (* 1 = 0.188389 loss)
I1122 04:39:18.117151  5428 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1122 04:39:23.352555  5428 solver.cpp:218] Iteration 17700 (19.101 iter/s, 5.23534s/100 iters), loss = 0.16077
I1122 04:39:23.352555  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:39:23.352555  5428 solver.cpp:237]     Train net output #1: loss = 0.16077 (* 1 = 0.16077 loss)
I1122 04:39:23.352555  5428 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1122 04:39:28.591908  5428 solver.cpp:218] Iteration 17800 (19.0891 iter/s, 5.2386s/100 iters), loss = 0.1493
I1122 04:39:28.591908  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:39:28.591908  5428 solver.cpp:237]     Train net output #1: loss = 0.149299 (* 1 = 0.149299 loss)
I1122 04:39:28.591908  5428 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1122 04:39:33.829272  5428 solver.cpp:218] Iteration 17900 (19.0945 iter/s, 5.23712s/100 iters), loss = 0.0950072
I1122 04:39:33.829272  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 04:39:33.829272  5428 solver.cpp:237]     Train net output #1: loss = 0.095007 (* 1 = 0.095007 loss)
I1122 04:39:33.829272  5428 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1122 04:39:38.820616  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:39:39.027626  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_18000.caffemodel
I1122 04:39:39.042626  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_18000.solverstate
I1122 04:39:39.046627  5428 solver.cpp:330] Iteration 18000, Testing net (#0)
I1122 04:39:39.046627  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:39:40.302703 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:39:40.353711  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8975
I1122 04:39:40.353711  5428 solver.cpp:397]     Test net output #1: loss = 0.306183 (* 1 = 0.306183 loss)
I1122 04:39:40.403714  5428 solver.cpp:218] Iteration 18000 (15.211 iter/s, 6.57419s/100 iters), loss = 0.121439
I1122 04:39:40.403714  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:39:40.403714  5428 solver.cpp:237]     Train net output #1: loss = 0.121439 (* 1 = 0.121439 loss)
I1122 04:39:40.403714  5428 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1122 04:39:45.631083  5428 solver.cpp:218] Iteration 18100 (19.1338 iter/s, 5.22634s/100 iters), loss = 0.179685
I1122 04:39:45.631083  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:39:45.631083  5428 solver.cpp:237]     Train net output #1: loss = 0.179685 (* 1 = 0.179685 loss)
I1122 04:39:45.631083  5428 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1122 04:39:50.857424  5428 solver.cpp:218] Iteration 18200 (19.1353 iter/s, 5.22595s/100 iters), loss = 0.168637
I1122 04:39:50.857424  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:39:50.857424  5428 solver.cpp:237]     Train net output #1: loss = 0.168637 (* 1 = 0.168637 loss)
I1122 04:39:50.857424  5428 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1122 04:39:56.078801  5428 solver.cpp:218] Iteration 18300 (19.1535 iter/s, 5.22097s/100 iters), loss = 0.184882
I1122 04:39:56.078801  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:39:56.078801  5428 solver.cpp:237]     Train net output #1: loss = 0.184882 (* 1 = 0.184882 loss)
I1122 04:39:56.078801  5428 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1122 04:40:01.345217  5428 solver.cpp:218] Iteration 18400 (18.9883 iter/s, 5.26641s/100 iters), loss = 0.0936625
I1122 04:40:01.345217  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 04:40:01.345217  5428 solver.cpp:237]     Train net output #1: loss = 0.0936622 (* 1 = 0.0936622 loss)
I1122 04:40:01.345217  5428 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1122 04:40:06.310588  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:40:06.517601  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_18500.caffemodel
I1122 04:40:06.530601  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_18500.solverstate
I1122 04:40:06.534601  5428 solver.cpp:330] Iteration 18500, Testing net (#0)
I1122 04:40:06.534601  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:40:07.795729 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:40:07.845727  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8971
I1122 04:40:07.845727  5428 solver.cpp:397]     Test net output #1: loss = 0.305985 (* 1 = 0.305985 loss)
I1122 04:40:07.895732  5428 solver.cpp:218] Iteration 18500 (15.2679 iter/s, 6.5497s/100 iters), loss = 0.0932262
I1122 04:40:07.895732  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:40:07.895732  5428 solver.cpp:237]     Train net output #1: loss = 0.0932259 (* 1 = 0.0932259 loss)
I1122 04:40:07.895732  5428 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1122 04:40:13.125087  5428 solver.cpp:218] Iteration 18600 (19.122 iter/s, 5.22959s/100 iters), loss = 0.202901
I1122 04:40:13.125087  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:40:13.126087  5428 solver.cpp:237]     Train net output #1: loss = 0.2029 (* 1 = 0.2029 loss)
I1122 04:40:13.126087  5428 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1122 04:40:18.351447  5428 solver.cpp:218] Iteration 18700 (19.1365 iter/s, 5.22562s/100 iters), loss = 0.196745
I1122 04:40:18.351447  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:40:18.351447  5428 solver.cpp:237]     Train net output #1: loss = 0.196744 (* 1 = 0.196744 loss)
I1122 04:40:18.351447  5428 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1122 04:40:23.587786  5428 solver.cpp:218] Iteration 18800 (19.1019 iter/s, 5.23507s/100 iters), loss = 0.160795
I1122 04:40:23.587786  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:40:23.587786  5428 solver.cpp:237]     Train net output #1: loss = 0.160795 (* 1 = 0.160795 loss)
I1122 04:40:23.587786  5428 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1122 04:40:28.820147  5428 solver.cpp:218] Iteration 18900 (19.1105 iter/s, 5.23273s/100 iters), loss = 0.103388
I1122 04:40:28.820147  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:40:28.820147  5428 solver.cpp:237]     Train net output #1: loss = 0.103387 (* 1 = 0.103387 loss)
I1122 04:40:28.820147  5428 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1122 04:40:33.797493  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:40:34.004505  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_19000.caffemodel
I1122 04:40:34.017508  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_19000.solverstate
I1122 04:40:34.020506  5428 solver.cpp:330] Iteration 19000, Testing net (#0)
I1122 04:40:34.020506  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:40:35.278612 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:40:35.328619  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8969
I1122 04:40:35.328619  5428 solver.cpp:397]     Test net output #1: loss = 0.306419 (* 1 = 0.306419 loss)
I1122 04:40:35.378618  5428 solver.cpp:218] Iteration 19000 (15.2494 iter/s, 6.55762s/100 iters), loss = 0.181068
I1122 04:40:35.378618  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:40:35.378618  5428 solver.cpp:237]     Train net output #1: loss = 0.181068 (* 1 = 0.181068 loss)
I1122 04:40:35.378618  5428 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1122 04:40:40.608053  5428 solver.cpp:218] Iteration 19100 (19.1256 iter/s, 5.22861s/100 iters), loss = 0.21056
I1122 04:40:40.608053  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:40:40.608053  5428 solver.cpp:237]     Train net output #1: loss = 0.21056 (* 1 = 0.21056 loss)
I1122 04:40:40.608053  5428 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1122 04:40:45.833442  5428 solver.cpp:218] Iteration 19200 (19.1382 iter/s, 5.22516s/100 iters), loss = 0.167321
I1122 04:40:45.833442  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:40:45.833442  5428 solver.cpp:237]     Train net output #1: loss = 0.167321 (* 1 = 0.167321 loss)
I1122 04:40:45.833442  5428 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1122 04:40:51.053828  5428 solver.cpp:218] Iteration 19300 (19.1566 iter/s, 5.22013s/100 iters), loss = 0.163251
I1122 04:40:51.053828  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:40:51.053828  5428 solver.cpp:237]     Train net output #1: loss = 0.163251 (* 1 = 0.163251 loss)
I1122 04:40:51.053828  5428 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1122 04:40:56.264190  5428 solver.cpp:218] Iteration 19400 (19.1956 iter/s, 5.20952s/100 iters), loss = 0.0798213
I1122 04:40:56.264190  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 04:40:56.264190  5428 solver.cpp:237]     Train net output #1: loss = 0.0798211 (* 1 = 0.0798211 loss)
I1122 04:40:56.264190  5428 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1122 04:41:01.234524  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:41:01.440539  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_19500.caffemodel
I1122 04:41:01.455539  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_19500.solverstate
I1122 04:41:01.459539  5428 solver.cpp:330] Iteration 19500, Testing net (#0)
I1122 04:41:01.459539  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:41:02.716687 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:41:02.766688  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8974
I1122 04:41:02.766688  5428 solver.cpp:397]     Test net output #1: loss = 0.30631 (* 1 = 0.30631 loss)
I1122 04:41:02.815693  5428 solver.cpp:218] Iteration 19500 (15.2636 iter/s, 6.55155s/100 iters), loss = 0.142932
I1122 04:41:02.815693  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:41:02.815693  5428 solver.cpp:237]     Train net output #1: loss = 0.142932 (* 1 = 0.142932 loss)
I1122 04:41:02.815693  5428 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1122 04:41:02.815693  5428 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1122 04:41:08.064014  5428 solver.cpp:218] Iteration 19600 (19.0568 iter/s, 5.24747s/100 iters), loss = 0.170924
I1122 04:41:08.064014  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:41:08.064014  5428 solver.cpp:237]     Train net output #1: loss = 0.170923 (* 1 = 0.170923 loss)
I1122 04:41:08.064014  5428 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1122 04:41:13.306337  5428 solver.cpp:218] Iteration 19700 (19.0767 iter/s, 5.24201s/100 iters), loss = 0.182978
I1122 04:41:13.306337  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:41:13.306337  5428 solver.cpp:237]     Train net output #1: loss = 0.182977 (* 1 = 0.182977 loss)
I1122 04:41:13.306337  5428 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1122 04:41:18.550706  5428 solver.cpp:218] Iteration 19800 (19.067 iter/s, 5.24468s/100 iters), loss = 0.156006
I1122 04:41:18.550706  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:41:18.550706  5428 solver.cpp:237]     Train net output #1: loss = 0.156006 (* 1 = 0.156006 loss)
I1122 04:41:18.550706  5428 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1122 04:41:23.791534  5428 solver.cpp:218] Iteration 19900 (19.0844 iter/s, 5.23987s/100 iters), loss = 0.0853649
I1122 04:41:23.791534  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 04:41:23.791534  5428 solver.cpp:237]     Train net output #1: loss = 0.0853647 (* 1 = 0.0853647 loss)
I1122 04:41:23.791534  5428 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1122 04:41:28.780374  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:41:28.987903  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_20000.caffemodel
I1122 04:41:29.000404  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_20000.solverstate
I1122 04:41:29.004410  5428 solver.cpp:330] Iteration 20000, Testing net (#0)
I1122 04:41:29.004410  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:41:30.262521 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:41:30.312528  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8977
I1122 04:41:30.312528  5428 solver.cpp:397]     Test net output #1: loss = 0.306295 (* 1 = 0.306295 loss)
I1122 04:41:30.362529  5428 solver.cpp:218] Iteration 20000 (15.2188 iter/s, 6.57083s/100 iters), loss = 0.128256
I1122 04:41:30.362529  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:41:30.362529  5428 solver.cpp:237]     Train net output #1: loss = 0.128255 (* 1 = 0.128255 loss)
I1122 04:41:30.362529  5428 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1122 04:41:35.581681  5428 solver.cpp:218] Iteration 20100 (19.1624 iter/s, 5.21854s/100 iters), loss = 0.214666
I1122 04:41:35.581681  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:41:35.581681  5428 solver.cpp:237]     Train net output #1: loss = 0.214666 (* 1 = 0.214666 loss)
I1122 04:41:35.581681  5428 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1122 04:41:40.808665  5428 solver.cpp:218] Iteration 20200 (19.1316 iter/s, 5.22697s/100 iters), loss = 0.162896
I1122 04:41:40.808665  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:41:40.808665  5428 solver.cpp:237]     Train net output #1: loss = 0.162896 (* 1 = 0.162896 loss)
I1122 04:41:40.808665  5428 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1122 04:41:46.028666  5428 solver.cpp:218] Iteration 20300 (19.1609 iter/s, 5.21897s/100 iters), loss = 0.16635
I1122 04:41:46.028666  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:41:46.028666  5428 solver.cpp:237]     Train net output #1: loss = 0.16635 (* 1 = 0.16635 loss)
I1122 04:41:46.028666  5428 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1122 04:41:51.272739  5428 solver.cpp:218] Iteration 20400 (19.0692 iter/s, 5.24405s/100 iters), loss = 0.136411
I1122 04:41:51.272739  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:41:51.272739  5428 solver.cpp:237]     Train net output #1: loss = 0.136411 (* 1 = 0.136411 loss)
I1122 04:41:51.272739  5428 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1122 04:41:56.270951  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:41:56.477236  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_20500.caffemodel
I1122 04:41:56.491236  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_20500.solverstate
I1122 04:41:56.496237  5428 solver.cpp:330] Iteration 20500, Testing net (#0)
I1122 04:41:56.496237  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:41:57.757704 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:41:57.806701  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8974
I1122 04:41:57.806701  5428 solver.cpp:397]     Test net output #1: loss = 0.306212 (* 1 = 0.306212 loss)
I1122 04:41:57.856868  5428 solver.cpp:218] Iteration 20500 (15.1893 iter/s, 6.58358s/100 iters), loss = 0.165333
I1122 04:41:57.856868  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:41:57.856868  5428 solver.cpp:237]     Train net output #1: loss = 0.165333 (* 1 = 0.165333 loss)
I1122 04:41:57.856868  5428 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1122 04:42:03.088052  5428 solver.cpp:218] Iteration 20600 (19.1163 iter/s, 5.23112s/100 iters), loss = 0.25605
I1122 04:42:03.088052  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1122 04:42:03.088052  5428 solver.cpp:237]     Train net output #1: loss = 0.25605 (* 1 = 0.25605 loss)
I1122 04:42:03.088052  5428 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1122 04:42:08.321487  5428 solver.cpp:218] Iteration 20700 (19.1113 iter/s, 5.23251s/100 iters), loss = 0.156362
I1122 04:42:08.321487  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:42:08.321487  5428 solver.cpp:237]     Train net output #1: loss = 0.156361 (* 1 = 0.156361 loss)
I1122 04:42:08.321487  5428 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1122 04:42:13.556197  5428 solver.cpp:218] Iteration 20800 (19.1052 iter/s, 5.23417s/100 iters), loss = 0.18489
I1122 04:42:13.556197  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:42:13.556197  5428 solver.cpp:237]     Train net output #1: loss = 0.18489 (* 1 = 0.18489 loss)
I1122 04:42:13.556197  5428 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1122 04:42:18.779057  5428 solver.cpp:218] Iteration 20900 (19.1486 iter/s, 5.22232s/100 iters), loss = 0.11231
I1122 04:42:18.779057  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 04:42:18.779057  5428 solver.cpp:237]     Train net output #1: loss = 0.11231 (* 1 = 0.11231 loss)
I1122 04:42:18.779057  5428 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1122 04:42:23.745121  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:42:23.951184  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_21000.caffemodel
I1122 04:42:23.965189  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_21000.solverstate
I1122 04:42:23.969190  5428 solver.cpp:330] Iteration 21000, Testing net (#0)
I1122 04:42:23.969190  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:42:25.227260 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:42:25.277267  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8973
I1122 04:42:25.277267  5428 solver.cpp:397]     Test net output #1: loss = 0.306195 (* 1 = 0.306195 loss)
I1122 04:42:25.327267  5428 solver.cpp:218] Iteration 21000 (15.2726 iter/s, 6.54765s/100 iters), loss = 0.16972
I1122 04:42:25.327267  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:42:25.327267  5428 solver.cpp:237]     Train net output #1: loss = 0.16972 (* 1 = 0.16972 loss)
I1122 04:42:25.327267  5428 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1122 04:42:30.555469  5428 solver.cpp:218] Iteration 21100 (19.1258 iter/s, 5.22854s/100 iters), loss = 0.177589
I1122 04:42:30.555469  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:42:30.555469  5428 solver.cpp:237]     Train net output #1: loss = 0.177589 (* 1 = 0.177589 loss)
I1122 04:42:30.555469  5428 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1122 04:42:35.776943  5428 solver.cpp:218] Iteration 21200 (19.1532 iter/s, 5.22106s/100 iters), loss = 0.176915
I1122 04:42:35.776943  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:42:35.776943  5428 solver.cpp:237]     Train net output #1: loss = 0.176915 (* 1 = 0.176915 loss)
I1122 04:42:35.776943  5428 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1122 04:42:41.002353  5428 solver.cpp:218] Iteration 21300 (19.1413 iter/s, 5.2243s/100 iters), loss = 0.208551
I1122 04:42:41.002353  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:42:41.002353  5428 solver.cpp:237]     Train net output #1: loss = 0.208551 (* 1 = 0.208551 loss)
I1122 04:42:41.002353  5428 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1122 04:42:46.228749  5428 solver.cpp:218] Iteration 21400 (19.1332 iter/s, 5.22653s/100 iters), loss = 0.100616
I1122 04:42:46.228749  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 04:42:46.228749  5428 solver.cpp:237]     Train net output #1: loss = 0.100616 (* 1 = 0.100616 loss)
I1122 04:42:46.228749  5428 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1122 04:42:51.198112  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:42:51.404135  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_21500.caffemodel
I1122 04:42:51.418138  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_21500.solverstate
I1122 04:42:51.422137  5428 solver.cpp:330] Iteration 21500, Testing net (#0)
I1122 04:42:51.422137  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:42:52.679262 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:42:52.729264  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8976
I1122 04:42:52.729264  5428 solver.cpp:397]     Test net output #1: loss = 0.306281 (* 1 = 0.306281 loss)
I1122 04:42:52.779265  5428 solver.cpp:218] Iteration 21500 (15.2665 iter/s, 6.55028s/100 iters), loss = 0.117713
I1122 04:42:52.779265  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:42:52.779265  5428 solver.cpp:237]     Train net output #1: loss = 0.117713 (* 1 = 0.117713 loss)
I1122 04:42:52.779265  5428 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1122 04:42:58.005640  5428 solver.cpp:218] Iteration 21600 (19.1379 iter/s, 5.22524s/100 iters), loss = 0.23107
I1122 04:42:58.005640  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:42:58.005640  5428 solver.cpp:237]     Train net output #1: loss = 0.23107 (* 1 = 0.23107 loss)
I1122 04:42:58.005640  5428 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1122 04:43:03.221976  5428 solver.cpp:218] Iteration 21700 (19.1719 iter/s, 5.21596s/100 iters), loss = 0.156493
I1122 04:43:03.221976  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:43:03.221976  5428 solver.cpp:237]     Train net output #1: loss = 0.156493 (* 1 = 0.156493 loss)
I1122 04:43:03.221976  5428 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1122 04:43:08.452419  5428 solver.cpp:218] Iteration 21800 (19.1182 iter/s, 5.2306s/100 iters), loss = 0.174838
I1122 04:43:08.452419  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:43:08.452419  5428 solver.cpp:237]     Train net output #1: loss = 0.174838 (* 1 = 0.174838 loss)
I1122 04:43:08.452419  5428 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1122 04:43:13.673795  5428 solver.cpp:218] Iteration 21900 (19.1554 iter/s, 5.22045s/100 iters), loss = 0.0698706
I1122 04:43:13.673795  5428 solver.cpp:237]     Train net output #0: accuracy_training = 1
I1122 04:43:13.673795  5428 solver.cpp:237]     Train net output #1: loss = 0.0698705 (* 1 = 0.0698705 loss)
I1122 04:43:13.673795  5428 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1122 04:43:18.641161  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:43:18.847178  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_22000.caffemodel
I1122 04:43:18.861179  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_22000.solverstate
I1122 04:43:18.865684  5428 solver.cpp:330] Iteration 22000, Testing net (#0)
I1122 04:43:18.866184  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:43:20.127352 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:43:20.177357  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8974
I1122 04:43:20.177357  5428 solver.cpp:397]     Test net output #1: loss = 0.306236 (* 1 = 0.306236 loss)
I1122 04:43:20.227360  5428 solver.cpp:218] Iteration 22000 (15.26 iter/s, 6.55309s/100 iters), loss = 0.101383
I1122 04:43:20.227360  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:43:20.227360  5428 solver.cpp:237]     Train net output #1: loss = 0.101383 (* 1 = 0.101383 loss)
I1122 04:43:20.227360  5428 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1122 04:43:20.227360  5428 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1122 04:43:25.460765  5428 solver.cpp:218] Iteration 22100 (19.109 iter/s, 5.23313s/100 iters), loss = 0.196963
I1122 04:43:25.460765  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:43:25.460765  5428 solver.cpp:237]     Train net output #1: loss = 0.196962 (* 1 = 0.196962 loss)
I1122 04:43:25.460765  5428 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1122 04:43:30.698166  5428 solver.cpp:218] Iteration 22200 (19.0953 iter/s, 5.23688s/100 iters), loss = 0.185547
I1122 04:43:30.698166  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 04:43:30.698166  5428 solver.cpp:237]     Train net output #1: loss = 0.185547 (* 1 = 0.185547 loss)
I1122 04:43:30.698166  5428 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1122 04:43:35.933521  5428 solver.cpp:218] Iteration 22300 (19.1004 iter/s, 5.2355s/100 iters), loss = 0.151035
I1122 04:43:35.933521  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:43:35.933521  5428 solver.cpp:237]     Train net output #1: loss = 0.151035 (* 1 = 0.151035 loss)
I1122 04:43:35.933521  5428 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1122 04:43:41.174124  5428 solver.cpp:218] Iteration 22400 (19.0848 iter/s, 5.23977s/100 iters), loss = 0.0857913
I1122 04:43:41.174124  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 04:43:41.174124  5428 solver.cpp:237]     Train net output #1: loss = 0.0857912 (* 1 = 0.0857912 loss)
I1122 04:43:41.174124  5428 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1122 04:43:46.160375  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:43:46.368446  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_22500.caffemodel
I1122 04:43:46.383448  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_22500.solverstate
I1122 04:43:46.387464  5428 solver.cpp:330] Iteration 22500, Testing net (#0)
I1122 04:43:46.387464  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:43:47.647342 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:43:47.698163  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8974
I1122 04:43:47.698163  5428 solver.cpp:397]     Test net output #1: loss = 0.306039 (* 1 = 0.306039 loss)
I1122 04:43:47.748147  5428 solver.cpp:218] Iteration 22500 (15.2116 iter/s, 6.57392s/100 iters), loss = 0.16711
I1122 04:43:47.748147  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:43:47.748147  5428 solver.cpp:237]     Train net output #1: loss = 0.16711 (* 1 = 0.16711 loss)
I1122 04:43:47.748147  5428 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1122 04:43:52.982560  5428 solver.cpp:218] Iteration 22600 (19.1074 iter/s, 5.23356s/100 iters), loss = 0.148009
I1122 04:43:52.982560  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:43:52.982560  5428 solver.cpp:237]     Train net output #1: loss = 0.148009 (* 1 = 0.148009 loss)
I1122 04:43:52.982560  5428 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1122 04:43:58.206691  5428 solver.cpp:218] Iteration 22700 (19.1405 iter/s, 5.22452s/100 iters), loss = 0.233662
I1122 04:43:58.207690  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 04:43:58.207690  5428 solver.cpp:237]     Train net output #1: loss = 0.233662 (* 1 = 0.233662 loss)
I1122 04:43:58.207690  5428 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1122 04:44:03.434067  5428 solver.cpp:218] Iteration 22800 (19.1336 iter/s, 5.2264s/100 iters), loss = 0.183886
I1122 04:44:03.434067  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:44:03.434067  5428 solver.cpp:237]     Train net output #1: loss = 0.183886 (* 1 = 0.183886 loss)
I1122 04:44:03.434067  5428 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1122 04:44:08.658062  5428 solver.cpp:218] Iteration 22900 (19.1425 iter/s, 5.22398s/100 iters), loss = 0.0946138
I1122 04:44:08.658062  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:44:08.658062  5428 solver.cpp:237]     Train net output #1: loss = 0.0946137 (* 1 = 0.0946137 loss)
I1122 04:44:08.658062  5428 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1122 04:44:13.646351  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:44:13.853989  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_23000.caffemodel
I1122 04:44:13.867988  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_23000.solverstate
I1122 04:44:13.871987  5428 solver.cpp:330] Iteration 23000, Testing net (#0)
I1122 04:44:13.871987  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:44:15.131100 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:44:15.182117  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8973
I1122 04:44:15.182117  5428 solver.cpp:397]     Test net output #1: loss = 0.306184 (* 1 = 0.306184 loss)
I1122 04:44:15.231156  5428 solver.cpp:218] Iteration 23000 (15.2141 iter/s, 6.57286s/100 iters), loss = 0.159804
I1122 04:44:15.232156  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:44:15.232156  5428 solver.cpp:237]     Train net output #1: loss = 0.159804 (* 1 = 0.159804 loss)
I1122 04:44:15.232156  5428 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1122 04:44:20.468686  5428 solver.cpp:218] Iteration 23100 (19.0964 iter/s, 5.23658s/100 iters), loss = 0.203589
I1122 04:44:20.468686  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:44:20.468686  5428 solver.cpp:237]     Train net output #1: loss = 0.203589 (* 1 = 0.203589 loss)
I1122 04:44:20.468686  5428 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1122 04:44:25.706625  5428 solver.cpp:218] Iteration 23200 (19.0932 iter/s, 5.23746s/100 iters), loss = 0.181741
I1122 04:44:25.706625  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:44:25.706625  5428 solver.cpp:237]     Train net output #1: loss = 0.181741 (* 1 = 0.181741 loss)
I1122 04:44:25.706625  5428 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1122 04:44:30.946166  5428 solver.cpp:218] Iteration 23300 (19.0862 iter/s, 5.23938s/100 iters), loss = 0.218673
I1122 04:44:30.946166  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 04:44:30.946166  5428 solver.cpp:237]     Train net output #1: loss = 0.218673 (* 1 = 0.218673 loss)
I1122 04:44:30.946166  5428 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1122 04:44:36.179008  5428 solver.cpp:218] Iteration 23400 (19.1102 iter/s, 5.2328s/100 iters), loss = 0.14911
I1122 04:44:36.180008  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:44:36.180008  5428 solver.cpp:237]     Train net output #1: loss = 0.14911 (* 1 = 0.14911 loss)
I1122 04:44:36.180008  5428 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1122 04:44:41.150254  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:44:41.356847  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_23500.caffemodel
I1122 04:44:41.371845  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_23500.solverstate
I1122 04:44:41.375844  5428 solver.cpp:330] Iteration 23500, Testing net (#0)
I1122 04:44:41.376844  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:44:42.633185 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:44:42.683183  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8976
I1122 04:44:42.683183  5428 solver.cpp:397]     Test net output #1: loss = 0.305979 (* 1 = 0.305979 loss)
I1122 04:44:42.732193  5428 solver.cpp:218] Iteration 23500 (15.2608 iter/s, 6.55275s/100 iters), loss = 0.18095
I1122 04:44:42.733194  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:44:42.733194  5428 solver.cpp:237]     Train net output #1: loss = 0.18095 (* 1 = 0.18095 loss)
I1122 04:44:42.733194  5428 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1122 04:44:47.956363  5428 solver.cpp:218] Iteration 23600 (19.1468 iter/s, 5.2228s/100 iters), loss = 0.167021
I1122 04:44:47.956363  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:44:47.956363  5428 solver.cpp:237]     Train net output #1: loss = 0.167021 (* 1 = 0.167021 loss)
I1122 04:44:47.956363  5428 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1122 04:44:53.178715  5428 solver.cpp:218] Iteration 23700 (19.1483 iter/s, 5.2224s/100 iters), loss = 0.151289
I1122 04:44:53.178715  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:44:53.178715  5428 solver.cpp:237]     Train net output #1: loss = 0.151289 (* 1 = 0.151289 loss)
I1122 04:44:53.178715  5428 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1122 04:44:58.400028  5428 solver.cpp:218] Iteration 23800 (19.152 iter/s, 5.22138s/100 iters), loss = 0.156033
I1122 04:44:58.401028  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:44:58.401028  5428 solver.cpp:237]     Train net output #1: loss = 0.156033 (* 1 = 0.156033 loss)
I1122 04:44:58.401028  5428 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1122 04:45:03.623381  5428 solver.cpp:218] Iteration 23900 (19.1472 iter/s, 5.22268s/100 iters), loss = 0.137572
I1122 04:45:03.623381  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:45:03.623381  5428 solver.cpp:237]     Train net output #1: loss = 0.137572 (* 1 = 0.137572 loss)
I1122 04:45:03.623381  5428 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1122 04:45:08.600741  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:45:08.806757  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_24000.caffemodel
I1122 04:45:08.820763  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_24000.solverstate
I1122 04:45:08.824764  5428 solver.cpp:330] Iteration 24000, Testing net (#0)
I1122 04:45:08.825767  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:45:10.087956 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:45:10.136962  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8977
I1122 04:45:10.137964  5428 solver.cpp:397]     Test net output #1: loss = 0.306072 (* 1 = 0.306072 loss)
I1122 04:45:10.186962  5428 solver.cpp:218] Iteration 24000 (15.2368 iter/s, 6.56307s/100 iters), loss = 0.187951
I1122 04:45:10.186962  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:45:10.186962  5428 solver.cpp:237]     Train net output #1: loss = 0.187951 (* 1 = 0.187951 loss)
I1122 04:45:10.186962  5428 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1122 04:45:15.409801  5428 solver.cpp:218] Iteration 24100 (19.1497 iter/s, 5.22201s/100 iters), loss = 0.232139
I1122 04:45:15.409801  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1122 04:45:15.409801  5428 solver.cpp:237]     Train net output #1: loss = 0.232139 (* 1 = 0.232139 loss)
I1122 04:45:15.409801  5428 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1122 04:45:20.641667  5428 solver.cpp:218] Iteration 24200 (19.1138 iter/s, 5.23182s/100 iters), loss = 0.175594
I1122 04:45:20.641667  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:45:20.641667  5428 solver.cpp:237]     Train net output #1: loss = 0.175594 (* 1 = 0.175594 loss)
I1122 04:45:20.641667  5428 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1122 04:45:25.872038  5428 solver.cpp:218] Iteration 24300 (19.1222 iter/s, 5.22952s/100 iters), loss = 0.17677
I1122 04:45:25.872038  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:45:25.872038  5428 solver.cpp:237]     Train net output #1: loss = 0.17677 (* 1 = 0.17677 loss)
I1122 04:45:25.872038  5428 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1122 04:45:31.089385  5428 solver.cpp:218] Iteration 24400 (19.1667 iter/s, 5.21738s/100 iters), loss = 0.117023
I1122 04:45:31.089385  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:45:31.089385  5428 solver.cpp:237]     Train net output #1: loss = 0.117023 (* 1 = 0.117023 loss)
I1122 04:45:31.089385  5428 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1122 04:45:36.063719  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:45:36.268729  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_24500.caffemodel
I1122 04:45:36.282729  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_24500.solverstate
I1122 04:45:36.287730  5428 solver.cpp:330] Iteration 24500, Testing net (#0)
I1122 04:45:36.287730  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:45:37.545848 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:45:37.595849  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8975
I1122 04:45:37.595849  5428 solver.cpp:397]     Test net output #1: loss = 0.306146 (* 1 = 0.306146 loss)
I1122 04:45:37.645853  5428 solver.cpp:218] Iteration 24500 (15.2524 iter/s, 6.55634s/100 iters), loss = 0.123457
I1122 04:45:37.645853  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:45:37.645853  5428 solver.cpp:237]     Train net output #1: loss = 0.123457 (* 1 = 0.123457 loss)
I1122 04:45:37.645853  5428 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1122 04:45:42.885181  5428 solver.cpp:218] Iteration 24600 (19.0895 iter/s, 5.23848s/100 iters), loss = 0.204057
I1122 04:45:42.885181  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:45:42.885181  5428 solver.cpp:237]     Train net output #1: loss = 0.204057 (* 1 = 0.204057 loss)
I1122 04:45:42.885181  5428 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1122 04:45:48.135515  5428 solver.cpp:218] Iteration 24700 (19.0468 iter/s, 5.25024s/100 iters), loss = 0.194728
I1122 04:45:48.135515  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:45:48.135515  5428 solver.cpp:237]     Train net output #1: loss = 0.194728 (* 1 = 0.194728 loss)
I1122 04:45:48.135515  5428 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1122 04:45:53.369082  5428 solver.cpp:218] Iteration 24800 (19.109 iter/s, 5.23313s/100 iters), loss = 0.162316
I1122 04:45:53.369082  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:45:53.369082  5428 solver.cpp:237]     Train net output #1: loss = 0.162315 (* 1 = 0.162315 loss)
I1122 04:45:53.369082  5428 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1122 04:45:58.612447  5428 solver.cpp:218] Iteration 24900 (19.0752 iter/s, 5.24241s/100 iters), loss = 0.114529
I1122 04:45:58.612447  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:45:58.612447  5428 solver.cpp:237]     Train net output #1: loss = 0.114529 (* 1 = 0.114529 loss)
I1122 04:45:58.612447  5428 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1122 04:46:03.608196  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:46:03.814302  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_25000.caffemodel
I1122 04:46:03.826303  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_25000.solverstate
I1122 04:46:03.831282  5428 solver.cpp:330] Iteration 25000, Testing net (#0)
I1122 04:46:03.831282  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:46:05.093480 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:46:05.142998  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8975
I1122 04:46:05.142998  5428 solver.cpp:397]     Test net output #1: loss = 0.306144 (* 1 = 0.306144 loss)
I1122 04:46:05.193507  5428 solver.cpp:218] Iteration 25000 (15.197 iter/s, 6.58025s/100 iters), loss = 0.165768
I1122 04:46:05.193507  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:46:05.193507  5428 solver.cpp:237]     Train net output #1: loss = 0.165768 (* 1 = 0.165768 loss)
I1122 04:46:05.193507  5428 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1122 04:46:10.445191  5428 solver.cpp:218] Iteration 25100 (19.0416 iter/s, 5.25166s/100 iters), loss = 0.206428
I1122 04:46:10.445191  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:46:10.445191  5428 solver.cpp:237]     Train net output #1: loss = 0.206428 (* 1 = 0.206428 loss)
I1122 04:46:10.445191  5428 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1122 04:46:15.692699  5428 solver.cpp:218] Iteration 25200 (19.0568 iter/s, 5.24747s/100 iters), loss = 0.130919
I1122 04:46:15.692699  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:46:15.692699  5428 solver.cpp:237]     Train net output #1: loss = 0.130919 (* 1 = 0.130919 loss)
I1122 04:46:15.692699  5428 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1122 04:46:20.927690  5428 solver.cpp:218] Iteration 25300 (19.1066 iter/s, 5.2338s/100 iters), loss = 0.149248
I1122 04:46:20.927690  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:46:20.927690  5428 solver.cpp:237]     Train net output #1: loss = 0.149248 (* 1 = 0.149248 loss)
I1122 04:46:20.927690  5428 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1122 04:46:26.153475  5428 solver.cpp:218] Iteration 25400 (19.1354 iter/s, 5.2259s/100 iters), loss = 0.108585
I1122 04:46:26.153475  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:46:26.153475  5428 solver.cpp:237]     Train net output #1: loss = 0.108585 (* 1 = 0.108585 loss)
I1122 04:46:26.153475  5428 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1122 04:46:31.127689  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:46:31.332641  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_25500.caffemodel
I1122 04:46:31.347628  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_25500.solverstate
I1122 04:46:31.351630  5428 solver.cpp:330] Iteration 25500, Testing net (#0)
I1122 04:46:31.351630  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:46:32.613214 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:46:32.662230  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8975
I1122 04:46:32.662230  5428 solver.cpp:397]     Test net output #1: loss = 0.306197 (* 1 = 0.306197 loss)
I1122 04:46:32.712877  5428 solver.cpp:218] Iteration 25500 (15.2466 iter/s, 6.55884s/100 iters), loss = 0.113898
I1122 04:46:32.712877  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:46:32.712877  5428 solver.cpp:237]     Train net output #1: loss = 0.113898 (* 1 = 0.113898 loss)
I1122 04:46:32.712877  5428 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1122 04:46:37.951911  5428 solver.cpp:218] Iteration 25600 (19.0905 iter/s, 5.23822s/100 iters), loss = 0.237289
I1122 04:46:37.951911  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:46:37.951911  5428 solver.cpp:237]     Train net output #1: loss = 0.237289 (* 1 = 0.237289 loss)
I1122 04:46:37.951911  5428 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1122 04:46:43.238296  5428 solver.cpp:218] Iteration 25700 (18.9163 iter/s, 5.28644s/100 iters), loss = 0.164907
I1122 04:46:43.238296  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:46:43.238296  5428 solver.cpp:237]     Train net output #1: loss = 0.164907 (* 1 = 0.164907 loss)
I1122 04:46:43.238296  5428 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1122 04:46:48.470674  5428 solver.cpp:218] Iteration 25800 (19.1143 iter/s, 5.23167s/100 iters), loss = 0.124531
I1122 04:46:48.470674  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:46:48.470674  5428 solver.cpp:237]     Train net output #1: loss = 0.124531 (* 1 = 0.124531 loss)
I1122 04:46:48.470674  5428 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1122 04:46:53.712657  5428 solver.cpp:218] Iteration 25900 (19.0767 iter/s, 5.24201s/100 iters), loss = 0.105152
I1122 04:46:53.712657  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:46:53.712657  5428 solver.cpp:237]     Train net output #1: loss = 0.105152 (* 1 = 0.105152 loss)
I1122 04:46:53.712657  5428 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1122 04:46:58.688758  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:46:58.895545  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_26000.caffemodel
I1122 04:46:58.910529  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_26000.solverstate
I1122 04:46:58.914549  5428 solver.cpp:330] Iteration 26000, Testing net (#0)
I1122 04:46:58.914549  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:47:00.173138 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:47:00.222649  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8973
I1122 04:47:00.223650  5428 solver.cpp:397]     Test net output #1: loss = 0.30611 (* 1 = 0.30611 loss)
I1122 04:47:00.273669  5428 solver.cpp:218] Iteration 26000 (15.2437 iter/s, 6.56008s/100 iters), loss = 0.181808
I1122 04:47:00.273669  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:47:00.273669  5428 solver.cpp:237]     Train net output #1: loss = 0.181808 (* 1 = 0.181808 loss)
I1122 04:47:00.273669  5428 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1122 04:47:05.503734  5428 solver.cpp:218] Iteration 26100 (19.1214 iter/s, 5.22974s/100 iters), loss = 0.189058
I1122 04:47:05.503734  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:47:05.503734  5428 solver.cpp:237]     Train net output #1: loss = 0.189058 (* 1 = 0.189058 loss)
I1122 04:47:05.503734  5428 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1122 04:47:10.723817  5428 solver.cpp:218] Iteration 26200 (19.1558 iter/s, 5.22036s/100 iters), loss = 0.158285
I1122 04:47:10.723817  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:47:10.723817  5428 solver.cpp:237]     Train net output #1: loss = 0.158285 (* 1 = 0.158285 loss)
I1122 04:47:10.724828  5428 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1122 04:47:15.934957  5428 solver.cpp:218] Iteration 26300 (19.191 iter/s, 5.21077s/100 iters), loss = 0.1461
I1122 04:47:15.935957  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:47:15.935957  5428 solver.cpp:237]     Train net output #1: loss = 0.1461 (* 1 = 0.1461 loss)
I1122 04:47:15.935957  5428 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1122 04:47:21.138273  5428 solver.cpp:218] Iteration 26400 (19.2234 iter/s, 5.20199s/100 iters), loss = 0.119478
I1122 04:47:21.138273  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:47:21.138273  5428 solver.cpp:237]     Train net output #1: loss = 0.119478 (* 1 = 0.119478 loss)
I1122 04:47:21.138273  5428 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1122 04:47:26.083168  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:47:26.289178  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_26500.caffemodel
I1122 04:47:26.302685  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_26500.solverstate
I1122 04:47:26.306684  5428 solver.cpp:330] Iteration 26500, Testing net (#0)
I1122 04:47:26.307684  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:47:27.560792 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:47:27.610796  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8973
I1122 04:47:27.610796  5428 solver.cpp:397]     Test net output #1: loss = 0.306277 (* 1 = 0.306277 loss)
I1122 04:47:27.660795  5428 solver.cpp:218] Iteration 26500 (15.3304 iter/s, 6.52299s/100 iters), loss = 0.142813
I1122 04:47:27.661795  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:47:27.661795  5428 solver.cpp:237]     Train net output #1: loss = 0.142813 (* 1 = 0.142813 loss)
I1122 04:47:27.661795  5428 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1122 04:47:32.882215  5428 solver.cpp:218] Iteration 26600 (19.1567 iter/s, 5.2201s/100 iters), loss = 0.210423
I1122 04:47:32.882215  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:47:32.882215  5428 solver.cpp:237]     Train net output #1: loss = 0.210423 (* 1 = 0.210423 loss)
I1122 04:47:32.882215  5428 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1122 04:47:38.096541  5428 solver.cpp:218] Iteration 26700 (19.1789 iter/s, 5.21408s/100 iters), loss = 0.130488
I1122 04:47:38.096541  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:47:38.096541  5428 solver.cpp:237]     Train net output #1: loss = 0.130488 (* 1 = 0.130488 loss)
I1122 04:47:38.096541  5428 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1122 04:47:43.322895  5428 solver.cpp:218] Iteration 26800 (19.136 iter/s, 5.22574s/100 iters), loss = 0.172745
I1122 04:47:43.322895  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:47:43.322895  5428 solver.cpp:237]     Train net output #1: loss = 0.172745 (* 1 = 0.172745 loss)
I1122 04:47:43.322895  5428 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1122 04:47:48.552388  5428 solver.cpp:218] Iteration 26900 (19.1237 iter/s, 5.2291s/100 iters), loss = 0.115777
I1122 04:47:48.552388  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:47:48.552388  5428 solver.cpp:237]     Train net output #1: loss = 0.115777 (* 1 = 0.115777 loss)
I1122 04:47:48.552388  5428 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1122 04:47:53.504686  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:47:53.709697  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_27000.caffemodel
I1122 04:47:53.723696  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_27000.solverstate
I1122 04:47:53.727697  5428 solver.cpp:330] Iteration 27000, Testing net (#0)
I1122 04:47:53.727697  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:47:54.980832 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:47:55.029834  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8974
I1122 04:47:55.029834  5428 solver.cpp:397]     Test net output #1: loss = 0.306069 (* 1 = 0.306069 loss)
I1122 04:47:55.079833  5428 solver.cpp:218] Iteration 27000 (15.3204 iter/s, 6.52723s/100 iters), loss = 0.118396
I1122 04:47:55.079833  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:47:55.079833  5428 solver.cpp:237]     Train net output #1: loss = 0.118396 (* 1 = 0.118396 loss)
I1122 04:47:55.079833  5428 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1122 04:47:55.079833  5428 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1122 04:48:00.293673  5428 solver.cpp:218] Iteration 27100 (19.1819 iter/s, 5.21326s/100 iters), loss = 0.20132
I1122 04:48:00.293673  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:48:00.293673  5428 solver.cpp:237]     Train net output #1: loss = 0.20132 (* 1 = 0.20132 loss)
I1122 04:48:00.293673  5428 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1122 04:48:05.501528  5428 solver.cpp:218] Iteration 27200 (19.2034 iter/s, 5.20741s/100 iters), loss = 0.170489
I1122 04:48:05.501528  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:48:05.501528  5428 solver.cpp:237]     Train net output #1: loss = 0.170489 (* 1 = 0.170489 loss)
I1122 04:48:05.501528  5428 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1122 04:48:10.703886  5428 solver.cpp:218] Iteration 27300 (19.2205 iter/s, 5.20279s/100 iters), loss = 0.180299
I1122 04:48:10.703886  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:48:10.703886  5428 solver.cpp:237]     Train net output #1: loss = 0.180299 (* 1 = 0.180299 loss)
I1122 04:48:10.703886  5428 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1122 04:48:15.916249  5428 solver.cpp:218] Iteration 27400 (19.1892 iter/s, 5.21126s/100 iters), loss = 0.0607858
I1122 04:48:15.916249  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 04:48:15.916249  5428 solver.cpp:237]     Train net output #1: loss = 0.0607857 (* 1 = 0.0607857 loss)
I1122 04:48:15.916249  5428 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1122 04:48:20.865623  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:48:21.070636  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_27500.caffemodel
I1122 04:48:21.085141  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_27500.solverstate
I1122 04:48:21.089141  5428 solver.cpp:330] Iteration 27500, Testing net (#0)
I1122 04:48:21.089141  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:48:22.346773 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:48:22.396775  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8974
I1122 04:48:22.396775  5428 solver.cpp:397]     Test net output #1: loss = 0.30624 (* 1 = 0.30624 loss)
I1122 04:48:22.445770  5428 solver.cpp:218] Iteration 27500 (15.3149 iter/s, 6.52961s/100 iters), loss = 0.151344
I1122 04:48:22.445770  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:48:22.445770  5428 solver.cpp:237]     Train net output #1: loss = 0.151343 (* 1 = 0.151343 loss)
I1122 04:48:22.445770  5428 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1122 04:48:27.670145  5428 solver.cpp:218] Iteration 27600 (19.1444 iter/s, 5.22347s/100 iters), loss = 0.191635
I1122 04:48:27.670145  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:48:27.670145  5428 solver.cpp:237]     Train net output #1: loss = 0.191635 (* 1 = 0.191635 loss)
I1122 04:48:27.670145  5428 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1122 04:48:32.890009  5428 solver.cpp:218] Iteration 27700 (19.1583 iter/s, 5.21968s/100 iters), loss = 0.175295
I1122 04:48:32.890009  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:48:32.890009  5428 solver.cpp:237]     Train net output #1: loss = 0.175295 (* 1 = 0.175295 loss)
I1122 04:48:32.890511  5428 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1122 04:48:38.110853  5428 solver.cpp:218] Iteration 27800 (19.154 iter/s, 5.22083s/100 iters), loss = 0.143498
I1122 04:48:38.110853  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:48:38.110853  5428 solver.cpp:237]     Train net output #1: loss = 0.143498 (* 1 = 0.143498 loss)
I1122 04:48:38.110853  5428 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1122 04:48:43.327190  5428 solver.cpp:218] Iteration 27900 (19.1736 iter/s, 5.2155s/100 iters), loss = 0.139243
I1122 04:48:43.327190  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:48:43.327190  5428 solver.cpp:237]     Train net output #1: loss = 0.139243 (* 1 = 0.139243 loss)
I1122 04:48:43.327190  5428 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1122 04:48:48.291538  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:48:48.497563  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_28000.caffemodel
I1122 04:48:48.511564  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_28000.solverstate
I1122 04:48:48.515564  5428 solver.cpp:330] Iteration 28000, Testing net (#0)
I1122 04:48:48.515564  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:48:49.769887 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:48:49.819900  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8976
I1122 04:48:49.820888  5428 solver.cpp:397]     Test net output #1: loss = 0.306189 (* 1 = 0.306189 loss)
I1122 04:48:49.870892  5428 solver.cpp:218] Iteration 28000 (15.2837 iter/s, 6.5429s/100 iters), loss = 0.10276
I1122 04:48:49.870892  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 04:48:49.870892  5428 solver.cpp:237]     Train net output #1: loss = 0.10276 (* 1 = 0.10276 loss)
I1122 04:48:49.870892  5428 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1122 04:48:55.074312  5428 solver.cpp:218] Iteration 28100 (19.2166 iter/s, 5.20382s/100 iters), loss = 0.176855
I1122 04:48:55.074312  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:48:55.074312  5428 solver.cpp:237]     Train net output #1: loss = 0.176855 (* 1 = 0.176855 loss)
I1122 04:48:55.074312  5428 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1122 04:49:00.282699  5428 solver.cpp:218] Iteration 28200 (19.2029 iter/s, 5.20754s/100 iters), loss = 0.157846
I1122 04:49:00.282699  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:49:00.282699  5428 solver.cpp:237]     Train net output #1: loss = 0.157846 (* 1 = 0.157846 loss)
I1122 04:49:00.282699  5428 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1122 04:49:05.499084  5428 solver.cpp:218] Iteration 28300 (19.1732 iter/s, 5.21562s/100 iters), loss = 0.141025
I1122 04:49:05.499084  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1122 04:49:05.499084  5428 solver.cpp:237]     Train net output #1: loss = 0.141025 (* 1 = 0.141025 loss)
I1122 04:49:05.499084  5428 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1122 04:49:10.715337  5428 solver.cpp:218] Iteration 28400 (19.1722 iter/s, 5.21587s/100 iters), loss = 0.0865482
I1122 04:49:10.715337  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 04:49:10.715337  5428 solver.cpp:237]     Train net output #1: loss = 0.0865482 (* 1 = 0.0865482 loss)
I1122 04:49:10.715337  5428 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1122 04:49:15.664189  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:49:15.868188  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_28500.caffemodel
I1122 04:49:15.882189  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_28500.solverstate
I1122 04:49:15.886189  5428 solver.cpp:330] Iteration 28500, Testing net (#0)
I1122 04:49:15.886189  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:49:17.141798 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:49:17.192301  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8974
I1122 04:49:17.192301  5428 solver.cpp:397]     Test net output #1: loss = 0.306117 (* 1 = 0.306117 loss)
I1122 04:49:17.241801  5428 solver.cpp:218] Iteration 28500 (15.3229 iter/s, 6.52616s/100 iters), loss = 0.139979
I1122 04:49:17.241801  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:49:17.241801  5428 solver.cpp:237]     Train net output #1: loss = 0.139979 (* 1 = 0.139979 loss)
I1122 04:49:17.241801  5428 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1122 04:49:22.463167  5428 solver.cpp:218] Iteration 28600 (19.1526 iter/s, 5.22122s/100 iters), loss = 0.258279
I1122 04:49:22.463167  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:49:22.463167  5428 solver.cpp:237]     Train net output #1: loss = 0.258279 (* 1 = 0.258279 loss)
I1122 04:49:22.463167  5428 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1122 04:49:27.682528  5428 solver.cpp:218] Iteration 28700 (19.1617 iter/s, 5.21874s/100 iters), loss = 0.187934
I1122 04:49:27.682528  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:49:27.682528  5428 solver.cpp:237]     Train net output #1: loss = 0.187934 (* 1 = 0.187934 loss)
I1122 04:49:27.682528  5428 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1122 04:49:32.890902  5428 solver.cpp:218] Iteration 28800 (19.2016 iter/s, 5.2079s/100 iters), loss = 0.186811
I1122 04:49:32.890902  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:49:32.890902  5428 solver.cpp:237]     Train net output #1: loss = 0.186811 (* 1 = 0.186811 loss)
I1122 04:49:32.890902  5428 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1122 04:49:38.097781  5428 solver.cpp:218] Iteration 28900 (19.2071 iter/s, 5.20642s/100 iters), loss = 0.105038
I1122 04:49:38.097781  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1122 04:49:38.097781  5428 solver.cpp:237]     Train net output #1: loss = 0.105038 (* 1 = 0.105038 loss)
I1122 04:49:38.097781  5428 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1122 04:49:43.058643  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:49:43.264654  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_29000.caffemodel
I1122 04:49:43.278654  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_29000.solverstate
I1122 04:49:43.282655  5428 solver.cpp:330] Iteration 29000, Testing net (#0)
I1122 04:49:43.282655  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:49:44.537773 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:49:44.587774  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8972
I1122 04:49:44.587774  5428 solver.cpp:397]     Test net output #1: loss = 0.306252 (* 1 = 0.306252 loss)
I1122 04:49:44.637780  5428 solver.cpp:218] Iteration 29000 (15.2911 iter/s, 6.53974s/100 iters), loss = 0.144665
I1122 04:49:44.637780  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:49:44.637780  5428 solver.cpp:237]     Train net output #1: loss = 0.144665 (* 1 = 0.144665 loss)
I1122 04:49:44.637780  5428 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1122 04:49:49.857131  5428 solver.cpp:218] Iteration 29100 (19.1621 iter/s, 5.21864s/100 iters), loss = 0.262061
I1122 04:49:49.857131  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:49:49.857131  5428 solver.cpp:237]     Train net output #1: loss = 0.262061 (* 1 = 0.262061 loss)
I1122 04:49:49.857131  5428 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1122 04:49:55.069444  5428 solver.cpp:218] Iteration 29200 (19.1861 iter/s, 5.2121s/100 iters), loss = 0.169691
I1122 04:49:55.069444  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:49:55.069444  5428 solver.cpp:237]     Train net output #1: loss = 0.169692 (* 1 = 0.169692 loss)
I1122 04:49:55.069444  5428 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1122 04:50:00.293308  5428 solver.cpp:218] Iteration 29300 (19.1458 iter/s, 5.22306s/100 iters), loss = 0.163021
I1122 04:50:00.293308  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1122 04:50:00.293308  5428 solver.cpp:237]     Train net output #1: loss = 0.163021 (* 1 = 0.163021 loss)
I1122 04:50:00.293308  5428 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1122 04:50:05.507143  5428 solver.cpp:218] Iteration 29400 (19.1779 iter/s, 5.21434s/100 iters), loss = 0.122791
I1122 04:50:05.508141  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1122 04:50:05.508141  5428 solver.cpp:237]     Train net output #1: loss = 0.122791 (* 1 = 0.122791 loss)
I1122 04:50:05.508141  5428 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1122 04:50:10.461482  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:50:10.667491  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_29500.caffemodel
I1122 04:50:10.682492  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_29500.solverstate
I1122 04:50:10.686491  5428 solver.cpp:330] Iteration 29500, Testing net (#0)
I1122 04:50:10.686491  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:50:11.942584 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:50:11.992588  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8974
I1122 04:50:11.992588  5428 solver.cpp:397]     Test net output #1: loss = 0.306066 (* 1 = 0.306066 loss)
I1122 04:50:12.042589  5428 solver.cpp:218] Iteration 29500 (15.3036 iter/s, 6.53443s/100 iters), loss = 0.160912
I1122 04:50:12.042589  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1122 04:50:12.042589  5428 solver.cpp:237]     Train net output #1: loss = 0.160912 (* 1 = 0.160912 loss)
I1122 04:50:12.042589  5428 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1122 04:50:17.247520  5428 solver.cpp:218] Iteration 29600 (19.2144 iter/s, 5.20442s/100 iters), loss = 0.12356
I1122 04:50:17.247520  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:50:17.247520  5428 solver.cpp:237]     Train net output #1: loss = 0.12356 (* 1 = 0.12356 loss)
I1122 04:50:17.247520  5428 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1122 04:50:22.454818  5428 solver.cpp:218] Iteration 29700 (19.2061 iter/s, 5.20667s/100 iters), loss = 0.137655
I1122 04:50:22.454818  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1122 04:50:22.454818  5428 solver.cpp:237]     Train net output #1: loss = 0.137655 (* 1 = 0.137655 loss)
I1122 04:50:22.454818  5428 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1122 04:50:27.670773  5428 solver.cpp:218] Iteration 29800 (19.1724 iter/s, 5.21583s/100 iters), loss = 0.170263
I1122 04:50:27.670773  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1122 04:50:27.670773  5428 solver.cpp:237]     Train net output #1: loss = 0.170263 (* 1 = 0.170263 loss)
I1122 04:50:27.670773  5428 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1122 04:50:32.880985  5428 solver.cpp:218] Iteration 29900 (19.1953 iter/s, 5.20962s/100 iters), loss = 0.0775622
I1122 04:50:32.880985  5428 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1122 04:50:32.880985  5428 solver.cpp:237]     Train net output #1: loss = 0.0775623 (* 1 = 0.0775623 loss)
I1122 04:50:32.880985  5428 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1122 04:50:37.834820  2416 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:50:38.039386  5428 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_30000.caffemodel
I1122 04:50:38.053382  5428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_13L_iter_30000.solverstate
I1122 04:50:38.072382  5428 solver.cpp:310] Iteration 30000, loss = 0.138308
I1122 04:50:38.072382  5428 solver.cpp:330] Iteration 30000, Testing net (#0)
I1122 04:50:38.072382  5428 net.cpp:676] Ignoring source layer accuracy_training
I1122 04:50:39.328119 17944 data_layer.cpp:73] Restarting data prefetching from start.
I1122 04:50:39.379118  5428 solver.cpp:397]     Test net output #0: accuracy = 0.8974
I1122 04:50:39.379118  5428 solver.cpp:397]     Test net output #1: loss = 0.306106 (* 1 = 0.306106 loss)
I1122 04:50:39.379118  5428 solver.cpp:315] Optimization Done.
I1122 04:50:39.379118  5428 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 

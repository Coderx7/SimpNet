
G:\Caffe\examples\cifar10>REM go to the caffe root 

G:\Caffe\examples\cifar10>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar10/cifar10_full_relu_solver_bn.prototxt 
I1202 13:05:53.860129 13416 caffe.cpp:219] Using GPUs 0
I1202 13:05:54.045132 13416 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1202 13:05:54.345692 13416 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1202 13:05:54.362195 13416 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 30000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar10/snaps/slimnet_300k_9L"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar10/cifar10_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 9500
stepvalue: 15300
stepvalue: 19500
stepvalue: 22000
stepvalue: 27000
type: "AdaDelta"
I1202 13:05:54.362696 13416 solver.cpp:87] Creating training net from net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1202 13:05:54.362696 13416 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1202 13:05:54.362696 13416 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1202 13:05:54.362696 13416 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1202 13:05:54.362696 13416 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1202 13:05:54.362696 13416 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1202 13:05:54.362696 13416 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1202 13:05:54.362696 13416 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1202 13:05:54.362696 13416 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1202 13:05:54.362696 13416 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1202 13:05:54.362696 13416 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1202 13:05:54.362696 13416 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1202 13:05:54.362696 13416 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1202 13:05:54.362696 13416 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1202 13:05:54.362696 13416 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_9L_Simple_NoGrpCon_NoDrp_300kv2"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 44
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 44
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 57
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 57
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 57
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 57
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 75
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 85
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1202 13:05:54.399024 13416 layer_factory.cpp:58] Creating layer cifar
I1202 13:05:54.537403 13416 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_train_lmdb_zeropad
I1202 13:05:54.537403 13416 net.cpp:84] Creating Layer cifar
I1202 13:05:54.537403 13416 net.cpp:380] cifar -> data
I1202 13:05:54.537403 13416 net.cpp:380] cifar -> label
I1202 13:05:54.538430 13416 data_layer.cpp:45] output data size: 100,3,32,32
I1202 13:05:54.545409 13416 net.cpp:122] Setting up cifar
I1202 13:05:54.545409 13416 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1202 13:05:54.545409 13416 net.cpp:129] Top shape: 100 (100)
I1202 13:05:54.545409 13416 net.cpp:137] Memory required for data: 1229200
I1202 13:05:54.545409 13416 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1202 13:05:54.545409 13416 net.cpp:84] Creating Layer label_cifar_1_split
I1202 13:05:54.545409 13416 net.cpp:406] label_cifar_1_split <- label
I1202 13:05:54.545409 13416 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1202 13:05:54.545409 13416 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1202 13:05:54.545409 13416 net.cpp:122] Setting up label_cifar_1_split
I1202 13:05:54.545409 13416 net.cpp:129] Top shape: 100 (100)
I1202 13:05:54.545409 13416 net.cpp:129] Top shape: 100 (100)
I1202 13:05:54.545409 13416 net.cpp:137] Memory required for data: 1230000
I1202 13:05:54.545409 13416 layer_factory.cpp:58] Creating layer conv1
I1202 13:05:54.545409 13416 net.cpp:84] Creating Layer conv1
I1202 13:05:54.545409 13416 net.cpp:406] conv1 <- data
I1202 13:05:54.545409 13416 net.cpp:380] conv1 -> conv1
I1202 13:05:54.547410 17664 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1202 13:05:54.805128 13416 net.cpp:122] Setting up conv1
I1202 13:05:54.805128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.805128 13416 net.cpp:137] Memory required for data: 19252400
I1202 13:05:54.805128 13416 layer_factory.cpp:58] Creating layer bn1
I1202 13:05:54.805128 13416 net.cpp:84] Creating Layer bn1
I1202 13:05:54.805128 13416 net.cpp:406] bn1 <- conv1
I1202 13:05:54.805128 13416 net.cpp:367] bn1 -> conv1 (in-place)
I1202 13:05:54.805128 13416 net.cpp:122] Setting up bn1
I1202 13:05:54.805128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.805128 13416 net.cpp:137] Memory required for data: 37274800
I1202 13:05:54.805128 13416 layer_factory.cpp:58] Creating layer scale1
I1202 13:05:54.805128 13416 net.cpp:84] Creating Layer scale1
I1202 13:05:54.805128 13416 net.cpp:406] scale1 <- conv1
I1202 13:05:54.805128 13416 net.cpp:367] scale1 -> conv1 (in-place)
I1202 13:05:54.805128 13416 layer_factory.cpp:58] Creating layer scale1
I1202 13:05:54.805128 13416 net.cpp:122] Setting up scale1
I1202 13:05:54.805128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.805128 13416 net.cpp:137] Memory required for data: 55297200
I1202 13:05:54.805128 13416 layer_factory.cpp:58] Creating layer relu1
I1202 13:05:54.806128 13416 net.cpp:84] Creating Layer relu1
I1202 13:05:54.806128 13416 net.cpp:406] relu1 <- conv1
I1202 13:05:54.806128 13416 net.cpp:367] relu1 -> conv1 (in-place)
I1202 13:05:54.806128 13416 net.cpp:122] Setting up relu1
I1202 13:05:54.806128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.806128 13416 net.cpp:137] Memory required for data: 73319600
I1202 13:05:54.806128 13416 layer_factory.cpp:58] Creating layer conv2
I1202 13:05:54.806128 13416 net.cpp:84] Creating Layer conv2
I1202 13:05:54.806128 13416 net.cpp:406] conv2 <- conv1
I1202 13:05:54.806128 13416 net.cpp:380] conv2 -> conv2
I1202 13:05:54.808128 13416 net.cpp:122] Setting up conv2
I1202 13:05:54.808128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.808128 13416 net.cpp:137] Memory required for data: 91342000
I1202 13:05:54.808128 13416 layer_factory.cpp:58] Creating layer bn2
I1202 13:05:54.808128 13416 net.cpp:84] Creating Layer bn2
I1202 13:05:54.808128 13416 net.cpp:406] bn2 <- conv2
I1202 13:05:54.808128 13416 net.cpp:367] bn2 -> conv2 (in-place)
I1202 13:05:54.808128 13416 net.cpp:122] Setting up bn2
I1202 13:05:54.808128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.808128 13416 net.cpp:137] Memory required for data: 109364400
I1202 13:05:54.808128 13416 layer_factory.cpp:58] Creating layer scale2
I1202 13:05:54.808128 13416 net.cpp:84] Creating Layer scale2
I1202 13:05:54.808128 13416 net.cpp:406] scale2 <- conv2
I1202 13:05:54.808128 13416 net.cpp:367] scale2 -> conv2 (in-place)
I1202 13:05:54.808128 13416 layer_factory.cpp:58] Creating layer scale2
I1202 13:05:54.808128 13416 net.cpp:122] Setting up scale2
I1202 13:05:54.808128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.808128 13416 net.cpp:137] Memory required for data: 127386800
I1202 13:05:54.808128 13416 layer_factory.cpp:58] Creating layer relu2
I1202 13:05:54.808128 13416 net.cpp:84] Creating Layer relu2
I1202 13:05:54.808128 13416 net.cpp:406] relu2 <- conv2
I1202 13:05:54.808128 13416 net.cpp:367] relu2 -> conv2 (in-place)
I1202 13:05:54.808128 13416 net.cpp:122] Setting up relu2
I1202 13:05:54.808128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.808128 13416 net.cpp:137] Memory required for data: 145409200
I1202 13:05:54.808128 13416 layer_factory.cpp:58] Creating layer conv2_2
I1202 13:05:54.809128 13416 net.cpp:84] Creating Layer conv2_2
I1202 13:05:54.809128 13416 net.cpp:406] conv2_2 <- conv2
I1202 13:05:54.809128 13416 net.cpp:380] conv2_2 -> conv2_2
I1202 13:05:54.810128 13416 net.cpp:122] Setting up conv2_2
I1202 13:05:54.810128 13416 net.cpp:129] Top shape: 100 57 32 32 (5836800)
I1202 13:05:54.810128 13416 net.cpp:137] Memory required for data: 168756400
I1202 13:05:54.810128 13416 layer_factory.cpp:58] Creating layer bn2_2
I1202 13:05:54.810128 13416 net.cpp:84] Creating Layer bn2_2
I1202 13:05:54.810128 13416 net.cpp:406] bn2_2 <- conv2_2
I1202 13:05:54.810128 13416 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1202 13:05:54.810128 13416 net.cpp:122] Setting up bn2_2
I1202 13:05:54.810128 13416 net.cpp:129] Top shape: 100 57 32 32 (5836800)
I1202 13:05:54.810128 13416 net.cpp:137] Memory required for data: 192103600
I1202 13:05:54.810128 13416 layer_factory.cpp:58] Creating layer scale2_2
I1202 13:05:54.810128 13416 net.cpp:84] Creating Layer scale2_2
I1202 13:05:54.810128 13416 net.cpp:406] scale2_2 <- conv2_2
I1202 13:05:54.810128 13416 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1202 13:05:54.810128 13416 layer_factory.cpp:58] Creating layer scale2_2
I1202 13:05:54.810128 13416 net.cpp:122] Setting up scale2_2
I1202 13:05:54.810128 13416 net.cpp:129] Top shape: 100 57 32 32 (5836800)
I1202 13:05:54.810128 13416 net.cpp:137] Memory required for data: 215450800
I1202 13:05:54.810128 13416 layer_factory.cpp:58] Creating layer relu2_2
I1202 13:05:54.810128 13416 net.cpp:84] Creating Layer relu2_2
I1202 13:05:54.810128 13416 net.cpp:406] relu2_2 <- conv2_2
I1202 13:05:54.810128 13416 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1202 13:05:54.810128 13416 net.cpp:122] Setting up relu2_2
I1202 13:05:54.810128 13416 net.cpp:129] Top shape: 100 57 32 32 (5836800)
I1202 13:05:54.810128 13416 net.cpp:137] Memory required for data: 238798000
I1202 13:05:54.810128 13416 layer_factory.cpp:58] Creating layer pool2_1
I1202 13:05:54.810128 13416 net.cpp:84] Creating Layer pool2_1
I1202 13:05:54.810128 13416 net.cpp:406] pool2_1 <- conv2_2
I1202 13:05:54.810128 13416 net.cpp:380] pool2_1 -> pool2_1
I1202 13:05:54.810128 13416 net.cpp:122] Setting up pool2_1
I1202 13:05:54.810128 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.810128 13416 net.cpp:137] Memory required for data: 244634800
I1202 13:05:54.810128 13416 layer_factory.cpp:58] Creating layer conv3
I1202 13:05:54.810128 13416 net.cpp:84] Creating Layer conv3
I1202 13:05:54.810128 13416 net.cpp:406] conv3 <- pool2_1
I1202 13:05:54.810128 13416 net.cpp:380] conv3 -> conv3
I1202 13:05:54.812127 13416 net.cpp:122] Setting up conv3
I1202 13:05:54.812127 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.812127 13416 net.cpp:137] Memory required for data: 250471600
I1202 13:05:54.812127 13416 layer_factory.cpp:58] Creating layer bn3
I1202 13:05:54.812127 13416 net.cpp:84] Creating Layer bn3
I1202 13:05:54.812127 13416 net.cpp:406] bn3 <- conv3
I1202 13:05:54.812127 13416 net.cpp:367] bn3 -> conv3 (in-place)
I1202 13:05:54.812127 13416 net.cpp:122] Setting up bn3
I1202 13:05:54.812127 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.812127 13416 net.cpp:137] Memory required for data: 256308400
I1202 13:05:54.812127 13416 layer_factory.cpp:58] Creating layer scale3
I1202 13:05:54.812127 13416 net.cpp:84] Creating Layer scale3
I1202 13:05:54.812127 13416 net.cpp:406] scale3 <- conv3
I1202 13:05:54.812127 13416 net.cpp:367] scale3 -> conv3 (in-place)
I1202 13:05:54.812127 13416 layer_factory.cpp:58] Creating layer scale3
I1202 13:05:54.812127 13416 net.cpp:122] Setting up scale3
I1202 13:05:54.812127 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.812127 13416 net.cpp:137] Memory required for data: 262145200
I1202 13:05:54.812127 13416 layer_factory.cpp:58] Creating layer relu3
I1202 13:05:54.812127 13416 net.cpp:84] Creating Layer relu3
I1202 13:05:54.812127 13416 net.cpp:406] relu3 <- conv3
I1202 13:05:54.812127 13416 net.cpp:367] relu3 -> conv3 (in-place)
I1202 13:05:54.812127 13416 net.cpp:122] Setting up relu3
I1202 13:05:54.812127 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.812127 13416 net.cpp:137] Memory required for data: 267982000
I1202 13:05:54.812127 13416 layer_factory.cpp:58] Creating layer conv4
I1202 13:05:54.812127 13416 net.cpp:84] Creating Layer conv4
I1202 13:05:54.812127 13416 net.cpp:406] conv4 <- conv3
I1202 13:05:54.812127 13416 net.cpp:380] conv4 -> conv4
I1202 13:05:54.814128 13416 net.cpp:122] Setting up conv4
I1202 13:05:54.814128 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.814128 13416 net.cpp:137] Memory required for data: 273818800
I1202 13:05:54.814128 13416 layer_factory.cpp:58] Creating layer bn4
I1202 13:05:54.814128 13416 net.cpp:84] Creating Layer bn4
I1202 13:05:54.814128 13416 net.cpp:406] bn4 <- conv4
I1202 13:05:54.814128 13416 net.cpp:367] bn4 -> conv4 (in-place)
I1202 13:05:54.814128 13416 net.cpp:122] Setting up bn4
I1202 13:05:54.814128 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.814128 13416 net.cpp:137] Memory required for data: 279655600
I1202 13:05:54.814128 13416 layer_factory.cpp:58] Creating layer scale4
I1202 13:05:54.814128 13416 net.cpp:84] Creating Layer scale4
I1202 13:05:54.814128 13416 net.cpp:406] scale4 <- conv4
I1202 13:05:54.814128 13416 net.cpp:367] scale4 -> conv4 (in-place)
I1202 13:05:54.814128 13416 layer_factory.cpp:58] Creating layer scale4
I1202 13:05:54.814128 13416 net.cpp:122] Setting up scale4
I1202 13:05:54.814128 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.814128 13416 net.cpp:137] Memory required for data: 285492400
I1202 13:05:54.814128 13416 layer_factory.cpp:58] Creating layer relu4
I1202 13:05:54.815129 13416 net.cpp:84] Creating Layer relu4
I1202 13:05:54.815129 13416 net.cpp:406] relu4 <- conv4
I1202 13:05:54.815129 13416 net.cpp:367] relu4 -> conv4 (in-place)
I1202 13:05:54.815129 13416 net.cpp:122] Setting up relu4
I1202 13:05:54.815129 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.815129 13416 net.cpp:137] Memory required for data: 291329200
I1202 13:05:54.815129 13416 layer_factory.cpp:58] Creating layer conv4_1
I1202 13:05:54.815129 13416 net.cpp:84] Creating Layer conv4_1
I1202 13:05:54.815129 13416 net.cpp:406] conv4_1 <- conv4
I1202 13:05:54.815129 13416 net.cpp:380] conv4_1 -> conv4_1
I1202 13:05:54.816128 13416 net.cpp:122] Setting up conv4_1
I1202 13:05:54.816128 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.816128 13416 net.cpp:137] Memory required for data: 297166000
I1202 13:05:54.816128 13416 layer_factory.cpp:58] Creating layer bn4_1
I1202 13:05:54.816128 13416 net.cpp:84] Creating Layer bn4_1
I1202 13:05:54.816128 13416 net.cpp:406] bn4_1 <- conv4_1
I1202 13:05:54.816128 13416 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1202 13:05:54.817128 13416 net.cpp:122] Setting up bn4_1
I1202 13:05:54.817128 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.817128 13416 net.cpp:137] Memory required for data: 303002800
I1202 13:05:54.817128 13416 layer_factory.cpp:58] Creating layer scale4_1
I1202 13:05:54.817128 13416 net.cpp:84] Creating Layer scale4_1
I1202 13:05:54.817128 13416 net.cpp:406] scale4_1 <- conv4_1
I1202 13:05:54.817128 13416 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1202 13:05:54.817128 13416 layer_factory.cpp:58] Creating layer scale4_1
I1202 13:05:54.817128 13416 net.cpp:122] Setting up scale4_1
I1202 13:05:54.817128 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.817128 13416 net.cpp:137] Memory required for data: 308839600
I1202 13:05:54.817128 13416 layer_factory.cpp:58] Creating layer relu4_1
I1202 13:05:54.817128 13416 net.cpp:84] Creating Layer relu4_1
I1202 13:05:54.817128 13416 net.cpp:406] relu4_1 <- conv4_1
I1202 13:05:54.817128 13416 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1202 13:05:54.817128 13416 net.cpp:122] Setting up relu4_1
I1202 13:05:54.817128 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.817128 13416 net.cpp:137] Memory required for data: 314676400
I1202 13:05:54.817128 13416 layer_factory.cpp:58] Creating layer conv4_2
I1202 13:05:54.817128 13416 net.cpp:84] Creating Layer conv4_2
I1202 13:05:54.817128 13416 net.cpp:406] conv4_2 <- conv4_1
I1202 13:05:54.817128 13416 net.cpp:380] conv4_2 -> conv4_2
I1202 13:05:54.819128 13416 net.cpp:122] Setting up conv4_2
I1202 13:05:54.819128 13416 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1202 13:05:54.819128 13416 net.cpp:137] Memory required for data: 322356400
I1202 13:05:54.819128 13416 layer_factory.cpp:58] Creating layer bn4_2
I1202 13:05:54.819128 13416 net.cpp:84] Creating Layer bn4_2
I1202 13:05:54.819128 13416 net.cpp:406] bn4_2 <- conv4_2
I1202 13:05:54.819128 13416 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1202 13:05:54.819128 13416 net.cpp:122] Setting up bn4_2
I1202 13:05:54.819128 13416 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1202 13:05:54.819128 13416 net.cpp:137] Memory required for data: 330036400
I1202 13:05:54.819128 13416 layer_factory.cpp:58] Creating layer scale4_2
I1202 13:05:54.819128 13416 net.cpp:84] Creating Layer scale4_2
I1202 13:05:54.819128 13416 net.cpp:406] scale4_2 <- conv4_2
I1202 13:05:54.819128 13416 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1202 13:05:54.819128 13416 layer_factory.cpp:58] Creating layer scale4_2
I1202 13:05:54.819128 13416 net.cpp:122] Setting up scale4_2
I1202 13:05:54.819128 13416 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1202 13:05:54.819128 13416 net.cpp:137] Memory required for data: 337716400
I1202 13:05:54.819128 13416 layer_factory.cpp:58] Creating layer relu4_2
I1202 13:05:54.819128 13416 net.cpp:84] Creating Layer relu4_2
I1202 13:05:54.819128 13416 net.cpp:406] relu4_2 <- conv4_2
I1202 13:05:54.819128 13416 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1202 13:05:54.820128 13416 net.cpp:122] Setting up relu4_2
I1202 13:05:54.820128 13416 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1202 13:05:54.820128 13416 net.cpp:137] Memory required for data: 345396400
I1202 13:05:54.820128 13416 layer_factory.cpp:58] Creating layer pool4_2
I1202 13:05:54.820128 13416 net.cpp:84] Creating Layer pool4_2
I1202 13:05:54.820128 13416 net.cpp:406] pool4_2 <- conv4_2
I1202 13:05:54.820128 13416 net.cpp:380] pool4_2 -> pool4_2
I1202 13:05:54.820128 13416 net.cpp:122] Setting up pool4_2
I1202 13:05:54.820128 13416 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1202 13:05:54.820128 13416 net.cpp:137] Memory required for data: 347316400
I1202 13:05:54.820128 13416 layer_factory.cpp:58] Creating layer conv11
I1202 13:05:54.820128 13416 net.cpp:84] Creating Layer conv11
I1202 13:05:54.820128 13416 net.cpp:406] conv11 <- pool4_2
I1202 13:05:54.820128 13416 net.cpp:380] conv11 -> conv11
I1202 13:05:54.821128 13416 net.cpp:122] Setting up conv11
I1202 13:05:54.821128 13416 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1202 13:05:54.821128 13416 net.cpp:137] Memory required for data: 349492400
I1202 13:05:54.821128 13416 layer_factory.cpp:58] Creating layer bn_conv11
I1202 13:05:54.822129 13416 net.cpp:84] Creating Layer bn_conv11
I1202 13:05:54.822129 13416 net.cpp:406] bn_conv11 <- conv11
I1202 13:05:54.822129 13416 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1202 13:05:54.822129 13416 net.cpp:122] Setting up bn_conv11
I1202 13:05:54.822129 13416 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1202 13:05:54.822129 13416 net.cpp:137] Memory required for data: 351668400
I1202 13:05:54.822129 13416 layer_factory.cpp:58] Creating layer scale_conv11
I1202 13:05:54.822129 13416 net.cpp:84] Creating Layer scale_conv11
I1202 13:05:54.822129 13416 net.cpp:406] scale_conv11 <- conv11
I1202 13:05:54.822129 13416 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1202 13:05:54.822129 13416 layer_factory.cpp:58] Creating layer scale_conv11
I1202 13:05:54.822129 13416 net.cpp:122] Setting up scale_conv11
I1202 13:05:54.822129 13416 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1202 13:05:54.822129 13416 net.cpp:137] Memory required for data: 353844400
I1202 13:05:54.822129 13416 layer_factory.cpp:58] Creating layer relu_conv11
I1202 13:05:54.822129 13416 net.cpp:84] Creating Layer relu_conv11
I1202 13:05:54.822129 13416 net.cpp:406] relu_conv11 <- conv11
I1202 13:05:54.822129 13416 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1202 13:05:54.822129 13416 net.cpp:122] Setting up relu_conv11
I1202 13:05:54.822129 13416 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1202 13:05:54.822129 13416 net.cpp:137] Memory required for data: 356020400
I1202 13:05:54.822129 13416 layer_factory.cpp:58] Creating layer conv12
I1202 13:05:54.822129 13416 net.cpp:84] Creating Layer conv12
I1202 13:05:54.822129 13416 net.cpp:406] conv12 <- conv11
I1202 13:05:54.822129 13416 net.cpp:380] conv12 -> conv12
I1202 13:05:54.824127 13416 net.cpp:122] Setting up conv12
I1202 13:05:54.824127 13416 net.cpp:129] Top shape: 100 96 8 8 (614400)
I1202 13:05:54.824127 13416 net.cpp:137] Memory required for data: 358478000
I1202 13:05:54.824127 13416 layer_factory.cpp:58] Creating layer bn_conv12
I1202 13:05:54.824127 13416 net.cpp:84] Creating Layer bn_conv12
I1202 13:05:54.824127 13416 net.cpp:406] bn_conv12 <- conv12
I1202 13:05:54.824127 13416 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1202 13:05:54.824127 13416 net.cpp:122] Setting up bn_conv12
I1202 13:05:54.824127 13416 net.cpp:129] Top shape: 100 96 8 8 (614400)
I1202 13:05:54.824127 13416 net.cpp:137] Memory required for data: 360935600
I1202 13:05:54.824127 13416 layer_factory.cpp:58] Creating layer scale_conv12
I1202 13:05:54.824127 13416 net.cpp:84] Creating Layer scale_conv12
I1202 13:05:54.824127 13416 net.cpp:406] scale_conv12 <- conv12
I1202 13:05:54.824127 13416 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1202 13:05:54.824127 13416 layer_factory.cpp:58] Creating layer scale_conv12
I1202 13:05:54.825129 13416 net.cpp:122] Setting up scale_conv12
I1202 13:05:54.825129 13416 net.cpp:129] Top shape: 100 96 8 8 (614400)
I1202 13:05:54.825129 13416 net.cpp:137] Memory required for data: 363393200
I1202 13:05:54.825129 13416 layer_factory.cpp:58] Creating layer relu_conv12
I1202 13:05:54.825129 13416 net.cpp:84] Creating Layer relu_conv12
I1202 13:05:54.825129 13416 net.cpp:406] relu_conv12 <- conv12
I1202 13:05:54.825129 13416 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1202 13:05:54.825129 13416 net.cpp:122] Setting up relu_conv12
I1202 13:05:54.825129 13416 net.cpp:129] Top shape: 100 96 8 8 (614400)
I1202 13:05:54.825129 13416 net.cpp:137] Memory required for data: 365850800
I1202 13:05:54.825129 13416 layer_factory.cpp:58] Creating layer poolcp6
I1202 13:05:54.825129 13416 net.cpp:84] Creating Layer poolcp6
I1202 13:05:54.825129 13416 net.cpp:406] poolcp6 <- conv12
I1202 13:05:54.825129 13416 net.cpp:380] poolcp6 -> poolcp6
I1202 13:05:54.825129 13416 net.cpp:122] Setting up poolcp6
I1202 13:05:54.825129 13416 net.cpp:129] Top shape: 100 96 1 1 (9600)
I1202 13:05:54.825129 13416 net.cpp:137] Memory required for data: 365889200
I1202 13:05:54.825129 13416 layer_factory.cpp:58] Creating layer ip1
I1202 13:05:54.825129 13416 net.cpp:84] Creating Layer ip1
I1202 13:05:54.825129 13416 net.cpp:406] ip1 <- poolcp6
I1202 13:05:54.825129 13416 net.cpp:380] ip1 -> ip1
I1202 13:05:54.825129 13416 net.cpp:122] Setting up ip1
I1202 13:05:54.825129 13416 net.cpp:129] Top shape: 100 10 (1000)
I1202 13:05:54.825129 13416 net.cpp:137] Memory required for data: 365893200
I1202 13:05:54.825129 13416 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1202 13:05:54.825129 13416 net.cpp:84] Creating Layer ip1_ip1_0_split
I1202 13:05:54.825129 13416 net.cpp:406] ip1_ip1_0_split <- ip1
I1202 13:05:54.825129 13416 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1202 13:05:54.825129 13416 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1202 13:05:54.825129 13416 net.cpp:122] Setting up ip1_ip1_0_split
I1202 13:05:54.825129 13416 net.cpp:129] Top shape: 100 10 (1000)
I1202 13:05:54.825129 13416 net.cpp:129] Top shape: 100 10 (1000)
I1202 13:05:54.825129 13416 net.cpp:137] Memory required for data: 365901200
I1202 13:05:54.825129 13416 layer_factory.cpp:58] Creating layer accuracy_training
I1202 13:05:54.825129 13416 net.cpp:84] Creating Layer accuracy_training
I1202 13:05:54.825129 13416 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1202 13:05:54.825129 13416 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1202 13:05:54.825129 13416 net.cpp:380] accuracy_training -> accuracy_training
I1202 13:05:54.825129 13416 net.cpp:122] Setting up accuracy_training
I1202 13:05:54.825129 13416 net.cpp:129] Top shape: (1)
I1202 13:05:54.825129 13416 net.cpp:137] Memory required for data: 365901204
I1202 13:05:54.825129 13416 layer_factory.cpp:58] Creating layer loss
I1202 13:05:54.825129 13416 net.cpp:84] Creating Layer loss
I1202 13:05:54.825129 13416 net.cpp:406] loss <- ip1_ip1_0_split_1
I1202 13:05:54.825129 13416 net.cpp:406] loss <- label_cifar_1_split_1
I1202 13:05:54.825129 13416 net.cpp:380] loss -> loss
I1202 13:05:54.825129 13416 layer_factory.cpp:58] Creating layer loss
I1202 13:05:54.826128 13416 net.cpp:122] Setting up loss
I1202 13:05:54.826128 13416 net.cpp:129] Top shape: (1)
I1202 13:05:54.826128 13416 net.cpp:132]     with loss weight 1
I1202 13:05:54.826128 13416 net.cpp:137] Memory required for data: 365901208
I1202 13:05:54.826128 13416 net.cpp:198] loss needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:200] accuracy_training does not need backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] ip1 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] poolcp6 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] relu_conv12 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] scale_conv12 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] bn_conv12 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] conv12 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] relu_conv11 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] scale_conv11 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] bn_conv11 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] conv11 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] pool4_2 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] relu4_2 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] scale4_2 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] bn4_2 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] conv4_2 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] relu4_1 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] scale4_1 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] bn4_1 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] conv4_1 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] relu4 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] scale4 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] bn4 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] conv4 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] relu3 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] scale3 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] bn3 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] conv3 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] pool2_1 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] relu2_2 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] scale2_2 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] bn2_2 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] conv2_2 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] relu2 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] scale2 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] bn2 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] conv2 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] relu1 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] scale1 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] bn1 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:198] conv1 needs backward computation.
I1202 13:05:54.826128 13416 net.cpp:200] label_cifar_1_split does not need backward computation.
I1202 13:05:54.826128 13416 net.cpp:200] cifar does not need backward computation.
I1202 13:05:54.826128 13416 net.cpp:242] This network produces output accuracy_training
I1202 13:05:54.826128 13416 net.cpp:242] This network produces output loss
I1202 13:05:54.826128 13416 net.cpp:255] Network initialization done.
I1202 13:05:54.827128 13416 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1202 13:05:54.827128 13416 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1202 13:05:54.827128 13416 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_relu_train_test_bn.prototxt
I1202 13:05:54.827128 13416 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1202 13:05:54.827128 13416 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1202 13:05:54.827128 13416 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1202 13:05:54.827128 13416 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1202 13:05:54.827128 13416 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1202 13:05:54.827128 13416 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1202 13:05:54.827128 13416 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1202 13:05:54.827128 13416 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1202 13:05:54.827128 13416 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1202 13:05:54.827128 13416 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1202 13:05:54.827128 13416 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1202 13:05:54.827128 13416 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_SimpleNet_GP_9L_Simple_NoGrpCon_NoDrp_300kv2"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb_zeropad"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 44
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 44
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 57
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 57
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 57
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 57
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 75
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 85
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1202 13:05:54.827128 13416 layer_factory.cpp:58] Creating layer cifar
I1202 13:05:54.835127 13416 db_lmdb.cpp:40] Opened lmdb examples/cifar10/cifar10_test_lmdb_zeropad
I1202 13:05:54.835127 13416 net.cpp:84] Creating Layer cifar
I1202 13:05:54.835127 13416 net.cpp:380] cifar -> data
I1202 13:05:54.835127 13416 net.cpp:380] cifar -> label
I1202 13:05:54.835127 13416 data_layer.cpp:45] output data size: 100,3,32,32
I1202 13:05:54.841136 13416 net.cpp:122] Setting up cifar
I1202 13:05:54.841136 13416 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1202 13:05:54.841136 13416 net.cpp:129] Top shape: 100 (100)
I1202 13:05:54.841136 13416 net.cpp:137] Memory required for data: 1229200
I1202 13:05:54.841136 13416 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1202 13:05:54.841136 13416 net.cpp:84] Creating Layer label_cifar_1_split
I1202 13:05:54.841136 13416 net.cpp:406] label_cifar_1_split <- label
I1202 13:05:54.841136 13416 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1202 13:05:54.841136 13416 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1202 13:05:54.842128 13416 net.cpp:122] Setting up label_cifar_1_split
I1202 13:05:54.842128 13416 net.cpp:129] Top shape: 100 (100)
I1202 13:05:54.842128 13416 net.cpp:129] Top shape: 100 (100)
I1202 13:05:54.842128 13416 net.cpp:137] Memory required for data: 1230000
I1202 13:05:54.842128 13416 layer_factory.cpp:58] Creating layer conv1
I1202 13:05:54.842128 13416 net.cpp:84] Creating Layer conv1
I1202 13:05:54.842128 13416 net.cpp:406] conv1 <- data
I1202 13:05:54.842128 13416 net.cpp:380] conv1 -> conv1
I1202 13:05:54.843128 13416 net.cpp:122] Setting up conv1
I1202 13:05:54.843128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.843128 13416 net.cpp:137] Memory required for data: 19252400
I1202 13:05:54.843128 13416 layer_factory.cpp:58] Creating layer bn1
I1202 13:05:54.843128  6724 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1202 13:05:54.843128 13416 net.cpp:84] Creating Layer bn1
I1202 13:05:54.843128 13416 net.cpp:406] bn1 <- conv1
I1202 13:05:54.843128 13416 net.cpp:367] bn1 -> conv1 (in-place)
I1202 13:05:54.843128 13416 net.cpp:122] Setting up bn1
I1202 13:05:54.843128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.843128 13416 net.cpp:137] Memory required for data: 37274800
I1202 13:05:54.843128 13416 layer_factory.cpp:58] Creating layer scale1
I1202 13:05:54.843128 13416 net.cpp:84] Creating Layer scale1
I1202 13:05:54.843128 13416 net.cpp:406] scale1 <- conv1
I1202 13:05:54.843128 13416 net.cpp:367] scale1 -> conv1 (in-place)
I1202 13:05:54.843128 13416 layer_factory.cpp:58] Creating layer scale1
I1202 13:05:54.843128 13416 net.cpp:122] Setting up scale1
I1202 13:05:54.843128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.843128 13416 net.cpp:137] Memory required for data: 55297200
I1202 13:05:54.843128 13416 layer_factory.cpp:58] Creating layer relu1
I1202 13:05:54.843128 13416 net.cpp:84] Creating Layer relu1
I1202 13:05:54.843128 13416 net.cpp:406] relu1 <- conv1
I1202 13:05:54.843128 13416 net.cpp:367] relu1 -> conv1 (in-place)
I1202 13:05:54.844128 13416 net.cpp:122] Setting up relu1
I1202 13:05:54.844128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.844128 13416 net.cpp:137] Memory required for data: 73319600
I1202 13:05:54.844128 13416 layer_factory.cpp:58] Creating layer conv2
I1202 13:05:54.844128 13416 net.cpp:84] Creating Layer conv2
I1202 13:05:54.844128 13416 net.cpp:406] conv2 <- conv1
I1202 13:05:54.844128 13416 net.cpp:380] conv2 -> conv2
I1202 13:05:54.845129 13416 net.cpp:122] Setting up conv2
I1202 13:05:54.845129 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.845129 13416 net.cpp:137] Memory required for data: 91342000
I1202 13:05:54.845129 13416 layer_factory.cpp:58] Creating layer bn2
I1202 13:05:54.845129 13416 net.cpp:84] Creating Layer bn2
I1202 13:05:54.845129 13416 net.cpp:406] bn2 <- conv2
I1202 13:05:54.845129 13416 net.cpp:367] bn2 -> conv2 (in-place)
I1202 13:05:54.846128 13416 net.cpp:122] Setting up bn2
I1202 13:05:54.846128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.846128 13416 net.cpp:137] Memory required for data: 109364400
I1202 13:05:54.846128 13416 layer_factory.cpp:58] Creating layer scale2
I1202 13:05:54.846128 13416 net.cpp:84] Creating Layer scale2
I1202 13:05:54.846128 13416 net.cpp:406] scale2 <- conv2
I1202 13:05:54.846128 13416 net.cpp:367] scale2 -> conv2 (in-place)
I1202 13:05:54.846128 13416 layer_factory.cpp:58] Creating layer scale2
I1202 13:05:54.846128 13416 net.cpp:122] Setting up scale2
I1202 13:05:54.846128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.846128 13416 net.cpp:137] Memory required for data: 127386800
I1202 13:05:54.846128 13416 layer_factory.cpp:58] Creating layer relu2
I1202 13:05:54.846128 13416 net.cpp:84] Creating Layer relu2
I1202 13:05:54.846128 13416 net.cpp:406] relu2 <- conv2
I1202 13:05:54.846128 13416 net.cpp:367] relu2 -> conv2 (in-place)
I1202 13:05:54.846128 13416 net.cpp:122] Setting up relu2
I1202 13:05:54.846128 13416 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1202 13:05:54.846128 13416 net.cpp:137] Memory required for data: 145409200
I1202 13:05:54.846128 13416 layer_factory.cpp:58] Creating layer conv2_2
I1202 13:05:54.846128 13416 net.cpp:84] Creating Layer conv2_2
I1202 13:05:54.846128 13416 net.cpp:406] conv2_2 <- conv2
I1202 13:05:54.846128 13416 net.cpp:380] conv2_2 -> conv2_2
I1202 13:05:54.848134 13416 net.cpp:122] Setting up conv2_2
I1202 13:05:54.848134 13416 net.cpp:129] Top shape: 100 57 32 32 (5836800)
I1202 13:05:54.848134 13416 net.cpp:137] Memory required for data: 168756400
I1202 13:05:54.848134 13416 layer_factory.cpp:58] Creating layer bn2_2
I1202 13:05:54.848134 13416 net.cpp:84] Creating Layer bn2_2
I1202 13:05:54.848134 13416 net.cpp:406] bn2_2 <- conv2_2
I1202 13:05:54.848634 13416 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1202 13:05:54.848634 13416 net.cpp:122] Setting up bn2_2
I1202 13:05:54.848634 13416 net.cpp:129] Top shape: 100 57 32 32 (5836800)
I1202 13:05:54.848634 13416 net.cpp:137] Memory required for data: 192103600
I1202 13:05:54.848634 13416 layer_factory.cpp:58] Creating layer scale2_2
I1202 13:05:54.848634 13416 net.cpp:84] Creating Layer scale2_2
I1202 13:05:54.848634 13416 net.cpp:406] scale2_2 <- conv2_2
I1202 13:05:54.848634 13416 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1202 13:05:54.848634 13416 layer_factory.cpp:58] Creating layer scale2_2
I1202 13:05:54.848634 13416 net.cpp:122] Setting up scale2_2
I1202 13:05:54.848634 13416 net.cpp:129] Top shape: 100 57 32 32 (5836800)
I1202 13:05:54.848634 13416 net.cpp:137] Memory required for data: 215450800
I1202 13:05:54.849134 13416 layer_factory.cpp:58] Creating layer relu2_2
I1202 13:05:54.849134 13416 net.cpp:84] Creating Layer relu2_2
I1202 13:05:54.849134 13416 net.cpp:406] relu2_2 <- conv2_2
I1202 13:05:54.849134 13416 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1202 13:05:54.849134 13416 net.cpp:122] Setting up relu2_2
I1202 13:05:54.849134 13416 net.cpp:129] Top shape: 100 57 32 32 (5836800)
I1202 13:05:54.849134 13416 net.cpp:137] Memory required for data: 238798000
I1202 13:05:54.849134 13416 layer_factory.cpp:58] Creating layer pool2_1
I1202 13:05:54.849134 13416 net.cpp:84] Creating Layer pool2_1
I1202 13:05:54.849134 13416 net.cpp:406] pool2_1 <- conv2_2
I1202 13:05:54.849134 13416 net.cpp:380] pool2_1 -> pool2_1
I1202 13:05:54.849634 13416 net.cpp:122] Setting up pool2_1
I1202 13:05:54.849634 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.849634 13416 net.cpp:137] Memory required for data: 244634800
I1202 13:05:54.849634 13416 layer_factory.cpp:58] Creating layer conv3
I1202 13:05:54.849634 13416 net.cpp:84] Creating Layer conv3
I1202 13:05:54.849634 13416 net.cpp:406] conv3 <- pool2_1
I1202 13:05:54.849634 13416 net.cpp:380] conv3 -> conv3
I1202 13:05:54.851133 13416 net.cpp:122] Setting up conv3
I1202 13:05:54.851133 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.851133 13416 net.cpp:137] Memory required for data: 250471600
I1202 13:05:54.851133 13416 layer_factory.cpp:58] Creating layer bn3
I1202 13:05:54.851133 13416 net.cpp:84] Creating Layer bn3
I1202 13:05:54.851133 13416 net.cpp:406] bn3 <- conv3
I1202 13:05:54.851133 13416 net.cpp:367] bn3 -> conv3 (in-place)
I1202 13:05:54.851133 13416 net.cpp:122] Setting up bn3
I1202 13:05:54.851133 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.851133 13416 net.cpp:137] Memory required for data: 256308400
I1202 13:05:54.851133 13416 layer_factory.cpp:58] Creating layer scale3
I1202 13:05:54.851634 13416 net.cpp:84] Creating Layer scale3
I1202 13:05:54.851634 13416 net.cpp:406] scale3 <- conv3
I1202 13:05:54.851634 13416 net.cpp:367] scale3 -> conv3 (in-place)
I1202 13:05:54.851634 13416 layer_factory.cpp:58] Creating layer scale3
I1202 13:05:54.851634 13416 net.cpp:122] Setting up scale3
I1202 13:05:54.851634 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.851634 13416 net.cpp:137] Memory required for data: 262145200
I1202 13:05:54.851634 13416 layer_factory.cpp:58] Creating layer relu3
I1202 13:05:54.851634 13416 net.cpp:84] Creating Layer relu3
I1202 13:05:54.851634 13416 net.cpp:406] relu3 <- conv3
I1202 13:05:54.851634 13416 net.cpp:367] relu3 -> conv3 (in-place)
I1202 13:05:54.851634 13416 net.cpp:122] Setting up relu3
I1202 13:05:54.851634 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.851634 13416 net.cpp:137] Memory required for data: 267982000
I1202 13:05:54.851634 13416 layer_factory.cpp:58] Creating layer conv4
I1202 13:05:54.851634 13416 net.cpp:84] Creating Layer conv4
I1202 13:05:54.851634 13416 net.cpp:406] conv4 <- conv3
I1202 13:05:54.851634 13416 net.cpp:380] conv4 -> conv4
I1202 13:05:54.853634 13416 net.cpp:122] Setting up conv4
I1202 13:05:54.853634 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.853634 13416 net.cpp:137] Memory required for data: 273818800
I1202 13:05:54.853634 13416 layer_factory.cpp:58] Creating layer bn4
I1202 13:05:54.853634 13416 net.cpp:84] Creating Layer bn4
I1202 13:05:54.853634 13416 net.cpp:406] bn4 <- conv4
I1202 13:05:54.853634 13416 net.cpp:367] bn4 -> conv4 (in-place)
I1202 13:05:54.853634 13416 net.cpp:122] Setting up bn4
I1202 13:05:54.853634 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.853634 13416 net.cpp:137] Memory required for data: 279655600
I1202 13:05:54.853634 13416 layer_factory.cpp:58] Creating layer scale4
I1202 13:05:54.853634 13416 net.cpp:84] Creating Layer scale4
I1202 13:05:54.853634 13416 net.cpp:406] scale4 <- conv4
I1202 13:05:54.853634 13416 net.cpp:367] scale4 -> conv4 (in-place)
I1202 13:05:54.854135 13416 layer_factory.cpp:58] Creating layer scale4
I1202 13:05:54.854135 13416 net.cpp:122] Setting up scale4
I1202 13:05:54.854135 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.854135 13416 net.cpp:137] Memory required for data: 285492400
I1202 13:05:54.854135 13416 layer_factory.cpp:58] Creating layer relu4
I1202 13:05:54.854135 13416 net.cpp:84] Creating Layer relu4
I1202 13:05:54.854135 13416 net.cpp:406] relu4 <- conv4
I1202 13:05:54.854135 13416 net.cpp:367] relu4 -> conv4 (in-place)
I1202 13:05:54.854135 13416 net.cpp:122] Setting up relu4
I1202 13:05:54.854135 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.854135 13416 net.cpp:137] Memory required for data: 291329200
I1202 13:05:54.854135 13416 layer_factory.cpp:58] Creating layer conv4_1
I1202 13:05:54.854135 13416 net.cpp:84] Creating Layer conv4_1
I1202 13:05:54.854135 13416 net.cpp:406] conv4_1 <- conv4
I1202 13:05:54.854135 13416 net.cpp:380] conv4_1 -> conv4_1
I1202 13:05:54.856134 13416 net.cpp:122] Setting up conv4_1
I1202 13:05:54.856134 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.856134 13416 net.cpp:137] Memory required for data: 297166000
I1202 13:05:54.856134 13416 layer_factory.cpp:58] Creating layer bn4_1
I1202 13:05:54.856134 13416 net.cpp:84] Creating Layer bn4_1
I1202 13:05:54.856134 13416 net.cpp:406] bn4_1 <- conv4_1
I1202 13:05:54.856134 13416 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1202 13:05:54.856134 13416 net.cpp:122] Setting up bn4_1
I1202 13:05:54.856134 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.856134 13416 net.cpp:137] Memory required for data: 303002800
I1202 13:05:54.856134 13416 layer_factory.cpp:58] Creating layer scale4_1
I1202 13:05:54.856134 13416 net.cpp:84] Creating Layer scale4_1
I1202 13:05:54.856134 13416 net.cpp:406] scale4_1 <- conv4_1
I1202 13:05:54.856134 13416 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1202 13:05:54.856134 13416 layer_factory.cpp:58] Creating layer scale4_1
I1202 13:05:54.856134 13416 net.cpp:122] Setting up scale4_1
I1202 13:05:54.856134 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.856634 13416 net.cpp:137] Memory required for data: 308839600
I1202 13:05:54.856634 13416 layer_factory.cpp:58] Creating layer relu4_1
I1202 13:05:54.856634 13416 net.cpp:84] Creating Layer relu4_1
I1202 13:05:54.856634 13416 net.cpp:406] relu4_1 <- conv4_1
I1202 13:05:54.856634 13416 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1202 13:05:54.857134 13416 net.cpp:122] Setting up relu4_1
I1202 13:05:54.857134 13416 net.cpp:129] Top shape: 100 57 16 16 (1459200)
I1202 13:05:54.857134 13416 net.cpp:137] Memory required for data: 314676400
I1202 13:05:54.857134 13416 layer_factory.cpp:58] Creating layer conv4_2
I1202 13:05:54.857134 13416 net.cpp:84] Creating Layer conv4_2
I1202 13:05:54.857134 13416 net.cpp:406] conv4_2 <- conv4_1
I1202 13:05:54.857134 13416 net.cpp:380] conv4_2 -> conv4_2
I1202 13:05:54.858134 13416 net.cpp:122] Setting up conv4_2
I1202 13:05:54.858134 13416 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1202 13:05:54.858134 13416 net.cpp:137] Memory required for data: 322356400
I1202 13:05:54.858134 13416 layer_factory.cpp:58] Creating layer bn4_2
I1202 13:05:54.858134 13416 net.cpp:84] Creating Layer bn4_2
I1202 13:05:54.858134 13416 net.cpp:406] bn4_2 <- conv4_2
I1202 13:05:54.858134 13416 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1202 13:05:54.858634 13416 net.cpp:122] Setting up bn4_2
I1202 13:05:54.858634 13416 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1202 13:05:54.858634 13416 net.cpp:137] Memory required for data: 330036400
I1202 13:05:54.858634 13416 layer_factory.cpp:58] Creating layer scale4_2
I1202 13:05:54.858634 13416 net.cpp:84] Creating Layer scale4_2
I1202 13:05:54.858634 13416 net.cpp:406] scale4_2 <- conv4_2
I1202 13:05:54.858634 13416 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1202 13:05:54.858634 13416 layer_factory.cpp:58] Creating layer scale4_2
I1202 13:05:54.858634 13416 net.cpp:122] Setting up scale4_2
I1202 13:05:54.858634 13416 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1202 13:05:54.858634 13416 net.cpp:137] Memory required for data: 337716400
I1202 13:05:54.858634 13416 layer_factory.cpp:58] Creating layer relu4_2
I1202 13:05:54.858634 13416 net.cpp:84] Creating Layer relu4_2
I1202 13:05:54.858634 13416 net.cpp:406] relu4_2 <- conv4_2
I1202 13:05:54.858634 13416 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1202 13:05:54.859134 13416 net.cpp:122] Setting up relu4_2
I1202 13:05:54.859134 13416 net.cpp:129] Top shape: 100 75 16 16 (1920000)
I1202 13:05:54.859134 13416 net.cpp:137] Memory required for data: 345396400
I1202 13:05:54.859134 13416 layer_factory.cpp:58] Creating layer pool4_2
I1202 13:05:54.859134 13416 net.cpp:84] Creating Layer pool4_2
I1202 13:05:54.859134 13416 net.cpp:406] pool4_2 <- conv4_2
I1202 13:05:54.859134 13416 net.cpp:380] pool4_2 -> pool4_2
I1202 13:05:54.859134 13416 net.cpp:122] Setting up pool4_2
I1202 13:05:54.859134 13416 net.cpp:129] Top shape: 100 75 8 8 (480000)
I1202 13:05:54.859134 13416 net.cpp:137] Memory required for data: 347316400
I1202 13:05:54.859134 13416 layer_factory.cpp:58] Creating layer conv11
I1202 13:05:54.859134 13416 net.cpp:84] Creating Layer conv11
I1202 13:05:54.859134 13416 net.cpp:406] conv11 <- pool4_2
I1202 13:05:54.859134 13416 net.cpp:380] conv11 -> conv11
I1202 13:05:54.861135 13416 net.cpp:122] Setting up conv11
I1202 13:05:54.861135 13416 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1202 13:05:54.861135 13416 net.cpp:137] Memory required for data: 349492400
I1202 13:05:54.861135 13416 layer_factory.cpp:58] Creating layer bn_conv11
I1202 13:05:54.861135 13416 net.cpp:84] Creating Layer bn_conv11
I1202 13:05:54.861135 13416 net.cpp:406] bn_conv11 <- conv11
I1202 13:05:54.861135 13416 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1202 13:05:54.861135 13416 net.cpp:122] Setting up bn_conv11
I1202 13:05:54.861135 13416 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1202 13:05:54.861135 13416 net.cpp:137] Memory required for data: 351668400
I1202 13:05:54.861135 13416 layer_factory.cpp:58] Creating layer scale_conv11
I1202 13:05:54.861634 13416 net.cpp:84] Creating Layer scale_conv11
I1202 13:05:54.861634 13416 net.cpp:406] scale_conv11 <- conv11
I1202 13:05:54.861634 13416 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1202 13:05:54.861634 13416 layer_factory.cpp:58] Creating layer scale_conv11
I1202 13:05:54.861634 13416 net.cpp:122] Setting up scale_conv11
I1202 13:05:54.861634 13416 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1202 13:05:54.861634 13416 net.cpp:137] Memory required for data: 353844400
I1202 13:05:54.861634 13416 layer_factory.cpp:58] Creating layer relu_conv11
I1202 13:05:54.861634 13416 net.cpp:84] Creating Layer relu_conv11
I1202 13:05:54.861634 13416 net.cpp:406] relu_conv11 <- conv11
I1202 13:05:54.861634 13416 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1202 13:05:54.861634 13416 net.cpp:122] Setting up relu_conv11
I1202 13:05:54.861634 13416 net.cpp:129] Top shape: 100 85 8 8 (544000)
I1202 13:05:54.861634 13416 net.cpp:137] Memory required for data: 356020400
I1202 13:05:54.861634 13416 layer_factory.cpp:58] Creating layer conv12
I1202 13:05:54.861634 13416 net.cpp:84] Creating Layer conv12
I1202 13:05:54.861634 13416 net.cpp:406] conv12 <- conv11
I1202 13:05:54.861634 13416 net.cpp:380] conv12 -> conv12
I1202 13:05:54.863134 13416 net.cpp:122] Setting up conv12
I1202 13:05:54.863134 13416 net.cpp:129] Top shape: 100 96 8 8 (614400)
I1202 13:05:54.863134 13416 net.cpp:137] Memory required for data: 358478000
I1202 13:05:54.863134 13416 layer_factory.cpp:58] Creating layer bn_conv12
I1202 13:05:54.863134 13416 net.cpp:84] Creating Layer bn_conv12
I1202 13:05:54.863134 13416 net.cpp:406] bn_conv12 <- conv12
I1202 13:05:54.863134 13416 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1202 13:05:54.864140 13416 net.cpp:122] Setting up bn_conv12
I1202 13:05:54.864140 13416 net.cpp:129] Top shape: 100 96 8 8 (614400)
I1202 13:05:54.864140 13416 net.cpp:137] Memory required for data: 360935600
I1202 13:05:54.864140 13416 layer_factory.cpp:58] Creating layer scale_conv12
I1202 13:05:54.864140 13416 net.cpp:84] Creating Layer scale_conv12
I1202 13:05:54.864140 13416 net.cpp:406] scale_conv12 <- conv12
I1202 13:05:54.864140 13416 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1202 13:05:54.864140 13416 layer_factory.cpp:58] Creating layer scale_conv12
I1202 13:05:54.864140 13416 net.cpp:122] Setting up scale_conv12
I1202 13:05:54.864140 13416 net.cpp:129] Top shape: 100 96 8 8 (614400)
I1202 13:05:54.864140 13416 net.cpp:137] Memory required for data: 363393200
I1202 13:05:54.864140 13416 layer_factory.cpp:58] Creating layer relu_conv12
I1202 13:05:54.864140 13416 net.cpp:84] Creating Layer relu_conv12
I1202 13:05:54.864140 13416 net.cpp:406] relu_conv12 <- conv12
I1202 13:05:54.864140 13416 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1202 13:05:54.864140 13416 net.cpp:122] Setting up relu_conv12
I1202 13:05:54.864140 13416 net.cpp:129] Top shape: 100 96 8 8 (614400)
I1202 13:05:54.864140 13416 net.cpp:137] Memory required for data: 365850800
I1202 13:05:54.864140 13416 layer_factory.cpp:58] Creating layer poolcp6
I1202 13:05:54.864140 13416 net.cpp:84] Creating Layer poolcp6
I1202 13:05:54.864140 13416 net.cpp:406] poolcp6 <- conv12
I1202 13:05:54.864140 13416 net.cpp:380] poolcp6 -> poolcp6
I1202 13:05:54.864140 13416 net.cpp:122] Setting up poolcp6
I1202 13:05:54.864140 13416 net.cpp:129] Top shape: 100 96 1 1 (9600)
I1202 13:05:54.864140 13416 net.cpp:137] Memory required for data: 365889200
I1202 13:05:54.864140 13416 layer_factory.cpp:58] Creating layer ip1
I1202 13:05:54.864140 13416 net.cpp:84] Creating Layer ip1
I1202 13:05:54.864140 13416 net.cpp:406] ip1 <- poolcp6
I1202 13:05:54.864140 13416 net.cpp:380] ip1 -> ip1
I1202 13:05:54.864140 13416 net.cpp:122] Setting up ip1
I1202 13:05:54.864140 13416 net.cpp:129] Top shape: 100 10 (1000)
I1202 13:05:54.864140 13416 net.cpp:137] Memory required for data: 365893200
I1202 13:05:54.864140 13416 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1202 13:05:54.864140 13416 net.cpp:84] Creating Layer ip1_ip1_0_split
I1202 13:05:54.864140 13416 net.cpp:406] ip1_ip1_0_split <- ip1
I1202 13:05:54.864140 13416 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1202 13:05:54.864140 13416 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1202 13:05:54.864140 13416 net.cpp:122] Setting up ip1_ip1_0_split
I1202 13:05:54.864140 13416 net.cpp:129] Top shape: 100 10 (1000)
I1202 13:05:54.864140 13416 net.cpp:129] Top shape: 100 10 (1000)
I1202 13:05:54.864140 13416 net.cpp:137] Memory required for data: 365901200
I1202 13:05:54.864140 13416 layer_factory.cpp:58] Creating layer accuracy
I1202 13:05:54.864140 13416 net.cpp:84] Creating Layer accuracy
I1202 13:05:54.864140 13416 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1202 13:05:54.864140 13416 net.cpp:406] accuracy <- label_cifar_1_split_0
I1202 13:05:54.864140 13416 net.cpp:380] accuracy -> accuracy
I1202 13:05:54.865139 13416 net.cpp:122] Setting up accuracy
I1202 13:05:54.865139 13416 net.cpp:129] Top shape: (1)
I1202 13:05:54.865139 13416 net.cpp:137] Memory required for data: 365901204
I1202 13:05:54.865139 13416 layer_factory.cpp:58] Creating layer loss
I1202 13:05:54.865139 13416 net.cpp:84] Creating Layer loss
I1202 13:05:54.865139 13416 net.cpp:406] loss <- ip1_ip1_0_split_1
I1202 13:05:54.865139 13416 net.cpp:406] loss <- label_cifar_1_split_1
I1202 13:05:54.865139 13416 net.cpp:380] loss -> loss
I1202 13:05:54.865139 13416 layer_factory.cpp:58] Creating layer loss
I1202 13:05:54.865139 13416 net.cpp:122] Setting up loss
I1202 13:05:54.865139 13416 net.cpp:129] Top shape: (1)
I1202 13:05:54.865139 13416 net.cpp:132]     with loss weight 1
I1202 13:05:54.865139 13416 net.cpp:137] Memory required for data: 365901208
I1202 13:05:54.865139 13416 net.cpp:198] loss needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:200] accuracy does not need backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] ip1 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] poolcp6 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] relu_conv12 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] scale_conv12 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] bn_conv12 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] conv12 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] relu_conv11 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] scale_conv11 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] bn_conv11 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] conv11 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] pool4_2 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] relu4_2 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] scale4_2 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] bn4_2 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] conv4_2 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] relu4_1 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] scale4_1 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] bn4_1 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] conv4_1 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] relu4 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] scale4 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] bn4 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] conv4 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] relu3 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] scale3 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] bn3 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] conv3 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] pool2_1 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] relu2_2 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] scale2_2 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] bn2_2 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] conv2_2 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] relu2 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] scale2 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] bn2 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] conv2 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] relu1 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] scale1 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] bn1 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:198] conv1 needs backward computation.
I1202 13:05:54.865139 13416 net.cpp:200] label_cifar_1_split does not need backward computation.
I1202 13:05:54.865139 13416 net.cpp:200] cifar does not need backward computation.
I1202 13:05:54.865139 13416 net.cpp:242] This network produces output accuracy
I1202 13:05:54.865139 13416 net.cpp:242] This network produces output loss
I1202 13:05:54.865139 13416 net.cpp:255] Network initialization done.
I1202 13:05:54.865139 13416 solver.cpp:56] Solver scaffolding done.
I1202 13:05:54.868139 13416 caffe.cpp:249] Starting Optimization
I1202 13:05:54.868139 13416 solver.cpp:272] Solving CIFAR10_SimpleNet_GP_9L_Simple_NoGrpCon_NoDrp_300kv2
I1202 13:05:54.868139 13416 solver.cpp:273] Learning Rate Policy: multistep
I1202 13:05:54.870141 13416 solver.cpp:330] Iteration 0, Testing net (#0)
I1202 13:05:54.871140 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:05:55.893209  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:05:55.931208 13416 solver.cpp:397]     Test net output #0: accuracy = 0.1
I1202 13:05:55.931208 13416 solver.cpp:397]     Test net output #1: loss = 78.6029 (* 1 = 78.6029 loss)
I1202 13:05:56.003232 13416 solver.cpp:218] Iteration 0 (0 iter/s, 1.13387s/100 iters), loss = 3.86788
I1202 13:05:56.003232 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.09
I1202 13:05:56.003232 13416 solver.cpp:237]     Train net output #1: loss = 3.86788 (* 1 = 3.86788 loss)
I1202 13:05:56.003232 13416 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1202 13:05:59.988482 13416 solver.cpp:218] Iteration 100 (25.0935 iter/s, 3.98509s/100 iters), loss = 1.51305
I1202 13:05:59.988482 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.46
I1202 13:05:59.988482 13416 solver.cpp:237]     Train net output #1: loss = 1.51305 (* 1 = 1.51305 loss)
I1202 13:05:59.988482 13416 sgd_solver.cpp:105] Iteration 100, lr = 0.1
I1202 13:06:03.966704 13416 solver.cpp:218] Iteration 200 (25.1396 iter/s, 3.97778s/100 iters), loss = 1.49735
I1202 13:06:03.966704 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.46
I1202 13:06:03.966704 13416 solver.cpp:237]     Train net output #1: loss = 1.49735 (* 1 = 1.49735 loss)
I1202 13:06:03.966704 13416 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1202 13:06:07.966986 13416 solver.cpp:218] Iteration 300 (25.0014 iter/s, 3.99977s/100 iters), loss = 1.17965
I1202 13:06:07.966986 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.59
I1202 13:06:07.966986 13416 solver.cpp:237]     Train net output #1: loss = 1.17965 (* 1 = 1.17965 loss)
I1202 13:06:07.966986 13416 sgd_solver.cpp:105] Iteration 300, lr = 0.1
I1202 13:06:11.982373 13416 solver.cpp:218] Iteration 400 (24.9065 iter/s, 4.01501s/100 iters), loss = 1.19394
I1202 13:06:11.982373 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.58
I1202 13:06:11.982373 13416 solver.cpp:237]     Train net output #1: loss = 1.19394 (* 1 = 1.19394 loss)
I1202 13:06:11.982373 13416 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1202 13:06:15.790638 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:06:15.946153 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_500.caffemodel
I1202 13:06:15.963153 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_500.solverstate
I1202 13:06:15.966153 13416 solver.cpp:330] Iteration 500, Testing net (#0)
I1202 13:06:15.966153 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:06:16.950224  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:06:16.988224 13416 solver.cpp:397]     Test net output #0: accuracy = 0.4262
I1202 13:06:16.988224 13416 solver.cpp:397]     Test net output #1: loss = 1.64111 (* 1 = 1.64111 loss)
I1202 13:06:17.026232 13416 solver.cpp:218] Iteration 500 (19.8263 iter/s, 5.0438s/100 iters), loss = 1.1231
I1202 13:06:17.026232 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.62
I1202 13:06:17.026232 13416 solver.cpp:237]     Train net output #1: loss = 1.1231 (* 1 = 1.1231 loss)
I1202 13:06:17.026232 13416 sgd_solver.cpp:105] Iteration 500, lr = 0.1
I1202 13:06:21.050509 13416 solver.cpp:218] Iteration 600 (24.8516 iter/s, 4.02389s/100 iters), loss = 0.968232
I1202 13:06:21.050509 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1202 13:06:21.050509 13416 solver.cpp:237]     Train net output #1: loss = 0.968232 (* 1 = 0.968232 loss)
I1202 13:06:21.050509 13416 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1202 13:06:25.041821 13416 solver.cpp:218] Iteration 700 (25.0587 iter/s, 3.99062s/100 iters), loss = 0.947421
I1202 13:06:25.041821 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1202 13:06:25.041821 13416 solver.cpp:237]     Train net output #1: loss = 0.947421 (* 1 = 0.947421 loss)
I1202 13:06:25.041821 13416 sgd_solver.cpp:105] Iteration 700, lr = 0.1
I1202 13:06:29.041028 13416 solver.cpp:218] Iteration 800 (25.0043 iter/s, 3.99931s/100 iters), loss = 0.87407
I1202 13:06:29.041028 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1202 13:06:29.041028 13416 solver.cpp:237]     Train net output #1: loss = 0.87407 (* 1 = 0.87407 loss)
I1202 13:06:29.041028 13416 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1202 13:06:33.032325 13416 solver.cpp:218] Iteration 900 (25.0599 iter/s, 3.99044s/100 iters), loss = 0.878124
I1202 13:06:33.032325 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1202 13:06:33.032325 13416 solver.cpp:237]     Train net output #1: loss = 0.878124 (* 1 = 0.878124 loss)
I1202 13:06:33.032325 13416 sgd_solver.cpp:105] Iteration 900, lr = 0.1
I1202 13:06:36.828577 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:06:36.984582 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_1000.caffemodel
I1202 13:06:36.996088 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_1000.solverstate
I1202 13:06:37.000089 13416 solver.cpp:330] Iteration 1000, Testing net (#0)
I1202 13:06:37.000089 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:06:37.993168  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:06:38.031672 13416 solver.cpp:397]     Test net output #0: accuracy = 0.5029
I1202 13:06:38.031672 13416 solver.cpp:397]     Test net output #1: loss = 1.3837 (* 1 = 1.3837 loss)
I1202 13:06:38.069669 13416 solver.cpp:218] Iteration 1000 (19.8542 iter/s, 5.03672s/100 iters), loss = 0.827707
I1202 13:06:38.069669 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1202 13:06:38.069669 13416 solver.cpp:237]     Train net output #1: loss = 0.827707 (* 1 = 0.827707 loss)
I1202 13:06:38.069669 13416 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1202 13:06:42.070389 13416 solver.cpp:218] Iteration 1100 (24.9962 iter/s, 4.00062s/100 iters), loss = 0.778553
I1202 13:06:42.070389 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1202 13:06:42.070389 13416 solver.cpp:237]     Train net output #1: loss = 0.778553 (* 1 = 0.778553 loss)
I1202 13:06:42.070389 13416 sgd_solver.cpp:105] Iteration 1100, lr = 0.1
I1202 13:06:46.104183 13416 solver.cpp:218] Iteration 1200 (24.7933 iter/s, 4.03335s/100 iters), loss = 0.699816
I1202 13:06:46.104183 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1202 13:06:46.104183 13416 solver.cpp:237]     Train net output #1: loss = 0.699816 (* 1 = 0.699816 loss)
I1202 13:06:46.104183 13416 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1202 13:06:50.129055 13416 solver.cpp:218] Iteration 1300 (24.8486 iter/s, 4.02438s/100 iters), loss = 0.767489
I1202 13:06:50.129055 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1202 13:06:50.129055 13416 solver.cpp:237]     Train net output #1: loss = 0.767489 (* 1 = 0.767489 loss)
I1202 13:06:50.129055 13416 sgd_solver.cpp:105] Iteration 1300, lr = 0.1
I1202 13:06:54.120329 13416 solver.cpp:218] Iteration 1400 (25.0585 iter/s, 3.99066s/100 iters), loss = 0.683938
I1202 13:06:54.120329 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1202 13:06:54.120329 13416 solver.cpp:237]     Train net output #1: loss = 0.683938 (* 1 = 0.683938 loss)
I1202 13:06:54.120329 13416 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1202 13:06:57.928649 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:06:58.085655 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_1500.caffemodel
I1202 13:06:58.095655 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_1500.solverstate
I1202 13:06:58.099656 13416 solver.cpp:330] Iteration 1500, Testing net (#0)
I1202 13:06:58.099656 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:06:59.090729  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:06:59.128736 13416 solver.cpp:397]     Test net output #0: accuracy = 0.514
I1202 13:06:59.128736 13416 solver.cpp:397]     Test net output #1: loss = 1.32828 (* 1 = 1.32828 loss)
I1202 13:06:59.166734 13416 solver.cpp:218] Iteration 1500 (19.8172 iter/s, 5.04612s/100 iters), loss = 0.666429
I1202 13:06:59.166734 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1202 13:06:59.166734 13416 solver.cpp:237]     Train net output #1: loss = 0.666429 (* 1 = 0.666429 loss)
I1202 13:06:59.166734 13416 sgd_solver.cpp:105] Iteration 1500, lr = 0.1
I1202 13:07:03.184054 13416 solver.cpp:218] Iteration 1600 (24.8916 iter/s, 4.01742s/100 iters), loss = 0.58253
I1202 13:07:03.184054 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1202 13:07:03.184054 13416 solver.cpp:237]     Train net output #1: loss = 0.58253 (* 1 = 0.58253 loss)
I1202 13:07:03.184054 13416 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1202 13:07:07.205909 13416 solver.cpp:218] Iteration 1700 (24.8693 iter/s, 4.02102s/100 iters), loss = 0.720134
I1202 13:07:07.205909 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1202 13:07:07.205909 13416 solver.cpp:237]     Train net output #1: loss = 0.720134 (* 1 = 0.720134 loss)
I1202 13:07:07.205909 13416 sgd_solver.cpp:105] Iteration 1700, lr = 0.1
I1202 13:07:11.252744 13416 solver.cpp:218] Iteration 1800 (24.7142 iter/s, 4.04625s/100 iters), loss = 0.609459
I1202 13:07:11.252744 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1202 13:07:11.252744 13416 solver.cpp:237]     Train net output #1: loss = 0.609459 (* 1 = 0.609459 loss)
I1202 13:07:11.252744 13416 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1202 13:07:15.250058 13416 solver.cpp:218] Iteration 1900 (25.0182 iter/s, 3.99708s/100 iters), loss = 0.536232
I1202 13:07:15.250058 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1202 13:07:15.250058 13416 solver.cpp:237]     Train net output #1: loss = 0.536232 (* 1 = 0.536232 loss)
I1202 13:07:15.250058 13416 sgd_solver.cpp:105] Iteration 1900, lr = 0.1
I1202 13:07:19.074673 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:07:19.230690 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_2000.caffemodel
I1202 13:07:19.241689 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_2000.solverstate
I1202 13:07:19.244690 13416 solver.cpp:330] Iteration 2000, Testing net (#0)
I1202 13:07:19.244690 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:07:20.244771  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:07:20.283771 13416 solver.cpp:397]     Test net output #0: accuracy = 0.59
I1202 13:07:20.283771 13416 solver.cpp:397]     Test net output #1: loss = 1.15902 (* 1 = 1.15902 loss)
I1202 13:07:20.321785 13416 solver.cpp:218] Iteration 2000 (19.717 iter/s, 5.07177s/100 iters), loss = 0.561429
I1202 13:07:20.321785 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1202 13:07:20.321785 13416 solver.cpp:237]     Train net output #1: loss = 0.561429 (* 1 = 0.561429 loss)
I1202 13:07:20.321785 13416 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1202 13:07:24.317535 13416 solver.cpp:218] Iteration 2100 (25.0314 iter/s, 3.99498s/100 iters), loss = 0.613648
I1202 13:07:24.317535 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1202 13:07:24.317535 13416 solver.cpp:237]     Train net output #1: loss = 0.613648 (* 1 = 0.613648 loss)
I1202 13:07:24.317535 13416 sgd_solver.cpp:105] Iteration 2100, lr = 0.1
I1202 13:07:28.306290 13416 solver.cpp:218] Iteration 2200 (25.0725 iter/s, 3.98843s/100 iters), loss = 0.563921
I1202 13:07:28.306792 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1202 13:07:28.306792 13416 solver.cpp:237]     Train net output #1: loss = 0.563921 (* 1 = 0.563921 loss)
I1202 13:07:28.306792 13416 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1202 13:07:32.310097 13416 solver.cpp:218] Iteration 2300 (24.9792 iter/s, 4.00334s/100 iters), loss = 0.737486
I1202 13:07:32.310097 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1202 13:07:32.310097 13416 solver.cpp:237]     Train net output #1: loss = 0.737486 (* 1 = 0.737486 loss)
I1202 13:07:32.310097 13416 sgd_solver.cpp:105] Iteration 2300, lr = 0.1
I1202 13:07:36.293418 13416 solver.cpp:218] Iteration 2400 (25.1031 iter/s, 3.98357s/100 iters), loss = 0.56802
I1202 13:07:36.294420 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1202 13:07:36.294420 13416 solver.cpp:237]     Train net output #1: loss = 0.56802 (* 1 = 0.56802 loss)
I1202 13:07:36.294420 13416 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1202 13:07:40.082643 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:07:40.238652 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_2500.caffemodel
I1202 13:07:40.248652 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_2500.solverstate
I1202 13:07:40.252652 13416 solver.cpp:330] Iteration 2500, Testing net (#0)
I1202 13:07:40.252652 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:07:41.236714  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:07:41.274724 13416 solver.cpp:397]     Test net output #0: accuracy = 0.6429
I1202 13:07:41.274724 13416 solver.cpp:397]     Test net output #1: loss = 1.03587 (* 1 = 1.03587 loss)
I1202 13:07:41.313217 13416 solver.cpp:218] Iteration 2500 (19.9251 iter/s, 5.0188s/100 iters), loss = 0.567646
I1202 13:07:41.313217 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1202 13:07:41.313217 13416 solver.cpp:237]     Train net output #1: loss = 0.567646 (* 1 = 0.567646 loss)
I1202 13:07:41.313217 13416 sgd_solver.cpp:105] Iteration 2500, lr = 0.1
I1202 13:07:45.290010 13416 solver.cpp:218] Iteration 2600 (25.1451 iter/s, 3.97691s/100 iters), loss = 0.413348
I1202 13:07:45.290010 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1202 13:07:45.290010 13416 solver.cpp:237]     Train net output #1: loss = 0.413348 (* 1 = 0.413348 loss)
I1202 13:07:45.290010 13416 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1202 13:07:49.275229 13416 solver.cpp:218] Iteration 2700 (25.0957 iter/s, 3.98475s/100 iters), loss = 0.551561
I1202 13:07:49.275229 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1202 13:07:49.275229 13416 solver.cpp:237]     Train net output #1: loss = 0.551561 (* 1 = 0.551561 loss)
I1202 13:07:49.275229 13416 sgd_solver.cpp:105] Iteration 2700, lr = 0.1
I1202 13:07:53.288508 13416 solver.cpp:218] Iteration 2800 (24.9241 iter/s, 4.01218s/100 iters), loss = 0.627468
I1202 13:07:53.288508 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1202 13:07:53.288508 13416 solver.cpp:237]     Train net output #1: loss = 0.627468 (* 1 = 0.627468 loss)
I1202 13:07:53.288508 13416 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1202 13:07:57.305033 13416 solver.cpp:218] Iteration 2900 (24.8943 iter/s, 4.01699s/100 iters), loss = 0.500708
I1202 13:07:57.305033 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1202 13:07:57.306035 13416 solver.cpp:237]     Train net output #1: loss = 0.500708 (* 1 = 0.500708 loss)
I1202 13:07:57.306035 13416 sgd_solver.cpp:105] Iteration 2900, lr = 0.1
I1202 13:08:01.106335 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:08:01.262344 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_3000.caffemodel
I1202 13:08:01.274344 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_3000.solverstate
I1202 13:08:01.277345 13416 solver.cpp:330] Iteration 3000, Testing net (#0)
I1202 13:08:01.277345 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:08:02.262403  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:08:02.301401 13416 solver.cpp:397]     Test net output #0: accuracy = 0.7192
I1202 13:08:02.301401 13416 solver.cpp:397]     Test net output #1: loss = 0.815159 (* 1 = 0.815159 loss)
I1202 13:08:02.339413 13416 solver.cpp:218] Iteration 3000 (19.8683 iter/s, 5.03313s/100 iters), loss = 0.516889
I1202 13:08:02.339413 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1202 13:08:02.339413 13416 solver.cpp:237]     Train net output #1: loss = 0.516889 (* 1 = 0.516889 loss)
I1202 13:08:02.339413 13416 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1202 13:08:06.334668 13416 solver.cpp:218] Iteration 3100 (25.0315 iter/s, 3.99496s/100 iters), loss = 0.487422
I1202 13:08:06.334668 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1202 13:08:06.334668 13416 solver.cpp:237]     Train net output #1: loss = 0.487422 (* 1 = 0.487422 loss)
I1202 13:08:06.334668 13416 sgd_solver.cpp:105] Iteration 3100, lr = 0.1
I1202 13:08:10.316684 13416 solver.cpp:218] Iteration 3200 (25.1147 iter/s, 3.98173s/100 iters), loss = 0.524186
I1202 13:08:10.316684 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1202 13:08:10.316684 13416 solver.cpp:237]     Train net output #1: loss = 0.524186 (* 1 = 0.524186 loss)
I1202 13:08:10.316684 13416 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1202 13:08:14.345579 13416 solver.cpp:218] Iteration 3300 (24.8229 iter/s, 4.02854s/100 iters), loss = 0.660554
I1202 13:08:14.345579 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1202 13:08:14.345579 13416 solver.cpp:237]     Train net output #1: loss = 0.660554 (* 1 = 0.660554 loss)
I1202 13:08:14.345579 13416 sgd_solver.cpp:105] Iteration 3300, lr = 0.1
I1202 13:08:18.350946 13416 solver.cpp:218] Iteration 3400 (24.9682 iter/s, 4.0051s/100 iters), loss = 0.492192
I1202 13:08:18.350946 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1202 13:08:18.350946 13416 solver.cpp:237]     Train net output #1: loss = 0.492192 (* 1 = 0.492192 loss)
I1202 13:08:18.350946 13416 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1202 13:08:22.148171 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:08:22.312875 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_3500.caffemodel
I1202 13:08:22.326375 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_3500.solverstate
I1202 13:08:22.330390 13416 solver.cpp:330] Iteration 3500, Testing net (#0)
I1202 13:08:22.330390 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:08:23.318442  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:08:23.356453 13416 solver.cpp:397]     Test net output #0: accuracy = 0.7111
I1202 13:08:23.356453 13416 solver.cpp:397]     Test net output #1: loss = 0.847255 (* 1 = 0.847255 loss)
I1202 13:08:23.395447 13416 solver.cpp:218] Iteration 3500 (19.825 iter/s, 5.04414s/100 iters), loss = 0.494378
I1202 13:08:23.395447 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1202 13:08:23.395447 13416 solver.cpp:237]     Train net output #1: loss = 0.494378 (* 1 = 0.494378 loss)
I1202 13:08:23.395447 13416 sgd_solver.cpp:105] Iteration 3500, lr = 0.1
I1202 13:08:27.384762 13416 solver.cpp:218] Iteration 3600 (25.0676 iter/s, 3.98921s/100 iters), loss = 0.417045
I1202 13:08:27.384762 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1202 13:08:27.384762 13416 solver.cpp:237]     Train net output #1: loss = 0.417045 (* 1 = 0.417045 loss)
I1202 13:08:27.384762 13416 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1202 13:08:31.374033 13416 solver.cpp:218] Iteration 3700 (25.074 iter/s, 3.9882s/100 iters), loss = 0.489872
I1202 13:08:31.374033 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1202 13:08:31.374033 13416 solver.cpp:237]     Train net output #1: loss = 0.489872 (* 1 = 0.489872 loss)
I1202 13:08:31.374033 13416 sgd_solver.cpp:105] Iteration 3700, lr = 0.1
I1202 13:08:35.358321 13416 solver.cpp:218] Iteration 3800 (25.0964 iter/s, 3.98464s/100 iters), loss = 0.594181
I1202 13:08:35.358321 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1202 13:08:35.358321 13416 solver.cpp:237]     Train net output #1: loss = 0.594181 (* 1 = 0.594181 loss)
I1202 13:08:35.358321 13416 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1202 13:08:39.356644 13416 solver.cpp:218] Iteration 3900 (25.0162 iter/s, 3.99741s/100 iters), loss = 0.58041
I1202 13:08:39.356644 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1202 13:08:39.356644 13416 solver.cpp:237]     Train net output #1: loss = 0.58041 (* 1 = 0.58041 loss)
I1202 13:08:39.356644 13416 sgd_solver.cpp:105] Iteration 3900, lr = 0.1
I1202 13:08:43.193562 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:08:43.350076 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_4000.caffemodel
I1202 13:08:43.361074 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_4000.solverstate
I1202 13:08:43.365075 13416 solver.cpp:330] Iteration 4000, Testing net (#0)
I1202 13:08:43.365075 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:08:44.350152  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:08:44.388149 13416 solver.cpp:397]     Test net output #0: accuracy = 0.7319
I1202 13:08:44.389158 13416 solver.cpp:397]     Test net output #1: loss = 0.78434 (* 1 = 0.78434 loss)
I1202 13:08:44.427161 13416 solver.cpp:218] Iteration 4000 (19.7231 iter/s, 5.07018s/100 iters), loss = 0.410962
I1202 13:08:44.427161 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1202 13:08:44.427161 13416 solver.cpp:237]     Train net output #1: loss = 0.410962 (* 1 = 0.410962 loss)
I1202 13:08:44.427161 13416 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1202 13:08:48.432456 13416 solver.cpp:218] Iteration 4100 (24.9667 iter/s, 4.00534s/100 iters), loss = 0.514159
I1202 13:08:48.432456 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1202 13:08:48.432456 13416 solver.cpp:237]     Train net output #1: loss = 0.514159 (* 1 = 0.514159 loss)
I1202 13:08:48.432456 13416 sgd_solver.cpp:105] Iteration 4100, lr = 0.1
I1202 13:08:52.446825 13416 solver.cpp:218] Iteration 4200 (24.9173 iter/s, 4.01328s/100 iters), loss = 0.535443
I1202 13:08:52.446825 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1202 13:08:52.446825 13416 solver.cpp:237]     Train net output #1: loss = 0.535443 (* 1 = 0.535443 loss)
I1202 13:08:52.446825 13416 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1202 13:08:56.425611 13416 solver.cpp:218] Iteration 4300 (25.1346 iter/s, 3.97858s/100 iters), loss = 0.496639
I1202 13:08:56.425611 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1202 13:08:56.425611 13416 solver.cpp:237]     Train net output #1: loss = 0.496639 (* 1 = 0.496639 loss)
I1202 13:08:56.425611 13416 sgd_solver.cpp:105] Iteration 4300, lr = 0.1
I1202 13:09:00.420387 13416 solver.cpp:218] Iteration 4400 (25.0353 iter/s, 3.99436s/100 iters), loss = 0.46876
I1202 13:09:00.420387 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1202 13:09:00.420387 13416 solver.cpp:237]     Train net output #1: loss = 0.46876 (* 1 = 0.46876 loss)
I1202 13:09:00.420387 13416 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1202 13:09:04.215767 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:09:04.372776 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_4500.caffemodel
I1202 13:09:04.384774 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_4500.solverstate
I1202 13:09:04.388774 13416 solver.cpp:330] Iteration 4500, Testing net (#0)
I1202 13:09:04.388774 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:09:05.375876  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:09:05.413874 13416 solver.cpp:397]     Test net output #0: accuracy = 0.7447
I1202 13:09:05.413874 13416 solver.cpp:397]     Test net output #1: loss = 0.756913 (* 1 = 0.756913 loss)
I1202 13:09:05.452885 13416 solver.cpp:218] Iteration 4500 (19.8731 iter/s, 5.03193s/100 iters), loss = 0.374647
I1202 13:09:05.452885 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1202 13:09:05.452885 13416 solver.cpp:237]     Train net output #1: loss = 0.374647 (* 1 = 0.374647 loss)
I1202 13:09:05.452885 13416 sgd_solver.cpp:105] Iteration 4500, lr = 0.1
I1202 13:09:09.442185 13416 solver.cpp:218] Iteration 4600 (25.0676 iter/s, 3.98921s/100 iters), loss = 0.465132
I1202 13:09:09.442185 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1202 13:09:09.442185 13416 solver.cpp:237]     Train net output #1: loss = 0.465132 (* 1 = 0.465132 loss)
I1202 13:09:09.442185 13416 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1202 13:09:13.429883 13416 solver.cpp:218] Iteration 4700 (25.0804 iter/s, 3.98718s/100 iters), loss = 0.593871
I1202 13:09:13.429883 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1202 13:09:13.429883 13416 solver.cpp:237]     Train net output #1: loss = 0.593871 (* 1 = 0.593871 loss)
I1202 13:09:13.429883 13416 sgd_solver.cpp:105] Iteration 4700, lr = 0.1
I1202 13:09:17.418159 13416 solver.cpp:218] Iteration 4800 (25.0742 iter/s, 3.98816s/100 iters), loss = 0.496903
I1202 13:09:17.418159 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1202 13:09:17.418159 13416 solver.cpp:237]     Train net output #1: loss = 0.496903 (* 1 = 0.496903 loss)
I1202 13:09:17.418159 13416 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1202 13:09:21.408994 13416 solver.cpp:218] Iteration 4900 (25.0621 iter/s, 3.99009s/100 iters), loss = 0.471235
I1202 13:09:21.408994 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1202 13:09:21.408994 13416 solver.cpp:237]     Train net output #1: loss = 0.471235 (* 1 = 0.471235 loss)
I1202 13:09:21.408994 13416 sgd_solver.cpp:105] Iteration 4900, lr = 0.1
I1202 13:09:25.201328 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:09:25.358343 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_5000.caffemodel
I1202 13:09:25.415343 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_5000.solverstate
I1202 13:09:25.419349 13416 solver.cpp:330] Iteration 5000, Testing net (#0)
I1202 13:09:25.419349 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:09:26.405426  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:09:26.443430 13416 solver.cpp:397]     Test net output #0: accuracy = 0.7993
I1202 13:09:26.443430 13416 solver.cpp:397]     Test net output #1: loss = 0.604345 (* 1 = 0.604345 loss)
I1202 13:09:26.482432 13416 solver.cpp:218] Iteration 5000 (19.7119 iter/s, 5.07307s/100 iters), loss = 0.391882
I1202 13:09:26.482432 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1202 13:09:26.482432 13416 solver.cpp:237]     Train net output #1: loss = 0.391882 (* 1 = 0.391882 loss)
I1202 13:09:26.482432 13416 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I1202 13:09:26.482432 13416 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I1202 13:09:30.463560 13416 solver.cpp:218] Iteration 5100 (25.1196 iter/s, 3.98096s/100 iters), loss = 0.331687
I1202 13:09:30.463560 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1202 13:09:30.464045 13416 solver.cpp:237]     Train net output #1: loss = 0.331687 (* 1 = 0.331687 loss)
I1202 13:09:30.464045 13416 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I1202 13:09:34.441576 13416 solver.cpp:218] Iteration 5200 (25.1396 iter/s, 3.97779s/100 iters), loss = 0.377113
I1202 13:09:34.441576 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1202 13:09:34.441576 13416 solver.cpp:237]     Train net output #1: loss = 0.377113 (* 1 = 0.377113 loss)
I1202 13:09:34.441576 13416 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I1202 13:09:38.424816 13416 solver.cpp:218] Iteration 5300 (25.1056 iter/s, 3.98318s/100 iters), loss = 0.363256
I1202 13:09:38.424816 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1202 13:09:38.424816 13416 solver.cpp:237]     Train net output #1: loss = 0.363256 (* 1 = 0.363256 loss)
I1202 13:09:38.424816 13416 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I1202 13:09:42.407307 13416 solver.cpp:218] Iteration 5400 (25.1125 iter/s, 3.98207s/100 iters), loss = 0.253242
I1202 13:09:42.407307 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:09:42.407307 13416 solver.cpp:237]     Train net output #1: loss = 0.253242 (* 1 = 0.253242 loss)
I1202 13:09:42.407307 13416 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I1202 13:09:46.197562 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:09:46.353574 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_5500.caffemodel
I1202 13:09:46.364574 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_5500.solverstate
I1202 13:09:46.368576 13416 solver.cpp:330] Iteration 5500, Testing net (#0)
I1202 13:09:46.368576 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:09:47.350641  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:09:47.388639 13416 solver.cpp:397]     Test net output #0: accuracy = 0.8768
I1202 13:09:47.388639 13416 solver.cpp:397]     Test net output #1: loss = 0.370366 (* 1 = 0.370366 loss)
I1202 13:09:47.427655 13416 solver.cpp:218] Iteration 5500 (19.9224 iter/s, 5.01946s/100 iters), loss = 0.255538
I1202 13:09:47.427655 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:09:47.427655 13416 solver.cpp:237]     Train net output #1: loss = 0.255538 (* 1 = 0.255538 loss)
I1202 13:09:47.427655 13416 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I1202 13:09:51.420956 13416 solver.cpp:218] Iteration 5600 (25.0447 iter/s, 3.99286s/100 iters), loss = 0.308536
I1202 13:09:51.420956 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1202 13:09:51.420956 13416 solver.cpp:237]     Train net output #1: loss = 0.308536 (* 1 = 0.308536 loss)
I1202 13:09:51.420956 13416 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I1202 13:09:55.414698 13416 solver.cpp:218] Iteration 5700 (25.0407 iter/s, 3.9935s/100 iters), loss = 0.294836
I1202 13:09:55.415199 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1202 13:09:55.415199 13416 solver.cpp:237]     Train net output #1: loss = 0.294836 (* 1 = 0.294836 loss)
I1202 13:09:55.415199 13416 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I1202 13:09:59.417942 13416 solver.cpp:218] Iteration 5800 (24.9849 iter/s, 4.00242s/100 iters), loss = 0.337912
I1202 13:09:59.417942 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1202 13:09:59.417942 13416 solver.cpp:237]     Train net output #1: loss = 0.337912 (* 1 = 0.337912 loss)
I1202 13:09:59.417942 13416 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I1202 13:10:03.468256 13416 solver.cpp:218] Iteration 5900 (24.686 iter/s, 4.05087s/100 iters), loss = 0.234405
I1202 13:10:03.469256 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:10:03.469256 13416 solver.cpp:237]     Train net output #1: loss = 0.234405 (* 1 = 0.234405 loss)
I1202 13:10:03.469256 13416 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I1202 13:10:07.265508 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:10:07.422520 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_6000.caffemodel
I1202 13:10:07.473563 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_6000.solverstate
I1202 13:10:07.477563 13416 solver.cpp:330] Iteration 6000, Testing net (#0)
I1202 13:10:07.477563 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:10:08.461634  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:10:08.499634 13416 solver.cpp:397]     Test net output #0: accuracy = 0.8805
I1202 13:10:08.499634 13416 solver.cpp:397]     Test net output #1: loss = 0.357115 (* 1 = 0.357115 loss)
I1202 13:10:08.538640 13416 solver.cpp:218] Iteration 6000 (19.7271 iter/s, 5.06916s/100 iters), loss = 0.249405
I1202 13:10:08.538640 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1202 13:10:08.538640 13416 solver.cpp:237]     Train net output #1: loss = 0.249405 (* 1 = 0.249405 loss)
I1202 13:10:08.538640 13416 sgd_solver.cpp:105] Iteration 6000, lr = 0.01
I1202 13:10:12.526906 13416 solver.cpp:218] Iteration 6100 (25.0734 iter/s, 3.98828s/100 iters), loss = 0.34587
I1202 13:10:12.526906 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1202 13:10:12.526906 13416 solver.cpp:237]     Train net output #1: loss = 0.34587 (* 1 = 0.34587 loss)
I1202 13:10:12.526906 13416 sgd_solver.cpp:105] Iteration 6100, lr = 0.01
I1202 13:10:16.525149 13416 solver.cpp:218] Iteration 6200 (25.015 iter/s, 3.99761s/100 iters), loss = 0.285952
I1202 13:10:16.525149 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1202 13:10:16.525149 13416 solver.cpp:237]     Train net output #1: loss = 0.285952 (* 1 = 0.285952 loss)
I1202 13:10:16.525149 13416 sgd_solver.cpp:105] Iteration 6200, lr = 0.01
I1202 13:10:20.542501 13416 solver.cpp:218] Iteration 6300 (24.8959 iter/s, 4.01673s/100 iters), loss = 0.284144
I1202 13:10:20.542501 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1202 13:10:20.542501 13416 solver.cpp:237]     Train net output #1: loss = 0.284144 (* 1 = 0.284144 loss)
I1202 13:10:20.542501 13416 sgd_solver.cpp:105] Iteration 6300, lr = 0.01
I1202 13:10:24.538820 13416 solver.cpp:218] Iteration 6400 (25.0249 iter/s, 3.99602s/100 iters), loss = 0.23006
I1202 13:10:24.538820 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:10:24.538820 13416 solver.cpp:237]     Train net output #1: loss = 0.23006 (* 1 = 0.23006 loss)
I1202 13:10:24.538820 13416 sgd_solver.cpp:105] Iteration 6400, lr = 0.01
I1202 13:10:28.340489 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:10:28.494997 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_6500.caffemodel
I1202 13:10:28.506005 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_6500.solverstate
I1202 13:10:28.509999 13416 solver.cpp:330] Iteration 6500, Testing net (#0)
I1202 13:10:28.509999 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:10:29.494094  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:10:29.532094 13416 solver.cpp:397]     Test net output #0: accuracy = 0.8841
I1202 13:10:29.532094 13416 solver.cpp:397]     Test net output #1: loss = 0.34388 (* 1 = 0.34388 loss)
I1202 13:10:29.570093 13416 solver.cpp:218] Iteration 6500 (19.8764 iter/s, 5.0311s/100 iters), loss = 0.173437
I1202 13:10:29.570093 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:10:29.570093 13416 solver.cpp:237]     Train net output #1: loss = 0.173437 (* 1 = 0.173437 loss)
I1202 13:10:29.570093 13416 sgd_solver.cpp:105] Iteration 6500, lr = 0.01
I1202 13:10:33.564604 13416 solver.cpp:218] Iteration 6600 (25.0363 iter/s, 3.99421s/100 iters), loss = 0.256593
I1202 13:10:33.564604 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:10:33.564604 13416 solver.cpp:237]     Train net output #1: loss = 0.256593 (* 1 = 0.256593 loss)
I1202 13:10:33.564604 13416 sgd_solver.cpp:105] Iteration 6600, lr = 0.01
I1202 13:10:37.551859 13416 solver.cpp:218] Iteration 6700 (25.0835 iter/s, 3.98669s/100 iters), loss = 0.294451
I1202 13:10:37.551859 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1202 13:10:37.551859 13416 solver.cpp:237]     Train net output #1: loss = 0.294451 (* 1 = 0.294451 loss)
I1202 13:10:37.551859 13416 sgd_solver.cpp:105] Iteration 6700, lr = 0.01
I1202 13:10:41.547547 13416 solver.cpp:218] Iteration 6800 (25.0301 iter/s, 3.99519s/100 iters), loss = 0.315304
I1202 13:10:41.547547 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1202 13:10:41.547547 13416 solver.cpp:237]     Train net output #1: loss = 0.315305 (* 1 = 0.315305 loss)
I1202 13:10:41.547547 13416 sgd_solver.cpp:105] Iteration 6800, lr = 0.01
I1202 13:10:45.546383 13416 solver.cpp:218] Iteration 6900 (25.0117 iter/s, 3.99813s/100 iters), loss = 0.160185
I1202 13:10:45.546383 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:10:45.546383 13416 solver.cpp:237]     Train net output #1: loss = 0.160185 (* 1 = 0.160185 loss)
I1202 13:10:45.546383 13416 sgd_solver.cpp:105] Iteration 6900, lr = 0.01
I1202 13:10:49.354172 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:10:49.509685 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_7000.caffemodel
I1202 13:10:49.520684 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_7000.solverstate
I1202 13:10:49.524685 13416 solver.cpp:330] Iteration 7000, Testing net (#0)
I1202 13:10:49.524685 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:10:50.510749  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:10:50.549257 13416 solver.cpp:397]     Test net output #0: accuracy = 0.8854
I1202 13:10:50.549257 13416 solver.cpp:397]     Test net output #1: loss = 0.340361 (* 1 = 0.340361 loss)
I1202 13:10:50.587752 13416 solver.cpp:218] Iteration 7000 (19.8374 iter/s, 5.04097s/100 iters), loss = 0.165881
I1202 13:10:50.587752 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:10:50.587752 13416 solver.cpp:237]     Train net output #1: loss = 0.165881 (* 1 = 0.165881 loss)
I1202 13:10:50.587752 13416 sgd_solver.cpp:105] Iteration 7000, lr = 0.01
I1202 13:10:54.576035 13416 solver.cpp:218] Iteration 7100 (25.0736 iter/s, 3.98827s/100 iters), loss = 0.206194
I1202 13:10:54.576035 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:10:54.576536 13416 solver.cpp:237]     Train net output #1: loss = 0.206194 (* 1 = 0.206194 loss)
I1202 13:10:54.576536 13416 sgd_solver.cpp:105] Iteration 7100, lr = 0.01
I1202 13:10:58.601671 13416 solver.cpp:218] Iteration 7200 (24.8469 iter/s, 4.02464s/100 iters), loss = 0.219064
I1202 13:10:58.601671 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:10:58.601671 13416 solver.cpp:237]     Train net output #1: loss = 0.219064 (* 1 = 0.219064 loss)
I1202 13:10:58.601671 13416 sgd_solver.cpp:105] Iteration 7200, lr = 0.01
I1202 13:11:02.605945 13416 solver.cpp:218] Iteration 7300 (24.9716 iter/s, 4.00455s/100 iters), loss = 0.243644
I1202 13:11:02.605945 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1202 13:11:02.605945 13416 solver.cpp:237]     Train net output #1: loss = 0.243644 (* 1 = 0.243644 loss)
I1202 13:11:02.605945 13416 sgd_solver.cpp:105] Iteration 7300, lr = 0.01
I1202 13:11:06.597189 13416 solver.cpp:218] Iteration 7400 (25.0584 iter/s, 3.99067s/100 iters), loss = 0.156343
I1202 13:11:06.597189 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:11:06.597189 13416 solver.cpp:237]     Train net output #1: loss = 0.156343 (* 1 = 0.156343 loss)
I1202 13:11:06.597189 13416 sgd_solver.cpp:105] Iteration 7400, lr = 0.01
I1202 13:11:10.399422 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:11:10.556427 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_7500.caffemodel
I1202 13:11:10.590471 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_7500.solverstate
I1202 13:11:10.594470 13416 solver.cpp:330] Iteration 7500, Testing net (#0)
I1202 13:11:10.594470 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:11:11.582535  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:11:11.621534 13416 solver.cpp:397]     Test net output #0: accuracy = 0.8804
I1202 13:11:11.621534 13416 solver.cpp:397]     Test net output #1: loss = 0.34538 (* 1 = 0.34538 loss)
I1202 13:11:11.659533 13416 solver.cpp:218] Iteration 7500 (19.7571 iter/s, 5.06148s/100 iters), loss = 0.254817
I1202 13:11:11.659533 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1202 13:11:11.659533 13416 solver.cpp:237]     Train net output #1: loss = 0.254817 (* 1 = 0.254817 loss)
I1202 13:11:11.659533 13416 sgd_solver.cpp:105] Iteration 7500, lr = 0.01
I1202 13:11:15.656832 13416 solver.cpp:218] Iteration 7600 (25.0179 iter/s, 3.99714s/100 iters), loss = 0.291496
I1202 13:11:15.656832 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1202 13:11:15.656832 13416 solver.cpp:237]     Train net output #1: loss = 0.291496 (* 1 = 0.291496 loss)
I1202 13:11:15.656832 13416 sgd_solver.cpp:105] Iteration 7600, lr = 0.01
I1202 13:11:19.657117 13416 solver.cpp:218] Iteration 7700 (25.0016 iter/s, 3.99974s/100 iters), loss = 0.180799
I1202 13:11:19.657117 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:11:19.657117 13416 solver.cpp:237]     Train net output #1: loss = 0.180799 (* 1 = 0.180799 loss)
I1202 13:11:19.657117 13416 sgd_solver.cpp:105] Iteration 7700, lr = 0.01
I1202 13:11:23.655323 13416 solver.cpp:218] Iteration 7800 (25.0144 iter/s, 3.99769s/100 iters), loss = 0.291114
I1202 13:11:23.655323 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1202 13:11:23.655323 13416 solver.cpp:237]     Train net output #1: loss = 0.291114 (* 1 = 0.291114 loss)
I1202 13:11:23.655323 13416 sgd_solver.cpp:105] Iteration 7800, lr = 0.01
I1202 13:11:27.644888 13416 solver.cpp:218] Iteration 7900 (25.066 iter/s, 3.98947s/100 iters), loss = 0.211484
I1202 13:11:27.644888 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1202 13:11:27.644888 13416 solver.cpp:237]     Train net output #1: loss = 0.211484 (* 1 = 0.211484 loss)
I1202 13:11:27.644888 13416 sgd_solver.cpp:105] Iteration 7900, lr = 0.01
I1202 13:11:31.432140 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:11:31.588145 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_8000.caffemodel
I1202 13:11:31.598649 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_8000.solverstate
I1202 13:11:31.602649 13416 solver.cpp:330] Iteration 8000, Testing net (#0)
I1202 13:11:31.602649 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:11:32.590220  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:11:32.628224 13416 solver.cpp:397]     Test net output #0: accuracy = 0.8822
I1202 13:11:32.628224 13416 solver.cpp:397]     Test net output #1: loss = 0.341452 (* 1 = 0.341452 loss)
I1202 13:11:32.667224 13416 solver.cpp:218] Iteration 8000 (19.9139 iter/s, 5.02162s/100 iters), loss = 0.190643
I1202 13:11:32.667224 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:11:32.667224 13416 solver.cpp:237]     Train net output #1: loss = 0.190643 (* 1 = 0.190643 loss)
I1202 13:11:32.667224 13416 sgd_solver.cpp:105] Iteration 8000, lr = 0.01
I1202 13:11:36.650629 13416 solver.cpp:218] Iteration 8100 (25.1023 iter/s, 3.98369s/100 iters), loss = 0.254948
I1202 13:11:36.650629 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:11:36.650629 13416 solver.cpp:237]     Train net output #1: loss = 0.254948 (* 1 = 0.254948 loss)
I1202 13:11:36.650629 13416 sgd_solver.cpp:105] Iteration 8100, lr = 0.01
I1202 13:11:40.637892 13416 solver.cpp:218] Iteration 8200 (25.0853 iter/s, 3.9864s/100 iters), loss = 0.239494
I1202 13:11:40.637892 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:11:40.637892 13416 solver.cpp:237]     Train net output #1: loss = 0.239494 (* 1 = 0.239494 loss)
I1202 13:11:40.637892 13416 sgd_solver.cpp:105] Iteration 8200, lr = 0.01
I1202 13:11:44.619138 13416 solver.cpp:218] Iteration 8300 (25.1207 iter/s, 3.98078s/100 iters), loss = 0.243831
I1202 13:11:44.619138 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1202 13:11:44.619138 13416 solver.cpp:237]     Train net output #1: loss = 0.243831 (* 1 = 0.243831 loss)
I1202 13:11:44.619138 13416 sgd_solver.cpp:105] Iteration 8300, lr = 0.01
I1202 13:11:48.609994 13416 solver.cpp:218] Iteration 8400 (25.0575 iter/s, 3.99081s/100 iters), loss = 0.162715
I1202 13:11:48.609994 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:11:48.609994 13416 solver.cpp:237]     Train net output #1: loss = 0.162715 (* 1 = 0.162715 loss)
I1202 13:11:48.609994 13416 sgd_solver.cpp:105] Iteration 8400, lr = 0.01
I1202 13:11:52.402995 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:11:52.559010 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_8500.caffemodel
I1202 13:11:52.570011 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_8500.solverstate
I1202 13:11:52.574012 13416 solver.cpp:330] Iteration 8500, Testing net (#0)
I1202 13:11:52.574012 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:11:53.557082  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:11:53.595082 13416 solver.cpp:397]     Test net output #0: accuracy = 0.8861
I1202 13:11:53.596082 13416 solver.cpp:397]     Test net output #1: loss = 0.336631 (* 1 = 0.336631 loss)
I1202 13:11:53.634093 13416 solver.cpp:218] Iteration 8500 (19.9066 iter/s, 5.02345s/100 iters), loss = 0.219398
I1202 13:11:53.634093 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:11:53.634093 13416 solver.cpp:237]     Train net output #1: loss = 0.219398 (* 1 = 0.219398 loss)
I1202 13:11:53.634093 13416 sgd_solver.cpp:105] Iteration 8500, lr = 0.01
I1202 13:11:57.632814 13416 solver.cpp:218] Iteration 8600 (25.0082 iter/s, 3.99868s/100 iters), loss = 0.253085
I1202 13:11:57.633314 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:11:57.633314 13416 solver.cpp:237]     Train net output #1: loss = 0.253085 (* 1 = 0.253085 loss)
I1202 13:11:57.633314 13416 sgd_solver.cpp:105] Iteration 8600, lr = 0.01
I1202 13:12:01.627586 13416 solver.cpp:218] Iteration 8700 (25.0361 iter/s, 3.99423s/100 iters), loss = 0.230485
I1202 13:12:01.627586 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1202 13:12:01.627586 13416 solver.cpp:237]     Train net output #1: loss = 0.230485 (* 1 = 0.230485 loss)
I1202 13:12:01.627586 13416 sgd_solver.cpp:105] Iteration 8700, lr = 0.01
I1202 13:12:05.624828 13416 solver.cpp:218] Iteration 8800 (25.0211 iter/s, 3.99662s/100 iters), loss = 0.181053
I1202 13:12:05.624828 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:12:05.624828 13416 solver.cpp:237]     Train net output #1: loss = 0.181053 (* 1 = 0.181053 loss)
I1202 13:12:05.624828 13416 sgd_solver.cpp:105] Iteration 8800, lr = 0.01
I1202 13:12:09.623169 13416 solver.cpp:218] Iteration 8900 (25.0135 iter/s, 3.99783s/100 iters), loss = 0.146441
I1202 13:12:09.623169 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:12:09.623169 13416 solver.cpp:237]     Train net output #1: loss = 0.146441 (* 1 = 0.146441 loss)
I1202 13:12:09.623169 13416 sgd_solver.cpp:105] Iteration 8900, lr = 0.01
I1202 13:12:13.446549 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:12:13.602562 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_9000.caffemodel
I1202 13:12:13.640561 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_9000.solverstate
I1202 13:12:13.644562 13416 solver.cpp:330] Iteration 9000, Testing net (#0)
I1202 13:12:13.644562 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:12:14.636653  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:12:14.675652 13416 solver.cpp:397]     Test net output #0: accuracy = 0.8964
I1202 13:12:14.675652 13416 solver.cpp:397]     Test net output #1: loss = 0.312094 (* 1 = 0.312094 loss)
I1202 13:12:14.714658 13416 solver.cpp:218] Iteration 9000 (19.6407 iter/s, 5.09147s/100 iters), loss = 0.224271
I1202 13:12:14.714658 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:12:14.714658 13416 solver.cpp:237]     Train net output #1: loss = 0.224271 (* 1 = 0.224271 loss)
I1202 13:12:14.714658 13416 sgd_solver.cpp:105] Iteration 9000, lr = 0.01
I1202 13:12:18.713935 13416 solver.cpp:218] Iteration 9100 (25.0045 iter/s, 3.99929s/100 iters), loss = 0.253016
I1202 13:12:18.713935 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1202 13:12:18.713935 13416 solver.cpp:237]     Train net output #1: loss = 0.253017 (* 1 = 0.253017 loss)
I1202 13:12:18.713935 13416 sgd_solver.cpp:105] Iteration 9100, lr = 0.01
I1202 13:12:22.717655 13416 solver.cpp:218] Iteration 9200 (24.9817 iter/s, 4.00293s/100 iters), loss = 0.226669
I1202 13:12:22.717655 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1202 13:12:22.717655 13416 solver.cpp:237]     Train net output #1: loss = 0.226669 (* 1 = 0.226669 loss)
I1202 13:12:22.717655 13416 sgd_solver.cpp:105] Iteration 9200, lr = 0.01
I1202 13:12:26.710940 13416 solver.cpp:218] Iteration 9300 (25.0437 iter/s, 3.99303s/100 iters), loss = 0.252991
I1202 13:12:26.710940 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1202 13:12:26.710940 13416 solver.cpp:237]     Train net output #1: loss = 0.252991 (* 1 = 0.252991 loss)
I1202 13:12:26.710940 13416 sgd_solver.cpp:105] Iteration 9300, lr = 0.01
I1202 13:12:30.704205 13416 solver.cpp:218] Iteration 9400 (25.0478 iter/s, 3.99236s/100 iters), loss = 0.169749
I1202 13:12:30.704205 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:12:30.704205 13416 solver.cpp:237]     Train net output #1: loss = 0.169749 (* 1 = 0.169749 loss)
I1202 13:12:30.704205 13416 sgd_solver.cpp:105] Iteration 9400, lr = 0.01
I1202 13:12:34.505961 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:12:34.662961 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_9500.caffemodel
I1202 13:12:34.698488 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_9500.solverstate
I1202 13:12:34.702491 13416 solver.cpp:330] Iteration 9500, Testing net (#0)
I1202 13:12:34.702491 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:12:35.685545  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:12:35.724545 13416 solver.cpp:397]     Test net output #0: accuracy = 0.8796
I1202 13:12:35.724545 13416 solver.cpp:397]     Test net output #1: loss = 0.348317 (* 1 = 0.348317 loss)
I1202 13:12:35.763046 13416 solver.cpp:218] Iteration 9500 (19.7684 iter/s, 5.05859s/100 iters), loss = 0.203021
I1202 13:12:35.763046 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:12:35.763046 13416 solver.cpp:237]     Train net output #1: loss = 0.203021 (* 1 = 0.203021 loss)
I1202 13:12:35.763046 13416 sgd_solver.cpp:46] MultiStep Status: Iteration 9500, step = 2
I1202 13:12:35.763046 13416 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I1202 13:12:39.745791 13416 solver.cpp:218] Iteration 9600 (25.1118 iter/s, 3.98219s/100 iters), loss = 0.258658
I1202 13:12:39.745791 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1202 13:12:39.745791 13416 solver.cpp:237]     Train net output #1: loss = 0.258658 (* 1 = 0.258658 loss)
I1202 13:12:39.745791 13416 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I1202 13:12:43.731148 13416 solver.cpp:218] Iteration 9700 (25.095 iter/s, 3.98485s/100 iters), loss = 0.179788
I1202 13:12:43.731148 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:12:43.731148 13416 solver.cpp:237]     Train net output #1: loss = 0.179788 (* 1 = 0.179788 loss)
I1202 13:12:43.731148 13416 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I1202 13:12:47.716337 13416 solver.cpp:218] Iteration 9800 (25.0949 iter/s, 3.98487s/100 iters), loss = 0.239987
I1202 13:12:47.716337 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1202 13:12:47.716337 13416 solver.cpp:237]     Train net output #1: loss = 0.239987 (* 1 = 0.239987 loss)
I1202 13:12:47.716337 13416 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I1202 13:12:51.694084 13416 solver.cpp:218] Iteration 9900 (25.1381 iter/s, 3.97803s/100 iters), loss = 0.1705
I1202 13:12:51.694084 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:12:51.694084 13416 solver.cpp:237]     Train net output #1: loss = 0.1705 (* 1 = 0.1705 loss)
I1202 13:12:51.694084 13416 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I1202 13:12:55.477319 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:12:55.633333 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_10000.caffemodel
I1202 13:12:55.645334 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_10000.solverstate
I1202 13:12:55.649334 13416 solver.cpp:330] Iteration 10000, Testing net (#0)
I1202 13:12:55.649334 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:12:56.631389  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:12:56.670389 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9003
I1202 13:12:56.670389 13416 solver.cpp:397]     Test net output #1: loss = 0.295231 (* 1 = 0.295231 loss)
I1202 13:12:56.708389 13416 solver.cpp:218] Iteration 10000 (19.9445 iter/s, 5.01393s/100 iters), loss = 0.15694
I1202 13:12:56.709389 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:12:56.709389 13416 solver.cpp:237]     Train net output #1: loss = 0.15694 (* 1 = 0.15694 loss)
I1202 13:12:56.709389 13416 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I1202 13:13:00.698671 13416 solver.cpp:218] Iteration 10100 (25.0645 iter/s, 3.98971s/100 iters), loss = 0.240409
I1202 13:13:00.699671 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:13:00.699671 13416 solver.cpp:237]     Train net output #1: loss = 0.240409 (* 1 = 0.240409 loss)
I1202 13:13:00.699671 13416 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I1202 13:13:04.688127 13416 solver.cpp:218] Iteration 10200 (25.0706 iter/s, 3.98874s/100 iters), loss = 0.208374
I1202 13:13:04.688127 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:13:04.688127 13416 solver.cpp:237]     Train net output #1: loss = 0.208374 (* 1 = 0.208374 loss)
I1202 13:13:04.688127 13416 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I1202 13:13:08.681351 13416 solver.cpp:218] Iteration 10300 (25.0474 iter/s, 3.99243s/100 iters), loss = 0.160172
I1202 13:13:08.681351 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:13:08.681351 13416 solver.cpp:237]     Train net output #1: loss = 0.160172 (* 1 = 0.160172 loss)
I1202 13:13:08.681351 13416 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I1202 13:13:12.668597 13416 solver.cpp:218] Iteration 10400 (25.0798 iter/s, 3.98727s/100 iters), loss = 0.110657
I1202 13:13:12.668597 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:13:12.668597 13416 solver.cpp:237]     Train net output #1: loss = 0.110657 (* 1 = 0.110657 loss)
I1202 13:13:12.668597 13416 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I1202 13:13:16.463806 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:13:16.620816 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_10500.caffemodel
I1202 13:13:16.657832 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_10500.solverstate
I1202 13:13:16.661833 13416 solver.cpp:330] Iteration 10500, Testing net (#0)
I1202 13:13:16.661833 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:13:17.644891  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:13:17.682890 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9028
I1202 13:13:17.682890 13416 solver.cpp:397]     Test net output #1: loss = 0.293277 (* 1 = 0.293277 loss)
I1202 13:13:17.721892 13416 solver.cpp:218] Iteration 10500 (19.7916 iter/s, 5.05265s/100 iters), loss = 0.182199
I1202 13:13:17.721892 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:13:17.721892 13416 solver.cpp:237]     Train net output #1: loss = 0.182199 (* 1 = 0.182199 loss)
I1202 13:13:17.721892 13416 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I1202 13:13:21.711257 13416 solver.cpp:218] Iteration 10600 (25.0699 iter/s, 3.98885s/100 iters), loss = 0.198715
I1202 13:13:21.711257 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:13:21.711257 13416 solver.cpp:237]     Train net output #1: loss = 0.198715 (* 1 = 0.198715 loss)
I1202 13:13:21.711257 13416 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I1202 13:13:25.699478 13416 solver.cpp:218] Iteration 10700 (25.0761 iter/s, 3.98785s/100 iters), loss = 0.174296
I1202 13:13:25.699478 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:13:25.699478 13416 solver.cpp:237]     Train net output #1: loss = 0.174296 (* 1 = 0.174296 loss)
I1202 13:13:25.699478 13416 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I1202 13:13:29.684595 13416 solver.cpp:218] Iteration 10800 (25.0929 iter/s, 3.98518s/100 iters), loss = 0.214851
I1202 13:13:29.684595 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:13:29.684595 13416 solver.cpp:237]     Train net output #1: loss = 0.214851 (* 1 = 0.214851 loss)
I1202 13:13:29.684595 13416 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I1202 13:13:33.668859 13416 solver.cpp:218] Iteration 10900 (25.1037 iter/s, 3.98347s/100 iters), loss = 0.122919
I1202 13:13:33.668859 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:13:33.668859 13416 solver.cpp:237]     Train net output #1: loss = 0.122919 (* 1 = 0.122919 loss)
I1202 13:13:33.668859 13416 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I1202 13:13:37.457115 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:13:37.612124 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_11000.caffemodel
I1202 13:13:37.623630 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_11000.solverstate
I1202 13:13:37.627130 13416 solver.cpp:330] Iteration 11000, Testing net (#0)
I1202 13:13:37.627130 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:13:38.609197  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:13:38.648216 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9033
I1202 13:13:38.648216 13416 solver.cpp:397]     Test net output #1: loss = 0.291122 (* 1 = 0.291122 loss)
I1202 13:13:38.686214 13416 solver.cpp:218] Iteration 11000 (19.9328 iter/s, 5.01685s/100 iters), loss = 0.116085
I1202 13:13:38.686214 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:13:38.686214 13416 solver.cpp:237]     Train net output #1: loss = 0.116085 (* 1 = 0.116085 loss)
I1202 13:13:38.686214 13416 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I1202 13:13:42.667538 13416 solver.cpp:218] Iteration 11100 (25.1168 iter/s, 3.9814s/100 iters), loss = 0.225834
I1202 13:13:42.667538 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:13:42.667538 13416 solver.cpp:237]     Train net output #1: loss = 0.225834 (* 1 = 0.225834 loss)
I1202 13:13:42.667538 13416 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I1202 13:13:46.650800 13416 solver.cpp:218] Iteration 11200 (25.1087 iter/s, 3.98268s/100 iters), loss = 0.172809
I1202 13:13:46.650800 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:13:46.650800 13416 solver.cpp:237]     Train net output #1: loss = 0.172809 (* 1 = 0.172809 loss)
I1202 13:13:46.650800 13416 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I1202 13:13:50.632525 13416 solver.cpp:218] Iteration 11300 (25.1202 iter/s, 3.98086s/100 iters), loss = 0.162206
I1202 13:13:50.632525 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:13:50.632525 13416 solver.cpp:237]     Train net output #1: loss = 0.162206 (* 1 = 0.162206 loss)
I1202 13:13:50.632525 13416 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I1202 13:13:54.613718 13416 solver.cpp:218] Iteration 11400 (25.1204 iter/s, 3.98083s/100 iters), loss = 0.126876
I1202 13:13:54.613718 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:13:54.613718 13416 solver.cpp:237]     Train net output #1: loss = 0.126876 (* 1 = 0.126876 loss)
I1202 13:13:54.613718 13416 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I1202 13:13:58.404423 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:13:58.560427 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_11500.caffemodel
I1202 13:13:58.582943 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_11500.solverstate
I1202 13:13:58.586447 13416 solver.cpp:330] Iteration 11500, Testing net (#0)
I1202 13:13:58.586447 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:13:59.569524  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:13:59.607529 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9034
I1202 13:13:59.607529 13416 solver.cpp:397]     Test net output #1: loss = 0.29182 (* 1 = 0.29182 loss)
I1202 13:13:59.645529 13416 solver.cpp:218] Iteration 11500 (19.8723 iter/s, 5.03212s/100 iters), loss = 0.185879
I1202 13:13:59.645529 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:13:59.645529 13416 solver.cpp:237]     Train net output #1: loss = 0.185879 (* 1 = 0.185879 loss)
I1202 13:13:59.645529 13416 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I1202 13:14:03.629770 13416 solver.cpp:218] Iteration 11600 (25.1038 iter/s, 3.98345s/100 iters), loss = 0.225696
I1202 13:14:03.629770 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:14:03.629770 13416 solver.cpp:237]     Train net output #1: loss = 0.225696 (* 1 = 0.225696 loss)
I1202 13:14:03.629770 13416 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I1202 13:14:07.618284 13416 solver.cpp:218] Iteration 11700 (25.0722 iter/s, 3.98848s/100 iters), loss = 0.16857
I1202 13:14:07.618284 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:14:07.618284 13416 solver.cpp:237]     Train net output #1: loss = 0.16857 (* 1 = 0.16857 loss)
I1202 13:14:07.618284 13416 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I1202 13:14:11.609701 13416 solver.cpp:218] Iteration 11800 (25.0581 iter/s, 3.99072s/100 iters), loss = 0.18087
I1202 13:14:11.609701 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:14:11.609701 13416 solver.cpp:237]     Train net output #1: loss = 0.18087 (* 1 = 0.18087 loss)
I1202 13:14:11.609701 13416 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I1202 13:14:15.599953 13416 solver.cpp:218] Iteration 11900 (25.0618 iter/s, 3.99014s/100 iters), loss = 0.157906
I1202 13:14:15.599953 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:14:15.599953 13416 solver.cpp:237]     Train net output #1: loss = 0.157906 (* 1 = 0.157906 loss)
I1202 13:14:15.599953 13416 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I1202 13:14:19.393203 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:14:19.549209 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_12000.caffemodel
I1202 13:14:19.560216 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_12000.solverstate
I1202 13:14:19.564211 13416 solver.cpp:330] Iteration 12000, Testing net (#0)
I1202 13:14:19.564211 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:14:20.546273  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:14:20.584776 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9045
I1202 13:14:20.584776 13416 solver.cpp:397]     Test net output #1: loss = 0.290232 (* 1 = 0.290232 loss)
I1202 13:14:20.622279 13416 solver.cpp:218] Iteration 12000 (19.9134 iter/s, 5.02174s/100 iters), loss = 0.210117
I1202 13:14:20.622279 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:14:20.622279 13416 solver.cpp:237]     Train net output #1: loss = 0.210117 (* 1 = 0.210117 loss)
I1202 13:14:20.622279 13416 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I1202 13:14:24.605512 13416 solver.cpp:218] Iteration 12100 (25.108 iter/s, 3.9828s/100 iters), loss = 0.18354
I1202 13:14:24.605512 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:14:24.605512 13416 solver.cpp:237]     Train net output #1: loss = 0.18354 (* 1 = 0.18354 loss)
I1202 13:14:24.605512 13416 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I1202 13:14:28.593621 13416 solver.cpp:218] Iteration 12200 (25.0743 iter/s, 3.98814s/100 iters), loss = 0.174227
I1202 13:14:28.593621 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:14:28.593621 13416 solver.cpp:237]     Train net output #1: loss = 0.174227 (* 1 = 0.174227 loss)
I1202 13:14:28.593621 13416 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I1202 13:14:32.565143 13416 solver.cpp:218] Iteration 12300 (25.1803 iter/s, 3.97136s/100 iters), loss = 0.135757
I1202 13:14:32.565143 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:14:32.565143 13416 solver.cpp:237]     Train net output #1: loss = 0.135757 (* 1 = 0.135757 loss)
I1202 13:14:32.565143 13416 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I1202 13:14:36.545384 13416 solver.cpp:218] Iteration 12400 (25.1301 iter/s, 3.9793s/100 iters), loss = 0.103558
I1202 13:14:36.545384 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:14:36.545384 13416 solver.cpp:237]     Train net output #1: loss = 0.103558 (* 1 = 0.103558 loss)
I1202 13:14:36.545384 13416 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I1202 13:14:40.331657 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:14:40.487663 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_12500.caffemodel
I1202 13:14:40.525696 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_12500.solverstate
I1202 13:14:40.528697 13416 solver.cpp:330] Iteration 12500, Testing net (#0)
I1202 13:14:40.528697 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:14:41.511764  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:14:41.550762 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9029
I1202 13:14:41.550762 13416 solver.cpp:397]     Test net output #1: loss = 0.289749 (* 1 = 0.289749 loss)
I1202 13:14:41.588762 13416 solver.cpp:218] Iteration 12500 (19.8302 iter/s, 5.04281s/100 iters), loss = 0.126242
I1202 13:14:41.588762 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:14:41.588762 13416 solver.cpp:237]     Train net output #1: loss = 0.126242 (* 1 = 0.126242 loss)
I1202 13:14:41.588762 13416 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I1202 13:14:45.569000 13416 solver.cpp:218] Iteration 12600 (25.1219 iter/s, 3.98058s/100 iters), loss = 0.236271
I1202 13:14:45.569000 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:14:45.570001 13416 solver.cpp:237]     Train net output #1: loss = 0.236271 (* 1 = 0.236271 loss)
I1202 13:14:45.570001 13416 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I1202 13:14:49.550261 13416 solver.cpp:218] Iteration 12700 (25.1202 iter/s, 3.98086s/100 iters), loss = 0.182909
I1202 13:14:49.550261 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:14:49.550261 13416 solver.cpp:237]     Train net output #1: loss = 0.182909 (* 1 = 0.182909 loss)
I1202 13:14:49.550261 13416 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I1202 13:14:53.533540 13416 solver.cpp:218] Iteration 12800 (25.108 iter/s, 3.9828s/100 iters), loss = 0.146833
I1202 13:14:53.533540 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:14:53.533540 13416 solver.cpp:237]     Train net output #1: loss = 0.146833 (* 1 = 0.146833 loss)
I1202 13:14:53.533540 13416 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I1202 13:14:57.511812 13416 solver.cpp:218] Iteration 12900 (25.1418 iter/s, 3.97744s/100 iters), loss = 0.112702
I1202 13:14:57.511812 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:14:57.511812 13416 solver.cpp:237]     Train net output #1: loss = 0.112702 (* 1 = 0.112702 loss)
I1202 13:14:57.511812 13416 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I1202 13:15:01.300057 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:15:01.455065 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_13000.caffemodel
I1202 13:15:01.466065 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_13000.solverstate
I1202 13:15:01.470067 13416 solver.cpp:330] Iteration 13000, Testing net (#0)
I1202 13:15:01.470067 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:15:02.454135  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:15:02.492136 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9038
I1202 13:15:02.492136 13416 solver.cpp:397]     Test net output #1: loss = 0.29028 (* 1 = 0.29028 loss)
I1202 13:15:02.530144 13416 solver.cpp:218] Iteration 13000 (19.9272 iter/s, 5.01826s/100 iters), loss = 0.168544
I1202 13:15:02.530144 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:15:02.530144 13416 solver.cpp:237]     Train net output #1: loss = 0.168544 (* 1 = 0.168544 loss)
I1202 13:15:02.530144 13416 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I1202 13:15:06.511399 13416 solver.cpp:218] Iteration 13100 (25.1203 iter/s, 3.98084s/100 iters), loss = 0.211812
I1202 13:15:06.511399 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:15:06.511399 13416 solver.cpp:237]     Train net output #1: loss = 0.211812 (* 1 = 0.211812 loss)
I1202 13:15:06.511399 13416 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I1202 13:15:10.490665 13416 solver.cpp:218] Iteration 13200 (25.134 iter/s, 3.97867s/100 iters), loss = 0.140683
I1202 13:15:10.490665 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:15:10.490665 13416 solver.cpp:237]     Train net output #1: loss = 0.140683 (* 1 = 0.140683 loss)
I1202 13:15:10.490665 13416 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I1202 13:15:14.468947 13416 solver.cpp:218] Iteration 13300 (25.1366 iter/s, 3.97827s/100 iters), loss = 0.180063
I1202 13:15:14.468947 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:15:14.468947 13416 solver.cpp:237]     Train net output #1: loss = 0.180063 (* 1 = 0.180063 loss)
I1202 13:15:14.468947 13416 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I1202 13:15:18.449196 13416 solver.cpp:218] Iteration 13400 (25.1307 iter/s, 3.97919s/100 iters), loss = 0.108566
I1202 13:15:18.449196 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:15:18.449196 13416 solver.cpp:237]     Train net output #1: loss = 0.108566 (* 1 = 0.108566 loss)
I1202 13:15:18.449196 13416 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I1202 13:15:22.235429 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:15:22.392433 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_13500.caffemodel
I1202 13:15:22.416455 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_13500.solverstate
I1202 13:15:22.419454 13416 solver.cpp:330] Iteration 13500, Testing net (#0)
I1202 13:15:22.420455 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:15:23.403014  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:15:23.441519 13416 solver.cpp:397]     Test net output #0: accuracy = 0.904
I1202 13:15:23.441519 13416 solver.cpp:397]     Test net output #1: loss = 0.289853 (* 1 = 0.289853 loss)
I1202 13:15:23.479517 13416 solver.cpp:218] Iteration 13500 (19.8793 iter/s, 5.03035s/100 iters), loss = 0.157316
I1202 13:15:23.479517 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:15:23.479517 13416 solver.cpp:237]     Train net output #1: loss = 0.157316 (* 1 = 0.157316 loss)
I1202 13:15:23.479517 13416 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I1202 13:15:27.463798 13416 solver.cpp:218] Iteration 13600 (25.0989 iter/s, 3.98424s/100 iters), loss = 0.189203
I1202 13:15:27.463798 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:15:27.463798 13416 solver.cpp:237]     Train net output #1: loss = 0.189203 (* 1 = 0.189203 loss)
I1202 13:15:27.463798 13416 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I1202 13:15:31.452040 13416 solver.cpp:218] Iteration 13700 (25.0783 iter/s, 3.98752s/100 iters), loss = 0.163424
I1202 13:15:31.452040 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:15:31.452040 13416 solver.cpp:237]     Train net output #1: loss = 0.163424 (* 1 = 0.163424 loss)
I1202 13:15:31.452040 13416 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I1202 13:15:35.439270 13416 solver.cpp:218] Iteration 13800 (25.0806 iter/s, 3.98714s/100 iters), loss = 0.182013
I1202 13:15:35.439270 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:15:35.439270 13416 solver.cpp:237]     Train net output #1: loss = 0.182013 (* 1 = 0.182013 loss)
I1202 13:15:35.439270 13416 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I1202 13:15:39.427567 13416 solver.cpp:218] Iteration 13900 (25.0795 iter/s, 3.98732s/100 iters), loss = 0.112993
I1202 13:15:39.427567 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1202 13:15:39.427567 13416 solver.cpp:237]     Train net output #1: loss = 0.112993 (* 1 = 0.112993 loss)
I1202 13:15:39.427567 13416 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I1202 13:15:43.221828 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:15:43.377833 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_14000.caffemodel
I1202 13:15:43.387833 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_14000.solverstate
I1202 13:15:43.391834 13416 solver.cpp:330] Iteration 14000, Testing net (#0)
I1202 13:15:43.391834 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:15:44.375916  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:15:44.413923 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9027
I1202 13:15:44.413923 13416 solver.cpp:397]     Test net output #1: loss = 0.289609 (* 1 = 0.289609 loss)
I1202 13:15:44.451925 13416 solver.cpp:218] Iteration 14000 (19.9021 iter/s, 5.02459s/100 iters), loss = 0.154795
I1202 13:15:44.451925 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:15:44.451925 13416 solver.cpp:237]     Train net output #1: loss = 0.154795 (* 1 = 0.154795 loss)
I1202 13:15:44.451925 13416 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I1202 13:15:48.432209 13416 solver.cpp:218] Iteration 14100 (25.1273 iter/s, 3.97974s/100 iters), loss = 0.216971
I1202 13:15:48.432209 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:15:48.432209 13416 solver.cpp:237]     Train net output #1: loss = 0.216971 (* 1 = 0.216971 loss)
I1202 13:15:48.432209 13416 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I1202 13:15:52.412159 13416 solver.cpp:218] Iteration 14200 (25.1294 iter/s, 3.9794s/100 iters), loss = 0.150923
I1202 13:15:52.412660 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:15:52.412660 13416 solver.cpp:237]     Train net output #1: loss = 0.150923 (* 1 = 0.150923 loss)
I1202 13:15:52.412660 13416 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I1202 13:15:56.398911 13416 solver.cpp:218] Iteration 14300 (25.086 iter/s, 3.98628s/100 iters), loss = 0.173523
I1202 13:15:56.398911 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:15:56.398911 13416 solver.cpp:237]     Train net output #1: loss = 0.173523 (* 1 = 0.173523 loss)
I1202 13:15:56.398911 13416 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I1202 13:16:00.379207 13416 solver.cpp:218] Iteration 14400 (25.1267 iter/s, 3.97983s/100 iters), loss = 0.127775
I1202 13:16:00.379207 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1202 13:16:00.379207 13416 solver.cpp:237]     Train net output #1: loss = 0.127775 (* 1 = 0.127775 loss)
I1202 13:16:00.379207 13416 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I1202 13:16:04.166447 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:16:04.322458 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_14500.caffemodel
I1202 13:16:04.358458 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_14500.solverstate
I1202 13:16:04.362459 13416 solver.cpp:330] Iteration 14500, Testing net (#0)
I1202 13:16:04.362459 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:16:05.344542  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:16:05.383548 13416 solver.cpp:397]     Test net output #0: accuracy = 0.904
I1202 13:16:05.383548 13416 solver.cpp:397]     Test net output #1: loss = 0.290032 (* 1 = 0.290032 loss)
I1202 13:16:05.421555 13416 solver.cpp:218] Iteration 14500 (19.8316 iter/s, 5.04245s/100 iters), loss = 0.146375
I1202 13:16:05.421555 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:16:05.421555 13416 solver.cpp:237]     Train net output #1: loss = 0.146375 (* 1 = 0.146375 loss)
I1202 13:16:05.421555 13416 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I1202 13:16:09.408792 13416 solver.cpp:218] Iteration 14600 (25.0875 iter/s, 3.98605s/100 iters), loss = 0.255565
I1202 13:16:09.408792 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:16:09.408792 13416 solver.cpp:237]     Train net output #1: loss = 0.255565 (* 1 = 0.255565 loss)
I1202 13:16:09.408792 13416 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I1202 13:16:13.388029 13416 solver.cpp:218] Iteration 14700 (25.1272 iter/s, 3.97975s/100 iters), loss = 0.184875
I1202 13:16:13.388029 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:16:13.388029 13416 solver.cpp:237]     Train net output #1: loss = 0.184875 (* 1 = 0.184875 loss)
I1202 13:16:13.388029 13416 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I1202 13:16:17.375272 13416 solver.cpp:218] Iteration 14800 (25.0861 iter/s, 3.98627s/100 iters), loss = 0.187565
I1202 13:16:17.375272 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:16:17.375272 13416 solver.cpp:237]     Train net output #1: loss = 0.187565 (* 1 = 0.187565 loss)
I1202 13:16:17.375272 13416 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I1202 13:16:21.359514 13416 solver.cpp:218] Iteration 14900 (25.0967 iter/s, 3.98458s/100 iters), loss = 0.128377
I1202 13:16:21.360515 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:16:21.360515 13416 solver.cpp:237]     Train net output #1: loss = 0.128377 (* 1 = 0.128377 loss)
I1202 13:16:21.360515 13416 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I1202 13:16:25.149766 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:16:25.305771 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_15000.caffemodel
I1202 13:16:25.316771 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_15000.solverstate
I1202 13:16:25.320771 13416 solver.cpp:330] Iteration 15000, Testing net (#0)
I1202 13:16:25.320771 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:16:26.303840  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:16:26.342842 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9048
I1202 13:16:26.342842 13416 solver.cpp:397]     Test net output #1: loss = 0.287599 (* 1 = 0.287599 loss)
I1202 13:16:26.380844 13416 solver.cpp:218] Iteration 15000 (19.9201 iter/s, 5.02005s/100 iters), loss = 0.152553
I1202 13:16:26.380844 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:16:26.380844 13416 solver.cpp:237]     Train net output #1: loss = 0.152553 (* 1 = 0.152553 loss)
I1202 13:16:26.380844 13416 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I1202 13:16:30.366097 13416 solver.cpp:218] Iteration 15100 (25.0912 iter/s, 3.98546s/100 iters), loss = 0.192021
I1202 13:16:30.366097 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:16:30.366097 13416 solver.cpp:237]     Train net output #1: loss = 0.192021 (* 1 = 0.192021 loss)
I1202 13:16:30.366097 13416 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I1202 13:16:34.350317 13416 solver.cpp:218] Iteration 15200 (25.1026 iter/s, 3.98366s/100 iters), loss = 0.183163
I1202 13:16:34.350317 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:16:34.350317 13416 solver.cpp:237]     Train net output #1: loss = 0.183164 (* 1 = 0.183164 loss)
I1202 13:16:34.350317 13416 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I1202 13:16:38.339907 13416 solver.cpp:218] Iteration 15300 (25.0669 iter/s, 3.98933s/100 iters), loss = 0.150394
I1202 13:16:38.340409 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:16:38.340409 13416 solver.cpp:237]     Train net output #1: loss = 0.150394 (* 1 = 0.150394 loss)
I1202 13:16:38.340409 13416 sgd_solver.cpp:46] MultiStep Status: Iteration 15300, step = 3
I1202 13:16:38.340409 13416 sgd_solver.cpp:105] Iteration 15300, lr = 0.0001
I1202 13:16:42.326136 13416 solver.cpp:218] Iteration 15400 (25.0916 iter/s, 3.98539s/100 iters), loss = 0.0824272
I1202 13:16:42.326136 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1202 13:16:42.326136 13416 solver.cpp:237]     Train net output #1: loss = 0.0824273 (* 1 = 0.0824273 loss)
I1202 13:16:42.326136 13416 sgd_solver.cpp:105] Iteration 15400, lr = 0.0001
I1202 13:16:46.117374 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:16:46.273397 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_15500.caffemodel
I1202 13:16:46.299396 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_15500.solverstate
I1202 13:16:46.303397 13416 solver.cpp:330] Iteration 15500, Testing net (#0)
I1202 13:16:46.303397 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:16:47.285461  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:16:47.324462 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9052
I1202 13:16:47.324462 13416 solver.cpp:397]     Test net output #1: loss = 0.289054 (* 1 = 0.289054 loss)
I1202 13:16:47.362465 13416 solver.cpp:218] Iteration 15500 (19.8564 iter/s, 5.03617s/100 iters), loss = 0.127583
I1202 13:16:47.362465 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:16:47.362465 13416 solver.cpp:237]     Train net output #1: loss = 0.127583 (* 1 = 0.127583 loss)
I1202 13:16:47.362465 13416 sgd_solver.cpp:105] Iteration 15500, lr = 0.0001
I1202 13:16:51.349182 13416 solver.cpp:218] Iteration 15600 (25.0815 iter/s, 3.987s/100 iters), loss = 0.20049
I1202 13:16:51.349182 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:16:51.349182 13416 solver.cpp:237]     Train net output #1: loss = 0.20049 (* 1 = 0.20049 loss)
I1202 13:16:51.349182 13416 sgd_solver.cpp:105] Iteration 15600, lr = 0.0001
I1202 13:16:55.337414 13416 solver.cpp:218] Iteration 15700 (25.0805 iter/s, 3.98715s/100 iters), loss = 0.172288
I1202 13:16:55.337414 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:16:55.337414 13416 solver.cpp:237]     Train net output #1: loss = 0.172288 (* 1 = 0.172288 loss)
I1202 13:16:55.337414 13416 sgd_solver.cpp:105] Iteration 15700, lr = 0.0001
I1202 13:16:59.324677 13416 solver.cpp:218] Iteration 15800 (25.0773 iter/s, 3.98768s/100 iters), loss = 0.164467
I1202 13:16:59.324677 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:16:59.324677 13416 solver.cpp:237]     Train net output #1: loss = 0.164467 (* 1 = 0.164467 loss)
I1202 13:16:59.325678 13416 sgd_solver.cpp:105] Iteration 15800, lr = 0.0001
I1202 13:17:03.311975 13416 solver.cpp:218] Iteration 15900 (25.0851 iter/s, 3.98643s/100 iters), loss = 0.135339
I1202 13:17:03.311975 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:17:03.311975 13416 solver.cpp:237]     Train net output #1: loss = 0.135339 (* 1 = 0.135339 loss)
I1202 13:17:03.311975 13416 sgd_solver.cpp:105] Iteration 15900, lr = 0.0001
I1202 13:17:07.105273 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:17:07.261811 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_16000.caffemodel
I1202 13:17:07.273315 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_16000.solverstate
I1202 13:17:07.277315 13416 solver.cpp:330] Iteration 16000, Testing net (#0)
I1202 13:17:07.277315 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:17:08.259891  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:17:08.298398 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9052
I1202 13:17:08.298398 13416 solver.cpp:397]     Test net output #1: loss = 0.288484 (* 1 = 0.288484 loss)
I1202 13:17:08.336395 13416 solver.cpp:218] Iteration 16000 (19.9033 iter/s, 5.02429s/100 iters), loss = 0.139947
I1202 13:17:08.336395 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:17:08.336395 13416 solver.cpp:237]     Train net output #1: loss = 0.139947 (* 1 = 0.139947 loss)
I1202 13:17:08.336395 13416 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I1202 13:17:12.320628 13416 solver.cpp:218] Iteration 16100 (25.0993 iter/s, 3.98418s/100 iters), loss = 0.196902
I1202 13:17:12.320628 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:17:12.320628 13416 solver.cpp:237]     Train net output #1: loss = 0.196902 (* 1 = 0.196902 loss)
I1202 13:17:12.321630 13416 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I1202 13:17:16.306864 13416 solver.cpp:218] Iteration 16200 (25.0926 iter/s, 3.98523s/100 iters), loss = 0.133012
I1202 13:17:16.306864 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:17:16.306864 13416 solver.cpp:237]     Train net output #1: loss = 0.133012 (* 1 = 0.133012 loss)
I1202 13:17:16.306864 13416 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I1202 13:17:20.291029 13416 solver.cpp:218] Iteration 16300 (25.0972 iter/s, 3.9845s/100 iters), loss = 0.189185
I1202 13:17:20.292031 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:17:20.292031 13416 solver.cpp:237]     Train net output #1: loss = 0.189185 (* 1 = 0.189185 loss)
I1202 13:17:20.292031 13416 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I1202 13:17:24.276262 13416 solver.cpp:218] Iteration 16400 (25.0982 iter/s, 3.98435s/100 iters), loss = 0.0783346
I1202 13:17:24.276262 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1202 13:17:24.276262 13416 solver.cpp:237]     Train net output #1: loss = 0.0783348 (* 1 = 0.0783348 loss)
I1202 13:17:24.276262 13416 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I1202 13:17:28.063535 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:17:28.220543 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_16500.caffemodel
I1202 13:17:28.258548 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_16500.solverstate
I1202 13:17:28.262046 13416 solver.cpp:330] Iteration 16500, Testing net (#0)
I1202 13:17:28.262046 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:17:29.243625  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:17:29.281630 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9054
I1202 13:17:29.281630 13416 solver.cpp:397]     Test net output #1: loss = 0.28794 (* 1 = 0.28794 loss)
I1202 13:17:29.320629 13416 solver.cpp:218] Iteration 16500 (19.8272 iter/s, 5.04358s/100 iters), loss = 0.153416
I1202 13:17:29.320629 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:17:29.320629 13416 solver.cpp:237]     Train net output #1: loss = 0.153417 (* 1 = 0.153417 loss)
I1202 13:17:29.320629 13416 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I1202 13:17:33.301858 13416 solver.cpp:218] Iteration 16600 (25.1137 iter/s, 3.98188s/100 iters), loss = 0.175235
I1202 13:17:33.302858 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:17:33.302858 13416 solver.cpp:237]     Train net output #1: loss = 0.175235 (* 1 = 0.175235 loss)
I1202 13:17:33.302858 13416 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I1202 13:17:37.286134 13416 solver.cpp:218] Iteration 16700 (25.1048 iter/s, 3.9833s/100 iters), loss = 0.186094
I1202 13:17:37.286134 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:17:37.286134 13416 solver.cpp:237]     Train net output #1: loss = 0.186094 (* 1 = 0.186094 loss)
I1202 13:17:37.286134 13416 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I1202 13:17:41.269346 13416 solver.cpp:218] Iteration 16800 (25.1084 iter/s, 3.98273s/100 iters), loss = 0.151508
I1202 13:17:41.269346 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:17:41.269346 13416 solver.cpp:237]     Train net output #1: loss = 0.151508 (* 1 = 0.151508 loss)
I1202 13:17:41.269346 13416 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I1202 13:17:45.247576 13416 solver.cpp:218] Iteration 16900 (25.1383 iter/s, 3.978s/100 iters), loss = 0.0957004
I1202 13:17:45.247576 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1202 13:17:45.247576 13416 solver.cpp:237]     Train net output #1: loss = 0.0957006 (* 1 = 0.0957006 loss)
I1202 13:17:45.247576 13416 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I1202 13:17:49.030838 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:17:49.186847 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_17000.caffemodel
I1202 13:17:49.198848 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_17000.solverstate
I1202 13:17:49.201846 13416 solver.cpp:330] Iteration 17000, Testing net (#0)
I1202 13:17:49.201846 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:17:50.183902  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:17:50.222900 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9054
I1202 13:17:50.222900 13416 solver.cpp:397]     Test net output #1: loss = 0.287936 (* 1 = 0.287936 loss)
I1202 13:17:50.261409 13416 solver.cpp:218] Iteration 17000 (19.9479 iter/s, 5.01305s/100 iters), loss = 0.12212
I1202 13:17:50.261409 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:17:50.261409 13416 solver.cpp:237]     Train net output #1: loss = 0.12212 (* 1 = 0.12212 loss)
I1202 13:17:50.261409 13416 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I1202 13:17:54.243126 13416 solver.cpp:218] Iteration 17100 (25.1165 iter/s, 3.98145s/100 iters), loss = 0.238915
I1202 13:17:54.243126 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:17:54.243126 13416 solver.cpp:237]     Train net output #1: loss = 0.238916 (* 1 = 0.238916 loss)
I1202 13:17:54.243126 13416 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I1202 13:17:58.225373 13416 solver.cpp:218] Iteration 17200 (25.1089 iter/s, 3.98265s/100 iters), loss = 0.174257
I1202 13:17:58.225373 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:17:58.225373 13416 solver.cpp:237]     Train net output #1: loss = 0.174258 (* 1 = 0.174258 loss)
I1202 13:17:58.225373 13416 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I1202 13:18:02.206687 13416 solver.cpp:218] Iteration 17300 (25.1206 iter/s, 3.9808s/100 iters), loss = 0.221428
I1202 13:18:02.206687 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:18:02.206687 13416 solver.cpp:237]     Train net output #1: loss = 0.221428 (* 1 = 0.221428 loss)
I1202 13:18:02.206687 13416 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I1202 13:18:06.184929 13416 solver.cpp:218] Iteration 17400 (25.1419 iter/s, 3.97743s/100 iters), loss = 0.131909
I1202 13:18:06.184929 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:18:06.184929 13416 solver.cpp:237]     Train net output #1: loss = 0.131909 (* 1 = 0.131909 loss)
I1202 13:18:06.184929 13416 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I1202 13:18:09.971168 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:18:10.126174 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_17500.caffemodel
I1202 13:18:10.141175 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_17500.solverstate
I1202 13:18:10.145174 13416 solver.cpp:330] Iteration 17500, Testing net (#0)
I1202 13:18:10.145174 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:18:11.128257  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:18:11.167254 13416 solver.cpp:397]     Test net output #0: accuracy = 0.905
I1202 13:18:11.167254 13416 solver.cpp:397]     Test net output #1: loss = 0.287977 (* 1 = 0.287977 loss)
I1202 13:18:11.205255 13416 solver.cpp:218] Iteration 17500 (19.9202 iter/s, 5.02002s/100 iters), loss = 0.163534
I1202 13:18:11.205255 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:18:11.205255 13416 solver.cpp:237]     Train net output #1: loss = 0.163534 (* 1 = 0.163534 loss)
I1202 13:18:11.205255 13416 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I1202 13:18:15.193567 13416 solver.cpp:218] Iteration 17600 (25.0722 iter/s, 3.98848s/100 iters), loss = 0.176038
I1202 13:18:15.193567 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:18:15.193567 13416 solver.cpp:237]     Train net output #1: loss = 0.176038 (* 1 = 0.176038 loss)
I1202 13:18:15.193567 13416 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I1202 13:18:19.179838 13416 solver.cpp:218] Iteration 17700 (25.0884 iter/s, 3.98591s/100 iters), loss = 0.123153
I1202 13:18:19.179838 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:18:19.179838 13416 solver.cpp:237]     Train net output #1: loss = 0.123153 (* 1 = 0.123153 loss)
I1202 13:18:19.179838 13416 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I1202 13:18:23.164095 13416 solver.cpp:218] Iteration 17800 (25.1044 iter/s, 3.98336s/100 iters), loss = 0.176235
I1202 13:18:23.164095 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:18:23.164095 13416 solver.cpp:237]     Train net output #1: loss = 0.176235 (* 1 = 0.176235 loss)
I1202 13:18:23.164095 13416 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I1202 13:18:27.148368 13416 solver.cpp:218] Iteration 17900 (25.1013 iter/s, 3.98386s/100 iters), loss = 0.083206
I1202 13:18:27.148368 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1202 13:18:27.148368 13416 solver.cpp:237]     Train net output #1: loss = 0.0832062 (* 1 = 0.0832062 loss)
I1202 13:18:27.148368 13416 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I1202 13:18:30.939610 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:18:31.095616 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_18000.caffemodel
I1202 13:18:31.107616 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_18000.solverstate
I1202 13:18:31.111616 13416 solver.cpp:330] Iteration 18000, Testing net (#0)
I1202 13:18:31.111616 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:18:32.093690  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:18:32.132692 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9049
I1202 13:18:32.132692 13416 solver.cpp:397]     Test net output #1: loss = 0.288006 (* 1 = 0.288006 loss)
I1202 13:18:32.171191 13416 solver.cpp:218] Iteration 18000 (19.9105 iter/s, 5.02247s/100 iters), loss = 0.167843
I1202 13:18:32.171191 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:18:32.171191 13416 solver.cpp:237]     Train net output #1: loss = 0.167843 (* 1 = 0.167843 loss)
I1202 13:18:32.171191 13416 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I1202 13:18:36.157893 13416 solver.cpp:218] Iteration 18100 (25.0855 iter/s, 3.98637s/100 iters), loss = 0.222713
I1202 13:18:36.157893 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:18:36.157893 13416 solver.cpp:237]     Train net output #1: loss = 0.222713 (* 1 = 0.222713 loss)
I1202 13:18:36.157893 13416 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I1202 13:18:40.137130 13416 solver.cpp:218] Iteration 18200 (25.1341 iter/s, 3.97866s/100 iters), loss = 0.152895
I1202 13:18:40.137130 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:18:40.137130 13416 solver.cpp:237]     Train net output #1: loss = 0.152895 (* 1 = 0.152895 loss)
I1202 13:18:40.137130 13416 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I1202 13:18:44.121515 13416 solver.cpp:218] Iteration 18300 (25.1004 iter/s, 3.98399s/100 iters), loss = 0.199341
I1202 13:18:44.121515 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:18:44.121515 13416 solver.cpp:237]     Train net output #1: loss = 0.199341 (* 1 = 0.199341 loss)
I1202 13:18:44.121515 13416 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I1202 13:18:48.110618 13416 solver.cpp:218] Iteration 18400 (25.0682 iter/s, 3.98911s/100 iters), loss = 0.136457
I1202 13:18:48.110618 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:18:48.110618 13416 solver.cpp:237]     Train net output #1: loss = 0.136457 (* 1 = 0.136457 loss)
I1202 13:18:48.110618 13416 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I1202 13:18:51.896847 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:18:52.052863 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_18500.caffemodel
I1202 13:18:52.090859 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_18500.solverstate
I1202 13:18:52.094859 13416 solver.cpp:330] Iteration 18500, Testing net (#0)
I1202 13:18:52.094859 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:18:53.076938  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:18:53.115937 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9055
I1202 13:18:53.115937 13416 solver.cpp:397]     Test net output #1: loss = 0.287668 (* 1 = 0.287668 loss)
I1202 13:18:53.153942 13416 solver.cpp:218] Iteration 18500 (19.8308 iter/s, 5.04266s/100 iters), loss = 0.176673
I1202 13:18:53.153942 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:18:53.153942 13416 solver.cpp:237]     Train net output #1: loss = 0.176674 (* 1 = 0.176674 loss)
I1202 13:18:53.153942 13416 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I1202 13:18:57.133680 13416 solver.cpp:218] Iteration 18600 (25.1303 iter/s, 3.97926s/100 iters), loss = 0.180867
I1202 13:18:57.133680 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:18:57.133680 13416 solver.cpp:237]     Train net output #1: loss = 0.180867 (* 1 = 0.180867 loss)
I1202 13:18:57.133680 13416 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I1202 13:19:01.123445 13416 solver.cpp:218] Iteration 18700 (25.0622 iter/s, 3.99007s/100 iters), loss = 0.116387
I1202 13:19:01.124445 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:19:01.124445 13416 solver.cpp:237]     Train net output #1: loss = 0.116388 (* 1 = 0.116388 loss)
I1202 13:19:01.124445 13416 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I1202 13:19:05.110672 13416 solver.cpp:218] Iteration 18800 (25.0882 iter/s, 3.98594s/100 iters), loss = 0.168896
I1202 13:19:05.110672 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:19:05.110672 13416 solver.cpp:237]     Train net output #1: loss = 0.168896 (* 1 = 0.168896 loss)
I1202 13:19:05.110672 13416 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I1202 13:19:09.091089 13416 solver.cpp:218] Iteration 18900 (25.1241 iter/s, 3.98024s/100 iters), loss = 0.142723
I1202 13:19:09.091089 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:19:09.091089 13416 solver.cpp:237]     Train net output #1: loss = 0.142723 (* 1 = 0.142723 loss)
I1202 13:19:09.091089 13416 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I1202 13:19:12.876317 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:19:13.032316 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_19000.caffemodel
I1202 13:19:13.043316 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_19000.solverstate
I1202 13:19:13.046816 13416 solver.cpp:330] Iteration 19000, Testing net (#0)
I1202 13:19:13.047317 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:19:14.029376  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:19:14.068383 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9049
I1202 13:19:14.068383 13416 solver.cpp:397]     Test net output #1: loss = 0.287723 (* 1 = 0.287723 loss)
I1202 13:19:14.106382 13416 solver.cpp:218] Iteration 19000 (19.9402 iter/s, 5.01498s/100 iters), loss = 0.136994
I1202 13:19:14.106382 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:19:14.106382 13416 solver.cpp:237]     Train net output #1: loss = 0.136995 (* 1 = 0.136995 loss)
I1202 13:19:14.106382 13416 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I1202 13:19:18.095671 13416 solver.cpp:218] Iteration 19100 (25.0658 iter/s, 3.9895s/100 iters), loss = 0.20593
I1202 13:19:18.095671 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:19:18.095671 13416 solver.cpp:237]     Train net output #1: loss = 0.205931 (* 1 = 0.205931 loss)
I1202 13:19:18.095671 13416 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I1202 13:19:22.085913 13416 solver.cpp:218] Iteration 19200 (25.0667 iter/s, 3.98936s/100 iters), loss = 0.124617
I1202 13:19:22.085913 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:19:22.085913 13416 solver.cpp:237]     Train net output #1: loss = 0.124617 (* 1 = 0.124617 loss)
I1202 13:19:22.085913 13416 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I1202 13:19:26.067198 13416 solver.cpp:218] Iteration 19300 (25.1194 iter/s, 3.98099s/100 iters), loss = 0.139625
I1202 13:19:26.067198 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:19:26.067198 13416 solver.cpp:237]     Train net output #1: loss = 0.139625 (* 1 = 0.139625 loss)
I1202 13:19:26.067198 13416 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I1202 13:19:30.049418 13416 solver.cpp:218] Iteration 19400 (25.1157 iter/s, 3.98158s/100 iters), loss = 0.124025
I1202 13:19:30.049418 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:19:30.049418 13416 solver.cpp:237]     Train net output #1: loss = 0.124025 (* 1 = 0.124025 loss)
I1202 13:19:30.049418 13416 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I1202 13:19:33.836297 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:19:33.992795 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_19500.caffemodel
I1202 13:19:34.033311 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_19500.solverstate
I1202 13:19:34.036811 13416 solver.cpp:330] Iteration 19500, Testing net (#0)
I1202 13:19:34.037312 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:19:35.020900  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:19:35.058907 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9048
I1202 13:19:35.058907 13416 solver.cpp:397]     Test net output #1: loss = 0.287455 (* 1 = 0.287455 loss)
I1202 13:19:35.096905 13416 solver.cpp:218] Iteration 19500 (19.8125 iter/s, 5.04732s/100 iters), loss = 0.145096
I1202 13:19:35.096905 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:19:35.096905 13416 solver.cpp:237]     Train net output #1: loss = 0.145096 (* 1 = 0.145096 loss)
I1202 13:19:35.096905 13416 sgd_solver.cpp:46] MultiStep Status: Iteration 19500, step = 4
I1202 13:19:35.096905 13416 sgd_solver.cpp:105] Iteration 19500, lr = 1e-05
I1202 13:19:39.077159 13416 solver.cpp:218] Iteration 19600 (25.1252 iter/s, 3.98007s/100 iters), loss = 0.209009
I1202 13:19:39.077159 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:19:39.077159 13416 solver.cpp:237]     Train net output #1: loss = 0.20901 (* 1 = 0.20901 loss)
I1202 13:19:39.077159 13416 sgd_solver.cpp:105] Iteration 19600, lr = 1e-05
I1202 13:19:43.062402 13416 solver.cpp:218] Iteration 19700 (25.0956 iter/s, 3.98476s/100 iters), loss = 0.166937
I1202 13:19:43.062402 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:19:43.062402 13416 solver.cpp:237]     Train net output #1: loss = 0.166937 (* 1 = 0.166937 loss)
I1202 13:19:43.062402 13416 sgd_solver.cpp:105] Iteration 19700, lr = 1e-05
I1202 13:19:47.040639 13416 solver.cpp:218] Iteration 19800 (25.1414 iter/s, 3.9775s/100 iters), loss = 0.176761
I1202 13:19:47.040639 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:19:47.040639 13416 solver.cpp:237]     Train net output #1: loss = 0.176761 (* 1 = 0.176761 loss)
I1202 13:19:47.040639 13416 sgd_solver.cpp:105] Iteration 19800, lr = 1e-05
I1202 13:19:51.013875 13416 solver.cpp:218] Iteration 19900 (25.1667 iter/s, 3.97351s/100 iters), loss = 0.0899071
I1202 13:19:51.013875 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1202 13:19:51.013875 13416 solver.cpp:237]     Train net output #1: loss = 0.0899073 (* 1 = 0.0899073 loss)
I1202 13:19:51.013875 13416 sgd_solver.cpp:105] Iteration 19900, lr = 1e-05
I1202 13:19:54.798127 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:19:54.954139 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_20000.caffemodel
I1202 13:19:54.965138 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_20000.solverstate
I1202 13:19:54.969139 13416 solver.cpp:330] Iteration 20000, Testing net (#0)
I1202 13:19:54.969139 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:19:55.952211  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:19:55.991210 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9054
I1202 13:19:55.991210 13416 solver.cpp:397]     Test net output #1: loss = 0.287499 (* 1 = 0.287499 loss)
I1202 13:19:56.029211 13416 solver.cpp:218] Iteration 20000 (19.9419 iter/s, 5.01458s/100 iters), loss = 0.176885
I1202 13:19:56.029211 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:19:56.029211 13416 solver.cpp:237]     Train net output #1: loss = 0.176885 (* 1 = 0.176885 loss)
I1202 13:19:56.029211 13416 sgd_solver.cpp:105] Iteration 20000, lr = 1e-05
I1202 13:20:00.010547 13416 solver.cpp:218] Iteration 20100 (25.1197 iter/s, 3.98095s/100 iters), loss = 0.167988
I1202 13:20:00.010547 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:20:00.010547 13416 solver.cpp:237]     Train net output #1: loss = 0.167988 (* 1 = 0.167988 loss)
I1202 13:20:00.010547 13416 sgd_solver.cpp:105] Iteration 20100, lr = 1e-05
I1202 13:20:04.031939 13416 solver.cpp:218] Iteration 20200 (24.8683 iter/s, 4.02119s/100 iters), loss = 0.174009
I1202 13:20:04.031939 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:20:04.031939 13416 solver.cpp:237]     Train net output #1: loss = 0.17401 (* 1 = 0.17401 loss)
I1202 13:20:04.031939 13416 sgd_solver.cpp:105] Iteration 20200, lr = 1e-05
I1202 13:20:08.012259 13416 solver.cpp:218] Iteration 20300 (25.126 iter/s, 3.97994s/100 iters), loss = 0.151155
I1202 13:20:08.012259 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:20:08.012259 13416 solver.cpp:237]     Train net output #1: loss = 0.151155 (* 1 = 0.151155 loss)
I1202 13:20:08.012259 13416 sgd_solver.cpp:105] Iteration 20300, lr = 1e-05
I1202 13:20:11.994025 13416 solver.cpp:218] Iteration 20400 (25.1182 iter/s, 3.98118s/100 iters), loss = 0.128868
I1202 13:20:11.994025 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:20:11.994025 13416 solver.cpp:237]     Train net output #1: loss = 0.128868 (* 1 = 0.128868 loss)
I1202 13:20:11.994025 13416 sgd_solver.cpp:105] Iteration 20400, lr = 1e-05
I1202 13:20:15.778249 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:20:15.934270 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_20500.caffemodel
I1202 13:20:15.966269 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_20500.solverstate
I1202 13:20:15.970270 13416 solver.cpp:330] Iteration 20500, Testing net (#0)
I1202 13:20:15.970270 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:20:16.954339  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:20:16.992338 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9051
I1202 13:20:16.992338 13416 solver.cpp:397]     Test net output #1: loss = 0.287327 (* 1 = 0.287327 loss)
I1202 13:20:17.030340 13416 solver.cpp:218] Iteration 20500 (19.857 iter/s, 5.036s/100 iters), loss = 0.138047
I1202 13:20:17.030340 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:20:17.030340 13416 solver.cpp:237]     Train net output #1: loss = 0.138047 (* 1 = 0.138047 loss)
I1202 13:20:17.030340 13416 sgd_solver.cpp:105] Iteration 20500, lr = 1e-05
I1202 13:20:21.014641 13416 solver.cpp:218] Iteration 20600 (25.0982 iter/s, 3.98435s/100 iters), loss = 0.214859
I1202 13:20:21.014641 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:20:21.014641 13416 solver.cpp:237]     Train net output #1: loss = 0.21486 (* 1 = 0.21486 loss)
I1202 13:20:21.014641 13416 sgd_solver.cpp:105] Iteration 20600, lr = 1e-05
I1202 13:20:25.000862 13416 solver.cpp:218] Iteration 20700 (25.0895 iter/s, 3.98572s/100 iters), loss = 0.162406
I1202 13:20:25.000862 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:20:25.000862 13416 solver.cpp:237]     Train net output #1: loss = 0.162406 (* 1 = 0.162406 loss)
I1202 13:20:25.000862 13416 sgd_solver.cpp:105] Iteration 20700, lr = 1e-05
I1202 13:20:28.990689 13416 solver.cpp:218] Iteration 20800 (25.068 iter/s, 3.98915s/100 iters), loss = 0.178625
I1202 13:20:28.990689 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:20:28.990689 13416 solver.cpp:237]     Train net output #1: loss = 0.178626 (* 1 = 0.178626 loss)
I1202 13:20:28.990689 13416 sgd_solver.cpp:105] Iteration 20800, lr = 1e-05
I1202 13:20:32.981555 13416 solver.cpp:218] Iteration 20900 (25.059 iter/s, 3.99058s/100 iters), loss = 0.0788693
I1202 13:20:32.981555 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1202 13:20:32.981555 13416 solver.cpp:237]     Train net output #1: loss = 0.0788697 (* 1 = 0.0788697 loss)
I1202 13:20:32.981555 13416 sgd_solver.cpp:105] Iteration 20900, lr = 1e-05
I1202 13:20:36.770784 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:20:36.927798 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_21000.caffemodel
I1202 13:20:36.938805 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_21000.solverstate
I1202 13:20:36.944803 13416 solver.cpp:330] Iteration 21000, Testing net (#0)
I1202 13:20:36.944803 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:20:37.927361  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:20:37.965862 13416 solver.cpp:397]     Test net output #0: accuracy = 0.905
I1202 13:20:37.965862 13416 solver.cpp:397]     Test net output #1: loss = 0.287471 (* 1 = 0.287471 loss)
I1202 13:20:38.003862 13416 solver.cpp:218] Iteration 21000 (19.9113 iter/s, 5.02228s/100 iters), loss = 0.112894
I1202 13:20:38.003862 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1202 13:20:38.003862 13416 solver.cpp:237]     Train net output #1: loss = 0.112894 (* 1 = 0.112894 loss)
I1202 13:20:38.003862 13416 sgd_solver.cpp:105] Iteration 21000, lr = 1e-05
I1202 13:20:41.982101 13416 solver.cpp:218] Iteration 21100 (25.1378 iter/s, 3.97807s/100 iters), loss = 0.217617
I1202 13:20:41.982101 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:20:41.982101 13416 solver.cpp:237]     Train net output #1: loss = 0.217617 (* 1 = 0.217617 loss)
I1202 13:20:41.982101 13416 sgd_solver.cpp:105] Iteration 21100, lr = 1e-05
I1202 13:20:45.966351 13416 solver.cpp:218] Iteration 21200 (25.1011 iter/s, 3.98389s/100 iters), loss = 0.145414
I1202 13:20:45.966351 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:20:45.966351 13416 solver.cpp:237]     Train net output #1: loss = 0.145415 (* 1 = 0.145415 loss)
I1202 13:20:45.966351 13416 sgd_solver.cpp:105] Iteration 21200, lr = 1e-05
I1202 13:20:49.945570 13416 solver.cpp:218] Iteration 21300 (25.1381 iter/s, 3.97803s/100 iters), loss = 0.173074
I1202 13:20:49.945570 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:20:49.945570 13416 solver.cpp:237]     Train net output #1: loss = 0.173074 (* 1 = 0.173074 loss)
I1202 13:20:49.945570 13416 sgd_solver.cpp:105] Iteration 21300, lr = 1e-05
I1202 13:20:53.925806 13416 solver.cpp:218] Iteration 21400 (25.1222 iter/s, 3.98054s/100 iters), loss = 0.103164
I1202 13:20:53.925806 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:20:53.925806 13416 solver.cpp:237]     Train net output #1: loss = 0.103165 (* 1 = 0.103165 loss)
I1202 13:20:53.925806 13416 sgd_solver.cpp:105] Iteration 21400, lr = 1e-05
I1202 13:20:57.713896 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:20:57.869407 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_21500.caffemodel
I1202 13:20:57.881407 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_21500.solverstate
I1202 13:20:57.885407 13416 solver.cpp:330] Iteration 21500, Testing net (#0)
I1202 13:20:57.885407 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:20:58.867502  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:20:58.906502 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9047
I1202 13:20:58.906502 13416 solver.cpp:397]     Test net output #1: loss = 0.287498 (* 1 = 0.287498 loss)
I1202 13:20:58.944516 13416 solver.cpp:218] Iteration 21500 (19.9271 iter/s, 5.01828s/100 iters), loss = 0.137738
I1202 13:20:58.944516 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:20:58.944516 13416 solver.cpp:237]     Train net output #1: loss = 0.137738 (* 1 = 0.137738 loss)
I1202 13:20:58.944516 13416 sgd_solver.cpp:105] Iteration 21500, lr = 1e-05
I1202 13:21:02.927848 13416 solver.cpp:218] Iteration 21600 (25.1093 iter/s, 3.98259s/100 iters), loss = 0.237586
I1202 13:21:02.927848 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1202 13:21:02.927848 13416 solver.cpp:237]     Train net output #1: loss = 0.237586 (* 1 = 0.237586 loss)
I1202 13:21:02.927848 13416 sgd_solver.cpp:105] Iteration 21600, lr = 1e-05
I1202 13:21:06.909096 13416 solver.cpp:218] Iteration 21700 (25.1179 iter/s, 3.98123s/100 iters), loss = 0.165987
I1202 13:21:06.909096 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:21:06.909096 13416 solver.cpp:237]     Train net output #1: loss = 0.165988 (* 1 = 0.165988 loss)
I1202 13:21:06.909096 13416 sgd_solver.cpp:105] Iteration 21700, lr = 1e-05
I1202 13:21:10.889319 13416 solver.cpp:218] Iteration 21800 (25.1274 iter/s, 3.97972s/100 iters), loss = 0.185761
I1202 13:21:10.889319 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:21:10.889319 13416 solver.cpp:237]     Train net output #1: loss = 0.185761 (* 1 = 0.185761 loss)
I1202 13:21:10.889319 13416 sgd_solver.cpp:105] Iteration 21800, lr = 1e-05
I1202 13:21:14.868574 13416 solver.cpp:218] Iteration 21900 (25.1371 iter/s, 3.97819s/100 iters), loss = 0.0794266
I1202 13:21:14.868574 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1202 13:21:14.868574 13416 solver.cpp:237]     Train net output #1: loss = 0.0794269 (* 1 = 0.0794269 loss)
I1202 13:21:14.868574 13416 sgd_solver.cpp:105] Iteration 21900, lr = 1e-05
I1202 13:21:18.653832 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:21:18.809837 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_22000.caffemodel
I1202 13:21:18.821341 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_22000.solverstate
I1202 13:21:18.825341 13416 solver.cpp:330] Iteration 22000, Testing net (#0)
I1202 13:21:18.825341 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:21:19.807904  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:21:19.845908 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9051
I1202 13:21:19.845908 13416 solver.cpp:397]     Test net output #1: loss = 0.287491 (* 1 = 0.287491 loss)
I1202 13:21:19.883908 13416 solver.cpp:218] Iteration 22000 (19.939 iter/s, 5.0153s/100 iters), loss = 0.133067
I1202 13:21:19.883908 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:21:19.883908 13416 solver.cpp:237]     Train net output #1: loss = 0.133068 (* 1 = 0.133068 loss)
I1202 13:21:19.883908 13416 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 5
I1202 13:21:19.883908 13416 sgd_solver.cpp:105] Iteration 22000, lr = 1e-06
I1202 13:21:23.868158 13416 solver.cpp:218] Iteration 22100 (25.1039 iter/s, 3.98345s/100 iters), loss = 0.187997
I1202 13:21:23.868158 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:21:23.868158 13416 solver.cpp:237]     Train net output #1: loss = 0.187997 (* 1 = 0.187997 loss)
I1202 13:21:23.868158 13416 sgd_solver.cpp:105] Iteration 22100, lr = 1e-06
I1202 13:21:27.855384 13416 solver.cpp:218] Iteration 22200 (25.0789 iter/s, 3.98742s/100 iters), loss = 0.156118
I1202 13:21:27.855384 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:21:27.855384 13416 solver.cpp:237]     Train net output #1: loss = 0.156118 (* 1 = 0.156118 loss)
I1202 13:21:27.855384 13416 sgd_solver.cpp:105] Iteration 22200, lr = 1e-06
I1202 13:21:31.840531 13416 solver.cpp:218] Iteration 22300 (25.0946 iter/s, 3.98492s/100 iters), loss = 0.137262
I1202 13:21:31.840531 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:21:31.840531 13416 solver.cpp:237]     Train net output #1: loss = 0.137262 (* 1 = 0.137262 loss)
I1202 13:21:31.840531 13416 sgd_solver.cpp:105] Iteration 22300, lr = 1e-06
I1202 13:21:35.826784 13416 solver.cpp:218] Iteration 22400 (25.0903 iter/s, 3.9856s/100 iters), loss = 0.0767193
I1202 13:21:35.826784 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1202 13:21:35.826784 13416 solver.cpp:237]     Train net output #1: loss = 0.0767197 (* 1 = 0.0767197 loss)
I1202 13:21:35.826784 13416 sgd_solver.cpp:105] Iteration 22400, lr = 1e-06
I1202 13:21:39.620038 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:21:39.776543 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_22500.caffemodel
I1202 13:21:39.808045 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_22500.solverstate
I1202 13:21:39.812047 13416 solver.cpp:330] Iteration 22500, Testing net (#0)
I1202 13:21:39.812047 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:21:40.796110  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:21:40.834110 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9047
I1202 13:21:40.834110 13416 solver.cpp:397]     Test net output #1: loss = 0.287448 (* 1 = 0.287448 loss)
I1202 13:21:40.872611 13416 solver.cpp:218] Iteration 22500 (19.8202 iter/s, 5.04535s/100 iters), loss = 0.179847
I1202 13:21:40.872611 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:21:40.872611 13416 solver.cpp:237]     Train net output #1: loss = 0.179847 (* 1 = 0.179847 loss)
I1202 13:21:40.872611 13416 sgd_solver.cpp:105] Iteration 22500, lr = 1e-06
I1202 13:21:44.858348 13416 solver.cpp:218] Iteration 22600 (25.0885 iter/s, 3.98589s/100 iters), loss = 0.206922
I1202 13:21:44.858348 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:21:44.858348 13416 solver.cpp:237]     Train net output #1: loss = 0.206923 (* 1 = 0.206923 loss)
I1202 13:21:44.858348 13416 sgd_solver.cpp:105] Iteration 22600, lr = 1e-06
I1202 13:21:48.844590 13416 solver.cpp:218] Iteration 22700 (25.0918 iter/s, 3.98536s/100 iters), loss = 0.172368
I1202 13:21:48.844590 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:21:48.844590 13416 solver.cpp:237]     Train net output #1: loss = 0.172369 (* 1 = 0.172369 loss)
I1202 13:21:48.844590 13416 sgd_solver.cpp:105] Iteration 22700, lr = 1e-06
I1202 13:21:52.837019 13416 solver.cpp:218] Iteration 22800 (25.048 iter/s, 3.99234s/100 iters), loss = 0.159149
I1202 13:21:52.837019 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:21:52.837019 13416 solver.cpp:237]     Train net output #1: loss = 0.15915 (* 1 = 0.15915 loss)
I1202 13:21:52.837019 13416 sgd_solver.cpp:105] Iteration 22800, lr = 1e-06
I1202 13:21:56.823267 13416 solver.cpp:218] Iteration 22900 (25.0889 iter/s, 3.98582s/100 iters), loss = 0.159949
I1202 13:21:56.823267 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:21:56.823267 13416 solver.cpp:237]     Train net output #1: loss = 0.15995 (* 1 = 0.15995 loss)
I1202 13:21:56.823267 13416 sgd_solver.cpp:105] Iteration 22900, lr = 1e-06
I1202 13:22:00.614703 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:22:00.770710 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_23000.caffemodel
I1202 13:22:00.780711 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_23000.solverstate
I1202 13:22:00.784714 13416 solver.cpp:330] Iteration 23000, Testing net (#0)
I1202 13:22:00.784714 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:22:01.768268  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:22:01.806771 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9052
I1202 13:22:01.806771 13416 solver.cpp:397]     Test net output #1: loss = 0.287532 (* 1 = 0.287532 loss)
I1202 13:22:01.844769 13416 solver.cpp:218] Iteration 23000 (19.9147 iter/s, 5.02141s/100 iters), loss = 0.123587
I1202 13:22:01.844769 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:22:01.844769 13416 solver.cpp:237]     Train net output #1: loss = 0.123587 (* 1 = 0.123587 loss)
I1202 13:22:01.844769 13416 sgd_solver.cpp:105] Iteration 23000, lr = 1e-06
I1202 13:22:05.828163 13416 solver.cpp:218] Iteration 23100 (25.1058 iter/s, 3.98314s/100 iters), loss = 0.23427
I1202 13:22:05.828163 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:22:05.828163 13416 solver.cpp:237]     Train net output #1: loss = 0.234271 (* 1 = 0.234271 loss)
I1202 13:22:05.828163 13416 sgd_solver.cpp:105] Iteration 23100, lr = 1e-06
I1202 13:22:09.810525 13416 solver.cpp:218] Iteration 23200 (25.1126 iter/s, 3.98206s/100 iters), loss = 0.170794
I1202 13:22:09.811525 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:22:09.811525 13416 solver.cpp:237]     Train net output #1: loss = 0.170794 (* 1 = 0.170794 loss)
I1202 13:22:09.811525 13416 sgd_solver.cpp:105] Iteration 23200, lr = 1e-06
I1202 13:22:13.787771 13416 solver.cpp:218] Iteration 23300 (25.1492 iter/s, 3.97626s/100 iters), loss = 0.234967
I1202 13:22:13.787771 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:22:13.787771 13416 solver.cpp:237]     Train net output #1: loss = 0.234967 (* 1 = 0.234967 loss)
I1202 13:22:13.787771 13416 sgd_solver.cpp:105] Iteration 23300, lr = 1e-06
I1202 13:22:17.770577 13416 solver.cpp:218] Iteration 23400 (25.1102 iter/s, 3.98244s/100 iters), loss = 0.0924921
I1202 13:22:17.770577 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:22:17.770577 13416 solver.cpp:237]     Train net output #1: loss = 0.0924924 (* 1 = 0.0924924 loss)
I1202 13:22:17.770577 13416 sgd_solver.cpp:105] Iteration 23400, lr = 1e-06
I1202 13:22:21.559798 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:22:21.715796 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_23500.caffemodel
I1202 13:22:21.749311 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_23500.solverstate
I1202 13:22:21.753312 13416 solver.cpp:330] Iteration 23500, Testing net (#0)
I1202 13:22:21.753312 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:22:22.735373  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:22:22.774372 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9051
I1202 13:22:22.774372 13416 solver.cpp:397]     Test net output #1: loss = 0.287419 (* 1 = 0.287419 loss)
I1202 13:22:22.812371 13416 solver.cpp:218] Iteration 23500 (19.8351 iter/s, 5.04156s/100 iters), loss = 0.121209
I1202 13:22:22.812371 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:22:22.812371 13416 solver.cpp:237]     Train net output #1: loss = 0.121209 (* 1 = 0.121209 loss)
I1202 13:22:22.812371 13416 sgd_solver.cpp:105] Iteration 23500, lr = 1e-06
I1202 13:22:26.801604 13416 solver.cpp:218] Iteration 23600 (25.0717 iter/s, 3.98856s/100 iters), loss = 0.180345
I1202 13:22:26.801604 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:22:26.801604 13416 solver.cpp:237]     Train net output #1: loss = 0.180346 (* 1 = 0.180346 loss)
I1202 13:22:26.801604 13416 sgd_solver.cpp:105] Iteration 23600, lr = 1e-06
I1202 13:22:30.784839 13416 solver.cpp:218] Iteration 23700 (25.1025 iter/s, 3.98367s/100 iters), loss = 0.138421
I1202 13:22:30.785840 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:22:30.785840 13416 solver.cpp:237]     Train net output #1: loss = 0.138421 (* 1 = 0.138421 loss)
I1202 13:22:30.785840 13416 sgd_solver.cpp:105] Iteration 23700, lr = 1e-06
I1202 13:22:34.772094 13416 solver.cpp:218] Iteration 23800 (25.0876 iter/s, 3.98604s/100 iters), loss = 0.169978
I1202 13:22:34.772094 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:22:34.772094 13416 solver.cpp:237]     Train net output #1: loss = 0.169978 (* 1 = 0.169978 loss)
I1202 13:22:34.772094 13416 sgd_solver.cpp:105] Iteration 23800, lr = 1e-06
I1202 13:22:38.762408 13416 solver.cpp:218] Iteration 23900 (25.0627 iter/s, 3.99s/100 iters), loss = 0.141642
I1202 13:22:38.762408 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:22:38.762408 13416 solver.cpp:237]     Train net output #1: loss = 0.141643 (* 1 = 0.141643 loss)
I1202 13:22:38.762408 13416 sgd_solver.cpp:105] Iteration 23900, lr = 1e-06
I1202 13:22:42.552695 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:22:42.709699 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_24000.caffemodel
I1202 13:22:42.720703 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_24000.solverstate
I1202 13:22:42.724704 13416 solver.cpp:330] Iteration 24000, Testing net (#0)
I1202 13:22:42.724704 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:22:43.706759  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:22:43.745766 13416 solver.cpp:397]     Test net output #0: accuracy = 0.905
I1202 13:22:43.745766 13416 solver.cpp:397]     Test net output #1: loss = 0.287509 (* 1 = 0.287509 loss)
I1202 13:22:43.783764 13416 solver.cpp:218] Iteration 24000 (19.9164 iter/s, 5.02099s/100 iters), loss = 0.197547
I1202 13:22:43.783764 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1202 13:22:43.783764 13416 solver.cpp:237]     Train net output #1: loss = 0.197548 (* 1 = 0.197548 loss)
I1202 13:22:43.783764 13416 sgd_solver.cpp:105] Iteration 24000, lr = 1e-06
I1202 13:22:47.773022 13416 solver.cpp:218] Iteration 24100 (25.0665 iter/s, 3.98939s/100 iters), loss = 0.243707
I1202 13:22:47.773022 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:22:47.773022 13416 solver.cpp:237]     Train net output #1: loss = 0.243708 (* 1 = 0.243708 loss)
I1202 13:22:47.773022 13416 sgd_solver.cpp:105] Iteration 24100, lr = 1e-06
I1202 13:22:51.761370 13416 solver.cpp:218] Iteration 24200 (25.0777 iter/s, 3.9876s/100 iters), loss = 0.134353
I1202 13:22:51.761370 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:22:51.761370 13416 solver.cpp:237]     Train net output #1: loss = 0.134353 (* 1 = 0.134353 loss)
I1202 13:22:51.761370 13416 sgd_solver.cpp:105] Iteration 24200, lr = 1e-06
I1202 13:22:55.744617 13416 solver.cpp:218] Iteration 24300 (25.1061 iter/s, 3.9831s/100 iters), loss = 0.207928
I1202 13:22:55.744617 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:22:55.744617 13416 solver.cpp:237]     Train net output #1: loss = 0.207928 (* 1 = 0.207928 loss)
I1202 13:22:55.744617 13416 sgd_solver.cpp:105] Iteration 24300, lr = 1e-06
I1202 13:22:59.732889 13416 solver.cpp:218] Iteration 24400 (25.0768 iter/s, 3.98776s/100 iters), loss = 0.112613
I1202 13:22:59.732889 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:22:59.732889 13416 solver.cpp:237]     Train net output #1: loss = 0.112614 (* 1 = 0.112614 loss)
I1202 13:22:59.732889 13416 sgd_solver.cpp:105] Iteration 24400, lr = 1e-06
I1202 13:23:03.528825 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:23:03.685333 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_24500.caffemodel
I1202 13:23:03.725337 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_24500.solverstate
I1202 13:23:03.729338 13416 solver.cpp:330] Iteration 24500, Testing net (#0)
I1202 13:23:03.729338 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:23:04.710395  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:23:04.749402 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9049
I1202 13:23:04.750403 13416 solver.cpp:397]     Test net output #1: loss = 0.287542 (* 1 = 0.287542 loss)
I1202 13:23:04.787417 13416 solver.cpp:218] Iteration 24500 (19.7846 iter/s, 5.05445s/100 iters), loss = 0.150822
I1202 13:23:04.787417 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:23:04.787417 13416 solver.cpp:237]     Train net output #1: loss = 0.150822 (* 1 = 0.150822 loss)
I1202 13:23:04.787417 13416 sgd_solver.cpp:105] Iteration 24500, lr = 1e-06
I1202 13:23:08.775648 13416 solver.cpp:218] Iteration 24600 (25.0793 iter/s, 3.98735s/100 iters), loss = 0.238786
I1202 13:23:08.775648 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1202 13:23:08.775648 13416 solver.cpp:237]     Train net output #1: loss = 0.238786 (* 1 = 0.238786 loss)
I1202 13:23:08.775648 13416 sgd_solver.cpp:105] Iteration 24600, lr = 1e-06
I1202 13:23:12.764006 13416 solver.cpp:218] Iteration 24700 (25.0734 iter/s, 3.98828s/100 iters), loss = 0.143033
I1202 13:23:12.764006 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:23:12.764006 13416 solver.cpp:237]     Train net output #1: loss = 0.143033 (* 1 = 0.143033 loss)
I1202 13:23:12.764006 13416 sgd_solver.cpp:105] Iteration 24700, lr = 1e-06
I1202 13:23:16.751266 13416 solver.cpp:218] Iteration 24800 (25.0823 iter/s, 3.98688s/100 iters), loss = 0.140166
I1202 13:23:16.751266 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:23:16.751266 13416 solver.cpp:237]     Train net output #1: loss = 0.140166 (* 1 = 0.140166 loss)
I1202 13:23:16.751266 13416 sgd_solver.cpp:105] Iteration 24800, lr = 1e-06
I1202 13:23:20.736966 13416 solver.cpp:218] Iteration 24900 (25.0932 iter/s, 3.98515s/100 iters), loss = 0.119331
I1202 13:23:20.736966 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:23:20.736966 13416 solver.cpp:237]     Train net output #1: loss = 0.119332 (* 1 = 0.119332 loss)
I1202 13:23:20.736966 13416 sgd_solver.cpp:105] Iteration 24900, lr = 1e-06
I1202 13:23:24.526684 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:23:24.683194 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_25000.caffemodel
I1202 13:23:24.694197 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_25000.solverstate
I1202 13:23:24.698196 13416 solver.cpp:330] Iteration 25000, Testing net (#0)
I1202 13:23:24.698196 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:23:25.680269  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:23:25.719269 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9052
I1202 13:23:25.719269 13416 solver.cpp:397]     Test net output #1: loss = 0.287453 (* 1 = 0.287453 loss)
I1202 13:23:25.757280 13416 solver.cpp:218] Iteration 25000 (19.919 iter/s, 5.02033s/100 iters), loss = 0.175663
I1202 13:23:25.757280 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:23:25.757280 13416 solver.cpp:237]     Train net output #1: loss = 0.175663 (* 1 = 0.175663 loss)
I1202 13:23:25.757280 13416 sgd_solver.cpp:105] Iteration 25000, lr = 1e-06
I1202 13:23:29.737022 13416 solver.cpp:218] Iteration 25100 (25.1304 iter/s, 3.97924s/100 iters), loss = 0.213413
I1202 13:23:29.737522 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:23:29.737522 13416 solver.cpp:237]     Train net output #1: loss = 0.213413 (* 1 = 0.213413 loss)
I1202 13:23:29.737522 13416 sgd_solver.cpp:105] Iteration 25100, lr = 1e-06
I1202 13:23:33.715800 13416 solver.cpp:218] Iteration 25200 (25.1353 iter/s, 3.97846s/100 iters), loss = 0.14441
I1202 13:23:33.715800 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:23:33.715800 13416 solver.cpp:237]     Train net output #1: loss = 0.14441 (* 1 = 0.14441 loss)
I1202 13:23:33.715800 13416 sgd_solver.cpp:105] Iteration 25200, lr = 1e-06
I1202 13:23:37.700486 13416 solver.cpp:218] Iteration 25300 (25.0961 iter/s, 3.98468s/100 iters), loss = 0.169624
I1202 13:23:37.701488 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:23:37.701488 13416 solver.cpp:237]     Train net output #1: loss = 0.169624 (* 1 = 0.169624 loss)
I1202 13:23:37.701488 13416 sgd_solver.cpp:105] Iteration 25300, lr = 1e-06
I1202 13:23:41.696753 13416 solver.cpp:218] Iteration 25400 (25.0277 iter/s, 3.99558s/100 iters), loss = 0.136579
I1202 13:23:41.696753 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:23:41.696753 13416 solver.cpp:237]     Train net output #1: loss = 0.13658 (* 1 = 0.13658 loss)
I1202 13:23:41.696753 13416 sgd_solver.cpp:105] Iteration 25400, lr = 1e-06
I1202 13:23:45.503108 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:23:45.660122 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_25500.caffemodel
I1202 13:23:45.700137 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_25500.solverstate
I1202 13:23:45.704138 13416 solver.cpp:330] Iteration 25500, Testing net (#0)
I1202 13:23:45.704138 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:23:46.687198  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:23:46.726197 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9049
I1202 13:23:46.726197 13416 solver.cpp:397]     Test net output #1: loss = 0.287513 (* 1 = 0.287513 loss)
I1202 13:23:46.765200 13416 solver.cpp:218] Iteration 25500 (19.7334 iter/s, 5.06755s/100 iters), loss = 0.140597
I1202 13:23:46.765200 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:23:46.765200 13416 solver.cpp:237]     Train net output #1: loss = 0.140597 (* 1 = 0.140597 loss)
I1202 13:23:46.765200 13416 sgd_solver.cpp:105] Iteration 25500, lr = 1e-06
I1202 13:23:50.756937 13416 solver.cpp:218] Iteration 25600 (25.0546 iter/s, 3.99129s/100 iters), loss = 0.207264
I1202 13:23:50.756937 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:23:50.756937 13416 solver.cpp:237]     Train net output #1: loss = 0.207264 (* 1 = 0.207264 loss)
I1202 13:23:50.756937 13416 sgd_solver.cpp:105] Iteration 25600, lr = 1e-06
I1202 13:23:54.750722 13416 solver.cpp:218] Iteration 25700 (25.0378 iter/s, 3.99396s/100 iters), loss = 0.157392
I1202 13:23:54.750722 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:23:54.750722 13416 solver.cpp:237]     Train net output #1: loss = 0.157392 (* 1 = 0.157392 loss)
I1202 13:23:54.750722 13416 sgd_solver.cpp:105] Iteration 25700, lr = 1e-06
I1202 13:23:58.743028 13416 solver.cpp:218] Iteration 25800 (25.0496 iter/s, 3.99207s/100 iters), loss = 0.176588
I1202 13:23:58.743028 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:23:58.743028 13416 solver.cpp:237]     Train net output #1: loss = 0.176588 (* 1 = 0.176588 loss)
I1202 13:23:58.743028 13416 sgd_solver.cpp:105] Iteration 25800, lr = 1e-06
I1202 13:24:02.732497 13416 solver.cpp:218] Iteration 25900 (25.0682 iter/s, 3.98911s/100 iters), loss = 0.0979908
I1202 13:24:02.732497 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:24:02.732497 13416 solver.cpp:237]     Train net output #1: loss = 0.0979912 (* 1 = 0.0979912 loss)
I1202 13:24:02.732497 13416 sgd_solver.cpp:105] Iteration 25900, lr = 1e-06
I1202 13:24:06.523777 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:24:06.679787 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_26000.caffemodel
I1202 13:24:06.689787 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_26000.solverstate
I1202 13:24:06.694788 13416 solver.cpp:330] Iteration 26000, Testing net (#0)
I1202 13:24:06.694788 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:24:07.678874  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:24:07.717875 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9054
I1202 13:24:07.717875 13416 solver.cpp:397]     Test net output #1: loss = 0.287509 (* 1 = 0.287509 loss)
I1202 13:24:07.756376 13416 solver.cpp:218] Iteration 26000 (19.9091 iter/s, 5.02283s/100 iters), loss = 0.169948
I1202 13:24:07.756376 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1202 13:24:07.756376 13416 solver.cpp:237]     Train net output #1: loss = 0.169948 (* 1 = 0.169948 loss)
I1202 13:24:07.756376 13416 sgd_solver.cpp:105] Iteration 26000, lr = 1e-06
I1202 13:24:11.746254 13416 solver.cpp:218] Iteration 26100 (25.0613 iter/s, 3.99021s/100 iters), loss = 0.19956
I1202 13:24:11.746254 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:24:11.746254 13416 solver.cpp:237]     Train net output #1: loss = 0.19956 (* 1 = 0.19956 loss)
I1202 13:24:11.746254 13416 sgd_solver.cpp:105] Iteration 26100, lr = 1e-06
I1202 13:24:15.732560 13416 solver.cpp:218] Iteration 26200 (25.0942 iter/s, 3.98498s/100 iters), loss = 0.135562
I1202 13:24:15.732560 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:24:15.732560 13416 solver.cpp:237]     Train net output #1: loss = 0.135563 (* 1 = 0.135563 loss)
I1202 13:24:15.732560 13416 sgd_solver.cpp:105] Iteration 26200, lr = 1e-06
I1202 13:24:19.717792 13416 solver.cpp:218] Iteration 26300 (25.0918 iter/s, 3.98536s/100 iters), loss = 0.109296
I1202 13:24:19.717792 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:24:19.717792 13416 solver.cpp:237]     Train net output #1: loss = 0.109297 (* 1 = 0.109297 loss)
I1202 13:24:19.717792 13416 sgd_solver.cpp:105] Iteration 26300, lr = 1e-06
I1202 13:24:23.705042 13416 solver.cpp:218] Iteration 26400 (25.0827 iter/s, 3.98681s/100 iters), loss = 0.103926
I1202 13:24:23.705042 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:24:23.705042 13416 solver.cpp:237]     Train net output #1: loss = 0.103926 (* 1 = 0.103926 loss)
I1202 13:24:23.705042 13416 sgd_solver.cpp:105] Iteration 26400, lr = 1e-06
I1202 13:24:27.501579 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:24:27.659085 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_26500.caffemodel
I1202 13:24:27.736603 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_26500.solverstate
I1202 13:24:27.742604 13416 solver.cpp:330] Iteration 26500, Testing net (#0)
I1202 13:24:27.742604 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:24:28.728718  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:24:28.767222 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9048
I1202 13:24:28.767222 13416 solver.cpp:397]     Test net output #1: loss = 0.28756 (* 1 = 0.28756 loss)
I1202 13:24:28.804723 13416 solver.cpp:218] Iteration 26500 (19.6094 iter/s, 5.09961s/100 iters), loss = 0.134916
I1202 13:24:28.804723 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:24:28.804723 13416 solver.cpp:237]     Train net output #1: loss = 0.134916 (* 1 = 0.134916 loss)
I1202 13:24:28.804723 13416 sgd_solver.cpp:105] Iteration 26500, lr = 1e-06
I1202 13:24:32.787190 13416 solver.cpp:218] Iteration 26600 (25.1153 iter/s, 3.98164s/100 iters), loss = 0.265154
I1202 13:24:32.787190 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1202 13:24:32.787190 13416 solver.cpp:237]     Train net output #1: loss = 0.265155 (* 1 = 0.265155 loss)
I1202 13:24:32.787190 13416 sgd_solver.cpp:105] Iteration 26600, lr = 1e-06
I1202 13:24:36.771931 13416 solver.cpp:218] Iteration 26700 (25.0991 iter/s, 3.98421s/100 iters), loss = 0.133047
I1202 13:24:36.771931 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:24:36.771931 13416 solver.cpp:237]     Train net output #1: loss = 0.133047 (* 1 = 0.133047 loss)
I1202 13:24:36.771931 13416 sgd_solver.cpp:105] Iteration 26700, lr = 1e-06
I1202 13:24:40.751673 13416 solver.cpp:218] Iteration 26800 (25.1268 iter/s, 3.97982s/100 iters), loss = 0.121881
I1202 13:24:40.751673 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1202 13:24:40.751673 13416 solver.cpp:237]     Train net output #1: loss = 0.121882 (* 1 = 0.121882 loss)
I1202 13:24:40.751673 13416 sgd_solver.cpp:105] Iteration 26800, lr = 1e-06
I1202 13:24:44.732921 13416 solver.cpp:218] Iteration 26900 (25.1205 iter/s, 3.98081s/100 iters), loss = 0.101551
I1202 13:24:44.732921 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:24:44.732921 13416 solver.cpp:237]     Train net output #1: loss = 0.101552 (* 1 = 0.101552 loss)
I1202 13:24:44.732921 13416 sgd_solver.cpp:105] Iteration 26900, lr = 1e-06
I1202 13:24:48.522146 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:24:48.679157 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_27000.caffemodel
I1202 13:24:48.689157 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_27000.solverstate
I1202 13:24:48.693156 13416 solver.cpp:330] Iteration 27000, Testing net (#0)
I1202 13:24:48.693156 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:24:49.677276  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:24:49.716274 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9049
I1202 13:24:49.716274 13416 solver.cpp:397]     Test net output #1: loss = 0.287451 (* 1 = 0.287451 loss)
I1202 13:24:49.754276 13416 solver.cpp:218] Iteration 27000 (19.9145 iter/s, 5.02145s/100 iters), loss = 0.103585
I1202 13:24:49.755275 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1202 13:24:49.755275 13416 solver.cpp:237]     Train net output #1: loss = 0.103585 (* 1 = 0.103585 loss)
I1202 13:24:49.755275 13416 sgd_solver.cpp:46] MultiStep Status: Iteration 27000, step = 6
I1202 13:24:49.755275 13416 sgd_solver.cpp:105] Iteration 27000, lr = 1e-07
I1202 13:24:53.734391 13416 solver.cpp:218] Iteration 27100 (25.1288 iter/s, 3.9795s/100 iters), loss = 0.200092
I1202 13:24:53.734391 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:24:53.734391 13416 solver.cpp:237]     Train net output #1: loss = 0.200092 (* 1 = 0.200092 loss)
I1202 13:24:53.734391 13416 sgd_solver.cpp:105] Iteration 27100, lr = 1e-07
I1202 13:24:57.710125 13416 solver.cpp:218] Iteration 27200 (25.1548 iter/s, 3.97538s/100 iters), loss = 0.178515
I1202 13:24:57.710125 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:24:57.711125 13416 solver.cpp:237]     Train net output #1: loss = 0.178516 (* 1 = 0.178516 loss)
I1202 13:24:57.711125 13416 sgd_solver.cpp:105] Iteration 27200, lr = 1e-07
I1202 13:25:01.691412 13416 solver.cpp:218] Iteration 27300 (25.1251 iter/s, 3.98009s/100 iters), loss = 0.169584
I1202 13:25:01.691412 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:25:01.691412 13416 solver.cpp:237]     Train net output #1: loss = 0.169585 (* 1 = 0.169585 loss)
I1202 13:25:01.691412 13416 sgd_solver.cpp:105] Iteration 27300, lr = 1e-07
I1202 13:25:05.673404 13416 solver.cpp:218] Iteration 27400 (25.1161 iter/s, 3.98151s/100 iters), loss = 0.0887931
I1202 13:25:05.673404 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1202 13:25:05.673404 13416 solver.cpp:237]     Train net output #1: loss = 0.0887934 (* 1 = 0.0887934 loss)
I1202 13:25:05.673404 13416 sgd_solver.cpp:105] Iteration 27400, lr = 1e-07
I1202 13:25:09.462772 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:25:09.620285 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_27500.caffemodel
I1202 13:25:09.630285 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_27500.solverstate
I1202 13:25:09.634285 13416 solver.cpp:330] Iteration 27500, Testing net (#0)
I1202 13:25:09.634285 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:25:10.618350  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:25:10.656348 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9053
I1202 13:25:10.656348 13416 solver.cpp:397]     Test net output #1: loss = 0.287326 (* 1 = 0.287326 loss)
I1202 13:25:10.694365 13416 solver.cpp:218] Iteration 27500 (19.9165 iter/s, 5.02097s/100 iters), loss = 0.15335
I1202 13:25:10.694365 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:25:10.694365 13416 solver.cpp:237]     Train net output #1: loss = 0.15335 (* 1 = 0.15335 loss)
I1202 13:25:10.694365 13416 sgd_solver.cpp:105] Iteration 27500, lr = 1e-07
I1202 13:25:14.678593 13416 solver.cpp:218] Iteration 27600 (25.0996 iter/s, 3.98413s/100 iters), loss = 0.175939
I1202 13:25:14.678593 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:25:14.678593 13416 solver.cpp:237]     Train net output #1: loss = 0.175939 (* 1 = 0.175939 loss)
I1202 13:25:14.678593 13416 sgd_solver.cpp:105] Iteration 27600, lr = 1e-07
I1202 13:25:18.662326 13416 solver.cpp:218] Iteration 27700 (25.1096 iter/s, 3.98254s/100 iters), loss = 0.116946
I1202 13:25:18.662326 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:25:18.662326 13416 solver.cpp:237]     Train net output #1: loss = 0.116947 (* 1 = 0.116947 loss)
I1202 13:25:18.662326 13416 sgd_solver.cpp:105] Iteration 27700, lr = 1e-07
I1202 13:25:22.649129 13416 solver.cpp:218] Iteration 27800 (25.0839 iter/s, 3.98662s/100 iters), loss = 0.190208
I1202 13:25:22.649129 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:25:22.649129 13416 solver.cpp:237]     Train net output #1: loss = 0.190209 (* 1 = 0.190209 loss)
I1202 13:25:22.649129 13416 sgd_solver.cpp:105] Iteration 27800, lr = 1e-07
I1202 13:25:26.636729 13416 solver.cpp:218] Iteration 27900 (25.0771 iter/s, 3.98771s/100 iters), loss = 0.089051
I1202 13:25:26.636729 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:25:26.636729 13416 solver.cpp:237]     Train net output #1: loss = 0.0890513 (* 1 = 0.0890513 loss)
I1202 13:25:26.636729 13416 sgd_solver.cpp:105] Iteration 27900, lr = 1e-07
I1202 13:25:30.427954 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:25:30.584966 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_28000.caffemodel
I1202 13:25:30.624965 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_28000.solverstate
I1202 13:25:30.627965 13416 solver.cpp:330] Iteration 28000, Testing net (#0)
I1202 13:25:30.627965 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:25:31.612027  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:25:31.651026 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9049
I1202 13:25:31.651026 13416 solver.cpp:397]     Test net output #1: loss = 0.287528 (* 1 = 0.287528 loss)
I1202 13:25:31.689030 13416 solver.cpp:218] Iteration 28000 (19.7948 iter/s, 5.05183s/100 iters), loss = 0.124337
I1202 13:25:31.689030 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:25:31.689030 13416 solver.cpp:237]     Train net output #1: loss = 0.124337 (* 1 = 0.124337 loss)
I1202 13:25:31.689030 13416 sgd_solver.cpp:105] Iteration 28000, lr = 1e-07
I1202 13:25:35.677358 13416 solver.cpp:218] Iteration 28100 (25.0767 iter/s, 3.98777s/100 iters), loss = 0.201857
I1202 13:25:35.677358 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:25:35.677860 13416 solver.cpp:237]     Train net output #1: loss = 0.201858 (* 1 = 0.201858 loss)
I1202 13:25:35.677860 13416 sgd_solver.cpp:105] Iteration 28100, lr = 1e-07
I1202 13:25:39.662853 13416 solver.cpp:218] Iteration 28200 (25.0936 iter/s, 3.98508s/100 iters), loss = 0.135219
I1202 13:25:39.662853 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:25:39.663355 13416 solver.cpp:237]     Train net output #1: loss = 0.13522 (* 1 = 0.13522 loss)
I1202 13:25:39.663355 13416 sgd_solver.cpp:105] Iteration 28200, lr = 1e-07
I1202 13:25:43.653457 13416 solver.cpp:218] Iteration 28300 (25.0622 iter/s, 3.99008s/100 iters), loss = 0.158229
I1202 13:25:43.653457 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:25:43.653457 13416 solver.cpp:237]     Train net output #1: loss = 0.158229 (* 1 = 0.158229 loss)
I1202 13:25:43.653457 13416 sgd_solver.cpp:105] Iteration 28300, lr = 1e-07
I1202 13:25:47.645869 13416 solver.cpp:218] Iteration 28400 (25.0466 iter/s, 3.99256s/100 iters), loss = 0.10726
I1202 13:25:47.645869 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1202 13:25:47.645869 13416 solver.cpp:237]     Train net output #1: loss = 0.107261 (* 1 = 0.107261 loss)
I1202 13:25:47.645869 13416 sgd_solver.cpp:105] Iteration 28400, lr = 1e-07
I1202 13:25:51.439136 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:25:51.595145 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_28500.caffemodel
I1202 13:25:51.605145 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_28500.solverstate
I1202 13:25:51.609146 13416 solver.cpp:330] Iteration 28500, Testing net (#0)
I1202 13:25:51.609146 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:25:52.592494  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:25:52.631494 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9049
I1202 13:25:52.631494 13416 solver.cpp:397]     Test net output #1: loss = 0.287496 (* 1 = 0.287496 loss)
I1202 13:25:52.670495 13416 solver.cpp:218] Iteration 28500 (19.9062 iter/s, 5.02355s/100 iters), loss = 0.152661
I1202 13:25:52.670495 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:25:52.670495 13416 solver.cpp:237]     Train net output #1: loss = 0.152661 (* 1 = 0.152661 loss)
I1202 13:25:52.670495 13416 sgd_solver.cpp:105] Iteration 28500, lr = 1e-07
I1202 13:25:56.663174 13416 solver.cpp:218] Iteration 28600 (25.0472 iter/s, 3.99246s/100 iters), loss = 0.190826
I1202 13:25:56.663174 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1202 13:25:56.663174 13416 solver.cpp:237]     Train net output #1: loss = 0.190826 (* 1 = 0.190826 loss)
I1202 13:25:56.663174 13416 sgd_solver.cpp:105] Iteration 28600, lr = 1e-07
I1202 13:26:00.658190 13416 solver.cpp:218] Iteration 28700 (25.0323 iter/s, 3.99484s/100 iters), loss = 0.196029
I1202 13:26:00.658190 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1202 13:26:00.658190 13416 solver.cpp:237]     Train net output #1: loss = 0.196029 (* 1 = 0.196029 loss)
I1202 13:26:00.658190 13416 sgd_solver.cpp:105] Iteration 28700, lr = 1e-07
I1202 13:26:04.654678 13416 solver.cpp:218] Iteration 28800 (25.0261 iter/s, 3.99583s/100 iters), loss = 0.175243
I1202 13:26:04.654678 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:26:04.654678 13416 solver.cpp:237]     Train net output #1: loss = 0.175243 (* 1 = 0.175243 loss)
I1202 13:26:04.654678 13416 sgd_solver.cpp:105] Iteration 28800, lr = 1e-07
I1202 13:26:08.649333 13416 solver.cpp:218] Iteration 28900 (25.0371 iter/s, 3.99408s/100 iters), loss = 0.0761199
I1202 13:26:08.649333 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1202 13:26:08.649333 13416 solver.cpp:237]     Train net output #1: loss = 0.0761203 (* 1 = 0.0761203 loss)
I1202 13:26:08.649333 13416 sgd_solver.cpp:105] Iteration 28900, lr = 1e-07
I1202 13:26:12.439576 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:26:12.595590 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_29000.caffemodel
I1202 13:26:12.633589 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_29000.solverstate
I1202 13:26:12.637590 13416 solver.cpp:330] Iteration 29000, Testing net (#0)
I1202 13:26:12.637590 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:26:13.621665  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:26:13.660686 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9046
I1202 13:26:13.660686 13416 solver.cpp:397]     Test net output #1: loss = 0.287577 (* 1 = 0.287577 loss)
I1202 13:26:13.698688 13416 solver.cpp:218] Iteration 29000 (19.8047 iter/s, 5.04931s/100 iters), loss = 0.163204
I1202 13:26:13.698688 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:26:13.698688 13416 solver.cpp:237]     Train net output #1: loss = 0.163205 (* 1 = 0.163205 loss)
I1202 13:26:13.698688 13416 sgd_solver.cpp:105] Iteration 29000, lr = 1e-07
I1202 13:26:17.688907 13416 solver.cpp:218] Iteration 29100 (25.0653 iter/s, 3.98958s/100 iters), loss = 0.156183
I1202 13:26:17.688907 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:26:17.688907 13416 solver.cpp:237]     Train net output #1: loss = 0.156183 (* 1 = 0.156183 loss)
I1202 13:26:17.688907 13416 sgd_solver.cpp:105] Iteration 29100, lr = 1e-07
I1202 13:26:21.677150 13416 solver.cpp:218] Iteration 29200 (25.0732 iter/s, 3.98832s/100 iters), loss = 0.161289
I1202 13:26:21.677150 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:26:21.677150 13416 solver.cpp:237]     Train net output #1: loss = 0.16129 (* 1 = 0.16129 loss)
I1202 13:26:21.677150 13416 sgd_solver.cpp:105] Iteration 29200, lr = 1e-07
I1202 13:26:25.676347 13416 solver.cpp:218] Iteration 29300 (25.0109 iter/s, 3.99826s/100 iters), loss = 0.153499
I1202 13:26:25.676347 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:26:25.676347 13416 solver.cpp:237]     Train net output #1: loss = 0.153499 (* 1 = 0.153499 loss)
I1202 13:26:25.676347 13416 sgd_solver.cpp:105] Iteration 29300, lr = 1e-07
I1202 13:26:29.664580 13416 solver.cpp:218] Iteration 29400 (25.0741 iter/s, 3.98819s/100 iters), loss = 0.0834612
I1202 13:26:29.664580 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1202 13:26:29.664580 13416 solver.cpp:237]     Train net output #1: loss = 0.0834615 (* 1 = 0.0834615 loss)
I1202 13:26:29.664580 13416 sgd_solver.cpp:105] Iteration 29400, lr = 1e-07
I1202 13:26:33.460897 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:26:33.618407 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_29500.caffemodel
I1202 13:26:33.628906 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_29500.solverstate
I1202 13:26:33.632907 13416 solver.cpp:330] Iteration 29500, Testing net (#0)
I1202 13:26:33.632907 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:26:34.615973  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:26:34.654979 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9052
I1202 13:26:34.654979 13416 solver.cpp:397]     Test net output #1: loss = 0.28746 (* 1 = 0.28746 loss)
I1202 13:26:34.692979 13416 solver.cpp:218] Iteration 29500 (19.8899 iter/s, 5.02768s/100 iters), loss = 0.133243
I1202 13:26:34.692979 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:26:34.692979 13416 solver.cpp:237]     Train net output #1: loss = 0.133243 (* 1 = 0.133243 loss)
I1202 13:26:34.692979 13416 sgd_solver.cpp:105] Iteration 29500, lr = 1e-07
I1202 13:26:38.683151 13416 solver.cpp:218] Iteration 29600 (25.0642 iter/s, 3.98975s/100 iters), loss = 0.19031
I1202 13:26:38.683151 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1202 13:26:38.683151 13416 solver.cpp:237]     Train net output #1: loss = 0.190311 (* 1 = 0.190311 loss)
I1202 13:26:38.683151 13416 sgd_solver.cpp:105] Iteration 29600, lr = 1e-07
I1202 13:26:42.676441 13416 solver.cpp:218] Iteration 29700 (25.0432 iter/s, 3.99311s/100 iters), loss = 0.139345
I1202 13:26:42.676441 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1202 13:26:42.676441 13416 solver.cpp:237]     Train net output #1: loss = 0.139345 (* 1 = 0.139345 loss)
I1202 13:26:42.676441 13416 sgd_solver.cpp:105] Iteration 29700, lr = 1e-07
I1202 13:26:46.662716 13416 solver.cpp:218] Iteration 29800 (25.0866 iter/s, 3.9862s/100 iters), loss = 0.15218
I1202 13:26:46.662716 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1202 13:26:46.662716 13416 solver.cpp:237]     Train net output #1: loss = 0.15218 (* 1 = 0.15218 loss)
I1202 13:26:46.662716 13416 sgd_solver.cpp:105] Iteration 29800, lr = 1e-07
I1202 13:26:50.639305 13416 solver.cpp:218] Iteration 29900 (25.1515 iter/s, 3.9759s/100 iters), loss = 0.102633
I1202 13:26:50.639305 13416 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1202 13:26:50.639305 13416 solver.cpp:237]     Train net output #1: loss = 0.102633 (* 1 = 0.102633 loss)
I1202 13:26:50.639305 13416 sgd_solver.cpp:105] Iteration 29900, lr = 1e-07
I1202 13:26:54.405946 17664 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:26:54.560446 13416 solver.cpp:447] Snapshotting to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_30000.caffemodel
I1202 13:26:54.591472 13416 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10/snaps/slimnet_300k_9L_iter_30000.solverstate
I1202 13:26:54.606472 13416 solver.cpp:310] Iteration 30000, loss = 0.154484
I1202 13:26:54.606472 13416 solver.cpp:330] Iteration 30000, Testing net (#0)
I1202 13:26:54.606472 13416 net.cpp:676] Ignoring source layer accuracy_training
I1202 13:26:55.590531  6724 data_layer.cpp:73] Restarting data prefetching from start.
I1202 13:26:55.628542 13416 solver.cpp:397]     Test net output #0: accuracy = 0.9049
I1202 13:26:55.628542 13416 solver.cpp:397]     Test net output #1: loss = 0.287508 (* 1 = 0.287508 loss)
I1202 13:26:55.628542 13416 solver.cpp:315] Optimization Done.
I1202 13:26:55.628542 13416 caffe.cpp:260] Optimization Done.

G:\Caffe>pause
Press any key to continue . . . 
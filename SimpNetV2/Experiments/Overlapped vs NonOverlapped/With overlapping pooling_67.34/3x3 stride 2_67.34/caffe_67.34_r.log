
G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_90000.solverstate 
I1109 15:30:59.161339 18124 caffe.cpp:219] Using GPUs 0
I1109 15:30:59.339820 18124 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1109 15:30:59.634222 18124 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1109 15:30:59.651726 18124 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1109 15:30:59.652726 18124 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1109 15:30:59.653728 18124 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1109 15:30:59.653728 18124 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1109 15:30:59.653728 18124 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1109 15:30:59.653728 18124 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_13L_Simple_NoGrpCon_NoDrp_maxdrp_300k_3x3s2_overlappool"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "drop5"
  type: "Dropout"
  bottom: "pool2_1"
  top: "pool2_1"
  dropout_param {
    dropout_ratio: 0.1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "drop9"
  type: "Dropout"
  bottom: "pool4_2"
  top: "pool4_2"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "drop12"
  type: "Dropout"
  bottom: "poolcp6"
  top: "poolcp6"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1109 15:30:59.654726 18124 layer_factory.cpp:58] Creating layer cifar
I1109 15:30:59.659726 18124 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1109 15:30:59.659726 18124 net.cpp:84] Creating Layer cifar
I1109 15:30:59.659726 18124 net.cpp:380] cifar -> data
I1109 15:30:59.659726 18124 net.cpp:380] cifar -> label
I1109 15:30:59.660728 18124 data_layer.cpp:45] output data size: 100,3,32,32
I1109 15:30:59.672726 18124 net.cpp:122] Setting up cifar
I1109 15:30:59.672726 18124 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1109 15:30:59.672726 18124 net.cpp:129] Top shape: 100 (100)
I1109 15:30:59.672726 18124 net.cpp:137] Memory required for data: 1229200
I1109 15:30:59.672726 18124 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1109 15:30:59.672726 18124 net.cpp:84] Creating Layer label_cifar_1_split
I1109 15:30:59.672726 18124 net.cpp:406] label_cifar_1_split <- label
I1109 15:30:59.672726 18124 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1109 15:30:59.672726 18124 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1109 15:30:59.672726 18124 net.cpp:122] Setting up label_cifar_1_split
I1109 15:30:59.672726 18124 net.cpp:129] Top shape: 100 (100)
I1109 15:30:59.672726 18124 net.cpp:129] Top shape: 100 (100)
I1109 15:30:59.672726 18124 net.cpp:137] Memory required for data: 1230000
I1109 15:30:59.672726 18124 layer_factory.cpp:58] Creating layer conv1
I1109 15:30:59.672726 18124 net.cpp:84] Creating Layer conv1
I1109 15:30:59.672726 18124 net.cpp:406] conv1 <- data
I1109 15:30:59.672726 18124 net.cpp:380] conv1 -> conv1
I1109 15:30:59.673728  8920 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1109 15:30:59.909754 18124 net.cpp:122] Setting up conv1
I1109 15:30:59.910753 18124 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1109 15:30:59.910753 18124 net.cpp:137] Memory required for data: 13518000
I1109 15:30:59.910753 18124 layer_factory.cpp:58] Creating layer bn1
I1109 15:30:59.910753 18124 net.cpp:84] Creating Layer bn1
I1109 15:30:59.910753 18124 net.cpp:406] bn1 <- conv1
I1109 15:30:59.910753 18124 net.cpp:367] bn1 -> conv1 (in-place)
I1109 15:30:59.910753 18124 net.cpp:122] Setting up bn1
I1109 15:30:59.910753 18124 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1109 15:30:59.910753 18124 net.cpp:137] Memory required for data: 25806000
I1109 15:30:59.910753 18124 layer_factory.cpp:58] Creating layer scale1
I1109 15:30:59.910753 18124 net.cpp:84] Creating Layer scale1
I1109 15:30:59.910753 18124 net.cpp:406] scale1 <- conv1
I1109 15:30:59.910753 18124 net.cpp:367] scale1 -> conv1 (in-place)
I1109 15:30:59.910753 18124 layer_factory.cpp:58] Creating layer scale1
I1109 15:30:59.910753 18124 net.cpp:122] Setting up scale1
I1109 15:30:59.910753 18124 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1109 15:30:59.910753 18124 net.cpp:137] Memory required for data: 38094000
I1109 15:30:59.910753 18124 layer_factory.cpp:58] Creating layer relu1
I1109 15:30:59.910753 18124 net.cpp:84] Creating Layer relu1
I1109 15:30:59.910753 18124 net.cpp:406] relu1 <- conv1
I1109 15:30:59.910753 18124 net.cpp:367] relu1 -> conv1 (in-place)
I1109 15:30:59.910753 18124 net.cpp:122] Setting up relu1
I1109 15:30:59.910753 18124 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1109 15:30:59.910753 18124 net.cpp:137] Memory required for data: 50382000
I1109 15:30:59.910753 18124 layer_factory.cpp:58] Creating layer conv1_0
I1109 15:30:59.910753 18124 net.cpp:84] Creating Layer conv1_0
I1109 15:30:59.910753 18124 net.cpp:406] conv1_0 <- conv1
I1109 15:30:59.910753 18124 net.cpp:380] conv1_0 -> conv1_0
I1109 15:30:59.912752 18124 net.cpp:122] Setting up conv1_0
I1109 15:30:59.912752 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.912752 18124 net.cpp:137] Memory required for data: 66766000
I1109 15:30:59.912752 18124 layer_factory.cpp:58] Creating layer bn1_0
I1109 15:30:59.912752 18124 net.cpp:84] Creating Layer bn1_0
I1109 15:30:59.912752 18124 net.cpp:406] bn1_0 <- conv1_0
I1109 15:30:59.912752 18124 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1109 15:30:59.912752 18124 net.cpp:122] Setting up bn1_0
I1109 15:30:59.912752 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.912752 18124 net.cpp:137] Memory required for data: 83150000
I1109 15:30:59.912752 18124 layer_factory.cpp:58] Creating layer scale1_0
I1109 15:30:59.912752 18124 net.cpp:84] Creating Layer scale1_0
I1109 15:30:59.912752 18124 net.cpp:406] scale1_0 <- conv1_0
I1109 15:30:59.912752 18124 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1109 15:30:59.912752 18124 layer_factory.cpp:58] Creating layer scale1_0
I1109 15:30:59.913753 18124 net.cpp:122] Setting up scale1_0
I1109 15:30:59.913753 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.913753 18124 net.cpp:137] Memory required for data: 99534000
I1109 15:30:59.913753 18124 layer_factory.cpp:58] Creating layer relu1_0
I1109 15:30:59.913753 18124 net.cpp:84] Creating Layer relu1_0
I1109 15:30:59.913753 18124 net.cpp:406] relu1_0 <- conv1_0
I1109 15:30:59.913753 18124 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1109 15:30:59.913753 18124 net.cpp:122] Setting up relu1_0
I1109 15:30:59.913753 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.913753 18124 net.cpp:137] Memory required for data: 115918000
I1109 15:30:59.913753 18124 layer_factory.cpp:58] Creating layer conv2
I1109 15:30:59.913753 18124 net.cpp:84] Creating Layer conv2
I1109 15:30:59.913753 18124 net.cpp:406] conv2 <- conv1_0
I1109 15:30:59.913753 18124 net.cpp:380] conv2 -> conv2
I1109 15:30:59.914753 18124 net.cpp:122] Setting up conv2
I1109 15:30:59.914753 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.914753 18124 net.cpp:137] Memory required for data: 132302000
I1109 15:30:59.914753 18124 layer_factory.cpp:58] Creating layer bn2
I1109 15:30:59.914753 18124 net.cpp:84] Creating Layer bn2
I1109 15:30:59.914753 18124 net.cpp:406] bn2 <- conv2
I1109 15:30:59.914753 18124 net.cpp:367] bn2 -> conv2 (in-place)
I1109 15:30:59.914753 18124 net.cpp:122] Setting up bn2
I1109 15:30:59.914753 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.914753 18124 net.cpp:137] Memory required for data: 148686000
I1109 15:30:59.914753 18124 layer_factory.cpp:58] Creating layer scale2
I1109 15:30:59.914753 18124 net.cpp:84] Creating Layer scale2
I1109 15:30:59.914753 18124 net.cpp:406] scale2 <- conv2
I1109 15:30:59.914753 18124 net.cpp:367] scale2 -> conv2 (in-place)
I1109 15:30:59.914753 18124 layer_factory.cpp:58] Creating layer scale2
I1109 15:30:59.914753 18124 net.cpp:122] Setting up scale2
I1109 15:30:59.914753 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.914753 18124 net.cpp:137] Memory required for data: 165070000
I1109 15:30:59.915752 18124 layer_factory.cpp:58] Creating layer relu2
I1109 15:30:59.915752 18124 net.cpp:84] Creating Layer relu2
I1109 15:30:59.915752 18124 net.cpp:406] relu2 <- conv2
I1109 15:30:59.915752 18124 net.cpp:367] relu2 -> conv2 (in-place)
I1109 15:30:59.915752 18124 net.cpp:122] Setting up relu2
I1109 15:30:59.915752 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.915752 18124 net.cpp:137] Memory required for data: 181454000
I1109 15:30:59.915752 18124 layer_factory.cpp:58] Creating layer conv2_1
I1109 15:30:59.915752 18124 net.cpp:84] Creating Layer conv2_1
I1109 15:30:59.915752 18124 net.cpp:406] conv2_1 <- conv2
I1109 15:30:59.915752 18124 net.cpp:380] conv2_1 -> conv2_1
I1109 15:30:59.916754 18124 net.cpp:122] Setting up conv2_1
I1109 15:30:59.916754 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.916754 18124 net.cpp:137] Memory required for data: 197838000
I1109 15:30:59.916754 18124 layer_factory.cpp:58] Creating layer bn2_1
I1109 15:30:59.916754 18124 net.cpp:84] Creating Layer bn2_1
I1109 15:30:59.916754 18124 net.cpp:406] bn2_1 <- conv2_1
I1109 15:30:59.916754 18124 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1109 15:30:59.916754 18124 net.cpp:122] Setting up bn2_1
I1109 15:30:59.916754 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.916754 18124 net.cpp:137] Memory required for data: 214222000
I1109 15:30:59.916754 18124 layer_factory.cpp:58] Creating layer scale2_1
I1109 15:30:59.916754 18124 net.cpp:84] Creating Layer scale2_1
I1109 15:30:59.916754 18124 net.cpp:406] scale2_1 <- conv2_1
I1109 15:30:59.916754 18124 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1109 15:30:59.916754 18124 layer_factory.cpp:58] Creating layer scale2_1
I1109 15:30:59.916754 18124 net.cpp:122] Setting up scale2_1
I1109 15:30:59.916754 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.916754 18124 net.cpp:137] Memory required for data: 230606000
I1109 15:30:59.917752 18124 layer_factory.cpp:58] Creating layer relu2_1
I1109 15:30:59.917752 18124 net.cpp:84] Creating Layer relu2_1
I1109 15:30:59.917752 18124 net.cpp:406] relu2_1 <- conv2_1
I1109 15:30:59.917752 18124 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1109 15:30:59.917752 18124 net.cpp:122] Setting up relu2_1
I1109 15:30:59.917752 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.917752 18124 net.cpp:137] Memory required for data: 246990000
I1109 15:30:59.917752 18124 layer_factory.cpp:58] Creating layer conv2_2
I1109 15:30:59.917752 18124 net.cpp:84] Creating Layer conv2_2
I1109 15:30:59.917752 18124 net.cpp:406] conv2_2 <- conv2_1
I1109 15:30:59.917752 18124 net.cpp:380] conv2_2 -> conv2_2
I1109 15:30:59.919752 18124 net.cpp:122] Setting up conv2_2
I1109 15:30:59.919752 18124 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1109 15:30:59.919752 18124 net.cpp:137] Memory required for data: 267470000
I1109 15:30:59.919752 18124 layer_factory.cpp:58] Creating layer bn2_2
I1109 15:30:59.919752 18124 net.cpp:84] Creating Layer bn2_2
I1109 15:30:59.919752 18124 net.cpp:406] bn2_2 <- conv2_2
I1109 15:30:59.919752 18124 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1109 15:30:59.919752 18124 net.cpp:122] Setting up bn2_2
I1109 15:30:59.919752 18124 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1109 15:30:59.919752 18124 net.cpp:137] Memory required for data: 287950000
I1109 15:30:59.919752 18124 layer_factory.cpp:58] Creating layer scale2_2
I1109 15:30:59.919752 18124 net.cpp:84] Creating Layer scale2_2
I1109 15:30:59.919752 18124 net.cpp:406] scale2_2 <- conv2_2
I1109 15:30:59.919752 18124 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1109 15:30:59.919752 18124 layer_factory.cpp:58] Creating layer scale2_2
I1109 15:30:59.919752 18124 net.cpp:122] Setting up scale2_2
I1109 15:30:59.919752 18124 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1109 15:30:59.919752 18124 net.cpp:137] Memory required for data: 308430000
I1109 15:30:59.919752 18124 layer_factory.cpp:58] Creating layer relu2_2
I1109 15:30:59.919752 18124 net.cpp:84] Creating Layer relu2_2
I1109 15:30:59.919752 18124 net.cpp:406] relu2_2 <- conv2_2
I1109 15:30:59.919752 18124 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1109 15:30:59.919752 18124 net.cpp:122] Setting up relu2_2
I1109 15:30:59.919752 18124 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1109 15:30:59.919752 18124 net.cpp:137] Memory required for data: 328910000
I1109 15:30:59.919752 18124 layer_factory.cpp:58] Creating layer pool2_1
I1109 15:30:59.919752 18124 net.cpp:84] Creating Layer pool2_1
I1109 15:30:59.919752 18124 net.cpp:406] pool2_1 <- conv2_2
I1109 15:30:59.919752 18124 net.cpp:380] pool2_1 -> pool2_1
I1109 15:30:59.919752 18124 net.cpp:122] Setting up pool2_1
I1109 15:30:59.919752 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.919752 18124 net.cpp:137] Memory required for data: 334030000
I1109 15:30:59.919752 18124 layer_factory.cpp:58] Creating layer drop5
I1109 15:30:59.919752 18124 net.cpp:84] Creating Layer drop5
I1109 15:30:59.919752 18124 net.cpp:406] drop5 <- pool2_1
I1109 15:30:59.919752 18124 net.cpp:367] drop5 -> pool2_1 (in-place)
I1109 15:30:59.919752 18124 net.cpp:122] Setting up drop5
I1109 15:30:59.919752 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.919752 18124 net.cpp:137] Memory required for data: 339150000
I1109 15:30:59.919752 18124 layer_factory.cpp:58] Creating layer conv3
I1109 15:30:59.919752 18124 net.cpp:84] Creating Layer conv3
I1109 15:30:59.919752 18124 net.cpp:406] conv3 <- pool2_1
I1109 15:30:59.919752 18124 net.cpp:380] conv3 -> conv3
I1109 15:30:59.921753 18124 net.cpp:122] Setting up conv3
I1109 15:30:59.921753 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.921753 18124 net.cpp:137] Memory required for data: 344270000
I1109 15:30:59.921753 18124 layer_factory.cpp:58] Creating layer bn3
I1109 15:30:59.921753 18124 net.cpp:84] Creating Layer bn3
I1109 15:30:59.921753 18124 net.cpp:406] bn3 <- conv3
I1109 15:30:59.921753 18124 net.cpp:367] bn3 -> conv3 (in-place)
I1109 15:30:59.921753 18124 net.cpp:122] Setting up bn3
I1109 15:30:59.921753 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.921753 18124 net.cpp:137] Memory required for data: 349390000
I1109 15:30:59.921753 18124 layer_factory.cpp:58] Creating layer scale3
I1109 15:30:59.921753 18124 net.cpp:84] Creating Layer scale3
I1109 15:30:59.921753 18124 net.cpp:406] scale3 <- conv3
I1109 15:30:59.921753 18124 net.cpp:367] scale3 -> conv3 (in-place)
I1109 15:30:59.921753 18124 layer_factory.cpp:58] Creating layer scale3
I1109 15:30:59.921753 18124 net.cpp:122] Setting up scale3
I1109 15:30:59.921753 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.921753 18124 net.cpp:137] Memory required for data: 354510000
I1109 15:30:59.921753 18124 layer_factory.cpp:58] Creating layer relu3
I1109 15:30:59.921753 18124 net.cpp:84] Creating Layer relu3
I1109 15:30:59.921753 18124 net.cpp:406] relu3 <- conv3
I1109 15:30:59.921753 18124 net.cpp:367] relu3 -> conv3 (in-place)
I1109 15:30:59.922752 18124 net.cpp:122] Setting up relu3
I1109 15:30:59.922752 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.922752 18124 net.cpp:137] Memory required for data: 359630000
I1109 15:30:59.922752 18124 layer_factory.cpp:58] Creating layer conv3_1
I1109 15:30:59.922752 18124 net.cpp:84] Creating Layer conv3_1
I1109 15:30:59.922752 18124 net.cpp:406] conv3_1 <- conv3
I1109 15:30:59.922752 18124 net.cpp:380] conv3_1 -> conv3_1
I1109 15:30:59.923754 18124 net.cpp:122] Setting up conv3_1
I1109 15:30:59.923754 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.923754 18124 net.cpp:137] Memory required for data: 364750000
I1109 15:30:59.923754 18124 layer_factory.cpp:58] Creating layer bn3_1
I1109 15:30:59.923754 18124 net.cpp:84] Creating Layer bn3_1
I1109 15:30:59.923754 18124 net.cpp:406] bn3_1 <- conv3_1
I1109 15:30:59.923754 18124 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1109 15:30:59.924752 18124 net.cpp:122] Setting up bn3_1
I1109 15:30:59.924752 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.924752 18124 net.cpp:137] Memory required for data: 369870000
I1109 15:30:59.924752 18124 layer_factory.cpp:58] Creating layer scale3_1
I1109 15:30:59.924752 18124 net.cpp:84] Creating Layer scale3_1
I1109 15:30:59.924752 18124 net.cpp:406] scale3_1 <- conv3_1
I1109 15:30:59.924752 18124 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1109 15:30:59.924752 18124 layer_factory.cpp:58] Creating layer scale3_1
I1109 15:30:59.924752 18124 net.cpp:122] Setting up scale3_1
I1109 15:30:59.924752 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.924752 18124 net.cpp:137] Memory required for data: 374990000
I1109 15:30:59.924752 18124 layer_factory.cpp:58] Creating layer relu3_1
I1109 15:30:59.924752 18124 net.cpp:84] Creating Layer relu3_1
I1109 15:30:59.924752 18124 net.cpp:406] relu3_1 <- conv3_1
I1109 15:30:59.924752 18124 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1109 15:30:59.924752 18124 net.cpp:122] Setting up relu3_1
I1109 15:30:59.924752 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.924752 18124 net.cpp:137] Memory required for data: 380110000
I1109 15:30:59.924752 18124 layer_factory.cpp:58] Creating layer conv4
I1109 15:30:59.924752 18124 net.cpp:84] Creating Layer conv4
I1109 15:30:59.924752 18124 net.cpp:406] conv4 <- conv3_1
I1109 15:30:59.924752 18124 net.cpp:380] conv4 -> conv4
I1109 15:30:59.925755 18124 net.cpp:122] Setting up conv4
I1109 15:30:59.925755 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.925755 18124 net.cpp:137] Memory required for data: 385230000
I1109 15:30:59.925755 18124 layer_factory.cpp:58] Creating layer bn4
I1109 15:30:59.925755 18124 net.cpp:84] Creating Layer bn4
I1109 15:30:59.925755 18124 net.cpp:406] bn4 <- conv4
I1109 15:30:59.925755 18124 net.cpp:367] bn4 -> conv4 (in-place)
I1109 15:30:59.926753 18124 net.cpp:122] Setting up bn4
I1109 15:30:59.926753 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.926753 18124 net.cpp:137] Memory required for data: 390350000
I1109 15:30:59.926753 18124 layer_factory.cpp:58] Creating layer scale4
I1109 15:30:59.926753 18124 net.cpp:84] Creating Layer scale4
I1109 15:30:59.926753 18124 net.cpp:406] scale4 <- conv4
I1109 15:30:59.926753 18124 net.cpp:367] scale4 -> conv4 (in-place)
I1109 15:30:59.926753 18124 layer_factory.cpp:58] Creating layer scale4
I1109 15:30:59.926753 18124 net.cpp:122] Setting up scale4
I1109 15:30:59.926753 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.926753 18124 net.cpp:137] Memory required for data: 395470000
I1109 15:30:59.926753 18124 layer_factory.cpp:58] Creating layer relu4
I1109 15:30:59.926753 18124 net.cpp:84] Creating Layer relu4
I1109 15:30:59.926753 18124 net.cpp:406] relu4 <- conv4
I1109 15:30:59.926753 18124 net.cpp:367] relu4 -> conv4 (in-place)
I1109 15:30:59.927754 18124 net.cpp:122] Setting up relu4
I1109 15:30:59.927754 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.927754 18124 net.cpp:137] Memory required for data: 400590000
I1109 15:30:59.927754 18124 layer_factory.cpp:58] Creating layer conv4_1
I1109 15:30:59.927754 18124 net.cpp:84] Creating Layer conv4_1
I1109 15:30:59.927754 18124 net.cpp:406] conv4_1 <- conv4
I1109 15:30:59.927754 18124 net.cpp:380] conv4_1 -> conv4_1
I1109 15:30:59.928752 18124 net.cpp:122] Setting up conv4_1
I1109 15:30:59.928752 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.928752 18124 net.cpp:137] Memory required for data: 405710000
I1109 15:30:59.928752 18124 layer_factory.cpp:58] Creating layer bn4_1
I1109 15:30:59.928752 18124 net.cpp:84] Creating Layer bn4_1
I1109 15:30:59.928752 18124 net.cpp:406] bn4_1 <- conv4_1
I1109 15:30:59.928752 18124 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1109 15:30:59.928752 18124 net.cpp:122] Setting up bn4_1
I1109 15:30:59.928752 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.928752 18124 net.cpp:137] Memory required for data: 410830000
I1109 15:30:59.928752 18124 layer_factory.cpp:58] Creating layer scale4_1
I1109 15:30:59.928752 18124 net.cpp:84] Creating Layer scale4_1
I1109 15:30:59.928752 18124 net.cpp:406] scale4_1 <- conv4_1
I1109 15:30:59.928752 18124 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1109 15:30:59.928752 18124 layer_factory.cpp:58] Creating layer scale4_1
I1109 15:30:59.928752 18124 net.cpp:122] Setting up scale4_1
I1109 15:30:59.928752 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.928752 18124 net.cpp:137] Memory required for data: 415950000
I1109 15:30:59.928752 18124 layer_factory.cpp:58] Creating layer relu4_1
I1109 15:30:59.928752 18124 net.cpp:84] Creating Layer relu4_1
I1109 15:30:59.928752 18124 net.cpp:406] relu4_1 <- conv4_1
I1109 15:30:59.928752 18124 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1109 15:30:59.929751 18124 net.cpp:122] Setting up relu4_1
I1109 15:30:59.929751 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.929751 18124 net.cpp:137] Memory required for data: 421070000
I1109 15:30:59.929751 18124 layer_factory.cpp:58] Creating layer conv4_2
I1109 15:30:59.929751 18124 net.cpp:84] Creating Layer conv4_2
I1109 15:30:59.929751 18124 net.cpp:406] conv4_2 <- conv4_1
I1109 15:30:59.929751 18124 net.cpp:380] conv4_2 -> conv4_2
I1109 15:30:59.930752 18124 net.cpp:122] Setting up conv4_2
I1109 15:30:59.930752 18124 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1109 15:30:59.930752 18124 net.cpp:137] Memory required for data: 427009200
I1109 15:30:59.930752 18124 layer_factory.cpp:58] Creating layer bn4_2
I1109 15:30:59.930752 18124 net.cpp:84] Creating Layer bn4_2
I1109 15:30:59.930752 18124 net.cpp:406] bn4_2 <- conv4_2
I1109 15:30:59.930752 18124 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1109 15:30:59.930752 18124 net.cpp:122] Setting up bn4_2
I1109 15:30:59.930752 18124 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1109 15:30:59.930752 18124 net.cpp:137] Memory required for data: 432948400
I1109 15:30:59.930752 18124 layer_factory.cpp:58] Creating layer scale4_2
I1109 15:30:59.930752 18124 net.cpp:84] Creating Layer scale4_2
I1109 15:30:59.930752 18124 net.cpp:406] scale4_2 <- conv4_2
I1109 15:30:59.930752 18124 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1109 15:30:59.930752 18124 layer_factory.cpp:58] Creating layer scale4_2
I1109 15:30:59.930752 18124 net.cpp:122] Setting up scale4_2
I1109 15:30:59.930752 18124 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1109 15:30:59.930752 18124 net.cpp:137] Memory required for data: 438887600
I1109 15:30:59.931752 18124 layer_factory.cpp:58] Creating layer relu4_2
I1109 15:30:59.931752 18124 net.cpp:84] Creating Layer relu4_2
I1109 15:30:59.931752 18124 net.cpp:406] relu4_2 <- conv4_2
I1109 15:30:59.931752 18124 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1109 15:30:59.931752 18124 net.cpp:122] Setting up relu4_2
I1109 15:30:59.931752 18124 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1109 15:30:59.931752 18124 net.cpp:137] Memory required for data: 444826800
I1109 15:30:59.931752 18124 layer_factory.cpp:58] Creating layer pool4_2
I1109 15:30:59.931752 18124 net.cpp:84] Creating Layer pool4_2
I1109 15:30:59.931752 18124 net.cpp:406] pool4_2 <- conv4_2
I1109 15:30:59.931752 18124 net.cpp:380] pool4_2 -> pool4_2
I1109 15:30:59.931752 18124 net.cpp:122] Setting up pool4_2
I1109 15:30:59.931752 18124 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1109 15:30:59.931752 18124 net.cpp:137] Memory required for data: 446311600
I1109 15:30:59.931752 18124 layer_factory.cpp:58] Creating layer drop9
I1109 15:30:59.931752 18124 net.cpp:84] Creating Layer drop9
I1109 15:30:59.931752 18124 net.cpp:406] drop9 <- pool4_2
I1109 15:30:59.931752 18124 net.cpp:367] drop9 -> pool4_2 (in-place)
I1109 15:30:59.931752 18124 net.cpp:122] Setting up drop9
I1109 15:30:59.931752 18124 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1109 15:30:59.931752 18124 net.cpp:137] Memory required for data: 447796400
I1109 15:30:59.931752 18124 layer_factory.cpp:58] Creating layer conv4_0
I1109 15:30:59.931752 18124 net.cpp:84] Creating Layer conv4_0
I1109 15:30:59.931752 18124 net.cpp:406] conv4_0 <- pool4_2
I1109 15:30:59.931752 18124 net.cpp:380] conv4_0 -> conv4_0
I1109 15:30:59.933256 18124 net.cpp:122] Setting up conv4_0
I1109 15:30:59.933256 18124 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1109 15:30:59.933256 18124 net.cpp:137] Memory required for data: 449281200
I1109 15:30:59.933256 18124 layer_factory.cpp:58] Creating layer bn4_0
I1109 15:30:59.933256 18124 net.cpp:84] Creating Layer bn4_0
I1109 15:30:59.933256 18124 net.cpp:406] bn4_0 <- conv4_0
I1109 15:30:59.933256 18124 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1109 15:30:59.933758 18124 net.cpp:122] Setting up bn4_0
I1109 15:30:59.933758 18124 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1109 15:30:59.933758 18124 net.cpp:137] Memory required for data: 450766000
I1109 15:30:59.933758 18124 layer_factory.cpp:58] Creating layer scale4_0
I1109 15:30:59.933758 18124 net.cpp:84] Creating Layer scale4_0
I1109 15:30:59.933758 18124 net.cpp:406] scale4_0 <- conv4_0
I1109 15:30:59.933758 18124 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1109 15:30:59.933758 18124 layer_factory.cpp:58] Creating layer scale4_0
I1109 15:30:59.933758 18124 net.cpp:122] Setting up scale4_0
I1109 15:30:59.933758 18124 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1109 15:30:59.933758 18124 net.cpp:137] Memory required for data: 452250800
I1109 15:30:59.933758 18124 layer_factory.cpp:58] Creating layer relu4_0
I1109 15:30:59.933758 18124 net.cpp:84] Creating Layer relu4_0
I1109 15:30:59.933758 18124 net.cpp:406] relu4_0 <- conv4_0
I1109 15:30:59.933758 18124 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1109 15:30:59.934255 18124 net.cpp:122] Setting up relu4_0
I1109 15:30:59.934255 18124 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1109 15:30:59.934255 18124 net.cpp:137] Memory required for data: 453735600
I1109 15:30:59.934255 18124 layer_factory.cpp:58] Creating layer conv11
I1109 15:30:59.934255 18124 net.cpp:84] Creating Layer conv11
I1109 15:30:59.934255 18124 net.cpp:406] conv11 <- conv4_0
I1109 15:30:59.934255 18124 net.cpp:380] conv11 -> conv11
I1109 15:30:59.935756 18124 net.cpp:122] Setting up conv11
I1109 15:30:59.935756 18124 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1109 15:30:59.935756 18124 net.cpp:137] Memory required for data: 455527600
I1109 15:30:59.935756 18124 layer_factory.cpp:58] Creating layer bn_conv11
I1109 15:30:59.935756 18124 net.cpp:84] Creating Layer bn_conv11
I1109 15:30:59.935756 18124 net.cpp:406] bn_conv11 <- conv11
I1109 15:30:59.935756 18124 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1109 15:30:59.935756 18124 net.cpp:122] Setting up bn_conv11
I1109 15:30:59.935756 18124 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1109 15:30:59.935756 18124 net.cpp:137] Memory required for data: 457319600
I1109 15:30:59.935756 18124 layer_factory.cpp:58] Creating layer scale_conv11
I1109 15:30:59.935756 18124 net.cpp:84] Creating Layer scale_conv11
I1109 15:30:59.935756 18124 net.cpp:406] scale_conv11 <- conv11
I1109 15:30:59.935756 18124 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1109 15:30:59.935756 18124 layer_factory.cpp:58] Creating layer scale_conv11
I1109 15:30:59.936255 18124 net.cpp:122] Setting up scale_conv11
I1109 15:30:59.936255 18124 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1109 15:30:59.936255 18124 net.cpp:137] Memory required for data: 459111600
I1109 15:30:59.936255 18124 layer_factory.cpp:58] Creating layer relu_conv11
I1109 15:30:59.936255 18124 net.cpp:84] Creating Layer relu_conv11
I1109 15:30:59.936255 18124 net.cpp:406] relu_conv11 <- conv11
I1109 15:30:59.936255 18124 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1109 15:30:59.936255 18124 net.cpp:122] Setting up relu_conv11
I1109 15:30:59.936255 18124 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1109 15:30:59.936255 18124 net.cpp:137] Memory required for data: 460903600
I1109 15:30:59.936255 18124 layer_factory.cpp:58] Creating layer conv12
I1109 15:30:59.936255 18124 net.cpp:84] Creating Layer conv12
I1109 15:30:59.936255 18124 net.cpp:406] conv12 <- conv11
I1109 15:30:59.936255 18124 net.cpp:380] conv12 -> conv12
I1109 15:30:59.937755 18124 net.cpp:122] Setting up conv12
I1109 15:30:59.937755 18124 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1109 15:30:59.937755 18124 net.cpp:137] Memory required for data: 463207600
I1109 15:30:59.937755 18124 layer_factory.cpp:58] Creating layer bn_conv12
I1109 15:30:59.937755 18124 net.cpp:84] Creating Layer bn_conv12
I1109 15:30:59.937755 18124 net.cpp:406] bn_conv12 <- conv12
I1109 15:30:59.937755 18124 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1109 15:30:59.938256 18124 net.cpp:122] Setting up bn_conv12
I1109 15:30:59.938256 18124 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1109 15:30:59.938256 18124 net.cpp:137] Memory required for data: 465511600
I1109 15:30:59.938256 18124 layer_factory.cpp:58] Creating layer scale_conv12
I1109 15:30:59.938256 18124 net.cpp:84] Creating Layer scale_conv12
I1109 15:30:59.938256 18124 net.cpp:406] scale_conv12 <- conv12
I1109 15:30:59.938256 18124 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1109 15:30:59.938256 18124 layer_factory.cpp:58] Creating layer scale_conv12
I1109 15:30:59.938256 18124 net.cpp:122] Setting up scale_conv12
I1109 15:30:59.938256 18124 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1109 15:30:59.938256 18124 net.cpp:137] Memory required for data: 467815600
I1109 15:30:59.938256 18124 layer_factory.cpp:58] Creating layer relu_conv12
I1109 15:30:59.938256 18124 net.cpp:84] Creating Layer relu_conv12
I1109 15:30:59.938256 18124 net.cpp:406] relu_conv12 <- conv12
I1109 15:30:59.938256 18124 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1109 15:30:59.938755 18124 net.cpp:122] Setting up relu_conv12
I1109 15:30:59.938755 18124 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1109 15:30:59.938755 18124 net.cpp:137] Memory required for data: 470119600
I1109 15:30:59.938755 18124 layer_factory.cpp:58] Creating layer poolcp6
I1109 15:30:59.938755 18124 net.cpp:84] Creating Layer poolcp6
I1109 15:30:59.938755 18124 net.cpp:406] poolcp6 <- conv12
I1109 15:30:59.938755 18124 net.cpp:380] poolcp6 -> poolcp6
I1109 15:30:59.938755 18124 net.cpp:122] Setting up poolcp6
I1109 15:30:59.938755 18124 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1109 15:30:59.938755 18124 net.cpp:137] Memory required for data: 470155600
I1109 15:30:59.938755 18124 layer_factory.cpp:58] Creating layer drop12
I1109 15:30:59.938755 18124 net.cpp:84] Creating Layer drop12
I1109 15:30:59.938755 18124 net.cpp:406] drop12 <- poolcp6
I1109 15:30:59.938755 18124 net.cpp:367] drop12 -> poolcp6 (in-place)
I1109 15:30:59.938755 18124 net.cpp:122] Setting up drop12
I1109 15:30:59.938755 18124 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1109 15:30:59.938755 18124 net.cpp:137] Memory required for data: 470191600
I1109 15:30:59.938755 18124 layer_factory.cpp:58] Creating layer ip1
I1109 15:30:59.938755 18124 net.cpp:84] Creating Layer ip1
I1109 15:30:59.938755 18124 net.cpp:406] ip1 <- poolcp6
I1109 15:30:59.938755 18124 net.cpp:380] ip1 -> ip1
I1109 15:30:59.938755 18124 net.cpp:122] Setting up ip1
I1109 15:30:59.938755 18124 net.cpp:129] Top shape: 100 100 (10000)
I1109 15:30:59.938755 18124 net.cpp:137] Memory required for data: 470231600
I1109 15:30:59.938755 18124 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1109 15:30:59.938755 18124 net.cpp:84] Creating Layer ip1_ip1_0_split
I1109 15:30:59.938755 18124 net.cpp:406] ip1_ip1_0_split <- ip1
I1109 15:30:59.938755 18124 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1109 15:30:59.939255 18124 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1109 15:30:59.939255 18124 net.cpp:122] Setting up ip1_ip1_0_split
I1109 15:30:59.939255 18124 net.cpp:129] Top shape: 100 100 (10000)
I1109 15:30:59.939255 18124 net.cpp:129] Top shape: 100 100 (10000)
I1109 15:30:59.939255 18124 net.cpp:137] Memory required for data: 470311600
I1109 15:30:59.939255 18124 layer_factory.cpp:58] Creating layer accuracy_training
I1109 15:30:59.939255 18124 net.cpp:84] Creating Layer accuracy_training
I1109 15:30:59.939255 18124 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1109 15:30:59.939255 18124 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1109 15:30:59.939255 18124 net.cpp:380] accuracy_training -> accuracy_training
I1109 15:30:59.939255 18124 net.cpp:122] Setting up accuracy_training
I1109 15:30:59.939255 18124 net.cpp:129] Top shape: (1)
I1109 15:30:59.939255 18124 net.cpp:137] Memory required for data: 470311604
I1109 15:30:59.939255 18124 layer_factory.cpp:58] Creating layer loss
I1109 15:30:59.939255 18124 net.cpp:84] Creating Layer loss
I1109 15:30:59.939255 18124 net.cpp:406] loss <- ip1_ip1_0_split_1
I1109 15:30:59.939255 18124 net.cpp:406] loss <- label_cifar_1_split_1
I1109 15:30:59.939255 18124 net.cpp:380] loss -> loss
I1109 15:30:59.939255 18124 layer_factory.cpp:58] Creating layer loss
I1109 15:30:59.939755 18124 net.cpp:122] Setting up loss
I1109 15:30:59.939755 18124 net.cpp:129] Top shape: (1)
I1109 15:30:59.939755 18124 net.cpp:132]     with loss weight 1
I1109 15:30:59.939755 18124 net.cpp:137] Memory required for data: 470311608
I1109 15:30:59.939755 18124 net.cpp:198] loss needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:200] accuracy_training does not need backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] ip1 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] drop12 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] poolcp6 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] relu_conv12 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] scale_conv12 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] bn_conv12 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] conv12 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] relu_conv11 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] scale_conv11 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] bn_conv11 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] conv11 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] relu4_0 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] scale4_0 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] bn4_0 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] conv4_0 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] drop9 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] pool4_2 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] relu4_2 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] scale4_2 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] bn4_2 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] conv4_2 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] relu4_1 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] scale4_1 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] bn4_1 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] conv4_1 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] relu4 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] scale4 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] bn4 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] conv4 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] relu3_1 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] scale3_1 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] bn3_1 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] conv3_1 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] relu3 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] scale3 needs backward computation.
I1109 15:30:59.939755 18124 net.cpp:198] bn3 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] conv3 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] drop5 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] pool2_1 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] relu2_2 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] scale2_2 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] bn2_2 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] conv2_2 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] relu2_1 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] scale2_1 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] bn2_1 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] conv2_1 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] relu2 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] scale2 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] bn2 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] conv2 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] relu1_0 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] scale1_0 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] bn1_0 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] conv1_0 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] relu1 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] scale1 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] bn1 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:198] conv1 needs backward computation.
I1109 15:30:59.940255 18124 net.cpp:200] label_cifar_1_split does not need backward computation.
I1109 15:30:59.940255 18124 net.cpp:200] cifar does not need backward computation.
I1109 15:30:59.940255 18124 net.cpp:242] This network produces output accuracy_training
I1109 15:30:59.940255 18124 net.cpp:242] This network produces output loss
I1109 15:30:59.940255 18124 net.cpp:255] Network initialization done.
I1109 15:30:59.940755 18124 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1109 15:30:59.940755 18124 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1109 15:30:59.940755 18124 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1109 15:30:59.941256 18124 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1109 15:30:59.941256 18124 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_13L_Simple_NoGrpCon_NoDrp_maxdrp_300k_3x3s2_overlappool"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "drop5"
  type: "Dropout"
  bottom: "pool2_1"
  top: "pool2_1"
  dropout_param {
    dropout_ratio: 0.1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "drop9"
  type: "Dropout"
  bottom: "pool4_2"
  top: "pool4_2"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "drop12"
  type: "Dropout"
  bottom: "poolcp6"
  top: "poolcp6"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1109 15:30:59.941756 18124 layer_factory.cpp:58] Creating layer cifar
I1109 15:30:59.944754 18124 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1109 15:30:59.944754 18124 net.cpp:84] Creating Layer cifar
I1109 15:30:59.944754 18124 net.cpp:380] cifar -> data
I1109 15:30:59.944754 18124 net.cpp:380] cifar -> label
I1109 15:30:59.944754 18124 data_layer.cpp:45] output data size: 100,3,32,32
I1109 15:30:59.952759 18124 net.cpp:122] Setting up cifar
I1109 15:30:59.952759 18124 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1109 15:30:59.952759 18124 net.cpp:129] Top shape: 100 (100)
I1109 15:30:59.952759 18124 net.cpp:137] Memory required for data: 1229200
I1109 15:30:59.952759 18124 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1109 15:30:59.952759 18124 net.cpp:84] Creating Layer label_cifar_1_split
I1109 15:30:59.952759 18124 net.cpp:406] label_cifar_1_split <- label
I1109 15:30:59.952759 18124 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1109 15:30:59.952759 18124 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1109 15:30:59.952759 18124 net.cpp:122] Setting up label_cifar_1_split
I1109 15:30:59.952759 18124 net.cpp:129] Top shape: 100 (100)
I1109 15:30:59.952759 18124 net.cpp:129] Top shape: 100 (100)
I1109 15:30:59.952759 18124 net.cpp:137] Memory required for data: 1230000
I1109 15:30:59.952759 18124 layer_factory.cpp:58] Creating layer conv1
I1109 15:30:59.952759 18124 net.cpp:84] Creating Layer conv1
I1109 15:30:59.952759 18124 net.cpp:406] conv1 <- data
I1109 15:30:59.952759 18124 net.cpp:380] conv1 -> conv1
I1109 15:30:59.953759 11152 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1109 15:30:59.954761 18124 net.cpp:122] Setting up conv1
I1109 15:30:59.954761 18124 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1109 15:30:59.954761 18124 net.cpp:137] Memory required for data: 13518000
I1109 15:30:59.954761 18124 layer_factory.cpp:58] Creating layer bn1
I1109 15:30:59.954761 18124 net.cpp:84] Creating Layer bn1
I1109 15:30:59.954761 18124 net.cpp:406] bn1 <- conv1
I1109 15:30:59.954761 18124 net.cpp:367] bn1 -> conv1 (in-place)
I1109 15:30:59.954761 18124 net.cpp:122] Setting up bn1
I1109 15:30:59.954761 18124 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1109 15:30:59.954761 18124 net.cpp:137] Memory required for data: 25806000
I1109 15:30:59.954761 18124 layer_factory.cpp:58] Creating layer scale1
I1109 15:30:59.954761 18124 net.cpp:84] Creating Layer scale1
I1109 15:30:59.954761 18124 net.cpp:406] scale1 <- conv1
I1109 15:30:59.954761 18124 net.cpp:367] scale1 -> conv1 (in-place)
I1109 15:30:59.954761 18124 layer_factory.cpp:58] Creating layer scale1
I1109 15:30:59.954761 18124 net.cpp:122] Setting up scale1
I1109 15:30:59.954761 18124 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1109 15:30:59.954761 18124 net.cpp:137] Memory required for data: 38094000
I1109 15:30:59.954761 18124 layer_factory.cpp:58] Creating layer relu1
I1109 15:30:59.954761 18124 net.cpp:84] Creating Layer relu1
I1109 15:30:59.954761 18124 net.cpp:406] relu1 <- conv1
I1109 15:30:59.954761 18124 net.cpp:367] relu1 -> conv1 (in-place)
I1109 15:30:59.954761 18124 net.cpp:122] Setting up relu1
I1109 15:30:59.954761 18124 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1109 15:30:59.954761 18124 net.cpp:137] Memory required for data: 50382000
I1109 15:30:59.954761 18124 layer_factory.cpp:58] Creating layer conv1_0
I1109 15:30:59.954761 18124 net.cpp:84] Creating Layer conv1_0
I1109 15:30:59.954761 18124 net.cpp:406] conv1_0 <- conv1
I1109 15:30:59.954761 18124 net.cpp:380] conv1_0 -> conv1_0
I1109 15:30:59.956759 18124 net.cpp:122] Setting up conv1_0
I1109 15:30:59.956759 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.956759 18124 net.cpp:137] Memory required for data: 66766000
I1109 15:30:59.956759 18124 layer_factory.cpp:58] Creating layer bn1_0
I1109 15:30:59.956759 18124 net.cpp:84] Creating Layer bn1_0
I1109 15:30:59.956759 18124 net.cpp:406] bn1_0 <- conv1_0
I1109 15:30:59.956759 18124 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1109 15:30:59.956759 18124 net.cpp:122] Setting up bn1_0
I1109 15:30:59.956759 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.956759 18124 net.cpp:137] Memory required for data: 83150000
I1109 15:30:59.956759 18124 layer_factory.cpp:58] Creating layer scale1_0
I1109 15:30:59.956759 18124 net.cpp:84] Creating Layer scale1_0
I1109 15:30:59.956759 18124 net.cpp:406] scale1_0 <- conv1_0
I1109 15:30:59.956759 18124 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1109 15:30:59.956759 18124 layer_factory.cpp:58] Creating layer scale1_0
I1109 15:30:59.956759 18124 net.cpp:122] Setting up scale1_0
I1109 15:30:59.956759 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.956759 18124 net.cpp:137] Memory required for data: 99534000
I1109 15:30:59.956759 18124 layer_factory.cpp:58] Creating layer relu1_0
I1109 15:30:59.956759 18124 net.cpp:84] Creating Layer relu1_0
I1109 15:30:59.956759 18124 net.cpp:406] relu1_0 <- conv1_0
I1109 15:30:59.956759 18124 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1109 15:30:59.956759 18124 net.cpp:122] Setting up relu1_0
I1109 15:30:59.956759 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.956759 18124 net.cpp:137] Memory required for data: 115918000
I1109 15:30:59.956759 18124 layer_factory.cpp:58] Creating layer conv2
I1109 15:30:59.956759 18124 net.cpp:84] Creating Layer conv2
I1109 15:30:59.956759 18124 net.cpp:406] conv2 <- conv1_0
I1109 15:30:59.956759 18124 net.cpp:380] conv2 -> conv2
I1109 15:30:59.958760 18124 net.cpp:122] Setting up conv2
I1109 15:30:59.958760 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.958760 18124 net.cpp:137] Memory required for data: 132302000
I1109 15:30:59.958760 18124 layer_factory.cpp:58] Creating layer bn2
I1109 15:30:59.958760 18124 net.cpp:84] Creating Layer bn2
I1109 15:30:59.958760 18124 net.cpp:406] bn2 <- conv2
I1109 15:30:59.958760 18124 net.cpp:367] bn2 -> conv2 (in-place)
I1109 15:30:59.958760 18124 net.cpp:122] Setting up bn2
I1109 15:30:59.958760 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.958760 18124 net.cpp:137] Memory required for data: 148686000
I1109 15:30:59.958760 18124 layer_factory.cpp:58] Creating layer scale2
I1109 15:30:59.958760 18124 net.cpp:84] Creating Layer scale2
I1109 15:30:59.958760 18124 net.cpp:406] scale2 <- conv2
I1109 15:30:59.958760 18124 net.cpp:367] scale2 -> conv2 (in-place)
I1109 15:30:59.958760 18124 layer_factory.cpp:58] Creating layer scale2
I1109 15:30:59.959761 18124 net.cpp:122] Setting up scale2
I1109 15:30:59.959761 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.959761 18124 net.cpp:137] Memory required for data: 165070000
I1109 15:30:59.959761 18124 layer_factory.cpp:58] Creating layer relu2
I1109 15:30:59.959761 18124 net.cpp:84] Creating Layer relu2
I1109 15:30:59.959761 18124 net.cpp:406] relu2 <- conv2
I1109 15:30:59.959761 18124 net.cpp:367] relu2 -> conv2 (in-place)
I1109 15:30:59.959761 18124 net.cpp:122] Setting up relu2
I1109 15:30:59.959761 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.959761 18124 net.cpp:137] Memory required for data: 181454000
I1109 15:30:59.959761 18124 layer_factory.cpp:58] Creating layer conv2_1
I1109 15:30:59.959761 18124 net.cpp:84] Creating Layer conv2_1
I1109 15:30:59.959761 18124 net.cpp:406] conv2_1 <- conv2
I1109 15:30:59.959761 18124 net.cpp:380] conv2_1 -> conv2_1
I1109 15:30:59.960759 18124 net.cpp:122] Setting up conv2_1
I1109 15:30:59.960759 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.960759 18124 net.cpp:137] Memory required for data: 197838000
I1109 15:30:59.960759 18124 layer_factory.cpp:58] Creating layer bn2_1
I1109 15:30:59.960759 18124 net.cpp:84] Creating Layer bn2_1
I1109 15:30:59.960759 18124 net.cpp:406] bn2_1 <- conv2_1
I1109 15:30:59.960759 18124 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1109 15:30:59.961760 18124 net.cpp:122] Setting up bn2_1
I1109 15:30:59.961760 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.961760 18124 net.cpp:137] Memory required for data: 214222000
I1109 15:30:59.961760 18124 layer_factory.cpp:58] Creating layer scale2_1
I1109 15:30:59.961760 18124 net.cpp:84] Creating Layer scale2_1
I1109 15:30:59.961760 18124 net.cpp:406] scale2_1 <- conv2_1
I1109 15:30:59.961760 18124 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1109 15:30:59.961760 18124 layer_factory.cpp:58] Creating layer scale2_1
I1109 15:30:59.961760 18124 net.cpp:122] Setting up scale2_1
I1109 15:30:59.961760 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.961760 18124 net.cpp:137] Memory required for data: 230606000
I1109 15:30:59.961760 18124 layer_factory.cpp:58] Creating layer relu2_1
I1109 15:30:59.961760 18124 net.cpp:84] Creating Layer relu2_1
I1109 15:30:59.961760 18124 net.cpp:406] relu2_1 <- conv2_1
I1109 15:30:59.961760 18124 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1109 15:30:59.961760 18124 net.cpp:122] Setting up relu2_1
I1109 15:30:59.961760 18124 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1109 15:30:59.961760 18124 net.cpp:137] Memory required for data: 246990000
I1109 15:30:59.961760 18124 layer_factory.cpp:58] Creating layer conv2_2
I1109 15:30:59.961760 18124 net.cpp:84] Creating Layer conv2_2
I1109 15:30:59.961760 18124 net.cpp:406] conv2_2 <- conv2_1
I1109 15:30:59.961760 18124 net.cpp:380] conv2_2 -> conv2_2
I1109 15:30:59.963759 18124 net.cpp:122] Setting up conv2_2
I1109 15:30:59.963759 18124 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1109 15:30:59.963759 18124 net.cpp:137] Memory required for data: 267470000
I1109 15:30:59.963759 18124 layer_factory.cpp:58] Creating layer bn2_2
I1109 15:30:59.963759 18124 net.cpp:84] Creating Layer bn2_2
I1109 15:30:59.963759 18124 net.cpp:406] bn2_2 <- conv2_2
I1109 15:30:59.963759 18124 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1109 15:30:59.963759 18124 net.cpp:122] Setting up bn2_2
I1109 15:30:59.963759 18124 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1109 15:30:59.963759 18124 net.cpp:137] Memory required for data: 287950000
I1109 15:30:59.963759 18124 layer_factory.cpp:58] Creating layer scale2_2
I1109 15:30:59.963759 18124 net.cpp:84] Creating Layer scale2_2
I1109 15:30:59.963759 18124 net.cpp:406] scale2_2 <- conv2_2
I1109 15:30:59.963759 18124 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1109 15:30:59.963759 18124 layer_factory.cpp:58] Creating layer scale2_2
I1109 15:30:59.963759 18124 net.cpp:122] Setting up scale2_2
I1109 15:30:59.963759 18124 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1109 15:30:59.963759 18124 net.cpp:137] Memory required for data: 308430000
I1109 15:30:59.963759 18124 layer_factory.cpp:58] Creating layer relu2_2
I1109 15:30:59.963759 18124 net.cpp:84] Creating Layer relu2_2
I1109 15:30:59.963759 18124 net.cpp:406] relu2_2 <- conv2_2
I1109 15:30:59.963759 18124 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1109 15:30:59.964761 18124 net.cpp:122] Setting up relu2_2
I1109 15:30:59.964761 18124 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1109 15:30:59.964761 18124 net.cpp:137] Memory required for data: 328910000
I1109 15:30:59.964761 18124 layer_factory.cpp:58] Creating layer pool2_1
I1109 15:30:59.964761 18124 net.cpp:84] Creating Layer pool2_1
I1109 15:30:59.964761 18124 net.cpp:406] pool2_1 <- conv2_2
I1109 15:30:59.964761 18124 net.cpp:380] pool2_1 -> pool2_1
I1109 15:30:59.964761 18124 net.cpp:122] Setting up pool2_1
I1109 15:30:59.964761 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.964761 18124 net.cpp:137] Memory required for data: 334030000
I1109 15:30:59.964761 18124 layer_factory.cpp:58] Creating layer drop5
I1109 15:30:59.964761 18124 net.cpp:84] Creating Layer drop5
I1109 15:30:59.964761 18124 net.cpp:406] drop5 <- pool2_1
I1109 15:30:59.964761 18124 net.cpp:367] drop5 -> pool2_1 (in-place)
I1109 15:30:59.964761 18124 net.cpp:122] Setting up drop5
I1109 15:30:59.964761 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.964761 18124 net.cpp:137] Memory required for data: 339150000
I1109 15:30:59.964761 18124 layer_factory.cpp:58] Creating layer conv3
I1109 15:30:59.964761 18124 net.cpp:84] Creating Layer conv3
I1109 15:30:59.964761 18124 net.cpp:406] conv3 <- pool2_1
I1109 15:30:59.964761 18124 net.cpp:380] conv3 -> conv3
I1109 15:30:59.965760 18124 net.cpp:122] Setting up conv3
I1109 15:30:59.965760 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.965760 18124 net.cpp:137] Memory required for data: 344270000
I1109 15:30:59.965760 18124 layer_factory.cpp:58] Creating layer bn3
I1109 15:30:59.965760 18124 net.cpp:84] Creating Layer bn3
I1109 15:30:59.965760 18124 net.cpp:406] bn3 <- conv3
I1109 15:30:59.965760 18124 net.cpp:367] bn3 -> conv3 (in-place)
I1109 15:30:59.965760 18124 net.cpp:122] Setting up bn3
I1109 15:30:59.965760 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.965760 18124 net.cpp:137] Memory required for data: 349390000
I1109 15:30:59.965760 18124 layer_factory.cpp:58] Creating layer scale3
I1109 15:30:59.965760 18124 net.cpp:84] Creating Layer scale3
I1109 15:30:59.965760 18124 net.cpp:406] scale3 <- conv3
I1109 15:30:59.965760 18124 net.cpp:367] scale3 -> conv3 (in-place)
I1109 15:30:59.965760 18124 layer_factory.cpp:58] Creating layer scale3
I1109 15:30:59.965760 18124 net.cpp:122] Setting up scale3
I1109 15:30:59.965760 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.965760 18124 net.cpp:137] Memory required for data: 354510000
I1109 15:30:59.965760 18124 layer_factory.cpp:58] Creating layer relu3
I1109 15:30:59.965760 18124 net.cpp:84] Creating Layer relu3
I1109 15:30:59.965760 18124 net.cpp:406] relu3 <- conv3
I1109 15:30:59.965760 18124 net.cpp:367] relu3 -> conv3 (in-place)
I1109 15:30:59.966759 18124 net.cpp:122] Setting up relu3
I1109 15:30:59.966759 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.966759 18124 net.cpp:137] Memory required for data: 359630000
I1109 15:30:59.966759 18124 layer_factory.cpp:58] Creating layer conv3_1
I1109 15:30:59.966759 18124 net.cpp:84] Creating Layer conv3_1
I1109 15:30:59.966759 18124 net.cpp:406] conv3_1 <- conv3
I1109 15:30:59.966759 18124 net.cpp:380] conv3_1 -> conv3_1
I1109 15:30:59.968760 18124 net.cpp:122] Setting up conv3_1
I1109 15:30:59.968760 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.968760 18124 net.cpp:137] Memory required for data: 364750000
I1109 15:30:59.968760 18124 layer_factory.cpp:58] Creating layer bn3_1
I1109 15:30:59.968760 18124 net.cpp:84] Creating Layer bn3_1
I1109 15:30:59.968760 18124 net.cpp:406] bn3_1 <- conv3_1
I1109 15:30:59.968760 18124 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1109 15:30:59.968760 18124 net.cpp:122] Setting up bn3_1
I1109 15:30:59.968760 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.968760 18124 net.cpp:137] Memory required for data: 369870000
I1109 15:30:59.968760 18124 layer_factory.cpp:58] Creating layer scale3_1
I1109 15:30:59.968760 18124 net.cpp:84] Creating Layer scale3_1
I1109 15:30:59.968760 18124 net.cpp:406] scale3_1 <- conv3_1
I1109 15:30:59.968760 18124 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1109 15:30:59.968760 18124 layer_factory.cpp:58] Creating layer scale3_1
I1109 15:30:59.968760 18124 net.cpp:122] Setting up scale3_1
I1109 15:30:59.968760 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.968760 18124 net.cpp:137] Memory required for data: 374990000
I1109 15:30:59.968760 18124 layer_factory.cpp:58] Creating layer relu3_1
I1109 15:30:59.968760 18124 net.cpp:84] Creating Layer relu3_1
I1109 15:30:59.968760 18124 net.cpp:406] relu3_1 <- conv3_1
I1109 15:30:59.968760 18124 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1109 15:30:59.968760 18124 net.cpp:122] Setting up relu3_1
I1109 15:30:59.968760 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.968760 18124 net.cpp:137] Memory required for data: 380110000
I1109 15:30:59.968760 18124 layer_factory.cpp:58] Creating layer conv4
I1109 15:30:59.968760 18124 net.cpp:84] Creating Layer conv4
I1109 15:30:59.968760 18124 net.cpp:406] conv4 <- conv3_1
I1109 15:30:59.968760 18124 net.cpp:380] conv4 -> conv4
I1109 15:30:59.970759 18124 net.cpp:122] Setting up conv4
I1109 15:30:59.970759 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.970759 18124 net.cpp:137] Memory required for data: 385230000
I1109 15:30:59.970759 18124 layer_factory.cpp:58] Creating layer bn4
I1109 15:30:59.970759 18124 net.cpp:84] Creating Layer bn4
I1109 15:30:59.970759 18124 net.cpp:406] bn4 <- conv4
I1109 15:30:59.970759 18124 net.cpp:367] bn4 -> conv4 (in-place)
I1109 15:30:59.970759 18124 net.cpp:122] Setting up bn4
I1109 15:30:59.970759 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.970759 18124 net.cpp:137] Memory required for data: 390350000
I1109 15:30:59.970759 18124 layer_factory.cpp:58] Creating layer scale4
I1109 15:30:59.970759 18124 net.cpp:84] Creating Layer scale4
I1109 15:30:59.970759 18124 net.cpp:406] scale4 <- conv4
I1109 15:30:59.970759 18124 net.cpp:367] scale4 -> conv4 (in-place)
I1109 15:30:59.970759 18124 layer_factory.cpp:58] Creating layer scale4
I1109 15:30:59.971760 18124 net.cpp:122] Setting up scale4
I1109 15:30:59.971760 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.971760 18124 net.cpp:137] Memory required for data: 395470000
I1109 15:30:59.971760 18124 layer_factory.cpp:58] Creating layer relu4
I1109 15:30:59.971760 18124 net.cpp:84] Creating Layer relu4
I1109 15:30:59.971760 18124 net.cpp:406] relu4 <- conv4
I1109 15:30:59.971760 18124 net.cpp:367] relu4 -> conv4 (in-place)
I1109 15:30:59.971760 18124 net.cpp:122] Setting up relu4
I1109 15:30:59.971760 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.971760 18124 net.cpp:137] Memory required for data: 400590000
I1109 15:30:59.971760 18124 layer_factory.cpp:58] Creating layer conv4_1
I1109 15:30:59.971760 18124 net.cpp:84] Creating Layer conv4_1
I1109 15:30:59.971760 18124 net.cpp:406] conv4_1 <- conv4
I1109 15:30:59.971760 18124 net.cpp:380] conv4_1 -> conv4_1
I1109 15:30:59.972760 18124 net.cpp:122] Setting up conv4_1
I1109 15:30:59.972760 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.972760 18124 net.cpp:137] Memory required for data: 405710000
I1109 15:30:59.972760 18124 layer_factory.cpp:58] Creating layer bn4_1
I1109 15:30:59.972760 18124 net.cpp:84] Creating Layer bn4_1
I1109 15:30:59.972760 18124 net.cpp:406] bn4_1 <- conv4_1
I1109 15:30:59.972760 18124 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1109 15:30:59.972760 18124 net.cpp:122] Setting up bn4_1
I1109 15:30:59.972760 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.972760 18124 net.cpp:137] Memory required for data: 410830000
I1109 15:30:59.972760 18124 layer_factory.cpp:58] Creating layer scale4_1
I1109 15:30:59.972760 18124 net.cpp:84] Creating Layer scale4_1
I1109 15:30:59.972760 18124 net.cpp:406] scale4_1 <- conv4_1
I1109 15:30:59.972760 18124 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1109 15:30:59.972760 18124 layer_factory.cpp:58] Creating layer scale4_1
I1109 15:30:59.972760 18124 net.cpp:122] Setting up scale4_1
I1109 15:30:59.973759 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.973759 18124 net.cpp:137] Memory required for data: 415950000
I1109 15:30:59.973759 18124 layer_factory.cpp:58] Creating layer relu4_1
I1109 15:30:59.973759 18124 net.cpp:84] Creating Layer relu4_1
I1109 15:30:59.973759 18124 net.cpp:406] relu4_1 <- conv4_1
I1109 15:30:59.973759 18124 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1109 15:30:59.973759 18124 net.cpp:122] Setting up relu4_1
I1109 15:30:59.973759 18124 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1109 15:30:59.973759 18124 net.cpp:137] Memory required for data: 421070000
I1109 15:30:59.973759 18124 layer_factory.cpp:58] Creating layer conv4_2
I1109 15:30:59.973759 18124 net.cpp:84] Creating Layer conv4_2
I1109 15:30:59.973759 18124 net.cpp:406] conv4_2 <- conv4_1
I1109 15:30:59.973759 18124 net.cpp:380] conv4_2 -> conv4_2
I1109 15:30:59.974761 18124 net.cpp:122] Setting up conv4_2
I1109 15:30:59.974761 18124 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1109 15:30:59.974761 18124 net.cpp:137] Memory required for data: 427009200
I1109 15:30:59.974761 18124 layer_factory.cpp:58] Creating layer bn4_2
I1109 15:30:59.974761 18124 net.cpp:84] Creating Layer bn4_2
I1109 15:30:59.974761 18124 net.cpp:406] bn4_2 <- conv4_2
I1109 15:30:59.974761 18124 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1109 15:30:59.974761 18124 net.cpp:122] Setting up bn4_2
I1109 15:30:59.974761 18124 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1109 15:30:59.974761 18124 net.cpp:137] Memory required for data: 432948400
I1109 15:30:59.974761 18124 layer_factory.cpp:58] Creating layer scale4_2
I1109 15:30:59.974761 18124 net.cpp:84] Creating Layer scale4_2
I1109 15:30:59.974761 18124 net.cpp:406] scale4_2 <- conv4_2
I1109 15:30:59.974761 18124 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1109 15:30:59.975759 18124 layer_factory.cpp:58] Creating layer scale4_2
I1109 15:30:59.975759 18124 net.cpp:122] Setting up scale4_2
I1109 15:30:59.975759 18124 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1109 15:30:59.975759 18124 net.cpp:137] Memory required for data: 438887600
I1109 15:30:59.975759 18124 layer_factory.cpp:58] Creating layer relu4_2
I1109 15:30:59.975759 18124 net.cpp:84] Creating Layer relu4_2
I1109 15:30:59.975759 18124 net.cpp:406] relu4_2 <- conv4_2
I1109 15:30:59.975759 18124 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1109 15:30:59.975759 18124 net.cpp:122] Setting up relu4_2
I1109 15:30:59.975759 18124 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1109 15:30:59.975759 18124 net.cpp:137] Memory required for data: 444826800
I1109 15:30:59.975759 18124 layer_factory.cpp:58] Creating layer pool4_2
I1109 15:30:59.975759 18124 net.cpp:84] Creating Layer pool4_2
I1109 15:30:59.975759 18124 net.cpp:406] pool4_2 <- conv4_2
I1109 15:30:59.975759 18124 net.cpp:380] pool4_2 -> pool4_2
I1109 15:30:59.975759 18124 net.cpp:122] Setting up pool4_2
I1109 15:30:59.975759 18124 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1109 15:30:59.975759 18124 net.cpp:137] Memory required for data: 446311600
I1109 15:30:59.975759 18124 layer_factory.cpp:58] Creating layer drop9
I1109 15:30:59.975759 18124 net.cpp:84] Creating Layer drop9
I1109 15:30:59.975759 18124 net.cpp:406] drop9 <- pool4_2
I1109 15:30:59.975759 18124 net.cpp:367] drop9 -> pool4_2 (in-place)
I1109 15:30:59.975759 18124 net.cpp:122] Setting up drop9
I1109 15:30:59.975759 18124 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1109 15:30:59.975759 18124 net.cpp:137] Memory required for data: 447796400
I1109 15:30:59.976759 18124 layer_factory.cpp:58] Creating layer conv4_0
I1109 15:30:59.976759 18124 net.cpp:84] Creating Layer conv4_0
I1109 15:30:59.976759 18124 net.cpp:406] conv4_0 <- pool4_2
I1109 15:30:59.976759 18124 net.cpp:380] conv4_0 -> conv4_0
I1109 15:30:59.977761 18124 net.cpp:122] Setting up conv4_0
I1109 15:30:59.977761 18124 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1109 15:30:59.977761 18124 net.cpp:137] Memory required for data: 449281200
I1109 15:30:59.977761 18124 layer_factory.cpp:58] Creating layer bn4_0
I1109 15:30:59.977761 18124 net.cpp:84] Creating Layer bn4_0
I1109 15:30:59.977761 18124 net.cpp:406] bn4_0 <- conv4_0
I1109 15:30:59.977761 18124 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1109 15:30:59.977761 18124 net.cpp:122] Setting up bn4_0
I1109 15:30:59.977761 18124 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1109 15:30:59.977761 18124 net.cpp:137] Memory required for data: 450766000
I1109 15:30:59.977761 18124 layer_factory.cpp:58] Creating layer scale4_0
I1109 15:30:59.977761 18124 net.cpp:84] Creating Layer scale4_0
I1109 15:30:59.977761 18124 net.cpp:406] scale4_0 <- conv4_0
I1109 15:30:59.977761 18124 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1109 15:30:59.977761 18124 layer_factory.cpp:58] Creating layer scale4_0
I1109 15:30:59.977761 18124 net.cpp:122] Setting up scale4_0
I1109 15:30:59.977761 18124 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1109 15:30:59.977761 18124 net.cpp:137] Memory required for data: 452250800
I1109 15:30:59.977761 18124 layer_factory.cpp:58] Creating layer relu4_0
I1109 15:30:59.977761 18124 net.cpp:84] Creating Layer relu4_0
I1109 15:30:59.977761 18124 net.cpp:406] relu4_0 <- conv4_0
I1109 15:30:59.977761 18124 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1109 15:30:59.978761 18124 net.cpp:122] Setting up relu4_0
I1109 15:30:59.978761 18124 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1109 15:30:59.978761 18124 net.cpp:137] Memory required for data: 453735600
I1109 15:30:59.978761 18124 layer_factory.cpp:58] Creating layer conv11
I1109 15:30:59.978761 18124 net.cpp:84] Creating Layer conv11
I1109 15:30:59.978761 18124 net.cpp:406] conv11 <- conv4_0
I1109 15:30:59.978761 18124 net.cpp:380] conv11 -> conv11
I1109 15:30:59.979760 18124 net.cpp:122] Setting up conv11
I1109 15:30:59.979760 18124 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1109 15:30:59.979760 18124 net.cpp:137] Memory required for data: 455527600
I1109 15:30:59.979760 18124 layer_factory.cpp:58] Creating layer bn_conv11
I1109 15:30:59.979760 18124 net.cpp:84] Creating Layer bn_conv11
I1109 15:30:59.979760 18124 net.cpp:406] bn_conv11 <- conv11
I1109 15:30:59.979760 18124 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1109 15:30:59.979760 18124 net.cpp:122] Setting up bn_conv11
I1109 15:30:59.979760 18124 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1109 15:30:59.979760 18124 net.cpp:137] Memory required for data: 457319600
I1109 15:30:59.979760 18124 layer_factory.cpp:58] Creating layer scale_conv11
I1109 15:30:59.979760 18124 net.cpp:84] Creating Layer scale_conv11
I1109 15:30:59.980762 18124 net.cpp:406] scale_conv11 <- conv11
I1109 15:30:59.980762 18124 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1109 15:30:59.980762 18124 layer_factory.cpp:58] Creating layer scale_conv11
I1109 15:30:59.980762 18124 net.cpp:122] Setting up scale_conv11
I1109 15:30:59.980762 18124 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1109 15:30:59.980762 18124 net.cpp:137] Memory required for data: 459111600
I1109 15:30:59.980762 18124 layer_factory.cpp:58] Creating layer relu_conv11
I1109 15:30:59.980762 18124 net.cpp:84] Creating Layer relu_conv11
I1109 15:30:59.980762 18124 net.cpp:406] relu_conv11 <- conv11
I1109 15:30:59.980762 18124 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1109 15:30:59.980762 18124 net.cpp:122] Setting up relu_conv11
I1109 15:30:59.980762 18124 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1109 15:30:59.980762 18124 net.cpp:137] Memory required for data: 460903600
I1109 15:30:59.980762 18124 layer_factory.cpp:58] Creating layer conv12
I1109 15:30:59.980762 18124 net.cpp:84] Creating Layer conv12
I1109 15:30:59.980762 18124 net.cpp:406] conv12 <- conv11
I1109 15:30:59.980762 18124 net.cpp:380] conv12 -> conv12
I1109 15:30:59.982761 18124 net.cpp:122] Setting up conv12
I1109 15:30:59.982761 18124 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1109 15:30:59.982761 18124 net.cpp:137] Memory required for data: 463207600
I1109 15:30:59.982761 18124 layer_factory.cpp:58] Creating layer bn_conv12
I1109 15:30:59.982761 18124 net.cpp:84] Creating Layer bn_conv12
I1109 15:30:59.982761 18124 net.cpp:406] bn_conv12 <- conv12
I1109 15:30:59.982761 18124 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1109 15:30:59.982761 18124 net.cpp:122] Setting up bn_conv12
I1109 15:30:59.982761 18124 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1109 15:30:59.982761 18124 net.cpp:137] Memory required for data: 465511600
I1109 15:30:59.982761 18124 layer_factory.cpp:58] Creating layer scale_conv12
I1109 15:30:59.982761 18124 net.cpp:84] Creating Layer scale_conv12
I1109 15:30:59.982761 18124 net.cpp:406] scale_conv12 <- conv12
I1109 15:30:59.982761 18124 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1109 15:30:59.982761 18124 layer_factory.cpp:58] Creating layer scale_conv12
I1109 15:30:59.982761 18124 net.cpp:122] Setting up scale_conv12
I1109 15:30:59.982761 18124 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1109 15:30:59.982761 18124 net.cpp:137] Memory required for data: 467815600
I1109 15:30:59.982761 18124 layer_factory.cpp:58] Creating layer relu_conv12
I1109 15:30:59.982761 18124 net.cpp:84] Creating Layer relu_conv12
I1109 15:30:59.982761 18124 net.cpp:406] relu_conv12 <- conv12
I1109 15:30:59.982761 18124 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1109 15:30:59.983762 18124 net.cpp:122] Setting up relu_conv12
I1109 15:30:59.983762 18124 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1109 15:30:59.983762 18124 net.cpp:137] Memory required for data: 470119600
I1109 15:30:59.983762 18124 layer_factory.cpp:58] Creating layer poolcp6
I1109 15:30:59.983762 18124 net.cpp:84] Creating Layer poolcp6
I1109 15:30:59.983762 18124 net.cpp:406] poolcp6 <- conv12
I1109 15:30:59.983762 18124 net.cpp:380] poolcp6 -> poolcp6
I1109 15:30:59.983762 18124 net.cpp:122] Setting up poolcp6
I1109 15:30:59.983762 18124 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1109 15:30:59.983762 18124 net.cpp:137] Memory required for data: 470155600
I1109 15:30:59.983762 18124 layer_factory.cpp:58] Creating layer drop12
I1109 15:30:59.983762 18124 net.cpp:84] Creating Layer drop12
I1109 15:30:59.983762 18124 net.cpp:406] drop12 <- poolcp6
I1109 15:30:59.983762 18124 net.cpp:367] drop12 -> poolcp6 (in-place)
I1109 15:30:59.983762 18124 net.cpp:122] Setting up drop12
I1109 15:30:59.983762 18124 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1109 15:30:59.983762 18124 net.cpp:137] Memory required for data: 470191600
I1109 15:30:59.983762 18124 layer_factory.cpp:58] Creating layer ip1
I1109 15:30:59.983762 18124 net.cpp:84] Creating Layer ip1
I1109 15:30:59.983762 18124 net.cpp:406] ip1 <- poolcp6
I1109 15:30:59.983762 18124 net.cpp:380] ip1 -> ip1
I1109 15:30:59.983762 18124 net.cpp:122] Setting up ip1
I1109 15:30:59.983762 18124 net.cpp:129] Top shape: 100 100 (10000)
I1109 15:30:59.983762 18124 net.cpp:137] Memory required for data: 470231600
I1109 15:30:59.983762 18124 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1109 15:30:59.983762 18124 net.cpp:84] Creating Layer ip1_ip1_0_split
I1109 15:30:59.983762 18124 net.cpp:406] ip1_ip1_0_split <- ip1
I1109 15:30:59.983762 18124 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1109 15:30:59.983762 18124 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1109 15:30:59.983762 18124 net.cpp:122] Setting up ip1_ip1_0_split
I1109 15:30:59.983762 18124 net.cpp:129] Top shape: 100 100 (10000)
I1109 15:30:59.983762 18124 net.cpp:129] Top shape: 100 100 (10000)
I1109 15:30:59.983762 18124 net.cpp:137] Memory required for data: 470311600
I1109 15:30:59.983762 18124 layer_factory.cpp:58] Creating layer accuracy
I1109 15:30:59.983762 18124 net.cpp:84] Creating Layer accuracy
I1109 15:30:59.983762 18124 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1109 15:30:59.983762 18124 net.cpp:406] accuracy <- label_cifar_1_split_0
I1109 15:30:59.983762 18124 net.cpp:380] accuracy -> accuracy
I1109 15:30:59.983762 18124 net.cpp:122] Setting up accuracy
I1109 15:30:59.983762 18124 net.cpp:129] Top shape: (1)
I1109 15:30:59.983762 18124 net.cpp:137] Memory required for data: 470311604
I1109 15:30:59.983762 18124 layer_factory.cpp:58] Creating layer loss
I1109 15:30:59.983762 18124 net.cpp:84] Creating Layer loss
I1109 15:30:59.983762 18124 net.cpp:406] loss <- ip1_ip1_0_split_1
I1109 15:30:59.983762 18124 net.cpp:406] loss <- label_cifar_1_split_1
I1109 15:30:59.983762 18124 net.cpp:380] loss -> loss
I1109 15:30:59.983762 18124 layer_factory.cpp:58] Creating layer loss
I1109 15:30:59.984761 18124 net.cpp:122] Setting up loss
I1109 15:30:59.984761 18124 net.cpp:129] Top shape: (1)
I1109 15:30:59.984761 18124 net.cpp:132]     with loss weight 1
I1109 15:30:59.984761 18124 net.cpp:137] Memory required for data: 470311608
I1109 15:30:59.984761 18124 net.cpp:198] loss needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:200] accuracy does not need backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] ip1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] drop12 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] poolcp6 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] relu_conv12 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] scale_conv12 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] bn_conv12 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] conv12 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] relu_conv11 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] scale_conv11 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] bn_conv11 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] conv11 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] relu4_0 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] scale4_0 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] bn4_0 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] conv4_0 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] drop9 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] pool4_2 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] relu4_2 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] scale4_2 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] bn4_2 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] conv4_2 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] relu4_1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] scale4_1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] bn4_1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] conv4_1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] relu4 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] scale4 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] bn4 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] conv4 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] relu3_1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] scale3_1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] bn3_1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] conv3_1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] relu3 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] scale3 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] bn3 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] conv3 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] drop5 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] pool2_1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] relu2_2 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] scale2_2 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] bn2_2 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] conv2_2 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] relu2_1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] scale2_1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] bn2_1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] conv2_1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] relu2 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] scale2 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] bn2 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] conv2 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] relu1_0 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] scale1_0 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] bn1_0 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] conv1_0 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] relu1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] scale1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] bn1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:198] conv1 needs backward computation.
I1109 15:30:59.984761 18124 net.cpp:200] label_cifar_1_split does not need backward computation.
I1109 15:30:59.984761 18124 net.cpp:200] cifar does not need backward computation.
I1109 15:30:59.984761 18124 net.cpp:242] This network produces output accuracy
I1109 15:30:59.984761 18124 net.cpp:242] This network produces output loss
I1109 15:30:59.984761 18124 net.cpp:255] Network initialization done.
I1109 15:30:59.984761 18124 solver.cpp:56] Solver scaffolding done.
I1109 15:30:59.988760 18124 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_90000.solverstate
I1109 15:30:59.991761 18124 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_90000.caffemodel
I1109 15:30:59.991761 18124 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1109 15:30:59.991761 18124 sgd_solver.cpp:318] SGDSolver: restoring history
I1109 15:30:59.995760 18124 caffe.cpp:249] Starting Optimization
I1109 15:30:59.995760 18124 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_13L_Simple_NoGrpCon_NoDrp_maxdrp_300k_3x3s2_overlappool
I1109 15:30:59.995760 18124 solver.cpp:273] Learning Rate Policy: multistep
I1109 15:30:59.997771 18124 solver.cpp:330] Iteration 90000, Testing net (#0)
I1109 15:30:59.999760 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:31:01.319862 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:31:01.369871 18124 solver.cpp:397]     Test net output #0: accuracy = 0.5807
I1109 15:31:01.369871 18124 solver.cpp:397]     Test net output #1: loss = 1.59987 (* 1 = 1.59987 loss)
I1109 15:31:01.477880 18124 solver.cpp:218] Iteration 90000 (-nan iter/s, 1.4803s/100 iters), loss = 1.07528
I1109 15:31:01.477880 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:31:01.477880 18124 solver.cpp:237]     Train net output #1: loss = 1.07528 (* 1 = 1.07528 loss)
I1109 15:31:01.477880 18124 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1109 15:31:06.799641 18124 solver.cpp:218] Iteration 90100 (18.7896 iter/s, 5.32209s/100 iters), loss = 1.14864
I1109 15:31:06.799641 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1109 15:31:06.799641 18124 solver.cpp:237]     Train net output #1: loss = 1.14864 (* 1 = 1.14864 loss)
I1109 15:31:06.799641 18124 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1109 15:31:12.107817 18124 solver.cpp:218] Iteration 90200 (18.8395 iter/s, 5.30799s/100 iters), loss = 0.941264
I1109 15:31:12.108817 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:31:12.108817 18124 solver.cpp:237]     Train net output #1: loss = 0.941264 (* 1 = 0.941264 loss)
I1109 15:31:12.108817 18124 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1109 15:31:17.438200 18124 solver.cpp:218] Iteration 90300 (18.762 iter/s, 5.32994s/100 iters), loss = 1.30705
I1109 15:31:17.439200 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1109 15:31:17.439200 18124 solver.cpp:237]     Train net output #1: loss = 1.30705 (* 1 = 1.30705 loss)
I1109 15:31:17.439200 18124 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1109 15:31:22.760272 18124 solver.cpp:218] Iteration 90400 (18.7941 iter/s, 5.32082s/100 iters), loss = 1.45856
I1109 15:31:22.760272 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.58
I1109 15:31:22.760272 18124 solver.cpp:237]     Train net output #1: loss = 1.45856 (* 1 = 1.45856 loss)
I1109 15:31:22.760272 18124 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1109 15:31:27.826526  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:31:28.040060 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_90500.caffemodel
I1109 15:31:28.058046 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_90500.solverstate
I1109 15:31:28.063575 18124 solver.cpp:330] Iteration 90500, Testing net (#0)
I1109 15:31:28.064069 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:31:29.364368 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:31:29.415372 18124 solver.cpp:397]     Test net output #0: accuracy = 0.5728
I1109 15:31:29.415372 18124 solver.cpp:397]     Test net output #1: loss = 1.62105 (* 1 = 1.62105 loss)
I1109 15:31:29.469374 18124 solver.cpp:218] Iteration 90500 (14.9067 iter/s, 6.70841s/100 iters), loss = 1.36758
I1109 15:31:29.469374 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:31:29.469374 18124 solver.cpp:237]     Train net output #1: loss = 1.36758 (* 1 = 1.36758 loss)
I1109 15:31:29.469374 18124 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1109 15:31:34.877132 18124 solver.cpp:218] Iteration 90600 (18.4933 iter/s, 5.40736s/100 iters), loss = 1.20898
I1109 15:31:34.877132 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.57
I1109 15:31:34.877132 18124 solver.cpp:237]     Train net output #1: loss = 1.20898 (* 1 = 1.20898 loss)
I1109 15:31:34.877132 18124 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1109 15:31:40.246347 18124 solver.cpp:218] Iteration 90700 (18.6247 iter/s, 5.36921s/100 iters), loss = 0.966186
I1109 15:31:40.246347 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:31:40.246347 18124 solver.cpp:237]     Train net output #1: loss = 0.966186 (* 1 = 0.966186 loss)
I1109 15:31:40.246347 18124 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1109 15:31:45.571313 18124 solver.cpp:218] Iteration 90800 (18.7835 iter/s, 5.32383s/100 iters), loss = 1.33962
I1109 15:31:45.571313 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.59
I1109 15:31:45.571313 18124 solver.cpp:237]     Train net output #1: loss = 1.33962 (* 1 = 1.33962 loss)
I1109 15:31:45.571313 18124 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1109 15:31:50.938788 18124 solver.cpp:218] Iteration 90900 (18.6325 iter/s, 5.36697s/100 iters), loss = 1.33208
I1109 15:31:50.938788 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.61
I1109 15:31:50.938788 18124 solver.cpp:237]     Train net output #1: loss = 1.33208 (* 1 = 1.33208 loss)
I1109 15:31:50.938788 18124 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1109 15:31:56.031808  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:31:56.243829 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_91000.caffemodel
I1109 15:31:56.258829 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_91000.solverstate
I1109 15:31:56.263829 18124 solver.cpp:330] Iteration 91000, Testing net (#0)
I1109 15:31:56.263829 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:31:57.557934 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:31:57.607950 18124 solver.cpp:397]     Test net output #0: accuracy = 0.596
I1109 15:31:57.607950 18124 solver.cpp:397]     Test net output #1: loss = 1.45507 (* 1 = 1.45507 loss)
I1109 15:31:57.659940 18124 solver.cpp:218] Iteration 91000 (14.8798 iter/s, 6.72054s/100 iters), loss = 1.13837
I1109 15:31:57.659940 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1109 15:31:57.659940 18124 solver.cpp:237]     Train net output #1: loss = 1.13837 (* 1 = 1.13837 loss)
I1109 15:31:57.659940 18124 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1109 15:32:02.980376 18124 solver.cpp:218] Iteration 91100 (18.7939 iter/s, 5.32087s/100 iters), loss = 1.03839
I1109 15:32:02.980376 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1109 15:32:02.980376 18124 solver.cpp:237]     Train net output #1: loss = 1.03839 (* 1 = 1.03839 loss)
I1109 15:32:02.980376 18124 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1109 15:32:08.393934 18124 solver.cpp:218] Iteration 91200 (18.4764 iter/s, 5.41231s/100 iters), loss = 1.0437
I1109 15:32:08.393934 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1109 15:32:08.393934 18124 solver.cpp:237]     Train net output #1: loss = 1.0437 (* 1 = 1.0437 loss)
I1109 15:32:08.393934 18124 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1109 15:32:13.833420 18124 solver.cpp:218] Iteration 91300 (18.3828 iter/s, 5.43985s/100 iters), loss = 1.29773
I1109 15:32:13.833420 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1109 15:32:13.833420 18124 solver.cpp:237]     Train net output #1: loss = 1.29773 (* 1 = 1.29773 loss)
I1109 15:32:13.833420 18124 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1109 15:32:19.312013 18124 solver.cpp:218] Iteration 91400 (18.2549 iter/s, 5.47798s/100 iters), loss = 1.34427
I1109 15:32:19.312013 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.61
I1109 15:32:19.312013 18124 solver.cpp:237]     Train net output #1: loss = 1.34427 (* 1 = 1.34427 loss)
I1109 15:32:19.312013 18124 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1109 15:32:24.514709  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:32:24.733736 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_91500.caffemodel
I1109 15:32:24.751744 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_91500.solverstate
I1109 15:32:24.755744 18124 solver.cpp:330] Iteration 91500, Testing net (#0)
I1109 15:32:24.755744 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:32:26.062852 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:32:26.114864 18124 solver.cpp:397]     Test net output #0: accuracy = 0.5765
I1109 15:32:26.114864 18124 solver.cpp:397]     Test net output #1: loss = 1.60656 (* 1 = 1.60656 loss)
I1109 15:32:26.166867 18124 solver.cpp:218] Iteration 91500 (14.5907 iter/s, 6.8537s/100 iters), loss = 1.11738
I1109 15:32:26.166867 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1109 15:32:26.166867 18124 solver.cpp:237]     Train net output #1: loss = 1.11738 (* 1 = 1.11738 loss)
I1109 15:32:26.166867 18124 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1109 15:32:31.535493 18124 solver.cpp:218] Iteration 91600 (18.6269 iter/s, 5.36858s/100 iters), loss = 0.966868
I1109 15:32:31.535493 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:32:31.535493 18124 solver.cpp:237]     Train net output #1: loss = 0.966868 (* 1 = 0.966868 loss)
I1109 15:32:31.535493 18124 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1109 15:32:36.990075 18124 solver.cpp:218] Iteration 91700 (18.3332 iter/s, 5.45459s/100 iters), loss = 0.998585
I1109 15:32:36.991076 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:32:36.991076 18124 solver.cpp:237]     Train net output #1: loss = 0.998585 (* 1 = 0.998585 loss)
I1109 15:32:36.991076 18124 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1109 15:32:42.456540 18124 solver.cpp:218] Iteration 91800 (18.2973 iter/s, 5.46528s/100 iters), loss = 1.2173
I1109 15:32:42.456540 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1109 15:32:42.456540 18124 solver.cpp:237]     Train net output #1: loss = 1.2173 (* 1 = 1.2173 loss)
I1109 15:32:42.456540 18124 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1109 15:32:47.812450 18124 solver.cpp:218] Iteration 91900 (18.6712 iter/s, 5.35583s/100 iters), loss = 1.35297
I1109 15:32:47.812450 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.59
I1109 15:32:47.812450 18124 solver.cpp:237]     Train net output #1: loss = 1.35297 (* 1 = 1.35297 loss)
I1109 15:32:47.812450 18124 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1109 15:32:52.893980  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:32:53.108623 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_92000.caffemodel
I1109 15:32:53.122620 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_92000.solverstate
I1109 15:32:53.127624 18124 solver.cpp:330] Iteration 92000, Testing net (#0)
I1109 15:32:53.127624 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:32:54.424454 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:32:54.474993 18124 solver.cpp:397]     Test net output #0: accuracy = 0.5808
I1109 15:32:54.474993 18124 solver.cpp:397]     Test net output #1: loss = 1.5686 (* 1 = 1.5686 loss)
I1109 15:32:54.526145 18124 solver.cpp:218] Iteration 92000 (14.8973 iter/s, 6.71263s/100 iters), loss = 1.15949
I1109 15:32:54.526145 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.62
I1109 15:32:54.526145 18124 solver.cpp:237]     Train net output #1: loss = 1.15949 (* 1 = 1.15949 loss)
I1109 15:32:54.526145 18124 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1109 15:32:59.844676 18124 solver.cpp:218] Iteration 92100 (18.8043 iter/s, 5.31792s/100 iters), loss = 1.16571
I1109 15:32:59.844676 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1109 15:32:59.844676 18124 solver.cpp:237]     Train net output #1: loss = 1.16571 (* 1 = 1.16571 loss)
I1109 15:32:59.844676 18124 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1109 15:33:05.160156 18124 solver.cpp:218] Iteration 92200 (18.8131 iter/s, 5.31545s/100 iters), loss = 0.973029
I1109 15:33:05.160156 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:33:05.160156 18124 solver.cpp:237]     Train net output #1: loss = 0.973029 (* 1 = 0.973029 loss)
I1109 15:33:05.160156 18124 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1109 15:33:10.514130 18124 solver.cpp:218] Iteration 92300 (18.6794 iter/s, 5.35349s/100 iters), loss = 1.23264
I1109 15:33:10.514130 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1109 15:33:10.514130 18124 solver.cpp:237]     Train net output #1: loss = 1.23264 (* 1 = 1.23264 loss)
I1109 15:33:10.514130 18124 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1109 15:33:15.855173 18124 solver.cpp:218] Iteration 92400 (18.7238 iter/s, 5.34081s/100 iters), loss = 1.31995
I1109 15:33:15.855173 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.54
I1109 15:33:15.855173 18124 solver.cpp:237]     Train net output #1: loss = 1.31995 (* 1 = 1.31995 loss)
I1109 15:33:15.855173 18124 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1109 15:33:20.916487  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:33:21.124501 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_92500.caffemodel
I1109 15:33:21.137501 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_92500.solverstate
I1109 15:33:21.141501 18124 solver.cpp:330] Iteration 92500, Testing net (#0)
I1109 15:33:21.142503 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:33:22.415604 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:33:22.465608 18124 solver.cpp:397]     Test net output #0: accuracy = 0.588
I1109 15:33:22.465608 18124 solver.cpp:397]     Test net output #1: loss = 1.51927 (* 1 = 1.51927 loss)
I1109 15:33:22.516608 18124 solver.cpp:218] Iteration 92500 (15.0143 iter/s, 6.66032s/100 iters), loss = 1.0285
I1109 15:33:22.516608 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:33:22.516608 18124 solver.cpp:237]     Train net output #1: loss = 1.0285 (* 1 = 1.0285 loss)
I1109 15:33:22.516608 18124 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1109 15:33:27.833982 18124 solver.cpp:218] Iteration 92600 (18.8054 iter/s, 5.31761s/100 iters), loss = 1.20601
I1109 15:33:27.833982 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.62
I1109 15:33:27.833982 18124 solver.cpp:237]     Train net output #1: loss = 1.20601 (* 1 = 1.20601 loss)
I1109 15:33:27.833982 18124 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1109 15:33:33.148417 18124 solver.cpp:218] Iteration 92700 (18.818 iter/s, 5.31405s/100 iters), loss = 1.12228
I1109 15:33:33.148417 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1109 15:33:33.148417 18124 solver.cpp:237]     Train net output #1: loss = 1.12228 (* 1 = 1.12228 loss)
I1109 15:33:33.148417 18124 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1109 15:33:38.472789 18124 solver.cpp:218] Iteration 92800 (18.7845 iter/s, 5.32353s/100 iters), loss = 1.2873
I1109 15:33:38.472789 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.6
I1109 15:33:38.472789 18124 solver.cpp:237]     Train net output #1: loss = 1.2873 (* 1 = 1.2873 loss)
I1109 15:33:38.472789 18124 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1109 15:33:43.791105 18124 solver.cpp:218] Iteration 92900 (18.8046 iter/s, 5.31785s/100 iters), loss = 1.21078
I1109 15:33:43.791105 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.59
I1109 15:33:43.791105 18124 solver.cpp:237]     Train net output #1: loss = 1.21078 (* 1 = 1.21078 loss)
I1109 15:33:43.791105 18124 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1109 15:33:48.846423  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:33:49.054433 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_93000.caffemodel
I1109 15:33:49.067432 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_93000.solverstate
I1109 15:33:49.072433 18124 solver.cpp:330] Iteration 93000, Testing net (#0)
I1109 15:33:49.072433 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:33:50.346573 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:33:50.395572 18124 solver.cpp:397]     Test net output #0: accuracy = 0.5543
I1109 15:33:50.395572 18124 solver.cpp:397]     Test net output #1: loss = 1.76295 (* 1 = 1.76295 loss)
I1109 15:33:50.447577 18124 solver.cpp:218] Iteration 93000 (15.0244 iter/s, 6.65582s/100 iters), loss = 1.06554
I1109 15:33:50.447577 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:33:50.447577 18124 solver.cpp:237]     Train net output #1: loss = 1.06554 (* 1 = 1.06554 loss)
I1109 15:33:50.447577 18124 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1109 15:33:55.762912 18124 solver.cpp:218] Iteration 93100 (18.8122 iter/s, 5.31571s/100 iters), loss = 1.20645
I1109 15:33:55.762912 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.6
I1109 15:33:55.762912 18124 solver.cpp:237]     Train net output #1: loss = 1.20645 (* 1 = 1.20645 loss)
I1109 15:33:55.762912 18124 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1109 15:34:01.082242 18124 solver.cpp:218] Iteration 93200 (18.8029 iter/s, 5.31833s/100 iters), loss = 1.04601
I1109 15:34:01.082242 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:34:01.082242 18124 solver.cpp:237]     Train net output #1: loss = 1.04601 (* 1 = 1.04601 loss)
I1109 15:34:01.082242 18124 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1109 15:34:06.407618 18124 solver.cpp:218] Iteration 93300 (18.7799 iter/s, 5.32483s/100 iters), loss = 1.32552
I1109 15:34:06.407618 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.61
I1109 15:34:06.407618 18124 solver.cpp:237]     Train net output #1: loss = 1.32552 (* 1 = 1.32552 loss)
I1109 15:34:06.407618 18124 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1109 15:34:11.734957 18124 solver.cpp:218] Iteration 93400 (18.7719 iter/s, 5.32712s/100 iters), loss = 1.29001
I1109 15:34:11.734957 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.62
I1109 15:34:11.734957 18124 solver.cpp:237]     Train net output #1: loss = 1.29001 (* 1 = 1.29001 loss)
I1109 15:34:11.734957 18124 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1109 15:34:16.794267  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:34:17.004284 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_93500.caffemodel
I1109 15:34:17.017292 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_93500.solverstate
I1109 15:34:17.021793 18124 solver.cpp:330] Iteration 93500, Testing net (#0)
I1109 15:34:17.021793 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:34:18.304419 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:34:18.355432 18124 solver.cpp:397]     Test net output #0: accuracy = 0.5938
I1109 15:34:18.355432 18124 solver.cpp:397]     Test net output #1: loss = 1.52795 (* 1 = 1.52795 loss)
I1109 15:34:18.407421 18124 solver.cpp:218] Iteration 93500 (14.9873 iter/s, 6.67234s/100 iters), loss = 0.921729
I1109 15:34:18.407421 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:34:18.407421 18124 solver.cpp:237]     Train net output #1: loss = 0.921729 (* 1 = 0.921729 loss)
I1109 15:34:18.407421 18124 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1109 15:34:23.761870 18124 solver.cpp:218] Iteration 93600 (18.6765 iter/s, 5.35432s/100 iters), loss = 1.04556
I1109 15:34:23.761870 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:34:23.762871 18124 solver.cpp:237]     Train net output #1: loss = 1.04556 (* 1 = 1.04556 loss)
I1109 15:34:23.762871 18124 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1109 15:34:29.116730 18124 solver.cpp:218] Iteration 93700 (18.6782 iter/s, 5.35384s/100 iters), loss = 1.01326
I1109 15:34:29.116730 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:34:29.116730 18124 solver.cpp:237]     Train net output #1: loss = 1.01326 (* 1 = 1.01326 loss)
I1109 15:34:29.116730 18124 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1109 15:34:34.429616 18124 solver.cpp:218] Iteration 93800 (18.8222 iter/s, 5.31286s/100 iters), loss = 1.16554
I1109 15:34:34.429616 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:34:34.429616 18124 solver.cpp:237]     Train net output #1: loss = 1.16554 (* 1 = 1.16554 loss)
I1109 15:34:34.429616 18124 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1109 15:34:39.744992 18124 solver.cpp:218] Iteration 93900 (18.8152 iter/s, 5.31486s/100 iters), loss = 1.38355
I1109 15:34:39.744992 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.59
I1109 15:34:39.744992 18124 solver.cpp:237]     Train net output #1: loss = 1.38355 (* 1 = 1.38355 loss)
I1109 15:34:39.744992 18124 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1109 15:34:44.800305  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:34:45.008328 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_94000.caffemodel
I1109 15:34:45.022842 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_94000.solverstate
I1109 15:34:45.026849 18124 solver.cpp:330] Iteration 94000, Testing net (#0)
I1109 15:34:45.026849 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:34:46.301475 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:34:46.352483 18124 solver.cpp:397]     Test net output #0: accuracy = 0.5734
I1109 15:34:46.352483 18124 solver.cpp:397]     Test net output #1: loss = 1.61628 (* 1 = 1.61628 loss)
I1109 15:34:46.403496 18124 solver.cpp:218] Iteration 94000 (15.0196 iter/s, 6.65798s/100 iters), loss = 1.14903
I1109 15:34:46.403496 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1109 15:34:46.403496 18124 solver.cpp:237]     Train net output #1: loss = 1.14903 (* 1 = 1.14903 loss)
I1109 15:34:46.403496 18124 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1109 15:34:51.818471 18124 solver.cpp:218] Iteration 94100 (18.4705 iter/s, 5.41403s/100 iters), loss = 1.075
I1109 15:34:51.818471 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1109 15:34:51.818471 18124 solver.cpp:237]     Train net output #1: loss = 1.075 (* 1 = 1.075 loss)
I1109 15:34:51.818471 18124 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1109 15:34:57.271350 18124 solver.cpp:218] Iteration 94200 (18.3404 iter/s, 5.45243s/100 iters), loss = 1.07159
I1109 15:34:57.271350 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1109 15:34:57.271350 18124 solver.cpp:237]     Train net output #1: loss = 1.07159 (* 1 = 1.07159 loss)
I1109 15:34:57.271350 18124 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1109 15:35:02.637939 18124 solver.cpp:218] Iteration 94300 (18.6336 iter/s, 5.36665s/100 iters), loss = 1.4138
I1109 15:35:02.637939 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.58
I1109 15:35:02.637939 18124 solver.cpp:237]     Train net output #1: loss = 1.4138 (* 1 = 1.4138 loss)
I1109 15:35:02.637939 18124 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1109 15:35:07.979352 18124 solver.cpp:218] Iteration 94400 (18.7235 iter/s, 5.34088s/100 iters), loss = 1.33618
I1109 15:35:07.979352 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.6
I1109 15:35:07.979352 18124 solver.cpp:237]     Train net output #1: loss = 1.33618 (* 1 = 1.33618 loss)
I1109 15:35:07.979352 18124 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1109 15:35:13.111755  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:35:13.332789 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_94500.caffemodel
I1109 15:35:13.346792 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_94500.solverstate
I1109 15:35:13.350792 18124 solver.cpp:330] Iteration 94500, Testing net (#0)
I1109 15:35:13.350792 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:35:14.646785 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:35:14.699784 18124 solver.cpp:397]     Test net output #0: accuracy = 0.5872
I1109 15:35:14.699784 18124 solver.cpp:397]     Test net output #1: loss = 1.54818 (* 1 = 1.54818 loss)
I1109 15:35:14.755807 18124 solver.cpp:218] Iteration 94500 (14.7581 iter/s, 6.77593s/100 iters), loss = 1.08686
I1109 15:35:14.755807 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1109 15:35:14.755807 18124 solver.cpp:237]     Train net output #1: loss = 1.08686 (* 1 = 1.08686 loss)
I1109 15:35:14.755807 18124 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1109 15:35:20.184161 18124 solver.cpp:218] Iteration 94600 (18.4244 iter/s, 5.42757s/100 iters), loss = 1.00913
I1109 15:35:20.184161 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:35:20.184161 18124 solver.cpp:237]     Train net output #1: loss = 1.00913 (* 1 = 1.00913 loss)
I1109 15:35:20.184161 18124 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1109 15:35:25.531601 18124 solver.cpp:218] Iteration 94700 (18.7033 iter/s, 5.34664s/100 iters), loss = 0.992208
I1109 15:35:25.531601 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:35:25.531601 18124 solver.cpp:237]     Train net output #1: loss = 0.992208 (* 1 = 0.992208 loss)
I1109 15:35:25.531601 18124 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1109 15:35:30.870908 18124 solver.cpp:218] Iteration 94800 (18.7304 iter/s, 5.33892s/100 iters), loss = 1.14539
I1109 15:35:30.870908 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:35:30.870908 18124 solver.cpp:237]     Train net output #1: loss = 1.14539 (* 1 = 1.14539 loss)
I1109 15:35:30.870908 18124 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1109 15:35:36.205287 18124 solver.cpp:218] Iteration 94900 (18.7476 iter/s, 5.33401s/100 iters), loss = 1.42609
I1109 15:35:36.205287 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1109 15:35:36.205287 18124 solver.cpp:237]     Train net output #1: loss = 1.42609 (* 1 = 1.42609 loss)
I1109 15:35:36.205287 18124 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1109 15:35:41.266695  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:35:41.475710 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_95000.caffemodel
I1109 15:35:41.490710 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_95000.solverstate
I1109 15:35:41.494711 18124 solver.cpp:330] Iteration 95000, Testing net (#0)
I1109 15:35:41.494711 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:35:42.769867 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:35:42.820369 18124 solver.cpp:397]     Test net output #0: accuracy = 0.5765
I1109 15:35:42.820369 18124 solver.cpp:397]     Test net output #1: loss = 1.60943 (* 1 = 1.60943 loss)
I1109 15:35:42.870880 18124 solver.cpp:218] Iteration 95000 (15.0019 iter/s, 6.66582s/100 iters), loss = 1.04151
I1109 15:35:42.870880 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:35:42.870880 18124 solver.cpp:237]     Train net output #1: loss = 1.04151 (* 1 = 1.04151 loss)
I1109 15:35:42.870880 18124 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1109 15:35:42.870880 18124 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1109 15:35:48.193236 18124 solver.cpp:218] Iteration 95100 (18.7927 iter/s, 5.3212s/100 iters), loss = 1.20782
I1109 15:35:48.193236 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:35:48.193236 18124 solver.cpp:237]     Train net output #1: loss = 1.20782 (* 1 = 1.20782 loss)
I1109 15:35:48.193236 18124 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1109 15:35:53.510689 18124 solver.cpp:218] Iteration 95200 (18.8074 iter/s, 5.31705s/100 iters), loss = 0.877593
I1109 15:35:53.510689 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:35:53.510689 18124 solver.cpp:237]     Train net output #1: loss = 0.877593 (* 1 = 0.877593 loss)
I1109 15:35:53.510689 18124 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1109 15:35:58.828146 18124 solver.cpp:218] Iteration 95300 (18.8054 iter/s, 5.31763s/100 iters), loss = 1.04495
I1109 15:35:58.828146 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:35:58.828146 18124 solver.cpp:237]     Train net output #1: loss = 1.04495 (* 1 = 1.04495 loss)
I1109 15:35:58.828146 18124 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1109 15:36:04.148119 18124 solver.cpp:218] Iteration 95400 (18.8004 iter/s, 5.31904s/100 iters), loss = 1.03597
I1109 15:36:04.148119 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:36:04.149119 18124 solver.cpp:237]     Train net output #1: loss = 1.03597 (* 1 = 1.03597 loss)
I1109 15:36:04.149119 18124 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1109 15:36:09.258049  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:36:09.472056 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_95500.caffemodel
I1109 15:36:09.486057 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_95500.solverstate
I1109 15:36:09.490057 18124 solver.cpp:330] Iteration 95500, Testing net (#0)
I1109 15:36:09.490057 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:36:10.770164 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:36:10.821164 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6645
I1109 15:36:10.821164 18124 solver.cpp:397]     Test net output #1: loss = 1.18293 (* 1 = 1.18293 loss)
I1109 15:36:10.873169 18124 solver.cpp:218] Iteration 95500 (14.872 iter/s, 6.72403s/100 iters), loss = 1.12763
I1109 15:36:10.873169 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:36:10.873169 18124 solver.cpp:237]     Train net output #1: loss = 1.12763 (* 1 = 1.12763 loss)
I1109 15:36:10.873169 18124 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1109 15:36:16.285718 18124 solver.cpp:218] Iteration 95600 (18.4796 iter/s, 5.41137s/100 iters), loss = 0.971699
I1109 15:36:16.285718 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1109 15:36:16.285718 18124 solver.cpp:237]     Train net output #1: loss = 0.971699 (* 1 = 0.971699 loss)
I1109 15:36:16.285718 18124 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1109 15:36:21.622200 18124 solver.cpp:218] Iteration 95700 (18.7396 iter/s, 5.33631s/100 iters), loss = 0.763835
I1109 15:36:21.622200 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:36:21.622200 18124 solver.cpp:237]     Train net output #1: loss = 0.763835 (* 1 = 0.763835 loss)
I1109 15:36:21.622200 18124 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1109 15:36:27.055879 18124 solver.cpp:218] Iteration 95800 (18.406 iter/s, 5.43301s/100 iters), loss = 1.0939
I1109 15:36:27.055879 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:36:27.055879 18124 solver.cpp:237]     Train net output #1: loss = 1.0939 (* 1 = 1.0939 loss)
I1109 15:36:27.055879 18124 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1109 15:36:32.462826 18124 solver.cpp:218] Iteration 95900 (18.4947 iter/s, 5.40696s/100 iters), loss = 1.01813
I1109 15:36:32.462826 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:36:32.462826 18124 solver.cpp:237]     Train net output #1: loss = 1.01813 (* 1 = 1.01813 loss)
I1109 15:36:32.462826 18124 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1109 15:36:37.554764  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:36:37.765292 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_96000.caffemodel
I1109 15:36:37.778291 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_96000.solverstate
I1109 15:36:37.782291 18124 solver.cpp:330] Iteration 96000, Testing net (#0)
I1109 15:36:37.783293 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:36:39.058410 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:36:39.108414 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6682
I1109 15:36:39.108414 18124 solver.cpp:397]     Test net output #1: loss = 1.16951 (* 1 = 1.16951 loss)
I1109 15:36:39.159416 18124 solver.cpp:218] Iteration 96000 (14.9352 iter/s, 6.6956s/100 iters), loss = 1.1239
I1109 15:36:39.159416 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:36:39.159416 18124 solver.cpp:237]     Train net output #1: loss = 1.1239 (* 1 = 1.1239 loss)
I1109 15:36:39.159416 18124 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1109 15:36:44.604898 18124 solver.cpp:218] Iteration 96100 (18.3644 iter/s, 5.44532s/100 iters), loss = 0.970869
I1109 15:36:44.604898 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:36:44.604898 18124 solver.cpp:237]     Train net output #1: loss = 0.970869 (* 1 = 0.970869 loss)
I1109 15:36:44.604898 18124 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1109 15:36:50.039331 18124 solver.cpp:218] Iteration 96200 (18.4041 iter/s, 5.43358s/100 iters), loss = 0.824888
I1109 15:36:50.039331 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:36:50.039331 18124 solver.cpp:237]     Train net output #1: loss = 0.824888 (* 1 = 0.824888 loss)
I1109 15:36:50.039331 18124 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1109 15:36:55.457469 18124 solver.cpp:218] Iteration 96300 (18.4581 iter/s, 5.41768s/100 iters), loss = 0.970386
I1109 15:36:55.457469 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:36:55.457469 18124 solver.cpp:237]     Train net output #1: loss = 0.970386 (* 1 = 0.970386 loss)
I1109 15:36:55.457469 18124 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1109 15:37:00.860889 18124 solver.cpp:218] Iteration 96400 (18.5075 iter/s, 5.40323s/100 iters), loss = 1.16654
I1109 15:37:00.860889 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1109 15:37:00.860889 18124 solver.cpp:237]     Train net output #1: loss = 1.16654 (* 1 = 1.16654 loss)
I1109 15:37:00.860889 18124 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1109 15:37:06.007654  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:37:06.222710 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_96500.caffemodel
I1109 15:37:06.237691 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_96500.solverstate
I1109 15:37:06.242705 18124 solver.cpp:330] Iteration 96500, Testing net (#0)
I1109 15:37:06.242705 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:37:07.547353 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:37:07.598387 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6706
I1109 15:37:07.598387 18124 solver.cpp:397]     Test net output #1: loss = 1.16769 (* 1 = 1.16769 loss)
I1109 15:37:07.651876 18124 solver.cpp:218] Iteration 96500 (14.7265 iter/s, 6.79048s/100 iters), loss = 1.06729
I1109 15:37:07.651876 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:37:07.651876 18124 solver.cpp:237]     Train net output #1: loss = 1.06729 (* 1 = 1.06729 loss)
I1109 15:37:07.651876 18124 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1109 15:37:13.121482 18124 solver.cpp:218] Iteration 96600 (18.2831 iter/s, 5.46952s/100 iters), loss = 0.911445
I1109 15:37:13.121482 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:37:13.121482 18124 solver.cpp:237]     Train net output #1: loss = 0.911445 (* 1 = 0.911445 loss)
I1109 15:37:13.121482 18124 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1109 15:37:18.474840 18124 solver.cpp:218] Iteration 96700 (18.684 iter/s, 5.35217s/100 iters), loss = 0.790844
I1109 15:37:18.474840 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:37:18.474840 18124 solver.cpp:237]     Train net output #1: loss = 0.790844 (* 1 = 0.790844 loss)
I1109 15:37:18.474840 18124 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1109 15:37:23.806239 18124 solver.cpp:218] Iteration 96800 (18.7577 iter/s, 5.33114s/100 iters), loss = 1.12721
I1109 15:37:23.806239 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1109 15:37:23.806239 18124 solver.cpp:237]     Train net output #1: loss = 1.12721 (* 1 = 1.12721 loss)
I1109 15:37:23.806239 18124 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1109 15:37:29.230588 18124 solver.cpp:218] Iteration 96900 (18.4386 iter/s, 5.42341s/100 iters), loss = 0.943026
I1109 15:37:29.230588 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1109 15:37:29.230588 18124 solver.cpp:237]     Train net output #1: loss = 0.943026 (* 1 = 0.943026 loss)
I1109 15:37:29.230588 18124 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1109 15:37:34.431044  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:37:34.645545 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_97000.caffemodel
I1109 15:37:34.660544 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_97000.solverstate
I1109 15:37:34.665045 18124 solver.cpp:330] Iteration 97000, Testing net (#0)
I1109 15:37:34.665045 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:37:35.980121 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:37:36.031121 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6697
I1109 15:37:36.032121 18124 solver.cpp:397]     Test net output #1: loss = 1.17302 (* 1 = 1.17302 loss)
I1109 15:37:36.084123 18124 solver.cpp:218] Iteration 97000 (14.5917 iter/s, 6.85323s/100 iters), loss = 0.846551
I1109 15:37:36.084123 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:37:36.084123 18124 solver.cpp:237]     Train net output #1: loss = 0.846551 (* 1 = 0.846551 loss)
I1109 15:37:36.084123 18124 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1109 15:37:41.530580 18124 solver.cpp:218] Iteration 97100 (18.3615 iter/s, 5.44617s/100 iters), loss = 0.921836
I1109 15:37:41.530580 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:37:41.530580 18124 solver.cpp:237]     Train net output #1: loss = 0.921836 (* 1 = 0.921836 loss)
I1109 15:37:41.530580 18124 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1109 15:37:46.914503 18124 solver.cpp:218] Iteration 97200 (18.5734 iter/s, 5.38406s/100 iters), loss = 0.723368
I1109 15:37:46.914503 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:37:46.914503 18124 solver.cpp:237]     Train net output #1: loss = 0.723368 (* 1 = 0.723368 loss)
I1109 15:37:46.915503 18124 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1109 15:37:52.320925 18124 solver.cpp:218] Iteration 97300 (18.498 iter/s, 5.40599s/100 iters), loss = 1.03236
I1109 15:37:52.321913 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1109 15:37:52.321913 18124 solver.cpp:237]     Train net output #1: loss = 1.03236 (* 1 = 1.03236 loss)
I1109 15:37:52.321913 18124 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1109 15:37:57.647310 18124 solver.cpp:218] Iteration 97400 (18.7786 iter/s, 5.3252s/100 iters), loss = 1.01341
I1109 15:37:57.647310 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1109 15:37:57.647310 18124 solver.cpp:237]     Train net output #1: loss = 1.01341 (* 1 = 1.01341 loss)
I1109 15:37:57.647310 18124 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1109 15:38:02.703634  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:38:02.913646 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_97500.caffemodel
I1109 15:38:02.927647 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_97500.solverstate
I1109 15:38:02.931646 18124 solver.cpp:330] Iteration 97500, Testing net (#0)
I1109 15:38:02.931646 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:38:04.207798 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:38:04.257797 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6709
I1109 15:38:04.257797 18124 solver.cpp:397]     Test net output #1: loss = 1.16868 (* 1 = 1.16868 loss)
I1109 15:38:04.308802 18124 solver.cpp:218] Iteration 97500 (15.0121 iter/s, 6.66129s/100 iters), loss = 0.951421
I1109 15:38:04.308802 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:38:04.308802 18124 solver.cpp:237]     Train net output #1: loss = 0.951421 (* 1 = 0.951421 loss)
I1109 15:38:04.308802 18124 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1109 15:38:09.627151 18124 solver.cpp:218] Iteration 97600 (18.8056 iter/s, 5.31757s/100 iters), loss = 0.948651
I1109 15:38:09.627151 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:38:09.627151 18124 solver.cpp:237]     Train net output #1: loss = 0.948651 (* 1 = 0.948651 loss)
I1109 15:38:09.627151 18124 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1109 15:38:14.949460 18124 solver.cpp:218] Iteration 97700 (18.7875 iter/s, 5.32268s/100 iters), loss = 0.875561
I1109 15:38:14.949460 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:38:14.949460 18124 solver.cpp:237]     Train net output #1: loss = 0.875561 (* 1 = 0.875561 loss)
I1109 15:38:14.949460 18124 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1109 15:38:20.381096 18124 solver.cpp:218] Iteration 97800 (18.4122 iter/s, 5.43117s/100 iters), loss = 0.943526
I1109 15:38:20.381096 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:38:20.381096 18124 solver.cpp:237]     Train net output #1: loss = 0.943526 (* 1 = 0.943526 loss)
I1109 15:38:20.381096 18124 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1109 15:38:25.722820 18124 solver.cpp:218] Iteration 97900 (18.7246 iter/s, 5.34056s/100 iters), loss = 1.00244
I1109 15:38:25.722820 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1109 15:38:25.722820 18124 solver.cpp:237]     Train net output #1: loss = 1.00244 (* 1 = 1.00244 loss)
I1109 15:38:25.722820 18124 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1109 15:38:30.787205  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:38:30.996213 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_98000.caffemodel
I1109 15:38:31.009214 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_98000.solverstate
I1109 15:38:31.013216 18124 solver.cpp:330] Iteration 98000, Testing net (#0)
I1109 15:38:31.013216 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:38:32.288437 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:38:32.338438 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6701
I1109 15:38:32.338438 18124 solver.cpp:397]     Test net output #1: loss = 1.16933 (* 1 = 1.16933 loss)
I1109 15:38:32.389441 18124 solver.cpp:218] Iteration 98000 (14.9997 iter/s, 6.6668s/100 iters), loss = 0.907003
I1109 15:38:32.389441 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:38:32.389441 18124 solver.cpp:237]     Train net output #1: loss = 0.907003 (* 1 = 0.907003 loss)
I1109 15:38:32.389441 18124 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1109 15:38:37.714833 18124 solver.cpp:218] Iteration 98100 (18.7789 iter/s, 5.32513s/100 iters), loss = 0.961197
I1109 15:38:37.714833 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:38:37.714833 18124 solver.cpp:237]     Train net output #1: loss = 0.961197 (* 1 = 0.961197 loss)
I1109 15:38:37.714833 18124 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1109 15:38:43.038177 18124 solver.cpp:218] Iteration 98200 (18.7869 iter/s, 5.32287s/100 iters), loss = 0.762688
I1109 15:38:43.038177 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:38:43.038177 18124 solver.cpp:237]     Train net output #1: loss = 0.762688 (* 1 = 0.762688 loss)
I1109 15:38:43.038177 18124 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1109 15:38:48.353574 18124 solver.cpp:218] Iteration 98300 (18.8146 iter/s, 5.31502s/100 iters), loss = 0.935699
I1109 15:38:48.353574 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:38:48.353574 18124 solver.cpp:237]     Train net output #1: loss = 0.935699 (* 1 = 0.935699 loss)
I1109 15:38:48.353574 18124 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1109 15:38:53.673475 18124 solver.cpp:218] Iteration 98400 (18.7997 iter/s, 5.31922s/100 iters), loss = 1.11158
I1109 15:38:53.673975 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1109 15:38:53.673975 18124 solver.cpp:237]     Train net output #1: loss = 1.11158 (* 1 = 1.11158 loss)
I1109 15:38:53.673975 18124 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1109 15:38:58.737319  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:38:58.945336 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_98500.caffemodel
I1109 15:38:58.959336 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_98500.solverstate
I1109 15:38:58.963336 18124 solver.cpp:330] Iteration 98500, Testing net (#0)
I1109 15:38:58.963336 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:39:00.237458 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:39:00.288463 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6693
I1109 15:39:00.288463 18124 solver.cpp:397]     Test net output #1: loss = 1.17735 (* 1 = 1.17735 loss)
I1109 15:39:00.339462 18124 solver.cpp:218] Iteration 98500 (15.0026 iter/s, 6.66552s/100 iters), loss = 0.932725
I1109 15:39:00.339462 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:39:00.339462 18124 solver.cpp:237]     Train net output #1: loss = 0.932725 (* 1 = 0.932725 loss)
I1109 15:39:00.339462 18124 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1109 15:39:05.671811 18124 solver.cpp:218] Iteration 98600 (18.756 iter/s, 5.33163s/100 iters), loss = 0.854838
I1109 15:39:05.671811 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:39:05.671811 18124 solver.cpp:237]     Train net output #1: loss = 0.854838 (* 1 = 0.854838 loss)
I1109 15:39:05.671811 18124 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1109 15:39:11.041684 18124 solver.cpp:218] Iteration 98700 (18.6231 iter/s, 5.36969s/100 iters), loss = 0.797208
I1109 15:39:11.041684 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:39:11.041684 18124 solver.cpp:237]     Train net output #1: loss = 0.797208 (* 1 = 0.797208 loss)
I1109 15:39:11.041684 18124 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1109 15:39:16.489076 18124 solver.cpp:218] Iteration 98800 (18.3574 iter/s, 5.4474s/100 iters), loss = 0.964065
I1109 15:39:16.489076 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1109 15:39:16.489076 18124 solver.cpp:237]     Train net output #1: loss = 0.964065 (* 1 = 0.964065 loss)
I1109 15:39:16.489076 18124 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1109 15:39:21.854975 18124 solver.cpp:218] Iteration 98900 (18.6389 iter/s, 5.36513s/100 iters), loss = 0.958431
I1109 15:39:21.854975 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:39:21.854975 18124 solver.cpp:237]     Train net output #1: loss = 0.958431 (* 1 = 0.958431 loss)
I1109 15:39:21.854975 18124 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1109 15:39:27.058765  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:39:27.275286 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_99000.caffemodel
I1109 15:39:27.289825 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_99000.solverstate
I1109 15:39:27.293826 18124 solver.cpp:330] Iteration 99000, Testing net (#0)
I1109 15:39:27.293826 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:39:28.582988 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:39:28.632989 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6684
I1109 15:39:28.632989 18124 solver.cpp:397]     Test net output #1: loss = 1.181 (* 1 = 1.181 loss)
I1109 15:39:28.684990 18124 solver.cpp:218] Iteration 99000 (14.6433 iter/s, 6.82908s/100 iters), loss = 0.928554
I1109 15:39:28.684990 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:39:28.684990 18124 solver.cpp:237]     Train net output #1: loss = 0.928554 (* 1 = 0.928554 loss)
I1109 15:39:28.684990 18124 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1109 15:39:34.122385 18124 solver.cpp:218] Iteration 99100 (18.3921 iter/s, 5.43711s/100 iters), loss = 0.838035
I1109 15:39:34.122385 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:39:34.122385 18124 solver.cpp:237]     Train net output #1: loss = 0.838035 (* 1 = 0.838035 loss)
I1109 15:39:34.122385 18124 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1109 15:39:39.479298 18124 solver.cpp:218] Iteration 99200 (18.6689 iter/s, 5.35649s/100 iters), loss = 0.827621
I1109 15:39:39.479298 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:39:39.479298 18124 solver.cpp:237]     Train net output #1: loss = 0.827621 (* 1 = 0.827621 loss)
I1109 15:39:39.479298 18124 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1109 15:39:44.923286 18124 solver.cpp:218] Iteration 99300 (18.3709 iter/s, 5.44338s/100 iters), loss = 0.92654
I1109 15:39:44.923286 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:39:44.923286 18124 solver.cpp:237]     Train net output #1: loss = 0.92654 (* 1 = 0.92654 loss)
I1109 15:39:44.923286 18124 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1109 15:39:50.313746 18124 solver.cpp:218] Iteration 99400 (18.5527 iter/s, 5.39004s/100 iters), loss = 0.917473
I1109 15:39:50.313746 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:39:50.313746 18124 solver.cpp:237]     Train net output #1: loss = 0.917473 (* 1 = 0.917473 loss)
I1109 15:39:50.313746 18124 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1109 15:39:55.392072  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:39:55.607105 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_99500.caffemodel
I1109 15:39:55.621105 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_99500.solverstate
I1109 15:39:55.626109 18124 solver.cpp:330] Iteration 99500, Testing net (#0)
I1109 15:39:55.626109 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:39:56.933243 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:39:56.984746 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6692
I1109 15:39:56.984746 18124 solver.cpp:397]     Test net output #1: loss = 1.18295 (* 1 = 1.18295 loss)
I1109 15:39:57.037252 18124 solver.cpp:218] Iteration 99500 (14.8736 iter/s, 6.72334s/100 iters), loss = 0.929377
I1109 15:39:57.037252 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:39:57.037252 18124 solver.cpp:237]     Train net output #1: loss = 0.929377 (* 1 = 0.929377 loss)
I1109 15:39:57.037252 18124 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1109 15:40:02.480932 18124 solver.cpp:218] Iteration 99600 (18.3723 iter/s, 5.44298s/100 iters), loss = 0.851791
I1109 15:40:02.480932 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:40:02.480932 18124 solver.cpp:237]     Train net output #1: loss = 0.851791 (* 1 = 0.851791 loss)
I1109 15:40:02.480932 18124 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1109 15:40:07.838428 18124 solver.cpp:218] Iteration 99700 (18.6644 iter/s, 5.3578s/100 iters), loss = 0.830557
I1109 15:40:07.838428 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:40:07.838428 18124 solver.cpp:237]     Train net output #1: loss = 0.830557 (* 1 = 0.830557 loss)
I1109 15:40:07.838428 18124 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1109 15:40:13.286164 18124 solver.cpp:218] Iteration 99800 (18.3594 iter/s, 5.44682s/100 iters), loss = 0.961222
I1109 15:40:13.286164 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:40:13.286164 18124 solver.cpp:237]     Train net output #1: loss = 0.961222 (* 1 = 0.961222 loss)
I1109 15:40:13.286164 18124 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1109 15:40:18.647284 18124 solver.cpp:218] Iteration 99900 (18.6536 iter/s, 5.36088s/100 iters), loss = 1.06684
I1109 15:40:18.647284 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1109 15:40:18.647284 18124 solver.cpp:237]     Train net output #1: loss = 1.06684 (* 1 = 1.06684 loss)
I1109 15:40:18.647284 18124 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1109 15:40:23.826046  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:40:24.041101 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_100000.caffemodel
I1109 15:40:24.056103 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_100000.solverstate
I1109 15:40:24.061102 18124 solver.cpp:330] Iteration 100000, Testing net (#0)
I1109 15:40:24.061102 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:40:25.358220 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:40:25.410230 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6687
I1109 15:40:25.410230 18124 solver.cpp:397]     Test net output #1: loss = 1.18091 (* 1 = 1.18091 loss)
I1109 15:40:25.462229 18124 solver.cpp:218] Iteration 100000 (14.6749 iter/s, 6.81437s/100 iters), loss = 0.77663
I1109 15:40:25.462229 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1109 15:40:25.462229 18124 solver.cpp:237]     Train net output #1: loss = 0.77663 (* 1 = 0.77663 loss)
I1109 15:40:25.462229 18124 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1109 15:40:30.857671 18124 solver.cpp:218] Iteration 100100 (18.5348 iter/s, 5.39526s/100 iters), loss = 0.880623
I1109 15:40:30.857671 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:40:30.857671 18124 solver.cpp:237]     Train net output #1: loss = 0.880623 (* 1 = 0.880623 loss)
I1109 15:40:30.857671 18124 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1109 15:40:36.195577 18124 solver.cpp:218] Iteration 100200 (18.7381 iter/s, 5.33673s/100 iters), loss = 0.662855
I1109 15:40:36.195577 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1109 15:40:36.195577 18124 solver.cpp:237]     Train net output #1: loss = 0.662855 (* 1 = 0.662855 loss)
I1109 15:40:36.195577 18124 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1109 15:40:41.651587 18124 solver.cpp:218] Iteration 100300 (18.3297 iter/s, 5.45563s/100 iters), loss = 0.911582
I1109 15:40:41.651587 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:40:41.651587 18124 solver.cpp:237]     Train net output #1: loss = 0.911582 (* 1 = 0.911582 loss)
I1109 15:40:41.651587 18124 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1109 15:40:47.097321 18124 solver.cpp:218] Iteration 100400 (18.3643 iter/s, 5.44534s/100 iters), loss = 0.966204
I1109 15:40:47.097321 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:40:47.097321 18124 solver.cpp:237]     Train net output #1: loss = 0.966204 (* 1 = 0.966204 loss)
I1109 15:40:47.097321 18124 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1109 15:40:52.255209  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:40:52.471230 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_100500.caffemodel
I1109 15:40:52.485734 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_100500.solverstate
I1109 15:40:52.490234 18124 solver.cpp:330] Iteration 100500, Testing net (#0)
I1109 15:40:52.490234 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:40:53.772318 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:40:53.822326 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6697
I1109 15:40:53.822326 18124 solver.cpp:397]     Test net output #1: loss = 1.18259 (* 1 = 1.18259 loss)
I1109 15:40:53.873327 18124 solver.cpp:218] Iteration 100500 (14.7592 iter/s, 6.77543s/100 iters), loss = 0.898875
I1109 15:40:53.873327 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:40:53.873327 18124 solver.cpp:237]     Train net output #1: loss = 0.898875 (* 1 = 0.898875 loss)
I1109 15:40:53.873327 18124 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1109 15:40:59.241755 18124 solver.cpp:218] Iteration 100600 (18.6288 iter/s, 5.36802s/100 iters), loss = 0.932811
I1109 15:40:59.241755 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:40:59.241755 18124 solver.cpp:237]     Train net output #1: loss = 0.932811 (* 1 = 0.932811 loss)
I1109 15:40:59.241755 18124 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1109 15:41:04.716190 18124 solver.cpp:218] Iteration 100700 (18.2679 iter/s, 5.47409s/100 iters), loss = 0.724498
I1109 15:41:04.716190 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:41:04.716190 18124 solver.cpp:237]     Train net output #1: loss = 0.724498 (* 1 = 0.724498 loss)
I1109 15:41:04.716190 18124 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1109 15:41:10.174924 18124 solver.cpp:218] Iteration 100800 (18.321 iter/s, 5.45822s/100 iters), loss = 0.91577
I1109 15:41:10.174924 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:41:10.174924 18124 solver.cpp:237]     Train net output #1: loss = 0.91577 (* 1 = 0.91577 loss)
I1109 15:41:10.174924 18124 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1109 15:41:15.594372 18124 solver.cpp:218] Iteration 100900 (18.4534 iter/s, 5.41905s/100 iters), loss = 0.914164
I1109 15:41:15.594372 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:41:15.594372 18124 solver.cpp:237]     Train net output #1: loss = 0.914164 (* 1 = 0.914164 loss)
I1109 15:41:15.594372 18124 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1109 15:41:20.726815  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:41:20.944833 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_101000.caffemodel
I1109 15:41:20.957834 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_101000.solverstate
I1109 15:41:20.962836 18124 solver.cpp:330] Iteration 101000, Testing net (#0)
I1109 15:41:20.962836 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:41:22.258946 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:41:22.310952 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6721
I1109 15:41:22.310952 18124 solver.cpp:397]     Test net output #1: loss = 1.18159 (* 1 = 1.18159 loss)
I1109 15:41:22.363953 18124 solver.cpp:218] Iteration 101000 (14.7736 iter/s, 6.76885s/100 iters), loss = 0.88453
I1109 15:41:22.363953 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:41:22.363953 18124 solver.cpp:237]     Train net output #1: loss = 0.88453 (* 1 = 0.88453 loss)
I1109 15:41:22.363953 18124 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1109 15:41:27.816422 18124 solver.cpp:218] Iteration 101100 (18.3399 iter/s, 5.4526s/100 iters), loss = 0.949169
I1109 15:41:27.816422 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:41:27.816422 18124 solver.cpp:237]     Train net output #1: loss = 0.949169 (* 1 = 0.949169 loss)
I1109 15:41:27.816422 18124 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1109 15:41:33.240857 18124 solver.cpp:218] Iteration 101200 (18.4386 iter/s, 5.4234s/100 iters), loss = 0.834851
I1109 15:41:33.240857 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:41:33.240857 18124 solver.cpp:237]     Train net output #1: loss = 0.834851 (* 1 = 0.834851 loss)
I1109 15:41:33.240857 18124 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1109 15:41:38.599243 18124 solver.cpp:218] Iteration 101300 (18.663 iter/s, 5.3582s/100 iters), loss = 0.996875
I1109 15:41:38.599243 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:41:38.599243 18124 solver.cpp:237]     Train net output #1: loss = 0.996875 (* 1 = 0.996875 loss)
I1109 15:41:38.599243 18124 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1109 15:41:44.004680 18124 solver.cpp:218] Iteration 101400 (18.5017 iter/s, 5.40492s/100 iters), loss = 0.859971
I1109 15:41:44.004680 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:41:44.004680 18124 solver.cpp:237]     Train net output #1: loss = 0.859971 (* 1 = 0.859971 loss)
I1109 15:41:44.004680 18124 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1109 15:41:49.078565  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:41:49.295138 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_101500.caffemodel
I1109 15:41:49.309139 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_101500.solverstate
I1109 15:41:49.314138 18124 solver.cpp:330] Iteration 101500, Testing net (#0)
I1109 15:41:49.314138 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:41:50.622697 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:41:50.673854 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6726
I1109 15:41:50.673854 18124 solver.cpp:397]     Test net output #1: loss = 1.17588 (* 1 = 1.17588 loss)
I1109 15:41:50.726853 18124 solver.cpp:218] Iteration 101500 (14.8774 iter/s, 6.72162s/100 iters), loss = 0.801771
I1109 15:41:50.726853 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1109 15:41:50.726853 18124 solver.cpp:237]     Train net output #1: loss = 0.801771 (* 1 = 0.801771 loss)
I1109 15:41:50.726853 18124 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1109 15:41:56.121670 18124 solver.cpp:218] Iteration 101600 (18.5357 iter/s, 5.39499s/100 iters), loss = 0.911383
I1109 15:41:56.121670 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:41:56.121670 18124 solver.cpp:237]     Train net output #1: loss = 0.911383 (* 1 = 0.911383 loss)
I1109 15:41:56.121670 18124 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1109 15:42:01.566179 18124 solver.cpp:218] Iteration 101700 (18.3711 iter/s, 5.44332s/100 iters), loss = 0.719474
I1109 15:42:01.566179 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:42:01.566179 18124 solver.cpp:237]     Train net output #1: loss = 0.719474 (* 1 = 0.719474 loss)
I1109 15:42:01.566179 18124 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1109 15:42:07.039196 18124 solver.cpp:218] Iteration 101800 (18.2722 iter/s, 5.47279s/100 iters), loss = 0.885684
I1109 15:42:07.039196 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:42:07.039196 18124 solver.cpp:237]     Train net output #1: loss = 0.885684 (* 1 = 0.885684 loss)
I1109 15:42:07.039196 18124 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1109 15:42:12.488606 18124 solver.cpp:218] Iteration 101900 (18.3507 iter/s, 5.4494s/100 iters), loss = 1.06959
I1109 15:42:12.488606 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:42:12.488606 18124 solver.cpp:237]     Train net output #1: loss = 1.06959 (* 1 = 1.06959 loss)
I1109 15:42:12.488606 18124 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1109 15:42:17.626499  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:42:17.845041 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_102000.caffemodel
I1109 15:42:17.863059 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_102000.solverstate
I1109 15:42:17.867058 18124 solver.cpp:330] Iteration 102000, Testing net (#0)
I1109 15:42:17.867058 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:42:19.153514 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:42:19.204510 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6717
I1109 15:42:19.204510 18124 solver.cpp:397]     Test net output #1: loss = 1.17402 (* 1 = 1.17402 loss)
I1109 15:42:19.257539 18124 solver.cpp:218] Iteration 102000 (14.776 iter/s, 6.76774s/100 iters), loss = 0.835386
I1109 15:42:19.257539 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:42:19.257539 18124 solver.cpp:237]     Train net output #1: loss = 0.835386 (* 1 = 0.835386 loss)
I1109 15:42:19.257539 18124 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1109 15:42:24.655625 18124 solver.cpp:218] Iteration 102100 (18.5256 iter/s, 5.39794s/100 iters), loss = 0.895459
I1109 15:42:24.655625 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:42:24.655625 18124 solver.cpp:237]     Train net output #1: loss = 0.895459 (* 1 = 0.895459 loss)
I1109 15:42:24.655625 18124 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1109 15:42:29.991288 18124 solver.cpp:218] Iteration 102200 (18.7441 iter/s, 5.33502s/100 iters), loss = 0.734513
I1109 15:42:29.991288 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:42:29.991288 18124 solver.cpp:237]     Train net output #1: loss = 0.734513 (* 1 = 0.734513 loss)
I1109 15:42:29.991288 18124 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1109 15:42:35.447259 18124 solver.cpp:218] Iteration 102300 (18.3304 iter/s, 5.45542s/100 iters), loss = 0.948888
I1109 15:42:35.447259 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:42:35.447259 18124 solver.cpp:237]     Train net output #1: loss = 0.948888 (* 1 = 0.948888 loss)
I1109 15:42:35.447259 18124 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1109 15:42:40.818274 18124 solver.cpp:218] Iteration 102400 (18.6178 iter/s, 5.37119s/100 iters), loss = 1.01265
I1109 15:42:40.818274 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1109 15:42:40.818274 18124 solver.cpp:237]     Train net output #1: loss = 1.01265 (* 1 = 1.01265 loss)
I1109 15:42:40.818274 18124 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1109 15:42:45.949265  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:42:46.158285 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_102500.caffemodel
I1109 15:42:46.175788 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_102500.solverstate
I1109 15:42:46.179788 18124 solver.cpp:330] Iteration 102500, Testing net (#0)
I1109 15:42:46.179788 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:42:47.455901 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:42:47.504904 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6729
I1109 15:42:47.505905 18124 solver.cpp:397]     Test net output #1: loss = 1.17864 (* 1 = 1.17864 loss)
I1109 15:42:47.557905 18124 solver.cpp:218] Iteration 102500 (14.8406 iter/s, 6.73827s/100 iters), loss = 0.73172
I1109 15:42:47.557905 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1109 15:42:47.557905 18124 solver.cpp:237]     Train net output #1: loss = 0.73172 (* 1 = 0.73172 loss)
I1109 15:42:47.557905 18124 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1109 15:42:52.892916 18124 solver.cpp:218] Iteration 102600 (18.7439 iter/s, 5.33508s/100 iters), loss = 0.815874
I1109 15:42:52.892916 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:42:52.892916 18124 solver.cpp:237]     Train net output #1: loss = 0.815874 (* 1 = 0.815874 loss)
I1109 15:42:52.892916 18124 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1109 15:42:58.227296 18124 solver.cpp:218] Iteration 102700 (18.7479 iter/s, 5.33393s/100 iters), loss = 0.65806
I1109 15:42:58.227296 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:42:58.227296 18124 solver.cpp:237]     Train net output #1: loss = 0.65806 (* 1 = 0.65806 loss)
I1109 15:42:58.227296 18124 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1109 15:43:03.565809 18124 solver.cpp:218] Iteration 102800 (18.7342 iter/s, 5.33784s/100 iters), loss = 0.895
I1109 15:43:03.565809 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:43:03.565809 18124 solver.cpp:237]     Train net output #1: loss = 0.895 (* 1 = 0.895 loss)
I1109 15:43:03.565809 18124 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1109 15:43:08.902130 18124 solver.cpp:218] Iteration 102900 (18.7405 iter/s, 5.33604s/100 iters), loss = 0.931149
I1109 15:43:08.902130 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1109 15:43:08.902130 18124 solver.cpp:237]     Train net output #1: loss = 0.931149 (* 1 = 0.931149 loss)
I1109 15:43:08.902130 18124 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1109 15:43:13.995501  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:43:14.205525 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_103000.caffemodel
I1109 15:43:14.218525 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_103000.solverstate
I1109 15:43:14.222525 18124 solver.cpp:330] Iteration 103000, Testing net (#0)
I1109 15:43:14.222525 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:43:15.509169 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:43:15.559670 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6717
I1109 15:43:15.559670 18124 solver.cpp:397]     Test net output #1: loss = 1.18483 (* 1 = 1.18483 loss)
I1109 15:43:15.610169 18124 solver.cpp:218] Iteration 103000 (14.9082 iter/s, 6.70771s/100 iters), loss = 0.899349
I1109 15:43:15.610671 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:43:15.610671 18124 solver.cpp:237]     Train net output #1: loss = 0.899349 (* 1 = 0.899349 loss)
I1109 15:43:15.610671 18124 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1109 15:43:20.940976 18124 solver.cpp:218] Iteration 103100 (18.7589 iter/s, 5.33081s/100 iters), loss = 0.899648
I1109 15:43:20.940976 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:43:20.941990 18124 solver.cpp:237]     Train net output #1: loss = 0.899648 (* 1 = 0.899648 loss)
I1109 15:43:20.941990 18124 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1109 15:43:26.267982 18124 solver.cpp:218] Iteration 103200 (18.7764 iter/s, 5.32584s/100 iters), loss = 0.859864
I1109 15:43:26.267982 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:43:26.267982 18124 solver.cpp:237]     Train net output #1: loss = 0.859864 (* 1 = 0.859864 loss)
I1109 15:43:26.267982 18124 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1109 15:43:31.602934 18124 solver.cpp:218] Iteration 103300 (18.7434 iter/s, 5.33522s/100 iters), loss = 1.00892
I1109 15:43:31.602934 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:43:31.602934 18124 solver.cpp:237]     Train net output #1: loss = 1.00892 (* 1 = 1.00892 loss)
I1109 15:43:31.602934 18124 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1109 15:43:36.968415 18124 solver.cpp:218] Iteration 103400 (18.6405 iter/s, 5.36467s/100 iters), loss = 0.981995
I1109 15:43:36.968415 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1109 15:43:36.968415 18124 solver.cpp:237]     Train net output #1: loss = 0.981995 (* 1 = 0.981995 loss)
I1109 15:43:36.968415 18124 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1109 15:43:42.034934  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:43:42.244946 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_103500.caffemodel
I1109 15:43:42.261946 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_103500.solverstate
I1109 15:43:42.265946 18124 solver.cpp:330] Iteration 103500, Testing net (#0)
I1109 15:43:42.265946 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:43:43.543048 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:43:43.593551 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6723
I1109 15:43:43.593551 18124 solver.cpp:397]     Test net output #1: loss = 1.18664 (* 1 = 1.18664 loss)
I1109 15:43:43.645053 18124 solver.cpp:218] Iteration 103500 (14.9789 iter/s, 6.67607s/100 iters), loss = 0.84603
I1109 15:43:43.645053 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:43:43.645053 18124 solver.cpp:237]     Train net output #1: loss = 0.84603 (* 1 = 0.84603 loss)
I1109 15:43:43.645053 18124 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1109 15:43:48.974414 18124 solver.cpp:218] Iteration 103600 (18.7661 iter/s, 5.32876s/100 iters), loss = 0.816905
I1109 15:43:48.974414 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:43:48.974414 18124 solver.cpp:237]     Train net output #1: loss = 0.816905 (* 1 = 0.816905 loss)
I1109 15:43:48.974414 18124 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1109 15:43:54.294419 18124 solver.cpp:218] Iteration 103700 (18.7972 iter/s, 5.31995s/100 iters), loss = 0.660891
I1109 15:43:54.294919 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1109 15:43:54.294919 18124 solver.cpp:237]     Train net output #1: loss = 0.660891 (* 1 = 0.660891 loss)
I1109 15:43:54.294919 18124 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1109 15:43:59.743321 18124 solver.cpp:218] Iteration 103800 (18.3547 iter/s, 5.44821s/100 iters), loss = 0.974219
I1109 15:43:59.743321 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:43:59.743321 18124 solver.cpp:237]     Train net output #1: loss = 0.974219 (* 1 = 0.974219 loss)
I1109 15:43:59.743321 18124 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1109 15:44:05.075656 18124 solver.cpp:218] Iteration 103900 (18.7549 iter/s, 5.33193s/100 iters), loss = 0.994592
I1109 15:44:05.075656 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:44:05.075656 18124 solver.cpp:237]     Train net output #1: loss = 0.994592 (* 1 = 0.994592 loss)
I1109 15:44:05.075656 18124 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1109 15:44:10.142971  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:44:10.352977 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_104000.caffemodel
I1109 15:44:10.365978 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_104000.solverstate
I1109 15:44:10.369979 18124 solver.cpp:330] Iteration 104000, Testing net (#0)
I1109 15:44:10.369979 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:44:11.648995 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:44:11.698998 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6734
I1109 15:44:11.698998 18124 solver.cpp:397]     Test net output #1: loss = 1.18711 (* 1 = 1.18711 loss)
I1109 15:44:11.750000 18124 solver.cpp:218] Iteration 104000 (14.9845 iter/s, 6.67358s/100 iters), loss = 0.844934
I1109 15:44:11.750000 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:44:11.750000 18124 solver.cpp:237]     Train net output #1: loss = 0.844934 (* 1 = 0.844934 loss)
I1109 15:44:11.750000 18124 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1109 15:44:17.071342 18124 solver.cpp:218] Iteration 104100 (18.7932 iter/s, 5.32107s/100 iters), loss = 0.79774
I1109 15:44:17.071342 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:44:17.071342 18124 solver.cpp:237]     Train net output #1: loss = 0.79774 (* 1 = 0.79774 loss)
I1109 15:44:17.071342 18124 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1109 15:44:22.395754 18124 solver.cpp:218] Iteration 104200 (18.7835 iter/s, 5.32383s/100 iters), loss = 0.781349
I1109 15:44:22.395754 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:44:22.395754 18124 solver.cpp:237]     Train net output #1: loss = 0.781349 (* 1 = 0.781349 loss)
I1109 15:44:22.395754 18124 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1109 15:44:27.723170 18124 solver.cpp:218] Iteration 104300 (18.7725 iter/s, 5.32695s/100 iters), loss = 0.964748
I1109 15:44:27.723170 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1109 15:44:27.723170 18124 solver.cpp:237]     Train net output #1: loss = 0.964748 (* 1 = 0.964748 loss)
I1109 15:44:27.723170 18124 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1109 15:44:33.045519 18124 solver.cpp:218] Iteration 104400 (18.7872 iter/s, 5.32276s/100 iters), loss = 0.891397
I1109 15:44:33.046520 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:44:33.046520 18124 solver.cpp:237]     Train net output #1: loss = 0.891397 (* 1 = 0.891397 loss)
I1109 15:44:33.046520 18124 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1109 15:44:38.105857  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:44:38.315871 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_104500.caffemodel
I1109 15:44:38.328871 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_104500.solverstate
I1109 15:44:38.332870 18124 solver.cpp:330] Iteration 104500, Testing net (#0)
I1109 15:44:38.332870 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:44:39.607950 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:44:39.657950 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6707
I1109 15:44:39.657950 18124 solver.cpp:397]     Test net output #1: loss = 1.19237 (* 1 = 1.19237 loss)
I1109 15:44:39.708955 18124 solver.cpp:218] Iteration 104500 (15.009 iter/s, 6.66268s/100 iters), loss = 0.876388
I1109 15:44:39.708955 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:44:39.708955 18124 solver.cpp:237]     Train net output #1: loss = 0.876388 (* 1 = 0.876388 loss)
I1109 15:44:39.708955 18124 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1109 15:44:45.033282 18124 solver.cpp:218] Iteration 104600 (18.783 iter/s, 5.32396s/100 iters), loss = 0.836531
I1109 15:44:45.033282 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1109 15:44:45.033282 18124 solver.cpp:237]     Train net output #1: loss = 0.836531 (* 1 = 0.836531 loss)
I1109 15:44:45.033282 18124 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1109 15:44:50.350641 18124 solver.cpp:218] Iteration 104700 (18.8077 iter/s, 5.31698s/100 iters), loss = 0.718497
I1109 15:44:50.350641 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1109 15:44:50.350641 18124 solver.cpp:237]     Train net output #1: loss = 0.718497 (* 1 = 0.718497 loss)
I1109 15:44:50.350641 18124 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1109 15:44:55.698103 18124 solver.cpp:218] Iteration 104800 (18.7035 iter/s, 5.3466s/100 iters), loss = 0.877188
I1109 15:44:55.698604 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:44:55.698604 18124 solver.cpp:237]     Train net output #1: loss = 0.877188 (* 1 = 0.877188 loss)
I1109 15:44:55.698604 18124 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1109 15:45:01.091001 18124 solver.cpp:218] Iteration 104900 (18.5456 iter/s, 5.39211s/100 iters), loss = 1.06058
I1109 15:45:01.091001 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.63
I1109 15:45:01.091001 18124 solver.cpp:237]     Train net output #1: loss = 1.06058 (* 1 = 1.06058 loss)
I1109 15:45:01.091001 18124 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1109 15:45:06.180639  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:45:06.393656 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_105000.caffemodel
I1109 15:45:06.408156 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_105000.solverstate
I1109 15:45:06.411671 18124 solver.cpp:330] Iteration 105000, Testing net (#0)
I1109 15:45:06.411671 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:45:07.702759 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:45:07.754763 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6725
I1109 15:45:07.754763 18124 solver.cpp:397]     Test net output #1: loss = 1.18929 (* 1 = 1.18929 loss)
I1109 15:45:07.807273 18124 solver.cpp:218] Iteration 105000 (14.8901 iter/s, 6.71587s/100 iters), loss = 0.905023
I1109 15:45:07.807273 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:45:07.807273 18124 solver.cpp:237]     Train net output #1: loss = 0.905023 (* 1 = 0.905023 loss)
I1109 15:45:07.807273 18124 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1109 15:45:13.253180 18124 solver.cpp:218] Iteration 105100 (18.3613 iter/s, 5.44624s/100 iters), loss = 0.70727
I1109 15:45:13.254180 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1109 15:45:13.254180 18124 solver.cpp:237]     Train net output #1: loss = 0.70727 (* 1 = 0.70727 loss)
I1109 15:45:13.254180 18124 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1109 15:45:18.631088 18124 solver.cpp:218] Iteration 105200 (18.5965 iter/s, 5.37736s/100 iters), loss = 0.710158
I1109 15:45:18.631088 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:45:18.631088 18124 solver.cpp:237]     Train net output #1: loss = 0.710158 (* 1 = 0.710158 loss)
I1109 15:45:18.631088 18124 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1109 15:45:24.021030 18124 solver.cpp:218] Iteration 105300 (18.5555 iter/s, 5.38924s/100 iters), loss = 0.93528
I1109 15:45:24.021030 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:45:24.021030 18124 solver.cpp:237]     Train net output #1: loss = 0.93528 (* 1 = 0.93528 loss)
I1109 15:45:24.021030 18124 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1109 15:45:29.378739 18124 solver.cpp:218] Iteration 105400 (18.6664 iter/s, 5.35722s/100 iters), loss = 0.939929
I1109 15:45:29.378739 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1109 15:45:29.378739 18124 solver.cpp:237]     Train net output #1: loss = 0.939929 (* 1 = 0.939929 loss)
I1109 15:45:29.378739 18124 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1109 15:45:34.509135  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:45:34.719167 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_105500.caffemodel
I1109 15:45:34.732151 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_105500.solverstate
I1109 15:45:34.737164 18124 solver.cpp:330] Iteration 105500, Testing net (#0)
I1109 15:45:34.737164 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:45:36.023259 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:45:36.073257 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6696
I1109 15:45:36.073257 18124 solver.cpp:397]     Test net output #1: loss = 1.19631 (* 1 = 1.19631 loss)
I1109 15:45:36.124264 18124 solver.cpp:218] Iteration 105500 (14.8267 iter/s, 6.74461s/100 iters), loss = 0.834767
I1109 15:45:36.124264 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:45:36.124264 18124 solver.cpp:237]     Train net output #1: loss = 0.834767 (* 1 = 0.834767 loss)
I1109 15:45:36.124264 18124 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1109 15:45:41.461632 18124 solver.cpp:218] Iteration 105600 (18.7346 iter/s, 5.33772s/100 iters), loss = 0.924057
I1109 15:45:41.461632 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:45:41.461632 18124 solver.cpp:237]     Train net output #1: loss = 0.924057 (* 1 = 0.924057 loss)
I1109 15:45:41.461632 18124 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1109 15:45:46.809041 18124 solver.cpp:218] Iteration 105700 (18.702 iter/s, 5.34703s/100 iters), loss = 0.760561
I1109 15:45:46.809041 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:45:46.809041 18124 solver.cpp:237]     Train net output #1: loss = 0.760561 (* 1 = 0.760561 loss)
I1109 15:45:46.809041 18124 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1109 15:45:52.334054 18124 solver.cpp:218] Iteration 105800 (18.1036 iter/s, 5.52377s/100 iters), loss = 0.886821
I1109 15:45:52.334054 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:45:52.334054 18124 solver.cpp:237]     Train net output #1: loss = 0.886821 (* 1 = 0.886821 loss)
I1109 15:45:52.334054 18124 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1109 15:45:57.767621 18124 solver.cpp:218] Iteration 105900 (18.4035 iter/s, 5.43376s/100 iters), loss = 1.04047
I1109 15:45:57.767621 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:45:57.767621 18124 solver.cpp:237]     Train net output #1: loss = 1.04047 (* 1 = 1.04047 loss)
I1109 15:45:57.767621 18124 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1109 15:46:02.904597  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:46:03.114636 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_106000.caffemodel
I1109 15:46:03.130637 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_106000.solverstate
I1109 15:46:03.134639 18124 solver.cpp:330] Iteration 106000, Testing net (#0)
I1109 15:46:03.135639 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:46:04.430744 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:46:04.482249 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6694
I1109 15:46:04.482249 18124 solver.cpp:397]     Test net output #1: loss = 1.19723 (* 1 = 1.19723 loss)
I1109 15:46:04.533752 18124 solver.cpp:218] Iteration 106000 (14.7807 iter/s, 6.76556s/100 iters), loss = 0.769171
I1109 15:46:04.533752 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1109 15:46:04.533752 18124 solver.cpp:237]     Train net output #1: loss = 0.769171 (* 1 = 0.769171 loss)
I1109 15:46:04.533752 18124 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1109 15:46:10.023593 18124 solver.cpp:218] Iteration 106100 (18.2173 iter/s, 5.4893s/100 iters), loss = 0.877552
I1109 15:46:10.023593 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:46:10.023593 18124 solver.cpp:237]     Train net output #1: loss = 0.877552 (* 1 = 0.877552 loss)
I1109 15:46:10.024593 18124 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1109 15:46:15.545333 18124 solver.cpp:218] Iteration 106200 (18.1122 iter/s, 5.52115s/100 iters), loss = 0.649109
I1109 15:46:15.545333 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1109 15:46:15.545333 18124 solver.cpp:237]     Train net output #1: loss = 0.649109 (* 1 = 0.649109 loss)
I1109 15:46:15.545333 18124 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1109 15:46:21.058387 18124 solver.cpp:218] Iteration 106300 (18.1429 iter/s, 5.5118s/100 iters), loss = 0.993131
I1109 15:46:21.058387 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:46:21.058387 18124 solver.cpp:237]     Train net output #1: loss = 0.993131 (* 1 = 0.993131 loss)
I1109 15:46:21.058387 18124 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1109 15:46:26.490716 18124 solver.cpp:218] Iteration 106400 (18.4099 iter/s, 5.43186s/100 iters), loss = 1.02725
I1109 15:46:26.490716 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:46:26.490716 18124 solver.cpp:237]     Train net output #1: loss = 1.02725 (* 1 = 1.02725 loss)
I1109 15:46:26.490716 18124 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1109 15:46:31.587729  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:46:31.800242 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_106500.caffemodel
I1109 15:46:31.814242 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_106500.solverstate
I1109 15:46:31.818243 18124 solver.cpp:330] Iteration 106500, Testing net (#0)
I1109 15:46:31.818243 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:46:33.107328 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:46:33.157327 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6702
I1109 15:46:33.157327 18124 solver.cpp:397]     Test net output #1: loss = 1.19914 (* 1 = 1.19914 loss)
I1109 15:46:33.207332 18124 solver.cpp:218] Iteration 106500 (14.8875 iter/s, 6.71704s/100 iters), loss = 0.959123
I1109 15:46:33.207332 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:46:33.207332 18124 solver.cpp:237]     Train net output #1: loss = 0.959123 (* 1 = 0.959123 loss)
I1109 15:46:33.208333 18124 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1109 15:46:38.569736 18124 solver.cpp:218] Iteration 106600 (18.6509 iter/s, 5.36166s/100 iters), loss = 0.885802
I1109 15:46:38.569736 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:46:38.569736 18124 solver.cpp:237]     Train net output #1: loss = 0.885802 (* 1 = 0.885802 loss)
I1109 15:46:38.569736 18124 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1109 15:46:43.978533 18124 solver.cpp:218] Iteration 106700 (18.4895 iter/s, 5.40847s/100 iters), loss = 0.714238
I1109 15:46:43.978533 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:46:43.978533 18124 solver.cpp:237]     Train net output #1: loss = 0.714238 (* 1 = 0.714238 loss)
I1109 15:46:43.978533 18124 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1109 15:46:49.354874 18124 solver.cpp:218] Iteration 106800 (18.6041 iter/s, 5.37516s/100 iters), loss = 0.820814
I1109 15:46:49.354874 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1109 15:46:49.354874 18124 solver.cpp:237]     Train net output #1: loss = 0.820814 (* 1 = 0.820814 loss)
I1109 15:46:49.354874 18124 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1109 15:46:54.714357 18124 solver.cpp:218] Iteration 106900 (18.6577 iter/s, 5.35972s/100 iters), loss = 1.01404
I1109 15:46:54.714357 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:46:54.714357 18124 solver.cpp:237]     Train net output #1: loss = 1.01404 (* 1 = 1.01404 loss)
I1109 15:46:54.714357 18124 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1109 15:46:59.799794  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:47:00.013798 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_107000.caffemodel
I1109 15:47:00.027297 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_107000.solverstate
I1109 15:47:00.031298 18124 solver.cpp:330] Iteration 107000, Testing net (#0)
I1109 15:47:00.031298 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:47:01.311892 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:47:01.361901 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6701
I1109 15:47:01.361901 18124 solver.cpp:397]     Test net output #1: loss = 1.19647 (* 1 = 1.19647 loss)
I1109 15:47:01.412900 18124 solver.cpp:218] Iteration 107000 (14.9296 iter/s, 6.69812s/100 iters), loss = 0.873787
I1109 15:47:01.412900 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1109 15:47:01.412900 18124 solver.cpp:237]     Train net output #1: loss = 0.873787 (* 1 = 0.873787 loss)
I1109 15:47:01.412900 18124 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1109 15:47:06.751173 18124 solver.cpp:218] Iteration 107100 (18.7352 iter/s, 5.33754s/100 iters), loss = 0.856975
I1109 15:47:06.751672 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:47:06.751672 18124 solver.cpp:237]     Train net output #1: loss = 0.856975 (* 1 = 0.856975 loss)
I1109 15:47:06.751672 18124 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1109 15:47:12.082219 18124 solver.cpp:218] Iteration 107200 (18.7614 iter/s, 5.33011s/100 iters), loss = 0.665499
I1109 15:47:12.082219 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1109 15:47:12.082219 18124 solver.cpp:237]     Train net output #1: loss = 0.665499 (* 1 = 0.665499 loss)
I1109 15:47:12.082219 18124 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1109 15:47:17.440876 18124 solver.cpp:218] Iteration 107300 (18.6628 iter/s, 5.35826s/100 iters), loss = 0.824047
I1109 15:47:17.440876 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:47:17.440876 18124 solver.cpp:237]     Train net output #1: loss = 0.824047 (* 1 = 0.824047 loss)
I1109 15:47:17.440876 18124 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1109 15:47:22.784247 18124 solver.cpp:218] Iteration 107400 (18.7171 iter/s, 5.3427s/100 iters), loss = 0.962896
I1109 15:47:22.784247 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:47:22.784247 18124 solver.cpp:237]     Train net output #1: loss = 0.962896 (* 1 = 0.962896 loss)
I1109 15:47:22.784247 18124 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1109 15:47:27.877696  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:47:28.092706 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_107500.caffemodel
I1109 15:47:28.106706 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_107500.solverstate
I1109 15:47:28.110707 18124 solver.cpp:330] Iteration 107500, Testing net (#0)
I1109 15:47:28.110707 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:47:29.396842 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:47:29.447842 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6706
I1109 15:47:29.447842 18124 solver.cpp:397]     Test net output #1: loss = 1.20161 (* 1 = 1.20161 loss)
I1109 15:47:29.497850 18124 solver.cpp:218] Iteration 107500 (14.8945 iter/s, 6.71389s/100 iters), loss = 0.798952
I1109 15:47:29.497850 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:47:29.497850 18124 solver.cpp:237]     Train net output #1: loss = 0.798952 (* 1 = 0.798952 loss)
I1109 15:47:29.497850 18124 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1109 15:47:34.865283 18124 solver.cpp:218] Iteration 107600 (18.6349 iter/s, 5.36628s/100 iters), loss = 0.916035
I1109 15:47:34.865283 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:47:34.865283 18124 solver.cpp:237]     Train net output #1: loss = 0.916035 (* 1 = 0.916035 loss)
I1109 15:47:34.865283 18124 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1109 15:47:40.224097 18124 solver.cpp:218] Iteration 107700 (18.6615 iter/s, 5.35862s/100 iters), loss = 0.705698
I1109 15:47:40.224097 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:47:40.224097 18124 solver.cpp:237]     Train net output #1: loss = 0.705698 (* 1 = 0.705698 loss)
I1109 15:47:40.224097 18124 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1109 15:47:45.575959 18124 solver.cpp:218] Iteration 107800 (18.6855 iter/s, 5.35173s/100 iters), loss = 0.896295
I1109 15:47:45.575959 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:47:45.575959 18124 solver.cpp:237]     Train net output #1: loss = 0.896295 (* 1 = 0.896295 loss)
I1109 15:47:45.575959 18124 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1109 15:47:50.970767 18124 solver.cpp:218] Iteration 107900 (18.5375 iter/s, 5.39447s/100 iters), loss = 1.0255
I1109 15:47:50.970767 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.62
I1109 15:47:50.970767 18124 solver.cpp:237]     Train net output #1: loss = 1.0255 (* 1 = 1.0255 loss)
I1109 15:47:50.970767 18124 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1109 15:47:56.104140  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:47:56.322755 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_108000.caffemodel
I1109 15:47:56.341842 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_108000.solverstate
I1109 15:47:56.345844 18124 solver.cpp:330] Iteration 108000, Testing net (#0)
I1109 15:47:56.345844 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:47:57.642457 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:47:57.693456 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6667
I1109 15:47:57.693456 18124 solver.cpp:397]     Test net output #1: loss = 1.20517 (* 1 = 1.20517 loss)
I1109 15:47:57.744472 18124 solver.cpp:218] Iteration 108000 (14.7654 iter/s, 6.77257s/100 iters), loss = 0.775683
I1109 15:47:57.744472 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:47:57.744472 18124 solver.cpp:237]     Train net output #1: loss = 0.775683 (* 1 = 0.775683 loss)
I1109 15:47:57.744472 18124 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1109 15:48:03.137966 18124 solver.cpp:218] Iteration 108100 (18.5426 iter/s, 5.393s/100 iters), loss = 0.849695
I1109 15:48:03.137966 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:48:03.137966 18124 solver.cpp:237]     Train net output #1: loss = 0.849695 (* 1 = 0.849695 loss)
I1109 15:48:03.137966 18124 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1109 15:48:08.508942 18124 solver.cpp:218] Iteration 108200 (18.6202 iter/s, 5.3705s/100 iters), loss = 0.729922
I1109 15:48:08.508942 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:48:08.508942 18124 solver.cpp:237]     Train net output #1: loss = 0.729922 (* 1 = 0.729922 loss)
I1109 15:48:08.508942 18124 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1109 15:48:13.889571 18124 solver.cpp:218] Iteration 108300 (18.584 iter/s, 5.38098s/100 iters), loss = 1.016
I1109 15:48:13.890573 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1109 15:48:13.890573 18124 solver.cpp:237]     Train net output #1: loss = 1.016 (* 1 = 1.016 loss)
I1109 15:48:13.890573 18124 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1109 15:48:19.275496 18124 solver.cpp:218] Iteration 108400 (18.5709 iter/s, 5.38476s/100 iters), loss = 0.902971
I1109 15:48:19.275496 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:48:19.275496 18124 solver.cpp:237]     Train net output #1: loss = 0.902971 (* 1 = 0.902971 loss)
I1109 15:48:19.275496 18124 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1109 15:48:24.374025  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:48:24.585026 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_108500.caffemodel
I1109 15:48:24.598026 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_108500.solverstate
I1109 15:48:24.602030 18124 solver.cpp:330] Iteration 108500, Testing net (#0)
I1109 15:48:24.602030 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:48:25.894033 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:48:25.945034 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6702
I1109 15:48:25.945034 18124 solver.cpp:397]     Test net output #1: loss = 1.19706 (* 1 = 1.19706 loss)
I1109 15:48:25.995038 18124 solver.cpp:218] Iteration 108500 (14.8817 iter/s, 6.71966s/100 iters), loss = 0.877097
I1109 15:48:25.996037 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:48:25.996037 18124 solver.cpp:237]     Train net output #1: loss = 0.877097 (* 1 = 0.877097 loss)
I1109 15:48:25.996037 18124 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1109 15:48:31.384871 18124 solver.cpp:218] Iteration 108600 (18.5572 iter/s, 5.38873s/100 iters), loss = 0.846046
I1109 15:48:31.384871 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:48:31.384871 18124 solver.cpp:237]     Train net output #1: loss = 0.846046 (* 1 = 0.846046 loss)
I1109 15:48:31.384871 18124 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1109 15:48:36.792991 18124 solver.cpp:218] Iteration 108700 (18.4903 iter/s, 5.40824s/100 iters), loss = 0.696175
I1109 15:48:36.792991 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:48:36.792991 18124 solver.cpp:237]     Train net output #1: loss = 0.696175 (* 1 = 0.696175 loss)
I1109 15:48:36.792991 18124 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1109 15:48:42.169517 18124 solver.cpp:218] Iteration 108800 (18.6008 iter/s, 5.37611s/100 iters), loss = 0.849103
I1109 15:48:42.169517 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:48:42.169517 18124 solver.cpp:237]     Train net output #1: loss = 0.849103 (* 1 = 0.849103 loss)
I1109 15:48:42.169517 18124 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1109 15:48:47.582082 18124 solver.cpp:218] Iteration 108900 (18.4797 iter/s, 5.41134s/100 iters), loss = 0.986068
I1109 15:48:47.582082 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.64
I1109 15:48:47.582082 18124 solver.cpp:237]     Train net output #1: loss = 0.986068 (* 1 = 0.986068 loss)
I1109 15:48:47.582082 18124 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1109 15:48:52.672468  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:48:52.882980 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_109000.caffemodel
I1109 15:48:52.896484 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_109000.solverstate
I1109 15:48:52.900485 18124 solver.cpp:330] Iteration 109000, Testing net (#0)
I1109 15:48:52.900485 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:48:54.181561 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:48:54.232566 18124 solver.cpp:397]     Test net output #0: accuracy = 0.669
I1109 15:48:54.232566 18124 solver.cpp:397]     Test net output #1: loss = 1.19156 (* 1 = 1.19156 loss)
I1109 15:48:54.284582 18124 solver.cpp:218] Iteration 109000 (14.9202 iter/s, 6.70232s/100 iters), loss = 0.740606
I1109 15:48:54.284582 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1109 15:48:54.284582 18124 solver.cpp:237]     Train net output #1: loss = 0.740606 (* 1 = 0.740606 loss)
I1109 15:48:54.284582 18124 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1109 15:48:59.669960 18124 solver.cpp:218] Iteration 109100 (18.5688 iter/s, 5.38538s/100 iters), loss = 0.788008
I1109 15:48:59.669960 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:48:59.669960 18124 solver.cpp:237]     Train net output #1: loss = 0.788008 (* 1 = 0.788008 loss)
I1109 15:48:59.669960 18124 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1109 15:49:05.056666 18124 solver.cpp:218] Iteration 109200 (18.5666 iter/s, 5.38601s/100 iters), loss = 0.755064
I1109 15:49:05.056666 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:49:05.056666 18124 solver.cpp:237]     Train net output #1: loss = 0.755064 (* 1 = 0.755064 loss)
I1109 15:49:05.056666 18124 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1109 15:49:10.515738 18124 solver.cpp:218] Iteration 109300 (18.3209 iter/s, 5.45824s/100 iters), loss = 1.01792
I1109 15:49:10.515738 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:49:10.515738 18124 solver.cpp:237]     Train net output #1: loss = 1.01792 (* 1 = 1.01792 loss)
I1109 15:49:10.515738 18124 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1109 15:49:15.922744 18124 solver.cpp:218] Iteration 109400 (18.4945 iter/s, 5.40701s/100 iters), loss = 0.95425
I1109 15:49:15.922744 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:49:15.922744 18124 solver.cpp:237]     Train net output #1: loss = 0.95425 (* 1 = 0.95425 loss)
I1109 15:49:15.922744 18124 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1109 15:49:20.989922  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:49:21.199939 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_109500.caffemodel
I1109 15:49:21.214951 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_109500.solverstate
I1109 15:49:21.218948 18124 solver.cpp:330] Iteration 109500, Testing net (#0)
I1109 15:49:21.218948 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:49:22.496068 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:49:22.546070 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6673
I1109 15:49:22.546070 18124 solver.cpp:397]     Test net output #1: loss = 1.21694 (* 1 = 1.21694 loss)
I1109 15:49:22.598073 18124 solver.cpp:218] Iteration 109500 (14.9826 iter/s, 6.67443s/100 iters), loss = 0.868943
I1109 15:49:22.598073 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:49:22.598073 18124 solver.cpp:237]     Train net output #1: loss = 0.868943 (* 1 = 0.868943 loss)
I1109 15:49:22.598073 18124 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1109 15:49:28.031597 18124 solver.cpp:218] Iteration 109600 (18.4043 iter/s, 5.43351s/100 iters), loss = 0.820094
I1109 15:49:28.031597 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:49:28.031597 18124 solver.cpp:237]     Train net output #1: loss = 0.820094 (* 1 = 0.820094 loss)
I1109 15:49:28.031597 18124 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1109 15:49:33.601655 18124 solver.cpp:218] Iteration 109700 (17.9557 iter/s, 5.56928s/100 iters), loss = 0.671766
I1109 15:49:33.601655 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1109 15:49:33.601655 18124 solver.cpp:237]     Train net output #1: loss = 0.671766 (* 1 = 0.671766 loss)
I1109 15:49:33.601655 18124 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1109 15:49:39.184715 18124 solver.cpp:218] Iteration 109800 (17.9127 iter/s, 5.58264s/100 iters), loss = 0.976268
I1109 15:49:39.184715 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:49:39.184715 18124 solver.cpp:237]     Train net output #1: loss = 0.976268 (* 1 = 0.976268 loss)
I1109 15:49:39.184715 18124 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1109 15:49:44.751379 18124 solver.cpp:218] Iteration 109900 (17.9658 iter/s, 5.56614s/100 iters), loss = 0.941921
I1109 15:49:44.751379 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:49:44.751379 18124 solver.cpp:237]     Train net output #1: loss = 0.941921 (* 1 = 0.941921 loss)
I1109 15:49:44.751379 18124 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1109 15:49:50.036823  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:49:50.254839 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_110000.caffemodel
I1109 15:49:50.269840 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_110000.solverstate
I1109 15:49:50.274842 18124 solver.cpp:330] Iteration 110000, Testing net (#0)
I1109 15:49:50.274842 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:49:51.593956 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:49:51.645974 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6693
I1109 15:49:51.645974 18124 solver.cpp:397]     Test net output #1: loss = 1.20166 (* 1 = 1.20166 loss)
I1109 15:49:51.698974 18124 solver.cpp:218] Iteration 110000 (14.3941 iter/s, 6.94728s/100 iters), loss = 0.85313
I1109 15:49:51.699475 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:49:51.699475 18124 solver.cpp:237]     Train net output #1: loss = 0.85313 (* 1 = 0.85313 loss)
I1109 15:49:51.699475 18124 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1109 15:49:57.187444 18124 solver.cpp:218] Iteration 110100 (18.223 iter/s, 5.48758s/100 iters), loss = 0.806028
I1109 15:49:57.187444 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:49:57.187444 18124 solver.cpp:237]     Train net output #1: loss = 0.806028 (* 1 = 0.806028 loss)
I1109 15:49:57.187444 18124 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1109 15:50:02.602401 18124 solver.cpp:218] Iteration 110200 (18.4685 iter/s, 5.41464s/100 iters), loss = 0.749443
I1109 15:50:02.602401 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1109 15:50:02.602401 18124 solver.cpp:237]     Train net output #1: loss = 0.749443 (* 1 = 0.749443 loss)
I1109 15:50:02.602401 18124 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1109 15:50:08.044209 18124 solver.cpp:218] Iteration 110300 (18.3779 iter/s, 5.44131s/100 iters), loss = 0.990453
I1109 15:50:08.044209 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:50:08.044209 18124 solver.cpp:237]     Train net output #1: loss = 0.990453 (* 1 = 0.990453 loss)
I1109 15:50:08.044209 18124 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1109 15:50:13.483358 18124 solver.cpp:218] Iteration 110400 (18.3873 iter/s, 5.43855s/100 iters), loss = 0.976899
I1109 15:50:13.483358 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1109 15:50:13.483358 18124 solver.cpp:237]     Train net output #1: loss = 0.976899 (* 1 = 0.976899 loss)
I1109 15:50:13.483358 18124 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1109 15:50:18.678324  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:50:18.895545 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_110500.caffemodel
I1109 15:50:18.910542 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_110500.solverstate
I1109 15:50:18.914542 18124 solver.cpp:330] Iteration 110500, Testing net (#0)
I1109 15:50:18.914542 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:50:20.232784 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:50:20.284303 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6692
I1109 15:50:20.284303 18124 solver.cpp:397]     Test net output #1: loss = 1.20086 (* 1 = 1.20086 loss)
I1109 15:50:20.336333 18124 solver.cpp:218] Iteration 110500 (14.5927 iter/s, 6.85275s/100 iters), loss = 0.761527
I1109 15:50:20.336333 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1109 15:50:20.336333 18124 solver.cpp:237]     Train net output #1: loss = 0.761527 (* 1 = 0.761527 loss)
I1109 15:50:20.336333 18124 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1109 15:50:25.852617 18124 solver.cpp:218] Iteration 110600 (18.1302 iter/s, 5.51567s/100 iters), loss = 0.866052
I1109 15:50:25.852617 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:50:25.852617 18124 solver.cpp:237]     Train net output #1: loss = 0.866052 (* 1 = 0.866052 loss)
I1109 15:50:25.852617 18124 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1109 15:50:31.362788 18124 solver.cpp:218] Iteration 110700 (18.1479 iter/s, 5.51028s/100 iters), loss = 0.744189
I1109 15:50:31.362788 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1109 15:50:31.362788 18124 solver.cpp:237]     Train net output #1: loss = 0.744189 (* 1 = 0.744189 loss)
I1109 15:50:31.362788 18124 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1109 15:50:36.870949 18124 solver.cpp:218] Iteration 110800 (18.156 iter/s, 5.50782s/100 iters), loss = 0.970861
I1109 15:50:36.870949 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:50:36.870949 18124 solver.cpp:237]     Train net output #1: loss = 0.970861 (* 1 = 0.970861 loss)
I1109 15:50:36.871949 18124 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1109 15:50:42.357067 18124 solver.cpp:218] Iteration 110900 (18.2297 iter/s, 5.48556s/100 iters), loss = 1.05039
I1109 15:50:42.357067 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:50:42.357067 18124 solver.cpp:237]     Train net output #1: loss = 1.05039 (* 1 = 1.05039 loss)
I1109 15:50:42.357067 18124 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1109 15:50:47.583014  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:50:47.800048 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_111000.caffemodel
I1109 15:50:47.815029 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_111000.solverstate
I1109 15:50:47.819043 18124 solver.cpp:330] Iteration 111000, Testing net (#0)
I1109 15:50:47.819043 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:50:49.132166 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:50:49.184181 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6711
I1109 15:50:49.184181 18124 solver.cpp:397]     Test net output #1: loss = 1.20659 (* 1 = 1.20659 loss)
I1109 15:50:49.237174 18124 solver.cpp:218] Iteration 111000 (14.5372 iter/s, 6.87889s/100 iters), loss = 0.797244
I1109 15:50:49.237174 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:50:49.237174 18124 solver.cpp:237]     Train net output #1: loss = 0.797244 (* 1 = 0.797244 loss)
I1109 15:50:49.237174 18124 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1109 15:50:54.720628 18124 solver.cpp:218] Iteration 111100 (18.2377 iter/s, 5.48315s/100 iters), loss = 0.905
I1109 15:50:54.720628 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:50:54.720628 18124 solver.cpp:237]     Train net output #1: loss = 0.905 (* 1 = 0.905 loss)
I1109 15:50:54.720628 18124 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1109 15:51:00.206035 18124 solver.cpp:218] Iteration 111200 (18.2296 iter/s, 5.48558s/100 iters), loss = 0.655116
I1109 15:51:00.207048 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1109 15:51:00.207048 18124 solver.cpp:237]     Train net output #1: loss = 0.655116 (* 1 = 0.655116 loss)
I1109 15:51:00.207048 18124 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1109 15:51:05.695492 18124 solver.cpp:218] Iteration 111300 (18.221 iter/s, 5.48819s/100 iters), loss = 0.883052
I1109 15:51:05.695492 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:51:05.695492 18124 solver.cpp:237]     Train net output #1: loss = 0.883052 (* 1 = 0.883052 loss)
I1109 15:51:05.695492 18124 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1109 15:51:11.188611 18124 solver.cpp:218] Iteration 111400 (18.2061 iter/s, 5.49267s/100 iters), loss = 0.957629
I1109 15:51:11.188611 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:51:11.188611 18124 solver.cpp:237]     Train net output #1: loss = 0.957629 (* 1 = 0.957629 loss)
I1109 15:51:11.188611 18124 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1109 15:51:16.406236  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:51:16.622251 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_111500.caffemodel
I1109 15:51:16.636250 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_111500.solverstate
I1109 15:51:16.640249 18124 solver.cpp:330] Iteration 111500, Testing net (#0)
I1109 15:51:16.640249 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:51:17.954361 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:51:18.005364 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6663
I1109 15:51:18.005364 18124 solver.cpp:397]     Test net output #1: loss = 1.22513 (* 1 = 1.22513 loss)
I1109 15:51:18.058369 18124 solver.cpp:218] Iteration 111500 (14.5569 iter/s, 6.86959s/100 iters), loss = 0.824881
I1109 15:51:18.058369 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:51:18.058369 18124 solver.cpp:237]     Train net output #1: loss = 0.824881 (* 1 = 0.824881 loss)
I1109 15:51:18.058369 18124 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1109 15:51:23.470970 18124 solver.cpp:218] Iteration 111600 (18.4775 iter/s, 5.41198s/100 iters), loss = 0.923733
I1109 15:51:23.470970 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1109 15:51:23.470970 18124 solver.cpp:237]     Train net output #1: loss = 0.923733 (* 1 = 0.923733 loss)
I1109 15:51:23.470970 18124 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1109 15:51:28.905660 18124 solver.cpp:218] Iteration 111700 (18.4018 iter/s, 5.43424s/100 iters), loss = 0.69562
I1109 15:51:28.905660 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:51:28.905660 18124 solver.cpp:237]     Train net output #1: loss = 0.69562 (* 1 = 0.69562 loss)
I1109 15:51:28.905660 18124 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1109 15:51:34.306057 18124 solver.cpp:218] Iteration 111800 (18.5177 iter/s, 5.40025s/100 iters), loss = 0.757516
I1109 15:51:34.307057 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:51:34.307057 18124 solver.cpp:237]     Train net output #1: loss = 0.757516 (* 1 = 0.757516 loss)
I1109 15:51:34.307057 18124 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1109 15:51:39.809104 18124 solver.cpp:218] Iteration 111900 (18.1753 iter/s, 5.50198s/100 iters), loss = 0.981502
I1109 15:51:39.809104 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:51:39.809104 18124 solver.cpp:237]     Train net output #1: loss = 0.981502 (* 1 = 0.981502 loss)
I1109 15:51:39.809104 18124 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1109 15:51:45.057750  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:51:45.273772 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_112000.caffemodel
I1109 15:51:45.287772 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_112000.solverstate
I1109 15:51:45.291772 18124 solver.cpp:330] Iteration 112000, Testing net (#0)
I1109 15:51:45.291772 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:51:46.610939 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:51:46.662945 18124 solver.cpp:397]     Test net output #0: accuracy = 0.67
I1109 15:51:46.662945 18124 solver.cpp:397]     Test net output #1: loss = 1.21695 (* 1 = 1.21695 loss)
I1109 15:51:46.715950 18124 solver.cpp:218] Iteration 112000 (14.4792 iter/s, 6.90646s/100 iters), loss = 0.793553
I1109 15:51:46.715950 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:51:46.715950 18124 solver.cpp:237]     Train net output #1: loss = 0.793553 (* 1 = 0.793553 loss)
I1109 15:51:46.715950 18124 sgd_solver.cpp:105] Iteration 112000, lr = 0.001
I1109 15:51:52.219987 18124 solver.cpp:218] Iteration 112100 (18.1712 iter/s, 5.50322s/100 iters), loss = 0.890525
I1109 15:51:52.219987 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:51:52.219987 18124 solver.cpp:237]     Train net output #1: loss = 0.890525 (* 1 = 0.890525 loss)
I1109 15:51:52.219987 18124 sgd_solver.cpp:105] Iteration 112100, lr = 0.001
I1109 15:51:57.721642 18124 solver.cpp:218] Iteration 112200 (18.176 iter/s, 5.50177s/100 iters), loss = 0.63645
I1109 15:51:57.721642 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1109 15:51:57.721642 18124 solver.cpp:237]     Train net output #1: loss = 0.63645 (* 1 = 0.63645 loss)
I1109 15:51:57.721642 18124 sgd_solver.cpp:105] Iteration 112200, lr = 0.001
I1109 15:52:03.219200 18124 solver.cpp:218] Iteration 112300 (18.192 iter/s, 5.49692s/100 iters), loss = 0.817548
I1109 15:52:03.219200 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:52:03.219200 18124 solver.cpp:237]     Train net output #1: loss = 0.817548 (* 1 = 0.817548 loss)
I1109 15:52:03.219200 18124 sgd_solver.cpp:105] Iteration 112300, lr = 0.001
I1109 15:52:08.730674 18124 solver.cpp:218] Iteration 112400 (18.1457 iter/s, 5.51096s/100 iters), loss = 1.01665
I1109 15:52:08.730674 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:52:08.730674 18124 solver.cpp:237]     Train net output #1: loss = 1.01665 (* 1 = 1.01665 loss)
I1109 15:52:08.730674 18124 sgd_solver.cpp:105] Iteration 112400, lr = 0.001
I1109 15:52:13.964292  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:52:14.181311 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_112500.caffemodel
I1109 15:52:14.196308 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_112500.solverstate
I1109 15:52:14.200304 18124 solver.cpp:330] Iteration 112500, Testing net (#0)
I1109 15:52:14.201304 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:52:15.514415 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:52:15.565917 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6691
I1109 15:52:15.565917 18124 solver.cpp:397]     Test net output #1: loss = 1.20356 (* 1 = 1.20356 loss)
I1109 15:52:15.618419 18124 solver.cpp:218] Iteration 112500 (14.519 iter/s, 6.88751s/100 iters), loss = 0.922616
I1109 15:52:15.618419 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:52:15.618419 18124 solver.cpp:237]     Train net output #1: loss = 0.922616 (* 1 = 0.922616 loss)
I1109 15:52:15.618419 18124 sgd_solver.cpp:105] Iteration 112500, lr = 0.001
I1109 15:52:21.134095 18124 solver.cpp:218] Iteration 112600 (18.133 iter/s, 5.51481s/100 iters), loss = 0.790628
I1109 15:52:21.134095 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:52:21.134095 18124 solver.cpp:237]     Train net output #1: loss = 0.790628 (* 1 = 0.790628 loss)
I1109 15:52:21.134095 18124 sgd_solver.cpp:105] Iteration 112600, lr = 0.001
I1109 15:52:26.642539 18124 solver.cpp:218] Iteration 112700 (18.1567 iter/s, 5.5076s/100 iters), loss = 0.68605
I1109 15:52:26.642539 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:52:26.642539 18124 solver.cpp:237]     Train net output #1: loss = 0.68605 (* 1 = 0.68605 loss)
I1109 15:52:26.642539 18124 sgd_solver.cpp:105] Iteration 112700, lr = 0.001
I1109 15:52:32.154052 18124 solver.cpp:218] Iteration 112800 (18.1446 iter/s, 5.51129s/100 iters), loss = 0.866497
I1109 15:52:32.154052 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:52:32.154052 18124 solver.cpp:237]     Train net output #1: loss = 0.866497 (* 1 = 0.866497 loss)
I1109 15:52:32.154052 18124 sgd_solver.cpp:105] Iteration 112800, lr = 0.001
I1109 15:52:37.665035 18124 solver.cpp:218] Iteration 112900 (18.1474 iter/s, 5.51042s/100 iters), loss = 0.825284
I1109 15:52:37.665035 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:52:37.665035 18124 solver.cpp:237]     Train net output #1: loss = 0.825284 (* 1 = 0.825284 loss)
I1109 15:52:37.665035 18124 sgd_solver.cpp:105] Iteration 112900, lr = 0.001
I1109 15:52:42.905982  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:52:43.124004 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_113000.caffemodel
I1109 15:52:43.138005 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_113000.solverstate
I1109 15:52:43.142005 18124 solver.cpp:330] Iteration 113000, Testing net (#0)
I1109 15:52:43.142005 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:52:44.457090 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:52:44.508095 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6682
I1109 15:52:44.508095 18124 solver.cpp:397]     Test net output #1: loss = 1.22485 (* 1 = 1.22485 loss)
I1109 15:52:44.561096 18124 solver.cpp:218] Iteration 113000 (14.5023 iter/s, 6.89544s/100 iters), loss = 0.671986
I1109 15:52:44.561096 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1109 15:52:44.561096 18124 solver.cpp:237]     Train net output #1: loss = 0.671986 (* 1 = 0.671986 loss)
I1109 15:52:44.561096 18124 sgd_solver.cpp:105] Iteration 113000, lr = 0.001
I1109 15:52:50.052551 18124 solver.cpp:218] Iteration 113100 (18.2103 iter/s, 5.49139s/100 iters), loss = 0.757409
I1109 15:52:50.052551 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1109 15:52:50.052551 18124 solver.cpp:237]     Train net output #1: loss = 0.757409 (* 1 = 0.757409 loss)
I1109 15:52:50.052551 18124 sgd_solver.cpp:105] Iteration 113100, lr = 0.001
I1109 15:52:55.544015 18124 solver.cpp:218] Iteration 113200 (18.214 iter/s, 5.49028s/100 iters), loss = 0.668734
I1109 15:52:55.544015 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1109 15:52:55.544015 18124 solver.cpp:237]     Train net output #1: loss = 0.668734 (* 1 = 0.668734 loss)
I1109 15:52:55.544015 18124 sgd_solver.cpp:105] Iteration 113200, lr = 0.001
I1109 15:53:01.074493 18124 solver.cpp:218] Iteration 113300 (18.0814 iter/s, 5.53054s/100 iters), loss = 0.900686
I1109 15:53:01.074493 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:53:01.074493 18124 solver.cpp:237]     Train net output #1: loss = 0.900686 (* 1 = 0.900686 loss)
I1109 15:53:01.074493 18124 sgd_solver.cpp:105] Iteration 113300, lr = 0.001
I1109 15:53:06.580946 18124 solver.cpp:218] Iteration 113400 (18.1612 iter/s, 5.50623s/100 iters), loss = 0.944712
I1109 15:53:06.581948 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:53:06.581948 18124 solver.cpp:237]     Train net output #1: loss = 0.944712 (* 1 = 0.944712 loss)
I1109 15:53:06.581948 18124 sgd_solver.cpp:105] Iteration 113400, lr = 0.001
I1109 15:53:11.823535  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:53:12.041590 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_113500.caffemodel
I1109 15:53:12.055590 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_113500.solverstate
I1109 15:53:12.059609 18124 solver.cpp:330] Iteration 113500, Testing net (#0)
I1109 15:53:12.059609 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:53:13.373085 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:53:13.425088 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6711
I1109 15:53:13.425088 18124 solver.cpp:397]     Test net output #1: loss = 1.21217 (* 1 = 1.21217 loss)
I1109 15:53:13.477103 18124 solver.cpp:218] Iteration 113500 (14.5033 iter/s, 6.89498s/100 iters), loss = 0.91274
I1109 15:53:13.477103 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:53:13.477103 18124 solver.cpp:237]     Train net output #1: loss = 0.91274 (* 1 = 0.91274 loss)
I1109 15:53:13.477103 18124 sgd_solver.cpp:105] Iteration 113500, lr = 0.001
I1109 15:53:18.973918 18124 solver.cpp:218] Iteration 113600 (18.1939 iter/s, 5.49633s/100 iters), loss = 0.777718
I1109 15:53:18.973918 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1109 15:53:18.973918 18124 solver.cpp:237]     Train net output #1: loss = 0.777718 (* 1 = 0.777718 loss)
I1109 15:53:18.973918 18124 sgd_solver.cpp:105] Iteration 113600, lr = 0.001
I1109 15:53:24.487845 18124 solver.cpp:218] Iteration 113700 (18.1367 iter/s, 5.51368s/100 iters), loss = 0.740082
I1109 15:53:24.487845 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:53:24.487845 18124 solver.cpp:237]     Train net output #1: loss = 0.740082 (* 1 = 0.740082 loss)
I1109 15:53:24.487845 18124 sgd_solver.cpp:105] Iteration 113700, lr = 0.001
I1109 15:53:29.992619 18124 solver.cpp:218] Iteration 113800 (18.1665 iter/s, 5.50463s/100 iters), loss = 0.853473
I1109 15:53:29.992619 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:53:29.992619 18124 solver.cpp:237]     Train net output #1: loss = 0.853473 (* 1 = 0.853473 loss)
I1109 15:53:29.992619 18124 sgd_solver.cpp:105] Iteration 113800, lr = 0.001
I1109 15:53:35.495100 18124 solver.cpp:218] Iteration 113900 (18.1777 iter/s, 5.50124s/100 iters), loss = 0.908484
I1109 15:53:35.495100 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:53:35.495100 18124 solver.cpp:237]     Train net output #1: loss = 0.908484 (* 1 = 0.908484 loss)
I1109 15:53:35.495100 18124 sgd_solver.cpp:105] Iteration 113900, lr = 0.001
I1109 15:53:40.724630  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:53:40.941642 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_114000.caffemodel
I1109 15:53:40.956641 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_114000.solverstate
I1109 15:53:40.961642 18124 solver.cpp:330] Iteration 114000, Testing net (#0)
I1109 15:53:40.961642 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:53:42.278260 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:53:42.329764 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6679
I1109 15:53:42.329764 18124 solver.cpp:397]     Test net output #1: loss = 1.2175 (* 1 = 1.2175 loss)
I1109 15:53:42.383266 18124 solver.cpp:218] Iteration 114000 (14.5181 iter/s, 6.88797s/100 iters), loss = 0.944978
I1109 15:53:42.383266 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:53:42.383266 18124 solver.cpp:237]     Train net output #1: loss = 0.944978 (* 1 = 0.944978 loss)
I1109 15:53:42.383266 18124 sgd_solver.cpp:105] Iteration 114000, lr = 0.001
I1109 15:53:47.895628 18124 solver.cpp:218] Iteration 114100 (18.1438 iter/s, 5.51152s/100 iters), loss = 0.939218
I1109 15:53:47.895628 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:53:47.895628 18124 solver.cpp:237]     Train net output #1: loss = 0.939218 (* 1 = 0.939218 loss)
I1109 15:53:47.895628 18124 sgd_solver.cpp:105] Iteration 114100, lr = 0.001
I1109 15:53:53.304282 18124 solver.cpp:218] Iteration 114200 (18.4884 iter/s, 5.40879s/100 iters), loss = 0.773035
I1109 15:53:53.304282 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:53:53.304282 18124 solver.cpp:237]     Train net output #1: loss = 0.773035 (* 1 = 0.773035 loss)
I1109 15:53:53.304282 18124 sgd_solver.cpp:105] Iteration 114200, lr = 0.001
I1109 15:53:58.694692 18124 solver.cpp:218] Iteration 114300 (18.5519 iter/s, 5.39028s/100 iters), loss = 0.974813
I1109 15:53:58.695677 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:53:58.695677 18124 solver.cpp:237]     Train net output #1: loss = 0.974813 (* 1 = 0.974813 loss)
I1109 15:53:58.695677 18124 sgd_solver.cpp:105] Iteration 114300, lr = 0.001
I1109 15:54:04.082118 18124 solver.cpp:218] Iteration 114400 (18.5663 iter/s, 5.3861s/100 iters), loss = 0.882952
I1109 15:54:04.082118 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:54:04.082118 18124 solver.cpp:237]     Train net output #1: loss = 0.882952 (* 1 = 0.882952 loss)
I1109 15:54:04.082118 18124 sgd_solver.cpp:105] Iteration 114400, lr = 0.001
I1109 15:54:09.203630  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:54:09.414641 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_114500.caffemodel
I1109 15:54:09.428642 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_114500.solverstate
I1109 15:54:09.433643 18124 solver.cpp:330] Iteration 114500, Testing net (#0)
I1109 15:54:09.433643 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:54:10.730756 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:54:10.781760 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6656
I1109 15:54:10.781760 18124 solver.cpp:397]     Test net output #1: loss = 1.22118 (* 1 = 1.22118 loss)
I1109 15:54:10.833763 18124 solver.cpp:218] Iteration 114500 (14.8117 iter/s, 6.75144s/100 iters), loss = 0.894616
I1109 15:54:10.833763 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:54:10.833763 18124 solver.cpp:237]     Train net output #1: loss = 0.894616 (* 1 = 0.894616 loss)
I1109 15:54:10.833763 18124 sgd_solver.cpp:105] Iteration 114500, lr = 0.001
I1109 15:54:16.233280 18124 solver.cpp:218] Iteration 114600 (18.5228 iter/s, 5.39875s/100 iters), loss = 0.837776
I1109 15:54:16.233280 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:54:16.233280 18124 solver.cpp:237]     Train net output #1: loss = 0.837776 (* 1 = 0.837776 loss)
I1109 15:54:16.233280 18124 sgd_solver.cpp:105] Iteration 114600, lr = 0.001
I1109 15:54:21.631471 18124 solver.cpp:218] Iteration 114700 (18.5269 iter/s, 5.39756s/100 iters), loss = 0.703106
I1109 15:54:21.631471 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:54:21.631471 18124 solver.cpp:237]     Train net output #1: loss = 0.703106 (* 1 = 0.703106 loss)
I1109 15:54:21.631471 18124 sgd_solver.cpp:105] Iteration 114700, lr = 0.001
I1109 15:54:27.028498 18124 solver.cpp:218] Iteration 114800 (18.5281 iter/s, 5.39721s/100 iters), loss = 0.877101
I1109 15:54:27.028498 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:54:27.028498 18124 solver.cpp:237]     Train net output #1: loss = 0.877101 (* 1 = 0.877101 loss)
I1109 15:54:27.028498 18124 sgd_solver.cpp:105] Iteration 114800, lr = 0.001
I1109 15:54:32.387318 18124 solver.cpp:218] Iteration 114900 (18.6631 iter/s, 5.35816s/100 iters), loss = 0.944827
I1109 15:54:32.387810 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:54:32.387810 18124 solver.cpp:237]     Train net output #1: loss = 0.944827 (* 1 = 0.944827 loss)
I1109 15:54:32.387810 18124 sgd_solver.cpp:105] Iteration 114900, lr = 0.001
I1109 15:54:37.482607  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:54:37.692137 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_115000.caffemodel
I1109 15:54:37.708122 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_115000.solverstate
I1109 15:54:37.712126 18124 solver.cpp:330] Iteration 115000, Testing net (#0)
I1109 15:54:37.712126 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:54:38.993208 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:54:39.043210 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6664
I1109 15:54:39.043210 18124 solver.cpp:397]     Test net output #1: loss = 1.24011 (* 1 = 1.24011 loss)
I1109 15:54:39.094727 18124 solver.cpp:218] Iteration 115000 (14.91 iter/s, 6.70689s/100 iters), loss = 0.850239
I1109 15:54:39.094727 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:54:39.094727 18124 solver.cpp:237]     Train net output #1: loss = 0.850239 (* 1 = 0.850239 loss)
I1109 15:54:39.094727 18124 sgd_solver.cpp:105] Iteration 115000, lr = 0.001
I1109 15:54:44.426790 18124 solver.cpp:218] Iteration 115100 (18.7539 iter/s, 5.33221s/100 iters), loss = 0.833642
I1109 15:54:44.427790 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:54:44.427790 18124 solver.cpp:237]     Train net output #1: loss = 0.833642 (* 1 = 0.833642 loss)
I1109 15:54:44.427790 18124 sgd_solver.cpp:105] Iteration 115100, lr = 0.001
I1109 15:54:49.779160 18124 solver.cpp:218] Iteration 115200 (18.6872 iter/s, 5.35125s/100 iters), loss = 0.696644
I1109 15:54:49.779160 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1109 15:54:49.779160 18124 solver.cpp:237]     Train net output #1: loss = 0.696644 (* 1 = 0.696644 loss)
I1109 15:54:49.779160 18124 sgd_solver.cpp:105] Iteration 115200, lr = 0.001
I1109 15:54:55.117539 18124 solver.cpp:218] Iteration 115300 (18.7318 iter/s, 5.33852s/100 iters), loss = 0.893357
I1109 15:54:55.117539 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:54:55.117539 18124 solver.cpp:237]     Train net output #1: loss = 0.893357 (* 1 = 0.893357 loss)
I1109 15:54:55.117539 18124 sgd_solver.cpp:105] Iteration 115300, lr = 0.001
I1109 15:55:00.460311 18124 solver.cpp:218] Iteration 115400 (18.7189 iter/s, 5.34219s/100 iters), loss = 0.824901
I1109 15:55:00.460311 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1109 15:55:00.460311 18124 solver.cpp:237]     Train net output #1: loss = 0.824901 (* 1 = 0.824901 loss)
I1109 15:55:00.460311 18124 sgd_solver.cpp:105] Iteration 115400, lr = 0.001
I1109 15:55:05.543413  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:55:05.752465 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_115500.caffemodel
I1109 15:55:05.765450 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_115500.solverstate
I1109 15:55:05.769459 18124 solver.cpp:330] Iteration 115500, Testing net (#0)
I1109 15:55:05.769459 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:55:07.049345 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:55:07.099850 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6706
I1109 15:55:07.099850 18124 solver.cpp:397]     Test net output #1: loss = 1.22169 (* 1 = 1.22169 loss)
I1109 15:55:07.150370 18124 solver.cpp:218] Iteration 115500 (14.9492 iter/s, 6.68932s/100 iters), loss = 0.763282
I1109 15:55:07.150370 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1109 15:55:07.150370 18124 solver.cpp:237]     Train net output #1: loss = 0.763282 (* 1 = 0.763282 loss)
I1109 15:55:07.150370 18124 sgd_solver.cpp:105] Iteration 115500, lr = 0.001
I1109 15:55:12.494655 18124 solver.cpp:218] Iteration 115600 (18.7131 iter/s, 5.34385s/100 iters), loss = 0.85919
I1109 15:55:12.495156 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:55:12.495156 18124 solver.cpp:237]     Train net output #1: loss = 0.85919 (* 1 = 0.85919 loss)
I1109 15:55:12.495156 18124 sgd_solver.cpp:105] Iteration 115600, lr = 0.001
I1109 15:55:17.841024 18124 solver.cpp:218] Iteration 115700 (18.7058 iter/s, 5.34594s/100 iters), loss = 0.632398
I1109 15:55:17.841024 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1109 15:55:17.841024 18124 solver.cpp:237]     Train net output #1: loss = 0.632398 (* 1 = 0.632398 loss)
I1109 15:55:17.841024 18124 sgd_solver.cpp:105] Iteration 115700, lr = 0.001
I1109 15:55:23.184528 18124 solver.cpp:218] Iteration 115800 (18.7154 iter/s, 5.3432s/100 iters), loss = 0.774377
I1109 15:55:23.184528 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1109 15:55:23.184528 18124 solver.cpp:237]     Train net output #1: loss = 0.774377 (* 1 = 0.774377 loss)
I1109 15:55:23.184528 18124 sgd_solver.cpp:105] Iteration 115800, lr = 0.001
I1109 15:55:28.541906 18124 solver.cpp:218] Iteration 115900 (18.6676 iter/s, 5.35688s/100 iters), loss = 0.973047
I1109 15:55:28.541906 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.65
I1109 15:55:28.541906 18124 solver.cpp:237]     Train net output #1: loss = 0.973047 (* 1 = 0.973047 loss)
I1109 15:55:28.541906 18124 sgd_solver.cpp:105] Iteration 115900, lr = 0.001
I1109 15:55:33.646337  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:55:33.856364 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_116000.caffemodel
I1109 15:55:33.870364 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_116000.solverstate
I1109 15:55:33.875365 18124 solver.cpp:330] Iteration 116000, Testing net (#0)
I1109 15:55:33.875365 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:55:35.162456 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:55:35.212465 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6649
I1109 15:55:35.212465 18124 solver.cpp:397]     Test net output #1: loss = 1.23021 (* 1 = 1.23021 loss)
I1109 15:55:35.263463 18124 solver.cpp:218] Iteration 116000 (14.8791 iter/s, 6.72085s/100 iters), loss = 0.7898
I1109 15:55:35.263463 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:55:35.263463 18124 solver.cpp:237]     Train net output #1: loss = 0.7898 (* 1 = 0.7898 loss)
I1109 15:55:35.263463 18124 sgd_solver.cpp:105] Iteration 116000, lr = 0.001
I1109 15:55:40.629897 18124 solver.cpp:218] Iteration 116100 (18.636 iter/s, 5.36595s/100 iters), loss = 0.825905
I1109 15:55:40.629897 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:55:40.629897 18124 solver.cpp:237]     Train net output #1: loss = 0.825905 (* 1 = 0.825905 loss)
I1109 15:55:40.629897 18124 sgd_solver.cpp:105] Iteration 116100, lr = 0.001
I1109 15:55:45.997344 18124 solver.cpp:218] Iteration 116200 (18.6326 iter/s, 5.36694s/100 iters), loss = 0.668885
I1109 15:55:45.997344 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1109 15:55:45.997344 18124 solver.cpp:237]     Train net output #1: loss = 0.668885 (* 1 = 0.668885 loss)
I1109 15:55:45.997344 18124 sgd_solver.cpp:105] Iteration 116200, lr = 0.001
I1109 15:55:51.355751 18124 solver.cpp:218] Iteration 116300 (18.664 iter/s, 5.3579s/100 iters), loss = 0.916291
I1109 15:55:51.355751 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1109 15:55:51.355751 18124 solver.cpp:237]     Train net output #1: loss = 0.916291 (* 1 = 0.916291 loss)
I1109 15:55:51.355751 18124 sgd_solver.cpp:105] Iteration 116300, lr = 0.001
I1109 15:55:56.788298 18124 solver.cpp:218] Iteration 116400 (18.4068 iter/s, 5.43279s/100 iters), loss = 0.939107
I1109 15:55:56.788298 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1109 15:55:56.788298 18124 solver.cpp:237]     Train net output #1: loss = 0.939107 (* 1 = 0.939107 loss)
I1109 15:55:56.788298 18124 sgd_solver.cpp:105] Iteration 116400, lr = 0.001
I1109 15:56:01.951347  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:56:02.163974 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_116500.caffemodel
I1109 15:56:02.180953 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_116500.solverstate
I1109 15:56:02.185961 18124 solver.cpp:330] Iteration 116500, Testing net (#0)
I1109 15:56:02.185961 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:56:03.478238 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:56:03.529237 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6688
I1109 15:56:03.529237 18124 solver.cpp:397]     Test net output #1: loss = 1.22958 (* 1 = 1.22958 loss)
I1109 15:56:03.580250 18124 solver.cpp:218] Iteration 116500 (14.7249 iter/s, 6.79122s/100 iters), loss = 0.822855
I1109 15:56:03.580250 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:56:03.580250 18124 solver.cpp:237]     Train net output #1: loss = 0.822855 (* 1 = 0.822855 loss)
I1109 15:56:03.580250 18124 sgd_solver.cpp:105] Iteration 116500, lr = 0.001
I1109 15:56:08.984159 18124 solver.cpp:218] Iteration 116600 (18.5074 iter/s, 5.40325s/100 iters), loss = 0.789271
I1109 15:56:08.984159 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:56:08.984159 18124 solver.cpp:237]     Train net output #1: loss = 0.789271 (* 1 = 0.789271 loss)
I1109 15:56:08.984159 18124 sgd_solver.cpp:105] Iteration 116600, lr = 0.001
I1109 15:56:14.386608 18124 solver.cpp:218] Iteration 116700 (18.5103 iter/s, 5.4024s/100 iters), loss = 0.74791
I1109 15:56:14.387609 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1109 15:56:14.387609 18124 solver.cpp:237]     Train net output #1: loss = 0.74791 (* 1 = 0.74791 loss)
I1109 15:56:14.387609 18124 sgd_solver.cpp:105] Iteration 116700, lr = 0.001
I1109 15:56:19.790596 18124 solver.cpp:218] Iteration 116800 (18.5077 iter/s, 5.40316s/100 iters), loss = 0.901774
I1109 15:56:19.790596 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:56:19.790596 18124 solver.cpp:237]     Train net output #1: loss = 0.901774 (* 1 = 0.901774 loss)
I1109 15:56:19.790596 18124 sgd_solver.cpp:105] Iteration 116800, lr = 0.001
I1109 15:56:25.171489 18124 solver.cpp:218] Iteration 116900 (18.5848 iter/s, 5.38074s/100 iters), loss = 0.892067
I1109 15:56:25.171489 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1109 15:56:25.171489 18124 solver.cpp:237]     Train net output #1: loss = 0.892067 (* 1 = 0.892067 loss)
I1109 15:56:25.171489 18124 sgd_solver.cpp:105] Iteration 116900, lr = 0.001
I1109 15:56:30.245357  8920 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:56:30.453897 18124 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_117000.caffemodel
I1109 15:56:30.466881 18124 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_maxdrp_overlappingpooling3x2_iter_117000.solverstate
I1109 15:56:30.470901 18124 solver.cpp:330] Iteration 117000, Testing net (#0)
I1109 15:56:30.470901 18124 net.cpp:676] Ignoring source layer accuracy_training
I1109 15:56:31.765550 11152 data_layer.cpp:73] Restarting data prefetching from start.
I1109 15:56:31.815549 18124 solver.cpp:397]     Test net output #0: accuracy = 0.6675
I1109 15:56:31.815549 18124 solver.cpp:397]     Test net output #1: loss = 1.22978 (* 1 = 1.22978 loss)
I1109 15:56:31.865578 18124 solver.cpp:218] Iteration 117000 (14.9402 iter/s, 6.69335s/100 iters), loss = 0.993718
I1109 15:56:31.865578 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:56:31.865578 18124 solver.cpp:237]     Train net output #1: loss = 0.993718 (* 1 = 0.993718 loss)
I1109 15:56:31.865578 18124 sgd_solver.cpp:105] Iteration 117000, lr = 0.001
I1109 15:56:37.290743 18124 solver.cpp:218] Iteration 117100 (18.4353 iter/s, 5.42437s/100 iters), loss = 0.693486
I1109 15:56:37.290743 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1109 15:56:37.290743 18124 solver.cpp:237]     Train net output #1: loss = 0.693486 (* 1 = 0.693486 loss)
I1109 15:56:37.290743 18124 sgd_solver.cpp:105] Iteration 117100, lr = 0.001
I1109 15:56:42.730517 18124 solver.cpp:218] Iteration 117200 (18.3826 iter/s, 5.43993s/100 iters), loss = 0.817633
I1109 15:56:42.731518 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1109 15:56:42.731518 18124 solver.cpp:237]     Train net output #1: loss = 0.817633 (* 1 = 0.817633 loss)
I1109 15:56:42.731518 18124 sgd_solver.cpp:105] Iteration 117200, lr = 0.001
I1109 15:56:48.124299 18124 solver.cpp:218] Iteration 117300 (18.5448 iter/s, 5.39235s/100 iters), loss = 0.92307
I1109 15:56:48.124299 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1109 15:56:48.124299 18124 solver.cpp:237]     Train net output #1: loss = 0.92307 (* 1 = 0.92307 loss)
I1109 15:56:48.124299 18124 sgd_solver.cpp:105] Iteration 117300, lr = 0.001
I1109 15:56:53.490861 18124 solver.cpp:218] Iteration 117400 (18.6337 iter/s, 5.36663s/100 iters), loss = 0.971716
I1109 15:56:53.490861 18124 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1109 15:56:53.490861 18124 solver.cpp:2

G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_90000.solverstate 
I1210 11:03:08.681633 13616 caffe.cpp:219] Using GPUs 0
I1210 11:03:08.863214 13616 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1210 11:03:09.180805 13616 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 11:03:09.196800 13616 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1210 11:03:09.197804 13616 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 11:03:09.198804 13616 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 11:03:09.198804 13616 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1210 11:03:09.198804 13616 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1210 11:03:09.198804 13616 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_13L_Simple_NoGrpCon_NoDrp_maxdrp_300k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 36
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 44
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 44
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 44
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 62
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 62
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 71
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 100
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1210 11:03:09.199805 13616 layer_factory.cpp:58] Creating layer cifar
I1210 11:03:09.202805 13616 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1210 11:03:09.202805 13616 net.cpp:84] Creating Layer cifar
I1210 11:03:09.202805 13616 net.cpp:380] cifar -> data
I1210 11:03:09.202805 13616 net.cpp:380] cifar -> label
I1210 11:03:09.203804 13616 data_layer.cpp:45] output data size: 100,3,32,32
I1210 11:03:09.208806 13616 net.cpp:122] Setting up cifar
I1210 11:03:09.209321 13616 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1210 11:03:09.209321 13616 net.cpp:129] Top shape: 100 (100)
I1210 11:03:09.209321 13616 net.cpp:137] Memory required for data: 1229200
I1210 11:03:09.209321 13616 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1210 11:03:09.209321 13616 net.cpp:84] Creating Layer label_cifar_1_split
I1210 11:03:09.209321 13616 net.cpp:406] label_cifar_1_split <- label
I1210 11:03:09.209321 13616 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1210 11:03:09.209321 13616 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1210 11:03:09.209321 13616 net.cpp:122] Setting up label_cifar_1_split
I1210 11:03:09.209321 13616 net.cpp:129] Top shape: 100 (100)
I1210 11:03:09.209321 13616 net.cpp:129] Top shape: 100 (100)
I1210 11:03:09.209321 13616 net.cpp:137] Memory required for data: 1230000
I1210 11:03:09.209321 13616 layer_factory.cpp:58] Creating layer conv1
I1210 11:03:09.209321 13616 net.cpp:84] Creating Layer conv1
I1210 11:03:09.209321 13616 net.cpp:406] conv1 <- data
I1210 11:03:09.209321 13616 net.cpp:380] conv1 -> conv1
I1210 11:03:09.210296  5296 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 11:03:09.451812 13616 net.cpp:122] Setting up conv1
I1210 11:03:09.451812 13616 net.cpp:129] Top shape: 100 36 32 32 (3686400)
I1210 11:03:09.451812 13616 net.cpp:137] Memory required for data: 15975600
I1210 11:03:09.451812 13616 layer_factory.cpp:58] Creating layer bn1
I1210 11:03:09.451812 13616 net.cpp:84] Creating Layer bn1
I1210 11:03:09.451812 13616 net.cpp:406] bn1 <- conv1
I1210 11:03:09.451812 13616 net.cpp:367] bn1 -> conv1 (in-place)
I1210 11:03:09.452811 13616 net.cpp:122] Setting up bn1
I1210 11:03:09.452811 13616 net.cpp:129] Top shape: 100 36 32 32 (3686400)
I1210 11:03:09.452811 13616 net.cpp:137] Memory required for data: 30721200
I1210 11:03:09.452811 13616 layer_factory.cpp:58] Creating layer scale1
I1210 11:03:09.452811 13616 net.cpp:84] Creating Layer scale1
I1210 11:03:09.452811 13616 net.cpp:406] scale1 <- conv1
I1210 11:03:09.452811 13616 net.cpp:367] scale1 -> conv1 (in-place)
I1210 11:03:09.452811 13616 layer_factory.cpp:58] Creating layer scale1
I1210 11:03:09.452811 13616 net.cpp:122] Setting up scale1
I1210 11:03:09.452811 13616 net.cpp:129] Top shape: 100 36 32 32 (3686400)
I1210 11:03:09.452811 13616 net.cpp:137] Memory required for data: 45466800
I1210 11:03:09.452811 13616 layer_factory.cpp:58] Creating layer relu1
I1210 11:03:09.452811 13616 net.cpp:84] Creating Layer relu1
I1210 11:03:09.452811 13616 net.cpp:406] relu1 <- conv1
I1210 11:03:09.452811 13616 net.cpp:367] relu1 -> conv1 (in-place)
I1210 11:03:09.452811 13616 net.cpp:122] Setting up relu1
I1210 11:03:09.452811 13616 net.cpp:129] Top shape: 100 36 32 32 (3686400)
I1210 11:03:09.452811 13616 net.cpp:137] Memory required for data: 60212400
I1210 11:03:09.452811 13616 layer_factory.cpp:58] Creating layer conv1_0
I1210 11:03:09.452811 13616 net.cpp:84] Creating Layer conv1_0
I1210 11:03:09.452811 13616 net.cpp:406] conv1_0 <- conv1
I1210 11:03:09.452811 13616 net.cpp:380] conv1_0 -> conv1_0
I1210 11:03:09.454810 13616 net.cpp:122] Setting up conv1_0
I1210 11:03:09.454810 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.454810 13616 net.cpp:137] Memory required for data: 78234800
I1210 11:03:09.454810 13616 layer_factory.cpp:58] Creating layer bn1_0
I1210 11:03:09.454810 13616 net.cpp:84] Creating Layer bn1_0
I1210 11:03:09.454810 13616 net.cpp:406] bn1_0 <- conv1_0
I1210 11:03:09.454810 13616 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1210 11:03:09.454810 13616 net.cpp:122] Setting up bn1_0
I1210 11:03:09.454810 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.454810 13616 net.cpp:137] Memory required for data: 96257200
I1210 11:03:09.454810 13616 layer_factory.cpp:58] Creating layer scale1_0
I1210 11:03:09.454810 13616 net.cpp:84] Creating Layer scale1_0
I1210 11:03:09.454810 13616 net.cpp:406] scale1_0 <- conv1_0
I1210 11:03:09.454810 13616 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1210 11:03:09.454810 13616 layer_factory.cpp:58] Creating layer scale1_0
I1210 11:03:09.454810 13616 net.cpp:122] Setting up scale1_0
I1210 11:03:09.454810 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.454810 13616 net.cpp:137] Memory required for data: 114279600
I1210 11:03:09.454810 13616 layer_factory.cpp:58] Creating layer relu1_0
I1210 11:03:09.454810 13616 net.cpp:84] Creating Layer relu1_0
I1210 11:03:09.454810 13616 net.cpp:406] relu1_0 <- conv1_0
I1210 11:03:09.454810 13616 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1210 11:03:09.455811 13616 net.cpp:122] Setting up relu1_0
I1210 11:03:09.455811 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.455811 13616 net.cpp:137] Memory required for data: 132302000
I1210 11:03:09.455811 13616 layer_factory.cpp:58] Creating layer conv2
I1210 11:03:09.455811 13616 net.cpp:84] Creating Layer conv2
I1210 11:03:09.455811 13616 net.cpp:406] conv2 <- conv1_0
I1210 11:03:09.455811 13616 net.cpp:380] conv2 -> conv2
I1210 11:03:09.456811 13616 net.cpp:122] Setting up conv2
I1210 11:03:09.456811 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.456811 13616 net.cpp:137] Memory required for data: 150324400
I1210 11:03:09.456811 13616 layer_factory.cpp:58] Creating layer bn2
I1210 11:03:09.456811 13616 net.cpp:84] Creating Layer bn2
I1210 11:03:09.456811 13616 net.cpp:406] bn2 <- conv2
I1210 11:03:09.456811 13616 net.cpp:367] bn2 -> conv2 (in-place)
I1210 11:03:09.456811 13616 net.cpp:122] Setting up bn2
I1210 11:03:09.456811 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.457811 13616 net.cpp:137] Memory required for data: 168346800
I1210 11:03:09.457811 13616 layer_factory.cpp:58] Creating layer scale2
I1210 11:03:09.457811 13616 net.cpp:84] Creating Layer scale2
I1210 11:03:09.457811 13616 net.cpp:406] scale2 <- conv2
I1210 11:03:09.457811 13616 net.cpp:367] scale2 -> conv2 (in-place)
I1210 11:03:09.457811 13616 layer_factory.cpp:58] Creating layer scale2
I1210 11:03:09.457811 13616 net.cpp:122] Setting up scale2
I1210 11:03:09.457811 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.457811 13616 net.cpp:137] Memory required for data: 186369200
I1210 11:03:09.457811 13616 layer_factory.cpp:58] Creating layer relu2
I1210 11:03:09.457811 13616 net.cpp:84] Creating Layer relu2
I1210 11:03:09.457811 13616 net.cpp:406] relu2 <- conv2
I1210 11:03:09.457811 13616 net.cpp:367] relu2 -> conv2 (in-place)
I1210 11:03:09.457811 13616 net.cpp:122] Setting up relu2
I1210 11:03:09.457811 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.457811 13616 net.cpp:137] Memory required for data: 204391600
I1210 11:03:09.457811 13616 layer_factory.cpp:58] Creating layer conv2_1
I1210 11:03:09.457811 13616 net.cpp:84] Creating Layer conv2_1
I1210 11:03:09.457811 13616 net.cpp:406] conv2_1 <- conv2
I1210 11:03:09.457811 13616 net.cpp:380] conv2_1 -> conv2_1
I1210 11:03:09.458793 13616 net.cpp:122] Setting up conv2_1
I1210 11:03:09.458793 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.458793 13616 net.cpp:137] Memory required for data: 222414000
I1210 11:03:09.458793 13616 layer_factory.cpp:58] Creating layer bn2_1
I1210 11:03:09.458793 13616 net.cpp:84] Creating Layer bn2_1
I1210 11:03:09.458793 13616 net.cpp:406] bn2_1 <- conv2_1
I1210 11:03:09.458793 13616 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1210 11:03:09.458793 13616 net.cpp:122] Setting up bn2_1
I1210 11:03:09.458793 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.458793 13616 net.cpp:137] Memory required for data: 240436400
I1210 11:03:09.458793 13616 layer_factory.cpp:58] Creating layer scale2_1
I1210 11:03:09.458793 13616 net.cpp:84] Creating Layer scale2_1
I1210 11:03:09.458793 13616 net.cpp:406] scale2_1 <- conv2_1
I1210 11:03:09.458793 13616 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1210 11:03:09.458793 13616 layer_factory.cpp:58] Creating layer scale2_1
I1210 11:03:09.458793 13616 net.cpp:122] Setting up scale2_1
I1210 11:03:09.458793 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.458793 13616 net.cpp:137] Memory required for data: 258458800
I1210 11:03:09.458793 13616 layer_factory.cpp:58] Creating layer relu2_1
I1210 11:03:09.458793 13616 net.cpp:84] Creating Layer relu2_1
I1210 11:03:09.458793 13616 net.cpp:406] relu2_1 <- conv2_1
I1210 11:03:09.458793 13616 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1210 11:03:09.459792 13616 net.cpp:122] Setting up relu2_1
I1210 11:03:09.459792 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.459792 13616 net.cpp:137] Memory required for data: 276481200
I1210 11:03:09.459792 13616 layer_factory.cpp:58] Creating layer conv2_2
I1210 11:03:09.459792 13616 net.cpp:84] Creating Layer conv2_2
I1210 11:03:09.459792 13616 net.cpp:406] conv2_2 <- conv2_1
I1210 11:03:09.459792 13616 net.cpp:380] conv2_2 -> conv2_2
I1210 11:03:09.460811 13616 net.cpp:122] Setting up conv2_2
I1210 11:03:09.460811 13616 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1210 11:03:09.460811 13616 net.cpp:137] Memory required for data: 299009200
I1210 11:03:09.460811 13616 layer_factory.cpp:58] Creating layer bn2_2
I1210 11:03:09.460811 13616 net.cpp:84] Creating Layer bn2_2
I1210 11:03:09.460811 13616 net.cpp:406] bn2_2 <- conv2_2
I1210 11:03:09.460811 13616 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1210 11:03:09.460811 13616 net.cpp:122] Setting up bn2_2
I1210 11:03:09.460811 13616 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1210 11:03:09.460811 13616 net.cpp:137] Memory required for data: 321537200
I1210 11:03:09.460811 13616 layer_factory.cpp:58] Creating layer scale2_2
I1210 11:03:09.460811 13616 net.cpp:84] Creating Layer scale2_2
I1210 11:03:09.460811 13616 net.cpp:406] scale2_2 <- conv2_2
I1210 11:03:09.460811 13616 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1210 11:03:09.460811 13616 layer_factory.cpp:58] Creating layer scale2_2
I1210 11:03:09.460811 13616 net.cpp:122] Setting up scale2_2
I1210 11:03:09.460811 13616 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1210 11:03:09.460811 13616 net.cpp:137] Memory required for data: 344065200
I1210 11:03:09.460811 13616 layer_factory.cpp:58] Creating layer relu2_2
I1210 11:03:09.460811 13616 net.cpp:84] Creating Layer relu2_2
I1210 11:03:09.460811 13616 net.cpp:406] relu2_2 <- conv2_2
I1210 11:03:09.460811 13616 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1210 11:03:09.461792 13616 net.cpp:122] Setting up relu2_2
I1210 11:03:09.461792 13616 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1210 11:03:09.461792 13616 net.cpp:137] Memory required for data: 366593200
I1210 11:03:09.461792 13616 layer_factory.cpp:58] Creating layer pool2_1
I1210 11:03:09.461792 13616 net.cpp:84] Creating Layer pool2_1
I1210 11:03:09.461792 13616 net.cpp:406] pool2_1 <- conv2_2
I1210 11:03:09.461792 13616 net.cpp:380] pool2_1 -> pool2_1
I1210 11:03:09.461792 13616 net.cpp:122] Setting up pool2_1
I1210 11:03:09.461792 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.461792 13616 net.cpp:137] Memory required for data: 372225200
I1210 11:03:09.461792 13616 layer_factory.cpp:58] Creating layer conv3
I1210 11:03:09.461792 13616 net.cpp:84] Creating Layer conv3
I1210 11:03:09.461792 13616 net.cpp:406] conv3 <- pool2_1
I1210 11:03:09.461792 13616 net.cpp:380] conv3 -> conv3
I1210 11:03:09.462792 13616 net.cpp:122] Setting up conv3
I1210 11:03:09.462792 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.462792 13616 net.cpp:137] Memory required for data: 377857200
I1210 11:03:09.462792 13616 layer_factory.cpp:58] Creating layer bn3
I1210 11:03:09.462792 13616 net.cpp:84] Creating Layer bn3
I1210 11:03:09.462792 13616 net.cpp:406] bn3 <- conv3
I1210 11:03:09.462792 13616 net.cpp:367] bn3 -> conv3 (in-place)
I1210 11:03:09.462792 13616 net.cpp:122] Setting up bn3
I1210 11:03:09.462792 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.462792 13616 net.cpp:137] Memory required for data: 383489200
I1210 11:03:09.462792 13616 layer_factory.cpp:58] Creating layer scale3
I1210 11:03:09.462792 13616 net.cpp:84] Creating Layer scale3
I1210 11:03:09.462792 13616 net.cpp:406] scale3 <- conv3
I1210 11:03:09.462792 13616 net.cpp:367] scale3 -> conv3 (in-place)
I1210 11:03:09.462792 13616 layer_factory.cpp:58] Creating layer scale3
I1210 11:03:09.462792 13616 net.cpp:122] Setting up scale3
I1210 11:03:09.462792 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.462792 13616 net.cpp:137] Memory required for data: 389121200
I1210 11:03:09.462792 13616 layer_factory.cpp:58] Creating layer relu3
I1210 11:03:09.462792 13616 net.cpp:84] Creating Layer relu3
I1210 11:03:09.462792 13616 net.cpp:406] relu3 <- conv3
I1210 11:03:09.463811 13616 net.cpp:367] relu3 -> conv3 (in-place)
I1210 11:03:09.463811 13616 net.cpp:122] Setting up relu3
I1210 11:03:09.463811 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.463811 13616 net.cpp:137] Memory required for data: 394753200
I1210 11:03:09.463811 13616 layer_factory.cpp:58] Creating layer conv3_1
I1210 11:03:09.463811 13616 net.cpp:84] Creating Layer conv3_1
I1210 11:03:09.463811 13616 net.cpp:406] conv3_1 <- conv3
I1210 11:03:09.463811 13616 net.cpp:380] conv3_1 -> conv3_1
I1210 11:03:09.464792 13616 net.cpp:122] Setting up conv3_1
I1210 11:03:09.464792 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.464792 13616 net.cpp:137] Memory required for data: 400385200
I1210 11:03:09.464792 13616 layer_factory.cpp:58] Creating layer bn3_1
I1210 11:03:09.464792 13616 net.cpp:84] Creating Layer bn3_1
I1210 11:03:09.464792 13616 net.cpp:406] bn3_1 <- conv3_1
I1210 11:03:09.464792 13616 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1210 11:03:09.464792 13616 net.cpp:122] Setting up bn3_1
I1210 11:03:09.464792 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.464792 13616 net.cpp:137] Memory required for data: 406017200
I1210 11:03:09.465811 13616 layer_factory.cpp:58] Creating layer scale3_1
I1210 11:03:09.465811 13616 net.cpp:84] Creating Layer scale3_1
I1210 11:03:09.465811 13616 net.cpp:406] scale3_1 <- conv3_1
I1210 11:03:09.465811 13616 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1210 11:03:09.465811 13616 layer_factory.cpp:58] Creating layer scale3_1
I1210 11:03:09.465811 13616 net.cpp:122] Setting up scale3_1
I1210 11:03:09.465811 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.465811 13616 net.cpp:137] Memory required for data: 411649200
I1210 11:03:09.465811 13616 layer_factory.cpp:58] Creating layer relu3_1
I1210 11:03:09.465811 13616 net.cpp:84] Creating Layer relu3_1
I1210 11:03:09.465811 13616 net.cpp:406] relu3_1 <- conv3_1
I1210 11:03:09.465811 13616 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1210 11:03:09.465811 13616 net.cpp:122] Setting up relu3_1
I1210 11:03:09.465811 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.465811 13616 net.cpp:137] Memory required for data: 417281200
I1210 11:03:09.465811 13616 layer_factory.cpp:58] Creating layer conv4
I1210 11:03:09.465811 13616 net.cpp:84] Creating Layer conv4
I1210 11:03:09.465811 13616 net.cpp:406] conv4 <- conv3_1
I1210 11:03:09.465811 13616 net.cpp:380] conv4 -> conv4
I1210 11:03:09.466811 13616 net.cpp:122] Setting up conv4
I1210 11:03:09.466811 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.466811 13616 net.cpp:137] Memory required for data: 422913200
I1210 11:03:09.466811 13616 layer_factory.cpp:58] Creating layer bn4
I1210 11:03:09.466811 13616 net.cpp:84] Creating Layer bn4
I1210 11:03:09.466811 13616 net.cpp:406] bn4 <- conv4
I1210 11:03:09.466811 13616 net.cpp:367] bn4 -> conv4 (in-place)
I1210 11:03:09.466811 13616 net.cpp:122] Setting up bn4
I1210 11:03:09.466811 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.466811 13616 net.cpp:137] Memory required for data: 428545200
I1210 11:03:09.466811 13616 layer_factory.cpp:58] Creating layer scale4
I1210 11:03:09.466811 13616 net.cpp:84] Creating Layer scale4
I1210 11:03:09.466811 13616 net.cpp:406] scale4 <- conv4
I1210 11:03:09.466811 13616 net.cpp:367] scale4 -> conv4 (in-place)
I1210 11:03:09.466811 13616 layer_factory.cpp:58] Creating layer scale4
I1210 11:03:09.467811 13616 net.cpp:122] Setting up scale4
I1210 11:03:09.467811 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.467811 13616 net.cpp:137] Memory required for data: 434177200
I1210 11:03:09.467811 13616 layer_factory.cpp:58] Creating layer relu4
I1210 11:03:09.467811 13616 net.cpp:84] Creating Layer relu4
I1210 11:03:09.467811 13616 net.cpp:406] relu4 <- conv4
I1210 11:03:09.467811 13616 net.cpp:367] relu4 -> conv4 (in-place)
I1210 11:03:09.467811 13616 net.cpp:122] Setting up relu4
I1210 11:03:09.467811 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.467811 13616 net.cpp:137] Memory required for data: 439809200
I1210 11:03:09.467811 13616 layer_factory.cpp:58] Creating layer conv4_1
I1210 11:03:09.467811 13616 net.cpp:84] Creating Layer conv4_1
I1210 11:03:09.467811 13616 net.cpp:406] conv4_1 <- conv4
I1210 11:03:09.467811 13616 net.cpp:380] conv4_1 -> conv4_1
I1210 11:03:09.468793 13616 net.cpp:122] Setting up conv4_1
I1210 11:03:09.468793 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.468793 13616 net.cpp:137] Memory required for data: 445441200
I1210 11:03:09.468793 13616 layer_factory.cpp:58] Creating layer bn4_1
I1210 11:03:09.468793 13616 net.cpp:84] Creating Layer bn4_1
I1210 11:03:09.468793 13616 net.cpp:406] bn4_1 <- conv4_1
I1210 11:03:09.469794 13616 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1210 11:03:09.469794 13616 net.cpp:122] Setting up bn4_1
I1210 11:03:09.469794 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.469794 13616 net.cpp:137] Memory required for data: 451073200
I1210 11:03:09.469794 13616 layer_factory.cpp:58] Creating layer scale4_1
I1210 11:03:09.469794 13616 net.cpp:84] Creating Layer scale4_1
I1210 11:03:09.469794 13616 net.cpp:406] scale4_1 <- conv4_1
I1210 11:03:09.469794 13616 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1210 11:03:09.469794 13616 layer_factory.cpp:58] Creating layer scale4_1
I1210 11:03:09.469794 13616 net.cpp:122] Setting up scale4_1
I1210 11:03:09.469794 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.469794 13616 net.cpp:137] Memory required for data: 456705200
I1210 11:03:09.469794 13616 layer_factory.cpp:58] Creating layer relu4_1
I1210 11:03:09.469794 13616 net.cpp:84] Creating Layer relu4_1
I1210 11:03:09.469794 13616 net.cpp:406] relu4_1 <- conv4_1
I1210 11:03:09.469794 13616 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1210 11:03:09.469794 13616 net.cpp:122] Setting up relu4_1
I1210 11:03:09.469794 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.469794 13616 net.cpp:137] Memory required for data: 462337200
I1210 11:03:09.469794 13616 layer_factory.cpp:58] Creating layer conv4_2
I1210 11:03:09.469794 13616 net.cpp:84] Creating Layer conv4_2
I1210 11:03:09.469794 13616 net.cpp:406] conv4_2 <- conv4_1
I1210 11:03:09.469794 13616 net.cpp:380] conv4_2 -> conv4_2
I1210 11:03:09.471812 13616 net.cpp:122] Setting up conv4_2
I1210 11:03:09.471812 13616 net.cpp:129] Top shape: 100 62 16 16 (1587200)
I1210 11:03:09.471812 13616 net.cpp:137] Memory required for data: 468686000
I1210 11:03:09.471812 13616 layer_factory.cpp:58] Creating layer bn4_2
I1210 11:03:09.471812 13616 net.cpp:84] Creating Layer bn4_2
I1210 11:03:09.471812 13616 net.cpp:406] bn4_2 <- conv4_2
I1210 11:03:09.471812 13616 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1210 11:03:09.471812 13616 net.cpp:122] Setting up bn4_2
I1210 11:03:09.471812 13616 net.cpp:129] Top shape: 100 62 16 16 (1587200)
I1210 11:03:09.471812 13616 net.cpp:137] Memory required for data: 475034800
I1210 11:03:09.471812 13616 layer_factory.cpp:58] Creating layer scale4_2
I1210 11:03:09.471812 13616 net.cpp:84] Creating Layer scale4_2
I1210 11:03:09.471812 13616 net.cpp:406] scale4_2 <- conv4_2
I1210 11:03:09.471812 13616 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1210 11:03:09.471812 13616 layer_factory.cpp:58] Creating layer scale4_2
I1210 11:03:09.471812 13616 net.cpp:122] Setting up scale4_2
I1210 11:03:09.471812 13616 net.cpp:129] Top shape: 100 62 16 16 (1587200)
I1210 11:03:09.471812 13616 net.cpp:137] Memory required for data: 481383600
I1210 11:03:09.471812 13616 layer_factory.cpp:58] Creating layer relu4_2
I1210 11:03:09.471812 13616 net.cpp:84] Creating Layer relu4_2
I1210 11:03:09.471812 13616 net.cpp:406] relu4_2 <- conv4_2
I1210 11:03:09.471812 13616 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1210 11:03:09.471812 13616 net.cpp:122] Setting up relu4_2
I1210 11:03:09.471812 13616 net.cpp:129] Top shape: 100 62 16 16 (1587200)
I1210 11:03:09.471812 13616 net.cpp:137] Memory required for data: 487732400
I1210 11:03:09.471812 13616 layer_factory.cpp:58] Creating layer pool4_2
I1210 11:03:09.471812 13616 net.cpp:84] Creating Layer pool4_2
I1210 11:03:09.471812 13616 net.cpp:406] pool4_2 <- conv4_2
I1210 11:03:09.471812 13616 net.cpp:380] pool4_2 -> pool4_2
I1210 11:03:09.471812 13616 net.cpp:122] Setting up pool4_2
I1210 11:03:09.471812 13616 net.cpp:129] Top shape: 100 62 8 8 (396800)
I1210 11:03:09.471812 13616 net.cpp:137] Memory required for data: 489319600
I1210 11:03:09.471812 13616 layer_factory.cpp:58] Creating layer conv4_0
I1210 11:03:09.471812 13616 net.cpp:84] Creating Layer conv4_0
I1210 11:03:09.471812 13616 net.cpp:406] conv4_0 <- pool4_2
I1210 11:03:09.471812 13616 net.cpp:380] conv4_0 -> conv4_0
I1210 11:03:09.473811 13616 net.cpp:122] Setting up conv4_0
I1210 11:03:09.473811 13616 net.cpp:129] Top shape: 100 62 8 8 (396800)
I1210 11:03:09.473811 13616 net.cpp:137] Memory required for data: 490906800
I1210 11:03:09.473811 13616 layer_factory.cpp:58] Creating layer bn4_0
I1210 11:03:09.473811 13616 net.cpp:84] Creating Layer bn4_0
I1210 11:03:09.473811 13616 net.cpp:406] bn4_0 <- conv4_0
I1210 11:03:09.473811 13616 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1210 11:03:09.473811 13616 net.cpp:122] Setting up bn4_0
I1210 11:03:09.473811 13616 net.cpp:129] Top shape: 100 62 8 8 (396800)
I1210 11:03:09.473811 13616 net.cpp:137] Memory required for data: 492494000
I1210 11:03:09.473811 13616 layer_factory.cpp:58] Creating layer scale4_0
I1210 11:03:09.473811 13616 net.cpp:84] Creating Layer scale4_0
I1210 11:03:09.473811 13616 net.cpp:406] scale4_0 <- conv4_0
I1210 11:03:09.473811 13616 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1210 11:03:09.473811 13616 layer_factory.cpp:58] Creating layer scale4_0
I1210 11:03:09.473811 13616 net.cpp:122] Setting up scale4_0
I1210 11:03:09.473811 13616 net.cpp:129] Top shape: 100 62 8 8 (396800)
I1210 11:03:09.473811 13616 net.cpp:137] Memory required for data: 494081200
I1210 11:03:09.473811 13616 layer_factory.cpp:58] Creating layer relu4_0
I1210 11:03:09.473811 13616 net.cpp:84] Creating Layer relu4_0
I1210 11:03:09.473811 13616 net.cpp:406] relu4_0 <- conv4_0
I1210 11:03:09.473811 13616 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1210 11:03:09.474794 13616 net.cpp:122] Setting up relu4_0
I1210 11:03:09.474794 13616 net.cpp:129] Top shape: 100 62 8 8 (396800)
I1210 11:03:09.474794 13616 net.cpp:137] Memory required for data: 495668400
I1210 11:03:09.474794 13616 layer_factory.cpp:58] Creating layer conv11
I1210 11:03:09.474794 13616 net.cpp:84] Creating Layer conv11
I1210 11:03:09.474794 13616 net.cpp:406] conv11 <- conv4_0
I1210 11:03:09.474794 13616 net.cpp:380] conv11 -> conv11
I1210 11:03:09.475811 13616 net.cpp:122] Setting up conv11
I1210 11:03:09.475811 13616 net.cpp:129] Top shape: 100 71 8 8 (454400)
I1210 11:03:09.475811 13616 net.cpp:137] Memory required for data: 497486000
I1210 11:03:09.475811 13616 layer_factory.cpp:58] Creating layer bn_conv11
I1210 11:03:09.475811 13616 net.cpp:84] Creating Layer bn_conv11
I1210 11:03:09.475811 13616 net.cpp:406] bn_conv11 <- conv11
I1210 11:03:09.475811 13616 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1210 11:03:09.475811 13616 net.cpp:122] Setting up bn_conv11
I1210 11:03:09.475811 13616 net.cpp:129] Top shape: 100 71 8 8 (454400)
I1210 11:03:09.475811 13616 net.cpp:137] Memory required for data: 499303600
I1210 11:03:09.475811 13616 layer_factory.cpp:58] Creating layer scale_conv11
I1210 11:03:09.475811 13616 net.cpp:84] Creating Layer scale_conv11
I1210 11:03:09.475811 13616 net.cpp:406] scale_conv11 <- conv11
I1210 11:03:09.475811 13616 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1210 11:03:09.475811 13616 layer_factory.cpp:58] Creating layer scale_conv11
I1210 11:03:09.475811 13616 net.cpp:122] Setting up scale_conv11
I1210 11:03:09.475811 13616 net.cpp:129] Top shape: 100 71 8 8 (454400)
I1210 11:03:09.475811 13616 net.cpp:137] Memory required for data: 501121200
I1210 11:03:09.475811 13616 layer_factory.cpp:58] Creating layer relu_conv11
I1210 11:03:09.475811 13616 net.cpp:84] Creating Layer relu_conv11
I1210 11:03:09.475811 13616 net.cpp:406] relu_conv11 <- conv11
I1210 11:03:09.475811 13616 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1210 11:03:09.476812 13616 net.cpp:122] Setting up relu_conv11
I1210 11:03:09.476812 13616 net.cpp:129] Top shape: 100 71 8 8 (454400)
I1210 11:03:09.476812 13616 net.cpp:137] Memory required for data: 502938800
I1210 11:03:09.476812 13616 layer_factory.cpp:58] Creating layer conv12
I1210 11:03:09.476812 13616 net.cpp:84] Creating Layer conv12
I1210 11:03:09.476812 13616 net.cpp:406] conv12 <- conv11
I1210 11:03:09.476812 13616 net.cpp:380] conv12 -> conv12
I1210 11:03:09.477813 13616 net.cpp:122] Setting up conv12
I1210 11:03:09.477813 13616 net.cpp:129] Top shape: 100 100 8 8 (640000)
I1210 11:03:09.477813 13616 net.cpp:137] Memory required for data: 505498800
I1210 11:03:09.477813 13616 layer_factory.cpp:58] Creating layer bn_conv12
I1210 11:03:09.477813 13616 net.cpp:84] Creating Layer bn_conv12
I1210 11:03:09.477813 13616 net.cpp:406] bn_conv12 <- conv12
I1210 11:03:09.477813 13616 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1210 11:03:09.478811 13616 net.cpp:122] Setting up bn_conv12
I1210 11:03:09.478811 13616 net.cpp:129] Top shape: 100 100 8 8 (640000)
I1210 11:03:09.478811 13616 net.cpp:137] Memory required for data: 508058800
I1210 11:03:09.478811 13616 layer_factory.cpp:58] Creating layer scale_conv12
I1210 11:03:09.478811 13616 net.cpp:84] Creating Layer scale_conv12
I1210 11:03:09.478811 13616 net.cpp:406] scale_conv12 <- conv12
I1210 11:03:09.478811 13616 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1210 11:03:09.478811 13616 layer_factory.cpp:58] Creating layer scale_conv12
I1210 11:03:09.478811 13616 net.cpp:122] Setting up scale_conv12
I1210 11:03:09.478811 13616 net.cpp:129] Top shape: 100 100 8 8 (640000)
I1210 11:03:09.478811 13616 net.cpp:137] Memory required for data: 510618800
I1210 11:03:09.478811 13616 layer_factory.cpp:58] Creating layer relu_conv12
I1210 11:03:09.478811 13616 net.cpp:84] Creating Layer relu_conv12
I1210 11:03:09.478811 13616 net.cpp:406] relu_conv12 <- conv12
I1210 11:03:09.478811 13616 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1210 11:03:09.478811 13616 net.cpp:122] Setting up relu_conv12
I1210 11:03:09.478811 13616 net.cpp:129] Top shape: 100 100 8 8 (640000)
I1210 11:03:09.478811 13616 net.cpp:137] Memory required for data: 513178800
I1210 11:03:09.478811 13616 layer_factory.cpp:58] Creating layer poolcp6
I1210 11:03:09.478811 13616 net.cpp:84] Creating Layer poolcp6
I1210 11:03:09.478811 13616 net.cpp:406] poolcp6 <- conv12
I1210 11:03:09.478811 13616 net.cpp:380] poolcp6 -> poolcp6
I1210 11:03:09.478811 13616 net.cpp:122] Setting up poolcp6
I1210 11:03:09.478811 13616 net.cpp:129] Top shape: 100 100 1 1 (10000)
I1210 11:03:09.478811 13616 net.cpp:137] Memory required for data: 513218800
I1210 11:03:09.478811 13616 layer_factory.cpp:58] Creating layer ip1
I1210 11:03:09.478811 13616 net.cpp:84] Creating Layer ip1
I1210 11:03:09.478811 13616 net.cpp:406] ip1 <- poolcp6
I1210 11:03:09.478811 13616 net.cpp:380] ip1 -> ip1
I1210 11:03:09.478811 13616 net.cpp:122] Setting up ip1
I1210 11:03:09.478811 13616 net.cpp:129] Top shape: 100 100 (10000)
I1210 11:03:09.478811 13616 net.cpp:137] Memory required for data: 513258800
I1210 11:03:09.478811 13616 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1210 11:03:09.478811 13616 net.cpp:84] Creating Layer ip1_ip1_0_split
I1210 11:03:09.478811 13616 net.cpp:406] ip1_ip1_0_split <- ip1
I1210 11:03:09.478811 13616 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1210 11:03:09.478811 13616 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1210 11:03:09.478811 13616 net.cpp:122] Setting up ip1_ip1_0_split
I1210 11:03:09.478811 13616 net.cpp:129] Top shape: 100 100 (10000)
I1210 11:03:09.479811 13616 net.cpp:129] Top shape: 100 100 (10000)
I1210 11:03:09.479811 13616 net.cpp:137] Memory required for data: 513338800
I1210 11:03:09.479811 13616 layer_factory.cpp:58] Creating layer accuracy_training
I1210 11:03:09.479811 13616 net.cpp:84] Creating Layer accuracy_training
I1210 11:03:09.479811 13616 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1210 11:03:09.479811 13616 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1210 11:03:09.479811 13616 net.cpp:380] accuracy_training -> accuracy_training
I1210 11:03:09.479811 13616 net.cpp:122] Setting up accuracy_training
I1210 11:03:09.479811 13616 net.cpp:129] Top shape: (1)
I1210 11:03:09.479811 13616 net.cpp:137] Memory required for data: 513338804
I1210 11:03:09.479811 13616 layer_factory.cpp:58] Creating layer loss
I1210 11:03:09.479811 13616 net.cpp:84] Creating Layer loss
I1210 11:03:09.479811 13616 net.cpp:406] loss <- ip1_ip1_0_split_1
I1210 11:03:09.479811 13616 net.cpp:406] loss <- label_cifar_1_split_1
I1210 11:03:09.479811 13616 net.cpp:380] loss -> loss
I1210 11:03:09.479811 13616 layer_factory.cpp:58] Creating layer loss
I1210 11:03:09.479811 13616 net.cpp:122] Setting up loss
I1210 11:03:09.479811 13616 net.cpp:129] Top shape: (1)
I1210 11:03:09.479811 13616 net.cpp:132]     with loss weight 1
I1210 11:03:09.479811 13616 net.cpp:137] Memory required for data: 513338808
I1210 11:03:09.479811 13616 net.cpp:198] loss needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:200] accuracy_training does not need backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] ip1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] poolcp6 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] relu_conv12 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] scale_conv12 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] bn_conv12 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] conv12 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] relu_conv11 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] scale_conv11 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] bn_conv11 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] conv11 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] relu4_0 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] scale4_0 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] bn4_0 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] conv4_0 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] pool4_2 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] relu4_2 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] scale4_2 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] bn4_2 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] conv4_2 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] relu4_1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] scale4_1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] bn4_1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] conv4_1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] relu4 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] scale4 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] bn4 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] conv4 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] relu3_1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] scale3_1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] bn3_1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] conv3_1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] relu3 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] scale3 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] bn3 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] conv3 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] pool2_1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] relu2_2 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] scale2_2 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] bn2_2 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] conv2_2 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] relu2_1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] scale2_1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] bn2_1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] conv2_1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] relu2 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] scale2 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] bn2 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] conv2 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] relu1_0 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] scale1_0 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] bn1_0 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] conv1_0 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] relu1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] scale1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] bn1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:198] conv1 needs backward computation.
I1210 11:03:09.479811 13616 net.cpp:200] label_cifar_1_split does not need backward computation.
I1210 11:03:09.479811 13616 net.cpp:200] cifar does not need backward computation.
I1210 11:03:09.479811 13616 net.cpp:242] This network produces output accuracy_training
I1210 11:03:09.479811 13616 net.cpp:242] This network produces output loss
I1210 11:03:09.479811 13616 net.cpp:255] Network initialization done.
I1210 11:03:09.480813 13616 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 11:03:09.480813 13616 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 11:03:09.480813 13616 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1210 11:03:09.480813 13616 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1210 11:03:09.481812 13616 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_13L_Simple_NoGrpCon_NoDrp_maxdrp_300k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 36
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 44
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 44
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 44
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 55
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 62
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "conv4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 62
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 71
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 100
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1210 11:03:09.481812 13616 layer_factory.cpp:58] Creating layer cifar
I1210 11:03:09.487812 13616 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1210 11:03:09.487812 13616 net.cpp:84] Creating Layer cifar
I1210 11:03:09.487812 13616 net.cpp:380] cifar -> data
I1210 11:03:09.487812 13616 net.cpp:380] cifar -> label
I1210 11:03:09.487812 13616 data_layer.cpp:45] output data size: 100,3,32,32
I1210 11:03:09.493811 13616 net.cpp:122] Setting up cifar
I1210 11:03:09.493811 13616 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1210 11:03:09.493811 13616 net.cpp:129] Top shape: 100 (100)
I1210 11:03:09.493811 13616 net.cpp:137] Memory required for data: 1229200
I1210 11:03:09.493811 13616 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1210 11:03:09.493811 13616 net.cpp:84] Creating Layer label_cifar_1_split
I1210 11:03:09.493811 13616 net.cpp:406] label_cifar_1_split <- label
I1210 11:03:09.493811 13616 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1210 11:03:09.493811 13616 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1210 11:03:09.493811 13616 net.cpp:122] Setting up label_cifar_1_split
I1210 11:03:09.493811 13616 net.cpp:129] Top shape: 100 (100)
I1210 11:03:09.493811 13616 net.cpp:129] Top shape: 100 (100)
I1210 11:03:09.493811 13616 net.cpp:137] Memory required for data: 1230000
I1210 11:03:09.493811 13616 layer_factory.cpp:58] Creating layer conv1
I1210 11:03:09.493811 13616 net.cpp:84] Creating Layer conv1
I1210 11:03:09.493811 13616 net.cpp:406] conv1 <- data
I1210 11:03:09.493811 13616 net.cpp:380] conv1 -> conv1
I1210 11:03:09.494810 16220 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 11:03:09.495811 13616 net.cpp:122] Setting up conv1
I1210 11:03:09.495811 13616 net.cpp:129] Top shape: 100 36 32 32 (3686400)
I1210 11:03:09.495811 13616 net.cpp:137] Memory required for data: 15975600
I1210 11:03:09.495811 13616 layer_factory.cpp:58] Creating layer bn1
I1210 11:03:09.495811 13616 net.cpp:84] Creating Layer bn1
I1210 11:03:09.495811 13616 net.cpp:406] bn1 <- conv1
I1210 11:03:09.495811 13616 net.cpp:367] bn1 -> conv1 (in-place)
I1210 11:03:09.495811 13616 net.cpp:122] Setting up bn1
I1210 11:03:09.495811 13616 net.cpp:129] Top shape: 100 36 32 32 (3686400)
I1210 11:03:09.495811 13616 net.cpp:137] Memory required for data: 30721200
I1210 11:03:09.495811 13616 layer_factory.cpp:58] Creating layer scale1
I1210 11:03:09.495811 13616 net.cpp:84] Creating Layer scale1
I1210 11:03:09.495811 13616 net.cpp:406] scale1 <- conv1
I1210 11:03:09.495811 13616 net.cpp:367] scale1 -> conv1 (in-place)
I1210 11:03:09.495811 13616 layer_factory.cpp:58] Creating layer scale1
I1210 11:03:09.495811 13616 net.cpp:122] Setting up scale1
I1210 11:03:09.495811 13616 net.cpp:129] Top shape: 100 36 32 32 (3686400)
I1210 11:03:09.495811 13616 net.cpp:137] Memory required for data: 45466800
I1210 11:03:09.495811 13616 layer_factory.cpp:58] Creating layer relu1
I1210 11:03:09.495811 13616 net.cpp:84] Creating Layer relu1
I1210 11:03:09.496809 13616 net.cpp:406] relu1 <- conv1
I1210 11:03:09.496809 13616 net.cpp:367] relu1 -> conv1 (in-place)
I1210 11:03:09.496809 13616 net.cpp:122] Setting up relu1
I1210 11:03:09.496809 13616 net.cpp:129] Top shape: 100 36 32 32 (3686400)
I1210 11:03:09.496809 13616 net.cpp:137] Memory required for data: 60212400
I1210 11:03:09.496809 13616 layer_factory.cpp:58] Creating layer conv1_0
I1210 11:03:09.496809 13616 net.cpp:84] Creating Layer conv1_0
I1210 11:03:09.496809 13616 net.cpp:406] conv1_0 <- conv1
I1210 11:03:09.496809 13616 net.cpp:380] conv1_0 -> conv1_0
I1210 11:03:09.497792 13616 net.cpp:122] Setting up conv1_0
I1210 11:03:09.497792 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.497792 13616 net.cpp:137] Memory required for data: 78234800
I1210 11:03:09.497792 13616 layer_factory.cpp:58] Creating layer bn1_0
I1210 11:03:09.497792 13616 net.cpp:84] Creating Layer bn1_0
I1210 11:03:09.497792 13616 net.cpp:406] bn1_0 <- conv1_0
I1210 11:03:09.497792 13616 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1210 11:03:09.497792 13616 net.cpp:122] Setting up bn1_0
I1210 11:03:09.497792 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.497792 13616 net.cpp:137] Memory required for data: 96257200
I1210 11:03:09.497792 13616 layer_factory.cpp:58] Creating layer scale1_0
I1210 11:03:09.497792 13616 net.cpp:84] Creating Layer scale1_0
I1210 11:03:09.497792 13616 net.cpp:406] scale1_0 <- conv1_0
I1210 11:03:09.497792 13616 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1210 11:03:09.497792 13616 layer_factory.cpp:58] Creating layer scale1_0
I1210 11:03:09.498807 13616 net.cpp:122] Setting up scale1_0
I1210 11:03:09.498807 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.498807 13616 net.cpp:137] Memory required for data: 114279600
I1210 11:03:09.498807 13616 layer_factory.cpp:58] Creating layer relu1_0
I1210 11:03:09.498807 13616 net.cpp:84] Creating Layer relu1_0
I1210 11:03:09.498807 13616 net.cpp:406] relu1_0 <- conv1_0
I1210 11:03:09.498807 13616 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1210 11:03:09.498807 13616 net.cpp:122] Setting up relu1_0
I1210 11:03:09.498807 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.498807 13616 net.cpp:137] Memory required for data: 132302000
I1210 11:03:09.498807 13616 layer_factory.cpp:58] Creating layer conv2
I1210 11:03:09.498807 13616 net.cpp:84] Creating Layer conv2
I1210 11:03:09.498807 13616 net.cpp:406] conv2 <- conv1_0
I1210 11:03:09.498807 13616 net.cpp:380] conv2 -> conv2
I1210 11:03:09.500793 13616 net.cpp:122] Setting up conv2
I1210 11:03:09.500793 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.500793 13616 net.cpp:137] Memory required for data: 150324400
I1210 11:03:09.500793 13616 layer_factory.cpp:58] Creating layer bn2
I1210 11:03:09.500793 13616 net.cpp:84] Creating Layer bn2
I1210 11:03:09.500793 13616 net.cpp:406] bn2 <- conv2
I1210 11:03:09.500793 13616 net.cpp:367] bn2 -> conv2 (in-place)
I1210 11:03:09.500793 13616 net.cpp:122] Setting up bn2
I1210 11:03:09.500793 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.500793 13616 net.cpp:137] Memory required for data: 168346800
I1210 11:03:09.500793 13616 layer_factory.cpp:58] Creating layer scale2
I1210 11:03:09.500793 13616 net.cpp:84] Creating Layer scale2
I1210 11:03:09.500793 13616 net.cpp:406] scale2 <- conv2
I1210 11:03:09.500793 13616 net.cpp:367] scale2 -> conv2 (in-place)
I1210 11:03:09.500793 13616 layer_factory.cpp:58] Creating layer scale2
I1210 11:03:09.500793 13616 net.cpp:122] Setting up scale2
I1210 11:03:09.500793 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.500793 13616 net.cpp:137] Memory required for data: 186369200
I1210 11:03:09.500793 13616 layer_factory.cpp:58] Creating layer relu2
I1210 11:03:09.500793 13616 net.cpp:84] Creating Layer relu2
I1210 11:03:09.500793 13616 net.cpp:406] relu2 <- conv2
I1210 11:03:09.500793 13616 net.cpp:367] relu2 -> conv2 (in-place)
I1210 11:03:09.501791 13616 net.cpp:122] Setting up relu2
I1210 11:03:09.501791 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.501791 13616 net.cpp:137] Memory required for data: 204391600
I1210 11:03:09.501791 13616 layer_factory.cpp:58] Creating layer conv2_1
I1210 11:03:09.501791 13616 net.cpp:84] Creating Layer conv2_1
I1210 11:03:09.501791 13616 net.cpp:406] conv2_1 <- conv2
I1210 11:03:09.501791 13616 net.cpp:380] conv2_1 -> conv2_1
I1210 11:03:09.503793 13616 net.cpp:122] Setting up conv2_1
I1210 11:03:09.503793 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.503793 13616 net.cpp:137] Memory required for data: 222414000
I1210 11:03:09.503793 13616 layer_factory.cpp:58] Creating layer bn2_1
I1210 11:03:09.503793 13616 net.cpp:84] Creating Layer bn2_1
I1210 11:03:09.503793 13616 net.cpp:406] bn2_1 <- conv2_1
I1210 11:03:09.503793 13616 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1210 11:03:09.503793 13616 net.cpp:122] Setting up bn2_1
I1210 11:03:09.503793 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.503793 13616 net.cpp:137] Memory required for data: 240436400
I1210 11:03:09.503793 13616 layer_factory.cpp:58] Creating layer scale2_1
I1210 11:03:09.503793 13616 net.cpp:84] Creating Layer scale2_1
I1210 11:03:09.503793 13616 net.cpp:406] scale2_1 <- conv2_1
I1210 11:03:09.503793 13616 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1210 11:03:09.503793 13616 layer_factory.cpp:58] Creating layer scale2_1
I1210 11:03:09.503793 13616 net.cpp:122] Setting up scale2_1
I1210 11:03:09.503793 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.503793 13616 net.cpp:137] Memory required for data: 258458800
I1210 11:03:09.503793 13616 layer_factory.cpp:58] Creating layer relu2_1
I1210 11:03:09.503793 13616 net.cpp:84] Creating Layer relu2_1
I1210 11:03:09.503793 13616 net.cpp:406] relu2_1 <- conv2_1
I1210 11:03:09.503793 13616 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1210 11:03:09.503793 13616 net.cpp:122] Setting up relu2_1
I1210 11:03:09.504806 13616 net.cpp:129] Top shape: 100 44 32 32 (4505600)
I1210 11:03:09.504806 13616 net.cpp:137] Memory required for data: 276481200
I1210 11:03:09.504806 13616 layer_factory.cpp:58] Creating layer conv2_2
I1210 11:03:09.504806 13616 net.cpp:84] Creating Layer conv2_2
I1210 11:03:09.504806 13616 net.cpp:406] conv2_2 <- conv2_1
I1210 11:03:09.504806 13616 net.cpp:380] conv2_2 -> conv2_2
I1210 11:03:09.505794 13616 net.cpp:122] Setting up conv2_2
I1210 11:03:09.505794 13616 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1210 11:03:09.505794 13616 net.cpp:137] Memory required for data: 299009200
I1210 11:03:09.505794 13616 layer_factory.cpp:58] Creating layer bn2_2
I1210 11:03:09.505794 13616 net.cpp:84] Creating Layer bn2_2
I1210 11:03:09.505794 13616 net.cpp:406] bn2_2 <- conv2_2
I1210 11:03:09.505794 13616 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1210 11:03:09.506314 13616 net.cpp:122] Setting up bn2_2
I1210 11:03:09.506314 13616 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1210 11:03:09.506314 13616 net.cpp:137] Memory required for data: 321537200
I1210 11:03:09.506314 13616 layer_factory.cpp:58] Creating layer scale2_2
I1210 11:03:09.506314 13616 net.cpp:84] Creating Layer scale2_2
I1210 11:03:09.506314 13616 net.cpp:406] scale2_2 <- conv2_2
I1210 11:03:09.506314 13616 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1210 11:03:09.506314 13616 layer_factory.cpp:58] Creating layer scale2_2
I1210 11:03:09.506314 13616 net.cpp:122] Setting up scale2_2
I1210 11:03:09.506314 13616 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1210 11:03:09.506314 13616 net.cpp:137] Memory required for data: 344065200
I1210 11:03:09.506314 13616 layer_factory.cpp:58] Creating layer relu2_2
I1210 11:03:09.506314 13616 net.cpp:84] Creating Layer relu2_2
I1210 11:03:09.506314 13616 net.cpp:406] relu2_2 <- conv2_2
I1210 11:03:09.506314 13616 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1210 11:03:09.506814 13616 net.cpp:122] Setting up relu2_2
I1210 11:03:09.506814 13616 net.cpp:129] Top shape: 100 55 32 32 (5632000)
I1210 11:03:09.506814 13616 net.cpp:137] Memory required for data: 366593200
I1210 11:03:09.506814 13616 layer_factory.cpp:58] Creating layer pool2_1
I1210 11:03:09.506814 13616 net.cpp:84] Creating Layer pool2_1
I1210 11:03:09.506814 13616 net.cpp:406] pool2_1 <- conv2_2
I1210 11:03:09.506814 13616 net.cpp:380] pool2_1 -> pool2_1
I1210 11:03:09.506814 13616 net.cpp:122] Setting up pool2_1
I1210 11:03:09.506814 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.506814 13616 net.cpp:137] Memory required for data: 372225200
I1210 11:03:09.506814 13616 layer_factory.cpp:58] Creating layer conv3
I1210 11:03:09.506814 13616 net.cpp:84] Creating Layer conv3
I1210 11:03:09.506814 13616 net.cpp:406] conv3 <- pool2_1
I1210 11:03:09.506814 13616 net.cpp:380] conv3 -> conv3
I1210 11:03:09.508800 13616 net.cpp:122] Setting up conv3
I1210 11:03:09.508800 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.508800 13616 net.cpp:137] Memory required for data: 377857200
I1210 11:03:09.508800 13616 layer_factory.cpp:58] Creating layer bn3
I1210 11:03:09.508800 13616 net.cpp:84] Creating Layer bn3
I1210 11:03:09.508800 13616 net.cpp:406] bn3 <- conv3
I1210 11:03:09.508800 13616 net.cpp:367] bn3 -> conv3 (in-place)
I1210 11:03:09.508800 13616 net.cpp:122] Setting up bn3
I1210 11:03:09.508800 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.508800 13616 net.cpp:137] Memory required for data: 383489200
I1210 11:03:09.508800 13616 layer_factory.cpp:58] Creating layer scale3
I1210 11:03:09.508800 13616 net.cpp:84] Creating Layer scale3
I1210 11:03:09.508800 13616 net.cpp:406] scale3 <- conv3
I1210 11:03:09.508800 13616 net.cpp:367] scale3 -> conv3 (in-place)
I1210 11:03:09.508800 13616 layer_factory.cpp:58] Creating layer scale3
I1210 11:03:09.509313 13616 net.cpp:122] Setting up scale3
I1210 11:03:09.509313 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.509313 13616 net.cpp:137] Memory required for data: 389121200
I1210 11:03:09.509313 13616 layer_factory.cpp:58] Creating layer relu3
I1210 11:03:09.509313 13616 net.cpp:84] Creating Layer relu3
I1210 11:03:09.509313 13616 net.cpp:406] relu3 <- conv3
I1210 11:03:09.509313 13616 net.cpp:367] relu3 -> conv3 (in-place)
I1210 11:03:09.509313 13616 net.cpp:122] Setting up relu3
I1210 11:03:09.509799 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.509799 13616 net.cpp:137] Memory required for data: 394753200
I1210 11:03:09.509799 13616 layer_factory.cpp:58] Creating layer conv3_1
I1210 11:03:09.509799 13616 net.cpp:84] Creating Layer conv3_1
I1210 11:03:09.509799 13616 net.cpp:406] conv3_1 <- conv3
I1210 11:03:09.509799 13616 net.cpp:380] conv3_1 -> conv3_1
I1210 11:03:09.511308 13616 net.cpp:122] Setting up conv3_1
I1210 11:03:09.511308 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.511308 13616 net.cpp:137] Memory required for data: 400385200
I1210 11:03:09.511308 13616 layer_factory.cpp:58] Creating layer bn3_1
I1210 11:03:09.511308 13616 net.cpp:84] Creating Layer bn3_1
I1210 11:03:09.511308 13616 net.cpp:406] bn3_1 <- conv3_1
I1210 11:03:09.511308 13616 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1210 11:03:09.511308 13616 net.cpp:122] Setting up bn3_1
I1210 11:03:09.511308 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.511308 13616 net.cpp:137] Memory required for data: 406017200
I1210 11:03:09.511308 13616 layer_factory.cpp:58] Creating layer scale3_1
I1210 11:03:09.511308 13616 net.cpp:84] Creating Layer scale3_1
I1210 11:03:09.511798 13616 net.cpp:406] scale3_1 <- conv3_1
I1210 11:03:09.511798 13616 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1210 11:03:09.511798 13616 layer_factory.cpp:58] Creating layer scale3_1
I1210 11:03:09.511798 13616 net.cpp:122] Setting up scale3_1
I1210 11:03:09.511798 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.511798 13616 net.cpp:137] Memory required for data: 411649200
I1210 11:03:09.511798 13616 layer_factory.cpp:58] Creating layer relu3_1
I1210 11:03:09.511798 13616 net.cpp:84] Creating Layer relu3_1
I1210 11:03:09.511798 13616 net.cpp:406] relu3_1 <- conv3_1
I1210 11:03:09.511798 13616 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1210 11:03:09.511798 13616 net.cpp:122] Setting up relu3_1
I1210 11:03:09.511798 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.511798 13616 net.cpp:137] Memory required for data: 417281200
I1210 11:03:09.511798 13616 layer_factory.cpp:58] Creating layer conv4
I1210 11:03:09.511798 13616 net.cpp:84] Creating Layer conv4
I1210 11:03:09.511798 13616 net.cpp:406] conv4 <- conv3_1
I1210 11:03:09.511798 13616 net.cpp:380] conv4 -> conv4
I1210 11:03:09.513309 13616 net.cpp:122] Setting up conv4
I1210 11:03:09.513309 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.513309 13616 net.cpp:137] Memory required for data: 422913200
I1210 11:03:09.513309 13616 layer_factory.cpp:58] Creating layer bn4
I1210 11:03:09.513309 13616 net.cpp:84] Creating Layer bn4
I1210 11:03:09.513309 13616 net.cpp:406] bn4 <- conv4
I1210 11:03:09.513309 13616 net.cpp:367] bn4 -> conv4 (in-place)
I1210 11:03:09.513309 13616 net.cpp:122] Setting up bn4
I1210 11:03:09.513309 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.513309 13616 net.cpp:137] Memory required for data: 428545200
I1210 11:03:09.513309 13616 layer_factory.cpp:58] Creating layer scale4
I1210 11:03:09.513309 13616 net.cpp:84] Creating Layer scale4
I1210 11:03:09.513808 13616 net.cpp:406] scale4 <- conv4
I1210 11:03:09.513808 13616 net.cpp:367] scale4 -> conv4 (in-place)
I1210 11:03:09.513808 13616 layer_factory.cpp:58] Creating layer scale4
I1210 11:03:09.513808 13616 net.cpp:122] Setting up scale4
I1210 11:03:09.513808 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.513808 13616 net.cpp:137] Memory required for data: 434177200
I1210 11:03:09.513808 13616 layer_factory.cpp:58] Creating layer relu4
I1210 11:03:09.513808 13616 net.cpp:84] Creating Layer relu4
I1210 11:03:09.513808 13616 net.cpp:406] relu4 <- conv4
I1210 11:03:09.513808 13616 net.cpp:367] relu4 -> conv4 (in-place)
I1210 11:03:09.514299 13616 net.cpp:122] Setting up relu4
I1210 11:03:09.514299 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.514299 13616 net.cpp:137] Memory required for data: 439809200
I1210 11:03:09.514299 13616 layer_factory.cpp:58] Creating layer conv4_1
I1210 11:03:09.514299 13616 net.cpp:84] Creating Layer conv4_1
I1210 11:03:09.514299 13616 net.cpp:406] conv4_1 <- conv4
I1210 11:03:09.514299 13616 net.cpp:380] conv4_1 -> conv4_1
I1210 11:03:09.515813 13616 net.cpp:122] Setting up conv4_1
I1210 11:03:09.515813 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.515813 13616 net.cpp:137] Memory required for data: 445441200
I1210 11:03:09.515813 13616 layer_factory.cpp:58] Creating layer bn4_1
I1210 11:03:09.515813 13616 net.cpp:84] Creating Layer bn4_1
I1210 11:03:09.515813 13616 net.cpp:406] bn4_1 <- conv4_1
I1210 11:03:09.515813 13616 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1210 11:03:09.515813 13616 net.cpp:122] Setting up bn4_1
I1210 11:03:09.515813 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.515813 13616 net.cpp:137] Memory required for data: 451073200
I1210 11:03:09.515813 13616 layer_factory.cpp:58] Creating layer scale4_1
I1210 11:03:09.515813 13616 net.cpp:84] Creating Layer scale4_1
I1210 11:03:09.515813 13616 net.cpp:406] scale4_1 <- conv4_1
I1210 11:03:09.515813 13616 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1210 11:03:09.515813 13616 layer_factory.cpp:58] Creating layer scale4_1
I1210 11:03:09.515813 13616 net.cpp:122] Setting up scale4_1
I1210 11:03:09.515813 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.515813 13616 net.cpp:137] Memory required for data: 456705200
I1210 11:03:09.515813 13616 layer_factory.cpp:58] Creating layer relu4_1
I1210 11:03:09.515813 13616 net.cpp:84] Creating Layer relu4_1
I1210 11:03:09.516309 13616 net.cpp:406] relu4_1 <- conv4_1
I1210 11:03:09.516309 13616 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1210 11:03:09.516309 13616 net.cpp:122] Setting up relu4_1
I1210 11:03:09.516816 13616 net.cpp:129] Top shape: 100 55 16 16 (1408000)
I1210 11:03:09.516816 13616 net.cpp:137] Memory required for data: 462337200
I1210 11:03:09.516816 13616 layer_factory.cpp:58] Creating layer conv4_2
I1210 11:03:09.516816 13616 net.cpp:84] Creating Layer conv4_2
I1210 11:03:09.516816 13616 net.cpp:406] conv4_2 <- conv4_1
I1210 11:03:09.516816 13616 net.cpp:380] conv4_2 -> conv4_2
I1210 11:03:09.518309 13616 net.cpp:122] Setting up conv4_2
I1210 11:03:09.518309 13616 net.cpp:129] Top shape: 100 62 16 16 (1587200)
I1210 11:03:09.518309 13616 net.cpp:137] Memory required for data: 468686000
I1210 11:03:09.518309 13616 layer_factory.cpp:58] Creating layer bn4_2
I1210 11:03:09.518810 13616 net.cpp:84] Creating Layer bn4_2
I1210 11:03:09.518810 13616 net.cpp:406] bn4_2 <- conv4_2
I1210 11:03:09.518810 13616 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1210 11:03:09.518810 13616 net.cpp:122] Setting up bn4_2
I1210 11:03:09.518810 13616 net.cpp:129] Top shape: 100 62 16 16 (1587200)
I1210 11:03:09.518810 13616 net.cpp:137] Memory required for data: 475034800
I1210 11:03:09.518810 13616 layer_factory.cpp:58] Creating layer scale4_2
I1210 11:03:09.518810 13616 net.cpp:84] Creating Layer scale4_2
I1210 11:03:09.518810 13616 net.cpp:406] scale4_2 <- conv4_2
I1210 11:03:09.518810 13616 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1210 11:03:09.518810 13616 layer_factory.cpp:58] Creating layer scale4_2
I1210 11:03:09.518810 13616 net.cpp:122] Setting up scale4_2
I1210 11:03:09.518810 13616 net.cpp:129] Top shape: 100 62 16 16 (1587200)
I1210 11:03:09.518810 13616 net.cpp:137] Memory required for data: 481383600
I1210 11:03:09.518810 13616 layer_factory.cpp:58] Creating layer relu4_2
I1210 11:03:09.518810 13616 net.cpp:84] Creating Layer relu4_2
I1210 11:03:09.518810 13616 net.cpp:406] relu4_2 <- conv4_2
I1210 11:03:09.518810 13616 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1210 11:03:09.519309 13616 net.cpp:122] Setting up relu4_2
I1210 11:03:09.519309 13616 net.cpp:129] Top shape: 100 62 16 16 (1587200)
I1210 11:03:09.519309 13616 net.cpp:137] Memory required for data: 487732400
I1210 11:03:09.519309 13616 layer_factory.cpp:58] Creating layer pool4_2
I1210 11:03:09.519309 13616 net.cpp:84] Creating Layer pool4_2
I1210 11:03:09.519309 13616 net.cpp:406] pool4_2 <- conv4_2
I1210 11:03:09.519309 13616 net.cpp:380] pool4_2 -> pool4_2
I1210 11:03:09.519309 13616 net.cpp:122] Setting up pool4_2
I1210 11:03:09.519309 13616 net.cpp:129] Top shape: 100 62 8 8 (396800)
I1210 11:03:09.519309 13616 net.cpp:137] Memory required for data: 489319600
I1210 11:03:09.519309 13616 layer_factory.cpp:58] Creating layer conv4_0
I1210 11:03:09.519309 13616 net.cpp:84] Creating Layer conv4_0
I1210 11:03:09.519309 13616 net.cpp:406] conv4_0 <- pool4_2
I1210 11:03:09.519309 13616 net.cpp:380] conv4_0 -> conv4_0
I1210 11:03:09.520809 13616 net.cpp:122] Setting up conv4_0
I1210 11:03:09.520809 13616 net.cpp:129] Top shape: 100 62 8 8 (396800)
I1210 11:03:09.520809 13616 net.cpp:137] Memory required for data: 490906800
I1210 11:03:09.520809 13616 layer_factory.cpp:58] Creating layer bn4_0
I1210 11:03:09.520809 13616 net.cpp:84] Creating Layer bn4_0
I1210 11:03:09.520809 13616 net.cpp:406] bn4_0 <- conv4_0
I1210 11:03:09.520809 13616 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1210 11:03:09.521309 13616 net.cpp:122] Setting up bn4_0
I1210 11:03:09.521309 13616 net.cpp:129] Top shape: 100 62 8 8 (396800)
I1210 11:03:09.521309 13616 net.cpp:137] Memory required for data: 492494000
I1210 11:03:09.521309 13616 layer_factory.cpp:58] Creating layer scale4_0
I1210 11:03:09.521309 13616 net.cpp:84] Creating Layer scale4_0
I1210 11:03:09.521309 13616 net.cpp:406] scale4_0 <- conv4_0
I1210 11:03:09.521309 13616 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1210 11:03:09.521309 13616 layer_factory.cpp:58] Creating layer scale4_0
I1210 11:03:09.521309 13616 net.cpp:122] Setting up scale4_0
I1210 11:03:09.521309 13616 net.cpp:129] Top shape: 100 62 8 8 (396800)
I1210 11:03:09.521309 13616 net.cpp:137] Memory required for data: 494081200
I1210 11:03:09.521309 13616 layer_factory.cpp:58] Creating layer relu4_0
I1210 11:03:09.521309 13616 net.cpp:84] Creating Layer relu4_0
I1210 11:03:09.521309 13616 net.cpp:406] relu4_0 <- conv4_0
I1210 11:03:09.521309 13616 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1210 11:03:09.521811 13616 net.cpp:122] Setting up relu4_0
I1210 11:03:09.521811 13616 net.cpp:129] Top shape: 100 62 8 8 (396800)
I1210 11:03:09.521811 13616 net.cpp:137] Memory required for data: 495668400
I1210 11:03:09.521811 13616 layer_factory.cpp:58] Creating layer conv11
I1210 11:03:09.521811 13616 net.cpp:84] Creating Layer conv11
I1210 11:03:09.521811 13616 net.cpp:406] conv11 <- conv4_0
I1210 11:03:09.521811 13616 net.cpp:380] conv11 -> conv11
I1210 11:03:09.522822 13616 net.cpp:122] Setting up conv11
I1210 11:03:09.522822 13616 net.cpp:129] Top shape: 100 71 8 8 (454400)
I1210 11:03:09.522822 13616 net.cpp:137] Memory required for data: 497486000
I1210 11:03:09.522822 13616 layer_factory.cpp:58] Creating layer bn_conv11
I1210 11:03:09.522822 13616 net.cpp:84] Creating Layer bn_conv11
I1210 11:03:09.522822 13616 net.cpp:406] bn_conv11 <- conv11
I1210 11:03:09.522822 13616 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1210 11:03:09.522822 13616 net.cpp:122] Setting up bn_conv11
I1210 11:03:09.522822 13616 net.cpp:129] Top shape: 100 71 8 8 (454400)
I1210 11:03:09.522822 13616 net.cpp:137] Memory required for data: 499303600
I1210 11:03:09.522822 13616 layer_factory.cpp:58] Creating layer scale_conv11
I1210 11:03:09.522822 13616 net.cpp:84] Creating Layer scale_conv11
I1210 11:03:09.522822 13616 net.cpp:406] scale_conv11 <- conv11
I1210 11:03:09.522822 13616 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1210 11:03:09.522822 13616 layer_factory.cpp:58] Creating layer scale_conv11
I1210 11:03:09.522822 13616 net.cpp:122] Setting up scale_conv11
I1210 11:03:09.522822 13616 net.cpp:129] Top shape: 100 71 8 8 (454400)
I1210 11:03:09.522822 13616 net.cpp:137] Memory required for data: 501121200
I1210 11:03:09.522822 13616 layer_factory.cpp:58] Creating layer relu_conv11
I1210 11:03:09.522822 13616 net.cpp:84] Creating Layer relu_conv11
I1210 11:03:09.522822 13616 net.cpp:406] relu_conv11 <- conv11
I1210 11:03:09.522822 13616 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1210 11:03:09.523821 13616 net.cpp:122] Setting up relu_conv11
I1210 11:03:09.523821 13616 net.cpp:129] Top shape: 100 71 8 8 (454400)
I1210 11:03:09.523821 13616 net.cpp:137] Memory required for data: 502938800
I1210 11:03:09.523821 13616 layer_factory.cpp:58] Creating layer conv12
I1210 11:03:09.523821 13616 net.cpp:84] Creating Layer conv12
I1210 11:03:09.523821 13616 net.cpp:406] conv12 <- conv11
I1210 11:03:09.523821 13616 net.cpp:380] conv12 -> conv12
I1210 11:03:09.524813 13616 net.cpp:122] Setting up conv12
I1210 11:03:09.524813 13616 net.cpp:129] Top shape: 100 100 8 8 (640000)
I1210 11:03:09.524813 13616 net.cpp:137] Memory required for data: 505498800
I1210 11:03:09.524813 13616 layer_factory.cpp:58] Creating layer bn_conv12
I1210 11:03:09.524813 13616 net.cpp:84] Creating Layer bn_conv12
I1210 11:03:09.524813 13616 net.cpp:406] bn_conv12 <- conv12
I1210 11:03:09.524813 13616 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1210 11:03:09.525811 13616 net.cpp:122] Setting up bn_conv12
I1210 11:03:09.525811 13616 net.cpp:129] Top shape: 100 100 8 8 (640000)
I1210 11:03:09.525811 13616 net.cpp:137] Memory required for data: 508058800
I1210 11:03:09.525811 13616 layer_factory.cpp:58] Creating layer scale_conv12
I1210 11:03:09.525811 13616 net.cpp:84] Creating Layer scale_conv12
I1210 11:03:09.525811 13616 net.cpp:406] scale_conv12 <- conv12
I1210 11:03:09.525811 13616 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1210 11:03:09.525811 13616 layer_factory.cpp:58] Creating layer scale_conv12
I1210 11:03:09.525811 13616 net.cpp:122] Setting up scale_conv12
I1210 11:03:09.525811 13616 net.cpp:129] Top shape: 100 100 8 8 (640000)
I1210 11:03:09.525811 13616 net.cpp:137] Memory required for data: 510618800
I1210 11:03:09.525811 13616 layer_factory.cpp:58] Creating layer relu_conv12
I1210 11:03:09.525811 13616 net.cpp:84] Creating Layer relu_conv12
I1210 11:03:09.525811 13616 net.cpp:406] relu_conv12 <- conv12
I1210 11:03:09.525811 13616 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1210 11:03:09.525811 13616 net.cpp:122] Setting up relu_conv12
I1210 11:03:09.525811 13616 net.cpp:129] Top shape: 100 100 8 8 (640000)
I1210 11:03:09.525811 13616 net.cpp:137] Memory required for data: 513178800
I1210 11:03:09.525811 13616 layer_factory.cpp:58] Creating layer poolcp6
I1210 11:03:09.525811 13616 net.cpp:84] Creating Layer poolcp6
I1210 11:03:09.525811 13616 net.cpp:406] poolcp6 <- conv12
I1210 11:03:09.525811 13616 net.cpp:380] poolcp6 -> poolcp6
I1210 11:03:09.525811 13616 net.cpp:122] Setting up poolcp6
I1210 11:03:09.525811 13616 net.cpp:129] Top shape: 100 100 1 1 (10000)
I1210 11:03:09.525811 13616 net.cpp:137] Memory required for data: 513218800
I1210 11:03:09.525811 13616 layer_factory.cpp:58] Creating layer ip1
I1210 11:03:09.525811 13616 net.cpp:84] Creating Layer ip1
I1210 11:03:09.525811 13616 net.cpp:406] ip1 <- poolcp6
I1210 11:03:09.525811 13616 net.cpp:380] ip1 -> ip1
I1210 11:03:09.525811 13616 net.cpp:122] Setting up ip1
I1210 11:03:09.525811 13616 net.cpp:129] Top shape: 100 100 (10000)
I1210 11:03:09.525811 13616 net.cpp:137] Memory required for data: 513258800
I1210 11:03:09.525811 13616 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1210 11:03:09.525811 13616 net.cpp:84] Creating Layer ip1_ip1_0_split
I1210 11:03:09.525811 13616 net.cpp:406] ip1_ip1_0_split <- ip1
I1210 11:03:09.525811 13616 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1210 11:03:09.525811 13616 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1210 11:03:09.526813 13616 net.cpp:122] Setting up ip1_ip1_0_split
I1210 11:03:09.526813 13616 net.cpp:129] Top shape: 100 100 (10000)
I1210 11:03:09.526813 13616 net.cpp:129] Top shape: 100 100 (10000)
I1210 11:03:09.526813 13616 net.cpp:137] Memory required for data: 513338800
I1210 11:03:09.526813 13616 layer_factory.cpp:58] Creating layer accuracy
I1210 11:03:09.526813 13616 net.cpp:84] Creating Layer accuracy
I1210 11:03:09.526813 13616 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1210 11:03:09.526813 13616 net.cpp:406] accuracy <- label_cifar_1_split_0
I1210 11:03:09.526813 13616 net.cpp:380] accuracy -> accuracy
I1210 11:03:09.526813 13616 net.cpp:122] Setting up accuracy
I1210 11:03:09.526813 13616 net.cpp:129] Top shape: (1)
I1210 11:03:09.526813 13616 net.cpp:137] Memory required for data: 513338804
I1210 11:03:09.526813 13616 layer_factory.cpp:58] Creating layer loss
I1210 11:03:09.526813 13616 net.cpp:84] Creating Layer loss
I1210 11:03:09.526813 13616 net.cpp:406] loss <- ip1_ip1_0_split_1
I1210 11:03:09.526813 13616 net.cpp:406] loss <- label_cifar_1_split_1
I1210 11:03:09.526813 13616 net.cpp:380] loss -> loss
I1210 11:03:09.526813 13616 layer_factory.cpp:58] Creating layer loss
I1210 11:03:09.526813 13616 net.cpp:122] Setting up loss
I1210 11:03:09.526813 13616 net.cpp:129] Top shape: (1)
I1210 11:03:09.526813 13616 net.cpp:132]     with loss weight 1
I1210 11:03:09.526813 13616 net.cpp:137] Memory required for data: 513338808
I1210 11:03:09.526813 13616 net.cpp:198] loss needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:200] accuracy does not need backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] ip1 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] poolcp6 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] relu_conv12 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] scale_conv12 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] bn_conv12 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] conv12 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] relu_conv11 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] scale_conv11 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] bn_conv11 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] conv11 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] relu4_0 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] scale4_0 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] bn4_0 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] conv4_0 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] pool4_2 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] relu4_2 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] scale4_2 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] bn4_2 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] conv4_2 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] relu4_1 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] scale4_1 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] bn4_1 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] conv4_1 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] relu4 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] scale4 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] bn4 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] conv4 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] relu3_1 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] scale3_1 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] bn3_1 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] conv3_1 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] relu3 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] scale3 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] bn3 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] conv3 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] pool2_1 needs backward computation.
I1210 11:03:09.526813 13616 net.cpp:198] relu2_2 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] scale2_2 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] bn2_2 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] conv2_2 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] relu2_1 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] scale2_1 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] bn2_1 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] conv2_1 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] relu2 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] scale2 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] bn2 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] conv2 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] relu1_0 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] scale1_0 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] bn1_0 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] conv1_0 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] relu1 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] scale1 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] bn1 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:198] conv1 needs backward computation.
I1210 11:03:09.527828 13616 net.cpp:200] label_cifar_1_split does not need backward computation.
I1210 11:03:09.527828 13616 net.cpp:200] cifar does not need backward computation.
I1210 11:03:09.527828 13616 net.cpp:242] This network produces output accuracy
I1210 11:03:09.527828 13616 net.cpp:242] This network produces output loss
I1210 11:03:09.527828 13616 net.cpp:255] Network initialization done.
I1210 11:03:09.527828 13616 solver.cpp:56] Solver scaffolding done.
I1210 11:03:09.531826 13616 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_90000.solverstate
I1210 11:03:09.535817 13616 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_90000.caffemodel
I1210 11:03:09.535817 13616 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 11:03:09.535817 13616 sgd_solver.cpp:318] SGDSolver: restoring history
I1210 11:03:09.538811 13616 caffe.cpp:249] Starting Optimization
I1210 11:03:09.539826 13616 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_13L_Simple_NoGrpCon_NoDrp_maxdrp_300k
I1210 11:03:09.539826 13616 solver.cpp:273] Learning Rate Policy: multistep
I1210 11:03:09.541816 13616 solver.cpp:330] Iteration 90000, Testing net (#0)
I1210 11:03:09.543812 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:03:10.961968 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:03:11.016510 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6051
I1210 11:03:11.016510 13616 solver.cpp:397]     Test net output #1: loss = 1.56211 (* 1 = 1.56211 loss)
I1210 11:03:11.125023 13616 solver.cpp:218] Iteration 90000 (56772.7 iter/s, 1.58527s/100 iters), loss = 0.662245
I1210 11:03:11.125023 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:03:11.125023 13616 solver.cpp:237]     Train net output #1: loss = 0.662245 (* 1 = 0.662245 loss)
I1210 11:03:11.125023 13616 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1210 11:03:16.820951 13616 solver.cpp:218] Iteration 90100 (17.5594 iter/s, 5.69497s/100 iters), loss = 0.560808
I1210 11:03:16.820951 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 11:03:16.820951 13616 solver.cpp:237]     Train net output #1: loss = 0.560808 (* 1 = 0.560808 loss)
I1210 11:03:16.820951 13616 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1210 11:03:22.478950 13616 solver.cpp:218] Iteration 90200 (17.6745 iter/s, 5.65786s/100 iters), loss = 0.650961
I1210 11:03:22.478950 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 11:03:22.478950 13616 solver.cpp:237]     Train net output #1: loss = 0.650961 (* 1 = 0.650961 loss)
I1210 11:03:22.478950 13616 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1210 11:03:28.178172 13616 solver.cpp:218] Iteration 90300 (17.5494 iter/s, 5.69821s/100 iters), loss = 0.8962
I1210 11:03:28.178172 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 11:03:28.178172 13616 solver.cpp:237]     Train net output #1: loss = 0.8962 (* 1 = 0.8962 loss)
I1210 11:03:28.178172 13616 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1210 11:03:33.861016 13616 solver.cpp:218] Iteration 90400 (17.5979 iter/s, 5.6825s/100 iters), loss = 0.790202
I1210 11:03:33.861016 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 11:03:33.861016 13616 solver.cpp:237]     Train net output #1: loss = 0.790202 (* 1 = 0.790202 loss)
I1210 11:03:33.861016 13616 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1210 11:03:39.282382  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:03:39.505255 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_90500.caffemodel
I1210 11:03:39.519270 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_90500.solverstate
I1210 11:03:39.524271 13616 solver.cpp:330] Iteration 90500, Testing net (#0)
I1210 11:03:39.524271 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:03:40.890413 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:03:40.944425 13616 solver.cpp:397]     Test net output #0: accuracy = 0.5964
I1210 11:03:40.944425 13616 solver.cpp:397]     Test net output #1: loss = 1.69014 (* 1 = 1.69014 loss)
I1210 11:03:40.999956 13616 solver.cpp:218] Iteration 90500 (14.0089 iter/s, 7.1383s/100 iters), loss = 0.475134
I1210 11:03:40.999956 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:03:40.999956 13616 solver.cpp:237]     Train net output #1: loss = 0.475134 (* 1 = 0.475134 loss)
I1210 11:03:40.999956 13616 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1210 11:03:46.706099 13616 solver.cpp:218] Iteration 90600 (17.5257 iter/s, 5.70591s/100 iters), loss = 0.638273
I1210 11:03:46.706099 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 11:03:46.706099 13616 solver.cpp:237]     Train net output #1: loss = 0.638273 (* 1 = 0.638273 loss)
I1210 11:03:46.706099 13616 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1210 11:03:52.367044 13616 solver.cpp:218] Iteration 90700 (17.6671 iter/s, 5.66024s/100 iters), loss = 0.558137
I1210 11:03:52.367044 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 11:03:52.367044 13616 solver.cpp:237]     Train net output #1: loss = 0.558137 (* 1 = 0.558137 loss)
I1210 11:03:52.367044 13616 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1210 11:03:58.053889 13616 solver.cpp:218] Iteration 90800 (17.5834 iter/s, 5.68717s/100 iters), loss = 0.81369
I1210 11:03:58.053889 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 11:03:58.053889 13616 solver.cpp:237]     Train net output #1: loss = 0.81369 (* 1 = 0.81369 loss)
I1210 11:03:58.053889 13616 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1210 11:04:03.752416 13616 solver.cpp:218] Iteration 90900 (17.5511 iter/s, 5.69765s/100 iters), loss = 0.801529
I1210 11:04:03.752416 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 11:04:03.752416 13616 solver.cpp:237]     Train net output #1: loss = 0.801529 (* 1 = 0.801529 loss)
I1210 11:04:03.752416 13616 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1210 11:04:09.145853  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:04:09.367862 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_91000.caffemodel
I1210 11:04:09.383867 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_91000.solverstate
I1210 11:04:09.388872 13616 solver.cpp:330] Iteration 91000, Testing net (#0)
I1210 11:04:09.388872 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:04:10.754007 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:04:10.808020 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6022
I1210 11:04:10.808020 13616 solver.cpp:397]     Test net output #1: loss = 1.6145 (* 1 = 1.6145 loss)
I1210 11:04:10.862013 13616 solver.cpp:218] Iteration 91000 (14.0671 iter/s, 7.10878s/100 iters), loss = 0.661483
I1210 11:04:10.862013 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 11:04:10.862013 13616 solver.cpp:237]     Train net output #1: loss = 0.661483 (* 1 = 0.661483 loss)
I1210 11:04:10.862013 13616 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1210 11:04:16.545591 13616 solver.cpp:218] Iteration 91100 (17.5953 iter/s, 5.68335s/100 iters), loss = 0.682813
I1210 11:04:16.545591 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 11:04:16.545591 13616 solver.cpp:237]     Train net output #1: loss = 0.682813 (* 1 = 0.682813 loss)
I1210 11:04:16.545591 13616 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1210 11:04:22.220024 13616 solver.cpp:218] Iteration 91200 (17.6221 iter/s, 5.67469s/100 iters), loss = 0.583949
I1210 11:04:22.221024 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 11:04:22.221024 13616 solver.cpp:237]     Train net output #1: loss = 0.583949 (* 1 = 0.583949 loss)
I1210 11:04:22.221024 13616 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1210 11:04:27.911448 13616 solver.cpp:218] Iteration 91300 (17.5723 iter/s, 5.69078s/100 iters), loss = 0.67555
I1210 11:04:27.911448 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 11:04:27.911448 13616 solver.cpp:237]     Train net output #1: loss = 0.67555 (* 1 = 0.67555 loss)
I1210 11:04:27.911448 13616 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1210 11:04:33.646868 13616 solver.cpp:218] Iteration 91400 (17.4372 iter/s, 5.73487s/100 iters), loss = 0.784476
I1210 11:04:33.646868 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 11:04:33.646868 13616 solver.cpp:237]     Train net output #1: loss = 0.784476 (* 1 = 0.784476 loss)
I1210 11:04:33.646868 13616 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1210 11:04:39.061250  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:04:39.283772 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_91500.caffemodel
I1210 11:04:39.299278 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_91500.solverstate
I1210 11:04:39.304278 13616 solver.cpp:330] Iteration 91500, Testing net (#0)
I1210 11:04:39.304278 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:04:40.677373 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:04:40.730378 13616 solver.cpp:397]     Test net output #0: accuracy = 0.5939
I1210 11:04:40.730378 13616 solver.cpp:397]     Test net output #1: loss = 1.6076 (* 1 = 1.6076 loss)
I1210 11:04:40.784891 13616 solver.cpp:218] Iteration 91500 (14.0114 iter/s, 7.13704s/100 iters), loss = 0.552783
I1210 11:04:40.784891 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:04:40.784891 13616 solver.cpp:237]     Train net output #1: loss = 0.552783 (* 1 = 0.552783 loss)
I1210 11:04:40.784891 13616 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1210 11:04:46.535809 13616 solver.cpp:218] Iteration 91600 (17.3902 iter/s, 5.75037s/100 iters), loss = 0.632337
I1210 11:04:46.535809 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 11:04:46.535809 13616 solver.cpp:237]     Train net output #1: loss = 0.632337 (* 1 = 0.632337 loss)
I1210 11:04:46.535809 13616 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1210 11:04:52.242236 13616 solver.cpp:218] Iteration 91700 (17.5251 iter/s, 5.7061s/100 iters), loss = 0.530837
I1210 11:04:52.242236 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 11:04:52.242236 13616 solver.cpp:237]     Train net output #1: loss = 0.530837 (* 1 = 0.530837 loss)
I1210 11:04:52.242236 13616 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1210 11:04:57.925709 13616 solver.cpp:218] Iteration 91800 (17.5956 iter/s, 5.68323s/100 iters), loss = 0.618428
I1210 11:04:57.925709 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 11:04:57.925709 13616 solver.cpp:237]     Train net output #1: loss = 0.618428 (* 1 = 0.618428 loss)
I1210 11:04:57.925709 13616 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1210 11:05:03.631157 13616 solver.cpp:218] Iteration 91900 (17.5281 iter/s, 5.70514s/100 iters), loss = 0.805962
I1210 11:05:03.631157 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1210 11:05:03.631157 13616 solver.cpp:237]     Train net output #1: loss = 0.805962 (* 1 = 0.805962 loss)
I1210 11:05:03.631157 13616 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1210 11:05:09.065554  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:05:09.290069 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_92000.caffemodel
I1210 11:05:09.303572 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_92000.solverstate
I1210 11:05:09.308579 13616 solver.cpp:330] Iteration 92000, Testing net (#0)
I1210 11:05:09.309573 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:05:10.673691 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:05:10.727699 13616 solver.cpp:397]     Test net output #0: accuracy = 0.5647
I1210 11:05:10.727699 13616 solver.cpp:397]     Test net output #1: loss = 1.86963 (* 1 = 1.86963 loss)
I1210 11:05:10.784201 13616 solver.cpp:218] Iteration 92000 (13.9821 iter/s, 7.152s/100 iters), loss = 0.644487
I1210 11:05:10.784201 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:05:10.784201 13616 solver.cpp:237]     Train net output #1: loss = 0.644487 (* 1 = 0.644487 loss)
I1210 11:05:10.784201 13616 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1210 11:05:16.496835 13616 solver.cpp:218] Iteration 92100 (17.505 iter/s, 5.71266s/100 iters), loss = 0.671741
I1210 11:05:16.496835 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:05:16.496835 13616 solver.cpp:237]     Train net output #1: loss = 0.671741 (* 1 = 0.671741 loss)
I1210 11:05:16.496835 13616 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1210 11:05:22.194795 13616 solver.cpp:218] Iteration 92200 (17.5516 iter/s, 5.69749s/100 iters), loss = 0.517805
I1210 11:05:22.195297 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:05:22.195297 13616 solver.cpp:237]     Train net output #1: loss = 0.517805 (* 1 = 0.517805 loss)
I1210 11:05:22.195297 13616 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1210 11:05:27.919499 13616 solver.cpp:218] Iteration 92300 (17.4705 iter/s, 5.72394s/100 iters), loss = 0.744971
I1210 11:05:27.919499 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 11:05:27.919499 13616 solver.cpp:237]     Train net output #1: loss = 0.744971 (* 1 = 0.744971 loss)
I1210 11:05:27.919499 13616 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1210 11:05:33.609911 13616 solver.cpp:218] Iteration 92400 (17.5744 iter/s, 5.69008s/100 iters), loss = 0.913224
I1210 11:05:33.609911 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1210 11:05:33.609911 13616 solver.cpp:237]     Train net output #1: loss = 0.913224 (* 1 = 0.913224 loss)
I1210 11:05:33.609911 13616 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1210 11:05:39.016315  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:05:39.239329 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_92500.caffemodel
I1210 11:05:39.253329 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_92500.solverstate
I1210 11:05:39.257329 13616 solver.cpp:330] Iteration 92500, Testing net (#0)
I1210 11:05:39.257329 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:05:40.621443 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:05:40.676443 13616 solver.cpp:397]     Test net output #0: accuracy = 0.5817
I1210 11:05:40.676443 13616 solver.cpp:397]     Test net output #1: loss = 1.77044 (* 1 = 1.77044 loss)
I1210 11:05:40.730448 13616 solver.cpp:218] Iteration 92500 (14.0439 iter/s, 7.12055s/100 iters), loss = 0.68784
I1210 11:05:40.730448 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 11:05:40.730448 13616 solver.cpp:237]     Train net output #1: loss = 0.68784 (* 1 = 0.68784 loss)
I1210 11:05:40.730448 13616 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1210 11:05:46.414988 13616 solver.cpp:218] Iteration 92600 (17.5945 iter/s, 5.68359s/100 iters), loss = 0.65768
I1210 11:05:46.414988 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 11:05:46.414988 13616 solver.cpp:237]     Train net output #1: loss = 0.65768 (* 1 = 0.65768 loss)
I1210 11:05:46.414988 13616 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1210 11:05:52.101918 13616 solver.cpp:218] Iteration 92700 (17.5861 iter/s, 5.68632s/100 iters), loss = 0.534491
I1210 11:05:52.101918 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 11:05:52.101918 13616 solver.cpp:237]     Train net output #1: loss = 0.534491 (* 1 = 0.534491 loss)
I1210 11:05:52.101918 13616 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1210 11:05:57.785056 13616 solver.cpp:218] Iteration 92800 (17.5964 iter/s, 5.68297s/100 iters), loss = 0.671143
I1210 11:05:57.785056 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 11:05:57.785056 13616 solver.cpp:237]     Train net output #1: loss = 0.671143 (* 1 = 0.671143 loss)
I1210 11:05:57.785056 13616 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1210 11:06:03.461472 13616 solver.cpp:218] Iteration 92900 (17.618 iter/s, 5.67602s/100 iters), loss = 0.652841
I1210 11:06:03.461472 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:06:03.461472 13616 solver.cpp:237]     Train net output #1: loss = 0.652841 (* 1 = 0.652841 loss)
I1210 11:06:03.461472 13616 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1210 11:06:08.858901  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:06:09.082916 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_93000.caffemodel
I1210 11:06:09.097419 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_93000.solverstate
I1210 11:06:09.101920 13616 solver.cpp:330] Iteration 93000, Testing net (#0)
I1210 11:06:09.101920 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:06:10.469054 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:06:10.523063 13616 solver.cpp:397]     Test net output #0: accuracy = 0.625
I1210 11:06:10.523063 13616 solver.cpp:397]     Test net output #1: loss = 1.44373 (* 1 = 1.44373 loss)
I1210 11:06:10.579063 13616 solver.cpp:218] Iteration 93000 (14.0498 iter/s, 7.11756s/100 iters), loss = 0.568563
I1210 11:06:10.579063 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:06:10.579063 13616 solver.cpp:237]     Train net output #1: loss = 0.568563 (* 1 = 0.568563 loss)
I1210 11:06:10.579063 13616 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1210 11:06:16.293601 13616 solver.cpp:218] Iteration 93100 (17.5028 iter/s, 5.71337s/100 iters), loss = 0.572615
I1210 11:06:16.293601 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 11:06:16.293601 13616 solver.cpp:237]     Train net output #1: loss = 0.572615 (* 1 = 0.572615 loss)
I1210 11:06:16.293601 13616 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1210 11:06:21.960011 13616 solver.cpp:218] Iteration 93200 (17.6464 iter/s, 5.66687s/100 iters), loss = 0.592576
I1210 11:06:21.961010 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 11:06:21.961010 13616 solver.cpp:237]     Train net output #1: loss = 0.592576 (* 1 = 0.592576 loss)
I1210 11:06:21.961010 13616 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1210 11:06:27.640467 13616 solver.cpp:218] Iteration 93300 (17.6084 iter/s, 5.6791s/100 iters), loss = 0.747811
I1210 11:06:27.640467 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 11:06:27.640467 13616 solver.cpp:237]     Train net output #1: loss = 0.747811 (* 1 = 0.747811 loss)
I1210 11:06:27.640467 13616 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1210 11:06:33.322885 13616 solver.cpp:218] Iteration 93400 (17.5985 iter/s, 5.68229s/100 iters), loss = 0.67963
I1210 11:06:33.322885 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 11:06:33.322885 13616 solver.cpp:237]     Train net output #1: loss = 0.67963 (* 1 = 0.67963 loss)
I1210 11:06:33.322885 13616 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1210 11:06:38.726307  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:06:38.952318 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_93500.caffemodel
I1210 11:06:38.967319 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_93500.solverstate
I1210 11:06:38.972318 13616 solver.cpp:330] Iteration 93500, Testing net (#0)
I1210 11:06:38.972318 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:06:40.338423 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:06:40.391422 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6026
I1210 11:06:40.391422 13616 solver.cpp:397]     Test net output #1: loss = 1.61463 (* 1 = 1.61463 loss)
I1210 11:06:40.445427 13616 solver.cpp:218] Iteration 93500 (14.0409 iter/s, 7.12204s/100 iters), loss = 0.564227
I1210 11:06:40.445427 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 11:06:40.445427 13616 solver.cpp:237]     Train net output #1: loss = 0.564227 (* 1 = 0.564227 loss)
I1210 11:06:40.445427 13616 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1210 11:06:46.134847 13616 solver.cpp:218] Iteration 93600 (17.5792 iter/s, 5.68855s/100 iters), loss = 0.668892
I1210 11:06:46.134847 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 11:06:46.134847 13616 solver.cpp:237]     Train net output #1: loss = 0.668892 (* 1 = 0.668892 loss)
I1210 11:06:46.134847 13616 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1210 11:06:51.820294 13616 solver.cpp:218] Iteration 93700 (17.59 iter/s, 5.68504s/100 iters), loss = 0.577722
I1210 11:06:51.820294 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 11:06:51.820294 13616 solver.cpp:237]     Train net output #1: loss = 0.577722 (* 1 = 0.577722 loss)
I1210 11:06:51.820294 13616 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1210 11:06:57.502131 13616 solver.cpp:218] Iteration 93800 (17.6003 iter/s, 5.68171s/100 iters), loss = 0.763739
I1210 11:06:57.502131 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 11:06:57.502131 13616 solver.cpp:237]     Train net output #1: loss = 0.763739 (* 1 = 0.763739 loss)
I1210 11:06:57.502632 13616 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1210 11:07:03.202534 13616 solver.cpp:218] Iteration 93900 (17.5451 iter/s, 5.69959s/100 iters), loss = 0.725132
I1210 11:07:03.202534 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 11:07:03.202534 13616 solver.cpp:237]     Train net output #1: loss = 0.725132 (* 1 = 0.725132 loss)
I1210 11:07:03.202534 13616 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1210 11:07:08.614504  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:07:08.839519 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_94000.caffemodel
I1210 11:07:08.852519 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_94000.solverstate
I1210 11:07:08.857519 13616 solver.cpp:330] Iteration 94000, Testing net (#0)
I1210 11:07:08.857519 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:07:10.224666 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:07:10.278666 13616 solver.cpp:397]     Test net output #0: accuracy = 0.5946
I1210 11:07:10.278666 13616 solver.cpp:397]     Test net output #1: loss = 1.64461 (* 1 = 1.64461 loss)
I1210 11:07:10.332670 13616 solver.cpp:218] Iteration 94000 (14.0247 iter/s, 7.13028s/100 iters), loss = 0.574761
I1210 11:07:10.332670 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:07:10.332670 13616 solver.cpp:237]     Train net output #1: loss = 0.574761 (* 1 = 0.574761 loss)
I1210 11:07:10.332670 13616 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1210 11:07:16.024051 13616 solver.cpp:218] Iteration 94100 (17.5729 iter/s, 5.69058s/100 iters), loss = 0.819654
I1210 11:07:16.024051 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 11:07:16.024051 13616 solver.cpp:237]     Train net output #1: loss = 0.819654 (* 1 = 0.819654 loss)
I1210 11:07:16.024051 13616 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1210 11:07:21.735512 13616 solver.cpp:218] Iteration 94200 (17.5109 iter/s, 5.71072s/100 iters), loss = 0.52413
I1210 11:07:21.735512 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 11:07:21.735512 13616 solver.cpp:237]     Train net output #1: loss = 0.52413 (* 1 = 0.52413 loss)
I1210 11:07:21.735512 13616 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1210 11:07:27.443928 13616 solver.cpp:218] Iteration 94300 (17.5169 iter/s, 5.70878s/100 iters), loss = 0.855506
I1210 11:07:27.444928 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 11:07:27.444928 13616 solver.cpp:237]     Train net output #1: loss = 0.855506 (* 1 = 0.855506 loss)
I1210 11:07:27.444928 13616 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1210 11:07:33.282352 13616 solver.cpp:218] Iteration 94400 (17.1295 iter/s, 5.8379s/100 iters), loss = 0.670736
I1210 11:07:33.282352 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 11:07:33.283354 13616 solver.cpp:237]     Train net output #1: loss = 0.670736 (* 1 = 0.670736 loss)
I1210 11:07:33.283354 13616 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1210 11:07:38.709106  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:07:38.932687 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_94500.caffemodel
I1210 11:07:38.946557 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_94500.solverstate
I1210 11:07:38.950556 13616 solver.cpp:330] Iteration 94500, Testing net (#0)
I1210 11:07:38.950556 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:07:40.325911 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:07:40.380468 13616 solver.cpp:397]     Test net output #0: accuracy = 0.5975
I1210 11:07:40.380468 13616 solver.cpp:397]     Test net output #1: loss = 1.63371 (* 1 = 1.63371 loss)
I1210 11:07:40.434986 13616 solver.cpp:218] Iteration 94500 (13.9831 iter/s, 7.15149s/100 iters), loss = 0.579337
I1210 11:07:40.434986 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 11:07:40.434986 13616 solver.cpp:237]     Train net output #1: loss = 0.579337 (* 1 = 0.579337 loss)
I1210 11:07:40.434986 13616 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1210 11:07:46.149431 13616 solver.cpp:218] Iteration 94600 (17.5009 iter/s, 5.71401s/100 iters), loss = 0.663403
I1210 11:07:46.149431 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 11:07:46.149431 13616 solver.cpp:237]     Train net output #1: loss = 0.663403 (* 1 = 0.663403 loss)
I1210 11:07:46.149431 13616 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1210 11:07:51.821864 13616 solver.cpp:218] Iteration 94700 (17.6307 iter/s, 5.67194s/100 iters), loss = 0.570469
I1210 11:07:51.821864 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:07:51.821864 13616 solver.cpp:237]     Train net output #1: loss = 0.570469 (* 1 = 0.570469 loss)
I1210 11:07:51.821864 13616 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1210 11:07:57.493471 13616 solver.cpp:218] Iteration 94800 (17.6343 iter/s, 5.67076s/100 iters), loss = 0.693379
I1210 11:07:57.493471 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 11:07:57.493471 13616 solver.cpp:237]     Train net output #1: loss = 0.693379 (* 1 = 0.693379 loss)
I1210 11:07:57.493471 13616 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1210 11:08:03.165942 13616 solver.cpp:218] Iteration 94900 (17.6291 iter/s, 5.67244s/100 iters), loss = 0.679851
I1210 11:08:03.165942 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 11:08:03.165942 13616 solver.cpp:237]     Train net output #1: loss = 0.679851 (* 1 = 0.679851 loss)
I1210 11:08:03.165942 13616 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1210 11:08:08.566373  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:08:08.788396 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_95000.caffemodel
I1210 11:08:08.802397 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_95000.solverstate
I1210 11:08:08.807397 13616 solver.cpp:330] Iteration 95000, Testing net (#0)
I1210 11:08:08.807397 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:08:10.175545 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:08:10.229547 13616 solver.cpp:397]     Test net output #0: accuracy = 0.5766
I1210 11:08:10.229547 13616 solver.cpp:397]     Test net output #1: loss = 1.74864 (* 1 = 1.74864 loss)
I1210 11:08:10.285569 13616 solver.cpp:218] Iteration 95000 (14.0469 iter/s, 7.11903s/100 iters), loss = 0.681928
I1210 11:08:10.285569 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 11:08:10.285569 13616 solver.cpp:237]     Train net output #1: loss = 0.681928 (* 1 = 0.681928 loss)
I1210 11:08:10.285569 13616 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1210 11:08:10.285569 13616 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1210 11:08:15.963035 13616 solver.cpp:218] Iteration 95100 (17.6141 iter/s, 5.67728s/100 iters), loss = 0.586441
I1210 11:08:15.963035 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 11:08:15.963035 13616 solver.cpp:237]     Train net output #1: loss = 0.586441 (* 1 = 0.586441 loss)
I1210 11:08:15.963035 13616 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1210 11:08:21.641968 13616 solver.cpp:218] Iteration 95200 (17.611 iter/s, 5.67827s/100 iters), loss = 0.487049
I1210 11:08:21.642475 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 11:08:21.642475 13616 solver.cpp:237]     Train net output #1: loss = 0.487049 (* 1 = 0.487049 loss)
I1210 11:08:21.642475 13616 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1210 11:08:27.318956 13616 solver.cpp:218] Iteration 95300 (17.6155 iter/s, 5.67682s/100 iters), loss = 0.622961
I1210 11:08:27.318956 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 11:08:27.318956 13616 solver.cpp:237]     Train net output #1: loss = 0.622961 (* 1 = 0.622961 loss)
I1210 11:08:27.318956 13616 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1210 11:08:32.998378 13616 solver.cpp:218] Iteration 95400 (17.6095 iter/s, 5.67875s/100 iters), loss = 0.490283
I1210 11:08:32.998378 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:08:32.998378 13616 solver.cpp:237]     Train net output #1: loss = 0.490283 (* 1 = 0.490283 loss)
I1210 11:08:32.998378 13616 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1210 11:08:38.397786  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:08:38.620800 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_95500.caffemodel
I1210 11:08:38.634799 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_95500.solverstate
I1210 11:08:38.639799 13616 solver.cpp:330] Iteration 95500, Testing net (#0)
I1210 11:08:38.639799 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:08:40.007942 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:08:40.061947 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6821
I1210 11:08:40.061947 13616 solver.cpp:397]     Test net output #1: loss = 1.17265 (* 1 = 1.17265 loss)
I1210 11:08:40.115947 13616 solver.cpp:218] Iteration 95500 (14.05 iter/s, 7.11743s/100 iters), loss = 0.525661
I1210 11:08:40.115947 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:08:40.115947 13616 solver.cpp:237]     Train net output #1: loss = 0.525661 (* 1 = 0.525661 loss)
I1210 11:08:40.115947 13616 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1210 11:08:45.803369 13616 solver.cpp:218] Iteration 95600 (17.5856 iter/s, 5.68647s/100 iters), loss = 0.456466
I1210 11:08:45.803369 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:08:45.803369 13616 solver.cpp:237]     Train net output #1: loss = 0.456466 (* 1 = 0.456466 loss)
I1210 11:08:45.803369 13616 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1210 11:08:51.488747 13616 solver.cpp:218] Iteration 95700 (17.5892 iter/s, 5.6853s/100 iters), loss = 0.469512
I1210 11:08:51.488747 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 11:08:51.488747 13616 solver.cpp:237]     Train net output #1: loss = 0.469512 (* 1 = 0.469512 loss)
I1210 11:08:51.488747 13616 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1210 11:08:57.174172 13616 solver.cpp:218] Iteration 95800 (17.5915 iter/s, 5.68457s/100 iters), loss = 0.554614
I1210 11:08:57.174172 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 11:08:57.174172 13616 solver.cpp:237]     Train net output #1: loss = 0.554614 (* 1 = 0.554614 loss)
I1210 11:08:57.174172 13616 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1210 11:09:02.860608 13616 solver.cpp:218] Iteration 95900 (17.5871 iter/s, 5.68597s/100 iters), loss = 0.506237
I1210 11:09:02.860608 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 11:09:02.860608 13616 solver.cpp:237]     Train net output #1: loss = 0.506237 (* 1 = 0.506237 loss)
I1210 11:09:02.860608 13616 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1210 11:09:08.270087  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:09:08.493103 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_96000.caffemodel
I1210 11:09:08.507102 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_96000.solverstate
I1210 11:09:08.512104 13616 solver.cpp:330] Iteration 96000, Testing net (#0)
I1210 11:09:08.512104 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:09:09.879250 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:09:09.933250 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6864
I1210 11:09:09.933250 13616 solver.cpp:397]     Test net output #1: loss = 1.16055 (* 1 = 1.16055 loss)
I1210 11:09:09.987262 13616 solver.cpp:218] Iteration 96000 (14.033 iter/s, 7.12604s/100 iters), loss = 0.498856
I1210 11:09:09.987262 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:09:09.987262 13616 solver.cpp:237]     Train net output #1: loss = 0.498856 (* 1 = 0.498856 loss)
I1210 11:09:09.987262 13616 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1210 11:09:15.661733 13616 solver.cpp:218] Iteration 96100 (17.6238 iter/s, 5.67414s/100 iters), loss = 0.558332
I1210 11:09:15.661733 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 11:09:15.661733 13616 solver.cpp:237]     Train net output #1: loss = 0.558332 (* 1 = 0.558332 loss)
I1210 11:09:15.661733 13616 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1210 11:09:21.334316 13616 solver.cpp:218] Iteration 96200 (17.6283 iter/s, 5.67268s/100 iters), loss = 0.451809
I1210 11:09:21.334316 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:09:21.334316 13616 solver.cpp:237]     Train net output #1: loss = 0.451809 (* 1 = 0.451809 loss)
I1210 11:09:21.334316 13616 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1210 11:09:27.003724 13616 solver.cpp:218] Iteration 96300 (17.6402 iter/s, 5.66886s/100 iters), loss = 0.504415
I1210 11:09:27.003724 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:09:27.003724 13616 solver.cpp:237]     Train net output #1: loss = 0.504415 (* 1 = 0.504415 loss)
I1210 11:09:27.003724 13616 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1210 11:09:32.675854 13616 solver.cpp:218] Iteration 96400 (17.6321 iter/s, 5.67146s/100 iters), loss = 0.429974
I1210 11:09:32.675854 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:09:32.675854 13616 solver.cpp:237]     Train net output #1: loss = 0.429973 (* 1 = 0.429973 loss)
I1210 11:09:32.675854 13616 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1210 11:09:38.071063  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:09:38.293937 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_96500.caffemodel
I1210 11:09:38.307936 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_96500.solverstate
I1210 11:09:38.312934 13616 solver.cpp:330] Iteration 96500, Testing net (#0)
I1210 11:09:38.312934 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:09:39.677194 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:09:39.730191 13616 solver.cpp:397]     Test net output #0: accuracy = 0.687
I1210 11:09:39.730191 13616 solver.cpp:397]     Test net output #1: loss = 1.16399 (* 1 = 1.16399 loss)
I1210 11:09:39.785897 13616 solver.cpp:218] Iteration 96500 (14.0661 iter/s, 7.10931s/100 iters), loss = 0.384296
I1210 11:09:39.785897 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:09:39.785897 13616 solver.cpp:237]     Train net output #1: loss = 0.384296 (* 1 = 0.384296 loss)
I1210 11:09:39.785897 13616 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1210 11:09:45.464583 13616 solver.cpp:218] Iteration 96600 (17.6114 iter/s, 5.67815s/100 iters), loss = 0.460562
I1210 11:09:45.464583 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 11:09:45.464583 13616 solver.cpp:237]     Train net output #1: loss = 0.460562 (* 1 = 0.460562 loss)
I1210 11:09:45.464583 13616 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1210 11:09:51.142190 13616 solver.cpp:218] Iteration 96700 (17.613 iter/s, 5.67761s/100 iters), loss = 0.330002
I1210 11:09:51.142190 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:09:51.142190 13616 solver.cpp:237]     Train net output #1: loss = 0.330002 (* 1 = 0.330002 loss)
I1210 11:09:51.142190 13616 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1210 11:09:56.818446 13616 solver.cpp:218] Iteration 96800 (17.6179 iter/s, 5.67606s/100 iters), loss = 0.465951
I1210 11:09:56.819437 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:09:56.819437 13616 solver.cpp:237]     Train net output #1: loss = 0.465951 (* 1 = 0.465951 loss)
I1210 11:09:56.819437 13616 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1210 11:10:02.519776 13616 solver.cpp:218] Iteration 96900 (17.5428 iter/s, 5.70036s/100 iters), loss = 0.56401
I1210 11:10:02.519776 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:10:02.519776 13616 solver.cpp:237]     Train net output #1: loss = 0.56401 (* 1 = 0.56401 loss)
I1210 11:10:02.519776 13616 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1210 11:10:07.925225  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:10:08.147943 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_97000.caffemodel
I1210 11:10:08.163951 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_97000.solverstate
I1210 11:10:08.168951 13616 solver.cpp:330] Iteration 97000, Testing net (#0)
I1210 11:10:08.168951 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:10:09.529536 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:10:09.583556 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6906
I1210 11:10:09.583556 13616 solver.cpp:397]     Test net output #1: loss = 1.16062 (* 1 = 1.16062 loss)
I1210 11:10:09.636554 13616 solver.cpp:218] Iteration 97000 (14.0512 iter/s, 7.11681s/100 iters), loss = 0.38179
I1210 11:10:09.636554 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:10:09.637567 13616 solver.cpp:237]     Train net output #1: loss = 0.38179 (* 1 = 0.38179 loss)
I1210 11:10:09.637567 13616 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1210 11:10:15.318217 13616 solver.cpp:218] Iteration 97100 (17.6035 iter/s, 5.6807s/100 iters), loss = 0.405339
I1210 11:10:15.318217 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:10:15.318217 13616 solver.cpp:237]     Train net output #1: loss = 0.405339 (* 1 = 0.405339 loss)
I1210 11:10:15.318217 13616 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1210 11:10:20.998543 13616 solver.cpp:218] Iteration 97200 (17.6052 iter/s, 5.68014s/100 iters), loss = 0.393205
I1210 11:10:20.998543 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:10:20.998543 13616 solver.cpp:237]     Train net output #1: loss = 0.393205 (* 1 = 0.393205 loss)
I1210 11:10:20.998543 13616 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1210 11:10:26.676017 13616 solver.cpp:218] Iteration 97300 (17.6159 iter/s, 5.67668s/100 iters), loss = 0.555732
I1210 11:10:26.676017 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:10:26.676017 13616 solver.cpp:237]     Train net output #1: loss = 0.555732 (* 1 = 0.555732 loss)
I1210 11:10:26.676017 13616 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1210 11:10:32.354332 13616 solver.cpp:218] Iteration 97400 (17.6112 iter/s, 5.6782s/100 iters), loss = 0.456457
I1210 11:10:32.354332 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:10:32.354332 13616 solver.cpp:237]     Train net output #1: loss = 0.456457 (* 1 = 0.456457 loss)
I1210 11:10:32.354332 13616 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1210 11:10:37.753358  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:10:37.976392 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_97500.caffemodel
I1210 11:10:37.990397 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_97500.solverstate
I1210 11:10:37.995379 13616 solver.cpp:330] Iteration 97500, Testing net (#0)
I1210 11:10:37.995379 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:10:39.360781 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:10:39.413797 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6879
I1210 11:10:39.413797 13616 solver.cpp:397]     Test net output #1: loss = 1.17451 (* 1 = 1.17451 loss)
I1210 11:10:39.467808 13616 solver.cpp:218] Iteration 97500 (14.0592 iter/s, 7.11279s/100 iters), loss = 0.39415
I1210 11:10:39.467808 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:10:39.467808 13616 solver.cpp:237]     Train net output #1: loss = 0.39415 (* 1 = 0.39415 loss)
I1210 11:10:39.467808 13616 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1210 11:10:45.160922 13616 solver.cpp:218] Iteration 97600 (17.5669 iter/s, 5.69251s/100 iters), loss = 0.392175
I1210 11:10:45.160922 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:10:45.160922 13616 solver.cpp:237]     Train net output #1: loss = 0.392175 (* 1 = 0.392175 loss)
I1210 11:10:45.160922 13616 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1210 11:10:50.838927 13616 solver.cpp:218] Iteration 97700 (17.6136 iter/s, 5.67744s/100 iters), loss = 0.3803
I1210 11:10:50.838927 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:10:50.838927 13616 solver.cpp:237]     Train net output #1: loss = 0.3803 (* 1 = 0.3803 loss)
I1210 11:10:50.838927 13616 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1210 11:10:56.516743 13616 solver.cpp:218] Iteration 97800 (17.6133 iter/s, 5.67751s/100 iters), loss = 0.520888
I1210 11:10:56.516743 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:10:56.516743 13616 solver.cpp:237]     Train net output #1: loss = 0.520888 (* 1 = 0.520888 loss)
I1210 11:10:56.516743 13616 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1210 11:11:02.192610 13616 solver.cpp:218] Iteration 97900 (17.6183 iter/s, 5.67592s/100 iters), loss = 0.472528
I1210 11:11:02.192610 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:11:02.192610 13616 solver.cpp:237]     Train net output #1: loss = 0.472528 (* 1 = 0.472528 loss)
I1210 11:11:02.192610 13616 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1210 11:11:07.588475  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:11:07.812045 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_98000.caffemodel
I1210 11:11:07.826041 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_98000.solverstate
I1210 11:11:07.831046 13616 solver.cpp:330] Iteration 98000, Testing net (#0)
I1210 11:11:07.831046 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:11:09.197638 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:11:09.250653 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6928
I1210 11:11:09.250653 13616 solver.cpp:397]     Test net output #1: loss = 1.1629 (* 1 = 1.1629 loss)
I1210 11:11:09.304688 13616 solver.cpp:218] Iteration 98000 (14.0616 iter/s, 7.11155s/100 iters), loss = 0.41395
I1210 11:11:09.304688 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:11:09.304688 13616 solver.cpp:237]     Train net output #1: loss = 0.41395 (* 1 = 0.41395 loss)
I1210 11:11:09.304688 13616 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1210 11:11:14.982710 13616 solver.cpp:218] Iteration 98100 (17.614 iter/s, 5.67731s/100 iters), loss = 0.450093
I1210 11:11:14.982710 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 11:11:14.982710 13616 solver.cpp:237]     Train net output #1: loss = 0.450093 (* 1 = 0.450093 loss)
I1210 11:11:14.982710 13616 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1210 11:11:20.657742 13616 solver.cpp:218] Iteration 98200 (17.6217 iter/s, 5.67483s/100 iters), loss = 0.376266
I1210 11:11:20.657742 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:11:20.657742 13616 solver.cpp:237]     Train net output #1: loss = 0.376266 (* 1 = 0.376266 loss)
I1210 11:11:20.657742 13616 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1210 11:11:26.339222 13616 solver.cpp:218] Iteration 98300 (17.6035 iter/s, 5.68069s/100 iters), loss = 0.463209
I1210 11:11:26.339222 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:11:26.339222 13616 solver.cpp:237]     Train net output #1: loss = 0.463209 (* 1 = 0.463209 loss)
I1210 11:11:26.339222 13616 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1210 11:11:32.015602 13616 solver.cpp:218] Iteration 98400 (17.6162 iter/s, 5.67658s/100 iters), loss = 0.483736
I1210 11:11:32.015602 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 11:11:32.015602 13616 solver.cpp:237]     Train net output #1: loss = 0.483736 (* 1 = 0.483736 loss)
I1210 11:11:32.015602 13616 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1210 11:11:37.414968  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:11:37.637989 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_98500.caffemodel
I1210 11:11:37.652989 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_98500.solverstate
I1210 11:11:37.657990 13616 solver.cpp:330] Iteration 98500, Testing net (#0)
I1210 11:11:37.657990 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:11:39.022105 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:11:39.075098 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6872
I1210 11:11:39.075098 13616 solver.cpp:397]     Test net output #1: loss = 1.16381 (* 1 = 1.16381 loss)
I1210 11:11:39.131106 13616 solver.cpp:218] Iteration 98500 (14.0548 iter/s, 7.11498s/100 iters), loss = 0.391821
I1210 11:11:39.131106 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:11:39.131106 13616 solver.cpp:237]     Train net output #1: loss = 0.391821 (* 1 = 0.391821 loss)
I1210 11:11:39.131106 13616 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1210 11:11:44.811555 13616 solver.cpp:218] Iteration 98600 (17.6057 iter/s, 5.67997s/100 iters), loss = 0.357541
I1210 11:11:44.811555 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:11:44.811555 13616 solver.cpp:237]     Train net output #1: loss = 0.357541 (* 1 = 0.357541 loss)
I1210 11:11:44.811555 13616 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1210 11:11:50.493082 13616 solver.cpp:218] Iteration 98700 (17.6033 iter/s, 5.68076s/100 iters), loss = 0.338436
I1210 11:11:50.493082 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:11:50.493082 13616 solver.cpp:237]     Train net output #1: loss = 0.338436 (* 1 = 0.338436 loss)
I1210 11:11:50.493082 13616 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1210 11:11:56.174507 13616 solver.cpp:218] Iteration 98800 (17.6029 iter/s, 5.68089s/100 iters), loss = 0.535755
I1210 11:11:56.174507 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 11:11:56.174507 13616 solver.cpp:237]     Train net output #1: loss = 0.535755 (* 1 = 0.535755 loss)
I1210 11:11:56.174507 13616 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1210 11:12:01.853924 13616 solver.cpp:218] Iteration 98900 (17.6065 iter/s, 5.67973s/100 iters), loss = 0.442016
I1210 11:12:01.853924 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:12:01.853924 13616 solver.cpp:237]     Train net output #1: loss = 0.442016 (* 1 = 0.442016 loss)
I1210 11:12:01.853924 13616 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1210 11:12:07.259352  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:12:07.483358 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_99000.caffemodel
I1210 11:12:07.498363 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_99000.solverstate
I1210 11:12:07.503365 13616 solver.cpp:330] Iteration 99000, Testing net (#0)
I1210 11:12:07.503365 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:12:08.868455 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:12:08.921463 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6894
I1210 11:12:08.921463 13616 solver.cpp:397]     Test net output #1: loss = 1.17011 (* 1 = 1.17011 loss)
I1210 11:12:08.975462 13616 solver.cpp:218] Iteration 99000 (14.0445 iter/s, 7.12025s/100 iters), loss = 0.397775
I1210 11:12:08.975462 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:12:08.975462 13616 solver.cpp:237]     Train net output #1: loss = 0.397775 (* 1 = 0.397775 loss)
I1210 11:12:08.975462 13616 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1210 11:12:14.649863 13616 solver.cpp:218] Iteration 99100 (17.6236 iter/s, 5.67423s/100 iters), loss = 0.340209
I1210 11:12:14.649863 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:12:14.649863 13616 solver.cpp:237]     Train net output #1: loss = 0.340209 (* 1 = 0.340209 loss)
I1210 11:12:14.649863 13616 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1210 11:12:20.326323 13616 solver.cpp:218] Iteration 99200 (17.6179 iter/s, 5.67605s/100 iters), loss = 0.358573
I1210 11:12:20.326323 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:12:20.326323 13616 solver.cpp:237]     Train net output #1: loss = 0.358573 (* 1 = 0.358573 loss)
I1210 11:12:20.326323 13616 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1210 11:12:25.999465 13616 solver.cpp:218] Iteration 99300 (17.6281 iter/s, 5.67275s/100 iters), loss = 0.580653
I1210 11:12:25.999465 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 11:12:25.999465 13616 solver.cpp:237]     Train net output #1: loss = 0.580653 (* 1 = 0.580653 loss)
I1210 11:12:25.999465 13616 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1210 11:12:31.669850 13616 solver.cpp:218] Iteration 99400 (17.6375 iter/s, 5.66975s/100 iters), loss = 0.389252
I1210 11:12:31.669850 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:12:31.669850 13616 solver.cpp:237]     Train net output #1: loss = 0.389252 (* 1 = 0.389252 loss)
I1210 11:12:31.669850 13616 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1210 11:12:37.058202  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:12:37.282240 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_99500.caffemodel
I1210 11:12:37.301242 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_99500.solverstate
I1210 11:12:37.306241 13616 solver.cpp:330] Iteration 99500, Testing net (#0)
I1210 11:12:37.306241 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:12:38.668484 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:12:38.722484 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6877
I1210 11:12:38.722484 13616 solver.cpp:397]     Test net output #1: loss = 1.1791 (* 1 = 1.1791 loss)
I1210 11:12:38.775588 13616 solver.cpp:218] Iteration 99500 (14.0723 iter/s, 7.10618s/100 iters), loss = 0.397714
I1210 11:12:38.776582 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:12:38.776582 13616 solver.cpp:237]     Train net output #1: loss = 0.397714 (* 1 = 0.397714 loss)
I1210 11:12:38.776582 13616 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1210 11:12:44.449970 13616 solver.cpp:218] Iteration 99600 (17.6261 iter/s, 5.6734s/100 iters), loss = 0.468706
I1210 11:12:44.449970 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 11:12:44.449970 13616 solver.cpp:237]     Train net output #1: loss = 0.468706 (* 1 = 0.468706 loss)
I1210 11:12:44.449970 13616 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1210 11:12:50.128145 13616 solver.cpp:218] Iteration 99700 (17.6137 iter/s, 5.67739s/100 iters), loss = 0.369904
I1210 11:12:50.128145 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:12:50.128145 13616 solver.cpp:237]     Train net output #1: loss = 0.369904 (* 1 = 0.369904 loss)
I1210 11:12:50.128145 13616 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1210 11:12:55.811563 13616 solver.cpp:218] Iteration 99800 (17.5947 iter/s, 5.68354s/100 iters), loss = 0.508411
I1210 11:12:55.811563 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 11:12:55.811563 13616 solver.cpp:237]     Train net output #1: loss = 0.508411 (* 1 = 0.508411 loss)
I1210 11:12:55.811563 13616 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1210 11:13:01.486035 13616 solver.cpp:218] Iteration 99900 (17.6241 iter/s, 5.67406s/100 iters), loss = 0.416461
I1210 11:13:01.486035 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:13:01.486035 13616 solver.cpp:237]     Train net output #1: loss = 0.416461 (* 1 = 0.416461 loss)
I1210 11:13:01.486035 13616 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1210 11:13:06.884405  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:13:07.108418 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_100000.caffemodel
I1210 11:13:07.122418 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_100000.solverstate
I1210 11:13:07.127419 13616 solver.cpp:330] Iteration 100000, Testing net (#0)
I1210 11:13:07.127419 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:13:08.492560 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:13:08.546063 13616 solver.cpp:397]     Test net output #0: accuracy = 0.686
I1210 11:13:08.546063 13616 solver.cpp:397]     Test net output #1: loss = 1.17856 (* 1 = 1.17856 loss)
I1210 11:13:08.601564 13616 solver.cpp:218] Iteration 100000 (14.0559 iter/s, 7.11446s/100 iters), loss = 0.366096
I1210 11:13:08.601564 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:13:08.601564 13616 solver.cpp:237]     Train net output #1: loss = 0.366096 (* 1 = 0.366096 loss)
I1210 11:13:08.601564 13616 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1210 11:13:14.291036 13616 solver.cpp:218] Iteration 100100 (17.5781 iter/s, 5.6889s/100 iters), loss = 0.365481
I1210 11:13:14.291036 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:13:14.291036 13616 solver.cpp:237]     Train net output #1: loss = 0.365481 (* 1 = 0.365481 loss)
I1210 11:13:14.291036 13616 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1210 11:13:19.978389 13616 solver.cpp:218] Iteration 100200 (17.5838 iter/s, 5.68705s/100 iters), loss = 0.383356
I1210 11:13:19.978389 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:13:19.978389 13616 solver.cpp:237]     Train net output #1: loss = 0.383355 (* 1 = 0.383355 loss)
I1210 11:13:19.978389 13616 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1210 11:13:25.658778 13616 solver.cpp:218] Iteration 100300 (17.6056 iter/s, 5.68001s/100 iters), loss = 0.519733
I1210 11:13:25.658778 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:13:25.658778 13616 solver.cpp:237]     Train net output #1: loss = 0.519733 (* 1 = 0.519733 loss)
I1210 11:13:25.658778 13616 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1210 11:13:31.344163 13616 solver.cpp:218] Iteration 100400 (17.5905 iter/s, 5.68488s/100 iters), loss = 0.510578
I1210 11:13:31.344163 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 11:13:31.344163 13616 solver.cpp:237]     Train net output #1: loss = 0.510578 (* 1 = 0.510578 loss)
I1210 11:13:31.344163 13616 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1210 11:13:36.741439  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:13:36.963970 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_100500.caffemodel
I1210 11:13:36.977969 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_100500.solverstate
I1210 11:13:36.982970 13616 solver.cpp:330] Iteration 100500, Testing net (#0)
I1210 11:13:36.982970 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:13:38.352063 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:13:38.406059 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6886
I1210 11:13:38.406059 13616 solver.cpp:397]     Test net output #1: loss = 1.18014 (* 1 = 1.18014 loss)
I1210 11:13:38.459619 13616 solver.cpp:218] Iteration 100500 (14.0552 iter/s, 7.11481s/100 iters), loss = 0.353488
I1210 11:13:38.459619 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:13:38.459619 13616 solver.cpp:237]     Train net output #1: loss = 0.353488 (* 1 = 0.353488 loss)
I1210 11:13:38.459619 13616 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1210 11:13:44.167623 13616 solver.cpp:218] Iteration 100600 (17.5192 iter/s, 5.70803s/100 iters), loss = 0.368418
I1210 11:13:44.167623 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:13:44.167623 13616 solver.cpp:237]     Train net output #1: loss = 0.368418 (* 1 = 0.368418 loss)
I1210 11:13:44.167623 13616 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1210 11:13:49.858690 13616 solver.cpp:218] Iteration 100700 (17.573 iter/s, 5.69055s/100 iters), loss = 0.367814
I1210 11:13:49.858690 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:13:49.859680 13616 solver.cpp:237]     Train net output #1: loss = 0.367814 (* 1 = 0.367814 loss)
I1210 11:13:49.859680 13616 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1210 11:13:55.554329 13616 solver.cpp:218] Iteration 100800 (17.5611 iter/s, 5.6944s/100 iters), loss = 0.408218
I1210 11:13:55.554329 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:13:55.554329 13616 solver.cpp:237]     Train net output #1: loss = 0.408218 (* 1 = 0.408218 loss)
I1210 11:13:55.554329 13616 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1210 11:14:01.285995 13616 solver.cpp:218] Iteration 100900 (17.4462 iter/s, 5.73191s/100 iters), loss = 0.420325
I1210 11:14:01.285995 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:14:01.285995 13616 solver.cpp:237]     Train net output #1: loss = 0.420325 (* 1 = 0.420325 loss)
I1210 11:14:01.285995 13616 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1210 11:14:06.735414  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:14:06.960429 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_101000.caffemodel
I1210 11:14:06.976429 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_101000.solverstate
I1210 11:14:06.981429 13616 solver.cpp:330] Iteration 101000, Testing net (#0)
I1210 11:14:06.981429 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:14:08.347569 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:14:08.400569 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6846
I1210 11:14:08.400569 13616 solver.cpp:397]     Test net output #1: loss = 1.1899 (* 1 = 1.1899 loss)
I1210 11:14:08.456574 13616 solver.cpp:218] Iteration 101000 (13.9477 iter/s, 7.16962s/100 iters), loss = 0.341997
I1210 11:14:08.456574 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:14:08.456574 13616 solver.cpp:237]     Train net output #1: loss = 0.341997 (* 1 = 0.341997 loss)
I1210 11:14:08.456574 13616 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1210 11:14:14.134981 13616 solver.cpp:218] Iteration 101100 (17.6121 iter/s, 5.67792s/100 iters), loss = 0.427759
I1210 11:14:14.134981 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:14:14.134981 13616 solver.cpp:237]     Train net output #1: loss = 0.427759 (* 1 = 0.427759 loss)
I1210 11:14:14.134981 13616 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1210 11:14:19.815412 13616 solver.cpp:218] Iteration 101200 (17.6063 iter/s, 5.6798s/100 iters), loss = 0.404173
I1210 11:14:19.815412 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:14:19.815412 13616 solver.cpp:237]     Train net output #1: loss = 0.404173 (* 1 = 0.404173 loss)
I1210 11:14:19.815412 13616 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1210 11:14:25.492887 13616 solver.cpp:218] Iteration 101300 (17.6135 iter/s, 5.67745s/100 iters), loss = 0.419716
I1210 11:14:25.492887 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:14:25.492887 13616 solver.cpp:237]     Train net output #1: loss = 0.419716 (* 1 = 0.419716 loss)
I1210 11:14:25.492887 13616 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1210 11:14:31.165334 13616 solver.cpp:218] Iteration 101400 (17.6322 iter/s, 5.67143s/100 iters), loss = 0.378566
I1210 11:14:31.165334 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:14:31.165334 13616 solver.cpp:237]     Train net output #1: loss = 0.378566 (* 1 = 0.378566 loss)
I1210 11:14:31.165334 13616 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1210 11:14:36.556741  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:14:36.779769 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_101500.caffemodel
I1210 11:14:36.793768 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_101500.solverstate
I1210 11:14:36.798768 13616 solver.cpp:330] Iteration 101500, Testing net (#0)
I1210 11:14:36.798768 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:14:38.169909 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:14:38.223414 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6914
I1210 11:14:38.223414 13616 solver.cpp:397]     Test net output #1: loss = 1.18084 (* 1 = 1.18084 loss)
I1210 11:14:38.276917 13616 solver.cpp:218] Iteration 101500 (14.0608 iter/s, 7.11195s/100 iters), loss = 0.30632
I1210 11:14:38.276917 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:14:38.276917 13616 solver.cpp:237]     Train net output #1: loss = 0.30632 (* 1 = 0.30632 loss)
I1210 11:14:38.276917 13616 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1210 11:14:43.951416 13616 solver.cpp:218] Iteration 101600 (17.626 iter/s, 5.67342s/100 iters), loss = 0.401982
I1210 11:14:43.951416 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:14:43.951416 13616 solver.cpp:237]     Train net output #1: loss = 0.401982 (* 1 = 0.401982 loss)
I1210 11:14:43.951416 13616 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1210 11:14:49.630883 13616 solver.cpp:218] Iteration 101700 (17.6085 iter/s, 5.67906s/100 iters), loss = 0.33145
I1210 11:14:49.630883 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:14:49.630883 13616 solver.cpp:237]     Train net output #1: loss = 0.33145 (* 1 = 0.33145 loss)
I1210 11:14:49.630883 13616 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1210 11:14:55.311336 13616 solver.cpp:218] Iteration 101800 (17.6034 iter/s, 5.68072s/100 iters), loss = 0.472561
I1210 11:14:55.312336 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:14:55.312336 13616 solver.cpp:237]     Train net output #1: loss = 0.472561 (* 1 = 0.472561 loss)
I1210 11:14:55.312336 13616 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1210 11:15:00.990751 13616 solver.cpp:218] Iteration 101900 (17.6098 iter/s, 5.67867s/100 iters), loss = 0.411442
I1210 11:15:00.990751 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:15:00.990751 13616 solver.cpp:237]     Train net output #1: loss = 0.411442 (* 1 = 0.411442 loss)
I1210 11:15:00.990751 13616 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1210 11:15:06.383160  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:15:06.606170 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_102000.caffemodel
I1210 11:15:06.621676 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_102000.solverstate
I1210 11:15:06.626178 13616 solver.cpp:330] Iteration 102000, Testing net (#0)
I1210 11:15:06.626178 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:15:07.991286 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:15:08.045291 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6854
I1210 11:15:08.045291 13616 solver.cpp:397]     Test net output #1: loss = 1.20013 (* 1 = 1.20013 loss)
I1210 11:15:08.100291 13616 solver.cpp:218] Iteration 102000 (14.0675 iter/s, 7.1086s/100 iters), loss = 0.339183
I1210 11:15:08.100291 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:15:08.100291 13616 solver.cpp:237]     Train net output #1: loss = 0.339183 (* 1 = 0.339183 loss)
I1210 11:15:08.100291 13616 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1210 11:15:13.771714 13616 solver.cpp:218] Iteration 102100 (17.6314 iter/s, 5.67169s/100 iters), loss = 0.39497
I1210 11:15:13.771714 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:15:13.771714 13616 solver.cpp:237]     Train net output #1: loss = 0.39497 (* 1 = 0.39497 loss)
I1210 11:15:13.771714 13616 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1210 11:15:19.452272 13616 solver.cpp:218] Iteration 102200 (17.6056 iter/s, 5.68001s/100 iters), loss = 0.27477
I1210 11:15:19.452272 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:15:19.452272 13616 solver.cpp:237]     Train net output #1: loss = 0.27477 (* 1 = 0.27477 loss)
I1210 11:15:19.452272 13616 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1210 11:15:25.120759 13616 solver.cpp:218] Iteration 102300 (17.6445 iter/s, 5.6675s/100 iters), loss = 0.419956
I1210 11:15:25.120759 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:15:25.120759 13616 solver.cpp:237]     Train net output #1: loss = 0.419956 (* 1 = 0.419956 loss)
I1210 11:15:25.120759 13616 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1210 11:15:30.796259 13616 solver.cpp:218] Iteration 102400 (17.6216 iter/s, 5.67485s/100 iters), loss = 0.439459
I1210 11:15:30.796259 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:15:30.796259 13616 solver.cpp:237]     Train net output #1: loss = 0.439459 (* 1 = 0.439459 loss)
I1210 11:15:30.796259 13616 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1210 11:15:36.189734  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:15:36.411748 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_102500.caffemodel
I1210 11:15:36.427253 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_102500.solverstate
I1210 11:15:36.432755 13616 solver.cpp:330] Iteration 102500, Testing net (#0)
I1210 11:15:36.432755 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:15:37.799911 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:15:37.853915 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6881
I1210 11:15:37.853915 13616 solver.cpp:397]     Test net output #1: loss = 1.19038 (* 1 = 1.19038 loss)
I1210 11:15:37.907915 13616 solver.cpp:218] Iteration 102500 (14.062 iter/s, 7.11136s/100 iters), loss = 0.237844
I1210 11:15:37.907915 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:15:37.907915 13616 solver.cpp:237]     Train net output #1: loss = 0.237844 (* 1 = 0.237844 loss)
I1210 11:15:37.907915 13616 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1210 11:15:43.587368 13616 solver.cpp:218] Iteration 102600 (17.6083 iter/s, 5.67914s/100 iters), loss = 0.395421
I1210 11:15:43.587368 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:15:43.587368 13616 solver.cpp:237]     Train net output #1: loss = 0.395421 (* 1 = 0.395421 loss)
I1210 11:15:43.587368 13616 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1210 11:15:49.272830 13616 solver.cpp:218] Iteration 102700 (17.5884 iter/s, 5.68557s/100 iters), loss = 0.235632
I1210 11:15:49.273831 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 11:15:49.273831 13616 solver.cpp:237]     Train net output #1: loss = 0.235632 (* 1 = 0.235632 loss)
I1210 11:15:49.273831 13616 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1210 11:15:54.955258 13616 solver.cpp:218] Iteration 102800 (17.6004 iter/s, 5.68168s/100 iters), loss = 0.363941
I1210 11:15:54.955258 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:15:54.955258 13616 solver.cpp:237]     Train net output #1: loss = 0.363941 (* 1 = 0.363941 loss)
I1210 11:15:54.955258 13616 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1210 11:16:00.638669 13616 solver.cpp:218] Iteration 102900 (17.5961 iter/s, 5.68309s/100 iters), loss = 0.38618
I1210 11:16:00.638669 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:16:00.638669 13616 solver.cpp:237]     Train net output #1: loss = 0.38618 (* 1 = 0.38618 loss)
I1210 11:16:00.638669 13616 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1210 11:16:06.042049  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:16:06.268064 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_103000.caffemodel
I1210 11:16:06.281064 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_103000.solverstate
I1210 11:16:06.286065 13616 solver.cpp:330] Iteration 103000, Testing net (#0)
I1210 11:16:06.286065 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:16:07.654171 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:16:07.707171 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6853
I1210 11:16:07.707171 13616 solver.cpp:397]     Test net output #1: loss = 1.19882 (* 1 = 1.19882 loss)
I1210 11:16:07.761175 13616 solver.cpp:218] Iteration 103000 (14.0408 iter/s, 7.12212s/100 iters), loss = 0.285261
I1210 11:16:07.761175 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:16:07.761175 13616 solver.cpp:237]     Train net output #1: loss = 0.285261 (* 1 = 0.285261 loss)
I1210 11:16:07.761175 13616 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1210 11:16:13.443660 13616 solver.cpp:218] Iteration 103100 (17.5998 iter/s, 5.68189s/100 iters), loss = 0.390332
I1210 11:16:13.443660 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:16:13.443660 13616 solver.cpp:237]     Train net output #1: loss = 0.390332 (* 1 = 0.390332 loss)
I1210 11:16:13.443660 13616 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1210 11:16:19.112767 13616 solver.cpp:218] Iteration 103200 (17.6422 iter/s, 5.66821s/100 iters), loss = 0.324493
I1210 11:16:19.112767 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:16:19.112767 13616 solver.cpp:237]     Train net output #1: loss = 0.324493 (* 1 = 0.324493 loss)
I1210 11:16:19.112767 13616 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1210 11:16:24.792209 13616 solver.cpp:218] Iteration 103300 (17.6088 iter/s, 5.67897s/100 iters), loss = 0.490454
I1210 11:16:24.792209 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 11:16:24.792209 13616 solver.cpp:237]     Train net output #1: loss = 0.490454 (* 1 = 0.490454 loss)
I1210 11:16:24.792209 13616 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1210 11:16:30.474669 13616 solver.cpp:218] Iteration 103400 (17.5977 iter/s, 5.68256s/100 iters), loss = 0.333276
I1210 11:16:30.474669 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:16:30.474669 13616 solver.cpp:237]     Train net output #1: loss = 0.333276 (* 1 = 0.333276 loss)
I1210 11:16:30.474669 13616 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1210 11:16:35.879112  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:16:36.104133 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_103500.caffemodel
I1210 11:16:36.118134 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_103500.solverstate
I1210 11:16:36.123638 13616 solver.cpp:330] Iteration 103500, Testing net (#0)
I1210 11:16:36.123638 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:16:37.490301 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:16:37.545323 13616 solver.cpp:397]     Test net output #0: accuracy = 0.686
I1210 11:16:37.545323 13616 solver.cpp:397]     Test net output #1: loss = 1.19906 (* 1 = 1.19906 loss)
I1210 11:16:37.599319 13616 solver.cpp:218] Iteration 103500 (14.0368 iter/s, 7.12414s/100 iters), loss = 0.268073
I1210 11:16:37.599319 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:16:37.599319 13616 solver.cpp:237]     Train net output #1: loss = 0.268073 (* 1 = 0.268073 loss)
I1210 11:16:37.599319 13616 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1210 11:16:43.275743 13616 solver.cpp:218] Iteration 103600 (17.6173 iter/s, 5.67625s/100 iters), loss = 0.311885
I1210 11:16:43.275743 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:16:43.276742 13616 solver.cpp:237]     Train net output #1: loss = 0.311885 (* 1 = 0.311885 loss)
I1210 11:16:43.276742 13616 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1210 11:16:48.955689 13616 solver.cpp:218] Iteration 103700 (17.609 iter/s, 5.67893s/100 iters), loss = 0.308738
I1210 11:16:48.955689 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:16:48.955689 13616 solver.cpp:237]     Train net output #1: loss = 0.308738 (* 1 = 0.308738 loss)
I1210 11:16:48.955689 13616 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1210 11:16:54.635630 13616 solver.cpp:218] Iteration 103800 (17.6052 iter/s, 5.68015s/100 iters), loss = 0.399708
I1210 11:16:54.636631 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:16:54.636631 13616 solver.cpp:237]     Train net output #1: loss = 0.399708 (* 1 = 0.399708 loss)
I1210 11:16:54.636631 13616 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1210 11:17:00.302494 13616 solver.cpp:218] Iteration 103900 (17.6481 iter/s, 5.66632s/100 iters), loss = 0.395782
I1210 11:17:00.302494 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:17:00.303493 13616 solver.cpp:237]     Train net output #1: loss = 0.395782 (* 1 = 0.395782 loss)
I1210 11:17:00.303493 13616 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1210 11:17:05.696719  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:17:05.921269 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_104000.caffemodel
I1210 11:17:05.935271 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_104000.solverstate
I1210 11:17:05.939270 13616 solver.cpp:330] Iteration 104000, Testing net (#0)
I1210 11:17:05.940269 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:17:07.302086 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:17:07.355592 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6851
I1210 11:17:07.355592 13616 solver.cpp:397]     Test net output #1: loss = 1.20334 (* 1 = 1.20334 loss)
I1210 11:17:07.409107 13616 solver.cpp:218] Iteration 104000 (14.0728 iter/s, 7.10589s/100 iters), loss = 0.422791
I1210 11:17:07.409107 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:17:07.409107 13616 solver.cpp:237]     Train net output #1: loss = 0.422791 (* 1 = 0.422791 loss)
I1210 11:17:07.409107 13616 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1210 11:17:13.082288 13616 solver.cpp:218] Iteration 104100 (17.6277 iter/s, 5.67288s/100 iters), loss = 0.308447
I1210 11:17:13.082288 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:17:13.082288 13616 solver.cpp:237]     Train net output #1: loss = 0.308447 (* 1 = 0.308447 loss)
I1210 11:17:13.082288 13616 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1210 11:17:18.755961 13616 solver.cpp:218] Iteration 104200 (17.6285 iter/s, 5.67263s/100 iters), loss = 0.292338
I1210 11:17:18.755961 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:17:18.755961 13616 solver.cpp:237]     Train net output #1: loss = 0.292338 (* 1 = 0.292338 loss)
I1210 11:17:18.755961 13616 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1210 11:17:24.432801 13616 solver.cpp:218] Iteration 104300 (17.6162 iter/s, 5.67661s/100 iters), loss = 0.396639
I1210 11:17:24.432801 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:17:24.432801 13616 solver.cpp:237]     Train net output #1: loss = 0.396639 (* 1 = 0.396639 loss)
I1210 11:17:24.432801 13616 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1210 11:17:30.113240 13616 solver.cpp:218] Iteration 104400 (17.6048 iter/s, 5.68026s/100 iters), loss = 0.318721
I1210 11:17:30.113240 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:17:30.113240 13616 solver.cpp:237]     Train net output #1: loss = 0.31872 (* 1 = 0.31872 loss)
I1210 11:17:30.113240 13616 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1210 11:17:35.515674  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:17:35.739684 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_104500.caffemodel
I1210 11:17:35.753689 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_104500.solverstate
I1210 11:17:35.758189 13616 solver.cpp:330] Iteration 104500, Testing net (#0)
I1210 11:17:35.758189 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:17:37.123796 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:17:37.177806 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6867
I1210 11:17:37.177806 13616 solver.cpp:397]     Test net output #1: loss = 1.20727 (* 1 = 1.20727 loss)
I1210 11:17:37.230805 13616 solver.cpp:218] Iteration 104500 (14.0509 iter/s, 7.117s/100 iters), loss = 0.307746
I1210 11:17:37.230805 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 11:17:37.230805 13616 solver.cpp:237]     Train net output #1: loss = 0.307746 (* 1 = 0.307746 loss)
I1210 11:17:37.230805 13616 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1210 11:17:42.915251 13616 solver.cpp:218] Iteration 104600 (17.5929 iter/s, 5.6841s/100 iters), loss = 0.305842
I1210 11:17:42.915251 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:17:42.915251 13616 solver.cpp:237]     Train net output #1: loss = 0.305842 (* 1 = 0.305842 loss)
I1210 11:17:42.915251 13616 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1210 11:17:48.595355 13616 solver.cpp:218] Iteration 104700 (17.608 iter/s, 5.67924s/100 iters), loss = 0.310274
I1210 11:17:48.595355 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:17:48.595355 13616 solver.cpp:237]     Train net output #1: loss = 0.310274 (* 1 = 0.310274 loss)
I1210 11:17:48.595355 13616 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1210 11:17:54.275101 13616 solver.cpp:218] Iteration 104800 (17.6087 iter/s, 5.679s/100 iters), loss = 0.423457
I1210 11:17:54.275101 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:17:54.275101 13616 solver.cpp:237]     Train net output #1: loss = 0.423457 (* 1 = 0.423457 loss)
I1210 11:17:54.275101 13616 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1210 11:17:59.950747 13616 solver.cpp:218] Iteration 104900 (17.6199 iter/s, 5.67539s/100 iters), loss = 0.342094
I1210 11:17:59.950747 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:17:59.950747 13616 solver.cpp:237]     Train net output #1: loss = 0.342094 (* 1 = 0.342094 loss)
I1210 11:17:59.950747 13616 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1210 11:18:05.348960  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:18:05.572005 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_105000.caffemodel
I1210 11:18:05.585592 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_105000.solverstate
I1210 11:18:05.590587 13616 solver.cpp:330] Iteration 105000, Testing net (#0)
I1210 11:18:05.590587 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:18:06.953444 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:18:07.008083 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6863
I1210 11:18:07.008083 13616 solver.cpp:397]     Test net output #1: loss = 1.20489 (* 1 = 1.20489 loss)
I1210 11:18:07.062608 13616 solver.cpp:218] Iteration 105000 (14.0621 iter/s, 7.11132s/100 iters), loss = 0.354671
I1210 11:18:07.062608 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 11:18:07.062608 13616 solver.cpp:237]     Train net output #1: loss = 0.354671 (* 1 = 0.354671 loss)
I1210 11:18:07.062608 13616 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1210 11:18:12.731567 13616 solver.cpp:218] Iteration 105100 (17.6409 iter/s, 5.66865s/100 iters), loss = 0.301186
I1210 11:18:12.731567 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:18:12.731567 13616 solver.cpp:237]     Train net output #1: loss = 0.301186 (* 1 = 0.301186 loss)
I1210 11:18:12.731567 13616 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1210 11:18:18.409896 13616 solver.cpp:218] Iteration 105200 (17.6104 iter/s, 5.67845s/100 iters), loss = 0.291911
I1210 11:18:18.409896 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:18:18.409896 13616 solver.cpp:237]     Train net output #1: loss = 0.291911 (* 1 = 0.291911 loss)
I1210 11:18:18.409896 13616 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1210 11:18:24.088080 13616 solver.cpp:218] Iteration 105300 (17.6127 iter/s, 5.67772s/100 iters), loss = 0.415162
I1210 11:18:24.088080 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:18:24.088080 13616 solver.cpp:237]     Train net output #1: loss = 0.415162 (* 1 = 0.415162 loss)
I1210 11:18:24.088080 13616 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1210 11:18:29.762395 13616 solver.cpp:218] Iteration 105400 (17.6269 iter/s, 5.67316s/100 iters), loss = 0.341548
I1210 11:18:29.762395 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:18:29.762395 13616 solver.cpp:237]     Train net output #1: loss = 0.341548 (* 1 = 0.341548 loss)
I1210 11:18:29.762395 13616 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1210 11:18:35.157618  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:18:35.379529 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_105500.caffemodel
I1210 11:18:35.393594 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_105500.solverstate
I1210 11:18:35.397595 13616 solver.cpp:330] Iteration 105500, Testing net (#0)
I1210 11:18:35.398594 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:18:36.761418 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:18:36.815604 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6872
I1210 11:18:36.815604 13616 solver.cpp:397]     Test net output #1: loss = 1.21228 (* 1 = 1.21228 loss)
I1210 11:18:36.870604 13616 solver.cpp:218] Iteration 105500 (14.0693 iter/s, 7.10765s/100 iters), loss = 0.307294
I1210 11:18:36.870604 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:18:36.870604 13616 solver.cpp:237]     Train net output #1: loss = 0.307294 (* 1 = 0.307294 loss)
I1210 11:18:36.870604 13616 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1210 11:18:42.550792 13616 solver.cpp:218] Iteration 105600 (17.6044 iter/s, 5.68039s/100 iters), loss = 0.350916
I1210 11:18:42.550792 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:18:42.550792 13616 solver.cpp:237]     Train net output #1: loss = 0.350916 (* 1 = 0.350916 loss)
I1210 11:18:42.550792 13616 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1210 11:18:48.227530 13616 solver.cpp:218] Iteration 105700 (17.6188 iter/s, 5.67576s/100 iters), loss = 0.344049
I1210 11:18:48.227530 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:18:48.227530 13616 solver.cpp:237]     Train net output #1: loss = 0.344049 (* 1 = 0.344049 loss)
I1210 11:18:48.227530 13616 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1210 11:18:53.907884 13616 solver.cpp:218] Iteration 105800 (17.6066 iter/s, 5.67969s/100 iters), loss = 0.351847
I1210 11:18:53.907884 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:18:53.907884 13616 solver.cpp:237]     Train net output #1: loss = 0.351847 (* 1 = 0.351847 loss)
I1210 11:18:53.907884 13616 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1210 11:18:59.591781 13616 solver.cpp:218] Iteration 105900 (17.5941 iter/s, 5.68371s/100 iters), loss = 0.356121
I1210 11:18:59.591781 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:18:59.591781 13616 solver.cpp:237]     Train net output #1: loss = 0.356121 (* 1 = 0.356121 loss)
I1210 11:18:59.591781 13616 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1210 11:19:04.991953  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:19:05.215966 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_106000.caffemodel
I1210 11:19:05.230976 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_106000.solverstate
I1210 11:19:05.235976 13616 solver.cpp:330] Iteration 106000, Testing net (#0)
I1210 11:19:05.235976 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:19:06.601092 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:19:06.654093 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6882
I1210 11:19:06.654093 13616 solver.cpp:397]     Test net output #1: loss = 1.2092 (* 1 = 1.2092 loss)
I1210 11:19:06.708097 13616 solver.cpp:218] Iteration 106000 (14.0523 iter/s, 7.11628s/100 iters), loss = 0.278853
I1210 11:19:06.708097 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:19:06.708097 13616 solver.cpp:237]     Train net output #1: loss = 0.278853 (* 1 = 0.278853 loss)
I1210 11:19:06.708097 13616 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1210 11:19:12.400571 13616 solver.cpp:218] Iteration 106100 (17.57 iter/s, 5.69153s/100 iters), loss = 0.307328
I1210 11:19:12.400571 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:19:12.400571 13616 solver.cpp:237]     Train net output #1: loss = 0.307328 (* 1 = 0.307328 loss)
I1210 11:19:12.400571 13616 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1210 11:19:18.086575 13616 solver.cpp:218] Iteration 106200 (17.5888 iter/s, 5.68545s/100 iters), loss = 0.253555
I1210 11:19:18.086575 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:19:18.086575 13616 solver.cpp:237]     Train net output #1: loss = 0.253555 (* 1 = 0.253555 loss)
I1210 11:19:18.086575 13616 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1210 11:19:23.771545 13616 solver.cpp:218] Iteration 106300 (17.5901 iter/s, 5.685s/100 iters), loss = 0.408212
I1210 11:19:23.771545 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:19:23.771545 13616 solver.cpp:237]     Train net output #1: loss = 0.408212 (* 1 = 0.408212 loss)
I1210 11:19:23.771545 13616 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1210 11:19:29.457079 13616 solver.cpp:218] Iteration 106400 (17.591 iter/s, 5.68472s/100 iters), loss = 0.316202
I1210 11:19:29.457079 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:19:29.457079 13616 solver.cpp:237]     Train net output #1: loss = 0.316202 (* 1 = 0.316202 loss)
I1210 11:19:29.457079 13616 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1210 11:19:34.865545  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:19:35.090064 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_106500.caffemodel
I1210 11:19:35.104569 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_106500.solverstate
I1210 11:19:35.109570 13616 solver.cpp:330] Iteration 106500, Testing net (#0)
I1210 11:19:35.109570 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:19:36.474704 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:19:36.528710 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6875
I1210 11:19:36.528710 13616 solver.cpp:397]     Test net output #1: loss = 1.2091 (* 1 = 1.2091 loss)
I1210 11:19:36.582712 13616 solver.cpp:218] Iteration 106500 (14.0347 iter/s, 7.12518s/100 iters), loss = 0.356617
I1210 11:19:36.582712 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:19:36.582712 13616 solver.cpp:237]     Train net output #1: loss = 0.356617 (* 1 = 0.356617 loss)
I1210 11:19:36.582712 13616 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1210 11:19:42.268106 13616 solver.cpp:218] Iteration 106600 (17.5908 iter/s, 5.68479s/100 iters), loss = 0.34284
I1210 11:19:42.268106 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:19:42.268106 13616 solver.cpp:237]     Train net output #1: loss = 0.34284 (* 1 = 0.34284 loss)
I1210 11:19:42.268106 13616 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1210 11:19:47.949529 13616 solver.cpp:218] Iteration 106700 (17.6008 iter/s, 5.68155s/100 iters), loss = 0.262381
I1210 11:19:47.949529 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 11:19:47.949529 13616 solver.cpp:237]     Train net output #1: loss = 0.262381 (* 1 = 0.262381 loss)
I1210 11:19:47.949529 13616 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1210 11:19:53.634477 13616 solver.cpp:218] Iteration 106800 (17.5938 iter/s, 5.68381s/100 iters), loss = 0.40202
I1210 11:19:53.634477 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:19:53.634477 13616 solver.cpp:237]     Train net output #1: loss = 0.40202 (* 1 = 0.40202 loss)
I1210 11:19:53.634477 13616 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1210 11:19:59.311969 13616 solver.cpp:218] Iteration 106900 (17.6144 iter/s, 5.67716s/100 iters), loss = 0.297421
I1210 11:19:59.311969 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:19:59.311969 13616 solver.cpp:237]     Train net output #1: loss = 0.297421 (* 1 = 0.297421 loss)
I1210 11:19:59.311969 13616 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1210 11:20:04.745504  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:20:04.969527 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_107000.caffemodel
I1210 11:20:04.984531 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_107000.solverstate
I1210 11:20:04.989531 13616 solver.cpp:330] Iteration 107000, Testing net (#0)
I1210 11:20:04.989531 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:20:06.356622 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:20:06.410635 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6872
I1210 11:20:06.411636 13616 solver.cpp:397]     Test net output #1: loss = 1.2179 (* 1 = 1.2179 loss)
I1210 11:20:06.464635 13616 solver.cpp:218] Iteration 107000 (13.9803 iter/s, 7.15293s/100 iters), loss = 0.343327
I1210 11:20:06.464635 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:20:06.464635 13616 solver.cpp:237]     Train net output #1: loss = 0.343327 (* 1 = 0.343327 loss)
I1210 11:20:06.464635 13616 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1210 11:20:12.147068 13616 solver.cpp:218] Iteration 107100 (17.5992 iter/s, 5.68208s/100 iters), loss = 0.383306
I1210 11:20:12.147068 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:20:12.147068 13616 solver.cpp:237]     Train net output #1: loss = 0.383306 (* 1 = 0.383306 loss)
I1210 11:20:12.147068 13616 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1210 11:20:17.828467 13616 solver.cpp:218] Iteration 107200 (17.6041 iter/s, 5.68049s/100 iters), loss = 0.327775
I1210 11:20:17.828467 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:20:17.828467 13616 solver.cpp:237]     Train net output #1: loss = 0.327775 (* 1 = 0.327775 loss)
I1210 11:20:17.828467 13616 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1210 11:20:23.493850 13616 solver.cpp:218] Iteration 107300 (17.6536 iter/s, 5.66456s/100 iters), loss = 0.401774
I1210 11:20:23.493850 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:20:23.493850 13616 solver.cpp:237]     Train net output #1: loss = 0.401774 (* 1 = 0.401774 loss)
I1210 11:20:23.493850 13616 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1210 11:20:29.170279 13616 solver.cpp:218] Iteration 107400 (17.6173 iter/s, 5.67625s/100 iters), loss = 0.368297
I1210 11:20:29.170279 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:20:29.170279 13616 solver.cpp:237]     Train net output #1: loss = 0.368297 (* 1 = 0.368297 loss)
I1210 11:20:29.170279 13616 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1210 11:20:34.570693  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:20:34.792712 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_107500.caffemodel
I1210 11:20:34.809720 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_107500.solverstate
I1210 11:20:34.815717 13616 solver.cpp:330] Iteration 107500, Testing net (#0)
I1210 11:20:34.815717 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:20:36.181854 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:20:36.235867 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6878
I1210 11:20:36.235867 13616 solver.cpp:397]     Test net output #1: loss = 1.21549 (* 1 = 1.21549 loss)
I1210 11:20:36.289887 13616 solver.cpp:218] Iteration 107500 (14.0467 iter/s, 7.1191s/100 iters), loss = 0.364247
I1210 11:20:36.289887 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:20:36.290387 13616 solver.cpp:237]     Train net output #1: loss = 0.364247 (* 1 = 0.364247 loss)
I1210 11:20:36.290387 13616 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1210 11:20:41.959322 13616 solver.cpp:218] Iteration 107600 (17.6393 iter/s, 5.66915s/100 iters), loss = 0.320758
I1210 11:20:41.959322 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:20:41.959322 13616 solver.cpp:237]     Train net output #1: loss = 0.320758 (* 1 = 0.320758 loss)
I1210 11:20:41.959322 13616 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1210 11:20:47.639760 13616 solver.cpp:218] Iteration 107700 (17.6073 iter/s, 5.67948s/100 iters), loss = 0.278566
I1210 11:20:47.639760 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 11:20:47.639760 13616 solver.cpp:237]     Train net output #1: loss = 0.278566 (* 1 = 0.278566 loss)
I1210 11:20:47.639760 13616 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1210 11:20:53.306191 13616 solver.cpp:218] Iteration 107800 (17.6474 iter/s, 5.66656s/100 iters), loss = 0.425604
I1210 11:20:53.306191 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:20:53.306191 13616 solver.cpp:237]     Train net output #1: loss = 0.425605 (* 1 = 0.425605 loss)
I1210 11:20:53.306191 13616 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1210 11:20:58.975821 13616 solver.cpp:218] Iteration 107900 (17.6404 iter/s, 5.66881s/100 iters), loss = 0.334417
I1210 11:20:58.975821 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:20:58.975821 13616 solver.cpp:237]     Train net output #1: loss = 0.334417 (* 1 = 0.334417 loss)
I1210 11:20:58.975821 13616 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1210 11:21:04.373188  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:21:04.598721 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_108000.caffemodel
I1210 11:21:04.614228 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_108000.solverstate
I1210 11:21:04.619227 13616 solver.cpp:330] Iteration 108000, Testing net (#0)
I1210 11:21:04.619227 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:21:05.980342 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:21:06.036337 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6886
I1210 11:21:06.036337 13616 solver.cpp:397]     Test net output #1: loss = 1.21294 (* 1 = 1.21294 loss)
I1210 11:21:06.090838 13616 solver.cpp:218] Iteration 108000 (14.0557 iter/s, 7.11453s/100 iters), loss = 0.221045
I1210 11:21:06.090838 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:21:06.090838 13616 solver.cpp:237]     Train net output #1: loss = 0.221045 (* 1 = 0.221045 loss)
I1210 11:21:06.090838 13616 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1210 11:21:11.767738 13616 solver.cpp:218] Iteration 108100 (17.6144 iter/s, 5.67717s/100 iters), loss = 0.329604
I1210 11:21:11.768739 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:21:11.768739 13616 solver.cpp:237]     Train net output #1: loss = 0.329604 (* 1 = 0.329604 loss)
I1210 11:21:11.768739 13616 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1210 11:21:17.447170 13616 solver.cpp:218] Iteration 108200 (17.6112 iter/s, 5.6782s/100 iters), loss = 0.283602
I1210 11:21:17.447170 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:21:17.447170 13616 solver.cpp:237]     Train net output #1: loss = 0.283602 (* 1 = 0.283602 loss)
I1210 11:21:17.447170 13616 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1210 11:21:23.119258 13616 solver.cpp:218] Iteration 108300 (17.6303 iter/s, 5.67205s/100 iters), loss = 0.426933
I1210 11:21:23.119258 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:21:23.119258 13616 solver.cpp:237]     Train net output #1: loss = 0.426933 (* 1 = 0.426933 loss)
I1210 11:21:23.119258 13616 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1210 11:21:28.787147 13616 solver.cpp:218] Iteration 108400 (17.6442 iter/s, 5.6676s/100 iters), loss = 0.278432
I1210 11:21:28.787147 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:21:28.787147 13616 solver.cpp:237]     Train net output #1: loss = 0.278433 (* 1 = 0.278433 loss)
I1210 11:21:28.787147 13616 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1210 11:21:34.189420  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:21:34.413472 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_108500.caffemodel
I1210 11:21:34.430477 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_108500.solverstate
I1210 11:21:34.435472 13616 solver.cpp:330] Iteration 108500, Testing net (#0)
I1210 11:21:34.435472 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:21:35.802793 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:21:35.856719 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6863
I1210 11:21:35.856719 13616 solver.cpp:397]     Test net output #1: loss = 1.21588 (* 1 = 1.21588 loss)
I1210 11:21:35.912719 13616 solver.cpp:218] Iteration 108500 (14.0366 iter/s, 7.12423s/100 iters), loss = 0.311789
I1210 11:21:35.912719 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:21:35.912719 13616 solver.cpp:237]     Train net output #1: loss = 0.311789 (* 1 = 0.311789 loss)
I1210 11:21:35.912719 13616 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1210 11:21:41.581481 13616 solver.cpp:218] Iteration 108600 (17.6391 iter/s, 5.66923s/100 iters), loss = 0.321477
I1210 11:21:41.581481 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:21:41.582482 13616 solver.cpp:237]     Train net output #1: loss = 0.321478 (* 1 = 0.321478 loss)
I1210 11:21:41.582482 13616 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1210 11:21:47.259884 13616 solver.cpp:218] Iteration 108700 (17.6141 iter/s, 5.67727s/100 iters), loss = 0.259578
I1210 11:21:47.259884 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:21:47.259884 13616 solver.cpp:237]     Train net output #1: loss = 0.259578 (* 1 = 0.259578 loss)
I1210 11:21:47.259884 13616 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1210 11:21:52.940832 13616 solver.cpp:218] Iteration 108800 (17.6047 iter/s, 5.68029s/100 iters), loss = 0.418758
I1210 11:21:52.940832 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:21:52.940832 13616 solver.cpp:237]     Train net output #1: loss = 0.418758 (* 1 = 0.418758 loss)
I1210 11:21:52.940832 13616 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1210 11:21:58.613783 13616 solver.cpp:218] Iteration 108900 (17.6273 iter/s, 5.67303s/100 iters), loss = 0.373122
I1210 11:21:58.613783 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:21:58.613783 13616 solver.cpp:237]     Train net output #1: loss = 0.373123 (* 1 = 0.373123 loss)
I1210 11:21:58.613783 13616 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1210 11:22:04.009178  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:22:04.232190 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_109000.caffemodel
I1210 11:22:04.248196 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_109000.solverstate
I1210 11:22:04.253197 13616 solver.cpp:330] Iteration 109000, Testing net (#0)
I1210 11:22:04.253197 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:22:05.619302 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:22:05.673341 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6869
I1210 11:22:05.673341 13616 solver.cpp:397]     Test net output #1: loss = 1.22073 (* 1 = 1.22073 loss)
I1210 11:22:05.727334 13616 solver.cpp:218] Iteration 109000 (14.0586 iter/s, 7.11306s/100 iters), loss = 0.304715
I1210 11:22:05.727334 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:22:05.727334 13616 solver.cpp:237]     Train net output #1: loss = 0.304715 (* 1 = 0.304715 loss)
I1210 11:22:05.727334 13616 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1210 11:22:11.407713 13616 solver.cpp:218] Iteration 109100 (17.6046 iter/s, 5.68032s/100 iters), loss = 0.25158
I1210 11:22:11.407713 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:22:11.407713 13616 solver.cpp:237]     Train net output #1: loss = 0.25158 (* 1 = 0.25158 loss)
I1210 11:22:11.407713 13616 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1210 11:22:17.080154 13616 solver.cpp:218] Iteration 109200 (17.6325 iter/s, 5.67136s/100 iters), loss = 0.325435
I1210 11:22:17.080154 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:22:17.080154 13616 solver.cpp:237]     Train net output #1: loss = 0.325435 (* 1 = 0.325435 loss)
I1210 11:22:17.080154 13616 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1210 11:22:22.746546 13616 solver.cpp:218] Iteration 109300 (17.6483 iter/s, 5.66626s/100 iters), loss = 0.394757
I1210 11:22:22.746546 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:22:22.746546 13616 solver.cpp:237]     Train net output #1: loss = 0.394757 (* 1 = 0.394757 loss)
I1210 11:22:22.746546 13616 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1210 11:22:28.414010 13616 solver.cpp:218] Iteration 109400 (17.6458 iter/s, 5.66706s/100 iters), loss = 0.424037
I1210 11:22:28.414010 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:22:28.414010 13616 solver.cpp:237]     Train net output #1: loss = 0.424037 (* 1 = 0.424037 loss)
I1210 11:22:28.414010 13616 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1210 11:22:33.810653  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:22:34.033666 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_109500.caffemodel
I1210 11:22:34.046671 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_109500.solverstate
I1210 11:22:34.051676 13616 solver.cpp:330] Iteration 109500, Testing net (#0)
I1210 11:22:34.051676 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:22:35.417811 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:22:35.470820 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6833
I1210 11:22:35.470820 13616 solver.cpp:397]     Test net output #1: loss = 1.23395 (* 1 = 1.23395 loss)
I1210 11:22:35.526820 13616 solver.cpp:218] Iteration 109500 (14.0601 iter/s, 7.11233s/100 iters), loss = 0.216999
I1210 11:22:35.526820 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 11:22:35.526820 13616 solver.cpp:237]     Train net output #1: loss = 0.216999 (* 1 = 0.216999 loss)
I1210 11:22:35.526820 13616 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1210 11:22:41.205780 13616 solver.cpp:218] Iteration 109600 (17.6099 iter/s, 5.67862s/100 iters), loss = 0.326032
I1210 11:22:41.205780 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:22:41.205780 13616 solver.cpp:237]     Train net output #1: loss = 0.326032 (* 1 = 0.326032 loss)
I1210 11:22:41.205780 13616 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1210 11:22:46.871678 13616 solver.cpp:218] Iteration 109700 (17.6496 iter/s, 5.66583s/100 iters), loss = 0.304463
I1210 11:22:46.871678 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:22:46.871678 13616 solver.cpp:237]     Train net output #1: loss = 0.304464 (* 1 = 0.304464 loss)
I1210 11:22:46.871678 13616 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1210 11:22:52.546123 13616 solver.cpp:218] Iteration 109800 (17.6242 iter/s, 5.67402s/100 iters), loss = 0.404861
I1210 11:22:52.546123 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:22:52.546123 13616 solver.cpp:237]     Train net output #1: loss = 0.404861 (* 1 = 0.404861 loss)
I1210 11:22:52.546123 13616 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1210 11:22:58.221441 13616 solver.cpp:218] Iteration 109900 (17.6221 iter/s, 5.6747s/100 iters), loss = 0.344832
I1210 11:22:58.221441 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:22:58.221441 13616 solver.cpp:237]     Train net output #1: loss = 0.344832 (* 1 = 0.344832 loss)
I1210 11:22:58.221441 13616 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1210 11:23:03.618832  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:23:03.842844 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_110000.caffemodel
I1210 11:23:03.857348 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_110000.solverstate
I1210 11:23:03.862349 13616 solver.cpp:330] Iteration 110000, Testing net (#0)
I1210 11:23:03.862349 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:23:05.228942 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:23:05.281946 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6834
I1210 11:23:05.281946 13616 solver.cpp:397]     Test net output #1: loss = 1.22179 (* 1 = 1.22179 loss)
I1210 11:23:05.336946 13616 solver.cpp:218] Iteration 110000 (14.0545 iter/s, 7.11513s/100 iters), loss = 0.308918
I1210 11:23:05.336946 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:23:05.336946 13616 solver.cpp:237]     Train net output #1: loss = 0.308919 (* 1 = 0.308919 loss)
I1210 11:23:05.336946 13616 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1210 11:23:11.021389 13616 solver.cpp:218] Iteration 110100 (17.5934 iter/s, 5.68396s/100 iters), loss = 0.366636
I1210 11:23:11.021389 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:23:11.021389 13616 solver.cpp:237]     Train net output #1: loss = 0.366636 (* 1 = 0.366636 loss)
I1210 11:23:11.021389 13616 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1210 11:23:16.702819 13616 solver.cpp:218] Iteration 110200 (17.6025 iter/s, 5.68101s/100 iters), loss = 0.285865
I1210 11:23:16.702819 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:23:16.702819 13616 solver.cpp:237]     Train net output #1: loss = 0.285865 (* 1 = 0.285865 loss)
I1210 11:23:16.702819 13616 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1210 11:23:22.376301 13616 solver.cpp:218] Iteration 110300 (17.6288 iter/s, 5.67252s/100 iters), loss = 0.394817
I1210 11:23:22.376301 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:23:22.376301 13616 solver.cpp:237]     Train net output #1: loss = 0.394817 (* 1 = 0.394817 loss)
I1210 11:23:22.376301 13616 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1210 11:23:28.049635 13616 solver.cpp:218] Iteration 110400 (17.6282 iter/s, 5.67274s/100 iters), loss = 0.422253
I1210 11:23:28.049635 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:23:28.049635 13616 solver.cpp:237]     Train net output #1: loss = 0.422254 (* 1 = 0.422254 loss)
I1210 11:23:28.049635 13616 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1210 11:23:33.444025  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:23:33.666055 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_110500.caffemodel
I1210 11:23:33.685060 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_110500.solverstate
I1210 11:23:33.694058 13616 solver.cpp:330] Iteration 110500, Testing net (#0)
I1210 11:23:33.694058 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:23:35.062233 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:23:35.116237 13616 solver.cpp:397]     Test net output #0: accuracy = 0.686
I1210 11:23:35.117238 13616 solver.cpp:397]     Test net output #1: loss = 1.22257 (* 1 = 1.22257 loss)
I1210 11:23:35.171254 13616 solver.cpp:218] Iteration 110500 (14.0426 iter/s, 7.12118s/100 iters), loss = 0.393023
I1210 11:23:35.171254 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:23:35.171254 13616 solver.cpp:237]     Train net output #1: loss = 0.393023 (* 1 = 0.393023 loss)
I1210 11:23:35.171254 13616 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1210 11:23:40.850864 13616 solver.cpp:218] Iteration 110600 (17.6056 iter/s, 5.68001s/100 iters), loss = 0.286744
I1210 11:23:40.851866 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:23:40.851866 13616 solver.cpp:237]     Train net output #1: loss = 0.286744 (* 1 = 0.286744 loss)
I1210 11:23:40.851866 13616 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1210 11:23:46.529335 13616 solver.cpp:218] Iteration 110700 (17.6139 iter/s, 5.67732s/100 iters), loss = 0.260935
I1210 11:23:46.529335 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:23:46.529335 13616 solver.cpp:237]     Train net output #1: loss = 0.260935 (* 1 = 0.260935 loss)
I1210 11:23:46.529335 13616 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1210 11:23:52.209717 13616 solver.cpp:218] Iteration 110800 (17.6061 iter/s, 5.67985s/100 iters), loss = 0.365656
I1210 11:23:52.209717 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:23:52.209717 13616 solver.cpp:237]     Train net output #1: loss = 0.365656 (* 1 = 0.365656 loss)
I1210 11:23:52.209717 13616 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1210 11:23:57.895529 13616 solver.cpp:218] Iteration 110900 (17.5895 iter/s, 5.68521s/100 iters), loss = 0.369462
I1210 11:23:57.895529 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:23:57.895529 13616 solver.cpp:237]     Train net output #1: loss = 0.369462 (* 1 = 0.369462 loss)
I1210 11:23:57.895529 13616 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1210 11:24:03.290544  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:24:03.514920 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_111000.caffemodel
I1210 11:24:03.530462 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_111000.solverstate
I1210 11:24:03.535461 13616 solver.cpp:330] Iteration 111000, Testing net (#0)
I1210 11:24:03.535461 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:24:04.897176 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:24:04.950764 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6844
I1210 11:24:04.950764 13616 solver.cpp:397]     Test net output #1: loss = 1.23352 (* 1 = 1.23352 loss)
I1210 11:24:05.006296 13616 solver.cpp:218] Iteration 111000 (14.064 iter/s, 7.11035s/100 iters), loss = 0.325129
I1210 11:24:05.006296 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:24:05.006296 13616 solver.cpp:237]     Train net output #1: loss = 0.325129 (* 1 = 0.325129 loss)
I1210 11:24:05.006296 13616 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1210 11:24:10.679301 13616 solver.cpp:218] Iteration 111100 (17.6262 iter/s, 5.67337s/100 iters), loss = 0.302805
I1210 11:24:10.679301 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 11:24:10.679301 13616 solver.cpp:237]     Train net output #1: loss = 0.302805 (* 1 = 0.302805 loss)
I1210 11:24:10.679301 13616 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1210 11:24:16.355108 13616 solver.cpp:218] Iteration 111200 (17.6212 iter/s, 5.67498s/100 iters), loss = 0.256248
I1210 11:24:16.355108 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:24:16.355108 13616 solver.cpp:237]     Train net output #1: loss = 0.256248 (* 1 = 0.256248 loss)
I1210 11:24:16.355108 13616 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1210 11:24:22.027524 13616 solver.cpp:218] Iteration 111300 (17.6291 iter/s, 5.67243s/100 iters), loss = 0.383473
I1210 11:24:22.028523 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:24:22.028523 13616 solver.cpp:237]     Train net output #1: loss = 0.383473 (* 1 = 0.383473 loss)
I1210 11:24:22.028523 13616 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1210 11:24:27.700043 13616 solver.cpp:218] Iteration 111400 (17.6328 iter/s, 5.67123s/100 iters), loss = 0.310405
I1210 11:24:27.700043 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:24:27.700043 13616 solver.cpp:237]     Train net output #1: loss = 0.310405 (* 1 = 0.310405 loss)
I1210 11:24:27.700043 13616 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1210 11:24:33.098459  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:24:33.321547 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_111500.caffemodel
I1210 11:24:33.335558 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_111500.solverstate
I1210 11:24:33.339566 13616 solver.cpp:330] Iteration 111500, Testing net (#0)
I1210 11:24:33.339566 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:24:34.707924 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:24:34.760951 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6844
I1210 11:24:34.760951 13616 solver.cpp:397]     Test net output #1: loss = 1.23374 (* 1 = 1.23374 loss)
I1210 11:24:34.815970 13616 solver.cpp:218] Iteration 111500 (14.0541 iter/s, 7.11537s/100 iters), loss = 0.241614
I1210 11:24:34.815970 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:24:34.815970 13616 solver.cpp:237]     Train net output #1: loss = 0.241614 (* 1 = 0.241614 loss)
I1210 11:24:34.815970 13616 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1210 11:24:40.481946 13616 solver.cpp:218] Iteration 111600 (17.6476 iter/s, 5.66651s/100 iters), loss = 0.229033
I1210 11:24:40.482947 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:24:40.482947 13616 solver.cpp:237]     Train net output #1: loss = 0.229034 (* 1 = 0.229034 loss)
I1210 11:24:40.482947 13616 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1210 11:24:46.156519 13616 solver.cpp:218] Iteration 111700 (17.6243 iter/s, 5.674s/100 iters), loss = 0.306088
I1210 11:24:46.156519 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:24:46.156519 13616 solver.cpp:237]     Train net output #1: loss = 0.306088 (* 1 = 0.306088 loss)
I1210 11:24:46.156519 13616 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1210 11:24:51.819689 13616 solver.cpp:218] Iteration 111800 (17.6607 iter/s, 5.66229s/100 iters), loss = 0.385666
I1210 11:24:51.820196 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:24:51.820196 13616 solver.cpp:237]     Train net output #1: loss = 0.385666 (* 1 = 0.385666 loss)
I1210 11:24:51.820196 13616 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1210 11:24:57.486026 13616 solver.cpp:218] Iteration 111900 (17.6508 iter/s, 5.66546s/100 iters), loss = 0.347203
I1210 11:24:57.486026 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:24:57.486026 13616 solver.cpp:237]     Train net output #1: loss = 0.347204 (* 1 = 0.347204 loss)
I1210 11:24:57.486026 13616 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1210 11:25:02.879032  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:25:03.101114 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_112000.caffemodel
I1210 11:25:03.116112 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_112000.solverstate
I1210 11:25:03.121137 13616 solver.cpp:330] Iteration 112000, Testing net (#0)
I1210 11:25:03.121137 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:25:04.482884 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:25:04.536902 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6849
I1210 11:25:04.536902 13616 solver.cpp:397]     Test net output #1: loss = 1.23304 (* 1 = 1.23304 loss)
I1210 11:25:04.591917 13616 solver.cpp:218] Iteration 112000 (14.0738 iter/s, 7.1054s/100 iters), loss = 0.306456
I1210 11:25:04.591917 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:25:04.591917 13616 solver.cpp:237]     Train net output #1: loss = 0.306456 (* 1 = 0.306456 loss)
I1210 11:25:04.591917 13616 sgd_solver.cpp:105] Iteration 112000, lr = 0.001
I1210 11:25:10.257995 13616 solver.cpp:218] Iteration 112100 (17.6489 iter/s, 5.66608s/100 iters), loss = 0.211887
I1210 11:25:10.257995 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 11:25:10.257995 13616 solver.cpp:237]     Train net output #1: loss = 0.211887 (* 1 = 0.211887 loss)
I1210 11:25:10.257995 13616 sgd_solver.cpp:105] Iteration 112100, lr = 0.001
I1210 11:25:15.927038 13616 solver.cpp:218] Iteration 112200 (17.6421 iter/s, 5.66825s/100 iters), loss = 0.24417
I1210 11:25:15.927038 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:25:15.927038 13616 solver.cpp:237]     Train net output #1: loss = 0.24417 (* 1 = 0.24417 loss)
I1210 11:25:15.927038 13616 sgd_solver.cpp:105] Iteration 112200, lr = 0.001
I1210 11:25:21.602246 13616 solver.cpp:218] Iteration 112300 (17.62 iter/s, 5.67537s/100 iters), loss = 0.365576
I1210 11:25:21.602246 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:25:21.602246 13616 solver.cpp:237]     Train net output #1: loss = 0.365576 (* 1 = 0.365576 loss)
I1210 11:25:21.602246 13616 sgd_solver.cpp:105] Iteration 112300, lr = 0.001
I1210 11:25:27.274255 13616 solver.cpp:218] Iteration 112400 (17.6316 iter/s, 5.67162s/100 iters), loss = 0.305946
I1210 11:25:27.274255 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:25:27.274255 13616 solver.cpp:237]     Train net output #1: loss = 0.305946 (* 1 = 0.305946 loss)
I1210 11:25:27.274255 13616 sgd_solver.cpp:105] Iteration 112400, lr = 0.001
I1210 11:25:32.674650  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:25:32.895721 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_112500.caffemodel
I1210 11:25:32.909736 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_112500.solverstate
I1210 11:25:32.914726 13616 solver.cpp:330] Iteration 112500, Testing net (#0)
I1210 11:25:32.914726 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:25:34.280277 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:25:34.334316 13616 solver.cpp:397]     Test net output #0: accuracy = 0.685
I1210 11:25:34.334316 13616 solver.cpp:397]     Test net output #1: loss = 1.23475 (* 1 = 1.23475 loss)
I1210 11:25:34.388332 13616 solver.cpp:218] Iteration 112500 (14.0587 iter/s, 7.11302s/100 iters), loss = 0.279144
I1210 11:25:34.388332 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 11:25:34.388332 13616 solver.cpp:237]     Train net output #1: loss = 0.279144 (* 1 = 0.279144 loss)
I1210 11:25:34.388332 13616 sgd_solver.cpp:105] Iteration 112500, lr = 0.001
I1210 11:25:40.059347 13616 solver.cpp:218] Iteration 112600 (17.6346 iter/s, 5.67068s/100 iters), loss = 0.269672
I1210 11:25:40.059347 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:25:40.059347 13616 solver.cpp:237]     Train net output #1: loss = 0.269672 (* 1 = 0.269672 loss)
I1210 11:25:40.059347 13616 sgd_solver.cpp:105] Iteration 112600, lr = 0.001
I1210 11:25:45.728754 13616 solver.cpp:218] Iteration 112700 (17.6375 iter/s, 5.66974s/100 iters), loss = 0.312961
I1210 11:25:45.729759 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:25:45.729759 13616 solver.cpp:237]     Train net output #1: loss = 0.312961 (* 1 = 0.312961 loss)
I1210 11:25:45.729759 13616 sgd_solver.cpp:105] Iteration 112700, lr = 0.001
I1210 11:25:51.401633 13616 solver.cpp:218] Iteration 112800 (17.6319 iter/s, 5.67154s/100 iters), loss = 0.369739
I1210 11:25:51.401633 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:25:51.401633 13616 solver.cpp:237]     Train net output #1: loss = 0.369739 (* 1 = 0.369739 loss)
I1210 11:25:51.401633 13616 sgd_solver.cpp:105] Iteration 112800, lr = 0.001
I1210 11:25:57.072027 13616 solver.cpp:218] Iteration 112900 (17.6346 iter/s, 5.67067s/100 iters), loss = 0.334891
I1210 11:25:57.072027 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:25:57.072027 13616 solver.cpp:237]     Train net output #1: loss = 0.334891 (* 1 = 0.334891 loss)
I1210 11:25:57.072027 13616 sgd_solver.cpp:105] Iteration 112900, lr = 0.001
I1210 11:26:02.460430  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:26:02.683655 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_113000.caffemodel
I1210 11:26:02.697654 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_113000.solverstate
I1210 11:26:02.702654 13616 solver.cpp:330] Iteration 113000, Testing net (#0)
I1210 11:26:02.702654 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:26:04.066046 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:26:04.120045 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6867
I1210 11:26:04.120045 13616 solver.cpp:397]     Test net output #1: loss = 1.22764 (* 1 = 1.22764 loss)
I1210 11:26:04.173575 13616 solver.cpp:218] Iteration 113000 (14.0835 iter/s, 7.10051s/100 iters), loss = 0.314876
I1210 11:26:04.173575 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:26:04.173575 13616 solver.cpp:237]     Train net output #1: loss = 0.314876 (* 1 = 0.314876 loss)
I1210 11:26:04.173575 13616 sgd_solver.cpp:105] Iteration 113000, lr = 0.001
I1210 11:26:09.846086 13616 solver.cpp:218] Iteration 113100 (17.6302 iter/s, 5.67209s/100 iters), loss = 0.284925
I1210 11:26:09.846086 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1210 11:26:09.846086 13616 solver.cpp:237]     Train net output #1: loss = 0.284925 (* 1 = 0.284925 loss)
I1210 11:26:09.846086 13616 sgd_solver.cpp:105] Iteration 113100, lr = 0.001
I1210 11:26:15.519496 13616 solver.cpp:218] Iteration 113200 (17.6257 iter/s, 5.67354s/100 iters), loss = 0.248148
I1210 11:26:15.519496 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:26:15.519496 13616 solver.cpp:237]     Train net output #1: loss = 0.248148 (* 1 = 0.248148 loss)
I1210 11:26:15.520498 13616 sgd_solver.cpp:105] Iteration 113200, lr = 0.001
I1210 11:26:21.195030 13616 solver.cpp:218] Iteration 113300 (17.6228 iter/s, 5.67446s/100 iters), loss = 0.38609
I1210 11:26:21.195030 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:26:21.195030 13616 solver.cpp:237]     Train net output #1: loss = 0.38609 (* 1 = 0.38609 loss)
I1210 11:26:21.195030 13616 sgd_solver.cpp:105] Iteration 113300, lr = 0.001
I1210 11:26:26.865432 13616 solver.cpp:218] Iteration 113400 (17.637 iter/s, 5.6699s/100 iters), loss = 0.367614
I1210 11:26:26.865432 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:26:26.865432 13616 solver.cpp:237]     Train net output #1: loss = 0.367614 (* 1 = 0.367614 loss)
I1210 11:26:26.865432 13616 sgd_solver.cpp:105] Iteration 113400, lr = 0.001
I1210 11:26:32.263602  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:26:32.486151 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_113500.caffemodel
I1210 11:26:32.500535 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_113500.solverstate
I1210 11:26:32.505534 13616 solver.cpp:330] Iteration 113500, Testing net (#0)
I1210 11:26:32.505534 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:26:33.866533 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:26:33.920043 13616 solver.cpp:397]     Test net output #0: accuracy = 0.684
I1210 11:26:33.920043 13616 solver.cpp:397]     Test net output #1: loss = 1.24541 (* 1 = 1.24541 loss)
I1210 11:26:33.974056 13616 solver.cpp:218] Iteration 113500 (14.0686 iter/s, 7.10804s/100 iters), loss = 0.23533
I1210 11:26:33.974056 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:26:33.974056 13616 solver.cpp:237]     Train net output #1: loss = 0.23533 (* 1 = 0.23533 loss)
I1210 11:26:33.974056 13616 sgd_solver.cpp:105] Iteration 113500, lr = 0.001
I1210 11:26:39.646312 13616 solver.cpp:218] Iteration 113600 (17.6286 iter/s, 5.67261s/100 iters), loss = 0.289008
I1210 11:26:39.646312 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:26:39.646312 13616 solver.cpp:237]     Train net output #1: loss = 0.289009 (* 1 = 0.289009 loss)
I1210 11:26:39.646312 13616 sgd_solver.cpp:105] Iteration 113600, lr = 0.001
I1210 11:26:45.318790 13616 solver.cpp:218] Iteration 113700 (17.6315 iter/s, 5.67166s/100 iters), loss = 0.258597
I1210 11:26:45.318790 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:26:45.318790 13616 solver.cpp:237]     Train net output #1: loss = 0.258597 (* 1 = 0.258597 loss)
I1210 11:26:45.318790 13616 sgd_solver.cpp:105] Iteration 113700, lr = 0.001
I1210 11:26:50.993108 13616 solver.cpp:218] Iteration 113800 (17.6253 iter/s, 5.67368s/100 iters), loss = 0.378016
I1210 11:26:50.993108 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:26:50.993108 13616 solver.cpp:237]     Train net output #1: loss = 0.378016 (* 1 = 0.378016 loss)
I1210 11:26:50.993108 13616 sgd_solver.cpp:105] Iteration 113800, lr = 0.001
I1210 11:26:56.661738 13616 solver.cpp:218] Iteration 113900 (17.6423 iter/s, 5.66819s/100 iters), loss = 0.266723
I1210 11:26:56.661738 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:26:56.661738 13616 solver.cpp:237]     Train net output #1: loss = 0.266723 (* 1 = 0.266723 loss)
I1210 11:26:56.661738 13616 sgd_solver.cpp:105] Iteration 113900, lr = 0.001
I1210 11:27:02.056828  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:27:02.281395 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_114000.caffemodel
I1210 11:27:02.295910 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_114000.solverstate
I1210 11:27:02.300415 13616 solver.cpp:330] Iteration 114000, Testing net (#0)
I1210 11:27:02.300415 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:27:03.665393 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:27:03.718428 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6857
I1210 11:27:03.718428 13616 solver.cpp:397]     Test net output #1: loss = 1.24268 (* 1 = 1.24268 loss)
I1210 11:27:03.772433 13616 solver.cpp:218] Iteration 114000 (14.0627 iter/s, 7.11099s/100 iters), loss = 0.250475
I1210 11:27:03.773433 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 11:27:03.773433 13616 solver.cpp:237]     Train net output #1: loss = 0.250475 (* 1 = 0.250475 loss)
I1210 11:27:03.773433 13616 sgd_solver.cpp:105] Iteration 114000, lr = 0.001
I1210 11:27:09.448606 13616 solver.cpp:218] Iteration 114100 (17.6192 iter/s, 5.67563s/100 iters), loss = 0.265713
I1210 11:27:09.448606 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:27:09.449606 13616 solver.cpp:237]     Train net output #1: loss = 0.265713 (* 1 = 0.265713 loss)
I1210 11:27:09.449606 13616 sgd_solver.cpp:105] Iteration 114100, lr = 0.001
I1210 11:27:15.122541 13616 solver.cpp:218] Iteration 114200 (17.626 iter/s, 5.67342s/100 iters), loss = 0.207573
I1210 11:27:15.122541 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 11:27:15.122541 13616 solver.cpp:237]     Train net output #1: loss = 0.207573 (* 1 = 0.207573 loss)
I1210 11:27:15.122541 13616 sgd_solver.cpp:105] Iteration 114200, lr = 0.001
I1210 11:27:20.796808 13616 solver.cpp:218] Iteration 114300 (17.6271 iter/s, 5.67309s/100 iters), loss = 0.357325
I1210 11:27:20.796808 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:27:20.796808 13616 solver.cpp:237]     Train net output #1: loss = 0.357325 (* 1 = 0.357325 loss)
I1210 11:27:20.796808 13616 sgd_solver.cpp:105] Iteration 114300, lr = 0.001
I1210 11:27:26.467078 13616 solver.cpp:218] Iteration 114400 (17.636 iter/s, 5.67021s/100 iters), loss = 0.326822
I1210 11:27:26.467078 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:27:26.467078 13616 solver.cpp:237]     Train net output #1: loss = 0.326822 (* 1 = 0.326822 loss)
I1210 11:27:26.467078 13616 sgd_solver.cpp:105] Iteration 114400, lr = 0.001
I1210 11:27:31.861611  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:27:32.086621 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_114500.caffemodel
I1210 11:27:32.099622 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_114500.solverstate
I1210 11:27:32.104622 13616 solver.cpp:330] Iteration 114500, Testing net (#0)
I1210 11:27:32.104622 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:27:33.468781 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:27:33.522794 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6835
I1210 11:27:33.522794 13616 solver.cpp:397]     Test net output #1: loss = 1.25407 (* 1 = 1.25407 loss)
I1210 11:27:33.577795 13616 solver.cpp:218] Iteration 114500 (14.064 iter/s, 7.11034s/100 iters), loss = 0.360861
I1210 11:27:33.577795 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:27:33.577795 13616 solver.cpp:237]     Train net output #1: loss = 0.360861 (* 1 = 0.360861 loss)
I1210 11:27:33.577795 13616 sgd_solver.cpp:105] Iteration 114500, lr = 0.001
I1210 11:27:39.242756 13616 solver.cpp:218] Iteration 114600 (17.6545 iter/s, 5.66428s/100 iters), loss = 0.303302
I1210 11:27:39.242756 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:27:39.242756 13616 solver.cpp:237]     Train net output #1: loss = 0.303302 (* 1 = 0.303302 loss)
I1210 11:27:39.242756 13616 sgd_solver.cpp:105] Iteration 114600, lr = 0.001
I1210 11:27:44.917366 13616 solver.cpp:218] Iteration 114700 (17.6238 iter/s, 5.67415s/100 iters), loss = 0.245365
I1210 11:27:44.917366 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 11:27:44.917366 13616 solver.cpp:237]     Train net output #1: loss = 0.245365 (* 1 = 0.245365 loss)
I1210 11:27:44.917366 13616 sgd_solver.cpp:105] Iteration 114700, lr = 0.001
I1210 11:27:50.592324 13616 solver.cpp:218] Iteration 114800 (17.6221 iter/s, 5.6747s/100 iters), loss = 0.421782
I1210 11:27:50.592324 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:27:50.592324 13616 solver.cpp:237]     Train net output #1: loss = 0.421782 (* 1 = 0.421782 loss)
I1210 11:27:50.592324 13616 sgd_solver.cpp:105] Iteration 114800, lr = 0.001
I1210 11:27:56.262941 13616 solver.cpp:218] Iteration 114900 (17.6358 iter/s, 5.6703s/100 iters), loss = 0.379123
I1210 11:27:56.262941 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:27:56.262941 13616 solver.cpp:237]     Train net output #1: loss = 0.379123 (* 1 = 0.379123 loss)
I1210 11:27:56.262941 13616 sgd_solver.cpp:105] Iteration 114900, lr = 0.001
I1210 11:28:01.663456  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:28:01.887465 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_115000.caffemodel
I1210 11:28:01.902465 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_115000.solverstate
I1210 11:28:01.906466 13616 solver.cpp:330] Iteration 115000, Testing net (#0)
I1210 11:28:01.907466 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:28:03.272574 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:28:03.326576 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6829
I1210 11:28:03.326576 13616 solver.cpp:397]     Test net output #1: loss = 1.25746 (* 1 = 1.25746 loss)
I1210 11:28:03.380579 13616 solver.cpp:218] Iteration 115000 (14.0518 iter/s, 7.11655s/100 iters), loss = 0.315837
I1210 11:28:03.380579 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:28:03.380579 13616 solver.cpp:237]     Train net output #1: loss = 0.315837 (* 1 = 0.315837 loss)
I1210 11:28:03.380579 13616 sgd_solver.cpp:105] Iteration 115000, lr = 0.001
I1210 11:28:09.056928 13616 solver.cpp:218] Iteration 115100 (17.6177 iter/s, 5.6761s/100 iters), loss = 0.283178
I1210 11:28:09.056928 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:28:09.056928 13616 solver.cpp:237]     Train net output #1: loss = 0.283178 (* 1 = 0.283178 loss)
I1210 11:28:09.056928 13616 sgd_solver.cpp:105] Iteration 115100, lr = 0.001
I1210 11:28:14.731428 13616 solver.cpp:218] Iteration 115200 (17.6224 iter/s, 5.6746s/100 iters), loss = 0.224735
I1210 11:28:14.731428 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 11:28:14.731428 13616 solver.cpp:237]     Train net output #1: loss = 0.224735 (* 1 = 0.224735 loss)
I1210 11:28:14.731428 13616 sgd_solver.cpp:105] Iteration 115200, lr = 0.001
I1210 11:28:20.408856 13616 solver.cpp:218] Iteration 115300 (17.6146 iter/s, 5.6771s/100 iters), loss = 0.364168
I1210 11:28:20.408856 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:28:20.408856 13616 solver.cpp:237]     Train net output #1: loss = 0.364169 (* 1 = 0.364169 loss)
I1210 11:28:20.408856 13616 sgd_solver.cpp:105] Iteration 115300, lr = 0.001
I1210 11:28:26.092264 13616 solver.cpp:218] Iteration 115400 (17.5968 iter/s, 5.68286s/100 iters), loss = 0.30778
I1210 11:28:26.092264 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:28:26.092264 13616 solver.cpp:237]     Train net output #1: loss = 0.30778 (* 1 = 0.30778 loss)
I1210 11:28:26.092264 13616 sgd_solver.cpp:105] Iteration 115400, lr = 0.001
I1210 11:28:31.489639  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:28:31.711652 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_115500.caffemodel
I1210 11:28:31.726655 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_115500.solverstate
I1210 11:28:31.731659 13616 solver.cpp:330] Iteration 115500, Testing net (#0)
I1210 11:28:31.731659 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:28:33.097790 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:28:33.152356 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6853
I1210 11:28:33.153357 13616 solver.cpp:397]     Test net output #1: loss = 1.24945 (* 1 = 1.24945 loss)
I1210 11:28:33.206336 13616 solver.cpp:218] Iteration 115500 (14.0575 iter/s, 7.11363s/100 iters), loss = 0.244539
I1210 11:28:33.206336 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:28:33.206336 13616 solver.cpp:237]     Train net output #1: loss = 0.244539 (* 1 = 0.244539 loss)
I1210 11:28:33.206336 13616 sgd_solver.cpp:105] Iteration 115500, lr = 0.001
I1210 11:28:38.889250 13616 solver.cpp:218] Iteration 115600 (17.5994 iter/s, 5.68202s/100 iters), loss = 0.290833
I1210 11:28:38.889250 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:28:38.889250 13616 solver.cpp:237]     Train net output #1: loss = 0.290833 (* 1 = 0.290833 loss)
I1210 11:28:38.889250 13616 sgd_solver.cpp:105] Iteration 115600, lr = 0.001
I1210 11:28:44.575814 13616 solver.cpp:218] Iteration 115700 (17.585 iter/s, 5.68668s/100 iters), loss = 0.245674
I1210 11:28:44.575814 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:28:44.575814 13616 solver.cpp:237]     Train net output #1: loss = 0.245674 (* 1 = 0.245674 loss)
I1210 11:28:44.575814 13616 sgd_solver.cpp:105] Iteration 115700, lr = 0.001
I1210 11:28:50.262284 13616 solver.cpp:218] Iteration 115800 (17.5882 iter/s, 5.68562s/100 iters), loss = 0.340094
I1210 11:28:50.262284 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:28:50.262284 13616 solver.cpp:237]     Train net output #1: loss = 0.340094 (* 1 = 0.340094 loss)
I1210 11:28:50.262284 13616 sgd_solver.cpp:105] Iteration 115800, lr = 0.001
I1210 11:28:55.939713 13616 solver.cpp:218] Iteration 115900 (17.6136 iter/s, 5.67742s/100 iters), loss = 0.362556
I1210 11:28:55.939713 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:28:55.939713 13616 solver.cpp:237]     Train net output #1: loss = 0.362556 (* 1 = 0.362556 loss)
I1210 11:28:55.939713 13616 sgd_solver.cpp:105] Iteration 115900, lr = 0.001
I1210 11:29:01.335211  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:29:01.558220 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_116000.caffemodel
I1210 11:29:01.572221 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_116000.solverstate
I1210 11:29:01.577221 13616 solver.cpp:330] Iteration 116000, Testing net (#0)
I1210 11:29:01.577221 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:29:02.941339 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:29:02.994338 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6816
I1210 11:29:02.994338 13616 solver.cpp:397]     Test net output #1: loss = 1.24773 (* 1 = 1.24773 loss)
I1210 11:29:03.048355 13616 solver.cpp:218] Iteration 116000 (14.0682 iter/s, 7.10825s/100 iters), loss = 0.289845
I1210 11:29:03.049356 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:29:03.049356 13616 solver.cpp:237]     Train net output #1: loss = 0.289845 (* 1 = 0.289845 loss)
I1210 11:29:03.049356 13616 sgd_solver.cpp:105] Iteration 116000, lr = 0.001
I1210 11:29:08.727315 13616 solver.cpp:218] Iteration 116100 (17.6122 iter/s, 5.67787s/100 iters), loss = 0.273495
I1210 11:29:08.727315 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:29:08.727315 13616 solver.cpp:237]     Train net output #1: loss = 0.273495 (* 1 = 0.273495 loss)
I1210 11:29:08.727315 13616 sgd_solver.cpp:105] Iteration 116100, lr = 0.001
I1210 11:29:14.410387 13616 solver.cpp:218] Iteration 116200 (17.598 iter/s, 5.68246s/100 iters), loss = 0.276332
I1210 11:29:14.410387 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:29:14.410387 13616 solver.cpp:237]     Train net output #1: loss = 0.276332 (* 1 = 0.276332 loss)
I1210 11:29:14.410387 13616 sgd_solver.cpp:105] Iteration 116200, lr = 0.001
I1210 11:29:20.090858 13616 solver.cpp:218] Iteration 116300 (17.6039 iter/s, 5.68057s/100 iters), loss = 0.424455
I1210 11:29:20.090858 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 11:29:20.090858 13616 solver.cpp:237]     Train net output #1: loss = 0.424455 (* 1 = 0.424455 loss)
I1210 11:29:20.090858 13616 sgd_solver.cpp:105] Iteration 116300, lr = 0.001
I1210 11:29:25.766546 13616 solver.cpp:218] Iteration 116400 (17.622 iter/s, 5.67471s/100 iters), loss = 0.339658
I1210 11:29:25.766546 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:29:25.766546 13616 solver.cpp:237]     Train net output #1: loss = 0.339658 (* 1 = 0.339658 loss)
I1210 11:29:25.766546 13616 sgd_solver.cpp:105] Iteration 116400, lr = 0.001
I1210 11:29:31.165114  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:29:31.390139 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_116500.caffemodel
I1210 11:29:31.404139 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_116500.solverstate
I1210 11:29:31.408139 13616 solver.cpp:330] Iteration 116500, Testing net (#0)
I1210 11:29:31.408139 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:29:32.774279 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:29:32.828284 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6812
I1210 11:29:32.828284 13616 solver.cpp:397]     Test net output #1: loss = 1.26002 (* 1 = 1.26002 loss)
I1210 11:29:32.882284 13616 solver.cpp:218] Iteration 116500 (14.0544 iter/s, 7.11519s/100 iters), loss = 0.244478
I1210 11:29:32.882284 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:29:32.882284 13616 solver.cpp:237]     Train net output #1: loss = 0.244478 (* 1 = 0.244478 loss)
I1210 11:29:32.882284 13616 sgd_solver.cpp:105] Iteration 116500, lr = 0.001
I1210 11:29:38.551755 13616 solver.cpp:218] Iteration 116600 (17.6382 iter/s, 5.66953s/100 iters), loss = 0.334293
I1210 11:29:38.551755 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:29:38.551755 13616 solver.cpp:237]     Train net output #1: loss = 0.334293 (* 1 = 0.334293 loss)
I1210 11:29:38.551755 13616 sgd_solver.cpp:105] Iteration 116600, lr = 0.001
I1210 11:29:44.228200 13616 solver.cpp:218] Iteration 116700 (17.619 iter/s, 5.67569s/100 iters), loss = 0.262347
I1210 11:29:44.228200 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:29:44.228200 13616 solver.cpp:237]     Train net output #1: loss = 0.262347 (* 1 = 0.262347 loss)
I1210 11:29:44.228200 13616 sgd_solver.cpp:105] Iteration 116700, lr = 0.001
I1210 11:29:49.899696 13616 solver.cpp:218] Iteration 116800 (17.6331 iter/s, 5.67116s/100 iters), loss = 0.297997
I1210 11:29:49.899696 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:29:49.899696 13616 solver.cpp:237]     Train net output #1: loss = 0.297997 (* 1 = 0.297997 loss)
I1210 11:29:49.899696 13616 sgd_solver.cpp:105] Iteration 116800, lr = 0.001
I1210 11:29:55.568239 13616 solver.cpp:218] Iteration 116900 (17.6423 iter/s, 5.66819s/100 iters), loss = 0.301664
I1210 11:29:55.568239 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:29:55.568239 13616 solver.cpp:237]     Train net output #1: loss = 0.301664 (* 1 = 0.301664 loss)
I1210 11:29:55.568239 13616 sgd_solver.cpp:105] Iteration 116900, lr = 0.001
I1210 11:30:00.979781  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:30:01.213829 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_117000.caffemodel
I1210 11:30:01.231832 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_117000.solverstate
I1210 11:30:01.236333 13616 solver.cpp:330] Iteration 117000, Testing net (#0)
I1210 11:30:01.236333 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:30:02.616267 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:30:02.671277 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6813
I1210 11:30:02.671277 13616 solver.cpp:397]     Test net output #1: loss = 1.25064 (* 1 = 1.25064 loss)
I1210 11:30:02.725280 13616 solver.cpp:218] Iteration 117000 (13.9732 iter/s, 7.15653s/100 iters), loss = 0.344273
I1210 11:30:02.725780 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:30:02.725780 13616 solver.cpp:237]     Train net output #1: loss = 0.344273 (* 1 = 0.344273 loss)
I1210 11:30:02.725780 13616 sgd_solver.cpp:105] Iteration 117000, lr = 0.001
I1210 11:30:08.404762 13616 solver.cpp:218] Iteration 117100 (17.608 iter/s, 5.67923s/100 iters), loss = 0.26458
I1210 11:30:08.404762 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:30:08.404762 13616 solver.cpp:237]     Train net output #1: loss = 0.26458 (* 1 = 0.26458 loss)
I1210 11:30:08.404762 13616 sgd_solver.cpp:105] Iteration 117100, lr = 0.001
I1210 11:30:14.073242 13616 solver.cpp:218] Iteration 117200 (17.6432 iter/s, 5.66792s/100 iters), loss = 0.300531
I1210 11:30:14.073242 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:30:14.073242 13616 solver.cpp:237]     Train net output #1: loss = 0.300531 (* 1 = 0.300531 loss)
I1210 11:30:14.073242 13616 sgd_solver.cpp:105] Iteration 117200, lr = 0.001
I1210 11:30:19.740808 13616 solver.cpp:218] Iteration 117300 (17.6449 iter/s, 5.66736s/100 iters), loss = 0.316001
I1210 11:30:19.740808 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:30:19.740808 13616 solver.cpp:237]     Train net output #1: loss = 0.316001 (* 1 = 0.316001 loss)
I1210 11:30:19.740808 13616 sgd_solver.cpp:105] Iteration 117300, lr = 0.001
I1210 11:30:25.412245 13616 solver.cpp:218] Iteration 117400 (17.6352 iter/s, 5.67048s/100 iters), loss = 0.347381
I1210 11:30:25.412245 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:30:25.412245 13616 solver.cpp:237]     Train net output #1: loss = 0.347381 (* 1 = 0.347381 loss)
I1210 11:30:25.412245 13616 sgd_solver.cpp:105] Iteration 117400, lr = 0.001
I1210 11:30:30.813691  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:30:31.034716 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_117500.caffemodel
I1210 11:30:31.049721 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_117500.solverstate
I1210 11:30:31.054721 13616 solver.cpp:330] Iteration 117500, Testing net (#0)
I1210 11:30:31.054721 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:30:32.427562 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:30:32.481067 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6847
I1210 11:30:32.481067 13616 solver.cpp:397]     Test net output #1: loss = 1.26043 (* 1 = 1.26043 loss)
I1210 11:30:32.535070 13616 solver.cpp:218] Iteration 117500 (14.0404 iter/s, 7.12229s/100 iters), loss = 0.292871
I1210 11:30:32.535070 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 11:30:32.535070 13616 solver.cpp:237]     Train net output #1: loss = 0.292871 (* 1 = 0.292871 loss)
I1210 11:30:32.535070 13616 sgd_solver.cpp:105] Iteration 117500, lr = 0.001
I1210 11:30:38.210481 13616 solver.cpp:218] Iteration 117600 (17.6199 iter/s, 5.6754s/100 iters), loss = 0.358973
I1210 11:30:38.210481 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:30:38.210481 13616 solver.cpp:237]     Train net output #1: loss = 0.358973 (* 1 = 0.358973 loss)
I1210 11:30:38.210481 13616 sgd_solver.cpp:105] Iteration 117600, lr = 0.001
I1210 11:30:43.879884 13616 solver.cpp:218] Iteration 117700 (17.6407 iter/s, 5.6687s/100 iters), loss = 0.237652
I1210 11:30:43.879884 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 11:30:43.879884 13616 solver.cpp:237]     Train net output #1: loss = 0.237652 (* 1 = 0.237652 loss)
I1210 11:30:43.879884 13616 sgd_solver.cpp:105] Iteration 117700, lr = 0.001
I1210 11:30:49.559325 13616 solver.cpp:218] Iteration 117800 (17.6087 iter/s, 5.679s/100 iters), loss = 0.319623
I1210 11:30:49.559325 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:30:49.559325 13616 solver.cpp:237]     Train net output #1: loss = 0.319623 (* 1 = 0.319623 loss)
I1210 11:30:49.559325 13616 sgd_solver.cpp:105] Iteration 117800, lr = 0.001
I1210 11:30:55.225880 13616 solver.cpp:218] Iteration 117900 (17.6493 iter/s, 5.66596s/100 iters), loss = 0.349406
I1210 11:30:55.225880 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 11:30:55.225880 13616 solver.cpp:237]     Train net output #1: loss = 0.349406 (* 1 = 0.349406 loss)
I1210 11:30:55.225880 13616 sgd_solver.cpp:105] Iteration 117900, lr = 0.001
I1210 11:31:00.622509  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:31:00.843171 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_118000.caffemodel
I1210 11:31:00.857170 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_118000.solverstate
I1210 11:31:00.861169 13616 solver.cpp:330] Iteration 118000, Testing net (#0)
I1210 11:31:00.861169 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:31:02.225930 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:31:02.280534 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6815
I1210 11:31:02.280534 13616 solver.cpp:397]     Test net output #1: loss = 1.25187 (* 1 = 1.25187 loss)
I1210 11:31:02.335530 13616 solver.cpp:218] Iteration 118000 (14.0665 iter/s, 7.10907s/100 iters), loss = 0.234817
I1210 11:31:02.335530 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 11:31:02.335530 13616 solver.cpp:237]     Train net output #1: loss = 0.234817 (* 1 = 0.234817 loss)
I1210 11:31:02.335530 13616 sgd_solver.cpp:105] Iteration 118000, lr = 0.001
I1210 11:31:08.009943 13616 solver.cpp:218] Iteration 118100 (17.6231 iter/s, 5.67436s/100 iters), loss = 0.262772
I1210 11:31:08.009943 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:31:08.009943 13616 solver.cpp:237]     Train net output #1: loss = 0.262772 (* 1 = 0.262772 loss)
I1210 11:31:08.009943 13616 sgd_solver.cpp:105] Iteration 118100, lr = 0.001
I1210 11:31:13.687757 13616 solver.cpp:218] Iteration 118200 (17.6125 iter/s, 5.67777s/100 iters), loss = 0.22067
I1210 11:31:13.687757 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 11:31:13.687757 13616 solver.cpp:237]     Train net output #1: loss = 0.22067 (* 1 = 0.22067 loss)
I1210 11:31:13.687757 13616 sgd_solver.cpp:105] Iteration 118200, lr = 0.001
I1210 11:31:19.370483 13616 solver.cpp:218] Iteration 118300 (17.5993 iter/s, 5.68205s/100 iters), loss = 0.324577
I1210 11:31:19.370483 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:31:19.370483 13616 solver.cpp:237]     Train net output #1: loss = 0.324577 (* 1 = 0.324577 loss)
I1210 11:31:19.370483 13616 sgd_solver.cpp:105] Iteration 118300, lr = 0.001
I1210 11:31:25.040966 13616 solver.cpp:218] Iteration 118400 (17.6377 iter/s, 5.66966s/100 iters), loss = 0.340162
I1210 11:31:25.040966 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 11:31:25.040966 13616 solver.cpp:237]     Train net output #1: loss = 0.340162 (* 1 = 0.340162 loss)
I1210 11:31:25.040966 13616 sgd_solver.cpp:105] Iteration 118400, lr = 0.001
I1210 11:31:30.438537  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:31:30.659775 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_118500.caffemodel
I1210 11:31:30.673771 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_118500.solverstate
I1210 11:31:30.677772 13616 solver.cpp:330] Iteration 118500, Testing net (#0)
I1210 11:31:30.678774 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:31:32.042884 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:31:32.095597 13616 solver.cpp:397]     Test net output #0: accuracy = 0.685
I1210 11:31:32.095597 13616 solver.cpp:397]     Test net output #1: loss = 1.25322 (* 1 = 1.25322 loss)
I1210 11:31:32.152613 13616 solver.cpp:218] Iteration 118500 (14.0624 iter/s, 7.11118s/100 iters), loss = 0.301078
I1210 11:31:32.152613 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:31:32.152613 13616 solver.cpp:237]     Train net output #1: loss = 0.301078 (* 1 = 0.301078 loss)
I1210 11:31:32.152613 13616 sgd_solver.cpp:105] Iteration 118500, lr = 0.001
I1210 11:31:37.828953 13616 solver.cpp:218] Iteration 118600 (17.6169 iter/s, 5.67636s/100 iters), loss = 0.305659
I1210 11:31:37.828953 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:31:37.828953 13616 solver.cpp:237]     Train net output #1: loss = 0.305659 (* 1 = 0.305659 loss)
I1210 11:31:37.828953 13616 sgd_solver.cpp:105] Iteration 118600, lr = 0.001
I1210 11:31:43.507966 13616 solver.cpp:218] Iteration 118700 (17.6105 iter/s, 5.67843s/100 iters), loss = 0.304546
I1210 11:31:43.507966 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 11:31:43.507966 13616 solver.cpp:237]     Train net output #1: loss = 0.304546 (* 1 = 0.304546 loss)
I1210 11:31:43.507966 13616 sgd_solver.cpp:105] Iteration 118700, lr = 0.001
I1210 11:31:49.184015 13616 solver.cpp:218] Iteration 118800 (17.621 iter/s, 5.67506s/100 iters), loss = 0.259671
I1210 11:31:49.184015 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 11:31:49.184015 13616 solver.cpp:237]     Train net output #1: loss = 0.259671 (* 1 = 0.259671 loss)
I1210 11:31:49.184015 13616 sgd_solver.cpp:105] Iteration 118800, lr = 0.001
I1210 11:31:54.869514 13616 solver.cpp:218] Iteration 118900 (17.5892 iter/s, 5.68529s/100 iters), loss = 0.280569
I1210 11:31:54.869514 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 11:31:54.869514 13616 solver.cpp:237]     Train net output #1: loss = 0.280569 (* 1 = 0.280569 loss)
I1210 11:31:54.869514 13616 sgd_solver.cpp:105] Iteration 118900, lr = 0.001
I1210 11:32:00.271566  5296 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:32:00.494593 13616 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_119000.caffemodel
I1210 11:32:00.508594 13616 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_13_v2_iter_119000.solverstate
I1210 11:32:00.513604 13616 solver.cpp:330] Iteration 119000, Testing net (#0)
I1210 11:32:00.513604 13616 net.cpp:676] Ignoring source layer accuracy_training
I1210 11:32:01.882961 16220 data_layer.cpp:73] Restarting data prefetching from start.
I1210 11:32:01.936980 13616 solver.cpp:397]     Test net output #0: accuracy = 0.6832
I1210 11:32:01.936980 13616 solver.cpp:397]     Test net output #1: loss = 1.26464 (* 1 = 1.26464 loss)
I1210 11:32:01.990972 13616 solver.cpp:218] Iteration 119000 (14.0417 iter/s, 7.12164s/100 iters), loss = 0.336016
I1210 11:32:01.990972 13616 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 11:32:01.991973 13616 solver.cpp:237]     Train net output #1: loss = 0.336016 (* 1 = 0.336016 loss)
I1210 11:32:01.991973 13616 sgd_solver.cpp:105] Iteration 119000, lr = 0.001
I1210 11:32:07.668421 13616 solver.cpp:218] Iteration 119100 (17.6159 iter/s, 5.67668s/100 iters), loss = 0.226564
I1210 11:32:07.668421 13616 solver.cpp:237]     Train net output #0: accuracy_t
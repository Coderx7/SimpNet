
G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_90000.solverstate 
I1210 21:25:23.402171 11576 caffe.cpp:219] Using GPUs 0
I1210 21:25:23.578196 11576 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1210 21:25:23.899116 11576 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 21:25:23.915616 11576 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1210 21:25:23.916116 11576 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 21:25:23.917115 11576 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 21:25:23.917115 11576 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1210 21:25:23.917115 11576 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1210 21:25:23.917618 11576 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV1_360k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "pool2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "pool4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1210 21:25:23.918618 11576 layer_factory.cpp:58] Creating layer cifar
I1210 21:25:23.921618 11576 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1210 21:25:23.921618 11576 net.cpp:84] Creating Layer cifar
I1210 21:25:23.921618 11576 net.cpp:380] cifar -> data
I1210 21:25:23.921618 11576 net.cpp:380] cifar -> label
I1210 21:25:23.922619 11576 data_layer.cpp:45] output data size: 100,3,32,32
I1210 21:25:23.930119 11576 net.cpp:122] Setting up cifar
I1210 21:25:23.930119 11576 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1210 21:25:23.930119 11576 net.cpp:129] Top shape: 100 (100)
I1210 21:25:23.930119 11576 net.cpp:137] Memory required for data: 1229200
I1210 21:25:23.930119 11576 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1210 21:25:23.930119 11576 net.cpp:84] Creating Layer label_cifar_1_split
I1210 21:25:23.930119 11576 net.cpp:406] label_cifar_1_split <- label
I1210 21:25:23.930119 11576 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1210 21:25:23.930119 11576 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1210 21:25:23.930119 11576 net.cpp:122] Setting up label_cifar_1_split
I1210 21:25:23.930119 11576 net.cpp:129] Top shape: 100 (100)
I1210 21:25:23.930119 11576 net.cpp:129] Top shape: 100 (100)
I1210 21:25:23.930119 11576 net.cpp:137] Memory required for data: 1230000
I1210 21:25:23.930119 11576 layer_factory.cpp:58] Creating layer conv1
I1210 21:25:23.930119 11576 net.cpp:84] Creating Layer conv1
I1210 21:25:23.930119 11576 net.cpp:406] conv1 <- data
I1210 21:25:23.930119 11576 net.cpp:380] conv1 -> conv1
I1210 21:25:23.931619 14632 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 21:25:24.182001 11576 net.cpp:122] Setting up conv1
I1210 21:25:24.182001 11576 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 21:25:24.182001 11576 net.cpp:137] Memory required for data: 13518000
I1210 21:25:24.182001 11576 layer_factory.cpp:58] Creating layer bn1
I1210 21:25:24.182001 11576 net.cpp:84] Creating Layer bn1
I1210 21:25:24.182001 11576 net.cpp:406] bn1 <- conv1
I1210 21:25:24.182001 11576 net.cpp:367] bn1 -> conv1 (in-place)
I1210 21:25:24.183001 11576 net.cpp:122] Setting up bn1
I1210 21:25:24.183001 11576 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 21:25:24.183001 11576 net.cpp:137] Memory required for data: 25806000
I1210 21:25:24.183001 11576 layer_factory.cpp:58] Creating layer scale1
I1210 21:25:24.183001 11576 net.cpp:84] Creating Layer scale1
I1210 21:25:24.183001 11576 net.cpp:406] scale1 <- conv1
I1210 21:25:24.183001 11576 net.cpp:367] scale1 -> conv1 (in-place)
I1210 21:25:24.183001 11576 layer_factory.cpp:58] Creating layer scale1
I1210 21:25:24.183001 11576 net.cpp:122] Setting up scale1
I1210 21:25:24.183001 11576 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 21:25:24.183001 11576 net.cpp:137] Memory required for data: 38094000
I1210 21:25:24.183001 11576 layer_factory.cpp:58] Creating layer relu1
I1210 21:25:24.183001 11576 net.cpp:84] Creating Layer relu1
I1210 21:25:24.183001 11576 net.cpp:406] relu1 <- conv1
I1210 21:25:24.183001 11576 net.cpp:367] relu1 -> conv1 (in-place)
I1210 21:25:24.183001 11576 net.cpp:122] Setting up relu1
I1210 21:25:24.183001 11576 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 21:25:24.183001 11576 net.cpp:137] Memory required for data: 50382000
I1210 21:25:24.183001 11576 layer_factory.cpp:58] Creating layer conv1_0
I1210 21:25:24.183001 11576 net.cpp:84] Creating Layer conv1_0
I1210 21:25:24.183001 11576 net.cpp:406] conv1_0 <- conv1
I1210 21:25:24.183001 11576 net.cpp:380] conv1_0 -> conv1_0
I1210 21:25:24.185001 11576 net.cpp:122] Setting up conv1_0
I1210 21:25:24.185001 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.185001 11576 net.cpp:137] Memory required for data: 66766000
I1210 21:25:24.185001 11576 layer_factory.cpp:58] Creating layer bn1_0
I1210 21:25:24.185001 11576 net.cpp:84] Creating Layer bn1_0
I1210 21:25:24.185001 11576 net.cpp:406] bn1_0 <- conv1_0
I1210 21:25:24.185001 11576 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1210 21:25:24.185001 11576 net.cpp:122] Setting up bn1_0
I1210 21:25:24.185001 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.185001 11576 net.cpp:137] Memory required for data: 83150000
I1210 21:25:24.185001 11576 layer_factory.cpp:58] Creating layer scale1_0
I1210 21:25:24.185001 11576 net.cpp:84] Creating Layer scale1_0
I1210 21:25:24.185001 11576 net.cpp:406] scale1_0 <- conv1_0
I1210 21:25:24.185001 11576 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1210 21:25:24.185001 11576 layer_factory.cpp:58] Creating layer scale1_0
I1210 21:25:24.185001 11576 net.cpp:122] Setting up scale1_0
I1210 21:25:24.185001 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.185001 11576 net.cpp:137] Memory required for data: 99534000
I1210 21:25:24.185001 11576 layer_factory.cpp:58] Creating layer relu1_0
I1210 21:25:24.185001 11576 net.cpp:84] Creating Layer relu1_0
I1210 21:25:24.185001 11576 net.cpp:406] relu1_0 <- conv1_0
I1210 21:25:24.185001 11576 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1210 21:25:24.185001 11576 net.cpp:122] Setting up relu1_0
I1210 21:25:24.185001 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.185001 11576 net.cpp:137] Memory required for data: 115918000
I1210 21:25:24.185001 11576 layer_factory.cpp:58] Creating layer conv2
I1210 21:25:24.185001 11576 net.cpp:84] Creating Layer conv2
I1210 21:25:24.185001 11576 net.cpp:406] conv2 <- conv1_0
I1210 21:25:24.185001 11576 net.cpp:380] conv2 -> conv2
I1210 21:25:24.187001 11576 net.cpp:122] Setting up conv2
I1210 21:25:24.187001 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.187001 11576 net.cpp:137] Memory required for data: 132302000
I1210 21:25:24.187001 11576 layer_factory.cpp:58] Creating layer bn2
I1210 21:25:24.187001 11576 net.cpp:84] Creating Layer bn2
I1210 21:25:24.187001 11576 net.cpp:406] bn2 <- conv2
I1210 21:25:24.187001 11576 net.cpp:367] bn2 -> conv2 (in-place)
I1210 21:25:24.187001 11576 net.cpp:122] Setting up bn2
I1210 21:25:24.187001 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.187001 11576 net.cpp:137] Memory required for data: 148686000
I1210 21:25:24.187001 11576 layer_factory.cpp:58] Creating layer scale2
I1210 21:25:24.187001 11576 net.cpp:84] Creating Layer scale2
I1210 21:25:24.187001 11576 net.cpp:406] scale2 <- conv2
I1210 21:25:24.187001 11576 net.cpp:367] scale2 -> conv2 (in-place)
I1210 21:25:24.187001 11576 layer_factory.cpp:58] Creating layer scale2
I1210 21:25:24.187001 11576 net.cpp:122] Setting up scale2
I1210 21:25:24.187001 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.187001 11576 net.cpp:137] Memory required for data: 165070000
I1210 21:25:24.187001 11576 layer_factory.cpp:58] Creating layer relu2
I1210 21:25:24.187001 11576 net.cpp:84] Creating Layer relu2
I1210 21:25:24.187001 11576 net.cpp:406] relu2 <- conv2
I1210 21:25:24.187001 11576 net.cpp:367] relu2 -> conv2 (in-place)
I1210 21:25:24.187001 11576 net.cpp:122] Setting up relu2
I1210 21:25:24.187001 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.187001 11576 net.cpp:137] Memory required for data: 181454000
I1210 21:25:24.187001 11576 layer_factory.cpp:58] Creating layer conv2_1
I1210 21:25:24.187001 11576 net.cpp:84] Creating Layer conv2_1
I1210 21:25:24.187001 11576 net.cpp:406] conv2_1 <- conv2
I1210 21:25:24.187001 11576 net.cpp:380] conv2_1 -> conv2_1
I1210 21:25:24.188001 11576 net.cpp:122] Setting up conv2_1
I1210 21:25:24.188001 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.188001 11576 net.cpp:137] Memory required for data: 197838000
I1210 21:25:24.188001 11576 layer_factory.cpp:58] Creating layer bn2_1
I1210 21:25:24.188001 11576 net.cpp:84] Creating Layer bn2_1
I1210 21:25:24.188001 11576 net.cpp:406] bn2_1 <- conv2_1
I1210 21:25:24.188001 11576 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1210 21:25:24.188001 11576 net.cpp:122] Setting up bn2_1
I1210 21:25:24.188001 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.188001 11576 net.cpp:137] Memory required for data: 214222000
I1210 21:25:24.188001 11576 layer_factory.cpp:58] Creating layer scale2_1
I1210 21:25:24.188001 11576 net.cpp:84] Creating Layer scale2_1
I1210 21:25:24.188001 11576 net.cpp:406] scale2_1 <- conv2_1
I1210 21:25:24.188001 11576 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1210 21:25:24.189002 11576 layer_factory.cpp:58] Creating layer scale2_1
I1210 21:25:24.189002 11576 net.cpp:122] Setting up scale2_1
I1210 21:25:24.189002 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.189002 11576 net.cpp:137] Memory required for data: 230606000
I1210 21:25:24.189002 11576 layer_factory.cpp:58] Creating layer relu2_1
I1210 21:25:24.189002 11576 net.cpp:84] Creating Layer relu2_1
I1210 21:25:24.189002 11576 net.cpp:406] relu2_1 <- conv2_1
I1210 21:25:24.189002 11576 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1210 21:25:24.189002 11576 net.cpp:122] Setting up relu2_1
I1210 21:25:24.189002 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.189002 11576 net.cpp:137] Memory required for data: 246990000
I1210 21:25:24.189002 11576 layer_factory.cpp:58] Creating layer conv2_2
I1210 21:25:24.189002 11576 net.cpp:84] Creating Layer conv2_2
I1210 21:25:24.189002 11576 net.cpp:406] conv2_2 <- conv2_1
I1210 21:25:24.189002 11576 net.cpp:380] conv2_2 -> conv2_2
I1210 21:25:24.191001 11576 net.cpp:122] Setting up conv2_2
I1210 21:25:24.191001 11576 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 21:25:24.191001 11576 net.cpp:137] Memory required for data: 267470000
I1210 21:25:24.191001 11576 layer_factory.cpp:58] Creating layer bn2_2
I1210 21:25:24.191001 11576 net.cpp:84] Creating Layer bn2_2
I1210 21:25:24.191001 11576 net.cpp:406] bn2_2 <- conv2_2
I1210 21:25:24.191001 11576 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1210 21:25:24.191001 11576 net.cpp:122] Setting up bn2_2
I1210 21:25:24.191001 11576 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 21:25:24.191001 11576 net.cpp:137] Memory required for data: 287950000
I1210 21:25:24.191001 11576 layer_factory.cpp:58] Creating layer scale2_2
I1210 21:25:24.191001 11576 net.cpp:84] Creating Layer scale2_2
I1210 21:25:24.191001 11576 net.cpp:406] scale2_2 <- conv2_2
I1210 21:25:24.191001 11576 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1210 21:25:24.191001 11576 layer_factory.cpp:58] Creating layer scale2_2
I1210 21:25:24.191001 11576 net.cpp:122] Setting up scale2_2
I1210 21:25:24.191001 11576 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 21:25:24.191001 11576 net.cpp:137] Memory required for data: 308430000
I1210 21:25:24.191001 11576 layer_factory.cpp:58] Creating layer relu2_2
I1210 21:25:24.191001 11576 net.cpp:84] Creating Layer relu2_2
I1210 21:25:24.191001 11576 net.cpp:406] relu2_2 <- conv2_2
I1210 21:25:24.191001 11576 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1210 21:25:24.191999 11576 net.cpp:122] Setting up relu2_2
I1210 21:25:24.191999 11576 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 21:25:24.191999 11576 net.cpp:137] Memory required for data: 328910000
I1210 21:25:24.191999 11576 layer_factory.cpp:58] Creating layer pool2_1
I1210 21:25:24.191999 11576 net.cpp:84] Creating Layer pool2_1
I1210 21:25:24.191999 11576 net.cpp:406] pool2_1 <- conv2_2
I1210 21:25:24.191999 11576 net.cpp:380] pool2_1 -> pool2_1
I1210 21:25:24.193001 11576 net.cpp:122] Setting up pool2_1
I1210 21:25:24.193001 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.193001 11576 net.cpp:137] Memory required for data: 334030000
I1210 21:25:24.193001 11576 layer_factory.cpp:58] Creating layer conv3
I1210 21:25:24.193001 11576 net.cpp:84] Creating Layer conv3
I1210 21:25:24.193001 11576 net.cpp:406] conv3 <- pool2_1
I1210 21:25:24.193001 11576 net.cpp:380] conv3 -> conv3
I1210 21:25:24.194001 11576 net.cpp:122] Setting up conv3
I1210 21:25:24.194001 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.194001 11576 net.cpp:137] Memory required for data: 339150000
I1210 21:25:24.194001 11576 layer_factory.cpp:58] Creating layer bn3
I1210 21:25:24.194001 11576 net.cpp:84] Creating Layer bn3
I1210 21:25:24.194001 11576 net.cpp:406] bn3 <- conv3
I1210 21:25:24.194001 11576 net.cpp:367] bn3 -> conv3 (in-place)
I1210 21:25:24.194001 11576 net.cpp:122] Setting up bn3
I1210 21:25:24.194001 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.194001 11576 net.cpp:137] Memory required for data: 344270000
I1210 21:25:24.194001 11576 layer_factory.cpp:58] Creating layer scale3
I1210 21:25:24.194001 11576 net.cpp:84] Creating Layer scale3
I1210 21:25:24.194001 11576 net.cpp:406] scale3 <- conv3
I1210 21:25:24.194001 11576 net.cpp:367] scale3 -> conv3 (in-place)
I1210 21:25:24.195001 11576 layer_factory.cpp:58] Creating layer scale3
I1210 21:25:24.195001 11576 net.cpp:122] Setting up scale3
I1210 21:25:24.195001 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.195001 11576 net.cpp:137] Memory required for data: 349390000
I1210 21:25:24.195001 11576 layer_factory.cpp:58] Creating layer relu3
I1210 21:25:24.195001 11576 net.cpp:84] Creating Layer relu3
I1210 21:25:24.195001 11576 net.cpp:406] relu3 <- conv3
I1210 21:25:24.195001 11576 net.cpp:367] relu3 -> conv3 (in-place)
I1210 21:25:24.195001 11576 net.cpp:122] Setting up relu3
I1210 21:25:24.195001 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.195001 11576 net.cpp:137] Memory required for data: 354510000
I1210 21:25:24.195001 11576 layer_factory.cpp:58] Creating layer conv3_1
I1210 21:25:24.195001 11576 net.cpp:84] Creating Layer conv3_1
I1210 21:25:24.195001 11576 net.cpp:406] conv3_1 <- conv3
I1210 21:25:24.195001 11576 net.cpp:380] conv3_1 -> conv3_1
I1210 21:25:24.197001 11576 net.cpp:122] Setting up conv3_1
I1210 21:25:24.197001 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.197001 11576 net.cpp:137] Memory required for data: 359630000
I1210 21:25:24.197001 11576 layer_factory.cpp:58] Creating layer bn3_1
I1210 21:25:24.197001 11576 net.cpp:84] Creating Layer bn3_1
I1210 21:25:24.197001 11576 net.cpp:406] bn3_1 <- conv3_1
I1210 21:25:24.197001 11576 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1210 21:25:24.197001 11576 net.cpp:122] Setting up bn3_1
I1210 21:25:24.197001 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.197001 11576 net.cpp:137] Memory required for data: 364750000
I1210 21:25:24.197001 11576 layer_factory.cpp:58] Creating layer scale3_1
I1210 21:25:24.197001 11576 net.cpp:84] Creating Layer scale3_1
I1210 21:25:24.197001 11576 net.cpp:406] scale3_1 <- conv3_1
I1210 21:25:24.197001 11576 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1210 21:25:24.197001 11576 layer_factory.cpp:58] Creating layer scale3_1
I1210 21:25:24.197001 11576 net.cpp:122] Setting up scale3_1
I1210 21:25:24.197001 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.197001 11576 net.cpp:137] Memory required for data: 369870000
I1210 21:25:24.197001 11576 layer_factory.cpp:58] Creating layer relu3_1
I1210 21:25:24.197001 11576 net.cpp:84] Creating Layer relu3_1
I1210 21:25:24.197001 11576 net.cpp:406] relu3_1 <- conv3_1
I1210 21:25:24.198004 11576 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1210 21:25:24.198004 11576 net.cpp:122] Setting up relu3_1
I1210 21:25:24.198004 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.198004 11576 net.cpp:137] Memory required for data: 374990000
I1210 21:25:24.198004 11576 layer_factory.cpp:58] Creating layer conv4
I1210 21:25:24.198004 11576 net.cpp:84] Creating Layer conv4
I1210 21:25:24.198004 11576 net.cpp:406] conv4 <- conv3_1
I1210 21:25:24.198004 11576 net.cpp:380] conv4 -> conv4
I1210 21:25:24.199002 11576 net.cpp:122] Setting up conv4
I1210 21:25:24.199002 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.199002 11576 net.cpp:137] Memory required for data: 380110000
I1210 21:25:24.199002 11576 layer_factory.cpp:58] Creating layer bn4
I1210 21:25:24.199002 11576 net.cpp:84] Creating Layer bn4
I1210 21:25:24.199002 11576 net.cpp:406] bn4 <- conv4
I1210 21:25:24.199002 11576 net.cpp:367] bn4 -> conv4 (in-place)
I1210 21:25:24.200003 11576 net.cpp:122] Setting up bn4
I1210 21:25:24.200003 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.200003 11576 net.cpp:137] Memory required for data: 385230000
I1210 21:25:24.200003 11576 layer_factory.cpp:58] Creating layer scale4
I1210 21:25:24.200003 11576 net.cpp:84] Creating Layer scale4
I1210 21:25:24.200003 11576 net.cpp:406] scale4 <- conv4
I1210 21:25:24.200003 11576 net.cpp:367] scale4 -> conv4 (in-place)
I1210 21:25:24.200003 11576 layer_factory.cpp:58] Creating layer scale4
I1210 21:25:24.200003 11576 net.cpp:122] Setting up scale4
I1210 21:25:24.200003 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.200003 11576 net.cpp:137] Memory required for data: 390350000
I1210 21:25:24.200003 11576 layer_factory.cpp:58] Creating layer relu4
I1210 21:25:24.200003 11576 net.cpp:84] Creating Layer relu4
I1210 21:25:24.200003 11576 net.cpp:406] relu4 <- conv4
I1210 21:25:24.200003 11576 net.cpp:367] relu4 -> conv4 (in-place)
I1210 21:25:24.200003 11576 net.cpp:122] Setting up relu4
I1210 21:25:24.200003 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.200003 11576 net.cpp:137] Memory required for data: 395470000
I1210 21:25:24.200003 11576 layer_factory.cpp:58] Creating layer conv4_1
I1210 21:25:24.200003 11576 net.cpp:84] Creating Layer conv4_1
I1210 21:25:24.200003 11576 net.cpp:406] conv4_1 <- conv4
I1210 21:25:24.200003 11576 net.cpp:380] conv4_1 -> conv4_1
I1210 21:25:24.201509 11576 net.cpp:122] Setting up conv4_1
I1210 21:25:24.201509 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.201509 11576 net.cpp:137] Memory required for data: 400590000
I1210 21:25:24.201509 11576 layer_factory.cpp:58] Creating layer bn4_1
I1210 21:25:24.201509 11576 net.cpp:84] Creating Layer bn4_1
I1210 21:25:24.201509 11576 net.cpp:406] bn4_1 <- conv4_1
I1210 21:25:24.201509 11576 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1210 21:25:24.202009 11576 net.cpp:122] Setting up bn4_1
I1210 21:25:24.202009 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.202009 11576 net.cpp:137] Memory required for data: 405710000
I1210 21:25:24.202009 11576 layer_factory.cpp:58] Creating layer scale4_1
I1210 21:25:24.202009 11576 net.cpp:84] Creating Layer scale4_1
I1210 21:25:24.202009 11576 net.cpp:406] scale4_1 <- conv4_1
I1210 21:25:24.202009 11576 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1210 21:25:24.202009 11576 layer_factory.cpp:58] Creating layer scale4_1
I1210 21:25:24.202009 11576 net.cpp:122] Setting up scale4_1
I1210 21:25:24.202009 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.202009 11576 net.cpp:137] Memory required for data: 410830000
I1210 21:25:24.202009 11576 layer_factory.cpp:58] Creating layer relu4_1
I1210 21:25:24.202009 11576 net.cpp:84] Creating Layer relu4_1
I1210 21:25:24.202009 11576 net.cpp:406] relu4_1 <- conv4_1
I1210 21:25:24.202009 11576 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1210 21:25:24.202009 11576 net.cpp:122] Setting up relu4_1
I1210 21:25:24.202009 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.202009 11576 net.cpp:137] Memory required for data: 415950000
I1210 21:25:24.202009 11576 layer_factory.cpp:58] Creating layer conv4_2
I1210 21:25:24.202009 11576 net.cpp:84] Creating Layer conv4_2
I1210 21:25:24.202508 11576 net.cpp:406] conv4_2 <- conv4_1
I1210 21:25:24.202508 11576 net.cpp:380] conv4_2 -> conv4_2
I1210 21:25:24.203508 11576 net.cpp:122] Setting up conv4_2
I1210 21:25:24.203508 11576 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 21:25:24.203508 11576 net.cpp:137] Memory required for data: 421889200
I1210 21:25:24.203508 11576 layer_factory.cpp:58] Creating layer bn4_2
I1210 21:25:24.203508 11576 net.cpp:84] Creating Layer bn4_2
I1210 21:25:24.203508 11576 net.cpp:406] bn4_2 <- conv4_2
I1210 21:25:24.203508 11576 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1210 21:25:24.204010 11576 net.cpp:122] Setting up bn4_2
I1210 21:25:24.204010 11576 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 21:25:24.204010 11576 net.cpp:137] Memory required for data: 427828400
I1210 21:25:24.204010 11576 layer_factory.cpp:58] Creating layer scale4_2
I1210 21:25:24.204010 11576 net.cpp:84] Creating Layer scale4_2
I1210 21:25:24.204010 11576 net.cpp:406] scale4_2 <- conv4_2
I1210 21:25:24.204010 11576 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1210 21:25:24.204010 11576 layer_factory.cpp:58] Creating layer scale4_2
I1210 21:25:24.204010 11576 net.cpp:122] Setting up scale4_2
I1210 21:25:24.204010 11576 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 21:25:24.204010 11576 net.cpp:137] Memory required for data: 433767600
I1210 21:25:24.204010 11576 layer_factory.cpp:58] Creating layer relu4_2
I1210 21:25:24.204010 11576 net.cpp:84] Creating Layer relu4_2
I1210 21:25:24.204010 11576 net.cpp:406] relu4_2 <- conv4_2
I1210 21:25:24.204010 11576 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1210 21:25:24.204509 11576 net.cpp:122] Setting up relu4_2
I1210 21:25:24.204509 11576 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 21:25:24.204509 11576 net.cpp:137] Memory required for data: 439706800
I1210 21:25:24.204509 11576 layer_factory.cpp:58] Creating layer pool4_2
I1210 21:25:24.204509 11576 net.cpp:84] Creating Layer pool4_2
I1210 21:25:24.204509 11576 net.cpp:406] pool4_2 <- conv4_2
I1210 21:25:24.204509 11576 net.cpp:380] pool4_2 -> pool4_2
I1210 21:25:24.205507 11576 net.cpp:122] Setting up pool4_2
I1210 21:25:24.205507 11576 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 21:25:24.205507 11576 net.cpp:137] Memory required for data: 441191600
I1210 21:25:24.205507 11576 layer_factory.cpp:58] Creating layer conv4_0
I1210 21:25:24.205507 11576 net.cpp:84] Creating Layer conv4_0
I1210 21:25:24.205507 11576 net.cpp:406] conv4_0 <- pool4_2
I1210 21:25:24.205507 11576 net.cpp:380] conv4_0 -> conv4_0
I1210 21:25:24.207010 11576 net.cpp:122] Setting up conv4_0
I1210 21:25:24.207010 11576 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 21:25:24.207010 11576 net.cpp:137] Memory required for data: 442676400
I1210 21:25:24.207010 11576 layer_factory.cpp:58] Creating layer bn4_0
I1210 21:25:24.207010 11576 net.cpp:84] Creating Layer bn4_0
I1210 21:25:24.207010 11576 net.cpp:406] bn4_0 <- conv4_0
I1210 21:25:24.207010 11576 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1210 21:25:24.207010 11576 net.cpp:122] Setting up bn4_0
I1210 21:25:24.207010 11576 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 21:25:24.207010 11576 net.cpp:137] Memory required for data: 444161200
I1210 21:25:24.207010 11576 layer_factory.cpp:58] Creating layer scale4_0
I1210 21:25:24.207509 11576 net.cpp:84] Creating Layer scale4_0
I1210 21:25:24.207509 11576 net.cpp:406] scale4_0 <- conv4_0
I1210 21:25:24.207509 11576 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1210 21:25:24.207509 11576 layer_factory.cpp:58] Creating layer scale4_0
I1210 21:25:24.207509 11576 net.cpp:122] Setting up scale4_0
I1210 21:25:24.207509 11576 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 21:25:24.207509 11576 net.cpp:137] Memory required for data: 445646000
I1210 21:25:24.207509 11576 layer_factory.cpp:58] Creating layer relu4_0
I1210 21:25:24.207509 11576 net.cpp:84] Creating Layer relu4_0
I1210 21:25:24.207509 11576 net.cpp:406] relu4_0 <- conv4_0
I1210 21:25:24.207509 11576 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1210 21:25:24.207509 11576 net.cpp:122] Setting up relu4_0
I1210 21:25:24.207509 11576 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 21:25:24.207509 11576 net.cpp:137] Memory required for data: 447130800
I1210 21:25:24.207509 11576 layer_factory.cpp:58] Creating layer conv11
I1210 21:25:24.207509 11576 net.cpp:84] Creating Layer conv11
I1210 21:25:24.207509 11576 net.cpp:406] conv11 <- conv4_0
I1210 21:25:24.207509 11576 net.cpp:380] conv11 -> conv11
I1210 21:25:24.209008 11576 net.cpp:122] Setting up conv11
I1210 21:25:24.209008 11576 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 21:25:24.209008 11576 net.cpp:137] Memory required for data: 448922800
I1210 21:25:24.209008 11576 layer_factory.cpp:58] Creating layer bn_conv11
I1210 21:25:24.209008 11576 net.cpp:84] Creating Layer bn_conv11
I1210 21:25:24.209008 11576 net.cpp:406] bn_conv11 <- conv11
I1210 21:25:24.209008 11576 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1210 21:25:24.209523 11576 net.cpp:122] Setting up bn_conv11
I1210 21:25:24.209523 11576 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 21:25:24.209523 11576 net.cpp:137] Memory required for data: 450714800
I1210 21:25:24.209523 11576 layer_factory.cpp:58] Creating layer scale_conv11
I1210 21:25:24.209523 11576 net.cpp:84] Creating Layer scale_conv11
I1210 21:25:24.209523 11576 net.cpp:406] scale_conv11 <- conv11
I1210 21:25:24.209523 11576 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1210 21:25:24.209523 11576 layer_factory.cpp:58] Creating layer scale_conv11
I1210 21:25:24.209523 11576 net.cpp:122] Setting up scale_conv11
I1210 21:25:24.209523 11576 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 21:25:24.209523 11576 net.cpp:137] Memory required for data: 452506800
I1210 21:25:24.209523 11576 layer_factory.cpp:58] Creating layer relu_conv11
I1210 21:25:24.209523 11576 net.cpp:84] Creating Layer relu_conv11
I1210 21:25:24.209523 11576 net.cpp:406] relu_conv11 <- conv11
I1210 21:25:24.209523 11576 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1210 21:25:24.210510 11576 net.cpp:122] Setting up relu_conv11
I1210 21:25:24.210510 11576 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 21:25:24.210510 11576 net.cpp:137] Memory required for data: 454298800
I1210 21:25:24.210510 11576 layer_factory.cpp:58] Creating layer conv12
I1210 21:25:24.210510 11576 net.cpp:84] Creating Layer conv12
I1210 21:25:24.210510 11576 net.cpp:406] conv12 <- conv11
I1210 21:25:24.210510 11576 net.cpp:380] conv12 -> conv12
I1210 21:25:24.212007 11576 net.cpp:122] Setting up conv12
I1210 21:25:24.212007 11576 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 21:25:24.212007 11576 net.cpp:137] Memory required for data: 456602800
I1210 21:25:24.212007 11576 layer_factory.cpp:58] Creating layer bn_conv12
I1210 21:25:24.212007 11576 net.cpp:84] Creating Layer bn_conv12
I1210 21:25:24.212007 11576 net.cpp:406] bn_conv12 <- conv12
I1210 21:25:24.212007 11576 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1210 21:25:24.212007 11576 net.cpp:122] Setting up bn_conv12
I1210 21:25:24.212007 11576 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 21:25:24.212007 11576 net.cpp:137] Memory required for data: 458906800
I1210 21:25:24.212007 11576 layer_factory.cpp:58] Creating layer scale_conv12
I1210 21:25:24.212007 11576 net.cpp:84] Creating Layer scale_conv12
I1210 21:25:24.212007 11576 net.cpp:406] scale_conv12 <- conv12
I1210 21:25:24.212007 11576 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1210 21:25:24.212007 11576 layer_factory.cpp:58] Creating layer scale_conv12
I1210 21:25:24.212007 11576 net.cpp:122] Setting up scale_conv12
I1210 21:25:24.212507 11576 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 21:25:24.212507 11576 net.cpp:137] Memory required for data: 461210800
I1210 21:25:24.212507 11576 layer_factory.cpp:58] Creating layer relu_conv12
I1210 21:25:24.212507 11576 net.cpp:84] Creating Layer relu_conv12
I1210 21:25:24.212507 11576 net.cpp:406] relu_conv12 <- conv12
I1210 21:25:24.212507 11576 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1210 21:25:24.213022 11576 net.cpp:122] Setting up relu_conv12
I1210 21:25:24.213022 11576 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 21:25:24.213022 11576 net.cpp:137] Memory required for data: 463514800
I1210 21:25:24.213022 11576 layer_factory.cpp:58] Creating layer poolcp6
I1210 21:25:24.213022 11576 net.cpp:84] Creating Layer poolcp6
I1210 21:25:24.213022 11576 net.cpp:406] poolcp6 <- conv12
I1210 21:25:24.213022 11576 net.cpp:380] poolcp6 -> poolcp6
I1210 21:25:24.213022 11576 net.cpp:122] Setting up poolcp6
I1210 21:25:24.213022 11576 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1210 21:25:24.213022 11576 net.cpp:137] Memory required for data: 463550800
I1210 21:25:24.213022 11576 layer_factory.cpp:58] Creating layer ip1
I1210 21:25:24.213022 11576 net.cpp:84] Creating Layer ip1
I1210 21:25:24.213022 11576 net.cpp:406] ip1 <- poolcp6
I1210 21:25:24.213022 11576 net.cpp:380] ip1 -> ip1
I1210 21:25:24.213510 11576 net.cpp:122] Setting up ip1
I1210 21:25:24.213510 11576 net.cpp:129] Top shape: 100 100 (10000)
I1210 21:25:24.213510 11576 net.cpp:137] Memory required for data: 463590800
I1210 21:25:24.213510 11576 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1210 21:25:24.213510 11576 net.cpp:84] Creating Layer ip1_ip1_0_split
I1210 21:25:24.213510 11576 net.cpp:406] ip1_ip1_0_split <- ip1
I1210 21:25:24.213510 11576 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1210 21:25:24.213510 11576 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1210 21:25:24.213510 11576 net.cpp:122] Setting up ip1_ip1_0_split
I1210 21:25:24.213510 11576 net.cpp:129] Top shape: 100 100 (10000)
I1210 21:25:24.213510 11576 net.cpp:129] Top shape: 100 100 (10000)
I1210 21:25:24.214030 11576 net.cpp:137] Memory required for data: 463670800
I1210 21:25:24.214030 11576 layer_factory.cpp:58] Creating layer accuracy_training
I1210 21:25:24.214030 11576 net.cpp:84] Creating Layer accuracy_training
I1210 21:25:24.214030 11576 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1210 21:25:24.214030 11576 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1210 21:25:24.214030 11576 net.cpp:380] accuracy_training -> accuracy_training
I1210 21:25:24.214030 11576 net.cpp:122] Setting up accuracy_training
I1210 21:25:24.214030 11576 net.cpp:129] Top shape: (1)
I1210 21:25:24.214030 11576 net.cpp:137] Memory required for data: 463670804
I1210 21:25:24.214030 11576 layer_factory.cpp:58] Creating layer loss
I1210 21:25:24.214030 11576 net.cpp:84] Creating Layer loss
I1210 21:25:24.214030 11576 net.cpp:406] loss <- ip1_ip1_0_split_1
I1210 21:25:24.214030 11576 net.cpp:406] loss <- label_cifar_1_split_1
I1210 21:25:24.214030 11576 net.cpp:380] loss -> loss
I1210 21:25:24.214030 11576 layer_factory.cpp:58] Creating layer loss
I1210 21:25:24.214509 11576 net.cpp:122] Setting up loss
I1210 21:25:24.214509 11576 net.cpp:129] Top shape: (1)
I1210 21:25:24.214509 11576 net.cpp:132]     with loss weight 1
I1210 21:25:24.214509 11576 net.cpp:137] Memory required for data: 463670808
I1210 21:25:24.214509 11576 net.cpp:198] loss needs backward computation.
I1210 21:25:24.214509 11576 net.cpp:200] accuracy_training does not need backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] ip1 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] poolcp6 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] relu_conv12 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] scale_conv12 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] bn_conv12 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] conv12 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] relu_conv11 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] scale_conv11 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] bn_conv11 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] conv11 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] relu4_0 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] scale4_0 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] bn4_0 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] conv4_0 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] pool4_2 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] relu4_2 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] scale4_2 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] bn4_2 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] conv4_2 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] relu4_1 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] scale4_1 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] bn4_1 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] conv4_1 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] relu4 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] scale4 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] bn4 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] conv4 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] relu3_1 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] scale3_1 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] bn3_1 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] conv3_1 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] relu3 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] scale3 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] bn3 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] conv3 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] pool2_1 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] relu2_2 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] scale2_2 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] bn2_2 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] conv2_2 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] relu2_1 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] scale2_1 needs backward computation.
I1210 21:25:24.215009 11576 net.cpp:198] bn2_1 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:198] conv2_1 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:198] relu2 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:198] scale2 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:198] bn2 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:198] conv2 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:198] relu1_0 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:198] scale1_0 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:198] bn1_0 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:198] conv1_0 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:198] relu1 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:198] scale1 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:198] bn1 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:198] conv1 needs backward computation.
I1210 21:25:24.215508 11576 net.cpp:200] label_cifar_1_split does not need backward computation.
I1210 21:25:24.215508 11576 net.cpp:200] cifar does not need backward computation.
I1210 21:25:24.215508 11576 net.cpp:242] This network produces output accuracy_training
I1210 21:25:24.215508 11576 net.cpp:242] This network produces output loss
I1210 21:25:24.215508 11576 net.cpp:255] Network initialization done.
I1210 21:25:24.216007 11576 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 21:25:24.216007 11576 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 21:25:24.216007 11576 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1210 21:25:24.216507 11576 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1210 21:25:24.216507 11576 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV1_360k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "pool2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "pool4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1210 21:25:24.217010 11576 layer_factory.cpp:58] Creating layer cifar
I1210 21:25:24.222010 11576 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1210 21:25:24.222010 11576 net.cpp:84] Creating Layer cifar
I1210 21:25:24.222010 11576 net.cpp:380] cifar -> data
I1210 21:25:24.222010 11576 net.cpp:380] cifar -> label
I1210 21:25:24.222010 11576 data_layer.cpp:45] output data size: 100,3,32,32
I1210 21:25:24.228011 11576 net.cpp:122] Setting up cifar
I1210 21:25:24.228011 11576 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1210 21:25:24.228011 11576 net.cpp:129] Top shape: 100 (100)
I1210 21:25:24.228011 11576 net.cpp:137] Memory required for data: 1229200
I1210 21:25:24.228011 11576 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1210 21:25:24.228011 11576 net.cpp:84] Creating Layer label_cifar_1_split
I1210 21:25:24.228011 11576 net.cpp:406] label_cifar_1_split <- label
I1210 21:25:24.228011 11576 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1210 21:25:24.228011 11576 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1210 21:25:24.228011 11576 net.cpp:122] Setting up label_cifar_1_split
I1210 21:25:24.229012 11576 net.cpp:129] Top shape: 100 (100)
I1210 21:25:24.229012 11576 net.cpp:129] Top shape: 100 (100)
I1210 21:25:24.229012 11576 net.cpp:137] Memory required for data: 1230000
I1210 21:25:24.229012 11576 layer_factory.cpp:58] Creating layer conv1
I1210 21:25:24.229012 11576 net.cpp:84] Creating Layer conv1
I1210 21:25:24.229012 11576 net.cpp:406] conv1 <- data
I1210 21:25:24.229012 11576 net.cpp:380] conv1 -> conv1
I1210 21:25:24.230013 11576 net.cpp:122] Setting up conv1
I1210 21:25:24.230013 21104 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 21:25:24.230013 11576 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 21:25:24.230013 11576 net.cpp:137] Memory required for data: 13518000
I1210 21:25:24.230013 11576 layer_factory.cpp:58] Creating layer bn1
I1210 21:25:24.230013 11576 net.cpp:84] Creating Layer bn1
I1210 21:25:24.230013 11576 net.cpp:406] bn1 <- conv1
I1210 21:25:24.230013 11576 net.cpp:367] bn1 -> conv1 (in-place)
I1210 21:25:24.231014 11576 net.cpp:122] Setting up bn1
I1210 21:25:24.231014 11576 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 21:25:24.231014 11576 net.cpp:137] Memory required for data: 25806000
I1210 21:25:24.231014 11576 layer_factory.cpp:58] Creating layer scale1
I1210 21:25:24.231014 11576 net.cpp:84] Creating Layer scale1
I1210 21:25:24.231014 11576 net.cpp:406] scale1 <- conv1
I1210 21:25:24.231014 11576 net.cpp:367] scale1 -> conv1 (in-place)
I1210 21:25:24.231014 11576 layer_factory.cpp:58] Creating layer scale1
I1210 21:25:24.231014 11576 net.cpp:122] Setting up scale1
I1210 21:25:24.231014 11576 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 21:25:24.231014 11576 net.cpp:137] Memory required for data: 38094000
I1210 21:25:24.231014 11576 layer_factory.cpp:58] Creating layer relu1
I1210 21:25:24.231014 11576 net.cpp:84] Creating Layer relu1
I1210 21:25:24.231014 11576 net.cpp:406] relu1 <- conv1
I1210 21:25:24.231014 11576 net.cpp:367] relu1 -> conv1 (in-place)
I1210 21:25:24.231014 11576 net.cpp:122] Setting up relu1
I1210 21:25:24.231014 11576 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 21:25:24.231014 11576 net.cpp:137] Memory required for data: 50382000
I1210 21:25:24.231014 11576 layer_factory.cpp:58] Creating layer conv1_0
I1210 21:25:24.231014 11576 net.cpp:84] Creating Layer conv1_0
I1210 21:25:24.231014 11576 net.cpp:406] conv1_0 <- conv1
I1210 21:25:24.231014 11576 net.cpp:380] conv1_0 -> conv1_0
I1210 21:25:24.233014 11576 net.cpp:122] Setting up conv1_0
I1210 21:25:24.233014 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.233014 11576 net.cpp:137] Memory required for data: 66766000
I1210 21:25:24.233014 11576 layer_factory.cpp:58] Creating layer bn1_0
I1210 21:25:24.233014 11576 net.cpp:84] Creating Layer bn1_0
I1210 21:25:24.233014 11576 net.cpp:406] bn1_0 <- conv1_0
I1210 21:25:24.233014 11576 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1210 21:25:24.233014 11576 net.cpp:122] Setting up bn1_0
I1210 21:25:24.233014 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.233014 11576 net.cpp:137] Memory required for data: 83150000
I1210 21:25:24.233014 11576 layer_factory.cpp:58] Creating layer scale1_0
I1210 21:25:24.233014 11576 net.cpp:84] Creating Layer scale1_0
I1210 21:25:24.233014 11576 net.cpp:406] scale1_0 <- conv1_0
I1210 21:25:24.233014 11576 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1210 21:25:24.233014 11576 layer_factory.cpp:58] Creating layer scale1_0
I1210 21:25:24.233014 11576 net.cpp:122] Setting up scale1_0
I1210 21:25:24.233014 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.233014 11576 net.cpp:137] Memory required for data: 99534000
I1210 21:25:24.233014 11576 layer_factory.cpp:58] Creating layer relu1_0
I1210 21:25:24.233014 11576 net.cpp:84] Creating Layer relu1_0
I1210 21:25:24.233014 11576 net.cpp:406] relu1_0 <- conv1_0
I1210 21:25:24.233014 11576 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1210 21:25:24.233014 11576 net.cpp:122] Setting up relu1_0
I1210 21:25:24.233014 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.234014 11576 net.cpp:137] Memory required for data: 115918000
I1210 21:25:24.234014 11576 layer_factory.cpp:58] Creating layer conv2
I1210 21:25:24.234014 11576 net.cpp:84] Creating Layer conv2
I1210 21:25:24.234014 11576 net.cpp:406] conv2 <- conv1_0
I1210 21:25:24.234014 11576 net.cpp:380] conv2 -> conv2
I1210 21:25:24.235014 11576 net.cpp:122] Setting up conv2
I1210 21:25:24.235014 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.235014 11576 net.cpp:137] Memory required for data: 132302000
I1210 21:25:24.235014 11576 layer_factory.cpp:58] Creating layer bn2
I1210 21:25:24.235014 11576 net.cpp:84] Creating Layer bn2
I1210 21:25:24.235014 11576 net.cpp:406] bn2 <- conv2
I1210 21:25:24.235014 11576 net.cpp:367] bn2 -> conv2 (in-place)
I1210 21:25:24.235014 11576 net.cpp:122] Setting up bn2
I1210 21:25:24.235014 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.235014 11576 net.cpp:137] Memory required for data: 148686000
I1210 21:25:24.235014 11576 layer_factory.cpp:58] Creating layer scale2
I1210 21:25:24.235014 11576 net.cpp:84] Creating Layer scale2
I1210 21:25:24.235014 11576 net.cpp:406] scale2 <- conv2
I1210 21:25:24.235014 11576 net.cpp:367] scale2 -> conv2 (in-place)
I1210 21:25:24.235014 11576 layer_factory.cpp:58] Creating layer scale2
I1210 21:25:24.235014 11576 net.cpp:122] Setting up scale2
I1210 21:25:24.235014 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.235014 11576 net.cpp:137] Memory required for data: 165070000
I1210 21:25:24.235014 11576 layer_factory.cpp:58] Creating layer relu2
I1210 21:25:24.235014 11576 net.cpp:84] Creating Layer relu2
I1210 21:25:24.235014 11576 net.cpp:406] relu2 <- conv2
I1210 21:25:24.236013 11576 net.cpp:367] relu2 -> conv2 (in-place)
I1210 21:25:24.236013 11576 net.cpp:122] Setting up relu2
I1210 21:25:24.236013 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.236013 11576 net.cpp:137] Memory required for data: 181454000
I1210 21:25:24.236013 11576 layer_factory.cpp:58] Creating layer conv2_1
I1210 21:25:24.236013 11576 net.cpp:84] Creating Layer conv2_1
I1210 21:25:24.236013 11576 net.cpp:406] conv2_1 <- conv2
I1210 21:25:24.236013 11576 net.cpp:380] conv2_1 -> conv2_1
I1210 21:25:24.237013 11576 net.cpp:122] Setting up conv2_1
I1210 21:25:24.237013 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.237013 11576 net.cpp:137] Memory required for data: 197838000
I1210 21:25:24.237013 11576 layer_factory.cpp:58] Creating layer bn2_1
I1210 21:25:24.237013 11576 net.cpp:84] Creating Layer bn2_1
I1210 21:25:24.237013 11576 net.cpp:406] bn2_1 <- conv2_1
I1210 21:25:24.237013 11576 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1210 21:25:24.237013 11576 net.cpp:122] Setting up bn2_1
I1210 21:25:24.237013 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.237013 11576 net.cpp:137] Memory required for data: 214222000
I1210 21:25:24.237013 11576 layer_factory.cpp:58] Creating layer scale2_1
I1210 21:25:24.237013 11576 net.cpp:84] Creating Layer scale2_1
I1210 21:25:24.237013 11576 net.cpp:406] scale2_1 <- conv2_1
I1210 21:25:24.237013 11576 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1210 21:25:24.237013 11576 layer_factory.cpp:58] Creating layer scale2_1
I1210 21:25:24.237013 11576 net.cpp:122] Setting up scale2_1
I1210 21:25:24.237013 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.237013 11576 net.cpp:137] Memory required for data: 230606000
I1210 21:25:24.238013 11576 layer_factory.cpp:58] Creating layer relu2_1
I1210 21:25:24.238013 11576 net.cpp:84] Creating Layer relu2_1
I1210 21:25:24.238013 11576 net.cpp:406] relu2_1 <- conv2_1
I1210 21:25:24.238013 11576 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1210 21:25:24.238013 11576 net.cpp:122] Setting up relu2_1
I1210 21:25:24.238013 11576 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 21:25:24.238013 11576 net.cpp:137] Memory required for data: 246990000
I1210 21:25:24.238013 11576 layer_factory.cpp:58] Creating layer conv2_2
I1210 21:25:24.238013 11576 net.cpp:84] Creating Layer conv2_2
I1210 21:25:24.238013 11576 net.cpp:406] conv2_2 <- conv2_1
I1210 21:25:24.238013 11576 net.cpp:380] conv2_2 -> conv2_2
I1210 21:25:24.240015 11576 net.cpp:122] Setting up conv2_2
I1210 21:25:24.240015 11576 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 21:25:24.240015 11576 net.cpp:137] Memory required for data: 267470000
I1210 21:25:24.240015 11576 layer_factory.cpp:58] Creating layer bn2_2
I1210 21:25:24.240015 11576 net.cpp:84] Creating Layer bn2_2
I1210 21:25:24.240015 11576 net.cpp:406] bn2_2 <- conv2_2
I1210 21:25:24.240015 11576 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1210 21:25:24.240015 11576 net.cpp:122] Setting up bn2_2
I1210 21:25:24.240015 11576 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 21:25:24.240015 11576 net.cpp:137] Memory required for data: 287950000
I1210 21:25:24.240015 11576 layer_factory.cpp:58] Creating layer scale2_2
I1210 21:25:24.240015 11576 net.cpp:84] Creating Layer scale2_2
I1210 21:25:24.240015 11576 net.cpp:406] scale2_2 <- conv2_2
I1210 21:25:24.240015 11576 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1210 21:25:24.240015 11576 layer_factory.cpp:58] Creating layer scale2_2
I1210 21:25:24.240015 11576 net.cpp:122] Setting up scale2_2
I1210 21:25:24.240015 11576 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 21:25:24.240015 11576 net.cpp:137] Memory required for data: 308430000
I1210 21:25:24.240015 11576 layer_factory.cpp:58] Creating layer relu2_2
I1210 21:25:24.240015 11576 net.cpp:84] Creating Layer relu2_2
I1210 21:25:24.240015 11576 net.cpp:406] relu2_2 <- conv2_2
I1210 21:25:24.240015 11576 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1210 21:25:24.240015 11576 net.cpp:122] Setting up relu2_2
I1210 21:25:24.240015 11576 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 21:25:24.240015 11576 net.cpp:137] Memory required for data: 328910000
I1210 21:25:24.240015 11576 layer_factory.cpp:58] Creating layer pool2_1
I1210 21:25:24.240015 11576 net.cpp:84] Creating Layer pool2_1
I1210 21:25:24.240015 11576 net.cpp:406] pool2_1 <- conv2_2
I1210 21:25:24.240015 11576 net.cpp:380] pool2_1 -> pool2_1
I1210 21:25:24.242013 11576 net.cpp:122] Setting up pool2_1
I1210 21:25:24.242013 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.242013 11576 net.cpp:137] Memory required for data: 334030000
I1210 21:25:24.242013 11576 layer_factory.cpp:58] Creating layer conv3
I1210 21:25:24.242013 11576 net.cpp:84] Creating Layer conv3
I1210 21:25:24.242013 11576 net.cpp:406] conv3 <- pool2_1
I1210 21:25:24.242013 11576 net.cpp:380] conv3 -> conv3
I1210 21:25:24.243013 11576 net.cpp:122] Setting up conv3
I1210 21:25:24.243013 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.243013 11576 net.cpp:137] Memory required for data: 339150000
I1210 21:25:24.243013 11576 layer_factory.cpp:58] Creating layer bn3
I1210 21:25:24.243013 11576 net.cpp:84] Creating Layer bn3
I1210 21:25:24.243013 11576 net.cpp:406] bn3 <- conv3
I1210 21:25:24.243013 11576 net.cpp:367] bn3 -> conv3 (in-place)
I1210 21:25:24.244015 11576 net.cpp:122] Setting up bn3
I1210 21:25:24.244015 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.244015 11576 net.cpp:137] Memory required for data: 344270000
I1210 21:25:24.244015 11576 layer_factory.cpp:58] Creating layer scale3
I1210 21:25:24.244015 11576 net.cpp:84] Creating Layer scale3
I1210 21:25:24.244015 11576 net.cpp:406] scale3 <- conv3
I1210 21:25:24.244015 11576 net.cpp:367] scale3 -> conv3 (in-place)
I1210 21:25:24.244015 11576 layer_factory.cpp:58] Creating layer scale3
I1210 21:25:24.244015 11576 net.cpp:122] Setting up scale3
I1210 21:25:24.244015 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.244015 11576 net.cpp:137] Memory required for data: 349390000
I1210 21:25:24.244015 11576 layer_factory.cpp:58] Creating layer relu3
I1210 21:25:24.244015 11576 net.cpp:84] Creating Layer relu3
I1210 21:25:24.244015 11576 net.cpp:406] relu3 <- conv3
I1210 21:25:24.244015 11576 net.cpp:367] relu3 -> conv3 (in-place)
I1210 21:25:24.244015 11576 net.cpp:122] Setting up relu3
I1210 21:25:24.244015 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.244015 11576 net.cpp:137] Memory required for data: 354510000
I1210 21:25:24.244015 11576 layer_factory.cpp:58] Creating layer conv3_1
I1210 21:25:24.244015 11576 net.cpp:84] Creating Layer conv3_1
I1210 21:25:24.244015 11576 net.cpp:406] conv3_1 <- conv3
I1210 21:25:24.244015 11576 net.cpp:380] conv3_1 -> conv3_1
I1210 21:25:24.245013 11576 net.cpp:122] Setting up conv3_1
I1210 21:25:24.245013 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.245013 11576 net.cpp:137] Memory required for data: 359630000
I1210 21:25:24.245013 11576 layer_factory.cpp:58] Creating layer bn3_1
I1210 21:25:24.245013 11576 net.cpp:84] Creating Layer bn3_1
I1210 21:25:24.245013 11576 net.cpp:406] bn3_1 <- conv3_1
I1210 21:25:24.246026 11576 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1210 21:25:24.246026 11576 net.cpp:122] Setting up bn3_1
I1210 21:25:24.246026 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.246026 11576 net.cpp:137] Memory required for data: 364750000
I1210 21:25:24.246026 11576 layer_factory.cpp:58] Creating layer scale3_1
I1210 21:25:24.246026 11576 net.cpp:84] Creating Layer scale3_1
I1210 21:25:24.246026 11576 net.cpp:406] scale3_1 <- conv3_1
I1210 21:25:24.246026 11576 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1210 21:25:24.246026 11576 layer_factory.cpp:58] Creating layer scale3_1
I1210 21:25:24.246026 11576 net.cpp:122] Setting up scale3_1
I1210 21:25:24.246026 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.246026 11576 net.cpp:137] Memory required for data: 369870000
I1210 21:25:24.246026 11576 layer_factory.cpp:58] Creating layer relu3_1
I1210 21:25:24.246026 11576 net.cpp:84] Creating Layer relu3_1
I1210 21:25:24.246026 11576 net.cpp:406] relu3_1 <- conv3_1
I1210 21:25:24.246026 11576 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1210 21:25:24.246026 11576 net.cpp:122] Setting up relu3_1
I1210 21:25:24.246026 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.246026 11576 net.cpp:137] Memory required for data: 374990000
I1210 21:25:24.246026 11576 layer_factory.cpp:58] Creating layer conv4
I1210 21:25:24.246026 11576 net.cpp:84] Creating Layer conv4
I1210 21:25:24.246026 11576 net.cpp:406] conv4 <- conv3_1
I1210 21:25:24.246026 11576 net.cpp:380] conv4 -> conv4
I1210 21:25:24.248009 11576 net.cpp:122] Setting up conv4
I1210 21:25:24.248009 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.248009 11576 net.cpp:137] Memory required for data: 380110000
I1210 21:25:24.248009 11576 layer_factory.cpp:58] Creating layer bn4
I1210 21:25:24.248009 11576 net.cpp:84] Creating Layer bn4
I1210 21:25:24.248009 11576 net.cpp:406] bn4 <- conv4
I1210 21:25:24.248009 11576 net.cpp:367] bn4 -> conv4 (in-place)
I1210 21:25:24.248009 11576 net.cpp:122] Setting up bn4
I1210 21:25:24.248009 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.248009 11576 net.cpp:137] Memory required for data: 385230000
I1210 21:25:24.248009 11576 layer_factory.cpp:58] Creating layer scale4
I1210 21:25:24.248009 11576 net.cpp:84] Creating Layer scale4
I1210 21:25:24.248009 11576 net.cpp:406] scale4 <- conv4
I1210 21:25:24.248009 11576 net.cpp:367] scale4 -> conv4 (in-place)
I1210 21:25:24.248009 11576 layer_factory.cpp:58] Creating layer scale4
I1210 21:25:24.248009 11576 net.cpp:122] Setting up scale4
I1210 21:25:24.248009 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.248009 11576 net.cpp:137] Memory required for data: 390350000
I1210 21:25:24.248009 11576 layer_factory.cpp:58] Creating layer relu4
I1210 21:25:24.248009 11576 net.cpp:84] Creating Layer relu4
I1210 21:25:24.248009 11576 net.cpp:406] relu4 <- conv4
I1210 21:25:24.248009 11576 net.cpp:367] relu4 -> conv4 (in-place)
I1210 21:25:24.249011 11576 net.cpp:122] Setting up relu4
I1210 21:25:24.249011 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.249011 11576 net.cpp:137] Memory required for data: 395470000
I1210 21:25:24.249011 11576 layer_factory.cpp:58] Creating layer conv4_1
I1210 21:25:24.249011 11576 net.cpp:84] Creating Layer conv4_1
I1210 21:25:24.249011 11576 net.cpp:406] conv4_1 <- conv4
I1210 21:25:24.249011 11576 net.cpp:380] conv4_1 -> conv4_1
I1210 21:25:24.250010 11576 net.cpp:122] Setting up conv4_1
I1210 21:25:24.250010 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.250010 11576 net.cpp:137] Memory required for data: 400590000
I1210 21:25:24.250010 11576 layer_factory.cpp:58] Creating layer bn4_1
I1210 21:25:24.250010 11576 net.cpp:84] Creating Layer bn4_1
I1210 21:25:24.250010 11576 net.cpp:406] bn4_1 <- conv4_1
I1210 21:25:24.250010 11576 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1210 21:25:24.250010 11576 net.cpp:122] Setting up bn4_1
I1210 21:25:24.250010 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.250010 11576 net.cpp:137] Memory required for data: 405710000
I1210 21:25:24.250010 11576 layer_factory.cpp:58] Creating layer scale4_1
I1210 21:25:24.250010 11576 net.cpp:84] Creating Layer scale4_1
I1210 21:25:24.250010 11576 net.cpp:406] scale4_1 <- conv4_1
I1210 21:25:24.250010 11576 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1210 21:25:24.250010 11576 layer_factory.cpp:58] Creating layer scale4_1
I1210 21:25:24.250010 11576 net.cpp:122] Setting up scale4_1
I1210 21:25:24.250010 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.250010 11576 net.cpp:137] Memory required for data: 410830000
I1210 21:25:24.250010 11576 layer_factory.cpp:58] Creating layer relu4_1
I1210 21:25:24.250010 11576 net.cpp:84] Creating Layer relu4_1
I1210 21:25:24.250010 11576 net.cpp:406] relu4_1 <- conv4_1
I1210 21:25:24.250010 11576 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1210 21:25:24.251010 11576 net.cpp:122] Setting up relu4_1
I1210 21:25:24.251010 11576 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 21:25:24.251010 11576 net.cpp:137] Memory required for data: 415950000
I1210 21:25:24.251010 11576 layer_factory.cpp:58] Creating layer conv4_2
I1210 21:25:24.251010 11576 net.cpp:84] Creating Layer conv4_2
I1210 21:25:24.251010 11576 net.cpp:406] conv4_2 <- conv4_1
I1210 21:25:24.251010 11576 net.cpp:380] conv4_2 -> conv4_2
I1210 21:25:24.252010 11576 net.cpp:122] Setting up conv4_2
I1210 21:25:24.252010 11576 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 21:25:24.252010 11576 net.cpp:137] Memory required for data: 421889200
I1210 21:25:24.252010 11576 layer_factory.cpp:58] Creating layer bn4_2
I1210 21:25:24.252010 11576 net.cpp:84] Creating Layer bn4_2
I1210 21:25:24.252010 11576 net.cpp:406] bn4_2 <- conv4_2
I1210 21:25:24.252010 11576 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1210 21:25:24.252010 11576 net.cpp:122] Setting up bn4_2
I1210 21:25:24.252010 11576 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 21:25:24.252010 11576 net.cpp:137] Memory required for data: 427828400
I1210 21:25:24.253010 11576 layer_factory.cpp:58] Creating layer scale4_2
I1210 21:25:24.253010 11576 net.cpp:84] Creating Layer scale4_2
I1210 21:25:24.253010 11576 net.cpp:406] scale4_2 <- conv4_2
I1210 21:25:24.253010 11576 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1210 21:25:24.253010 11576 layer_factory.cpp:58] Creating layer scale4_2
I1210 21:25:24.253010 11576 net.cpp:122] Setting up scale4_2
I1210 21:25:24.253010 11576 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 21:25:24.253010 11576 net.cpp:137] Memory required for data: 433767600
I1210 21:25:24.253010 11576 layer_factory.cpp:58] Creating layer relu4_2
I1210 21:25:24.253010 11576 net.cpp:84] Creating Layer relu4_2
I1210 21:25:24.253010 11576 net.cpp:406] relu4_2 <- conv4_2
I1210 21:25:24.253010 11576 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1210 21:25:24.253010 11576 net.cpp:122] Setting up relu4_2
I1210 21:25:24.253010 11576 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 21:25:24.253010 11576 net.cpp:137] Memory required for data: 439706800
I1210 21:25:24.253010 11576 layer_factory.cpp:58] Creating layer pool4_2
I1210 21:25:24.253010 11576 net.cpp:84] Creating Layer pool4_2
I1210 21:25:24.253010 11576 net.cpp:406] pool4_2 <- conv4_2
I1210 21:25:24.253010 11576 net.cpp:380] pool4_2 -> pool4_2
I1210 21:25:24.254010 11576 net.cpp:122] Setting up pool4_2
I1210 21:25:24.254010 11576 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 21:25:24.254010 11576 net.cpp:137] Memory required for data: 441191600
I1210 21:25:24.254010 11576 layer_factory.cpp:58] Creating layer conv4_0
I1210 21:25:24.254010 11576 net.cpp:84] Creating Layer conv4_0
I1210 21:25:24.254010 11576 net.cpp:406] conv4_0 <- pool4_2
I1210 21:25:24.254010 11576 net.cpp:380] conv4_0 -> conv4_0
I1210 21:25:24.256011 11576 net.cpp:122] Setting up conv4_0
I1210 21:25:24.256011 11576 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 21:25:24.256011 11576 net.cpp:137] Memory required for data: 442676400
I1210 21:25:24.256011 11576 layer_factory.cpp:58] Creating layer bn4_0
I1210 21:25:24.256011 11576 net.cpp:84] Creating Layer bn4_0
I1210 21:25:24.256011 11576 net.cpp:406] bn4_0 <- conv4_0
I1210 21:25:24.256011 11576 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1210 21:25:24.256011 11576 net.cpp:122] Setting up bn4_0
I1210 21:25:24.256011 11576 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 21:25:24.256011 11576 net.cpp:137] Memory required for data: 444161200
I1210 21:25:24.256011 11576 layer_factory.cpp:58] Creating layer scale4_0
I1210 21:25:24.256011 11576 net.cpp:84] Creating Layer scale4_0
I1210 21:25:24.256011 11576 net.cpp:406] scale4_0 <- conv4_0
I1210 21:25:24.256011 11576 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1210 21:25:24.256011 11576 layer_factory.cpp:58] Creating layer scale4_0
I1210 21:25:24.256011 11576 net.cpp:122] Setting up scale4_0
I1210 21:25:24.257010 11576 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 21:25:24.257010 11576 net.cpp:137] Memory required for data: 445646000
I1210 21:25:24.257010 11576 layer_factory.cpp:58] Creating layer relu4_0
I1210 21:25:24.257010 11576 net.cpp:84] Creating Layer relu4_0
I1210 21:25:24.257010 11576 net.cpp:406] relu4_0 <- conv4_0
I1210 21:25:24.257010 11576 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1210 21:25:24.257010 11576 net.cpp:122] Setting up relu4_0
I1210 21:25:24.257010 11576 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 21:25:24.257010 11576 net.cpp:137] Memory required for data: 447130800
I1210 21:25:24.257010 11576 layer_factory.cpp:58] Creating layer conv11
I1210 21:25:24.257010 11576 net.cpp:84] Creating Layer conv11
I1210 21:25:24.257010 11576 net.cpp:406] conv11 <- conv4_0
I1210 21:25:24.257010 11576 net.cpp:380] conv11 -> conv11
I1210 21:25:24.259011 11576 net.cpp:122] Setting up conv11
I1210 21:25:24.259011 11576 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 21:25:24.259011 11576 net.cpp:137] Memory required for data: 448922800
I1210 21:25:24.259011 11576 layer_factory.cpp:58] Creating layer bn_conv11
I1210 21:25:24.259011 11576 net.cpp:84] Creating Layer bn_conv11
I1210 21:25:24.259011 11576 net.cpp:406] bn_conv11 <- conv11
I1210 21:25:24.259011 11576 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1210 21:25:24.259011 11576 net.cpp:122] Setting up bn_conv11
I1210 21:25:24.259011 11576 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 21:25:24.259011 11576 net.cpp:137] Memory required for data: 450714800
I1210 21:25:24.259011 11576 layer_factory.cpp:58] Creating layer scale_conv11
I1210 21:25:24.259011 11576 net.cpp:84] Creating Layer scale_conv11
I1210 21:25:24.259011 11576 net.cpp:406] scale_conv11 <- conv11
I1210 21:25:24.259011 11576 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1210 21:25:24.259011 11576 layer_factory.cpp:58] Creating layer scale_conv11
I1210 21:25:24.259011 11576 net.cpp:122] Setting up scale_conv11
I1210 21:25:24.259011 11576 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 21:25:24.259011 11576 net.cpp:137] Memory required for data: 452506800
I1210 21:25:24.259011 11576 layer_factory.cpp:58] Creating layer relu_conv11
I1210 21:25:24.259011 11576 net.cpp:84] Creating Layer relu_conv11
I1210 21:25:24.259011 11576 net.cpp:406] relu_conv11 <- conv11
I1210 21:25:24.259011 11576 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1210 21:25:24.259011 11576 net.cpp:122] Setting up relu_conv11
I1210 21:25:24.259011 11576 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 21:25:24.259011 11576 net.cpp:137] Memory required for data: 454298800
I1210 21:25:24.260010 11576 layer_factory.cpp:58] Creating layer conv12
I1210 21:25:24.260010 11576 net.cpp:84] Creating Layer conv12
I1210 21:25:24.260010 11576 net.cpp:406] conv12 <- conv11
I1210 21:25:24.260010 11576 net.cpp:380] conv12 -> conv12
I1210 21:25:24.261009 11576 net.cpp:122] Setting up conv12
I1210 21:25:24.261009 11576 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 21:25:24.261009 11576 net.cpp:137] Memory required for data: 456602800
I1210 21:25:24.261009 11576 layer_factory.cpp:58] Creating layer bn_conv12
I1210 21:25:24.261009 11576 net.cpp:84] Creating Layer bn_conv12
I1210 21:25:24.261009 11576 net.cpp:406] bn_conv12 <- conv12
I1210 21:25:24.261009 11576 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1210 21:25:24.261009 11576 net.cpp:122] Setting up bn_conv12
I1210 21:25:24.261009 11576 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 21:25:24.261009 11576 net.cpp:137] Memory required for data: 458906800
I1210 21:25:24.261009 11576 layer_factory.cpp:58] Creating layer scale_conv12
I1210 21:25:24.261009 11576 net.cpp:84] Creating Layer scale_conv12
I1210 21:25:24.261009 11576 net.cpp:406] scale_conv12 <- conv12
I1210 21:25:24.261009 11576 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1210 21:25:24.262010 11576 layer_factory.cpp:58] Creating layer scale_conv12
I1210 21:25:24.262010 11576 net.cpp:122] Setting up scale_conv12
I1210 21:25:24.262010 11576 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 21:25:24.262010 11576 net.cpp:137] Memory required for data: 461210800
I1210 21:25:24.262010 11576 layer_factory.cpp:58] Creating layer relu_conv12
I1210 21:25:24.262010 11576 net.cpp:84] Creating Layer relu_conv12
I1210 21:25:24.262010 11576 net.cpp:406] relu_conv12 <- conv12
I1210 21:25:24.262010 11576 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1210 21:25:24.262010 11576 net.cpp:122] Setting up relu_conv12
I1210 21:25:24.262010 11576 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 21:25:24.262010 11576 net.cpp:137] Memory required for data: 463514800
I1210 21:25:24.262010 11576 layer_factory.cpp:58] Creating layer poolcp6
I1210 21:25:24.262010 11576 net.cpp:84] Creating Layer poolcp6
I1210 21:25:24.262010 11576 net.cpp:406] poolcp6 <- conv12
I1210 21:25:24.262010 11576 net.cpp:380] poolcp6 -> poolcp6
I1210 21:25:24.262010 11576 net.cpp:122] Setting up poolcp6
I1210 21:25:24.262010 11576 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1210 21:25:24.262010 11576 net.cpp:137] Memory required for data: 463550800
I1210 21:25:24.262010 11576 layer_factory.cpp:58] Creating layer ip1
I1210 21:25:24.262010 11576 net.cpp:84] Creating Layer ip1
I1210 21:25:24.262010 11576 net.cpp:406] ip1 <- poolcp6
I1210 21:25:24.262010 11576 net.cpp:380] ip1 -> ip1
I1210 21:25:24.262010 11576 net.cpp:122] Setting up ip1
I1210 21:25:24.262010 11576 net.cpp:129] Top shape: 100 100 (10000)
I1210 21:25:24.262010 11576 net.cpp:137] Memory required for data: 463590800
I1210 21:25:24.262010 11576 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1210 21:25:24.262010 11576 net.cpp:84] Creating Layer ip1_ip1_0_split
I1210 21:25:24.262010 11576 net.cpp:406] ip1_ip1_0_split <- ip1
I1210 21:25:24.262010 11576 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1210 21:25:24.262010 11576 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1210 21:25:24.262010 11576 net.cpp:122] Setting up ip1_ip1_0_split
I1210 21:25:24.262010 11576 net.cpp:129] Top shape: 100 100 (10000)
I1210 21:25:24.262010 11576 net.cpp:129] Top shape: 100 100 (10000)
I1210 21:25:24.262010 11576 net.cpp:137] Memory required for data: 463670800
I1210 21:25:24.262010 11576 layer_factory.cpp:58] Creating layer accuracy
I1210 21:25:24.263010 11576 net.cpp:84] Creating Layer accuracy
I1210 21:25:24.263010 11576 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1210 21:25:24.263010 11576 net.cpp:406] accuracy <- label_cifar_1_split_0
I1210 21:25:24.263010 11576 net.cpp:380] accuracy -> accuracy
I1210 21:25:24.263010 11576 net.cpp:122] Setting up accuracy
I1210 21:25:24.263010 11576 net.cpp:129] Top shape: (1)
I1210 21:25:24.263010 11576 net.cpp:137] Memory required for data: 463670804
I1210 21:25:24.263010 11576 layer_factory.cpp:58] Creating layer loss
I1210 21:25:24.263010 11576 net.cpp:84] Creating Layer loss
I1210 21:25:24.263010 11576 net.cpp:406] loss <- ip1_ip1_0_split_1
I1210 21:25:24.263010 11576 net.cpp:406] loss <- label_cifar_1_split_1
I1210 21:25:24.263010 11576 net.cpp:380] loss -> loss
I1210 21:25:24.263010 11576 layer_factory.cpp:58] Creating layer loss
I1210 21:25:24.263010 11576 net.cpp:122] Setting up loss
I1210 21:25:24.263010 11576 net.cpp:129] Top shape: (1)
I1210 21:25:24.263010 11576 net.cpp:132]     with loss weight 1
I1210 21:25:24.263010 11576 net.cpp:137] Memory required for data: 463670808
I1210 21:25:24.263010 11576 net.cpp:198] loss needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:200] accuracy does not need backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] ip1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] poolcp6 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] relu_conv12 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] scale_conv12 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] bn_conv12 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] conv12 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] relu_conv11 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] scale_conv11 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] bn_conv11 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] conv11 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] relu4_0 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] scale4_0 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] bn4_0 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] conv4_0 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] pool4_2 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] relu4_2 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] scale4_2 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] bn4_2 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] conv4_2 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] relu4_1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] scale4_1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] bn4_1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] conv4_1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] relu4 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] scale4 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] bn4 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] conv4 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] relu3_1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] scale3_1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] bn3_1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] conv3_1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] relu3 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] scale3 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] bn3 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] conv3 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] pool2_1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] relu2_2 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] scale2_2 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] bn2_2 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] conv2_2 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] relu2_1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] scale2_1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] bn2_1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] conv2_1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] relu2 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] scale2 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] bn2 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] conv2 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] relu1_0 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] scale1_0 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] bn1_0 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] conv1_0 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] relu1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] scale1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] bn1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:198] conv1 needs backward computation.
I1210 21:25:24.263010 11576 net.cpp:200] label_cifar_1_split does not need backward computation.
I1210 21:25:24.263010 11576 net.cpp:200] cifar does not need backward computation.
I1210 21:25:24.263010 11576 net.cpp:242] This network produces output accuracy
I1210 21:25:24.263010 11576 net.cpp:242] This network produces output loss
I1210 21:25:24.263010 11576 net.cpp:255] Network initialization done.
I1210 21:25:24.264014 11576 solver.cpp:56] Solver scaffolding done.
I1210 21:25:24.268029 11576 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_90000.solverstate
I1210 21:25:24.272029 11576 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_90000.caffemodel
I1210 21:25:24.272029 11576 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 21:25:24.272029 11576 sgd_solver.cpp:318] SGDSolver: restoring history
I1210 21:25:24.275028 11576 caffe.cpp:249] Starting Optimization
I1210 21:25:24.276028 11576 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV1_360k
I1210 21:25:24.276028 11576 solver.cpp:273] Learning Rate Policy: multistep
I1210 21:25:24.278012 11576 solver.cpp:330] Iteration 90000, Testing net (#0)
I1210 21:25:24.280021 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:25:25.631575 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:25:25.681578 11576 solver.cpp:397]     Test net output #0: accuracy = 0.5569
I1210 21:25:25.681578 11576 solver.cpp:397]     Test net output #1: loss = 1.78659 (* 1 = 1.78659 loss)
I1210 21:25:25.789151 11576 solver.cpp:218] Iteration 90000 (59495.1 iter/s, 1.51273s/100 iters), loss = 0.843073
I1210 21:25:25.789151 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 21:25:25.789151 11576 solver.cpp:237]     Train net output #1: loss = 0.843073 (* 1 = 0.843073 loss)
I1210 21:25:25.789151 11576 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1210 21:25:31.729725 11576 solver.cpp:218] Iteration 90100 (16.836 iter/s, 5.93967s/100 iters), loss = 0.731157
I1210 21:25:31.729725 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 21:25:31.729725 11576 solver.cpp:237]     Train net output #1: loss = 0.731157 (* 1 = 0.731157 loss)
I1210 21:25:31.729725 11576 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1210 21:25:37.693395 11576 solver.cpp:218] Iteration 90200 (16.7691 iter/s, 5.96336s/100 iters), loss = 0.588407
I1210 21:25:37.693395 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:25:37.693395 11576 solver.cpp:237]     Train net output #1: loss = 0.588407 (* 1 = 0.588407 loss)
I1210 21:25:37.693395 11576 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1210 21:25:43.679023 11576 solver.cpp:218] Iteration 90300 (16.7075 iter/s, 5.98533s/100 iters), loss = 0.92492
I1210 21:25:43.679023 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1210 21:25:43.679023 11576 solver.cpp:237]     Train net output #1: loss = 0.92492 (* 1 = 0.92492 loss)
I1210 21:25:43.679023 11576 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1210 21:25:49.584806 11576 solver.cpp:218] Iteration 90400 (16.9326 iter/s, 5.90576s/100 iters), loss = 0.958294
I1210 21:25:49.584806 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1210 21:25:49.584806 11576 solver.cpp:237]     Train net output #1: loss = 0.958294 (* 1 = 0.958294 loss)
I1210 21:25:49.584806 11576 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1210 21:25:55.309033 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:25:55.541635 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_90500.caffemodel
I1210 21:25:55.556155 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_90500.solverstate
I1210 21:25:55.561151 11576 solver.cpp:330] Iteration 90500, Testing net (#0)
I1210 21:25:55.561151 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:25:56.860014 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:25:56.910024 11576 solver.cpp:397]     Test net output #0: accuracy = 0.5836
I1210 21:25:56.910024 11576 solver.cpp:397]     Test net output #1: loss = 1.63722 (* 1 = 1.63722 loss)
I1210 21:25:56.968525 11576 solver.cpp:218] Iteration 90500 (13.5439 iter/s, 7.3834s/100 iters), loss = 0.744145
I1210 21:25:56.969529 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 21:25:56.969529 11576 solver.cpp:237]     Train net output #1: loss = 0.744145 (* 1 = 0.744145 loss)
I1210 21:25:56.969529 11576 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1210 21:26:02.890682 11576 solver.cpp:218] Iteration 90600 (16.8894 iter/s, 5.92088s/100 iters), loss = 0.769853
I1210 21:26:02.890682 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 21:26:02.890682 11576 solver.cpp:237]     Train net output #1: loss = 0.769853 (* 1 = 0.769853 loss)
I1210 21:26:02.890682 11576 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1210 21:26:08.912564 11576 solver.cpp:218] Iteration 90700 (16.6074 iter/s, 6.02142s/100 iters), loss = 0.59142
I1210 21:26:08.912564 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 21:26:08.912564 11576 solver.cpp:237]     Train net output #1: loss = 0.59142 (* 1 = 0.59142 loss)
I1210 21:26:08.912564 11576 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1210 21:26:14.911103 11576 solver.cpp:218] Iteration 90800 (16.6716 iter/s, 5.99824s/100 iters), loss = 0.892107
I1210 21:26:14.911103 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1210 21:26:14.911103 11576 solver.cpp:237]     Train net output #1: loss = 0.892107 (* 1 = 0.892107 loss)
I1210 21:26:14.911103 11576 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1210 21:26:20.837730 11576 solver.cpp:218] Iteration 90900 (16.8751 iter/s, 5.92589s/100 iters), loss = 0.740154
I1210 21:26:20.837730 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 21:26:20.837730 11576 solver.cpp:237]     Train net output #1: loss = 0.740154 (* 1 = 0.740154 loss)
I1210 21:26:20.837730 11576 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1210 21:26:26.468760 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:26:26.701827 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_91000.caffemodel
I1210 21:26:26.715826 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_91000.solverstate
I1210 21:26:26.720825 11576 solver.cpp:330] Iteration 91000, Testing net (#0)
I1210 21:26:26.720825 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:26:28.016173 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:26:28.068171 11576 solver.cpp:397]     Test net output #0: accuracy = 0.575
I1210 21:26:28.068171 11576 solver.cpp:397]     Test net output #1: loss = 1.67284 (* 1 = 1.67284 loss)
I1210 21:26:28.127183 11576 solver.cpp:218] Iteration 91000 (13.7193 iter/s, 7.28902s/100 iters), loss = 0.738344
I1210 21:26:28.127183 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 21:26:28.127183 11576 solver.cpp:237]     Train net output #1: loss = 0.738344 (* 1 = 0.738344 loss)
I1210 21:26:28.127183 11576 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1210 21:26:34.063498 11576 solver.cpp:218] Iteration 91100 (16.8469 iter/s, 5.93582s/100 iters), loss = 0.808392
I1210 21:26:34.063498 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1210 21:26:34.063498 11576 solver.cpp:237]     Train net output #1: loss = 0.808392 (* 1 = 0.808392 loss)
I1210 21:26:34.063498 11576 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1210 21:26:39.995434 11576 solver.cpp:218] Iteration 91200 (16.8579 iter/s, 5.93192s/100 iters), loss = 0.600734
I1210 21:26:39.995434 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:26:39.995434 11576 solver.cpp:237]     Train net output #1: loss = 0.600734 (* 1 = 0.600734 loss)
I1210 21:26:39.995434 11576 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1210 21:26:45.931602 11576 solver.cpp:218] Iteration 91300 (16.8486 iter/s, 5.93521s/100 iters), loss = 0.687325
I1210 21:26:45.931602 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 21:26:45.931602 11576 solver.cpp:237]     Train net output #1: loss = 0.687325 (* 1 = 0.687325 loss)
I1210 21:26:45.931602 11576 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1210 21:26:51.891873 11576 solver.cpp:218] Iteration 91400 (16.7783 iter/s, 5.96009s/100 iters), loss = 0.894502
I1210 21:26:51.891873 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 21:26:51.891873 11576 solver.cpp:237]     Train net output #1: loss = 0.894502 (* 1 = 0.894502 loss)
I1210 21:26:51.891873 11576 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1210 21:26:57.693305 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:26:57.927245 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_91500.caffemodel
I1210 21:26:57.942261 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_91500.solverstate
I1210 21:26:57.947259 11576 solver.cpp:330] Iteration 91500, Testing net (#0)
I1210 21:26:57.947259 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:26:59.274817 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:26:59.327836 11576 solver.cpp:397]     Test net output #0: accuracy = 0.5586
I1210 21:26:59.327836 11576 solver.cpp:397]     Test net output #1: loss = 1.80264 (* 1 = 1.80264 loss)
I1210 21:26:59.384873 11576 solver.cpp:218] Iteration 91500 (13.3458 iter/s, 7.49299s/100 iters), loss = 0.704623
I1210 21:26:59.384873 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 21:26:59.384873 11576 solver.cpp:237]     Train net output #1: loss = 0.704623 (* 1 = 0.704623 loss)
I1210 21:26:59.384873 11576 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1210 21:27:05.391187 11576 solver.cpp:218] Iteration 91600 (16.6505 iter/s, 6.00583s/100 iters), loss = 0.769385
I1210 21:27:05.391187 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 21:27:05.391187 11576 solver.cpp:237]     Train net output #1: loss = 0.769385 (* 1 = 0.769385 loss)
I1210 21:27:05.391187 11576 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1210 21:27:11.467738 11576 solver.cpp:218] Iteration 91700 (16.4596 iter/s, 6.07548s/100 iters), loss = 0.625165
I1210 21:27:11.467738 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 21:27:11.467738 11576 solver.cpp:237]     Train net output #1: loss = 0.625165 (* 1 = 0.625165 loss)
I1210 21:27:11.467738 11576 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1210 21:27:17.550256 11576 solver.cpp:218] Iteration 91800 (16.441 iter/s, 6.08234s/100 iters), loss = 0.756779
I1210 21:27:17.550256 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 21:27:17.550256 11576 solver.cpp:237]     Train net output #1: loss = 0.756779 (* 1 = 0.756779 loss)
I1210 21:27:17.550256 11576 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1210 21:27:23.592725 11576 solver.cpp:218] Iteration 91900 (16.5531 iter/s, 6.04118s/100 iters), loss = 0.824658
I1210 21:27:23.592725 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 21:27:23.592725 11576 solver.cpp:237]     Train net output #1: loss = 0.824658 (* 1 = 0.824658 loss)
I1210 21:27:23.592725 11576 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1210 21:27:29.330193 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:27:29.573210 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_92000.caffemodel
I1210 21:27:29.588213 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_92000.solverstate
I1210 21:27:29.594214 11576 solver.cpp:330] Iteration 92000, Testing net (#0)
I1210 21:27:29.594214 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:27:30.922317 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:27:30.973315 11576 solver.cpp:397]     Test net output #0: accuracy = 0.5604
I1210 21:27:30.973315 11576 solver.cpp:397]     Test net output #1: loss = 1.72464 (* 1 = 1.72464 loss)
I1210 21:27:31.032318 11576 solver.cpp:218] Iteration 92000 (13.4426 iter/s, 7.43906s/100 iters), loss = 0.698608
I1210 21:27:31.032318 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 21:27:31.032318 11576 solver.cpp:237]     Train net output #1: loss = 0.698608 (* 1 = 0.698608 loss)
I1210 21:27:31.032318 11576 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1210 21:27:37.121872 11576 solver.cpp:218] Iteration 92100 (16.422 iter/s, 6.08937s/100 iters), loss = 0.872807
I1210 21:27:37.121872 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 21:27:37.121872 11576 solver.cpp:237]     Train net output #1: loss = 0.872807 (* 1 = 0.872807 loss)
I1210 21:27:37.121872 11576 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1210 21:27:43.208329 11576 solver.cpp:218] Iteration 92200 (16.4301 iter/s, 6.08638s/100 iters), loss = 0.652656
I1210 21:27:43.209331 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:27:43.209331 11576 solver.cpp:237]     Train net output #1: loss = 0.652656 (* 1 = 0.652656 loss)
I1210 21:27:43.209331 11576 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1210 21:27:49.300684 11576 solver.cpp:218] Iteration 92300 (16.4181 iter/s, 6.09083s/100 iters), loss = 0.931536
I1210 21:27:49.300684 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 21:27:49.300684 11576 solver.cpp:237]     Train net output #1: loss = 0.931536 (* 1 = 0.931536 loss)
I1210 21:27:49.300684 11576 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1210 21:27:55.260383 11576 solver.cpp:218] Iteration 92400 (16.7809 iter/s, 5.95916s/100 iters), loss = 0.80317
I1210 21:27:55.260383 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 21:27:55.260383 11576 solver.cpp:237]     Train net output #1: loss = 0.80317 (* 1 = 0.80317 loss)
I1210 21:27:55.260383 11576 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1210 21:28:00.901523 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:28:01.134567 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_92500.caffemodel
I1210 21:28:01.148573 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_92500.solverstate
I1210 21:28:01.153571 11576 solver.cpp:330] Iteration 92500, Testing net (#0)
I1210 21:28:01.153571 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:28:02.457432 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:28:02.510450 11576 solver.cpp:397]     Test net output #0: accuracy = 0.5575
I1210 21:28:02.510450 11576 solver.cpp:397]     Test net output #1: loss = 1.81872 (* 1 = 1.81872 loss)
I1210 21:28:02.569463 11576 solver.cpp:218] Iteration 92500 (13.6811 iter/s, 7.30934s/100 iters), loss = 0.707155
I1210 21:28:02.569463 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 21:28:02.570454 11576 solver.cpp:237]     Train net output #1: loss = 0.707155 (* 1 = 0.707155 loss)
I1210 21:28:02.570454 11576 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1210 21:28:08.691965 11576 solver.cpp:218] Iteration 92600 (16.337 iter/s, 6.12106s/100 iters), loss = 0.705415
I1210 21:28:08.691965 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 21:28:08.691965 11576 solver.cpp:237]     Train net output #1: loss = 0.705415 (* 1 = 0.705415 loss)
I1210 21:28:08.691965 11576 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1210 21:28:14.707381 11576 solver.cpp:218] Iteration 92700 (16.6245 iter/s, 6.01522s/100 iters), loss = 0.546036
I1210 21:28:14.707381 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 21:28:14.707381 11576 solver.cpp:237]     Train net output #1: loss = 0.546036 (* 1 = 0.546036 loss)
I1210 21:28:14.707381 11576 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1210 21:28:20.816938 11576 solver.cpp:218] Iteration 92800 (16.3696 iter/s, 6.1089s/100 iters), loss = 0.827927
I1210 21:28:20.816938 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 21:28:20.816938 11576 solver.cpp:237]     Train net output #1: loss = 0.827927 (* 1 = 0.827927 loss)
I1210 21:28:20.816938 11576 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1210 21:28:26.847178 11576 solver.cpp:218] Iteration 92900 (16.5825 iter/s, 6.03044s/100 iters), loss = 0.84287
I1210 21:28:26.847178 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 21:28:26.847178 11576 solver.cpp:237]     Train net output #1: loss = 0.84287 (* 1 = 0.84287 loss)
I1210 21:28:26.847178 11576 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1210 21:28:32.636024 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:28:32.873036 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_93000.caffemodel
I1210 21:28:32.888540 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_93000.solverstate
I1210 21:28:32.893541 11576 solver.cpp:330] Iteration 93000, Testing net (#0)
I1210 21:28:32.893541 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:28:34.227196 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:28:34.280198 11576 solver.cpp:397]     Test net output #0: accuracy = 0.5798
I1210 21:28:34.280198 11576 solver.cpp:397]     Test net output #1: loss = 1.69306 (* 1 = 1.69306 loss)
I1210 21:28:34.339215 11576 solver.cpp:218] Iteration 93000 (13.349 iter/s, 7.49118s/100 iters), loss = 0.762973
I1210 21:28:34.339215 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 21:28:34.339215 11576 solver.cpp:237]     Train net output #1: loss = 0.762973 (* 1 = 0.762973 loss)
I1210 21:28:34.339215 11576 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1210 21:28:40.405712 11576 solver.cpp:218] Iteration 93100 (16.4863 iter/s, 6.06565s/100 iters), loss = 0.70077
I1210 21:28:40.405712 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 21:28:40.405712 11576 solver.cpp:237]     Train net output #1: loss = 0.70077 (* 1 = 0.70077 loss)
I1210 21:28:40.405712 11576 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1210 21:28:46.527539 11576 solver.cpp:218] Iteration 93200 (16.3352 iter/s, 6.12175s/100 iters), loss = 0.661711
I1210 21:28:46.527539 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 21:28:46.527539 11576 solver.cpp:237]     Train net output #1: loss = 0.661711 (* 1 = 0.661711 loss)
I1210 21:28:46.527539 11576 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1210 21:28:52.545018 11576 solver.cpp:218] Iteration 93300 (16.6215 iter/s, 6.01631s/100 iters), loss = 1.00108
I1210 21:28:52.545018 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1210 21:28:52.545018 11576 solver.cpp:237]     Train net output #1: loss = 1.00108 (* 1 = 1.00108 loss)
I1210 21:28:52.545018 11576 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1210 21:28:58.568622 11576 solver.cpp:218] Iteration 93400 (16.6004 iter/s, 6.02396s/100 iters), loss = 0.787909
I1210 21:28:58.568622 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 21:28:58.568622 11576 solver.cpp:237]     Train net output #1: loss = 0.787909 (* 1 = 0.787909 loss)
I1210 21:28:58.568622 11576 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1210 21:29:04.294641 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:29:04.530154 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_93500.caffemodel
I1210 21:29:04.546154 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_93500.solverstate
I1210 21:29:04.551154 11576 solver.cpp:330] Iteration 93500, Testing net (#0)
I1210 21:29:04.551154 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:29:05.867287 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:29:05.919292 11576 solver.cpp:397]     Test net output #0: accuracy = 0.5496
I1210 21:29:05.919292 11576 solver.cpp:397]     Test net output #1: loss = 1.8025 (* 1 = 1.8025 loss)
I1210 21:29:05.976290 11576 solver.cpp:218] Iteration 93500 (13.5015 iter/s, 7.40659s/100 iters), loss = 0.688375
I1210 21:29:05.976290 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 21:29:05.976290 11576 solver.cpp:237]     Train net output #1: loss = 0.688375 (* 1 = 0.688375 loss)
I1210 21:29:05.976290 11576 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1210 21:29:11.993791 11576 solver.cpp:218] Iteration 93600 (16.6197 iter/s, 6.01697s/100 iters), loss = 0.698969
I1210 21:29:11.993791 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 21:29:11.993791 11576 solver.cpp:237]     Train net output #1: loss = 0.698969 (* 1 = 0.698969 loss)
I1210 21:29:11.993791 11576 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1210 21:29:18.027349 11576 solver.cpp:218] Iteration 93700 (16.5741 iter/s, 6.03351s/100 iters), loss = 0.663627
I1210 21:29:18.027349 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 21:29:18.027349 11576 solver.cpp:237]     Train net output #1: loss = 0.663627 (* 1 = 0.663627 loss)
I1210 21:29:18.027349 11576 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1210 21:29:24.015800 11576 solver.cpp:218] Iteration 93800 (16.7007 iter/s, 5.98777s/100 iters), loss = 0.820682
I1210 21:29:24.015800 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1210 21:29:24.015800 11576 solver.cpp:237]     Train net output #1: loss = 0.820682 (* 1 = 0.820682 loss)
I1210 21:29:24.015800 11576 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1210 21:29:29.949277 11576 solver.cpp:218] Iteration 93900 (16.856 iter/s, 5.93261s/100 iters), loss = 0.912973
I1210 21:29:29.949277 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1210 21:29:29.949277 11576 solver.cpp:237]     Train net output #1: loss = 0.912973 (* 1 = 0.912973 loss)
I1210 21:29:29.949277 11576 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1210 21:29:35.621764 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:29:35.855779 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_94000.caffemodel
I1210 21:29:35.873773 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_94000.solverstate
I1210 21:29:35.878775 11576 solver.cpp:330] Iteration 94000, Testing net (#0)
I1210 21:29:35.878775 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:29:37.262975 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:29:37.301980 11576 solver.cpp:397]     Test net output #0: accuracy = 0.5729
I1210 21:29:37.301980 11576 solver.cpp:397]     Test net output #1: loss = 1.67246 (* 1 = 1.67246 loss)
I1210 21:29:37.358999 11576 solver.cpp:218] Iteration 94000 (13.4959 iter/s, 7.40967s/100 iters), loss = 0.712194
I1210 21:29:37.358999 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 21:29:37.358999 11576 solver.cpp:237]     Train net output #1: loss = 0.712194 (* 1 = 0.712194 loss)
I1210 21:29:37.358999 11576 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1210 21:29:43.418790 11576 solver.cpp:218] Iteration 94100 (16.5032 iter/s, 6.05945s/100 iters), loss = 0.737573
I1210 21:29:43.418790 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 21:29:43.418790 11576 solver.cpp:237]     Train net output #1: loss = 0.737573 (* 1 = 0.737573 loss)
I1210 21:29:43.418790 11576 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1210 21:29:49.398190 11576 solver.cpp:218] Iteration 94200 (16.7257 iter/s, 5.97881s/100 iters), loss = 0.61783
I1210 21:29:49.398190 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:29:49.398190 11576 solver.cpp:237]     Train net output #1: loss = 0.61783 (* 1 = 0.61783 loss)
I1210 21:29:49.398190 11576 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1210 21:29:55.372644 11576 solver.cpp:218] Iteration 94300 (16.7412 iter/s, 5.97327s/100 iters), loss = 0.860267
I1210 21:29:55.372644 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 21:29:55.372644 11576 solver.cpp:237]     Train net output #1: loss = 0.860267 (* 1 = 0.860267 loss)
I1210 21:29:55.372644 11576 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1210 21:30:01.405061 11576 solver.cpp:218] Iteration 94400 (16.5779 iter/s, 6.03212s/100 iters), loss = 0.788469
I1210 21:30:01.405061 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 21:30:01.405061 11576 solver.cpp:237]     Train net output #1: loss = 0.788469 (* 1 = 0.788469 loss)
I1210 21:30:01.405061 11576 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1210 21:30:07.165653 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:30:07.402189 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_94500.caffemodel
I1210 21:30:07.415693 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_94500.solverstate
I1210 21:30:07.421694 11576 solver.cpp:330] Iteration 94500, Testing net (#0)
I1210 21:30:07.421694 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:30:08.750808 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:30:08.803812 11576 solver.cpp:397]     Test net output #0: accuracy = 0.4933
I1210 21:30:08.803812 11576 solver.cpp:397]     Test net output #1: loss = 2.24093 (* 1 = 2.24093 loss)
I1210 21:30:08.864825 11576 solver.cpp:218] Iteration 94500 (13.406 iter/s, 7.45937s/100 iters), loss = 0.70349
I1210 21:30:08.864825 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 21:30:08.864825 11576 solver.cpp:237]     Train net output #1: loss = 0.70349 (* 1 = 0.70349 loss)
I1210 21:30:08.864825 11576 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1210 21:30:14.872367 11576 solver.cpp:218] Iteration 94600 (16.6474 iter/s, 6.00695s/100 iters), loss = 0.648499
I1210 21:30:14.872367 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 21:30:14.872367 11576 solver.cpp:237]     Train net output #1: loss = 0.648499 (* 1 = 0.648499 loss)
I1210 21:30:14.872367 11576 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1210 21:30:20.868880 11576 solver.cpp:218] Iteration 94700 (16.6776 iter/s, 5.99607s/100 iters), loss = 0.553679
I1210 21:30:20.868880 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 21:30:20.868880 11576 solver.cpp:237]     Train net output #1: loss = 0.553679 (* 1 = 0.553679 loss)
I1210 21:30:20.868880 11576 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1210 21:30:26.853374 11576 solver.cpp:218] Iteration 94800 (16.7117 iter/s, 5.98385s/100 iters), loss = 0.918982
I1210 21:30:26.853374 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1210 21:30:26.853374 11576 solver.cpp:237]     Train net output #1: loss = 0.918982 (* 1 = 0.918982 loss)
I1210 21:30:26.853374 11576 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1210 21:30:32.871922 11576 solver.cpp:218] Iteration 94900 (16.6174 iter/s, 6.01778s/100 iters), loss = 0.761976
I1210 21:30:32.871922 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 21:30:32.871922 11576 solver.cpp:237]     Train net output #1: loss = 0.761976 (* 1 = 0.761976 loss)
I1210 21:30:32.871922 11576 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1210 21:30:38.591775 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:30:38.832865 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_95000.caffemodel
I1210 21:30:38.848865 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_95000.solverstate
I1210 21:30:38.853868 11576 solver.cpp:330] Iteration 95000, Testing net (#0)
I1210 21:30:38.853868 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:30:40.187281 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:30:40.239292 11576 solver.cpp:397]     Test net output #0: accuracy = 0.5702
I1210 21:30:40.239292 11576 solver.cpp:397]     Test net output #1: loss = 1.71754 (* 1 = 1.71754 loss)
I1210 21:30:40.297830 11576 solver.cpp:218] Iteration 95000 (13.4672 iter/s, 7.42546s/100 iters), loss = 0.853464
I1210 21:30:40.297830 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 21:30:40.297830 11576 solver.cpp:237]     Train net output #1: loss = 0.853464 (* 1 = 0.853464 loss)
I1210 21:30:40.297830 11576 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1210 21:30:40.297830 11576 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1210 21:30:46.414564 11576 solver.cpp:218] Iteration 95100 (16.349 iter/s, 6.11656s/100 iters), loss = 0.703186
I1210 21:30:46.414564 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 21:30:46.414564 11576 solver.cpp:237]     Train net output #1: loss = 0.703187 (* 1 = 0.703187 loss)
I1210 21:30:46.414564 11576 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1210 21:30:52.481026 11576 solver.cpp:218] Iteration 95200 (16.4867 iter/s, 6.06551s/100 iters), loss = 0.541238
I1210 21:30:52.481026 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 21:30:52.481026 11576 solver.cpp:237]     Train net output #1: loss = 0.541238 (* 1 = 0.541238 loss)
I1210 21:30:52.481026 11576 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1210 21:30:58.521325 11576 solver.cpp:218] Iteration 95300 (16.5561 iter/s, 6.04007s/100 iters), loss = 0.717275
I1210 21:30:58.521325 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 21:30:58.521325 11576 solver.cpp:237]     Train net output #1: loss = 0.717275 (* 1 = 0.717275 loss)
I1210 21:30:58.521325 11576 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1210 21:31:04.496992 11576 solver.cpp:218] Iteration 95400 (16.7354 iter/s, 5.97535s/100 iters), loss = 0.625943
I1210 21:31:04.496992 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 21:31:04.496992 11576 solver.cpp:237]     Train net output #1: loss = 0.625943 (* 1 = 0.625943 loss)
I1210 21:31:04.496992 11576 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1210 21:31:10.309442 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:31:10.547464 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_95500.caffemodel
I1210 21:31:10.561465 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_95500.solverstate
I1210 21:31:10.566465 11576 solver.cpp:330] Iteration 95500, Testing net (#0)
I1210 21:31:10.566465 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:31:11.885570 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:31:11.936568 11576 solver.cpp:397]     Test net output #0: accuracy = 0.672
I1210 21:31:11.936568 11576 solver.cpp:397]     Test net output #1: loss = 1.19753 (* 1 = 1.19753 loss)
I1210 21:31:11.993573 11576 solver.cpp:218] Iteration 95500 (13.34 iter/s, 7.49623s/100 iters), loss = 0.534363
I1210 21:31:11.993573 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:31:11.993573 11576 solver.cpp:237]     Train net output #1: loss = 0.534363 (* 1 = 0.534363 loss)
I1210 21:31:11.993573 11576 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1210 21:31:18.114076 11576 solver.cpp:218] Iteration 95600 (16.3408 iter/s, 6.11967s/100 iters), loss = 0.54907
I1210 21:31:18.114076 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 21:31:18.114076 11576 solver.cpp:237]     Train net output #1: loss = 0.54907 (* 1 = 0.54907 loss)
I1210 21:31:18.114076 11576 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1210 21:31:24.133527 11576 solver.cpp:218] Iteration 95700 (16.6125 iter/s, 6.01955s/100 iters), loss = 0.545948
I1210 21:31:24.134527 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:31:24.134527 11576 solver.cpp:237]     Train net output #1: loss = 0.545948 (* 1 = 0.545948 loss)
I1210 21:31:24.134527 11576 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1210 21:31:30.119990 11576 solver.cpp:218] Iteration 95800 (16.7064 iter/s, 5.98571s/100 iters), loss = 0.570826
I1210 21:31:30.119990 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 21:31:30.119990 11576 solver.cpp:237]     Train net output #1: loss = 0.570826 (* 1 = 0.570826 loss)
I1210 21:31:30.119990 11576 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1210 21:31:36.147409 11576 solver.cpp:218] Iteration 95900 (16.5926 iter/s, 6.02677s/100 iters), loss = 0.649632
I1210 21:31:36.147409 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 21:31:36.147409 11576 solver.cpp:237]     Train net output #1: loss = 0.649633 (* 1 = 0.649633 loss)
I1210 21:31:36.147409 11576 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1210 21:31:41.847154 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:31:42.084669 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_96000.caffemodel
I1210 21:31:42.099195 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_96000.solverstate
I1210 21:31:42.104176 11576 solver.cpp:330] Iteration 96000, Testing net (#0)
I1210 21:31:42.104176 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:31:43.416429 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:31:43.466944 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6719
I1210 21:31:43.466944 11576 solver.cpp:397]     Test net output #1: loss = 1.19423 (* 1 = 1.19423 loss)
I1210 21:31:43.522943 11576 solver.cpp:218] Iteration 96000 (13.5594 iter/s, 7.37496s/100 iters), loss = 0.563843
I1210 21:31:43.523444 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 21:31:43.523444 11576 solver.cpp:237]     Train net output #1: loss = 0.563843 (* 1 = 0.563843 loss)
I1210 21:31:43.523444 11576 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1210 21:31:49.525717 11576 solver.cpp:218] Iteration 96100 (16.6597 iter/s, 6.00251s/100 iters), loss = 0.630348
I1210 21:31:49.525717 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 21:31:49.525717 11576 solver.cpp:237]     Train net output #1: loss = 0.630349 (* 1 = 0.630349 loss)
I1210 21:31:49.525717 11576 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1210 21:31:55.529651 11576 solver.cpp:218] Iteration 96200 (16.6584 iter/s, 6.003s/100 iters), loss = 0.442573
I1210 21:31:55.529651 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:31:55.529651 11576 solver.cpp:237]     Train net output #1: loss = 0.442574 (* 1 = 0.442574 loss)
I1210 21:31:55.529651 11576 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1210 21:32:01.486647 11576 solver.cpp:218] Iteration 96300 (16.7886 iter/s, 5.95644s/100 iters), loss = 0.571282
I1210 21:32:01.486647 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:32:01.486647 11576 solver.cpp:237]     Train net output #1: loss = 0.571282 (* 1 = 0.571282 loss)
I1210 21:32:01.486647 11576 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1210 21:32:07.413605 11576 solver.cpp:218] Iteration 96400 (16.8717 iter/s, 5.92707s/100 iters), loss = 0.569284
I1210 21:32:07.413605 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 21:32:07.413605 11576 solver.cpp:237]     Train net output #1: loss = 0.569284 (* 1 = 0.569284 loss)
I1210 21:32:07.413605 11576 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1210 21:32:13.049057 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:32:13.283572 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_96500.caffemodel
I1210 21:32:13.298072 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_96500.solverstate
I1210 21:32:13.303077 11576 solver.cpp:330] Iteration 96500, Testing net (#0)
I1210 21:32:13.303077 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:32:14.599184 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:32:14.649197 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6696
I1210 21:32:14.649197 11576 solver.cpp:397]     Test net output #1: loss = 1.19142 (* 1 = 1.19142 loss)
I1210 21:32:14.706192 11576 solver.cpp:218] Iteration 96500 (13.7138 iter/s, 7.29193s/100 iters), loss = 0.504
I1210 21:32:14.706192 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:32:14.706192 11576 solver.cpp:237]     Train net output #1: loss = 0.504 (* 1 = 0.504 loss)
I1210 21:32:14.706192 11576 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1210 21:32:20.677628 11576 solver.cpp:218] Iteration 96600 (16.7491 iter/s, 5.97048s/100 iters), loss = 0.526406
I1210 21:32:20.677628 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:32:20.677628 11576 solver.cpp:237]     Train net output #1: loss = 0.526406 (* 1 = 0.526406 loss)
I1210 21:32:20.677628 11576 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1210 21:32:26.723187 11576 solver.cpp:218] Iteration 96700 (16.5421 iter/s, 6.04519s/100 iters), loss = 0.42761
I1210 21:32:26.723187 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:32:26.723187 11576 solver.cpp:237]     Train net output #1: loss = 0.42761 (* 1 = 0.42761 loss)
I1210 21:32:26.723187 11576 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1210 21:32:32.725631 11576 solver.cpp:218] Iteration 96800 (16.6593 iter/s, 6.00264s/100 iters), loss = 0.509166
I1210 21:32:32.726632 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:32:32.726632 11576 solver.cpp:237]     Train net output #1: loss = 0.509166 (* 1 = 0.509166 loss)
I1210 21:32:32.726632 11576 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1210 21:32:38.706048 11576 solver.cpp:218] Iteration 96900 (16.7247 iter/s, 5.9792s/100 iters), loss = 0.600015
I1210 21:32:38.706048 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 21:32:38.706048 11576 solver.cpp:237]     Train net output #1: loss = 0.600015 (* 1 = 0.600015 loss)
I1210 21:32:38.706048 11576 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1210 21:32:44.426355 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:32:44.673915 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_97000.caffemodel
I1210 21:32:44.687901 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_97000.solverstate
I1210 21:32:44.692903 11576 solver.cpp:330] Iteration 97000, Testing net (#0)
I1210 21:32:44.692903 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:32:46.010257 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:32:46.062243 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6676
I1210 21:32:46.062243 11576 solver.cpp:397]     Test net output #1: loss = 1.19891 (* 1 = 1.19891 loss)
I1210 21:32:46.117818 11576 solver.cpp:218] Iteration 97000 (13.4924 iter/s, 7.41155s/100 iters), loss = 0.481337
I1210 21:32:46.117818 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:32:46.117818 11576 solver.cpp:237]     Train net output #1: loss = 0.481337 (* 1 = 0.481337 loss)
I1210 21:32:46.117818 11576 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1210 21:32:52.055356 11576 solver.cpp:218] Iteration 97100 (16.8448 iter/s, 5.93656s/100 iters), loss = 0.519291
I1210 21:32:52.055356 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:32:52.055356 11576 solver.cpp:237]     Train net output #1: loss = 0.519291 (* 1 = 0.519291 loss)
I1210 21:32:52.055356 11576 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1210 21:32:57.982621 11576 solver.cpp:218] Iteration 97200 (16.8702 iter/s, 5.9276s/100 iters), loss = 0.395743
I1210 21:32:57.983630 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:32:57.983630 11576 solver.cpp:237]     Train net output #1: loss = 0.395743 (* 1 = 0.395743 loss)
I1210 21:32:57.983630 11576 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1210 21:33:03.927634 11576 solver.cpp:218] Iteration 97300 (16.8249 iter/s, 5.94357s/100 iters), loss = 0.472353
I1210 21:33:03.927634 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:33:03.927634 11576 solver.cpp:237]     Train net output #1: loss = 0.472353 (* 1 = 0.472353 loss)
I1210 21:33:03.927634 11576 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1210 21:33:09.856956 11576 solver.cpp:218] Iteration 97400 (16.8656 iter/s, 5.92922s/100 iters), loss = 0.568247
I1210 21:33:09.856956 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 21:33:09.856956 11576 solver.cpp:237]     Train net output #1: loss = 0.568248 (* 1 = 0.568248 loss)
I1210 21:33:09.856956 11576 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1210 21:33:15.494349 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:33:15.727360 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_97500.caffemodel
I1210 21:33:15.741360 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_97500.solverstate
I1210 21:33:15.745360 11576 solver.cpp:330] Iteration 97500, Testing net (#0)
I1210 21:33:15.746361 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:33:17.044448 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:33:17.094954 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6684
I1210 21:33:17.094954 11576 solver.cpp:397]     Test net output #1: loss = 1.19516 (* 1 = 1.19516 loss)
I1210 21:33:17.151455 11576 solver.cpp:218] Iteration 97500 (13.7103 iter/s, 7.29377s/100 iters), loss = 0.429636
I1210 21:33:17.151455 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 21:33:17.151455 11576 solver.cpp:237]     Train net output #1: loss = 0.429636 (* 1 = 0.429636 loss)
I1210 21:33:17.151455 11576 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1210 21:33:23.071364 11576 solver.cpp:218] Iteration 97600 (16.8913 iter/s, 5.92022s/100 iters), loss = 0.527008
I1210 21:33:23.071364 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 21:33:23.071364 11576 solver.cpp:237]     Train net output #1: loss = 0.527009 (* 1 = 0.527009 loss)
I1210 21:33:23.071364 11576 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1210 21:33:28.999301 11576 solver.cpp:218] Iteration 97700 (16.8728 iter/s, 5.9267s/100 iters), loss = 0.412489
I1210 21:33:28.999301 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:33:28.999301 11576 solver.cpp:237]     Train net output #1: loss = 0.41249 (* 1 = 0.41249 loss)
I1210 21:33:28.999301 11576 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1210 21:33:34.998585 11576 solver.cpp:218] Iteration 97800 (16.6673 iter/s, 5.99978s/100 iters), loss = 0.505047
I1210 21:33:34.999585 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:33:34.999585 11576 solver.cpp:237]     Train net output #1: loss = 0.505048 (* 1 = 0.505048 loss)
I1210 21:33:34.999585 11576 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1210 21:33:41.080176 11576 solver.cpp:218] Iteration 97900 (16.4462 iter/s, 6.08042s/100 iters), loss = 0.62946
I1210 21:33:41.080176 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 21:33:41.080176 11576 solver.cpp:237]     Train net output #1: loss = 0.62946 (* 1 = 0.62946 loss)
I1210 21:33:41.080176 11576 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1210 21:33:46.858644 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:33:47.098664 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_98000.caffemodel
I1210 21:33:47.113665 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_98000.solverstate
I1210 21:33:47.117666 11576 solver.cpp:330] Iteration 98000, Testing net (#0)
I1210 21:33:47.117666 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:33:48.443790 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:33:48.495801 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6716
I1210 21:33:48.496803 11576 solver.cpp:397]     Test net output #1: loss = 1.19837 (* 1 = 1.19837 loss)
I1210 21:33:48.554800 11576 solver.cpp:218] Iteration 98000 (13.38 iter/s, 7.47386s/100 iters), loss = 0.452549
I1210 21:33:48.554800 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:33:48.554800 11576 solver.cpp:237]     Train net output #1: loss = 0.45255 (* 1 = 0.45255 loss)
I1210 21:33:48.554800 11576 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1210 21:33:54.587329 11576 solver.cpp:218] Iteration 98100 (16.5781 iter/s, 6.03204s/100 iters), loss = 0.490878
I1210 21:33:54.587329 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:33:54.587329 11576 solver.cpp:237]     Train net output #1: loss = 0.490879 (* 1 = 0.490879 loss)
I1210 21:33:54.587329 11576 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1210 21:34:00.594396 11576 solver.cpp:218] Iteration 98200 (16.6471 iter/s, 6.00707s/100 iters), loss = 0.373185
I1210 21:34:00.594902 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:34:00.594902 11576 solver.cpp:237]     Train net output #1: loss = 0.373185 (* 1 = 0.373185 loss)
I1210 21:34:00.594902 11576 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1210 21:34:06.601426 11576 solver.cpp:218] Iteration 98300 (16.6486 iter/s, 6.0065s/100 iters), loss = 0.484529
I1210 21:34:06.601426 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:34:06.601426 11576 solver.cpp:237]     Train net output #1: loss = 0.484529 (* 1 = 0.484529 loss)
I1210 21:34:06.601426 11576 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1210 21:34:12.625988 11576 solver.cpp:218] Iteration 98400 (16.6011 iter/s, 6.02368s/100 iters), loss = 0.628461
I1210 21:34:12.625988 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 21:34:12.625988 11576 solver.cpp:237]     Train net output #1: loss = 0.628461 (* 1 = 0.628461 loss)
I1210 21:34:12.625988 11576 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1210 21:34:18.387578 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:34:18.627601 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_98500.caffemodel
I1210 21:34:18.649600 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_98500.solverstate
I1210 21:34:18.653599 11576 solver.cpp:330] Iteration 98500, Testing net (#0)
I1210 21:34:18.653599 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:34:19.970753 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:34:20.022773 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6726
I1210 21:34:20.022773 11576 solver.cpp:397]     Test net output #1: loss = 1.20131 (* 1 = 1.20131 loss)
I1210 21:34:20.081270 11576 solver.cpp:218] Iteration 98500 (13.4141 iter/s, 7.45485s/100 iters), loss = 0.47981
I1210 21:34:20.081270 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:34:20.081270 11576 solver.cpp:237]     Train net output #1: loss = 0.47981 (* 1 = 0.47981 loss)
I1210 21:34:20.081270 11576 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1210 21:34:26.086818 11576 solver.cpp:218] Iteration 98600 (16.6524 iter/s, 6.00513s/100 iters), loss = 0.417449
I1210 21:34:26.087318 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:34:26.087318 11576 solver.cpp:237]     Train net output #1: loss = 0.417449 (* 1 = 0.417449 loss)
I1210 21:34:26.087318 11576 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1210 21:34:32.119896 11576 solver.cpp:218] Iteration 98700 (16.5767 iter/s, 6.03256s/100 iters), loss = 0.431379
I1210 21:34:32.119896 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:34:32.119896 11576 solver.cpp:237]     Train net output #1: loss = 0.431379 (* 1 = 0.431379 loss)
I1210 21:34:32.119896 11576 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1210 21:34:38.168460 11576 solver.cpp:218] Iteration 98800 (16.5342 iter/s, 6.04808s/100 iters), loss = 0.466695
I1210 21:34:38.168460 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:34:38.168460 11576 solver.cpp:237]     Train net output #1: loss = 0.466695 (* 1 = 0.466695 loss)
I1210 21:34:38.168460 11576 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1210 21:34:44.273073 11576 solver.cpp:218] Iteration 98900 (16.3812 iter/s, 6.10455s/100 iters), loss = 0.513788
I1210 21:34:44.273073 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 21:34:44.273073 11576 solver.cpp:237]     Train net output #1: loss = 0.513788 (* 1 = 0.513788 loss)
I1210 21:34:44.273073 11576 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1210 21:34:50.074488 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:34:50.316514 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_99000.caffemodel
I1210 21:34:50.331511 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_99000.solverstate
I1210 21:34:50.336511 11576 solver.cpp:330] Iteration 99000, Testing net (#0)
I1210 21:34:50.336511 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:34:51.665643 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:34:51.717648 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6755
I1210 21:34:51.717648 11576 solver.cpp:397]     Test net output #1: loss = 1.20569 (* 1 = 1.20569 loss)
I1210 21:34:51.775647 11576 solver.cpp:218] Iteration 99000 (13.3306 iter/s, 7.50156s/100 iters), loss = 0.491964
I1210 21:34:51.775647 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:34:51.775647 11576 solver.cpp:237]     Train net output #1: loss = 0.491964 (* 1 = 0.491964 loss)
I1210 21:34:51.775647 11576 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1210 21:34:57.759117 11576 solver.cpp:218] Iteration 99100 (16.7123 iter/s, 5.98361s/100 iters), loss = 0.406212
I1210 21:34:57.760118 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:34:57.760118 11576 solver.cpp:237]     Train net output #1: loss = 0.406212 (* 1 = 0.406212 loss)
I1210 21:34:57.760118 11576 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1210 21:35:03.825645 11576 solver.cpp:218] Iteration 99200 (16.4857 iter/s, 6.06585s/100 iters), loss = 0.403422
I1210 21:35:03.826645 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:35:03.826645 11576 solver.cpp:237]     Train net output #1: loss = 0.403422 (* 1 = 0.403422 loss)
I1210 21:35:03.826645 11576 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1210 21:35:09.873260 11576 solver.cpp:218] Iteration 99300 (16.539 iter/s, 6.0463s/100 iters), loss = 0.47302
I1210 21:35:09.873260 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:35:09.873260 11576 solver.cpp:237]     Train net output #1: loss = 0.47302 (* 1 = 0.47302 loss)
I1210 21:35:09.873260 11576 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1210 21:35:15.867712 11576 solver.cpp:218] Iteration 99400 (16.6824 iter/s, 5.99434s/100 iters), loss = 0.476647
I1210 21:35:15.867712 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:35:15.867712 11576 solver.cpp:237]     Train net output #1: loss = 0.476647 (* 1 = 0.476647 loss)
I1210 21:35:15.867712 11576 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1210 21:35:21.752365 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:35:21.995394 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_99500.caffemodel
I1210 21:35:22.009408 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_99500.solverstate
I1210 21:35:22.014410 11576 solver.cpp:330] Iteration 99500, Testing net (#0)
I1210 21:35:22.015409 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:35:23.360529 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:35:23.411527 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6717
I1210 21:35:23.411527 11576 solver.cpp:397]     Test net output #1: loss = 1.2025 (* 1 = 1.2025 loss)
I1210 21:35:23.469527 11576 solver.cpp:218] Iteration 99500 (13.1556 iter/s, 7.60132s/100 iters), loss = 0.446364
I1210 21:35:23.469527 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:35:23.469527 11576 solver.cpp:237]     Train net output #1: loss = 0.446364 (* 1 = 0.446364 loss)
I1210 21:35:23.469527 11576 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1210 21:35:29.412431 11576 solver.cpp:218] Iteration 99600 (16.8284 iter/s, 5.94235s/100 iters), loss = 0.465721
I1210 21:35:29.412931 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:35:29.412931 11576 solver.cpp:237]     Train net output #1: loss = 0.465721 (* 1 = 0.465721 loss)
I1210 21:35:29.412931 11576 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1210 21:35:35.344378 11576 solver.cpp:218] Iteration 99700 (16.8605 iter/s, 5.93102s/100 iters), loss = 0.415745
I1210 21:35:35.344378 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:35:35.344378 11576 solver.cpp:237]     Train net output #1: loss = 0.415746 (* 1 = 0.415746 loss)
I1210 21:35:35.344378 11576 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1210 21:35:41.278790 11576 solver.cpp:218] Iteration 99800 (16.8507 iter/s, 5.93446s/100 iters), loss = 0.482386
I1210 21:35:41.278790 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:35:41.278790 11576 solver.cpp:237]     Train net output #1: loss = 0.482386 (* 1 = 0.482386 loss)
I1210 21:35:41.278790 11576 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1210 21:35:47.215226 11576 solver.cpp:218] Iteration 99900 (16.8469 iter/s, 5.93582s/100 iters), loss = 0.539703
I1210 21:35:47.215226 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 21:35:47.215226 11576 solver.cpp:237]     Train net output #1: loss = 0.539703 (* 1 = 0.539703 loss)
I1210 21:35:47.215226 11576 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1210 21:35:52.849648 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:35:53.083664 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_100000.caffemodel
I1210 21:35:53.098670 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_100000.solverstate
I1210 21:35:53.103670 11576 solver.cpp:330] Iteration 100000, Testing net (#0)
I1210 21:35:53.103670 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:35:54.403767 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:35:54.454767 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6709
I1210 21:35:54.454767 11576 solver.cpp:397]     Test net output #1: loss = 1.20432 (* 1 = 1.20432 loss)
I1210 21:35:54.511770 11576 solver.cpp:218] Iteration 100000 (13.707 iter/s, 7.29554s/100 iters), loss = 0.385589
I1210 21:35:54.511770 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 21:35:54.511770 11576 solver.cpp:237]     Train net output #1: loss = 0.385589 (* 1 = 0.385589 loss)
I1210 21:35:54.511770 11576 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1210 21:36:00.438154 11576 solver.cpp:218] Iteration 100100 (16.8733 iter/s, 5.92654s/100 iters), loss = 0.511338
I1210 21:36:00.438154 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:36:00.438154 11576 solver.cpp:237]     Train net output #1: loss = 0.511339 (* 1 = 0.511339 loss)
I1210 21:36:00.438154 11576 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1210 21:36:06.362592 11576 solver.cpp:218] Iteration 100200 (16.882 iter/s, 5.92346s/100 iters), loss = 0.392981
I1210 21:36:06.362592 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:36:06.362592 11576 solver.cpp:237]     Train net output #1: loss = 0.392982 (* 1 = 0.392982 loss)
I1210 21:36:06.362592 11576 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1210 21:36:12.294602 11576 solver.cpp:218] Iteration 100300 (16.8584 iter/s, 5.93174s/100 iters), loss = 0.502047
I1210 21:36:12.294602 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 21:36:12.294602 11576 solver.cpp:237]     Train net output #1: loss = 0.502047 (* 1 = 0.502047 loss)
I1210 21:36:12.294602 11576 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1210 21:36:18.222661 11576 solver.cpp:218] Iteration 100400 (16.8707 iter/s, 5.92743s/100 iters), loss = 0.51746
I1210 21:36:18.222661 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:36:18.222661 11576 solver.cpp:237]     Train net output #1: loss = 0.51746 (* 1 = 0.51746 loss)
I1210 21:36:18.222661 11576 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1210 21:36:23.859035 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:36:24.094051 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_100500.caffemodel
I1210 21:36:24.108057 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_100500.solverstate
I1210 21:36:24.113055 11576 solver.cpp:330] Iteration 100500, Testing net (#0)
I1210 21:36:24.113055 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:36:25.409145 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:36:25.459144 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6734
I1210 21:36:25.459144 11576 solver.cpp:397]     Test net output #1: loss = 1.20767 (* 1 = 1.20767 loss)
I1210 21:36:25.516150 11576 solver.cpp:218] Iteration 100500 (13.7105 iter/s, 7.29368s/100 iters), loss = 0.470364
I1210 21:36:25.516150 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:36:25.516150 11576 solver.cpp:237]     Train net output #1: loss = 0.470365 (* 1 = 0.470365 loss)
I1210 21:36:25.516150 11576 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1210 21:36:31.445571 11576 solver.cpp:218] Iteration 100600 (16.8664 iter/s, 5.92894s/100 iters), loss = 0.482034
I1210 21:36:31.445571 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:36:31.445571 11576 solver.cpp:237]     Train net output #1: loss = 0.482034 (* 1 = 0.482034 loss)
I1210 21:36:31.445571 11576 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1210 21:36:37.370931 11576 solver.cpp:218] Iteration 100700 (16.8776 iter/s, 5.92502s/100 iters), loss = 0.36104
I1210 21:36:37.370931 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:36:37.370931 11576 solver.cpp:237]     Train net output #1: loss = 0.361041 (* 1 = 0.361041 loss)
I1210 21:36:37.370931 11576 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1210 21:36:43.293428 11576 solver.cpp:218] Iteration 100800 (16.8861 iter/s, 5.92204s/100 iters), loss = 0.439866
I1210 21:36:43.293428 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:36:43.293428 11576 solver.cpp:237]     Train net output #1: loss = 0.439866 (* 1 = 0.439866 loss)
I1210 21:36:43.293428 11576 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1210 21:36:49.219761 11576 solver.cpp:218] Iteration 100900 (16.8766 iter/s, 5.92537s/100 iters), loss = 0.444552
I1210 21:36:49.219761 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:36:49.219761 11576 solver.cpp:237]     Train net output #1: loss = 0.444552 (* 1 = 0.444552 loss)
I1210 21:36:49.219761 11576 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1210 21:36:54.853194 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:36:55.087205 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_101000.caffemodel
I1210 21:36:55.102210 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_101000.solverstate
I1210 21:36:55.107209 11576 solver.cpp:330] Iteration 101000, Testing net (#0)
I1210 21:36:55.107209 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:36:56.403296 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:36:56.454298 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6716
I1210 21:36:56.454298 11576 solver.cpp:397]     Test net output #1: loss = 1.21535 (* 1 = 1.21535 loss)
I1210 21:36:56.510800 11576 solver.cpp:218] Iteration 101000 (13.7163 iter/s, 7.29062s/100 iters), loss = 0.455036
I1210 21:36:56.510800 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:36:56.510800 11576 solver.cpp:237]     Train net output #1: loss = 0.455036 (* 1 = 0.455036 loss)
I1210 21:36:56.510800 11576 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1210 21:37:02.437742 11576 solver.cpp:218] Iteration 101100 (16.8734 iter/s, 5.92648s/100 iters), loss = 0.495484
I1210 21:37:02.437742 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:37:02.437742 11576 solver.cpp:237]     Train net output #1: loss = 0.495484 (* 1 = 0.495484 loss)
I1210 21:37:02.437742 11576 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1210 21:37:08.363174 11576 solver.cpp:218] Iteration 101200 (16.8787 iter/s, 5.92463s/100 iters), loss = 0.433236
I1210 21:37:08.363174 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:37:08.363174 11576 solver.cpp:237]     Train net output #1: loss = 0.433236 (* 1 = 0.433236 loss)
I1210 21:37:08.363174 11576 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1210 21:37:14.291580 11576 solver.cpp:218] Iteration 101300 (16.8687 iter/s, 5.92815s/100 iters), loss = 0.469916
I1210 21:37:14.291580 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:37:14.291580 11576 solver.cpp:237]     Train net output #1: loss = 0.469916 (* 1 = 0.469916 loss)
I1210 21:37:14.291580 11576 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1210 21:37:20.214260 11576 solver.cpp:218] Iteration 101400 (16.8841 iter/s, 5.92272s/100 iters), loss = 0.544737
I1210 21:37:20.214260 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 21:37:20.214260 11576 solver.cpp:237]     Train net output #1: loss = 0.544738 (* 1 = 0.544738 loss)
I1210 21:37:20.214260 11576 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1210 21:37:25.854704 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:37:26.087713 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_101500.caffemodel
I1210 21:37:26.102217 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_101500.solverstate
I1210 21:37:26.107218 11576 solver.cpp:330] Iteration 101500, Testing net (#0)
I1210 21:37:26.107218 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:37:27.406738 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:37:27.457741 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6719
I1210 21:37:27.457741 11576 solver.cpp:397]     Test net output #1: loss = 1.22057 (* 1 = 1.22057 loss)
I1210 21:37:27.514742 11576 solver.cpp:218] Iteration 101500 (13.6986 iter/s, 7.3s/100 iters), loss = 0.399308
I1210 21:37:27.514742 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:37:27.514742 11576 solver.cpp:237]     Train net output #1: loss = 0.399308 (* 1 = 0.399308 loss)
I1210 21:37:27.514742 11576 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1210 21:37:33.440198 11576 solver.cpp:218] Iteration 101600 (16.8791 iter/s, 5.92448s/100 iters), loss = 0.496238
I1210 21:37:33.440198 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:37:33.440198 11576 solver.cpp:237]     Train net output #1: loss = 0.496239 (* 1 = 0.496239 loss)
I1210 21:37:33.440198 11576 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1210 21:37:39.368631 11576 solver.cpp:218] Iteration 101700 (16.8697 iter/s, 5.9278s/100 iters), loss = 0.381202
I1210 21:37:39.368631 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:37:39.368631 11576 solver.cpp:237]     Train net output #1: loss = 0.381202 (* 1 = 0.381202 loss)
I1210 21:37:39.368631 11576 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1210 21:37:45.302117 11576 solver.cpp:218] Iteration 101800 (16.8543 iter/s, 5.93319s/100 iters), loss = 0.497816
I1210 21:37:45.302117 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:37:45.302117 11576 solver.cpp:237]     Train net output #1: loss = 0.497816 (* 1 = 0.497816 loss)
I1210 21:37:45.302117 11576 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1210 21:37:51.225567 11576 solver.cpp:218] Iteration 101900 (16.8823 iter/s, 5.92335s/100 iters), loss = 0.551848
I1210 21:37:51.225567 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 21:37:51.225567 11576 solver.cpp:237]     Train net output #1: loss = 0.551848 (* 1 = 0.551848 loss)
I1210 21:37:51.225567 11576 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1210 21:37:56.862980 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:37:57.097995 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_102000.caffemodel
I1210 21:37:57.112499 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_102000.solverstate
I1210 21:37:57.118016 11576 solver.cpp:330] Iteration 102000, Testing net (#0)
I1210 21:37:57.118016 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:37:58.417155 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:37:58.468159 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6704
I1210 21:37:58.468159 11576 solver.cpp:397]     Test net output #1: loss = 1.2146 (* 1 = 1.2146 loss)
I1210 21:37:58.524163 11576 solver.cpp:218] Iteration 102000 (13.7028 iter/s, 7.29778s/100 iters), loss = 0.397831
I1210 21:37:58.524163 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:37:58.524163 11576 solver.cpp:237]     Train net output #1: loss = 0.397832 (* 1 = 0.397832 loss)
I1210 21:37:58.524163 11576 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1210 21:38:04.457562 11576 solver.cpp:218] Iteration 102100 (16.8534 iter/s, 5.93352s/100 iters), loss = 0.428113
I1210 21:38:04.457562 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:38:04.457562 11576 solver.cpp:237]     Train net output #1: loss = 0.428113 (* 1 = 0.428113 loss)
I1210 21:38:04.457562 11576 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1210 21:38:10.378963 11576 solver.cpp:218] Iteration 102200 (16.891 iter/s, 5.92031s/100 iters), loss = 0.307006
I1210 21:38:10.378963 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:38:10.378963 11576 solver.cpp:237]     Train net output #1: loss = 0.307006 (* 1 = 0.307006 loss)
I1210 21:38:10.378963 11576 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1210 21:38:16.300377 11576 solver.cpp:218] Iteration 102300 (16.8885 iter/s, 5.92119s/100 iters), loss = 0.429604
I1210 21:38:16.300377 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:38:16.300377 11576 solver.cpp:237]     Train net output #1: loss = 0.429604 (* 1 = 0.429604 loss)
I1210 21:38:16.300377 11576 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1210 21:38:22.226841 11576 solver.cpp:218] Iteration 102400 (16.873 iter/s, 5.92663s/100 iters), loss = 0.510737
I1210 21:38:22.226841 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 21:38:22.226841 11576 solver.cpp:237]     Train net output #1: loss = 0.510737 (* 1 = 0.510737 loss)
I1210 21:38:22.226841 11576 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1210 21:38:27.861233 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:38:28.094251 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_102500.caffemodel
I1210 21:38:28.109753 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_102500.solverstate
I1210 21:38:28.114754 11576 solver.cpp:330] Iteration 102500, Testing net (#0)
I1210 21:38:28.114754 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:38:29.415395 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:38:29.466434 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6702
I1210 21:38:29.466434 11576 solver.cpp:397]     Test net output #1: loss = 1.22462 (* 1 = 1.22462 loss)
I1210 21:38:29.521414 11576 solver.cpp:218] Iteration 102500 (13.7102 iter/s, 7.29384s/100 iters), loss = 0.410846
I1210 21:38:29.521414 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:38:29.521414 11576 solver.cpp:237]     Train net output #1: loss = 0.410847 (* 1 = 0.410847 loss)
I1210 21:38:29.521414 11576 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1210 21:38:35.456845 11576 solver.cpp:218] Iteration 102600 (16.8502 iter/s, 5.93466s/100 iters), loss = 0.467131
I1210 21:38:35.456845 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:38:35.456845 11576 solver.cpp:237]     Train net output #1: loss = 0.467131 (* 1 = 0.467131 loss)
I1210 21:38:35.456845 11576 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1210 21:38:41.388244 11576 solver.cpp:218] Iteration 102700 (16.861 iter/s, 5.93085s/100 iters), loss = 0.335822
I1210 21:38:41.388244 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 21:38:41.388244 11576 solver.cpp:237]     Train net output #1: loss = 0.335822 (* 1 = 0.335822 loss)
I1210 21:38:41.388244 11576 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1210 21:38:47.320626 11576 solver.cpp:218] Iteration 102800 (16.858 iter/s, 5.9319s/100 iters), loss = 0.43629
I1210 21:38:47.320626 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:38:47.320626 11576 solver.cpp:237]     Train net output #1: loss = 0.43629 (* 1 = 0.43629 loss)
I1210 21:38:47.320626 11576 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1210 21:38:53.248235 11576 solver.cpp:218] Iteration 102900 (16.8718 iter/s, 5.92706s/100 iters), loss = 0.480179
I1210 21:38:53.248235 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:38:53.248235 11576 solver.cpp:237]     Train net output #1: loss = 0.480179 (* 1 = 0.480179 loss)
I1210 21:38:53.248235 11576 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1210 21:38:58.885622 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:38:59.118651 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_103000.caffemodel
I1210 21:38:59.134650 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_103000.solverstate
I1210 21:38:59.139652 11576 solver.cpp:330] Iteration 103000, Testing net (#0)
I1210 21:38:59.139652 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:39:00.438794 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:39:00.489802 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6677
I1210 21:39:00.489802 11576 solver.cpp:397]     Test net output #1: loss = 1.23275 (* 1 = 1.23275 loss)
I1210 21:39:00.546802 11576 solver.cpp:218] Iteration 103000 (13.7009 iter/s, 7.29879s/100 iters), loss = 0.401505
I1210 21:39:00.546802 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:39:00.546802 11576 solver.cpp:237]     Train net output #1: loss = 0.401505 (* 1 = 0.401505 loss)
I1210 21:39:00.546802 11576 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1210 21:39:06.484246 11576 solver.cpp:218] Iteration 103100 (16.843 iter/s, 5.93717s/100 iters), loss = 0.430625
I1210 21:39:06.485246 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:39:06.485246 11576 solver.cpp:237]     Train net output #1: loss = 0.430625 (* 1 = 0.430625 loss)
I1210 21:39:06.485246 11576 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1210 21:39:12.412703 11576 solver.cpp:218] Iteration 103200 (16.8694 iter/s, 5.92791s/100 iters), loss = 0.400478
I1210 21:39:12.412703 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:39:12.412703 11576 solver.cpp:237]     Train net output #1: loss = 0.400479 (* 1 = 0.400479 loss)
I1210 21:39:12.412703 11576 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1210 21:39:18.333190 11576 solver.cpp:218] Iteration 103300 (16.894 iter/s, 5.91925s/100 iters), loss = 0.495339
I1210 21:39:18.333190 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:39:18.333190 11576 solver.cpp:237]     Train net output #1: loss = 0.495339 (* 1 = 0.495339 loss)
I1210 21:39:18.333190 11576 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1210 21:39:24.260637 11576 solver.cpp:218] Iteration 103400 (16.8703 iter/s, 5.92758s/100 iters), loss = 0.397973
I1210 21:39:24.260637 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:39:24.260637 11576 solver.cpp:237]     Train net output #1: loss = 0.397974 (* 1 = 0.397974 loss)
I1210 21:39:24.260637 11576 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1210 21:39:29.895079 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:39:30.128093 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_103500.caffemodel
I1210 21:39:30.142093 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_103500.solverstate
I1210 21:39:30.148093 11576 solver.cpp:330] Iteration 103500, Testing net (#0)
I1210 21:39:30.148093 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:39:31.445235 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:39:31.496242 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6693
I1210 21:39:31.496242 11576 solver.cpp:397]     Test net output #1: loss = 1.23365 (* 1 = 1.23365 loss)
I1210 21:39:31.552242 11576 solver.cpp:218] Iteration 103500 (13.715 iter/s, 7.29129s/100 iters), loss = 0.413616
I1210 21:39:31.552242 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:39:31.552242 11576 solver.cpp:237]     Train net output #1: loss = 0.413616 (* 1 = 0.413616 loss)
I1210 21:39:31.552242 11576 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1210 21:39:37.476183 11576 solver.cpp:218] Iteration 103600 (16.8829 iter/s, 5.92316s/100 iters), loss = 0.428815
I1210 21:39:37.476183 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:39:37.476685 11576 solver.cpp:237]     Train net output #1: loss = 0.428815 (* 1 = 0.428815 loss)
I1210 21:39:37.476685 11576 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1210 21:39:43.405076 11576 solver.cpp:218] Iteration 103700 (16.8669 iter/s, 5.92879s/100 iters), loss = 0.352507
I1210 21:39:43.405076 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:39:43.405076 11576 solver.cpp:237]     Train net output #1: loss = 0.352507 (* 1 = 0.352507 loss)
I1210 21:39:43.405076 11576 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1210 21:39:49.333528 11576 solver.cpp:218] Iteration 103800 (16.8696 iter/s, 5.92782s/100 iters), loss = 0.505839
I1210 21:39:49.333528 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:39:49.333528 11576 solver.cpp:237]     Train net output #1: loss = 0.505839 (* 1 = 0.505839 loss)
I1210 21:39:49.333528 11576 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1210 21:39:55.256969 11576 solver.cpp:218] Iteration 103900 (16.8823 iter/s, 5.92337s/100 iters), loss = 0.42412
I1210 21:39:55.256969 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:39:55.257971 11576 solver.cpp:237]     Train net output #1: loss = 0.42412 (* 1 = 0.42412 loss)
I1210 21:39:55.257971 11576 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1210 21:40:00.932417 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:40:01.172446 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_104000.caffemodel
I1210 21:40:01.192443 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_104000.solverstate
I1210 21:40:01.197444 11576 solver.cpp:330] Iteration 104000, Testing net (#0)
I1210 21:40:01.197444 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:40:02.519573 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:40:02.570590 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6683
I1210 21:40:02.571591 11576 solver.cpp:397]     Test net output #1: loss = 1.23801 (* 1 = 1.23801 loss)
I1210 21:40:02.629590 11576 solver.cpp:218] Iteration 104000 (13.5653 iter/s, 7.37174s/100 iters), loss = 0.42642
I1210 21:40:02.629590 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:40:02.629590 11576 solver.cpp:237]     Train net output #1: loss = 0.42642 (* 1 = 0.42642 loss)
I1210 21:40:02.629590 11576 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1210 21:40:08.581094 11576 solver.cpp:218] Iteration 104100 (16.8033 iter/s, 5.9512s/100 iters), loss = 0.511865
I1210 21:40:08.581094 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:40:08.581094 11576 solver.cpp:237]     Train net output #1: loss = 0.511866 (* 1 = 0.511866 loss)
I1210 21:40:08.581094 11576 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1210 21:40:14.544564 11576 solver.cpp:218] Iteration 104200 (16.7718 iter/s, 5.96239s/100 iters), loss = 0.411162
I1210 21:40:14.544564 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 21:40:14.544564 11576 solver.cpp:237]     Train net output #1: loss = 0.411162 (* 1 = 0.411162 loss)
I1210 21:40:14.544564 11576 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1210 21:40:20.483203 11576 solver.cpp:218] Iteration 104300 (16.8391 iter/s, 5.93856s/100 iters), loss = 0.432787
I1210 21:40:20.483203 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:40:20.483203 11576 solver.cpp:237]     Train net output #1: loss = 0.432787 (* 1 = 0.432787 loss)
I1210 21:40:20.483203 11576 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1210 21:40:26.500826 11576 solver.cpp:218] Iteration 104400 (16.62 iter/s, 6.01685s/100 iters), loss = 0.403797
I1210 21:40:26.500826 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:40:26.500826 11576 solver.cpp:237]     Train net output #1: loss = 0.403797 (* 1 = 0.403797 loss)
I1210 21:40:26.500826 11576 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1210 21:40:32.158113 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:40:32.394124 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_104500.caffemodel
I1210 21:40:32.411123 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_104500.solverstate
I1210 21:40:32.416124 11576 solver.cpp:330] Iteration 104500, Testing net (#0)
I1210 21:40:32.417124 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:40:33.719255 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:40:33.770272 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6688
I1210 21:40:33.770272 11576 solver.cpp:397]     Test net output #1: loss = 1.23591 (* 1 = 1.23591 loss)
I1210 21:40:33.829259 11576 solver.cpp:218] Iteration 104500 (13.6469 iter/s, 7.32766s/100 iters), loss = 0.437604
I1210 21:40:33.829259 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:40:33.829259 11576 solver.cpp:237]     Train net output #1: loss = 0.437604 (* 1 = 0.437604 loss)
I1210 21:40:33.829259 11576 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1210 21:40:39.765715 11576 solver.cpp:218] Iteration 104600 (16.8452 iter/s, 5.93641s/100 iters), loss = 0.38886
I1210 21:40:39.765715 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 21:40:39.766216 11576 solver.cpp:237]     Train net output #1: loss = 0.38886 (* 1 = 0.38886 loss)
I1210 21:40:39.766216 11576 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1210 21:40:45.777020 11576 solver.cpp:218] Iteration 104700 (16.6353 iter/s, 6.01132s/100 iters), loss = 0.315552
I1210 21:40:45.777020 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:40:45.777020 11576 solver.cpp:237]     Train net output #1: loss = 0.315552 (* 1 = 0.315552 loss)
I1210 21:40:45.777020 11576 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1210 21:40:51.716590 11576 solver.cpp:218] Iteration 104800 (16.8382 iter/s, 5.93888s/100 iters), loss = 0.459793
I1210 21:40:51.716590 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:40:51.716590 11576 solver.cpp:237]     Train net output #1: loss = 0.459793 (* 1 = 0.459793 loss)
I1210 21:40:51.716590 11576 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1210 21:40:57.750362 11576 solver.cpp:218] Iteration 104900 (16.5741 iter/s, 6.03351s/100 iters), loss = 0.42154
I1210 21:40:57.750362 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:40:57.750362 11576 solver.cpp:237]     Train net output #1: loss = 0.42154 (* 1 = 0.42154 loss)
I1210 21:40:57.750362 11576 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1210 21:41:03.525895 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:41:03.767446 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_105000.caffemodel
I1210 21:41:03.786949 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_105000.solverstate
I1210 21:41:03.792951 11576 solver.cpp:330] Iteration 105000, Testing net (#0)
I1210 21:41:03.792951 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:41:05.127370 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:41:05.178891 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6689
I1210 21:41:05.178891 11576 solver.cpp:397]     Test net output #1: loss = 1.24369 (* 1 = 1.24369 loss)
I1210 21:41:05.236887 11576 solver.cpp:218] Iteration 105000 (13.3585 iter/s, 7.48586s/100 iters), loss = 0.36947
I1210 21:41:05.236887 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:41:05.236887 11576 solver.cpp:237]     Train net output #1: loss = 0.369471 (* 1 = 0.369471 loss)
I1210 21:41:05.236887 11576 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1210 21:41:11.310530 11576 solver.cpp:218] Iteration 105100 (16.4677 iter/s, 6.0725s/100 iters), loss = 0.386657
I1210 21:41:11.310530 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:41:11.310530 11576 solver.cpp:237]     Train net output #1: loss = 0.386657 (* 1 = 0.386657 loss)
I1210 21:41:11.310530 11576 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1210 21:41:17.379340 11576 solver.cpp:218] Iteration 105200 (16.4789 iter/s, 6.06836s/100 iters), loss = 0.350324
I1210 21:41:17.379340 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:41:17.379340 11576 solver.cpp:237]     Train net output #1: loss = 0.350324 (* 1 = 0.350324 loss)
I1210 21:41:17.379340 11576 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1210 21:41:23.443296 11576 solver.cpp:218] Iteration 105300 (16.4897 iter/s, 6.0644s/100 iters), loss = 0.447516
I1210 21:41:23.444296 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:41:23.444296 11576 solver.cpp:237]     Train net output #1: loss = 0.447516 (* 1 = 0.447516 loss)
I1210 21:41:23.444296 11576 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1210 21:41:29.445353 11576 solver.cpp:218] Iteration 105400 (16.6635 iter/s, 6.00115s/100 iters), loss = 0.486118
I1210 21:41:29.445353 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:41:29.445353 11576 solver.cpp:237]     Train net output #1: loss = 0.486118 (* 1 = 0.486118 loss)
I1210 21:41:29.445353 11576 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1210 21:41:35.102453 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:41:35.335467 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_105500.caffemodel
I1210 21:41:35.352468 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_105500.solverstate
I1210 21:41:35.356467 11576 solver.cpp:330] Iteration 105500, Testing net (#0)
I1210 21:41:35.357468 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:41:36.659590 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:41:36.710594 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6715
I1210 21:41:36.710594 11576 solver.cpp:397]     Test net output #1: loss = 1.23226 (* 1 = 1.23226 loss)
I1210 21:41:36.767594 11576 solver.cpp:218] Iteration 105500 (13.6585 iter/s, 7.32147s/100 iters), loss = 0.42041
I1210 21:41:36.767594 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:41:36.767594 11576 solver.cpp:237]     Train net output #1: loss = 0.42041 (* 1 = 0.42041 loss)
I1210 21:41:36.767594 11576 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1210 21:41:42.741204 11576 solver.cpp:218] Iteration 105600 (16.7407 iter/s, 5.97347s/100 iters), loss = 0.371822
I1210 21:41:42.741204 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:41:42.741204 11576 solver.cpp:237]     Train net output #1: loss = 0.371823 (* 1 = 0.371823 loss)
I1210 21:41:42.741204 11576 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1210 21:41:48.678691 11576 solver.cpp:218] Iteration 105700 (16.8453 iter/s, 5.93639s/100 iters), loss = 0.335931
I1210 21:41:48.678691 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 21:41:48.678691 11576 solver.cpp:237]     Train net output #1: loss = 0.335931 (* 1 = 0.335931 loss)
I1210 21:41:48.678691 11576 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1210 21:41:54.622272 11576 solver.cpp:218] Iteration 105800 (16.8263 iter/s, 5.94309s/100 iters), loss = 0.453872
I1210 21:41:54.622272 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:41:54.622272 11576 solver.cpp:237]     Train net output #1: loss = 0.453872 (* 1 = 0.453872 loss)
I1210 21:41:54.622272 11576 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1210 21:42:00.563757 11576 solver.cpp:218] Iteration 105900 (16.8311 iter/s, 5.94139s/100 iters), loss = 0.465592
I1210 21:42:00.563757 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 21:42:00.563757 11576 solver.cpp:237]     Train net output #1: loss = 0.465593 (* 1 = 0.465593 loss)
I1210 21:42:00.563757 11576 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1210 21:42:06.205452 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:42:06.438484 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_106000.caffemodel
I1210 21:42:06.453482 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_106000.solverstate
I1210 21:42:06.457484 11576 solver.cpp:330] Iteration 106000, Testing net (#0)
I1210 21:42:06.457484 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:42:07.763115 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:42:07.816136 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6718
I1210 21:42:07.816136 11576 solver.cpp:397]     Test net output #1: loss = 1.24208 (* 1 = 1.24208 loss)
I1210 21:42:07.872133 11576 solver.cpp:218] Iteration 106000 (13.683 iter/s, 7.30831s/100 iters), loss = 0.413528
I1210 21:42:07.872133 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:42:07.872133 11576 solver.cpp:237]     Train net output #1: loss = 0.413528 (* 1 = 0.413528 loss)
I1210 21:42:07.872133 11576 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1210 21:42:13.803930 11576 solver.cpp:218] Iteration 106100 (16.8617 iter/s, 5.93059s/100 iters), loss = 0.400317
I1210 21:42:13.803930 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 21:42:13.803930 11576 solver.cpp:237]     Train net output #1: loss = 0.400317 (* 1 = 0.400317 loss)
I1210 21:42:13.803930 11576 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1210 21:42:19.863476 11576 solver.cpp:218] Iteration 106200 (16.5039 iter/s, 6.05917s/100 iters), loss = 0.322567
I1210 21:42:19.863476 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 21:42:19.863476 11576 solver.cpp:237]     Train net output #1: loss = 0.322567 (* 1 = 0.322567 loss)
I1210 21:42:19.863476 11576 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1210 21:42:25.948060 11576 solver.cpp:218] Iteration 106300 (16.4352 iter/s, 6.0845s/100 iters), loss = 0.441763
I1210 21:42:25.948060 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:42:25.948060 11576 solver.cpp:237]     Train net output #1: loss = 0.441763 (* 1 = 0.441763 loss)
I1210 21:42:25.948060 11576 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1210 21:42:31.995326 11576 solver.cpp:218] Iteration 106400 (16.5389 iter/s, 6.04636s/100 iters), loss = 0.483531
I1210 21:42:31.995326 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 21:42:31.995326 11576 solver.cpp:237]     Train net output #1: loss = 0.483531 (* 1 = 0.483531 loss)
I1210 21:42:31.995326 11576 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1210 21:42:37.729715 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:42:37.968730 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_106500.caffemodel
I1210 21:42:37.983731 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_106500.solverstate
I1210 21:42:37.988731 11576 solver.cpp:330] Iteration 106500, Testing net (#0)
I1210 21:42:37.988731 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:42:39.317873 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:42:39.369874 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6688
I1210 21:42:39.369874 11576 solver.cpp:397]     Test net output #1: loss = 1.24443 (* 1 = 1.24443 loss)
I1210 21:42:39.427880 11576 solver.cpp:218] Iteration 106500 (13.4555 iter/s, 7.4319s/100 iters), loss = 0.363587
I1210 21:42:39.427880 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 21:42:39.427880 11576 solver.cpp:237]     Train net output #1: loss = 0.363588 (* 1 = 0.363588 loss)
I1210 21:42:39.427880 11576 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1210 21:42:45.491586 11576 solver.cpp:218] Iteration 106600 (16.492 iter/s, 6.06353s/100 iters), loss = 0.430269
I1210 21:42:45.491586 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:42:45.491586 11576 solver.cpp:237]     Train net output #1: loss = 0.430269 (* 1 = 0.430269 loss)
I1210 21:42:45.491586 11576 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1210 21:42:51.525825 11576 solver.cpp:218] Iteration 106700 (16.5737 iter/s, 6.03366s/100 iters), loss = 0.304932
I1210 21:42:51.525825 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:42:51.525825 11576 solver.cpp:237]     Train net output #1: loss = 0.304932 (* 1 = 0.304932 loss)
I1210 21:42:51.525825 11576 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1210 21:42:57.552752 11576 solver.cpp:218] Iteration 106800 (16.5927 iter/s, 6.02674s/100 iters), loss = 0.480536
I1210 21:42:57.553752 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:42:57.553752 11576 solver.cpp:237]     Train net output #1: loss = 0.480536 (* 1 = 0.480536 loss)
I1210 21:42:57.553752 11576 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1210 21:43:03.572576 11576 solver.cpp:218] Iteration 106900 (16.6148 iter/s, 6.01874s/100 iters), loss = 0.373236
I1210 21:43:03.572576 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:43:03.572576 11576 solver.cpp:237]     Train net output #1: loss = 0.373236 (* 1 = 0.373236 loss)
I1210 21:43:03.572576 11576 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1210 21:43:09.371206 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:43:09.613243 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_107000.caffemodel
I1210 21:43:09.627256 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_107000.solverstate
I1210 21:43:09.633256 11576 solver.cpp:330] Iteration 107000, Testing net (#0)
I1210 21:43:09.633256 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:43:10.956403 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:43:11.008400 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6695
I1210 21:43:11.008400 11576 solver.cpp:397]     Test net output #1: loss = 1.25932 (* 1 = 1.25932 loss)
I1210 21:43:11.067409 11576 solver.cpp:218] Iteration 107000 (13.3438 iter/s, 7.49412s/100 iters), loss = 0.427492
I1210 21:43:11.067409 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:43:11.067409 11576 solver.cpp:237]     Train net output #1: loss = 0.427493 (* 1 = 0.427493 loss)
I1210 21:43:11.067409 11576 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1210 21:43:17.120584 11576 solver.cpp:218] Iteration 107100 (16.521 iter/s, 6.0529s/100 iters), loss = 0.511904
I1210 21:43:17.120584 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:43:17.120584 11576 solver.cpp:237]     Train net output #1: loss = 0.511904 (* 1 = 0.511904 loss)
I1210 21:43:17.120584 11576 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1210 21:43:23.144620 11576 solver.cpp:218] Iteration 107200 (16.6 iter/s, 6.02409s/100 iters), loss = 0.279163
I1210 21:43:23.144620 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 21:43:23.144620 11576 solver.cpp:237]     Train net output #1: loss = 0.279163 (* 1 = 0.279163 loss)
I1210 21:43:23.145622 11576 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1210 21:43:29.231528 11576 solver.cpp:218] Iteration 107300 (16.4311 iter/s, 6.08602s/100 iters), loss = 0.455975
I1210 21:43:29.231528 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:43:29.231528 11576 solver.cpp:237]     Train net output #1: loss = 0.455975 (* 1 = 0.455975 loss)
I1210 21:43:29.231528 11576 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1210 21:43:35.327162 11576 solver.cpp:218] Iteration 107400 (16.4073 iter/s, 6.09483s/100 iters), loss = 0.465407
I1210 21:43:35.327162 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 21:43:35.327162 11576 solver.cpp:237]     Train net output #1: loss = 0.465407 (* 1 = 0.465407 loss)
I1210 21:43:35.327162 11576 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1210 21:43:41.140050 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:43:41.382097 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_107500.caffemodel
I1210 21:43:41.397116 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_107500.solverstate
I1210 21:43:41.402112 11576 solver.cpp:330] Iteration 107500, Testing net (#0)
I1210 21:43:41.402112 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:43:42.718086 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:43:42.768616 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6694
I1210 21:43:42.768616 11576 solver.cpp:397]     Test net output #1: loss = 1.24958 (* 1 = 1.24958 loss)
I1210 21:43:42.825631 11576 solver.cpp:218] Iteration 107500 (13.3377 iter/s, 7.49755s/100 iters), loss = 0.392854
I1210 21:43:42.825631 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:43:42.825631 11576 solver.cpp:237]     Train net output #1: loss = 0.392854 (* 1 = 0.392854 loss)
I1210 21:43:42.825631 11576 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1210 21:43:48.818122 11576 solver.cpp:218] Iteration 107600 (16.6871 iter/s, 5.99266s/100 iters), loss = 0.460732
I1210 21:43:48.818122 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:43:48.818122 11576 solver.cpp:237]     Train net output #1: loss = 0.460732 (* 1 = 0.460732 loss)
I1210 21:43:48.818122 11576 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1210 21:43:54.824609 11576 solver.cpp:218] Iteration 107700 (16.6499 iter/s, 6.00604s/100 iters), loss = 0.325996
I1210 21:43:54.824609 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 21:43:54.824609 11576 solver.cpp:237]     Train net output #1: loss = 0.325996 (* 1 = 0.325996 loss)
I1210 21:43:54.824609 11576 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1210 21:44:00.816290 11576 solver.cpp:218] Iteration 107800 (16.6922 iter/s, 5.99083s/100 iters), loss = 0.348128
I1210 21:44:00.816290 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 21:44:00.816290 11576 solver.cpp:237]     Train net output #1: loss = 0.348128 (* 1 = 0.348128 loss)
I1210 21:44:00.816290 11576 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1210 21:44:06.842962 11576 solver.cpp:218] Iteration 107900 (16.5944 iter/s, 6.02612s/100 iters), loss = 0.473682
I1210 21:44:06.842962 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:44:06.842962 11576 solver.cpp:237]     Train net output #1: loss = 0.473683 (* 1 = 0.473683 loss)
I1210 21:44:06.842962 11576 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1210 21:44:12.609813 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:44:12.851897 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_108000.caffemodel
I1210 21:44:12.867908 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_108000.solverstate
I1210 21:44:12.872901 11576 solver.cpp:330] Iteration 108000, Testing net (#0)
I1210 21:44:12.872901 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:44:14.199319 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:44:14.251835 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6675
I1210 21:44:14.251835 11576 solver.cpp:397]     Test net output #1: loss = 1.25035 (* 1 = 1.25035 loss)
I1210 21:44:14.310860 11576 solver.cpp:218] Iteration 108000 (13.3909 iter/s, 7.46776s/100 iters), loss = 0.324893
I1210 21:44:14.310860 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 21:44:14.310860 11576 solver.cpp:237]     Train net output #1: loss = 0.324893 (* 1 = 0.324893 loss)
I1210 21:44:14.310860 11576 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1210 21:44:20.413784 11576 solver.cpp:218] Iteration 108100 (16.3884 iter/s, 6.10186s/100 iters), loss = 0.46707
I1210 21:44:20.413784 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:44:20.413784 11576 solver.cpp:237]     Train net output #1: loss = 0.467071 (* 1 = 0.467071 loss)
I1210 21:44:20.413784 11576 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1210 21:44:26.522516 11576 solver.cpp:218] Iteration 108200 (16.37 iter/s, 6.10874s/100 iters), loss = 0.31138
I1210 21:44:26.522516 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 21:44:26.522516 11576 solver.cpp:237]     Train net output #1: loss = 0.311381 (* 1 = 0.311381 loss)
I1210 21:44:26.522516 11576 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1210 21:44:32.620635 11576 solver.cpp:218] Iteration 108300 (16.3995 iter/s, 6.09774s/100 iters), loss = 0.411095
I1210 21:44:32.620635 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:44:32.620635 11576 solver.cpp:237]     Train net output #1: loss = 0.411095 (* 1 = 0.411095 loss)
I1210 21:44:32.620635 11576 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1210 21:44:38.687947 11576 solver.cpp:218] Iteration 108400 (16.4828 iter/s, 6.06694s/100 iters), loss = 0.388031
I1210 21:44:38.687947 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:44:38.687947 11576 solver.cpp:237]     Train net output #1: loss = 0.388032 (* 1 = 0.388032 loss)
I1210 21:44:38.688946 11576 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1210 21:44:44.497290 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:44:44.739332 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_108500.caffemodel
I1210 21:44:44.753337 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_108500.solverstate
I1210 21:44:44.758339 11576 solver.cpp:330] Iteration 108500, Testing net (#0)
I1210 21:44:44.758339 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:44:46.089565 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:44:46.142580 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6661
I1210 21:44:46.142580 11576 solver.cpp:397]     Test net output #1: loss = 1.2643 (* 1 = 1.2643 loss)
I1210 21:44:46.200578 11576 solver.cpp:218] Iteration 108500 (13.3134 iter/s, 7.51124s/100 iters), loss = 0.417345
I1210 21:44:46.200578 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:44:46.200578 11576 solver.cpp:237]     Train net output #1: loss = 0.417345 (* 1 = 0.417345 loss)
I1210 21:44:46.200578 11576 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1210 21:44:52.274839 11576 solver.cpp:218] Iteration 108600 (16.464 iter/s, 6.07388s/100 iters), loss = 0.459305
I1210 21:44:52.274839 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:44:52.274839 11576 solver.cpp:237]     Train net output #1: loss = 0.459305 (* 1 = 0.459305 loss)
I1210 21:44:52.274839 11576 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1210 21:44:58.272400 11576 solver.cpp:218] Iteration 108700 (16.6751 iter/s, 5.99698s/100 iters), loss = 0.315346
I1210 21:44:58.272400 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 21:44:58.272400 11576 solver.cpp:237]     Train net output #1: loss = 0.315346 (* 1 = 0.315346 loss)
I1210 21:44:58.272400 11576 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1210 21:45:04.288316 11576 solver.cpp:218] Iteration 108800 (16.6241 iter/s, 6.01537s/100 iters), loss = 0.461737
I1210 21:45:04.288316 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:45:04.288316 11576 solver.cpp:237]     Train net output #1: loss = 0.461737 (* 1 = 0.461737 loss)
I1210 21:45:04.288316 11576 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1210 21:45:10.338119 11576 solver.cpp:218] Iteration 108900 (16.5292 iter/s, 6.04989s/100 iters), loss = 0.54378
I1210 21:45:10.338119 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 21:45:10.338119 11576 solver.cpp:237]     Train net output #1: loss = 0.543781 (* 1 = 0.543781 loss)
I1210 21:45:10.338119 11576 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1210 21:45:16.046787 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:45:16.282722 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_109000.caffemodel
I1210 21:45:16.297716 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_109000.solverstate
I1210 21:45:16.302711 11576 solver.cpp:330] Iteration 109000, Testing net (#0)
I1210 21:45:16.302711 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:45:17.613224 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:45:17.664223 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6669
I1210 21:45:17.664223 11576 solver.cpp:397]     Test net output #1: loss = 1.2649 (* 1 = 1.2649 loss)
I1210 21:45:17.721984 11576 solver.cpp:218] Iteration 109000 (13.5439 iter/s, 7.38339s/100 iters), loss = 0.39979
I1210 21:45:17.721984 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 21:45:17.721984 11576 solver.cpp:237]     Train net output #1: loss = 0.39979 (* 1 = 0.39979 loss)
I1210 21:45:17.721984 11576 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1210 21:45:23.692164 11576 solver.cpp:218] Iteration 109100 (16.7526 iter/s, 5.96923s/100 iters), loss = 0.483169
I1210 21:45:23.692164 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:45:23.692164 11576 solver.cpp:237]     Train net output #1: loss = 0.483169 (* 1 = 0.483169 loss)
I1210 21:45:23.692164 11576 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1210 21:45:29.663300 11576 solver.cpp:218] Iteration 109200 (16.748 iter/s, 5.97085s/100 iters), loss = 0.370804
I1210 21:45:29.663300 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:45:29.663300 11576 solver.cpp:237]     Train net output #1: loss = 0.370805 (* 1 = 0.370805 loss)
I1210 21:45:29.663300 11576 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1210 21:45:35.735721 11576 solver.cpp:218] Iteration 109300 (16.4684 iter/s, 6.07224s/100 iters), loss = 0.455102
I1210 21:45:35.735721 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:45:35.735721 11576 solver.cpp:237]     Train net output #1: loss = 0.455102 (* 1 = 0.455102 loss)
I1210 21:45:35.735721 11576 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1210 21:45:41.767686 11576 solver.cpp:218] Iteration 109400 (16.5802 iter/s, 6.03128s/100 iters), loss = 0.42134
I1210 21:45:41.767686 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:45:41.768187 11576 solver.cpp:237]     Train net output #1: loss = 0.42134 (* 1 = 0.42134 loss)
I1210 21:45:41.768187 11576 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1210 21:45:47.489356 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:45:47.729027 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_109500.caffemodel
I1210 21:45:47.750027 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_109500.solverstate
I1210 21:45:47.756029 11576 solver.cpp:330] Iteration 109500, Testing net (#0)
I1210 21:45:47.756029 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:45:49.070889 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:45:49.120905 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6658
I1210 21:45:49.120905 11576 solver.cpp:397]     Test net output #1: loss = 1.27484 (* 1 = 1.27484 loss)
I1210 21:45:49.177929 11576 solver.cpp:218] Iteration 109500 (13.4963 iter/s, 7.40945s/100 iters), loss = 0.328862
I1210 21:45:49.177929 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 21:45:49.177929 11576 solver.cpp:237]     Train net output #1: loss = 0.328863 (* 1 = 0.328863 loss)
I1210 21:45:49.177929 11576 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1210 21:45:55.207855 11576 solver.cpp:218] Iteration 109600 (16.5844 iter/s, 6.02976s/100 iters), loss = 0.449923
I1210 21:45:55.207855 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:45:55.207855 11576 solver.cpp:237]     Train net output #1: loss = 0.449923 (* 1 = 0.449923 loss)
I1210 21:45:55.207855 11576 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1210 21:46:01.262010 11576 solver.cpp:218] Iteration 109700 (16.5174 iter/s, 6.05421s/100 iters), loss = 0.318313
I1210 21:46:01.262010 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:46:01.262010 11576 solver.cpp:237]     Train net output #1: loss = 0.318314 (* 1 = 0.318314 loss)
I1210 21:46:01.262010 11576 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1210 21:46:07.315467 11576 solver.cpp:218] Iteration 109800 (16.5213 iter/s, 6.05279s/100 iters), loss = 0.391741
I1210 21:46:07.315467 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:46:07.315467 11576 solver.cpp:237]     Train net output #1: loss = 0.391741 (* 1 = 0.391741 loss)
I1210 21:46:07.315467 11576 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1210 21:46:13.396647 11576 solver.cpp:218] Iteration 109900 (16.4459 iter/s, 6.08053s/100 iters), loss = 0.477566
I1210 21:46:13.396647 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:46:13.396647 11576 solver.cpp:237]     Train net output #1: loss = 0.477566 (* 1 = 0.477566 loss)
I1210 21:46:13.396647 11576 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1210 21:46:19.134119 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:46:19.374130 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_110000.caffemodel
I1210 21:46:19.388634 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_110000.solverstate
I1210 21:46:19.393134 11576 solver.cpp:330] Iteration 110000, Testing net (#0)
I1210 21:46:19.393134 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:46:20.723249 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:46:20.775249 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6698
I1210 21:46:20.775249 11576 solver.cpp:397]     Test net output #1: loss = 1.26677 (* 1 = 1.26677 loss)
I1210 21:46:20.833266 11576 solver.cpp:218] Iteration 110000 (13.4484 iter/s, 7.43582s/100 iters), loss = 0.311038
I1210 21:46:20.833266 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 21:46:20.833266 11576 solver.cpp:237]     Train net output #1: loss = 0.311038 (* 1 = 0.311038 loss)
I1210 21:46:20.833266 11576 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1210 21:46:26.907770 11576 solver.cpp:218] Iteration 110100 (16.4631 iter/s, 6.0742s/100 iters), loss = 0.374004
I1210 21:46:26.907770 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:46:26.907770 11576 solver.cpp:237]     Train net output #1: loss = 0.374004 (* 1 = 0.374004 loss)
I1210 21:46:26.907770 11576 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1210 21:46:32.995784 11576 solver.cpp:218] Iteration 110200 (16.4276 iter/s, 6.08731s/100 iters), loss = 0.357243
I1210 21:46:32.995784 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:46:32.995784 11576 solver.cpp:237]     Train net output #1: loss = 0.357243 (* 1 = 0.357243 loss)
I1210 21:46:32.995784 11576 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1210 21:46:39.072500 11576 solver.cpp:218] Iteration 110300 (16.4577 iter/s, 6.0762s/100 iters), loss = 0.483857
I1210 21:46:39.072500 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:46:39.072500 11576 solver.cpp:237]     Train net output #1: loss = 0.483857 (* 1 = 0.483857 loss)
I1210 21:46:39.072500 11576 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1210 21:46:45.096915 11576 solver.cpp:218] Iteration 110400 (16.6002 iter/s, 6.02402s/100 iters), loss = 0.42926
I1210 21:46:45.097404 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:46:45.097404 11576 solver.cpp:237]     Train net output #1: loss = 0.42926 (* 1 = 0.42926 loss)
I1210 21:46:45.097404 11576 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1210 21:46:50.821691 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:46:51.051695 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_110500.caffemodel
I1210 21:46:51.067694 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_110500.solverstate
I1210 21:46:51.072695 11576 solver.cpp:330] Iteration 110500, Testing net (#0)
I1210 21:46:51.072695 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:46:52.385834 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:46:52.437841 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6677
I1210 21:46:52.437841 11576 solver.cpp:397]     Test net output #1: loss = 1.26884 (* 1 = 1.26884 loss)
I1210 21:46:52.496345 11576 solver.cpp:218] Iteration 110500 (13.5154 iter/s, 7.39895s/100 iters), loss = 0.322619
I1210 21:46:52.496845 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 21:46:52.496845 11576 solver.cpp:237]     Train net output #1: loss = 0.32262 (* 1 = 0.32262 loss)
I1210 21:46:52.496845 11576 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1210 21:46:58.516074 11576 solver.cpp:218] Iteration 110600 (16.6123 iter/s, 6.01964s/100 iters), loss = 0.43227
I1210 21:46:58.516074 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:46:58.516074 11576 solver.cpp:237]     Train net output #1: loss = 0.43227 (* 1 = 0.43227 loss)
I1210 21:46:58.516074 11576 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1210 21:47:04.520236 11576 solver.cpp:218] Iteration 110700 (16.6565 iter/s, 6.00368s/100 iters), loss = 0.363213
I1210 21:47:04.520236 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:47:04.520236 11576 solver.cpp:237]     Train net output #1: loss = 0.363213 (* 1 = 0.363213 loss)
I1210 21:47:04.520236 11576 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1210 21:47:10.587604 11576 solver.cpp:218] Iteration 110800 (16.4827 iter/s, 6.06696s/100 iters), loss = 0.392715
I1210 21:47:10.588603 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 21:47:10.588603 11576 solver.cpp:237]     Train net output #1: loss = 0.392715 (* 1 = 0.392715 loss)
I1210 21:47:10.588603 11576 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1210 21:47:16.632556 11576 solver.cpp:218] Iteration 110900 (16.5449 iter/s, 6.04415s/100 iters), loss = 0.416117
I1210 21:47:16.632556 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:47:16.632556 11576 solver.cpp:237]     Train net output #1: loss = 0.416117 (* 1 = 0.416117 loss)
I1210 21:47:16.632556 11576 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1210 21:47:22.424515 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:47:22.661031 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_111000.caffemodel
I1210 21:47:22.677031 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_111000.solverstate
I1210 21:47:22.682031 11576 solver.cpp:330] Iteration 111000, Testing net (#0)
I1210 21:47:22.683032 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:47:24.012646 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:47:24.065156 11576 solver.cpp:397]     Test net output #0: accuracy = 0.669
I1210 21:47:24.065156 11576 solver.cpp:397]     Test net output #1: loss = 1.26077 (* 1 = 1.26077 loss)
I1210 21:47:24.123658 11576 solver.cpp:218] Iteration 111000 (13.3509 iter/s, 7.49015s/100 iters), loss = 0.42274
I1210 21:47:24.123658 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:47:24.123658 11576 solver.cpp:237]     Train net output #1: loss = 0.42274 (* 1 = 0.42274 loss)
I1210 21:47:24.123658 11576 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1210 21:47:30.210685 11576 solver.cpp:218] Iteration 111100 (16.4293 iter/s, 6.0867s/100 iters), loss = 0.370618
I1210 21:47:30.210685 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:47:30.210685 11576 solver.cpp:237]     Train net output #1: loss = 0.370618 (* 1 = 0.370618 loss)
I1210 21:47:30.210685 11576 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1210 21:47:36.248200 11576 solver.cpp:218] Iteration 111200 (16.5645 iter/s, 6.03702s/100 iters), loss = 0.261674
I1210 21:47:36.248200 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 21:47:36.248200 11576 solver.cpp:237]     Train net output #1: loss = 0.261674 (* 1 = 0.261674 loss)
I1210 21:47:36.248200 11576 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1210 21:47:42.324532 11576 solver.cpp:218] Iteration 111300 (16.459 iter/s, 6.07569s/100 iters), loss = 0.299627
I1210 21:47:42.324532 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 21:47:42.324532 11576 solver.cpp:237]     Train net output #1: loss = 0.299628 (* 1 = 0.299628 loss)
I1210 21:47:42.324532 11576 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1210 21:47:48.408180 11576 solver.cpp:218] Iteration 111400 (16.4371 iter/s, 6.08378s/100 iters), loss = 0.402457
I1210 21:47:48.408180 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:47:48.408180 11576 solver.cpp:237]     Train net output #1: loss = 0.402457 (* 1 = 0.402457 loss)
I1210 21:47:48.408180 11576 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1210 21:47:54.127668 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:47:54.362687 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_111500.caffemodel
I1210 21:47:54.376688 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_111500.solverstate
I1210 21:47:54.381687 11576 solver.cpp:330] Iteration 111500, Testing net (#0)
I1210 21:47:54.381687 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:47:55.708776 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:47:55.760781 11576 solver.cpp:397]     Test net output #0: accuracy = 0.664
I1210 21:47:55.760781 11576 solver.cpp:397]     Test net output #1: loss = 1.28119 (* 1 = 1.28119 loss)
I1210 21:47:55.819286 11576 solver.cpp:218] Iteration 111500 (13.4952 iter/s, 7.41003s/100 iters), loss = 0.318031
I1210 21:47:55.819286 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 21:47:55.819286 11576 solver.cpp:237]     Train net output #1: loss = 0.318031 (* 1 = 0.318031 loss)
I1210 21:47:55.819286 11576 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1210 21:48:01.826242 11576 solver.cpp:218] Iteration 111600 (16.6496 iter/s, 6.00617s/100 iters), loss = 0.324019
I1210 21:48:01.826242 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 21:48:01.826242 11576 solver.cpp:237]     Train net output #1: loss = 0.32402 (* 1 = 0.32402 loss)
I1210 21:48:01.826242 11576 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1210 21:48:07.924329 11576 solver.cpp:218] Iteration 111700 (16.3995 iter/s, 6.09773s/100 iters), loss = 0.319871
I1210 21:48:07.924329 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 21:48:07.924329 11576 solver.cpp:237]     Train net output #1: loss = 0.319872 (* 1 = 0.319872 loss)
I1210 21:48:07.924329 11576 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1210 21:48:13.932595 11576 solver.cpp:218] Iteration 111800 (16.6454 iter/s, 6.00766s/100 iters), loss = 0.3912
I1210 21:48:13.932595 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:48:13.932595 11576 solver.cpp:237]     Train net output #1: loss = 0.3912 (* 1 = 0.3912 loss)
I1210 21:48:13.932595 11576 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1210 21:48:20.007454 11576 solver.cpp:218] Iteration 111900 (16.4606 iter/s, 6.07513s/100 iters), loss = 0.410104
I1210 21:48:20.007454 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:48:20.007454 11576 solver.cpp:237]     Train net output #1: loss = 0.410105 (* 1 = 0.410105 loss)
I1210 21:48:20.007454 11576 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1210 21:48:25.738759 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:48:25.978271 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_112000.caffemodel
I1210 21:48:25.994261 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_112000.solverstate
I1210 21:48:25.999274 11576 solver.cpp:330] Iteration 112000, Testing net (#0)
I1210 21:48:25.999274 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:48:27.327384 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:48:27.377882 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6706
I1210 21:48:27.377882 11576 solver.cpp:397]     Test net output #1: loss = 1.2708 (* 1 = 1.2708 loss)
I1210 21:48:27.434870 11576 solver.cpp:218] Iteration 112000 (13.4658 iter/s, 7.42622s/100 iters), loss = 0.381005
I1210 21:48:27.434870 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:48:27.434870 11576 solver.cpp:237]     Train net output #1: loss = 0.381005 (* 1 = 0.381005 loss)
I1210 21:48:27.434870 11576 sgd_solver.cpp:105] Iteration 112000, lr = 0.001
I1210 21:48:33.451560 11576 solver.cpp:218] Iteration 112100 (16.6216 iter/s, 6.01628s/100 iters), loss = 0.393023
I1210 21:48:33.451560 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:48:33.451560 11576 solver.cpp:237]     Train net output #1: loss = 0.393023 (* 1 = 0.393023 loss)
I1210 21:48:33.451560 11576 sgd_solver.cpp:105] Iteration 112100, lr = 0.001
I1210 21:48:39.508436 11576 solver.cpp:218] Iteration 112200 (16.5109 iter/s, 6.05661s/100 iters), loss = 0.358018
I1210 21:48:39.508436 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 21:48:39.508436 11576 solver.cpp:237]     Train net output #1: loss = 0.358018 (* 1 = 0.358018 loss)
I1210 21:48:39.508436 11576 sgd_solver.cpp:105] Iteration 112200, lr = 0.001
I1210 21:48:45.502799 11576 solver.cpp:218] Iteration 112300 (16.6848 iter/s, 5.99349s/100 iters), loss = 0.427729
I1210 21:48:45.502799 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:48:45.502799 11576 solver.cpp:237]     Train net output #1: loss = 0.427729 (* 1 = 0.427729 loss)
I1210 21:48:45.502799 11576 sgd_solver.cpp:105] Iteration 112300, lr = 0.001
I1210 21:48:51.543519 11576 solver.cpp:218] Iteration 112400 (16.5555 iter/s, 6.04029s/100 iters), loss = 0.467183
I1210 21:48:51.543519 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 21:48:51.543519 11576 solver.cpp:237]     Train net output #1: loss = 0.467183 (* 1 = 0.467183 loss)
I1210 21:48:51.543519 11576 sgd_solver.cpp:105] Iteration 112400, lr = 0.001
I1210 21:48:57.344426 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:48:57.581491 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_112500.caffemodel
I1210 21:48:57.598476 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_112500.solverstate
I1210 21:48:57.604477 11576 solver.cpp:330] Iteration 112500, Testing net (#0)
I1210 21:48:57.604477 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:48:58.910706 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:48:58.962955 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6693
I1210 21:48:58.962955 11576 solver.cpp:397]     Test net output #1: loss = 1.27746 (* 1 = 1.27746 loss)
I1210 21:48:59.020946 11576 solver.cpp:218] Iteration 112500 (13.3733 iter/s, 7.47761s/100 iters), loss = 0.393007
I1210 21:48:59.020946 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:48:59.021944 11576 solver.cpp:237]     Train net output #1: loss = 0.393007 (* 1 = 0.393007 loss)
I1210 21:48:59.021944 11576 sgd_solver.cpp:105] Iteration 112500, lr = 0.001
I1210 21:49:05.117395 11576 solver.cpp:218] Iteration 112600 (16.4057 iter/s, 6.09545s/100 iters), loss = 0.459579
I1210 21:49:05.117395 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:49:05.118377 11576 solver.cpp:237]     Train net output #1: loss = 0.459579 (* 1 = 0.459579 loss)
I1210 21:49:05.118377 11576 sgd_solver.cpp:105] Iteration 112600, lr = 0.001
I1210 21:49:11.123772 11576 solver.cpp:218] Iteration 112700 (16.6506 iter/s, 6.00578s/100 iters), loss = 0.368085
I1210 21:49:11.123772 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 21:49:11.123772 11576 solver.cpp:237]     Train net output #1: loss = 0.368086 (* 1 = 0.368086 loss)
I1210 21:49:11.123772 11576 sgd_solver.cpp:105] Iteration 112700, lr = 0.001
I1210 21:49:17.133632 11576 solver.cpp:218] Iteration 112800 (16.6405 iter/s, 6.00943s/100 iters), loss = 0.404799
I1210 21:49:17.133632 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:49:17.133632 11576 solver.cpp:237]     Train net output #1: loss = 0.404799 (* 1 = 0.404799 loss)
I1210 21:49:17.133632 11576 sgd_solver.cpp:105] Iteration 112800, lr = 0.001
I1210 21:49:23.229816 11576 solver.cpp:218] Iteration 112900 (16.4059 iter/s, 6.09536s/100 iters), loss = 0.421086
I1210 21:49:23.229816 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:49:23.229816 11576 solver.cpp:237]     Train net output #1: loss = 0.421086 (* 1 = 0.421086 loss)
I1210 21:49:23.229816 11576 sgd_solver.cpp:105] Iteration 112900, lr = 0.001
I1210 21:49:28.969350 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:49:29.203369 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_113000.caffemodel
I1210 21:49:29.219370 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_113000.solverstate
I1210 21:49:29.224370 11576 solver.cpp:330] Iteration 113000, Testing net (#0)
I1210 21:49:29.224370 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:49:30.525470 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:49:30.576474 11576 solver.cpp:397]     Test net output #0: accuracy = 0.6668
I1210 21:49:30.576474 11576 solver.cpp:397]     Test net output #1: loss = 1.27493 (* 1 = 1.27493 loss)
I1210 21:49:30.633473 11576 solver.cpp:218] Iteration 113000 (13.509 iter/s, 7.40249s/100 iters), loss = 0.333656
I1210 21:49:30.633473 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 21:49:30.633473 11576 solver.cpp:237]     Train net output #1: loss = 0.333657 (* 1 = 0.333657 loss)
I1210 21:49:30.633473 11576 sgd_solver.cpp:105] Iteration 113000, lr = 0.001
I1210 21:49:36.583978 11576 solver.cpp:218] Iteration 113100 (16.8055 iter/s, 5.95044s/100 iters), loss = 0.518959
I1210 21:49:36.583978 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 21:49:36.583978 11576 solver.cpp:237]     Train net output #1: loss = 0.518959 (* 1 = 0.518959 loss)
I1210 21:49:36.583978 11576 sgd_solver.cpp:105] Iteration 113100, lr = 0.001
I1210 21:49:42.666154 11576 solver.cpp:218] Iteration 113200 (16.4432 iter/s, 6.08155s/100 iters), loss = 0.269673
I1210 21:49:42.666154 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 21:49:42.666154 11576 solver.cpp:237]     Train net output #1: loss = 0.269673 (* 1 = 0.269673 loss)
I1210 21:49:42.666154 11576 sgd_solver.cpp:105] Iteration 113200, lr = 0.001
I1210 21:49:48.698456 11576 solver.cpp:218] Iteration 113300 (16.5787 iter/s, 6.03184s/100 iters), loss = 0.381697
I1210 21:49:48.698456 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 21:49:48.698456 11576 solver.cpp:237]     Train net output #1: loss = 0.381697 (* 1 = 0.381697 loss)
I1210 21:49:48.698456 11576 sgd_solver.cpp:105] Iteration 113300, lr = 0.001
I1210 21:49:54.664957 11576 solver.cpp:218] Iteration 113400 (16.7622 iter/s, 5.9658s/100 iters), loss = 0.435755
I1210 21:49:54.664957 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 21:49:54.664957 11576 solver.cpp:237]     Train net output #1: loss = 0.435756 (* 1 = 0.435756 loss)
I1210 21:49:54.664957 11576 sgd_solver.cpp:105] Iteration 113400, lr = 0.001
I1210 21:50:00.383426 14632 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:50:00.623457 11576 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_113500.caffemodel
I1210 21:50:00.639461 11576 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_iter_113500.solverstate
I1210 21:50:00.644459 11576 solver.cpp:330] Iteration 113500, Testing net (#0)
I1210 21:50:00.644459 11576 net.cpp:676] Ignoring source layer accuracy_training
I1210 21:50:01.978286 21104 data_layer.cpp:73] Restarting data prefetching from start.
I1210 21:50:02.030294 11576 solver.cpp:397]     Test net output #0: accuracy = 0.666
I1210 21:50:02.030294 11576 solver.cpp:397]     Test net output #1: loss = 1.27833 (* 1 = 1.27833 loss)
I1210 21:50:02.087802 11576 solver.cpp:218] Iteration 113500 (13.4714 iter/s, 7.42312s/100 iters), loss = 0.338501
I1210 21:50:02.088804 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 21:50:02.088804 11576 solver.cpp:237]     Train net output #1: loss = 0.338501 (* 1 = 0.338501 loss)
I1210 21:50:02.088804 11576 sgd_solver.cpp:105] Iteration 113500, lr = 0.001
I1210 21:50:08.264050 11576 solver.cpp:218] Iteration 113600 (16.1933 iter/s, 6.1754s/100 iters), loss = 0.39406
I1210 21:50:08.264050 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 21:50:08.264050 11576 solver.cpp:237]     Train net output #1: loss = 0.394061 (* 1 = 0.394061 loss)
I1210 21:50:08.264050 11576 sgd_solver.cpp:105] Iteration 113600, lr = 0.001
I1210 21:50:14.265907 11576 solver.cpp:218] Iteration 113700 (16.6643 iter/s, 6.00084s/100 iters), loss = 0.236057
I1210 21:50:14.265907 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1210 21:50:14.265907 11576 solver.cpp:237]     Train net output #1: loss = 0.236057 (* 1 = 0.236057 loss)
I1210 21:50:14.265907 11576 sgd_solver.cpp:105] Iteration 113700, lr = 0.001
I1210 21:50:20.320513 11576 solver.cpp:218] Iteration 113800 (16.5157 iter/s, 6.05485s/100 iters), loss = 0.411731
I1210 21:50:20.320513 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:50:20.320513 11576 solver.cpp:237]     Train net output #1: loss = 0.411731 (* 1 = 0.411731 loss)
I1210 21:50:20.320513 11576 sgd_solver.cpp:105] Iteration 113800, lr = 0.001
I1210 21:50:26.283200 11576 solver.cpp:218] Iteration 113900 (16.7734 iter/s, 5.96182s/100 iters), loss = 0.303077
I1210 21:50:26.283200 11576 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 21:50:26.283200 11576 solver.cpp:237]     Train net output #1: loss = 0.303077 (*
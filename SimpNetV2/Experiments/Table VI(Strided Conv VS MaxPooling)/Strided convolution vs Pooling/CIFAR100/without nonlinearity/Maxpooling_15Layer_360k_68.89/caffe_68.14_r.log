
G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90000.solverstate 
I1210 14:45:17.969856 15352 caffe.cpp:219] Using GPUs 0
I1210 14:45:18.154434 15352 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1210 14:45:18.461109 15352 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 14:45:18.477110 15352 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1210 14:45:18.478117 15352 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 14:45:18.479110 15352 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 14:45:18.479110 15352 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1210 14:45:18.479110 15352 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1210 14:45:18.479620 15352 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_v2_360k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1210 14:45:18.480120 15352 layer_factory.cpp:58] Creating layer cifar
I1210 14:45:18.483134 15352 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1210 14:45:18.483633 15352 net.cpp:84] Creating Layer cifar
I1210 14:45:18.483633 15352 net.cpp:380] cifar -> data
I1210 14:45:18.483633 15352 net.cpp:380] cifar -> label
I1210 14:45:18.484131 15352 data_layer.cpp:45] output data size: 100,3,32,32
I1210 14:45:18.489616 15352 net.cpp:122] Setting up cifar
I1210 14:45:18.489616 15352 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1210 14:45:18.489616 15352 net.cpp:129] Top shape: 100 (100)
I1210 14:45:18.489616 15352 net.cpp:137] Memory required for data: 1229200
I1210 14:45:18.489616 15352 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1210 14:45:18.489616 15352 net.cpp:84] Creating Layer label_cifar_1_split
I1210 14:45:18.489616 15352 net.cpp:406] label_cifar_1_split <- label
I1210 14:45:18.489616 15352 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1210 14:45:18.490128 15352 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1210 14:45:18.490128 15352 net.cpp:122] Setting up label_cifar_1_split
I1210 14:45:18.490128 15352 net.cpp:129] Top shape: 100 (100)
I1210 14:45:18.490128 15352 net.cpp:129] Top shape: 100 (100)
I1210 14:45:18.490128 15352 net.cpp:137] Memory required for data: 1230000
I1210 14:45:18.490128 15352 layer_factory.cpp:58] Creating layer conv1
I1210 14:45:18.490128 15352 net.cpp:84] Creating Layer conv1
I1210 14:45:18.490128 15352 net.cpp:406] conv1 <- data
I1210 14:45:18.490128 15352 net.cpp:380] conv1 -> conv1
I1210 14:45:18.491117 18100 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 14:45:18.748726 15352 net.cpp:122] Setting up conv1
I1210 14:45:18.748726 15352 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 14:45:18.748726 15352 net.cpp:137] Memory required for data: 13518000
I1210 14:45:18.748726 15352 layer_factory.cpp:58] Creating layer bn1
I1210 14:45:18.748726 15352 net.cpp:84] Creating Layer bn1
I1210 14:45:18.748726 15352 net.cpp:406] bn1 <- conv1
I1210 14:45:18.748726 15352 net.cpp:367] bn1 -> conv1 (in-place)
I1210 14:45:18.748726 15352 net.cpp:122] Setting up bn1
I1210 14:45:18.748726 15352 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 14:45:18.748726 15352 net.cpp:137] Memory required for data: 25806000
I1210 14:45:18.748726 15352 layer_factory.cpp:58] Creating layer scale1
I1210 14:45:18.748726 15352 net.cpp:84] Creating Layer scale1
I1210 14:45:18.748726 15352 net.cpp:406] scale1 <- conv1
I1210 14:45:18.748726 15352 net.cpp:367] scale1 -> conv1 (in-place)
I1210 14:45:18.748726 15352 layer_factory.cpp:58] Creating layer scale1
I1210 14:45:18.748726 15352 net.cpp:122] Setting up scale1
I1210 14:45:18.748726 15352 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 14:45:18.748726 15352 net.cpp:137] Memory required for data: 38094000
I1210 14:45:18.748726 15352 layer_factory.cpp:58] Creating layer relu1
I1210 14:45:18.748726 15352 net.cpp:84] Creating Layer relu1
I1210 14:45:18.748726 15352 net.cpp:406] relu1 <- conv1
I1210 14:45:18.748726 15352 net.cpp:367] relu1 -> conv1 (in-place)
I1210 14:45:18.748726 15352 net.cpp:122] Setting up relu1
I1210 14:45:18.748726 15352 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 14:45:18.748726 15352 net.cpp:137] Memory required for data: 50382000
I1210 14:45:18.748726 15352 layer_factory.cpp:58] Creating layer conv1_0
I1210 14:45:18.748726 15352 net.cpp:84] Creating Layer conv1_0
I1210 14:45:18.748726 15352 net.cpp:406] conv1_0 <- conv1
I1210 14:45:18.748726 15352 net.cpp:380] conv1_0 -> conv1_0
I1210 14:45:18.750725 15352 net.cpp:122] Setting up conv1_0
I1210 14:45:18.750725 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.750725 15352 net.cpp:137] Memory required for data: 66766000
I1210 14:45:18.750725 15352 layer_factory.cpp:58] Creating layer bn1_0
I1210 14:45:18.750725 15352 net.cpp:84] Creating Layer bn1_0
I1210 14:45:18.750725 15352 net.cpp:406] bn1_0 <- conv1_0
I1210 14:45:18.750725 15352 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1210 14:45:18.750725 15352 net.cpp:122] Setting up bn1_0
I1210 14:45:18.750725 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.750725 15352 net.cpp:137] Memory required for data: 83150000
I1210 14:45:18.750725 15352 layer_factory.cpp:58] Creating layer scale1_0
I1210 14:45:18.750725 15352 net.cpp:84] Creating Layer scale1_0
I1210 14:45:18.750725 15352 net.cpp:406] scale1_0 <- conv1_0
I1210 14:45:18.750725 15352 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1210 14:45:18.750725 15352 layer_factory.cpp:58] Creating layer scale1_0
I1210 14:45:18.750725 15352 net.cpp:122] Setting up scale1_0
I1210 14:45:18.750725 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.750725 15352 net.cpp:137] Memory required for data: 99534000
I1210 14:45:18.750725 15352 layer_factory.cpp:58] Creating layer relu1_0
I1210 14:45:18.750725 15352 net.cpp:84] Creating Layer relu1_0
I1210 14:45:18.750725 15352 net.cpp:406] relu1_0 <- conv1_0
I1210 14:45:18.751720 15352 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1210 14:45:18.751720 15352 net.cpp:122] Setting up relu1_0
I1210 14:45:18.751720 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.751720 15352 net.cpp:137] Memory required for data: 115918000
I1210 14:45:18.751720 15352 layer_factory.cpp:58] Creating layer conv2
I1210 14:45:18.751720 15352 net.cpp:84] Creating Layer conv2
I1210 14:45:18.751720 15352 net.cpp:406] conv2 <- conv1_0
I1210 14:45:18.751720 15352 net.cpp:380] conv2 -> conv2
I1210 14:45:18.752718 15352 net.cpp:122] Setting up conv2
I1210 14:45:18.752718 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.752718 15352 net.cpp:137] Memory required for data: 132302000
I1210 14:45:18.752718 15352 layer_factory.cpp:58] Creating layer bn2
I1210 14:45:18.752718 15352 net.cpp:84] Creating Layer bn2
I1210 14:45:18.752718 15352 net.cpp:406] bn2 <- conv2
I1210 14:45:18.752718 15352 net.cpp:367] bn2 -> conv2 (in-place)
I1210 14:45:18.752718 15352 net.cpp:122] Setting up bn2
I1210 14:45:18.752718 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.752718 15352 net.cpp:137] Memory required for data: 148686000
I1210 14:45:18.752718 15352 layer_factory.cpp:58] Creating layer scale2
I1210 14:45:18.752718 15352 net.cpp:84] Creating Layer scale2
I1210 14:45:18.752718 15352 net.cpp:406] scale2 <- conv2
I1210 14:45:18.752718 15352 net.cpp:367] scale2 -> conv2 (in-place)
I1210 14:45:18.752718 15352 layer_factory.cpp:58] Creating layer scale2
I1210 14:45:18.752718 15352 net.cpp:122] Setting up scale2
I1210 14:45:18.752718 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.752718 15352 net.cpp:137] Memory required for data: 165070000
I1210 14:45:18.752718 15352 layer_factory.cpp:58] Creating layer relu2
I1210 14:45:18.752718 15352 net.cpp:84] Creating Layer relu2
I1210 14:45:18.752718 15352 net.cpp:406] relu2 <- conv2
I1210 14:45:18.752718 15352 net.cpp:367] relu2 -> conv2 (in-place)
I1210 14:45:18.752718 15352 net.cpp:122] Setting up relu2
I1210 14:45:18.752718 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.752718 15352 net.cpp:137] Memory required for data: 181454000
I1210 14:45:18.752718 15352 layer_factory.cpp:58] Creating layer conv2_1
I1210 14:45:18.752718 15352 net.cpp:84] Creating Layer conv2_1
I1210 14:45:18.752718 15352 net.cpp:406] conv2_1 <- conv2
I1210 14:45:18.752718 15352 net.cpp:380] conv2_1 -> conv2_1
I1210 14:45:18.754704 15352 net.cpp:122] Setting up conv2_1
I1210 14:45:18.754704 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.754704 15352 net.cpp:137] Memory required for data: 197838000
I1210 14:45:18.754704 15352 layer_factory.cpp:58] Creating layer bn2_1
I1210 14:45:18.754704 15352 net.cpp:84] Creating Layer bn2_1
I1210 14:45:18.754704 15352 net.cpp:406] bn2_1 <- conv2_1
I1210 14:45:18.754704 15352 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1210 14:45:18.754704 15352 net.cpp:122] Setting up bn2_1
I1210 14:45:18.754704 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.754704 15352 net.cpp:137] Memory required for data: 214222000
I1210 14:45:18.754704 15352 layer_factory.cpp:58] Creating layer scale2_1
I1210 14:45:18.754704 15352 net.cpp:84] Creating Layer scale2_1
I1210 14:45:18.754704 15352 net.cpp:406] scale2_1 <- conv2_1
I1210 14:45:18.754704 15352 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1210 14:45:18.754704 15352 layer_factory.cpp:58] Creating layer scale2_1
I1210 14:45:18.754704 15352 net.cpp:122] Setting up scale2_1
I1210 14:45:18.754704 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.754704 15352 net.cpp:137] Memory required for data: 230606000
I1210 14:45:18.754704 15352 layer_factory.cpp:58] Creating layer relu2_1
I1210 14:45:18.754704 15352 net.cpp:84] Creating Layer relu2_1
I1210 14:45:18.754704 15352 net.cpp:406] relu2_1 <- conv2_1
I1210 14:45:18.754704 15352 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1210 14:45:18.754704 15352 net.cpp:122] Setting up relu2_1
I1210 14:45:18.754704 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.754704 15352 net.cpp:137] Memory required for data: 246990000
I1210 14:45:18.754704 15352 layer_factory.cpp:58] Creating layer conv2_2
I1210 14:45:18.754704 15352 net.cpp:84] Creating Layer conv2_2
I1210 14:45:18.754704 15352 net.cpp:406] conv2_2 <- conv2_1
I1210 14:45:18.754704 15352 net.cpp:380] conv2_2 -> conv2_2
I1210 14:45:18.756721 15352 net.cpp:122] Setting up conv2_2
I1210 14:45:18.756721 15352 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 14:45:18.756721 15352 net.cpp:137] Memory required for data: 267470000
I1210 14:45:18.756721 15352 layer_factory.cpp:58] Creating layer bn2_2
I1210 14:45:18.756721 15352 net.cpp:84] Creating Layer bn2_2
I1210 14:45:18.756721 15352 net.cpp:406] bn2_2 <- conv2_2
I1210 14:45:18.756721 15352 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1210 14:45:18.757704 15352 net.cpp:122] Setting up bn2_2
I1210 14:45:18.757704 15352 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 14:45:18.757704 15352 net.cpp:137] Memory required for data: 287950000
I1210 14:45:18.757704 15352 layer_factory.cpp:58] Creating layer scale2_2
I1210 14:45:18.757704 15352 net.cpp:84] Creating Layer scale2_2
I1210 14:45:18.757704 15352 net.cpp:406] scale2_2 <- conv2_2
I1210 14:45:18.757704 15352 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1210 14:45:18.757704 15352 layer_factory.cpp:58] Creating layer scale2_2
I1210 14:45:18.757704 15352 net.cpp:122] Setting up scale2_2
I1210 14:45:18.757704 15352 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 14:45:18.757704 15352 net.cpp:137] Memory required for data: 308430000
I1210 14:45:18.757704 15352 layer_factory.cpp:58] Creating layer relu2_2
I1210 14:45:18.757704 15352 net.cpp:84] Creating Layer relu2_2
I1210 14:45:18.757704 15352 net.cpp:406] relu2_2 <- conv2_2
I1210 14:45:18.757704 15352 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1210 14:45:18.757704 15352 net.cpp:122] Setting up relu2_2
I1210 14:45:18.757704 15352 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 14:45:18.757704 15352 net.cpp:137] Memory required for data: 328910000
I1210 14:45:18.757704 15352 layer_factory.cpp:58] Creating layer newconv_added1
I1210 14:45:18.757704 15352 net.cpp:84] Creating Layer newconv_added1
I1210 14:45:18.757704 15352 net.cpp:406] newconv_added1 <- conv2_2
I1210 14:45:18.757704 15352 net.cpp:380] newconv_added1 -> newconv_added1
I1210 14:45:18.758705 15352 net.cpp:122] Setting up newconv_added1
I1210 14:45:18.758705 15352 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 14:45:18.758705 15352 net.cpp:137] Memory required for data: 349390000
I1210 14:45:18.758705 15352 layer_factory.cpp:58] Creating layer pool2_1
I1210 14:45:18.758705 15352 net.cpp:84] Creating Layer pool2_1
I1210 14:45:18.758705 15352 net.cpp:406] pool2_1 <- newconv_added1
I1210 14:45:18.758705 15352 net.cpp:380] pool2_1 -> pool2_1
I1210 14:45:18.759706 15352 net.cpp:122] Setting up pool2_1
I1210 14:45:18.759706 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.759706 15352 net.cpp:137] Memory required for data: 354510000
I1210 14:45:18.759706 15352 layer_factory.cpp:58] Creating layer conv3
I1210 14:45:18.759706 15352 net.cpp:84] Creating Layer conv3
I1210 14:45:18.759706 15352 net.cpp:406] conv3 <- pool2_1
I1210 14:45:18.759706 15352 net.cpp:380] conv3 -> conv3
I1210 14:45:18.760705 15352 net.cpp:122] Setting up conv3
I1210 14:45:18.760705 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.760705 15352 net.cpp:137] Memory required for data: 359630000
I1210 14:45:18.760705 15352 layer_factory.cpp:58] Creating layer bn3
I1210 14:45:18.760705 15352 net.cpp:84] Creating Layer bn3
I1210 14:45:18.760705 15352 net.cpp:406] bn3 <- conv3
I1210 14:45:18.760705 15352 net.cpp:367] bn3 -> conv3 (in-place)
I1210 14:45:18.760705 15352 net.cpp:122] Setting up bn3
I1210 14:45:18.760705 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.760705 15352 net.cpp:137] Memory required for data: 364750000
I1210 14:45:18.760705 15352 layer_factory.cpp:58] Creating layer scale3
I1210 14:45:18.760705 15352 net.cpp:84] Creating Layer scale3
I1210 14:45:18.760705 15352 net.cpp:406] scale3 <- conv3
I1210 14:45:18.760705 15352 net.cpp:367] scale3 -> conv3 (in-place)
I1210 14:45:18.760705 15352 layer_factory.cpp:58] Creating layer scale3
I1210 14:45:18.760705 15352 net.cpp:122] Setting up scale3
I1210 14:45:18.760705 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.760705 15352 net.cpp:137] Memory required for data: 369870000
I1210 14:45:18.760705 15352 layer_factory.cpp:58] Creating layer relu3
I1210 14:45:18.760705 15352 net.cpp:84] Creating Layer relu3
I1210 14:45:18.760705 15352 net.cpp:406] relu3 <- conv3
I1210 14:45:18.760705 15352 net.cpp:367] relu3 -> conv3 (in-place)
I1210 14:45:18.761720 15352 net.cpp:122] Setting up relu3
I1210 14:45:18.761720 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.761720 15352 net.cpp:137] Memory required for data: 374990000
I1210 14:45:18.761720 15352 layer_factory.cpp:58] Creating layer conv3_1
I1210 14:45:18.761720 15352 net.cpp:84] Creating Layer conv3_1
I1210 14:45:18.761720 15352 net.cpp:406] conv3_1 <- conv3
I1210 14:45:18.761720 15352 net.cpp:380] conv3_1 -> conv3_1
I1210 14:45:18.762719 15352 net.cpp:122] Setting up conv3_1
I1210 14:45:18.762719 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.762719 15352 net.cpp:137] Memory required for data: 380110000
I1210 14:45:18.762719 15352 layer_factory.cpp:58] Creating layer bn3_1
I1210 14:45:18.762719 15352 net.cpp:84] Creating Layer bn3_1
I1210 14:45:18.762719 15352 net.cpp:406] bn3_1 <- conv3_1
I1210 14:45:18.762719 15352 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1210 14:45:18.762719 15352 net.cpp:122] Setting up bn3_1
I1210 14:45:18.762719 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.762719 15352 net.cpp:137] Memory required for data: 385230000
I1210 14:45:18.762719 15352 layer_factory.cpp:58] Creating layer scale3_1
I1210 14:45:18.762719 15352 net.cpp:84] Creating Layer scale3_1
I1210 14:45:18.762719 15352 net.cpp:406] scale3_1 <- conv3_1
I1210 14:45:18.762719 15352 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1210 14:45:18.762719 15352 layer_factory.cpp:58] Creating layer scale3_1
I1210 14:45:18.762719 15352 net.cpp:122] Setting up scale3_1
I1210 14:45:18.762719 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.762719 15352 net.cpp:137] Memory required for data: 390350000
I1210 14:45:18.762719 15352 layer_factory.cpp:58] Creating layer relu3_1
I1210 14:45:18.762719 15352 net.cpp:84] Creating Layer relu3_1
I1210 14:45:18.762719 15352 net.cpp:406] relu3_1 <- conv3_1
I1210 14:45:18.762719 15352 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1210 14:45:18.763720 15352 net.cpp:122] Setting up relu3_1
I1210 14:45:18.763720 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.763720 15352 net.cpp:137] Memory required for data: 395470000
I1210 14:45:18.763720 15352 layer_factory.cpp:58] Creating layer conv4
I1210 14:45:18.763720 15352 net.cpp:84] Creating Layer conv4
I1210 14:45:18.763720 15352 net.cpp:406] conv4 <- conv3_1
I1210 14:45:18.763720 15352 net.cpp:380] conv4 -> conv4
I1210 14:45:18.764725 15352 net.cpp:122] Setting up conv4
I1210 14:45:18.764725 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.764725 15352 net.cpp:137] Memory required for data: 400590000
I1210 14:45:18.764725 15352 layer_factory.cpp:58] Creating layer bn4
I1210 14:45:18.764725 15352 net.cpp:84] Creating Layer bn4
I1210 14:45:18.764725 15352 net.cpp:406] bn4 <- conv4
I1210 14:45:18.764725 15352 net.cpp:367] bn4 -> conv4 (in-place)
I1210 14:45:18.764725 15352 net.cpp:122] Setting up bn4
I1210 14:45:18.764725 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.764725 15352 net.cpp:137] Memory required for data: 405710000
I1210 14:45:18.764725 15352 layer_factory.cpp:58] Creating layer scale4
I1210 14:45:18.764725 15352 net.cpp:84] Creating Layer scale4
I1210 14:45:18.764725 15352 net.cpp:406] scale4 <- conv4
I1210 14:45:18.764725 15352 net.cpp:367] scale4 -> conv4 (in-place)
I1210 14:45:18.764725 15352 layer_factory.cpp:58] Creating layer scale4
I1210 14:45:18.764725 15352 net.cpp:122] Setting up scale4
I1210 14:45:18.764725 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.764725 15352 net.cpp:137] Memory required for data: 410830000
I1210 14:45:18.764725 15352 layer_factory.cpp:58] Creating layer relu4
I1210 14:45:18.764725 15352 net.cpp:84] Creating Layer relu4
I1210 14:45:18.764725 15352 net.cpp:406] relu4 <- conv4
I1210 14:45:18.764725 15352 net.cpp:367] relu4 -> conv4 (in-place)
I1210 14:45:18.764725 15352 net.cpp:122] Setting up relu4
I1210 14:45:18.765720 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.765720 15352 net.cpp:137] Memory required for data: 415950000
I1210 14:45:18.765720 15352 layer_factory.cpp:58] Creating layer conv4_1
I1210 14:45:18.765720 15352 net.cpp:84] Creating Layer conv4_1
I1210 14:45:18.765720 15352 net.cpp:406] conv4_1 <- conv4
I1210 14:45:18.765720 15352 net.cpp:380] conv4_1 -> conv4_1
I1210 14:45:18.766721 15352 net.cpp:122] Setting up conv4_1
I1210 14:45:18.766721 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.766721 15352 net.cpp:137] Memory required for data: 421070000
I1210 14:45:18.766721 15352 layer_factory.cpp:58] Creating layer bn4_1
I1210 14:45:18.766721 15352 net.cpp:84] Creating Layer bn4_1
I1210 14:45:18.766721 15352 net.cpp:406] bn4_1 <- conv4_1
I1210 14:45:18.766721 15352 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1210 14:45:18.766721 15352 net.cpp:122] Setting up bn4_1
I1210 14:45:18.766721 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.766721 15352 net.cpp:137] Memory required for data: 426190000
I1210 14:45:18.766721 15352 layer_factory.cpp:58] Creating layer scale4_1
I1210 14:45:18.766721 15352 net.cpp:84] Creating Layer scale4_1
I1210 14:45:18.766721 15352 net.cpp:406] scale4_1 <- conv4_1
I1210 14:45:18.766721 15352 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1210 14:45:18.766721 15352 layer_factory.cpp:58] Creating layer scale4_1
I1210 14:45:18.766721 15352 net.cpp:122] Setting up scale4_1
I1210 14:45:18.766721 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.766721 15352 net.cpp:137] Memory required for data: 431310000
I1210 14:45:18.766721 15352 layer_factory.cpp:58] Creating layer relu4_1
I1210 14:45:18.766721 15352 net.cpp:84] Creating Layer relu4_1
I1210 14:45:18.766721 15352 net.cpp:406] relu4_1 <- conv4_1
I1210 14:45:18.766721 15352 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1210 14:45:18.767721 15352 net.cpp:122] Setting up relu4_1
I1210 14:45:18.767721 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.767721 15352 net.cpp:137] Memory required for data: 436430000
I1210 14:45:18.767721 15352 layer_factory.cpp:58] Creating layer conv4_2
I1210 14:45:18.767721 15352 net.cpp:84] Creating Layer conv4_2
I1210 14:45:18.767721 15352 net.cpp:406] conv4_2 <- conv4_1
I1210 14:45:18.767721 15352 net.cpp:380] conv4_2 -> conv4_2
I1210 14:45:18.768720 15352 net.cpp:122] Setting up conv4_2
I1210 14:45:18.768720 15352 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 14:45:18.768720 15352 net.cpp:137] Memory required for data: 442369200
I1210 14:45:18.768720 15352 layer_factory.cpp:58] Creating layer bn4_2
I1210 14:45:18.768720 15352 net.cpp:84] Creating Layer bn4_2
I1210 14:45:18.768720 15352 net.cpp:406] bn4_2 <- conv4_2
I1210 14:45:18.768720 15352 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1210 14:45:18.768720 15352 net.cpp:122] Setting up bn4_2
I1210 14:45:18.768720 15352 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 14:45:18.768720 15352 net.cpp:137] Memory required for data: 448308400
I1210 14:45:18.768720 15352 layer_factory.cpp:58] Creating layer scale4_2
I1210 14:45:18.768720 15352 net.cpp:84] Creating Layer scale4_2
I1210 14:45:18.768720 15352 net.cpp:406] scale4_2 <- conv4_2
I1210 14:45:18.768720 15352 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1210 14:45:18.768720 15352 layer_factory.cpp:58] Creating layer scale4_2
I1210 14:45:18.768720 15352 net.cpp:122] Setting up scale4_2
I1210 14:45:18.768720 15352 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 14:45:18.768720 15352 net.cpp:137] Memory required for data: 454247600
I1210 14:45:18.768720 15352 layer_factory.cpp:58] Creating layer relu4_2
I1210 14:45:18.768720 15352 net.cpp:84] Creating Layer relu4_2
I1210 14:45:18.768720 15352 net.cpp:406] relu4_2 <- conv4_2
I1210 14:45:18.768720 15352 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1210 14:45:18.769721 15352 net.cpp:122] Setting up relu4_2
I1210 14:45:18.769721 15352 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 14:45:18.769721 15352 net.cpp:137] Memory required for data: 460186800
I1210 14:45:18.769721 15352 layer_factory.cpp:58] Creating layer added_new_conv2
I1210 14:45:18.769721 15352 net.cpp:84] Creating Layer added_new_conv2
I1210 14:45:18.769721 15352 net.cpp:406] added_new_conv2 <- conv4_2
I1210 14:45:18.769721 15352 net.cpp:380] added_new_conv2 -> added_new_conv2
I1210 14:45:18.770720 15352 net.cpp:122] Setting up added_new_conv2
I1210 14:45:18.770720 15352 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 14:45:18.770720 15352 net.cpp:137] Memory required for data: 466126000
I1210 14:45:18.770720 15352 layer_factory.cpp:58] Creating layer pool4_2
I1210 14:45:18.770720 15352 net.cpp:84] Creating Layer pool4_2
I1210 14:45:18.770720 15352 net.cpp:406] pool4_2 <- added_new_conv2
I1210 14:45:18.770720 15352 net.cpp:380] pool4_2 -> pool4_2
I1210 14:45:18.770720 15352 net.cpp:122] Setting up pool4_2
I1210 14:45:18.770720 15352 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 14:45:18.770720 15352 net.cpp:137] Memory required for data: 467610800
I1210 14:45:18.770720 15352 layer_factory.cpp:58] Creating layer conv4_0
I1210 14:45:18.770720 15352 net.cpp:84] Creating Layer conv4_0
I1210 14:45:18.770720 15352 net.cpp:406] conv4_0 <- pool4_2
I1210 14:45:18.770720 15352 net.cpp:380] conv4_0 -> conv4_0
I1210 14:45:18.772717 15352 net.cpp:122] Setting up conv4_0
I1210 14:45:18.772717 15352 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 14:45:18.772717 15352 net.cpp:137] Memory required for data: 469095600
I1210 14:45:18.772717 15352 layer_factory.cpp:58] Creating layer bn4_0
I1210 14:45:18.772717 15352 net.cpp:84] Creating Layer bn4_0
I1210 14:45:18.772717 15352 net.cpp:406] bn4_0 <- conv4_0
I1210 14:45:18.772717 15352 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1210 14:45:18.772717 15352 net.cpp:122] Setting up bn4_0
I1210 14:45:18.772717 15352 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 14:45:18.772717 15352 net.cpp:137] Memory required for data: 470580400
I1210 14:45:18.772717 15352 layer_factory.cpp:58] Creating layer scale4_0
I1210 14:45:18.772717 15352 net.cpp:84] Creating Layer scale4_0
I1210 14:45:18.772717 15352 net.cpp:406] scale4_0 <- conv4_0
I1210 14:45:18.772717 15352 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1210 14:45:18.772717 15352 layer_factory.cpp:58] Creating layer scale4_0
I1210 14:45:18.772717 15352 net.cpp:122] Setting up scale4_0
I1210 14:45:18.772717 15352 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 14:45:18.772717 15352 net.cpp:137] Memory required for data: 472065200
I1210 14:45:18.772717 15352 layer_factory.cpp:58] Creating layer relu4_0
I1210 14:45:18.772717 15352 net.cpp:84] Creating Layer relu4_0
I1210 14:45:18.772717 15352 net.cpp:406] relu4_0 <- conv4_0
I1210 14:45:18.772717 15352 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1210 14:45:18.773717 15352 net.cpp:122] Setting up relu4_0
I1210 14:45:18.773717 15352 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 14:45:18.773717 15352 net.cpp:137] Memory required for data: 473550000
I1210 14:45:18.773717 15352 layer_factory.cpp:58] Creating layer conv11
I1210 14:45:18.773717 15352 net.cpp:84] Creating Layer conv11
I1210 14:45:18.773717 15352 net.cpp:406] conv11 <- conv4_0
I1210 14:45:18.773717 15352 net.cpp:380] conv11 -> conv11
I1210 14:45:18.774716 15352 net.cpp:122] Setting up conv11
I1210 14:45:18.774716 15352 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 14:45:18.774716 15352 net.cpp:137] Memory required for data: 475342000
I1210 14:45:18.774716 15352 layer_factory.cpp:58] Creating layer bn_conv11
I1210 14:45:18.774716 15352 net.cpp:84] Creating Layer bn_conv11
I1210 14:45:18.774716 15352 net.cpp:406] bn_conv11 <- conv11
I1210 14:45:18.774716 15352 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1210 14:45:18.774716 15352 net.cpp:122] Setting up bn_conv11
I1210 14:45:18.775717 15352 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 14:45:18.775717 15352 net.cpp:137] Memory required for data: 477134000
I1210 14:45:18.775717 15352 layer_factory.cpp:58] Creating layer scale_conv11
I1210 14:45:18.775717 15352 net.cpp:84] Creating Layer scale_conv11
I1210 14:45:18.775717 15352 net.cpp:406] scale_conv11 <- conv11
I1210 14:45:18.775717 15352 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1210 14:45:18.775717 15352 layer_factory.cpp:58] Creating layer scale_conv11
I1210 14:45:18.775717 15352 net.cpp:122] Setting up scale_conv11
I1210 14:45:18.775717 15352 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 14:45:18.775717 15352 net.cpp:137] Memory required for data: 478926000
I1210 14:45:18.775717 15352 layer_factory.cpp:58] Creating layer relu_conv11
I1210 14:45:18.775717 15352 net.cpp:84] Creating Layer relu_conv11
I1210 14:45:18.775717 15352 net.cpp:406] relu_conv11 <- conv11
I1210 14:45:18.775717 15352 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1210 14:45:18.775717 15352 net.cpp:122] Setting up relu_conv11
I1210 14:45:18.775717 15352 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 14:45:18.775717 15352 net.cpp:137] Memory required for data: 480718000
I1210 14:45:18.775717 15352 layer_factory.cpp:58] Creating layer conv12
I1210 14:45:18.775717 15352 net.cpp:84] Creating Layer conv12
I1210 14:45:18.775717 15352 net.cpp:406] conv12 <- conv11
I1210 14:45:18.775717 15352 net.cpp:380] conv12 -> conv12
I1210 14:45:18.777724 15352 net.cpp:122] Setting up conv12
I1210 14:45:18.777724 15352 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 14:45:18.777724 15352 net.cpp:137] Memory required for data: 483022000
I1210 14:45:18.777724 15352 layer_factory.cpp:58] Creating layer bn_conv12
I1210 14:45:18.777724 15352 net.cpp:84] Creating Layer bn_conv12
I1210 14:45:18.777724 15352 net.cpp:406] bn_conv12 <- conv12
I1210 14:45:18.777724 15352 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1210 14:45:18.777724 15352 net.cpp:122] Setting up bn_conv12
I1210 14:45:18.777724 15352 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 14:45:18.777724 15352 net.cpp:137] Memory required for data: 485326000
I1210 14:45:18.777724 15352 layer_factory.cpp:58] Creating layer scale_conv12
I1210 14:45:18.777724 15352 net.cpp:84] Creating Layer scale_conv12
I1210 14:45:18.777724 15352 net.cpp:406] scale_conv12 <- conv12
I1210 14:45:18.777724 15352 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1210 14:45:18.777724 15352 layer_factory.cpp:58] Creating layer scale_conv12
I1210 14:45:18.777724 15352 net.cpp:122] Setting up scale_conv12
I1210 14:45:18.777724 15352 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 14:45:18.777724 15352 net.cpp:137] Memory required for data: 487630000
I1210 14:45:18.777724 15352 layer_factory.cpp:58] Creating layer relu_conv12
I1210 14:45:18.777724 15352 net.cpp:84] Creating Layer relu_conv12
I1210 14:45:18.777724 15352 net.cpp:406] relu_conv12 <- conv12
I1210 14:45:18.777724 15352 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1210 14:45:18.778717 15352 net.cpp:122] Setting up relu_conv12
I1210 14:45:18.778717 15352 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 14:45:18.778717 15352 net.cpp:137] Memory required for data: 489934000
I1210 14:45:18.778717 15352 layer_factory.cpp:58] Creating layer poolcp6
I1210 14:45:18.778717 15352 net.cpp:84] Creating Layer poolcp6
I1210 14:45:18.778717 15352 net.cpp:406] poolcp6 <- conv12
I1210 14:45:18.778717 15352 net.cpp:380] poolcp6 -> poolcp6
I1210 14:45:18.778717 15352 net.cpp:122] Setting up poolcp6
I1210 14:45:18.778717 15352 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1210 14:45:18.778717 15352 net.cpp:137] Memory required for data: 489970000
I1210 14:45:18.778717 15352 layer_factory.cpp:58] Creating layer ip1
I1210 14:45:18.778717 15352 net.cpp:84] Creating Layer ip1
I1210 14:45:18.778717 15352 net.cpp:406] ip1 <- poolcp6
I1210 14:45:18.778717 15352 net.cpp:380] ip1 -> ip1
I1210 14:45:18.778717 15352 net.cpp:122] Setting up ip1
I1210 14:45:18.778717 15352 net.cpp:129] Top shape: 100 100 (10000)
I1210 14:45:18.778717 15352 net.cpp:137] Memory required for data: 490010000
I1210 14:45:18.778717 15352 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1210 14:45:18.778717 15352 net.cpp:84] Creating Layer ip1_ip1_0_split
I1210 14:45:18.778717 15352 net.cpp:406] ip1_ip1_0_split <- ip1
I1210 14:45:18.778717 15352 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1210 14:45:18.778717 15352 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1210 14:45:18.779230 15352 net.cpp:122] Setting up ip1_ip1_0_split
I1210 14:45:18.779230 15352 net.cpp:129] Top shape: 100 100 (10000)
I1210 14:45:18.779230 15352 net.cpp:129] Top shape: 100 100 (10000)
I1210 14:45:18.779230 15352 net.cpp:137] Memory required for data: 490090000
I1210 14:45:18.779230 15352 layer_factory.cpp:58] Creating layer accuracy_training
I1210 14:45:18.779230 15352 net.cpp:84] Creating Layer accuracy_training
I1210 14:45:18.779230 15352 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1210 14:45:18.779230 15352 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1210 14:45:18.779230 15352 net.cpp:380] accuracy_training -> accuracy_training
I1210 14:45:18.779230 15352 net.cpp:122] Setting up accuracy_training
I1210 14:45:18.779230 15352 net.cpp:129] Top shape: (1)
I1210 14:45:18.779230 15352 net.cpp:137] Memory required for data: 490090004
I1210 14:45:18.779230 15352 layer_factory.cpp:58] Creating layer loss
I1210 14:45:18.779230 15352 net.cpp:84] Creating Layer loss
I1210 14:45:18.779230 15352 net.cpp:406] loss <- ip1_ip1_0_split_1
I1210 14:45:18.779230 15352 net.cpp:406] loss <- label_cifar_1_split_1
I1210 14:45:18.779230 15352 net.cpp:380] loss -> loss
I1210 14:45:18.779230 15352 layer_factory.cpp:58] Creating layer loss
I1210 14:45:18.779732 15352 net.cpp:122] Setting up loss
I1210 14:45:18.779732 15352 net.cpp:129] Top shape: (1)
I1210 14:45:18.779732 15352 net.cpp:132]     with loss weight 1
I1210 14:45:18.779732 15352 net.cpp:137] Memory required for data: 490090008
I1210 14:45:18.779732 15352 net.cpp:198] loss needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:200] accuracy_training does not need backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] ip1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] poolcp6 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] relu_conv12 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] scale_conv12 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] bn_conv12 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] conv12 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] relu_conv11 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] scale_conv11 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] bn_conv11 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] conv11 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] relu4_0 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] scale4_0 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] bn4_0 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] conv4_0 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] pool4_2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] added_new_conv2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] relu4_2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] scale4_2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] bn4_2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] conv4_2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] relu4_1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] scale4_1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] bn4_1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] conv4_1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] relu4 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] scale4 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] bn4 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] conv4 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] relu3_1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] scale3_1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] bn3_1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] conv3_1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] relu3 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] scale3 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] bn3 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] conv3 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] pool2_1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] newconv_added1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] relu2_2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] scale2_2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] bn2_2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] conv2_2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] relu2_1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] scale2_1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] bn2_1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] conv2_1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] relu2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] scale2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] bn2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] conv2 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] relu1_0 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] scale1_0 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] bn1_0 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] conv1_0 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] relu1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] scale1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] bn1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:198] conv1 needs backward computation.
I1210 14:45:18.779732 15352 net.cpp:200] label_cifar_1_split does not need backward computation.
I1210 14:45:18.780230 15352 net.cpp:200] cifar does not need backward computation.
I1210 14:45:18.780230 15352 net.cpp:242] This network produces output accuracy_training
I1210 14:45:18.780230 15352 net.cpp:242] This network produces output loss
I1210 14:45:18.780230 15352 net.cpp:255] Network initialization done.
I1210 14:45:18.780732 15352 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 14:45:18.780732 15352 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 14:45:18.780732 15352 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 14:45:18.780732 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1210 14:45:18.780732 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1210 14:45:18.780732 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1210 14:45:18.780732 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1210 14:45:18.780732 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1210 14:45:18.781222 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1210 14:45:18.781222 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1210 14:45:18.781222 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1210 14:45:18.781222 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1210 14:45:18.781222 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1210 14:45:18.781222 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1210 14:45:18.781222 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1210 14:45:18.781222 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1210 14:45:18.781222 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1210 14:45:18.781222 15352 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1210 14:45:18.781222 15352 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_v2_360k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1210 14:45:18.781723 15352 layer_factory.cpp:58] Creating layer cifar
I1210 14:45:18.786221 15352 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1210 14:45:18.786721 15352 net.cpp:84] Creating Layer cifar
I1210 14:45:18.786721 15352 net.cpp:380] cifar -> data
I1210 14:45:18.786721 15352 net.cpp:380] cifar -> label
I1210 14:45:18.786721 15352 data_layer.cpp:45] output data size: 100,3,32,32
I1210 14:45:18.793221 15352 net.cpp:122] Setting up cifar
I1210 14:45:18.793221 15352 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1210 14:45:18.793221 15352 net.cpp:129] Top shape: 100 (100)
I1210 14:45:18.793221 15352 net.cpp:137] Memory required for data: 1229200
I1210 14:45:18.793221 15352 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1210 14:45:18.793221 15352 net.cpp:84] Creating Layer label_cifar_1_split
I1210 14:45:18.793722 15352 net.cpp:406] label_cifar_1_split <- label
I1210 14:45:18.793722 15352 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1210 14:45:18.793722 15352 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1210 14:45:18.793722 15352 net.cpp:122] Setting up label_cifar_1_split
I1210 14:45:18.793722 15352 net.cpp:129] Top shape: 100 (100)
I1210 14:45:18.793722 15352 net.cpp:129] Top shape: 100 (100)
I1210 14:45:18.793722 15352 net.cpp:137] Memory required for data: 1230000
I1210 14:45:18.793722 15352 layer_factory.cpp:58] Creating layer conv1
I1210 14:45:18.793722 15352 net.cpp:84] Creating Layer conv1
I1210 14:45:18.793722 15352 net.cpp:406] conv1 <- data
I1210 14:45:18.793722 15352 net.cpp:380] conv1 -> conv1
I1210 14:45:18.794236 11996 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 14:45:18.795636 15352 net.cpp:122] Setting up conv1
I1210 14:45:18.795636 15352 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 14:45:18.795636 15352 net.cpp:137] Memory required for data: 13518000
I1210 14:45:18.795636 15352 layer_factory.cpp:58] Creating layer bn1
I1210 14:45:18.795636 15352 net.cpp:84] Creating Layer bn1
I1210 14:45:18.795636 15352 net.cpp:406] bn1 <- conv1
I1210 14:45:18.795636 15352 net.cpp:367] bn1 -> conv1 (in-place)
I1210 14:45:18.795636 15352 net.cpp:122] Setting up bn1
I1210 14:45:18.795636 15352 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 14:45:18.795636 15352 net.cpp:137] Memory required for data: 25806000
I1210 14:45:18.795636 15352 layer_factory.cpp:58] Creating layer scale1
I1210 14:45:18.795636 15352 net.cpp:84] Creating Layer scale1
I1210 14:45:18.795636 15352 net.cpp:406] scale1 <- conv1
I1210 14:45:18.795636 15352 net.cpp:367] scale1 -> conv1 (in-place)
I1210 14:45:18.795636 15352 layer_factory.cpp:58] Creating layer scale1
I1210 14:45:18.795636 15352 net.cpp:122] Setting up scale1
I1210 14:45:18.795636 15352 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 14:45:18.795636 15352 net.cpp:137] Memory required for data: 38094000
I1210 14:45:18.795636 15352 layer_factory.cpp:58] Creating layer relu1
I1210 14:45:18.795636 15352 net.cpp:84] Creating Layer relu1
I1210 14:45:18.795636 15352 net.cpp:406] relu1 <- conv1
I1210 14:45:18.795636 15352 net.cpp:367] relu1 -> conv1 (in-place)
I1210 14:45:18.795636 15352 net.cpp:122] Setting up relu1
I1210 14:45:18.795636 15352 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 14:45:18.795636 15352 net.cpp:137] Memory required for data: 50382000
I1210 14:45:18.795636 15352 layer_factory.cpp:58] Creating layer conv1_0
I1210 14:45:18.795636 15352 net.cpp:84] Creating Layer conv1_0
I1210 14:45:18.795636 15352 net.cpp:406] conv1_0 <- conv1
I1210 14:45:18.795636 15352 net.cpp:380] conv1_0 -> conv1_0
I1210 14:45:18.797621 15352 net.cpp:122] Setting up conv1_0
I1210 14:45:18.797621 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.797621 15352 net.cpp:137] Memory required for data: 66766000
I1210 14:45:18.797621 15352 layer_factory.cpp:58] Creating layer bn1_0
I1210 14:45:18.797621 15352 net.cpp:84] Creating Layer bn1_0
I1210 14:45:18.797621 15352 net.cpp:406] bn1_0 <- conv1_0
I1210 14:45:18.797621 15352 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1210 14:45:18.797621 15352 net.cpp:122] Setting up bn1_0
I1210 14:45:18.797621 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.797621 15352 net.cpp:137] Memory required for data: 83150000
I1210 14:45:18.797621 15352 layer_factory.cpp:58] Creating layer scale1_0
I1210 14:45:18.797621 15352 net.cpp:84] Creating Layer scale1_0
I1210 14:45:18.797621 15352 net.cpp:406] scale1_0 <- conv1_0
I1210 14:45:18.797621 15352 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1210 14:45:18.797621 15352 layer_factory.cpp:58] Creating layer scale1_0
I1210 14:45:18.797621 15352 net.cpp:122] Setting up scale1_0
I1210 14:45:18.797621 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.797621 15352 net.cpp:137] Memory required for data: 99534000
I1210 14:45:18.797621 15352 layer_factory.cpp:58] Creating layer relu1_0
I1210 14:45:18.797621 15352 net.cpp:84] Creating Layer relu1_0
I1210 14:45:18.797621 15352 net.cpp:406] relu1_0 <- conv1_0
I1210 14:45:18.797621 15352 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1210 14:45:18.798621 15352 net.cpp:122] Setting up relu1_0
I1210 14:45:18.798621 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.798621 15352 net.cpp:137] Memory required for data: 115918000
I1210 14:45:18.798621 15352 layer_factory.cpp:58] Creating layer conv2
I1210 14:45:18.798621 15352 net.cpp:84] Creating Layer conv2
I1210 14:45:18.798621 15352 net.cpp:406] conv2 <- conv1_0
I1210 14:45:18.798621 15352 net.cpp:380] conv2 -> conv2
I1210 14:45:18.799638 15352 net.cpp:122] Setting up conv2
I1210 14:45:18.799638 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.799638 15352 net.cpp:137] Memory required for data: 132302000
I1210 14:45:18.799638 15352 layer_factory.cpp:58] Creating layer bn2
I1210 14:45:18.799638 15352 net.cpp:84] Creating Layer bn2
I1210 14:45:18.799638 15352 net.cpp:406] bn2 <- conv2
I1210 14:45:18.799638 15352 net.cpp:367] bn2 -> conv2 (in-place)
I1210 14:45:18.799638 15352 net.cpp:122] Setting up bn2
I1210 14:45:18.799638 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.799638 15352 net.cpp:137] Memory required for data: 148686000
I1210 14:45:18.799638 15352 layer_factory.cpp:58] Creating layer scale2
I1210 14:45:18.799638 15352 net.cpp:84] Creating Layer scale2
I1210 14:45:18.799638 15352 net.cpp:406] scale2 <- conv2
I1210 14:45:18.799638 15352 net.cpp:367] scale2 -> conv2 (in-place)
I1210 14:45:18.800637 15352 layer_factory.cpp:58] Creating layer scale2
I1210 14:45:18.800637 15352 net.cpp:122] Setting up scale2
I1210 14:45:18.800637 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.800637 15352 net.cpp:137] Memory required for data: 165070000
I1210 14:45:18.800637 15352 layer_factory.cpp:58] Creating layer relu2
I1210 14:45:18.800637 15352 net.cpp:84] Creating Layer relu2
I1210 14:45:18.800637 15352 net.cpp:406] relu2 <- conv2
I1210 14:45:18.800637 15352 net.cpp:367] relu2 -> conv2 (in-place)
I1210 14:45:18.800637 15352 net.cpp:122] Setting up relu2
I1210 14:45:18.800637 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.800637 15352 net.cpp:137] Memory required for data: 181454000
I1210 14:45:18.800637 15352 layer_factory.cpp:58] Creating layer conv2_1
I1210 14:45:18.800637 15352 net.cpp:84] Creating Layer conv2_1
I1210 14:45:18.800637 15352 net.cpp:406] conv2_1 <- conv2
I1210 14:45:18.800637 15352 net.cpp:380] conv2_1 -> conv2_1
I1210 14:45:18.801621 15352 net.cpp:122] Setting up conv2_1
I1210 14:45:18.801621 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.801621 15352 net.cpp:137] Memory required for data: 197838000
I1210 14:45:18.801621 15352 layer_factory.cpp:58] Creating layer bn2_1
I1210 14:45:18.801621 15352 net.cpp:84] Creating Layer bn2_1
I1210 14:45:18.801621 15352 net.cpp:406] bn2_1 <- conv2_1
I1210 14:45:18.801621 15352 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1210 14:45:18.802637 15352 net.cpp:122] Setting up bn2_1
I1210 14:45:18.802637 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.802637 15352 net.cpp:137] Memory required for data: 214222000
I1210 14:45:18.802637 15352 layer_factory.cpp:58] Creating layer scale2_1
I1210 14:45:18.802637 15352 net.cpp:84] Creating Layer scale2_1
I1210 14:45:18.802637 15352 net.cpp:406] scale2_1 <- conv2_1
I1210 14:45:18.802637 15352 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1210 14:45:18.802637 15352 layer_factory.cpp:58] Creating layer scale2_1
I1210 14:45:18.802637 15352 net.cpp:122] Setting up scale2_1
I1210 14:45:18.802637 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.802637 15352 net.cpp:137] Memory required for data: 230606000
I1210 14:45:18.802637 15352 layer_factory.cpp:58] Creating layer relu2_1
I1210 14:45:18.802637 15352 net.cpp:84] Creating Layer relu2_1
I1210 14:45:18.802637 15352 net.cpp:406] relu2_1 <- conv2_1
I1210 14:45:18.802637 15352 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1210 14:45:18.803622 15352 net.cpp:122] Setting up relu2_1
I1210 14:45:18.803622 15352 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 14:45:18.803622 15352 net.cpp:137] Memory required for data: 246990000
I1210 14:45:18.803622 15352 layer_factory.cpp:58] Creating layer conv2_2
I1210 14:45:18.803622 15352 net.cpp:84] Creating Layer conv2_2
I1210 14:45:18.803622 15352 net.cpp:406] conv2_2 <- conv2_1
I1210 14:45:18.803622 15352 net.cpp:380] conv2_2 -> conv2_2
I1210 14:45:18.804621 15352 net.cpp:122] Setting up conv2_2
I1210 14:45:18.804621 15352 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 14:45:18.804621 15352 net.cpp:137] Memory required for data: 267470000
I1210 14:45:18.804621 15352 layer_factory.cpp:58] Creating layer bn2_2
I1210 14:45:18.804621 15352 net.cpp:84] Creating Layer bn2_2
I1210 14:45:18.804621 15352 net.cpp:406] bn2_2 <- conv2_2
I1210 14:45:18.804621 15352 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1210 14:45:18.804621 15352 net.cpp:122] Setting up bn2_2
I1210 14:45:18.804621 15352 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 14:45:18.804621 15352 net.cpp:137] Memory required for data: 287950000
I1210 14:45:18.804621 15352 layer_factory.cpp:58] Creating layer scale2_2
I1210 14:45:18.804621 15352 net.cpp:84] Creating Layer scale2_2
I1210 14:45:18.804621 15352 net.cpp:406] scale2_2 <- conv2_2
I1210 14:45:18.804621 15352 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1210 14:45:18.804621 15352 layer_factory.cpp:58] Creating layer scale2_2
I1210 14:45:18.805637 15352 net.cpp:122] Setting up scale2_2
I1210 14:45:18.805637 15352 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 14:45:18.805637 15352 net.cpp:137] Memory required for data: 308430000
I1210 14:45:18.805637 15352 layer_factory.cpp:58] Creating layer relu2_2
I1210 14:45:18.805637 15352 net.cpp:84] Creating Layer relu2_2
I1210 14:45:18.805637 15352 net.cpp:406] relu2_2 <- conv2_2
I1210 14:45:18.805637 15352 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1210 14:45:18.805637 15352 net.cpp:122] Setting up relu2_2
I1210 14:45:18.805637 15352 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 14:45:18.805637 15352 net.cpp:137] Memory required for data: 328910000
I1210 14:45:18.805637 15352 layer_factory.cpp:58] Creating layer newconv_added1
I1210 14:45:18.805637 15352 net.cpp:84] Creating Layer newconv_added1
I1210 14:45:18.805637 15352 net.cpp:406] newconv_added1 <- conv2_2
I1210 14:45:18.805637 15352 net.cpp:380] newconv_added1 -> newconv_added1
I1210 14:45:18.806635 15352 net.cpp:122] Setting up newconv_added1
I1210 14:45:18.806635 15352 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 14:45:18.806635 15352 net.cpp:137] Memory required for data: 349390000
I1210 14:45:18.806635 15352 layer_factory.cpp:58] Creating layer pool2_1
I1210 14:45:18.806635 15352 net.cpp:84] Creating Layer pool2_1
I1210 14:45:18.806635 15352 net.cpp:406] pool2_1 <- newconv_added1
I1210 14:45:18.806635 15352 net.cpp:380] pool2_1 -> pool2_1
I1210 14:45:18.806635 15352 net.cpp:122] Setting up pool2_1
I1210 14:45:18.806635 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.806635 15352 net.cpp:137] Memory required for data: 354510000
I1210 14:45:18.807621 15352 layer_factory.cpp:58] Creating layer conv3
I1210 14:45:18.807621 15352 net.cpp:84] Creating Layer conv3
I1210 14:45:18.807621 15352 net.cpp:406] conv3 <- pool2_1
I1210 14:45:18.807621 15352 net.cpp:380] conv3 -> conv3
I1210 14:45:18.808621 15352 net.cpp:122] Setting up conv3
I1210 14:45:18.808621 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.808621 15352 net.cpp:137] Memory required for data: 359630000
I1210 14:45:18.808621 15352 layer_factory.cpp:58] Creating layer bn3
I1210 14:45:18.808621 15352 net.cpp:84] Creating Layer bn3
I1210 14:45:18.808621 15352 net.cpp:406] bn3 <- conv3
I1210 14:45:18.808621 15352 net.cpp:367] bn3 -> conv3 (in-place)
I1210 14:45:18.808621 15352 net.cpp:122] Setting up bn3
I1210 14:45:18.808621 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.808621 15352 net.cpp:137] Memory required for data: 364750000
I1210 14:45:18.808621 15352 layer_factory.cpp:58] Creating layer scale3
I1210 14:45:18.808621 15352 net.cpp:84] Creating Layer scale3
I1210 14:45:18.808621 15352 net.cpp:406] scale3 <- conv3
I1210 14:45:18.808621 15352 net.cpp:367] scale3 -> conv3 (in-place)
I1210 14:45:18.808621 15352 layer_factory.cpp:58] Creating layer scale3
I1210 14:45:18.808621 15352 net.cpp:122] Setting up scale3
I1210 14:45:18.808621 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.808621 15352 net.cpp:137] Memory required for data: 369870000
I1210 14:45:18.808621 15352 layer_factory.cpp:58] Creating layer relu3
I1210 14:45:18.808621 15352 net.cpp:84] Creating Layer relu3
I1210 14:45:18.808621 15352 net.cpp:406] relu3 <- conv3
I1210 14:45:18.808621 15352 net.cpp:367] relu3 -> conv3 (in-place)
I1210 14:45:18.809620 15352 net.cpp:122] Setting up relu3
I1210 14:45:18.809620 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.809620 15352 net.cpp:137] Memory required for data: 374990000
I1210 14:45:18.809620 15352 layer_factory.cpp:58] Creating layer conv3_1
I1210 14:45:18.809620 15352 net.cpp:84] Creating Layer conv3_1
I1210 14:45:18.809620 15352 net.cpp:406] conv3_1 <- conv3
I1210 14:45:18.809620 15352 net.cpp:380] conv3_1 -> conv3_1
I1210 14:45:18.811640 15352 net.cpp:122] Setting up conv3_1
I1210 14:45:18.811640 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.811640 15352 net.cpp:137] Memory required for data: 380110000
I1210 14:45:18.811640 15352 layer_factory.cpp:58] Creating layer bn3_1
I1210 14:45:18.811640 15352 net.cpp:84] Creating Layer bn3_1
I1210 14:45:18.811640 15352 net.cpp:406] bn3_1 <- conv3_1
I1210 14:45:18.811640 15352 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1210 14:45:18.811640 15352 net.cpp:122] Setting up bn3_1
I1210 14:45:18.811640 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.811640 15352 net.cpp:137] Memory required for data: 385230000
I1210 14:45:18.811640 15352 layer_factory.cpp:58] Creating layer scale3_1
I1210 14:45:18.811640 15352 net.cpp:84] Creating Layer scale3_1
I1210 14:45:18.811640 15352 net.cpp:406] scale3_1 <- conv3_1
I1210 14:45:18.811640 15352 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1210 14:45:18.811640 15352 layer_factory.cpp:58] Creating layer scale3_1
I1210 14:45:18.811640 15352 net.cpp:122] Setting up scale3_1
I1210 14:45:18.811640 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.811640 15352 net.cpp:137] Memory required for data: 390350000
I1210 14:45:18.811640 15352 layer_factory.cpp:58] Creating layer relu3_1
I1210 14:45:18.811640 15352 net.cpp:84] Creating Layer relu3_1
I1210 14:45:18.811640 15352 net.cpp:406] relu3_1 <- conv3_1
I1210 14:45:18.811640 15352 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1210 14:45:18.812647 15352 net.cpp:122] Setting up relu3_1
I1210 14:45:18.812647 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.812647 15352 net.cpp:137] Memory required for data: 395470000
I1210 14:45:18.812647 15352 layer_factory.cpp:58] Creating layer conv4
I1210 14:45:18.812647 15352 net.cpp:84] Creating Layer conv4
I1210 14:45:18.812647 15352 net.cpp:406] conv4 <- conv3_1
I1210 14:45:18.812647 15352 net.cpp:380] conv4 -> conv4
I1210 14:45:18.813649 15352 net.cpp:122] Setting up conv4
I1210 14:45:18.813649 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.813649 15352 net.cpp:137] Memory required for data: 400590000
I1210 14:45:18.813649 15352 layer_factory.cpp:58] Creating layer bn4
I1210 14:45:18.813649 15352 net.cpp:84] Creating Layer bn4
I1210 14:45:18.813649 15352 net.cpp:406] bn4 <- conv4
I1210 14:45:18.813649 15352 net.cpp:367] bn4 -> conv4 (in-place)
I1210 14:45:18.813649 15352 net.cpp:122] Setting up bn4
I1210 14:45:18.813649 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.813649 15352 net.cpp:137] Memory required for data: 405710000
I1210 14:45:18.813649 15352 layer_factory.cpp:58] Creating layer scale4
I1210 14:45:18.813649 15352 net.cpp:84] Creating Layer scale4
I1210 14:45:18.813649 15352 net.cpp:406] scale4 <- conv4
I1210 14:45:18.813649 15352 net.cpp:367] scale4 -> conv4 (in-place)
I1210 14:45:18.813649 15352 layer_factory.cpp:58] Creating layer scale4
I1210 14:45:18.813649 15352 net.cpp:122] Setting up scale4
I1210 14:45:18.813649 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.813649 15352 net.cpp:137] Memory required for data: 410830000
I1210 14:45:18.813649 15352 layer_factory.cpp:58] Creating layer relu4
I1210 14:45:18.813649 15352 net.cpp:84] Creating Layer relu4
I1210 14:45:18.813649 15352 net.cpp:406] relu4 <- conv4
I1210 14:45:18.813649 15352 net.cpp:367] relu4 -> conv4 (in-place)
I1210 14:45:18.814640 15352 net.cpp:122] Setting up relu4
I1210 14:45:18.814640 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.814640 15352 net.cpp:137] Memory required for data: 415950000
I1210 14:45:18.814640 15352 layer_factory.cpp:58] Creating layer conv4_1
I1210 14:45:18.814640 15352 net.cpp:84] Creating Layer conv4_1
I1210 14:45:18.814640 15352 net.cpp:406] conv4_1 <- conv4
I1210 14:45:18.814640 15352 net.cpp:380] conv4_1 -> conv4_1
I1210 14:45:18.815649 15352 net.cpp:122] Setting up conv4_1
I1210 14:45:18.815649 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.815649 15352 net.cpp:137] Memory required for data: 421070000
I1210 14:45:18.815649 15352 layer_factory.cpp:58] Creating layer bn4_1
I1210 14:45:18.815649 15352 net.cpp:84] Creating Layer bn4_1
I1210 14:45:18.815649 15352 net.cpp:406] bn4_1 <- conv4_1
I1210 14:45:18.815649 15352 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1210 14:45:18.815649 15352 net.cpp:122] Setting up bn4_1
I1210 14:45:18.815649 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.815649 15352 net.cpp:137] Memory required for data: 426190000
I1210 14:45:18.815649 15352 layer_factory.cpp:58] Creating layer scale4_1
I1210 14:45:18.815649 15352 net.cpp:84] Creating Layer scale4_1
I1210 14:45:18.815649 15352 net.cpp:406] scale4_1 <- conv4_1
I1210 14:45:18.815649 15352 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1210 14:45:18.815649 15352 layer_factory.cpp:58] Creating layer scale4_1
I1210 14:45:18.815649 15352 net.cpp:122] Setting up scale4_1
I1210 14:45:18.815649 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.815649 15352 net.cpp:137] Memory required for data: 431310000
I1210 14:45:18.815649 15352 layer_factory.cpp:58] Creating layer relu4_1
I1210 14:45:18.815649 15352 net.cpp:84] Creating Layer relu4_1
I1210 14:45:18.815649 15352 net.cpp:406] relu4_1 <- conv4_1
I1210 14:45:18.815649 15352 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1210 14:45:18.816635 15352 net.cpp:122] Setting up relu4_1
I1210 14:45:18.816635 15352 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 14:45:18.816635 15352 net.cpp:137] Memory required for data: 436430000
I1210 14:45:18.816635 15352 layer_factory.cpp:58] Creating layer conv4_2
I1210 14:45:18.816635 15352 net.cpp:84] Creating Layer conv4_2
I1210 14:45:18.816635 15352 net.cpp:406] conv4_2 <- conv4_1
I1210 14:45:18.816635 15352 net.cpp:380] conv4_2 -> conv4_2
I1210 14:45:18.817639 15352 net.cpp:122] Setting up conv4_2
I1210 14:45:18.817639 15352 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 14:45:18.817639 15352 net.cpp:137] Memory required for data: 442369200
I1210 14:45:18.817639 15352 layer_factory.cpp:58] Creating layer bn4_2
I1210 14:45:18.817639 15352 net.cpp:84] Creating Layer bn4_2
I1210 14:45:18.817639 15352 net.cpp:406] bn4_2 <- conv4_2
I1210 14:45:18.817639 15352 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1210 14:45:18.817639 15352 net.cpp:122] Setting up bn4_2
I1210 14:45:18.817639 15352 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 14:45:18.818635 15352 net.cpp:137] Memory required for data: 448308400
I1210 14:45:18.818635 15352 layer_factory.cpp:58] Creating layer scale4_2
I1210 14:45:18.818635 15352 net.cpp:84] Creating Layer scale4_2
I1210 14:45:18.818635 15352 net.cpp:406] scale4_2 <- conv4_2
I1210 14:45:18.818635 15352 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1210 14:45:18.818635 15352 layer_factory.cpp:58] Creating layer scale4_2
I1210 14:45:18.818635 15352 net.cpp:122] Setting up scale4_2
I1210 14:45:18.818635 15352 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 14:45:18.818635 15352 net.cpp:137] Memory required for data: 454247600
I1210 14:45:18.818635 15352 layer_factory.cpp:58] Creating layer relu4_2
I1210 14:45:18.818635 15352 net.cpp:84] Creating Layer relu4_2
I1210 14:45:18.818635 15352 net.cpp:406] relu4_2 <- conv4_2
I1210 14:45:18.818635 15352 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1210 14:45:18.818635 15352 net.cpp:122] Setting up relu4_2
I1210 14:45:18.818635 15352 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 14:45:18.818635 15352 net.cpp:137] Memory required for data: 460186800
I1210 14:45:18.818635 15352 layer_factory.cpp:58] Creating layer added_new_conv2
I1210 14:45:18.818635 15352 net.cpp:84] Creating Layer added_new_conv2
I1210 14:45:18.818635 15352 net.cpp:406] added_new_conv2 <- conv4_2
I1210 14:45:18.818635 15352 net.cpp:380] added_new_conv2 -> added_new_conv2
I1210 14:45:18.820621 15352 net.cpp:122] Setting up added_new_conv2
I1210 14:45:18.820621 15352 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 14:45:18.820621 15352 net.cpp:137] Memory required for data: 466126000
I1210 14:45:18.820621 15352 layer_factory.cpp:58] Creating layer pool4_2
I1210 14:45:18.820621 15352 net.cpp:84] Creating Layer pool4_2
I1210 14:45:18.820621 15352 net.cpp:406] pool4_2 <- added_new_conv2
I1210 14:45:18.820621 15352 net.cpp:380] pool4_2 -> pool4_2
I1210 14:45:18.820621 15352 net.cpp:122] Setting up pool4_2
I1210 14:45:18.820621 15352 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 14:45:18.820621 15352 net.cpp:137] Memory required for data: 467610800
I1210 14:45:18.820621 15352 layer_factory.cpp:58] Creating layer conv4_0
I1210 14:45:18.820621 15352 net.cpp:84] Creating Layer conv4_0
I1210 14:45:18.820621 15352 net.cpp:406] conv4_0 <- pool4_2
I1210 14:45:18.820621 15352 net.cpp:380] conv4_0 -> conv4_0
I1210 14:45:18.822620 15352 net.cpp:122] Setting up conv4_0
I1210 14:45:18.822620 15352 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 14:45:18.822620 15352 net.cpp:137] Memory required for data: 469095600
I1210 14:45:18.822620 15352 layer_factory.cpp:58] Creating layer bn4_0
I1210 14:45:18.822620 15352 net.cpp:84] Creating Layer bn4_0
I1210 14:45:18.822620 15352 net.cpp:406] bn4_0 <- conv4_0
I1210 14:45:18.822620 15352 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1210 14:45:18.822620 15352 net.cpp:122] Setting up bn4_0
I1210 14:45:18.822620 15352 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 14:45:18.822620 15352 net.cpp:137] Memory required for data: 470580400
I1210 14:45:18.822620 15352 layer_factory.cpp:58] Creating layer scale4_0
I1210 14:45:18.822620 15352 net.cpp:84] Creating Layer scale4_0
I1210 14:45:18.822620 15352 net.cpp:406] scale4_0 <- conv4_0
I1210 14:45:18.822620 15352 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1210 14:45:18.822620 15352 layer_factory.cpp:58] Creating layer scale4_0
I1210 14:45:18.823621 15352 net.cpp:122] Setting up scale4_0
I1210 14:45:18.823621 15352 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 14:45:18.823621 15352 net.cpp:137] Memory required for data: 472065200
I1210 14:45:18.823621 15352 layer_factory.cpp:58] Creating layer relu4_0
I1210 14:45:18.823621 15352 net.cpp:84] Creating Layer relu4_0
I1210 14:45:18.823621 15352 net.cpp:406] relu4_0 <- conv4_0
I1210 14:45:18.823621 15352 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1210 14:45:18.823621 15352 net.cpp:122] Setting up relu4_0
I1210 14:45:18.823621 15352 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 14:45:18.823621 15352 net.cpp:137] Memory required for data: 473550000
I1210 14:45:18.823621 15352 layer_factory.cpp:58] Creating layer conv11
I1210 14:45:18.823621 15352 net.cpp:84] Creating Layer conv11
I1210 14:45:18.823621 15352 net.cpp:406] conv11 <- conv4_0
I1210 14:45:18.823621 15352 net.cpp:380] conv11 -> conv11
I1210 14:45:18.825621 15352 net.cpp:122] Setting up conv11
I1210 14:45:18.825621 15352 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 14:45:18.825621 15352 net.cpp:137] Memory required for data: 475342000
I1210 14:45:18.825621 15352 layer_factory.cpp:58] Creating layer bn_conv11
I1210 14:45:18.825621 15352 net.cpp:84] Creating Layer bn_conv11
I1210 14:45:18.825621 15352 net.cpp:406] bn_conv11 <- conv11
I1210 14:45:18.825621 15352 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1210 14:45:18.825621 15352 net.cpp:122] Setting up bn_conv11
I1210 14:45:18.825621 15352 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 14:45:18.825621 15352 net.cpp:137] Memory required for data: 477134000
I1210 14:45:18.825621 15352 layer_factory.cpp:58] Creating layer scale_conv11
I1210 14:45:18.825621 15352 net.cpp:84] Creating Layer scale_conv11
I1210 14:45:18.825621 15352 net.cpp:406] scale_conv11 <- conv11
I1210 14:45:18.825621 15352 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1210 14:45:18.825621 15352 layer_factory.cpp:58] Creating layer scale_conv11
I1210 14:45:18.825621 15352 net.cpp:122] Setting up scale_conv11
I1210 14:45:18.825621 15352 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 14:45:18.825621 15352 net.cpp:137] Memory required for data: 478926000
I1210 14:45:18.825621 15352 layer_factory.cpp:58] Creating layer relu_conv11
I1210 14:45:18.825621 15352 net.cpp:84] Creating Layer relu_conv11
I1210 14:45:18.825621 15352 net.cpp:406] relu_conv11 <- conv11
I1210 14:45:18.825621 15352 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1210 14:45:18.825621 15352 net.cpp:122] Setting up relu_conv11
I1210 14:45:18.825621 15352 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 14:45:18.825621 15352 net.cpp:137] Memory required for data: 480718000
I1210 14:45:18.825621 15352 layer_factory.cpp:58] Creating layer conv12
I1210 14:45:18.825621 15352 net.cpp:84] Creating Layer conv12
I1210 14:45:18.825621 15352 net.cpp:406] conv12 <- conv11
I1210 14:45:18.825621 15352 net.cpp:380] conv12 -> conv12
I1210 14:45:18.827641 15352 net.cpp:122] Setting up conv12
I1210 14:45:18.827641 15352 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 14:45:18.827641 15352 net.cpp:137] Memory required for data: 483022000
I1210 14:45:18.827641 15352 layer_factory.cpp:58] Creating layer bn_conv12
I1210 14:45:18.827641 15352 net.cpp:84] Creating Layer bn_conv12
I1210 14:45:18.827641 15352 net.cpp:406] bn_conv12 <- conv12
I1210 14:45:18.827641 15352 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1210 14:45:18.828634 15352 net.cpp:122] Setting up bn_conv12
I1210 14:45:18.828634 15352 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 14:45:18.828634 15352 net.cpp:137] Memory required for data: 485326000
I1210 14:45:18.828634 15352 layer_factory.cpp:58] Creating layer scale_conv12
I1210 14:45:18.828634 15352 net.cpp:84] Creating Layer scale_conv12
I1210 14:45:18.828634 15352 net.cpp:406] scale_conv12 <- conv12
I1210 14:45:18.828634 15352 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1210 14:45:18.828634 15352 layer_factory.cpp:58] Creating layer scale_conv12
I1210 14:45:18.828634 15352 net.cpp:122] Setting up scale_conv12
I1210 14:45:18.828634 15352 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 14:45:18.828634 15352 net.cpp:137] Memory required for data: 487630000
I1210 14:45:18.828634 15352 layer_factory.cpp:58] Creating layer relu_conv12
I1210 14:45:18.828634 15352 net.cpp:84] Creating Layer relu_conv12
I1210 14:45:18.828634 15352 net.cpp:406] relu_conv12 <- conv12
I1210 14:45:18.828634 15352 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1210 14:45:18.828634 15352 net.cpp:122] Setting up relu_conv12
I1210 14:45:18.828634 15352 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 14:45:18.828634 15352 net.cpp:137] Memory required for data: 489934000
I1210 14:45:18.828634 15352 layer_factory.cpp:58] Creating layer poolcp6
I1210 14:45:18.828634 15352 net.cpp:84] Creating Layer poolcp6
I1210 14:45:18.828634 15352 net.cpp:406] poolcp6 <- conv12
I1210 14:45:18.828634 15352 net.cpp:380] poolcp6 -> poolcp6
I1210 14:45:18.828634 15352 net.cpp:122] Setting up poolcp6
I1210 14:45:18.828634 15352 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1210 14:45:18.828634 15352 net.cpp:137] Memory required for data: 489970000
I1210 14:45:18.828634 15352 layer_factory.cpp:58] Creating layer ip1
I1210 14:45:18.828634 15352 net.cpp:84] Creating Layer ip1
I1210 14:45:18.828634 15352 net.cpp:406] ip1 <- poolcp6
I1210 14:45:18.828634 15352 net.cpp:380] ip1 -> ip1
I1210 14:45:18.828634 15352 net.cpp:122] Setting up ip1
I1210 14:45:18.828634 15352 net.cpp:129] Top shape: 100 100 (10000)
I1210 14:45:18.828634 15352 net.cpp:137] Memory required for data: 490010000
I1210 14:45:18.828634 15352 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1210 14:45:18.828634 15352 net.cpp:84] Creating Layer ip1_ip1_0_split
I1210 14:45:18.828634 15352 net.cpp:406] ip1_ip1_0_split <- ip1
I1210 14:45:18.828634 15352 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1210 14:45:18.828634 15352 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1210 14:45:18.828634 15352 net.cpp:122] Setting up ip1_ip1_0_split
I1210 14:45:18.828634 15352 net.cpp:129] Top shape: 100 100 (10000)
I1210 14:45:18.828634 15352 net.cpp:129] Top shape: 100 100 (10000)
I1210 14:45:18.828634 15352 net.cpp:137] Memory required for data: 490090000
I1210 14:45:18.828634 15352 layer_factory.cpp:58] Creating layer accuracy
I1210 14:45:18.828634 15352 net.cpp:84] Creating Layer accuracy
I1210 14:45:18.828634 15352 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1210 14:45:18.828634 15352 net.cpp:406] accuracy <- label_cifar_1_split_0
I1210 14:45:18.829634 15352 net.cpp:380] accuracy -> accuracy
I1210 14:45:18.829634 15352 net.cpp:122] Setting up accuracy
I1210 14:45:18.829634 15352 net.cpp:129] Top shape: (1)
I1210 14:45:18.829634 15352 net.cpp:137] Memory required for data: 490090004
I1210 14:45:18.829634 15352 layer_factory.cpp:58] Creating layer loss
I1210 14:45:18.829634 15352 net.cpp:84] Creating Layer loss
I1210 14:45:18.829634 15352 net.cpp:406] loss <- ip1_ip1_0_split_1
I1210 14:45:18.829634 15352 net.cpp:406] loss <- label_cifar_1_split_1
I1210 14:45:18.829634 15352 net.cpp:380] loss -> loss
I1210 14:45:18.829634 15352 layer_factory.cpp:58] Creating layer loss
I1210 14:45:18.829634 15352 net.cpp:122] Setting up loss
I1210 14:45:18.829634 15352 net.cpp:129] Top shape: (1)
I1210 14:45:18.829634 15352 net.cpp:132]     with loss weight 1
I1210 14:45:18.829634 15352 net.cpp:137] Memory required for data: 490090008
I1210 14:45:18.829634 15352 net.cpp:198] loss needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:200] accuracy does not need backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] ip1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] poolcp6 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] relu_conv12 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] scale_conv12 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] bn_conv12 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] conv12 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] relu_conv11 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] scale_conv11 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] bn_conv11 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] conv11 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] relu4_0 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] scale4_0 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] bn4_0 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] conv4_0 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] pool4_2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] added_new_conv2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] relu4_2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] scale4_2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] bn4_2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] conv4_2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] relu4_1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] scale4_1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] bn4_1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] conv4_1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] relu4 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] scale4 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] bn4 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] conv4 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] relu3_1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] scale3_1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] bn3_1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] conv3_1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] relu3 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] scale3 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] bn3 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] conv3 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] pool2_1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] newconv_added1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] relu2_2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] scale2_2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] bn2_2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] conv2_2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] relu2_1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] scale2_1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] bn2_1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] conv2_1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] relu2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] scale2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] bn2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] conv2 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] relu1_0 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] scale1_0 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] bn1_0 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] conv1_0 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] relu1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] scale1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] bn1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:198] conv1 needs backward computation.
I1210 14:45:18.829634 15352 net.cpp:200] label_cifar_1_split does not need backward computation.
I1210 14:45:18.829634 15352 net.cpp:200] cifar does not need backward computation.
I1210 14:45:18.829634 15352 net.cpp:242] This network produces output accuracy
I1210 14:45:18.829634 15352 net.cpp:242] This network produces output loss
I1210 14:45:18.829634 15352 net.cpp:255] Network initialization done.
I1210 14:45:18.830634 15352 solver.cpp:56] Solver scaffolding done.
I1210 14:45:18.834635 15352 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90000.solverstate
I1210 14:45:18.837635 15352 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90000.caffemodel
I1210 14:45:18.837635 15352 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 14:45:18.838635 15352 sgd_solver.cpp:318] SGDSolver: restoring history
I1210 14:45:18.842639 15352 caffe.cpp:249] Starting Optimization
I1210 14:45:18.842639 15352 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_v2_360k
I1210 14:45:18.842639 15352 solver.cpp:273] Learning Rate Policy: multistep
I1210 14:45:18.844647 15352 solver.cpp:330] Iteration 90000, Testing net (#0)
I1210 14:45:18.846638 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:45:20.275835 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:45:20.331384 15352 solver.cpp:397]     Test net output #0: accuracy = 0.5871
I1210 14:45:20.331384 15352 solver.cpp:397]     Test net output #1: loss = 1.66452 (* 1 = 1.66452 loss)
I1210 14:45:20.438417 15352 solver.cpp:218] Iteration 90000 (56431 iter/s, 1.59487s/100 iters), loss = 0.686856
I1210 14:45:20.438417 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 14:45:20.438417 15352 solver.cpp:237]     Train net output #1: loss = 0.686856 (* 1 = 0.686856 loss)
I1210 14:45:20.438417 15352 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1210 14:45:26.228222 15352 solver.cpp:218] Iteration 90100 (17.2742 iter/s, 5.78899s/100 iters), loss = 0.642396
I1210 14:45:26.228222 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 14:45:26.228222 15352 solver.cpp:237]     Train net output #1: loss = 0.642396 (* 1 = 0.642396 loss)
I1210 14:45:26.228222 15352 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1210 14:45:32.026466 15352 solver.cpp:218] Iteration 90200 (17.2489 iter/s, 5.79746s/100 iters), loss = 0.541101
I1210 14:45:32.026466 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 14:45:32.026466 15352 solver.cpp:237]     Train net output #1: loss = 0.541101 (* 1 = 0.541101 loss)
I1210 14:45:32.026968 15352 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1210 14:45:37.780736 15352 solver.cpp:218] Iteration 90300 (17.3803 iter/s, 5.75365s/100 iters), loss = 0.773773
I1210 14:45:37.780736 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 14:45:37.780736 15352 solver.cpp:237]     Train net output #1: loss = 0.773773 (* 1 = 0.773773 loss)
I1210 14:45:37.780736 15352 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1210 14:45:43.550504 15352 solver.cpp:218] Iteration 90400 (17.3324 iter/s, 5.76954s/100 iters), loss = 0.854011
I1210 14:45:43.550504 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1210 14:45:43.550504 15352 solver.cpp:237]     Train net output #1: loss = 0.854011 (* 1 = 0.854011 loss)
I1210 14:45:43.550504 15352 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1210 14:45:48.982672 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:45:49.204372 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90500.caffemodel
I1210 14:45:49.219372 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90500.solverstate
I1210 14:45:49.224372 15352 solver.cpp:330] Iteration 90500, Testing net (#0)
I1210 14:45:49.224372 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:45:50.608527 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:45:50.664546 15352 solver.cpp:397]     Test net output #0: accuracy = 0.5533
I1210 14:45:50.664546 15352 solver.cpp:397]     Test net output #1: loss = 1.76842 (* 1 = 1.76842 loss)
I1210 14:45:50.719542 15352 solver.cpp:218] Iteration 90500 (13.9503 iter/s, 7.16829s/100 iters), loss = 0.594373
I1210 14:45:50.719542 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 14:45:50.719542 15352 solver.cpp:237]     Train net output #1: loss = 0.594373 (* 1 = 0.594373 loss)
I1210 14:45:50.719542 15352 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1210 14:45:56.382751 15352 solver.cpp:218] Iteration 90600 (17.6596 iter/s, 5.66266s/100 iters), loss = 0.752892
I1210 14:45:56.382751 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 14:45:56.382751 15352 solver.cpp:237]     Train net output #1: loss = 0.752892 (* 1 = 0.752892 loss)
I1210 14:45:56.382751 15352 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1210 14:46:02.039727 15352 solver.cpp:218] Iteration 90700 (17.6798 iter/s, 5.65619s/100 iters), loss = 0.558305
I1210 14:46:02.039727 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 14:46:02.039727 15352 solver.cpp:237]     Train net output #1: loss = 0.558305 (* 1 = 0.558305 loss)
I1210 14:46:02.039727 15352 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1210 14:46:07.737561 15352 solver.cpp:218] Iteration 90800 (17.5523 iter/s, 5.69727s/100 iters), loss = 0.883633
I1210 14:46:07.737561 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1210 14:46:07.737561 15352 solver.cpp:237]     Train net output #1: loss = 0.883633 (* 1 = 0.883633 loss)
I1210 14:46:07.737561 15352 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1210 14:46:13.378686 15352 solver.cpp:218] Iteration 90900 (17.7284 iter/s, 5.64066s/100 iters), loss = 0.890121
I1210 14:46:13.378686 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1210 14:46:13.378686 15352 solver.cpp:237]     Train net output #1: loss = 0.890121 (* 1 = 0.890121 loss)
I1210 14:46:13.378686 15352 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1210 14:46:18.812361 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:46:19.036388 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_91000.caffemodel
I1210 14:46:19.053386 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_91000.solverstate
I1210 14:46:19.058385 15352 solver.cpp:330] Iteration 91000, Testing net (#0)
I1210 14:46:19.058385 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:46:20.434036 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:46:20.487540 15352 solver.cpp:397]     Test net output #0: accuracy = 0.5889
I1210 14:46:20.487540 15352 solver.cpp:397]     Test net output #1: loss = 1.57597 (* 1 = 1.57597 loss)
I1210 14:46:20.541054 15352 solver.cpp:218] Iteration 91000 (13.9629 iter/s, 7.16186s/100 iters), loss = 0.545856
I1210 14:46:20.541054 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 14:46:20.541054 15352 solver.cpp:237]     Train net output #1: loss = 0.545856 (* 1 = 0.545856 loss)
I1210 14:46:20.541054 15352 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1210 14:46:26.185030 15352 solver.cpp:218] Iteration 91100 (17.7196 iter/s, 5.64348s/100 iters), loss = 0.604084
I1210 14:46:26.185030 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 14:46:26.185030 15352 solver.cpp:237]     Train net output #1: loss = 0.604084 (* 1 = 0.604084 loss)
I1210 14:46:26.185030 15352 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1210 14:46:31.812479 15352 solver.cpp:218] Iteration 91200 (17.7723 iter/s, 5.62672s/100 iters), loss = 0.551106
I1210 14:46:31.812479 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:46:31.812479 15352 solver.cpp:237]     Train net output #1: loss = 0.551106 (* 1 = 0.551106 loss)
I1210 14:46:31.812479 15352 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1210 14:46:37.450034 15352 solver.cpp:218] Iteration 91300 (17.7394 iter/s, 5.63718s/100 iters), loss = 0.706329
I1210 14:46:37.450034 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 14:46:37.450034 15352 solver.cpp:237]     Train net output #1: loss = 0.706329 (* 1 = 0.706329 loss)
I1210 14:46:37.450034 15352 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1210 14:46:43.092383 15352 solver.cpp:218] Iteration 91400 (17.7236 iter/s, 5.64219s/100 iters), loss = 0.761202
I1210 14:46:43.092383 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 14:46:43.092383 15352 solver.cpp:237]     Train net output #1: loss = 0.761202 (* 1 = 0.761202 loss)
I1210 14:46:43.092383 15352 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1210 14:46:48.473825 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:46:48.695839 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_91500.caffemodel
I1210 14:46:48.711856 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_91500.solverstate
I1210 14:46:48.716856 15352 solver.cpp:330] Iteration 91500, Testing net (#0)
I1210 14:46:48.716856 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:46:50.089051 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:46:50.143054 15352 solver.cpp:397]     Test net output #0: accuracy = 0.5909
I1210 14:46:50.143054 15352 solver.cpp:397]     Test net output #1: loss = 1.568 (* 1 = 1.568 loss)
I1210 14:46:50.196058 15352 solver.cpp:218] Iteration 91500 (14.0775 iter/s, 7.10355s/100 iters), loss = 0.589431
I1210 14:46:50.196058 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:46:50.196058 15352 solver.cpp:237]     Train net output #1: loss = 0.589431 (* 1 = 0.589431 loss)
I1210 14:46:50.196058 15352 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1210 14:46:55.842953 15352 solver.cpp:218] Iteration 91600 (17.7127 iter/s, 5.64567s/100 iters), loss = 0.755473
I1210 14:46:55.842953 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 14:46:55.842953 15352 solver.cpp:237]     Train net output #1: loss = 0.755473 (* 1 = 0.755473 loss)
I1210 14:46:55.842953 15352 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1210 14:47:01.503933 15352 solver.cpp:218] Iteration 91700 (17.6643 iter/s, 5.66113s/100 iters), loss = 0.481639
I1210 14:47:01.503933 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 14:47:01.503933 15352 solver.cpp:237]     Train net output #1: loss = 0.481639 (* 1 = 0.481639 loss)
I1210 14:47:01.503933 15352 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1210 14:47:07.178462 15352 solver.cpp:218] Iteration 91800 (17.6256 iter/s, 5.67358s/100 iters), loss = 0.760255
I1210 14:47:07.178462 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 14:47:07.178462 15352 solver.cpp:237]     Train net output #1: loss = 0.760255 (* 1 = 0.760255 loss)
I1210 14:47:07.178462 15352 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1210 14:47:12.810915 15352 solver.cpp:218] Iteration 91900 (17.7543 iter/s, 5.63244s/100 iters), loss = 0.82661
I1210 14:47:12.810915 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 14:47:12.810915 15352 solver.cpp:237]     Train net output #1: loss = 0.82661 (* 1 = 0.82661 loss)
I1210 14:47:12.810915 15352 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1210 14:47:18.163525 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:47:18.384572 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_92000.caffemodel
I1210 14:47:18.399572 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_92000.solverstate
I1210 14:47:18.404587 15352 solver.cpp:330] Iteration 92000, Testing net (#0)
I1210 14:47:18.404587 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:47:19.769304 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:47:19.823320 15352 solver.cpp:397]     Test net output #0: accuracy = 0.5695
I1210 14:47:19.823320 15352 solver.cpp:397]     Test net output #1: loss = 1.63311 (* 1 = 1.63311 loss)
I1210 14:47:19.877354 15352 solver.cpp:218] Iteration 92000 (14.1527 iter/s, 7.06577s/100 iters), loss = 0.593097
I1210 14:47:19.877354 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:47:19.877354 15352 solver.cpp:237]     Train net output #1: loss = 0.593097 (* 1 = 0.593097 loss)
I1210 14:47:19.877354 15352 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1210 14:47:25.499750 15352 solver.cpp:218] Iteration 92100 (17.789 iter/s, 5.62146s/100 iters), loss = 0.913936
I1210 14:47:25.499750 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 14:47:25.499750 15352 solver.cpp:237]     Train net output #1: loss = 0.913936 (* 1 = 0.913936 loss)
I1210 14:47:25.499750 15352 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1210 14:47:31.128784 15352 solver.cpp:218] Iteration 92200 (17.7659 iter/s, 5.62876s/100 iters), loss = 0.674299
I1210 14:47:31.128784 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 14:47:31.128784 15352 solver.cpp:237]     Train net output #1: loss = 0.674299 (* 1 = 0.674299 loss)
I1210 14:47:31.128784 15352 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1210 14:47:36.760285 15352 solver.cpp:218] Iteration 92300 (17.7567 iter/s, 5.63168s/100 iters), loss = 0.779872
I1210 14:47:36.760285 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 14:47:36.760285 15352 solver.cpp:237]     Train net output #1: loss = 0.779872 (* 1 = 0.779872 loss)
I1210 14:47:36.760285 15352 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1210 14:47:42.426105 15352 solver.cpp:218] Iteration 92400 (17.6523 iter/s, 5.66498s/100 iters), loss = 0.698158
I1210 14:47:42.426105 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 14:47:42.426105 15352 solver.cpp:237]     Train net output #1: loss = 0.698158 (* 1 = 0.698158 loss)
I1210 14:47:42.426105 15352 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1210 14:47:47.831697 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:47:48.055215 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_92500.caffemodel
I1210 14:47:48.072721 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_92500.solverstate
I1210 14:47:48.077721 15352 solver.cpp:330] Iteration 92500, Testing net (#0)
I1210 14:47:48.077721 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:47:49.453835 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:47:49.506836 15352 solver.cpp:397]     Test net output #0: accuracy = 0.5938
I1210 14:47:49.506836 15352 solver.cpp:397]     Test net output #1: loss = 1.54407 (* 1 = 1.54407 loss)
I1210 14:47:49.562845 15352 solver.cpp:218] Iteration 92500 (14.0122 iter/s, 7.13662s/100 iters), loss = 0.693349
I1210 14:47:49.563848 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 14:47:49.563848 15352 solver.cpp:237]     Train net output #1: loss = 0.693349 (* 1 = 0.693349 loss)
I1210 14:47:49.563848 15352 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1210 14:47:55.246553 15352 solver.cpp:218] Iteration 92600 (17.5975 iter/s, 5.68262s/100 iters), loss = 0.787561
I1210 14:47:55.246553 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 14:47:55.246553 15352 solver.cpp:237]     Train net output #1: loss = 0.787561 (* 1 = 0.787561 loss)
I1210 14:47:55.246553 15352 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1210 14:48:00.925503 15352 solver.cpp:218] Iteration 92700 (17.6099 iter/s, 5.67862s/100 iters), loss = 0.537572
I1210 14:48:00.925503 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 14:48:00.925503 15352 solver.cpp:237]     Train net output #1: loss = 0.537572 (* 1 = 0.537572 loss)
I1210 14:48:00.925503 15352 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1210 14:48:06.589669 15352 solver.cpp:218] Iteration 92800 (17.6564 iter/s, 5.66366s/100 iters), loss = 0.725072
I1210 14:48:06.589669 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 14:48:06.589669 15352 solver.cpp:237]     Train net output #1: loss = 0.725072 (* 1 = 0.725072 loss)
I1210 14:48:06.589669 15352 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1210 14:48:12.254535 15352 solver.cpp:218] Iteration 92900 (17.6548 iter/s, 5.66419s/100 iters), loss = 0.790324
I1210 14:48:12.254535 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 14:48:12.254535 15352 solver.cpp:237]     Train net output #1: loss = 0.790324 (* 1 = 0.790324 loss)
I1210 14:48:12.254535 15352 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1210 14:48:17.633520 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:48:17.858074 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_93000.caffemodel
I1210 14:48:17.874563 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_93000.solverstate
I1210 14:48:17.879581 15352 solver.cpp:330] Iteration 93000, Testing net (#0)
I1210 14:48:17.879581 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:48:19.267225 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:48:19.320734 15352 solver.cpp:397]     Test net output #0: accuracy = 0.5997
I1210 14:48:19.320734 15352 solver.cpp:397]     Test net output #1: loss = 1.52851 (* 1 = 1.52851 loss)
I1210 14:48:19.372772 15352 solver.cpp:218] Iteration 93000 (14.0482 iter/s, 7.11833s/100 iters), loss = 0.665989
I1210 14:48:19.372772 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 14:48:19.372772 15352 solver.cpp:237]     Train net output #1: loss = 0.665989 (* 1 = 0.665989 loss)
I1210 14:48:19.372772 15352 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1210 14:48:25.021281 15352 solver.cpp:218] Iteration 93100 (17.7048 iter/s, 5.64818s/100 iters), loss = 0.692495
I1210 14:48:25.021281 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 14:48:25.021281 15352 solver.cpp:237]     Train net output #1: loss = 0.692495 (* 1 = 0.692495 loss)
I1210 14:48:25.021281 15352 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1210 14:48:30.676861 15352 solver.cpp:218] Iteration 93200 (17.6833 iter/s, 5.65505s/100 iters), loss = 0.571368
I1210 14:48:30.676861 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 14:48:30.676861 15352 solver.cpp:237]     Train net output #1: loss = 0.571368 (* 1 = 0.571368 loss)
I1210 14:48:30.676861 15352 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1210 14:48:36.321400 15352 solver.cpp:218] Iteration 93300 (17.7198 iter/s, 5.64339s/100 iters), loss = 0.902248
I1210 14:48:36.321400 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1210 14:48:36.321400 15352 solver.cpp:237]     Train net output #1: loss = 0.902248 (* 1 = 0.902248 loss)
I1210 14:48:36.321400 15352 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1210 14:48:41.979869 15352 solver.cpp:218] Iteration 93400 (17.6729 iter/s, 5.65837s/100 iters), loss = 0.788026
I1210 14:48:41.979869 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 14:48:41.979869 15352 solver.cpp:237]     Train net output #1: loss = 0.788026 (* 1 = 0.788026 loss)
I1210 14:48:41.979869 15352 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1210 14:48:47.365692 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:48:47.586210 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_93500.caffemodel
I1210 14:48:47.601210 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_93500.solverstate
I1210 14:48:47.606210 15352 solver.cpp:330] Iteration 93500, Testing net (#0)
I1210 14:48:47.606210 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:48:48.984380 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:48:49.038379 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6078
I1210 14:48:49.038379 15352 solver.cpp:397]     Test net output #1: loss = 1.48927 (* 1 = 1.48927 loss)
I1210 14:48:49.092391 15352 solver.cpp:218] Iteration 93500 (14.062 iter/s, 7.11138s/100 iters), loss = 0.69182
I1210 14:48:49.092391 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 14:48:49.092391 15352 solver.cpp:237]     Train net output #1: loss = 0.69182 (* 1 = 0.69182 loss)
I1210 14:48:49.092391 15352 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1210 14:48:54.744838 15352 solver.cpp:218] Iteration 93600 (17.6913 iter/s, 5.6525s/100 iters), loss = 0.632917
I1210 14:48:54.744838 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 14:48:54.744838 15352 solver.cpp:237]     Train net output #1: loss = 0.632917 (* 1 = 0.632917 loss)
I1210 14:48:54.744838 15352 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1210 14:49:00.397394 15352 solver.cpp:218] Iteration 93700 (17.6929 iter/s, 5.65199s/100 iters), loss = 0.424647
I1210 14:49:00.397394 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 14:49:00.397394 15352 solver.cpp:237]     Train net output #1: loss = 0.424647 (* 1 = 0.424647 loss)
I1210 14:49:00.397394 15352 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1210 14:49:06.072815 15352 solver.cpp:218] Iteration 93800 (17.621 iter/s, 5.67504s/100 iters), loss = 0.804963
I1210 14:49:06.073315 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 14:49:06.073315 15352 solver.cpp:237]     Train net output #1: loss = 0.804963 (* 1 = 0.804963 loss)
I1210 14:49:06.073315 15352 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1210 14:49:11.711781 15352 solver.cpp:218] Iteration 93900 (17.7338 iter/s, 5.63894s/100 iters), loss = 0.943557
I1210 14:49:11.712781 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1210 14:49:11.712781 15352 solver.cpp:237]     Train net output #1: loss = 0.943557 (* 1 = 0.943557 loss)
I1210 14:49:11.712781 15352 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1210 14:49:17.101362 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:49:17.326382 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_94000.caffemodel
I1210 14:49:17.341382 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_94000.solverstate
I1210 14:49:17.345381 15352 solver.cpp:330] Iteration 94000, Testing net (#0)
I1210 14:49:17.345381 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:49:18.719568 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:49:18.774072 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6085
I1210 14:49:18.774072 15352 solver.cpp:397]     Test net output #1: loss = 1.46546 (* 1 = 1.46546 loss)
I1210 14:49:18.828577 15352 solver.cpp:218] Iteration 94000 (14.0522 iter/s, 7.11631s/100 iters), loss = 0.590627
I1210 14:49:18.829577 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 14:49:18.829577 15352 solver.cpp:237]     Train net output #1: loss = 0.590627 (* 1 = 0.590627 loss)
I1210 14:49:18.829577 15352 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1210 14:49:24.502961 15352 solver.cpp:218] Iteration 94100 (17.6268 iter/s, 5.67318s/100 iters), loss = 0.685196
I1210 14:49:24.502961 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 14:49:24.502961 15352 solver.cpp:237]     Train net output #1: loss = 0.685196 (* 1 = 0.685196 loss)
I1210 14:49:24.502961 15352 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1210 14:49:30.146366 15352 solver.cpp:218] Iteration 94200 (17.7192 iter/s, 5.6436s/100 iters), loss = 0.459943
I1210 14:49:30.146366 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 14:49:30.147367 15352 solver.cpp:237]     Train net output #1: loss = 0.459943 (* 1 = 0.459943 loss)
I1210 14:49:30.147367 15352 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1210 14:49:35.800853 15352 solver.cpp:218] Iteration 94300 (17.6875 iter/s, 5.65372s/100 iters), loss = 0.897908
I1210 14:49:35.800853 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1210 14:49:35.800853 15352 solver.cpp:237]     Train net output #1: loss = 0.897908 (* 1 = 0.897908 loss)
I1210 14:49:35.800853 15352 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1210 14:49:41.444284 15352 solver.cpp:218] Iteration 94400 (17.7209 iter/s, 5.64306s/100 iters), loss = 0.712783
I1210 14:49:41.444284 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 14:49:41.444284 15352 solver.cpp:237]     Train net output #1: loss = 0.712783 (* 1 = 0.712783 loss)
I1210 14:49:41.444284 15352 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1210 14:49:46.823812 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:49:47.045831 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_94500.caffemodel
I1210 14:49:47.060827 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_94500.solverstate
I1210 14:49:47.066838 15352 solver.cpp:330] Iteration 94500, Testing net (#0)
I1210 14:49:47.066838 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:49:48.434787 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:49:48.489796 15352 solver.cpp:397]     Test net output #0: accuracy = 0.5886
I1210 14:49:48.489796 15352 solver.cpp:397]     Test net output #1: loss = 1.58757 (* 1 = 1.58757 loss)
I1210 14:49:48.542796 15352 solver.cpp:218] Iteration 94500 (14.0892 iter/s, 7.09764s/100 iters), loss = 0.672122
I1210 14:49:48.542796 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 14:49:48.542796 15352 solver.cpp:237]     Train net output #1: loss = 0.672122 (* 1 = 0.672122 loss)
I1210 14:49:48.542796 15352 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1210 14:49:54.193228 15352 solver.cpp:218] Iteration 94600 (17.7002 iter/s, 5.64966s/100 iters), loss = 0.66784
I1210 14:49:54.193228 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 14:49:54.193228 15352 solver.cpp:237]     Train net output #1: loss = 0.66784 (* 1 = 0.66784 loss)
I1210 14:49:54.193228 15352 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1210 14:49:59.843767 15352 solver.cpp:218] Iteration 94700 (17.6987 iter/s, 5.65013s/100 iters), loss = 0.543673
I1210 14:49:59.843767 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 14:49:59.843767 15352 solver.cpp:237]     Train net output #1: loss = 0.543673 (* 1 = 0.543673 loss)
I1210 14:49:59.843767 15352 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1210 14:50:05.514737 15352 solver.cpp:218] Iteration 94800 (17.6325 iter/s, 5.67135s/100 iters), loss = 0.69611
I1210 14:50:05.514737 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 14:50:05.514737 15352 solver.cpp:237]     Train net output #1: loss = 0.69611 (* 1 = 0.69611 loss)
I1210 14:50:05.514737 15352 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1210 14:50:11.164279 15352 solver.cpp:218] Iteration 94900 (17.7044 iter/s, 5.6483s/100 iters), loss = 0.814744
I1210 14:50:11.164279 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 14:50:11.164279 15352 solver.cpp:237]     Train net output #1: loss = 0.814744 (* 1 = 0.814744 loss)
I1210 14:50:11.164279 15352 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1210 14:50:16.520706 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:50:16.741719 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_95000.caffemodel
I1210 14:50:16.756721 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_95000.solverstate
I1210 14:50:16.764721 15352 solver.cpp:330] Iteration 95000, Testing net (#0)
I1210 14:50:16.764721 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:50:18.132822 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:50:18.186833 15352 solver.cpp:397]     Test net output #0: accuracy = 0.5737
I1210 14:50:18.186833 15352 solver.cpp:397]     Test net output #1: loss = 1.67993 (* 1 = 1.67993 loss)
I1210 14:50:18.239827 15352 solver.cpp:218] Iteration 95000 (14.1337 iter/s, 7.0753s/100 iters), loss = 0.566593
I1210 14:50:18.239827 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 14:50:18.239827 15352 solver.cpp:237]     Train net output #1: loss = 0.566593 (* 1 = 0.566593 loss)
I1210 14:50:18.239827 15352 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1210 14:50:18.239827 15352 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1210 14:50:23.871769 15352 solver.cpp:218] Iteration 95100 (17.7586 iter/s, 5.63107s/100 iters), loss = 0.673073
I1210 14:50:23.871769 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 14:50:23.871769 15352 solver.cpp:237]     Train net output #1: loss = 0.673073 (* 1 = 0.673073 loss)
I1210 14:50:23.871769 15352 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1210 14:50:29.532080 15352 solver.cpp:218] Iteration 95200 (17.6669 iter/s, 5.66031s/100 iters), loss = 0.38733
I1210 14:50:29.532080 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 14:50:29.532080 15352 solver.cpp:237]     Train net output #1: loss = 0.38733 (* 1 = 0.38733 loss)
I1210 14:50:29.532080 15352 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1210 14:50:35.195606 15352 solver.cpp:218] Iteration 95300 (17.6588 iter/s, 5.66289s/100 iters), loss = 0.618757
I1210 14:50:35.195606 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:50:35.195606 15352 solver.cpp:237]     Train net output #1: loss = 0.618757 (* 1 = 0.618757 loss)
I1210 14:50:35.195606 15352 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1210 14:50:40.862298 15352 solver.cpp:218] Iteration 95400 (17.6481 iter/s, 5.66632s/100 iters), loss = 0.569348
I1210 14:50:40.862298 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 14:50:40.862298 15352 solver.cpp:237]     Train net output #1: loss = 0.569348 (* 1 = 0.569348 loss)
I1210 14:50:40.862298 15352 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1210 14:50:46.241775 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:50:46.467795 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_95500.caffemodel
I1210 14:50:46.483302 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_95500.solverstate
I1210 14:50:46.487807 15352 solver.cpp:330] Iteration 95500, Testing net (#0)
I1210 14:50:46.487807 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:50:47.873935 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:50:47.926940 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6699
I1210 14:50:47.926940 15352 solver.cpp:397]     Test net output #1: loss = 1.1934 (* 1 = 1.1934 loss)
I1210 14:50:47.981947 15352 solver.cpp:218] Iteration 95500 (14.0473 iter/s, 7.11882s/100 iters), loss = 0.548301
I1210 14:50:47.981947 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 14:50:47.981947 15352 solver.cpp:237]     Train net output #1: loss = 0.548301 (* 1 = 0.548301 loss)
I1210 14:50:47.981947 15352 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1210 14:50:53.658505 15352 solver.cpp:218] Iteration 95600 (17.6162 iter/s, 5.67658s/100 iters), loss = 0.512051
I1210 14:50:53.658505 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 14:50:53.658505 15352 solver.cpp:237]     Train net output #1: loss = 0.512051 (* 1 = 0.512051 loss)
I1210 14:50:53.658505 15352 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1210 14:50:59.330159 15352 solver.cpp:218] Iteration 95700 (17.6342 iter/s, 5.6708s/100 iters), loss = 0.359706
I1210 14:50:59.330159 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 14:50:59.330159 15352 solver.cpp:237]     Train net output #1: loss = 0.359706 (* 1 = 0.359706 loss)
I1210 14:50:59.330159 15352 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1210 14:51:04.997670 15352 solver.cpp:218] Iteration 95800 (17.6446 iter/s, 5.66744s/100 iters), loss = 0.501119
I1210 14:51:04.997670 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:51:04.997670 15352 solver.cpp:237]     Train net output #1: loss = 0.501119 (* 1 = 0.501119 loss)
I1210 14:51:04.997670 15352 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1210 14:51:10.649106 15352 solver.cpp:218] Iteration 95900 (17.6968 iter/s, 5.65075s/100 iters), loss = 0.605418
I1210 14:51:10.649106 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 14:51:10.649106 15352 solver.cpp:237]     Train net output #1: loss = 0.605418 (* 1 = 0.605418 loss)
I1210 14:51:10.649106 15352 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1210 14:51:16.053423 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:51:16.277940 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_96000.caffemodel
I1210 14:51:16.292443 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_96000.solverstate
I1210 14:51:16.297442 15352 solver.cpp:330] Iteration 96000, Testing net (#0)
I1210 14:51:16.297442 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:51:17.666551 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:51:17.719555 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6757
I1210 14:51:17.719555 15352 solver.cpp:397]     Test net output #1: loss = 1.17749 (* 1 = 1.17749 loss)
I1210 14:51:17.775058 15352 solver.cpp:218] Iteration 96000 (14.0349 iter/s, 7.12509s/100 iters), loss = 0.509731
I1210 14:51:17.775058 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 14:51:17.775058 15352 solver.cpp:237]     Train net output #1: loss = 0.509731 (* 1 = 0.509731 loss)
I1210 14:51:17.775058 15352 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1210 14:51:23.447263 15352 solver.cpp:218] Iteration 96100 (17.6306 iter/s, 5.67197s/100 iters), loss = 0.567533
I1210 14:51:23.447263 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 14:51:23.447263 15352 solver.cpp:237]     Train net output #1: loss = 0.567533 (* 1 = 0.567533 loss)
I1210 14:51:23.447263 15352 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1210 14:51:29.107091 15352 solver.cpp:218] Iteration 96200 (17.6692 iter/s, 5.65956s/100 iters), loss = 0.32385
I1210 14:51:29.107091 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 14:51:29.107091 15352 solver.cpp:237]     Train net output #1: loss = 0.32385 (* 1 = 0.32385 loss)
I1210 14:51:29.107091 15352 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1210 14:51:34.785542 15352 solver.cpp:218] Iteration 96300 (17.613 iter/s, 5.67763s/100 iters), loss = 0.470922
I1210 14:51:34.786053 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 14:51:34.786053 15352 solver.cpp:237]     Train net output #1: loss = 0.470922 (* 1 = 0.470922 loss)
I1210 14:51:34.786053 15352 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1210 14:51:40.463054 15352 solver.cpp:218] Iteration 96400 (17.6154 iter/s, 5.67683s/100 iters), loss = 0.582893
I1210 14:51:40.463054 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 14:51:40.463054 15352 solver.cpp:237]     Train net output #1: loss = 0.582893 (* 1 = 0.582893 loss)
I1210 14:51:40.463054 15352 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1210 14:51:45.889590 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:51:46.110604 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_96500.caffemodel
I1210 14:51:46.125603 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_96500.solverstate
I1210 14:51:46.130604 15352 solver.cpp:330] Iteration 96500, Testing net (#0)
I1210 14:51:46.130604 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:51:47.516780 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:51:47.569779 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6752
I1210 14:51:47.570780 15352 solver.cpp:397]     Test net output #1: loss = 1.18049 (* 1 = 1.18049 loss)
I1210 14:51:47.623785 15352 solver.cpp:218] Iteration 96500 (13.9649 iter/s, 7.16079s/100 iters), loss = 0.455564
I1210 14:51:47.623785 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 14:51:47.624788 15352 solver.cpp:237]     Train net output #1: loss = 0.455564 (* 1 = 0.455564 loss)
I1210 14:51:47.624788 15352 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1210 14:51:53.273340 15352 solver.cpp:218] Iteration 96600 (17.7044 iter/s, 5.64833s/100 iters), loss = 0.46039
I1210 14:51:53.273340 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:51:53.273340 15352 solver.cpp:237]     Train net output #1: loss = 0.46039 (* 1 = 0.46039 loss)
I1210 14:51:53.273340 15352 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1210 14:51:58.936833 15352 solver.cpp:218] Iteration 96700 (17.6565 iter/s, 5.66364s/100 iters), loss = 0.265437
I1210 14:51:58.937832 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 14:51:58.937832 15352 solver.cpp:237]     Train net output #1: loss = 0.265437 (* 1 = 0.265437 loss)
I1210 14:51:58.937832 15352 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1210 14:52:04.585711 15352 solver.cpp:218] Iteration 96800 (17.7065 iter/s, 5.64764s/100 iters), loss = 0.540246
I1210 14:52:04.585711 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 14:52:04.585711 15352 solver.cpp:237]     Train net output #1: loss = 0.540246 (* 1 = 0.540246 loss)
I1210 14:52:04.585711 15352 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1210 14:52:10.222683 15352 solver.cpp:218] Iteration 96900 (17.7396 iter/s, 5.6371s/100 iters), loss = 0.628385
I1210 14:52:10.222683 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 14:52:10.222683 15352 solver.cpp:237]     Train net output #1: loss = 0.628385 (* 1 = 0.628385 loss)
I1210 14:52:10.222683 15352 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1210 14:52:15.591548 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:52:15.813056 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_97000.caffemodel
I1210 14:52:15.830056 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_97000.solverstate
I1210 14:52:15.835057 15352 solver.cpp:330] Iteration 97000, Testing net (#0)
I1210 14:52:15.835057 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:52:17.204175 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:52:17.256175 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6775
I1210 14:52:17.256175 15352 solver.cpp:397]     Test net output #1: loss = 1.17377 (* 1 = 1.17377 loss)
I1210 14:52:17.309180 15352 solver.cpp:218] Iteration 97000 (14.1119 iter/s, 7.08622s/100 iters), loss = 0.422567
I1210 14:52:17.310189 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:52:17.310189 15352 solver.cpp:237]     Train net output #1: loss = 0.422567 (* 1 = 0.422567 loss)
I1210 14:52:17.310189 15352 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1210 14:52:22.949717 15352 solver.cpp:218] Iteration 97100 (17.7313 iter/s, 5.63976s/100 iters), loss = 0.572649
I1210 14:52:22.949717 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 14:52:22.949717 15352 solver.cpp:237]     Train net output #1: loss = 0.572649 (* 1 = 0.572649 loss)
I1210 14:52:22.949717 15352 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1210 14:52:28.604187 15352 solver.cpp:218] Iteration 97200 (17.6873 iter/s, 5.65379s/100 iters), loss = 0.297562
I1210 14:52:28.604187 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 14:52:28.604187 15352 solver.cpp:237]     Train net output #1: loss = 0.297562 (* 1 = 0.297562 loss)
I1210 14:52:28.604187 15352 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1210 14:52:34.243811 15352 solver.cpp:218] Iteration 97300 (17.7348 iter/s, 5.63863s/100 iters), loss = 0.459514
I1210 14:52:34.243811 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 14:52:34.243811 15352 solver.cpp:237]     Train net output #1: loss = 0.459514 (* 1 = 0.459514 loss)
I1210 14:52:34.243811 15352 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1210 14:52:39.887495 15352 solver.cpp:218] Iteration 97400 (17.7186 iter/s, 5.64378s/100 iters), loss = 0.520252
I1210 14:52:39.887495 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 14:52:39.887495 15352 solver.cpp:237]     Train net output #1: loss = 0.520252 (* 1 = 0.520252 loss)
I1210 14:52:39.887495 15352 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1210 14:52:45.256506 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:52:45.482050 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_97500.caffemodel
I1210 14:52:45.496050 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_97500.solverstate
I1210 14:52:45.501055 15352 solver.cpp:330] Iteration 97500, Testing net (#0)
I1210 14:52:45.501055 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:52:46.870306 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:52:46.922974 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6785
I1210 14:52:46.922974 15352 solver.cpp:397]     Test net output #1: loss = 1.17648 (* 1 = 1.17648 loss)
I1210 14:52:46.978971 15352 solver.cpp:218] Iteration 97500 (14.1035 iter/s, 7.09044s/100 iters), loss = 0.476442
I1210 14:52:46.978971 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:52:46.978971 15352 solver.cpp:237]     Train net output #1: loss = 0.476442 (* 1 = 0.476442 loss)
I1210 14:52:46.978971 15352 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1210 14:52:52.617885 15352 solver.cpp:218] Iteration 97600 (17.7345 iter/s, 5.63874s/100 iters), loss = 0.469562
I1210 14:52:52.617885 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:52:52.617885 15352 solver.cpp:237]     Train net output #1: loss = 0.469562 (* 1 = 0.469562 loss)
I1210 14:52:52.617885 15352 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1210 14:52:58.275450 15352 solver.cpp:218] Iteration 97700 (17.6758 iter/s, 5.65745s/100 iters), loss = 0.321386
I1210 14:52:58.275450 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 14:52:58.275450 15352 solver.cpp:237]     Train net output #1: loss = 0.321386 (* 1 = 0.321386 loss)
I1210 14:52:58.275450 15352 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1210 14:53:03.910573 15352 solver.cpp:218] Iteration 97800 (17.7496 iter/s, 5.63394s/100 iters), loss = 0.43955
I1210 14:53:03.910573 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:53:03.910573 15352 solver.cpp:237]     Train net output #1: loss = 0.43955 (* 1 = 0.43955 loss)
I1210 14:53:03.910573 15352 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1210 14:53:09.581204 15352 solver.cpp:218] Iteration 97900 (17.636 iter/s, 5.67023s/100 iters), loss = 0.51734
I1210 14:53:09.581204 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:53:09.581204 15352 solver.cpp:237]     Train net output #1: loss = 0.51734 (* 1 = 0.51734 loss)
I1210 14:53:09.581204 15352 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1210 14:53:14.964985 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:53:15.187000 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_98000.caffemodel
I1210 14:53:15.202002 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_98000.solverstate
I1210 14:53:15.207000 15352 solver.cpp:330] Iteration 98000, Testing net (#0)
I1210 14:53:15.207000 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:53:16.580121 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:53:16.635128 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6794
I1210 14:53:16.635128 15352 solver.cpp:397]     Test net output #1: loss = 1.17642 (* 1 = 1.17642 loss)
I1210 14:53:16.688127 15352 solver.cpp:218] Iteration 98000 (14.0702 iter/s, 7.10722s/100 iters), loss = 0.409727
I1210 14:53:16.689128 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:53:16.689128 15352 solver.cpp:237]     Train net output #1: loss = 0.409727 (* 1 = 0.409727 loss)
I1210 14:53:16.689128 15352 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1210 14:53:22.344633 15352 solver.cpp:218] Iteration 98100 (17.6808 iter/s, 5.65585s/100 iters), loss = 0.460528
I1210 14:53:22.344633 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 14:53:22.344633 15352 solver.cpp:237]     Train net output #1: loss = 0.460528 (* 1 = 0.460528 loss)
I1210 14:53:22.344633 15352 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1210 14:53:27.975312 15352 solver.cpp:218] Iteration 98200 (17.7609 iter/s, 5.63035s/100 iters), loss = 0.281282
I1210 14:53:27.976313 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 14:53:27.976313 15352 solver.cpp:237]     Train net output #1: loss = 0.281282 (* 1 = 0.281282 loss)
I1210 14:53:27.976313 15352 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1210 14:53:33.649829 15352 solver.cpp:218] Iteration 98300 (17.6264 iter/s, 5.67331s/100 iters), loss = 0.571617
I1210 14:53:33.649829 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 14:53:33.649829 15352 solver.cpp:237]     Train net output #1: loss = 0.571617 (* 1 = 0.571617 loss)
I1210 14:53:33.649829 15352 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1210 14:53:39.309911 15352 solver.cpp:218] Iteration 98400 (17.6672 iter/s, 5.66022s/100 iters), loss = 0.600927
I1210 14:53:39.309911 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 14:53:39.309911 15352 solver.cpp:237]     Train net output #1: loss = 0.600927 (* 1 = 0.600927 loss)
I1210 14:53:39.309911 15352 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1210 14:53:44.656584 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:53:44.876065 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_98500.caffemodel
I1210 14:53:44.891067 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_98500.solverstate
I1210 14:53:44.896065 15352 solver.cpp:330] Iteration 98500, Testing net (#0)
I1210 14:53:44.896065 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:53:46.257169 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:53:46.313168 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6797
I1210 14:53:46.313168 15352 solver.cpp:397]     Test net output #1: loss = 1.17349 (* 1 = 1.17349 loss)
I1210 14:53:46.366209 15352 solver.cpp:218] Iteration 98500 (14.1736 iter/s, 7.05538s/100 iters), loss = 0.397387
I1210 14:53:46.366209 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 14:53:46.366209 15352 solver.cpp:237]     Train net output #1: loss = 0.397387 (* 1 = 0.397387 loss)
I1210 14:53:46.366209 15352 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1210 14:53:51.984838 15352 solver.cpp:218] Iteration 98600 (17.7991 iter/s, 5.61826s/100 iters), loss = 0.494658
I1210 14:53:51.984838 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 14:53:51.984838 15352 solver.cpp:237]     Train net output #1: loss = 0.494658 (* 1 = 0.494658 loss)
I1210 14:53:51.984838 15352 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1210 14:53:57.599426 15352 solver.cpp:218] Iteration 98700 (17.8111 iter/s, 5.61448s/100 iters), loss = 0.348942
I1210 14:53:57.599426 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 14:53:57.599426 15352 solver.cpp:237]     Train net output #1: loss = 0.348942 (* 1 = 0.348942 loss)
I1210 14:53:57.599426 15352 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1210 14:54:03.227269 15352 solver.cpp:218] Iteration 98800 (17.7711 iter/s, 5.62711s/100 iters), loss = 0.468919
I1210 14:54:03.227269 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:54:03.227269 15352 solver.cpp:237]     Train net output #1: loss = 0.468919 (* 1 = 0.468919 loss)
I1210 14:54:03.227269 15352 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1210 14:54:08.884129 15352 solver.cpp:218] Iteration 98900 (17.6779 iter/s, 5.65679s/100 iters), loss = 0.553368
I1210 14:54:08.884129 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 14:54:08.884129 15352 solver.cpp:237]     Train net output #1: loss = 0.553368 (* 1 = 0.553368 loss)
I1210 14:54:08.884129 15352 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1210 14:54:14.257856 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:54:14.480649 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_99000.caffemodel
I1210 14:54:14.498152 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_99000.solverstate
I1210 14:54:14.503152 15352 solver.cpp:330] Iteration 99000, Testing net (#0)
I1210 14:54:14.503152 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:54:15.876580 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:54:15.931164 15352 solver.cpp:397]     Test net output #0: accuracy = 0.676
I1210 14:54:15.931164 15352 solver.cpp:397]     Test net output #1: loss = 1.19209 (* 1 = 1.19209 loss)
I1210 14:54:15.984163 15352 solver.cpp:218] Iteration 99000 (14.0858 iter/s, 7.09937s/100 iters), loss = 0.467614
I1210 14:54:15.984163 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 14:54:15.984163 15352 solver.cpp:237]     Train net output #1: loss = 0.467614 (* 1 = 0.467614 loss)
I1210 14:54:15.984163 15352 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1210 14:54:21.635483 15352 solver.cpp:218] Iteration 99100 (17.6972 iter/s, 5.6506s/100 iters), loss = 0.430588
I1210 14:54:21.635483 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:54:21.635483 15352 solver.cpp:237]     Train net output #1: loss = 0.430588 (* 1 = 0.430588 loss)
I1210 14:54:21.635483 15352 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1210 14:54:27.278901 15352 solver.cpp:218] Iteration 99200 (17.7209 iter/s, 5.64304s/100 iters), loss = 0.338869
I1210 14:54:27.278901 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 14:54:27.278901 15352 solver.cpp:237]     Train net output #1: loss = 0.338869 (* 1 = 0.338869 loss)
I1210 14:54:27.278901 15352 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1210 14:54:32.923327 15352 solver.cpp:218] Iteration 99300 (17.718 iter/s, 5.64398s/100 iters), loss = 0.450947
I1210 14:54:32.923327 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 14:54:32.923327 15352 solver.cpp:237]     Train net output #1: loss = 0.450947 (* 1 = 0.450947 loss)
I1210 14:54:32.923327 15352 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1210 14:54:38.560766 15352 solver.cpp:218] Iteration 99400 (17.7402 iter/s, 5.63691s/100 iters), loss = 0.479671
I1210 14:54:38.560766 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 14:54:38.560766 15352 solver.cpp:237]     Train net output #1: loss = 0.479671 (* 1 = 0.479671 loss)
I1210 14:54:38.560766 15352 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1210 14:54:43.990432 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:54:44.220466 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_99500.caffemodel
I1210 14:54:44.235466 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_99500.solverstate
I1210 14:54:44.240468 15352 solver.cpp:330] Iteration 99500, Testing net (#0)
I1210 14:54:44.240468 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:54:45.661664 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:54:45.715672 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1210 14:54:45.715672 15352 solver.cpp:397]     Test net output #1: loss = 1.18491 (* 1 = 1.18491 loss)
I1210 14:54:45.769675 15352 solver.cpp:218] Iteration 99500 (13.8739 iter/s, 7.20777s/100 iters), loss = 0.362928
I1210 14:54:45.769675 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 14:54:45.769675 15352 solver.cpp:237]     Train net output #1: loss = 0.362928 (* 1 = 0.362928 loss)
I1210 14:54:45.769675 15352 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1210 14:54:51.516983 15352 solver.cpp:218] Iteration 99600 (17.4005 iter/s, 5.74697s/100 iters), loss = 0.422642
I1210 14:54:51.516983 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:54:51.516983 15352 solver.cpp:237]     Train net output #1: loss = 0.422642 (* 1 = 0.422642 loss)
I1210 14:54:51.516983 15352 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1210 14:54:57.230082 15352 solver.cpp:218] Iteration 99700 (17.5057 iter/s, 5.71242s/100 iters), loss = 0.303622
I1210 14:54:57.230082 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 14:54:57.230082 15352 solver.cpp:237]     Train net output #1: loss = 0.303622 (* 1 = 0.303622 loss)
I1210 14:54:57.230082 15352 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1210 14:55:02.882405 15352 solver.cpp:218] Iteration 99800 (17.6921 iter/s, 5.65223s/100 iters), loss = 0.393031
I1210 14:55:02.882405 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 14:55:02.882405 15352 solver.cpp:237]     Train net output #1: loss = 0.393031 (* 1 = 0.393031 loss)
I1210 14:55:02.882405 15352 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1210 14:55:08.545531 15352 solver.cpp:218] Iteration 99900 (17.6583 iter/s, 5.66306s/100 iters), loss = 0.464173
I1210 14:55:08.545531 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:55:08.545531 15352 solver.cpp:237]     Train net output #1: loss = 0.464173 (* 1 = 0.464173 loss)
I1210 14:55:08.545531 15352 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1210 14:55:13.911718 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:55:14.133976 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_100000.caffemodel
I1210 14:55:14.148105 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_100000.solverstate
I1210 14:55:14.153105 15352 solver.cpp:330] Iteration 100000, Testing net (#0)
I1210 14:55:14.153105 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:55:15.524628 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:55:15.577147 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6756
I1210 14:55:15.577147 15352 solver.cpp:397]     Test net output #1: loss = 1.1829 (* 1 = 1.1829 loss)
I1210 14:55:15.631165 15352 solver.cpp:218] Iteration 100000 (14.1157 iter/s, 7.0843s/100 iters), loss = 0.39656
I1210 14:55:15.631165 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 14:55:15.631165 15352 solver.cpp:237]     Train net output #1: loss = 0.39656 (* 1 = 0.39656 loss)
I1210 14:55:15.631165 15352 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1210 14:55:21.295872 15352 solver.cpp:218] Iteration 100100 (17.6533 iter/s, 5.66467s/100 iters), loss = 0.417997
I1210 14:55:21.295872 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 14:55:21.295872 15352 solver.cpp:237]     Train net output #1: loss = 0.417997 (* 1 = 0.417997 loss)
I1210 14:55:21.295872 15352 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1210 14:55:26.951431 15352 solver.cpp:218] Iteration 100200 (17.6824 iter/s, 5.65534s/100 iters), loss = 0.324897
I1210 14:55:26.951431 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 14:55:26.951431 15352 solver.cpp:237]     Train net output #1: loss = 0.324897 (* 1 = 0.324897 loss)
I1210 14:55:26.951431 15352 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1210 14:55:32.608930 15352 solver.cpp:218] Iteration 100300 (17.677 iter/s, 5.65708s/100 iters), loss = 0.450349
I1210 14:55:32.608930 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:55:32.608930 15352 solver.cpp:237]     Train net output #1: loss = 0.450349 (* 1 = 0.450349 loss)
I1210 14:55:32.608930 15352 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1210 14:55:38.264497 15352 solver.cpp:218] Iteration 100400 (17.6831 iter/s, 5.65512s/100 iters), loss = 0.517745
I1210 14:55:38.264497 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 14:55:38.264497 15352 solver.cpp:237]     Train net output #1: loss = 0.517745 (* 1 = 0.517745 loss)
I1210 14:55:38.264497 15352 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1210 14:55:43.677003 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:55:43.899015 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_100500.caffemodel
I1210 14:55:43.915014 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_100500.solverstate
I1210 14:55:43.920016 15352 solver.cpp:330] Iteration 100500, Testing net (#0)
I1210 14:55:43.920016 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:55:45.295161 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:55:45.350165 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6759
I1210 14:55:45.350165 15352 solver.cpp:397]     Test net output #1: loss = 1.19247 (* 1 = 1.19247 loss)
I1210 14:55:45.406167 15352 solver.cpp:218] Iteration 100500 (14.004 iter/s, 7.14082s/100 iters), loss = 0.346966
I1210 14:55:45.406167 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 14:55:45.406167 15352 solver.cpp:237]     Train net output #1: loss = 0.346966 (* 1 = 0.346966 loss)
I1210 14:55:45.406167 15352 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1210 14:55:51.083885 15352 solver.cpp:218] Iteration 100600 (17.6145 iter/s, 5.67713s/100 iters), loss = 0.43087
I1210 14:55:51.083885 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 14:55:51.083885 15352 solver.cpp:237]     Train net output #1: loss = 0.43087 (* 1 = 0.43087 loss)
I1210 14:55:51.083885 15352 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1210 14:55:56.739554 15352 solver.cpp:218] Iteration 100700 (17.6834 iter/s, 5.65502s/100 iters), loss = 0.277992
I1210 14:55:56.739554 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 14:55:56.739554 15352 solver.cpp:237]     Train net output #1: loss = 0.277992 (* 1 = 0.277992 loss)
I1210 14:55:56.739554 15352 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1210 14:56:02.422977 15352 solver.cpp:218] Iteration 100800 (17.5951 iter/s, 5.68341s/100 iters), loss = 0.469834
I1210 14:56:02.422977 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 14:56:02.422977 15352 solver.cpp:237]     Train net output #1: loss = 0.469834 (* 1 = 0.469834 loss)
I1210 14:56:02.422977 15352 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1210 14:56:08.062482 15352 solver.cpp:218] Iteration 100900 (17.7328 iter/s, 5.63926s/100 iters), loss = 0.424755
I1210 14:56:08.062482 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:56:08.062482 15352 solver.cpp:237]     Train net output #1: loss = 0.424755 (* 1 = 0.424755 loss)
I1210 14:56:08.062482 15352 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1210 14:56:13.429953 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:56:13.651968 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_101000.caffemodel
I1210 14:56:13.666967 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_101000.solverstate
I1210 14:56:13.671968 15352 solver.cpp:330] Iteration 101000, Testing net (#0)
I1210 14:56:13.671968 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:56:15.049077 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:56:15.103076 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6794
I1210 14:56:15.103076 15352 solver.cpp:397]     Test net output #1: loss = 1.1866 (* 1 = 1.1866 loss)
I1210 14:56:15.157083 15352 solver.cpp:218] Iteration 101000 (14.0976 iter/s, 7.09339s/100 iters), loss = 0.386004
I1210 14:56:15.157083 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 14:56:15.157083 15352 solver.cpp:237]     Train net output #1: loss = 0.386004 (* 1 = 0.386004 loss)
I1210 14:56:15.157083 15352 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1210 14:56:20.812544 15352 solver.cpp:218] Iteration 101100 (17.6832 iter/s, 5.65509s/100 iters), loss = 0.469432
I1210 14:56:20.812544 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:56:20.812544 15352 solver.cpp:237]     Train net output #1: loss = 0.469432 (* 1 = 0.469432 loss)
I1210 14:56:20.812544 15352 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1210 14:56:26.478145 15352 solver.cpp:218] Iteration 101200 (17.6517 iter/s, 5.66518s/100 iters), loss = 0.282861
I1210 14:56:26.478145 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 14:56:26.478145 15352 solver.cpp:237]     Train net output #1: loss = 0.282861 (* 1 = 0.282861 loss)
I1210 14:56:26.478145 15352 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1210 14:56:32.152896 15352 solver.cpp:218] Iteration 101300 (17.6226 iter/s, 5.67452s/100 iters), loss = 0.419157
I1210 14:56:32.152896 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:56:32.152896 15352 solver.cpp:237]     Train net output #1: loss = 0.419157 (* 1 = 0.419157 loss)
I1210 14:56:32.152896 15352 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1210 14:56:37.821279 15352 solver.cpp:218] Iteration 101400 (17.6441 iter/s, 5.66763s/100 iters), loss = 0.471666
I1210 14:56:37.821279 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:56:37.821279 15352 solver.cpp:237]     Train net output #1: loss = 0.471666 (* 1 = 0.471666 loss)
I1210 14:56:37.821279 15352 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1210 14:56:43.218964 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:56:43.441975 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_101500.caffemodel
I1210 14:56:43.456975 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_101500.solverstate
I1210 14:56:43.461977 15352 solver.cpp:330] Iteration 101500, Testing net (#0)
I1210 14:56:43.461977 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:56:44.829082 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:56:44.883088 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6783
I1210 14:56:44.883088 15352 solver.cpp:397]     Test net output #1: loss = 1.18802 (* 1 = 1.18802 loss)
I1210 14:56:44.936085 15352 solver.cpp:218] Iteration 101500 (14.0551 iter/s, 7.11487s/100 iters), loss = 0.318752
I1210 14:56:44.936085 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 14:56:44.936085 15352 solver.cpp:237]     Train net output #1: loss = 0.318752 (* 1 = 0.318752 loss)
I1210 14:56:44.936085 15352 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1210 14:56:50.614509 15352 solver.cpp:218] Iteration 101600 (17.6131 iter/s, 5.6776s/100 iters), loss = 0.373574
I1210 14:56:50.614509 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 14:56:50.614509 15352 solver.cpp:237]     Train net output #1: loss = 0.373574 (* 1 = 0.373574 loss)
I1210 14:56:50.614509 15352 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1210 14:56:56.305002 15352 solver.cpp:218] Iteration 101700 (17.5737 iter/s, 5.69031s/100 iters), loss = 0.297238
I1210 14:56:56.305002 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 14:56:56.305002 15352 solver.cpp:237]     Train net output #1: loss = 0.297238 (* 1 = 0.297238 loss)
I1210 14:56:56.305002 15352 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1210 14:57:02.000588 15352 solver.cpp:218] Iteration 101800 (17.5578 iter/s, 5.69549s/100 iters), loss = 0.382861
I1210 14:57:02.000588 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 14:57:02.000588 15352 solver.cpp:237]     Train net output #1: loss = 0.382861 (* 1 = 0.382861 loss)
I1210 14:57:02.001588 15352 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1210 14:57:07.705101 15352 solver.cpp:218] Iteration 101900 (17.5335 iter/s, 5.70337s/100 iters), loss = 0.513485
I1210 14:57:07.705101 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 14:57:07.705101 15352 solver.cpp:237]     Train net output #1: loss = 0.513485 (* 1 = 0.513485 loss)
I1210 14:57:07.705101 15352 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1210 14:57:13.129493 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:57:13.351034 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_102000.caffemodel
I1210 14:57:13.367031 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_102000.solverstate
I1210 14:57:13.372045 15352 solver.cpp:330] Iteration 102000, Testing net (#0)
I1210 14:57:13.372045 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:57:14.758368 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:57:14.814380 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6772
I1210 14:57:14.814380 15352 solver.cpp:397]     Test net output #1: loss = 1.20757 (* 1 = 1.20757 loss)
I1210 14:57:14.869384 15352 solver.cpp:218] Iteration 102000 (13.9576 iter/s, 7.16457s/100 iters), loss = 0.362851
I1210 14:57:14.870380 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 14:57:14.870380 15352 solver.cpp:237]     Train net output #1: loss = 0.362851 (* 1 = 0.362851 loss)
I1210 14:57:14.870380 15352 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1210 14:57:20.545791 15352 solver.cpp:218] Iteration 102100 (17.6206 iter/s, 5.67519s/100 iters), loss = 0.411992
I1210 14:57:20.545791 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:57:20.545791 15352 solver.cpp:237]     Train net output #1: loss = 0.411992 (* 1 = 0.411992 loss)
I1210 14:57:20.545791 15352 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1210 14:57:26.198887 15352 solver.cpp:218] Iteration 102200 (17.6914 iter/s, 5.65248s/100 iters), loss = 0.245164
I1210 14:57:26.198887 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 14:57:26.198887 15352 solver.cpp:237]     Train net output #1: loss = 0.245164 (* 1 = 0.245164 loss)
I1210 14:57:26.198887 15352 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1210 14:57:31.856705 15352 solver.cpp:218] Iteration 102300 (17.6737 iter/s, 5.65812s/100 iters), loss = 0.400555
I1210 14:57:31.856705 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:57:31.856705 15352 solver.cpp:237]     Train net output #1: loss = 0.400555 (* 1 = 0.400555 loss)
I1210 14:57:31.856705 15352 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1210 14:57:37.554666 15352 solver.cpp:218] Iteration 102400 (17.5523 iter/s, 5.69725s/100 iters), loss = 0.512624
I1210 14:57:37.554666 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 14:57:37.554666 15352 solver.cpp:237]     Train net output #1: loss = 0.512624 (* 1 = 0.512624 loss)
I1210 14:57:37.554666 15352 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1210 14:57:42.982846 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:57:43.212895 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_102500.caffemodel
I1210 14:57:43.231911 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_102500.solverstate
I1210 14:57:43.236894 15352 solver.cpp:330] Iteration 102500, Testing net (#0)
I1210 14:57:43.236894 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:57:44.613044 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:57:44.665043 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6781
I1210 14:57:44.666044 15352 solver.cpp:397]     Test net output #1: loss = 1.19294 (* 1 = 1.19294 loss)
I1210 14:57:44.719048 15352 solver.cpp:218] Iteration 102500 (13.9591 iter/s, 7.16379s/100 iters), loss = 0.33387
I1210 14:57:44.719048 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 14:57:44.719048 15352 solver.cpp:237]     Train net output #1: loss = 0.33387 (* 1 = 0.33387 loss)
I1210 14:57:44.719048 15352 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1210 14:57:50.406050 15352 solver.cpp:218] Iteration 102600 (17.5863 iter/s, 5.68625s/100 iters), loss = 0.410829
I1210 14:57:50.406050 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 14:57:50.406050 15352 solver.cpp:237]     Train net output #1: loss = 0.410829 (* 1 = 0.410829 loss)
I1210 14:57:50.406050 15352 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1210 14:57:56.162413 15352 solver.cpp:218] Iteration 102700 (17.3724 iter/s, 5.75625s/100 iters), loss = 0.267362
I1210 14:57:56.162413 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 14:57:56.162413 15352 solver.cpp:237]     Train net output #1: loss = 0.267362 (* 1 = 0.267362 loss)
I1210 14:57:56.162413 15352 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1210 14:58:01.937011 15352 solver.cpp:218] Iteration 102800 (17.3196 iter/s, 5.77382s/100 iters), loss = 0.390168
I1210 14:58:01.937011 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:58:01.937011 15352 solver.cpp:237]     Train net output #1: loss = 0.390168 (* 1 = 0.390168 loss)
I1210 14:58:01.937011 15352 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1210 14:58:07.759080 15352 solver.cpp:218] Iteration 102900 (17.1778 iter/s, 5.82147s/100 iters), loss = 0.522236
I1210 14:58:07.759080 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:58:07.759080 15352 solver.cpp:237]     Train net output #1: loss = 0.522236 (* 1 = 0.522236 loss)
I1210 14:58:07.759080 15352 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1210 14:58:13.287089 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:58:13.514158 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_103000.caffemodel
I1210 14:58:13.529155 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_103000.solverstate
I1210 14:58:13.535157 15352 solver.cpp:330] Iteration 103000, Testing net (#0)
I1210 14:58:13.535157 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:58:14.942064 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:58:14.998081 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6793
I1210 14:58:14.998081 15352 solver.cpp:397]     Test net output #1: loss = 1.20237 (* 1 = 1.20237 loss)
I1210 14:58:15.052661 15352 solver.cpp:218] Iteration 103000 (13.7108 iter/s, 7.29351s/100 iters), loss = 0.354429
I1210 14:58:15.052661 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 14:58:15.052661 15352 solver.cpp:237]     Train net output #1: loss = 0.354429 (* 1 = 0.354429 loss)
I1210 14:58:15.052661 15352 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1210 14:58:20.854873 15352 solver.cpp:218] Iteration 103100 (17.2383 iter/s, 5.80103s/100 iters), loss = 0.463087
I1210 14:58:20.854873 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 14:58:20.854873 15352 solver.cpp:237]     Train net output #1: loss = 0.463087 (* 1 = 0.463087 loss)
I1210 14:58:20.854873 15352 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1210 14:58:26.651813 15352 solver.cpp:218] Iteration 103200 (17.2519 iter/s, 5.79645s/100 iters), loss = 0.294075
I1210 14:58:26.651813 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 14:58:26.651813 15352 solver.cpp:237]     Train net output #1: loss = 0.294075 (* 1 = 0.294075 loss)
I1210 14:58:26.651813 15352 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1210 14:58:32.465152 15352 solver.cpp:218] Iteration 103300 (17.2032 iter/s, 5.81289s/100 iters), loss = 0.440684
I1210 14:58:32.465152 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:58:32.465152 15352 solver.cpp:237]     Train net output #1: loss = 0.440684 (* 1 = 0.440684 loss)
I1210 14:58:32.465152 15352 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1210 14:58:38.266633 15352 solver.cpp:218] Iteration 103400 (17.2371 iter/s, 5.80145s/100 iters), loss = 0.462779
I1210 14:58:38.266633 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 14:58:38.266633 15352 solver.cpp:237]     Train net output #1: loss = 0.462779 (* 1 = 0.462779 loss)
I1210 14:58:38.266633 15352 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1210 14:58:43.769932 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:58:43.997972 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_103500.caffemodel
I1210 14:58:44.020481 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_103500.solverstate
I1210 14:58:44.026980 15352 solver.cpp:330] Iteration 103500, Testing net (#0)
I1210 14:58:44.026980 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:58:45.425191 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:58:45.479086 15352 solver.cpp:397]     Test net output #0: accuracy = 0.68
I1210 14:58:45.479086 15352 solver.cpp:397]     Test net output #1: loss = 1.2017 (* 1 = 1.2017 loss)
I1210 14:58:45.534090 15352 solver.cpp:218] Iteration 103500 (13.7605 iter/s, 7.26716s/100 iters), loss = 0.346559
I1210 14:58:45.535087 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 14:58:45.535087 15352 solver.cpp:237]     Train net output #1: loss = 0.346559 (* 1 = 0.346559 loss)
I1210 14:58:45.535087 15352 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1210 14:58:51.310817 15352 solver.cpp:218] Iteration 103600 (17.3142 iter/s, 5.77559s/100 iters), loss = 0.333612
I1210 14:58:51.310817 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 14:58:51.310817 15352 solver.cpp:237]     Train net output #1: loss = 0.333612 (* 1 = 0.333612 loss)
I1210 14:58:51.310817 15352 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1210 14:58:57.077322 15352 solver.cpp:218] Iteration 103700 (17.3439 iter/s, 5.76572s/100 iters), loss = 0.289833
I1210 14:58:57.077322 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 14:58:57.077322 15352 solver.cpp:237]     Train net output #1: loss = 0.289833 (* 1 = 0.289833 loss)
I1210 14:58:57.077322 15352 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1210 14:59:02.850821 15352 solver.cpp:218] Iteration 103800 (17.3223 iter/s, 5.77292s/100 iters), loss = 0.39372
I1210 14:59:02.850821 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:59:02.850821 15352 solver.cpp:237]     Train net output #1: loss = 0.39372 (* 1 = 0.39372 loss)
I1210 14:59:02.850821 15352 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1210 14:59:08.618413 15352 solver.cpp:218] Iteration 103900 (17.3377 iter/s, 5.76776s/100 iters), loss = 0.510809
I1210 14:59:08.618413 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:59:08.618413 15352 solver.cpp:237]     Train net output #1: loss = 0.510809 (* 1 = 0.510809 loss)
I1210 14:59:08.618413 15352 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1210 14:59:14.115016 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:59:14.343045 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_104000.caffemodel
I1210 14:59:14.360044 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_104000.solverstate
I1210 14:59:14.365046 15352 solver.cpp:330] Iteration 104000, Testing net (#0)
I1210 14:59:14.365046 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:59:15.763159 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:59:15.819159 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6786
I1210 14:59:15.819159 15352 solver.cpp:397]     Test net output #1: loss = 1.21812 (* 1 = 1.21812 loss)
I1210 14:59:15.873162 15352 solver.cpp:218] Iteration 104000 (13.7859 iter/s, 7.2538s/100 iters), loss = 0.345498
I1210 14:59:15.873162 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 14:59:15.873162 15352 solver.cpp:237]     Train net output #1: loss = 0.345498 (* 1 = 0.345498 loss)
I1210 14:59:15.873162 15352 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1210 14:59:21.668764 15352 solver.cpp:218] Iteration 104100 (17.2541 iter/s, 5.79572s/100 iters), loss = 0.359744
I1210 14:59:21.669764 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 14:59:21.669764 15352 solver.cpp:237]     Train net output #1: loss = 0.359744 (* 1 = 0.359744 loss)
I1210 14:59:21.669764 15352 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1210 14:59:27.449146 15352 solver.cpp:218] Iteration 104200 (17.303 iter/s, 5.77936s/100 iters), loss = 0.282289
I1210 14:59:27.449146 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 14:59:27.449146 15352 solver.cpp:237]     Train net output #1: loss = 0.282289 (* 1 = 0.282289 loss)
I1210 14:59:27.449146 15352 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1210 14:59:33.226131 15352 solver.cpp:218] Iteration 104300 (17.3119 iter/s, 5.77637s/100 iters), loss = 0.396007
I1210 14:59:33.226131 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 14:59:33.226131 15352 solver.cpp:237]     Train net output #1: loss = 0.396007 (* 1 = 0.396007 loss)
I1210 14:59:33.226131 15352 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1210 14:59:39.012434 15352 solver.cpp:218] Iteration 104400 (17.2831 iter/s, 5.786s/100 iters), loss = 0.452712
I1210 14:59:39.012434 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 14:59:39.012434 15352 solver.cpp:237]     Train net output #1: loss = 0.452712 (* 1 = 0.452712 loss)
I1210 14:59:39.012434 15352 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1210 14:59:44.512917 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:59:44.739192 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_104500.caffemodel
I1210 14:59:44.754195 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_104500.solverstate
I1210 14:59:44.759196 15352 solver.cpp:330] Iteration 104500, Testing net (#0)
I1210 14:59:44.759196 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 14:59:46.158303 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 14:59:46.213301 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6779
I1210 14:59:46.213301 15352 solver.cpp:397]     Test net output #1: loss = 1.21841 (* 1 = 1.21841 loss)
I1210 14:59:46.271306 15352 solver.cpp:218] Iteration 104500 (13.7782 iter/s, 7.25785s/100 iters), loss = 0.35675
I1210 14:59:46.271306 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 14:59:46.271306 15352 solver.cpp:237]     Train net output #1: loss = 0.35675 (* 1 = 0.35675 loss)
I1210 14:59:46.271306 15352 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1210 14:59:52.069818 15352 solver.cpp:218] Iteration 104600 (17.2454 iter/s, 5.79865s/100 iters), loss = 0.409068
I1210 14:59:52.069818 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 14:59:52.069818 15352 solver.cpp:237]     Train net output #1: loss = 0.409068 (* 1 = 0.409068 loss)
I1210 14:59:52.069818 15352 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1210 14:59:57.864341 15352 solver.cpp:218] Iteration 104700 (17.2606 iter/s, 5.79353s/100 iters), loss = 0.263719
I1210 14:59:57.864341 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 14:59:57.864341 15352 solver.cpp:237]     Train net output #1: loss = 0.263719 (* 1 = 0.263719 loss)
I1210 14:59:57.864341 15352 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1210 15:00:03.650028 15352 solver.cpp:218] Iteration 104800 (17.2836 iter/s, 5.78584s/100 iters), loss = 0.414041
I1210 15:00:03.650028 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 15:00:03.650028 15352 solver.cpp:237]     Train net output #1: loss = 0.414041 (* 1 = 0.414041 loss)
I1210 15:00:03.651029 15352 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1210 15:00:09.463129 15352 solver.cpp:218] Iteration 104900 (17.2057 iter/s, 5.81203s/100 iters), loss = 0.419235
I1210 15:00:09.463129 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 15:00:09.463129 15352 solver.cpp:237]     Train net output #1: loss = 0.419235 (* 1 = 0.419235 loss)
I1210 15:00:09.463129 15352 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1210 15:00:14.958657 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:00:15.189357 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_105000.caffemodel
I1210 15:00:15.204356 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_105000.solverstate
I1210 15:00:15.209357 15352 solver.cpp:330] Iteration 105000, Testing net (#0)
I1210 15:00:15.210356 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:00:16.611016 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:00:16.667019 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6784
I1210 15:00:16.667019 15352 solver.cpp:397]     Test net output #1: loss = 1.21777 (* 1 = 1.21777 loss)
I1210 15:00:16.722019 15352 solver.cpp:218] Iteration 105000 (13.7779 iter/s, 7.25802s/100 iters), loss = 0.294685
I1210 15:00:16.722019 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 15:00:16.722019 15352 solver.cpp:237]     Train net output #1: loss = 0.294685 (* 1 = 0.294685 loss)
I1210 15:00:16.722019 15352 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1210 15:00:22.523639 15352 solver.cpp:218] Iteration 105100 (17.2382 iter/s, 5.80107s/100 iters), loss = 0.302105
I1210 15:00:22.523639 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 15:00:22.523639 15352 solver.cpp:237]     Train net output #1: loss = 0.302105 (* 1 = 0.302105 loss)
I1210 15:00:22.523639 15352 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1210 15:00:28.337589 15352 solver.cpp:218] Iteration 105200 (17.2009 iter/s, 5.81363s/100 iters), loss = 0.199112
I1210 15:00:28.337589 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1210 15:00:28.337589 15352 solver.cpp:237]     Train net output #1: loss = 0.199112 (* 1 = 0.199112 loss)
I1210 15:00:28.337589 15352 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1210 15:00:34.153930 15352 solver.cpp:218] Iteration 105300 (17.1943 iter/s, 5.81587s/100 iters), loss = 0.402831
I1210 15:00:34.153930 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 15:00:34.153930 15352 solver.cpp:237]     Train net output #1: loss = 0.402831 (* 1 = 0.402831 loss)
I1210 15:00:34.153930 15352 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1210 15:00:39.952517 15352 solver.cpp:218] Iteration 105400 (17.2464 iter/s, 5.7983s/100 iters), loss = 0.36702
I1210 15:00:39.952517 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 15:00:39.952517 15352 solver.cpp:237]     Train net output #1: loss = 0.36702 (* 1 = 0.36702 loss)
I1210 15:00:39.952517 15352 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1210 15:00:45.495209 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:00:45.723872 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_105500.caffemodel
I1210 15:00:45.739863 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_105500.solverstate
I1210 15:00:45.744887 15352 solver.cpp:330] Iteration 105500, Testing net (#0)
I1210 15:00:45.744887 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:00:47.149401 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:00:47.204298 15352 solver.cpp:397]     Test net output #0: accuracy = 0.677
I1210 15:00:47.204298 15352 solver.cpp:397]     Test net output #1: loss = 1.21742 (* 1 = 1.21742 loss)
I1210 15:00:47.259317 15352 solver.cpp:218] Iteration 105500 (13.6864 iter/s, 7.3065s/100 iters), loss = 0.417227
I1210 15:00:47.259317 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 15:00:47.259317 15352 solver.cpp:237]     Train net output #1: loss = 0.417227 (* 1 = 0.417227 loss)
I1210 15:00:47.259317 15352 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1210 15:00:53.083943 15352 solver.cpp:218] Iteration 105600 (17.1688 iter/s, 5.82453s/100 iters), loss = 0.395522
I1210 15:00:53.084966 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 15:00:53.084966 15352 solver.cpp:237]     Train net output #1: loss = 0.395522 (* 1 = 0.395522 loss)
I1210 15:00:53.084966 15352 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1210 15:00:58.898910 15352 solver.cpp:218] Iteration 105700 (17.2014 iter/s, 5.81348s/100 iters), loss = 0.240424
I1210 15:00:58.898910 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 15:00:58.898910 15352 solver.cpp:237]     Train net output #1: loss = 0.240424 (* 1 = 0.240424 loss)
I1210 15:00:58.898910 15352 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1210 15:01:04.685873 15352 solver.cpp:218] Iteration 105800 (17.2793 iter/s, 5.78729s/100 iters), loss = 0.379868
I1210 15:01:04.685873 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 15:01:04.685873 15352 solver.cpp:237]     Train net output #1: loss = 0.379868 (* 1 = 0.379868 loss)
I1210 15:01:04.685873 15352 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1210 15:01:10.503659 15352 solver.cpp:218] Iteration 105900 (17.1921 iter/s, 5.81664s/100 iters), loss = 0.481893
I1210 15:01:10.503659 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 15:01:10.503659 15352 solver.cpp:237]     Train net output #1: loss = 0.481893 (* 1 = 0.481893 loss)
I1210 15:01:10.503659 15352 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1210 15:01:16.003675 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:01:16.233222 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_106000.caffemodel
I1210 15:01:16.247220 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_106000.solverstate
I1210 15:01:16.253736 15352 solver.cpp:330] Iteration 106000, Testing net (#0)
I1210 15:01:16.253736 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:01:17.655560 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:01:17.708565 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6805
I1210 15:01:17.709565 15352 solver.cpp:397]     Test net output #1: loss = 1.22134 (* 1 = 1.22134 loss)
I1210 15:01:17.765569 15352 solver.cpp:218] Iteration 106000 (13.7713 iter/s, 7.26149s/100 iters), loss = 0.346517
I1210 15:01:17.765569 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 15:01:17.765569 15352 solver.cpp:237]     Train net output #1: loss = 0.346517 (* 1 = 0.346517 loss)
I1210 15:01:17.765569 15352 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1210 15:01:23.555063 15352 solver.cpp:218] Iteration 106100 (17.2735 iter/s, 5.7892s/100 iters), loss = 0.363747
I1210 15:01:23.555563 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 15:01:23.555563 15352 solver.cpp:237]     Train net output #1: loss = 0.363747 (* 1 = 0.363747 loss)
I1210 15:01:23.555563 15352 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1210 15:01:29.344846 15352 solver.cpp:218] Iteration 106200 (17.2737 iter/s, 5.78916s/100 iters), loss = 0.269032
I1210 15:01:29.344846 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 15:01:29.344846 15352 solver.cpp:237]     Train net output #1: loss = 0.269032 (* 1 = 0.269032 loss)
I1210 15:01:29.344846 15352 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1210 15:01:35.142493 15352 solver.cpp:218] Iteration 106300 (17.2493 iter/s, 5.79733s/100 iters), loss = 0.366961
I1210 15:01:35.142493 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 15:01:35.142493 15352 solver.cpp:237]     Train net output #1: loss = 0.366961 (* 1 = 0.366961 loss)
I1210 15:01:35.142493 15352 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1210 15:01:40.934175 15352 solver.cpp:218] Iteration 106400 (17.2675 iter/s, 5.79123s/100 iters), loss = 0.48638
I1210 15:01:40.934175 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 15:01:40.934175 15352 solver.cpp:237]     Train net output #1: loss = 0.48638 (* 1 = 0.48638 loss)
I1210 15:01:40.934175 15352 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1210 15:01:46.465689 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:01:46.692713 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_106500.caffemodel
I1210 15:01:46.709715 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_106500.solverstate
I1210 15:01:46.714723 15352 solver.cpp:330] Iteration 106500, Testing net (#0)
I1210 15:01:46.714723 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:01:48.111518 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:01:48.167546 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6814
I1210 15:01:48.167546 15352 solver.cpp:397]     Test net output #1: loss = 1.21283 (* 1 = 1.21283 loss)
I1210 15:01:48.221932 15352 solver.cpp:218] Iteration 106500 (13.7223 iter/s, 7.28742s/100 iters), loss = 0.374337
I1210 15:01:48.221932 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 15:01:48.221932 15352 solver.cpp:237]     Train net output #1: loss = 0.374337 (* 1 = 0.374337 loss)
I1210 15:01:48.221932 15352 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1210 15:01:54.011567 15352 solver.cpp:218] Iteration 106600 (17.2735 iter/s, 5.78922s/100 iters), loss = 0.446698
I1210 15:01:54.011567 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 15:01:54.011567 15352 solver.cpp:237]     Train net output #1: loss = 0.446698 (* 1 = 0.446698 loss)
I1210 15:01:54.011567 15352 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1210 15:01:59.816249 15352 solver.cpp:218] Iteration 106700 (17.2297 iter/s, 5.80393s/100 iters), loss = 0.265692
I1210 15:01:59.816249 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 15:01:59.816249 15352 solver.cpp:237]     Train net output #1: loss = 0.265692 (* 1 = 0.265692 loss)
I1210 15:01:59.816249 15352 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1210 15:02:05.597359 15352 solver.cpp:218] Iteration 106800 (17.2985 iter/s, 5.78084s/100 iters), loss = 0.399383
I1210 15:02:05.597359 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 15:02:05.597359 15352 solver.cpp:237]     Train net output #1: loss = 0.399383 (* 1 = 0.399383 loss)
I1210 15:02:05.597359 15352 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1210 15:02:11.393273 15352 solver.cpp:218] Iteration 106900 (17.2548 iter/s, 5.79548s/100 iters), loss = 0.512028
I1210 15:02:11.393273 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 15:02:11.393273 15352 solver.cpp:237]     Train net output #1: loss = 0.512028 (* 1 = 0.512028 loss)
I1210 15:02:11.393273 15352 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1210 15:02:16.901017 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:02:17.129290 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_107000.caffemodel
I1210 15:02:17.144289 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_107000.solverstate
I1210 15:02:17.149289 15352 solver.cpp:330] Iteration 107000, Testing net (#0)
I1210 15:02:17.149289 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:02:18.552673 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:02:18.607687 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6788
I1210 15:02:18.607687 15352 solver.cpp:397]     Test net output #1: loss = 1.21613 (* 1 = 1.21613 loss)
I1210 15:02:18.662675 15352 solver.cpp:218] Iteration 107000 (13.7585 iter/s, 7.26823s/100 iters), loss = 0.386884
I1210 15:02:18.662675 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 15:02:18.662675 15352 solver.cpp:237]     Train net output #1: loss = 0.386884 (* 1 = 0.386884 loss)
I1210 15:02:18.662675 15352 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1210 15:02:24.403497 15352 solver.cpp:218] Iteration 107100 (17.4205 iter/s, 5.74036s/100 iters), loss = 0.405867
I1210 15:02:24.403497 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 15:02:24.403497 15352 solver.cpp:237]     Train net output #1: loss = 0.405867 (* 1 = 0.405867 loss)
I1210 15:02:24.403497 15352 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1210 15:02:30.117435 15352 solver.cpp:218] Iteration 107200 (17.5005 iter/s, 5.71413s/100 iters), loss = 0.254032
I1210 15:02:30.117435 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 15:02:30.117435 15352 solver.cpp:237]     Train net output #1: loss = 0.254032 (* 1 = 0.254032 loss)
I1210 15:02:30.117435 15352 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1210 15:02:35.799933 15352 solver.cpp:218] Iteration 107300 (17.6017 iter/s, 5.68127s/100 iters), loss = 0.311945
I1210 15:02:35.799933 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 15:02:35.799933 15352 solver.cpp:237]     Train net output #1: loss = 0.311945 (* 1 = 0.311945 loss)
I1210 15:02:35.799933 15352 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1210 15:02:41.452421 15352 solver.cpp:218] Iteration 107400 (17.6922 iter/s, 5.65221s/100 iters), loss = 0.42399
I1210 15:02:41.452421 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 15:02:41.452421 15352 solver.cpp:237]     Train net output #1: loss = 0.42399 (* 1 = 0.42399 loss)
I1210 15:02:41.452421 15352 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1210 15:02:46.851591 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:02:47.074683 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_107500.caffemodel
I1210 15:02:47.088682 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_107500.solverstate
I1210 15:02:47.094187 15352 solver.cpp:330] Iteration 107500, Testing net (#0)
I1210 15:02:47.094187 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:02:48.467108 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:02:48.521980 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6756
I1210 15:02:48.521980 15352 solver.cpp:397]     Test net output #1: loss = 1.22773 (* 1 = 1.22773 loss)
I1210 15:02:48.574977 15352 solver.cpp:218] Iteration 107500 (14.0406 iter/s, 7.12222s/100 iters), loss = 0.333015
I1210 15:02:48.574977 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 15:02:48.574977 15352 solver.cpp:237]     Train net output #1: loss = 0.333015 (* 1 = 0.333015 loss)
I1210 15:02:48.574977 15352 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1210 15:02:54.260391 15352 solver.cpp:218] Iteration 107600 (17.5888 iter/s, 5.68543s/100 iters), loss = 0.382381
I1210 15:02:54.260391 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 15:02:54.260391 15352 solver.cpp:237]     Train net output #1: loss = 0.382381 (* 1 = 0.382381 loss)
I1210 15:02:54.260391 15352 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1210 15:02:59.931329 15352 solver.cpp:218] Iteration 107700 (17.6378 iter/s, 5.66965s/100 iters), loss = 0.222228
I1210 15:02:59.931329 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 15:02:59.931329 15352 solver.cpp:237]     Train net output #1: loss = 0.222228 (* 1 = 0.222228 loss)
I1210 15:02:59.931329 15352 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1210 15:03:05.612923 15352 solver.cpp:218] Iteration 107800 (17.6014 iter/s, 5.68137s/100 iters), loss = 0.36333
I1210 15:03:05.612923 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 15:03:05.613425 15352 solver.cpp:237]     Train net output #1: loss = 0.36333 (* 1 = 0.36333 loss)
I1210 15:03:05.613425 15352 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1210 15:03:11.294463 15352 solver.cpp:218] Iteration 107900 (17.603 iter/s, 5.68084s/100 iters), loss = 0.437777
I1210 15:03:11.294463 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 15:03:11.294463 15352 solver.cpp:237]     Train net output #1: loss = 0.437777 (* 1 = 0.437777 loss)
I1210 15:03:11.294463 15352 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1210 15:03:16.682703 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:03:16.906239 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_108000.caffemodel
I1210 15:03:16.920745 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_108000.solverstate
I1210 15:03:16.925742 15352 solver.cpp:330] Iteration 108000, Testing net (#0)
I1210 15:03:16.925742 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:03:18.292835 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:03:18.345839 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6785
I1210 15:03:18.345839 15352 solver.cpp:397]     Test net output #1: loss = 1.21468 (* 1 = 1.21468 loss)
I1210 15:03:18.400838 15352 solver.cpp:218] Iteration 108000 (14.073 iter/s, 7.10582s/100 iters), loss = 0.295249
I1210 15:03:18.400838 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 15:03:18.400838 15352 solver.cpp:237]     Train net output #1: loss = 0.295249 (* 1 = 0.295249 loss)
I1210 15:03:18.400838 15352 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1210 15:03:24.049345 15352 solver.cpp:218] Iteration 108100 (17.7039 iter/s, 5.64846s/100 iters), loss = 0.400743
I1210 15:03:24.049345 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 15:03:24.049345 15352 solver.cpp:237]     Train net output #1: loss = 0.400743 (* 1 = 0.400743 loss)
I1210 15:03:24.049345 15352 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1210 15:03:29.731927 15352 solver.cpp:218] Iteration 108200 (17.5982 iter/s, 5.6824s/100 iters), loss = 0.234579
I1210 15:03:29.731927 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 15:03:29.731927 15352 solver.cpp:237]     Train net output #1: loss = 0.234579 (* 1 = 0.234579 loss)
I1210 15:03:29.731927 15352 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1210 15:03:35.379366 15352 solver.cpp:218] Iteration 108300 (17.7098 iter/s, 5.6466s/100 iters), loss = 0.469278
I1210 15:03:35.379366 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 15:03:35.379366 15352 solver.cpp:237]     Train net output #1: loss = 0.469278 (* 1 = 0.469278 loss)
I1210 15:03:35.379366 15352 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1210 15:03:41.049376 15352 solver.cpp:218] Iteration 108400 (17.6366 iter/s, 5.67002s/100 iters), loss = 0.432792
I1210 15:03:41.050374 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 15:03:41.050374 15352 solver.cpp:237]     Train net output #1: loss = 0.432792 (* 1 = 0.432792 loss)
I1210 15:03:41.050374 15352 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1210 15:03:46.444552 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:03:46.667232 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_108500.caffemodel
I1210 15:03:46.683233 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_108500.solverstate
I1210 15:03:46.688232 15352 solver.cpp:330] Iteration 108500, Testing net (#0)
I1210 15:03:46.688232 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:03:48.055112 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:03:48.108114 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6793
I1210 15:03:48.108114 15352 solver.cpp:397]     Test net output #1: loss = 1.22999 (* 1 = 1.22999 loss)
I1210 15:03:48.161921 15352 solver.cpp:218] Iteration 108500 (14.0626 iter/s, 7.11107s/100 iters), loss = 0.376949
I1210 15:03:48.161921 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 15:03:48.161921 15352 solver.cpp:237]     Train net output #1: loss = 0.376949 (* 1 = 0.376949 loss)
I1210 15:03:48.161921 15352 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1210 15:03:53.784699 15352 solver.cpp:218] Iteration 108600 (17.7843 iter/s, 5.62295s/100 iters), loss = 0.338933
I1210 15:03:53.784699 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 15:03:53.784699 15352 solver.cpp:237]     Train net output #1: loss = 0.338933 (* 1 = 0.338933 loss)
I1210 15:03:53.784699 15352 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1210 15:03:59.450135 15352 solver.cpp:218] Iteration 108700 (17.6519 iter/s, 5.66511s/100 iters), loss = 0.305576
I1210 15:03:59.450135 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 15:03:59.450135 15352 solver.cpp:237]     Train net output #1: loss = 0.305576 (* 1 = 0.305576 loss)
I1210 15:03:59.450135 15352 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1210 15:04:05.108065 15352 solver.cpp:218] Iteration 108800 (17.6782 iter/s, 5.65669s/100 iters), loss = 0.367386
I1210 15:04:05.108065 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 15:04:05.108065 15352 solver.cpp:237]     Train net output #1: loss = 0.367386 (* 1 = 0.367386 loss)
I1210 15:04:05.108065 15352 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1210 15:04:10.735982 15352 solver.cpp:218] Iteration 108900 (17.7682 iter/s, 5.62803s/100 iters), loss = 0.503551
I1210 15:04:10.735982 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 15:04:10.735982 15352 solver.cpp:237]     Train net output #1: loss = 0.50355 (* 1 = 0.50355 loss)
I1210 15:04:10.735982 15352 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1210 15:04:16.083535 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:04:16.302798 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_109000.caffemodel
I1210 15:04:16.318800 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_109000.solverstate
I1210 15:04:16.324797 15352 solver.cpp:330] Iteration 109000, Testing net (#0)
I1210 15:04:16.324797 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:04:17.694851 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:04:17.748837 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6795
I1210 15:04:17.748837 15352 solver.cpp:397]     Test net output #1: loss = 1.22466 (* 1 = 1.22466 loss)
I1210 15:04:17.801856 15352 solver.cpp:218] Iteration 109000 (14.1549 iter/s, 7.0647s/100 iters), loss = 0.251153
I1210 15:04:17.801856 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 15:04:17.801856 15352 solver.cpp:237]     Train net output #1: loss = 0.251153 (* 1 = 0.251153 loss)
I1210 15:04:17.801856 15352 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1210 15:04:23.435490 15352 solver.cpp:218] Iteration 109100 (17.7507 iter/s, 5.63359s/100 iters), loss = 0.368628
I1210 15:04:23.435490 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 15:04:23.435490 15352 solver.cpp:237]     Train net output #1: loss = 0.368628 (* 1 = 0.368628 loss)
I1210 15:04:23.435490 15352 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1210 15:04:29.069856 15352 solver.cpp:218] Iteration 109200 (17.7509 iter/s, 5.63353s/100 iters), loss = 0.219755
I1210 15:04:29.069856 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1210 15:04:29.069856 15352 solver.cpp:237]     Train net output #1: loss = 0.219755 (* 1 = 0.219755 loss)
I1210 15:04:29.069856 15352 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1210 15:04:34.700001 15352 solver.cpp:218] Iteration 109300 (17.7609 iter/s, 5.63035s/100 iters), loss = 0.382054
I1210 15:04:34.700001 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 15:04:34.700001 15352 solver.cpp:237]     Train net output #1: loss = 0.382054 (* 1 = 0.382054 loss)
I1210 15:04:34.700001 15352 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1210 15:04:40.342734 15352 solver.cpp:218] Iteration 109400 (17.7233 iter/s, 5.64228s/100 iters), loss = 0.442845
I1210 15:04:40.342734 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 15:04:40.342734 15352 solver.cpp:237]     Train net output #1: loss = 0.442845 (* 1 = 0.442845 loss)
I1210 15:04:40.342734 15352 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1210 15:04:45.702014 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:04:45.923029 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_109500.caffemodel
I1210 15:04:45.939028 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_109500.solverstate
I1210 15:04:45.943029 15352 solver.cpp:330] Iteration 109500, Testing net (#0)
I1210 15:04:45.943029 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:04:47.312197 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:04:47.365200 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6755
I1210 15:04:47.365200 15352 solver.cpp:397]     Test net output #1: loss = 1.23506 (* 1 = 1.23506 loss)
I1210 15:04:47.419201 15352 solver.cpp:218] Iteration 109500 (14.1338 iter/s, 7.07525s/100 iters), loss = 0.298836
I1210 15:04:47.419201 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 15:04:47.419201 15352 solver.cpp:237]     Train net output #1: loss = 0.298836 (* 1 = 0.298836 loss)
I1210 15:04:47.419201 15352 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1210 15:04:53.084374 15352 solver.cpp:218] Iteration 109600 (17.6529 iter/s, 5.66479s/100 iters), loss = 0.357539
I1210 15:04:53.084374 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 15:04:53.084374 15352 solver.cpp:237]     Train net output #1: loss = 0.357539 (* 1 = 0.357539 loss)
I1210 15:04:53.084374 15352 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1210 15:04:58.747578 15352 solver.cpp:218] Iteration 109700 (17.6568 iter/s, 5.66355s/100 iters), loss = 0.223692
I1210 15:04:58.747578 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 15:04:58.748579 15352 solver.cpp:237]     Train net output #1: loss = 0.223692 (* 1 = 0.223692 loss)
I1210 15:04:58.748579 15352 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1210 15:05:04.412143 15352 solver.cpp:218] Iteration 109800 (17.6557 iter/s, 5.66389s/100 iters), loss = 0.363647
I1210 15:05:04.412143 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 15:05:04.412143 15352 solver.cpp:237]     Train net output #1: loss = 0.363647 (* 1 = 0.363647 loss)
I1210 15:05:04.412143 15352 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1210 15:05:10.059466 15352 solver.cpp:218] Iteration 109900 (17.7096 iter/s, 5.64666s/100 iters), loss = 0.387317
I1210 15:05:10.059466 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 15:05:10.059466 15352 solver.cpp:237]     Train net output #1: loss = 0.387317 (* 1 = 0.387317 loss)
I1210 15:05:10.059466 15352 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1210 15:05:15.452993 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:05:15.674010 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_110000.caffemodel
I1210 15:05:15.692015 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_110000.solverstate
I1210 15:05:15.698010 15352 solver.cpp:330] Iteration 110000, Testing net (#0)
I1210 15:05:15.698010 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:05:17.074443 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:05:17.127946 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6795
I1210 15:05:17.128445 15352 solver.cpp:397]     Test net output #1: loss = 1.23089 (* 1 = 1.23089 loss)
I1210 15:05:17.181447 15352 solver.cpp:218] Iteration 110000 (14.0428 iter/s, 7.12108s/100 iters), loss = 0.305145
I1210 15:05:17.181447 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 15:05:17.181447 15352 solver.cpp:237]     Train net output #1: loss = 0.305145 (* 1 = 0.305145 loss)
I1210 15:05:17.181447 15352 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1210 15:05:22.842310 15352 solver.cpp:218] Iteration 110100 (17.6672 iter/s, 5.66019s/100 iters), loss = 0.393456
I1210 15:05:22.842310 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 15:05:22.842310 15352 solver.cpp:237]     Train net output #1: loss = 0.393456 (* 1 = 0.393456 loss)
I1210 15:05:22.842310 15352 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1210 15:05:28.525671 15352 solver.cpp:218] Iteration 110200 (17.5949 iter/s, 5.68347s/100 iters), loss = 0.225334
I1210 15:05:28.525671 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 15:05:28.525671 15352 solver.cpp:237]     Train net output #1: loss = 0.225334 (* 1 = 0.225334 loss)
I1210 15:05:28.525671 15352 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1210 15:05:34.192229 15352 solver.cpp:218] Iteration 110300 (17.6511 iter/s, 5.66538s/100 iters), loss = 0.38997
I1210 15:05:34.192229 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 15:05:34.192229 15352 solver.cpp:237]     Train net output #1: loss = 0.38997 (* 1 = 0.38997 loss)
I1210 15:05:34.192229 15352 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1210 15:05:39.836848 15352 solver.cpp:218] Iteration 110400 (17.7169 iter/s, 5.64434s/100 iters), loss = 0.442218
I1210 15:05:39.836848 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 15:05:39.837348 15352 solver.cpp:237]     Train net output #1: loss = 0.442218 (* 1 = 0.442218 loss)
I1210 15:05:39.837348 15352 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1210 15:05:45.215332 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:05:45.436858 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_110500.caffemodel
I1210 15:05:45.452363 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_110500.solverstate
I1210 15:05:45.457362 15352 solver.cpp:330] Iteration 110500, Testing net (#0)
I1210 15:05:45.457362 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:05:46.822489 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:05:46.876495 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6745
I1210 15:05:46.876495 15352 solver.cpp:397]     Test net output #1: loss = 1.26482 (* 1 = 1.26482 loss)
I1210 15:05:46.930495 15352 solver.cpp:218] Iteration 110500 (14.098 iter/s, 7.0932s/100 iters), loss = 0.303988
I1210 15:05:46.931006 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 15:05:46.931006 15352 solver.cpp:237]     Train net output #1: loss = 0.303988 (* 1 = 0.303988 loss)
I1210 15:05:46.931006 15352 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1210 15:05:52.587219 15352 solver.cpp:218] Iteration 110600 (17.6801 iter/s, 5.65607s/100 iters), loss = 0.390216
I1210 15:05:52.587219 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 15:05:52.587219 15352 solver.cpp:237]     Train net output #1: loss = 0.390216 (* 1 = 0.390216 loss)
I1210 15:05:52.587219 15352 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1210 15:05:58.245899 15352 solver.cpp:218] Iteration 110700 (17.6729 iter/s, 5.65839s/100 iters), loss = 0.215017
I1210 15:05:58.246417 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 15:05:58.246417 15352 solver.cpp:237]     Train net output #1: loss = 0.215017 (* 1 = 0.215017 loss)
I1210 15:05:58.246417 15352 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1210 15:06:03.914664 15352 solver.cpp:218] Iteration 110800 (17.6411 iter/s, 5.66857s/100 iters), loss = 0.389591
I1210 15:06:03.914664 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 15:06:03.914664 15352 solver.cpp:237]     Train net output #1: loss = 0.389591 (* 1 = 0.389591 loss)
I1210 15:06:03.914664 15352 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1210 15:06:09.555467 15352 solver.cpp:218] Iteration 110900 (17.7308 iter/s, 5.63991s/100 iters), loss = 0.427674
I1210 15:06:09.555467 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 15:06:09.555467 15352 solver.cpp:237]     Train net output #1: loss = 0.427674 (* 1 = 0.427674 loss)
I1210 15:06:09.555467 15352 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1210 15:06:14.938989 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:06:15.163818 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_111000.caffemodel
I1210 15:06:15.178800 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_111000.solverstate
I1210 15:06:15.183820 15352 solver.cpp:330] Iteration 111000, Testing net (#0)
I1210 15:06:15.183820 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:06:16.564839 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:06:16.618835 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6741
I1210 15:06:16.618835 15352 solver.cpp:397]     Test net output #1: loss = 1.25282 (* 1 = 1.25282 loss)
I1210 15:06:16.673089 15352 solver.cpp:218] Iteration 111000 (14.0507 iter/s, 7.11709s/100 iters), loss = 0.373892
I1210 15:06:16.673089 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 15:06:16.673089 15352 solver.cpp:237]     Train net output #1: loss = 0.373892 (* 1 = 0.373892 loss)
I1210 15:06:16.673089 15352 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1210 15:06:22.348726 15352 solver.cpp:218] Iteration 111100 (17.6208 iter/s, 5.6751s/100 iters), loss = 0.355604
I1210 15:06:22.348726 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 15:06:22.348726 15352 solver.cpp:237]     Train net output #1: loss = 0.355604 (* 1 = 0.355604 loss)
I1210 15:06:22.348726 15352 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1210 15:06:28.016921 15352 solver.cpp:218] Iteration 111200 (17.6436 iter/s, 5.66776s/100 iters), loss = 0.218714
I1210 15:06:28.016921 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 15:06:28.016921 15352 solver.cpp:237]     Train net output #1: loss = 0.218714 (* 1 = 0.218714 loss)
I1210 15:06:28.016921 15352 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1210 15:06:33.708582 15352 solver.cpp:218] Iteration 111300 (17.5714 iter/s, 5.69106s/100 iters), loss = 0.34336
I1210 15:06:33.708582 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 15:06:33.708582 15352 solver.cpp:237]     Train net output #1: loss = 0.34336 (* 1 = 0.34336 loss)
I1210 15:06:33.708582 15352 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1210 15:06:39.380677 15352 solver.cpp:218] Iteration 111400 (17.6317 iter/s, 5.6716s/100 iters), loss = 0.426806
I1210 15:06:39.380677 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 15:06:39.380677 15352 solver.cpp:237]     Train net output #1: loss = 0.426806 (* 1 = 0.426806 loss)
I1210 15:06:39.380677 15352 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1210 15:06:44.761680 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:06:44.982193 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_111500.caffemodel
I1210 15:06:44.997195 15352 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_111500.solverstate
I1210 15:06:45.002193 15352 solver.cpp:330] Iteration 111500, Testing net (#0)
I1210 15:06:45.002193 15352 net.cpp:676] Ignoring source layer accuracy_training
I1210 15:06:46.374310 11996 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:06:46.430310 15352 solver.cpp:397]     Test net output #0: accuracy = 0.6753
I1210 15:06:46.430310 15352 solver.cpp:397]     Test net output #1: loss = 1.25063 (* 1 = 1.25063 loss)
I1210 15:06:46.483315 15352 solver.cpp:218] Iteration 111500 (14.0799 iter/s, 7.10231s/100 iters), loss = 0.339846
I1210 15:06:46.483315 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 15:06:46.483315 15352 solver.cpp:237]     Train net output #1: loss = 0.339846 (* 1 = 0.339846 loss)
I1210 15:06:46.483315 15352 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1210 15:06:52.150113 15352 solver.cpp:218] Iteration 111600 (17.6478 iter/s, 5.66644s/100 iters), loss = 0.302934
I1210 15:06:52.150113 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 15:06:52.150113 15352 solver.cpp:237]     Train net output #1: loss = 0.302934 (* 1 = 0.302934 loss)
I1210 15:06:52.150113 15352 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1210 15:06:57.799662 15352 solver.cpp:218] Iteration 111700 (17.7017 iter/s, 5.64916s/100 iters), loss = 0.20326
I1210 15:06:57.799662 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 15:06:57.799662 15352 solver.cpp:237]     Train net output #1: loss = 0.20326 (* 1 = 0.20326 loss)
I1210 15:06:57.799662 15352 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1210 15:07:03.465695 15352 solver.cpp:218] Iteration 111800 (17.6503 iter/s, 5.66561s/100 iters), loss = 0.373622
I1210 15:07:03.466195 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 15:07:03.466195 15352 solver.cpp:237]     Train net output #1: loss = 0.373622 (* 1 = 0.373622 loss)
I1210 15:07:03.466195 15352 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1210 15:07:09.123661 15352 solver.cpp:218] Iteration 111900 (17.676 iter/s, 5.65739s/100 iters), loss = 0.384817
I1210 15:07:09.123661 15352 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 15:07:09.123661 15352 solver.cpp:237]     Train net output #1: loss = 0.384817 (* 1 = 0.384817 loss)
I1210 15:07:09.123661 15352 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1210 15:07:14.493199 18100 data_layer.cpp:73] Restarting data prefetching from start.
I1210 15:07:14.719240 15352 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_112000.caffemodel
I1210 15:07:14.735239 15352 sgd_solver.cpp:273] Snapshotting solver state to binary 
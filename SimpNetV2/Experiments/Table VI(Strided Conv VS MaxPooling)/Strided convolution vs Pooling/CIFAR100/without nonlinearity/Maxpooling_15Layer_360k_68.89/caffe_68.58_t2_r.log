
G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90000.solverstate 
I1210 18:03:19.088829 22120 caffe.cpp:219] Using GPUs 0
I1210 18:03:19.276870 22120 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1210 18:03:19.589507 22120 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 18:03:19.607507 22120 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1210 18:03:19.607507 22120 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 18:03:19.610507 22120 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 18:03:19.610507 22120 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1210 18:03:19.610507 22120 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1210 18:03:19.610507 22120 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_v2_360k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1210 18:03:19.678022 22120 layer_factory.cpp:58] Creating layer cifar
I1210 18:03:19.685017 22120 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1210 18:03:19.687017 22120 net.cpp:84] Creating Layer cifar
I1210 18:03:19.687017 22120 net.cpp:380] cifar -> data
I1210 18:03:19.687017 22120 net.cpp:380] cifar -> label
I1210 18:03:19.688019 22120 data_layer.cpp:45] output data size: 100,3,32,32
I1210 18:03:19.694015 22120 net.cpp:122] Setting up cifar
I1210 18:03:19.694015 22120 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1210 18:03:19.694015 22120 net.cpp:129] Top shape: 100 (100)
I1210 18:03:19.694015 22120 net.cpp:137] Memory required for data: 1229200
I1210 18:03:19.694015 22120 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1210 18:03:19.694015 22120 net.cpp:84] Creating Layer label_cifar_1_split
I1210 18:03:19.694015 22120 net.cpp:406] label_cifar_1_split <- label
I1210 18:03:19.694015 22120 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1210 18:03:19.694015 22120 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1210 18:03:19.694015 22120 net.cpp:122] Setting up label_cifar_1_split
I1210 18:03:19.694015 22120 net.cpp:129] Top shape: 100 (100)
I1210 18:03:19.694015 22120 net.cpp:129] Top shape: 100 (100)
I1210 18:03:19.694015 22120 net.cpp:137] Memory required for data: 1230000
I1210 18:03:19.694015 22120 layer_factory.cpp:58] Creating layer conv1
I1210 18:03:19.694015 22120 net.cpp:84] Creating Layer conv1
I1210 18:03:19.694015 22120 net.cpp:406] conv1 <- data
I1210 18:03:19.694015 22120 net.cpp:380] conv1 -> conv1
I1210 18:03:19.695019 19904 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 18:03:19.942054 22120 net.cpp:122] Setting up conv1
I1210 18:03:19.942054 22120 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 18:03:19.942054 22120 net.cpp:137] Memory required for data: 13518000
I1210 18:03:19.942054 22120 layer_factory.cpp:58] Creating layer bn1
I1210 18:03:19.942554 22120 net.cpp:84] Creating Layer bn1
I1210 18:03:19.942554 22120 net.cpp:406] bn1 <- conv1
I1210 18:03:19.942554 22120 net.cpp:367] bn1 -> conv1 (in-place)
I1210 18:03:19.942554 22120 net.cpp:122] Setting up bn1
I1210 18:03:19.942554 22120 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 18:03:19.942554 22120 net.cpp:137] Memory required for data: 25806000
I1210 18:03:19.942554 22120 layer_factory.cpp:58] Creating layer scale1
I1210 18:03:19.942554 22120 net.cpp:84] Creating Layer scale1
I1210 18:03:19.942554 22120 net.cpp:406] scale1 <- conv1
I1210 18:03:19.942554 22120 net.cpp:367] scale1 -> conv1 (in-place)
I1210 18:03:19.942554 22120 layer_factory.cpp:58] Creating layer scale1
I1210 18:03:19.942554 22120 net.cpp:122] Setting up scale1
I1210 18:03:19.942554 22120 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 18:03:19.942554 22120 net.cpp:137] Memory required for data: 38094000
I1210 18:03:19.942554 22120 layer_factory.cpp:58] Creating layer relu1
I1210 18:03:19.942554 22120 net.cpp:84] Creating Layer relu1
I1210 18:03:19.942554 22120 net.cpp:406] relu1 <- conv1
I1210 18:03:19.942554 22120 net.cpp:367] relu1 -> conv1 (in-place)
I1210 18:03:19.943053 22120 net.cpp:122] Setting up relu1
I1210 18:03:19.943053 22120 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 18:03:19.943053 22120 net.cpp:137] Memory required for data: 50382000
I1210 18:03:19.943053 22120 layer_factory.cpp:58] Creating layer conv1_0
I1210 18:03:19.943053 22120 net.cpp:84] Creating Layer conv1_0
I1210 18:03:19.943053 22120 net.cpp:406] conv1_0 <- conv1
I1210 18:03:19.943053 22120 net.cpp:380] conv1_0 -> conv1_0
I1210 18:03:19.944552 22120 net.cpp:122] Setting up conv1_0
I1210 18:03:19.944552 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.944552 22120 net.cpp:137] Memory required for data: 66766000
I1210 18:03:19.944552 22120 layer_factory.cpp:58] Creating layer bn1_0
I1210 18:03:19.944552 22120 net.cpp:84] Creating Layer bn1_0
I1210 18:03:19.944552 22120 net.cpp:406] bn1_0 <- conv1_0
I1210 18:03:19.944552 22120 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1210 18:03:19.945053 22120 net.cpp:122] Setting up bn1_0
I1210 18:03:19.945053 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.945053 22120 net.cpp:137] Memory required for data: 83150000
I1210 18:03:19.945053 22120 layer_factory.cpp:58] Creating layer scale1_0
I1210 18:03:19.945053 22120 net.cpp:84] Creating Layer scale1_0
I1210 18:03:19.945053 22120 net.cpp:406] scale1_0 <- conv1_0
I1210 18:03:19.945053 22120 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1210 18:03:19.945053 22120 layer_factory.cpp:58] Creating layer scale1_0
I1210 18:03:19.945053 22120 net.cpp:122] Setting up scale1_0
I1210 18:03:19.945053 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.945053 22120 net.cpp:137] Memory required for data: 99534000
I1210 18:03:19.945053 22120 layer_factory.cpp:58] Creating layer relu1_0
I1210 18:03:19.945053 22120 net.cpp:84] Creating Layer relu1_0
I1210 18:03:19.945053 22120 net.cpp:406] relu1_0 <- conv1_0
I1210 18:03:19.945053 22120 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1210 18:03:19.945554 22120 net.cpp:122] Setting up relu1_0
I1210 18:03:19.945554 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.945554 22120 net.cpp:137] Memory required for data: 115918000
I1210 18:03:19.945554 22120 layer_factory.cpp:58] Creating layer conv2
I1210 18:03:19.945554 22120 net.cpp:84] Creating Layer conv2
I1210 18:03:19.945554 22120 net.cpp:406] conv2 <- conv1_0
I1210 18:03:19.945554 22120 net.cpp:380] conv2 -> conv2
I1210 18:03:19.946553 22120 net.cpp:122] Setting up conv2
I1210 18:03:19.946553 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.946553 22120 net.cpp:137] Memory required for data: 132302000
I1210 18:03:19.946553 22120 layer_factory.cpp:58] Creating layer bn2
I1210 18:03:19.946553 22120 net.cpp:84] Creating Layer bn2
I1210 18:03:19.946553 22120 net.cpp:406] bn2 <- conv2
I1210 18:03:19.946553 22120 net.cpp:367] bn2 -> conv2 (in-place)
I1210 18:03:19.946553 22120 net.cpp:122] Setting up bn2
I1210 18:03:19.946553 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.946553 22120 net.cpp:137] Memory required for data: 148686000
I1210 18:03:19.946553 22120 layer_factory.cpp:58] Creating layer scale2
I1210 18:03:19.946553 22120 net.cpp:84] Creating Layer scale2
I1210 18:03:19.946553 22120 net.cpp:406] scale2 <- conv2
I1210 18:03:19.947053 22120 net.cpp:367] scale2 -> conv2 (in-place)
I1210 18:03:19.947053 22120 layer_factory.cpp:58] Creating layer scale2
I1210 18:03:19.947053 22120 net.cpp:122] Setting up scale2
I1210 18:03:19.947053 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.947053 22120 net.cpp:137] Memory required for data: 165070000
I1210 18:03:19.947053 22120 layer_factory.cpp:58] Creating layer relu2
I1210 18:03:19.947053 22120 net.cpp:84] Creating Layer relu2
I1210 18:03:19.947053 22120 net.cpp:406] relu2 <- conv2
I1210 18:03:19.947053 22120 net.cpp:367] relu2 -> conv2 (in-place)
I1210 18:03:19.947053 22120 net.cpp:122] Setting up relu2
I1210 18:03:19.947053 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.947053 22120 net.cpp:137] Memory required for data: 181454000
I1210 18:03:19.947053 22120 layer_factory.cpp:58] Creating layer conv2_1
I1210 18:03:19.947053 22120 net.cpp:84] Creating Layer conv2_1
I1210 18:03:19.947053 22120 net.cpp:406] conv2_1 <- conv2
I1210 18:03:19.947053 22120 net.cpp:380] conv2_1 -> conv2_1
I1210 18:03:19.948559 22120 net.cpp:122] Setting up conv2_1
I1210 18:03:19.948559 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.948559 22120 net.cpp:137] Memory required for data: 197838000
I1210 18:03:19.948559 22120 layer_factory.cpp:58] Creating layer bn2_1
I1210 18:03:19.948559 22120 net.cpp:84] Creating Layer bn2_1
I1210 18:03:19.948559 22120 net.cpp:406] bn2_1 <- conv2_1
I1210 18:03:19.949054 22120 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1210 18:03:19.949054 22120 net.cpp:122] Setting up bn2_1
I1210 18:03:19.949054 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.949054 22120 net.cpp:137] Memory required for data: 214222000
I1210 18:03:19.949054 22120 layer_factory.cpp:58] Creating layer scale2_1
I1210 18:03:19.949054 22120 net.cpp:84] Creating Layer scale2_1
I1210 18:03:19.949054 22120 net.cpp:406] scale2_1 <- conv2_1
I1210 18:03:19.949054 22120 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1210 18:03:19.949054 22120 layer_factory.cpp:58] Creating layer scale2_1
I1210 18:03:19.949054 22120 net.cpp:122] Setting up scale2_1
I1210 18:03:19.949054 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.949054 22120 net.cpp:137] Memory required for data: 230606000
I1210 18:03:19.949054 22120 layer_factory.cpp:58] Creating layer relu2_1
I1210 18:03:19.949054 22120 net.cpp:84] Creating Layer relu2_1
I1210 18:03:19.949553 22120 net.cpp:406] relu2_1 <- conv2_1
I1210 18:03:19.949553 22120 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1210 18:03:19.949553 22120 net.cpp:122] Setting up relu2_1
I1210 18:03:19.949553 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.949553 22120 net.cpp:137] Memory required for data: 246990000
I1210 18:03:19.949553 22120 layer_factory.cpp:58] Creating layer conv2_2
I1210 18:03:19.949553 22120 net.cpp:84] Creating Layer conv2_2
I1210 18:03:19.949553 22120 net.cpp:406] conv2_2 <- conv2_1
I1210 18:03:19.949553 22120 net.cpp:380] conv2_2 -> conv2_2
I1210 18:03:19.951553 22120 net.cpp:122] Setting up conv2_2
I1210 18:03:19.951553 22120 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 18:03:19.951553 22120 net.cpp:137] Memory required for data: 267470000
I1210 18:03:19.951553 22120 layer_factory.cpp:58] Creating layer bn2_2
I1210 18:03:19.951553 22120 net.cpp:84] Creating Layer bn2_2
I1210 18:03:19.951553 22120 net.cpp:406] bn2_2 <- conv2_2
I1210 18:03:19.951553 22120 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1210 18:03:19.951553 22120 net.cpp:122] Setting up bn2_2
I1210 18:03:19.951553 22120 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 18:03:19.951553 22120 net.cpp:137] Memory required for data: 287950000
I1210 18:03:19.951553 22120 layer_factory.cpp:58] Creating layer scale2_2
I1210 18:03:19.951553 22120 net.cpp:84] Creating Layer scale2_2
I1210 18:03:19.951553 22120 net.cpp:406] scale2_2 <- conv2_2
I1210 18:03:19.951553 22120 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1210 18:03:19.951553 22120 layer_factory.cpp:58] Creating layer scale2_2
I1210 18:03:19.951553 22120 net.cpp:122] Setting up scale2_2
I1210 18:03:19.951553 22120 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 18:03:19.952056 22120 net.cpp:137] Memory required for data: 308430000
I1210 18:03:19.952056 22120 layer_factory.cpp:58] Creating layer relu2_2
I1210 18:03:19.952056 22120 net.cpp:84] Creating Layer relu2_2
I1210 18:03:19.952056 22120 net.cpp:406] relu2_2 <- conv2_2
I1210 18:03:19.952056 22120 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1210 18:03:19.952056 22120 net.cpp:122] Setting up relu2_2
I1210 18:03:19.952056 22120 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 18:03:19.952056 22120 net.cpp:137] Memory required for data: 328910000
I1210 18:03:19.952056 22120 layer_factory.cpp:58] Creating layer newconv_added1
I1210 18:03:19.952056 22120 net.cpp:84] Creating Layer newconv_added1
I1210 18:03:19.952056 22120 net.cpp:406] newconv_added1 <- conv2_2
I1210 18:03:19.952056 22120 net.cpp:380] newconv_added1 -> newconv_added1
I1210 18:03:19.953058 22120 net.cpp:122] Setting up newconv_added1
I1210 18:03:19.953058 22120 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 18:03:19.953058 22120 net.cpp:137] Memory required for data: 349390000
I1210 18:03:19.953058 22120 layer_factory.cpp:58] Creating layer pool2_1
I1210 18:03:19.953058 22120 net.cpp:84] Creating Layer pool2_1
I1210 18:03:19.953058 22120 net.cpp:406] pool2_1 <- newconv_added1
I1210 18:03:19.953058 22120 net.cpp:380] pool2_1 -> pool2_1
I1210 18:03:19.953058 22120 net.cpp:122] Setting up pool2_1
I1210 18:03:19.953058 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.953058 22120 net.cpp:137] Memory required for data: 354510000
I1210 18:03:19.953058 22120 layer_factory.cpp:58] Creating layer conv3
I1210 18:03:19.953058 22120 net.cpp:84] Creating Layer conv3
I1210 18:03:19.953058 22120 net.cpp:406] conv3 <- pool2_1
I1210 18:03:19.953058 22120 net.cpp:380] conv3 -> conv3
I1210 18:03:19.955058 22120 net.cpp:122] Setting up conv3
I1210 18:03:19.955058 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.955058 22120 net.cpp:137] Memory required for data: 359630000
I1210 18:03:19.955058 22120 layer_factory.cpp:58] Creating layer bn3
I1210 18:03:19.955058 22120 net.cpp:84] Creating Layer bn3
I1210 18:03:19.955058 22120 net.cpp:406] bn3 <- conv3
I1210 18:03:19.955058 22120 net.cpp:367] bn3 -> conv3 (in-place)
I1210 18:03:19.955058 22120 net.cpp:122] Setting up bn3
I1210 18:03:19.955058 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.955058 22120 net.cpp:137] Memory required for data: 364750000
I1210 18:03:19.955058 22120 layer_factory.cpp:58] Creating layer scale3
I1210 18:03:19.955058 22120 net.cpp:84] Creating Layer scale3
I1210 18:03:19.955058 22120 net.cpp:406] scale3 <- conv3
I1210 18:03:19.955058 22120 net.cpp:367] scale3 -> conv3 (in-place)
I1210 18:03:19.955058 22120 layer_factory.cpp:58] Creating layer scale3
I1210 18:03:19.955058 22120 net.cpp:122] Setting up scale3
I1210 18:03:19.955058 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.955058 22120 net.cpp:137] Memory required for data: 369870000
I1210 18:03:19.955058 22120 layer_factory.cpp:58] Creating layer relu3
I1210 18:03:19.955058 22120 net.cpp:84] Creating Layer relu3
I1210 18:03:19.955058 22120 net.cpp:406] relu3 <- conv3
I1210 18:03:19.955058 22120 net.cpp:367] relu3 -> conv3 (in-place)
I1210 18:03:19.956068 22120 net.cpp:122] Setting up relu3
I1210 18:03:19.956068 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.956068 22120 net.cpp:137] Memory required for data: 374990000
I1210 18:03:19.956068 22120 layer_factory.cpp:58] Creating layer conv3_1
I1210 18:03:19.956068 22120 net.cpp:84] Creating Layer conv3_1
I1210 18:03:19.956068 22120 net.cpp:406] conv3_1 <- conv3
I1210 18:03:19.956068 22120 net.cpp:380] conv3_1 -> conv3_1
I1210 18:03:19.957059 22120 net.cpp:122] Setting up conv3_1
I1210 18:03:19.957059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.957059 22120 net.cpp:137] Memory required for data: 380110000
I1210 18:03:19.957059 22120 layer_factory.cpp:58] Creating layer bn3_1
I1210 18:03:19.957059 22120 net.cpp:84] Creating Layer bn3_1
I1210 18:03:19.957059 22120 net.cpp:406] bn3_1 <- conv3_1
I1210 18:03:19.957059 22120 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1210 18:03:19.957059 22120 net.cpp:122] Setting up bn3_1
I1210 18:03:19.957059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.957059 22120 net.cpp:137] Memory required for data: 385230000
I1210 18:03:19.957059 22120 layer_factory.cpp:58] Creating layer scale3_1
I1210 18:03:19.957059 22120 net.cpp:84] Creating Layer scale3_1
I1210 18:03:19.957059 22120 net.cpp:406] scale3_1 <- conv3_1
I1210 18:03:19.957059 22120 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1210 18:03:19.957059 22120 layer_factory.cpp:58] Creating layer scale3_1
I1210 18:03:19.957059 22120 net.cpp:122] Setting up scale3_1
I1210 18:03:19.957059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.957059 22120 net.cpp:137] Memory required for data: 390350000
I1210 18:03:19.957059 22120 layer_factory.cpp:58] Creating layer relu3_1
I1210 18:03:19.957059 22120 net.cpp:84] Creating Layer relu3_1
I1210 18:03:19.957059 22120 net.cpp:406] relu3_1 <- conv3_1
I1210 18:03:19.957059 22120 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1210 18:03:19.957059 22120 net.cpp:122] Setting up relu3_1
I1210 18:03:19.957059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.957059 22120 net.cpp:137] Memory required for data: 395470000
I1210 18:03:19.957059 22120 layer_factory.cpp:58] Creating layer conv4
I1210 18:03:19.957059 22120 net.cpp:84] Creating Layer conv4
I1210 18:03:19.957059 22120 net.cpp:406] conv4 <- conv3_1
I1210 18:03:19.957059 22120 net.cpp:380] conv4 -> conv4
I1210 18:03:19.959059 22120 net.cpp:122] Setting up conv4
I1210 18:03:19.959059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.959059 22120 net.cpp:137] Memory required for data: 400590000
I1210 18:03:19.959059 22120 layer_factory.cpp:58] Creating layer bn4
I1210 18:03:19.959059 22120 net.cpp:84] Creating Layer bn4
I1210 18:03:19.959059 22120 net.cpp:406] bn4 <- conv4
I1210 18:03:19.959059 22120 net.cpp:367] bn4 -> conv4 (in-place)
I1210 18:03:19.959059 22120 net.cpp:122] Setting up bn4
I1210 18:03:19.959059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.959059 22120 net.cpp:137] Memory required for data: 405710000
I1210 18:03:19.959059 22120 layer_factory.cpp:58] Creating layer scale4
I1210 18:03:19.959059 22120 net.cpp:84] Creating Layer scale4
I1210 18:03:19.959059 22120 net.cpp:406] scale4 <- conv4
I1210 18:03:19.959059 22120 net.cpp:367] scale4 -> conv4 (in-place)
I1210 18:03:19.959059 22120 layer_factory.cpp:58] Creating layer scale4
I1210 18:03:19.959059 22120 net.cpp:122] Setting up scale4
I1210 18:03:19.959059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.959059 22120 net.cpp:137] Memory required for data: 410830000
I1210 18:03:19.959059 22120 layer_factory.cpp:58] Creating layer relu4
I1210 18:03:19.959059 22120 net.cpp:84] Creating Layer relu4
I1210 18:03:19.959059 22120 net.cpp:406] relu4 <- conv4
I1210 18:03:19.959059 22120 net.cpp:367] relu4 -> conv4 (in-place)
I1210 18:03:19.960059 22120 net.cpp:122] Setting up relu4
I1210 18:03:19.960059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.960059 22120 net.cpp:137] Memory required for data: 415950000
I1210 18:03:19.960059 22120 layer_factory.cpp:58] Creating layer conv4_1
I1210 18:03:19.960059 22120 net.cpp:84] Creating Layer conv4_1
I1210 18:03:19.960059 22120 net.cpp:406] conv4_1 <- conv4
I1210 18:03:19.960059 22120 net.cpp:380] conv4_1 -> conv4_1
I1210 18:03:19.961058 22120 net.cpp:122] Setting up conv4_1
I1210 18:03:19.961058 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.961058 22120 net.cpp:137] Memory required for data: 421070000
I1210 18:03:19.961058 22120 layer_factory.cpp:58] Creating layer bn4_1
I1210 18:03:19.961058 22120 net.cpp:84] Creating Layer bn4_1
I1210 18:03:19.961058 22120 net.cpp:406] bn4_1 <- conv4_1
I1210 18:03:19.961058 22120 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1210 18:03:19.961058 22120 net.cpp:122] Setting up bn4_1
I1210 18:03:19.961058 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.961058 22120 net.cpp:137] Memory required for data: 426190000
I1210 18:03:19.961058 22120 layer_factory.cpp:58] Creating layer scale4_1
I1210 18:03:19.961058 22120 net.cpp:84] Creating Layer scale4_1
I1210 18:03:19.961058 22120 net.cpp:406] scale4_1 <- conv4_1
I1210 18:03:19.961058 22120 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1210 18:03:19.961058 22120 layer_factory.cpp:58] Creating layer scale4_1
I1210 18:03:19.962059 22120 net.cpp:122] Setting up scale4_1
I1210 18:03:19.962059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.962059 22120 net.cpp:137] Memory required for data: 431310000
I1210 18:03:19.962059 22120 layer_factory.cpp:58] Creating layer relu4_1
I1210 18:03:19.962059 22120 net.cpp:84] Creating Layer relu4_1
I1210 18:03:19.962059 22120 net.cpp:406] relu4_1 <- conv4_1
I1210 18:03:19.962059 22120 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1210 18:03:19.962059 22120 net.cpp:122] Setting up relu4_1
I1210 18:03:19.962059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:19.962059 22120 net.cpp:137] Memory required for data: 436430000
I1210 18:03:19.962059 22120 layer_factory.cpp:58] Creating layer conv4_2
I1210 18:03:19.962059 22120 net.cpp:84] Creating Layer conv4_2
I1210 18:03:19.962059 22120 net.cpp:406] conv4_2 <- conv4_1
I1210 18:03:19.962059 22120 net.cpp:380] conv4_2 -> conv4_2
I1210 18:03:19.963058 22120 net.cpp:122] Setting up conv4_2
I1210 18:03:19.963058 22120 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 18:03:19.963058 22120 net.cpp:137] Memory required for data: 442369200
I1210 18:03:19.963058 22120 layer_factory.cpp:58] Creating layer bn4_2
I1210 18:03:19.963058 22120 net.cpp:84] Creating Layer bn4_2
I1210 18:03:19.963058 22120 net.cpp:406] bn4_2 <- conv4_2
I1210 18:03:19.963058 22120 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1210 18:03:19.963058 22120 net.cpp:122] Setting up bn4_2
I1210 18:03:19.963058 22120 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 18:03:19.963058 22120 net.cpp:137] Memory required for data: 448308400
I1210 18:03:19.963058 22120 layer_factory.cpp:58] Creating layer scale4_2
I1210 18:03:19.963058 22120 net.cpp:84] Creating Layer scale4_2
I1210 18:03:19.963058 22120 net.cpp:406] scale4_2 <- conv4_2
I1210 18:03:19.963058 22120 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1210 18:03:19.963058 22120 layer_factory.cpp:58] Creating layer scale4_2
I1210 18:03:19.963058 22120 net.cpp:122] Setting up scale4_2
I1210 18:03:19.963058 22120 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 18:03:19.963058 22120 net.cpp:137] Memory required for data: 454247600
I1210 18:03:19.963058 22120 layer_factory.cpp:58] Creating layer relu4_2
I1210 18:03:19.963058 22120 net.cpp:84] Creating Layer relu4_2
I1210 18:03:19.964059 22120 net.cpp:406] relu4_2 <- conv4_2
I1210 18:03:19.964059 22120 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1210 18:03:19.964059 22120 net.cpp:122] Setting up relu4_2
I1210 18:03:19.964059 22120 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 18:03:19.964059 22120 net.cpp:137] Memory required for data: 460186800
I1210 18:03:19.964059 22120 layer_factory.cpp:58] Creating layer added_new_conv2
I1210 18:03:19.964059 22120 net.cpp:84] Creating Layer added_new_conv2
I1210 18:03:19.964059 22120 net.cpp:406] added_new_conv2 <- conv4_2
I1210 18:03:19.964059 22120 net.cpp:380] added_new_conv2 -> added_new_conv2
I1210 18:03:19.965059 22120 net.cpp:122] Setting up added_new_conv2
I1210 18:03:19.965059 22120 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 18:03:19.965059 22120 net.cpp:137] Memory required for data: 466126000
I1210 18:03:19.965059 22120 layer_factory.cpp:58] Creating layer pool4_2
I1210 18:03:19.965059 22120 net.cpp:84] Creating Layer pool4_2
I1210 18:03:19.965059 22120 net.cpp:406] pool4_2 <- added_new_conv2
I1210 18:03:19.965059 22120 net.cpp:380] pool4_2 -> pool4_2
I1210 18:03:19.965059 22120 net.cpp:122] Setting up pool4_2
I1210 18:03:19.965059 22120 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 18:03:19.965059 22120 net.cpp:137] Memory required for data: 467610800
I1210 18:03:19.965059 22120 layer_factory.cpp:58] Creating layer conv4_0
I1210 18:03:19.965059 22120 net.cpp:84] Creating Layer conv4_0
I1210 18:03:19.965059 22120 net.cpp:406] conv4_0 <- pool4_2
I1210 18:03:19.965059 22120 net.cpp:380] conv4_0 -> conv4_0
I1210 18:03:19.966059 22120 net.cpp:122] Setting up conv4_0
I1210 18:03:19.966059 22120 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 18:03:19.966059 22120 net.cpp:137] Memory required for data: 469095600
I1210 18:03:19.966059 22120 layer_factory.cpp:58] Creating layer bn4_0
I1210 18:03:19.966059 22120 net.cpp:84] Creating Layer bn4_0
I1210 18:03:19.967059 22120 net.cpp:406] bn4_0 <- conv4_0
I1210 18:03:19.967059 22120 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1210 18:03:19.967059 22120 net.cpp:122] Setting up bn4_0
I1210 18:03:19.967059 22120 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 18:03:19.967059 22120 net.cpp:137] Memory required for data: 470580400
I1210 18:03:19.967059 22120 layer_factory.cpp:58] Creating layer scale4_0
I1210 18:03:19.967059 22120 net.cpp:84] Creating Layer scale4_0
I1210 18:03:19.967059 22120 net.cpp:406] scale4_0 <- conv4_0
I1210 18:03:19.967059 22120 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1210 18:03:19.967059 22120 layer_factory.cpp:58] Creating layer scale4_0
I1210 18:03:19.967059 22120 net.cpp:122] Setting up scale4_0
I1210 18:03:19.967059 22120 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 18:03:19.967059 22120 net.cpp:137] Memory required for data: 472065200
I1210 18:03:19.967059 22120 layer_factory.cpp:58] Creating layer relu4_0
I1210 18:03:19.967059 22120 net.cpp:84] Creating Layer relu4_0
I1210 18:03:19.967059 22120 net.cpp:406] relu4_0 <- conv4_0
I1210 18:03:19.967059 22120 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1210 18:03:19.967059 22120 net.cpp:122] Setting up relu4_0
I1210 18:03:19.967059 22120 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 18:03:19.967059 22120 net.cpp:137] Memory required for data: 473550000
I1210 18:03:19.967059 22120 layer_factory.cpp:58] Creating layer conv11
I1210 18:03:19.967059 22120 net.cpp:84] Creating Layer conv11
I1210 18:03:19.967059 22120 net.cpp:406] conv11 <- conv4_0
I1210 18:03:19.967059 22120 net.cpp:380] conv11 -> conv11
I1210 18:03:19.969059 22120 net.cpp:122] Setting up conv11
I1210 18:03:19.969059 22120 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 18:03:19.969059 22120 net.cpp:137] Memory required for data: 475342000
I1210 18:03:19.969059 22120 layer_factory.cpp:58] Creating layer bn_conv11
I1210 18:03:19.969059 22120 net.cpp:84] Creating Layer bn_conv11
I1210 18:03:19.969059 22120 net.cpp:406] bn_conv11 <- conv11
I1210 18:03:19.969059 22120 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1210 18:03:19.969059 22120 net.cpp:122] Setting up bn_conv11
I1210 18:03:19.969059 22120 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 18:03:19.969059 22120 net.cpp:137] Memory required for data: 477134000
I1210 18:03:19.969059 22120 layer_factory.cpp:58] Creating layer scale_conv11
I1210 18:03:19.969059 22120 net.cpp:84] Creating Layer scale_conv11
I1210 18:03:19.969059 22120 net.cpp:406] scale_conv11 <- conv11
I1210 18:03:19.969059 22120 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1210 18:03:19.969059 22120 layer_factory.cpp:58] Creating layer scale_conv11
I1210 18:03:19.969059 22120 net.cpp:122] Setting up scale_conv11
I1210 18:03:19.969059 22120 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 18:03:19.969059 22120 net.cpp:137] Memory required for data: 478926000
I1210 18:03:19.969059 22120 layer_factory.cpp:58] Creating layer relu_conv11
I1210 18:03:19.969059 22120 net.cpp:84] Creating Layer relu_conv11
I1210 18:03:19.970059 22120 net.cpp:406] relu_conv11 <- conv11
I1210 18:03:19.970059 22120 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1210 18:03:19.970059 22120 net.cpp:122] Setting up relu_conv11
I1210 18:03:19.970059 22120 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 18:03:19.970059 22120 net.cpp:137] Memory required for data: 480718000
I1210 18:03:19.970059 22120 layer_factory.cpp:58] Creating layer conv12
I1210 18:03:19.970059 22120 net.cpp:84] Creating Layer conv12
I1210 18:03:19.970059 22120 net.cpp:406] conv12 <- conv11
I1210 18:03:19.970059 22120 net.cpp:380] conv12 -> conv12
I1210 18:03:19.972059 22120 net.cpp:122] Setting up conv12
I1210 18:03:19.972059 22120 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 18:03:19.972059 22120 net.cpp:137] Memory required for data: 483022000
I1210 18:03:19.972059 22120 layer_factory.cpp:58] Creating layer bn_conv12
I1210 18:03:19.972059 22120 net.cpp:84] Creating Layer bn_conv12
I1210 18:03:19.972059 22120 net.cpp:406] bn_conv12 <- conv12
I1210 18:03:19.972059 22120 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1210 18:03:19.972059 22120 net.cpp:122] Setting up bn_conv12
I1210 18:03:19.972059 22120 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 18:03:19.972059 22120 net.cpp:137] Memory required for data: 485326000
I1210 18:03:19.972059 22120 layer_factory.cpp:58] Creating layer scale_conv12
I1210 18:03:19.972059 22120 net.cpp:84] Creating Layer scale_conv12
I1210 18:03:19.972059 22120 net.cpp:406] scale_conv12 <- conv12
I1210 18:03:19.972059 22120 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1210 18:03:19.972059 22120 layer_factory.cpp:58] Creating layer scale_conv12
I1210 18:03:19.972059 22120 net.cpp:122] Setting up scale_conv12
I1210 18:03:19.972059 22120 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 18:03:19.972059 22120 net.cpp:137] Memory required for data: 487630000
I1210 18:03:19.972059 22120 layer_factory.cpp:58] Creating layer relu_conv12
I1210 18:03:19.972059 22120 net.cpp:84] Creating Layer relu_conv12
I1210 18:03:19.972059 22120 net.cpp:406] relu_conv12 <- conv12
I1210 18:03:19.972059 22120 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1210 18:03:19.972059 22120 net.cpp:122] Setting up relu_conv12
I1210 18:03:19.972059 22120 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 18:03:19.972059 22120 net.cpp:137] Memory required for data: 489934000
I1210 18:03:19.972059 22120 layer_factory.cpp:58] Creating layer poolcp6
I1210 18:03:19.972059 22120 net.cpp:84] Creating Layer poolcp6
I1210 18:03:19.973059 22120 net.cpp:406] poolcp6 <- conv12
I1210 18:03:19.973059 22120 net.cpp:380] poolcp6 -> poolcp6
I1210 18:03:19.973059 22120 net.cpp:122] Setting up poolcp6
I1210 18:03:19.973059 22120 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1210 18:03:19.973059 22120 net.cpp:137] Memory required for data: 489970000
I1210 18:03:19.973059 22120 layer_factory.cpp:58] Creating layer ip1
I1210 18:03:19.973059 22120 net.cpp:84] Creating Layer ip1
I1210 18:03:19.973059 22120 net.cpp:406] ip1 <- poolcp6
I1210 18:03:19.973059 22120 net.cpp:380] ip1 -> ip1
I1210 18:03:19.973059 22120 net.cpp:122] Setting up ip1
I1210 18:03:19.973059 22120 net.cpp:129] Top shape: 100 100 (10000)
I1210 18:03:19.973059 22120 net.cpp:137] Memory required for data: 490010000
I1210 18:03:19.973059 22120 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1210 18:03:19.973059 22120 net.cpp:84] Creating Layer ip1_ip1_0_split
I1210 18:03:19.973059 22120 net.cpp:406] ip1_ip1_0_split <- ip1
I1210 18:03:19.973059 22120 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1210 18:03:19.973059 22120 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1210 18:03:19.973059 22120 net.cpp:122] Setting up ip1_ip1_0_split
I1210 18:03:19.973059 22120 net.cpp:129] Top shape: 100 100 (10000)
I1210 18:03:19.973059 22120 net.cpp:129] Top shape: 100 100 (10000)
I1210 18:03:19.973059 22120 net.cpp:137] Memory required for data: 490090000
I1210 18:03:19.973059 22120 layer_factory.cpp:58] Creating layer accuracy_training
I1210 18:03:19.973059 22120 net.cpp:84] Creating Layer accuracy_training
I1210 18:03:19.973059 22120 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1210 18:03:19.973059 22120 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1210 18:03:19.973059 22120 net.cpp:380] accuracy_training -> accuracy_training
I1210 18:03:19.973059 22120 net.cpp:122] Setting up accuracy_training
I1210 18:03:19.973059 22120 net.cpp:129] Top shape: (1)
I1210 18:03:19.973059 22120 net.cpp:137] Memory required for data: 490090004
I1210 18:03:19.973059 22120 layer_factory.cpp:58] Creating layer loss
I1210 18:03:19.973059 22120 net.cpp:84] Creating Layer loss
I1210 18:03:19.973059 22120 net.cpp:406] loss <- ip1_ip1_0_split_1
I1210 18:03:19.973059 22120 net.cpp:406] loss <- label_cifar_1_split_1
I1210 18:03:19.973059 22120 net.cpp:380] loss -> loss
I1210 18:03:19.973059 22120 layer_factory.cpp:58] Creating layer loss
I1210 18:03:19.973059 22120 net.cpp:122] Setting up loss
I1210 18:03:19.973059 22120 net.cpp:129] Top shape: (1)
I1210 18:03:19.973059 22120 net.cpp:132]     with loss weight 1
I1210 18:03:19.973059 22120 net.cpp:137] Memory required for data: 490090008
I1210 18:03:19.973059 22120 net.cpp:198] loss needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:200] accuracy_training does not need backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] ip1 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] poolcp6 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] relu_conv12 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] scale_conv12 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] bn_conv12 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] conv12 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] relu_conv11 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] scale_conv11 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] bn_conv11 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] conv11 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] relu4_0 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] scale4_0 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] bn4_0 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] conv4_0 needs backward computation.
I1210 18:03:19.973059 22120 net.cpp:198] pool4_2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] added_new_conv2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] relu4_2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] scale4_2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] bn4_2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] conv4_2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] relu4_1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] scale4_1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] bn4_1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] conv4_1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] relu4 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] scale4 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] bn4 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] conv4 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] relu3_1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] scale3_1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] bn3_1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] conv3_1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] relu3 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] scale3 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] bn3 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] conv3 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] pool2_1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] newconv_added1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] relu2_2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] scale2_2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] bn2_2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] conv2_2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] relu2_1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] scale2_1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] bn2_1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] conv2_1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] relu2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] scale2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] bn2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] conv2 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] relu1_0 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] scale1_0 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] bn1_0 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] conv1_0 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] relu1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] scale1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] bn1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:198] conv1 needs backward computation.
I1210 18:03:19.974059 22120 net.cpp:200] label_cifar_1_split does not need backward computation.
I1210 18:03:19.974059 22120 net.cpp:200] cifar does not need backward computation.
I1210 18:03:19.974059 22120 net.cpp:242] This network produces output accuracy_training
I1210 18:03:19.974059 22120 net.cpp:242] This network produces output loss
I1210 18:03:19.974059 22120 net.cpp:255] Network initialization done.
I1210 18:03:19.975059 22120 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 18:03:19.975059 22120 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 18:03:19.975059 22120 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1210 18:03:19.975059 22120 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1210 18:03:19.975059 22120 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_v2_360k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1210 18:03:19.975059 22120 layer_factory.cpp:58] Creating layer cifar
I1210 18:03:19.981060 22120 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1210 18:03:19.983055 22120 net.cpp:84] Creating Layer cifar
I1210 18:03:19.983055 22120 net.cpp:380] cifar -> data
I1210 18:03:19.983055 22120 net.cpp:380] cifar -> label
I1210 18:03:19.983055 22120 data_layer.cpp:45] output data size: 100,3,32,32
I1210 18:03:19.988059 22120 net.cpp:122] Setting up cifar
I1210 18:03:19.989059 22120 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1210 18:03:19.989059 22120 net.cpp:129] Top shape: 100 (100)
I1210 18:03:19.989059 22120 net.cpp:137] Memory required for data: 1229200
I1210 18:03:19.989059 22120 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1210 18:03:19.989059 22120 net.cpp:84] Creating Layer label_cifar_1_split
I1210 18:03:19.989059 22120 net.cpp:406] label_cifar_1_split <- label
I1210 18:03:19.989059 22120 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1210 18:03:19.989059 22120 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1210 18:03:19.989059 22120 net.cpp:122] Setting up label_cifar_1_split
I1210 18:03:19.989059 22120 net.cpp:129] Top shape: 100 (100)
I1210 18:03:19.989059 22120 net.cpp:129] Top shape: 100 (100)
I1210 18:03:19.989059 22120 net.cpp:137] Memory required for data: 1230000
I1210 18:03:19.989059 22120 layer_factory.cpp:58] Creating layer conv1
I1210 18:03:19.989059 22120 net.cpp:84] Creating Layer conv1
I1210 18:03:19.989059 22120 net.cpp:406] conv1 <- data
I1210 18:03:19.989059 22120 net.cpp:380] conv1 -> conv1
I1210 18:03:19.990058 20188 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 18:03:19.990058 22120 net.cpp:122] Setting up conv1
I1210 18:03:19.990058 22120 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 18:03:19.990058 22120 net.cpp:137] Memory required for data: 13518000
I1210 18:03:19.990058 22120 layer_factory.cpp:58] Creating layer bn1
I1210 18:03:19.990058 22120 net.cpp:84] Creating Layer bn1
I1210 18:03:19.990058 22120 net.cpp:406] bn1 <- conv1
I1210 18:03:19.990058 22120 net.cpp:367] bn1 -> conv1 (in-place)
I1210 18:03:19.991058 22120 net.cpp:122] Setting up bn1
I1210 18:03:19.991058 22120 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 18:03:19.991058 22120 net.cpp:137] Memory required for data: 25806000
I1210 18:03:19.991058 22120 layer_factory.cpp:58] Creating layer scale1
I1210 18:03:19.991058 22120 net.cpp:84] Creating Layer scale1
I1210 18:03:19.991058 22120 net.cpp:406] scale1 <- conv1
I1210 18:03:19.991058 22120 net.cpp:367] scale1 -> conv1 (in-place)
I1210 18:03:19.991058 22120 layer_factory.cpp:58] Creating layer scale1
I1210 18:03:19.991058 22120 net.cpp:122] Setting up scale1
I1210 18:03:19.991058 22120 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 18:03:19.991058 22120 net.cpp:137] Memory required for data: 38094000
I1210 18:03:19.991058 22120 layer_factory.cpp:58] Creating layer relu1
I1210 18:03:19.991058 22120 net.cpp:84] Creating Layer relu1
I1210 18:03:19.991058 22120 net.cpp:406] relu1 <- conv1
I1210 18:03:19.991058 22120 net.cpp:367] relu1 -> conv1 (in-place)
I1210 18:03:19.991058 22120 net.cpp:122] Setting up relu1
I1210 18:03:19.991058 22120 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 18:03:19.991058 22120 net.cpp:137] Memory required for data: 50382000
I1210 18:03:19.991058 22120 layer_factory.cpp:58] Creating layer conv1_0
I1210 18:03:19.991058 22120 net.cpp:84] Creating Layer conv1_0
I1210 18:03:19.991058 22120 net.cpp:406] conv1_0 <- conv1
I1210 18:03:19.991058 22120 net.cpp:380] conv1_0 -> conv1_0
I1210 18:03:19.993067 22120 net.cpp:122] Setting up conv1_0
I1210 18:03:19.993067 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.993067 22120 net.cpp:137] Memory required for data: 66766000
I1210 18:03:19.993067 22120 layer_factory.cpp:58] Creating layer bn1_0
I1210 18:03:19.993067 22120 net.cpp:84] Creating Layer bn1_0
I1210 18:03:19.993067 22120 net.cpp:406] bn1_0 <- conv1_0
I1210 18:03:19.993067 22120 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1210 18:03:19.993067 22120 net.cpp:122] Setting up bn1_0
I1210 18:03:19.993067 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.993067 22120 net.cpp:137] Memory required for data: 83150000
I1210 18:03:19.993067 22120 layer_factory.cpp:58] Creating layer scale1_0
I1210 18:03:19.993067 22120 net.cpp:84] Creating Layer scale1_0
I1210 18:03:19.993067 22120 net.cpp:406] scale1_0 <- conv1_0
I1210 18:03:19.993067 22120 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1210 18:03:19.993067 22120 layer_factory.cpp:58] Creating layer scale1_0
I1210 18:03:19.993067 22120 net.cpp:122] Setting up scale1_0
I1210 18:03:19.993067 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.993067 22120 net.cpp:137] Memory required for data: 99534000
I1210 18:03:19.993067 22120 layer_factory.cpp:58] Creating layer relu1_0
I1210 18:03:19.993067 22120 net.cpp:84] Creating Layer relu1_0
I1210 18:03:19.993067 22120 net.cpp:406] relu1_0 <- conv1_0
I1210 18:03:19.993067 22120 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1210 18:03:19.993067 22120 net.cpp:122] Setting up relu1_0
I1210 18:03:19.993067 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.993067 22120 net.cpp:137] Memory required for data: 115918000
I1210 18:03:19.993067 22120 layer_factory.cpp:58] Creating layer conv2
I1210 18:03:19.993067 22120 net.cpp:84] Creating Layer conv2
I1210 18:03:19.993067 22120 net.cpp:406] conv2 <- conv1_0
I1210 18:03:19.994060 22120 net.cpp:380] conv2 -> conv2
I1210 18:03:19.995056 22120 net.cpp:122] Setting up conv2
I1210 18:03:19.995056 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.995056 22120 net.cpp:137] Memory required for data: 132302000
I1210 18:03:19.995056 22120 layer_factory.cpp:58] Creating layer bn2
I1210 18:03:19.995056 22120 net.cpp:84] Creating Layer bn2
I1210 18:03:19.995056 22120 net.cpp:406] bn2 <- conv2
I1210 18:03:19.995056 22120 net.cpp:367] bn2 -> conv2 (in-place)
I1210 18:03:19.995056 22120 net.cpp:122] Setting up bn2
I1210 18:03:19.995056 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.995056 22120 net.cpp:137] Memory required for data: 148686000
I1210 18:03:19.995056 22120 layer_factory.cpp:58] Creating layer scale2
I1210 18:03:19.995056 22120 net.cpp:84] Creating Layer scale2
I1210 18:03:19.995056 22120 net.cpp:406] scale2 <- conv2
I1210 18:03:19.995056 22120 net.cpp:367] scale2 -> conv2 (in-place)
I1210 18:03:19.996057 22120 layer_factory.cpp:58] Creating layer scale2
I1210 18:03:19.996057 22120 net.cpp:122] Setting up scale2
I1210 18:03:19.996057 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.996057 22120 net.cpp:137] Memory required for data: 165070000
I1210 18:03:19.996057 22120 layer_factory.cpp:58] Creating layer relu2
I1210 18:03:19.996057 22120 net.cpp:84] Creating Layer relu2
I1210 18:03:19.996057 22120 net.cpp:406] relu2 <- conv2
I1210 18:03:19.996057 22120 net.cpp:367] relu2 -> conv2 (in-place)
I1210 18:03:19.996057 22120 net.cpp:122] Setting up relu2
I1210 18:03:19.996057 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.996057 22120 net.cpp:137] Memory required for data: 181454000
I1210 18:03:19.996057 22120 layer_factory.cpp:58] Creating layer conv2_1
I1210 18:03:19.996057 22120 net.cpp:84] Creating Layer conv2_1
I1210 18:03:19.996057 22120 net.cpp:406] conv2_1 <- conv2
I1210 18:03:19.996057 22120 net.cpp:380] conv2_1 -> conv2_1
I1210 18:03:19.998059 22120 net.cpp:122] Setting up conv2_1
I1210 18:03:19.998059 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.998059 22120 net.cpp:137] Memory required for data: 197838000
I1210 18:03:19.998059 22120 layer_factory.cpp:58] Creating layer bn2_1
I1210 18:03:19.998059 22120 net.cpp:84] Creating Layer bn2_1
I1210 18:03:19.998059 22120 net.cpp:406] bn2_1 <- conv2_1
I1210 18:03:19.998059 22120 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1210 18:03:19.998059 22120 net.cpp:122] Setting up bn2_1
I1210 18:03:19.998059 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.998059 22120 net.cpp:137] Memory required for data: 214222000
I1210 18:03:19.998059 22120 layer_factory.cpp:58] Creating layer scale2_1
I1210 18:03:19.998059 22120 net.cpp:84] Creating Layer scale2_1
I1210 18:03:19.998059 22120 net.cpp:406] scale2_1 <- conv2_1
I1210 18:03:19.998059 22120 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1210 18:03:19.998059 22120 layer_factory.cpp:58] Creating layer scale2_1
I1210 18:03:19.998059 22120 net.cpp:122] Setting up scale2_1
I1210 18:03:19.998059 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.998059 22120 net.cpp:137] Memory required for data: 230606000
I1210 18:03:19.998059 22120 layer_factory.cpp:58] Creating layer relu2_1
I1210 18:03:19.998059 22120 net.cpp:84] Creating Layer relu2_1
I1210 18:03:19.998059 22120 net.cpp:406] relu2_1 <- conv2_1
I1210 18:03:19.998059 22120 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1210 18:03:19.999058 22120 net.cpp:122] Setting up relu2_1
I1210 18:03:19.999058 22120 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 18:03:19.999058 22120 net.cpp:137] Memory required for data: 246990000
I1210 18:03:19.999058 22120 layer_factory.cpp:58] Creating layer conv2_2
I1210 18:03:19.999058 22120 net.cpp:84] Creating Layer conv2_2
I1210 18:03:19.999058 22120 net.cpp:406] conv2_2 <- conv2_1
I1210 18:03:19.999058 22120 net.cpp:380] conv2_2 -> conv2_2
I1210 18:03:20.000056 22120 net.cpp:122] Setting up conv2_2
I1210 18:03:20.000056 22120 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 18:03:20.000056 22120 net.cpp:137] Memory required for data: 267470000
I1210 18:03:20.000056 22120 layer_factory.cpp:58] Creating layer bn2_2
I1210 18:03:20.000056 22120 net.cpp:84] Creating Layer bn2_2
I1210 18:03:20.000056 22120 net.cpp:406] bn2_2 <- conv2_2
I1210 18:03:20.000056 22120 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1210 18:03:20.001058 22120 net.cpp:122] Setting up bn2_2
I1210 18:03:20.001058 22120 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 18:03:20.001058 22120 net.cpp:137] Memory required for data: 287950000
I1210 18:03:20.001058 22120 layer_factory.cpp:58] Creating layer scale2_2
I1210 18:03:20.001058 22120 net.cpp:84] Creating Layer scale2_2
I1210 18:03:20.001058 22120 net.cpp:406] scale2_2 <- conv2_2
I1210 18:03:20.001058 22120 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1210 18:03:20.001058 22120 layer_factory.cpp:58] Creating layer scale2_2
I1210 18:03:20.001058 22120 net.cpp:122] Setting up scale2_2
I1210 18:03:20.001058 22120 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 18:03:20.001058 22120 net.cpp:137] Memory required for data: 308430000
I1210 18:03:20.001058 22120 layer_factory.cpp:58] Creating layer relu2_2
I1210 18:03:20.001058 22120 net.cpp:84] Creating Layer relu2_2
I1210 18:03:20.001058 22120 net.cpp:406] relu2_2 <- conv2_2
I1210 18:03:20.001058 22120 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1210 18:03:20.001058 22120 net.cpp:122] Setting up relu2_2
I1210 18:03:20.001058 22120 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 18:03:20.001058 22120 net.cpp:137] Memory required for data: 328910000
I1210 18:03:20.001058 22120 layer_factory.cpp:58] Creating layer newconv_added1
I1210 18:03:20.001058 22120 net.cpp:84] Creating Layer newconv_added1
I1210 18:03:20.001058 22120 net.cpp:406] newconv_added1 <- conv2_2
I1210 18:03:20.001058 22120 net.cpp:380] newconv_added1 -> newconv_added1
I1210 18:03:20.003057 22120 net.cpp:122] Setting up newconv_added1
I1210 18:03:20.003057 22120 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 18:03:20.003057 22120 net.cpp:137] Memory required for data: 349390000
I1210 18:03:20.003057 22120 layer_factory.cpp:58] Creating layer pool2_1
I1210 18:03:20.003057 22120 net.cpp:84] Creating Layer pool2_1
I1210 18:03:20.003057 22120 net.cpp:406] pool2_1 <- newconv_added1
I1210 18:03:20.003057 22120 net.cpp:380] pool2_1 -> pool2_1
I1210 18:03:20.003057 22120 net.cpp:122] Setting up pool2_1
I1210 18:03:20.003057 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.003057 22120 net.cpp:137] Memory required for data: 354510000
I1210 18:03:20.003057 22120 layer_factory.cpp:58] Creating layer conv3
I1210 18:03:20.003057 22120 net.cpp:84] Creating Layer conv3
I1210 18:03:20.003057 22120 net.cpp:406] conv3 <- pool2_1
I1210 18:03:20.003057 22120 net.cpp:380] conv3 -> conv3
I1210 18:03:20.004058 22120 net.cpp:122] Setting up conv3
I1210 18:03:20.004058 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.004058 22120 net.cpp:137] Memory required for data: 359630000
I1210 18:03:20.004058 22120 layer_factory.cpp:58] Creating layer bn3
I1210 18:03:20.004058 22120 net.cpp:84] Creating Layer bn3
I1210 18:03:20.004058 22120 net.cpp:406] bn3 <- conv3
I1210 18:03:20.004058 22120 net.cpp:367] bn3 -> conv3 (in-place)
I1210 18:03:20.005059 22120 net.cpp:122] Setting up bn3
I1210 18:03:20.005059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.005059 22120 net.cpp:137] Memory required for data: 364750000
I1210 18:03:20.005059 22120 layer_factory.cpp:58] Creating layer scale3
I1210 18:03:20.005059 22120 net.cpp:84] Creating Layer scale3
I1210 18:03:20.005059 22120 net.cpp:406] scale3 <- conv3
I1210 18:03:20.005059 22120 net.cpp:367] scale3 -> conv3 (in-place)
I1210 18:03:20.005059 22120 layer_factory.cpp:58] Creating layer scale3
I1210 18:03:20.005059 22120 net.cpp:122] Setting up scale3
I1210 18:03:20.005059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.005059 22120 net.cpp:137] Memory required for data: 369870000
I1210 18:03:20.005059 22120 layer_factory.cpp:58] Creating layer relu3
I1210 18:03:20.005059 22120 net.cpp:84] Creating Layer relu3
I1210 18:03:20.005059 22120 net.cpp:406] relu3 <- conv3
I1210 18:03:20.005059 22120 net.cpp:367] relu3 -> conv3 (in-place)
I1210 18:03:20.005059 22120 net.cpp:122] Setting up relu3
I1210 18:03:20.005059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.005059 22120 net.cpp:137] Memory required for data: 374990000
I1210 18:03:20.005059 22120 layer_factory.cpp:58] Creating layer conv3_1
I1210 18:03:20.005059 22120 net.cpp:84] Creating Layer conv3_1
I1210 18:03:20.005059 22120 net.cpp:406] conv3_1 <- conv3
I1210 18:03:20.005059 22120 net.cpp:380] conv3_1 -> conv3_1
I1210 18:03:20.007067 22120 net.cpp:122] Setting up conv3_1
I1210 18:03:20.007067 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.007067 22120 net.cpp:137] Memory required for data: 380110000
I1210 18:03:20.007067 22120 layer_factory.cpp:58] Creating layer bn3_1
I1210 18:03:20.007067 22120 net.cpp:84] Creating Layer bn3_1
I1210 18:03:20.007067 22120 net.cpp:406] bn3_1 <- conv3_1
I1210 18:03:20.007067 22120 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1210 18:03:20.007067 22120 net.cpp:122] Setting up bn3_1
I1210 18:03:20.007067 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.007067 22120 net.cpp:137] Memory required for data: 385230000
I1210 18:03:20.007067 22120 layer_factory.cpp:58] Creating layer scale3_1
I1210 18:03:20.007067 22120 net.cpp:84] Creating Layer scale3_1
I1210 18:03:20.007067 22120 net.cpp:406] scale3_1 <- conv3_1
I1210 18:03:20.007067 22120 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1210 18:03:20.007067 22120 layer_factory.cpp:58] Creating layer scale3_1
I1210 18:03:20.007067 22120 net.cpp:122] Setting up scale3_1
I1210 18:03:20.007067 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.007067 22120 net.cpp:137] Memory required for data: 390350000
I1210 18:03:20.007067 22120 layer_factory.cpp:58] Creating layer relu3_1
I1210 18:03:20.007067 22120 net.cpp:84] Creating Layer relu3_1
I1210 18:03:20.007067 22120 net.cpp:406] relu3_1 <- conv3_1
I1210 18:03:20.007067 22120 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1210 18:03:20.007067 22120 net.cpp:122] Setting up relu3_1
I1210 18:03:20.007067 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.007067 22120 net.cpp:137] Memory required for data: 395470000
I1210 18:03:20.007067 22120 layer_factory.cpp:58] Creating layer conv4
I1210 18:03:20.007067 22120 net.cpp:84] Creating Layer conv4
I1210 18:03:20.007067 22120 net.cpp:406] conv4 <- conv3_1
I1210 18:03:20.007067 22120 net.cpp:380] conv4 -> conv4
I1210 18:03:20.009066 22120 net.cpp:122] Setting up conv4
I1210 18:03:20.009066 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.009066 22120 net.cpp:137] Memory required for data: 400590000
I1210 18:03:20.009066 22120 layer_factory.cpp:58] Creating layer bn4
I1210 18:03:20.009066 22120 net.cpp:84] Creating Layer bn4
I1210 18:03:20.009066 22120 net.cpp:406] bn4 <- conv4
I1210 18:03:20.009066 22120 net.cpp:367] bn4 -> conv4 (in-place)
I1210 18:03:20.009066 22120 net.cpp:122] Setting up bn4
I1210 18:03:20.009066 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.009066 22120 net.cpp:137] Memory required for data: 405710000
I1210 18:03:20.009066 22120 layer_factory.cpp:58] Creating layer scale4
I1210 18:03:20.009066 22120 net.cpp:84] Creating Layer scale4
I1210 18:03:20.009066 22120 net.cpp:406] scale4 <- conv4
I1210 18:03:20.009066 22120 net.cpp:367] scale4 -> conv4 (in-place)
I1210 18:03:20.009066 22120 layer_factory.cpp:58] Creating layer scale4
I1210 18:03:20.009066 22120 net.cpp:122] Setting up scale4
I1210 18:03:20.009066 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.009066 22120 net.cpp:137] Memory required for data: 410830000
I1210 18:03:20.009066 22120 layer_factory.cpp:58] Creating layer relu4
I1210 18:03:20.009066 22120 net.cpp:84] Creating Layer relu4
I1210 18:03:20.009066 22120 net.cpp:406] relu4 <- conv4
I1210 18:03:20.009066 22120 net.cpp:367] relu4 -> conv4 (in-place)
I1210 18:03:20.009066 22120 net.cpp:122] Setting up relu4
I1210 18:03:20.009066 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.009066 22120 net.cpp:137] Memory required for data: 415950000
I1210 18:03:20.009066 22120 layer_factory.cpp:58] Creating layer conv4_1
I1210 18:03:20.009066 22120 net.cpp:84] Creating Layer conv4_1
I1210 18:03:20.009066 22120 net.cpp:406] conv4_1 <- conv4
I1210 18:03:20.009066 22120 net.cpp:380] conv4_1 -> conv4_1
I1210 18:03:20.011059 22120 net.cpp:122] Setting up conv4_1
I1210 18:03:20.011059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.011059 22120 net.cpp:137] Memory required for data: 421070000
I1210 18:03:20.011059 22120 layer_factory.cpp:58] Creating layer bn4_1
I1210 18:03:20.011059 22120 net.cpp:84] Creating Layer bn4_1
I1210 18:03:20.011059 22120 net.cpp:406] bn4_1 <- conv4_1
I1210 18:03:20.011059 22120 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1210 18:03:20.011059 22120 net.cpp:122] Setting up bn4_1
I1210 18:03:20.011059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.011059 22120 net.cpp:137] Memory required for data: 426190000
I1210 18:03:20.011059 22120 layer_factory.cpp:58] Creating layer scale4_1
I1210 18:03:20.011059 22120 net.cpp:84] Creating Layer scale4_1
I1210 18:03:20.011059 22120 net.cpp:406] scale4_1 <- conv4_1
I1210 18:03:20.011059 22120 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1210 18:03:20.011059 22120 layer_factory.cpp:58] Creating layer scale4_1
I1210 18:03:20.011059 22120 net.cpp:122] Setting up scale4_1
I1210 18:03:20.011059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.011059 22120 net.cpp:137] Memory required for data: 431310000
I1210 18:03:20.011059 22120 layer_factory.cpp:58] Creating layer relu4_1
I1210 18:03:20.011059 22120 net.cpp:84] Creating Layer relu4_1
I1210 18:03:20.011059 22120 net.cpp:406] relu4_1 <- conv4_1
I1210 18:03:20.011059 22120 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1210 18:03:20.011059 22120 net.cpp:122] Setting up relu4_1
I1210 18:03:20.011059 22120 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 18:03:20.011059 22120 net.cpp:137] Memory required for data: 436430000
I1210 18:03:20.011059 22120 layer_factory.cpp:58] Creating layer conv4_2
I1210 18:03:20.011059 22120 net.cpp:84] Creating Layer conv4_2
I1210 18:03:20.011059 22120 net.cpp:406] conv4_2 <- conv4_1
I1210 18:03:20.011059 22120 net.cpp:380] conv4_2 -> conv4_2
I1210 18:03:20.013058 22120 net.cpp:122] Setting up conv4_2
I1210 18:03:20.013058 22120 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 18:03:20.013058 22120 net.cpp:137] Memory required for data: 442369200
I1210 18:03:20.013058 22120 layer_factory.cpp:58] Creating layer bn4_2
I1210 18:03:20.013058 22120 net.cpp:84] Creating Layer bn4_2
I1210 18:03:20.013058 22120 net.cpp:406] bn4_2 <- conv4_2
I1210 18:03:20.013058 22120 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1210 18:03:20.014058 22120 net.cpp:122] Setting up bn4_2
I1210 18:03:20.014058 22120 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 18:03:20.014058 22120 net.cpp:137] Memory required for data: 448308400
I1210 18:03:20.014058 22120 layer_factory.cpp:58] Creating layer scale4_2
I1210 18:03:20.014058 22120 net.cpp:84] Creating Layer scale4_2
I1210 18:03:20.014058 22120 net.cpp:406] scale4_2 <- conv4_2
I1210 18:03:20.014058 22120 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1210 18:03:20.014058 22120 layer_factory.cpp:58] Creating layer scale4_2
I1210 18:03:20.014058 22120 net.cpp:122] Setting up scale4_2
I1210 18:03:20.014058 22120 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 18:03:20.014058 22120 net.cpp:137] Memory required for data: 454247600
I1210 18:03:20.014058 22120 layer_factory.cpp:58] Creating layer relu4_2
I1210 18:03:20.014058 22120 net.cpp:84] Creating Layer relu4_2
I1210 18:03:20.014058 22120 net.cpp:406] relu4_2 <- conv4_2
I1210 18:03:20.014058 22120 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1210 18:03:20.014058 22120 net.cpp:122] Setting up relu4_2
I1210 18:03:20.014058 22120 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 18:03:20.014058 22120 net.cpp:137] Memory required for data: 460186800
I1210 18:03:20.014058 22120 layer_factory.cpp:58] Creating layer added_new_conv2
I1210 18:03:20.014058 22120 net.cpp:84] Creating Layer added_new_conv2
I1210 18:03:20.014058 22120 net.cpp:406] added_new_conv2 <- conv4_2
I1210 18:03:20.014058 22120 net.cpp:380] added_new_conv2 -> added_new_conv2
I1210 18:03:20.016067 22120 net.cpp:122] Setting up added_new_conv2
I1210 18:03:20.016067 22120 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 18:03:20.016067 22120 net.cpp:137] Memory required for data: 466126000
I1210 18:03:20.016067 22120 layer_factory.cpp:58] Creating layer pool4_2
I1210 18:03:20.016067 22120 net.cpp:84] Creating Layer pool4_2
I1210 18:03:20.016067 22120 net.cpp:406] pool4_2 <- added_new_conv2
I1210 18:03:20.016067 22120 net.cpp:380] pool4_2 -> pool4_2
I1210 18:03:20.016067 22120 net.cpp:122] Setting up pool4_2
I1210 18:03:20.016067 22120 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 18:03:20.016067 22120 net.cpp:137] Memory required for data: 467610800
I1210 18:03:20.016067 22120 layer_factory.cpp:58] Creating layer conv4_0
I1210 18:03:20.016067 22120 net.cpp:84] Creating Layer conv4_0
I1210 18:03:20.016067 22120 net.cpp:406] conv4_0 <- pool4_2
I1210 18:03:20.016067 22120 net.cpp:380] conv4_0 -> conv4_0
I1210 18:03:20.019058 22120 net.cpp:122] Setting up conv4_0
I1210 18:03:20.019058 22120 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 18:03:20.019058 22120 net.cpp:137] Memory required for data: 469095600
I1210 18:03:20.019058 22120 layer_factory.cpp:58] Creating layer bn4_0
I1210 18:03:20.019058 22120 net.cpp:84] Creating Layer bn4_0
I1210 18:03:20.019058 22120 net.cpp:406] bn4_0 <- conv4_0
I1210 18:03:20.019058 22120 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1210 18:03:20.020062 22120 net.cpp:122] Setting up bn4_0
I1210 18:03:20.020062 22120 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 18:03:20.020062 22120 net.cpp:137] Memory required for data: 470580400
I1210 18:03:20.020062 22120 layer_factory.cpp:58] Creating layer scale4_0
I1210 18:03:20.020062 22120 net.cpp:84] Creating Layer scale4_0
I1210 18:03:20.020062 22120 net.cpp:406] scale4_0 <- conv4_0
I1210 18:03:20.020062 22120 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1210 18:03:20.020062 22120 layer_factory.cpp:58] Creating layer scale4_0
I1210 18:03:20.020062 22120 net.cpp:122] Setting up scale4_0
I1210 18:03:20.020062 22120 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 18:03:20.020062 22120 net.cpp:137] Memory required for data: 472065200
I1210 18:03:20.020062 22120 layer_factory.cpp:58] Creating layer relu4_0
I1210 18:03:20.020062 22120 net.cpp:84] Creating Layer relu4_0
I1210 18:03:20.020062 22120 net.cpp:406] relu4_0 <- conv4_0
I1210 18:03:20.020062 22120 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1210 18:03:20.021065 22120 net.cpp:122] Setting up relu4_0
I1210 18:03:20.022063 22120 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 18:03:20.022063 22120 net.cpp:137] Memory required for data: 473550000
I1210 18:03:20.022063 22120 layer_factory.cpp:58] Creating layer conv11
I1210 18:03:20.022063 22120 net.cpp:84] Creating Layer conv11
I1210 18:03:20.022063 22120 net.cpp:406] conv11 <- conv4_0
I1210 18:03:20.022063 22120 net.cpp:380] conv11 -> conv11
I1210 18:03:20.024058 22120 net.cpp:122] Setting up conv11
I1210 18:03:20.024058 22120 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 18:03:20.024058 22120 net.cpp:137] Memory required for data: 475342000
I1210 18:03:20.024058 22120 layer_factory.cpp:58] Creating layer bn_conv11
I1210 18:03:20.024058 22120 net.cpp:84] Creating Layer bn_conv11
I1210 18:03:20.024058 22120 net.cpp:406] bn_conv11 <- conv11
I1210 18:03:20.024058 22120 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1210 18:03:20.024058 22120 net.cpp:122] Setting up bn_conv11
I1210 18:03:20.025061 22120 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 18:03:20.025061 22120 net.cpp:137] Memory required for data: 477134000
I1210 18:03:20.025061 22120 layer_factory.cpp:58] Creating layer scale_conv11
I1210 18:03:20.025061 22120 net.cpp:84] Creating Layer scale_conv11
I1210 18:03:20.025061 22120 net.cpp:406] scale_conv11 <- conv11
I1210 18:03:20.025061 22120 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1210 18:03:20.025061 22120 layer_factory.cpp:58] Creating layer scale_conv11
I1210 18:03:20.025061 22120 net.cpp:122] Setting up scale_conv11
I1210 18:03:20.025061 22120 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 18:03:20.025061 22120 net.cpp:137] Memory required for data: 478926000
I1210 18:03:20.025061 22120 layer_factory.cpp:58] Creating layer relu_conv11
I1210 18:03:20.025061 22120 net.cpp:84] Creating Layer relu_conv11
I1210 18:03:20.025061 22120 net.cpp:406] relu_conv11 <- conv11
I1210 18:03:20.025061 22120 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1210 18:03:20.025061 22120 net.cpp:122] Setting up relu_conv11
I1210 18:03:20.025061 22120 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 18:03:20.025061 22120 net.cpp:137] Memory required for data: 480718000
I1210 18:03:20.025061 22120 layer_factory.cpp:58] Creating layer conv12
I1210 18:03:20.025061 22120 net.cpp:84] Creating Layer conv12
I1210 18:03:20.025061 22120 net.cpp:406] conv12 <- conv11
I1210 18:03:20.025061 22120 net.cpp:380] conv12 -> conv12
I1210 18:03:20.028059 22120 net.cpp:122] Setting up conv12
I1210 18:03:20.028059 22120 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 18:03:20.028059 22120 net.cpp:137] Memory required for data: 483022000
I1210 18:03:20.028059 22120 layer_factory.cpp:58] Creating layer bn_conv12
I1210 18:03:20.028059 22120 net.cpp:84] Creating Layer bn_conv12
I1210 18:03:20.028059 22120 net.cpp:406] bn_conv12 <- conv12
I1210 18:03:20.028059 22120 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1210 18:03:20.028059 22120 net.cpp:122] Setting up bn_conv12
I1210 18:03:20.028059 22120 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 18:03:20.028059 22120 net.cpp:137] Memory required for data: 485326000
I1210 18:03:20.028059 22120 layer_factory.cpp:58] Creating layer scale_conv12
I1210 18:03:20.028059 22120 net.cpp:84] Creating Layer scale_conv12
I1210 18:03:20.028059 22120 net.cpp:406] scale_conv12 <- conv12
I1210 18:03:20.028059 22120 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1210 18:03:20.028059 22120 layer_factory.cpp:58] Creating layer scale_conv12
I1210 18:03:20.028059 22120 net.cpp:122] Setting up scale_conv12
I1210 18:03:20.028059 22120 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 18:03:20.028059 22120 net.cpp:137] Memory required for data: 487630000
I1210 18:03:20.029059 22120 layer_factory.cpp:58] Creating layer relu_conv12
I1210 18:03:20.029059 22120 net.cpp:84] Creating Layer relu_conv12
I1210 18:03:20.029059 22120 net.cpp:406] relu_conv12 <- conv12
I1210 18:03:20.029059 22120 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1210 18:03:20.029059 22120 net.cpp:122] Setting up relu_conv12
I1210 18:03:20.029059 22120 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 18:03:20.029059 22120 net.cpp:137] Memory required for data: 489934000
I1210 18:03:20.029059 22120 layer_factory.cpp:58] Creating layer poolcp6
I1210 18:03:20.029059 22120 net.cpp:84] Creating Layer poolcp6
I1210 18:03:20.029059 22120 net.cpp:406] poolcp6 <- conv12
I1210 18:03:20.029059 22120 net.cpp:380] poolcp6 -> poolcp6
I1210 18:03:20.029059 22120 net.cpp:122] Setting up poolcp6
I1210 18:03:20.029059 22120 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1210 18:03:20.029059 22120 net.cpp:137] Memory required for data: 489970000
I1210 18:03:20.029059 22120 layer_factory.cpp:58] Creating layer ip1
I1210 18:03:20.029059 22120 net.cpp:84] Creating Layer ip1
I1210 18:03:20.029059 22120 net.cpp:406] ip1 <- poolcp6
I1210 18:03:20.029059 22120 net.cpp:380] ip1 -> ip1
I1210 18:03:20.029059 22120 net.cpp:122] Setting up ip1
I1210 18:03:20.029059 22120 net.cpp:129] Top shape: 100 100 (10000)
I1210 18:03:20.029059 22120 net.cpp:137] Memory required for data: 490010000
I1210 18:03:20.029059 22120 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1210 18:03:20.029059 22120 net.cpp:84] Creating Layer ip1_ip1_0_split
I1210 18:03:20.029059 22120 net.cpp:406] ip1_ip1_0_split <- ip1
I1210 18:03:20.029059 22120 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1210 18:03:20.029059 22120 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1210 18:03:20.029059 22120 net.cpp:122] Setting up ip1_ip1_0_split
I1210 18:03:20.029059 22120 net.cpp:129] Top shape: 100 100 (10000)
I1210 18:03:20.029059 22120 net.cpp:129] Top shape: 100 100 (10000)
I1210 18:03:20.029059 22120 net.cpp:137] Memory required for data: 490090000
I1210 18:03:20.029059 22120 layer_factory.cpp:58] Creating layer accuracy
I1210 18:03:20.029059 22120 net.cpp:84] Creating Layer accuracy
I1210 18:03:20.029059 22120 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1210 18:03:20.030057 22120 net.cpp:406] accuracy <- label_cifar_1_split_0
I1210 18:03:20.030057 22120 net.cpp:380] accuracy -> accuracy
I1210 18:03:20.030057 22120 net.cpp:122] Setting up accuracy
I1210 18:03:20.030057 22120 net.cpp:129] Top shape: (1)
I1210 18:03:20.030057 22120 net.cpp:137] Memory required for data: 490090004
I1210 18:03:20.030057 22120 layer_factory.cpp:58] Creating layer loss
I1210 18:03:20.030057 22120 net.cpp:84] Creating Layer loss
I1210 18:03:20.030057 22120 net.cpp:406] loss <- ip1_ip1_0_split_1
I1210 18:03:20.030057 22120 net.cpp:406] loss <- label_cifar_1_split_1
I1210 18:03:20.030057 22120 net.cpp:380] loss -> loss
I1210 18:03:20.030057 22120 layer_factory.cpp:58] Creating layer loss
I1210 18:03:20.030057 22120 net.cpp:122] Setting up loss
I1210 18:03:20.030057 22120 net.cpp:129] Top shape: (1)
I1210 18:03:20.030057 22120 net.cpp:132]     with loss weight 1
I1210 18:03:20.030057 22120 net.cpp:137] Memory required for data: 490090008
I1210 18:03:20.030057 22120 net.cpp:198] loss needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:200] accuracy does not need backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] ip1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] poolcp6 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] relu_conv12 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] scale_conv12 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] bn_conv12 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] conv12 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] relu_conv11 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] scale_conv11 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] bn_conv11 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] conv11 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] relu4_0 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] scale4_0 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] bn4_0 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] conv4_0 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] pool4_2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] added_new_conv2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] relu4_2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] scale4_2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] bn4_2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] conv4_2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] relu4_1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] scale4_1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] bn4_1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] conv4_1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] relu4 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] scale4 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] bn4 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] conv4 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] relu3_1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] scale3_1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] bn3_1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] conv3_1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] relu3 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] scale3 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] bn3 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] conv3 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] pool2_1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] newconv_added1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] relu2_2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] scale2_2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] bn2_2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] conv2_2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] relu2_1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] scale2_1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] bn2_1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] conv2_1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] relu2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] scale2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] bn2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] conv2 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] relu1_0 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] scale1_0 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] bn1_0 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] conv1_0 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] relu1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] scale1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] bn1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:198] conv1 needs backward computation.
I1210 18:03:20.030057 22120 net.cpp:200] label_cifar_1_split does not need backward computation.
I1210 18:03:20.030057 22120 net.cpp:200] cifar does not need backward computation.
I1210 18:03:20.030057 22120 net.cpp:242] This network produces output accuracy
I1210 18:03:20.030057 22120 net.cpp:242] This network produces output loss
I1210 18:03:20.030057 22120 net.cpp:255] Network initialization done.
I1210 18:03:20.031059 22120 solver.cpp:56] Solver scaffolding done.
I1210 18:03:20.035055 22120 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90000.solverstate
I1210 18:03:20.043064 22120 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90000.caffemodel
I1210 18:03:20.043064 22120 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 18:03:20.043565 22120 sgd_solver.cpp:318] SGDSolver: restoring history
I1210 18:03:20.047564 22120 caffe.cpp:249] Starting Optimization
I1210 18:03:20.047564 22120 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_v2_360k
I1210 18:03:20.047564 22120 solver.cpp:273] Learning Rate Policy: multistep
I1210 18:03:20.050565 22120 solver.cpp:330] Iteration 90000, Testing net (#0)
I1210 18:03:20.052065 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:03:21.508136 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:03:21.563151 22120 solver.cpp:397]     Test net output #0: accuracy = 0.5732
I1210 18:03:21.563151 22120 solver.cpp:397]     Test net output #1: loss = 1.65759 (* 1 = 1.65759 loss)
I1210 18:03:21.679162 22120 solver.cpp:218] Iteration 90000 (55211.9 iter/s, 1.63008s/100 iters), loss = 0.612613
I1210 18:03:21.679162 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 18:03:21.679162 22120 solver.cpp:237]     Train net output #1: loss = 0.612613 (* 1 = 0.612613 loss)
I1210 18:03:21.679162 22120 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1210 18:03:27.493584 22120 solver.cpp:218] Iteration 90100 (17.2001 iter/s, 5.81393s/100 iters), loss = 0.790728
I1210 18:03:27.493584 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1210 18:03:27.493584 22120 solver.cpp:237]     Train net output #1: loss = 0.790728 (* 1 = 0.790728 loss)
I1210 18:03:27.493584 22120 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1210 18:03:33.156726 22120 solver.cpp:218] Iteration 90200 (17.6585 iter/s, 5.66298s/100 iters), loss = 0.620409
I1210 18:03:33.156726 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 18:03:33.156726 22120 solver.cpp:237]     Train net output #1: loss = 0.620409 (* 1 = 0.620409 loss)
I1210 18:03:33.156726 22120 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1210 18:03:38.780333 22120 solver.cpp:218] Iteration 90300 (17.7821 iter/s, 5.62362s/100 iters), loss = 0.772133
I1210 18:03:38.780333 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 18:03:38.780333 22120 solver.cpp:237]     Train net output #1: loss = 0.772133 (* 1 = 0.772133 loss)
I1210 18:03:38.780333 22120 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1210 18:03:44.459049 22120 solver.cpp:218] Iteration 90400 (17.6124 iter/s, 5.67782s/100 iters), loss = 0.824872
I1210 18:03:44.459550 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1210 18:03:44.459550 22120 solver.cpp:237]     Train net output #1: loss = 0.824872 (* 1 = 0.824872 loss)
I1210 18:03:44.459550 22120 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1210 18:03:49.816959 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:03:50.039968 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90500.caffemodel
I1210 18:03:50.063477 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90500.solverstate
I1210 18:03:50.068982 22120 solver.cpp:330] Iteration 90500, Testing net (#0)
I1210 18:03:50.068982 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:03:51.435137 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:03:51.489140 22120 solver.cpp:397]     Test net output #0: accuracy = 0.5742
I1210 18:03:51.489140 22120 solver.cpp:397]     Test net output #1: loss = 1.68956 (* 1 = 1.68956 loss)
I1210 18:03:51.543138 22120 solver.cpp:218] Iteration 90500 (14.1168 iter/s, 7.08376s/100 iters), loss = 0.68955
I1210 18:03:51.543138 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:03:51.543138 22120 solver.cpp:237]     Train net output #1: loss = 0.68955 (* 1 = 0.68955 loss)
I1210 18:03:51.543138 22120 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1210 18:03:57.188638 22120 solver.cpp:218] Iteration 90600 (17.7159 iter/s, 5.64464s/100 iters), loss = 0.762944
I1210 18:03:57.188638 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 18:03:57.188638 22120 solver.cpp:237]     Train net output #1: loss = 0.762944 (* 1 = 0.762944 loss)
I1210 18:03:57.188638 22120 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1210 18:04:02.820333 22120 solver.cpp:218] Iteration 90700 (17.7583 iter/s, 5.63119s/100 iters), loss = 0.639665
I1210 18:04:02.820333 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 18:04:02.820333 22120 solver.cpp:237]     Train net output #1: loss = 0.639665 (* 1 = 0.639665 loss)
I1210 18:04:02.820333 22120 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1210 18:04:08.454733 22120 solver.cpp:218] Iteration 90800 (17.749 iter/s, 5.63411s/100 iters), loss = 0.817114
I1210 18:04:08.454733 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 18:04:08.454733 22120 solver.cpp:237]     Train net output #1: loss = 0.817114 (* 1 = 0.817114 loss)
I1210 18:04:08.454733 22120 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1210 18:04:14.094113 22120 solver.cpp:218] Iteration 90900 (17.7345 iter/s, 5.63873s/100 iters), loss = 0.931312
I1210 18:04:14.094113 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 18:04:14.094113 22120 solver.cpp:237]     Train net output #1: loss = 0.931312 (* 1 = 0.931312 loss)
I1210 18:04:14.094113 22120 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1210 18:04:19.460507 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:04:19.679529 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_91000.caffemodel
I1210 18:04:19.694530 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_91000.solverstate
I1210 18:04:19.700531 22120 solver.cpp:330] Iteration 91000, Testing net (#0)
I1210 18:04:19.700531 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:04:21.069648 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:04:21.122648 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6047
I1210 18:04:21.122648 22120 solver.cpp:397]     Test net output #1: loss = 1.50575 (* 1 = 1.50575 loss)
I1210 18:04:21.175659 22120 solver.cpp:218] Iteration 91000 (14.1211 iter/s, 7.08161s/100 iters), loss = 0.576915
I1210 18:04:21.175659 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:04:21.175659 22120 solver.cpp:237]     Train net output #1: loss = 0.576915 (* 1 = 0.576915 loss)
I1210 18:04:21.175659 22120 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1210 18:04:26.817095 22120 solver.cpp:218] Iteration 91100 (17.7277 iter/s, 5.6409s/100 iters), loss = 0.662712
I1210 18:04:26.817095 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 18:04:26.817095 22120 solver.cpp:237]     Train net output #1: loss = 0.662712 (* 1 = 0.662712 loss)
I1210 18:04:26.817095 22120 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1210 18:04:32.460496 22120 solver.cpp:218] Iteration 91200 (17.7227 iter/s, 5.64248s/100 iters), loss = 0.566759
I1210 18:04:32.460496 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 18:04:32.460496 22120 solver.cpp:237]     Train net output #1: loss = 0.566759 (* 1 = 0.566759 loss)
I1210 18:04:32.460496 22120 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1210 18:04:38.097961 22120 solver.cpp:218] Iteration 91300 (17.7384 iter/s, 5.63748s/100 iters), loss = 0.721494
I1210 18:04:38.097961 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 18:04:38.097961 22120 solver.cpp:237]     Train net output #1: loss = 0.721494 (* 1 = 0.721494 loss)
I1210 18:04:38.097961 22120 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1210 18:04:43.737393 22120 solver.cpp:218] Iteration 91400 (17.7339 iter/s, 5.63893s/100 iters), loss = 0.840363
I1210 18:04:43.737393 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 18:04:43.737393 22120 solver.cpp:237]     Train net output #1: loss = 0.840363 (* 1 = 0.840363 loss)
I1210 18:04:43.737393 22120 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1210 18:04:49.111269 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:04:49.331971 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_91500.caffemodel
I1210 18:04:49.346971 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_91500.solverstate
I1210 18:04:49.352970 22120 solver.cpp:330] Iteration 91500, Testing net (#0)
I1210 18:04:49.352970 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:04:50.718933 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:04:50.772929 22120 solver.cpp:397]     Test net output #0: accuracy = 0.568
I1210 18:04:50.772929 22120 solver.cpp:397]     Test net output #1: loss = 1.73764 (* 1 = 1.73764 loss)
I1210 18:04:50.826925 22120 solver.cpp:218] Iteration 91500 (14.1065 iter/s, 7.08891s/100 iters), loss = 0.604392
I1210 18:04:50.826925 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:04:50.826925 22120 solver.cpp:237]     Train net output #1: loss = 0.604392 (* 1 = 0.604392 loss)
I1210 18:04:50.826925 22120 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1210 18:04:56.456765 22120 solver.cpp:218] Iteration 91600 (17.7643 iter/s, 5.62927s/100 iters), loss = 0.752695
I1210 18:04:56.456765 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 18:04:56.456765 22120 solver.cpp:237]     Train net output #1: loss = 0.752695 (* 1 = 0.752695 loss)
I1210 18:04:56.456765 22120 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1210 18:05:02.080677 22120 solver.cpp:218] Iteration 91700 (17.7819 iter/s, 5.6237s/100 iters), loss = 0.455493
I1210 18:05:02.080677 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:05:02.080677 22120 solver.cpp:237]     Train net output #1: loss = 0.455493 (* 1 = 0.455493 loss)
I1210 18:05:02.080677 22120 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1210 18:05:07.726409 22120 solver.cpp:218] Iteration 91800 (17.7136 iter/s, 5.64537s/100 iters), loss = 0.69273
I1210 18:05:07.726409 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 18:05:07.726409 22120 solver.cpp:237]     Train net output #1: loss = 0.69273 (* 1 = 0.69273 loss)
I1210 18:05:07.726409 22120 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1210 18:05:13.373862 22120 solver.cpp:218] Iteration 91900 (17.7077 iter/s, 5.64727s/100 iters), loss = 0.934724
I1210 18:05:13.373862 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 18:05:13.374862 22120 solver.cpp:237]     Train net output #1: loss = 0.934724 (* 1 = 0.934724 loss)
I1210 18:05:13.374862 22120 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1210 18:05:18.732296 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:05:18.954308 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_92000.caffemodel
I1210 18:05:18.969307 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_92000.solverstate
I1210 18:05:18.974308 22120 solver.cpp:330] Iteration 92000, Testing net (#0)
I1210 18:05:18.974308 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:05:20.343439 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:05:20.397440 22120 solver.cpp:397]     Test net output #0: accuracy = 0.5502
I1210 18:05:20.397440 22120 solver.cpp:397]     Test net output #1: loss = 1.80544 (* 1 = 1.80544 loss)
I1210 18:05:20.451444 22120 solver.cpp:218] Iteration 92000 (14.1318 iter/s, 7.07623s/100 iters), loss = 0.591469
I1210 18:05:20.451444 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 18:05:20.451444 22120 solver.cpp:237]     Train net output #1: loss = 0.591469 (* 1 = 0.591469 loss)
I1210 18:05:20.451444 22120 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1210 18:05:26.120937 22120 solver.cpp:218] Iteration 92100 (17.6389 iter/s, 5.66928s/100 iters), loss = 0.778749
I1210 18:05:26.120937 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 18:05:26.120937 22120 solver.cpp:237]     Train net output #1: loss = 0.778749 (* 1 = 0.778749 loss)
I1210 18:05:26.120937 22120 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1210 18:05:31.889910 22120 solver.cpp:218] Iteration 92200 (17.3352 iter/s, 5.7686s/100 iters), loss = 0.637435
I1210 18:05:31.889910 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 18:05:31.889910 22120 solver.cpp:237]     Train net output #1: loss = 0.637435 (* 1 = 0.637435 loss)
I1210 18:05:31.889910 22120 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1210 18:05:37.570435 22120 solver.cpp:218] Iteration 92300 (17.6041 iter/s, 5.68048s/100 iters), loss = 0.860813
I1210 18:05:37.570435 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 18:05:37.570435 22120 solver.cpp:237]     Train net output #1: loss = 0.860813 (* 1 = 0.860813 loss)
I1210 18:05:37.570435 22120 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1210 18:05:43.215528 22120 solver.cpp:218] Iteration 92400 (17.7174 iter/s, 5.64418s/100 iters), loss = 0.747092
I1210 18:05:43.215528 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 18:05:43.215528 22120 solver.cpp:237]     Train net output #1: loss = 0.747092 (* 1 = 0.747092 loss)
I1210 18:05:43.215528 22120 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1210 18:05:48.582690 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:05:48.804700 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_92500.caffemodel
I1210 18:05:48.821210 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_92500.solverstate
I1210 18:05:48.826207 22120 solver.cpp:330] Iteration 92500, Testing net (#0)
I1210 18:05:48.826207 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:05:50.210186 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:05:50.265194 22120 solver.cpp:397]     Test net output #0: accuracy = 0.5686
I1210 18:05:50.265194 22120 solver.cpp:397]     Test net output #1: loss = 1.7446 (* 1 = 1.7446 loss)
I1210 18:05:50.319720 22120 solver.cpp:218] Iteration 92500 (14.0767 iter/s, 7.10396s/100 iters), loss = 0.58304
I1210 18:05:50.320204 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 18:05:50.320204 22120 solver.cpp:237]     Train net output #1: loss = 0.58304 (* 1 = 0.58304 loss)
I1210 18:05:50.320204 22120 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1210 18:05:56.075636 22120 solver.cpp:218] Iteration 92600 (17.3759 iter/s, 5.7551s/100 iters), loss = 0.687495
I1210 18:05:56.075636 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 18:05:56.075636 22120 solver.cpp:237]     Train net output #1: loss = 0.687495 (* 1 = 0.687495 loss)
I1210 18:05:56.075636 22120 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1210 18:06:01.720832 22120 solver.cpp:218] Iteration 92700 (17.7156 iter/s, 5.64473s/100 iters), loss = 0.607742
I1210 18:06:01.720832 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 18:06:01.720832 22120 solver.cpp:237]     Train net output #1: loss = 0.607742 (* 1 = 0.607742 loss)
I1210 18:06:01.720832 22120 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1210 18:06:07.374392 22120 solver.cpp:218] Iteration 92800 (17.6866 iter/s, 5.65399s/100 iters), loss = 0.756442
I1210 18:06:07.375391 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 18:06:07.375391 22120 solver.cpp:237]     Train net output #1: loss = 0.756442 (* 1 = 0.756442 loss)
I1210 18:06:07.375391 22120 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1210 18:06:13.049825 22120 solver.cpp:218] Iteration 92900 (17.6223 iter/s, 5.67464s/100 iters), loss = 0.883173
I1210 18:06:13.049825 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 18:06:13.049825 22120 solver.cpp:237]     Train net output #1: loss = 0.883173 (* 1 = 0.883173 loss)
I1210 18:06:13.049825 22120 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1210 18:06:18.449187 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:06:18.672207 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_93000.caffemodel
I1210 18:06:18.686209 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_93000.solverstate
I1210 18:06:18.691208 22120 solver.cpp:330] Iteration 93000, Testing net (#0)
I1210 18:06:18.691208 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:06:20.066449 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:06:20.121450 22120 solver.cpp:397]     Test net output #0: accuracy = 0.5811
I1210 18:06:20.121450 22120 solver.cpp:397]     Test net output #1: loss = 1.65851 (* 1 = 1.65851 loss)
I1210 18:06:20.175459 22120 solver.cpp:218] Iteration 93000 (14.0357 iter/s, 7.12471s/100 iters), loss = 0.599842
I1210 18:06:20.175459 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:06:20.175459 22120 solver.cpp:237]     Train net output #1: loss = 0.599842 (* 1 = 0.599842 loss)
I1210 18:06:20.175459 22120 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1210 18:06:25.830921 22120 solver.cpp:218] Iteration 93100 (17.683 iter/s, 5.65515s/100 iters), loss = 0.843182
I1210 18:06:25.830921 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 18:06:25.830921 22120 solver.cpp:237]     Train net output #1: loss = 0.843182 (* 1 = 0.843182 loss)
I1210 18:06:25.830921 22120 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1210 18:06:31.488379 22120 solver.cpp:218] Iteration 93200 (17.676 iter/s, 5.6574s/100 iters), loss = 0.594721
I1210 18:06:31.488379 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 18:06:31.488379 22120 solver.cpp:237]     Train net output #1: loss = 0.594721 (* 1 = 0.594721 loss)
I1210 18:06:31.488379 22120 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1210 18:06:37.139333 22120 solver.cpp:218] Iteration 93300 (17.6982 iter/s, 5.6503s/100 iters), loss = 0.795355
I1210 18:06:37.139833 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1210 18:06:37.139833 22120 solver.cpp:237]     Train net output #1: loss = 0.795355 (* 1 = 0.795355 loss)
I1210 18:06:37.139833 22120 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1210 18:06:42.798297 22120 solver.cpp:218] Iteration 93400 (17.6728 iter/s, 5.65842s/100 iters), loss = 0.706776
I1210 18:06:42.798297 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 18:06:42.798297 22120 solver.cpp:237]     Train net output #1: loss = 0.706776 (* 1 = 0.706776 loss)
I1210 18:06:42.798297 22120 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1210 18:06:48.171771 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:06:48.393784 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_93500.caffemodel
I1210 18:06:48.407783 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_93500.solverstate
I1210 18:06:48.412783 22120 solver.cpp:330] Iteration 93500, Testing net (#0)
I1210 18:06:48.412783 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:06:49.781936 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:06:49.835939 22120 solver.cpp:397]     Test net output #0: accuracy = 0.5811
I1210 18:06:49.835939 22120 solver.cpp:397]     Test net output #1: loss = 1.62967 (* 1 = 1.62967 loss)
I1210 18:06:49.889941 22120 solver.cpp:218] Iteration 93500 (14.1012 iter/s, 7.09159s/100 iters), loss = 0.618592
I1210 18:06:49.889941 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 18:06:49.889941 22120 solver.cpp:237]     Train net output #1: loss = 0.618592 (* 1 = 0.618592 loss)
I1210 18:06:49.889941 22120 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1210 18:06:55.542412 22120 solver.cpp:218] Iteration 93600 (17.6936 iter/s, 5.65175s/100 iters), loss = 0.639865
I1210 18:06:55.542412 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 18:06:55.542412 22120 solver.cpp:237]     Train net output #1: loss = 0.639865 (* 1 = 0.639865 loss)
I1210 18:06:55.542412 22120 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1210 18:07:01.192119 22120 solver.cpp:218] Iteration 93700 (17.7013 iter/s, 5.64931s/100 iters), loss = 0.494222
I1210 18:07:01.192119 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:07:01.192119 22120 solver.cpp:237]     Train net output #1: loss = 0.494222 (* 1 = 0.494222 loss)
I1210 18:07:01.192119 22120 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1210 18:07:06.851579 22120 solver.cpp:218] Iteration 93800 (17.6709 iter/s, 5.65904s/100 iters), loss = 0.795842
I1210 18:07:06.851579 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 18:07:06.851579 22120 solver.cpp:237]     Train net output #1: loss = 0.795842 (* 1 = 0.795842 loss)
I1210 18:07:06.851579 22120 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1210 18:07:12.499094 22120 solver.cpp:218] Iteration 93900 (17.7074 iter/s, 5.64735s/100 iters), loss = 0.818246
I1210 18:07:12.499094 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 18:07:12.499094 22120 solver.cpp:237]     Train net output #1: loss = 0.818246 (* 1 = 0.818246 loss)
I1210 18:07:12.499094 22120 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1210 18:07:17.868757 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:07:18.090772 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_94000.caffemodel
I1210 18:07:18.106771 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_94000.solverstate
I1210 18:07:18.110771 22120 solver.cpp:330] Iteration 94000, Testing net (#0)
I1210 18:07:18.110771 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:07:19.486927 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:07:19.540931 22120 solver.cpp:397]     Test net output #0: accuracy = 0.5832
I1210 18:07:19.540931 22120 solver.cpp:397]     Test net output #1: loss = 1.59532 (* 1 = 1.59532 loss)
I1210 18:07:19.593935 22120 solver.cpp:218] Iteration 94000 (14.0958 iter/s, 7.09432s/100 iters), loss = 0.713075
I1210 18:07:19.593935 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 18:07:19.593935 22120 solver.cpp:237]     Train net output #1: loss = 0.713075 (* 1 = 0.713075 loss)
I1210 18:07:19.593935 22120 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1210 18:07:25.244390 22120 solver.cpp:218] Iteration 94100 (17.6984 iter/s, 5.65024s/100 iters), loss = 0.650394
I1210 18:07:25.244390 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 18:07:25.244390 22120 solver.cpp:237]     Train net output #1: loss = 0.650394 (* 1 = 0.650394 loss)
I1210 18:07:25.244390 22120 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1210 18:07:30.918931 22120 solver.cpp:218] Iteration 94200 (17.6258 iter/s, 5.67349s/100 iters), loss = 0.554566
I1210 18:07:30.918931 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 18:07:30.918931 22120 solver.cpp:237]     Train net output #1: loss = 0.554566 (* 1 = 0.554566 loss)
I1210 18:07:30.918931 22120 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1210 18:07:36.563380 22120 solver.cpp:218] Iteration 94300 (17.7186 iter/s, 5.6438s/100 iters), loss = 0.751016
I1210 18:07:36.563380 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 18:07:36.563380 22120 solver.cpp:237]     Train net output #1: loss = 0.751016 (* 1 = 0.751016 loss)
I1210 18:07:36.563380 22120 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1210 18:07:42.204818 22120 solver.cpp:218] Iteration 94400 (17.7253 iter/s, 5.64166s/100 iters), loss = 0.718572
I1210 18:07:42.204818 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 18:07:42.204818 22120 solver.cpp:237]     Train net output #1: loss = 0.718572 (* 1 = 0.718572 loss)
I1210 18:07:42.204818 22120 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1210 18:07:47.559188 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:07:47.783201 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_94500.caffemodel
I1210 18:07:47.797200 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_94500.solverstate
I1210 18:07:47.802202 22120 solver.cpp:330] Iteration 94500, Testing net (#0)
I1210 18:07:47.802202 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:07:49.175506 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:07:49.229504 22120 solver.cpp:397]     Test net output #0: accuracy = 0.5909
I1210 18:07:49.229504 22120 solver.cpp:397]     Test net output #1: loss = 1.58005 (* 1 = 1.58005 loss)
I1210 18:07:49.283514 22120 solver.cpp:218] Iteration 94500 (14.1285 iter/s, 7.0779s/100 iters), loss = 0.653206
I1210 18:07:49.283514 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 18:07:49.283514 22120 solver.cpp:237]     Train net output #1: loss = 0.653206 (* 1 = 0.653206 loss)
I1210 18:07:49.283514 22120 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1210 18:07:54.926975 22120 solver.cpp:218] Iteration 94600 (17.7202 iter/s, 5.64328s/100 iters), loss = 0.788897
I1210 18:07:54.926975 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 18:07:54.926975 22120 solver.cpp:237]     Train net output #1: loss = 0.788897 (* 1 = 0.788897 loss)
I1210 18:07:54.926975 22120 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1210 18:08:00.561419 22120 solver.cpp:218] Iteration 94700 (17.7496 iter/s, 5.63392s/100 iters), loss = 0.517
I1210 18:08:00.561419 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 18:08:00.561419 22120 solver.cpp:237]     Train net output #1: loss = 0.517 (* 1 = 0.517 loss)
I1210 18:08:00.561419 22120 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1210 18:08:06.192874 22120 solver.cpp:218] Iteration 94800 (17.7597 iter/s, 5.63073s/100 iters), loss = 0.725312
I1210 18:08:06.192874 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 18:08:06.192874 22120 solver.cpp:237]     Train net output #1: loss = 0.725312 (* 1 = 0.725312 loss)
I1210 18:08:06.192874 22120 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1210 18:08:11.875396 22120 solver.cpp:218] Iteration 94900 (17.5989 iter/s, 5.68217s/100 iters), loss = 0.903003
I1210 18:08:11.875396 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 18:08:11.875396 22120 solver.cpp:237]     Train net output #1: loss = 0.903003 (* 1 = 0.903003 loss)
I1210 18:08:11.875396 22120 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1210 18:08:17.220263 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:08:17.441285 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_95000.caffemodel
I1210 18:08:17.457289 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_95000.solverstate
I1210 18:08:17.464292 22120 solver.cpp:330] Iteration 95000, Testing net (#0)
I1210 18:08:17.464292 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:08:18.836406 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:08:18.890411 22120 solver.cpp:397]     Test net output #0: accuracy = 0.5866
I1210 18:08:18.890411 22120 solver.cpp:397]     Test net output #1: loss = 1.61424 (* 1 = 1.61424 loss)
I1210 18:08:18.943913 22120 solver.cpp:218] Iteration 95000 (14.148 iter/s, 7.06816s/100 iters), loss = 0.584119
I1210 18:08:18.944414 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 18:08:18.944414 22120 solver.cpp:237]     Train net output #1: loss = 0.584119 (* 1 = 0.584119 loss)
I1210 18:08:18.944414 22120 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1210 18:08:18.944414 22120 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1210 18:08:24.565979 22120 solver.cpp:218] Iteration 95100 (17.7887 iter/s, 5.62154s/100 iters), loss = 0.595726
I1210 18:08:24.565979 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 18:08:24.565979 22120 solver.cpp:237]     Train net output #1: loss = 0.595726 (* 1 = 0.595726 loss)
I1210 18:08:24.566480 22120 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1210 18:08:30.267241 22120 solver.cpp:218] Iteration 95200 (17.5421 iter/s, 5.70057s/100 iters), loss = 0.46081
I1210 18:08:30.267241 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:08:30.267241 22120 solver.cpp:237]     Train net output #1: loss = 0.46081 (* 1 = 0.46081 loss)
I1210 18:08:30.267241 22120 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1210 18:08:35.923712 22120 solver.cpp:218] Iteration 95300 (17.6802 iter/s, 5.65604s/100 iters), loss = 0.62257
I1210 18:08:35.923712 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 18:08:35.923712 22120 solver.cpp:237]     Train net output #1: loss = 0.62257 (* 1 = 0.62257 loss)
I1210 18:08:35.923712 22120 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1210 18:08:41.600124 22120 solver.cpp:218] Iteration 95400 (17.6182 iter/s, 5.67595s/100 iters), loss = 0.555138
I1210 18:08:41.600124 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 18:08:41.600124 22120 solver.cpp:237]     Train net output #1: loss = 0.555138 (* 1 = 0.555138 loss)
I1210 18:08:41.600124 22120 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1210 18:08:47.014225 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:08:47.237426 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_95500.caffemodel
I1210 18:08:47.252441 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_95500.solverstate
I1210 18:08:47.256440 22120 solver.cpp:330] Iteration 95500, Testing net (#0)
I1210 18:08:47.256440 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:08:48.650496 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:08:48.704002 22120 solver.cpp:397]     Test net output #0: accuracy = 0.676
I1210 18:08:48.704002 22120 solver.cpp:397]     Test net output #1: loss = 1.17139 (* 1 = 1.17139 loss)
I1210 18:08:48.759001 22120 solver.cpp:218] Iteration 95500 (13.9686 iter/s, 7.15891s/100 iters), loss = 0.489068
I1210 18:08:48.759001 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:08:48.759001 22120 solver.cpp:237]     Train net output #1: loss = 0.489068 (* 1 = 0.489068 loss)
I1210 18:08:48.759001 22120 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1210 18:08:54.400528 22120 solver.cpp:218] Iteration 95600 (17.7302 iter/s, 5.6401s/100 iters), loss = 0.526393
I1210 18:08:54.400528 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 18:08:54.400528 22120 solver.cpp:237]     Train net output #1: loss = 0.526393 (* 1 = 0.526393 loss)
I1210 18:08:54.400528 22120 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1210 18:09:00.037647 22120 solver.cpp:218] Iteration 95700 (17.7404 iter/s, 5.63684s/100 iters), loss = 0.37626
I1210 18:09:00.037647 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:09:00.037647 22120 solver.cpp:237]     Train net output #1: loss = 0.37626 (* 1 = 0.37626 loss)
I1210 18:09:00.037647 22120 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1210 18:09:05.697492 22120 solver.cpp:218] Iteration 95800 (17.6702 iter/s, 5.65926s/100 iters), loss = 0.503111
I1210 18:09:05.697492 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 18:09:05.697492 22120 solver.cpp:237]     Train net output #1: loss = 0.503111 (* 1 = 0.503111 loss)
I1210 18:09:05.697492 22120 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1210 18:09:11.329020 22120 solver.cpp:218] Iteration 95900 (17.7591 iter/s, 5.63093s/100 iters), loss = 0.567569
I1210 18:09:11.329020 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 18:09:11.329020 22120 solver.cpp:237]     Train net output #1: loss = 0.567569 (* 1 = 0.567569 loss)
I1210 18:09:11.329020 22120 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1210 18:09:16.701431 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:09:16.922941 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_96000.caffemodel
I1210 18:09:16.937444 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_96000.solverstate
I1210 18:09:16.942445 22120 solver.cpp:330] Iteration 96000, Testing net (#0)
I1210 18:09:16.942445 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:09:18.310528 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:09:18.364532 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6789
I1210 18:09:18.364532 22120 solver.cpp:397]     Test net output #1: loss = 1.16802 (* 1 = 1.16802 loss)
I1210 18:09:18.419034 22120 solver.cpp:218] Iteration 96000 (14.1044 iter/s, 7.08997s/100 iters), loss = 0.41898
I1210 18:09:18.419534 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:09:18.419534 22120 solver.cpp:237]     Train net output #1: loss = 0.41898 (* 1 = 0.41898 loss)
I1210 18:09:18.419534 22120 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1210 18:09:24.062049 22120 solver.cpp:218] Iteration 96100 (17.7216 iter/s, 5.64283s/100 iters), loss = 0.541892
I1210 18:09:24.062049 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 18:09:24.062049 22120 solver.cpp:237]     Train net output #1: loss = 0.541892 (* 1 = 0.541892 loss)
I1210 18:09:24.062049 22120 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1210 18:09:29.708570 22120 solver.cpp:218] Iteration 96200 (17.7129 iter/s, 5.64561s/100 iters), loss = 0.424411
I1210 18:09:29.708570 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:09:29.708570 22120 solver.cpp:237]     Train net output #1: loss = 0.424411 (* 1 = 0.424411 loss)
I1210 18:09:29.708570 22120 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1210 18:09:35.360916 22120 solver.cpp:218] Iteration 96300 (17.6939 iter/s, 5.65167s/100 iters), loss = 0.437995
I1210 18:09:35.360916 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:09:35.360916 22120 solver.cpp:237]     Train net output #1: loss = 0.437995 (* 1 = 0.437995 loss)
I1210 18:09:35.360916 22120 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1210 18:09:41.002394 22120 solver.cpp:218] Iteration 96400 (17.7279 iter/s, 5.64081s/100 iters), loss = 0.480603
I1210 18:09:41.002394 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 18:09:41.002394 22120 solver.cpp:237]     Train net output #1: loss = 0.480603 (* 1 = 0.480603 loss)
I1210 18:09:41.002394 22120 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1210 18:09:46.366966 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:09:46.588984 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_96500.caffemodel
I1210 18:09:46.602984 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_96500.solverstate
I1210 18:09:46.607983 22120 solver.cpp:330] Iteration 96500, Testing net (#0)
I1210 18:09:46.607983 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:09:47.975102 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:09:48.029615 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6804
I1210 18:09:48.029615 22120 solver.cpp:397]     Test net output #1: loss = 1.16343 (* 1 = 1.16343 loss)
I1210 18:09:48.085119 22120 solver.cpp:218] Iteration 96500 (14.1188 iter/s, 7.08275s/100 iters), loss = 0.420656
I1210 18:09:48.085119 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:09:48.085119 22120 solver.cpp:237]     Train net output #1: loss = 0.420656 (* 1 = 0.420656 loss)
I1210 18:09:48.085119 22120 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1210 18:09:53.716632 22120 solver.cpp:218] Iteration 96600 (17.76 iter/s, 5.63062s/100 iters), loss = 0.473024
I1210 18:09:53.716632 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:09:53.716632 22120 solver.cpp:237]     Train net output #1: loss = 0.473024 (* 1 = 0.473024 loss)
I1210 18:09:53.716632 22120 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1210 18:09:59.361105 22120 solver.cpp:218] Iteration 96700 (17.7179 iter/s, 5.64401s/100 iters), loss = 0.367314
I1210 18:09:59.361105 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:09:59.361105 22120 solver.cpp:237]     Train net output #1: loss = 0.367314 (* 1 = 0.367314 loss)
I1210 18:09:59.361105 22120 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1210 18:10:05.020107 22120 solver.cpp:218] Iteration 96800 (17.6724 iter/s, 5.65852s/100 iters), loss = 0.498384
I1210 18:10:05.020107 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 18:10:05.020107 22120 solver.cpp:237]     Train net output #1: loss = 0.498384 (* 1 = 0.498384 loss)
I1210 18:10:05.020107 22120 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1210 18:10:10.666553 22120 solver.cpp:218] Iteration 96900 (17.7124 iter/s, 5.64576s/100 iters), loss = 0.520105
I1210 18:10:10.666553 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:10:10.666553 22120 solver.cpp:237]     Train net output #1: loss = 0.520105 (* 1 = 0.520105 loss)
I1210 18:10:10.666553 22120 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1210 18:10:16.032971 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:10:16.253995 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_97000.caffemodel
I1210 18:10:16.270500 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_97000.solverstate
I1210 18:10:16.275501 22120 solver.cpp:330] Iteration 97000, Testing net (#0)
I1210 18:10:16.276001 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:10:17.647132 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:10:17.700139 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6821
I1210 18:10:17.700139 22120 solver.cpp:397]     Test net output #1: loss = 1.16528 (* 1 = 1.16528 loss)
I1210 18:10:17.755138 22120 solver.cpp:218] Iteration 97000 (14.1078 iter/s, 7.08828s/100 iters), loss = 0.39429
I1210 18:10:17.755138 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:10:17.755138 22120 solver.cpp:237]     Train net output #1: loss = 0.39429 (* 1 = 0.39429 loss)
I1210 18:10:17.755138 22120 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1210 18:10:23.414852 22120 solver.cpp:218] Iteration 97100 (17.6702 iter/s, 5.65925s/100 iters), loss = 0.546662
I1210 18:10:23.414852 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:10:23.414852 22120 solver.cpp:237]     Train net output #1: loss = 0.546662 (* 1 = 0.546662 loss)
I1210 18:10:23.414852 22120 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1210 18:10:29.107550 22120 solver.cpp:218] Iteration 97200 (17.5657 iter/s, 5.69291s/100 iters), loss = 0.334737
I1210 18:10:29.108542 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:10:29.108542 22120 solver.cpp:237]     Train net output #1: loss = 0.334737 (* 1 = 0.334737 loss)
I1210 18:10:29.108542 22120 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1210 18:10:34.769011 22120 solver.cpp:218] Iteration 97300 (17.6667 iter/s, 5.66035s/100 iters), loss = 0.457341
I1210 18:10:34.769011 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:10:34.769011 22120 solver.cpp:237]     Train net output #1: loss = 0.457341 (* 1 = 0.457341 loss)
I1210 18:10:34.769011 22120 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1210 18:10:40.444413 22120 solver.cpp:218] Iteration 97400 (17.6195 iter/s, 5.67554s/100 iters), loss = 0.393062
I1210 18:10:40.444413 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:10:40.444413 22120 solver.cpp:237]     Train net output #1: loss = 0.393062 (* 1 = 0.393062 loss)
I1210 18:10:40.444413 22120 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1210 18:10:45.817600 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:10:46.040958 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_97500.caffemodel
I1210 18:10:46.058470 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_97500.solverstate
I1210 18:10:46.063987 22120 solver.cpp:330] Iteration 97500, Testing net (#0)
I1210 18:10:46.063987 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:10:47.434751 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:10:47.487761 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6822
I1210 18:10:47.487761 22120 solver.cpp:397]     Test net output #1: loss = 1.16902 (* 1 = 1.16902 loss)
I1210 18:10:47.543759 22120 solver.cpp:218] Iteration 97500 (14.0874 iter/s, 7.09852s/100 iters), loss = 0.399195
I1210 18:10:47.543759 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:10:47.543759 22120 solver.cpp:237]     Train net output #1: loss = 0.399195 (* 1 = 0.399195 loss)
I1210 18:10:47.543759 22120 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1210 18:10:53.185279 22120 solver.cpp:218] Iteration 97600 (17.7275 iter/s, 5.64096s/100 iters), loss = 0.516839
I1210 18:10:53.185279 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:10:53.185279 22120 solver.cpp:237]     Train net output #1: loss = 0.516839 (* 1 = 0.516839 loss)
I1210 18:10:53.185279 22120 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1210 18:10:58.856451 22120 solver.cpp:218] Iteration 97700 (17.633 iter/s, 5.67118s/100 iters), loss = 0.382932
I1210 18:10:58.856451 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:10:58.856451 22120 solver.cpp:237]     Train net output #1: loss = 0.382932 (* 1 = 0.382932 loss)
I1210 18:10:58.856451 22120 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1210 18:11:04.501863 22120 solver.cpp:218] Iteration 97800 (17.7154 iter/s, 5.64482s/100 iters), loss = 0.417273
I1210 18:11:04.501863 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:11:04.501863 22120 solver.cpp:237]     Train net output #1: loss = 0.417273 (* 1 = 0.417273 loss)
I1210 18:11:04.501863 22120 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1210 18:11:10.143282 22120 solver.cpp:218] Iteration 97900 (17.7276 iter/s, 5.64091s/100 iters), loss = 0.476663
I1210 18:11:10.143282 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 18:11:10.143282 22120 solver.cpp:237]     Train net output #1: loss = 0.476663 (* 1 = 0.476663 loss)
I1210 18:11:10.143282 22120 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1210 18:11:15.503677 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:11:15.726688 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_98000.caffemodel
I1210 18:11:15.742686 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_98000.solverstate
I1210 18:11:15.747686 22120 solver.cpp:330] Iteration 98000, Testing net (#0)
I1210 18:11:15.747686 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:11:17.130030 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:11:17.185031 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6823
I1210 18:11:17.185031 22120 solver.cpp:397]     Test net output #1: loss = 1.17184 (* 1 = 1.17184 loss)
I1210 18:11:17.238046 22120 solver.cpp:218] Iteration 98000 (14.0959 iter/s, 7.09427s/100 iters), loss = 0.341969
I1210 18:11:17.238046 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:11:17.238046 22120 solver.cpp:237]     Train net output #1: loss = 0.341969 (* 1 = 0.341969 loss)
I1210 18:11:17.238046 22120 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1210 18:11:22.910444 22120 solver.cpp:218] Iteration 98100 (17.6297 iter/s, 5.67223s/100 iters), loss = 0.414452
I1210 18:11:22.911445 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:11:22.911445 22120 solver.cpp:237]     Train net output #1: loss = 0.414452 (* 1 = 0.414452 loss)
I1210 18:11:22.911445 22120 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1210 18:11:28.560884 22120 solver.cpp:218] Iteration 98200 (17.7014 iter/s, 5.64926s/100 iters), loss = 0.330918
I1210 18:11:28.560884 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:11:28.560884 22120 solver.cpp:237]     Train net output #1: loss = 0.330918 (* 1 = 0.330918 loss)
I1210 18:11:28.560884 22120 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1210 18:11:34.304117 22120 solver.cpp:218] Iteration 98300 (17.413 iter/s, 5.74283s/100 iters), loss = 0.51123
I1210 18:11:34.304117 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 18:11:34.304117 22120 solver.cpp:237]     Train net output #1: loss = 0.51123 (* 1 = 0.51123 loss)
I1210 18:11:34.304117 22120 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1210 18:11:39.981547 22120 solver.cpp:218] Iteration 98400 (17.6151 iter/s, 5.67695s/100 iters), loss = 0.449418
I1210 18:11:39.981547 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 18:11:39.982048 22120 solver.cpp:237]     Train net output #1: loss = 0.449418 (* 1 = 0.449418 loss)
I1210 18:11:39.982048 22120 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1210 18:11:45.352949 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:11:45.574473 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_98500.caffemodel
I1210 18:11:45.588977 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_98500.solverstate
I1210 18:11:45.593977 22120 solver.cpp:330] Iteration 98500, Testing net (#0)
I1210 18:11:45.593977 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:11:46.963063 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:11:47.016067 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6807
I1210 18:11:47.017067 22120 solver.cpp:397]     Test net output #1: loss = 1.17979 (* 1 = 1.17979 loss)
I1210 18:11:47.071568 22120 solver.cpp:218] Iteration 98500 (14.1057 iter/s, 7.08935s/100 iters), loss = 0.36002
I1210 18:11:47.071568 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:11:47.071568 22120 solver.cpp:237]     Train net output #1: loss = 0.36002 (* 1 = 0.36002 loss)
I1210 18:11:47.071568 22120 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1210 18:11:52.713507 22120 solver.cpp:218] Iteration 98600 (17.7245 iter/s, 5.6419s/100 iters), loss = 0.421769
I1210 18:11:52.713507 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:11:52.713507 22120 solver.cpp:237]     Train net output #1: loss = 0.421769 (* 1 = 0.421769 loss)
I1210 18:11:52.713507 22120 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1210 18:11:58.359874 22120 solver.cpp:218] Iteration 98700 (17.7142 iter/s, 5.64517s/100 iters), loss = 0.338698
I1210 18:11:58.359874 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:11:58.359874 22120 solver.cpp:237]     Train net output #1: loss = 0.338698 (* 1 = 0.338698 loss)
I1210 18:11:58.359874 22120 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1210 18:12:03.996285 22120 solver.cpp:218] Iteration 98800 (17.7432 iter/s, 5.63598s/100 iters), loss = 0.507903
I1210 18:12:03.996285 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 18:12:03.996285 22120 solver.cpp:237]     Train net output #1: loss = 0.507903 (* 1 = 0.507903 loss)
I1210 18:12:03.996285 22120 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1210 18:12:09.640749 22120 solver.cpp:218] Iteration 98900 (17.7157 iter/s, 5.64472s/100 iters), loss = 0.426939
I1210 18:12:09.640749 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 18:12:09.640749 22120 solver.cpp:237]     Train net output #1: loss = 0.426939 (* 1 = 0.426939 loss)
I1210 18:12:09.640749 22120 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1210 18:12:15.009141 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:12:15.233157 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_99000.caffemodel
I1210 18:12:15.248159 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_99000.solverstate
I1210 18:12:15.253157 22120 solver.cpp:330] Iteration 99000, Testing net (#0)
I1210 18:12:15.253157 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:12:16.633271 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:12:16.687271 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6823
I1210 18:12:16.687271 22120 solver.cpp:397]     Test net output #1: loss = 1.17513 (* 1 = 1.17513 loss)
I1210 18:12:16.741276 22120 solver.cpp:218] Iteration 99000 (14.0847 iter/s, 7.09992s/100 iters), loss = 0.335412
I1210 18:12:16.741276 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:12:16.741276 22120 solver.cpp:237]     Train net output #1: loss = 0.335412 (* 1 = 0.335412 loss)
I1210 18:12:16.741276 22120 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1210 18:12:22.408730 22120 solver.cpp:218] Iteration 99100 (17.6469 iter/s, 5.66671s/100 iters), loss = 0.429513
I1210 18:12:22.408730 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:12:22.408730 22120 solver.cpp:237]     Train net output #1: loss = 0.429513 (* 1 = 0.429513 loss)
I1210 18:12:22.408730 22120 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1210 18:12:28.062193 22120 solver.cpp:218] Iteration 99200 (17.6901 iter/s, 5.65286s/100 iters), loss = 0.358105
I1210 18:12:28.062193 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 18:12:28.062193 22120 solver.cpp:237]     Train net output #1: loss = 0.358105 (* 1 = 0.358105 loss)
I1210 18:12:28.062193 22120 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1210 18:12:33.691591 22120 solver.cpp:218] Iteration 99300 (17.7645 iter/s, 5.6292s/100 iters), loss = 0.512845
I1210 18:12:33.691591 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 18:12:33.691591 22120 solver.cpp:237]     Train net output #1: loss = 0.512845 (* 1 = 0.512845 loss)
I1210 18:12:33.691591 22120 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1210 18:12:39.330972 22120 solver.cpp:218] Iteration 99400 (17.7327 iter/s, 5.63929s/100 iters), loss = 0.541784
I1210 18:12:39.330972 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 18:12:39.330972 22120 solver.cpp:237]     Train net output #1: loss = 0.541784 (* 1 = 0.541784 loss)
I1210 18:12:39.330972 22120 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1210 18:12:44.705348 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:12:44.926367 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_99500.caffemodel
I1210 18:12:44.940372 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_99500.solverstate
I1210 18:12:44.945372 22120 solver.cpp:330] Iteration 99500, Testing net (#0)
I1210 18:12:44.945372 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:12:46.316483 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:12:46.370486 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6839
I1210 18:12:46.370486 22120 solver.cpp:397]     Test net output #1: loss = 1.18113 (* 1 = 1.18113 loss)
I1210 18:12:46.424487 22120 solver.cpp:218] Iteration 99500 (14.0995 iter/s, 7.09243s/100 iters), loss = 0.398862
I1210 18:12:46.424487 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 18:12:46.424487 22120 solver.cpp:237]     Train net output #1: loss = 0.398862 (* 1 = 0.398862 loss)
I1210 18:12:46.424487 22120 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1210 18:12:52.072752 22120 solver.cpp:218] Iteration 99600 (17.7059 iter/s, 5.64783s/100 iters), loss = 0.402663
I1210 18:12:52.072752 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 18:12:52.072752 22120 solver.cpp:237]     Train net output #1: loss = 0.402663 (* 1 = 0.402663 loss)
I1210 18:12:52.072752 22120 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1210 18:12:57.742377 22120 solver.cpp:218] Iteration 99700 (17.6392 iter/s, 5.66918s/100 iters), loss = 0.399911
I1210 18:12:57.742377 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 18:12:57.742377 22120 solver.cpp:237]     Train net output #1: loss = 0.399911 (* 1 = 0.399911 loss)
I1210 18:12:57.742377 22120 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1210 18:13:03.396699 22120 solver.cpp:218] Iteration 99800 (17.6871 iter/s, 5.65382s/100 iters), loss = 0.444582
I1210 18:13:03.396699 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:13:03.396699 22120 solver.cpp:237]     Train net output #1: loss = 0.444582 (* 1 = 0.444582 loss)
I1210 18:13:03.396699 22120 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1210 18:13:09.042261 22120 solver.cpp:218] Iteration 99900 (17.715 iter/s, 5.64494s/100 iters), loss = 0.540011
I1210 18:13:09.042261 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:13:09.042261 22120 solver.cpp:237]     Train net output #1: loss = 0.540011 (* 1 = 0.540011 loss)
I1210 18:13:09.042261 22120 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1210 18:13:14.404980 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:13:14.628511 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_100000.caffemodel
I1210 18:13:14.643015 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_100000.solverstate
I1210 18:13:14.647014 22120 solver.cpp:330] Iteration 100000, Testing net (#0)
I1210 18:13:14.648015 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:13:16.022115 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:13:16.075131 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6856
I1210 18:13:16.075131 22120 solver.cpp:397]     Test net output #1: loss = 1.17664 (* 1 = 1.17664 loss)
I1210 18:13:16.129158 22120 solver.cpp:218] Iteration 100000 (14.1119 iter/s, 7.0862s/100 iters), loss = 0.263827
I1210 18:13:16.129158 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:13:16.129158 22120 solver.cpp:237]     Train net output #1: loss = 0.263827 (* 1 = 0.263827 loss)
I1210 18:13:16.129158 22120 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1210 18:13:21.904033 22120 solver.cpp:218] Iteration 100100 (17.3162 iter/s, 5.77493s/100 iters), loss = 0.436252
I1210 18:13:21.904033 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 18:13:21.904033 22120 solver.cpp:237]     Train net output #1: loss = 0.436252 (* 1 = 0.436252 loss)
I1210 18:13:21.904033 22120 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1210 18:13:27.591596 22120 solver.cpp:218] Iteration 100200 (17.5833 iter/s, 5.68723s/100 iters), loss = 0.412041
I1210 18:13:27.591596 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:13:27.591596 22120 solver.cpp:237]     Train net output #1: loss = 0.412041 (* 1 = 0.412041 loss)
I1210 18:13:27.591596 22120 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1210 18:13:33.229118 22120 solver.cpp:218] Iteration 100300 (17.7408 iter/s, 5.63672s/100 iters), loss = 0.441688
I1210 18:13:33.229118 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:13:33.229118 22120 solver.cpp:237]     Train net output #1: loss = 0.441688 (* 1 = 0.441688 loss)
I1210 18:13:33.229118 22120 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1210 18:13:38.872576 22120 solver.cpp:218] Iteration 100400 (17.7195 iter/s, 5.64351s/100 iters), loss = 0.461187
I1210 18:13:38.873579 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 18:13:38.873579 22120 solver.cpp:237]     Train net output #1: loss = 0.461187 (* 1 = 0.461187 loss)
I1210 18:13:38.873579 22120 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1210 18:13:44.243989 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:13:44.467005 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_100500.caffemodel
I1210 18:13:44.482004 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_100500.solverstate
I1210 18:13:44.487005 22120 solver.cpp:330] Iteration 100500, Testing net (#0)
I1210 18:13:44.487005 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:13:45.856142 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:13:45.911132 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6814
I1210 18:13:45.911132 22120 solver.cpp:397]     Test net output #1: loss = 1.1926 (* 1 = 1.1926 loss)
I1210 18:13:45.966138 22120 solver.cpp:218] Iteration 100500 (14.0997 iter/s, 7.09237s/100 iters), loss = 0.359039
I1210 18:13:45.966138 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 18:13:45.966138 22120 solver.cpp:237]     Train net output #1: loss = 0.359039 (* 1 = 0.359039 loss)
I1210 18:13:45.966138 22120 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1210 18:13:51.604593 22120 solver.cpp:218] Iteration 100600 (17.7358 iter/s, 5.63832s/100 iters), loss = 0.43485
I1210 18:13:51.604593 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 18:13:51.604593 22120 solver.cpp:237]     Train net output #1: loss = 0.43485 (* 1 = 0.43485 loss)
I1210 18:13:51.604593 22120 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1210 18:13:57.255200 22120 solver.cpp:218] Iteration 100700 (17.6995 iter/s, 5.64988s/100 iters), loss = 0.342532
I1210 18:13:57.255200 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:13:57.255200 22120 solver.cpp:237]     Train net output #1: loss = 0.342532 (* 1 = 0.342532 loss)
I1210 18:13:57.255200 22120 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1210 18:14:02.891707 22120 solver.cpp:218] Iteration 100800 (17.7435 iter/s, 5.63588s/100 iters), loss = 0.447263
I1210 18:14:02.891707 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:14:02.891707 22120 solver.cpp:237]     Train net output #1: loss = 0.447263 (* 1 = 0.447263 loss)
I1210 18:14:02.891707 22120 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1210 18:14:08.521206 22120 solver.cpp:218] Iteration 100900 (17.7635 iter/s, 5.62953s/100 iters), loss = 0.402991
I1210 18:14:08.521206 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:14:08.521206 22120 solver.cpp:237]     Train net output #1: loss = 0.402991 (* 1 = 0.402991 loss)
I1210 18:14:08.521206 22120 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1210 18:14:13.882634 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:14:14.104642 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_101000.caffemodel
I1210 18:14:14.118649 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_101000.solverstate
I1210 18:14:14.123648 22120 solver.cpp:330] Iteration 101000, Testing net (#0)
I1210 18:14:14.123648 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:14:15.492748 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:14:15.545743 22120 solver.cpp:397]     Test net output #0: accuracy = 0.684
I1210 18:14:15.546244 22120 solver.cpp:397]     Test net output #1: loss = 1.19402 (* 1 = 1.19402 loss)
I1210 18:14:15.601749 22120 solver.cpp:218] Iteration 101000 (14.1244 iter/s, 7.07997s/100 iters), loss = 0.255795
I1210 18:14:15.601749 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:14:15.601749 22120 solver.cpp:237]     Train net output #1: loss = 0.255795 (* 1 = 0.255795 loss)
I1210 18:14:15.601749 22120 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1210 18:14:21.248610 22120 solver.cpp:218] Iteration 101100 (17.7116 iter/s, 5.646s/100 iters), loss = 0.461362
I1210 18:14:21.248610 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:14:21.248610 22120 solver.cpp:237]     Train net output #1: loss = 0.461362 (* 1 = 0.461362 loss)
I1210 18:14:21.248610 22120 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1210 18:14:26.889050 22120 solver.cpp:218] Iteration 101200 (17.7283 iter/s, 5.64071s/100 iters), loss = 0.351584
I1210 18:14:26.889050 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:14:26.889050 22120 solver.cpp:237]     Train net output #1: loss = 0.351583 (* 1 = 0.351583 loss)
I1210 18:14:26.889050 22120 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1210 18:14:32.525748 22120 solver.cpp:218] Iteration 101300 (17.7419 iter/s, 5.63639s/100 iters), loss = 0.382094
I1210 18:14:32.526736 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:14:32.526736 22120 solver.cpp:237]     Train net output #1: loss = 0.382094 (* 1 = 0.382094 loss)
I1210 18:14:32.526736 22120 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1210 18:14:38.165863 22120 solver.cpp:218] Iteration 101400 (17.7331 iter/s, 5.63917s/100 iters), loss = 0.399243
I1210 18:14:38.165863 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:14:38.165863 22120 solver.cpp:237]     Train net output #1: loss = 0.399243 (* 1 = 0.399243 loss)
I1210 18:14:38.165863 22120 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1210 18:14:43.582597 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:14:43.806612 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_101500.caffemodel
I1210 18:14:43.821611 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_101500.solverstate
I1210 18:14:43.826611 22120 solver.cpp:330] Iteration 101500, Testing net (#0)
I1210 18:14:43.826611 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:14:45.213757 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:14:45.270763 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6842
I1210 18:14:45.270763 22120 solver.cpp:397]     Test net output #1: loss = 1.19357 (* 1 = 1.19357 loss)
I1210 18:14:45.325763 22120 solver.cpp:218] Iteration 101500 (13.9678 iter/s, 7.15932s/100 iters), loss = 0.323435
I1210 18:14:45.325763 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:14:45.325763 22120 solver.cpp:237]     Train net output #1: loss = 0.323435 (* 1 = 0.323435 loss)
I1210 18:14:45.325763 22120 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1210 18:14:51.001284 22120 solver.cpp:218] Iteration 101600 (17.6217 iter/s, 5.67481s/100 iters), loss = 0.430237
I1210 18:14:51.001284 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:14:51.001284 22120 solver.cpp:237]     Train net output #1: loss = 0.430237 (* 1 = 0.430237 loss)
I1210 18:14:51.001284 22120 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1210 18:14:56.652734 22120 solver.cpp:218] Iteration 101700 (17.6939 iter/s, 5.65165s/100 iters), loss = 0.30463
I1210 18:14:56.652734 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:14:56.652734 22120 solver.cpp:237]     Train net output #1: loss = 0.30463 (* 1 = 0.30463 loss)
I1210 18:14:56.652734 22120 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1210 18:15:02.291225 22120 solver.cpp:218] Iteration 101800 (17.7374 iter/s, 5.63781s/100 iters), loss = 0.415501
I1210 18:15:02.291726 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 18:15:02.291726 22120 solver.cpp:237]     Train net output #1: loss = 0.415501 (* 1 = 0.415501 loss)
I1210 18:15:02.291726 22120 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1210 18:15:07.950680 22120 solver.cpp:218] Iteration 101900 (17.6725 iter/s, 5.65851s/100 iters), loss = 0.369207
I1210 18:15:07.950680 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:15:07.950680 22120 solver.cpp:237]     Train net output #1: loss = 0.369207 (* 1 = 0.369207 loss)
I1210 18:15:07.950680 22120 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1210 18:15:13.332070 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:15:13.554097 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_102000.caffemodel
I1210 18:15:13.569097 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_102000.solverstate
I1210 18:15:13.573097 22120 solver.cpp:330] Iteration 102000, Testing net (#0)
I1210 18:15:13.574098 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:15:14.950290 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:15:15.004295 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6824
I1210 18:15:15.004295 22120 solver.cpp:397]     Test net output #1: loss = 1.20813 (* 1 = 1.20813 loss)
I1210 18:15:15.057301 22120 solver.cpp:218] Iteration 102000 (14.0706 iter/s, 7.10703s/100 iters), loss = 0.256303
I1210 18:15:15.058300 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:15:15.058300 22120 solver.cpp:237]     Train net output #1: loss = 0.256303 (* 1 = 0.256303 loss)
I1210 18:15:15.058300 22120 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1210 18:15:20.704257 22120 solver.cpp:218] Iteration 102100 (17.7121 iter/s, 5.64587s/100 iters), loss = 0.391818
I1210 18:15:20.704257 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:15:20.704257 22120 solver.cpp:237]     Train net output #1: loss = 0.391818 (* 1 = 0.391818 loss)
I1210 18:15:20.704257 22120 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1210 18:15:26.368146 22120 solver.cpp:218] Iteration 102200 (17.658 iter/s, 5.66315s/100 iters), loss = 0.309076
I1210 18:15:26.368146 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:15:26.368146 22120 solver.cpp:237]     Train net output #1: loss = 0.309076 (* 1 = 0.309076 loss)
I1210 18:15:26.368146 22120 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1210 18:15:32.024581 22120 solver.cpp:218] Iteration 102300 (17.679 iter/s, 5.65643s/100 iters), loss = 0.424147
I1210 18:15:32.025581 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:15:32.025581 22120 solver.cpp:237]     Train net output #1: loss = 0.424147 (* 1 = 0.424147 loss)
I1210 18:15:32.025581 22120 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1210 18:15:37.682247 22120 solver.cpp:218] Iteration 102400 (17.6804 iter/s, 5.65599s/100 iters), loss = 0.35946
I1210 18:15:37.682247 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 18:15:37.682247 22120 solver.cpp:237]     Train net output #1: loss = 0.35946 (* 1 = 0.35946 loss)
I1210 18:15:37.682247 22120 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1210 18:15:43.058393 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:15:43.281405 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_102500.caffemodel
I1210 18:15:43.297914 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_102500.solverstate
I1210 18:15:43.303413 22120 solver.cpp:330] Iteration 102500, Testing net (#0)
I1210 18:15:43.303413 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:15:44.672549 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:15:44.726562 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6812
I1210 18:15:44.726562 22120 solver.cpp:397]     Test net output #1: loss = 1.2092 (* 1 = 1.2092 loss)
I1210 18:15:44.780562 22120 solver.cpp:218] Iteration 102500 (14.0889 iter/s, 7.09778s/100 iters), loss = 0.313788
I1210 18:15:44.780562 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:15:44.780562 22120 solver.cpp:237]     Train net output #1: loss = 0.313788 (* 1 = 0.313788 loss)
I1210 18:15:44.780562 22120 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1210 18:15:50.414041 22120 solver.cpp:218] Iteration 102600 (17.7503 iter/s, 5.6337s/100 iters), loss = 0.393527
I1210 18:15:50.414041 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:15:50.414041 22120 solver.cpp:237]     Train net output #1: loss = 0.393527 (* 1 = 0.393527 loss)
I1210 18:15:50.414041 22120 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1210 18:15:56.063479 22120 solver.cpp:218] Iteration 102700 (17.7023 iter/s, 5.649s/100 iters), loss = 0.290036
I1210 18:15:56.063479 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:15:56.063479 22120 solver.cpp:237]     Train net output #1: loss = 0.290036 (* 1 = 0.290036 loss)
I1210 18:15:56.063479 22120 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1210 18:16:01.701398 22120 solver.cpp:218] Iteration 102800 (17.74 iter/s, 5.63697s/100 iters), loss = 0.379192
I1210 18:16:01.701398 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:16:01.701398 22120 solver.cpp:237]     Train net output #1: loss = 0.379192 (* 1 = 0.379192 loss)
I1210 18:16:01.701398 22120 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1210 18:16:07.349325 22120 solver.cpp:218] Iteration 102900 (17.7077 iter/s, 5.64725s/100 iters), loss = 0.389349
I1210 18:16:07.349325 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 18:16:07.349325 22120 solver.cpp:237]     Train net output #1: loss = 0.389349 (* 1 = 0.389349 loss)
I1210 18:16:07.349325 22120 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1210 18:16:12.704723 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:16:12.926738 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_103000.caffemodel
I1210 18:16:12.941738 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_103000.solverstate
I1210 18:16:12.946738 22120 solver.cpp:330] Iteration 103000, Testing net (#0)
I1210 18:16:12.946738 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:16:14.326815 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:16:14.379814 22120 solver.cpp:397]     Test net output #0: accuracy = 0.68
I1210 18:16:14.379814 22120 solver.cpp:397]     Test net output #1: loss = 1.21208 (* 1 = 1.21208 loss)
I1210 18:16:14.433820 22120 solver.cpp:218] Iteration 103000 (14.1159 iter/s, 7.08423s/100 iters), loss = 0.295319
I1210 18:16:14.433820 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:16:14.433820 22120 solver.cpp:237]     Train net output #1: loss = 0.295319 (* 1 = 0.295319 loss)
I1210 18:16:14.433820 22120 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1210 18:16:20.081241 22120 solver.cpp:218] Iteration 103100 (17.7075 iter/s, 5.64732s/100 iters), loss = 0.403277
I1210 18:16:20.081241 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 18:16:20.081241 22120 solver.cpp:237]     Train net output #1: loss = 0.403277 (* 1 = 0.403277 loss)
I1210 18:16:20.081241 22120 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1210 18:16:25.744664 22120 solver.cpp:218] Iteration 103200 (17.6603 iter/s, 5.66241s/100 iters), loss = 0.343754
I1210 18:16:25.744664 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:16:25.744664 22120 solver.cpp:237]     Train net output #1: loss = 0.343754 (* 1 = 0.343754 loss)
I1210 18:16:25.744664 22120 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1210 18:16:31.409554 22120 solver.cpp:218] Iteration 103300 (17.6538 iter/s, 5.66451s/100 iters), loss = 0.427391
I1210 18:16:31.409554 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:16:31.409554 22120 solver.cpp:237]     Train net output #1: loss = 0.427391 (* 1 = 0.427391 loss)
I1210 18:16:31.409554 22120 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1210 18:16:37.081681 22120 solver.cpp:218] Iteration 103400 (17.6308 iter/s, 5.67189s/100 iters), loss = 0.344406
I1210 18:16:37.081681 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:16:37.081681 22120 solver.cpp:237]     Train net output #1: loss = 0.344406 (* 1 = 0.344406 loss)
I1210 18:16:37.081681 22120 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1210 18:16:42.464285 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:16:42.684295 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_103500.caffemodel
I1210 18:16:42.702800 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_103500.solverstate
I1210 18:16:42.707801 22120 solver.cpp:330] Iteration 103500, Testing net (#0)
I1210 18:16:42.708302 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:16:44.081420 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:16:44.134433 22120 solver.cpp:397]     Test net output #0: accuracy = 0.684
I1210 18:16:44.135432 22120 solver.cpp:397]     Test net output #1: loss = 1.20501 (* 1 = 1.20501 loss)
I1210 18:16:44.189431 22120 solver.cpp:218] Iteration 103500 (14.07 iter/s, 7.10731s/100 iters), loss = 0.34811
I1210 18:16:44.189431 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:16:44.189431 22120 solver.cpp:237]     Train net output #1: loss = 0.34811 (* 1 = 0.34811 loss)
I1210 18:16:44.189431 22120 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1210 18:16:49.832862 22120 solver.cpp:218] Iteration 103600 (17.7216 iter/s, 5.64284s/100 iters), loss = 0.350025
I1210 18:16:49.832862 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:16:49.832862 22120 solver.cpp:237]     Train net output #1: loss = 0.350025 (* 1 = 0.350025 loss)
I1210 18:16:49.832862 22120 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1210 18:16:55.501291 22120 solver.cpp:218] Iteration 103700 (17.6422 iter/s, 5.66822s/100 iters), loss = 0.313084
I1210 18:16:55.501291 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:16:55.501291 22120 solver.cpp:237]     Train net output #1: loss = 0.313084 (* 1 = 0.313084 loss)
I1210 18:16:55.501291 22120 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1210 18:17:01.152745 22120 solver.cpp:218] Iteration 103800 (17.6973 iter/s, 5.65056s/100 iters), loss = 0.379614
I1210 18:17:01.152745 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:17:01.152745 22120 solver.cpp:237]     Train net output #1: loss = 0.379614 (* 1 = 0.379614 loss)
I1210 18:17:01.152745 22120 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1210 18:17:06.836238 22120 solver.cpp:218] Iteration 103900 (17.5957 iter/s, 5.68321s/100 iters), loss = 0.434961
I1210 18:17:06.836238 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:17:06.836238 22120 solver.cpp:237]     Train net output #1: loss = 0.434961 (* 1 = 0.434961 loss)
I1210 18:17:06.836238 22120 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1210 18:17:12.197659 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:17:12.421691 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_104000.caffemodel
I1210 18:17:12.437690 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_104000.solverstate
I1210 18:17:12.442693 22120 solver.cpp:330] Iteration 104000, Testing net (#0)
I1210 18:17:12.442693 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:17:13.817811 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:17:13.870813 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6767
I1210 18:17:13.870813 22120 solver.cpp:397]     Test net output #1: loss = 1.21706 (* 1 = 1.21706 loss)
I1210 18:17:13.924819 22120 solver.cpp:218] Iteration 104000 (14.1077 iter/s, 7.08833s/100 iters), loss = 0.312525
I1210 18:17:13.924819 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:17:13.924819 22120 solver.cpp:237]     Train net output #1: loss = 0.312525 (* 1 = 0.312525 loss)
I1210 18:17:13.924819 22120 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1210 18:17:19.589282 22120 solver.cpp:218] Iteration 104100 (17.6565 iter/s, 5.66363s/100 iters), loss = 0.401805
I1210 18:17:19.589282 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:17:19.589282 22120 solver.cpp:237]     Train net output #1: loss = 0.401805 (* 1 = 0.401805 loss)
I1210 18:17:19.589282 22120 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1210 18:17:25.249752 22120 solver.cpp:218] Iteration 104200 (17.6681 iter/s, 5.65992s/100 iters), loss = 0.379302
I1210 18:17:25.249752 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:17:25.249752 22120 solver.cpp:237]     Train net output #1: loss = 0.379302 (* 1 = 0.379302 loss)
I1210 18:17:25.249752 22120 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1210 18:17:30.907701 22120 solver.cpp:218] Iteration 104300 (17.6758 iter/s, 5.65745s/100 iters), loss = 0.33794
I1210 18:17:30.907701 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:17:30.907701 22120 solver.cpp:237]     Train net output #1: loss = 0.33794 (* 1 = 0.33794 loss)
I1210 18:17:30.907701 22120 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1210 18:17:36.558617 22120 solver.cpp:218] Iteration 104400 (17.6982 iter/s, 5.65029s/100 iters), loss = 0.358135
I1210 18:17:36.558617 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:17:36.558617 22120 solver.cpp:237]     Train net output #1: loss = 0.358135 (* 1 = 0.358135 loss)
I1210 18:17:36.558617 22120 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1210 18:17:41.946090 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:17:42.168260 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_104500.caffemodel
I1210 18:17:42.183260 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_104500.solverstate
I1210 18:17:42.188261 22120 solver.cpp:330] Iteration 104500, Testing net (#0)
I1210 18:17:42.188261 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:17:43.558398 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:17:43.614914 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6788
I1210 18:17:43.614914 22120 solver.cpp:397]     Test net output #1: loss = 1.21886 (* 1 = 1.21886 loss)
I1210 18:17:43.672416 22120 solver.cpp:218] Iteration 104500 (14.0575 iter/s, 7.11365s/100 iters), loss = 0.337887
I1210 18:17:43.672416 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:17:43.672416 22120 solver.cpp:237]     Train net output #1: loss = 0.337887 (* 1 = 0.337887 loss)
I1210 18:17:43.672416 22120 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1210 18:17:49.447088 22120 solver.cpp:218] Iteration 104600 (17.3168 iter/s, 5.77474s/100 iters), loss = 0.419924
I1210 18:17:49.448089 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:17:49.448089 22120 solver.cpp:237]     Train net output #1: loss = 0.419924 (* 1 = 0.419924 loss)
I1210 18:17:49.448089 22120 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1210 18:17:55.091722 22120 solver.cpp:218] Iteration 104700 (17.7194 iter/s, 5.64353s/100 iters), loss = 0.308682
I1210 18:17:55.091722 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:17:55.091722 22120 solver.cpp:237]     Train net output #1: loss = 0.308682 (* 1 = 0.308682 loss)
I1210 18:17:55.091722 22120 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1210 18:18:00.730197 22120 solver.cpp:218] Iteration 104800 (17.7371 iter/s, 5.6379s/100 iters), loss = 0.400835
I1210 18:18:00.730197 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:18:00.730197 22120 solver.cpp:237]     Train net output #1: loss = 0.400835 (* 1 = 0.400835 loss)
I1210 18:18:00.730197 22120 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1210 18:18:06.371904 22120 solver.cpp:218] Iteration 104900 (17.727 iter/s, 5.64112s/100 iters), loss = 0.392827
I1210 18:18:06.371904 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:18:06.371904 22120 solver.cpp:237]     Train net output #1: loss = 0.392827 (* 1 = 0.392827 loss)
I1210 18:18:06.371904 22120 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1210 18:18:11.753363 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:18:11.976373 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_105000.caffemodel
I1210 18:18:11.990373 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_105000.solverstate
I1210 18:18:11.995373 22120 solver.cpp:330] Iteration 105000, Testing net (#0)
I1210 18:18:11.995373 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:18:13.371529 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:18:13.426534 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6814
I1210 18:18:13.426534 22120 solver.cpp:397]     Test net output #1: loss = 1.22312 (* 1 = 1.22312 loss)
I1210 18:18:13.480537 22120 solver.cpp:218] Iteration 105000 (14.0669 iter/s, 7.10891s/100 iters), loss = 0.231758
I1210 18:18:13.480537 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:18:13.480537 22120 solver.cpp:237]     Train net output #1: loss = 0.231758 (* 1 = 0.231758 loss)
I1210 18:18:13.480537 22120 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1210 18:18:19.248190 22120 solver.cpp:218] Iteration 105100 (17.3419 iter/s, 5.76639s/100 iters), loss = 0.347272
I1210 18:18:19.248190 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:18:19.248190 22120 solver.cpp:237]     Train net output #1: loss = 0.347272 (* 1 = 0.347272 loss)
I1210 18:18:19.248190 22120 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1210 18:18:25.010002 22120 solver.cpp:218] Iteration 105200 (17.3564 iter/s, 5.76156s/100 iters), loss = 0.300153
I1210 18:18:25.010002 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:18:25.010002 22120 solver.cpp:237]     Train net output #1: loss = 0.300153 (* 1 = 0.300153 loss)
I1210 18:18:25.010002 22120 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1210 18:18:30.683720 22120 solver.cpp:218] Iteration 105300 (17.6267 iter/s, 5.67322s/100 iters), loss = 0.343637
I1210 18:18:30.683720 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:18:30.683720 22120 solver.cpp:237]     Train net output #1: loss = 0.343636 (* 1 = 0.343636 loss)
I1210 18:18:30.683720 22120 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1210 18:18:36.358016 22120 solver.cpp:218] Iteration 105400 (17.625 iter/s, 5.67375s/100 iters), loss = 0.370801
I1210 18:18:36.358016 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:18:36.358016 22120 solver.cpp:237]     Train net output #1: loss = 0.370801 (* 1 = 0.370801 loss)
I1210 18:18:36.358016 22120 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1210 18:18:41.746446 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:18:41.971462 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_105500.caffemodel
I1210 18:18:41.986459 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_105500.solverstate
I1210 18:18:41.992460 22120 solver.cpp:330] Iteration 105500, Testing net (#0)
I1210 18:18:41.992460 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:18:43.370913 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:18:43.424907 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6789
I1210 18:18:43.424907 22120 solver.cpp:397]     Test net output #1: loss = 1.22496 (* 1 = 1.22496 loss)
I1210 18:18:43.477908 22120 solver.cpp:218] Iteration 105500 (14.0463 iter/s, 7.11929s/100 iters), loss = 0.273461
I1210 18:18:43.477908 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:18:43.477908 22120 solver.cpp:237]     Train net output #1: loss = 0.273461 (* 1 = 0.273461 loss)
I1210 18:18:43.477908 22120 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1210 18:18:49.146126 22120 solver.cpp:218] Iteration 105600 (17.6428 iter/s, 5.66805s/100 iters), loss = 0.32201
I1210 18:18:49.146126 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:18:49.146126 22120 solver.cpp:237]     Train net output #1: loss = 0.32201 (* 1 = 0.32201 loss)
I1210 18:18:49.146126 22120 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1210 18:18:54.781601 22120 solver.cpp:218] Iteration 105700 (17.7457 iter/s, 5.63517s/100 iters), loss = 0.310597
I1210 18:18:54.781601 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:18:54.781601 22120 solver.cpp:237]     Train net output #1: loss = 0.310597 (* 1 = 0.310597 loss)
I1210 18:18:54.781601 22120 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1210 18:19:00.425642 22120 solver.cpp:218] Iteration 105800 (17.7201 iter/s, 5.64329s/100 iters), loss = 0.477108
I1210 18:19:00.426144 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:19:00.426144 22120 solver.cpp:237]     Train net output #1: loss = 0.477108 (* 1 = 0.477108 loss)
I1210 18:19:00.426144 22120 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1210 18:19:06.063323 22120 solver.cpp:218] Iteration 105900 (17.7378 iter/s, 5.63768s/100 iters), loss = 0.355985
I1210 18:19:06.063323 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:19:06.063323 22120 solver.cpp:237]     Train net output #1: loss = 0.355985 (* 1 = 0.355985 loss)
I1210 18:19:06.063323 22120 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1210 18:19:11.438284 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:19:11.660243 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_106000.caffemodel
I1210 18:19:11.677242 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_106000.solverstate
I1210 18:19:11.682242 22120 solver.cpp:330] Iteration 106000, Testing net (#0)
I1210 18:19:11.682242 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:19:13.066356 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:19:13.121356 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6795
I1210 18:19:13.121356 22120 solver.cpp:397]     Test net output #1: loss = 1.2202 (* 1 = 1.2202 loss)
I1210 18:19:13.175892 22120 solver.cpp:218] Iteration 106000 (14.0614 iter/s, 7.11169s/100 iters), loss = 0.31093
I1210 18:19:13.175892 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:19:13.175892 22120 solver.cpp:237]     Train net output #1: loss = 0.31093 (* 1 = 0.31093 loss)
I1210 18:19:13.175892 22120 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1210 18:19:18.875514 22120 solver.cpp:218] Iteration 106100 (17.5474 iter/s, 5.69886s/100 iters), loss = 0.335503
I1210 18:19:18.875514 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:19:18.875514 22120 solver.cpp:237]     Train net output #1: loss = 0.335502 (* 1 = 0.335502 loss)
I1210 18:19:18.875514 22120 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1210 18:19:24.581892 22120 solver.cpp:218] Iteration 106200 (17.5239 iter/s, 5.70649s/100 iters), loss = 0.33108
I1210 18:19:24.581892 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:19:24.581892 22120 solver.cpp:237]     Train net output #1: loss = 0.33108 (* 1 = 0.33108 loss)
I1210 18:19:24.581892 22120 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1210 18:19:30.241286 22120 solver.cpp:218] Iteration 106300 (17.6732 iter/s, 5.65828s/100 iters), loss = 0.434624
I1210 18:19:30.241286 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 18:19:30.241286 22120 solver.cpp:237]     Train net output #1: loss = 0.434624 (* 1 = 0.434624 loss)
I1210 18:19:30.241286 22120 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1210 18:19:35.888906 22120 solver.cpp:218] Iteration 106400 (17.7087 iter/s, 5.64694s/100 iters), loss = 0.405799
I1210 18:19:35.888906 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:19:35.888906 22120 solver.cpp:237]     Train net output #1: loss = 0.405798 (* 1 = 0.405798 loss)
I1210 18:19:35.888906 22120 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1210 18:19:41.253057 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:19:41.473906 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_106500.caffemodel
I1210 18:19:41.487901 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_106500.solverstate
I1210 18:19:41.492885 22120 solver.cpp:330] Iteration 106500, Testing net (#0)
I1210 18:19:41.492885 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:19:42.864722 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:19:42.918709 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6808
I1210 18:19:42.918709 22120 solver.cpp:397]     Test net output #1: loss = 1.22151 (* 1 = 1.22151 loss)
I1210 18:19:42.972242 22120 solver.cpp:218] Iteration 106500 (14.1178 iter/s, 7.08324s/100 iters), loss = 0.30588
I1210 18:19:42.972242 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:19:42.972242 22120 solver.cpp:237]     Train net output #1: loss = 0.30588 (* 1 = 0.30588 loss)
I1210 18:19:42.972242 22120 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1210 18:19:48.683542 22120 solver.cpp:218] Iteration 106600 (17.5098 iter/s, 5.71108s/100 iters), loss = 0.354853
I1210 18:19:48.683542 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:19:48.683542 22120 solver.cpp:237]     Train net output #1: loss = 0.354853 (* 1 = 0.354853 loss)
I1210 18:19:48.683542 22120 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1210 18:19:54.306123 22120 solver.cpp:218] Iteration 106700 (17.7884 iter/s, 5.62163s/100 iters), loss = 0.248454
I1210 18:19:54.306123 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 18:19:54.306123 22120 solver.cpp:237]     Train net output #1: loss = 0.248454 (* 1 = 0.248454 loss)
I1210 18:19:54.306123 22120 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1210 18:19:59.947526 22120 solver.cpp:218] Iteration 106800 (17.7275 iter/s, 5.64095s/100 iters), loss = 0.415027
I1210 18:19:59.947526 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 18:19:59.947526 22120 solver.cpp:237]     Train net output #1: loss = 0.415027 (* 1 = 0.415027 loss)
I1210 18:19:59.947526 22120 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1210 18:20:05.646371 22120 solver.cpp:218] Iteration 106900 (17.5485 iter/s, 5.69851s/100 iters), loss = 0.425021
I1210 18:20:05.646371 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:20:05.646371 22120 solver.cpp:237]     Train net output #1: loss = 0.425021 (* 1 = 0.425021 loss)
I1210 18:20:05.646371 22120 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1210 18:20:11.055162 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:20:11.275173 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_107000.caffemodel
I1210 18:20:11.290174 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_107000.solverstate
I1210 18:20:11.295174 22120 solver.cpp:330] Iteration 107000, Testing net (#0)
I1210 18:20:11.295174 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:20:12.667490 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:20:12.722993 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6812
I1210 18:20:12.722993 22120 solver.cpp:397]     Test net output #1: loss = 1.2251 (* 1 = 1.2251 loss)
I1210 18:20:12.776497 22120 solver.cpp:218] Iteration 107000 (14.0272 iter/s, 7.12899s/100 iters), loss = 0.267122
I1210 18:20:12.776497 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:20:12.776497 22120 solver.cpp:237]     Train net output #1: loss = 0.267122 (* 1 = 0.267122 loss)
I1210 18:20:12.776497 22120 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1210 18:20:18.481873 22120 solver.cpp:218] Iteration 107100 (17.5262 iter/s, 5.70574s/100 iters), loss = 0.389311
I1210 18:20:18.481873 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:20:18.481873 22120 solver.cpp:237]     Train net output #1: loss = 0.389311 (* 1 = 0.389311 loss)
I1210 18:20:18.481873 22120 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1210 18:20:24.126296 22120 solver.cpp:218] Iteration 107200 (17.7183 iter/s, 5.64388s/100 iters), loss = 0.364637
I1210 18:20:24.126296 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:20:24.126296 22120 solver.cpp:237]     Train net output #1: loss = 0.364637 (* 1 = 0.364637 loss)
I1210 18:20:24.126296 22120 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1210 18:20:29.794598 22120 solver.cpp:218] Iteration 107300 (17.6454 iter/s, 5.6672s/100 iters), loss = 0.357732
I1210 18:20:29.794598 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:20:29.794598 22120 solver.cpp:237]     Train net output #1: loss = 0.357732 (* 1 = 0.357732 loss)
I1210 18:20:29.794598 22120 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1210 18:20:35.482062 22120 solver.cpp:218] Iteration 107400 (17.5818 iter/s, 5.68769s/100 iters), loss = 0.424592
I1210 18:20:35.483063 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:20:35.483063 22120 solver.cpp:237]     Train net output #1: loss = 0.424592 (* 1 = 0.424592 loss)
I1210 18:20:35.483063 22120 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1210 18:20:41.028342 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:20:41.259357 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_107500.caffemodel
I1210 18:20:41.273357 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_107500.solverstate
I1210 18:20:41.278358 22120 solver.cpp:330] Iteration 107500, Testing net (#0)
I1210 18:20:41.278358 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:20:42.679755 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:20:42.732759 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6811
I1210 18:20:42.732759 22120 solver.cpp:397]     Test net output #1: loss = 1.23481 (* 1 = 1.23481 loss)
I1210 18:20:42.785760 22120 solver.cpp:218] Iteration 107500 (13.694 iter/s, 7.30245s/100 iters), loss = 0.297608
I1210 18:20:42.785760 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:20:42.785760 22120 solver.cpp:237]     Train net output #1: loss = 0.297608 (* 1 = 0.297608 loss)
I1210 18:20:42.785760 22120 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1210 18:20:48.423211 22120 solver.cpp:218] Iteration 107600 (17.7411 iter/s, 5.63664s/100 iters), loss = 0.397482
I1210 18:20:48.423211 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 18:20:48.423211 22120 solver.cpp:237]     Train net output #1: loss = 0.397482 (* 1 = 0.397482 loss)
I1210 18:20:48.423211 22120 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1210 18:20:54.060621 22120 solver.cpp:218] Iteration 107700 (17.7399 iter/s, 5.63701s/100 iters), loss = 0.260885
I1210 18:20:54.060621 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:20:54.060621 22120 solver.cpp:237]     Train net output #1: loss = 0.260884 (* 1 = 0.260884 loss)
I1210 18:20:54.060621 22120 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1210 18:20:59.767006 22120 solver.cpp:218] Iteration 107800 (17.5261 iter/s, 5.70579s/100 iters), loss = 0.294208
I1210 18:20:59.767006 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:20:59.767006 22120 solver.cpp:237]     Train net output #1: loss = 0.294208 (* 1 = 0.294208 loss)
I1210 18:20:59.767006 22120 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1210 18:21:05.569420 22120 solver.cpp:218] Iteration 107900 (17.2352 iter/s, 5.80208s/100 iters), loss = 0.402286
I1210 18:21:05.569420 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:21:05.569420 22120 solver.cpp:237]     Train net output #1: loss = 0.402285 (* 1 = 0.402285 loss)
I1210 18:21:05.569420 22120 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1210 18:21:10.961036 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:21:11.183050 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_108000.caffemodel
I1210 18:21:11.198051 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_108000.solverstate
I1210 18:21:11.203052 22120 solver.cpp:330] Iteration 108000, Testing net (#0)
I1210 18:21:11.203052 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:21:12.570185 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:21:12.623687 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6858
I1210 18:21:12.623687 22120 solver.cpp:397]     Test net output #1: loss = 1.22403 (* 1 = 1.22403 loss)
I1210 18:21:12.676190 22120 solver.cpp:218] Iteration 108000 (14.0729 iter/s, 7.10583s/100 iters), loss = 0.313301
I1210 18:21:12.676190 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:21:12.676190 22120 solver.cpp:237]     Train net output #1: loss = 0.313301 (* 1 = 0.313301 loss)
I1210 18:21:12.676190 22120 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1210 18:21:18.365561 22120 solver.cpp:218] Iteration 108100 (17.5776 iter/s, 5.68907s/100 iters), loss = 0.384589
I1210 18:21:18.365561 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 18:21:18.365561 22120 solver.cpp:237]     Train net output #1: loss = 0.384589 (* 1 = 0.384589 loss)
I1210 18:21:18.365561 22120 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1210 18:21:24.187675 22120 solver.cpp:218] Iteration 108200 (17.1769 iter/s, 5.82178s/100 iters), loss = 0.315318
I1210 18:21:24.187675 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:21:24.187675 22120 solver.cpp:237]     Train net output #1: loss = 0.315318 (* 1 = 0.315318 loss)
I1210 18:21:24.187675 22120 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1210 18:21:29.885506 22120 solver.cpp:218] Iteration 108300 (17.5528 iter/s, 5.6971s/100 iters), loss = 0.431542
I1210 18:21:29.885506 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 18:21:29.885506 22120 solver.cpp:237]     Train net output #1: loss = 0.431542 (* 1 = 0.431542 loss)
I1210 18:21:29.885506 22120 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1210 18:21:35.571847 22120 solver.cpp:218] Iteration 108400 (17.5886 iter/s, 5.68551s/100 iters), loss = 0.352041
I1210 18:21:35.571847 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:21:35.571847 22120 solver.cpp:237]     Train net output #1: loss = 0.352041 (* 1 = 0.352041 loss)
I1210 18:21:35.571847 22120 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1210 18:21:40.985688 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:21:41.213232 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_108500.caffemodel
I1210 18:21:41.228232 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_108500.solverstate
I1210 18:21:41.232734 22120 solver.cpp:330] Iteration 108500, Testing net (#0)
I1210 18:21:41.232734 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:21:42.602265 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:21:42.656260 22120 solver.cpp:397]     Test net output #0: accuracy = 0.679
I1210 18:21:42.656260 22120 solver.cpp:397]     Test net output #1: loss = 1.24316 (* 1 = 1.24316 loss)
I1210 18:21:42.712270 22120 solver.cpp:218] Iteration 108500 (14.0048 iter/s, 7.1404s/100 iters), loss = 0.246675
I1210 18:21:42.713269 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:21:42.713269 22120 solver.cpp:237]     Train net output #1: loss = 0.246675 (* 1 = 0.246675 loss)
I1210 18:21:42.713269 22120 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1210 18:21:48.375620 22120 solver.cpp:218] Iteration 108600 (17.6614 iter/s, 5.66208s/100 iters), loss = 0.415265
I1210 18:21:48.375620 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:21:48.375620 22120 solver.cpp:237]     Train net output #1: loss = 0.415265 (* 1 = 0.415265 loss)
I1210 18:21:48.375620 22120 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1210 18:21:54.041576 22120 solver.cpp:218] Iteration 108700 (17.6513 iter/s, 5.66531s/100 iters), loss = 0.336705
I1210 18:21:54.041576 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:21:54.041576 22120 solver.cpp:237]     Train net output #1: loss = 0.336705 (* 1 = 0.336705 loss)
I1210 18:21:54.041576 22120 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1210 18:21:59.729499 22120 solver.cpp:218] Iteration 108800 (17.5802 iter/s, 5.68821s/100 iters), loss = 0.427693
I1210 18:21:59.729499 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:21:59.729499 22120 solver.cpp:237]     Train net output #1: loss = 0.427693 (* 1 = 0.427693 loss)
I1210 18:21:59.729499 22120 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1210 18:22:05.379251 22120 solver.cpp:218] Iteration 108900 (17.7031 iter/s, 5.64872s/100 iters), loss = 0.363035
I1210 18:22:05.379251 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 18:22:05.379251 22120 solver.cpp:237]     Train net output #1: loss = 0.363035 (* 1 = 0.363035 loss)
I1210 18:22:05.379251 22120 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1210 18:22:10.752306 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:22:10.975323 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_109000.caffemodel
I1210 18:22:10.990324 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_109000.solverstate
I1210 18:22:10.995326 22120 solver.cpp:330] Iteration 109000, Testing net (#0)
I1210 18:22:10.995326 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:22:12.370985 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:22:12.424489 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6787
I1210 18:22:12.424489 22120 solver.cpp:397]     Test net output #1: loss = 1.23941 (* 1 = 1.23941 loss)
I1210 18:22:12.478492 22120 solver.cpp:218] Iteration 109000 (14.0869 iter/s, 7.0988s/100 iters), loss = 0.271986
I1210 18:22:12.478492 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:22:12.478492 22120 solver.cpp:237]     Train net output #1: loss = 0.271986 (* 1 = 0.271986 loss)
I1210 18:22:12.478492 22120 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1210 18:22:18.142947 22120 solver.cpp:218] Iteration 109100 (17.6542 iter/s, 5.66438s/100 iters), loss = 0.382113
I1210 18:22:18.142947 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:22:18.142947 22120 solver.cpp:237]     Train net output #1: loss = 0.382113 (* 1 = 0.382113 loss)
I1210 18:22:18.142947 22120 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1210 18:22:23.797461 22120 solver.cpp:218] Iteration 109200 (17.6885 iter/s, 5.65338s/100 iters), loss = 0.313748
I1210 18:22:23.797461 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:22:23.797461 22120 solver.cpp:237]     Train net output #1: loss = 0.313748 (* 1 = 0.313748 loss)
I1210 18:22:23.797461 22120 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1210 18:22:29.483007 22120 solver.cpp:218] Iteration 109300 (17.5885 iter/s, 5.68554s/100 iters), loss = 0.396338
I1210 18:22:29.483007 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:22:29.483007 22120 solver.cpp:237]     Train net output #1: loss = 0.396338 (* 1 = 0.396338 loss)
I1210 18:22:29.483007 22120 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1210 18:22:35.146489 22120 solver.cpp:218] Iteration 109400 (17.6598 iter/s, 5.66259s/100 iters), loss = 0.337838
I1210 18:22:35.146489 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:22:35.146489 22120 solver.cpp:237]     Train net output #1: loss = 0.337838 (* 1 = 0.337838 loss)
I1210 18:22:35.146489 22120 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1210 18:22:40.557247 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:22:40.783267 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_109500.caffemodel
I1210 18:22:40.797266 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_109500.solverstate
I1210 18:22:40.802268 22120 solver.cpp:330] Iteration 109500, Testing net (#0)
I1210 18:22:40.802268 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:22:42.199434 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:22:42.254432 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6812
I1210 18:22:42.254432 22120 solver.cpp:397]     Test net output #1: loss = 1.2414 (* 1 = 1.2414 loss)
I1210 18:22:42.310441 22120 solver.cpp:218] Iteration 109500 (13.9588 iter/s, 7.16392s/100 iters), loss = 0.241568
I1210 18:22:42.310441 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:22:42.310441 22120 solver.cpp:237]     Train net output #1: loss = 0.241568 (* 1 = 0.241568 loss)
I1210 18:22:42.310441 22120 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1210 18:22:47.988538 22120 solver.cpp:218] Iteration 109600 (17.6133 iter/s, 5.67754s/100 iters), loss = 0.435741
I1210 18:22:47.988538 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:22:47.988538 22120 solver.cpp:237]     Train net output #1: loss = 0.435741 (* 1 = 0.435741 loss)
I1210 18:22:47.988538 22120 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1210 18:22:53.678098 22120 solver.cpp:218] Iteration 109700 (17.5786 iter/s, 5.68874s/100 iters), loss = 0.291961
I1210 18:22:53.678098 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:22:53.678098 22120 solver.cpp:237]     Train net output #1: loss = 0.291961 (* 1 = 0.291961 loss)
I1210 18:22:53.678098 22120 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1210 18:22:59.335487 22120 solver.cpp:218] Iteration 109800 (17.6771 iter/s, 5.65703s/100 iters), loss = 0.376594
I1210 18:22:59.335487 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:22:59.335487 22120 solver.cpp:237]     Train net output #1: loss = 0.376594 (* 1 = 0.376594 loss)
I1210 18:22:59.335487 22120 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1210 18:23:05.000910 22120 solver.cpp:218] Iteration 109900 (17.6521 iter/s, 5.66504s/100 iters), loss = 0.388524
I1210 18:23:05.000910 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:23:05.000910 22120 solver.cpp:237]     Train net output #1: loss = 0.388524 (* 1 = 0.388524 loss)
I1210 18:23:05.000910 22120 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1210 18:23:10.383321 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:23:10.610334 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_110000.caffemodel
I1210 18:23:10.624335 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_110000.solverstate
I1210 18:23:10.629334 22120 solver.cpp:330] Iteration 110000, Testing net (#0)
I1210 18:23:10.629334 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:23:12.010488 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:23:12.064993 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6804
I1210 18:23:12.065493 22120 solver.cpp:397]     Test net output #1: loss = 1.23729 (* 1 = 1.23729 loss)
I1210 18:23:12.119494 22120 solver.cpp:218] Iteration 110000 (14.0497 iter/s, 7.1176s/100 iters), loss = 0.297371
I1210 18:23:12.119494 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:23:12.119494 22120 solver.cpp:237]     Train net output #1: loss = 0.297371 (* 1 = 0.297371 loss)
I1210 18:23:12.119494 22120 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1210 18:23:17.796968 22120 solver.cpp:218] Iteration 110100 (17.6135 iter/s, 5.67747s/100 iters), loss = 0.321734
I1210 18:23:17.796968 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:23:17.796968 22120 solver.cpp:237]     Train net output #1: loss = 0.321733 (* 1 = 0.321733 loss)
I1210 18:23:17.796968 22120 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1210 18:23:23.455510 22120 solver.cpp:218] Iteration 110200 (17.6757 iter/s, 5.65748s/100 iters), loss = 0.260001
I1210 18:23:23.455510 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 18:23:23.455510 22120 solver.cpp:237]     Train net output #1: loss = 0.260001 (* 1 = 0.260001 loss)
I1210 18:23:23.455510 22120 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1210 18:23:29.105952 22120 solver.cpp:218] Iteration 110300 (17.6986 iter/s, 5.65018s/100 iters), loss = 0.432349
I1210 18:23:29.105952 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:23:29.105952 22120 solver.cpp:237]     Train net output #1: loss = 0.432349 (* 1 = 0.432349 loss)
I1210 18:23:29.105952 22120 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1210 18:23:34.787619 22120 solver.cpp:218] Iteration 110400 (17.6016 iter/s, 5.68129s/100 iters), loss = 0.336663
I1210 18:23:34.787619 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:23:34.787619 22120 solver.cpp:237]     Train net output #1: loss = 0.336663 (* 1 = 0.336663 loss)
I1210 18:23:34.787619 22120 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1210 18:23:40.186072 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:23:40.409096 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_110500.caffemodel
I1210 18:23:40.424098 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_110500.solverstate
I1210 18:23:40.429096 22120 solver.cpp:330] Iteration 110500, Testing net (#0)
I1210 18:23:40.429096 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:23:41.801194 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:23:41.855193 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6813
I1210 18:23:41.855193 22120 solver.cpp:397]     Test net output #1: loss = 1.23822 (* 1 = 1.23822 loss)
I1210 18:23:41.910212 22120 solver.cpp:218] Iteration 110500 (14.0406 iter/s, 7.12219s/100 iters), loss = 0.214897
I1210 18:23:41.910212 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 18:23:41.910212 22120 solver.cpp:237]     Train net output #1: loss = 0.214897 (* 1 = 0.214897 loss)
I1210 18:23:41.910212 22120 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1210 18:23:47.592689 22120 solver.cpp:218] Iteration 110600 (17.5998 iter/s, 5.68187s/100 iters), loss = 0.311151
I1210 18:23:47.592689 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:23:47.592689 22120 solver.cpp:237]     Train net output #1: loss = 0.311151 (* 1 = 0.311151 loss)
I1210 18:23:47.592689 22120 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1210 18:23:53.247079 22120 solver.cpp:218] Iteration 110700 (17.6851 iter/s, 5.65447s/100 iters), loss = 0.30288
I1210 18:23:53.247079 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 18:23:53.247079 22120 solver.cpp:237]     Train net output #1: loss = 0.30288 (* 1 = 0.30288 loss)
I1210 18:23:53.247079 22120 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1210 18:23:58.902485 22120 solver.cpp:218] Iteration 110800 (17.684 iter/s, 5.65482s/100 iters), loss = 0.393045
I1210 18:23:58.902485 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:23:58.902485 22120 solver.cpp:237]     Train net output #1: loss = 0.393044 (* 1 = 0.393044 loss)
I1210 18:23:58.902485 22120 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1210 18:24:04.559329 22120 solver.cpp:218] Iteration 110900 (17.6782 iter/s, 5.65668s/100 iters), loss = 0.301105
I1210 18:24:04.560329 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:24:04.560329 22120 solver.cpp:237]     Train net output #1: loss = 0.301105 (* 1 = 0.301105 loss)
I1210 18:24:04.560329 22120 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1210 18:24:09.995755 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:24:10.218387 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_111000.caffemodel
I1210 18:24:10.232390 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_111000.solverstate
I1210 18:24:10.237401 22120 solver.cpp:330] Iteration 111000, Testing net (#0)
I1210 18:24:10.237401 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:24:11.616966 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:24:11.670994 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6778
I1210 18:24:11.670994 22120 solver.cpp:397]     Test net output #1: loss = 1.25167 (* 1 = 1.25167 loss)
I1210 18:24:11.723970 22120 solver.cpp:218] Iteration 111000 (13.9589 iter/s, 7.16391s/100 iters), loss = 0.226825
I1210 18:24:11.723970 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 18:24:11.723970 22120 solver.cpp:237]     Train net output #1: loss = 0.226825 (* 1 = 0.226825 loss)
I1210 18:24:11.723970 22120 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1210 18:24:17.389019 22120 solver.cpp:218] Iteration 111100 (17.6549 iter/s, 5.66415s/100 iters), loss = 0.319849
I1210 18:24:17.389019 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:24:17.389019 22120 solver.cpp:237]     Train net output #1: loss = 0.319849 (* 1 = 0.319849 loss)
I1210 18:24:17.389019 22120 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1210 18:24:23.038251 22120 solver.cpp:218] Iteration 111200 (17.704 iter/s, 5.64845s/100 iters), loss = 0.287942
I1210 18:24:23.038251 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 18:24:23.038251 22120 solver.cpp:237]     Train net output #1: loss = 0.287942 (* 1 = 0.287942 loss)
I1210 18:24:23.038251 22120 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1210 18:24:28.700778 22120 solver.cpp:218] Iteration 111300 (17.6609 iter/s, 5.66223s/100 iters), loss = 0.373786
I1210 18:24:28.700778 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:24:28.700778 22120 solver.cpp:237]     Train net output #1: loss = 0.373786 (* 1 = 0.373786 loss)
I1210 18:24:28.700778 22120 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1210 18:24:34.330432 22120 solver.cpp:218] Iteration 111400 (17.7641 iter/s, 5.62932s/100 iters), loss = 0.322058
I1210 18:24:34.330432 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:24:34.330432 22120 solver.cpp:237]     Train net output #1: loss = 0.322058 (* 1 = 0.322058 loss)
I1210 18:24:34.330432 22120 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1210 18:24:39.713011 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:24:39.934882 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_111500.caffemodel
I1210 18:24:39.949882 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_111500.solverstate
I1210 18:24:39.954900 22120 solver.cpp:330] Iteration 111500, Testing net (#0)
I1210 18:24:39.954900 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:24:41.329169 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:24:41.383205 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6821
I1210 18:24:41.383205 22120 solver.cpp:397]     Test net output #1: loss = 1.24776 (* 1 = 1.24776 loss)
I1210 18:24:41.437232 22120 solver.cpp:218] Iteration 111500 (14.0718 iter/s, 7.10639s/100 iters), loss = 0.218716
I1210 18:24:41.437232 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 18:24:41.437232 22120 solver.cpp:237]     Train net output #1: loss = 0.218716 (* 1 = 0.218716 loss)
I1210 18:24:41.437232 22120 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1210 18:24:47.092514 22120 solver.cpp:218] Iteration 111600 (17.6848 iter/s, 5.65459s/100 iters), loss = 0.446561
I1210 18:24:47.092514 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:24:47.092514 22120 solver.cpp:237]     Train net output #1: loss = 0.446561 (* 1 = 0.446561 loss)
I1210 18:24:47.092514 22120 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1210 18:24:52.783232 22120 solver.cpp:218] Iteration 111700 (17.5718 iter/s, 5.69094s/100 iters), loss = 0.345647
I1210 18:24:52.783232 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:24:52.783232 22120 solver.cpp:237]     Train net output #1: loss = 0.345647 (* 1 = 0.345647 loss)
I1210 18:24:52.783232 22120 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1210 18:24:58.456442 22120 solver.cpp:218] Iteration 111800 (17.63 iter/s, 5.67215s/100 iters), loss = 0.296944
I1210 18:24:58.456442 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:24:58.456442 22120 solver.cpp:237]     Train net output #1: loss = 0.296943 (* 1 = 0.296943 loss)
I1210 18:24:58.456442 22120 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1210 18:25:04.112112 22120 solver.cpp:218] Iteration 111900 (17.681 iter/s, 5.65579s/100 iters), loss = 0.347615
I1210 18:25:04.112112 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:25:04.112112 22120 solver.cpp:237]     Train net output #1: loss = 0.347615 (* 1 = 0.347615 loss)
I1210 18:25:04.112112 22120 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1210 18:25:09.498591 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:25:09.722111 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_112000.caffemodel
I1210 18:25:09.736112 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_112000.solverstate
I1210 18:25:09.740111 22120 solver.cpp:330] Iteration 112000, Testing net (#0)
I1210 18:25:09.741113 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:25:11.113296 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:25:11.167301 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6795
I1210 18:25:11.167301 22120 solver.cpp:397]     Test net output #1: loss = 1.25178 (* 1 = 1.25178 loss)
I1210 18:25:11.220304 22120 solver.cpp:218] Iteration 112000 (14.0693 iter/s, 7.10769s/100 iters), loss = 0.266737
I1210 18:25:11.220304 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 18:25:11.220304 22120 solver.cpp:237]     Train net output #1: loss = 0.266737 (* 1 = 0.266737 loss)
I1210 18:25:11.220304 22120 sgd_solver.cpp:105] Iteration 112000, lr = 0.001
I1210 18:25:16.930924 22120 solver.cpp:218] Iteration 112100 (17.5127 iter/s, 5.71014s/100 iters), loss = 0.326763
I1210 18:25:16.930924 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:25:16.930924 22120 solver.cpp:237]     Train net output #1: loss = 0.326763 (* 1 = 0.326763 loss)
I1210 18:25:16.930924 22120 sgd_solver.cpp:105] Iteration 112100, lr = 0.001
I1210 18:25:22.689512 22120 solver.cpp:218] Iteration 112200 (17.3681 iter/s, 5.75768s/100 iters), loss = 0.276335
I1210 18:25:22.689512 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:25:22.689512 22120 solver.cpp:237]     Train net output #1: loss = 0.276335 (* 1 = 0.276335 loss)
I1210 18:25:22.689512 22120 sgd_solver.cpp:105] Iteration 112200, lr = 0.001
I1210 18:25:28.385918 22120 solver.cpp:218] Iteration 112300 (17.5558 iter/s, 5.69612s/100 iters), loss = 0.367727
I1210 18:25:28.385918 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 18:25:28.385918 22120 solver.cpp:237]     Train net output #1: loss = 0.367727 (* 1 = 0.367727 loss)
I1210 18:25:28.385918 22120 sgd_solver.cpp:105] Iteration 112300, lr = 0.001
I1210 18:25:34.072444 22120 solver.cpp:218] Iteration 112400 (17.5881 iter/s, 5.68567s/100 iters), loss = 0.426778
I1210 18:25:34.072444 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 18:25:34.072444 22120 solver.cpp:237]     Train net output #1: loss = 0.426778 (* 1 = 0.426778 loss)
I1210 18:25:34.072444 22120 sgd_solver.cpp:105] Iteration 112400, lr = 0.001
I1210 18:25:39.532737 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:25:39.765252 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_112500.caffemodel
I1210 18:25:39.780757 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_112500.solverstate
I1210 18:25:39.785755 22120 solver.cpp:330] Iteration 112500, Testing net (#0)
I1210 18:25:39.785755 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:25:41.212904 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:25:41.253902 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6802
I1210 18:25:41.253902 22120 solver.cpp:397]     Test net output #1: loss = 1.26517 (* 1 = 1.26517 loss)
I1210 18:25:41.309913 22120 solver.cpp:218] Iteration 112500 (13.8178 iter/s, 7.23702s/100 iters), loss = 0.259797
I1210 18:25:41.309913 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:25:41.309913 22120 solver.cpp:237]     Train net output #1: loss = 0.259797 (* 1 = 0.259797 loss)
I1210 18:25:41.309913 22120 sgd_solver.cpp:105] Iteration 112500, lr = 0.001
I1210 18:25:47.030588 22120 solver.cpp:218] Iteration 112600 (17.4821 iter/s, 5.72013s/100 iters), loss = 0.403665
I1210 18:25:47.030588 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:25:47.030588 22120 solver.cpp:237]     Train net output #1: loss = 0.403665 (* 1 = 0.403665 loss)
I1210 18:25:47.030588 22120 sgd_solver.cpp:105] Iteration 112600, lr = 0.001
I1210 18:25:52.727198 22120 solver.cpp:218] Iteration 112700 (17.5552 iter/s, 5.69633s/100 iters), loss = 0.283469
I1210 18:25:52.727198 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1210 18:25:52.727198 22120 solver.cpp:237]     Train net output #1: loss = 0.283469 (* 1 = 0.283469 loss)
I1210 18:25:52.727198 22120 sgd_solver.cpp:105] Iteration 112700, lr = 0.001
I1210 18:25:58.445016 22120 solver.cpp:218] Iteration 112800 (17.4895 iter/s, 5.71772s/100 iters), loss = 0.361115
I1210 18:25:58.445016 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:25:58.445016 22120 solver.cpp:237]     Train net output #1: loss = 0.361115 (* 1 = 0.361115 loss)
I1210 18:25:58.446017 22120 sgd_solver.cpp:105] Iteration 112800, lr = 0.001
I1210 18:26:04.094676 22120 solver.cpp:218] Iteration 112900 (17.7015 iter/s, 5.64923s/100 iters), loss = 0.412928
I1210 18:26:04.095674 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:26:04.095674 22120 solver.cpp:237]     Train net output #1: loss = 0.412928 (* 1 = 0.412928 loss)
I1210 18:26:04.095674 22120 sgd_solver.cpp:105] Iteration 112900, lr = 0.001
I1210 18:26:09.478129 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:26:09.701159 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_113000.caffemodel
I1210 18:26:09.715149 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_113000.solverstate
I1210 18:26:09.720149 22120 solver.cpp:330] Iteration 113000, Testing net (#0)
I1210 18:26:09.720149 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:26:11.092308 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:26:11.146307 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6822
I1210 18:26:11.146307 22120 solver.cpp:397]     Test net output #1: loss = 1.25762 (* 1 = 1.25762 loss)
I1210 18:26:11.200330 22120 solver.cpp:218] Iteration 113000 (14.0745 iter/s, 7.10507s/100 iters), loss = 0.18621
I1210 18:26:11.200330 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1210 18:26:11.200330 22120 solver.cpp:237]     Train net output #1: loss = 0.18621 (* 1 = 0.18621 loss)
I1210 18:26:11.200330 22120 sgd_solver.cpp:105] Iteration 113000, lr = 0.001
I1210 18:26:16.886432 22120 solver.cpp:218] Iteration 113100 (17.59 iter/s, 5.68505s/100 iters), loss = 0.393347
I1210 18:26:16.886432 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:26:16.886432 22120 solver.cpp:237]     Train net output #1: loss = 0.393347 (* 1 = 0.393347 loss)
I1210 18:26:16.886432 22120 sgd_solver.cpp:105] Iteration 113100, lr = 0.001
I1210 18:26:22.574822 22120 solver.cpp:218] Iteration 113200 (17.5792 iter/s, 5.68854s/100 iters), loss = 0.269766
I1210 18:26:22.574822 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:26:22.574822 22120 solver.cpp:237]     Train net output #1: loss = 0.269766 (* 1 = 0.269766 loss)
I1210 18:26:22.574822 22120 sgd_solver.cpp:105] Iteration 113200, lr = 0.001
I1210 18:26:28.320483 22120 solver.cpp:218] Iteration 113300 (17.4075 iter/s, 5.74466s/100 iters), loss = 0.346853
I1210 18:26:28.320483 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:26:28.320483 22120 solver.cpp:237]     Train net output #1: loss = 0.346853 (* 1 = 0.346853 loss)
I1210 18:26:28.320483 22120 sgd_solver.cpp:105] Iteration 113300, lr = 0.001
I1210 18:26:34.055927 22120 solver.cpp:218] Iteration 113400 (17.437 iter/s, 5.73492s/100 iters), loss = 0.364645
I1210 18:26:34.055927 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:26:34.055927 22120 solver.cpp:237]     Train net output #1: loss = 0.364645 (* 1 = 0.364645 loss)
I1210 18:26:34.055927 22120 sgd_solver.cpp:105] Iteration 113400, lr = 0.001
I1210 18:26:39.576046 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:26:39.800063 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_113500.caffemodel
I1210 18:26:39.816064 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_113500.solverstate
I1210 18:26:39.821065 22120 solver.cpp:330] Iteration 113500, Testing net (#0)
I1210 18:26:39.821065 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:26:41.218221 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:26:41.272243 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6767
I1210 18:26:41.272243 22120 solver.cpp:397]     Test net output #1: loss = 1.26832 (* 1 = 1.26832 loss)
I1210 18:26:41.325242 22120 solver.cpp:218] Iteration 113500 (13.7564 iter/s, 7.26935s/100 iters), loss = 0.1664
I1210 18:26:41.326242 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1210 18:26:41.326242 22120 solver.cpp:237]     Train net output #1: loss = 0.1664 (* 1 = 0.1664 loss)
I1210 18:26:41.326242 22120 sgd_solver.cpp:105] Iteration 113500, lr = 0.001
I1210 18:26:46.985709 22120 solver.cpp:218] Iteration 113600 (17.6692 iter/s, 5.65958s/100 iters), loss = 0.33257
I1210 18:26:46.985709 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:26:46.985709 22120 solver.cpp:237]     Train net output #1: loss = 0.33257 (* 1 = 0.33257 loss)
I1210 18:26:46.985709 22120 sgd_solver.cpp:105] Iteration 113600, lr = 0.001
I1210 18:26:52.680613 22120 solver.cpp:218] Iteration 113700 (17.5621 iter/s, 5.69407s/100 iters), loss = 0.319135
I1210 18:26:52.680613 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:26:52.680613 22120 solver.cpp:237]     Train net output #1: loss = 0.319135 (* 1 = 0.319135 loss)
I1210 18:26:52.680613 22120 sgd_solver.cpp:105] Iteration 113700, lr = 0.001
I1210 18:26:58.322129 22120 solver.cpp:218] Iteration 113800 (17.7283 iter/s, 5.64069s/100 iters), loss = 0.373608
I1210 18:26:58.322129 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:26:58.322129 22120 solver.cpp:237]     Train net output #1: loss = 0.373608 (* 1 = 0.373608 loss)
I1210 18:26:58.322129 22120 sgd_solver.cpp:105] Iteration 113800, lr = 0.001
I1210 18:27:03.961239 22120 solver.cpp:218] Iteration 113900 (17.734 iter/s, 5.63889s/100 iters), loss = 0.376089
I1210 18:27:03.961239 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 18:27:03.961239 22120 solver.cpp:237]     Train net output #1: loss = 0.376089 (* 1 = 0.376089 loss)
I1210 18:27:03.961239 22120 sgd_solver.cpp:105] Iteration 113900, lr = 0.001
I1210 18:27:09.338153 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:27:09.561173 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_114000.caffemodel
I1210 18:27:09.576182 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_114000.solverstate
I1210 18:27:09.580683 22120 solver.cpp:330] Iteration 114000, Testing net (#0)
I1210 18:27:09.580683 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:27:10.969521 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:27:11.023530 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6825
I1210 18:27:11.023530 22120 solver.cpp:397]     Test net output #1: loss = 1.25005 (* 1 = 1.25005 loss)
I1210 18:27:11.080047 22120 solver.cpp:218] Iteration 114000 (14.0485 iter/s, 7.11818s/100 iters), loss = 0.23275
I1210 18:27:11.080047 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 18:27:11.080047 22120 solver.cpp:237]     Train net output #1: loss = 0.23275 (* 1 = 0.23275 loss)
I1210 18:27:11.080047 22120 sgd_solver.cpp:105] Iteration 114000, lr = 0.001
I1210 18:27:16.804997 22120 solver.cpp:218] Iteration 114100 (17.4692 iter/s, 5.72437s/100 iters), loss = 0.290413
I1210 18:27:16.804997 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:27:16.804997 22120 solver.cpp:237]     Train net output #1: loss = 0.290413 (* 1 = 0.290413 loss)
I1210 18:27:16.804997 22120 sgd_solver.cpp:105] Iteration 114100, lr = 0.001
I1210 18:27:22.515476 22120 solver.cpp:218] Iteration 114200 (17.5108 iter/s, 5.71077s/100 iters), loss = 0.261064
I1210 18:27:22.515476 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:27:22.515476 22120 solver.cpp:237]     Train net output #1: loss = 0.261064 (* 1 = 0.261064 loss)
I1210 18:27:22.515476 22120 sgd_solver.cpp:105] Iteration 114200, lr = 0.001
I1210 18:27:28.205886 22120 solver.cpp:218] Iteration 114300 (17.5767 iter/s, 5.68934s/100 iters), loss = 0.381249
I1210 18:27:28.205886 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:27:28.205886 22120 solver.cpp:237]     Train net output #1: loss = 0.381249 (* 1 = 0.381249 loss)
I1210 18:27:28.205886 22120 sgd_solver.cpp:105] Iteration 114300, lr = 0.001
I1210 18:27:33.849366 22120 solver.cpp:218] Iteration 114400 (17.7187 iter/s, 5.64375s/100 iters), loss = 0.331998
I1210 18:27:33.849366 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:27:33.849366 22120 solver.cpp:237]     Train net output #1: loss = 0.331998 (* 1 = 0.331998 loss)
I1210 18:27:33.849366 22120 sgd_solver.cpp:105] Iteration 114400, lr = 0.001
I1210 18:27:39.232810 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:27:39.454834 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_114500.caffemodel
I1210 18:27:39.470837 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_114500.solverstate
I1210 18:27:39.475338 22120 solver.cpp:330] Iteration 114500, Testing net (#0)
I1210 18:27:39.475838 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:27:40.851948 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:27:40.904952 22120 solver.cpp:397]     Test net output #0: accuracy = 0.674
I1210 18:27:40.904952 22120 solver.cpp:397]     Test net output #1: loss = 1.27078 (* 1 = 1.27078 loss)
I1210 18:27:40.958951 22120 solver.cpp:218] Iteration 114500 (14.0683 iter/s, 7.1082s/100 iters), loss = 0.201314
I1210 18:27:40.958951 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 18:27:40.958951 22120 solver.cpp:237]     Train net output #1: loss = 0.201314 (* 1 = 0.201314 loss)
I1210 18:27:40.958951 22120 sgd_solver.cpp:105] Iteration 114500, lr = 0.001
I1210 18:27:46.596415 22120 solver.cpp:218] Iteration 114600 (17.7391 iter/s, 5.63726s/100 iters), loss = 0.429243
I1210 18:27:46.596415 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:27:46.596415 22120 solver.cpp:237]     Train net output #1: loss = 0.429243 (* 1 = 0.429243 loss)
I1210 18:27:46.596415 22120 sgd_solver.cpp:105] Iteration 114600, lr = 0.001
I1210 18:27:52.238888 22120 solver.cpp:218] Iteration 114700 (17.724 iter/s, 5.64206s/100 iters), loss = 0.270711
I1210 18:27:52.238888 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:27:52.238888 22120 solver.cpp:237]     Train net output #1: loss = 0.270711 (* 1 = 0.270711 loss)
I1210 18:27:52.238888 22120 sgd_solver.cpp:105] Iteration 114700, lr = 0.001
I1210 18:27:57.894392 22120 solver.cpp:218] Iteration 114800 (17.6842 iter/s, 5.65477s/100 iters), loss = 0.321241
I1210 18:27:57.894392 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:27:57.894392 22120 solver.cpp:237]     Train net output #1: loss = 0.321241 (* 1 = 0.321241 loss)
I1210 18:27:57.894392 22120 sgd_solver.cpp:105] Iteration 114800, lr = 0.001
I1210 18:28:03.543817 22120 solver.cpp:218] Iteration 114900 (17.7027 iter/s, 5.64884s/100 iters), loss = 0.315037
I1210 18:28:03.543817 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:28:03.543817 22120 solver.cpp:237]     Train net output #1: loss = 0.315037 (* 1 = 0.315037 loss)
I1210 18:28:03.543817 22120 sgd_solver.cpp:105] Iteration 114900, lr = 0.001
I1210 18:28:08.918233 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:28:09.143266 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_115000.caffemodel
I1210 18:28:09.157266 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_115000.solverstate
I1210 18:28:09.162266 22120 solver.cpp:330] Iteration 115000, Testing net (#0)
I1210 18:28:09.162266 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:28:10.533430 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:28:10.588438 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6824
I1210 18:28:10.588438 22120 solver.cpp:397]     Test net output #1: loss = 1.25869 (* 1 = 1.25869 loss)
I1210 18:28:10.643438 22120 solver.cpp:218] Iteration 115000 (14.085 iter/s, 7.09975s/100 iters), loss = 0.238683
I1210 18:28:10.643438 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:28:10.644439 22120 solver.cpp:237]     Train net output #1: loss = 0.238683 (* 1 = 0.238683 loss)
I1210 18:28:10.644439 22120 sgd_solver.cpp:105] Iteration 115000, lr = 0.001
I1210 18:28:16.357195 22120 solver.cpp:218] Iteration 115100 (17.5046 iter/s, 5.7128s/100 iters), loss = 0.367321
I1210 18:28:16.357195 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 18:28:16.357195 22120 solver.cpp:237]     Train net output #1: loss = 0.367321 (* 1 = 0.367321 loss)
I1210 18:28:16.357195 22120 sgd_solver.cpp:105] Iteration 115100, lr = 0.001
I1210 18:28:22.019636 22120 solver.cpp:218] Iteration 115200 (17.6626 iter/s, 5.66169s/100 iters), loss = 0.242051
I1210 18:28:22.019636 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 18:28:22.019636 22120 solver.cpp:237]     Train net output #1: loss = 0.242051 (* 1 = 0.242051 loss)
I1210 18:28:22.019636 22120 sgd_solver.cpp:105] Iteration 115200, lr = 0.001
I1210 18:28:27.732110 22120 solver.cpp:218] Iteration 115300 (17.5078 iter/s, 5.71173s/100 iters), loss = 0.345305
I1210 18:28:27.732110 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:28:27.732110 22120 solver.cpp:237]     Train net output #1: loss = 0.345305 (* 1 = 0.345305 loss)
I1210 18:28:27.732110 22120 sgd_solver.cpp:105] Iteration 115300, lr = 0.001
I1210 18:28:33.543659 22120 solver.cpp:218] Iteration 115400 (17.2068 iter/s, 5.81165s/100 iters), loss = 0.321538
I1210 18:28:33.543659 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:28:33.543659 22120 solver.cpp:237]     Train net output #1: loss = 0.321538 (* 1 = 0.321538 loss)
I1210 18:28:33.543659 22120 sgd_solver.cpp:105] Iteration 115400, lr = 0.001
I1210 18:28:38.909080 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:28:39.132091 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_115500.caffemodel
I1210 18:28:39.146092 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_115500.solverstate
I1210 18:28:39.151091 22120 solver.cpp:330] Iteration 115500, Testing net (#0)
I1210 18:28:39.151091 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:28:40.527251 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:28:40.581264 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6789
I1210 18:28:40.581264 22120 solver.cpp:397]     Test net output #1: loss = 1.26731 (* 1 = 1.26731 loss)
I1210 18:28:40.633280 22120 solver.cpp:218] Iteration 115500 (14.1058 iter/s, 7.08929s/100 iters), loss = 0.283378
I1210 18:28:40.634264 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:28:40.634264 22120 solver.cpp:237]     Train net output #1: loss = 0.283378 (* 1 = 0.283378 loss)
I1210 18:28:40.634264 22120 sgd_solver.cpp:105] Iteration 115500, lr = 0.001
I1210 18:28:46.313730 22120 solver.cpp:218] Iteration 115600 (17.6084 iter/s, 5.67912s/100 iters), loss = 0.379839
I1210 18:28:46.313730 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:28:46.313730 22120 solver.cpp:237]     Train net output #1: loss = 0.379839 (* 1 = 0.379839 loss)
I1210 18:28:46.313730 22120 sgd_solver.cpp:105] Iteration 115600, lr = 0.001
I1210 18:28:51.997227 22120 solver.cpp:218] Iteration 115700 (17.5964 iter/s, 5.68297s/100 iters), loss = 0.265675
I1210 18:28:51.997227 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:28:51.997227 22120 solver.cpp:237]     Train net output #1: loss = 0.265675 (* 1 = 0.265675 loss)
I1210 18:28:51.997227 22120 sgd_solver.cpp:105] Iteration 115700, lr = 0.001
I1210 18:28:57.749781 22120 solver.cpp:218] Iteration 115800 (17.3828 iter/s, 5.7528s/100 iters), loss = 0.290861
I1210 18:28:57.749781 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:28:57.749781 22120 solver.cpp:237]     Train net output #1: loss = 0.290861 (* 1 = 0.290861 loss)
I1210 18:28:57.749781 22120 sgd_solver.cpp:105] Iteration 115800, lr = 0.001
I1210 18:29:03.403215 22120 solver.cpp:218] Iteration 115900 (17.6908 iter/s, 5.65266s/100 iters), loss = 0.364749
I1210 18:29:03.403215 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 18:29:03.403215 22120 solver.cpp:237]     Train net output #1: loss = 0.364749 (* 1 = 0.364749 loss)
I1210 18:29:03.403215 22120 sgd_solver.cpp:105] Iteration 115900, lr = 0.001
I1210 18:29:08.781591 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:29:09.002612 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_116000.caffemodel
I1210 18:29:09.017612 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_116000.solverstate
I1210 18:29:09.025614 22120 solver.cpp:330] Iteration 116000, Testing net (#0)
I1210 18:29:09.025614 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:29:10.419744 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:29:10.473748 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6758
I1210 18:29:10.473748 22120 solver.cpp:397]     Test net output #1: loss = 1.26835 (* 1 = 1.26835 loss)
I1210 18:29:10.527748 22120 solver.cpp:218] Iteration 116000 (14.0371 iter/s, 7.12399s/100 iters), loss = 0.265082
I1210 18:29:10.527748 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 18:29:10.527748 22120 solver.cpp:237]     Train net output #1: loss = 0.265082 (* 1 = 0.265082 loss)
I1210 18:29:10.527748 22120 sgd_solver.cpp:105] Iteration 116000, lr = 0.001
I1210 18:29:16.213207 22120 solver.cpp:218] Iteration 116100 (17.5917 iter/s, 5.6845s/100 iters), loss = 0.360732
I1210 18:29:16.213207 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:29:16.213207 22120 solver.cpp:237]     Train net output #1: loss = 0.360732 (* 1 = 0.360732 loss)
I1210 18:29:16.213207 22120 sgd_solver.cpp:105] Iteration 116100, lr = 0.001
I1210 18:29:22.063064 22120 solver.cpp:218] Iteration 116200 (17.0963 iter/s, 5.84922s/100 iters), loss = 0.32196
I1210 18:29:22.063064 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:29:22.063064 22120 solver.cpp:237]     Train net output #1: loss = 0.321959 (* 1 = 0.321959 loss)
I1210 18:29:22.063064 22120 sgd_solver.cpp:105] Iteration 116200, lr = 0.001
I1210 18:29:27.926100 22120 solver.cpp:218] Iteration 116300 (17.0571 iter/s, 5.86267s/100 iters), loss = 0.335245
I1210 18:29:27.926100 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:29:27.926100 22120 solver.cpp:237]     Train net output #1: loss = 0.335245 (* 1 = 0.335245 loss)
I1210 18:29:27.926100 22120 sgd_solver.cpp:105] Iteration 116300, lr = 0.001
I1210 18:29:33.671385 22120 solver.cpp:218] Iteration 116400 (17.4066 iter/s, 5.74493s/100 iters), loss = 0.306323
I1210 18:29:33.671385 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:29:33.671385 22120 solver.cpp:237]     Train net output #1: loss = 0.306323 (* 1 = 0.306323 loss)
I1210 18:29:33.671385 22120 sgd_solver.cpp:105] Iteration 116400, lr = 0.001
I1210 18:29:39.122300 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:29:39.344391 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_116500.caffemodel
I1210 18:29:39.358392 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_116500.solverstate
I1210 18:29:39.362392 22120 solver.cpp:330] Iteration 116500, Testing net (#0)
I1210 18:29:39.362392 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:29:40.745050 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:29:40.799088 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6756
I1210 18:29:40.799088 22120 solver.cpp:397]     Test net output #1: loss = 1.2787 (* 1 = 1.2787 loss)
I1210 18:29:40.853086 22120 solver.cpp:218] Iteration 116500 (13.925 iter/s, 7.18135s/100 iters), loss = 0.225842
I1210 18:29:40.853086 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 18:29:40.853086 22120 solver.cpp:237]     Train net output #1: loss = 0.225842 (* 1 = 0.225842 loss)
I1210 18:29:40.853086 22120 sgd_solver.cpp:105] Iteration 116500, lr = 0.001
I1210 18:29:46.519273 22120 solver.cpp:218] Iteration 116600 (17.6511 iter/s, 5.66537s/100 iters), loss = 0.298491
I1210 18:29:46.519273 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:29:46.519273 22120 solver.cpp:237]     Train net output #1: loss = 0.298491 (* 1 = 0.298491 loss)
I1210 18:29:46.519273 22120 sgd_solver.cpp:105] Iteration 116600, lr = 0.001
I1210 18:29:52.158705 22120 solver.cpp:218] Iteration 116700 (17.7332 iter/s, 5.63916s/100 iters), loss = 0.253141
I1210 18:29:52.158705 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:29:52.158705 22120 solver.cpp:237]     Train net output #1: loss = 0.253141 (* 1 = 0.253141 loss)
I1210 18:29:52.158705 22120 sgd_solver.cpp:105] Iteration 116700, lr = 0.001
I1210 18:29:57.790449 22120 solver.cpp:218] Iteration 116800 (17.7564 iter/s, 5.63177s/100 iters), loss = 0.338657
I1210 18:29:57.790449 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:29:57.790449 22120 solver.cpp:237]     Train net output #1: loss = 0.338657 (* 1 = 0.338657 loss)
I1210 18:29:57.790449 22120 sgd_solver.cpp:105] Iteration 116800, lr = 0.001
I1210 18:30:03.445171 22120 solver.cpp:218] Iteration 116900 (17.6864 iter/s, 5.65406s/100 iters), loss = 0.320576
I1210 18:30:03.445171 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:30:03.445171 22120 solver.cpp:237]     Train net output #1: loss = 0.320576 (* 1 = 0.320576 loss)
I1210 18:30:03.445171 22120 sgd_solver.cpp:105] Iteration 116900, lr = 0.001
I1210 18:30:08.803711 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:30:09.026738 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_117000.caffemodel
I1210 18:30:09.041738 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_117000.solverstate
I1210 18:30:09.045738 22120 solver.cpp:330] Iteration 117000, Testing net (#0)
I1210 18:30:09.045738 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:30:10.411864 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:30:10.464864 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6753
I1210 18:30:10.464864 22120 solver.cpp:397]     Test net output #1: loss = 1.27363 (* 1 = 1.27363 loss)
I1210 18:30:10.519867 22120 solver.cpp:218] Iteration 117000 (14.1367 iter/s, 7.07376s/100 iters), loss = 0.283563
I1210 18:30:10.519867 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:30:10.519867 22120 solver.cpp:237]     Train net output #1: loss = 0.283563 (* 1 = 0.283563 loss)
I1210 18:30:10.519867 22120 sgd_solver.cpp:105] Iteration 117000, lr = 0.001
I1210 18:30:16.171277 22120 solver.cpp:218] Iteration 117100 (17.6938 iter/s, 5.65169s/100 iters), loss = 0.32616
I1210 18:30:16.171277 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:30:16.171277 22120 solver.cpp:237]     Train net output #1: loss = 0.32616 (* 1 = 0.32616 loss)
I1210 18:30:16.171277 22120 sgd_solver.cpp:105] Iteration 117100, lr = 0.001
I1210 18:30:21.868728 22120 solver.cpp:218] Iteration 117200 (17.5524 iter/s, 5.69723s/100 iters), loss = 0.260531
I1210 18:30:21.869729 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:30:21.869729 22120 solver.cpp:237]     Train net output #1: loss = 0.260531 (* 1 = 0.260531 loss)
I1210 18:30:21.869729 22120 sgd_solver.cpp:105] Iteration 117200, lr = 0.001
I1210 18:30:27.520136 22120 solver.cpp:218] Iteration 117300 (17.6973 iter/s, 5.65058s/100 iters), loss = 0.366931
I1210 18:30:27.520136 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:30:27.520136 22120 solver.cpp:237]     Train net output #1: loss = 0.366931 (* 1 = 0.366931 loss)
I1210 18:30:27.520136 22120 sgd_solver.cpp:105] Iteration 117300, lr = 0.001
I1210 18:30:33.161813 22120 solver.cpp:218] Iteration 117400 (17.7263 iter/s, 5.64134s/100 iters), loss = 0.344694
I1210 18:30:33.161813 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:30:33.161813 22120 solver.cpp:237]     Train net output #1: loss = 0.344694 (* 1 = 0.344694 loss)
I1210 18:30:33.161813 22120 sgd_solver.cpp:105] Iteration 117400, lr = 0.001
I1210 18:30:38.523202 19904 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:30:38.745218 22120 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_117500.caffemodel
I1210 18:30:38.760217 22120 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_117500.solverstate
I1210 18:30:38.764217 22120 solver.cpp:330] Iteration 117500, Testing net (#0)
I1210 18:30:38.764217 22120 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:30:40.136534 20188 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:30:40.190538 22120 solver.cpp:397]     Test net output #0: accuracy = 0.6754
I1210 18:30:40.190538 22120 solver.cpp:397]     Test net output #1: loss = 1.27107 (* 1 = 1.27107 loss)
I1210 18:30:40.243546 22120 solver.cpp:218] Iteration 117500 (14.1229 iter/s, 7.08071s/100 iters), loss = 0.189855
I1210 18:30:40.243546 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 18:30:40.243546 22120 solver.cpp:237]     Train net output #1: loss = 0.189855 (* 1 = 0.189855 loss)
I1210 18:30:40.243546 22120 sgd_solver.cpp:105] Iteration 117500, lr = 0.001
I1210 18:30:45.881449 22120 solver.cpp:218] Iteration 117600 (17.7377 iter/s, 5.63772s/100 iters), loss = 0.337595
I1210 18:30:45.881950 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 18:30:45.881950 22120 solver.cpp:237]     Train net output #1: loss = 0.337595 (* 1 = 0.337595 loss)
I1210 18:30:45.881950 22120 sgd_solver.cpp:105] Iteration 117600, lr = 0.001
I1210 18:30:51.535406 22120 solver.cpp:218] Iteration 117700 (17.6891 iter/s, 5.65321s/100 iters), loss = 0.224222
I1210 18:30:51.535406 22120 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 18:30:51.535406 22120 solver.cpp:237]     Train net output #1: loss = 0.224222 (* 1 = 0.224222 loss)
I1210 18:30:51.535406 22120 sgd_solver.cpp:105] Iteration 117700, lr = 0.001
I1210 18:30:57.485075 22120 solver.cpp:218] Iteration 117800 (16.8
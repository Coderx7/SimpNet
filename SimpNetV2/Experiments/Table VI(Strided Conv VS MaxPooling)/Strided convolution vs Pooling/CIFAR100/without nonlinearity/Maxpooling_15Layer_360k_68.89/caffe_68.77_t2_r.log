
G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90000.solverstate 
I1210 17:36:01.569939 22232 caffe.cpp:219] Using GPUs 0
I1210 17:36:01.744001 22232 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1210 17:36:02.051587 22232 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 17:36:02.070586 22232 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1210 17:36:02.070586 22232 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 17:36:02.071588 22232 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 17:36:02.071588 22232 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1210 17:36:02.071588 22232 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1210 17:36:02.072589 22232 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_v2_360k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1210 17:36:02.077589 22232 layer_factory.cpp:58] Creating layer cifar
I1210 17:36:02.083590 22232 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1210 17:36:02.083590 22232 net.cpp:84] Creating Layer cifar
I1210 17:36:02.083590 22232 net.cpp:380] cifar -> data
I1210 17:36:02.083590 22232 net.cpp:380] cifar -> label
I1210 17:36:02.084599 22232 data_layer.cpp:45] output data size: 100,3,32,32
I1210 17:36:02.091586 22232 net.cpp:122] Setting up cifar
I1210 17:36:02.091586 22232 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1210 17:36:02.091586 22232 net.cpp:129] Top shape: 100 (100)
I1210 17:36:02.091586 22232 net.cpp:137] Memory required for data: 1229200
I1210 17:36:02.091586 22232 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1210 17:36:02.091586 22232 net.cpp:84] Creating Layer label_cifar_1_split
I1210 17:36:02.091586 22232 net.cpp:406] label_cifar_1_split <- label
I1210 17:36:02.091586 22232 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1210 17:36:02.091586 22232 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1210 17:36:02.091586 22232 net.cpp:122] Setting up label_cifar_1_split
I1210 17:36:02.091586 22232 net.cpp:129] Top shape: 100 (100)
I1210 17:36:02.091586 22232 net.cpp:129] Top shape: 100 (100)
I1210 17:36:02.091586 22232 net.cpp:137] Memory required for data: 1230000
I1210 17:36:02.091586 22232 layer_factory.cpp:58] Creating layer conv1
I1210 17:36:02.091586 22232 net.cpp:84] Creating Layer conv1
I1210 17:36:02.091586 22232 net.cpp:406] conv1 <- data
I1210 17:36:02.091586 22232 net.cpp:380] conv1 -> conv1
I1210 17:36:02.095587 21404 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 17:36:02.336714 22232 net.cpp:122] Setting up conv1
I1210 17:36:02.336714 22232 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 17:36:02.336714 22232 net.cpp:137] Memory required for data: 13518000
I1210 17:36:02.336714 22232 layer_factory.cpp:58] Creating layer bn1
I1210 17:36:02.336714 22232 net.cpp:84] Creating Layer bn1
I1210 17:36:02.336714 22232 net.cpp:406] bn1 <- conv1
I1210 17:36:02.336714 22232 net.cpp:367] bn1 -> conv1 (in-place)
I1210 17:36:02.337716 22232 net.cpp:122] Setting up bn1
I1210 17:36:02.337716 22232 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 17:36:02.337716 22232 net.cpp:137] Memory required for data: 25806000
I1210 17:36:02.337716 22232 layer_factory.cpp:58] Creating layer scale1
I1210 17:36:02.337716 22232 net.cpp:84] Creating Layer scale1
I1210 17:36:02.337716 22232 net.cpp:406] scale1 <- conv1
I1210 17:36:02.337716 22232 net.cpp:367] scale1 -> conv1 (in-place)
I1210 17:36:02.337716 22232 layer_factory.cpp:58] Creating layer scale1
I1210 17:36:02.337716 22232 net.cpp:122] Setting up scale1
I1210 17:36:02.337716 22232 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 17:36:02.337716 22232 net.cpp:137] Memory required for data: 38094000
I1210 17:36:02.337716 22232 layer_factory.cpp:58] Creating layer relu1
I1210 17:36:02.337716 22232 net.cpp:84] Creating Layer relu1
I1210 17:36:02.337716 22232 net.cpp:406] relu1 <- conv1
I1210 17:36:02.337716 22232 net.cpp:367] relu1 -> conv1 (in-place)
I1210 17:36:02.337716 22232 net.cpp:122] Setting up relu1
I1210 17:36:02.337716 22232 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 17:36:02.337716 22232 net.cpp:137] Memory required for data: 50382000
I1210 17:36:02.337716 22232 layer_factory.cpp:58] Creating layer conv1_0
I1210 17:36:02.337716 22232 net.cpp:84] Creating Layer conv1_0
I1210 17:36:02.337716 22232 net.cpp:406] conv1_0 <- conv1
I1210 17:36:02.337716 22232 net.cpp:380] conv1_0 -> conv1_0
I1210 17:36:02.339715 22232 net.cpp:122] Setting up conv1_0
I1210 17:36:02.339715 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.339715 22232 net.cpp:137] Memory required for data: 66766000
I1210 17:36:02.339715 22232 layer_factory.cpp:58] Creating layer bn1_0
I1210 17:36:02.339715 22232 net.cpp:84] Creating Layer bn1_0
I1210 17:36:02.339715 22232 net.cpp:406] bn1_0 <- conv1_0
I1210 17:36:02.339715 22232 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1210 17:36:02.339715 22232 net.cpp:122] Setting up bn1_0
I1210 17:36:02.339715 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.339715 22232 net.cpp:137] Memory required for data: 83150000
I1210 17:36:02.339715 22232 layer_factory.cpp:58] Creating layer scale1_0
I1210 17:36:02.339715 22232 net.cpp:84] Creating Layer scale1_0
I1210 17:36:02.339715 22232 net.cpp:406] scale1_0 <- conv1_0
I1210 17:36:02.339715 22232 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1210 17:36:02.339715 22232 layer_factory.cpp:58] Creating layer scale1_0
I1210 17:36:02.339715 22232 net.cpp:122] Setting up scale1_0
I1210 17:36:02.339715 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.339715 22232 net.cpp:137] Memory required for data: 99534000
I1210 17:36:02.339715 22232 layer_factory.cpp:58] Creating layer relu1_0
I1210 17:36:02.339715 22232 net.cpp:84] Creating Layer relu1_0
I1210 17:36:02.339715 22232 net.cpp:406] relu1_0 <- conv1_0
I1210 17:36:02.339715 22232 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1210 17:36:02.340713 22232 net.cpp:122] Setting up relu1_0
I1210 17:36:02.340713 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.340713 22232 net.cpp:137] Memory required for data: 115918000
I1210 17:36:02.340713 22232 layer_factory.cpp:58] Creating layer conv2
I1210 17:36:02.340713 22232 net.cpp:84] Creating Layer conv2
I1210 17:36:02.340713 22232 net.cpp:406] conv2 <- conv1_0
I1210 17:36:02.340713 22232 net.cpp:380] conv2 -> conv2
I1210 17:36:02.341713 22232 net.cpp:122] Setting up conv2
I1210 17:36:02.341713 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.341713 22232 net.cpp:137] Memory required for data: 132302000
I1210 17:36:02.341713 22232 layer_factory.cpp:58] Creating layer bn2
I1210 17:36:02.341713 22232 net.cpp:84] Creating Layer bn2
I1210 17:36:02.341713 22232 net.cpp:406] bn2 <- conv2
I1210 17:36:02.341713 22232 net.cpp:367] bn2 -> conv2 (in-place)
I1210 17:36:02.341713 22232 net.cpp:122] Setting up bn2
I1210 17:36:02.341713 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.341713 22232 net.cpp:137] Memory required for data: 148686000
I1210 17:36:02.341713 22232 layer_factory.cpp:58] Creating layer scale2
I1210 17:36:02.341713 22232 net.cpp:84] Creating Layer scale2
I1210 17:36:02.341713 22232 net.cpp:406] scale2 <- conv2
I1210 17:36:02.341713 22232 net.cpp:367] scale2 -> conv2 (in-place)
I1210 17:36:02.341713 22232 layer_factory.cpp:58] Creating layer scale2
I1210 17:36:02.341713 22232 net.cpp:122] Setting up scale2
I1210 17:36:02.341713 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.341713 22232 net.cpp:137] Memory required for data: 165070000
I1210 17:36:02.341713 22232 layer_factory.cpp:58] Creating layer relu2
I1210 17:36:02.341713 22232 net.cpp:84] Creating Layer relu2
I1210 17:36:02.341713 22232 net.cpp:406] relu2 <- conv2
I1210 17:36:02.341713 22232 net.cpp:367] relu2 -> conv2 (in-place)
I1210 17:36:02.341713 22232 net.cpp:122] Setting up relu2
I1210 17:36:02.341713 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.341713 22232 net.cpp:137] Memory required for data: 181454000
I1210 17:36:02.341713 22232 layer_factory.cpp:58] Creating layer conv2_1
I1210 17:36:02.341713 22232 net.cpp:84] Creating Layer conv2_1
I1210 17:36:02.341713 22232 net.cpp:406] conv2_1 <- conv2
I1210 17:36:02.341713 22232 net.cpp:380] conv2_1 -> conv2_1
I1210 17:36:02.342713 22232 net.cpp:122] Setting up conv2_1
I1210 17:36:02.342713 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.342713 22232 net.cpp:137] Memory required for data: 197838000
I1210 17:36:02.342713 22232 layer_factory.cpp:58] Creating layer bn2_1
I1210 17:36:02.342713 22232 net.cpp:84] Creating Layer bn2_1
I1210 17:36:02.342713 22232 net.cpp:406] bn2_1 <- conv2_1
I1210 17:36:02.342713 22232 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1210 17:36:02.342713 22232 net.cpp:122] Setting up bn2_1
I1210 17:36:02.343713 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.343713 22232 net.cpp:137] Memory required for data: 214222000
I1210 17:36:02.343713 22232 layer_factory.cpp:58] Creating layer scale2_1
I1210 17:36:02.343713 22232 net.cpp:84] Creating Layer scale2_1
I1210 17:36:02.343713 22232 net.cpp:406] scale2_1 <- conv2_1
I1210 17:36:02.343713 22232 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1210 17:36:02.343713 22232 layer_factory.cpp:58] Creating layer scale2_1
I1210 17:36:02.343713 22232 net.cpp:122] Setting up scale2_1
I1210 17:36:02.343713 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.343713 22232 net.cpp:137] Memory required for data: 230606000
I1210 17:36:02.343713 22232 layer_factory.cpp:58] Creating layer relu2_1
I1210 17:36:02.343713 22232 net.cpp:84] Creating Layer relu2_1
I1210 17:36:02.343713 22232 net.cpp:406] relu2_1 <- conv2_1
I1210 17:36:02.343713 22232 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1210 17:36:02.343713 22232 net.cpp:122] Setting up relu2_1
I1210 17:36:02.343713 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.343713 22232 net.cpp:137] Memory required for data: 246990000
I1210 17:36:02.343713 22232 layer_factory.cpp:58] Creating layer conv2_2
I1210 17:36:02.343713 22232 net.cpp:84] Creating Layer conv2_2
I1210 17:36:02.343713 22232 net.cpp:406] conv2_2 <- conv2_1
I1210 17:36:02.343713 22232 net.cpp:380] conv2_2 -> conv2_2
I1210 17:36:02.345713 22232 net.cpp:122] Setting up conv2_2
I1210 17:36:02.345713 22232 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 17:36:02.345713 22232 net.cpp:137] Memory required for data: 267470000
I1210 17:36:02.345713 22232 layer_factory.cpp:58] Creating layer bn2_2
I1210 17:36:02.345713 22232 net.cpp:84] Creating Layer bn2_2
I1210 17:36:02.345713 22232 net.cpp:406] bn2_2 <- conv2_2
I1210 17:36:02.345713 22232 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1210 17:36:02.345713 22232 net.cpp:122] Setting up bn2_2
I1210 17:36:02.345713 22232 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 17:36:02.345713 22232 net.cpp:137] Memory required for data: 287950000
I1210 17:36:02.345713 22232 layer_factory.cpp:58] Creating layer scale2_2
I1210 17:36:02.345713 22232 net.cpp:84] Creating Layer scale2_2
I1210 17:36:02.345713 22232 net.cpp:406] scale2_2 <- conv2_2
I1210 17:36:02.345713 22232 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1210 17:36:02.345713 22232 layer_factory.cpp:58] Creating layer scale2_2
I1210 17:36:02.345713 22232 net.cpp:122] Setting up scale2_2
I1210 17:36:02.345713 22232 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 17:36:02.345713 22232 net.cpp:137] Memory required for data: 308430000
I1210 17:36:02.345713 22232 layer_factory.cpp:58] Creating layer relu2_2
I1210 17:36:02.345713 22232 net.cpp:84] Creating Layer relu2_2
I1210 17:36:02.345713 22232 net.cpp:406] relu2_2 <- conv2_2
I1210 17:36:02.345713 22232 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1210 17:36:02.346714 22232 net.cpp:122] Setting up relu2_2
I1210 17:36:02.346714 22232 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 17:36:02.346714 22232 net.cpp:137] Memory required for data: 328910000
I1210 17:36:02.346714 22232 layer_factory.cpp:58] Creating layer newconv_added1
I1210 17:36:02.346714 22232 net.cpp:84] Creating Layer newconv_added1
I1210 17:36:02.346714 22232 net.cpp:406] newconv_added1 <- conv2_2
I1210 17:36:02.346714 22232 net.cpp:380] newconv_added1 -> newconv_added1
I1210 17:36:02.347712 22232 net.cpp:122] Setting up newconv_added1
I1210 17:36:02.347712 22232 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 17:36:02.347712 22232 net.cpp:137] Memory required for data: 349390000
I1210 17:36:02.347712 22232 layer_factory.cpp:58] Creating layer pool2_1
I1210 17:36:02.347712 22232 net.cpp:84] Creating Layer pool2_1
I1210 17:36:02.347712 22232 net.cpp:406] pool2_1 <- newconv_added1
I1210 17:36:02.347712 22232 net.cpp:380] pool2_1 -> pool2_1
I1210 17:36:02.347712 22232 net.cpp:122] Setting up pool2_1
I1210 17:36:02.347712 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.347712 22232 net.cpp:137] Memory required for data: 354510000
I1210 17:36:02.347712 22232 layer_factory.cpp:58] Creating layer conv3
I1210 17:36:02.347712 22232 net.cpp:84] Creating Layer conv3
I1210 17:36:02.347712 22232 net.cpp:406] conv3 <- pool2_1
I1210 17:36:02.347712 22232 net.cpp:380] conv3 -> conv3
I1210 17:36:02.348718 22232 net.cpp:122] Setting up conv3
I1210 17:36:02.348718 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.348718 22232 net.cpp:137] Memory required for data: 359630000
I1210 17:36:02.348718 22232 layer_factory.cpp:58] Creating layer bn3
I1210 17:36:02.348718 22232 net.cpp:84] Creating Layer bn3
I1210 17:36:02.348718 22232 net.cpp:406] bn3 <- conv3
I1210 17:36:02.348718 22232 net.cpp:367] bn3 -> conv3 (in-place)
I1210 17:36:02.349714 22232 net.cpp:122] Setting up bn3
I1210 17:36:02.349714 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.349714 22232 net.cpp:137] Memory required for data: 364750000
I1210 17:36:02.349714 22232 layer_factory.cpp:58] Creating layer scale3
I1210 17:36:02.349714 22232 net.cpp:84] Creating Layer scale3
I1210 17:36:02.349714 22232 net.cpp:406] scale3 <- conv3
I1210 17:36:02.349714 22232 net.cpp:367] scale3 -> conv3 (in-place)
I1210 17:36:02.349714 22232 layer_factory.cpp:58] Creating layer scale3
I1210 17:36:02.349714 22232 net.cpp:122] Setting up scale3
I1210 17:36:02.349714 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.349714 22232 net.cpp:137] Memory required for data: 369870000
I1210 17:36:02.349714 22232 layer_factory.cpp:58] Creating layer relu3
I1210 17:36:02.349714 22232 net.cpp:84] Creating Layer relu3
I1210 17:36:02.349714 22232 net.cpp:406] relu3 <- conv3
I1210 17:36:02.349714 22232 net.cpp:367] relu3 -> conv3 (in-place)
I1210 17:36:02.349714 22232 net.cpp:122] Setting up relu3
I1210 17:36:02.349714 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.349714 22232 net.cpp:137] Memory required for data: 374990000
I1210 17:36:02.349714 22232 layer_factory.cpp:58] Creating layer conv3_1
I1210 17:36:02.349714 22232 net.cpp:84] Creating Layer conv3_1
I1210 17:36:02.349714 22232 net.cpp:406] conv3_1 <- conv3
I1210 17:36:02.349714 22232 net.cpp:380] conv3_1 -> conv3_1
I1210 17:36:02.350713 22232 net.cpp:122] Setting up conv3_1
I1210 17:36:02.350713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.350713 22232 net.cpp:137] Memory required for data: 380110000
I1210 17:36:02.350713 22232 layer_factory.cpp:58] Creating layer bn3_1
I1210 17:36:02.350713 22232 net.cpp:84] Creating Layer bn3_1
I1210 17:36:02.350713 22232 net.cpp:406] bn3_1 <- conv3_1
I1210 17:36:02.350713 22232 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1210 17:36:02.351713 22232 net.cpp:122] Setting up bn3_1
I1210 17:36:02.351713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.351713 22232 net.cpp:137] Memory required for data: 385230000
I1210 17:36:02.351713 22232 layer_factory.cpp:58] Creating layer scale3_1
I1210 17:36:02.351713 22232 net.cpp:84] Creating Layer scale3_1
I1210 17:36:02.351713 22232 net.cpp:406] scale3_1 <- conv3_1
I1210 17:36:02.351713 22232 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1210 17:36:02.351713 22232 layer_factory.cpp:58] Creating layer scale3_1
I1210 17:36:02.351713 22232 net.cpp:122] Setting up scale3_1
I1210 17:36:02.351713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.351713 22232 net.cpp:137] Memory required for data: 390350000
I1210 17:36:02.351713 22232 layer_factory.cpp:58] Creating layer relu3_1
I1210 17:36:02.351713 22232 net.cpp:84] Creating Layer relu3_1
I1210 17:36:02.351713 22232 net.cpp:406] relu3_1 <- conv3_1
I1210 17:36:02.351713 22232 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1210 17:36:02.351713 22232 net.cpp:122] Setting up relu3_1
I1210 17:36:02.351713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.351713 22232 net.cpp:137] Memory required for data: 395470000
I1210 17:36:02.351713 22232 layer_factory.cpp:58] Creating layer conv4
I1210 17:36:02.351713 22232 net.cpp:84] Creating Layer conv4
I1210 17:36:02.351713 22232 net.cpp:406] conv4 <- conv3_1
I1210 17:36:02.351713 22232 net.cpp:380] conv4 -> conv4
I1210 17:36:02.352715 22232 net.cpp:122] Setting up conv4
I1210 17:36:02.352715 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.352715 22232 net.cpp:137] Memory required for data: 400590000
I1210 17:36:02.352715 22232 layer_factory.cpp:58] Creating layer bn4
I1210 17:36:02.352715 22232 net.cpp:84] Creating Layer bn4
I1210 17:36:02.352715 22232 net.cpp:406] bn4 <- conv4
I1210 17:36:02.352715 22232 net.cpp:367] bn4 -> conv4 (in-place)
I1210 17:36:02.353713 22232 net.cpp:122] Setting up bn4
I1210 17:36:02.353713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.353713 22232 net.cpp:137] Memory required for data: 405710000
I1210 17:36:02.353713 22232 layer_factory.cpp:58] Creating layer scale4
I1210 17:36:02.353713 22232 net.cpp:84] Creating Layer scale4
I1210 17:36:02.353713 22232 net.cpp:406] scale4 <- conv4
I1210 17:36:02.353713 22232 net.cpp:367] scale4 -> conv4 (in-place)
I1210 17:36:02.353713 22232 layer_factory.cpp:58] Creating layer scale4
I1210 17:36:02.353713 22232 net.cpp:122] Setting up scale4
I1210 17:36:02.353713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.353713 22232 net.cpp:137] Memory required for data: 410830000
I1210 17:36:02.353713 22232 layer_factory.cpp:58] Creating layer relu4
I1210 17:36:02.353713 22232 net.cpp:84] Creating Layer relu4
I1210 17:36:02.353713 22232 net.cpp:406] relu4 <- conv4
I1210 17:36:02.353713 22232 net.cpp:367] relu4 -> conv4 (in-place)
I1210 17:36:02.353713 22232 net.cpp:122] Setting up relu4
I1210 17:36:02.353713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.353713 22232 net.cpp:137] Memory required for data: 415950000
I1210 17:36:02.353713 22232 layer_factory.cpp:58] Creating layer conv4_1
I1210 17:36:02.353713 22232 net.cpp:84] Creating Layer conv4_1
I1210 17:36:02.353713 22232 net.cpp:406] conv4_1 <- conv4
I1210 17:36:02.353713 22232 net.cpp:380] conv4_1 -> conv4_1
I1210 17:36:02.355721 22232 net.cpp:122] Setting up conv4_1
I1210 17:36:02.355721 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.355721 22232 net.cpp:137] Memory required for data: 421070000
I1210 17:36:02.355721 22232 layer_factory.cpp:58] Creating layer bn4_1
I1210 17:36:02.355721 22232 net.cpp:84] Creating Layer bn4_1
I1210 17:36:02.355721 22232 net.cpp:406] bn4_1 <- conv4_1
I1210 17:36:02.355721 22232 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1210 17:36:02.355721 22232 net.cpp:122] Setting up bn4_1
I1210 17:36:02.355721 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.355721 22232 net.cpp:137] Memory required for data: 426190000
I1210 17:36:02.355721 22232 layer_factory.cpp:58] Creating layer scale4_1
I1210 17:36:02.355721 22232 net.cpp:84] Creating Layer scale4_1
I1210 17:36:02.355721 22232 net.cpp:406] scale4_1 <- conv4_1
I1210 17:36:02.355721 22232 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1210 17:36:02.355721 22232 layer_factory.cpp:58] Creating layer scale4_1
I1210 17:36:02.355721 22232 net.cpp:122] Setting up scale4_1
I1210 17:36:02.355721 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.355721 22232 net.cpp:137] Memory required for data: 431310000
I1210 17:36:02.355721 22232 layer_factory.cpp:58] Creating layer relu4_1
I1210 17:36:02.355721 22232 net.cpp:84] Creating Layer relu4_1
I1210 17:36:02.355721 22232 net.cpp:406] relu4_1 <- conv4_1
I1210 17:36:02.355721 22232 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1210 17:36:02.356714 22232 net.cpp:122] Setting up relu4_1
I1210 17:36:02.356714 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.356714 22232 net.cpp:137] Memory required for data: 436430000
I1210 17:36:02.356714 22232 layer_factory.cpp:58] Creating layer conv4_2
I1210 17:36:02.356714 22232 net.cpp:84] Creating Layer conv4_2
I1210 17:36:02.356714 22232 net.cpp:406] conv4_2 <- conv4_1
I1210 17:36:02.356714 22232 net.cpp:380] conv4_2 -> conv4_2
I1210 17:36:02.357713 22232 net.cpp:122] Setting up conv4_2
I1210 17:36:02.357713 22232 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 17:36:02.357713 22232 net.cpp:137] Memory required for data: 442369200
I1210 17:36:02.357713 22232 layer_factory.cpp:58] Creating layer bn4_2
I1210 17:36:02.357713 22232 net.cpp:84] Creating Layer bn4_2
I1210 17:36:02.357713 22232 net.cpp:406] bn4_2 <- conv4_2
I1210 17:36:02.357713 22232 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1210 17:36:02.357713 22232 net.cpp:122] Setting up bn4_2
I1210 17:36:02.357713 22232 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 17:36:02.357713 22232 net.cpp:137] Memory required for data: 448308400
I1210 17:36:02.357713 22232 layer_factory.cpp:58] Creating layer scale4_2
I1210 17:36:02.357713 22232 net.cpp:84] Creating Layer scale4_2
I1210 17:36:02.357713 22232 net.cpp:406] scale4_2 <- conv4_2
I1210 17:36:02.357713 22232 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1210 17:36:02.357713 22232 layer_factory.cpp:58] Creating layer scale4_2
I1210 17:36:02.357713 22232 net.cpp:122] Setting up scale4_2
I1210 17:36:02.357713 22232 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 17:36:02.357713 22232 net.cpp:137] Memory required for data: 454247600
I1210 17:36:02.357713 22232 layer_factory.cpp:58] Creating layer relu4_2
I1210 17:36:02.357713 22232 net.cpp:84] Creating Layer relu4_2
I1210 17:36:02.357713 22232 net.cpp:406] relu4_2 <- conv4_2
I1210 17:36:02.357713 22232 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1210 17:36:02.358713 22232 net.cpp:122] Setting up relu4_2
I1210 17:36:02.358713 22232 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 17:36:02.358713 22232 net.cpp:137] Memory required for data: 460186800
I1210 17:36:02.358713 22232 layer_factory.cpp:58] Creating layer added_new_conv2
I1210 17:36:02.358713 22232 net.cpp:84] Creating Layer added_new_conv2
I1210 17:36:02.358713 22232 net.cpp:406] added_new_conv2 <- conv4_2
I1210 17:36:02.358713 22232 net.cpp:380] added_new_conv2 -> added_new_conv2
I1210 17:36:02.359722 22232 net.cpp:122] Setting up added_new_conv2
I1210 17:36:02.359722 22232 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 17:36:02.359722 22232 net.cpp:137] Memory required for data: 466126000
I1210 17:36:02.359722 22232 layer_factory.cpp:58] Creating layer pool4_2
I1210 17:36:02.359722 22232 net.cpp:84] Creating Layer pool4_2
I1210 17:36:02.359722 22232 net.cpp:406] pool4_2 <- added_new_conv2
I1210 17:36:02.359722 22232 net.cpp:380] pool4_2 -> pool4_2
I1210 17:36:02.359722 22232 net.cpp:122] Setting up pool4_2
I1210 17:36:02.359722 22232 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 17:36:02.359722 22232 net.cpp:137] Memory required for data: 467610800
I1210 17:36:02.359722 22232 layer_factory.cpp:58] Creating layer conv4_0
I1210 17:36:02.359722 22232 net.cpp:84] Creating Layer conv4_0
I1210 17:36:02.359722 22232 net.cpp:406] conv4_0 <- pool4_2
I1210 17:36:02.359722 22232 net.cpp:380] conv4_0 -> conv4_0
I1210 17:36:02.361713 22232 net.cpp:122] Setting up conv4_0
I1210 17:36:02.361713 22232 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 17:36:02.361713 22232 net.cpp:137] Memory required for data: 469095600
I1210 17:36:02.361713 22232 layer_factory.cpp:58] Creating layer bn4_0
I1210 17:36:02.361713 22232 net.cpp:84] Creating Layer bn4_0
I1210 17:36:02.361713 22232 net.cpp:406] bn4_0 <- conv4_0
I1210 17:36:02.361713 22232 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1210 17:36:02.361713 22232 net.cpp:122] Setting up bn4_0
I1210 17:36:02.361713 22232 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 17:36:02.361713 22232 net.cpp:137] Memory required for data: 470580400
I1210 17:36:02.361713 22232 layer_factory.cpp:58] Creating layer scale4_0
I1210 17:36:02.361713 22232 net.cpp:84] Creating Layer scale4_0
I1210 17:36:02.361713 22232 net.cpp:406] scale4_0 <- conv4_0
I1210 17:36:02.361713 22232 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1210 17:36:02.361713 22232 layer_factory.cpp:58] Creating layer scale4_0
I1210 17:36:02.361713 22232 net.cpp:122] Setting up scale4_0
I1210 17:36:02.361713 22232 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 17:36:02.361713 22232 net.cpp:137] Memory required for data: 472065200
I1210 17:36:02.361713 22232 layer_factory.cpp:58] Creating layer relu4_0
I1210 17:36:02.361713 22232 net.cpp:84] Creating Layer relu4_0
I1210 17:36:02.361713 22232 net.cpp:406] relu4_0 <- conv4_0
I1210 17:36:02.361713 22232 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1210 17:36:02.361713 22232 net.cpp:122] Setting up relu4_0
I1210 17:36:02.361713 22232 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 17:36:02.361713 22232 net.cpp:137] Memory required for data: 473550000
I1210 17:36:02.361713 22232 layer_factory.cpp:58] Creating layer conv11
I1210 17:36:02.362715 22232 net.cpp:84] Creating Layer conv11
I1210 17:36:02.362715 22232 net.cpp:406] conv11 <- conv4_0
I1210 17:36:02.362715 22232 net.cpp:380] conv11 -> conv11
I1210 17:36:02.363713 22232 net.cpp:122] Setting up conv11
I1210 17:36:02.363713 22232 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 17:36:02.363713 22232 net.cpp:137] Memory required for data: 475342000
I1210 17:36:02.363713 22232 layer_factory.cpp:58] Creating layer bn_conv11
I1210 17:36:02.363713 22232 net.cpp:84] Creating Layer bn_conv11
I1210 17:36:02.363713 22232 net.cpp:406] bn_conv11 <- conv11
I1210 17:36:02.363713 22232 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1210 17:36:02.363713 22232 net.cpp:122] Setting up bn_conv11
I1210 17:36:02.363713 22232 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 17:36:02.363713 22232 net.cpp:137] Memory required for data: 477134000
I1210 17:36:02.363713 22232 layer_factory.cpp:58] Creating layer scale_conv11
I1210 17:36:02.363713 22232 net.cpp:84] Creating Layer scale_conv11
I1210 17:36:02.363713 22232 net.cpp:406] scale_conv11 <- conv11
I1210 17:36:02.363713 22232 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1210 17:36:02.363713 22232 layer_factory.cpp:58] Creating layer scale_conv11
I1210 17:36:02.364713 22232 net.cpp:122] Setting up scale_conv11
I1210 17:36:02.364713 22232 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 17:36:02.364713 22232 net.cpp:137] Memory required for data: 478926000
I1210 17:36:02.364713 22232 layer_factory.cpp:58] Creating layer relu_conv11
I1210 17:36:02.364713 22232 net.cpp:84] Creating Layer relu_conv11
I1210 17:36:02.364713 22232 net.cpp:406] relu_conv11 <- conv11
I1210 17:36:02.364713 22232 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1210 17:36:02.364713 22232 net.cpp:122] Setting up relu_conv11
I1210 17:36:02.364713 22232 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 17:36:02.364713 22232 net.cpp:137] Memory required for data: 480718000
I1210 17:36:02.364713 22232 layer_factory.cpp:58] Creating layer conv12
I1210 17:36:02.364713 22232 net.cpp:84] Creating Layer conv12
I1210 17:36:02.364713 22232 net.cpp:406] conv12 <- conv11
I1210 17:36:02.364713 22232 net.cpp:380] conv12 -> conv12
I1210 17:36:02.366713 22232 net.cpp:122] Setting up conv12
I1210 17:36:02.366713 22232 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 17:36:02.366713 22232 net.cpp:137] Memory required for data: 483022000
I1210 17:36:02.366713 22232 layer_factory.cpp:58] Creating layer bn_conv12
I1210 17:36:02.366713 22232 net.cpp:84] Creating Layer bn_conv12
I1210 17:36:02.366713 22232 net.cpp:406] bn_conv12 <- conv12
I1210 17:36:02.366713 22232 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1210 17:36:02.366713 22232 net.cpp:122] Setting up bn_conv12
I1210 17:36:02.366713 22232 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 17:36:02.366713 22232 net.cpp:137] Memory required for data: 485326000
I1210 17:36:02.366713 22232 layer_factory.cpp:58] Creating layer scale_conv12
I1210 17:36:02.366713 22232 net.cpp:84] Creating Layer scale_conv12
I1210 17:36:02.366713 22232 net.cpp:406] scale_conv12 <- conv12
I1210 17:36:02.366713 22232 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1210 17:36:02.366713 22232 layer_factory.cpp:58] Creating layer scale_conv12
I1210 17:36:02.366713 22232 net.cpp:122] Setting up scale_conv12
I1210 17:36:02.366713 22232 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 17:36:02.366713 22232 net.cpp:137] Memory required for data: 487630000
I1210 17:36:02.366713 22232 layer_factory.cpp:58] Creating layer relu_conv12
I1210 17:36:02.366713 22232 net.cpp:84] Creating Layer relu_conv12
I1210 17:36:02.366713 22232 net.cpp:406] relu_conv12 <- conv12
I1210 17:36:02.366713 22232 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1210 17:36:02.366713 22232 net.cpp:122] Setting up relu_conv12
I1210 17:36:02.366713 22232 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 17:36:02.367713 22232 net.cpp:137] Memory required for data: 489934000
I1210 17:36:02.367713 22232 layer_factory.cpp:58] Creating layer poolcp6
I1210 17:36:02.367713 22232 net.cpp:84] Creating Layer poolcp6
I1210 17:36:02.367713 22232 net.cpp:406] poolcp6 <- conv12
I1210 17:36:02.367713 22232 net.cpp:380] poolcp6 -> poolcp6
I1210 17:36:02.367713 22232 net.cpp:122] Setting up poolcp6
I1210 17:36:02.367713 22232 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1210 17:36:02.367713 22232 net.cpp:137] Memory required for data: 489970000
I1210 17:36:02.367713 22232 layer_factory.cpp:58] Creating layer ip1
I1210 17:36:02.367713 22232 net.cpp:84] Creating Layer ip1
I1210 17:36:02.367713 22232 net.cpp:406] ip1 <- poolcp6
I1210 17:36:02.367713 22232 net.cpp:380] ip1 -> ip1
I1210 17:36:02.367713 22232 net.cpp:122] Setting up ip1
I1210 17:36:02.367713 22232 net.cpp:129] Top shape: 100 100 (10000)
I1210 17:36:02.367713 22232 net.cpp:137] Memory required for data: 490010000
I1210 17:36:02.367713 22232 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1210 17:36:02.367713 22232 net.cpp:84] Creating Layer ip1_ip1_0_split
I1210 17:36:02.367713 22232 net.cpp:406] ip1_ip1_0_split <- ip1
I1210 17:36:02.367713 22232 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1210 17:36:02.367713 22232 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1210 17:36:02.367713 22232 net.cpp:122] Setting up ip1_ip1_0_split
I1210 17:36:02.367713 22232 net.cpp:129] Top shape: 100 100 (10000)
I1210 17:36:02.367713 22232 net.cpp:129] Top shape: 100 100 (10000)
I1210 17:36:02.367713 22232 net.cpp:137] Memory required for data: 490090000
I1210 17:36:02.367713 22232 layer_factory.cpp:58] Creating layer accuracy_training
I1210 17:36:02.367713 22232 net.cpp:84] Creating Layer accuracy_training
I1210 17:36:02.367713 22232 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1210 17:36:02.367713 22232 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1210 17:36:02.367713 22232 net.cpp:380] accuracy_training -> accuracy_training
I1210 17:36:02.367713 22232 net.cpp:122] Setting up accuracy_training
I1210 17:36:02.367713 22232 net.cpp:129] Top shape: (1)
I1210 17:36:02.367713 22232 net.cpp:137] Memory required for data: 490090004
I1210 17:36:02.367713 22232 layer_factory.cpp:58] Creating layer loss
I1210 17:36:02.367713 22232 net.cpp:84] Creating Layer loss
I1210 17:36:02.367713 22232 net.cpp:406] loss <- ip1_ip1_0_split_1
I1210 17:36:02.367713 22232 net.cpp:406] loss <- label_cifar_1_split_1
I1210 17:36:02.367713 22232 net.cpp:380] loss -> loss
I1210 17:36:02.367713 22232 layer_factory.cpp:58] Creating layer loss
I1210 17:36:02.367713 22232 net.cpp:122] Setting up loss
I1210 17:36:02.367713 22232 net.cpp:129] Top shape: (1)
I1210 17:36:02.367713 22232 net.cpp:132]     with loss weight 1
I1210 17:36:02.367713 22232 net.cpp:137] Memory required for data: 490090008
I1210 17:36:02.367713 22232 net.cpp:198] loss needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:200] accuracy_training does not need backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] ip1 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] poolcp6 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] relu_conv12 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] scale_conv12 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] bn_conv12 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] conv12 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] relu_conv11 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] scale_conv11 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] bn_conv11 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] conv11 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] relu4_0 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] scale4_0 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] bn4_0 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] conv4_0 needs backward computation.
I1210 17:36:02.367713 22232 net.cpp:198] pool4_2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] added_new_conv2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] relu4_2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] scale4_2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] bn4_2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] conv4_2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] relu4_1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] scale4_1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] bn4_1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] conv4_1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] relu4 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] scale4 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] bn4 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] conv4 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] relu3_1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] scale3_1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] bn3_1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] conv3_1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] relu3 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] scale3 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] bn3 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] conv3 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] pool2_1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] newconv_added1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] relu2_2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] scale2_2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] bn2_2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] conv2_2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] relu2_1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] scale2_1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] bn2_1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] conv2_1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] relu2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] scale2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] bn2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] conv2 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] relu1_0 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] scale1_0 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] bn1_0 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] conv1_0 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] relu1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] scale1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] bn1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:198] conv1 needs backward computation.
I1210 17:36:02.368715 22232 net.cpp:200] label_cifar_1_split does not need backward computation.
I1210 17:36:02.368715 22232 net.cpp:200] cifar does not need backward computation.
I1210 17:36:02.368715 22232 net.cpp:242] This network produces output accuracy_training
I1210 17:36:02.368715 22232 net.cpp:242] This network produces output loss
I1210 17:36:02.368715 22232 net.cpp:255] Network initialization done.
I1210 17:36:02.369714 22232 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 17:36:02.369714 22232 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 17:36:02.369714 22232 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1210 17:36:02.369714 22232 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1210 17:36:02.369714 22232 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_v2_360k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1210 17:36:02.369714 22232 layer_factory.cpp:58] Creating layer cifar
I1210 17:36:02.372714 22232 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1210 17:36:02.372714 22232 net.cpp:84] Creating Layer cifar
I1210 17:36:02.372714 22232 net.cpp:380] cifar -> data
I1210 17:36:02.372714 22232 net.cpp:380] cifar -> label
I1210 17:36:02.372714 22232 data_layer.cpp:45] output data size: 100,3,32,32
I1210 17:36:02.378715 22232 net.cpp:122] Setting up cifar
I1210 17:36:02.378715 22232 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1210 17:36:02.378715 22232 net.cpp:129] Top shape: 100 (100)
I1210 17:36:02.378715 22232 net.cpp:137] Memory required for data: 1229200
I1210 17:36:02.378715 22232 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1210 17:36:02.378715 22232 net.cpp:84] Creating Layer label_cifar_1_split
I1210 17:36:02.378715 22232 net.cpp:406] label_cifar_1_split <- label
I1210 17:36:02.378715 22232 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1210 17:36:02.378715 22232 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1210 17:36:02.379714 22232 net.cpp:122] Setting up label_cifar_1_split
I1210 17:36:02.379714 22232 net.cpp:129] Top shape: 100 (100)
I1210 17:36:02.379714 22232 net.cpp:129] Top shape: 100 (100)
I1210 17:36:02.379714 22232 net.cpp:137] Memory required for data: 1230000
I1210 17:36:02.379714 22232 layer_factory.cpp:58] Creating layer conv1
I1210 17:36:02.379714 22232 net.cpp:84] Creating Layer conv1
I1210 17:36:02.379714 22232 net.cpp:406] conv1 <- data
I1210 17:36:02.379714 22232 net.cpp:380] conv1 -> conv1
I1210 17:36:02.380714 13776 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1210 17:36:02.381713 22232 net.cpp:122] Setting up conv1
I1210 17:36:02.381713 22232 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 17:36:02.381713 22232 net.cpp:137] Memory required for data: 13518000
I1210 17:36:02.381713 22232 layer_factory.cpp:58] Creating layer bn1
I1210 17:36:02.381713 22232 net.cpp:84] Creating Layer bn1
I1210 17:36:02.381713 22232 net.cpp:406] bn1 <- conv1
I1210 17:36:02.381713 22232 net.cpp:367] bn1 -> conv1 (in-place)
I1210 17:36:02.381713 22232 net.cpp:122] Setting up bn1
I1210 17:36:02.381713 22232 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 17:36:02.381713 22232 net.cpp:137] Memory required for data: 25806000
I1210 17:36:02.381713 22232 layer_factory.cpp:58] Creating layer scale1
I1210 17:36:02.381713 22232 net.cpp:84] Creating Layer scale1
I1210 17:36:02.381713 22232 net.cpp:406] scale1 <- conv1
I1210 17:36:02.381713 22232 net.cpp:367] scale1 -> conv1 (in-place)
I1210 17:36:02.381713 22232 layer_factory.cpp:58] Creating layer scale1
I1210 17:36:02.381713 22232 net.cpp:122] Setting up scale1
I1210 17:36:02.381713 22232 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 17:36:02.381713 22232 net.cpp:137] Memory required for data: 38094000
I1210 17:36:02.381713 22232 layer_factory.cpp:58] Creating layer relu1
I1210 17:36:02.381713 22232 net.cpp:84] Creating Layer relu1
I1210 17:36:02.381713 22232 net.cpp:406] relu1 <- conv1
I1210 17:36:02.381713 22232 net.cpp:367] relu1 -> conv1 (in-place)
I1210 17:36:02.381713 22232 net.cpp:122] Setting up relu1
I1210 17:36:02.381713 22232 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1210 17:36:02.381713 22232 net.cpp:137] Memory required for data: 50382000
I1210 17:36:02.381713 22232 layer_factory.cpp:58] Creating layer conv1_0
I1210 17:36:02.381713 22232 net.cpp:84] Creating Layer conv1_0
I1210 17:36:02.381713 22232 net.cpp:406] conv1_0 <- conv1
I1210 17:36:02.381713 22232 net.cpp:380] conv1_0 -> conv1_0
I1210 17:36:02.383714 22232 net.cpp:122] Setting up conv1_0
I1210 17:36:02.383714 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.383714 22232 net.cpp:137] Memory required for data: 66766000
I1210 17:36:02.383714 22232 layer_factory.cpp:58] Creating layer bn1_0
I1210 17:36:02.383714 22232 net.cpp:84] Creating Layer bn1_0
I1210 17:36:02.383714 22232 net.cpp:406] bn1_0 <- conv1_0
I1210 17:36:02.383714 22232 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1210 17:36:02.383714 22232 net.cpp:122] Setting up bn1_0
I1210 17:36:02.383714 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.383714 22232 net.cpp:137] Memory required for data: 83150000
I1210 17:36:02.383714 22232 layer_factory.cpp:58] Creating layer scale1_0
I1210 17:36:02.383714 22232 net.cpp:84] Creating Layer scale1_0
I1210 17:36:02.383714 22232 net.cpp:406] scale1_0 <- conv1_0
I1210 17:36:02.383714 22232 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1210 17:36:02.383714 22232 layer_factory.cpp:58] Creating layer scale1_0
I1210 17:36:02.384714 22232 net.cpp:122] Setting up scale1_0
I1210 17:36:02.384714 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.384714 22232 net.cpp:137] Memory required for data: 99534000
I1210 17:36:02.384714 22232 layer_factory.cpp:58] Creating layer relu1_0
I1210 17:36:02.384714 22232 net.cpp:84] Creating Layer relu1_0
I1210 17:36:02.384714 22232 net.cpp:406] relu1_0 <- conv1_0
I1210 17:36:02.384714 22232 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1210 17:36:02.384714 22232 net.cpp:122] Setting up relu1_0
I1210 17:36:02.384714 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.384714 22232 net.cpp:137] Memory required for data: 115918000
I1210 17:36:02.384714 22232 layer_factory.cpp:58] Creating layer conv2
I1210 17:36:02.384714 22232 net.cpp:84] Creating Layer conv2
I1210 17:36:02.384714 22232 net.cpp:406] conv2 <- conv1_0
I1210 17:36:02.384714 22232 net.cpp:380] conv2 -> conv2
I1210 17:36:02.386741 22232 net.cpp:122] Setting up conv2
I1210 17:36:02.386741 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.386741 22232 net.cpp:137] Memory required for data: 132302000
I1210 17:36:02.386741 22232 layer_factory.cpp:58] Creating layer bn2
I1210 17:36:02.386741 22232 net.cpp:84] Creating Layer bn2
I1210 17:36:02.386741 22232 net.cpp:406] bn2 <- conv2
I1210 17:36:02.386741 22232 net.cpp:367] bn2 -> conv2 (in-place)
I1210 17:36:02.386741 22232 net.cpp:122] Setting up bn2
I1210 17:36:02.386741 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.386741 22232 net.cpp:137] Memory required for data: 148686000
I1210 17:36:02.386741 22232 layer_factory.cpp:58] Creating layer scale2
I1210 17:36:02.386741 22232 net.cpp:84] Creating Layer scale2
I1210 17:36:02.386741 22232 net.cpp:406] scale2 <- conv2
I1210 17:36:02.386741 22232 net.cpp:367] scale2 -> conv2 (in-place)
I1210 17:36:02.386741 22232 layer_factory.cpp:58] Creating layer scale2
I1210 17:36:02.387717 22232 net.cpp:122] Setting up scale2
I1210 17:36:02.387717 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.387717 22232 net.cpp:137] Memory required for data: 165070000
I1210 17:36:02.387717 22232 layer_factory.cpp:58] Creating layer relu2
I1210 17:36:02.387717 22232 net.cpp:84] Creating Layer relu2
I1210 17:36:02.387717 22232 net.cpp:406] relu2 <- conv2
I1210 17:36:02.387717 22232 net.cpp:367] relu2 -> conv2 (in-place)
I1210 17:36:02.387717 22232 net.cpp:122] Setting up relu2
I1210 17:36:02.387717 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.387717 22232 net.cpp:137] Memory required for data: 181454000
I1210 17:36:02.387717 22232 layer_factory.cpp:58] Creating layer conv2_1
I1210 17:36:02.387717 22232 net.cpp:84] Creating Layer conv2_1
I1210 17:36:02.387717 22232 net.cpp:406] conv2_1 <- conv2
I1210 17:36:02.387717 22232 net.cpp:380] conv2_1 -> conv2_1
I1210 17:36:02.389729 22232 net.cpp:122] Setting up conv2_1
I1210 17:36:02.389729 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.389729 22232 net.cpp:137] Memory required for data: 197838000
I1210 17:36:02.389729 22232 layer_factory.cpp:58] Creating layer bn2_1
I1210 17:36:02.389729 22232 net.cpp:84] Creating Layer bn2_1
I1210 17:36:02.389729 22232 net.cpp:406] bn2_1 <- conv2_1
I1210 17:36:02.389729 22232 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1210 17:36:02.389729 22232 net.cpp:122] Setting up bn2_1
I1210 17:36:02.389729 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.389729 22232 net.cpp:137] Memory required for data: 214222000
I1210 17:36:02.389729 22232 layer_factory.cpp:58] Creating layer scale2_1
I1210 17:36:02.389729 22232 net.cpp:84] Creating Layer scale2_1
I1210 17:36:02.389729 22232 net.cpp:406] scale2_1 <- conv2_1
I1210 17:36:02.389729 22232 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1210 17:36:02.389729 22232 layer_factory.cpp:58] Creating layer scale2_1
I1210 17:36:02.389729 22232 net.cpp:122] Setting up scale2_1
I1210 17:36:02.389729 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.389729 22232 net.cpp:137] Memory required for data: 230606000
I1210 17:36:02.389729 22232 layer_factory.cpp:58] Creating layer relu2_1
I1210 17:36:02.389729 22232 net.cpp:84] Creating Layer relu2_1
I1210 17:36:02.389729 22232 net.cpp:406] relu2_1 <- conv2_1
I1210 17:36:02.389729 22232 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1210 17:36:02.390733 22232 net.cpp:122] Setting up relu2_1
I1210 17:36:02.390733 22232 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1210 17:36:02.390733 22232 net.cpp:137] Memory required for data: 246990000
I1210 17:36:02.390733 22232 layer_factory.cpp:58] Creating layer conv2_2
I1210 17:36:02.390733 22232 net.cpp:84] Creating Layer conv2_2
I1210 17:36:02.390733 22232 net.cpp:406] conv2_2 <- conv2_1
I1210 17:36:02.390733 22232 net.cpp:380] conv2_2 -> conv2_2
I1210 17:36:02.391715 22232 net.cpp:122] Setting up conv2_2
I1210 17:36:02.391715 22232 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 17:36:02.391715 22232 net.cpp:137] Memory required for data: 267470000
I1210 17:36:02.391715 22232 layer_factory.cpp:58] Creating layer bn2_2
I1210 17:36:02.391715 22232 net.cpp:84] Creating Layer bn2_2
I1210 17:36:02.392722 22232 net.cpp:406] bn2_2 <- conv2_2
I1210 17:36:02.392722 22232 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1210 17:36:02.392722 22232 net.cpp:122] Setting up bn2_2
I1210 17:36:02.392722 22232 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 17:36:02.392722 22232 net.cpp:137] Memory required for data: 287950000
I1210 17:36:02.392722 22232 layer_factory.cpp:58] Creating layer scale2_2
I1210 17:36:02.392722 22232 net.cpp:84] Creating Layer scale2_2
I1210 17:36:02.392722 22232 net.cpp:406] scale2_2 <- conv2_2
I1210 17:36:02.392722 22232 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1210 17:36:02.392722 22232 layer_factory.cpp:58] Creating layer scale2_2
I1210 17:36:02.392722 22232 net.cpp:122] Setting up scale2_2
I1210 17:36:02.392722 22232 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 17:36:02.392722 22232 net.cpp:137] Memory required for data: 308430000
I1210 17:36:02.392722 22232 layer_factory.cpp:58] Creating layer relu2_2
I1210 17:36:02.392722 22232 net.cpp:84] Creating Layer relu2_2
I1210 17:36:02.392722 22232 net.cpp:406] relu2_2 <- conv2_2
I1210 17:36:02.392722 22232 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1210 17:36:02.393713 22232 net.cpp:122] Setting up relu2_2
I1210 17:36:02.393713 22232 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 17:36:02.393713 22232 net.cpp:137] Memory required for data: 328910000
I1210 17:36:02.393713 22232 layer_factory.cpp:58] Creating layer newconv_added1
I1210 17:36:02.393713 22232 net.cpp:84] Creating Layer newconv_added1
I1210 17:36:02.393713 22232 net.cpp:406] newconv_added1 <- conv2_2
I1210 17:36:02.393713 22232 net.cpp:380] newconv_added1 -> newconv_added1
I1210 17:36:02.394713 22232 net.cpp:122] Setting up newconv_added1
I1210 17:36:02.394713 22232 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1210 17:36:02.394713 22232 net.cpp:137] Memory required for data: 349390000
I1210 17:36:02.394713 22232 layer_factory.cpp:58] Creating layer pool2_1
I1210 17:36:02.394713 22232 net.cpp:84] Creating Layer pool2_1
I1210 17:36:02.394713 22232 net.cpp:406] pool2_1 <- newconv_added1
I1210 17:36:02.394713 22232 net.cpp:380] pool2_1 -> pool2_1
I1210 17:36:02.394713 22232 net.cpp:122] Setting up pool2_1
I1210 17:36:02.394713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.394713 22232 net.cpp:137] Memory required for data: 354510000
I1210 17:36:02.394713 22232 layer_factory.cpp:58] Creating layer conv3
I1210 17:36:02.394713 22232 net.cpp:84] Creating Layer conv3
I1210 17:36:02.394713 22232 net.cpp:406] conv3 <- pool2_1
I1210 17:36:02.394713 22232 net.cpp:380] conv3 -> conv3
I1210 17:36:02.396716 22232 net.cpp:122] Setting up conv3
I1210 17:36:02.396716 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.396716 22232 net.cpp:137] Memory required for data: 359630000
I1210 17:36:02.396716 22232 layer_factory.cpp:58] Creating layer bn3
I1210 17:36:02.396716 22232 net.cpp:84] Creating Layer bn3
I1210 17:36:02.396716 22232 net.cpp:406] bn3 <- conv3
I1210 17:36:02.396716 22232 net.cpp:367] bn3 -> conv3 (in-place)
I1210 17:36:02.396716 22232 net.cpp:122] Setting up bn3
I1210 17:36:02.396716 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.396716 22232 net.cpp:137] Memory required for data: 364750000
I1210 17:36:02.396716 22232 layer_factory.cpp:58] Creating layer scale3
I1210 17:36:02.396716 22232 net.cpp:84] Creating Layer scale3
I1210 17:36:02.396716 22232 net.cpp:406] scale3 <- conv3
I1210 17:36:02.396716 22232 net.cpp:367] scale3 -> conv3 (in-place)
I1210 17:36:02.396716 22232 layer_factory.cpp:58] Creating layer scale3
I1210 17:36:02.396716 22232 net.cpp:122] Setting up scale3
I1210 17:36:02.396716 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.396716 22232 net.cpp:137] Memory required for data: 369870000
I1210 17:36:02.396716 22232 layer_factory.cpp:58] Creating layer relu3
I1210 17:36:02.396716 22232 net.cpp:84] Creating Layer relu3
I1210 17:36:02.396716 22232 net.cpp:406] relu3 <- conv3
I1210 17:36:02.396716 22232 net.cpp:367] relu3 -> conv3 (in-place)
I1210 17:36:02.396716 22232 net.cpp:122] Setting up relu3
I1210 17:36:02.396716 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.396716 22232 net.cpp:137] Memory required for data: 374990000
I1210 17:36:02.396716 22232 layer_factory.cpp:58] Creating layer conv3_1
I1210 17:36:02.396716 22232 net.cpp:84] Creating Layer conv3_1
I1210 17:36:02.396716 22232 net.cpp:406] conv3_1 <- conv3
I1210 17:36:02.396716 22232 net.cpp:380] conv3_1 -> conv3_1
I1210 17:36:02.398715 22232 net.cpp:122] Setting up conv3_1
I1210 17:36:02.398715 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.398715 22232 net.cpp:137] Memory required for data: 380110000
I1210 17:36:02.398715 22232 layer_factory.cpp:58] Creating layer bn3_1
I1210 17:36:02.398715 22232 net.cpp:84] Creating Layer bn3_1
I1210 17:36:02.398715 22232 net.cpp:406] bn3_1 <- conv3_1
I1210 17:36:02.398715 22232 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1210 17:36:02.399724 22232 net.cpp:122] Setting up bn3_1
I1210 17:36:02.399724 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.399724 22232 net.cpp:137] Memory required for data: 385230000
I1210 17:36:02.399724 22232 layer_factory.cpp:58] Creating layer scale3_1
I1210 17:36:02.399724 22232 net.cpp:84] Creating Layer scale3_1
I1210 17:36:02.399724 22232 net.cpp:406] scale3_1 <- conv3_1
I1210 17:36:02.399724 22232 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1210 17:36:02.399724 22232 layer_factory.cpp:58] Creating layer scale3_1
I1210 17:36:02.399724 22232 net.cpp:122] Setting up scale3_1
I1210 17:36:02.399724 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.399724 22232 net.cpp:137] Memory required for data: 390350000
I1210 17:36:02.399724 22232 layer_factory.cpp:58] Creating layer relu3_1
I1210 17:36:02.399724 22232 net.cpp:84] Creating Layer relu3_1
I1210 17:36:02.399724 22232 net.cpp:406] relu3_1 <- conv3_1
I1210 17:36:02.399724 22232 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1210 17:36:02.399724 22232 net.cpp:122] Setting up relu3_1
I1210 17:36:02.399724 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.399724 22232 net.cpp:137] Memory required for data: 395470000
I1210 17:36:02.399724 22232 layer_factory.cpp:58] Creating layer conv4
I1210 17:36:02.399724 22232 net.cpp:84] Creating Layer conv4
I1210 17:36:02.399724 22232 net.cpp:406] conv4 <- conv3_1
I1210 17:36:02.399724 22232 net.cpp:380] conv4 -> conv4
I1210 17:36:02.400723 22232 net.cpp:122] Setting up conv4
I1210 17:36:02.401713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.401713 22232 net.cpp:137] Memory required for data: 400590000
I1210 17:36:02.401713 22232 layer_factory.cpp:58] Creating layer bn4
I1210 17:36:02.401713 22232 net.cpp:84] Creating Layer bn4
I1210 17:36:02.401713 22232 net.cpp:406] bn4 <- conv4
I1210 17:36:02.401713 22232 net.cpp:367] bn4 -> conv4 (in-place)
I1210 17:36:02.401713 22232 net.cpp:122] Setting up bn4
I1210 17:36:02.401713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.401713 22232 net.cpp:137] Memory required for data: 405710000
I1210 17:36:02.401713 22232 layer_factory.cpp:58] Creating layer scale4
I1210 17:36:02.401713 22232 net.cpp:84] Creating Layer scale4
I1210 17:36:02.401713 22232 net.cpp:406] scale4 <- conv4
I1210 17:36:02.401713 22232 net.cpp:367] scale4 -> conv4 (in-place)
I1210 17:36:02.401713 22232 layer_factory.cpp:58] Creating layer scale4
I1210 17:36:02.401713 22232 net.cpp:122] Setting up scale4
I1210 17:36:02.401713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.401713 22232 net.cpp:137] Memory required for data: 410830000
I1210 17:36:02.401713 22232 layer_factory.cpp:58] Creating layer relu4
I1210 17:36:02.401713 22232 net.cpp:84] Creating Layer relu4
I1210 17:36:02.401713 22232 net.cpp:406] relu4 <- conv4
I1210 17:36:02.401713 22232 net.cpp:367] relu4 -> conv4 (in-place)
I1210 17:36:02.401713 22232 net.cpp:122] Setting up relu4
I1210 17:36:02.401713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.401713 22232 net.cpp:137] Memory required for data: 415950000
I1210 17:36:02.401713 22232 layer_factory.cpp:58] Creating layer conv4_1
I1210 17:36:02.401713 22232 net.cpp:84] Creating Layer conv4_1
I1210 17:36:02.401713 22232 net.cpp:406] conv4_1 <- conv4
I1210 17:36:02.401713 22232 net.cpp:380] conv4_1 -> conv4_1
I1210 17:36:02.402714 22232 net.cpp:122] Setting up conv4_1
I1210 17:36:02.402714 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.402714 22232 net.cpp:137] Memory required for data: 421070000
I1210 17:36:02.402714 22232 layer_factory.cpp:58] Creating layer bn4_1
I1210 17:36:02.402714 22232 net.cpp:84] Creating Layer bn4_1
I1210 17:36:02.402714 22232 net.cpp:406] bn4_1 <- conv4_1
I1210 17:36:02.402714 22232 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1210 17:36:02.403713 22232 net.cpp:122] Setting up bn4_1
I1210 17:36:02.403713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.403713 22232 net.cpp:137] Memory required for data: 426190000
I1210 17:36:02.403713 22232 layer_factory.cpp:58] Creating layer scale4_1
I1210 17:36:02.403713 22232 net.cpp:84] Creating Layer scale4_1
I1210 17:36:02.403713 22232 net.cpp:406] scale4_1 <- conv4_1
I1210 17:36:02.403713 22232 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1210 17:36:02.403713 22232 layer_factory.cpp:58] Creating layer scale4_1
I1210 17:36:02.403713 22232 net.cpp:122] Setting up scale4_1
I1210 17:36:02.403713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.403713 22232 net.cpp:137] Memory required for data: 431310000
I1210 17:36:02.403713 22232 layer_factory.cpp:58] Creating layer relu4_1
I1210 17:36:02.403713 22232 net.cpp:84] Creating Layer relu4_1
I1210 17:36:02.403713 22232 net.cpp:406] relu4_1 <- conv4_1
I1210 17:36:02.403713 22232 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1210 17:36:02.403713 22232 net.cpp:122] Setting up relu4_1
I1210 17:36:02.403713 22232 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1210 17:36:02.403713 22232 net.cpp:137] Memory required for data: 436430000
I1210 17:36:02.403713 22232 layer_factory.cpp:58] Creating layer conv4_2
I1210 17:36:02.403713 22232 net.cpp:84] Creating Layer conv4_2
I1210 17:36:02.403713 22232 net.cpp:406] conv4_2 <- conv4_1
I1210 17:36:02.403713 22232 net.cpp:380] conv4_2 -> conv4_2
I1210 17:36:02.404713 22232 net.cpp:122] Setting up conv4_2
I1210 17:36:02.405714 22232 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 17:36:02.405714 22232 net.cpp:137] Memory required for data: 442369200
I1210 17:36:02.405714 22232 layer_factory.cpp:58] Creating layer bn4_2
I1210 17:36:02.405714 22232 net.cpp:84] Creating Layer bn4_2
I1210 17:36:02.405714 22232 net.cpp:406] bn4_2 <- conv4_2
I1210 17:36:02.405714 22232 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1210 17:36:02.405714 22232 net.cpp:122] Setting up bn4_2
I1210 17:36:02.405714 22232 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 17:36:02.405714 22232 net.cpp:137] Memory required for data: 448308400
I1210 17:36:02.405714 22232 layer_factory.cpp:58] Creating layer scale4_2
I1210 17:36:02.405714 22232 net.cpp:84] Creating Layer scale4_2
I1210 17:36:02.405714 22232 net.cpp:406] scale4_2 <- conv4_2
I1210 17:36:02.405714 22232 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1210 17:36:02.405714 22232 layer_factory.cpp:58] Creating layer scale4_2
I1210 17:36:02.405714 22232 net.cpp:122] Setting up scale4_2
I1210 17:36:02.405714 22232 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 17:36:02.405714 22232 net.cpp:137] Memory required for data: 454247600
I1210 17:36:02.405714 22232 layer_factory.cpp:58] Creating layer relu4_2
I1210 17:36:02.405714 22232 net.cpp:84] Creating Layer relu4_2
I1210 17:36:02.405714 22232 net.cpp:406] relu4_2 <- conv4_2
I1210 17:36:02.405714 22232 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1210 17:36:02.406713 22232 net.cpp:122] Setting up relu4_2
I1210 17:36:02.406713 22232 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 17:36:02.406713 22232 net.cpp:137] Memory required for data: 460186800
I1210 17:36:02.406713 22232 layer_factory.cpp:58] Creating layer added_new_conv2
I1210 17:36:02.406713 22232 net.cpp:84] Creating Layer added_new_conv2
I1210 17:36:02.406713 22232 net.cpp:406] added_new_conv2 <- conv4_2
I1210 17:36:02.406713 22232 net.cpp:380] added_new_conv2 -> added_new_conv2
I1210 17:36:02.407713 22232 net.cpp:122] Setting up added_new_conv2
I1210 17:36:02.407713 22232 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1210 17:36:02.407713 22232 net.cpp:137] Memory required for data: 466126000
I1210 17:36:02.407713 22232 layer_factory.cpp:58] Creating layer pool4_2
I1210 17:36:02.407713 22232 net.cpp:84] Creating Layer pool4_2
I1210 17:36:02.407713 22232 net.cpp:406] pool4_2 <- added_new_conv2
I1210 17:36:02.407713 22232 net.cpp:380] pool4_2 -> pool4_2
I1210 17:36:02.407713 22232 net.cpp:122] Setting up pool4_2
I1210 17:36:02.407713 22232 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 17:36:02.407713 22232 net.cpp:137] Memory required for data: 467610800
I1210 17:36:02.407713 22232 layer_factory.cpp:58] Creating layer conv4_0
I1210 17:36:02.407713 22232 net.cpp:84] Creating Layer conv4_0
I1210 17:36:02.407713 22232 net.cpp:406] conv4_0 <- pool4_2
I1210 17:36:02.407713 22232 net.cpp:380] conv4_0 -> conv4_0
I1210 17:36:02.409713 22232 net.cpp:122] Setting up conv4_0
I1210 17:36:02.409713 22232 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 17:36:02.409713 22232 net.cpp:137] Memory required for data: 469095600
I1210 17:36:02.409713 22232 layer_factory.cpp:58] Creating layer bn4_0
I1210 17:36:02.409713 22232 net.cpp:84] Creating Layer bn4_0
I1210 17:36:02.409713 22232 net.cpp:406] bn4_0 <- conv4_0
I1210 17:36:02.409713 22232 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1210 17:36:02.409713 22232 net.cpp:122] Setting up bn4_0
I1210 17:36:02.409713 22232 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 17:36:02.409713 22232 net.cpp:137] Memory required for data: 470580400
I1210 17:36:02.409713 22232 layer_factory.cpp:58] Creating layer scale4_0
I1210 17:36:02.409713 22232 net.cpp:84] Creating Layer scale4_0
I1210 17:36:02.409713 22232 net.cpp:406] scale4_0 <- conv4_0
I1210 17:36:02.409713 22232 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1210 17:36:02.409713 22232 layer_factory.cpp:58] Creating layer scale4_0
I1210 17:36:02.409713 22232 net.cpp:122] Setting up scale4_0
I1210 17:36:02.409713 22232 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 17:36:02.409713 22232 net.cpp:137] Memory required for data: 472065200
I1210 17:36:02.409713 22232 layer_factory.cpp:58] Creating layer relu4_0
I1210 17:36:02.409713 22232 net.cpp:84] Creating Layer relu4_0
I1210 17:36:02.409713 22232 net.cpp:406] relu4_0 <- conv4_0
I1210 17:36:02.409713 22232 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1210 17:36:02.410713 22232 net.cpp:122] Setting up relu4_0
I1210 17:36:02.410713 22232 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1210 17:36:02.410713 22232 net.cpp:137] Memory required for data: 473550000
I1210 17:36:02.410713 22232 layer_factory.cpp:58] Creating layer conv11
I1210 17:36:02.410713 22232 net.cpp:84] Creating Layer conv11
I1210 17:36:02.410713 22232 net.cpp:406] conv11 <- conv4_0
I1210 17:36:02.410713 22232 net.cpp:380] conv11 -> conv11
I1210 17:36:02.411713 22232 net.cpp:122] Setting up conv11
I1210 17:36:02.411713 22232 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 17:36:02.411713 22232 net.cpp:137] Memory required for data: 475342000
I1210 17:36:02.411713 22232 layer_factory.cpp:58] Creating layer bn_conv11
I1210 17:36:02.411713 22232 net.cpp:84] Creating Layer bn_conv11
I1210 17:36:02.411713 22232 net.cpp:406] bn_conv11 <- conv11
I1210 17:36:02.411713 22232 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1210 17:36:02.411713 22232 net.cpp:122] Setting up bn_conv11
I1210 17:36:02.411713 22232 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 17:36:02.411713 22232 net.cpp:137] Memory required for data: 477134000
I1210 17:36:02.411713 22232 layer_factory.cpp:58] Creating layer scale_conv11
I1210 17:36:02.411713 22232 net.cpp:84] Creating Layer scale_conv11
I1210 17:36:02.411713 22232 net.cpp:406] scale_conv11 <- conv11
I1210 17:36:02.411713 22232 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1210 17:36:02.411713 22232 layer_factory.cpp:58] Creating layer scale_conv11
I1210 17:36:02.412714 22232 net.cpp:122] Setting up scale_conv11
I1210 17:36:02.412714 22232 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 17:36:02.412714 22232 net.cpp:137] Memory required for data: 478926000
I1210 17:36:02.412714 22232 layer_factory.cpp:58] Creating layer relu_conv11
I1210 17:36:02.412714 22232 net.cpp:84] Creating Layer relu_conv11
I1210 17:36:02.412714 22232 net.cpp:406] relu_conv11 <- conv11
I1210 17:36:02.412714 22232 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1210 17:36:02.412714 22232 net.cpp:122] Setting up relu_conv11
I1210 17:36:02.412714 22232 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1210 17:36:02.412714 22232 net.cpp:137] Memory required for data: 480718000
I1210 17:36:02.412714 22232 layer_factory.cpp:58] Creating layer conv12
I1210 17:36:02.412714 22232 net.cpp:84] Creating Layer conv12
I1210 17:36:02.412714 22232 net.cpp:406] conv12 <- conv11
I1210 17:36:02.412714 22232 net.cpp:380] conv12 -> conv12
I1210 17:36:02.415216 22232 net.cpp:122] Setting up conv12
I1210 17:36:02.415216 22232 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 17:36:02.415216 22232 net.cpp:137] Memory required for data: 483022000
I1210 17:36:02.415216 22232 layer_factory.cpp:58] Creating layer bn_conv12
I1210 17:36:02.415216 22232 net.cpp:84] Creating Layer bn_conv12
I1210 17:36:02.415216 22232 net.cpp:406] bn_conv12 <- conv12
I1210 17:36:02.415216 22232 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1210 17:36:02.415216 22232 net.cpp:122] Setting up bn_conv12
I1210 17:36:02.415216 22232 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 17:36:02.415216 22232 net.cpp:137] Memory required for data: 485326000
I1210 17:36:02.415216 22232 layer_factory.cpp:58] Creating layer scale_conv12
I1210 17:36:02.415216 22232 net.cpp:84] Creating Layer scale_conv12
I1210 17:36:02.415216 22232 net.cpp:406] scale_conv12 <- conv12
I1210 17:36:02.415216 22232 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1210 17:36:02.415717 22232 layer_factory.cpp:58] Creating layer scale_conv12
I1210 17:36:02.415717 22232 net.cpp:122] Setting up scale_conv12
I1210 17:36:02.415717 22232 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 17:36:02.415717 22232 net.cpp:137] Memory required for data: 487630000
I1210 17:36:02.415717 22232 layer_factory.cpp:58] Creating layer relu_conv12
I1210 17:36:02.415717 22232 net.cpp:84] Creating Layer relu_conv12
I1210 17:36:02.415717 22232 net.cpp:406] relu_conv12 <- conv12
I1210 17:36:02.415717 22232 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1210 17:36:02.416218 22232 net.cpp:122] Setting up relu_conv12
I1210 17:36:02.416218 22232 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1210 17:36:02.416218 22232 net.cpp:137] Memory required for data: 489934000
I1210 17:36:02.416218 22232 layer_factory.cpp:58] Creating layer poolcp6
I1210 17:36:02.416218 22232 net.cpp:84] Creating Layer poolcp6
I1210 17:36:02.416218 22232 net.cpp:406] poolcp6 <- conv12
I1210 17:36:02.416218 22232 net.cpp:380] poolcp6 -> poolcp6
I1210 17:36:02.416218 22232 net.cpp:122] Setting up poolcp6
I1210 17:36:02.416218 22232 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1210 17:36:02.416218 22232 net.cpp:137] Memory required for data: 489970000
I1210 17:36:02.416218 22232 layer_factory.cpp:58] Creating layer ip1
I1210 17:36:02.416218 22232 net.cpp:84] Creating Layer ip1
I1210 17:36:02.416218 22232 net.cpp:406] ip1 <- poolcp6
I1210 17:36:02.416218 22232 net.cpp:380] ip1 -> ip1
I1210 17:36:02.416218 22232 net.cpp:122] Setting up ip1
I1210 17:36:02.416218 22232 net.cpp:129] Top shape: 100 100 (10000)
I1210 17:36:02.416218 22232 net.cpp:137] Memory required for data: 490010000
I1210 17:36:02.416718 22232 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1210 17:36:02.416718 22232 net.cpp:84] Creating Layer ip1_ip1_0_split
I1210 17:36:02.416718 22232 net.cpp:406] ip1_ip1_0_split <- ip1
I1210 17:36:02.416718 22232 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1210 17:36:02.416718 22232 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1210 17:36:02.416718 22232 net.cpp:122] Setting up ip1_ip1_0_split
I1210 17:36:02.416718 22232 net.cpp:129] Top shape: 100 100 (10000)
I1210 17:36:02.416718 22232 net.cpp:129] Top shape: 100 100 (10000)
I1210 17:36:02.416718 22232 net.cpp:137] Memory required for data: 490090000
I1210 17:36:02.416718 22232 layer_factory.cpp:58] Creating layer accuracy
I1210 17:36:02.416718 22232 net.cpp:84] Creating Layer accuracy
I1210 17:36:02.416718 22232 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1210 17:36:02.416718 22232 net.cpp:406] accuracy <- label_cifar_1_split_0
I1210 17:36:02.416718 22232 net.cpp:380] accuracy -> accuracy
I1210 17:36:02.416718 22232 net.cpp:122] Setting up accuracy
I1210 17:36:02.416718 22232 net.cpp:129] Top shape: (1)
I1210 17:36:02.416718 22232 net.cpp:137] Memory required for data: 490090004
I1210 17:36:02.416718 22232 layer_factory.cpp:58] Creating layer loss
I1210 17:36:02.416718 22232 net.cpp:84] Creating Layer loss
I1210 17:36:02.416718 22232 net.cpp:406] loss <- ip1_ip1_0_split_1
I1210 17:36:02.416718 22232 net.cpp:406] loss <- label_cifar_1_split_1
I1210 17:36:02.416718 22232 net.cpp:380] loss -> loss
I1210 17:36:02.416718 22232 layer_factory.cpp:58] Creating layer loss
I1210 17:36:02.417218 22232 net.cpp:122] Setting up loss
I1210 17:36:02.417218 22232 net.cpp:129] Top shape: (1)
I1210 17:36:02.417218 22232 net.cpp:132]     with loss weight 1
I1210 17:36:02.417218 22232 net.cpp:137] Memory required for data: 490090008
I1210 17:36:02.417218 22232 net.cpp:198] loss needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:200] accuracy does not need backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] ip1 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] poolcp6 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] relu_conv12 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] scale_conv12 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] bn_conv12 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] conv12 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] relu_conv11 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] scale_conv11 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] bn_conv11 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] conv11 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] relu4_0 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] scale4_0 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] bn4_0 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] conv4_0 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] pool4_2 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] added_new_conv2 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] relu4_2 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] scale4_2 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] bn4_2 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] conv4_2 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] relu4_1 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] scale4_1 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] bn4_1 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] conv4_1 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] relu4 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] scale4 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] bn4 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] conv4 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] relu3_1 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] scale3_1 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] bn3_1 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] conv3_1 needs backward computation.
I1210 17:36:02.417218 22232 net.cpp:198] relu3 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] scale3 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] bn3 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] conv3 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] pool2_1 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] newconv_added1 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] relu2_2 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] scale2_2 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] bn2_2 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] conv2_2 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] relu2_1 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] scale2_1 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] bn2_1 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] conv2_1 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] relu2 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] scale2 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] bn2 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] conv2 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] relu1_0 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] scale1_0 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] bn1_0 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] conv1_0 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] relu1 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] scale1 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] bn1 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:198] conv1 needs backward computation.
I1210 17:36:02.417717 22232 net.cpp:200] label_cifar_1_split does not need backward computation.
I1210 17:36:02.417717 22232 net.cpp:200] cifar does not need backward computation.
I1210 17:36:02.417717 22232 net.cpp:242] This network produces output accuracy
I1210 17:36:02.417717 22232 net.cpp:242] This network produces output loss
I1210 17:36:02.417717 22232 net.cpp:255] Network initialization done.
I1210 17:36:02.417717 22232 solver.cpp:56] Solver scaffolding done.
I1210 17:36:02.422236 22232 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90000.solverstate
I1210 17:36:02.425735 22232 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90000.caffemodel
I1210 17:36:02.425735 22232 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1210 17:36:02.426236 22232 sgd_solver.cpp:318] SGDSolver: restoring history
I1210 17:36:02.429718 22232 caffe.cpp:249] Starting Optimization
I1210 17:36:02.429718 22232 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_v2_360k
I1210 17:36:02.429718 22232 solver.cpp:273] Learning Rate Policy: multistep
I1210 17:36:02.432935 22232 solver.cpp:330] Iteration 90000, Testing net (#0)
I1210 17:36:02.434926 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:36:03.845379 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:36:03.899381 22232 solver.cpp:397]     Test net output #0: accuracy = 0.5732
I1210 17:36:03.899381 22232 solver.cpp:397]     Test net output #1: loss = 1.65759 (* 1 = 1.65759 loss)
I1210 17:36:04.002902 22232 solver.cpp:218] Iteration 90000 (57259.7 iter/s, 1.57179s/100 iters), loss = 0.612613
I1210 17:36:04.002902 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 17:36:04.002902 22232 solver.cpp:237]     Train net output #1: loss = 0.612613 (* 1 = 0.612613 loss)
I1210 17:36:04.002902 22232 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1210 17:36:09.656898 22232 solver.cpp:218] Iteration 90100 (17.6893 iter/s, 5.65312s/100 iters), loss = 0.753202
I1210 17:36:09.656898 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 17:36:09.656898 22232 solver.cpp:237]     Train net output #1: loss = 0.753202 (* 1 = 0.753202 loss)
I1210 17:36:09.656898 22232 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1210 17:36:15.294472 22232 solver.cpp:218] Iteration 90200 (17.737 iter/s, 5.63792s/100 iters), loss = 0.621414
I1210 17:36:15.294472 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 17:36:15.294472 22232 solver.cpp:237]     Train net output #1: loss = 0.621414 (* 1 = 0.621414 loss)
I1210 17:36:15.295473 22232 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1210 17:36:20.908964 22232 solver.cpp:218] Iteration 90300 (17.8153 iter/s, 5.61315s/100 iters), loss = 0.78297
I1210 17:36:20.908964 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 17:36:20.908964 22232 solver.cpp:237]     Train net output #1: loss = 0.78297 (* 1 = 0.78297 loss)
I1210 17:36:20.908964 22232 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1210 17:36:26.553454 22232 solver.cpp:218] Iteration 90400 (17.7176 iter/s, 5.64411s/100 iters), loss = 0.868574
I1210 17:36:26.553454 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1210 17:36:26.553454 22232 solver.cpp:237]     Train net output #1: loss = 0.868574 (* 1 = 0.868574 loss)
I1210 17:36:26.553454 22232 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1210 17:36:31.975961 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:36:32.198060 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90500.caffemodel
I1210 17:36:32.218061 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_90500.solverstate
I1210 17:36:32.223065 22232 solver.cpp:330] Iteration 90500, Testing net (#0)
I1210 17:36:32.223564 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:36:33.584686 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:36:33.638201 22232 solver.cpp:397]     Test net output #0: accuracy = 0.5772
I1210 17:36:33.638201 22232 solver.cpp:397]     Test net output #1: loss = 1.68623 (* 1 = 1.68623 loss)
I1210 17:36:33.692292 22232 solver.cpp:218] Iteration 90500 (14.0084 iter/s, 7.13859s/100 iters), loss = 0.673816
I1210 17:36:33.692292 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 17:36:33.692292 22232 solver.cpp:237]     Train net output #1: loss = 0.673816 (* 1 = 0.673816 loss)
I1210 17:36:33.692292 22232 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1210 17:36:39.425225 22232 solver.cpp:218] Iteration 90600 (17.444 iter/s, 5.73262s/100 iters), loss = 0.684857
I1210 17:36:39.425225 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 17:36:39.425225 22232 solver.cpp:237]     Train net output #1: loss = 0.684857 (* 1 = 0.684857 loss)
I1210 17:36:39.425225 22232 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1210 17:36:45.059043 22232 solver.cpp:218] Iteration 90700 (17.7509 iter/s, 5.63353s/100 iters), loss = 0.638837
I1210 17:36:45.059043 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 17:36:45.059043 22232 solver.cpp:237]     Train net output #1: loss = 0.638837 (* 1 = 0.638837 loss)
I1210 17:36:45.059043 22232 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1210 17:36:50.699811 22232 solver.cpp:218] Iteration 90800 (17.7298 iter/s, 5.64023s/100 iters), loss = 0.811
I1210 17:36:50.699811 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 17:36:50.699811 22232 solver.cpp:237]     Train net output #1: loss = 0.811 (* 1 = 0.811 loss)
I1210 17:36:50.699811 22232 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1210 17:36:56.333315 22232 solver.cpp:218] Iteration 90900 (17.7525 iter/s, 5.63302s/100 iters), loss = 0.859573
I1210 17:36:56.333315 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1210 17:36:56.333315 22232 solver.cpp:237]     Train net output #1: loss = 0.859573 (* 1 = 0.859573 loss)
I1210 17:36:56.333315 22232 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1210 17:37:01.712021 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:37:01.934533 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_91000.caffemodel
I1210 17:37:01.950038 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_91000.solverstate
I1210 17:37:01.954542 22232 solver.cpp:330] Iteration 91000, Testing net (#0)
I1210 17:37:01.954542 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:37:03.338714 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:37:03.394722 22232 solver.cpp:397]     Test net output #0: accuracy = 0.5676
I1210 17:37:03.394722 22232 solver.cpp:397]     Test net output #1: loss = 1.70138 (* 1 = 1.70138 loss)
I1210 17:37:03.448223 22232 solver.cpp:218] Iteration 91000 (14.057 iter/s, 7.1139s/100 iters), loss = 0.609525
I1210 17:37:03.448223 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:37:03.448223 22232 solver.cpp:237]     Train net output #1: loss = 0.609525 (* 1 = 0.609525 loss)
I1210 17:37:03.448223 22232 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1210 17:37:09.157917 22232 solver.cpp:218] Iteration 91100 (17.513 iter/s, 5.71005s/100 iters), loss = 0.658854
I1210 17:37:09.157917 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 17:37:09.157917 22232 solver.cpp:237]     Train net output #1: loss = 0.658854 (* 1 = 0.658854 loss)
I1210 17:37:09.157917 22232 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1210 17:37:14.843900 22232 solver.cpp:218] Iteration 91200 (17.5903 iter/s, 5.68494s/100 iters), loss = 0.65335
I1210 17:37:14.843900 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 17:37:14.843900 22232 solver.cpp:237]     Train net output #1: loss = 0.65335 (* 1 = 0.65335 loss)
I1210 17:37:14.843900 22232 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1210 17:37:20.486850 22232 solver.cpp:218] Iteration 91300 (17.7226 iter/s, 5.64252s/100 iters), loss = 0.710681
I1210 17:37:20.486850 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 17:37:20.486850 22232 solver.cpp:237]     Train net output #1: loss = 0.710681 (* 1 = 0.710681 loss)
I1210 17:37:20.486850 22232 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1210 17:37:26.130270 22232 solver.cpp:218] Iteration 91400 (17.7208 iter/s, 5.64308s/100 iters), loss = 0.881355
I1210 17:37:26.130270 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1210 17:37:26.130270 22232 solver.cpp:237]     Train net output #1: loss = 0.881355 (* 1 = 0.881355 loss)
I1210 17:37:26.130270 22232 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1210 17:37:31.482952 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:37:31.705976 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_91500.caffemodel
I1210 17:37:31.719975 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_91500.solverstate
I1210 17:37:31.724975 22232 solver.cpp:330] Iteration 91500, Testing net (#0)
I1210 17:37:31.724975 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:37:33.091068 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:37:33.145071 22232 solver.cpp:397]     Test net output #0: accuracy = 0.5915
I1210 17:37:33.145071 22232 solver.cpp:397]     Test net output #1: loss = 1.62062 (* 1 = 1.62062 loss)
I1210 17:37:33.199074 22232 solver.cpp:218] Iteration 91500 (14.1472 iter/s, 7.06855s/100 iters), loss = 0.538153
I1210 17:37:33.199074 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 17:37:33.199074 22232 solver.cpp:237]     Train net output #1: loss = 0.538153 (* 1 = 0.538153 loss)
I1210 17:37:33.199074 22232 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1210 17:37:38.831615 22232 solver.cpp:218] Iteration 91600 (17.7545 iter/s, 5.63239s/100 iters), loss = 0.784689
I1210 17:37:38.831615 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 17:37:38.832617 22232 solver.cpp:237]     Train net output #1: loss = 0.784689 (* 1 = 0.784689 loss)
I1210 17:37:38.832617 22232 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1210 17:37:44.462105 22232 solver.cpp:218] Iteration 91700 (17.7643 iter/s, 5.62927s/100 iters), loss = 0.556847
I1210 17:37:44.462105 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:37:44.462105 22232 solver.cpp:237]     Train net output #1: loss = 0.556847 (* 1 = 0.556847 loss)
I1210 17:37:44.462105 22232 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1210 17:37:50.086817 22232 solver.cpp:218] Iteration 91800 (17.7786 iter/s, 5.62473s/100 iters), loss = 0.77394
I1210 17:37:50.086817 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 17:37:50.086817 22232 solver.cpp:237]     Train net output #1: loss = 0.77394 (* 1 = 0.77394 loss)
I1210 17:37:50.086817 22232 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1210 17:37:55.716226 22232 solver.cpp:218] Iteration 91900 (17.7662 iter/s, 5.62866s/100 iters), loss = 0.88596
I1210 17:37:55.716226 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1210 17:37:55.716226 22232 solver.cpp:237]     Train net output #1: loss = 0.88596 (* 1 = 0.88596 loss)
I1210 17:37:55.716226 22232 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1210 17:38:01.067651 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:38:01.290655 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_92000.caffemodel
I1210 17:38:01.305672 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_92000.solverstate
I1210 17:38:01.310665 22232 solver.cpp:330] Iteration 92000, Testing net (#0)
I1210 17:38:01.310665 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:38:02.674845 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:38:02.729845 22232 solver.cpp:397]     Test net output #0: accuracy = 0.5521
I1210 17:38:02.729845 22232 solver.cpp:397]     Test net output #1: loss = 1.88298 (* 1 = 1.88298 loss)
I1210 17:38:02.783849 22232 solver.cpp:218] Iteration 92000 (14.1502 iter/s, 7.06706s/100 iters), loss = 0.554184
I1210 17:38:02.783849 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 17:38:02.783849 22232 solver.cpp:237]     Train net output #1: loss = 0.554184 (* 1 = 0.554184 loss)
I1210 17:38:02.783849 22232 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1210 17:38:08.416290 22232 solver.cpp:218] Iteration 92100 (17.7535 iter/s, 5.6327s/100 iters), loss = 0.702908
I1210 17:38:08.417290 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 17:38:08.417290 22232 solver.cpp:237]     Train net output #1: loss = 0.702908 (* 1 = 0.702908 loss)
I1210 17:38:08.417290 22232 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1210 17:38:14.058709 22232 solver.cpp:218] Iteration 92200 (17.7267 iter/s, 5.64121s/100 iters), loss = 0.68578
I1210 17:38:14.058709 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 17:38:14.058709 22232 solver.cpp:237]     Train net output #1: loss = 0.68578 (* 1 = 0.68578 loss)
I1210 17:38:14.058709 22232 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1210 17:38:19.699004 22232 solver.cpp:218] Iteration 92300 (17.7299 iter/s, 5.64018s/100 iters), loss = 0.816914
I1210 17:38:19.699004 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1210 17:38:19.699004 22232 solver.cpp:237]     Train net output #1: loss = 0.816914 (* 1 = 0.816914 loss)
I1210 17:38:19.699004 22232 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1210 17:38:25.334378 22232 solver.cpp:218] Iteration 92400 (17.7458 iter/s, 5.63515s/100 iters), loss = 0.656563
I1210 17:38:25.335378 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1210 17:38:25.335378 22232 solver.cpp:237]     Train net output #1: loss = 0.656563 (* 1 = 0.656563 loss)
I1210 17:38:25.335378 22232 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1210 17:38:30.690461 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:38:30.911028 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_92500.caffemodel
I1210 17:38:30.925027 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_92500.solverstate
I1210 17:38:30.930034 22232 solver.cpp:330] Iteration 92500, Testing net (#0)
I1210 17:38:30.930034 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:38:32.296097 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:38:32.350080 22232 solver.cpp:397]     Test net output #0: accuracy = 0.566
I1210 17:38:32.350080 22232 solver.cpp:397]     Test net output #1: loss = 1.68029 (* 1 = 1.68029 loss)
I1210 17:38:32.402660 22232 solver.cpp:218] Iteration 92500 (14.1489 iter/s, 7.06767s/100 iters), loss = 0.533479
I1210 17:38:32.402660 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 17:38:32.402660 22232 solver.cpp:237]     Train net output #1: loss = 0.533479 (* 1 = 0.533479 loss)
I1210 17:38:32.402660 22232 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1210 17:38:38.044260 22232 solver.cpp:218] Iteration 92600 (17.7282 iter/s, 5.64072s/100 iters), loss = 0.706958
I1210 17:38:38.044260 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 17:38:38.044260 22232 solver.cpp:237]     Train net output #1: loss = 0.706958 (* 1 = 0.706958 loss)
I1210 17:38:38.044260 22232 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1210 17:38:43.679950 22232 solver.cpp:218] Iteration 92700 (17.7459 iter/s, 5.63512s/100 iters), loss = 0.680505
I1210 17:38:43.679950 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 17:38:43.679950 22232 solver.cpp:237]     Train net output #1: loss = 0.680505 (* 1 = 0.680505 loss)
I1210 17:38:43.679950 22232 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1210 17:38:49.323853 22232 solver.cpp:218] Iteration 92800 (17.7202 iter/s, 5.64327s/100 iters), loss = 0.749823
I1210 17:38:49.323853 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1210 17:38:49.323853 22232 solver.cpp:237]     Train net output #1: loss = 0.749823 (* 1 = 0.749823 loss)
I1210 17:38:49.323853 22232 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1210 17:38:54.953333 22232 solver.cpp:218] Iteration 92900 (17.7642 iter/s, 5.6293s/100 iters), loss = 0.899799
I1210 17:38:54.953333 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1210 17:38:54.953333 22232 solver.cpp:237]     Train net output #1: loss = 0.899799 (* 1 = 0.899799 loss)
I1210 17:38:54.953333 22232 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1210 17:39:00.314409 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:39:00.536283 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_93000.caffemodel
I1210 17:39:00.551283 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_93000.solverstate
I1210 17:39:00.556282 22232 solver.cpp:330] Iteration 93000, Testing net (#0)
I1210 17:39:00.556282 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:39:01.918382 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:39:01.971907 22232 solver.cpp:397]     Test net output #0: accuracy = 0.5715
I1210 17:39:01.971907 22232 solver.cpp:397]     Test net output #1: loss = 1.6895 (* 1 = 1.6895 loss)
I1210 17:39:02.026547 22232 solver.cpp:218] Iteration 93000 (14.1384 iter/s, 7.07294s/100 iters), loss = 0.557819
I1210 17:39:02.026547 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:39:02.026547 22232 solver.cpp:237]     Train net output #1: loss = 0.557819 (* 1 = 0.557819 loss)
I1210 17:39:02.026547 22232 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1210 17:39:07.652634 22232 solver.cpp:218] Iteration 93100 (17.7772 iter/s, 5.62517s/100 iters), loss = 0.804969
I1210 17:39:07.652634 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 17:39:07.652634 22232 solver.cpp:237]     Train net output #1: loss = 0.804969 (* 1 = 0.804969 loss)
I1210 17:39:07.652634 22232 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1210 17:39:13.277088 22232 solver.cpp:218] Iteration 93200 (17.7818 iter/s, 5.62373s/100 iters), loss = 0.587186
I1210 17:39:13.277088 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 17:39:13.277088 22232 solver.cpp:237]     Train net output #1: loss = 0.587186 (* 1 = 0.587186 loss)
I1210 17:39:13.277088 22232 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1210 17:39:18.906116 22232 solver.cpp:218] Iteration 93300 (17.7665 iter/s, 5.62858s/100 iters), loss = 0.798727
I1210 17:39:18.906116 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1210 17:39:18.906116 22232 solver.cpp:237]     Train net output #1: loss = 0.798727 (* 1 = 0.798727 loss)
I1210 17:39:18.906116 22232 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1210 17:39:24.530578 22232 solver.cpp:218] Iteration 93400 (17.7801 iter/s, 5.62426s/100 iters), loss = 0.828764
I1210 17:39:24.530578 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1210 17:39:24.530578 22232 solver.cpp:237]     Train net output #1: loss = 0.828764 (* 1 = 0.828764 loss)
I1210 17:39:24.530578 22232 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1210 17:39:29.889892 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:39:30.109938 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_93500.caffemodel
I1210 17:39:30.125946 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_93500.solverstate
I1210 17:39:30.130942 22232 solver.cpp:330] Iteration 93500, Testing net (#0)
I1210 17:39:30.130942 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:39:31.497236 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:39:31.550240 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6031
I1210 17:39:31.550240 22232 solver.cpp:397]     Test net output #1: loss = 1.49948 (* 1 = 1.49948 loss)
I1210 17:39:31.604244 22232 solver.cpp:218] Iteration 93500 (14.1385 iter/s, 7.07291s/100 iters), loss = 0.529471
I1210 17:39:31.604244 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 17:39:31.604244 22232 solver.cpp:237]     Train net output #1: loss = 0.529471 (* 1 = 0.529471 loss)
I1210 17:39:31.604244 22232 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1210 17:39:37.226574 22232 solver.cpp:218] Iteration 93600 (17.7866 iter/s, 5.6222s/100 iters), loss = 0.629314
I1210 17:39:37.226574 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 17:39:37.226574 22232 solver.cpp:237]     Train net output #1: loss = 0.629314 (* 1 = 0.629314 loss)
I1210 17:39:37.226574 22232 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1210 17:39:42.851049 22232 solver.cpp:218] Iteration 93700 (17.7799 iter/s, 5.62432s/100 iters), loss = 0.568479
I1210 17:39:42.851049 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 17:39:42.851049 22232 solver.cpp:237]     Train net output #1: loss = 0.568479 (* 1 = 0.568479 loss)
I1210 17:39:42.852075 22232 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1210 17:39:48.480931 22232 solver.cpp:218] Iteration 93800 (17.7655 iter/s, 5.62888s/100 iters), loss = 0.818796
I1210 17:39:48.480931 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1210 17:39:48.480931 22232 solver.cpp:237]     Train net output #1: loss = 0.818796 (* 1 = 0.818796 loss)
I1210 17:39:48.480931 22232 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1210 17:39:54.101404 22232 solver.cpp:218] Iteration 93900 (17.7947 iter/s, 5.61964s/100 iters), loss = 0.691658
I1210 17:39:54.101404 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 17:39:54.101404 22232 solver.cpp:237]     Train net output #1: loss = 0.691658 (* 1 = 0.691658 loss)
I1210 17:39:54.101404 22232 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1210 17:39:59.453452 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:39:59.675462 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_94000.caffemodel
I1210 17:39:59.690464 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_94000.solverstate
I1210 17:39:59.695463 22232 solver.cpp:330] Iteration 94000, Testing net (#0)
I1210 17:39:59.695463 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:40:01.074635 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:40:01.128644 22232 solver.cpp:397]     Test net output #0: accuracy = 0.5959
I1210 17:40:01.128644 22232 solver.cpp:397]     Test net output #1: loss = 1.56321 (* 1 = 1.56321 loss)
I1210 17:40:01.183642 22232 solver.cpp:218] Iteration 94000 (14.1206 iter/s, 7.08184s/100 iters), loss = 0.650231
I1210 17:40:01.183642 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 17:40:01.183642 22232 solver.cpp:237]     Train net output #1: loss = 0.650231 (* 1 = 0.650231 loss)
I1210 17:40:01.183642 22232 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1210 17:40:06.837136 22232 solver.cpp:218] Iteration 94100 (17.6886 iter/s, 5.65337s/100 iters), loss = 0.649537
I1210 17:40:06.837136 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 17:40:06.837136 22232 solver.cpp:237]     Train net output #1: loss = 0.649537 (* 1 = 0.649537 loss)
I1210 17:40:06.837136 22232 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1210 17:40:12.492660 22232 solver.cpp:218] Iteration 94200 (17.6836 iter/s, 5.65496s/100 iters), loss = 0.58994
I1210 17:40:12.492660 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 17:40:12.492660 22232 solver.cpp:237]     Train net output #1: loss = 0.58994 (* 1 = 0.58994 loss)
I1210 17:40:12.492660 22232 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1210 17:40:18.139051 22232 solver.cpp:218] Iteration 94300 (17.7113 iter/s, 5.64612s/100 iters), loss = 0.742315
I1210 17:40:18.139051 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 17:40:18.139051 22232 solver.cpp:237]     Train net output #1: loss = 0.742315 (* 1 = 0.742315 loss)
I1210 17:40:18.139051 22232 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1210 17:40:23.790462 22232 solver.cpp:218] Iteration 94400 (17.6975 iter/s, 5.65051s/100 iters), loss = 0.768848
I1210 17:40:23.790462 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1210 17:40:23.790462 22232 solver.cpp:237]     Train net output #1: loss = 0.768848 (* 1 = 0.768848 loss)
I1210 17:40:23.790462 22232 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1210 17:40:29.162870 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:40:29.386883 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_94500.caffemodel
I1210 17:40:29.401886 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_94500.solverstate
I1210 17:40:29.406386 22232 solver.cpp:330] Iteration 94500, Testing net (#0)
I1210 17:40:29.406386 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:40:30.773977 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:40:30.827983 22232 solver.cpp:397]     Test net output #0: accuracy = 0.5882
I1210 17:40:30.827983 22232 solver.cpp:397]     Test net output #1: loss = 1.60336 (* 1 = 1.60336 loss)
I1210 17:40:30.880982 22232 solver.cpp:218] Iteration 94500 (14.1044 iter/s, 7.08998s/100 iters), loss = 0.562968
I1210 17:40:30.880982 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 17:40:30.880982 22232 solver.cpp:237]     Train net output #1: loss = 0.562968 (* 1 = 0.562968 loss)
I1210 17:40:30.880982 22232 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1210 17:40:36.526433 22232 solver.cpp:218] Iteration 94600 (17.7158 iter/s, 5.64469s/100 iters), loss = 0.691068
I1210 17:40:36.526433 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1210 17:40:36.526433 22232 solver.cpp:237]     Train net output #1: loss = 0.691068 (* 1 = 0.691068 loss)
I1210 17:40:36.526433 22232 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1210 17:40:42.164839 22232 solver.cpp:218] Iteration 94700 (17.7341 iter/s, 5.63885s/100 iters), loss = 0.581097
I1210 17:40:42.164839 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 17:40:42.164839 22232 solver.cpp:237]     Train net output #1: loss = 0.581097 (* 1 = 0.581097 loss)
I1210 17:40:42.164839 22232 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1210 17:40:47.802266 22232 solver.cpp:218] Iteration 94800 (17.7422 iter/s, 5.63627s/100 iters), loss = 0.795552
I1210 17:40:47.802266 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1210 17:40:47.802266 22232 solver.cpp:237]     Train net output #1: loss = 0.795552 (* 1 = 0.795552 loss)
I1210 17:40:47.802266 22232 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1210 17:40:53.440676 22232 solver.cpp:218] Iteration 94900 (17.735 iter/s, 5.63858s/100 iters), loss = 0.720168
I1210 17:40:53.440676 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 17:40:53.440676 22232 solver.cpp:237]     Train net output #1: loss = 0.720168 (* 1 = 0.720168 loss)
I1210 17:40:53.440676 22232 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1210 17:40:58.801281 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:40:59.023674 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_95000.caffemodel
I1210 17:40:59.041669 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_95000.solverstate
I1210 17:40:59.046672 22232 solver.cpp:330] Iteration 95000, Testing net (#0)
I1210 17:40:59.046672 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:41:00.414306 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:41:00.467308 22232 solver.cpp:397]     Test net output #0: accuracy = 0.5805
I1210 17:41:00.467308 22232 solver.cpp:397]     Test net output #1: loss = 1.66154 (* 1 = 1.66154 loss)
I1210 17:41:00.521095 22232 solver.cpp:218] Iteration 95000 (14.1258 iter/s, 7.07924s/100 iters), loss = 0.53904
I1210 17:41:00.521095 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 17:41:00.521095 22232 solver.cpp:237]     Train net output #1: loss = 0.53904 (* 1 = 0.53904 loss)
I1210 17:41:00.521095 22232 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1210 17:41:00.521095 22232 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1210 17:41:06.158255 22232 solver.cpp:218] Iteration 95100 (17.7392 iter/s, 5.63723s/100 iters), loss = 0.602189
I1210 17:41:06.158255 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1210 17:41:06.158255 22232 solver.cpp:237]     Train net output #1: loss = 0.602189 (* 1 = 0.602189 loss)
I1210 17:41:06.158255 22232 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1210 17:41:11.793485 22232 solver.cpp:218] Iteration 95200 (17.7472 iter/s, 5.63468s/100 iters), loss = 0.472463
I1210 17:41:11.793485 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 17:41:11.793485 22232 solver.cpp:237]     Train net output #1: loss = 0.472463 (* 1 = 0.472463 loss)
I1210 17:41:11.793485 22232 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1210 17:41:17.429424 22232 solver.cpp:218] Iteration 95300 (17.7452 iter/s, 5.63533s/100 iters), loss = 0.584924
I1210 17:41:17.429424 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 17:41:17.429424 22232 solver.cpp:237]     Train net output #1: loss = 0.584924 (* 1 = 0.584924 loss)
I1210 17:41:17.429424 22232 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1210 17:41:23.077213 22232 solver.cpp:218] Iteration 95400 (17.7081 iter/s, 5.64714s/100 iters), loss = 0.544586
I1210 17:41:23.077213 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 17:41:23.077213 22232 solver.cpp:237]     Train net output #1: loss = 0.544586 (* 1 = 0.544586 loss)
I1210 17:41:23.077213 22232 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1210 17:41:28.437425 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:41:28.659437 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_95500.caffemodel
I1210 17:41:28.674438 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_95500.solverstate
I1210 17:41:28.678438 22232 solver.cpp:330] Iteration 95500, Testing net (#0)
I1210 17:41:28.679440 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:41:30.048887 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:41:30.103886 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1210 17:41:30.103886 22232 solver.cpp:397]     Test net output #1: loss = 1.18268 (* 1 = 1.18268 loss)
I1210 17:41:30.156890 22232 solver.cpp:218] Iteration 95500 (14.1252 iter/s, 7.07954s/100 iters), loss = 0.486811
I1210 17:41:30.156890 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:41:30.156890 22232 solver.cpp:237]     Train net output #1: loss = 0.486811 (* 1 = 0.486811 loss)
I1210 17:41:30.156890 22232 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1210 17:41:35.796352 22232 solver.cpp:218] Iteration 95600 (17.7332 iter/s, 5.63915s/100 iters), loss = 0.52508
I1210 17:41:35.796352 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 17:41:35.796352 22232 solver.cpp:237]     Train net output #1: loss = 0.52508 (* 1 = 0.52508 loss)
I1210 17:41:35.796352 22232 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1210 17:41:41.430769 22232 solver.cpp:218] Iteration 95700 (17.7516 iter/s, 5.63331s/100 iters), loss = 0.396191
I1210 17:41:41.430769 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:41:41.430769 22232 solver.cpp:237]     Train net output #1: loss = 0.396191 (* 1 = 0.396191 loss)
I1210 17:41:41.430769 22232 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1210 17:41:47.057955 22232 solver.cpp:218] Iteration 95800 (17.7707 iter/s, 5.62723s/100 iters), loss = 0.513241
I1210 17:41:47.057955 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 17:41:47.057955 22232 solver.cpp:237]     Train net output #1: loss = 0.513241 (* 1 = 0.513241 loss)
I1210 17:41:47.057955 22232 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1210 17:41:52.697441 22232 solver.cpp:218] Iteration 95900 (17.7346 iter/s, 5.63869s/100 iters), loss = 0.517923
I1210 17:41:52.697441 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 17:41:52.697441 22232 solver.cpp:237]     Train net output #1: loss = 0.517923 (* 1 = 0.517923 loss)
I1210 17:41:52.697441 22232 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1210 17:41:58.048833 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:41:58.271843 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_96000.caffemodel
I1210 17:41:58.286844 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_96000.solverstate
I1210 17:41:58.292845 22232 solver.cpp:330] Iteration 96000, Testing net (#0)
I1210 17:41:58.292845 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:41:59.661929 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:41:59.714943 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6786
I1210 17:41:59.714943 22232 solver.cpp:397]     Test net output #1: loss = 1.18227 (* 1 = 1.18227 loss)
I1210 17:41:59.767931 22232 solver.cpp:218] Iteration 96000 (14.143 iter/s, 7.07064s/100 iters), loss = 0.393801
I1210 17:41:59.768941 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:41:59.768941 22232 solver.cpp:237]     Train net output #1: loss = 0.393801 (* 1 = 0.393801 loss)
I1210 17:41:59.768941 22232 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1210 17:42:05.401409 22232 solver.cpp:218] Iteration 96100 (17.7554 iter/s, 5.6321s/100 iters), loss = 0.524083
I1210 17:42:05.401409 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 17:42:05.401409 22232 solver.cpp:237]     Train net output #1: loss = 0.524083 (* 1 = 0.524083 loss)
I1210 17:42:05.401409 22232 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1210 17:42:11.047857 22232 solver.cpp:218] Iteration 96200 (17.7098 iter/s, 5.64659s/100 iters), loss = 0.421904
I1210 17:42:11.047857 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:42:11.047857 22232 solver.cpp:237]     Train net output #1: loss = 0.421904 (* 1 = 0.421904 loss)
I1210 17:42:11.047857 22232 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1210 17:42:16.682308 22232 solver.cpp:218] Iteration 96300 (17.7506 iter/s, 5.63361s/100 iters), loss = 0.466918
I1210 17:42:16.682308 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 17:42:16.682308 22232 solver.cpp:237]     Train net output #1: loss = 0.466918 (* 1 = 0.466918 loss)
I1210 17:42:16.682308 22232 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1210 17:42:22.325740 22232 solver.cpp:218] Iteration 96400 (17.7191 iter/s, 5.64364s/100 iters), loss = 0.492573
I1210 17:42:22.326740 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 17:42:22.326740 22232 solver.cpp:237]     Train net output #1: loss = 0.492573 (* 1 = 0.492573 loss)
I1210 17:42:22.326740 22232 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1210 17:42:27.694200 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:42:27.917212 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_96500.caffemodel
I1210 17:42:27.931216 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_96500.solverstate
I1210 17:42:27.936218 22232 solver.cpp:330] Iteration 96500, Testing net (#0)
I1210 17:42:27.936218 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:42:29.303326 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:42:29.358332 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6824
I1210 17:42:29.358332 22232 solver.cpp:397]     Test net output #1: loss = 1.17474 (* 1 = 1.17474 loss)
I1210 17:42:29.411345 22232 solver.cpp:218] Iteration 96500 (14.1156 iter/s, 7.08435s/100 iters), loss = 0.435759
I1210 17:42:29.411345 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 17:42:29.411345 22232 solver.cpp:237]     Train net output #1: loss = 0.435759 (* 1 = 0.435759 loss)
I1210 17:42:29.411345 22232 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1210 17:42:35.044128 22232 solver.cpp:218] Iteration 96600 (17.7552 iter/s, 5.63214s/100 iters), loss = 0.468627
I1210 17:42:35.044128 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:42:35.044128 22232 solver.cpp:237]     Train net output #1: loss = 0.468627 (* 1 = 0.468627 loss)
I1210 17:42:35.044128 22232 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1210 17:42:40.674187 22232 solver.cpp:218] Iteration 96700 (17.7635 iter/s, 5.62953s/100 iters), loss = 0.351896
I1210 17:42:40.674187 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:42:40.674187 22232 solver.cpp:237]     Train net output #1: loss = 0.351896 (* 1 = 0.351896 loss)
I1210 17:42:40.674187 22232 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1210 17:42:46.293630 22232 solver.cpp:218] Iteration 96800 (17.7946 iter/s, 5.61968s/100 iters), loss = 0.444912
I1210 17:42:46.293630 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:42:46.293630 22232 solver.cpp:237]     Train net output #1: loss = 0.444912 (* 1 = 0.444912 loss)
I1210 17:42:46.293630 22232 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1210 17:42:51.917426 22232 solver.cpp:218] Iteration 96900 (17.7848 iter/s, 5.62277s/100 iters), loss = 0.499866
I1210 17:42:51.917426 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 17:42:51.917426 22232 solver.cpp:237]     Train net output #1: loss = 0.499866 (* 1 = 0.499866 loss)
I1210 17:42:51.917426 22232 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1210 17:42:57.266902 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:42:57.488916 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_97000.caffemodel
I1210 17:42:57.503916 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_97000.solverstate
I1210 17:42:57.507916 22232 solver.cpp:330] Iteration 97000, Testing net (#0)
I1210 17:42:57.508916 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:42:58.876068 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:42:58.929070 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6842
I1210 17:42:58.929070 22232 solver.cpp:397]     Test net output #1: loss = 1.16724 (* 1 = 1.16724 loss)
I1210 17:42:58.983078 22232 solver.cpp:218] Iteration 97000 (14.1526 iter/s, 7.06586s/100 iters), loss = 0.413172
I1210 17:42:58.983078 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:42:58.983078 22232 solver.cpp:237]     Train net output #1: loss = 0.413172 (* 1 = 0.413172 loss)
I1210 17:42:58.983078 22232 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1210 17:43:04.619488 22232 solver.cpp:218] Iteration 97100 (17.7447 iter/s, 5.63547s/100 iters), loss = 0.524148
I1210 17:43:04.619488 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 17:43:04.619488 22232 solver.cpp:237]     Train net output #1: loss = 0.524148 (* 1 = 0.524148 loss)
I1210 17:43:04.619488 22232 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1210 17:43:10.258239 22232 solver.cpp:218] Iteration 97200 (17.7354 iter/s, 5.63843s/100 iters), loss = 0.315822
I1210 17:43:10.258239 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:43:10.258239 22232 solver.cpp:237]     Train net output #1: loss = 0.315822 (* 1 = 0.315822 loss)
I1210 17:43:10.258239 22232 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1210 17:43:15.890599 22232 solver.cpp:218] Iteration 97300 (17.7558 iter/s, 5.63196s/100 iters), loss = 0.42002
I1210 17:43:15.890599 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 17:43:15.890599 22232 solver.cpp:237]     Train net output #1: loss = 0.42002 (* 1 = 0.42002 loss)
I1210 17:43:15.890599 22232 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1210 17:43:21.525056 22232 solver.cpp:218] Iteration 97400 (17.7485 iter/s, 5.63427s/100 iters), loss = 0.445884
I1210 17:43:21.525056 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 17:43:21.525056 22232 solver.cpp:237]     Train net output #1: loss = 0.445884 (* 1 = 0.445884 loss)
I1210 17:43:21.525056 22232 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1210 17:43:26.886103 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:43:27.108161 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_97500.caffemodel
I1210 17:43:27.122160 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_97500.solverstate
I1210 17:43:27.127164 22232 solver.cpp:330] Iteration 97500, Testing net (#0)
I1210 17:43:27.127164 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:43:28.494565 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:43:28.547569 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6838
I1210 17:43:28.547569 22232 solver.cpp:397]     Test net output #1: loss = 1.17442 (* 1 = 1.17442 loss)
I1210 17:43:28.600575 22232 solver.cpp:218] Iteration 97500 (14.1354 iter/s, 7.07444s/100 iters), loss = 0.376457
I1210 17:43:28.600575 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:43:28.600575 22232 solver.cpp:237]     Train net output #1: loss = 0.376457 (* 1 = 0.376457 loss)
I1210 17:43:28.600575 22232 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1210 17:43:34.246786 22232 solver.cpp:218] Iteration 97600 (17.7126 iter/s, 5.64569s/100 iters), loss = 0.536443
I1210 17:43:34.246786 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 17:43:34.246786 22232 solver.cpp:237]     Train net output #1: loss = 0.536443 (* 1 = 0.536443 loss)
I1210 17:43:34.246786 22232 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1210 17:43:39.890390 22232 solver.cpp:218] Iteration 97700 (17.7215 iter/s, 5.64288s/100 iters), loss = 0.352461
I1210 17:43:39.890390 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:43:39.890390 22232 solver.cpp:237]     Train net output #1: loss = 0.352461 (* 1 = 0.352461 loss)
I1210 17:43:39.890390 22232 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1210 17:43:45.531769 22232 solver.cpp:218] Iteration 97800 (17.7255 iter/s, 5.64158s/100 iters), loss = 0.443306
I1210 17:43:45.531769 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:43:45.531769 22232 solver.cpp:237]     Train net output #1: loss = 0.443306 (* 1 = 0.443306 loss)
I1210 17:43:45.531769 22232 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1210 17:43:51.172132 22232 solver.cpp:218] Iteration 97900 (17.731 iter/s, 5.63984s/100 iters), loss = 0.472069
I1210 17:43:51.172132 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 17:43:51.172132 22232 solver.cpp:237]     Train net output #1: loss = 0.472069 (* 1 = 0.472069 loss)
I1210 17:43:51.172132 22232 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1210 17:43:56.532675 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:43:56.755190 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_98000.caffemodel
I1210 17:43:56.769695 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_98000.solverstate
I1210 17:43:56.774694 22232 solver.cpp:330] Iteration 98000, Testing net (#0)
I1210 17:43:56.774694 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:43:58.144819 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:43:58.198823 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6875
I1210 17:43:58.198823 22232 solver.cpp:397]     Test net output #1: loss = 1.17753 (* 1 = 1.17753 loss)
I1210 17:43:58.252826 22232 solver.cpp:218] Iteration 98000 (14.1245 iter/s, 7.07989s/100 iters), loss = 0.332072
I1210 17:43:58.252826 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:43:58.252826 22232 solver.cpp:237]     Train net output #1: loss = 0.332072 (* 1 = 0.332072 loss)
I1210 17:43:58.252826 22232 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1210 17:44:03.889384 22232 solver.cpp:218] Iteration 98100 (17.7428 iter/s, 5.63609s/100 iters), loss = 0.465707
I1210 17:44:03.889384 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:44:03.889384 22232 solver.cpp:237]     Train net output #1: loss = 0.465707 (* 1 = 0.465707 loss)
I1210 17:44:03.889384 22232 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1210 17:44:09.513908 22232 solver.cpp:218] Iteration 98200 (17.7794 iter/s, 5.6245s/100 iters), loss = 0.306777
I1210 17:44:09.513908 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:44:09.513908 22232 solver.cpp:237]     Train net output #1: loss = 0.306777 (* 1 = 0.306777 loss)
I1210 17:44:09.513908 22232 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1210 17:44:15.145328 22232 solver.cpp:218] Iteration 98300 (17.7577 iter/s, 5.63136s/100 iters), loss = 0.516605
I1210 17:44:15.145328 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 17:44:15.146328 22232 solver.cpp:237]     Train net output #1: loss = 0.516605 (* 1 = 0.516605 loss)
I1210 17:44:15.146328 22232 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1210 17:44:20.774713 22232 solver.cpp:218] Iteration 98400 (17.7673 iter/s, 5.62831s/100 iters), loss = 0.482142
I1210 17:44:20.774713 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:44:20.774713 22232 solver.cpp:237]     Train net output #1: loss = 0.482142 (* 1 = 0.482142 loss)
I1210 17:44:20.774713 22232 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1210 17:44:26.133510 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:44:26.355080 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_98500.caffemodel
I1210 17:44:26.369618 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_98500.solverstate
I1210 17:44:26.373612 22232 solver.cpp:330] Iteration 98500, Testing net (#0)
I1210 17:44:26.374613 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:44:27.741062 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:44:27.795086 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6816
I1210 17:44:27.795086 22232 solver.cpp:397]     Test net output #1: loss = 1.18455 (* 1 = 1.18455 loss)
I1210 17:44:27.849102 22232 solver.cpp:218] Iteration 98500 (14.1357 iter/s, 7.07429s/100 iters), loss = 0.325932
I1210 17:44:27.849102 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 17:44:27.849102 22232 solver.cpp:237]     Train net output #1: loss = 0.325932 (* 1 = 0.325932 loss)
I1210 17:44:27.849102 22232 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1210 17:44:33.461073 22232 solver.cpp:218] Iteration 98600 (17.8222 iter/s, 5.61099s/100 iters), loss = 0.441515
I1210 17:44:33.461073 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 17:44:33.461073 22232 solver.cpp:237]     Train net output #1: loss = 0.441515 (* 1 = 0.441515 loss)
I1210 17:44:33.461073 22232 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1210 17:44:39.087149 22232 solver.cpp:218] Iteration 98700 (17.7748 iter/s, 5.62596s/100 iters), loss = 0.342794
I1210 17:44:39.087149 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:44:39.087149 22232 solver.cpp:237]     Train net output #1: loss = 0.342794 (* 1 = 0.342794 loss)
I1210 17:44:39.087149 22232 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1210 17:44:44.712546 22232 solver.cpp:218] Iteration 98800 (17.7782 iter/s, 5.62488s/100 iters), loss = 0.468785
I1210 17:44:44.712546 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:44:44.712546 22232 solver.cpp:237]     Train net output #1: loss = 0.468785 (* 1 = 0.468785 loss)
I1210 17:44:44.712546 22232 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1210 17:44:50.327105 22232 solver.cpp:218] Iteration 98900 (17.8131 iter/s, 5.61384s/100 iters), loss = 0.448412
I1210 17:44:50.327105 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:44:50.327105 22232 solver.cpp:237]     Train net output #1: loss = 0.448412 (* 1 = 0.448412 loss)
I1210 17:44:50.327105 22232 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1210 17:44:55.678530 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:44:55.900547 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_99000.caffemodel
I1210 17:44:55.915545 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_99000.solverstate
I1210 17:44:55.920547 22232 solver.cpp:330] Iteration 99000, Testing net (#0)
I1210 17:44:55.920547 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:44:57.291697 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:44:57.344696 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6867
I1210 17:44:57.344696 22232 solver.cpp:397]     Test net output #1: loss = 1.18344 (* 1 = 1.18344 loss)
I1210 17:44:57.400703 22232 solver.cpp:218] Iteration 99000 (14.1375 iter/s, 7.07339s/100 iters), loss = 0.318646
I1210 17:44:57.400703 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:44:57.400703 22232 solver.cpp:237]     Train net output #1: loss = 0.318646 (* 1 = 0.318646 loss)
I1210 17:44:57.400703 22232 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1210 17:45:03.053141 22232 solver.cpp:218] Iteration 99100 (17.6938 iter/s, 5.6517s/100 iters), loss = 0.418382
I1210 17:45:03.053642 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:45:03.053642 22232 solver.cpp:237]     Train net output #1: loss = 0.418382 (* 1 = 0.418382 loss)
I1210 17:45:03.053642 22232 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1210 17:45:08.702643 22232 solver.cpp:218] Iteration 99200 (17.7022 iter/s, 5.64903s/100 iters), loss = 0.348043
I1210 17:45:08.702643 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:45:08.702643 22232 solver.cpp:237]     Train net output #1: loss = 0.348043 (* 1 = 0.348043 loss)
I1210 17:45:08.702643 22232 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1210 17:45:14.352660 22232 solver.cpp:218] Iteration 99300 (17.7007 iter/s, 5.64948s/100 iters), loss = 0.475117
I1210 17:45:14.353168 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 17:45:14.353168 22232 solver.cpp:237]     Train net output #1: loss = 0.475117 (* 1 = 0.475117 loss)
I1210 17:45:14.353168 22232 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1210 17:45:19.992640 22232 solver.cpp:218] Iteration 99400 (17.7315 iter/s, 5.63969s/100 iters), loss = 0.515402
I1210 17:45:19.992640 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 17:45:19.992640 22232 solver.cpp:237]     Train net output #1: loss = 0.515402 (* 1 = 0.515402 loss)
I1210 17:45:19.992640 22232 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1210 17:45:25.366139 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:45:25.588151 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_99500.caffemodel
I1210 17:45:25.607151 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_99500.solverstate
I1210 17:45:25.612151 22232 solver.cpp:330] Iteration 99500, Testing net (#0)
I1210 17:45:25.612151 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:45:26.978268 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:45:27.031267 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6859
I1210 17:45:27.031267 22232 solver.cpp:397]     Test net output #1: loss = 1.18523 (* 1 = 1.18523 loss)
I1210 17:45:27.086277 22232 solver.cpp:218] Iteration 99500 (14.0984 iter/s, 7.09302s/100 iters), loss = 0.400193
I1210 17:45:27.086277 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:45:27.086277 22232 solver.cpp:237]     Train net output #1: loss = 0.400193 (* 1 = 0.400193 loss)
I1210 17:45:27.086277 22232 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1210 17:45:32.725795 22232 solver.cpp:218] Iteration 99600 (17.7332 iter/s, 5.63915s/100 iters), loss = 0.41695
I1210 17:45:32.725795 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:45:32.725795 22232 solver.cpp:237]     Train net output #1: loss = 0.41695 (* 1 = 0.41695 loss)
I1210 17:45:32.725795 22232 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1210 17:45:38.366226 22232 solver.cpp:218] Iteration 99700 (17.7319 iter/s, 5.63955s/100 iters), loss = 0.414037
I1210 17:45:38.366226 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 17:45:38.366226 22232 solver.cpp:237]     Train net output #1: loss = 0.414037 (* 1 = 0.414037 loss)
I1210 17:45:38.366226 22232 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1210 17:45:44.007684 22232 solver.cpp:218] Iteration 99800 (17.7258 iter/s, 5.6415s/100 iters), loss = 0.468147
I1210 17:45:44.007684 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:45:44.007684 22232 solver.cpp:237]     Train net output #1: loss = 0.468147 (* 1 = 0.468147 loss)
I1210 17:45:44.007684 22232 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1210 17:45:49.645190 22232 solver.cpp:218] Iteration 99900 (17.7417 iter/s, 5.63643s/100 iters), loss = 0.498385
I1210 17:45:49.645190 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 17:45:49.645190 22232 solver.cpp:237]     Train net output #1: loss = 0.498385 (* 1 = 0.498385 loss)
I1210 17:45:49.645190 22232 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1210 17:45:55.010601 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:45:55.232623 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_100000.caffemodel
I1210 17:45:55.247614 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_100000.solverstate
I1210 17:45:55.252619 22232 solver.cpp:330] Iteration 100000, Testing net (#0)
I1210 17:45:55.252619 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:45:56.617754 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:45:56.670758 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6869
I1210 17:45:56.670758 22232 solver.cpp:397]     Test net output #1: loss = 1.18339 (* 1 = 1.18339 loss)
I1210 17:45:56.726763 22232 solver.cpp:218] Iteration 100000 (14.121 iter/s, 7.08167s/100 iters), loss = 0.249089
I1210 17:45:56.726763 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:45:56.726763 22232 solver.cpp:237]     Train net output #1: loss = 0.249089 (* 1 = 0.249089 loss)
I1210 17:45:56.726763 22232 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1210 17:46:02.358479 22232 solver.cpp:218] Iteration 100100 (17.7598 iter/s, 5.63069s/100 iters), loss = 0.387004
I1210 17:46:02.358479 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:46:02.358479 22232 solver.cpp:237]     Train net output #1: loss = 0.387004 (* 1 = 0.387004 loss)
I1210 17:46:02.358479 22232 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1210 17:46:07.994518 22232 solver.cpp:218] Iteration 100200 (17.7421 iter/s, 5.63631s/100 iters), loss = 0.380827
I1210 17:46:07.994518 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:46:07.994518 22232 solver.cpp:237]     Train net output #1: loss = 0.380827 (* 1 = 0.380827 loss)
I1210 17:46:07.994518 22232 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1210 17:46:13.628532 22232 solver.cpp:218] Iteration 100300 (17.7503 iter/s, 5.63372s/100 iters), loss = 0.412249
I1210 17:46:13.629534 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:46:13.629534 22232 solver.cpp:237]     Train net output #1: loss = 0.412249 (* 1 = 0.412249 loss)
I1210 17:46:13.629534 22232 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1210 17:46:19.274256 22232 solver.cpp:218] Iteration 100400 (17.7141 iter/s, 5.64522s/100 iters), loss = 0.449996
I1210 17:46:19.274256 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1210 17:46:19.275257 22232 solver.cpp:237]     Train net output #1: loss = 0.449996 (* 1 = 0.449996 loss)
I1210 17:46:19.275257 22232 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1210 17:46:24.637385 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:46:24.858417 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_100500.caffemodel
I1210 17:46:24.872936 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_100500.solverstate
I1210 17:46:24.877454 22232 solver.cpp:330] Iteration 100500, Testing net (#0)
I1210 17:46:24.877454 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:46:26.244622 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:46:26.298652 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6827
I1210 17:46:26.298652 22232 solver.cpp:397]     Test net output #1: loss = 1.19579 (* 1 = 1.19579 loss)
I1210 17:46:26.353659 22232 solver.cpp:218] Iteration 100500 (14.1277 iter/s, 7.07832s/100 iters), loss = 0.330801
I1210 17:46:26.353659 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:46:26.353659 22232 solver.cpp:237]     Train net output #1: loss = 0.3308 (* 1 = 0.3308 loss)
I1210 17:46:26.353659 22232 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1210 17:46:31.978829 22232 solver.cpp:218] Iteration 100600 (17.7784 iter/s, 5.62481s/100 iters), loss = 0.442052
I1210 17:46:31.978829 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:46:31.978829 22232 solver.cpp:237]     Train net output #1: loss = 0.442052 (* 1 = 0.442052 loss)
I1210 17:46:31.978829 22232 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1210 17:46:37.602823 22232 solver.cpp:218] Iteration 100700 (17.782 iter/s, 5.62368s/100 iters), loss = 0.330171
I1210 17:46:37.602823 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:46:37.602823 22232 solver.cpp:237]     Train net output #1: loss = 0.330171 (* 1 = 0.330171 loss)
I1210 17:46:37.602823 22232 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1210 17:46:43.236243 22232 solver.cpp:218] Iteration 100800 (17.7518 iter/s, 5.63323s/100 iters), loss = 0.425115
I1210 17:46:43.236243 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:46:43.236243 22232 solver.cpp:237]     Train net output #1: loss = 0.425115 (* 1 = 0.425115 loss)
I1210 17:46:43.236243 22232 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1210 17:46:48.857496 22232 solver.cpp:218] Iteration 100900 (17.7911 iter/s, 5.62078s/100 iters), loss = 0.435559
I1210 17:46:48.857496 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:46:48.857496 22232 solver.cpp:237]     Train net output #1: loss = 0.435559 (* 1 = 0.435559 loss)
I1210 17:46:48.857496 22232 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1210 17:46:54.220804 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:46:54.442145 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_101000.caffemodel
I1210 17:46:54.457144 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_101000.solverstate
I1210 17:46:54.462143 22232 solver.cpp:330] Iteration 101000, Testing net (#0)
I1210 17:46:54.462143 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:46:55.828173 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:46:55.881683 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6835
I1210 17:46:55.881683 22232 solver.cpp:397]     Test net output #1: loss = 1.20228 (* 1 = 1.20228 loss)
I1210 17:46:55.935214 22232 solver.cpp:218] Iteration 101000 (14.1307 iter/s, 7.0768s/100 iters), loss = 0.238839
I1210 17:46:55.935214 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1210 17:46:55.935214 22232 solver.cpp:237]     Train net output #1: loss = 0.238839 (* 1 = 0.238839 loss)
I1210 17:46:55.935214 22232 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1210 17:47:01.567484 22232 solver.cpp:218] Iteration 101100 (17.7559 iter/s, 5.63194s/100 iters), loss = 0.432903
I1210 17:47:01.567484 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:47:01.567484 22232 solver.cpp:237]     Train net output #1: loss = 0.432903 (* 1 = 0.432903 loss)
I1210 17:47:01.567484 22232 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1210 17:47:07.206276 22232 solver.cpp:218] Iteration 101200 (17.7348 iter/s, 5.63863s/100 iters), loss = 0.36145
I1210 17:47:07.206276 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:47:07.206276 22232 solver.cpp:237]     Train net output #1: loss = 0.36145 (* 1 = 0.36145 loss)
I1210 17:47:07.206276 22232 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1210 17:47:12.838655 22232 solver.cpp:218] Iteration 101300 (17.7564 iter/s, 5.63176s/100 iters), loss = 0.400331
I1210 17:47:12.838655 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:47:12.838655 22232 solver.cpp:237]     Train net output #1: loss = 0.400331 (* 1 = 0.400331 loss)
I1210 17:47:12.838655 22232 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1210 17:47:18.475113 22232 solver.cpp:218] Iteration 101400 (17.742 iter/s, 5.63635s/100 iters), loss = 0.368276
I1210 17:47:18.475113 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:47:18.475113 22232 solver.cpp:237]     Train net output #1: loss = 0.368276 (* 1 = 0.368276 loss)
I1210 17:47:18.475113 22232 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1210 17:47:23.838546 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:47:24.061571 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_101500.caffemodel
I1210 17:47:24.076568 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_101500.solverstate
I1210 17:47:24.082075 22232 solver.cpp:330] Iteration 101500, Testing net (#0)
I1210 17:47:24.082075 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:47:25.450711 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:47:25.503716 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6843
I1210 17:47:25.503716 22232 solver.cpp:397]     Test net output #1: loss = 1.20201 (* 1 = 1.20201 loss)
I1210 17:47:25.556716 22232 solver.cpp:218] Iteration 101500 (14.1227 iter/s, 7.08082s/100 iters), loss = 0.271328
I1210 17:47:25.556716 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 17:47:25.556716 22232 solver.cpp:237]     Train net output #1: loss = 0.271328 (* 1 = 0.271328 loss)
I1210 17:47:25.556716 22232 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1210 17:47:31.185686 22232 solver.cpp:218] Iteration 101600 (17.7687 iter/s, 5.62786s/100 iters), loss = 0.416137
I1210 17:47:31.185686 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:47:31.185686 22232 solver.cpp:237]     Train net output #1: loss = 0.416137 (* 1 = 0.416137 loss)
I1210 17:47:31.185686 22232 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1210 17:47:36.816654 22232 solver.cpp:218] Iteration 101700 (17.7589 iter/s, 5.63098s/100 iters), loss = 0.266289
I1210 17:47:36.816654 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 17:47:36.816654 22232 solver.cpp:237]     Train net output #1: loss = 0.266289 (* 1 = 0.266289 loss)
I1210 17:47:36.816654 22232 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1210 17:47:42.438146 22232 solver.cpp:218] Iteration 101800 (17.7915 iter/s, 5.62066s/100 iters), loss = 0.444669
I1210 17:47:42.438146 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1210 17:47:42.438146 22232 solver.cpp:237]     Train net output #1: loss = 0.444669 (* 1 = 0.444669 loss)
I1210 17:47:42.438146 22232 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1210 17:47:48.064577 22232 solver.cpp:218] Iteration 101900 (17.7736 iter/s, 5.62633s/100 iters), loss = 0.383351
I1210 17:47:48.064577 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:47:48.064577 22232 solver.cpp:237]     Train net output #1: loss = 0.383351 (* 1 = 0.383351 loss)
I1210 17:47:48.064577 22232 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1210 17:47:53.419915 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:47:53.640928 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_102000.caffemodel
I1210 17:47:53.655927 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_102000.solverstate
I1210 17:47:53.660928 22232 solver.cpp:330] Iteration 102000, Testing net (#0)
I1210 17:47:53.660928 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:47:55.028934 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:47:55.082937 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6877
I1210 17:47:55.082937 22232 solver.cpp:397]     Test net output #1: loss = 1.20359 (* 1 = 1.20359 loss)
I1210 17:47:55.136941 22232 solver.cpp:218] Iteration 102000 (14.1418 iter/s, 7.07124s/100 iters), loss = 0.265211
I1210 17:47:55.136941 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:47:55.136941 22232 solver.cpp:237]     Train net output #1: loss = 0.265211 (* 1 = 0.265211 loss)
I1210 17:47:55.136941 22232 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1210 17:48:00.762528 22232 solver.cpp:218] Iteration 102100 (17.7763 iter/s, 5.62547s/100 iters), loss = 0.39808
I1210 17:48:00.762528 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:48:00.762528 22232 solver.cpp:237]     Train net output #1: loss = 0.39808 (* 1 = 0.39808 loss)
I1210 17:48:00.762528 22232 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1210 17:48:06.409941 22232 solver.cpp:218] Iteration 102200 (17.7093 iter/s, 5.64674s/100 iters), loss = 0.269834
I1210 17:48:06.409941 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:48:06.409941 22232 solver.cpp:237]     Train net output #1: loss = 0.269834 (* 1 = 0.269834 loss)
I1210 17:48:06.409941 22232 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1210 17:48:12.043393 22232 solver.cpp:218] Iteration 102300 (17.7512 iter/s, 5.63341s/100 iters), loss = 0.418492
I1210 17:48:12.043393 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:48:12.043393 22232 solver.cpp:237]     Train net output #1: loss = 0.418492 (* 1 = 0.418492 loss)
I1210 17:48:12.043393 22232 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1210 17:48:17.676833 22232 solver.cpp:218] Iteration 102400 (17.7525 iter/s, 5.63299s/100 iters), loss = 0.403217
I1210 17:48:17.676833 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:48:17.676833 22232 solver.cpp:237]     Train net output #1: loss = 0.403217 (* 1 = 0.403217 loss)
I1210 17:48:17.676833 22232 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1210 17:48:23.039263 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:48:23.260272 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_102500.caffemodel
I1210 17:48:23.276271 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_102500.solverstate
I1210 17:48:23.281271 22232 solver.cpp:330] Iteration 102500, Testing net (#0)
I1210 17:48:23.281271 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:48:24.648708 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:48:24.702716 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6871
I1210 17:48:24.702716 22232 solver.cpp:397]     Test net output #1: loss = 1.19947 (* 1 = 1.19947 loss)
I1210 17:48:24.756718 22232 solver.cpp:218] Iteration 102500 (14.1261 iter/s, 7.0791s/100 iters), loss = 0.292827
I1210 17:48:24.756718 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:48:24.756718 22232 solver.cpp:237]     Train net output #1: loss = 0.292827 (* 1 = 0.292827 loss)
I1210 17:48:24.756718 22232 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1210 17:48:30.395720 22232 solver.cpp:218] Iteration 102600 (17.7358 iter/s, 5.63832s/100 iters), loss = 0.39542
I1210 17:48:30.395720 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:48:30.395720 22232 solver.cpp:237]     Train net output #1: loss = 0.39542 (* 1 = 0.39542 loss)
I1210 17:48:30.395720 22232 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1210 17:48:36.037609 22232 solver.cpp:218] Iteration 102700 (17.7242 iter/s, 5.64201s/100 iters), loss = 0.275454
I1210 17:48:36.038610 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:48:36.038610 22232 solver.cpp:237]     Train net output #1: loss = 0.275454 (* 1 = 0.275454 loss)
I1210 17:48:36.038610 22232 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1210 17:48:41.686558 22232 solver.cpp:218] Iteration 102800 (17.7062 iter/s, 5.64776s/100 iters), loss = 0.382879
I1210 17:48:41.686558 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:48:41.686558 22232 solver.cpp:237]     Train net output #1: loss = 0.382879 (* 1 = 0.382879 loss)
I1210 17:48:41.687059 22232 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1210 17:48:47.328524 22232 solver.cpp:218] Iteration 102900 (17.7267 iter/s, 5.64121s/100 iters), loss = 0.361538
I1210 17:48:47.328524 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:48:47.328524 22232 solver.cpp:237]     Train net output #1: loss = 0.361538 (* 1 = 0.361538 loss)
I1210 17:48:47.328524 22232 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1210 17:48:52.685914 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:48:52.907927 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_103000.caffemodel
I1210 17:48:52.922927 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_103000.solverstate
I1210 17:48:52.927928 22232 solver.cpp:330] Iteration 103000, Testing net (#0)
I1210 17:48:52.927928 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:48:54.293654 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:48:54.347157 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6851
I1210 17:48:54.347157 22232 solver.cpp:397]     Test net output #1: loss = 1.20071 (* 1 = 1.20071 loss)
I1210 17:48:54.400669 22232 solver.cpp:218] Iteration 103000 (14.1401 iter/s, 7.07206s/100 iters), loss = 0.296974
I1210 17:48:54.401170 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 17:48:54.401170 22232 solver.cpp:237]     Train net output #1: loss = 0.296974 (* 1 = 0.296974 loss)
I1210 17:48:54.401170 22232 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1210 17:49:00.025781 22232 solver.cpp:218] Iteration 103100 (17.7782 iter/s, 5.62488s/100 iters), loss = 0.405192
I1210 17:49:00.025781 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:49:00.025781 22232 solver.cpp:237]     Train net output #1: loss = 0.405192 (* 1 = 0.405192 loss)
I1210 17:49:00.025781 22232 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1210 17:49:05.648674 22232 solver.cpp:218] Iteration 103200 (17.7884 iter/s, 5.62164s/100 iters), loss = 0.389153
I1210 17:49:05.648674 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:49:05.648674 22232 solver.cpp:237]     Train net output #1: loss = 0.389153 (* 1 = 0.389153 loss)
I1210 17:49:05.648674 22232 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1210 17:49:11.284221 22232 solver.cpp:218] Iteration 103300 (17.7454 iter/s, 5.63525s/100 iters), loss = 0.432587
I1210 17:49:11.284221 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 17:49:11.284221 22232 solver.cpp:237]     Train net output #1: loss = 0.432587 (* 1 = 0.432587 loss)
I1210 17:49:11.284221 22232 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1210 17:49:16.908226 22232 solver.cpp:218] Iteration 103400 (17.7816 iter/s, 5.62378s/100 iters), loss = 0.34045
I1210 17:49:16.908725 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:49:16.908725 22232 solver.cpp:237]     Train net output #1: loss = 0.34045 (* 1 = 0.34045 loss)
I1210 17:49:16.908725 22232 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1210 17:49:22.256255 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:49:22.475993 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_103500.caffemodel
I1210 17:49:22.490993 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_103500.solverstate
I1210 17:49:22.495992 22232 solver.cpp:330] Iteration 103500, Testing net (#0)
I1210 17:49:22.495992 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:49:23.861317 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:49:23.914826 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6857
I1210 17:49:23.914826 22232 solver.cpp:397]     Test net output #1: loss = 1.20118 (* 1 = 1.20118 loss)
I1210 17:49:23.967800 22232 solver.cpp:218] Iteration 103500 (14.1653 iter/s, 7.05951s/100 iters), loss = 0.339073
I1210 17:49:23.967800 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:49:23.967800 22232 solver.cpp:237]     Train net output #1: loss = 0.339073 (* 1 = 0.339073 loss)
I1210 17:49:23.967800 22232 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1210 17:49:29.585724 22232 solver.cpp:218] Iteration 103600 (17.8023 iter/s, 5.61725s/100 iters), loss = 0.365635
I1210 17:49:29.585724 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:49:29.585724 22232 solver.cpp:237]     Train net output #1: loss = 0.365635 (* 1 = 0.365635 loss)
I1210 17:49:29.585724 22232 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1210 17:49:35.205021 22232 solver.cpp:218] Iteration 103700 (17.7965 iter/s, 5.61907s/100 iters), loss = 0.318363
I1210 17:49:35.205021 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:49:35.205021 22232 solver.cpp:237]     Train net output #1: loss = 0.318363 (* 1 = 0.318363 loss)
I1210 17:49:35.205021 22232 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1210 17:49:40.833514 22232 solver.cpp:218] Iteration 103800 (17.7705 iter/s, 5.62732s/100 iters), loss = 0.350993
I1210 17:49:40.833514 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:49:40.833514 22232 solver.cpp:237]     Train net output #1: loss = 0.350993 (* 1 = 0.350993 loss)
I1210 17:49:40.833514 22232 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1210 17:49:46.467948 22232 solver.cpp:218] Iteration 103900 (17.7475 iter/s, 5.63458s/100 iters), loss = 0.430503
I1210 17:49:46.467948 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:49:46.467948 22232 solver.cpp:237]     Train net output #1: loss = 0.430503 (* 1 = 0.430503 loss)
I1210 17:49:46.467948 22232 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1210 17:49:51.826373 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:49:52.046394 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_104000.caffemodel
I1210 17:49:52.064394 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_104000.solverstate
I1210 17:49:52.069394 22232 solver.cpp:330] Iteration 104000, Testing net (#0)
I1210 17:49:52.069394 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:49:53.437603 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:49:53.492604 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6838
I1210 17:49:53.492604 22232 solver.cpp:397]     Test net output #1: loss = 1.21724 (* 1 = 1.21724 loss)
I1210 17:49:53.545619 22232 solver.cpp:218] Iteration 104000 (14.1295 iter/s, 7.07741s/100 iters), loss = 0.355174
I1210 17:49:53.545619 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:49:53.546620 22232 solver.cpp:237]     Train net output #1: loss = 0.355174 (* 1 = 0.355174 loss)
I1210 17:49:53.546620 22232 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1210 17:49:59.194031 22232 solver.cpp:218] Iteration 104100 (17.7081 iter/s, 5.64714s/100 iters), loss = 0.414747
I1210 17:49:59.194031 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:49:59.194031 22232 solver.cpp:237]     Train net output #1: loss = 0.414747 (* 1 = 0.414747 loss)
I1210 17:49:59.194031 22232 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1210 17:50:04.858284 22232 solver.cpp:218] Iteration 104200 (17.6556 iter/s, 5.66393s/100 iters), loss = 0.349584
I1210 17:50:04.858284 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:50:04.858284 22232 solver.cpp:237]     Train net output #1: loss = 0.349584 (* 1 = 0.349584 loss)
I1210 17:50:04.858284 22232 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1210 17:50:10.494949 22232 solver.cpp:218] Iteration 104300 (17.7424 iter/s, 5.63623s/100 iters), loss = 0.323255
I1210 17:50:10.494949 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:50:10.494949 22232 solver.cpp:237]     Train net output #1: loss = 0.323255 (* 1 = 0.323255 loss)
I1210 17:50:10.494949 22232 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1210 17:50:16.137359 22232 solver.cpp:218] Iteration 104400 (17.7229 iter/s, 5.6424s/100 iters), loss = 0.384856
I1210 17:50:16.137359 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:50:16.137359 22232 solver.cpp:237]     Train net output #1: loss = 0.384856 (* 1 = 0.384856 loss)
I1210 17:50:16.137359 22232 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1210 17:50:21.498491 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:50:21.721284 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_104500.caffemodel
I1210 17:50:21.735283 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_104500.solverstate
I1210 17:50:21.740286 22232 solver.cpp:330] Iteration 104500, Testing net (#0)
I1210 17:50:21.740286 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:50:23.105559 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:50:23.158574 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6852
I1210 17:50:23.158574 22232 solver.cpp:397]     Test net output #1: loss = 1.20771 (* 1 = 1.20771 loss)
I1210 17:50:23.212579 22232 solver.cpp:218] Iteration 104500 (14.1356 iter/s, 7.07435s/100 iters), loss = 0.344011
I1210 17:50:23.212579 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:50:23.212579 22232 solver.cpp:237]     Train net output #1: loss = 0.344011 (* 1 = 0.344011 loss)
I1210 17:50:23.212579 22232 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1210 17:50:28.848246 22232 solver.cpp:218] Iteration 104600 (17.744 iter/s, 5.63571s/100 iters), loss = 0.390425
I1210 17:50:28.848246 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:50:28.848246 22232 solver.cpp:237]     Train net output #1: loss = 0.390425 (* 1 = 0.390425 loss)
I1210 17:50:28.848246 22232 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1210 17:50:34.484455 22232 solver.cpp:218] Iteration 104700 (17.7451 iter/s, 5.63537s/100 iters), loss = 0.319363
I1210 17:50:34.484455 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:50:34.484455 22232 solver.cpp:237]     Train net output #1: loss = 0.319363 (* 1 = 0.319363 loss)
I1210 17:50:34.484455 22232 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1210 17:50:40.125280 22232 solver.cpp:218] Iteration 104800 (17.7285 iter/s, 5.64064s/100 iters), loss = 0.420855
I1210 17:50:40.125280 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:50:40.125280 22232 solver.cpp:237]     Train net output #1: loss = 0.420855 (* 1 = 0.420855 loss)
I1210 17:50:40.125280 22232 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1210 17:50:45.763344 22232 solver.cpp:218] Iteration 104900 (17.7372 iter/s, 5.63787s/100 iters), loss = 0.349576
I1210 17:50:45.763344 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:50:45.763344 22232 solver.cpp:237]     Train net output #1: loss = 0.349576 (* 1 = 0.349576 loss)
I1210 17:50:45.763344 22232 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1210 17:50:51.125020 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:50:51.346258 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_105000.caffemodel
I1210 17:50:51.360781 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_105000.solverstate
I1210 17:50:51.364780 22232 solver.cpp:330] Iteration 105000, Testing net (#0)
I1210 17:50:51.364780 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:50:52.727181 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:50:52.782748 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6808
I1210 17:50:52.782748 22232 solver.cpp:397]     Test net output #1: loss = 1.22286 (* 1 = 1.22286 loss)
I1210 17:50:52.836758 22232 solver.cpp:218] Iteration 105000 (14.1385 iter/s, 7.07289s/100 iters), loss = 0.225866
I1210 17:50:52.836758 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:50:52.836758 22232 solver.cpp:237]     Train net output #1: loss = 0.225866 (* 1 = 0.225866 loss)
I1210 17:50:52.836758 22232 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1210 17:50:58.467579 22232 solver.cpp:218] Iteration 105100 (17.7621 iter/s, 5.62997s/100 iters), loss = 0.350551
I1210 17:50:58.467579 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:50:58.467579 22232 solver.cpp:237]     Train net output #1: loss = 0.350551 (* 1 = 0.350551 loss)
I1210 17:50:58.467579 22232 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1210 17:51:04.101238 22232 solver.cpp:218] Iteration 105200 (17.7519 iter/s, 5.6332s/100 iters), loss = 0.316796
I1210 17:51:04.101238 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:51:04.101238 22232 solver.cpp:237]     Train net output #1: loss = 0.316796 (* 1 = 0.316796 loss)
I1210 17:51:04.101238 22232 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1210 17:51:09.738642 22232 solver.cpp:218] Iteration 105300 (17.7381 iter/s, 5.63758s/100 iters), loss = 0.332662
I1210 17:51:09.738642 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:51:09.738642 22232 solver.cpp:237]     Train net output #1: loss = 0.332661 (* 1 = 0.332661 loss)
I1210 17:51:09.738642 22232 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1210 17:51:15.376085 22232 solver.cpp:218] Iteration 105400 (17.7421 iter/s, 5.63632s/100 iters), loss = 0.408155
I1210 17:51:15.376085 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:51:15.376085 22232 solver.cpp:237]     Train net output #1: loss = 0.408155 (* 1 = 0.408155 loss)
I1210 17:51:15.376085 22232 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1210 17:51:20.731464 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:51:20.954982 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_105500.caffemodel
I1210 17:51:20.969485 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_105500.solverstate
I1210 17:51:20.973485 22232 solver.cpp:330] Iteration 105500, Testing net (#0)
I1210 17:51:20.973485 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:51:22.341624 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:51:22.394639 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6841
I1210 17:51:22.394639 22232 solver.cpp:397]     Test net output #1: loss = 1.21027 (* 1 = 1.21027 loss)
I1210 17:51:22.450131 22232 solver.cpp:218] Iteration 105500 (14.1377 iter/s, 7.07327s/100 iters), loss = 0.289958
I1210 17:51:22.450131 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:51:22.450131 22232 solver.cpp:237]     Train net output #1: loss = 0.289958 (* 1 = 0.289958 loss)
I1210 17:51:22.450131 22232 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1210 17:51:28.088075 22232 solver.cpp:218] Iteration 105600 (17.7374 iter/s, 5.63782s/100 iters), loss = 0.322905
I1210 17:51:28.088075 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:51:28.088075 22232 solver.cpp:237]     Train net output #1: loss = 0.322905 (* 1 = 0.322905 loss)
I1210 17:51:28.088075 22232 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1210 17:51:33.717481 22232 solver.cpp:218] Iteration 105700 (17.7657 iter/s, 5.62883s/100 iters), loss = 0.313999
I1210 17:51:33.717481 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:51:33.717481 22232 solver.cpp:237]     Train net output #1: loss = 0.313999 (* 1 = 0.313999 loss)
I1210 17:51:33.717481 22232 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1210 17:51:39.349457 22232 solver.cpp:218] Iteration 105800 (17.7573 iter/s, 5.63149s/100 iters), loss = 0.437314
I1210 17:51:39.349457 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:51:39.349457 22232 solver.cpp:237]     Train net output #1: loss = 0.437314 (* 1 = 0.437314 loss)
I1210 17:51:39.349457 22232 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1210 17:51:44.977407 22232 solver.cpp:218] Iteration 105900 (17.7676 iter/s, 5.62822s/100 iters), loss = 0.359913
I1210 17:51:44.977407 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:51:44.977407 22232 solver.cpp:237]     Train net output #1: loss = 0.359913 (* 1 = 0.359913 loss)
I1210 17:51:44.977407 22232 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1210 17:51:50.331816 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:51:50.550829 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_106000.caffemodel
I1210 17:51:50.565840 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_106000.solverstate
I1210 17:51:50.570837 22232 solver.cpp:330] Iteration 106000, Testing net (#0)
I1210 17:51:50.570837 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:51:51.936946 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:51:51.990958 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6809
I1210 17:51:51.990958 22232 solver.cpp:397]     Test net output #1: loss = 1.21912 (* 1 = 1.21912 loss)
I1210 17:51:52.043952 22232 solver.cpp:218] Iteration 106000 (14.1522 iter/s, 7.06602s/100 iters), loss = 0.270901
I1210 17:51:52.043952 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 17:51:52.043952 22232 solver.cpp:237]     Train net output #1: loss = 0.270901 (* 1 = 0.270901 loss)
I1210 17:51:52.043952 22232 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1210 17:51:57.682394 22232 solver.cpp:218] Iteration 106100 (17.7364 iter/s, 5.63811s/100 iters), loss = 0.368621
I1210 17:51:57.683395 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:51:57.683395 22232 solver.cpp:237]     Train net output #1: loss = 0.368621 (* 1 = 0.368621 loss)
I1210 17:51:57.683395 22232 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1210 17:52:03.317806 22232 solver.cpp:218] Iteration 106200 (17.7483 iter/s, 5.63436s/100 iters), loss = 0.36396
I1210 17:52:03.317806 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:52:03.317806 22232 solver.cpp:237]     Train net output #1: loss = 0.36396 (* 1 = 0.36396 loss)
I1210 17:52:03.317806 22232 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1210 17:52:08.955246 22232 solver.cpp:218] Iteration 106300 (17.7405 iter/s, 5.63683s/100 iters), loss = 0.433027
I1210 17:52:08.955246 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1210 17:52:08.955246 22232 solver.cpp:237]     Train net output #1: loss = 0.433027 (* 1 = 0.433027 loss)
I1210 17:52:08.955246 22232 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1210 17:52:14.593641 22232 solver.cpp:218] Iteration 106400 (17.7364 iter/s, 5.63812s/100 iters), loss = 0.381849
I1210 17:52:14.593641 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:52:14.593641 22232 solver.cpp:237]     Train net output #1: loss = 0.381849 (* 1 = 0.381849 loss)
I1210 17:52:14.593641 22232 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1210 17:52:19.954532 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:52:20.177062 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_106500.caffemodel
I1210 17:52:20.191061 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_106500.solverstate
I1210 17:52:20.196061 22232 solver.cpp:330] Iteration 106500, Testing net (#0)
I1210 17:52:20.196061 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:52:21.565505 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:52:21.619009 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6819
I1210 17:52:21.619009 22232 solver.cpp:397]     Test net output #1: loss = 1.21963 (* 1 = 1.21963 loss)
I1210 17:52:21.672013 22232 solver.cpp:218] Iteration 106500 (14.1277 iter/s, 7.07827s/100 iters), loss = 0.29779
I1210 17:52:21.672013 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 17:52:21.672013 22232 solver.cpp:237]     Train net output #1: loss = 0.29779 (* 1 = 0.29779 loss)
I1210 17:52:21.672013 22232 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1210 17:52:27.310406 22232 solver.cpp:218] Iteration 106600 (17.7382 iter/s, 5.63754s/100 iters), loss = 0.381276
I1210 17:52:27.310406 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:52:27.310406 22232 solver.cpp:237]     Train net output #1: loss = 0.381276 (* 1 = 0.381276 loss)
I1210 17:52:27.310406 22232 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1210 17:52:32.944846 22232 solver.cpp:218] Iteration 106700 (17.7485 iter/s, 5.63429s/100 iters), loss = 0.236741
I1210 17:52:32.944846 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 17:52:32.944846 22232 solver.cpp:237]     Train net output #1: loss = 0.236741 (* 1 = 0.236741 loss)
I1210 17:52:32.944846 22232 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1210 17:52:38.584262 22232 solver.cpp:218] Iteration 106800 (17.7342 iter/s, 5.63882s/100 iters), loss = 0.399283
I1210 17:52:38.584262 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:52:38.584262 22232 solver.cpp:237]     Train net output #1: loss = 0.399283 (* 1 = 0.399283 loss)
I1210 17:52:38.584262 22232 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1210 17:52:44.221653 22232 solver.cpp:218] Iteration 106900 (17.7405 iter/s, 5.63683s/100 iters), loss = 0.376596
I1210 17:52:44.221653 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:52:44.221653 22232 solver.cpp:237]     Train net output #1: loss = 0.376596 (* 1 = 0.376596 loss)
I1210 17:52:44.221653 22232 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1210 17:52:49.582075 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:52:49.802088 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_107000.caffemodel
I1210 17:52:49.817088 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_107000.solverstate
I1210 17:52:49.822089 22232 solver.cpp:330] Iteration 107000, Testing net (#0)
I1210 17:52:49.822089 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:52:51.193212 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:52:51.246212 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6826
I1210 17:52:51.246212 22232 solver.cpp:397]     Test net output #1: loss = 1.22198 (* 1 = 1.22198 loss)
I1210 17:52:51.301219 22232 solver.cpp:218] Iteration 107000 (14.1269 iter/s, 7.07869s/100 iters), loss = 0.264571
I1210 17:52:51.301219 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 17:52:51.301219 22232 solver.cpp:237]     Train net output #1: loss = 0.264571 (* 1 = 0.264571 loss)
I1210 17:52:51.301219 22232 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1210 17:52:56.940724 22232 solver.cpp:218] Iteration 107100 (17.7327 iter/s, 5.6393s/100 iters), loss = 0.376406
I1210 17:52:56.940724 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:52:56.940724 22232 solver.cpp:237]     Train net output #1: loss = 0.376406 (* 1 = 0.376406 loss)
I1210 17:52:56.940724 22232 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1210 17:53:02.582136 22232 solver.cpp:218] Iteration 107200 (17.7276 iter/s, 5.64091s/100 iters), loss = 0.34718
I1210 17:53:02.582136 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:53:02.582136 22232 solver.cpp:237]     Train net output #1: loss = 0.34718 (* 1 = 0.34718 loss)
I1210 17:53:02.582136 22232 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1210 17:53:08.222554 22232 solver.cpp:218] Iteration 107300 (17.7302 iter/s, 5.64011s/100 iters), loss = 0.371578
I1210 17:53:08.222554 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:53:08.222554 22232 solver.cpp:237]     Train net output #1: loss = 0.371578 (* 1 = 0.371578 loss)
I1210 17:53:08.222554 22232 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1210 17:53:13.859467 22232 solver.cpp:218] Iteration 107400 (17.742 iter/s, 5.63634s/100 iters), loss = 0.441344
I1210 17:53:13.859467 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:53:13.859467 22232 solver.cpp:237]     Train net output #1: loss = 0.441344 (* 1 = 0.441344 loss)
I1210 17:53:13.859467 22232 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1210 17:53:19.217378 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:53:19.440392 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_107500.caffemodel
I1210 17:53:19.454895 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_107500.solverstate
I1210 17:53:19.459395 22232 solver.cpp:330] Iteration 107500, Testing net (#0)
I1210 17:53:19.459895 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:53:20.826556 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:53:20.881567 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6843
I1210 17:53:20.881567 22232 solver.cpp:397]     Test net output #1: loss = 1.23424 (* 1 = 1.23424 loss)
I1210 17:53:20.934566 22232 solver.cpp:218] Iteration 107500 (14.1344 iter/s, 7.07493s/100 iters), loss = 0.246806
I1210 17:53:20.934566 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:53:20.934566 22232 solver.cpp:237]     Train net output #1: loss = 0.246806 (* 1 = 0.246806 loss)
I1210 17:53:20.934566 22232 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1210 17:53:26.573148 22232 solver.cpp:218] Iteration 107600 (17.7384 iter/s, 5.6375s/100 iters), loss = 0.383277
I1210 17:53:26.573148 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:53:26.573148 22232 solver.cpp:237]     Train net output #1: loss = 0.383277 (* 1 = 0.383277 loss)
I1210 17:53:26.573148 22232 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1210 17:53:32.215626 22232 solver.cpp:218] Iteration 107700 (17.7228 iter/s, 5.64246s/100 iters), loss = 0.27563
I1210 17:53:32.215626 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:53:32.215626 22232 solver.cpp:237]     Train net output #1: loss = 0.27563 (* 1 = 0.27563 loss)
I1210 17:53:32.215626 22232 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1210 17:53:37.853121 22232 solver.cpp:218] Iteration 107800 (17.7401 iter/s, 5.63693s/100 iters), loss = 0.307904
I1210 17:53:37.853121 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:53:37.853121 22232 solver.cpp:237]     Train net output #1: loss = 0.307904 (* 1 = 0.307904 loss)
I1210 17:53:37.853121 22232 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1210 17:53:43.498615 22232 solver.cpp:218] Iteration 107900 (17.7136 iter/s, 5.64537s/100 iters), loss = 0.377012
I1210 17:53:43.498615 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:53:43.498615 22232 solver.cpp:237]     Train net output #1: loss = 0.377012 (* 1 = 0.377012 loss)
I1210 17:53:43.498615 22232 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1210 17:53:48.863916 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:53:49.085930 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_108000.caffemodel
I1210 17:53:49.099930 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_108000.solverstate
I1210 17:53:49.104929 22232 solver.cpp:330] Iteration 108000, Testing net (#0)
I1210 17:53:49.104929 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:53:50.474046 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:53:50.529047 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6846
I1210 17:53:50.529047 22232 solver.cpp:397]     Test net output #1: loss = 1.22579 (* 1 = 1.22579 loss)
I1210 17:53:50.582049 22232 solver.cpp:218] Iteration 108000 (14.118 iter/s, 7.08316s/100 iters), loss = 0.321037
I1210 17:53:50.583050 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:53:50.583050 22232 solver.cpp:237]     Train net output #1: loss = 0.321037 (* 1 = 0.321037 loss)
I1210 17:53:50.583050 22232 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1210 17:53:56.228032 22232 solver.cpp:218] Iteration 108100 (17.7153 iter/s, 5.64485s/100 iters), loss = 0.409828
I1210 17:53:56.228032 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:53:56.228032 22232 solver.cpp:237]     Train net output #1: loss = 0.409828 (* 1 = 0.409828 loss)
I1210 17:53:56.228032 22232 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1210 17:54:01.871085 22232 solver.cpp:218] Iteration 108200 (17.7227 iter/s, 5.64247s/100 iters), loss = 0.310346
I1210 17:54:01.871085 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:54:01.871085 22232 solver.cpp:237]     Train net output #1: loss = 0.310346 (* 1 = 0.310346 loss)
I1210 17:54:01.871085 22232 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1210 17:54:07.522471 22232 solver.cpp:218] Iteration 108300 (17.6969 iter/s, 5.6507s/100 iters), loss = 0.392592
I1210 17:54:07.522471 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:54:07.522471 22232 solver.cpp:237]     Train net output #1: loss = 0.392592 (* 1 = 0.392592 loss)
I1210 17:54:07.522471 22232 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1210 17:54:13.158807 22232 solver.cpp:218] Iteration 108400 (17.7436 iter/s, 5.63583s/100 iters), loss = 0.386884
I1210 17:54:13.158807 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:54:13.158807 22232 solver.cpp:237]     Train net output #1: loss = 0.386884 (* 1 = 0.386884 loss)
I1210 17:54:13.158807 22232 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1210 17:54:18.534215 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:54:18.756227 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_108500.caffemodel
I1210 17:54:18.770731 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_108500.solverstate
I1210 17:54:18.775233 22232 solver.cpp:330] Iteration 108500, Testing net (#0)
I1210 17:54:18.775233 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:54:20.140339 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:54:20.195353 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6826
I1210 17:54:20.195353 22232 solver.cpp:397]     Test net output #1: loss = 1.23997 (* 1 = 1.23997 loss)
I1210 17:54:20.248344 22232 solver.cpp:218] Iteration 108500 (14.1057 iter/s, 7.08933s/100 iters), loss = 0.226394
I1210 17:54:20.248344 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:54:20.248344 22232 solver.cpp:237]     Train net output #1: loss = 0.226394 (* 1 = 0.226394 loss)
I1210 17:54:20.248344 22232 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1210 17:54:25.873785 22232 solver.cpp:218] Iteration 108600 (17.7778 iter/s, 5.625s/100 iters), loss = 0.394085
I1210 17:54:25.873785 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:54:25.873785 22232 solver.cpp:237]     Train net output #1: loss = 0.394085 (* 1 = 0.394085 loss)
I1210 17:54:25.873785 22232 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1210 17:54:31.501190 22232 solver.cpp:218] Iteration 108700 (17.7709 iter/s, 5.62718s/100 iters), loss = 0.346613
I1210 17:54:31.501190 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:54:31.501190 22232 solver.cpp:237]     Train net output #1: loss = 0.346613 (* 1 = 0.346613 loss)
I1210 17:54:31.501190 22232 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1210 17:54:37.124581 22232 solver.cpp:218] Iteration 108800 (17.7833 iter/s, 5.62327s/100 iters), loss = 0.376581
I1210 17:54:37.124581 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:54:37.124581 22232 solver.cpp:237]     Train net output #1: loss = 0.376581 (* 1 = 0.376581 loss)
I1210 17:54:37.124581 22232 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1210 17:54:42.751998 22232 solver.cpp:218] Iteration 108900 (17.7742 iter/s, 5.62614s/100 iters), loss = 0.348564
I1210 17:54:42.751998 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:54:42.751998 22232 solver.cpp:237]     Train net output #1: loss = 0.348564 (* 1 = 0.348564 loss)
I1210 17:54:42.751998 22232 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1210 17:54:48.102346 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:54:48.323380 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_109000.caffemodel
I1210 17:54:48.339383 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_109000.solverstate
I1210 17:54:48.343384 22232 solver.cpp:330] Iteration 109000, Testing net (#0)
I1210 17:54:48.343384 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:54:49.709475 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:54:49.763481 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6862
I1210 17:54:49.763481 22232 solver.cpp:397]     Test net output #1: loss = 1.23025 (* 1 = 1.23025 loss)
I1210 17:54:49.817984 22232 solver.cpp:218] Iteration 109000 (14.1532 iter/s, 7.06552s/100 iters), loss = 0.266231
I1210 17:54:49.817984 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:54:49.817984 22232 solver.cpp:237]     Train net output #1: loss = 0.266232 (* 1 = 0.266232 loss)
I1210 17:54:49.817984 22232 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1210 17:54:55.453904 22232 solver.cpp:218] Iteration 109100 (17.7435 iter/s, 5.63586s/100 iters), loss = 0.383051
I1210 17:54:55.453904 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:54:55.453904 22232 solver.cpp:237]     Train net output #1: loss = 0.383051 (* 1 = 0.383051 loss)
I1210 17:54:55.453904 22232 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1210 17:55:01.083348 22232 solver.cpp:218] Iteration 109200 (17.7651 iter/s, 5.62903s/100 iters), loss = 0.318312
I1210 17:55:01.083348 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:55:01.083348 22232 solver.cpp:237]     Train net output #1: loss = 0.318312 (* 1 = 0.318312 loss)
I1210 17:55:01.083348 22232 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1210 17:55:06.705788 22232 solver.cpp:218] Iteration 109300 (17.7891 iter/s, 5.62142s/100 iters), loss = 0.404455
I1210 17:55:06.705788 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:55:06.705788 22232 solver.cpp:237]     Train net output #1: loss = 0.404455 (* 1 = 0.404455 loss)
I1210 17:55:06.705788 22232 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1210 17:55:12.335525 22232 solver.cpp:218] Iteration 109400 (17.7615 iter/s, 5.63016s/100 iters), loss = 0.319036
I1210 17:55:12.335525 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:55:12.335525 22232 solver.cpp:237]     Train net output #1: loss = 0.319036 (* 1 = 0.319036 loss)
I1210 17:55:12.335525 22232 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1210 17:55:17.682976 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:55:17.903998 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_109500.caffemodel
I1210 17:55:17.917999 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_109500.solverstate
I1210 17:55:17.923002 22232 solver.cpp:330] Iteration 109500, Testing net (#0)
I1210 17:55:17.923503 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:55:19.289275 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:55:19.342280 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6823
I1210 17:55:19.343281 22232 solver.cpp:397]     Test net output #1: loss = 1.23489 (* 1 = 1.23489 loss)
I1210 17:55:19.396281 22232 solver.cpp:218] Iteration 109500 (14.1641 iter/s, 7.06008s/100 iters), loss = 0.23607
I1210 17:55:19.396281 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:55:19.396281 22232 solver.cpp:237]     Train net output #1: loss = 0.23607 (* 1 = 0.23607 loss)
I1210 17:55:19.396281 22232 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1210 17:55:25.037806 22232 solver.cpp:218] Iteration 109600 (17.7271 iter/s, 5.64109s/100 iters), loss = 0.415611
I1210 17:55:25.037806 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:55:25.037806 22232 solver.cpp:237]     Train net output #1: loss = 0.415611 (* 1 = 0.415611 loss)
I1210 17:55:25.037806 22232 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1210 17:55:30.679288 22232 solver.cpp:218] Iteration 109700 (17.7273 iter/s, 5.641s/100 iters), loss = 0.262891
I1210 17:55:30.679288 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 17:55:30.679288 22232 solver.cpp:237]     Train net output #1: loss = 0.262891 (* 1 = 0.262891 loss)
I1210 17:55:30.679288 22232 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1210 17:55:36.317800 22232 solver.cpp:218] Iteration 109800 (17.7383 iter/s, 5.63751s/100 iters), loss = 0.371066
I1210 17:55:36.317800 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:55:36.317800 22232 solver.cpp:237]     Train net output #1: loss = 0.371066 (* 1 = 0.371066 loss)
I1210 17:55:36.317800 22232 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1210 17:55:41.964377 22232 solver.cpp:218] Iteration 109900 (17.7114 iter/s, 5.64607s/100 iters), loss = 0.421446
I1210 17:55:41.964377 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:55:41.964377 22232 solver.cpp:237]     Train net output #1: loss = 0.421446 (* 1 = 0.421446 loss)
I1210 17:55:41.964377 22232 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1210 17:55:47.333387 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:55:47.554919 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_110000.caffemodel
I1210 17:55:47.569911 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_110000.solverstate
I1210 17:55:47.574910 22232 solver.cpp:330] Iteration 110000, Testing net (#0)
I1210 17:55:47.574910 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:55:48.943020 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:55:48.996021 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6826
I1210 17:55:48.996021 22232 solver.cpp:397]     Test net output #1: loss = 1.22579 (* 1 = 1.22579 loss)
I1210 17:55:49.050046 22232 solver.cpp:218] Iteration 110000 (14.1131 iter/s, 7.0856s/100 iters), loss = 0.289945
I1210 17:55:49.050046 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 17:55:49.050046 22232 solver.cpp:237]     Train net output #1: loss = 0.289945 (* 1 = 0.289945 loss)
I1210 17:55:49.050046 22232 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1210 17:55:54.686476 22232 solver.cpp:218] Iteration 110100 (17.7433 iter/s, 5.63593s/100 iters), loss = 0.358001
I1210 17:55:54.686476 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:55:54.686476 22232 solver.cpp:237]     Train net output #1: loss = 0.358001 (* 1 = 0.358001 loss)
I1210 17:55:54.686476 22232 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1210 17:56:00.329962 22232 solver.cpp:218] Iteration 110200 (17.7208 iter/s, 5.64309s/100 iters), loss = 0.252681
I1210 17:56:00.330463 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 17:56:00.330463 22232 solver.cpp:237]     Train net output #1: loss = 0.252681 (* 1 = 0.252681 loss)
I1210 17:56:00.330463 22232 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1210 17:56:05.976389 22232 solver.cpp:218] Iteration 110300 (17.7131 iter/s, 5.64554s/100 iters), loss = 0.398982
I1210 17:56:05.976389 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1210 17:56:05.976389 22232 solver.cpp:237]     Train net output #1: loss = 0.398982 (* 1 = 0.398982 loss)
I1210 17:56:05.976389 22232 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1210 17:56:11.627068 22232 solver.cpp:218] Iteration 110400 (17.6982 iter/s, 5.65028s/100 iters), loss = 0.358348
I1210 17:56:11.627068 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:56:11.627068 22232 solver.cpp:237]     Train net output #1: loss = 0.358348 (* 1 = 0.358348 loss)
I1210 17:56:11.627068 22232 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1210 17:56:16.998522 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:56:17.220535 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_110500.caffemodel
I1210 17:56:17.238540 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_110500.solverstate
I1210 17:56:17.243546 22232 solver.cpp:330] Iteration 110500, Testing net (#0)
I1210 17:56:17.243546 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:56:18.613699 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:56:18.667711 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6846
I1210 17:56:18.667711 22232 solver.cpp:397]     Test net output #1: loss = 1.2343 (* 1 = 1.2343 loss)
I1210 17:56:18.720708 22232 solver.cpp:218] Iteration 110500 (14.0979 iter/s, 7.09325s/100 iters), loss = 0.218639
I1210 17:56:18.720708 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 17:56:18.720708 22232 solver.cpp:237]     Train net output #1: loss = 0.218639 (* 1 = 0.218639 loss)
I1210 17:56:18.720708 22232 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1210 17:56:24.360285 22232 solver.cpp:218] Iteration 110600 (17.732 iter/s, 5.63952s/100 iters), loss = 0.329842
I1210 17:56:24.360285 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:56:24.360285 22232 solver.cpp:237]     Train net output #1: loss = 0.329842 (* 1 = 0.329842 loss)
I1210 17:56:24.360285 22232 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1210 17:56:29.978020 22232 solver.cpp:218] Iteration 110700 (17.8031 iter/s, 5.61699s/100 iters), loss = 0.310642
I1210 17:56:29.978020 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 17:56:29.978020 22232 solver.cpp:237]     Train net output #1: loss = 0.310642 (* 1 = 0.310642 loss)
I1210 17:56:29.978020 22232 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1210 17:56:35.608633 22232 solver.cpp:218] Iteration 110800 (17.7613 iter/s, 5.63021s/100 iters), loss = 0.364517
I1210 17:56:35.608633 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:56:35.608633 22232 solver.cpp:237]     Train net output #1: loss = 0.364517 (* 1 = 0.364517 loss)
I1210 17:56:35.608633 22232 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1210 17:56:41.245136 22232 solver.cpp:218] Iteration 110900 (17.7414 iter/s, 5.63653s/100 iters), loss = 0.329649
I1210 17:56:41.246137 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:56:41.246137 22232 solver.cpp:237]     Train net output #1: loss = 0.329649 (* 1 = 0.329649 loss)
I1210 17:56:41.246137 22232 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1210 17:56:46.586510 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:56:46.808527 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_111000.caffemodel
I1210 17:56:46.823527 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_111000.solverstate
I1210 17:56:46.827527 22232 solver.cpp:330] Iteration 111000, Testing net (#0)
I1210 17:56:46.827527 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:56:48.196635 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:56:48.250634 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6861
I1210 17:56:48.250634 22232 solver.cpp:397]     Test net output #1: loss = 1.23938 (* 1 = 1.23938 loss)
I1210 17:56:48.304643 22232 solver.cpp:218] Iteration 111000 (14.1671 iter/s, 7.05862s/100 iters), loss = 0.224503
I1210 17:56:48.304643 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:56:48.304643 22232 solver.cpp:237]     Train net output #1: loss = 0.224503 (* 1 = 0.224503 loss)
I1210 17:56:48.304643 22232 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1210 17:56:53.937093 22232 solver.cpp:218] Iteration 111100 (17.756 iter/s, 5.63189s/100 iters), loss = 0.338666
I1210 17:56:53.937602 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:56:53.937602 22232 solver.cpp:237]     Train net output #1: loss = 0.338666 (* 1 = 0.338666 loss)
I1210 17:56:53.937602 22232 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1210 17:56:59.591543 22232 solver.cpp:218] Iteration 111200 (17.6856 iter/s, 5.65433s/100 iters), loss = 0.277947
I1210 17:56:59.591543 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 17:56:59.591543 22232 solver.cpp:237]     Train net output #1: loss = 0.277947 (* 1 = 0.277947 loss)
I1210 17:56:59.591543 22232 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1210 17:57:05.233310 22232 solver.cpp:218] Iteration 111300 (17.7273 iter/s, 5.64103s/100 iters), loss = 0.375255
I1210 17:57:05.233310 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:57:05.233310 22232 solver.cpp:237]     Train net output #1: loss = 0.375255 (* 1 = 0.375255 loss)
I1210 17:57:05.233310 22232 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1210 17:57:10.879199 22232 solver.cpp:218] Iteration 111400 (17.7141 iter/s, 5.64521s/100 iters), loss = 0.370867
I1210 17:57:10.879199 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:57:10.879199 22232 solver.cpp:237]     Train net output #1: loss = 0.370867 (* 1 = 0.370867 loss)
I1210 17:57:10.879199 22232 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1210 17:57:16.241109 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:57:16.464119 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_111500.caffemodel
I1210 17:57:16.479624 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_111500.solverstate
I1210 17:57:16.484623 22232 solver.cpp:330] Iteration 111500, Testing net (#0)
I1210 17:57:16.484623 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:57:17.855238 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:57:17.907249 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6813
I1210 17:57:17.907249 22232 solver.cpp:397]     Test net output #1: loss = 1.24655 (* 1 = 1.24655 loss)
I1210 17:57:17.961264 22232 solver.cpp:218] Iteration 111500 (14.1203 iter/s, 7.082s/100 iters), loss = 0.225317
I1210 17:57:17.961264 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 17:57:17.961264 22232 solver.cpp:237]     Train net output #1: loss = 0.225317 (* 1 = 0.225317 loss)
I1210 17:57:17.961264 22232 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1210 17:57:23.597625 22232 solver.cpp:218] Iteration 111600 (17.7437 iter/s, 5.63581s/100 iters), loss = 0.414296
I1210 17:57:23.597625 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1210 17:57:23.597625 22232 solver.cpp:237]     Train net output #1: loss = 0.414296 (* 1 = 0.414296 loss)
I1210 17:57:23.597625 22232 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1210 17:57:29.238010 22232 solver.cpp:218] Iteration 111700 (17.7316 iter/s, 5.63965s/100 iters), loss = 0.314065
I1210 17:57:29.238010 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:57:29.238010 22232 solver.cpp:237]     Train net output #1: loss = 0.314065 (* 1 = 0.314065 loss)
I1210 17:57:29.238010 22232 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1210 17:57:34.869413 22232 solver.cpp:218] Iteration 111800 (17.758 iter/s, 5.63126s/100 iters), loss = 0.257909
I1210 17:57:34.869413 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1210 17:57:34.869413 22232 solver.cpp:237]     Train net output #1: loss = 0.257909 (* 1 = 0.257909 loss)
I1210 17:57:34.869413 22232 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1210 17:57:40.501839 22232 solver.cpp:218] Iteration 111900 (17.7556 iter/s, 5.63202s/100 iters), loss = 0.336625
I1210 17:57:40.501839 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:57:40.501839 22232 solver.cpp:237]     Train net output #1: loss = 0.336625 (* 1 = 0.336625 loss)
I1210 17:57:40.501839 22232 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1210 17:57:45.867220 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:57:46.088737 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_112000.caffemodel
I1210 17:57:46.108240 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_112000.solverstate
I1210 17:57:46.113241 22232 solver.cpp:330] Iteration 112000, Testing net (#0)
I1210 17:57:46.113241 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:57:47.478864 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:57:47.531957 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6833
I1210 17:57:47.531957 22232 solver.cpp:397]     Test net output #1: loss = 1.25068 (* 1 = 1.25068 loss)
I1210 17:57:47.587460 22232 solver.cpp:218] Iteration 112000 (14.1149 iter/s, 7.08471s/100 iters), loss = 0.254057
I1210 17:57:47.587460 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 17:57:47.587460 22232 solver.cpp:237]     Train net output #1: loss = 0.254057 (* 1 = 0.254057 loss)
I1210 17:57:47.587460 22232 sgd_solver.cpp:105] Iteration 112000, lr = 0.001
I1210 17:57:53.221897 22232 solver.cpp:218] Iteration 112100 (17.7482 iter/s, 5.63436s/100 iters), loss = 0.322598
I1210 17:57:53.221897 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:57:53.221897 22232 solver.cpp:237]     Train net output #1: loss = 0.322598 (* 1 = 0.322598 loss)
I1210 17:57:53.221897 22232 sgd_solver.cpp:105] Iteration 112100, lr = 0.001
I1210 17:57:58.863313 22232 solver.cpp:218] Iteration 112200 (17.7286 iter/s, 5.6406s/100 iters), loss = 0.314216
I1210 17:57:58.863313 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:57:58.863313 22232 solver.cpp:237]     Train net output #1: loss = 0.314216 (* 1 = 0.314216 loss)
I1210 17:57:58.863313 22232 sgd_solver.cpp:105] Iteration 112200, lr = 0.001
I1210 17:58:04.504716 22232 solver.cpp:218] Iteration 112300 (17.7253 iter/s, 5.64164s/100 iters), loss = 0.338712
I1210 17:58:04.505714 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:58:04.505714 22232 solver.cpp:237]     Train net output #1: loss = 0.338712 (* 1 = 0.338712 loss)
I1210 17:58:04.505714 22232 sgd_solver.cpp:105] Iteration 112300, lr = 0.001
I1210 17:58:10.144194 22232 solver.cpp:218] Iteration 112400 (17.7338 iter/s, 5.63894s/100 iters), loss = 0.461834
I1210 17:58:10.144194 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1210 17:58:10.144194 22232 solver.cpp:237]     Train net output #1: loss = 0.461834 (* 1 = 0.461834 loss)
I1210 17:58:10.144194 22232 sgd_solver.cpp:105] Iteration 112400, lr = 0.001
I1210 17:58:15.513656 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:58:15.734668 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_112500.caffemodel
I1210 17:58:15.752671 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_112500.solverstate
I1210 17:58:15.757670 22232 solver.cpp:330] Iteration 112500, Testing net (#0)
I1210 17:58:15.757670 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:58:17.124783 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:58:17.179785 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6833
I1210 17:58:17.179785 22232 solver.cpp:397]     Test net output #1: loss = 1.25448 (* 1 = 1.25448 loss)
I1210 17:58:17.233788 22232 solver.cpp:218] Iteration 112500 (14.1074 iter/s, 7.08848s/100 iters), loss = 0.296388
I1210 17:58:17.233788 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:58:17.233788 22232 solver.cpp:237]     Train net output #1: loss = 0.296388 (* 1 = 0.296388 loss)
I1210 17:58:17.233788 22232 sgd_solver.cpp:105] Iteration 112500, lr = 0.001
I1210 17:58:22.866201 22232 solver.cpp:218] Iteration 112600 (17.756 iter/s, 5.6319s/100 iters), loss = 0.404357
I1210 17:58:22.866201 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:58:22.866201 22232 solver.cpp:237]     Train net output #1: loss = 0.404357 (* 1 = 0.404357 loss)
I1210 17:58:22.866201 22232 sgd_solver.cpp:105] Iteration 112600, lr = 0.001
I1210 17:58:28.509639 22232 solver.cpp:218] Iteration 112700 (17.7198 iter/s, 5.64339s/100 iters), loss = 0.298986
I1210 17:58:28.509639 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:58:28.509639 22232 solver.cpp:237]     Train net output #1: loss = 0.298986 (* 1 = 0.298986 loss)
I1210 17:58:28.509639 22232 sgd_solver.cpp:105] Iteration 112700, lr = 0.001
I1210 17:58:34.146037 22232 solver.cpp:218] Iteration 112800 (17.7421 iter/s, 5.6363s/100 iters), loss = 0.331149
I1210 17:58:34.146037 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:58:34.146037 22232 solver.cpp:237]     Train net output #1: loss = 0.331149 (* 1 = 0.331149 loss)
I1210 17:58:34.146037 22232 sgd_solver.cpp:105] Iteration 112800, lr = 0.001
I1210 17:58:39.785024 22232 solver.cpp:218] Iteration 112900 (17.7374 iter/s, 5.63782s/100 iters), loss = 0.351448
I1210 17:58:39.785024 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 17:58:39.785024 22232 solver.cpp:237]     Train net output #1: loss = 0.351448 (* 1 = 0.351448 loss)
I1210 17:58:39.785024 22232 sgd_solver.cpp:105] Iteration 112900, lr = 0.001
I1210 17:58:45.151926 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:58:45.374956 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_113000.caffemodel
I1210 17:58:45.389466 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_113000.solverstate
I1210 17:58:45.393965 22232 solver.cpp:330] Iteration 113000, Testing net (#0)
I1210 17:58:45.393965 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:58:46.761081 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:58:46.815124 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6821
I1210 17:58:46.815124 22232 solver.cpp:397]     Test net output #1: loss = 1.25335 (* 1 = 1.25335 loss)
I1210 17:58:46.868120 22232 solver.cpp:218] Iteration 113000 (14.1182 iter/s, 7.08307s/100 iters), loss = 0.180882
I1210 17:58:46.868120 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1210 17:58:46.868120 22232 solver.cpp:237]     Train net output #1: loss = 0.180882 (* 1 = 0.180882 loss)
I1210 17:58:46.868120 22232 sgd_solver.cpp:105] Iteration 113000, lr = 0.001
I1210 17:58:52.513520 22232 solver.cpp:218] Iteration 113100 (17.7145 iter/s, 5.64508s/100 iters), loss = 0.367147
I1210 17:58:52.514519 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 17:58:52.514519 22232 solver.cpp:237]     Train net output #1: loss = 0.367147 (* 1 = 0.367147 loss)
I1210 17:58:52.514519 22232 sgd_solver.cpp:105] Iteration 113100, lr = 0.001
I1210 17:58:58.172999 22232 solver.cpp:218] Iteration 113200 (17.6735 iter/s, 5.6582s/100 iters), loss = 0.239632
I1210 17:58:58.172999 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1210 17:58:58.172999 22232 solver.cpp:237]     Train net output #1: loss = 0.239632 (* 1 = 0.239632 loss)
I1210 17:58:58.172999 22232 sgd_solver.cpp:105] Iteration 113200, lr = 0.001
I1210 17:59:03.819489 22232 solver.cpp:218] Iteration 113300 (17.7095 iter/s, 5.64668s/100 iters), loss = 0.360583
I1210 17:59:03.819489 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 17:59:03.819489 22232 solver.cpp:237]     Train net output #1: loss = 0.360583 (* 1 = 0.360583 loss)
I1210 17:59:03.819489 22232 sgd_solver.cpp:105] Iteration 113300, lr = 0.001
I1210 17:59:09.469952 22232 solver.cpp:218] Iteration 113400 (17.6996 iter/s, 5.64986s/100 iters), loss = 0.382571
I1210 17:59:09.469952 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:59:09.469952 22232 solver.cpp:237]     Train net output #1: loss = 0.382571 (* 1 = 0.382571 loss)
I1210 17:59:09.469952 22232 sgd_solver.cpp:105] Iteration 113400, lr = 0.001
I1210 17:59:14.839481 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:59:15.062505 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_113500.caffemodel
I1210 17:59:15.077503 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_113500.solverstate
I1210 17:59:15.081504 22232 solver.cpp:330] Iteration 113500, Testing net (#0)
I1210 17:59:15.081504 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:59:16.450681 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:59:16.504681 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6843
I1210 17:59:16.504681 22232 solver.cpp:397]     Test net output #1: loss = 1.25331 (* 1 = 1.25331 loss)
I1210 17:59:16.557694 22232 solver.cpp:218] Iteration 113500 (14.1095 iter/s, 7.08742s/100 iters), loss = 0.181877
I1210 17:59:16.557694 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1210 17:59:16.557694 22232 solver.cpp:237]     Train net output #1: loss = 0.181877 (* 1 = 0.181877 loss)
I1210 17:59:16.557694 22232 sgd_solver.cpp:105] Iteration 113500, lr = 0.001
I1210 17:59:22.177268 22232 solver.cpp:218] Iteration 113600 (17.7975 iter/s, 5.61878s/100 iters), loss = 0.350325
I1210 17:59:22.177268 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:59:22.177268 22232 solver.cpp:237]     Train net output #1: loss = 0.350325 (* 1 = 0.350325 loss)
I1210 17:59:22.177268 22232 sgd_solver.cpp:105] Iteration 113600, lr = 0.001
I1210 17:59:27.803833 22232 solver.cpp:218] Iteration 113700 (17.7757 iter/s, 5.62567s/100 iters), loss = 0.336003
I1210 17:59:27.803833 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:59:27.803833 22232 solver.cpp:237]     Train net output #1: loss = 0.336003 (* 1 = 0.336003 loss)
I1210 17:59:27.803833 22232 sgd_solver.cpp:105] Iteration 113700, lr = 0.001
I1210 17:59:33.426343 22232 solver.cpp:218] Iteration 113800 (17.7872 iter/s, 5.62201s/100 iters), loss = 0.344437
I1210 17:59:33.426343 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 17:59:33.426343 22232 solver.cpp:237]     Train net output #1: loss = 0.344437 (* 1 = 0.344437 loss)
I1210 17:59:33.426343 22232 sgd_solver.cpp:105] Iteration 113800, lr = 0.001
I1210 17:59:39.052846 22232 solver.cpp:218] Iteration 113900 (17.772 iter/s, 5.62684s/100 iters), loss = 0.344762
I1210 17:59:39.053848 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1210 17:59:39.053848 22232 solver.cpp:237]     Train net output #1: loss = 0.344762 (* 1 = 0.344762 loss)
I1210 17:59:39.053848 22232 sgd_solver.cpp:105] Iteration 113900, lr = 0.001
I1210 17:59:44.404533 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:59:44.627053 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_114000.caffemodel
I1210 17:59:44.641558 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_114000.solverstate
I1210 17:59:44.645560 22232 solver.cpp:330] Iteration 114000, Testing net (#0)
I1210 17:59:44.645560 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 17:59:46.010674 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 17:59:46.065697 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6821
I1210 17:59:46.065697 22232 solver.cpp:397]     Test net output #1: loss = 1.25928 (* 1 = 1.25928 loss)
I1210 17:59:46.119696 22232 solver.cpp:218] Iteration 114000 (14.1535 iter/s, 7.06537s/100 iters), loss = 0.210151
I1210 17:59:46.119696 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1210 17:59:46.119696 22232 solver.cpp:237]     Train net output #1: loss = 0.210151 (* 1 = 0.210151 loss)
I1210 17:59:46.119696 22232 sgd_solver.cpp:105] Iteration 114000, lr = 0.001
I1210 17:59:51.756194 22232 solver.cpp:218] Iteration 114100 (17.7421 iter/s, 5.6363s/100 iters), loss = 0.304763
I1210 17:59:51.756194 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 17:59:51.756194 22232 solver.cpp:237]     Train net output #1: loss = 0.304763 (* 1 = 0.304763 loss)
I1210 17:59:51.756194 22232 sgd_solver.cpp:105] Iteration 114100, lr = 0.001
I1210 17:59:57.387780 22232 solver.cpp:218] Iteration 114200 (17.7579 iter/s, 5.63128s/100 iters), loss = 0.286536
I1210 17:59:57.387780 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 17:59:57.387780 22232 solver.cpp:237]     Train net output #1: loss = 0.286536 (* 1 = 0.286536 loss)
I1210 17:59:57.387780 22232 sgd_solver.cpp:105] Iteration 114200, lr = 0.001
I1210 18:00:03.040524 22232 solver.cpp:218] Iteration 114300 (17.6935 iter/s, 5.6518s/100 iters), loss = 0.347658
I1210 18:00:03.040524 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:00:03.040524 22232 solver.cpp:237]     Train net output #1: loss = 0.347658 (* 1 = 0.347658 loss)
I1210 18:00:03.040524 22232 sgd_solver.cpp:105] Iteration 114300, lr = 0.001
I1210 18:00:08.668059 22232 solver.cpp:218] Iteration 114400 (17.7691 iter/s, 5.62774s/100 iters), loss = 0.335857
I1210 18:00:08.668059 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1210 18:00:08.668059 22232 solver.cpp:237]     Train net output #1: loss = 0.335857 (* 1 = 0.335857 loss)
I1210 18:00:08.668059 22232 sgd_solver.cpp:105] Iteration 114400, lr = 0.001
I1210 18:00:14.025640 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:00:14.248178 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_114500.caffemodel
I1210 18:00:14.269680 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_114500.solverstate
I1210 18:00:14.273680 22232 solver.cpp:330] Iteration 114500, Testing net (#0)
I1210 18:00:14.274682 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:00:15.637521 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:00:15.691495 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6816
I1210 18:00:15.691495 22232 solver.cpp:397]     Test net output #1: loss = 1.26654 (* 1 = 1.26654 loss)
I1210 18:00:15.745513 22232 solver.cpp:218] Iteration 114500 (14.1315 iter/s, 7.0764s/100 iters), loss = 0.209714
I1210 18:00:15.745513 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1210 18:00:15.745513 22232 solver.cpp:237]     Train net output #1: loss = 0.209714 (* 1 = 0.209714 loss)
I1210 18:00:15.745513 22232 sgd_solver.cpp:105] Iteration 114500, lr = 0.001
I1210 18:00:21.392562 22232 solver.cpp:218] Iteration 114600 (17.7101 iter/s, 5.64649s/100 iters), loss = 0.438247
I1210 18:00:21.392562 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:00:21.392562 22232 solver.cpp:237]     Train net output #1: loss = 0.438247 (* 1 = 0.438247 loss)
I1210 18:00:21.392562 22232 sgd_solver.cpp:105] Iteration 114600, lr = 0.001
I1210 18:00:27.027623 22232 solver.cpp:218] Iteration 114700 (17.7465 iter/s, 5.6349s/100 iters), loss = 0.272811
I1210 18:00:27.027623 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:00:27.027623 22232 solver.cpp:237]     Train net output #1: loss = 0.272811 (* 1 = 0.272811 loss)
I1210 18:00:27.027623 22232 sgd_solver.cpp:105] Iteration 114700, lr = 0.001
I1210 18:00:32.664098 22232 solver.cpp:218] Iteration 114800 (17.7411 iter/s, 5.63664s/100 iters), loss = 0.360943
I1210 18:00:32.665096 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 18:00:32.665096 22232 solver.cpp:237]     Train net output #1: loss = 0.360943 (* 1 = 0.360943 loss)
I1210 18:00:32.665096 22232 sgd_solver.cpp:105] Iteration 114800, lr = 0.001
I1210 18:00:38.306406 22232 solver.cpp:218] Iteration 114900 (17.7271 iter/s, 5.64107s/100 iters), loss = 0.315621
I1210 18:00:38.306406 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:00:38.306406 22232 solver.cpp:237]     Train net output #1: loss = 0.315621 (* 1 = 0.315621 loss)
I1210 18:00:38.306406 22232 sgd_solver.cpp:105] Iteration 114900, lr = 0.001
I1210 18:00:43.666878 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:00:43.887943 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_115000.caffemodel
I1210 18:00:43.902930 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_115000.solverstate
I1210 18:00:43.906937 22232 solver.cpp:330] Iteration 115000, Testing net (#0)
I1210 18:00:43.906937 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:00:45.273000 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:00:45.327018 22232 solver.cpp:397]     Test net output #0: accuracy = 0.681
I1210 18:00:45.327018 22232 solver.cpp:397]     Test net output #1: loss = 1.26313 (* 1 = 1.26313 loss)
I1210 18:00:45.380643 22232 solver.cpp:218] Iteration 115000 (14.136 iter/s, 7.07414s/100 iters), loss = 0.252961
I1210 18:00:45.380643 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1210 18:00:45.380643 22232 solver.cpp:237]     Train net output #1: loss = 0.252961 (* 1 = 0.252961 loss)
I1210 18:00:45.380643 22232 sgd_solver.cpp:105] Iteration 115000, lr = 0.001
I1210 18:00:51.016680 22232 solver.cpp:218] Iteration 115100 (17.7456 iter/s, 5.6352s/100 iters), loss = 0.335607
I1210 18:00:51.016680 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1210 18:00:51.016680 22232 solver.cpp:237]     Train net output #1: loss = 0.335607 (* 1 = 0.335607 loss)
I1210 18:00:51.016680 22232 sgd_solver.cpp:105] Iteration 115100, lr = 0.001
I1210 18:00:56.656945 22232 solver.cpp:218] Iteration 115200 (17.7324 iter/s, 5.6394s/100 iters), loss = 0.257382
I1210 18:00:56.656945 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:00:56.656945 22232 solver.cpp:237]     Train net output #1: loss = 0.257382 (* 1 = 0.257382 loss)
I1210 18:00:56.656945 22232 sgd_solver.cpp:105] Iteration 115200, lr = 0.001
I1210 18:01:02.288038 22232 solver.cpp:218] Iteration 115300 (17.7592 iter/s, 5.6309s/100 iters), loss = 0.341668
I1210 18:01:02.288038 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:01:02.288038 22232 solver.cpp:237]     Train net output #1: loss = 0.341668 (* 1 = 0.341668 loss)
I1210 18:01:02.288038 22232 sgd_solver.cpp:105] Iteration 115300, lr = 0.001
I1210 18:01:07.923235 22232 solver.cpp:218] Iteration 115400 (17.7479 iter/s, 5.63448s/100 iters), loss = 0.289678
I1210 18:01:07.923235 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:01:07.923235 22232 solver.cpp:237]     Train net output #1: loss = 0.289678 (* 1 = 0.289678 loss)
I1210 18:01:07.923235 22232 sgd_solver.cpp:105] Iteration 115400, lr = 0.001
I1210 18:01:13.290624 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:01:13.511675 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_115500.caffemodel
I1210 18:01:13.530676 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_115500.solverstate
I1210 18:01:13.535676 22232 solver.cpp:330] Iteration 115500, Testing net (#0)
I1210 18:01:13.535676 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:01:14.905818 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:01:14.958818 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6794
I1210 18:01:14.958818 22232 solver.cpp:397]     Test net output #1: loss = 1.27493 (* 1 = 1.27493 loss)
I1210 18:01:15.013824 22232 solver.cpp:218] Iteration 115500 (14.1031 iter/s, 7.09063s/100 iters), loss = 0.246106
I1210 18:01:15.013824 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1210 18:01:15.013824 22232 solver.cpp:237]     Train net output #1: loss = 0.246106 (* 1 = 0.246106 loss)
I1210 18:01:15.013824 22232 sgd_solver.cpp:105] Iteration 115500, lr = 0.001
I1210 18:01:20.654193 22232 solver.cpp:218] Iteration 115600 (17.7307 iter/s, 5.63995s/100 iters), loss = 0.384671
I1210 18:01:20.654193 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1210 18:01:20.654193 22232 solver.cpp:237]     Train net output #1: loss = 0.384671 (* 1 = 0.384671 loss)
I1210 18:01:20.654193 22232 sgd_solver.cpp:105] Iteration 115600, lr = 0.001
I1210 18:01:26.290031 22232 solver.cpp:218] Iteration 115700 (17.7467 iter/s, 5.63486s/100 iters), loss = 0.275854
I1210 18:01:26.290031 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1210 18:01:26.290031 22232 solver.cpp:237]     Train net output #1: loss = 0.275854 (* 1 = 0.275854 loss)
I1210 18:01:26.290031 22232 sgd_solver.cpp:105] Iteration 115700, lr = 0.001
I1210 18:01:31.922475 22232 solver.cpp:218] Iteration 115800 (17.7561 iter/s, 5.63186s/100 iters), loss = 0.319376
I1210 18:01:31.922475 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:01:31.922475 22232 solver.cpp:237]     Train net output #1: loss = 0.319376 (* 1 = 0.319376 loss)
I1210 18:01:31.922475 22232 sgd_solver.cpp:105] Iteration 115800, lr = 0.001
I1210 18:01:37.556921 22232 solver.cpp:218] Iteration 115900 (17.7492 iter/s, 5.63406s/100 iters), loss = 0.358852
I1210 18:01:37.556921 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:01:37.556921 22232 solver.cpp:237]     Train net output #1: loss = 0.358852 (* 1 = 0.358852 loss)
I1210 18:01:37.556921 22232 sgd_solver.cpp:105] Iteration 115900, lr = 0.001
I1210 18:01:42.915484 21404 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:01:43.137501 22232 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_116000.caffemodel
I1210 18:01:43.151509 22232 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_iter_116000.solverstate
I1210 18:01:43.156498 22232 solver.cpp:330] Iteration 116000, Testing net (#0)
I1210 18:01:43.156498 22232 net.cpp:676] Ignoring source layer accuracy_training
I1210 18:01:44.522609 13776 data_layer.cpp:73] Restarting data prefetching from start.
I1210 18:01:44.576110 22232 solver.cpp:397]     Test net output #0: accuracy = 0.6811
I1210 18:01:44.576110 22232 solver.cpp:397]     Test net output #1: loss = 1.26614 (* 1 = 1.26614 loss)
I1210 18:01:44.629613 22232 solver.cpp:218] Iteration 116000 (14.1392 iter/s, 7.07252s/100 iters), loss = 0.277146
I1210 18:01:44.629613 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1210 18:01:44.629613 22232 solver.cpp:237]     Train net output #1: loss = 0.277146 (* 1 = 0.277146 loss)
I1210 18:01:44.629613 22232 sgd_solver.cpp:105] Iteration 116000, lr = 0.001
I1210 18:01:50.272109 22232 solver.cpp:218] Iteration 116100 (17.7253 iter/s, 5.64167s/100 iters), loss = 0.389345
I1210 18:01:50.272109 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1210 18:01:50.272109 22232 solver.cpp:237]     Train net output #1: loss = 0.389345 (* 1 = 0.389345 loss)
I1210 18:01:50.272109 22232 sgd_solver.cpp:105] Iteration 116100, lr = 0.001
I1210 18:01:55.920047 22232 solver.cpp:218] Iteration 116200 (17.7047 iter/s, 5.64822s/100 iters), loss = 0.308634
I1210 18:01:55.920047 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:01:55.920047 22232 solver.cpp:237]     Train net output #1: loss = 0.308634 (* 1 = 0.308634 loss)
I1210 18:01:55.920047 22232 sgd_solver.cpp:105] Iteration 116200, lr = 0.001
I1210 18:02:01.596076 22232 solver.cpp:218] Iteration 116300 (17.6218 iter/s, 5.67478s/100 iters), loss = 0.314581
I1210 18:02:01.596076 22232 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1210 18:02:01.596076 22232 solver.cpp:237]     Train net output #1: loss = 0.314581 (* 1 = 
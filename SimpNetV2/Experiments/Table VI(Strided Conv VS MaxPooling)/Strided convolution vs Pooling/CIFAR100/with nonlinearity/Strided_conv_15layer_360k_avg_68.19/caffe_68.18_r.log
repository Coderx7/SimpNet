
G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90000.solverstate 
I1211 06:26:53.469552  5076 caffe.cpp:219] Using GPUs 0
I1211 06:26:53.651062  5076 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1211 06:26:53.941905  5076 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 06:26:53.958899  5076 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1211 06:26:53.959385  5076 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 06:26:53.961386  5076 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 06:26:53.961386  5076 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_pool2_1
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_pool4_2
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1211 06:26:53.961887  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1211 06:26:53.962388  5076 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV2_WnonLin_360k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "pool2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_pool2_1"
  type: "BatchNorm"
  bottom: "pool2_1"
  top: "pool2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_pool2_1"
  type: "Scale"
  bottom: "pool2_1"
  top: "pool2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_pool2_1"
  type: "ReLU"
  bottom: "pool2_1"
  top: "pool2_1"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "pool4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_pool4_2"
  type: "BatchNorm"
  bottom: "pool4_2"
  top: "pool4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_pool4_2"
  type: "Scale"
  bottom: "pool4_2"
  top: "pool4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_pool4_2"
  type: "ReLU"
  bottom: "pool4_2"
  top: "pool4_2"
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 06:26:53.962895  5076 layer_factory.cpp:58] Creating layer cifar
I1211 06:26:53.968384  5076 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1211 06:26:53.969892  5076 net.cpp:84] Creating Layer cifar
I1211 06:26:53.969892  5076 net.cpp:380] cifar -> data
I1211 06:26:53.969892  5076 net.cpp:380] cifar -> label
I1211 06:26:53.970883  5076 data_layer.cpp:45] output data size: 100,3,32,32
I1211 06:26:53.976883  5076 net.cpp:122] Setting up cifar
I1211 06:26:53.976883  5076 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 06:26:53.976883  5076 net.cpp:129] Top shape: 100 (100)
I1211 06:26:53.976883  5076 net.cpp:137] Memory required for data: 1229200
I1211 06:26:53.976883  5076 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 06:26:53.976883  5076 net.cpp:84] Creating Layer label_cifar_1_split
I1211 06:26:53.976883  5076 net.cpp:406] label_cifar_1_split <- label
I1211 06:26:53.976883  5076 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 06:26:53.976883  5076 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 06:26:53.977383  5076 net.cpp:122] Setting up label_cifar_1_split
I1211 06:26:53.977383  5076 net.cpp:129] Top shape: 100 (100)
I1211 06:26:53.977383  5076 net.cpp:129] Top shape: 100 (100)
I1211 06:26:53.977383  5076 net.cpp:137] Memory required for data: 1230000
I1211 06:26:53.977383  5076 layer_factory.cpp:58] Creating layer conv1
I1211 06:26:53.977383  5076 net.cpp:84] Creating Layer conv1
I1211 06:26:53.977383  5076 net.cpp:406] conv1 <- data
I1211 06:26:53.977383  5076 net.cpp:380] conv1 -> conv1
I1211 06:26:53.979884 17008 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 06:26:54.228384  5076 net.cpp:122] Setting up conv1
I1211 06:26:54.228384  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:26:54.228384  5076 net.cpp:137] Memory required for data: 13518000
I1211 06:26:54.228384  5076 layer_factory.cpp:58] Creating layer bn1
I1211 06:26:54.228384  5076 net.cpp:84] Creating Layer bn1
I1211 06:26:54.228384  5076 net.cpp:406] bn1 <- conv1
I1211 06:26:54.228384  5076 net.cpp:367] bn1 -> conv1 (in-place)
I1211 06:26:54.228884  5076 net.cpp:122] Setting up bn1
I1211 06:26:54.228884  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:26:54.228884  5076 net.cpp:137] Memory required for data: 25806000
I1211 06:26:54.228884  5076 layer_factory.cpp:58] Creating layer scale1
I1211 06:26:54.228884  5076 net.cpp:84] Creating Layer scale1
I1211 06:26:54.228884  5076 net.cpp:406] scale1 <- conv1
I1211 06:26:54.228884  5076 net.cpp:367] scale1 -> conv1 (in-place)
I1211 06:26:54.228884  5076 layer_factory.cpp:58] Creating layer scale1
I1211 06:26:54.228884  5076 net.cpp:122] Setting up scale1
I1211 06:26:54.228884  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:26:54.228884  5076 net.cpp:137] Memory required for data: 38094000
I1211 06:26:54.228884  5076 layer_factory.cpp:58] Creating layer relu1
I1211 06:26:54.228884  5076 net.cpp:84] Creating Layer relu1
I1211 06:26:54.228884  5076 net.cpp:406] relu1 <- conv1
I1211 06:26:54.228884  5076 net.cpp:367] relu1 -> conv1 (in-place)
I1211 06:26:54.229384  5076 net.cpp:122] Setting up relu1
I1211 06:26:54.229384  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:26:54.229384  5076 net.cpp:137] Memory required for data: 50382000
I1211 06:26:54.229384  5076 layer_factory.cpp:58] Creating layer conv1_0
I1211 06:26:54.229384  5076 net.cpp:84] Creating Layer conv1_0
I1211 06:26:54.229384  5076 net.cpp:406] conv1_0 <- conv1
I1211 06:26:54.229384  5076 net.cpp:380] conv1_0 -> conv1_0
I1211 06:26:54.231384  5076 net.cpp:122] Setting up conv1_0
I1211 06:26:54.231384  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.231384  5076 net.cpp:137] Memory required for data: 66766000
I1211 06:26:54.231384  5076 layer_factory.cpp:58] Creating layer bn1_0
I1211 06:26:54.231384  5076 net.cpp:84] Creating Layer bn1_0
I1211 06:26:54.231384  5076 net.cpp:406] bn1_0 <- conv1_0
I1211 06:26:54.231384  5076 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 06:26:54.231384  5076 net.cpp:122] Setting up bn1_0
I1211 06:26:54.231384  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.231384  5076 net.cpp:137] Memory required for data: 83150000
I1211 06:26:54.231384  5076 layer_factory.cpp:58] Creating layer scale1_0
I1211 06:26:54.231384  5076 net.cpp:84] Creating Layer scale1_0
I1211 06:26:54.231384  5076 net.cpp:406] scale1_0 <- conv1_0
I1211 06:26:54.231384  5076 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 06:26:54.231384  5076 layer_factory.cpp:58] Creating layer scale1_0
I1211 06:26:54.231384  5076 net.cpp:122] Setting up scale1_0
I1211 06:26:54.231384  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.231384  5076 net.cpp:137] Memory required for data: 99534000
I1211 06:26:54.231384  5076 layer_factory.cpp:58] Creating layer relu1_0
I1211 06:26:54.231884  5076 net.cpp:84] Creating Layer relu1_0
I1211 06:26:54.231884  5076 net.cpp:406] relu1_0 <- conv1_0
I1211 06:26:54.231884  5076 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 06:26:54.231884  5076 net.cpp:122] Setting up relu1_0
I1211 06:26:54.231884  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.231884  5076 net.cpp:137] Memory required for data: 115918000
I1211 06:26:54.231884  5076 layer_factory.cpp:58] Creating layer conv2
I1211 06:26:54.231884  5076 net.cpp:84] Creating Layer conv2
I1211 06:26:54.231884  5076 net.cpp:406] conv2 <- conv1_0
I1211 06:26:54.231884  5076 net.cpp:380] conv2 -> conv2
I1211 06:26:54.232883  5076 net.cpp:122] Setting up conv2
I1211 06:26:54.232883  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.232883  5076 net.cpp:137] Memory required for data: 132302000
I1211 06:26:54.232883  5076 layer_factory.cpp:58] Creating layer bn2
I1211 06:26:54.232883  5076 net.cpp:84] Creating Layer bn2
I1211 06:26:54.232883  5076 net.cpp:406] bn2 <- conv2
I1211 06:26:54.232883  5076 net.cpp:367] bn2 -> conv2 (in-place)
I1211 06:26:54.232883  5076 net.cpp:122] Setting up bn2
I1211 06:26:54.232883  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.232883  5076 net.cpp:137] Memory required for data: 148686000
I1211 06:26:54.232883  5076 layer_factory.cpp:58] Creating layer scale2
I1211 06:26:54.233383  5076 net.cpp:84] Creating Layer scale2
I1211 06:26:54.233383  5076 net.cpp:406] scale2 <- conv2
I1211 06:26:54.233383  5076 net.cpp:367] scale2 -> conv2 (in-place)
I1211 06:26:54.233383  5076 layer_factory.cpp:58] Creating layer scale2
I1211 06:26:54.233383  5076 net.cpp:122] Setting up scale2
I1211 06:26:54.233383  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.233383  5076 net.cpp:137] Memory required for data: 165070000
I1211 06:26:54.233383  5076 layer_factory.cpp:58] Creating layer relu2
I1211 06:26:54.233383  5076 net.cpp:84] Creating Layer relu2
I1211 06:26:54.233383  5076 net.cpp:406] relu2 <- conv2
I1211 06:26:54.233383  5076 net.cpp:367] relu2 -> conv2 (in-place)
I1211 06:26:54.233383  5076 net.cpp:122] Setting up relu2
I1211 06:26:54.233383  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.233383  5076 net.cpp:137] Memory required for data: 181454000
I1211 06:26:54.233383  5076 layer_factory.cpp:58] Creating layer conv2_1
I1211 06:26:54.233383  5076 net.cpp:84] Creating Layer conv2_1
I1211 06:26:54.233383  5076 net.cpp:406] conv2_1 <- conv2
I1211 06:26:54.233383  5076 net.cpp:380] conv2_1 -> conv2_1
I1211 06:26:54.234884  5076 net.cpp:122] Setting up conv2_1
I1211 06:26:54.234884  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.234884  5076 net.cpp:137] Memory required for data: 197838000
I1211 06:26:54.234884  5076 layer_factory.cpp:58] Creating layer bn2_1
I1211 06:26:54.234884  5076 net.cpp:84] Creating Layer bn2_1
I1211 06:26:54.234884  5076 net.cpp:406] bn2_1 <- conv2_1
I1211 06:26:54.234884  5076 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 06:26:54.234884  5076 net.cpp:122] Setting up bn2_1
I1211 06:26:54.234884  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.234884  5076 net.cpp:137] Memory required for data: 214222000
I1211 06:26:54.234884  5076 layer_factory.cpp:58] Creating layer scale2_1
I1211 06:26:54.234884  5076 net.cpp:84] Creating Layer scale2_1
I1211 06:26:54.234884  5076 net.cpp:406] scale2_1 <- conv2_1
I1211 06:26:54.234884  5076 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 06:26:54.234884  5076 layer_factory.cpp:58] Creating layer scale2_1
I1211 06:26:54.234884  5076 net.cpp:122] Setting up scale2_1
I1211 06:26:54.234884  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.234884  5076 net.cpp:137] Memory required for data: 230606000
I1211 06:26:54.234884  5076 layer_factory.cpp:58] Creating layer relu2_1
I1211 06:26:54.234884  5076 net.cpp:84] Creating Layer relu2_1
I1211 06:26:54.234884  5076 net.cpp:406] relu2_1 <- conv2_1
I1211 06:26:54.234884  5076 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 06:26:54.235383  5076 net.cpp:122] Setting up relu2_1
I1211 06:26:54.235383  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.235383  5076 net.cpp:137] Memory required for data: 246990000
I1211 06:26:54.235383  5076 layer_factory.cpp:58] Creating layer conv2_2
I1211 06:26:54.235383  5076 net.cpp:84] Creating Layer conv2_2
I1211 06:26:54.235383  5076 net.cpp:406] conv2_2 <- conv2_1
I1211 06:26:54.235383  5076 net.cpp:380] conv2_2 -> conv2_2
I1211 06:26:54.236884  5076 net.cpp:122] Setting up conv2_2
I1211 06:26:54.236884  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:26:54.236884  5076 net.cpp:137] Memory required for data: 267470000
I1211 06:26:54.236884  5076 layer_factory.cpp:58] Creating layer bn2_2
I1211 06:26:54.236884  5076 net.cpp:84] Creating Layer bn2_2
I1211 06:26:54.236884  5076 net.cpp:406] bn2_2 <- conv2_2
I1211 06:26:54.236884  5076 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 06:26:54.237385  5076 net.cpp:122] Setting up bn2_2
I1211 06:26:54.237385  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:26:54.237385  5076 net.cpp:137] Memory required for data: 287950000
I1211 06:26:54.237385  5076 layer_factory.cpp:58] Creating layer scale2_2
I1211 06:26:54.237385  5076 net.cpp:84] Creating Layer scale2_2
I1211 06:26:54.237385  5076 net.cpp:406] scale2_2 <- conv2_2
I1211 06:26:54.237385  5076 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 06:26:54.237385  5076 layer_factory.cpp:58] Creating layer scale2_2
I1211 06:26:54.237884  5076 net.cpp:122] Setting up scale2_2
I1211 06:26:54.237884  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:26:54.237884  5076 net.cpp:137] Memory required for data: 308430000
I1211 06:26:54.237884  5076 layer_factory.cpp:58] Creating layer relu2_2
I1211 06:26:54.237884  5076 net.cpp:84] Creating Layer relu2_2
I1211 06:26:54.237884  5076 net.cpp:406] relu2_2 <- conv2_2
I1211 06:26:54.237884  5076 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 06:26:54.238384  5076 net.cpp:122] Setting up relu2_2
I1211 06:26:54.238384  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:26:54.238384  5076 net.cpp:137] Memory required for data: 328910000
I1211 06:26:54.238384  5076 layer_factory.cpp:58] Creating layer pool2_1
I1211 06:26:54.238384  5076 net.cpp:84] Creating Layer pool2_1
I1211 06:26:54.238384  5076 net.cpp:406] pool2_1 <- conv2_2
I1211 06:26:54.238384  5076 net.cpp:380] pool2_1 -> pool2_1
I1211 06:26:54.239383  5076 net.cpp:122] Setting up pool2_1
I1211 06:26:54.239383  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.239383  5076 net.cpp:137] Memory required for data: 334030000
I1211 06:26:54.239383  5076 layer_factory.cpp:58] Creating layer bn2_pool2_1
I1211 06:26:54.239883  5076 net.cpp:84] Creating Layer bn2_pool2_1
I1211 06:26:54.239883  5076 net.cpp:406] bn2_pool2_1 <- pool2_1
I1211 06:26:54.239883  5076 net.cpp:367] bn2_pool2_1 -> pool2_1 (in-place)
I1211 06:26:54.239883  5076 net.cpp:122] Setting up bn2_pool2_1
I1211 06:26:54.239883  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.239883  5076 net.cpp:137] Memory required for data: 339150000
I1211 06:26:54.239883  5076 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 06:26:54.239883  5076 net.cpp:84] Creating Layer scale2_pool2_1
I1211 06:26:54.239883  5076 net.cpp:406] scale2_pool2_1 <- pool2_1
I1211 06:26:54.239883  5076 net.cpp:367] scale2_pool2_1 -> pool2_1 (in-place)
I1211 06:26:54.239883  5076 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 06:26:54.239883  5076 net.cpp:122] Setting up scale2_pool2_1
I1211 06:26:54.239883  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.239883  5076 net.cpp:137] Memory required for data: 344270000
I1211 06:26:54.239883  5076 layer_factory.cpp:58] Creating layer relu2_pool2_1
I1211 06:26:54.239883  5076 net.cpp:84] Creating Layer relu2_pool2_1
I1211 06:26:54.239883  5076 net.cpp:406] relu2_pool2_1 <- pool2_1
I1211 06:26:54.239883  5076 net.cpp:367] relu2_pool2_1 -> pool2_1 (in-place)
I1211 06:26:54.240383  5076 net.cpp:122] Setting up relu2_pool2_1
I1211 06:26:54.240383  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.240383  5076 net.cpp:137] Memory required for data: 349390000
I1211 06:26:54.240383  5076 layer_factory.cpp:58] Creating layer conv3
I1211 06:26:54.240383  5076 net.cpp:84] Creating Layer conv3
I1211 06:26:54.240383  5076 net.cpp:406] conv3 <- pool2_1
I1211 06:26:54.240383  5076 net.cpp:380] conv3 -> conv3
I1211 06:26:54.241884  5076 net.cpp:122] Setting up conv3
I1211 06:26:54.241884  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.241884  5076 net.cpp:137] Memory required for data: 354510000
I1211 06:26:54.241884  5076 layer_factory.cpp:58] Creating layer bn3
I1211 06:26:54.241884  5076 net.cpp:84] Creating Layer bn3
I1211 06:26:54.241884  5076 net.cpp:406] bn3 <- conv3
I1211 06:26:54.241884  5076 net.cpp:367] bn3 -> conv3 (in-place)
I1211 06:26:54.241884  5076 net.cpp:122] Setting up bn3
I1211 06:26:54.241884  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.241884  5076 net.cpp:137] Memory required for data: 359630000
I1211 06:26:54.241884  5076 layer_factory.cpp:58] Creating layer scale3
I1211 06:26:54.241884  5076 net.cpp:84] Creating Layer scale3
I1211 06:26:54.241884  5076 net.cpp:406] scale3 <- conv3
I1211 06:26:54.241884  5076 net.cpp:367] scale3 -> conv3 (in-place)
I1211 06:26:54.241884  5076 layer_factory.cpp:58] Creating layer scale3
I1211 06:26:54.241884  5076 net.cpp:122] Setting up scale3
I1211 06:26:54.241884  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.241884  5076 net.cpp:137] Memory required for data: 364750000
I1211 06:26:54.241884  5076 layer_factory.cpp:58] Creating layer relu3
I1211 06:26:54.241884  5076 net.cpp:84] Creating Layer relu3
I1211 06:26:54.241884  5076 net.cpp:406] relu3 <- conv3
I1211 06:26:54.241884  5076 net.cpp:367] relu3 -> conv3 (in-place)
I1211 06:26:54.242384  5076 net.cpp:122] Setting up relu3
I1211 06:26:54.242384  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.242384  5076 net.cpp:137] Memory required for data: 369870000
I1211 06:26:54.242384  5076 layer_factory.cpp:58] Creating layer conv3_1
I1211 06:26:54.242384  5076 net.cpp:84] Creating Layer conv3_1
I1211 06:26:54.242384  5076 net.cpp:406] conv3_1 <- conv3
I1211 06:26:54.242384  5076 net.cpp:380] conv3_1 -> conv3_1
I1211 06:26:54.244390  5076 net.cpp:122] Setting up conv3_1
I1211 06:26:54.244390  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.244390  5076 net.cpp:137] Memory required for data: 374990000
I1211 06:26:54.244390  5076 layer_factory.cpp:58] Creating layer bn3_1
I1211 06:26:54.244390  5076 net.cpp:84] Creating Layer bn3_1
I1211 06:26:54.244390  5076 net.cpp:406] bn3_1 <- conv3_1
I1211 06:26:54.244390  5076 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 06:26:54.244390  5076 net.cpp:122] Setting up bn3_1
I1211 06:26:54.244390  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.244390  5076 net.cpp:137] Memory required for data: 380110000
I1211 06:26:54.244390  5076 layer_factory.cpp:58] Creating layer scale3_1
I1211 06:26:54.244390  5076 net.cpp:84] Creating Layer scale3_1
I1211 06:26:54.244390  5076 net.cpp:406] scale3_1 <- conv3_1
I1211 06:26:54.244390  5076 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 06:26:54.244886  5076 layer_factory.cpp:58] Creating layer scale3_1
I1211 06:26:54.244886  5076 net.cpp:122] Setting up scale3_1
I1211 06:26:54.244886  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.244886  5076 net.cpp:137] Memory required for data: 385230000
I1211 06:26:54.244886  5076 layer_factory.cpp:58] Creating layer relu3_1
I1211 06:26:54.244886  5076 net.cpp:84] Creating Layer relu3_1
I1211 06:26:54.244886  5076 net.cpp:406] relu3_1 <- conv3_1
I1211 06:26:54.244886  5076 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 06:26:54.244886  5076 net.cpp:122] Setting up relu3_1
I1211 06:26:54.245388  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.245388  5076 net.cpp:137] Memory required for data: 390350000
I1211 06:26:54.245388  5076 layer_factory.cpp:58] Creating layer conv4
I1211 06:26:54.245388  5076 net.cpp:84] Creating Layer conv4
I1211 06:26:54.245388  5076 net.cpp:406] conv4 <- conv3_1
I1211 06:26:54.245388  5076 net.cpp:380] conv4 -> conv4
I1211 06:26:54.246889  5076 net.cpp:122] Setting up conv4
I1211 06:26:54.246889  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.246889  5076 net.cpp:137] Memory required for data: 395470000
I1211 06:26:54.246889  5076 layer_factory.cpp:58] Creating layer bn4
I1211 06:26:54.246889  5076 net.cpp:84] Creating Layer bn4
I1211 06:26:54.246889  5076 net.cpp:406] bn4 <- conv4
I1211 06:26:54.246889  5076 net.cpp:367] bn4 -> conv4 (in-place)
I1211 06:26:54.246889  5076 net.cpp:122] Setting up bn4
I1211 06:26:54.246889  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.246889  5076 net.cpp:137] Memory required for data: 400590000
I1211 06:26:54.246889  5076 layer_factory.cpp:58] Creating layer scale4
I1211 06:26:54.246889  5076 net.cpp:84] Creating Layer scale4
I1211 06:26:54.246889  5076 net.cpp:406] scale4 <- conv4
I1211 06:26:54.246889  5076 net.cpp:367] scale4 -> conv4 (in-place)
I1211 06:26:54.246889  5076 layer_factory.cpp:58] Creating layer scale4
I1211 06:26:54.247383  5076 net.cpp:122] Setting up scale4
I1211 06:26:54.247383  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.247383  5076 net.cpp:137] Memory required for data: 405710000
I1211 06:26:54.247383  5076 layer_factory.cpp:58] Creating layer relu4
I1211 06:26:54.247383  5076 net.cpp:84] Creating Layer relu4
I1211 06:26:54.247383  5076 net.cpp:406] relu4 <- conv4
I1211 06:26:54.247383  5076 net.cpp:367] relu4 -> conv4 (in-place)
I1211 06:26:54.247383  5076 net.cpp:122] Setting up relu4
I1211 06:26:54.247383  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.247383  5076 net.cpp:137] Memory required for data: 410830000
I1211 06:26:54.247383  5076 layer_factory.cpp:58] Creating layer conv4_1
I1211 06:26:54.247887  5076 net.cpp:84] Creating Layer conv4_1
I1211 06:26:54.247887  5076 net.cpp:406] conv4_1 <- conv4
I1211 06:26:54.247887  5076 net.cpp:380] conv4_1 -> conv4_1
I1211 06:26:54.248888  5076 net.cpp:122] Setting up conv4_1
I1211 06:26:54.248888  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.249387  5076 net.cpp:137] Memory required for data: 415950000
I1211 06:26:54.249387  5076 layer_factory.cpp:58] Creating layer bn4_1
I1211 06:26:54.249387  5076 net.cpp:84] Creating Layer bn4_1
I1211 06:26:54.249387  5076 net.cpp:406] bn4_1 <- conv4_1
I1211 06:26:54.249387  5076 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 06:26:54.249387  5076 net.cpp:122] Setting up bn4_1
I1211 06:26:54.249387  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.249387  5076 net.cpp:137] Memory required for data: 421070000
I1211 06:26:54.249387  5076 layer_factory.cpp:58] Creating layer scale4_1
I1211 06:26:54.249387  5076 net.cpp:84] Creating Layer scale4_1
I1211 06:26:54.249387  5076 net.cpp:406] scale4_1 <- conv4_1
I1211 06:26:54.249387  5076 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 06:26:54.249387  5076 layer_factory.cpp:58] Creating layer scale4_1
I1211 06:26:54.249884  5076 net.cpp:122] Setting up scale4_1
I1211 06:26:54.249884  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.249884  5076 net.cpp:137] Memory required for data: 426190000
I1211 06:26:54.249884  5076 layer_factory.cpp:58] Creating layer relu4_1
I1211 06:26:54.249884  5076 net.cpp:84] Creating Layer relu4_1
I1211 06:26:54.249884  5076 net.cpp:406] relu4_1 <- conv4_1
I1211 06:26:54.249884  5076 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 06:26:54.249884  5076 net.cpp:122] Setting up relu4_1
I1211 06:26:54.249884  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.250413  5076 net.cpp:137] Memory required for data: 431310000
I1211 06:26:54.250413  5076 layer_factory.cpp:58] Creating layer conv4_2
I1211 06:26:54.250413  5076 net.cpp:84] Creating Layer conv4_2
I1211 06:26:54.250413  5076 net.cpp:406] conv4_2 <- conv4_1
I1211 06:26:54.250413  5076 net.cpp:380] conv4_2 -> conv4_2
I1211 06:26:54.251893  5076 net.cpp:122] Setting up conv4_2
I1211 06:26:54.251893  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:26:54.251893  5076 net.cpp:137] Memory required for data: 437249200
I1211 06:26:54.251893  5076 layer_factory.cpp:58] Creating layer bn4_2
I1211 06:26:54.251893  5076 net.cpp:84] Creating Layer bn4_2
I1211 06:26:54.251893  5076 net.cpp:406] bn4_2 <- conv4_2
I1211 06:26:54.251893  5076 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 06:26:54.251893  5076 net.cpp:122] Setting up bn4_2
I1211 06:26:54.251893  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:26:54.251893  5076 net.cpp:137] Memory required for data: 443188400
I1211 06:26:54.251893  5076 layer_factory.cpp:58] Creating layer scale4_2
I1211 06:26:54.251893  5076 net.cpp:84] Creating Layer scale4_2
I1211 06:26:54.251893  5076 net.cpp:406] scale4_2 <- conv4_2
I1211 06:26:54.251893  5076 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 06:26:54.251893  5076 layer_factory.cpp:58] Creating layer scale4_2
I1211 06:26:54.252403  5076 net.cpp:122] Setting up scale4_2
I1211 06:26:54.252403  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:26:54.252403  5076 net.cpp:137] Memory required for data: 449127600
I1211 06:26:54.252403  5076 layer_factory.cpp:58] Creating layer relu4_2
I1211 06:26:54.252403  5076 net.cpp:84] Creating Layer relu4_2
I1211 06:26:54.252403  5076 net.cpp:406] relu4_2 <- conv4_2
I1211 06:26:54.252403  5076 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 06:26:54.252883  5076 net.cpp:122] Setting up relu4_2
I1211 06:26:54.252883  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:26:54.252883  5076 net.cpp:137] Memory required for data: 455066800
I1211 06:26:54.252883  5076 layer_factory.cpp:58] Creating layer pool4_2
I1211 06:26:54.252883  5076 net.cpp:84] Creating Layer pool4_2
I1211 06:26:54.252883  5076 net.cpp:406] pool4_2 <- conv4_2
I1211 06:26:54.252883  5076 net.cpp:380] pool4_2 -> pool4_2
I1211 06:26:54.253882  5076 net.cpp:122] Setting up pool4_2
I1211 06:26:54.253882  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.253882  5076 net.cpp:137] Memory required for data: 456551600
I1211 06:26:54.253882  5076 layer_factory.cpp:58] Creating layer bn4_pool4_2
I1211 06:26:54.253882  5076 net.cpp:84] Creating Layer bn4_pool4_2
I1211 06:26:54.253882  5076 net.cpp:406] bn4_pool4_2 <- pool4_2
I1211 06:26:54.253882  5076 net.cpp:367] bn4_pool4_2 -> pool4_2 (in-place)
I1211 06:26:54.253882  5076 net.cpp:122] Setting up bn4_pool4_2
I1211 06:26:54.254384  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.254384  5076 net.cpp:137] Memory required for data: 458036400
I1211 06:26:54.254384  5076 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 06:26:54.254384  5076 net.cpp:84] Creating Layer scale4_pool4_2
I1211 06:26:54.254384  5076 net.cpp:406] scale4_pool4_2 <- pool4_2
I1211 06:26:54.254384  5076 net.cpp:367] scale4_pool4_2 -> pool4_2 (in-place)
I1211 06:26:54.254384  5076 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 06:26:54.254384  5076 net.cpp:122] Setting up scale4_pool4_2
I1211 06:26:54.254384  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.254384  5076 net.cpp:137] Memory required for data: 459521200
I1211 06:26:54.254384  5076 layer_factory.cpp:58] Creating layer relu4_pool4_2
I1211 06:26:54.254384  5076 net.cpp:84] Creating Layer relu4_pool4_2
I1211 06:26:54.254384  5076 net.cpp:406] relu4_pool4_2 <- pool4_2
I1211 06:26:54.254384  5076 net.cpp:367] relu4_pool4_2 -> pool4_2 (in-place)
I1211 06:26:54.254890  5076 net.cpp:122] Setting up relu4_pool4_2
I1211 06:26:54.254890  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.254890  5076 net.cpp:137] Memory required for data: 461006000
I1211 06:26:54.254890  5076 layer_factory.cpp:58] Creating layer conv4_0
I1211 06:26:54.254890  5076 net.cpp:84] Creating Layer conv4_0
I1211 06:26:54.254890  5076 net.cpp:406] conv4_0 <- pool4_2
I1211 06:26:54.254890  5076 net.cpp:380] conv4_0 -> conv4_0
I1211 06:26:54.256384  5076 net.cpp:122] Setting up conv4_0
I1211 06:26:54.256384  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.256384  5076 net.cpp:137] Memory required for data: 462490800
I1211 06:26:54.256384  5076 layer_factory.cpp:58] Creating layer bn4_0
I1211 06:26:54.256384  5076 net.cpp:84] Creating Layer bn4_0
I1211 06:26:54.256384  5076 net.cpp:406] bn4_0 <- conv4_0
I1211 06:26:54.256384  5076 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 06:26:54.256384  5076 net.cpp:122] Setting up bn4_0
I1211 06:26:54.256384  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.256384  5076 net.cpp:137] Memory required for data: 463975600
I1211 06:26:54.256384  5076 layer_factory.cpp:58] Creating layer scale4_0
I1211 06:26:54.256384  5076 net.cpp:84] Creating Layer scale4_0
I1211 06:26:54.256384  5076 net.cpp:406] scale4_0 <- conv4_0
I1211 06:26:54.256384  5076 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 06:26:54.256902  5076 layer_factory.cpp:58] Creating layer scale4_0
I1211 06:26:54.256902  5076 net.cpp:122] Setting up scale4_0
I1211 06:26:54.256902  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.256902  5076 net.cpp:137] Memory required for data: 465460400
I1211 06:26:54.256902  5076 layer_factory.cpp:58] Creating layer relu4_0
I1211 06:26:54.256902  5076 net.cpp:84] Creating Layer relu4_0
I1211 06:26:54.256902  5076 net.cpp:406] relu4_0 <- conv4_0
I1211 06:26:54.256902  5076 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 06:26:54.256902  5076 net.cpp:122] Setting up relu4_0
I1211 06:26:54.256902  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.256902  5076 net.cpp:137] Memory required for data: 466945200
I1211 06:26:54.256902  5076 layer_factory.cpp:58] Creating layer conv11
I1211 06:26:54.256902  5076 net.cpp:84] Creating Layer conv11
I1211 06:26:54.256902  5076 net.cpp:406] conv11 <- conv4_0
I1211 06:26:54.256902  5076 net.cpp:380] conv11 -> conv11
I1211 06:26:54.258898  5076 net.cpp:122] Setting up conv11
I1211 06:26:54.258898  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:26:54.258898  5076 net.cpp:137] Memory required for data: 468737200
I1211 06:26:54.258898  5076 layer_factory.cpp:58] Creating layer bn_conv11
I1211 06:26:54.258898  5076 net.cpp:84] Creating Layer bn_conv11
I1211 06:26:54.258898  5076 net.cpp:406] bn_conv11 <- conv11
I1211 06:26:54.258898  5076 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 06:26:54.258898  5076 net.cpp:122] Setting up bn_conv11
I1211 06:26:54.258898  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:26:54.258898  5076 net.cpp:137] Memory required for data: 470529200
I1211 06:26:54.258898  5076 layer_factory.cpp:58] Creating layer scale_conv11
I1211 06:26:54.258898  5076 net.cpp:84] Creating Layer scale_conv11
I1211 06:26:54.258898  5076 net.cpp:406] scale_conv11 <- conv11
I1211 06:26:54.258898  5076 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 06:26:54.258898  5076 layer_factory.cpp:58] Creating layer scale_conv11
I1211 06:26:54.259397  5076 net.cpp:122] Setting up scale_conv11
I1211 06:26:54.259397  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:26:54.259397  5076 net.cpp:137] Memory required for data: 472321200
I1211 06:26:54.259397  5076 layer_factory.cpp:58] Creating layer relu_conv11
I1211 06:26:54.259397  5076 net.cpp:84] Creating Layer relu_conv11
I1211 06:26:54.259397  5076 net.cpp:406] relu_conv11 <- conv11
I1211 06:26:54.259397  5076 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 06:26:54.259397  5076 net.cpp:122] Setting up relu_conv11
I1211 06:26:54.259397  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:26:54.259397  5076 net.cpp:137] Memory required for data: 474113200
I1211 06:26:54.259397  5076 layer_factory.cpp:58] Creating layer conv12
I1211 06:26:54.259397  5076 net.cpp:84] Creating Layer conv12
I1211 06:26:54.259397  5076 net.cpp:406] conv12 <- conv11
I1211 06:26:54.259397  5076 net.cpp:380] conv12 -> conv12
I1211 06:26:54.260896  5076 net.cpp:122] Setting up conv12
I1211 06:26:54.260896  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:26:54.260896  5076 net.cpp:137] Memory required for data: 476417200
I1211 06:26:54.260896  5076 layer_factory.cpp:58] Creating layer bn_conv12
I1211 06:26:54.261384  5076 net.cpp:84] Creating Layer bn_conv12
I1211 06:26:54.261384  5076 net.cpp:406] bn_conv12 <- conv12
I1211 06:26:54.261384  5076 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 06:26:54.261384  5076 net.cpp:122] Setting up bn_conv12
I1211 06:26:54.261384  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:26:54.261384  5076 net.cpp:137] Memory required for data: 478721200
I1211 06:26:54.261384  5076 layer_factory.cpp:58] Creating layer scale_conv12
I1211 06:26:54.261384  5076 net.cpp:84] Creating Layer scale_conv12
I1211 06:26:54.261384  5076 net.cpp:406] scale_conv12 <- conv12
I1211 06:26:54.261384  5076 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 06:26:54.261384  5076 layer_factory.cpp:58] Creating layer scale_conv12
I1211 06:26:54.261384  5076 net.cpp:122] Setting up scale_conv12
I1211 06:26:54.261384  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:26:54.261384  5076 net.cpp:137] Memory required for data: 481025200
I1211 06:26:54.261384  5076 layer_factory.cpp:58] Creating layer relu_conv12
I1211 06:26:54.261384  5076 net.cpp:84] Creating Layer relu_conv12
I1211 06:26:54.261384  5076 net.cpp:406] relu_conv12 <- conv12
I1211 06:26:54.261384  5076 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 06:26:54.261893  5076 net.cpp:122] Setting up relu_conv12
I1211 06:26:54.261893  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:26:54.261893  5076 net.cpp:137] Memory required for data: 483329200
I1211 06:26:54.261893  5076 layer_factory.cpp:58] Creating layer poolcp6
I1211 06:26:54.261893  5076 net.cpp:84] Creating Layer poolcp6
I1211 06:26:54.261893  5076 net.cpp:406] poolcp6 <- conv12
I1211 06:26:54.261893  5076 net.cpp:380] poolcp6 -> poolcp6
I1211 06:26:54.261893  5076 net.cpp:122] Setting up poolcp6
I1211 06:26:54.261893  5076 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 06:26:54.261893  5076 net.cpp:137] Memory required for data: 483365200
I1211 06:26:54.261893  5076 layer_factory.cpp:58] Creating layer ip1
I1211 06:26:54.261893  5076 net.cpp:84] Creating Layer ip1
I1211 06:26:54.261893  5076 net.cpp:406] ip1 <- poolcp6
I1211 06:26:54.261893  5076 net.cpp:380] ip1 -> ip1
I1211 06:26:54.261893  5076 net.cpp:122] Setting up ip1
I1211 06:26:54.261893  5076 net.cpp:129] Top shape: 100 100 (10000)
I1211 06:26:54.261893  5076 net.cpp:137] Memory required for data: 483405200
I1211 06:26:54.261893  5076 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 06:26:54.261893  5076 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 06:26:54.261893  5076 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 06:26:54.261893  5076 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 06:26:54.261893  5076 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 06:26:54.262393  5076 net.cpp:122] Setting up ip1_ip1_0_split
I1211 06:26:54.262393  5076 net.cpp:129] Top shape: 100 100 (10000)
I1211 06:26:54.262393  5076 net.cpp:129] Top shape: 100 100 (10000)
I1211 06:26:54.262393  5076 net.cpp:137] Memory required for data: 483485200
I1211 06:26:54.262393  5076 layer_factory.cpp:58] Creating layer accuracy_training
I1211 06:26:54.262393  5076 net.cpp:84] Creating Layer accuracy_training
I1211 06:26:54.262393  5076 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1211 06:26:54.262393  5076 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1211 06:26:54.262393  5076 net.cpp:380] accuracy_training -> accuracy_training
I1211 06:26:54.262393  5076 net.cpp:122] Setting up accuracy_training
I1211 06:26:54.262393  5076 net.cpp:129] Top shape: (1)
I1211 06:26:54.262393  5076 net.cpp:137] Memory required for data: 483485204
I1211 06:26:54.262393  5076 layer_factory.cpp:58] Creating layer loss
I1211 06:26:54.262393  5076 net.cpp:84] Creating Layer loss
I1211 06:26:54.262393  5076 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 06:26:54.262393  5076 net.cpp:406] loss <- label_cifar_1_split_1
I1211 06:26:54.262393  5076 net.cpp:380] loss -> loss
I1211 06:26:54.262393  5076 layer_factory.cpp:58] Creating layer loss
I1211 06:26:54.262892  5076 net.cpp:122] Setting up loss
I1211 06:26:54.262892  5076 net.cpp:129] Top shape: (1)
I1211 06:26:54.262892  5076 net.cpp:132]     with loss weight 1
I1211 06:26:54.262892  5076 net.cpp:137] Memory required for data: 483485208
I1211 06:26:54.262892  5076 net.cpp:198] loss needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:200] accuracy_training does not need backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] ip1 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] poolcp6 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] relu_conv12 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] scale_conv12 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] bn_conv12 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] conv12 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] relu_conv11 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] scale_conv11 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] bn_conv11 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] conv11 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] relu4_0 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] scale4_0 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] bn4_0 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] conv4_0 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] relu4_pool4_2 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] scale4_pool4_2 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] bn4_pool4_2 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] pool4_2 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] relu4_2 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] scale4_2 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] bn4_2 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] conv4_2 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] relu4_1 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] scale4_1 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] bn4_1 needs backward computation.
I1211 06:26:54.262892  5076 net.cpp:198] conv4_1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] relu4 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] scale4 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] bn4 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] conv4 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] relu3_1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] scale3_1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] bn3_1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] conv3_1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] relu3 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] scale3 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] bn3 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] conv3 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] relu2_pool2_1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] scale2_pool2_1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] bn2_pool2_1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] pool2_1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] relu2_2 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] scale2_2 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] bn2_2 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] conv2_2 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] relu2_1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] scale2_1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] bn2_1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] conv2_1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] relu2 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] scale2 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] bn2 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] conv2 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] relu1_0 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] scale1_0 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] bn1_0 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] conv1_0 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] relu1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] scale1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] bn1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:198] conv1 needs backward computation.
I1211 06:26:54.263386  5076 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 06:26:54.263386  5076 net.cpp:200] cifar does not need backward computation.
I1211 06:26:54.263386  5076 net.cpp:242] This network produces output accuracy_training
I1211 06:26:54.263386  5076 net.cpp:242] This network produces output loss
I1211 06:26:54.263386  5076 net.cpp:255] Network initialization done.
I1211 06:26:54.264392  5076 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 06:26:54.264392  5076 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 06:26:54.264392  5076 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_pool2_1
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_pool4_2
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1211 06:26:54.264392  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1211 06:26:54.264894  5076 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV2_WnonLin_360k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "pool2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_pool2_1"
  type: "BatchNorm"
  bottom: "pool2_1"
  top: "pool2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_pool2_1"
  type: "Scale"
  bottom: "pool2_1"
  top: "pool2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_pool2_1"
  type: "ReLU"
  bottom: "pool2_1"
  top: "pool2_1"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "pool4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_pool4_2"
  type: "BatchNorm"
  bottom: "pool4_2"
  top: "pool4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_pool4_2"
  type: "Scale"
  bottom: "pool4_2"
  top: "pool4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_pool4_2"
  type: "ReLU"
  bottom: "pool4_2"
  top: "pool4_2"
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 06:26:54.290385  5076 layer_factory.cpp:58] Creating layer cifar
I1211 06:26:54.295883  5076 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1211 06:26:54.297384  5076 net.cpp:84] Creating Layer cifar
I1211 06:26:54.297384  5076 net.cpp:380] cifar -> data
I1211 06:26:54.297384  5076 net.cpp:380] cifar -> label
I1211 06:26:54.297384  5076 data_layer.cpp:45] output data size: 100,3,32,32
I1211 06:26:54.303400  5076 net.cpp:122] Setting up cifar
I1211 06:26:54.303400  5076 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 06:26:54.303400  5076 net.cpp:129] Top shape: 100 (100)
I1211 06:26:54.303400  5076 net.cpp:137] Memory required for data: 1229200
I1211 06:26:54.303400  5076 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 06:26:54.303400  5076 net.cpp:84] Creating Layer label_cifar_1_split
I1211 06:26:54.303400  5076 net.cpp:406] label_cifar_1_split <- label
I1211 06:26:54.303400  5076 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 06:26:54.303400  5076 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 06:26:54.303400  5076 net.cpp:122] Setting up label_cifar_1_split
I1211 06:26:54.303400  5076 net.cpp:129] Top shape: 100 (100)
I1211 06:26:54.303400  5076 net.cpp:129] Top shape: 100 (100)
I1211 06:26:54.303400  5076 net.cpp:137] Memory required for data: 1230000
I1211 06:26:54.303400  5076 layer_factory.cpp:58] Creating layer conv1
I1211 06:26:54.303884  5076 net.cpp:84] Creating Layer conv1
I1211 06:26:54.303884  5076 net.cpp:406] conv1 <- data
I1211 06:26:54.303884  5076 net.cpp:380] conv1 -> conv1
I1211 06:26:54.305389 17172 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 06:26:54.305886  5076 net.cpp:122] Setting up conv1
I1211 06:26:54.305886  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:26:54.305886  5076 net.cpp:137] Memory required for data: 13518000
I1211 06:26:54.305886  5076 layer_factory.cpp:58] Creating layer bn1
I1211 06:26:54.305886  5076 net.cpp:84] Creating Layer bn1
I1211 06:26:54.305886  5076 net.cpp:406] bn1 <- conv1
I1211 06:26:54.305886  5076 net.cpp:367] bn1 -> conv1 (in-place)
I1211 06:26:54.305886  5076 net.cpp:122] Setting up bn1
I1211 06:26:54.305886  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:26:54.305886  5076 net.cpp:137] Memory required for data: 25806000
I1211 06:26:54.305886  5076 layer_factory.cpp:58] Creating layer scale1
I1211 06:26:54.305886  5076 net.cpp:84] Creating Layer scale1
I1211 06:26:54.305886  5076 net.cpp:406] scale1 <- conv1
I1211 06:26:54.306383  5076 net.cpp:367] scale1 -> conv1 (in-place)
I1211 06:26:54.306383  5076 layer_factory.cpp:58] Creating layer scale1
I1211 06:26:54.306383  5076 net.cpp:122] Setting up scale1
I1211 06:26:54.306383  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:26:54.306383  5076 net.cpp:137] Memory required for data: 38094000
I1211 06:26:54.306383  5076 layer_factory.cpp:58] Creating layer relu1
I1211 06:26:54.306383  5076 net.cpp:84] Creating Layer relu1
I1211 06:26:54.306383  5076 net.cpp:406] relu1 <- conv1
I1211 06:26:54.306383  5076 net.cpp:367] relu1 -> conv1 (in-place)
I1211 06:26:54.306885  5076 net.cpp:122] Setting up relu1
I1211 06:26:54.306885  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:26:54.306885  5076 net.cpp:137] Memory required for data: 50382000
I1211 06:26:54.306885  5076 layer_factory.cpp:58] Creating layer conv1_0
I1211 06:26:54.306885  5076 net.cpp:84] Creating Layer conv1_0
I1211 06:26:54.306885  5076 net.cpp:406] conv1_0 <- conv1
I1211 06:26:54.306885  5076 net.cpp:380] conv1_0 -> conv1_0
I1211 06:26:54.308385  5076 net.cpp:122] Setting up conv1_0
I1211 06:26:54.308385  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.308885  5076 net.cpp:137] Memory required for data: 66766000
I1211 06:26:54.308885  5076 layer_factory.cpp:58] Creating layer bn1_0
I1211 06:26:54.308885  5076 net.cpp:84] Creating Layer bn1_0
I1211 06:26:54.308885  5076 net.cpp:406] bn1_0 <- conv1_0
I1211 06:26:54.308885  5076 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 06:26:54.308885  5076 net.cpp:122] Setting up bn1_0
I1211 06:26:54.308885  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.308885  5076 net.cpp:137] Memory required for data: 83150000
I1211 06:26:54.308885  5076 layer_factory.cpp:58] Creating layer scale1_0
I1211 06:26:54.308885  5076 net.cpp:84] Creating Layer scale1_0
I1211 06:26:54.308885  5076 net.cpp:406] scale1_0 <- conv1_0
I1211 06:26:54.308885  5076 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 06:26:54.308885  5076 layer_factory.cpp:58] Creating layer scale1_0
I1211 06:26:54.308885  5076 net.cpp:122] Setting up scale1_0
I1211 06:26:54.308885  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.308885  5076 net.cpp:137] Memory required for data: 99534000
I1211 06:26:54.308885  5076 layer_factory.cpp:58] Creating layer relu1_0
I1211 06:26:54.309384  5076 net.cpp:84] Creating Layer relu1_0
I1211 06:26:54.309384  5076 net.cpp:406] relu1_0 <- conv1_0
I1211 06:26:54.309384  5076 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 06:26:54.309384  5076 net.cpp:122] Setting up relu1_0
I1211 06:26:54.309384  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.309384  5076 net.cpp:137] Memory required for data: 115918000
I1211 06:26:54.309384  5076 layer_factory.cpp:58] Creating layer conv2
I1211 06:26:54.309384  5076 net.cpp:84] Creating Layer conv2
I1211 06:26:54.309384  5076 net.cpp:406] conv2 <- conv1_0
I1211 06:26:54.309384  5076 net.cpp:380] conv2 -> conv2
I1211 06:26:54.310885  5076 net.cpp:122] Setting up conv2
I1211 06:26:54.310885  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.310885  5076 net.cpp:137] Memory required for data: 132302000
I1211 06:26:54.310885  5076 layer_factory.cpp:58] Creating layer bn2
I1211 06:26:54.310885  5076 net.cpp:84] Creating Layer bn2
I1211 06:26:54.310885  5076 net.cpp:406] bn2 <- conv2
I1211 06:26:54.310885  5076 net.cpp:367] bn2 -> conv2 (in-place)
I1211 06:26:54.311384  5076 net.cpp:122] Setting up bn2
I1211 06:26:54.311384  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.311384  5076 net.cpp:137] Memory required for data: 148686000
I1211 06:26:54.311384  5076 layer_factory.cpp:58] Creating layer scale2
I1211 06:26:54.311384  5076 net.cpp:84] Creating Layer scale2
I1211 06:26:54.311384  5076 net.cpp:406] scale2 <- conv2
I1211 06:26:54.311384  5076 net.cpp:367] scale2 -> conv2 (in-place)
I1211 06:26:54.311384  5076 layer_factory.cpp:58] Creating layer scale2
I1211 06:26:54.311384  5076 net.cpp:122] Setting up scale2
I1211 06:26:54.311384  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.311384  5076 net.cpp:137] Memory required for data: 165070000
I1211 06:26:54.311384  5076 layer_factory.cpp:58] Creating layer relu2
I1211 06:26:54.311384  5076 net.cpp:84] Creating Layer relu2
I1211 06:26:54.311384  5076 net.cpp:406] relu2 <- conv2
I1211 06:26:54.311384  5076 net.cpp:367] relu2 -> conv2 (in-place)
I1211 06:26:54.311884  5076 net.cpp:122] Setting up relu2
I1211 06:26:54.311884  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.311884  5076 net.cpp:137] Memory required for data: 181454000
I1211 06:26:54.311884  5076 layer_factory.cpp:58] Creating layer conv2_1
I1211 06:26:54.311884  5076 net.cpp:84] Creating Layer conv2_1
I1211 06:26:54.311884  5076 net.cpp:406] conv2_1 <- conv2
I1211 06:26:54.311884  5076 net.cpp:380] conv2_1 -> conv2_1
I1211 06:26:54.313385  5076 net.cpp:122] Setting up conv2_1
I1211 06:26:54.313385  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.313385  5076 net.cpp:137] Memory required for data: 197838000
I1211 06:26:54.313385  5076 layer_factory.cpp:58] Creating layer bn2_1
I1211 06:26:54.313385  5076 net.cpp:84] Creating Layer bn2_1
I1211 06:26:54.313385  5076 net.cpp:406] bn2_1 <- conv2_1
I1211 06:26:54.313385  5076 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 06:26:54.313385  5076 net.cpp:122] Setting up bn2_1
I1211 06:26:54.313385  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.313385  5076 net.cpp:137] Memory required for data: 214222000
I1211 06:26:54.313385  5076 layer_factory.cpp:58] Creating layer scale2_1
I1211 06:26:54.313385  5076 net.cpp:84] Creating Layer scale2_1
I1211 06:26:54.313385  5076 net.cpp:406] scale2_1 <- conv2_1
I1211 06:26:54.313385  5076 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 06:26:54.313385  5076 layer_factory.cpp:58] Creating layer scale2_1
I1211 06:26:54.313884  5076 net.cpp:122] Setting up scale2_1
I1211 06:26:54.313884  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.313884  5076 net.cpp:137] Memory required for data: 230606000
I1211 06:26:54.313884  5076 layer_factory.cpp:58] Creating layer relu2_1
I1211 06:26:54.313884  5076 net.cpp:84] Creating Layer relu2_1
I1211 06:26:54.313884  5076 net.cpp:406] relu2_1 <- conv2_1
I1211 06:26:54.313884  5076 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 06:26:54.313884  5076 net.cpp:122] Setting up relu2_1
I1211 06:26:54.313884  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:26:54.313884  5076 net.cpp:137] Memory required for data: 246990000
I1211 06:26:54.313884  5076 layer_factory.cpp:58] Creating layer conv2_2
I1211 06:26:54.313884  5076 net.cpp:84] Creating Layer conv2_2
I1211 06:26:54.313884  5076 net.cpp:406] conv2_2 <- conv2_1
I1211 06:26:54.313884  5076 net.cpp:380] conv2_2 -> conv2_2
I1211 06:26:54.315384  5076 net.cpp:122] Setting up conv2_2
I1211 06:26:54.315384  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:26:54.315384  5076 net.cpp:137] Memory required for data: 267470000
I1211 06:26:54.315384  5076 layer_factory.cpp:58] Creating layer bn2_2
I1211 06:26:54.315384  5076 net.cpp:84] Creating Layer bn2_2
I1211 06:26:54.315384  5076 net.cpp:406] bn2_2 <- conv2_2
I1211 06:26:54.315384  5076 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 06:26:54.315884  5076 net.cpp:122] Setting up bn2_2
I1211 06:26:54.315884  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:26:54.315884  5076 net.cpp:137] Memory required for data: 287950000
I1211 06:26:54.315884  5076 layer_factory.cpp:58] Creating layer scale2_2
I1211 06:26:54.315884  5076 net.cpp:84] Creating Layer scale2_2
I1211 06:26:54.315884  5076 net.cpp:406] scale2_2 <- conv2_2
I1211 06:26:54.315884  5076 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 06:26:54.315884  5076 layer_factory.cpp:58] Creating layer scale2_2
I1211 06:26:54.315884  5076 net.cpp:122] Setting up scale2_2
I1211 06:26:54.315884  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:26:54.315884  5076 net.cpp:137] Memory required for data: 308430000
I1211 06:26:54.315884  5076 layer_factory.cpp:58] Creating layer relu2_2
I1211 06:26:54.315884  5076 net.cpp:84] Creating Layer relu2_2
I1211 06:26:54.315884  5076 net.cpp:406] relu2_2 <- conv2_2
I1211 06:26:54.315884  5076 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 06:26:54.316385  5076 net.cpp:122] Setting up relu2_2
I1211 06:26:54.316385  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:26:54.316385  5076 net.cpp:137] Memory required for data: 328910000
I1211 06:26:54.316385  5076 layer_factory.cpp:58] Creating layer pool2_1
I1211 06:26:54.316385  5076 net.cpp:84] Creating Layer pool2_1
I1211 06:26:54.316385  5076 net.cpp:406] pool2_1 <- conv2_2
I1211 06:26:54.316385  5076 net.cpp:380] pool2_1 -> pool2_1
I1211 06:26:54.317883  5076 net.cpp:122] Setting up pool2_1
I1211 06:26:54.317883  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.317883  5076 net.cpp:137] Memory required for data: 334030000
I1211 06:26:54.317883  5076 layer_factory.cpp:58] Creating layer bn2_pool2_1
I1211 06:26:54.317883  5076 net.cpp:84] Creating Layer bn2_pool2_1
I1211 06:26:54.317883  5076 net.cpp:406] bn2_pool2_1 <- pool2_1
I1211 06:26:54.317883  5076 net.cpp:367] bn2_pool2_1 -> pool2_1 (in-place)
I1211 06:26:54.318384  5076 net.cpp:122] Setting up bn2_pool2_1
I1211 06:26:54.318384  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.318384  5076 net.cpp:137] Memory required for data: 339150000
I1211 06:26:54.318384  5076 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 06:26:54.318384  5076 net.cpp:84] Creating Layer scale2_pool2_1
I1211 06:26:54.318384  5076 net.cpp:406] scale2_pool2_1 <- pool2_1
I1211 06:26:54.318384  5076 net.cpp:367] scale2_pool2_1 -> pool2_1 (in-place)
I1211 06:26:54.318384  5076 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 06:26:54.318384  5076 net.cpp:122] Setting up scale2_pool2_1
I1211 06:26:54.318384  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.318384  5076 net.cpp:137] Memory required for data: 344270000
I1211 06:26:54.318384  5076 layer_factory.cpp:58] Creating layer relu2_pool2_1
I1211 06:26:54.318384  5076 net.cpp:84] Creating Layer relu2_pool2_1
I1211 06:26:54.318384  5076 net.cpp:406] relu2_pool2_1 <- pool2_1
I1211 06:26:54.318384  5076 net.cpp:367] relu2_pool2_1 -> pool2_1 (in-place)
I1211 06:26:54.319383  5076 net.cpp:122] Setting up relu2_pool2_1
I1211 06:26:54.319383  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.319383  5076 net.cpp:137] Memory required for data: 349390000
I1211 06:26:54.319383  5076 layer_factory.cpp:58] Creating layer conv3
I1211 06:26:54.319383  5076 net.cpp:84] Creating Layer conv3
I1211 06:26:54.319383  5076 net.cpp:406] conv3 <- pool2_1
I1211 06:26:54.319383  5076 net.cpp:380] conv3 -> conv3
I1211 06:26:54.320384  5076 net.cpp:122] Setting up conv3
I1211 06:26:54.320384  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.320384  5076 net.cpp:137] Memory required for data: 354510000
I1211 06:26:54.320384  5076 layer_factory.cpp:58] Creating layer bn3
I1211 06:26:54.320384  5076 net.cpp:84] Creating Layer bn3
I1211 06:26:54.320884  5076 net.cpp:406] bn3 <- conv3
I1211 06:26:54.320884  5076 net.cpp:367] bn3 -> conv3 (in-place)
I1211 06:26:54.320884  5076 net.cpp:122] Setting up bn3
I1211 06:26:54.320884  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.320884  5076 net.cpp:137] Memory required for data: 359630000
I1211 06:26:54.320884  5076 layer_factory.cpp:58] Creating layer scale3
I1211 06:26:54.320884  5076 net.cpp:84] Creating Layer scale3
I1211 06:26:54.320884  5076 net.cpp:406] scale3 <- conv3
I1211 06:26:54.320884  5076 net.cpp:367] scale3 -> conv3 (in-place)
I1211 06:26:54.320884  5076 layer_factory.cpp:58] Creating layer scale3
I1211 06:26:54.321383  5076 net.cpp:122] Setting up scale3
I1211 06:26:54.321383  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.321383  5076 net.cpp:137] Memory required for data: 364750000
I1211 06:26:54.321383  5076 layer_factory.cpp:58] Creating layer relu3
I1211 06:26:54.321383  5076 net.cpp:84] Creating Layer relu3
I1211 06:26:54.321383  5076 net.cpp:406] relu3 <- conv3
I1211 06:26:54.321383  5076 net.cpp:367] relu3 -> conv3 (in-place)
I1211 06:26:54.321884  5076 net.cpp:122] Setting up relu3
I1211 06:26:54.321884  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.321884  5076 net.cpp:137] Memory required for data: 369870000
I1211 06:26:54.321884  5076 layer_factory.cpp:58] Creating layer conv3_1
I1211 06:26:54.321884  5076 net.cpp:84] Creating Layer conv3_1
I1211 06:26:54.321884  5076 net.cpp:406] conv3_1 <- conv3
I1211 06:26:54.321884  5076 net.cpp:380] conv3_1 -> conv3_1
I1211 06:26:54.323405  5076 net.cpp:122] Setting up conv3_1
I1211 06:26:54.323405  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.323405  5076 net.cpp:137] Memory required for data: 374990000
I1211 06:26:54.323405  5076 layer_factory.cpp:58] Creating layer bn3_1
I1211 06:26:54.323405  5076 net.cpp:84] Creating Layer bn3_1
I1211 06:26:54.323405  5076 net.cpp:406] bn3_1 <- conv3_1
I1211 06:26:54.323405  5076 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 06:26:54.323884  5076 net.cpp:122] Setting up bn3_1
I1211 06:26:54.323884  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.323884  5076 net.cpp:137] Memory required for data: 380110000
I1211 06:26:54.323884  5076 layer_factory.cpp:58] Creating layer scale3_1
I1211 06:26:54.323884  5076 net.cpp:84] Creating Layer scale3_1
I1211 06:26:54.323884  5076 net.cpp:406] scale3_1 <- conv3_1
I1211 06:26:54.323884  5076 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 06:26:54.323884  5076 layer_factory.cpp:58] Creating layer scale3_1
I1211 06:26:54.323884  5076 net.cpp:122] Setting up scale3_1
I1211 06:26:54.323884  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.323884  5076 net.cpp:137] Memory required for data: 385230000
I1211 06:26:54.323884  5076 layer_factory.cpp:58] Creating layer relu3_1
I1211 06:26:54.323884  5076 net.cpp:84] Creating Layer relu3_1
I1211 06:26:54.323884  5076 net.cpp:406] relu3_1 <- conv3_1
I1211 06:26:54.323884  5076 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 06:26:54.324404  5076 net.cpp:122] Setting up relu3_1
I1211 06:26:54.324404  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.324404  5076 net.cpp:137] Memory required for data: 390350000
I1211 06:26:54.324404  5076 layer_factory.cpp:58] Creating layer conv4
I1211 06:26:54.324404  5076 net.cpp:84] Creating Layer conv4
I1211 06:26:54.324404  5076 net.cpp:406] conv4 <- conv3_1
I1211 06:26:54.324404  5076 net.cpp:380] conv4 -> conv4
I1211 06:26:54.325387  5076 net.cpp:122] Setting up conv4
I1211 06:26:54.325886  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.325886  5076 net.cpp:137] Memory required for data: 395470000
I1211 06:26:54.325886  5076 layer_factory.cpp:58] Creating layer bn4
I1211 06:26:54.325886  5076 net.cpp:84] Creating Layer bn4
I1211 06:26:54.325886  5076 net.cpp:406] bn4 <- conv4
I1211 06:26:54.325886  5076 net.cpp:367] bn4 -> conv4 (in-place)
I1211 06:26:54.325886  5076 net.cpp:122] Setting up bn4
I1211 06:26:54.325886  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.325886  5076 net.cpp:137] Memory required for data: 400590000
I1211 06:26:54.325886  5076 layer_factory.cpp:58] Creating layer scale4
I1211 06:26:54.325886  5076 net.cpp:84] Creating Layer scale4
I1211 06:26:54.325886  5076 net.cpp:406] scale4 <- conv4
I1211 06:26:54.325886  5076 net.cpp:367] scale4 -> conv4 (in-place)
I1211 06:26:54.325886  5076 layer_factory.cpp:58] Creating layer scale4
I1211 06:26:54.325886  5076 net.cpp:122] Setting up scale4
I1211 06:26:54.325886  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.325886  5076 net.cpp:137] Memory required for data: 405710000
I1211 06:26:54.325886  5076 layer_factory.cpp:58] Creating layer relu4
I1211 06:26:54.325886  5076 net.cpp:84] Creating Layer relu4
I1211 06:26:54.325886  5076 net.cpp:406] relu4 <- conv4
I1211 06:26:54.325886  5076 net.cpp:367] relu4 -> conv4 (in-place)
I1211 06:26:54.326403  5076 net.cpp:122] Setting up relu4
I1211 06:26:54.326403  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.326403  5076 net.cpp:137] Memory required for data: 410830000
I1211 06:26:54.326403  5076 layer_factory.cpp:58] Creating layer conv4_1
I1211 06:26:54.326403  5076 net.cpp:84] Creating Layer conv4_1
I1211 06:26:54.326403  5076 net.cpp:406] conv4_1 <- conv4
I1211 06:26:54.326403  5076 net.cpp:380] conv4_1 -> conv4_1
I1211 06:26:54.327898  5076 net.cpp:122] Setting up conv4_1
I1211 06:26:54.327898  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.327898  5076 net.cpp:137] Memory required for data: 415950000
I1211 06:26:54.327898  5076 layer_factory.cpp:58] Creating layer bn4_1
I1211 06:26:54.327898  5076 net.cpp:84] Creating Layer bn4_1
I1211 06:26:54.327898  5076 net.cpp:406] bn4_1 <- conv4_1
I1211 06:26:54.327898  5076 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 06:26:54.327898  5076 net.cpp:122] Setting up bn4_1
I1211 06:26:54.327898  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.327898  5076 net.cpp:137] Memory required for data: 421070000
I1211 06:26:54.327898  5076 layer_factory.cpp:58] Creating layer scale4_1
I1211 06:26:54.327898  5076 net.cpp:84] Creating Layer scale4_1
I1211 06:26:54.327898  5076 net.cpp:406] scale4_1 <- conv4_1
I1211 06:26:54.327898  5076 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 06:26:54.327898  5076 layer_factory.cpp:58] Creating layer scale4_1
I1211 06:26:54.328398  5076 net.cpp:122] Setting up scale4_1
I1211 06:26:54.328398  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.328398  5076 net.cpp:137] Memory required for data: 426190000
I1211 06:26:54.328398  5076 layer_factory.cpp:58] Creating layer relu4_1
I1211 06:26:54.328398  5076 net.cpp:84] Creating Layer relu4_1
I1211 06:26:54.328398  5076 net.cpp:406] relu4_1 <- conv4_1
I1211 06:26:54.328398  5076 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 06:26:54.328398  5076 net.cpp:122] Setting up relu4_1
I1211 06:26:54.328398  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:26:54.328398  5076 net.cpp:137] Memory required for data: 431310000
I1211 06:26:54.328398  5076 layer_factory.cpp:58] Creating layer conv4_2
I1211 06:26:54.328398  5076 net.cpp:84] Creating Layer conv4_2
I1211 06:26:54.328398  5076 net.cpp:406] conv4_2 <- conv4_1
I1211 06:26:54.328398  5076 net.cpp:380] conv4_2 -> conv4_2
I1211 06:26:54.329895  5076 net.cpp:122] Setting up conv4_2
I1211 06:26:54.330384  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:26:54.330384  5076 net.cpp:137] Memory required for data: 437249200
I1211 06:26:54.330384  5076 layer_factory.cpp:58] Creating layer bn4_2
I1211 06:26:54.330384  5076 net.cpp:84] Creating Layer bn4_2
I1211 06:26:54.330384  5076 net.cpp:406] bn4_2 <- conv4_2
I1211 06:26:54.330384  5076 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 06:26:54.330384  5076 net.cpp:122] Setting up bn4_2
I1211 06:26:54.330384  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:26:54.330384  5076 net.cpp:137] Memory required for data: 443188400
I1211 06:26:54.330384  5076 layer_factory.cpp:58] Creating layer scale4_2
I1211 06:26:54.330384  5076 net.cpp:84] Creating Layer scale4_2
I1211 06:26:54.330384  5076 net.cpp:406] scale4_2 <- conv4_2
I1211 06:26:54.330384  5076 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 06:26:54.330384  5076 layer_factory.cpp:58] Creating layer scale4_2
I1211 06:26:54.330884  5076 net.cpp:122] Setting up scale4_2
I1211 06:26:54.330884  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:26:54.330884  5076 net.cpp:137] Memory required for data: 449127600
I1211 06:26:54.330884  5076 layer_factory.cpp:58] Creating layer relu4_2
I1211 06:26:54.330884  5076 net.cpp:84] Creating Layer relu4_2
I1211 06:26:54.330884  5076 net.cpp:406] relu4_2 <- conv4_2
I1211 06:26:54.330884  5076 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 06:26:54.330884  5076 net.cpp:122] Setting up relu4_2
I1211 06:26:54.330884  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:26:54.330884  5076 net.cpp:137] Memory required for data: 455066800
I1211 06:26:54.330884  5076 layer_factory.cpp:58] Creating layer pool4_2
I1211 06:26:54.330884  5076 net.cpp:84] Creating Layer pool4_2
I1211 06:26:54.330884  5076 net.cpp:406] pool4_2 <- conv4_2
I1211 06:26:54.330884  5076 net.cpp:380] pool4_2 -> pool4_2
I1211 06:26:54.332384  5076 net.cpp:122] Setting up pool4_2
I1211 06:26:54.332384  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.332384  5076 net.cpp:137] Memory required for data: 456551600
I1211 06:26:54.332384  5076 layer_factory.cpp:58] Creating layer bn4_pool4_2
I1211 06:26:54.332384  5076 net.cpp:84] Creating Layer bn4_pool4_2
I1211 06:26:54.332384  5076 net.cpp:406] bn4_pool4_2 <- pool4_2
I1211 06:26:54.332384  5076 net.cpp:367] bn4_pool4_2 -> pool4_2 (in-place)
I1211 06:26:54.332384  5076 net.cpp:122] Setting up bn4_pool4_2
I1211 06:26:54.332384  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.332384  5076 net.cpp:137] Memory required for data: 458036400
I1211 06:26:54.332384  5076 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 06:26:54.332384  5076 net.cpp:84] Creating Layer scale4_pool4_2
I1211 06:26:54.332384  5076 net.cpp:406] scale4_pool4_2 <- pool4_2
I1211 06:26:54.332384  5076 net.cpp:367] scale4_pool4_2 -> pool4_2 (in-place)
I1211 06:26:54.332384  5076 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 06:26:54.332885  5076 net.cpp:122] Setting up scale4_pool4_2
I1211 06:26:54.332885  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.332885  5076 net.cpp:137] Memory required for data: 459521200
I1211 06:26:54.332885  5076 layer_factory.cpp:58] Creating layer relu4_pool4_2
I1211 06:26:54.332885  5076 net.cpp:84] Creating Layer relu4_pool4_2
I1211 06:26:54.332885  5076 net.cpp:406] relu4_pool4_2 <- pool4_2
I1211 06:26:54.332885  5076 net.cpp:367] relu4_pool4_2 -> pool4_2 (in-place)
I1211 06:26:54.333384  5076 net.cpp:122] Setting up relu4_pool4_2
I1211 06:26:54.333384  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.333384  5076 net.cpp:137] Memory required for data: 461006000
I1211 06:26:54.333384  5076 layer_factory.cpp:58] Creating layer conv4_0
I1211 06:26:54.333384  5076 net.cpp:84] Creating Layer conv4_0
I1211 06:26:54.333384  5076 net.cpp:406] conv4_0 <- pool4_2
I1211 06:26:54.333384  5076 net.cpp:380] conv4_0 -> conv4_0
I1211 06:26:54.334705  5076 net.cpp:122] Setting up conv4_0
I1211 06:26:54.334705  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.334705  5076 net.cpp:137] Memory required for data: 462490800
I1211 06:26:54.334705  5076 layer_factory.cpp:58] Creating layer bn4_0
I1211 06:26:54.334705  5076 net.cpp:84] Creating Layer bn4_0
I1211 06:26:54.334705  5076 net.cpp:406] bn4_0 <- conv4_0
I1211 06:26:54.334705  5076 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 06:26:54.335206  5076 net.cpp:122] Setting up bn4_0
I1211 06:26:54.335206  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.335206  5076 net.cpp:137] Memory required for data: 463975600
I1211 06:26:54.335206  5076 layer_factory.cpp:58] Creating layer scale4_0
I1211 06:26:54.335206  5076 net.cpp:84] Creating Layer scale4_0
I1211 06:26:54.335206  5076 net.cpp:406] scale4_0 <- conv4_0
I1211 06:26:54.335206  5076 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 06:26:54.335206  5076 layer_factory.cpp:58] Creating layer scale4_0
I1211 06:26:54.335206  5076 net.cpp:122] Setting up scale4_0
I1211 06:26:54.335206  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.335206  5076 net.cpp:137] Memory required for data: 465460400
I1211 06:26:54.335206  5076 layer_factory.cpp:58] Creating layer relu4_0
I1211 06:26:54.335206  5076 net.cpp:84] Creating Layer relu4_0
I1211 06:26:54.335206  5076 net.cpp:406] relu4_0 <- conv4_0
I1211 06:26:54.335206  5076 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 06:26:54.335726  5076 net.cpp:122] Setting up relu4_0
I1211 06:26:54.335726  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:26:54.335726  5076 net.cpp:137] Memory required for data: 466945200
I1211 06:26:54.335726  5076 layer_factory.cpp:58] Creating layer conv11
I1211 06:26:54.335726  5076 net.cpp:84] Creating Layer conv11
I1211 06:26:54.335726  5076 net.cpp:406] conv11 <- conv4_0
I1211 06:26:54.335726  5076 net.cpp:380] conv11 -> conv11
I1211 06:26:54.337705  5076 net.cpp:122] Setting up conv11
I1211 06:26:54.337705  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:26:54.337705  5076 net.cpp:137] Memory required for data: 468737200
I1211 06:26:54.337705  5076 layer_factory.cpp:58] Creating layer bn_conv11
I1211 06:26:54.337705  5076 net.cpp:84] Creating Layer bn_conv11
I1211 06:26:54.337705  5076 net.cpp:406] bn_conv11 <- conv11
I1211 06:26:54.337705  5076 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 06:26:54.337705  5076 net.cpp:122] Setting up bn_conv11
I1211 06:26:54.337705  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:26:54.337705  5076 net.cpp:137] Memory required for data: 470529200
I1211 06:26:54.338225  5076 layer_factory.cpp:58] Creating layer scale_conv11
I1211 06:26:54.338225  5076 net.cpp:84] Creating Layer scale_conv11
I1211 06:26:54.338225  5076 net.cpp:406] scale_conv11 <- conv11
I1211 06:26:54.338225  5076 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 06:26:54.338225  5076 layer_factory.cpp:58] Creating layer scale_conv11
I1211 06:26:54.338225  5076 net.cpp:122] Setting up scale_conv11
I1211 06:26:54.338225  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:26:54.338225  5076 net.cpp:137] Memory required for data: 472321200
I1211 06:26:54.338225  5076 layer_factory.cpp:58] Creating layer relu_conv11
I1211 06:26:54.338225  5076 net.cpp:84] Creating Layer relu_conv11
I1211 06:26:54.338225  5076 net.cpp:406] relu_conv11 <- conv11
I1211 06:26:54.338225  5076 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 06:26:54.338726  5076 net.cpp:122] Setting up relu_conv11
I1211 06:26:54.338726  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:26:54.338726  5076 net.cpp:137] Memory required for data: 474113200
I1211 06:26:54.338726  5076 layer_factory.cpp:58] Creating layer conv12
I1211 06:26:54.338726  5076 net.cpp:84] Creating Layer conv12
I1211 06:26:54.338726  5076 net.cpp:406] conv12 <- conv11
I1211 06:26:54.338726  5076 net.cpp:380] conv12 -> conv12
I1211 06:26:54.340224  5076 net.cpp:122] Setting up conv12
I1211 06:26:54.340224  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:26:54.340708  5076 net.cpp:137] Memory required for data: 476417200
I1211 06:26:54.340708  5076 layer_factory.cpp:58] Creating layer bn_conv12
I1211 06:26:54.340708  5076 net.cpp:84] Creating Layer bn_conv12
I1211 06:26:54.340708  5076 net.cpp:406] bn_conv12 <- conv12
I1211 06:26:54.340708  5076 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 06:26:54.340708  5076 net.cpp:122] Setting up bn_conv12
I1211 06:26:54.340708  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:26:54.340708  5076 net.cpp:137] Memory required for data: 478721200
I1211 06:26:54.341207  5076 layer_factory.cpp:58] Creating layer scale_conv12
I1211 06:26:54.341207  5076 net.cpp:84] Creating Layer scale_conv12
I1211 06:26:54.341207  5076 net.cpp:406] scale_conv12 <- conv12
I1211 06:26:54.341207  5076 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 06:26:54.341207  5076 layer_factory.cpp:58] Creating layer scale_conv12
I1211 06:26:54.341207  5076 net.cpp:122] Setting up scale_conv12
I1211 06:26:54.341207  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:26:54.341207  5076 net.cpp:137] Memory required for data: 481025200
I1211 06:26:54.341207  5076 layer_factory.cpp:58] Creating layer relu_conv12
I1211 06:26:54.341207  5076 net.cpp:84] Creating Layer relu_conv12
I1211 06:26:54.341207  5076 net.cpp:406] relu_conv12 <- conv12
I1211 06:26:54.341207  5076 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 06:26:54.341207  5076 net.cpp:122] Setting up relu_conv12
I1211 06:26:54.341207  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:26:54.341207  5076 net.cpp:137] Memory required for data: 483329200
I1211 06:26:54.341207  5076 layer_factory.cpp:58] Creating layer poolcp6
I1211 06:26:54.341207  5076 net.cpp:84] Creating Layer poolcp6
I1211 06:26:54.341207  5076 net.cpp:406] poolcp6 <- conv12
I1211 06:26:54.341207  5076 net.cpp:380] poolcp6 -> poolcp6
I1211 06:26:54.341706  5076 net.cpp:122] Setting up poolcp6
I1211 06:26:54.341706  5076 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 06:26:54.341706  5076 net.cpp:137] Memory required for data: 483365200
I1211 06:26:54.341706  5076 layer_factory.cpp:58] Creating layer ip1
I1211 06:26:54.341706  5076 net.cpp:84] Creating Layer ip1
I1211 06:26:54.341706  5076 net.cpp:406] ip1 <- poolcp6
I1211 06:26:54.341706  5076 net.cpp:380] ip1 -> ip1
I1211 06:26:54.341706  5076 net.cpp:122] Setting up ip1
I1211 06:26:54.341706  5076 net.cpp:129] Top shape: 100 100 (10000)
I1211 06:26:54.341706  5076 net.cpp:137] Memory required for data: 483405200
I1211 06:26:54.341706  5076 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 06:26:54.341706  5076 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 06:26:54.341706  5076 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 06:26:54.341706  5076 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 06:26:54.341706  5076 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 06:26:54.341706  5076 net.cpp:122] Setting up ip1_ip1_0_split
I1211 06:26:54.341706  5076 net.cpp:129] Top shape: 100 100 (10000)
I1211 06:26:54.341706  5076 net.cpp:129] Top shape: 100 100 (10000)
I1211 06:26:54.341706  5076 net.cpp:137] Memory required for data: 483485200
I1211 06:26:54.341706  5076 layer_factory.cpp:58] Creating layer accuracy
I1211 06:26:54.341706  5076 net.cpp:84] Creating Layer accuracy
I1211 06:26:54.341706  5076 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1211 06:26:54.341706  5076 net.cpp:406] accuracy <- label_cifar_1_split_0
I1211 06:26:54.341706  5076 net.cpp:380] accuracy -> accuracy
I1211 06:26:54.341706  5076 net.cpp:122] Setting up accuracy
I1211 06:26:54.341706  5076 net.cpp:129] Top shape: (1)
I1211 06:26:54.341706  5076 net.cpp:137] Memory required for data: 483485204
I1211 06:26:54.341706  5076 layer_factory.cpp:58] Creating layer loss
I1211 06:26:54.341706  5076 net.cpp:84] Creating Layer loss
I1211 06:26:54.341706  5076 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 06:26:54.341706  5076 net.cpp:406] loss <- label_cifar_1_split_1
I1211 06:26:54.341706  5076 net.cpp:380] loss -> loss
I1211 06:26:54.341706  5076 layer_factory.cpp:58] Creating layer loss
I1211 06:26:54.342206  5076 net.cpp:122] Setting up loss
I1211 06:26:54.342206  5076 net.cpp:129] Top shape: (1)
I1211 06:26:54.342206  5076 net.cpp:132]     with loss weight 1
I1211 06:26:54.342206  5076 net.cpp:137] Memory required for data: 483485208
I1211 06:26:54.342206  5076 net.cpp:198] loss needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:200] accuracy does not need backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] ip1 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] poolcp6 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] relu_conv12 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] scale_conv12 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] bn_conv12 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] conv12 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] relu_conv11 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] scale_conv11 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] bn_conv11 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] conv11 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] relu4_0 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] scale4_0 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] bn4_0 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] conv4_0 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] relu4_pool4_2 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] scale4_pool4_2 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] bn4_pool4_2 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] pool4_2 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] relu4_2 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] scale4_2 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] bn4_2 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] conv4_2 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] relu4_1 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] scale4_1 needs backward computation.
I1211 06:26:54.342206  5076 net.cpp:198] bn4_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] conv4_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] relu4 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] scale4 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] bn4 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] conv4 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] relu3_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] scale3_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] bn3_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] conv3_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] relu3 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] scale3 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] bn3 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] conv3 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] relu2_pool2_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] scale2_pool2_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] bn2_pool2_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] pool2_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] relu2_2 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] scale2_2 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] bn2_2 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] conv2_2 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] relu2_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] scale2_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] bn2_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] conv2_1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] relu2 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] scale2 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] bn2 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] conv2 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] relu1_0 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] scale1_0 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] bn1_0 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] conv1_0 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] relu1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] scale1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] bn1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:198] conv1 needs backward computation.
I1211 06:26:54.342705  5076 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 06:26:54.342705  5076 net.cpp:200] cifar does not need backward computation.
I1211 06:26:54.342705  5076 net.cpp:242] This network produces output accuracy
I1211 06:26:54.342705  5076 net.cpp:242] This network produces output loss
I1211 06:26:54.342705  5076 net.cpp:255] Network initialization done.
I1211 06:26:54.342705  5076 solver.cpp:56] Solver scaffolding done.
I1211 06:26:54.347723  5076 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90000.solverstate
I1211 06:26:54.354707  5076 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90000.caffemodel
I1211 06:26:54.354707  5076 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 06:26:54.355226  5076 sgd_solver.cpp:318] SGDSolver: restoring history
I1211 06:26:54.359205  5076 caffe.cpp:249] Starting Optimization
I1211 06:26:54.359705  5076 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV2_WnonLin_360k
I1211 06:26:54.359705  5076 solver.cpp:273] Learning Rate Policy: multistep
I1211 06:26:54.362207  5076 solver.cpp:330] Iteration 90000, Testing net (#0)
I1211 06:26:54.364215  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:26:55.789609 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:26:55.842109  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5975
I1211 06:26:55.842607  5076 solver.cpp:397]     Test net output #1: loss = 1.57719 (* 1 = 1.57719 loss)
I1211 06:26:55.961567  5076 solver.cpp:218] Iteration 90000 (56225 iter/s, 1.60071s/100 iters), loss = 0.713892
I1211 06:26:55.961567  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:26:55.961567  5076 solver.cpp:237]     Train net output #1: loss = 0.713892 (* 1 = 0.713892 loss)
I1211 06:26:55.961567  5076 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1211 06:27:02.254082  5076 solver.cpp:218] Iteration 90100 (15.8922 iter/s, 6.29239s/100 iters), loss = 0.543353
I1211 06:27:02.254082  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 06:27:02.254082  5076 solver.cpp:237]     Train net output #1: loss = 0.543353 (* 1 = 0.543353 loss)
I1211 06:27:02.254082  5076 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1211 06:27:08.496956  5076 solver.cpp:218] Iteration 90200 (16.02 iter/s, 6.24219s/100 iters), loss = 0.550481
I1211 06:27:08.496956  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 06:27:08.496956  5076 solver.cpp:237]     Train net output #1: loss = 0.550481 (* 1 = 0.550481 loss)
I1211 06:27:08.496956  5076 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1211 06:27:14.782135  5076 solver.cpp:218] Iteration 90300 (15.9113 iter/s, 6.28485s/100 iters), loss = 0.677714
I1211 06:27:14.782135  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 06:27:14.782636  5076 solver.cpp:237]     Train net output #1: loss = 0.677714 (* 1 = 0.677714 loss)
I1211 06:27:14.782636  5076 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1211 06:27:21.025471  5076 solver.cpp:218] Iteration 90400 (16.0185 iter/s, 6.24279s/100 iters), loss = 0.757216
I1211 06:27:21.025471  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 06:27:21.025471  5076 solver.cpp:237]     Train net output #1: loss = 0.757216 (* 1 = 0.757216 loss)
I1211 06:27:21.025471  5076 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1211 06:27:26.966205 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:27:27.212206  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90500.caffemodel
I1211 06:27:27.228704  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90500.solverstate
I1211 06:27:27.233705  5076 solver.cpp:330] Iteration 90500, Testing net (#0)
I1211 06:27:27.233705  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:27:28.590206 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:27:28.643705  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5828
I1211 06:27:28.643705  5076 solver.cpp:397]     Test net output #1: loss = 1.62494 (* 1 = 1.62494 loss)
I1211 06:27:28.703307  5076 solver.cpp:218] Iteration 90500 (13.0253 iter/s, 7.67734s/100 iters), loss = 0.686173
I1211 06:27:28.703307  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 06:27:28.703307  5076 solver.cpp:237]     Train net output #1: loss = 0.686173 (* 1 = 0.686173 loss)
I1211 06:27:28.703807  5076 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1211 06:27:34.944424  5076 solver.cpp:218] Iteration 90600 (16.0247 iter/s, 6.24036s/100 iters), loss = 0.653809
I1211 06:27:34.944424  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 06:27:34.944424  5076 solver.cpp:237]     Train net output #1: loss = 0.653809 (* 1 = 0.653809 loss)
I1211 06:27:34.944424  5076 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1211 06:27:41.167589  5076 solver.cpp:218] Iteration 90700 (16.0702 iter/s, 6.22271s/100 iters), loss = 0.543867
I1211 06:27:41.167589  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:27:41.167589  5076 solver.cpp:237]     Train net output #1: loss = 0.543867 (* 1 = 0.543867 loss)
I1211 06:27:41.167589  5076 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1211 06:27:47.403709  5076 solver.cpp:218] Iteration 90800 (16.037 iter/s, 6.23559s/100 iters), loss = 0.866266
I1211 06:27:47.403709  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 06:27:47.403709  5076 solver.cpp:237]     Train net output #1: loss = 0.866266 (* 1 = 0.866266 loss)
I1211 06:27:47.403709  5076 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1211 06:27:53.648267  5076 solver.cpp:218] Iteration 90900 (16.0139 iter/s, 6.24457s/100 iters), loss = 0.704166
I1211 06:27:53.648767  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 06:27:53.648767  5076 solver.cpp:237]     Train net output #1: loss = 0.704166 (* 1 = 0.704166 loss)
I1211 06:27:53.648767  5076 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1211 06:27:59.579102 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:27:59.826092  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91000.caffemodel
I1211 06:27:59.846591  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91000.solverstate
I1211 06:27:59.851591  5076 solver.cpp:330] Iteration 91000, Testing net (#0)
I1211 06:27:59.852102  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:28:01.204107 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:28:01.257592  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5552
I1211 06:28:01.257592  5076 solver.cpp:397]     Test net output #1: loss = 1.7748 (* 1 = 1.7748 loss)
I1211 06:28:01.316592  5076 solver.cpp:218] Iteration 91000 (13.0419 iter/s, 7.66758s/100 iters), loss = 0.681424
I1211 06:28:01.316592  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 06:28:01.316592  5076 solver.cpp:237]     Train net output #1: loss = 0.681424 (* 1 = 0.681424 loss)
I1211 06:28:01.316592  5076 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1211 06:28:07.557231  5076 solver.cpp:218] Iteration 91100 (16.0248 iter/s, 6.24034s/100 iters), loss = 0.594308
I1211 06:28:07.557231  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 06:28:07.557231  5076 solver.cpp:237]     Train net output #1: loss = 0.594308 (* 1 = 0.594308 loss)
I1211 06:28:07.557231  5076 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1211 06:28:13.800283  5076 solver.cpp:218] Iteration 91200 (16.0194 iter/s, 6.24245s/100 iters), loss = 0.765016
I1211 06:28:13.800283  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 06:28:13.800283  5076 solver.cpp:237]     Train net output #1: loss = 0.765016 (* 1 = 0.765016 loss)
I1211 06:28:13.800283  5076 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1211 06:28:20.066076  5076 solver.cpp:218] Iteration 91300 (15.9614 iter/s, 6.26511s/100 iters), loss = 0.657888
I1211 06:28:20.066076  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 06:28:20.066076  5076 solver.cpp:237]     Train net output #1: loss = 0.657888 (* 1 = 0.657888 loss)
I1211 06:28:20.066076  5076 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1211 06:28:26.404039  5076 solver.cpp:218] Iteration 91400 (15.7785 iter/s, 6.33772s/100 iters), loss = 0.752448
I1211 06:28:26.404039  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 06:28:26.404039  5076 solver.cpp:237]     Train net output #1: loss = 0.752448 (* 1 = 0.752448 loss)
I1211 06:28:26.404039  5076 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1211 06:28:32.348716 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:28:32.595716  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91500.caffemodel
I1211 06:28:32.611217  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91500.solverstate
I1211 06:28:32.616217  5076 solver.cpp:330] Iteration 91500, Testing net (#0)
I1211 06:28:32.616217  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:28:33.975718 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:28:34.029223  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5836
I1211 06:28:34.029223  5076 solver.cpp:397]     Test net output #1: loss = 1.63652 (* 1 = 1.63652 loss)
I1211 06:28:34.089716  5076 solver.cpp:218] Iteration 91500 (13.0122 iter/s, 7.68512s/100 iters), loss = 0.598339
I1211 06:28:34.089716  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 06:28:34.089716  5076 solver.cpp:237]     Train net output #1: loss = 0.598339 (* 1 = 0.598339 loss)
I1211 06:28:34.089716  5076 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1211 06:28:40.357961  5076 solver.cpp:218] Iteration 91600 (15.9555 iter/s, 6.26742s/100 iters), loss = 0.696684
I1211 06:28:40.357961  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 06:28:40.357961  5076 solver.cpp:237]     Train net output #1: loss = 0.696684 (* 1 = 0.696684 loss)
I1211 06:28:40.357961  5076 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1211 06:28:46.625917  5076 solver.cpp:218] Iteration 91700 (15.9555 iter/s, 6.26743s/100 iters), loss = 0.573526
I1211 06:28:46.625917  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 06:28:46.625917  5076 solver.cpp:237]     Train net output #1: loss = 0.573527 (* 1 = 0.573527 loss)
I1211 06:28:46.625917  5076 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1211 06:28:52.893210  5076 solver.cpp:218] Iteration 91800 (15.9565 iter/s, 6.26705s/100 iters), loss = 0.678031
I1211 06:28:52.893210  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 06:28:52.893210  5076 solver.cpp:237]     Train net output #1: loss = 0.678031 (* 1 = 0.678031 loss)
I1211 06:28:52.893210  5076 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1211 06:28:59.264114  5076 solver.cpp:218] Iteration 91900 (15.6982 iter/s, 6.37015s/100 iters), loss = 0.848559
I1211 06:28:59.264114  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1211 06:28:59.264114  5076 solver.cpp:237]     Train net output #1: loss = 0.848559 (* 1 = 0.848559 loss)
I1211 06:28:59.264114  5076 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1211 06:29:05.229867 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:29:05.478368  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92000.caffemodel
I1211 06:29:05.495368  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92000.solverstate
I1211 06:29:05.500869  5076 solver.cpp:330] Iteration 92000, Testing net (#0)
I1211 06:29:05.500869  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:29:06.858875 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:29:06.911865  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5963
I1211 06:29:06.911865  5076 solver.cpp:397]     Test net output #1: loss = 1.57606 (* 1 = 1.57606 loss)
I1211 06:29:06.971366  5076 solver.cpp:218] Iteration 92000 (12.9752 iter/s, 7.707s/100 iters), loss = 0.559021
I1211 06:29:06.971366  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 06:29:06.971366  5076 solver.cpp:237]     Train net output #1: loss = 0.559021 (* 1 = 0.559021 loss)
I1211 06:29:06.971366  5076 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1211 06:29:13.233345  5076 solver.cpp:218] Iteration 92100 (15.9704 iter/s, 6.26159s/100 iters), loss = 0.683953
I1211 06:29:13.233845  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 06:29:13.233845  5076 solver.cpp:237]     Train net output #1: loss = 0.683953 (* 1 = 0.683953 loss)
I1211 06:29:13.233845  5076 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1211 06:29:19.559922  5076 solver.cpp:218] Iteration 92200 (15.8083 iter/s, 6.3258s/100 iters), loss = 0.672824
I1211 06:29:19.559922  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 06:29:19.559922  5076 solver.cpp:237]     Train net output #1: loss = 0.672824 (* 1 = 0.672824 loss)
I1211 06:29:19.559922  5076 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1211 06:29:25.835649  5076 solver.cpp:218] Iteration 92300 (15.935 iter/s, 6.27548s/100 iters), loss = 0.652465
I1211 06:29:25.835649  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:29:25.835649  5076 solver.cpp:237]     Train net output #1: loss = 0.652465 (* 1 = 0.652465 loss)
I1211 06:29:25.835649  5076 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1211 06:29:32.103298  5076 solver.cpp:218] Iteration 92400 (15.9571 iter/s, 6.26682s/100 iters), loss = 0.845512
I1211 06:29:32.103298  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.68
I1211 06:29:32.103298  5076 solver.cpp:237]     Train net output #1: loss = 0.845512 (* 1 = 0.845512 loss)
I1211 06:29:32.103298  5076 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1211 06:29:38.076766 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:29:38.328764  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92500.caffemodel
I1211 06:29:38.344764  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92500.solverstate
I1211 06:29:38.350263  5076 solver.cpp:330] Iteration 92500, Testing net (#0)
I1211 06:29:38.350263  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:29:39.711766 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:29:39.765266  5076 solver.cpp:397]     Test net output #0: accuracy = 0.598
I1211 06:29:39.765266  5076 solver.cpp:397]     Test net output #1: loss = 1.54264 (* 1 = 1.54264 loss)
I1211 06:29:39.824264  5076 solver.cpp:218] Iteration 92500 (12.9525 iter/s, 7.72051s/100 iters), loss = 0.614874
I1211 06:29:39.824264  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:29:39.824264  5076 solver.cpp:237]     Train net output #1: loss = 0.614874 (* 1 = 0.614874 loss)
I1211 06:29:39.824264  5076 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1211 06:29:46.150969  5076 solver.cpp:218] Iteration 92600 (15.8066 iter/s, 6.32648s/100 iters), loss = 0.697187
I1211 06:29:46.150969  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 06:29:46.150969  5076 solver.cpp:237]     Train net output #1: loss = 0.697187 (* 1 = 0.697187 loss)
I1211 06:29:46.150969  5076 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1211 06:29:52.412060  5076 solver.cpp:218] Iteration 92700 (15.9727 iter/s, 6.26068s/100 iters), loss = 0.621664
I1211 06:29:52.412559  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:29:52.412559  5076 solver.cpp:237]     Train net output #1: loss = 0.621664 (* 1 = 0.621664 loss)
I1211 06:29:52.412559  5076 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1211 06:29:58.667176  5076 solver.cpp:218] Iteration 92800 (15.9887 iter/s, 6.25441s/100 iters), loss = 0.653705
I1211 06:29:58.667176  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 06:29:58.667176  5076 solver.cpp:237]     Train net output #1: loss = 0.653705 (* 1 = 0.653705 loss)
I1211 06:29:58.667176  5076 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1211 06:30:04.866916  5076 solver.cpp:218] Iteration 92900 (16.1307 iter/s, 6.19935s/100 iters), loss = 0.886829
I1211 06:30:04.866916  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1211 06:30:04.866916  5076 solver.cpp:237]     Train net output #1: loss = 0.886829 (* 1 = 0.886829 loss)
I1211 06:30:04.866916  5076 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1211 06:30:10.730917 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:30:10.972918  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93000.caffemodel
I1211 06:30:10.988417  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93000.solverstate
I1211 06:30:10.993918  5076 solver.cpp:330] Iteration 93000, Testing net (#0)
I1211 06:30:10.993918  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:30:12.331918 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:30:12.384418  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5626
I1211 06:30:12.384418  5076 solver.cpp:397]     Test net output #1: loss = 1.74907 (* 1 = 1.74907 loss)
I1211 06:30:12.443418  5076 solver.cpp:218] Iteration 93000 (13.1994 iter/s, 7.5761s/100 iters), loss = 0.598283
I1211 06:30:12.443918  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 06:30:12.443918  5076 solver.cpp:237]     Train net output #1: loss = 0.598283 (* 1 = 0.598283 loss)
I1211 06:30:12.443918  5076 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1211 06:30:18.586072  5076 solver.cpp:218] Iteration 93100 (16.2814 iter/s, 6.14199s/100 iters), loss = 0.76181
I1211 06:30:18.586072  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 06:30:18.586072  5076 solver.cpp:237]     Train net output #1: loss = 0.76181 (* 1 = 0.76181 loss)
I1211 06:30:18.586072  5076 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1211 06:30:24.741914  5076 solver.cpp:218] Iteration 93200 (16.2459 iter/s, 6.1554s/100 iters), loss = 0.67388
I1211 06:30:24.741914  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 06:30:24.741914  5076 solver.cpp:237]     Train net output #1: loss = 0.67388 (* 1 = 0.67388 loss)
I1211 06:30:24.741914  5076 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1211 06:30:30.915617  5076 solver.cpp:218] Iteration 93300 (16.1989 iter/s, 6.17326s/100 iters), loss = 0.728333
I1211 06:30:30.915617  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 06:30:30.915617  5076 solver.cpp:237]     Train net output #1: loss = 0.728333 (* 1 = 0.728333 loss)
I1211 06:30:30.915617  5076 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1211 06:30:37.081320  5076 solver.cpp:218] Iteration 93400 (16.2204 iter/s, 6.16507s/100 iters), loss = 0.814311
I1211 06:30:37.081320  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1211 06:30:37.081320  5076 solver.cpp:237]     Train net output #1: loss = 0.814311 (* 1 = 0.814311 loss)
I1211 06:30:37.081320  5076 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1211 06:30:42.938321 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:30:43.180326  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93500.caffemodel
I1211 06:30:43.196826  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93500.solverstate
I1211 06:30:43.201827  5076 solver.cpp:330] Iteration 93500, Testing net (#0)
I1211 06:30:43.201827  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:30:44.544327 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:30:44.596827  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6059
I1211 06:30:44.596827  5076 solver.cpp:397]     Test net output #1: loss = 1.50283 (* 1 = 1.50283 loss)
I1211 06:30:44.655827  5076 solver.cpp:218] Iteration 93500 (13.2027 iter/s, 7.57424s/100 iters), loss = 0.647076
I1211 06:30:44.655827  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 06:30:44.655827  5076 solver.cpp:237]     Train net output #1: loss = 0.647076 (* 1 = 0.647076 loss)
I1211 06:30:44.655827  5076 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1211 06:30:50.824326  5076 solver.cpp:218] Iteration 93600 (16.2121 iter/s, 6.16825s/100 iters), loss = 0.615404
I1211 06:30:50.824326  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 06:30:50.824326  5076 solver.cpp:237]     Train net output #1: loss = 0.615404 (* 1 = 0.615404 loss)
I1211 06:30:50.824326  5076 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1211 06:30:56.983683  5076 solver.cpp:218] Iteration 93700 (16.2369 iter/s, 6.15881s/100 iters), loss = 0.567354
I1211 06:30:56.983683  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:30:56.983683  5076 solver.cpp:237]     Train net output #1: loss = 0.567354 (* 1 = 0.567354 loss)
I1211 06:30:56.983683  5076 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1211 06:31:03.151182  5076 solver.cpp:218] Iteration 93800 (16.2146 iter/s, 6.16728s/100 iters), loss = 0.889706
I1211 06:31:03.151684  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 06:31:03.151684  5076 solver.cpp:237]     Train net output #1: loss = 0.889705 (* 1 = 0.889705 loss)
I1211 06:31:03.151684  5076 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1211 06:31:09.312183  5076 solver.cpp:218] Iteration 93900 (16.2335 iter/s, 6.1601s/100 iters), loss = 0.890594
I1211 06:31:09.312183  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 06:31:09.312183  5076 solver.cpp:237]     Train net output #1: loss = 0.890594 (* 1 = 0.890594 loss)
I1211 06:31:09.312183  5076 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1211 06:31:15.159700 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:31:15.400701  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94000.caffemodel
I1211 06:31:15.417201  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94000.solverstate
I1211 06:31:15.421715  5076 solver.cpp:330] Iteration 94000, Testing net (#0)
I1211 06:31:15.422200  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:31:16.755242 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:31:16.807742  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5873
I1211 06:31:16.807742  5076 solver.cpp:397]     Test net output #1: loss = 1.59236 (* 1 = 1.59236 loss)
I1211 06:31:16.865754  5076 solver.cpp:218] Iteration 94000 (13.2394 iter/s, 7.5532s/100 iters), loss = 0.676301
I1211 06:31:16.865754  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:31:16.865754  5076 solver.cpp:237]     Train net output #1: loss = 0.6763 (* 1 = 0.6763 loss)
I1211 06:31:16.865754  5076 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1211 06:31:23.009014  5076 solver.cpp:218] Iteration 94100 (16.279 iter/s, 6.14287s/100 iters), loss = 0.638357
I1211 06:31:23.009014  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:31:23.009014  5076 solver.cpp:237]     Train net output #1: loss = 0.638357 (* 1 = 0.638357 loss)
I1211 06:31:23.009014  5076 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1211 06:31:29.169147  5076 solver.cpp:218] Iteration 94200 (16.2342 iter/s, 6.15984s/100 iters), loss = 0.552974
I1211 06:31:29.169147  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:31:29.169147  5076 solver.cpp:237]     Train net output #1: loss = 0.552973 (* 1 = 0.552973 loss)
I1211 06:31:29.169147  5076 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1211 06:31:35.323206  5076 solver.cpp:218] Iteration 94300 (16.2503 iter/s, 6.15373s/100 iters), loss = 0.795709
I1211 06:31:35.323206  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1211 06:31:35.323206  5076 solver.cpp:237]     Train net output #1: loss = 0.795709 (* 1 = 0.795709 loss)
I1211 06:31:35.323206  5076 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1211 06:31:41.462321  5076 solver.cpp:218] Iteration 94400 (16.291 iter/s, 6.13836s/100 iters), loss = 0.70563
I1211 06:31:41.462321  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 06:31:41.462321  5076 solver.cpp:237]     Train net output #1: loss = 0.70563 (* 1 = 0.70563 loss)
I1211 06:31:41.462321  5076 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1211 06:31:47.320032 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:31:47.562531  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94500.caffemodel
I1211 06:31:47.577531  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94500.solverstate
I1211 06:31:47.582032  5076 solver.cpp:330] Iteration 94500, Testing net (#0)
I1211 06:31:47.582532  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:31:48.920531 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:31:48.973031  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5918
I1211 06:31:48.973031  5076 solver.cpp:397]     Test net output #1: loss = 1.56579 (* 1 = 1.56579 loss)
I1211 06:31:49.032531  5076 solver.cpp:218] Iteration 94500 (13.2103 iter/s, 7.56985s/100 iters), loss = 0.670035
I1211 06:31:49.032531  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:31:49.032531  5076 solver.cpp:237]     Train net output #1: loss = 0.670035 (* 1 = 0.670035 loss)
I1211 06:31:49.032531  5076 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1211 06:31:55.197031  5076 solver.cpp:218] Iteration 94600 (16.2232 iter/s, 6.16401s/100 iters), loss = 0.651565
I1211 06:31:55.197031  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:31:55.197031  5076 solver.cpp:237]     Train net output #1: loss = 0.651565 (* 1 = 0.651565 loss)
I1211 06:31:55.197031  5076 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1211 06:32:01.358530  5076 solver.cpp:218] Iteration 94700 (16.2314 iter/s, 6.16092s/100 iters), loss = 0.536313
I1211 06:32:01.358530  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:32:01.358530  5076 solver.cpp:237]     Train net output #1: loss = 0.536312 (* 1 = 0.536312 loss)
I1211 06:32:01.358530  5076 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1211 06:32:07.538031  5076 solver.cpp:218] Iteration 94800 (16.1838 iter/s, 6.17901s/100 iters), loss = 0.773213
I1211 06:32:07.538031  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 06:32:07.538031  5076 solver.cpp:237]     Train net output #1: loss = 0.773212 (* 1 = 0.773212 loss)
I1211 06:32:07.538031  5076 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1211 06:32:13.706531  5076 solver.cpp:218] Iteration 94900 (16.2115 iter/s, 6.16847s/100 iters), loss = 0.860839
I1211 06:32:13.707031  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1211 06:32:13.707031  5076 solver.cpp:237]     Train net output #1: loss = 0.860839 (* 1 = 0.860839 loss)
I1211 06:32:13.707031  5076 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1211 06:32:19.563267 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:32:19.807283  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95000.caffemodel
I1211 06:32:19.823278  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95000.solverstate
I1211 06:32:19.828279  5076 solver.cpp:330] Iteration 95000, Testing net (#0)
I1211 06:32:19.828279  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:32:21.166777 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:32:21.219269  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5967
I1211 06:32:21.219269  5076 solver.cpp:397]     Test net output #1: loss = 1.57434 (* 1 = 1.57434 loss)
I1211 06:32:21.278795  5076 solver.cpp:218] Iteration 95000 (13.2078 iter/s, 7.57129s/100 iters), loss = 0.714598
I1211 06:32:21.278795  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 06:32:21.278795  5076 solver.cpp:237]     Train net output #1: loss = 0.714598 (* 1 = 0.714598 loss)
I1211 06:32:21.278795  5076 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1211 06:32:21.278795  5076 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1211 06:32:27.442497  5076 solver.cpp:218] Iteration 95100 (16.2244 iter/s, 6.16354s/100 iters), loss = 0.682036
I1211 06:32:27.442497  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:32:27.442497  5076 solver.cpp:237]     Train net output #1: loss = 0.682036 (* 1 = 0.682036 loss)
I1211 06:32:27.442497  5076 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1211 06:32:33.593714  5076 solver.cpp:218] Iteration 95200 (16.2585 iter/s, 6.15064s/100 iters), loss = 0.414368
I1211 06:32:33.593714  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:32:33.593714  5076 solver.cpp:237]     Train net output #1: loss = 0.414368 (* 1 = 0.414368 loss)
I1211 06:32:33.593714  5076 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1211 06:32:39.752679  5076 solver.cpp:218] Iteration 95300 (16.2375 iter/s, 6.15858s/100 iters), loss = 0.523122
I1211 06:32:39.752679  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:32:39.752679  5076 solver.cpp:237]     Train net output #1: loss = 0.523122 (* 1 = 0.523122 loss)
I1211 06:32:39.752679  5076 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1211 06:32:45.909179  5076 solver.cpp:218] Iteration 95400 (16.2446 iter/s, 6.15591s/100 iters), loss = 0.481116
I1211 06:32:45.909179  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 06:32:45.909179  5076 solver.cpp:237]     Train net output #1: loss = 0.481115 (* 1 = 0.481115 loss)
I1211 06:32:45.909179  5076 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1211 06:32:51.763190 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:32:52.005681  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95500.caffemodel
I1211 06:32:52.021180  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95500.solverstate
I1211 06:32:52.026181  5076 solver.cpp:330] Iteration 95500, Testing net (#0)
I1211 06:32:52.026181  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:32:53.364679 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:32:53.417179  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6736
I1211 06:32:53.417179  5076 solver.cpp:397]     Test net output #1: loss = 1.20457 (* 1 = 1.20457 loss)
I1211 06:32:53.475678  5076 solver.cpp:218] Iteration 95500 (13.2166 iter/s, 7.56625s/100 iters), loss = 0.470907
I1211 06:32:53.476179  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:32:53.476179  5076 solver.cpp:237]     Train net output #1: loss = 0.470907 (* 1 = 0.470907 loss)
I1211 06:32:53.476179  5076 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1211 06:32:59.639679  5076 solver.cpp:218] Iteration 95600 (16.2256 iter/s, 6.16311s/100 iters), loss = 0.482613
I1211 06:32:59.639679  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 06:32:59.639679  5076 solver.cpp:237]     Train net output #1: loss = 0.482613 (* 1 = 0.482613 loss)
I1211 06:32:59.639679  5076 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1211 06:33:05.786717  5076 solver.cpp:218] Iteration 95700 (16.269 iter/s, 6.14666s/100 iters), loss = 0.394952
I1211 06:33:05.786717  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:33:05.786717  5076 solver.cpp:237]     Train net output #1: loss = 0.394952 (* 1 = 0.394952 loss)
I1211 06:33:05.786717  5076 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1211 06:33:11.924940  5076 solver.cpp:218] Iteration 95800 (16.2918 iter/s, 6.13805s/100 iters), loss = 0.486494
I1211 06:33:11.924940  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:33:11.924940  5076 solver.cpp:237]     Train net output #1: loss = 0.486494 (* 1 = 0.486494 loss)
I1211 06:33:11.924940  5076 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1211 06:33:18.081934  5076 solver.cpp:218] Iteration 95900 (16.2429 iter/s, 6.15652s/100 iters), loss = 0.621552
I1211 06:33:18.081934  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 06:33:18.081934  5076 solver.cpp:237]     Train net output #1: loss = 0.621551 (* 1 = 0.621551 loss)
I1211 06:33:18.081934  5076 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1211 06:33:23.935923 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:33:24.178426  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96000.caffemodel
I1211 06:33:24.194423  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96000.solverstate
I1211 06:33:24.198918  5076 solver.cpp:330] Iteration 96000, Testing net (#0)
I1211 06:33:24.199419  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:33:25.535423 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:33:25.587923  5076 solver.cpp:397]     Test net output #0: accuracy = 0.674
I1211 06:33:25.587923  5076 solver.cpp:397]     Test net output #1: loss = 1.19445 (* 1 = 1.19445 loss)
I1211 06:33:25.646404  5076 solver.cpp:218] Iteration 96000 (13.221 iter/s, 7.5637s/100 iters), loss = 0.459577
I1211 06:33:25.646404  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:33:25.646404  5076 solver.cpp:237]     Train net output #1: loss = 0.459577 (* 1 = 0.459577 loss)
I1211 06:33:25.646404  5076 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1211 06:33:31.804639  5076 solver.cpp:218] Iteration 96100 (16.239 iter/s, 6.15802s/100 iters), loss = 0.50581
I1211 06:33:31.804639  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 06:33:31.804639  5076 solver.cpp:237]     Train net output #1: loss = 0.50581 (* 1 = 0.50581 loss)
I1211 06:33:31.804639  5076 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1211 06:33:38.067065  5076 solver.cpp:218] Iteration 96200 (15.9686 iter/s, 6.26231s/100 iters), loss = 0.39298
I1211 06:33:38.067065  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:33:38.067065  5076 solver.cpp:237]     Train net output #1: loss = 0.39298 (* 1 = 0.39298 loss)
I1211 06:33:38.067065  5076 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1211 06:33:44.265499  5076 solver.cpp:218] Iteration 96300 (16.1358 iter/s, 6.1974s/100 iters), loss = 0.420435
I1211 06:33:44.265499  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:33:44.265499  5076 solver.cpp:237]     Train net output #1: loss = 0.420435 (* 1 = 0.420435 loss)
I1211 06:33:44.265499  5076 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1211 06:33:50.475054  5076 solver.cpp:218] Iteration 96400 (16.105 iter/s, 6.20927s/100 iters), loss = 0.544574
I1211 06:33:50.475054  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:33:50.475054  5076 solver.cpp:237]     Train net output #1: loss = 0.544573 (* 1 = 0.544573 loss)
I1211 06:33:50.475054  5076 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1211 06:33:56.410542 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:33:56.653558  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96500.caffemodel
I1211 06:33:56.670557  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96500.solverstate
I1211 06:33:56.676556  5076 solver.cpp:330] Iteration 96500, Testing net (#0)
I1211 06:33:56.676556  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:33:58.011667 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:33:58.063675  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6784
I1211 06:33:58.063675  5076 solver.cpp:397]     Test net output #1: loss = 1.19269 (* 1 = 1.19269 loss)
I1211 06:33:58.122674  5076 solver.cpp:218] Iteration 96500 (13.0772 iter/s, 7.64692s/100 iters), loss = 0.41639
I1211 06:33:58.122674  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:33:58.122674  5076 solver.cpp:237]     Train net output #1: loss = 0.41639 (* 1 = 0.41639 loss)
I1211 06:33:58.122674  5076 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1211 06:34:04.307147  5076 solver.cpp:218] Iteration 96600 (16.1702 iter/s, 6.1842s/100 iters), loss = 0.444094
I1211 06:34:04.307147  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:34:04.307147  5076 solver.cpp:237]     Train net output #1: loss = 0.444094 (* 1 = 0.444094 loss)
I1211 06:34:04.307147  5076 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1211 06:34:10.469889  5076 solver.cpp:218] Iteration 96700 (16.2279 iter/s, 6.16224s/100 iters), loss = 0.356113
I1211 06:34:10.469889  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:34:10.469889  5076 solver.cpp:237]     Train net output #1: loss = 0.356113 (* 1 = 0.356113 loss)
I1211 06:34:10.469889  5076 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1211 06:34:16.708853  5076 solver.cpp:218] Iteration 96800 (16.0299 iter/s, 6.23834s/100 iters), loss = 0.447391
I1211 06:34:16.708853  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:34:16.708853  5076 solver.cpp:237]     Train net output #1: loss = 0.447391 (* 1 = 0.447391 loss)
I1211 06:34:16.708853  5076 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1211 06:34:22.954803  5076 solver.cpp:218] Iteration 96900 (16.0113 iter/s, 6.24557s/100 iters), loss = 0.535094
I1211 06:34:22.954803  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:34:22.954803  5076 solver.cpp:237]     Train net output #1: loss = 0.535094 (* 1 = 0.535094 loss)
I1211 06:34:22.954803  5076 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1211 06:34:28.837730 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:34:29.082777  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97000.caffemodel
I1211 06:34:29.096776  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97000.solverstate
I1211 06:34:29.102778  5076 solver.cpp:330] Iteration 97000, Testing net (#0)
I1211 06:34:29.102778  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:34:30.439867 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:34:30.491873  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 06:34:30.492872  5076 solver.cpp:397]     Test net output #1: loss = 1.18581 (* 1 = 1.18581 loss)
I1211 06:34:30.551869  5076 solver.cpp:218] Iteration 97000 (13.1636 iter/s, 7.59673s/100 iters), loss = 0.411706
I1211 06:34:30.551869  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:34:30.551869  5076 solver.cpp:237]     Train net output #1: loss = 0.411706 (* 1 = 0.411706 loss)
I1211 06:34:30.551869  5076 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1211 06:34:36.732342  5076 solver.cpp:218] Iteration 97100 (16.1819 iter/s, 6.17974s/100 iters), loss = 0.516968
I1211 06:34:36.732342  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 06:34:36.732342  5076 solver.cpp:237]     Train net output #1: loss = 0.516968 (* 1 = 0.516968 loss)
I1211 06:34:36.732342  5076 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1211 06:34:42.908665  5076 solver.cpp:218] Iteration 97200 (16.1898 iter/s, 6.17673s/100 iters), loss = 0.33098
I1211 06:34:42.908665  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 06:34:42.908665  5076 solver.cpp:237]     Train net output #1: loss = 0.33098 (* 1 = 0.33098 loss)
I1211 06:34:42.908665  5076 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1211 06:34:49.075659  5076 solver.cpp:218] Iteration 97300 (16.2176 iter/s, 6.16615s/100 iters), loss = 0.436356
I1211 06:34:49.076159  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:34:49.076159  5076 solver.cpp:237]     Train net output #1: loss = 0.436355 (* 1 = 0.436355 loss)
I1211 06:34:49.076159  5076 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1211 06:34:55.238607  5076 solver.cpp:218] Iteration 97400 (16.2267 iter/s, 6.16268s/100 iters), loss = 0.525615
I1211 06:34:55.238607  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:34:55.238607  5076 solver.cpp:237]     Train net output #1: loss = 0.525615 (* 1 = 0.525615 loss)
I1211 06:34:55.238607  5076 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1211 06:35:01.101107 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:35:01.344121  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97500.caffemodel
I1211 06:35:01.359122  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97500.solverstate
I1211 06:35:01.364126  5076 solver.cpp:330] Iteration 97500, Testing net (#0)
I1211 06:35:01.364126  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:35:02.704318 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:35:02.756317  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6803
I1211 06:35:02.756317  5076 solver.cpp:397]     Test net output #1: loss = 1.19157 (* 1 = 1.19157 loss)
I1211 06:35:02.814323  5076 solver.cpp:218] Iteration 97500 (13.2006 iter/s, 7.57543s/100 iters), loss = 0.353229
I1211 06:35:02.814323  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:35:02.814323  5076 solver.cpp:237]     Train net output #1: loss = 0.353229 (* 1 = 0.353229 loss)
I1211 06:35:02.814323  5076 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1211 06:35:08.973285  5076 solver.cpp:218] Iteration 97600 (16.2393 iter/s, 6.15791s/100 iters), loss = 0.509755
I1211 06:35:08.973285  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 06:35:08.973285  5076 solver.cpp:237]     Train net output #1: loss = 0.509755 (* 1 = 0.509755 loss)
I1211 06:35:08.973285  5076 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1211 06:35:15.131391  5076 solver.cpp:218] Iteration 97700 (16.2386 iter/s, 6.15815s/100 iters), loss = 0.296624
I1211 06:35:15.131391  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:35:15.131391  5076 solver.cpp:237]     Train net output #1: loss = 0.296624 (* 1 = 0.296624 loss)
I1211 06:35:15.131391  5076 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1211 06:35:21.390908  5076 solver.cpp:218] Iteration 97800 (15.9777 iter/s, 6.25874s/100 iters), loss = 0.356356
I1211 06:35:21.390908  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:35:21.390908  5076 solver.cpp:237]     Train net output #1: loss = 0.356355 (* 1 = 0.356355 loss)
I1211 06:35:21.390908  5076 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1211 06:35:27.656450  5076 solver.cpp:218] Iteration 97900 (15.9604 iter/s, 6.26552s/100 iters), loss = 0.52106
I1211 06:35:27.656450  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 06:35:27.656450  5076 solver.cpp:237]     Train net output #1: loss = 0.52106 (* 1 = 0.52106 loss)
I1211 06:35:27.656450  5076 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1211 06:35:33.606987 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:35:33.850010  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98000.caffemodel
I1211 06:35:33.866010  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98000.solverstate
I1211 06:35:33.871016  5076 solver.cpp:330] Iteration 98000, Testing net (#0)
I1211 06:35:33.871515  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:35:35.219128 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:35:35.273635  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6771
I1211 06:35:35.273635  5076 solver.cpp:397]     Test net output #1: loss = 1.19675 (* 1 = 1.19675 loss)
I1211 06:35:35.332139  5076 solver.cpp:218] Iteration 98000 (13.0292 iter/s, 7.67509s/100 iters), loss = 0.394482
I1211 06:35:35.332139  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:35:35.332139  5076 solver.cpp:237]     Train net output #1: loss = 0.394482 (* 1 = 0.394482 loss)
I1211 06:35:35.332139  5076 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1211 06:35:41.496646  5076 solver.cpp:218] Iteration 98100 (16.2246 iter/s, 6.16349s/100 iters), loss = 0.481615
I1211 06:35:41.496646  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:35:41.496646  5076 solver.cpp:237]     Train net output #1: loss = 0.481615 (* 1 = 0.481615 loss)
I1211 06:35:41.496646  5076 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1211 06:35:47.658123  5076 solver.cpp:218] Iteration 98200 (16.2296 iter/s, 6.16159s/100 iters), loss = 0.30535
I1211 06:35:47.658123  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 06:35:47.658123  5076 solver.cpp:237]     Train net output #1: loss = 0.30535 (* 1 = 0.30535 loss)
I1211 06:35:47.658123  5076 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1211 06:35:53.828582  5076 solver.cpp:218] Iteration 98300 (16.2077 iter/s, 6.16992s/100 iters), loss = 0.407447
I1211 06:35:53.828582  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:35:53.828582  5076 solver.cpp:237]     Train net output #1: loss = 0.407447 (* 1 = 0.407447 loss)
I1211 06:35:53.828582  5076 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1211 06:35:59.985079  5076 solver.cpp:218] Iteration 98400 (16.2442 iter/s, 6.15604s/100 iters), loss = 0.573677
I1211 06:35:59.985579  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 06:35:59.985579  5076 solver.cpp:237]     Train net output #1: loss = 0.573676 (* 1 = 0.573676 loss)
I1211 06:35:59.985579  5076 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1211 06:36:05.849551 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:36:06.091570  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98500.caffemodel
I1211 06:36:06.106570  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98500.solverstate
I1211 06:36:06.111572  5076 solver.cpp:330] Iteration 98500, Testing net (#0)
I1211 06:36:06.111572  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:36:07.449656 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:36:07.501665  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6775
I1211 06:36:07.501665  5076 solver.cpp:397]     Test net output #1: loss = 1.20099 (* 1 = 1.20099 loss)
I1211 06:36:07.560662  5076 solver.cpp:218] Iteration 98500 (13.2008 iter/s, 7.57528s/100 iters), loss = 0.339918
I1211 06:36:07.560662  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:36:07.560662  5076 solver.cpp:237]     Train net output #1: loss = 0.339918 (* 1 = 0.339918 loss)
I1211 06:36:07.560662  5076 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1211 06:36:13.740141  5076 solver.cpp:218] Iteration 98600 (16.1852 iter/s, 6.17847s/100 iters), loss = 0.411537
I1211 06:36:13.740141  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:36:13.740141  5076 solver.cpp:237]     Train net output #1: loss = 0.411537 (* 1 = 0.411537 loss)
I1211 06:36:13.740141  5076 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1211 06:36:19.900643  5076 solver.cpp:218] Iteration 98700 (16.233 iter/s, 6.16027s/100 iters), loss = 0.334502
I1211 06:36:19.900643  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:36:19.900643  5076 solver.cpp:237]     Train net output #1: loss = 0.334502 (* 1 = 0.334502 loss)
I1211 06:36:19.900643  5076 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1211 06:36:26.078397  5076 solver.cpp:218] Iteration 98800 (16.1877 iter/s, 6.17755s/100 iters), loss = 0.421664
I1211 06:36:26.078897  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:36:26.078897  5076 solver.cpp:237]     Train net output #1: loss = 0.421663 (* 1 = 0.421663 loss)
I1211 06:36:26.078897  5076 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1211 06:36:32.252213  5076 solver.cpp:218] Iteration 98900 (16.1987 iter/s, 6.17332s/100 iters), loss = 0.470993
I1211 06:36:32.252213  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:36:32.252213  5076 solver.cpp:237]     Train net output #1: loss = 0.470993 (* 1 = 0.470993 loss)
I1211 06:36:32.252213  5076 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1211 06:36:38.101845 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:36:38.343600  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99000.caffemodel
I1211 06:36:38.359602  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99000.solverstate
I1211 06:36:38.364601  5076 solver.cpp:330] Iteration 99000, Testing net (#0)
I1211 06:36:38.364601  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:36:39.698211 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:36:39.750499  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6791
I1211 06:36:39.750499  5076 solver.cpp:397]     Test net output #1: loss = 1.19937 (* 1 = 1.19937 loss)
I1211 06:36:39.809479  5076 solver.cpp:218] Iteration 99000 (13.2325 iter/s, 7.55712s/100 iters), loss = 0.405904
I1211 06:36:39.809479  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:36:39.809479  5076 solver.cpp:237]     Train net output #1: loss = 0.405904 (* 1 = 0.405904 loss)
I1211 06:36:39.809479  5076 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1211 06:36:45.957906  5076 solver.cpp:218] Iteration 99100 (16.2664 iter/s, 6.14764s/100 iters), loss = 0.42537
I1211 06:36:45.957906  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:36:45.957906  5076 solver.cpp:237]     Train net output #1: loss = 0.42537 (* 1 = 0.42537 loss)
I1211 06:36:45.957906  5076 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1211 06:36:52.174685  5076 solver.cpp:218] Iteration 99200 (16.0862 iter/s, 6.2165s/100 iters), loss = 0.25967
I1211 06:36:52.174685  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 06:36:52.174685  5076 solver.cpp:237]     Train net output #1: loss = 0.25967 (* 1 = 0.25967 loss)
I1211 06:36:52.174685  5076 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1211 06:36:58.325973  5076 solver.cpp:218] Iteration 99300 (16.2596 iter/s, 6.1502s/100 iters), loss = 0.44446
I1211 06:36:58.325973  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 06:36:58.325973  5076 solver.cpp:237]     Train net output #1: loss = 0.444459 (* 1 = 0.444459 loss)
I1211 06:36:58.325973  5076 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1211 06:37:04.476940  5076 solver.cpp:218] Iteration 99400 (16.2582 iter/s, 6.15072s/100 iters), loss = 0.462908
I1211 06:37:04.476940  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:37:04.476940  5076 solver.cpp:237]     Train net output #1: loss = 0.462908 (* 1 = 0.462908 loss)
I1211 06:37:04.476940  5076 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1211 06:37:10.338963 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:37:10.580492  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99500.caffemodel
I1211 06:37:10.597494  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99500.solverstate
I1211 06:37:10.601492  5076 solver.cpp:330] Iteration 99500, Testing net (#0)
I1211 06:37:10.602494  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:37:11.934803 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:37:11.987823  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6801
I1211 06:37:11.987823  5076 solver.cpp:397]     Test net output #1: loss = 1.20152 (* 1 = 1.20152 loss)
I1211 06:37:12.046820  5076 solver.cpp:218] Iteration 99500 (13.2113 iter/s, 7.56925s/100 iters), loss = 0.360217
I1211 06:37:12.046820  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:37:12.046820  5076 solver.cpp:237]     Train net output #1: loss = 0.360217 (* 1 = 0.360217 loss)
I1211 06:37:12.046820  5076 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1211 06:37:18.194895  5076 solver.cpp:218] Iteration 99600 (16.2656 iter/s, 6.14796s/100 iters), loss = 0.407554
I1211 06:37:18.194895  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:37:18.194895  5076 solver.cpp:237]     Train net output #1: loss = 0.407554 (* 1 = 0.407554 loss)
I1211 06:37:18.194895  5076 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1211 06:37:24.350949  5076 solver.cpp:218] Iteration 99700 (16.2457 iter/s, 6.15546s/100 iters), loss = 0.31099
I1211 06:37:24.350949  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:37:24.350949  5076 solver.cpp:237]     Train net output #1: loss = 0.31099 (* 1 = 0.31099 loss)
I1211 06:37:24.350949  5076 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1211 06:37:30.501833  5076 solver.cpp:218] Iteration 99800 (16.2584 iter/s, 6.15066s/100 iters), loss = 0.394282
I1211 06:37:30.501833  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:37:30.501833  5076 solver.cpp:237]     Train net output #1: loss = 0.394281 (* 1 = 0.394281 loss)
I1211 06:37:30.501833  5076 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1211 06:37:36.668133  5076 solver.cpp:218] Iteration 99900 (16.2184 iter/s, 6.16583s/100 iters), loss = 0.49275
I1211 06:37:36.668133  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:37:36.668133  5076 solver.cpp:237]     Train net output #1: loss = 0.49275 (* 1 = 0.49275 loss)
I1211 06:37:36.668133  5076 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1211 06:37:42.583523 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:37:42.824573  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100000.caffemodel
I1211 06:37:42.839576  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100000.solverstate
I1211 06:37:42.844584  5076 solver.cpp:330] Iteration 100000, Testing net (#0)
I1211 06:37:42.844584  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:37:44.181903 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:37:44.234901  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6791
I1211 06:37:44.234901  5076 solver.cpp:397]     Test net output #1: loss = 1.20365 (* 1 = 1.20365 loss)
I1211 06:37:44.294914  5076 solver.cpp:218] Iteration 100000 (13.113 iter/s, 7.626s/100 iters), loss = 0.284077
I1211 06:37:44.294914  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:37:44.294914  5076 solver.cpp:237]     Train net output #1: loss = 0.284077 (* 1 = 0.284077 loss)
I1211 06:37:44.294914  5076 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1211 06:37:50.449204  5076 solver.cpp:218] Iteration 100100 (16.25 iter/s, 6.15383s/100 iters), loss = 0.42567
I1211 06:37:50.449204  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:37:50.449204  5076 solver.cpp:237]     Train net output #1: loss = 0.42567 (* 1 = 0.42567 loss)
I1211 06:37:50.449204  5076 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1211 06:37:56.598742  5076 solver.cpp:218] Iteration 100200 (16.2615 iter/s, 6.1495s/100 iters), loss = 0.314859
I1211 06:37:56.598742  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:37:56.598742  5076 solver.cpp:237]     Train net output #1: loss = 0.314859 (* 1 = 0.314859 loss)
I1211 06:37:56.598742  5076 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1211 06:38:02.750017  5076 solver.cpp:218] Iteration 100300 (16.257 iter/s, 6.15121s/100 iters), loss = 0.424203
I1211 06:38:02.750017  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:38:02.750017  5076 solver.cpp:237]     Train net output #1: loss = 0.424203 (* 1 = 0.424203 loss)
I1211 06:38:02.750017  5076 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1211 06:38:08.905143  5076 solver.cpp:218] Iteration 100400 (16.2487 iter/s, 6.15433s/100 iters), loss = 0.448645
I1211 06:38:08.905143  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:38:08.905143  5076 solver.cpp:237]     Train net output #1: loss = 0.448645 (* 1 = 0.448645 loss)
I1211 06:38:08.905143  5076 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1211 06:38:14.750154 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:38:14.992298  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100500.caffemodel
I1211 06:38:15.007297  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100500.solverstate
I1211 06:38:15.012820  5076 solver.cpp:330] Iteration 100500, Testing net (#0)
I1211 06:38:15.012820  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:38:16.348417 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:38:16.400401  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6794
I1211 06:38:16.401417  5076 solver.cpp:397]     Test net output #1: loss = 1.2046 (* 1 = 1.2046 loss)
I1211 06:38:16.459434  5076 solver.cpp:218] Iteration 100500 (13.2377 iter/s, 7.55416s/100 iters), loss = 0.40314
I1211 06:38:16.459434  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:38:16.459434  5076 solver.cpp:237]     Train net output #1: loss = 0.40314 (* 1 = 0.40314 loss)
I1211 06:38:16.459434  5076 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1211 06:38:22.603667  5076 solver.cpp:218] Iteration 100600 (16.2782 iter/s, 6.14317s/100 iters), loss = 0.428515
I1211 06:38:22.603667  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:38:22.603667  5076 solver.cpp:237]     Train net output #1: loss = 0.428514 (* 1 = 0.428514 loss)
I1211 06:38:22.603667  5076 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1211 06:38:28.747670  5076 solver.cpp:218] Iteration 100700 (16.2755 iter/s, 6.1442s/100 iters), loss = 0.34741
I1211 06:38:28.747670  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:38:28.747670  5076 solver.cpp:237]     Train net output #1: loss = 0.34741 (* 1 = 0.34741 loss)
I1211 06:38:28.747670  5076 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1211 06:38:34.906458  5076 solver.cpp:218] Iteration 100800 (16.2376 iter/s, 6.15855s/100 iters), loss = 0.38414
I1211 06:38:34.906458  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:38:34.906458  5076 solver.cpp:237]     Train net output #1: loss = 0.384139 (* 1 = 0.384139 loss)
I1211 06:38:34.906458  5076 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1211 06:38:41.055822  5076 solver.cpp:218] Iteration 100900 (16.2641 iter/s, 6.1485s/100 iters), loss = 0.570354
I1211 06:38:41.055822  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:38:41.055822  5076 solver.cpp:237]     Train net output #1: loss = 0.570354 (* 1 = 0.570354 loss)
I1211 06:38:41.055822  5076 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1211 06:38:46.908562 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:38:47.150588  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101000.caffemodel
I1211 06:38:47.171589  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101000.solverstate
I1211 06:38:47.176589  5076 solver.cpp:330] Iteration 101000, Testing net (#0)
I1211 06:38:47.176589  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:38:48.509690 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:38:48.561741  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6789
I1211 06:38:48.561741  5076 solver.cpp:397]     Test net output #1: loss = 1.20929 (* 1 = 1.20929 loss)
I1211 06:38:48.620724  5076 solver.cpp:218] Iteration 101000 (13.219 iter/s, 7.56485s/100 iters), loss = 0.311555
I1211 06:38:48.620724  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:38:48.620724  5076 solver.cpp:237]     Train net output #1: loss = 0.311555 (* 1 = 0.311555 loss)
I1211 06:38:48.620724  5076 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1211 06:38:54.770161  5076 solver.cpp:218] Iteration 101100 (16.2629 iter/s, 6.14897s/100 iters), loss = 0.379847
I1211 06:38:54.770161  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:38:54.770161  5076 solver.cpp:237]     Train net output #1: loss = 0.379847 (* 1 = 0.379847 loss)
I1211 06:38:54.770161  5076 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1211 06:39:00.914654  5076 solver.cpp:218] Iteration 101200 (16.2764 iter/s, 6.14386s/100 iters), loss = 0.371054
I1211 06:39:00.914654  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:39:00.914654  5076 solver.cpp:237]     Train net output #1: loss = 0.371054 (* 1 = 0.371054 loss)
I1211 06:39:00.914654  5076 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1211 06:39:07.058130  5076 solver.cpp:218] Iteration 101300 (16.2799 iter/s, 6.14254s/100 iters), loss = 0.415492
I1211 06:39:07.058130  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:39:07.058130  5076 solver.cpp:237]     Train net output #1: loss = 0.415492 (* 1 = 0.415492 loss)
I1211 06:39:07.058130  5076 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1211 06:39:13.229765  5076 solver.cpp:218] Iteration 101400 (16.203 iter/s, 6.1717s/100 iters), loss = 0.401393
I1211 06:39:13.229765  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:39:13.229765  5076 solver.cpp:237]     Train net output #1: loss = 0.401393 (* 1 = 0.401393 loss)
I1211 06:39:13.229765  5076 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1211 06:39:19.144706 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:39:19.387235  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101500.caffemodel
I1211 06:39:19.402235  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101500.solverstate
I1211 06:39:19.407235  5076 solver.cpp:330] Iteration 101500, Testing net (#0)
I1211 06:39:19.407235  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:39:20.745460 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:39:20.797462  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6798
I1211 06:39:20.797462  5076 solver.cpp:397]     Test net output #1: loss = 1.20741 (* 1 = 1.20741 loss)
I1211 06:39:20.857466  5076 solver.cpp:218] Iteration 101500 (13.1121 iter/s, 7.62656s/100 iters), loss = 0.326628
I1211 06:39:20.857466  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:39:20.857466  5076 solver.cpp:237]     Train net output #1: loss = 0.326628 (* 1 = 0.326628 loss)
I1211 06:39:20.857466  5076 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1211 06:39:27.019932  5076 solver.cpp:218] Iteration 101600 (16.2279 iter/s, 6.16222s/100 iters), loss = 0.402234
I1211 06:39:27.019932  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:39:27.019932  5076 solver.cpp:237]     Train net output #1: loss = 0.402234 (* 1 = 0.402234 loss)
I1211 06:39:27.019932  5076 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1211 06:39:33.199833  5076 solver.cpp:218] Iteration 101700 (16.1818 iter/s, 6.17978s/100 iters), loss = 0.302225
I1211 06:39:33.200333  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:39:33.200333  5076 solver.cpp:237]     Train net output #1: loss = 0.302225 (* 1 = 0.302225 loss)
I1211 06:39:33.200333  5076 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1211 06:39:39.356503  5076 solver.cpp:218] Iteration 101800 (16.2425 iter/s, 6.15669s/100 iters), loss = 0.359287
I1211 06:39:39.357489  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:39:39.357489  5076 solver.cpp:237]     Train net output #1: loss = 0.359287 (* 1 = 0.359287 loss)
I1211 06:39:39.357489  5076 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1211 06:39:45.556428  5076 solver.cpp:218] Iteration 101900 (16.1303 iter/s, 6.19951s/100 iters), loss = 0.477375
I1211 06:39:45.557428  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 06:39:45.557428  5076 solver.cpp:237]     Train net output #1: loss = 0.477375 (* 1 = 0.477375 loss)
I1211 06:39:45.557428  5076 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1211 06:39:51.406361 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:39:51.649873  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102000.caffemodel
I1211 06:39:51.664875  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102000.solverstate
I1211 06:39:51.668874  5076 solver.cpp:330] Iteration 102000, Testing net (#0)
I1211 06:39:51.669875  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:39:53.001502 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:39:53.054003  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 06:39:53.054003  5076 solver.cpp:397]     Test net output #1: loss = 1.21354 (* 1 = 1.21354 loss)
I1211 06:39:53.114006  5076 solver.cpp:218] Iteration 102000 (13.2342 iter/s, 7.55619s/100 iters), loss = 0.266343
I1211 06:39:53.114006  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:39:53.114006  5076 solver.cpp:237]     Train net output #1: loss = 0.266343 (* 1 = 0.266343 loss)
I1211 06:39:53.114006  5076 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1211 06:39:59.266516  5076 solver.cpp:218] Iteration 102100 (16.2525 iter/s, 6.15291s/100 iters), loss = 0.346645
I1211 06:39:59.266516  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:39:59.266516  5076 solver.cpp:237]     Train net output #1: loss = 0.346645 (* 1 = 0.346645 loss)
I1211 06:39:59.267518  5076 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1211 06:40:05.457023  5076 solver.cpp:218] Iteration 102200 (16.1559 iter/s, 6.18967s/100 iters), loss = 0.25794
I1211 06:40:05.457023  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:40:05.457023  5076 solver.cpp:237]     Train net output #1: loss = 0.25794 (* 1 = 0.25794 loss)
I1211 06:40:05.457023  5076 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1211 06:40:11.636310  5076 solver.cpp:218] Iteration 102300 (16.1845 iter/s, 6.17874s/100 iters), loss = 0.387075
I1211 06:40:11.636310  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:40:11.636310  5076 solver.cpp:237]     Train net output #1: loss = 0.387075 (* 1 = 0.387075 loss)
I1211 06:40:11.636310  5076 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1211 06:40:17.812732  5076 solver.cpp:218] Iteration 102400 (16.194 iter/s, 6.17514s/100 iters), loss = 0.423935
I1211 06:40:17.812732  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:40:17.812732  5076 solver.cpp:237]     Train net output #1: loss = 0.423935 (* 1 = 0.423935 loss)
I1211 06:40:17.812732  5076 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1211 06:40:23.701712 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:40:23.945724  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102500.caffemodel
I1211 06:40:23.962723  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102500.solverstate
I1211 06:40:23.967725  5076 solver.cpp:330] Iteration 102500, Testing net (#0)
I1211 06:40:23.967725  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:40:25.315851 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:40:25.368850  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6818
I1211 06:40:25.368850  5076 solver.cpp:397]     Test net output #1: loss = 1.21462 (* 1 = 1.21462 loss)
I1211 06:40:25.427857  5076 solver.cpp:218] Iteration 102500 (13.1316 iter/s, 7.61525s/100 iters), loss = 0.306269
I1211 06:40:25.427857  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:40:25.427857  5076 solver.cpp:237]     Train net output #1: loss = 0.306269 (* 1 = 0.306269 loss)
I1211 06:40:25.427857  5076 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1211 06:40:31.581897  5076 solver.cpp:218] Iteration 102600 (16.2521 iter/s, 6.15305s/100 iters), loss = 0.312691
I1211 06:40:31.581897  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:40:31.581897  5076 solver.cpp:237]     Train net output #1: loss = 0.312691 (* 1 = 0.312691 loss)
I1211 06:40:31.581897  5076 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1211 06:40:37.738803  5076 solver.cpp:218] Iteration 102700 (16.2421 iter/s, 6.15685s/100 iters), loss = 0.316473
I1211 06:40:37.738803  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:40:37.738803  5076 solver.cpp:237]     Train net output #1: loss = 0.316472 (* 1 = 0.316472 loss)
I1211 06:40:37.738803  5076 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1211 06:40:43.902258  5076 solver.cpp:218] Iteration 102800 (16.225 iter/s, 6.16332s/100 iters), loss = 0.295864
I1211 06:40:43.902258  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:40:43.902258  5076 solver.cpp:237]     Train net output #1: loss = 0.295864 (* 1 = 0.295864 loss)
I1211 06:40:43.902258  5076 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1211 06:40:50.055930  5076 solver.cpp:218] Iteration 102900 (16.2531 iter/s, 6.15266s/100 iters), loss = 0.471419
I1211 06:40:50.055930  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:40:50.055930  5076 solver.cpp:237]     Train net output #1: loss = 0.471419 (* 1 = 0.471419 loss)
I1211 06:40:50.055930  5076 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1211 06:40:55.963323 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:40:56.206336  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103000.caffemodel
I1211 06:40:56.221336  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103000.solverstate
I1211 06:40:56.226337  5076 solver.cpp:330] Iteration 103000, Testing net (#0)
I1211 06:40:56.226337  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:40:57.562456 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:40:57.614467  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6778
I1211 06:40:57.614467  5076 solver.cpp:397]     Test net output #1: loss = 1.2263 (* 1 = 1.2263 loss)
I1211 06:40:57.672461  5076 solver.cpp:218] Iteration 103000 (13.1295 iter/s, 7.61642s/100 iters), loss = 0.311311
I1211 06:40:57.672461  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:40:57.672461  5076 solver.cpp:237]     Train net output #1: loss = 0.311311 (* 1 = 0.311311 loss)
I1211 06:40:57.672461  5076 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1211 06:41:03.827919  5076 solver.cpp:218] Iteration 103100 (16.2471 iter/s, 6.15494s/100 iters), loss = 0.399791
I1211 06:41:03.827919  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:41:03.827919  5076 solver.cpp:237]     Train net output #1: loss = 0.399791 (* 1 = 0.399791 loss)
I1211 06:41:03.827919  5076 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1211 06:41:10.009913  5076 solver.cpp:218] Iteration 103200 (16.1774 iter/s, 6.18145s/100 iters), loss = 0.308674
I1211 06:41:10.009913  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:41:10.009913  5076 solver.cpp:237]     Train net output #1: loss = 0.308674 (* 1 = 0.308674 loss)
I1211 06:41:10.009913  5076 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1211 06:41:16.219983  5076 solver.cpp:218] Iteration 103300 (16.1052 iter/s, 6.20919s/100 iters), loss = 0.436805
I1211 06:41:16.219983  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:41:16.219983  5076 solver.cpp:237]     Train net output #1: loss = 0.436805 (* 1 = 0.436805 loss)
I1211 06:41:16.219983  5076 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1211 06:41:22.422233  5076 solver.cpp:218] Iteration 103400 (16.1239 iter/s, 6.20199s/100 iters), loss = 0.445036
I1211 06:41:22.422233  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:41:22.422233  5076 solver.cpp:237]     Train net output #1: loss = 0.445036 (* 1 = 0.445036 loss)
I1211 06:41:22.422233  5076 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1211 06:41:28.273164 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:41:28.515173  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103500.caffemodel
I1211 06:41:28.530179  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103500.solverstate
I1211 06:41:28.534679  5076 solver.cpp:330] Iteration 103500, Testing net (#0)
I1211 06:41:28.534679  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:41:29.869261 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:41:29.922260  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6787
I1211 06:41:29.922260  5076 solver.cpp:397]     Test net output #1: loss = 1.2212 (* 1 = 1.2212 loss)
I1211 06:41:29.981268  5076 solver.cpp:218] Iteration 103500 (13.2302 iter/s, 7.55845s/100 iters), loss = 0.303791
I1211 06:41:29.981268  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:41:29.981268  5076 solver.cpp:237]     Train net output #1: loss = 0.303791 (* 1 = 0.303791 loss)
I1211 06:41:29.981268  5076 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1211 06:41:36.137188  5076 solver.cpp:218] Iteration 103600 (16.2457 iter/s, 6.15548s/100 iters), loss = 0.353912
I1211 06:41:36.137188  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:41:36.137188  5076 solver.cpp:237]     Train net output #1: loss = 0.353912 (* 1 = 0.353912 loss)
I1211 06:41:36.137188  5076 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1211 06:41:42.324188  5076 solver.cpp:218] Iteration 103700 (16.1644 iter/s, 6.18642s/100 iters), loss = 0.292853
I1211 06:41:42.324188  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:41:42.324188  5076 solver.cpp:237]     Train net output #1: loss = 0.292853 (* 1 = 0.292853 loss)
I1211 06:41:42.324188  5076 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1211 06:41:48.495939  5076 solver.cpp:218] Iteration 103800 (16.2031 iter/s, 6.17166s/100 iters), loss = 0.350424
I1211 06:41:48.495939  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:41:48.495939  5076 solver.cpp:237]     Train net output #1: loss = 0.350424 (* 1 = 0.350424 loss)
I1211 06:41:48.495939  5076 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1211 06:41:54.638710  5076 solver.cpp:218] Iteration 103900 (16.2813 iter/s, 6.14202s/100 iters), loss = 0.50266
I1211 06:41:54.638710  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:41:54.638710  5076 solver.cpp:237]     Train net output #1: loss = 0.50266 (* 1 = 0.50266 loss)
I1211 06:41:54.638710  5076 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1211 06:42:00.484524 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:42:00.726661  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104000.caffemodel
I1211 06:42:00.743170  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104000.solverstate
I1211 06:42:00.748383  5076 solver.cpp:330] Iteration 104000, Testing net (#0)
I1211 06:42:00.748383  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:42:02.084188 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:42:02.136706  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6805
I1211 06:42:02.136706  5076 solver.cpp:397]     Test net output #1: loss = 1.22449 (* 1 = 1.22449 loss)
I1211 06:42:02.195233  5076 solver.cpp:218] Iteration 104000 (13.2335 iter/s, 7.55658s/100 iters), loss = 0.386913
I1211 06:42:02.195233  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:42:02.195233  5076 solver.cpp:237]     Train net output #1: loss = 0.386913 (* 1 = 0.386913 loss)
I1211 06:42:02.195233  5076 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1211 06:42:08.423363  5076 solver.cpp:218] Iteration 104100 (16.0585 iter/s, 6.22722s/100 iters), loss = 0.389442
I1211 06:42:08.423363  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:42:08.423363  5076 solver.cpp:237]     Train net output #1: loss = 0.389441 (* 1 = 0.389441 loss)
I1211 06:42:08.423363  5076 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1211 06:42:14.773419  5076 solver.cpp:218] Iteration 104200 (15.748 iter/s, 6.35001s/100 iters), loss = 0.255104
I1211 06:42:14.773419  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 06:42:14.773419  5076 solver.cpp:237]     Train net output #1: loss = 0.255103 (* 1 = 0.255103 loss)
I1211 06:42:14.773419  5076 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1211 06:42:21.112409  5076 solver.cpp:218] Iteration 104300 (15.776 iter/s, 6.33873s/100 iters), loss = 0.300934
I1211 06:42:21.113414  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:42:21.113414  5076 solver.cpp:237]     Train net output #1: loss = 0.300934 (* 1 = 0.300934 loss)
I1211 06:42:21.113414  5076 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1211 06:42:27.451021  5076 solver.cpp:218] Iteration 104400 (15.7797 iter/s, 6.33726s/100 iters), loss = 0.446515
I1211 06:42:27.451021  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:42:27.451021  5076 solver.cpp:237]     Train net output #1: loss = 0.446515 (* 1 = 0.446515 loss)
I1211 06:42:27.451021  5076 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1211 06:42:33.427608 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:42:33.673122  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104500.caffemodel
I1211 06:42:33.688625  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104500.solverstate
I1211 06:42:33.693626  5076 solver.cpp:330] Iteration 104500, Testing net (#0)
I1211 06:42:33.693626  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:42:35.058253 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:42:35.111754  5076 solver.cpp:397]     Test net output #0: accuracy = 0.676
I1211 06:42:35.111754  5076 solver.cpp:397]     Test net output #1: loss = 1.22491 (* 1 = 1.22491 loss)
I1211 06:42:35.173758  5076 solver.cpp:218] Iteration 104500 (12.9494 iter/s, 7.72235s/100 iters), loss = 0.327052
I1211 06:42:35.173758  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:42:35.173758  5076 solver.cpp:237]     Train net output #1: loss = 0.327052 (* 1 = 0.327052 loss)
I1211 06:42:35.173758  5076 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1211 06:42:41.376293  5076 solver.cpp:218] Iteration 104600 (16.1222 iter/s, 6.20265s/100 iters), loss = 0.387759
I1211 06:42:41.377295  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:42:41.377295  5076 solver.cpp:237]     Train net output #1: loss = 0.387759 (* 1 = 0.387759 loss)
I1211 06:42:41.377295  5076 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1211 06:42:47.664439  5076 solver.cpp:218] Iteration 104700 (15.906 iter/s, 6.28692s/100 iters), loss = 0.281457
I1211 06:42:47.664439  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:42:47.664439  5076 solver.cpp:237]     Train net output #1: loss = 0.281457 (* 1 = 0.281457 loss)
I1211 06:42:47.664439  5076 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1211 06:42:53.917062  5076 solver.cpp:218] Iteration 104800 (15.9945 iter/s, 6.25216s/100 iters), loss = 0.368943
I1211 06:42:53.917062  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:42:53.917062  5076 solver.cpp:237]     Train net output #1: loss = 0.368943 (* 1 = 0.368943 loss)
I1211 06:42:53.917062  5076 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1211 06:43:00.075043  5076 solver.cpp:218] Iteration 104900 (16.2408 iter/s, 6.15735s/100 iters), loss = 0.423908
I1211 06:43:00.075043  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:43:00.075043  5076 solver.cpp:237]     Train net output #1: loss = 0.423908 (* 1 = 0.423908 loss)
I1211 06:43:00.075043  5076 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1211 06:43:05.933990 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:43:06.177004  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105000.caffemodel
I1211 06:43:06.192008  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105000.solverstate
I1211 06:43:06.197007  5076 solver.cpp:330] Iteration 105000, Testing net (#0)
I1211 06:43:06.197007  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:43:07.535135 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:43:07.588143  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6775
I1211 06:43:07.588143  5076 solver.cpp:397]     Test net output #1: loss = 1.23012 (* 1 = 1.23012 loss)
I1211 06:43:07.646142  5076 solver.cpp:218] Iteration 105000 (13.2079 iter/s, 7.57124s/100 iters), loss = 0.308476
I1211 06:43:07.646142  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:43:07.646142  5076 solver.cpp:237]     Train net output #1: loss = 0.308476 (* 1 = 0.308476 loss)
I1211 06:43:07.646142  5076 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1211 06:43:13.809574  5076 solver.cpp:218] Iteration 105100 (16.2272 iter/s, 6.16249s/100 iters), loss = 0.321442
I1211 06:43:13.809574  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:43:13.809574  5076 solver.cpp:237]     Train net output #1: loss = 0.321442 (* 1 = 0.321442 loss)
I1211 06:43:13.809574  5076 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1211 06:43:19.970526  5076 solver.cpp:218] Iteration 105200 (16.2324 iter/s, 6.16051s/100 iters), loss = 0.257057
I1211 06:43:19.970526  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:43:19.970526  5076 solver.cpp:237]     Train net output #1: loss = 0.257057 (* 1 = 0.257057 loss)
I1211 06:43:19.970526  5076 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1211 06:43:26.132438  5076 solver.cpp:218] Iteration 105300 (16.2283 iter/s, 6.16209s/100 iters), loss = 0.328522
I1211 06:43:26.132438  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:43:26.132438  5076 solver.cpp:237]     Train net output #1: loss = 0.328522 (* 1 = 0.328522 loss)
I1211 06:43:26.132438  5076 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1211 06:43:32.295953  5076 solver.cpp:218] Iteration 105400 (16.2268 iter/s, 6.16265s/100 iters), loss = 0.387972
I1211 06:43:32.295953  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:43:32.295953  5076 solver.cpp:237]     Train net output #1: loss = 0.387972 (* 1 = 0.387972 loss)
I1211 06:43:32.295953  5076 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1211 06:43:38.148382 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:43:38.392400  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105500.caffemodel
I1211 06:43:38.408399  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105500.solverstate
I1211 06:43:38.413400  5076 solver.cpp:330] Iteration 105500, Testing net (#0)
I1211 06:43:38.413400  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:43:39.750495 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:43:39.802501  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6784
I1211 06:43:39.802501  5076 solver.cpp:397]     Test net output #1: loss = 1.2349 (* 1 = 1.2349 loss)
I1211 06:43:39.861502  5076 solver.cpp:218] Iteration 105500 (13.2177 iter/s, 7.56562s/100 iters), loss = 0.305232
I1211 06:43:39.861502  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:43:39.861502  5076 solver.cpp:237]     Train net output #1: loss = 0.305232 (* 1 = 0.305232 loss)
I1211 06:43:39.861502  5076 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1211 06:43:46.016005  5076 solver.cpp:218] Iteration 105600 (16.2507 iter/s, 6.1536s/100 iters), loss = 0.344595
I1211 06:43:46.016005  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:43:46.016005  5076 solver.cpp:237]     Train net output #1: loss = 0.344595 (* 1 = 0.344595 loss)
I1211 06:43:46.016005  5076 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1211 06:43:52.208593  5076 solver.cpp:218] Iteration 105700 (16.1499 iter/s, 6.19199s/100 iters), loss = 0.254463
I1211 06:43:52.208593  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 06:43:52.208593  5076 solver.cpp:237]     Train net output #1: loss = 0.254463 (* 1 = 0.254463 loss)
I1211 06:43:52.208593  5076 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1211 06:43:58.524631  5076 solver.cpp:218] Iteration 105800 (15.8322 iter/s, 6.31624s/100 iters), loss = 0.33016
I1211 06:43:58.524631  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:43:58.525632  5076 solver.cpp:237]     Train net output #1: loss = 0.330159 (* 1 = 0.330159 loss)
I1211 06:43:58.525632  5076 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1211 06:44:04.695230  5076 solver.cpp:218] Iteration 105900 (16.2094 iter/s, 6.16926s/100 iters), loss = 0.388042
I1211 06:44:04.695230  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:44:04.695230  5076 solver.cpp:237]     Train net output #1: loss = 0.388042 (* 1 = 0.388042 loss)
I1211 06:44:04.695230  5076 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1211 06:44:10.548684 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:44:10.792699  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106000.caffemodel
I1211 06:44:10.809700  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106000.solverstate
I1211 06:44:10.813700  5076 solver.cpp:330] Iteration 106000, Testing net (#0)
I1211 06:44:10.814699  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:44:12.150872 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:44:12.202867  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6778
I1211 06:44:12.202867  5076 solver.cpp:397]     Test net output #1: loss = 1.24459 (* 1 = 1.24459 loss)
I1211 06:44:12.262867  5076 solver.cpp:218] Iteration 106000 (13.2146 iter/s, 7.56737s/100 iters), loss = 0.260076
I1211 06:44:12.262867  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:44:12.262867  5076 solver.cpp:237]     Train net output #1: loss = 0.260076 (* 1 = 0.260076 loss)
I1211 06:44:12.262867  5076 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1211 06:44:18.421283  5076 solver.cpp:218] Iteration 106100 (16.2396 iter/s, 6.15778s/100 iters), loss = 0.393154
I1211 06:44:18.421283  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:44:18.421283  5076 solver.cpp:237]     Train net output #1: loss = 0.393154 (* 1 = 0.393154 loss)
I1211 06:44:18.421283  5076 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1211 06:44:24.566696  5076 solver.cpp:218] Iteration 106200 (16.2728 iter/s, 6.14523s/100 iters), loss = 0.233158
I1211 06:44:24.566696  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:44:24.566696  5076 solver.cpp:237]     Train net output #1: loss = 0.233158 (* 1 = 0.233158 loss)
I1211 06:44:24.566696  5076 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1211 06:44:30.721263  5076 solver.cpp:218] Iteration 106300 (16.2496 iter/s, 6.154s/100 iters), loss = 0.3905
I1211 06:44:30.721263  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:44:30.721263  5076 solver.cpp:237]     Train net output #1: loss = 0.3905 (* 1 = 0.3905 loss)
I1211 06:44:30.721263  5076 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1211 06:44:36.876246  5076 solver.cpp:218] Iteration 106400 (16.2493 iter/s, 6.1541s/100 iters), loss = 0.395162
I1211 06:44:36.876246  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:44:36.876246  5076 solver.cpp:237]     Train net output #1: loss = 0.395162 (* 1 = 0.395162 loss)
I1211 06:44:36.876246  5076 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1211 06:44:42.744962 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:44:42.987593  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106500.caffemodel
I1211 06:44:43.003592  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106500.solverstate
I1211 06:44:43.008594  5076 solver.cpp:330] Iteration 106500, Testing net (#0)
I1211 06:44:43.008594  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:44:44.343353 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:44:44.395393  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6777
I1211 06:44:44.395393  5076 solver.cpp:397]     Test net output #1: loss = 1.23232 (* 1 = 1.23232 loss)
I1211 06:44:44.454403  5076 solver.cpp:218] Iteration 106500 (13.1953 iter/s, 7.57844s/100 iters), loss = 0.343649
I1211 06:44:44.454403  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:44:44.454403  5076 solver.cpp:237]     Train net output #1: loss = 0.343649 (* 1 = 0.343649 loss)
I1211 06:44:44.454403  5076 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1211 06:44:50.613432  5076 solver.cpp:218] Iteration 106600 (16.237 iter/s, 6.15878s/100 iters), loss = 0.434493
I1211 06:44:50.613432  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:44:50.613432  5076 solver.cpp:237]     Train net output #1: loss = 0.434493 (* 1 = 0.434493 loss)
I1211 06:44:50.613432  5076 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1211 06:44:56.767164  5076 solver.cpp:218] Iteration 106700 (16.2528 iter/s, 6.15279s/100 iters), loss = 0.266033
I1211 06:44:56.767164  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 06:44:56.767164  5076 solver.cpp:237]     Train net output #1: loss = 0.266033 (* 1 = 0.266033 loss)
I1211 06:44:56.767164  5076 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1211 06:45:02.935003  5076 solver.cpp:218] Iteration 106800 (16.2136 iter/s, 6.16764s/100 iters), loss = 0.383676
I1211 06:45:02.935003  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:45:02.935003  5076 solver.cpp:237]     Train net output #1: loss = 0.383676 (* 1 = 0.383676 loss)
I1211 06:45:02.935003  5076 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1211 06:45:09.079293  5076 solver.cpp:218] Iteration 106900 (16.2765 iter/s, 6.14384s/100 iters), loss = 0.402103
I1211 06:45:09.079293  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:45:09.079293  5076 solver.cpp:237]     Train net output #1: loss = 0.402103 (* 1 = 0.402103 loss)
I1211 06:45:09.079293  5076 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1211 06:45:14.930769 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:45:15.171785  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107000.caffemodel
I1211 06:45:15.187790  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107000.solverstate
I1211 06:45:15.192791  5076 solver.cpp:330] Iteration 107000, Testing net (#0)
I1211 06:45:15.192791  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:45:16.526883 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:45:16.578882  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 06:45:16.578882  5076 solver.cpp:397]     Test net output #1: loss = 1.23608 (* 1 = 1.23608 loss)
I1211 06:45:16.638890  5076 solver.cpp:218] Iteration 107000 (13.2292 iter/s, 7.55903s/100 iters), loss = 0.349613
I1211 06:45:16.638890  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:45:16.638890  5076 solver.cpp:237]     Train net output #1: loss = 0.349613 (* 1 = 0.349613 loss)
I1211 06:45:16.638890  5076 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1211 06:45:22.810333  5076 solver.cpp:218] Iteration 107100 (16.2035 iter/s, 6.1715s/100 iters), loss = 0.380512
I1211 06:45:22.811333  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:45:22.811333  5076 solver.cpp:237]     Train net output #1: loss = 0.380512 (* 1 = 0.380512 loss)
I1211 06:45:22.811333  5076 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1211 06:45:28.988759  5076 solver.cpp:218] Iteration 107200 (16.1879 iter/s, 6.17747s/100 iters), loss = 0.33886
I1211 06:45:28.988759  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:45:28.989259  5076 solver.cpp:237]     Train net output #1: loss = 0.33886 (* 1 = 0.33886 loss)
I1211 06:45:28.989259  5076 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1211 06:45:35.261283  5076 solver.cpp:218] Iteration 107300 (15.943 iter/s, 6.27236s/100 iters), loss = 0.397708
I1211 06:45:35.261283  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:45:35.261283  5076 solver.cpp:237]     Train net output #1: loss = 0.397708 (* 1 = 0.397708 loss)
I1211 06:45:35.261283  5076 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1211 06:45:41.469646  5076 solver.cpp:218] Iteration 107400 (16.1083 iter/s, 6.208s/100 iters), loss = 0.441309
I1211 06:45:41.469646  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:45:41.469646  5076 solver.cpp:237]     Train net output #1: loss = 0.441309 (* 1 = 0.441309 loss)
I1211 06:45:41.469646  5076 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1211 06:45:47.314931 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:45:47.558059  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107500.caffemodel
I1211 06:45:47.573045  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107500.solverstate
I1211 06:45:47.578063  5076 solver.cpp:330] Iteration 107500, Testing net (#0)
I1211 06:45:47.578063  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:45:48.912582 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:45:48.965586  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6755
I1211 06:45:48.965586  5076 solver.cpp:397]     Test net output #1: loss = 1.24462 (* 1 = 1.24462 loss)
I1211 06:45:49.023628  5076 solver.cpp:218] Iteration 107500 (13.2389 iter/s, 7.55347s/100 iters), loss = 0.267666
I1211 06:45:49.023628  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:45:49.023628  5076 solver.cpp:237]     Train net output #1: loss = 0.267666 (* 1 = 0.267666 loss)
I1211 06:45:49.023628  5076 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1211 06:45:55.215785  5076 solver.cpp:218] Iteration 107600 (16.1507 iter/s, 6.19169s/100 iters), loss = 0.34936
I1211 06:45:55.215785  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:45:55.215785  5076 solver.cpp:237]     Train net output #1: loss = 0.34936 (* 1 = 0.34936 loss)
I1211 06:45:55.215785  5076 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1211 06:46:01.607333  5076 solver.cpp:218] Iteration 107700 (15.6487 iter/s, 6.39029s/100 iters), loss = 0.272661
I1211 06:46:01.607333  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:46:01.607333  5076 solver.cpp:237]     Train net output #1: loss = 0.27266 (* 1 = 0.27266 loss)
I1211 06:46:01.607333  5076 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1211 06:46:07.961432  5076 solver.cpp:218] Iteration 107800 (15.7374 iter/s, 6.35431s/100 iters), loss = 0.305791
I1211 06:46:07.961432  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:46:07.961432  5076 solver.cpp:237]     Train net output #1: loss = 0.305791 (* 1 = 0.305791 loss)
I1211 06:46:07.961432  5076 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1211 06:46:14.193507  5076 solver.cpp:218] Iteration 107900 (16.0485 iter/s, 6.2311s/100 iters), loss = 0.451967
I1211 06:46:14.193507  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:46:14.193507  5076 solver.cpp:237]     Train net output #1: loss = 0.451966 (* 1 = 0.451966 loss)
I1211 06:46:14.193507  5076 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1211 06:46:20.098721 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:46:20.341408  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108000.caffemodel
I1211 06:46:20.355413  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108000.solverstate
I1211 06:46:20.360412  5076 solver.cpp:330] Iteration 108000, Testing net (#0)
I1211 06:46:20.360412  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:46:21.696449 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:46:21.749151  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6776
I1211 06:46:21.750152  5076 solver.cpp:397]     Test net output #1: loss = 1.24043 (* 1 = 1.24043 loss)
I1211 06:46:21.808156  5076 solver.cpp:218] Iteration 108000 (13.1337 iter/s, 7.61403s/100 iters), loss = 0.260455
I1211 06:46:21.808156  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 06:46:21.808156  5076 solver.cpp:237]     Train net output #1: loss = 0.260455 (* 1 = 0.260455 loss)
I1211 06:46:21.808156  5076 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1211 06:46:27.978927  5076 solver.cpp:218] Iteration 108100 (16.2061 iter/s, 6.17053s/100 iters), loss = 0.349059
I1211 06:46:27.978927  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:46:27.978927  5076 solver.cpp:237]     Train net output #1: loss = 0.349059 (* 1 = 0.349059 loss)
I1211 06:46:27.978927  5076 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1211 06:46:34.150089  5076 solver.cpp:218] Iteration 108200 (16.2049 iter/s, 6.17096s/100 iters), loss = 0.284147
I1211 06:46:34.150590  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:46:34.150590  5076 solver.cpp:237]     Train net output #1: loss = 0.284147 (* 1 = 0.284147 loss)
I1211 06:46:34.150590  5076 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1211 06:46:40.304989  5076 solver.cpp:218] Iteration 108300 (16.2488 iter/s, 6.1543s/100 iters), loss = 0.33827
I1211 06:46:40.304989  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:46:40.304989  5076 solver.cpp:237]     Train net output #1: loss = 0.33827 (* 1 = 0.33827 loss)
I1211 06:46:40.304989  5076 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1211 06:46:46.493450  5076 solver.cpp:218] Iteration 108400 (16.1589 iter/s, 6.18853s/100 iters), loss = 0.397658
I1211 06:46:46.493450  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:46:46.493450  5076 solver.cpp:237]     Train net output #1: loss = 0.397658 (* 1 = 0.397658 loss)
I1211 06:46:46.493450  5076 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1211 06:46:52.377140 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:46:52.621183  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108500.caffemodel
I1211 06:46:52.636184  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108500.solverstate
I1211 06:46:52.641185  5076 solver.cpp:330] Iteration 108500, Testing net (#0)
I1211 06:46:52.641185  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:46:53.980320 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:46:54.032322  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6772
I1211 06:46:54.032322  5076 solver.cpp:397]     Test net output #1: loss = 1.24213 (* 1 = 1.24213 loss)
I1211 06:46:54.091323  5076 solver.cpp:218] Iteration 108500 (13.1624 iter/s, 7.59737s/100 iters), loss = 0.281679
I1211 06:46:54.091323  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:46:54.091323  5076 solver.cpp:237]     Train net output #1: loss = 0.281679 (* 1 = 0.281679 loss)
I1211 06:46:54.092324  5076 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1211 06:47:00.289834  5076 solver.cpp:218] Iteration 108600 (16.1343 iter/s, 6.19796s/100 iters), loss = 0.361699
I1211 06:47:00.289834  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:47:00.289834  5076 solver.cpp:237]     Train net output #1: loss = 0.361699 (* 1 = 0.361699 loss)
I1211 06:47:00.289834  5076 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1211 06:47:06.481379  5076 solver.cpp:218] Iteration 108700 (16.1538 iter/s, 6.19049s/100 iters), loss = 0.256878
I1211 06:47:06.481379  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 06:47:06.481379  5076 solver.cpp:237]     Train net output #1: loss = 0.256878 (* 1 = 0.256878 loss)
I1211 06:47:06.481379  5076 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1211 06:47:12.673837  5076 solver.cpp:218] Iteration 108800 (16.1486 iter/s, 6.19249s/100 iters), loss = 0.407572
I1211 06:47:12.673837  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:47:12.673837  5076 solver.cpp:237]     Train net output #1: loss = 0.407572 (* 1 = 0.407572 loss)
I1211 06:47:12.673837  5076 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1211 06:47:18.898169  5076 solver.cpp:218] Iteration 108900 (16.0688 iter/s, 6.22325s/100 iters), loss = 0.380512
I1211 06:47:18.898169  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:47:18.898169  5076 solver.cpp:237]     Train net output #1: loss = 0.380512 (* 1 = 0.380512 loss)
I1211 06:47:18.898169  5076 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1211 06:47:24.767580 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:47:25.011090  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109000.caffemodel
I1211 06:47:25.027595  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109000.solverstate
I1211 06:47:25.031594  5076 solver.cpp:330] Iteration 109000, Testing net (#0)
I1211 06:47:25.031594  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:47:26.368688 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:47:26.420692  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 06:47:26.420692  5076 solver.cpp:397]     Test net output #1: loss = 1.24127 (* 1 = 1.24127 loss)
I1211 06:47:26.479696  5076 solver.cpp:218] Iteration 109000 (13.1896 iter/s, 7.58176s/100 iters), loss = 0.298751
I1211 06:47:26.479696  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:47:26.479696  5076 solver.cpp:237]     Train net output #1: loss = 0.298751 (* 1 = 0.298751 loss)
I1211 06:47:26.479696  5076 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1211 06:47:32.639173  5076 solver.cpp:218] Iteration 109100 (16.2372 iter/s, 6.15868s/100 iters), loss = 0.41465
I1211 06:47:32.639173  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:47:32.639173  5076 solver.cpp:237]     Train net output #1: loss = 0.41465 (* 1 = 0.41465 loss)
I1211 06:47:32.639173  5076 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1211 06:47:38.801609  5076 solver.cpp:218] Iteration 109200 (16.2271 iter/s, 6.16255s/100 iters), loss = 0.273035
I1211 06:47:38.802610  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:47:38.802610  5076 solver.cpp:237]     Train net output #1: loss = 0.273035 (* 1 = 0.273035 loss)
I1211 06:47:38.802610  5076 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1211 06:47:44.982053  5076 solver.cpp:218] Iteration 109300 (16.1824 iter/s, 6.17954s/100 iters), loss = 0.318653
I1211 06:47:44.982053  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:47:44.982053  5076 solver.cpp:237]     Train net output #1: loss = 0.318653 (* 1 = 0.318653 loss)
I1211 06:47:44.982053  5076 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1211 06:47:51.133990  5076 solver.cpp:218] Iteration 109400 (16.2574 iter/s, 6.15105s/100 iters), loss = 0.436118
I1211 06:47:51.133990  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:47:51.133990  5076 solver.cpp:237]     Train net output #1: loss = 0.436118 (* 1 = 0.436118 loss)
I1211 06:47:51.133990  5076 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1211 06:47:56.996940 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:47:57.239969  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109500.caffemodel
I1211 06:47:57.255973  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109500.solverstate
I1211 06:47:57.260974  5076 solver.cpp:330] Iteration 109500, Testing net (#0)
I1211 06:47:57.260974  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:47:58.598078 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:47:58.650082  5076 solver.cpp:397]     Test net output #0: accuracy = 0.678
I1211 06:47:58.650082  5076 solver.cpp:397]     Test net output #1: loss = 1.24644 (* 1 = 1.24644 loss)
I1211 06:47:58.709082  5076 solver.cpp:218] Iteration 109500 (13.2009 iter/s, 7.57524s/100 iters), loss = 0.298528
I1211 06:47:58.709082  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:47:58.709082  5076 solver.cpp:237]     Train net output #1: loss = 0.298528 (* 1 = 0.298528 loss)
I1211 06:47:58.709082  5076 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1211 06:48:04.924782  5076 solver.cpp:218] Iteration 109600 (16.0909 iter/s, 6.2147s/100 iters), loss = 0.379246
I1211 06:48:04.924782  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:48:04.924782  5076 solver.cpp:237]     Train net output #1: loss = 0.379246 (* 1 = 0.379246 loss)
I1211 06:48:04.924782  5076 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1211 06:48:11.297353  5076 solver.cpp:218] Iteration 109700 (15.6941 iter/s, 6.37182s/100 iters), loss = 0.237766
I1211 06:48:11.297353  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 06:48:11.297353  5076 solver.cpp:237]     Train net output #1: loss = 0.237766 (* 1 = 0.237766 loss)
I1211 06:48:11.297353  5076 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1211 06:48:17.583797  5076 solver.cpp:218] Iteration 109800 (15.9079 iter/s, 6.2862s/100 iters), loss = 0.385381
I1211 06:48:17.583797  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:48:17.583797  5076 solver.cpp:237]     Train net output #1: loss = 0.385381 (* 1 = 0.385381 loss)
I1211 06:48:17.583797  5076 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1211 06:48:23.917407  5076 solver.cpp:218] Iteration 109900 (15.79 iter/s, 6.33311s/100 iters), loss = 0.386853
I1211 06:48:23.917407  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:48:23.917407  5076 solver.cpp:237]     Train net output #1: loss = 0.386853 (* 1 = 0.386853 loss)
I1211 06:48:23.917407  5076 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1211 06:48:29.955536 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:48:30.207546  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110000.caffemodel
I1211 06:48:30.223547  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110000.solverstate
I1211 06:48:30.228548  5076 solver.cpp:330] Iteration 110000, Testing net (#0)
I1211 06:48:30.228548  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:48:31.602656 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:48:31.656661  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6765
I1211 06:48:31.656661  5076 solver.cpp:397]     Test net output #1: loss = 1.25113 (* 1 = 1.25113 loss)
I1211 06:48:31.716660  5076 solver.cpp:218] Iteration 110000 (12.8226 iter/s, 7.79872s/100 iters), loss = 0.317626
I1211 06:48:31.716660  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:48:31.716660  5076 solver.cpp:237]     Train net output #1: loss = 0.317626 (* 1 = 0.317626 loss)
I1211 06:48:31.716660  5076 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1211 06:48:38.058586  5076 solver.cpp:218] Iteration 110100 (15.7686 iter/s, 6.3417s/100 iters), loss = 0.371108
I1211 06:48:38.058586  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:48:38.058586  5076 solver.cpp:237]     Train net output #1: loss = 0.371107 (* 1 = 0.371107 loss)
I1211 06:48:38.058586  5076 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1211 06:48:44.396263  5076 solver.cpp:218] Iteration 110200 (15.7791 iter/s, 6.33752s/100 iters), loss = 0.225522
I1211 06:48:44.396263  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1211 06:48:44.396263  5076 solver.cpp:237]     Train net output #1: loss = 0.225522 (* 1 = 0.225522 loss)
I1211 06:48:44.396263  5076 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1211 06:48:50.567813  5076 solver.cpp:218] Iteration 110300 (16.2061 iter/s, 6.17051s/100 iters), loss = 0.37391
I1211 06:48:50.567813  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:48:50.567813  5076 solver.cpp:237]     Train net output #1: loss = 0.37391 (* 1 = 0.37391 loss)
I1211 06:48:50.567813  5076 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1211 06:48:56.742302  5076 solver.cpp:218] Iteration 110400 (16.1963 iter/s, 6.17427s/100 iters), loss = 0.395818
I1211 06:48:56.742799  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:48:56.742799  5076 solver.cpp:237]     Train net output #1: loss = 0.395818 (* 1 = 0.395818 loss)
I1211 06:48:56.742799  5076 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1211 06:49:02.612818 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:49:02.855832  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110500.caffemodel
I1211 06:49:02.871836  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110500.solverstate
I1211 06:49:02.876837  5076 solver.cpp:330] Iteration 110500, Testing net (#0)
I1211 06:49:02.876837  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:49:04.226968 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:49:04.278961  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6755
I1211 06:49:04.278961  5076 solver.cpp:397]     Test net output #1: loss = 1.25791 (* 1 = 1.25791 loss)
I1211 06:49:04.338961  5076 solver.cpp:218] Iteration 110500 (13.1653 iter/s, 7.59575s/100 iters), loss = 0.28684
I1211 06:49:04.338961  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:49:04.338961  5076 solver.cpp:237]     Train net output #1: loss = 0.28684 (* 1 = 0.28684 loss)
I1211 06:49:04.338961  5076 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1211 06:49:10.605760  5076 solver.cpp:218] Iteration 110600 (15.9582 iter/s, 6.26636s/100 iters), loss = 0.302717
I1211 06:49:10.605760  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:49:10.605760  5076 solver.cpp:237]     Train net output #1: loss = 0.302717 (* 1 = 0.302717 loss)
I1211 06:49:10.605760  5076 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1211 06:49:16.939375  5076 solver.cpp:218] Iteration 110700 (15.7885 iter/s, 6.33374s/100 iters), loss = 0.217139
I1211 06:49:16.939375  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 06:49:16.939375  5076 solver.cpp:237]     Train net output #1: loss = 0.217138 (* 1 = 0.217138 loss)
I1211 06:49:16.939375  5076 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1211 06:49:23.276075  5076 solver.cpp:218] Iteration 110800 (15.782 iter/s, 6.33632s/100 iters), loss = 0.379273
I1211 06:49:23.277076  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:49:23.277076  5076 solver.cpp:237]     Train net output #1: loss = 0.379272 (* 1 = 0.379272 loss)
I1211 06:49:23.277076  5076 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1211 06:49:29.602864  5076 solver.cpp:218] Iteration 110900 (15.8088 iter/s, 6.32558s/100 iters), loss = 0.359599
I1211 06:49:29.602864  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:49:29.602864  5076 solver.cpp:237]     Train net output #1: loss = 0.359598 (* 1 = 0.359598 loss)
I1211 06:49:29.602864  5076 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1211 06:49:35.581327 17008 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:49:35.823340  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111000.caffemodel
I1211 06:49:35.839340  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111000.solverstate
I1211 06:49:35.843340  5076 solver.cpp:330] Iteration 111000, Testing net (#0)
I1211 06:49:35.843340  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:49:37.183468 17172 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:49:37.236474  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6757
I1211 06:49:37.236474  5076 solver.cpp:397]     Test net output #1: loss = 1.2642 (* 1 = 1.2642 loss)
I1211 06:49:37.294474  5076 solver.cpp:218] Iteration 111000 (13.0009 iter/s, 7.69176s/100 iters), loss = 0.298082
I1211 06:49:37.295473  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:49:37.295473  5076 solver.cpp:237]     Train net output #1: loss = 0.298082 (* 1 = 0.298082 loss)
I1211 06:49:37.295473  5076 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1211 06:49:43.456554  5076 solver.cpp:218] Iteration 111100 (16.2309 iter/s, 6.16107s/100 iters), loss = 0.283129
I1211 06:49:43.456554  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:49:43.456554  5076 solver.cpp:237]     Train net output #1: loss = 0.283129 (* 1 = 0.283129 loss)
I1211 06:49:43.456554  5076 sgd_solver.cpp:105] Iteration 111
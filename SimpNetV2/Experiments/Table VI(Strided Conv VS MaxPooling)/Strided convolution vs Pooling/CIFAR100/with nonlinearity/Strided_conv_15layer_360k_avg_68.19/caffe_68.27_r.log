
G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90000.solverstate 
I1211 18:01:49.219364  2360 caffe.cpp:219] Using GPUs 0
I1211 18:01:49.409070  2360 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1211 18:01:49.717856  2360 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 18:01:49.736838  2360 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1211 18:01:49.737841  2360 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 18:01:49.738837  2360 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 18:01:49.738837  2360 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_pool2_1
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_pool4_2
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1211 18:01:49.738837  2360 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1211 18:01:49.739840  2360 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV2_WnonLin_360k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "pool2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_pool2_1"
  type: "BatchNorm"
  bottom: "pool2_1"
  top: "pool2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_pool2_1"
  type: "Scale"
  bottom: "pool2_1"
  top: "pool2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_pool2_1"
  type: "ReLU"
  bottom: "pool2_1"
  top: "pool2_1"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "pool4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_pool4_2"
  type: "BatchNorm"
  bottom: "pool4_2"
  top: "pool4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_pool4_2"
  type: "Scale"
  bottom: "pool4_2"
  top: "pool4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_pool4_2"
  type: "ReLU"
  bottom: "pool4_2"
  top: "pool4_2"
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 18:01:49.748839  2360 layer_factory.cpp:58] Creating layer cifar
I1211 18:01:49.838907  2360 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1211 18:01:49.838907  2360 net.cpp:84] Creating Layer cifar
I1211 18:01:49.838907  2360 net.cpp:380] cifar -> data
I1211 18:01:49.838907  2360 net.cpp:380] cifar -> label
I1211 18:01:49.839901  2360 data_layer.cpp:45] output data size: 100,3,32,32
I1211 18:01:49.845888  2360 net.cpp:122] Setting up cifar
I1211 18:01:49.845888  2360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 18:01:49.845888  2360 net.cpp:129] Top shape: 100 (100)
I1211 18:01:49.845888  2360 net.cpp:137] Memory required for data: 1229200
I1211 18:01:49.845888  2360 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 18:01:49.845888  2360 net.cpp:84] Creating Layer label_cifar_1_split
I1211 18:01:49.845888  2360 net.cpp:406] label_cifar_1_split <- label
I1211 18:01:49.845888  2360 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 18:01:49.845888  2360 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 18:01:49.845888  2360 net.cpp:122] Setting up label_cifar_1_split
I1211 18:01:49.845888  2360 net.cpp:129] Top shape: 100 (100)
I1211 18:01:49.845888  2360 net.cpp:129] Top shape: 100 (100)
I1211 18:01:49.845888  2360 net.cpp:137] Memory required for data: 1230000
I1211 18:01:49.845888  2360 layer_factory.cpp:58] Creating layer conv1
I1211 18:01:49.845888  2360 net.cpp:84] Creating Layer conv1
I1211 18:01:49.845888  2360 net.cpp:406] conv1 <- data
I1211 18:01:49.845888  2360 net.cpp:380] conv1 -> conv1
I1211 18:01:49.847889  5852 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 18:01:50.108943  2360 net.cpp:122] Setting up conv1
I1211 18:01:50.108943  2360 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 18:01:50.108943  2360 net.cpp:137] Memory required for data: 13518000
I1211 18:01:50.108943  2360 layer_factory.cpp:58] Creating layer bn1
I1211 18:01:50.108943  2360 net.cpp:84] Creating Layer bn1
I1211 18:01:50.108943  2360 net.cpp:406] bn1 <- conv1
I1211 18:01:50.108943  2360 net.cpp:367] bn1 -> conv1 (in-place)
I1211 18:01:50.108943  2360 net.cpp:122] Setting up bn1
I1211 18:01:50.108943  2360 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 18:01:50.108943  2360 net.cpp:137] Memory required for data: 25806000
I1211 18:01:50.108943  2360 layer_factory.cpp:58] Creating layer scale1
I1211 18:01:50.108943  2360 net.cpp:84] Creating Layer scale1
I1211 18:01:50.108943  2360 net.cpp:406] scale1 <- conv1
I1211 18:01:50.108943  2360 net.cpp:367] scale1 -> conv1 (in-place)
I1211 18:01:50.108943  2360 layer_factory.cpp:58] Creating layer scale1
I1211 18:01:50.108943  2360 net.cpp:122] Setting up scale1
I1211 18:01:50.108943  2360 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 18:01:50.108943  2360 net.cpp:137] Memory required for data: 38094000
I1211 18:01:50.108943  2360 layer_factory.cpp:58] Creating layer relu1
I1211 18:01:50.108943  2360 net.cpp:84] Creating Layer relu1
I1211 18:01:50.108943  2360 net.cpp:406] relu1 <- conv1
I1211 18:01:50.108943  2360 net.cpp:367] relu1 -> conv1 (in-place)
I1211 18:01:50.109942  2360 net.cpp:122] Setting up relu1
I1211 18:01:50.109942  2360 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 18:01:50.109942  2360 net.cpp:137] Memory required for data: 50382000
I1211 18:01:50.109942  2360 layer_factory.cpp:58] Creating layer conv1_0
I1211 18:01:50.109942  2360 net.cpp:84] Creating Layer conv1_0
I1211 18:01:50.109942  2360 net.cpp:406] conv1_0 <- conv1
I1211 18:01:50.109942  2360 net.cpp:380] conv1_0 -> conv1_0
I1211 18:01:50.111954  2360 net.cpp:122] Setting up conv1_0
I1211 18:01:50.111954  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.111954  2360 net.cpp:137] Memory required for data: 66766000
I1211 18:01:50.111954  2360 layer_factory.cpp:58] Creating layer bn1_0
I1211 18:01:50.111954  2360 net.cpp:84] Creating Layer bn1_0
I1211 18:01:50.111954  2360 net.cpp:406] bn1_0 <- conv1_0
I1211 18:01:50.111954  2360 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 18:01:50.111954  2360 net.cpp:122] Setting up bn1_0
I1211 18:01:50.111954  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.111954  2360 net.cpp:137] Memory required for data: 83150000
I1211 18:01:50.111954  2360 layer_factory.cpp:58] Creating layer scale1_0
I1211 18:01:50.111954  2360 net.cpp:84] Creating Layer scale1_0
I1211 18:01:50.111954  2360 net.cpp:406] scale1_0 <- conv1_0
I1211 18:01:50.111954  2360 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 18:01:50.111954  2360 layer_factory.cpp:58] Creating layer scale1_0
I1211 18:01:50.111954  2360 net.cpp:122] Setting up scale1_0
I1211 18:01:50.111954  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.111954  2360 net.cpp:137] Memory required for data: 99534000
I1211 18:01:50.111954  2360 layer_factory.cpp:58] Creating layer relu1_0
I1211 18:01:50.111954  2360 net.cpp:84] Creating Layer relu1_0
I1211 18:01:50.111954  2360 net.cpp:406] relu1_0 <- conv1_0
I1211 18:01:50.111954  2360 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 18:01:50.112946  2360 net.cpp:122] Setting up relu1_0
I1211 18:01:50.112946  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.112946  2360 net.cpp:137] Memory required for data: 115918000
I1211 18:01:50.112946  2360 layer_factory.cpp:58] Creating layer conv2
I1211 18:01:50.112946  2360 net.cpp:84] Creating Layer conv2
I1211 18:01:50.112946  2360 net.cpp:406] conv2 <- conv1_0
I1211 18:01:50.112946  2360 net.cpp:380] conv2 -> conv2
I1211 18:01:50.113945  2360 net.cpp:122] Setting up conv2
I1211 18:01:50.113945  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.113945  2360 net.cpp:137] Memory required for data: 132302000
I1211 18:01:50.113945  2360 layer_factory.cpp:58] Creating layer bn2
I1211 18:01:50.113945  2360 net.cpp:84] Creating Layer bn2
I1211 18:01:50.113945  2360 net.cpp:406] bn2 <- conv2
I1211 18:01:50.113945  2360 net.cpp:367] bn2 -> conv2 (in-place)
I1211 18:01:50.113945  2360 net.cpp:122] Setting up bn2
I1211 18:01:50.113945  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.113945  2360 net.cpp:137] Memory required for data: 148686000
I1211 18:01:50.113945  2360 layer_factory.cpp:58] Creating layer scale2
I1211 18:01:50.113945  2360 net.cpp:84] Creating Layer scale2
I1211 18:01:50.113945  2360 net.cpp:406] scale2 <- conv2
I1211 18:01:50.113945  2360 net.cpp:367] scale2 -> conv2 (in-place)
I1211 18:01:50.113945  2360 layer_factory.cpp:58] Creating layer scale2
I1211 18:01:50.113945  2360 net.cpp:122] Setting up scale2
I1211 18:01:50.113945  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.113945  2360 net.cpp:137] Memory required for data: 165070000
I1211 18:01:50.113945  2360 layer_factory.cpp:58] Creating layer relu2
I1211 18:01:50.113945  2360 net.cpp:84] Creating Layer relu2
I1211 18:01:50.113945  2360 net.cpp:406] relu2 <- conv2
I1211 18:01:50.113945  2360 net.cpp:367] relu2 -> conv2 (in-place)
I1211 18:01:50.113945  2360 net.cpp:122] Setting up relu2
I1211 18:01:50.113945  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.113945  2360 net.cpp:137] Memory required for data: 181454000
I1211 18:01:50.113945  2360 layer_factory.cpp:58] Creating layer conv2_1
I1211 18:01:50.113945  2360 net.cpp:84] Creating Layer conv2_1
I1211 18:01:50.113945  2360 net.cpp:406] conv2_1 <- conv2
I1211 18:01:50.113945  2360 net.cpp:380] conv2_1 -> conv2_1
I1211 18:01:50.115943  2360 net.cpp:122] Setting up conv2_1
I1211 18:01:50.115943  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.115943  2360 net.cpp:137] Memory required for data: 197838000
I1211 18:01:50.115943  2360 layer_factory.cpp:58] Creating layer bn2_1
I1211 18:01:50.115943  2360 net.cpp:84] Creating Layer bn2_1
I1211 18:01:50.115943  2360 net.cpp:406] bn2_1 <- conv2_1
I1211 18:01:50.115943  2360 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 18:01:50.115943  2360 net.cpp:122] Setting up bn2_1
I1211 18:01:50.115943  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.115943  2360 net.cpp:137] Memory required for data: 214222000
I1211 18:01:50.115943  2360 layer_factory.cpp:58] Creating layer scale2_1
I1211 18:01:50.115943  2360 net.cpp:84] Creating Layer scale2_1
I1211 18:01:50.115943  2360 net.cpp:406] scale2_1 <- conv2_1
I1211 18:01:50.115943  2360 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 18:01:50.115943  2360 layer_factory.cpp:58] Creating layer scale2_1
I1211 18:01:50.115943  2360 net.cpp:122] Setting up scale2_1
I1211 18:01:50.115943  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.115943  2360 net.cpp:137] Memory required for data: 230606000
I1211 18:01:50.115943  2360 layer_factory.cpp:58] Creating layer relu2_1
I1211 18:01:50.115943  2360 net.cpp:84] Creating Layer relu2_1
I1211 18:01:50.115943  2360 net.cpp:406] relu2_1 <- conv2_1
I1211 18:01:50.115943  2360 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 18:01:50.115943  2360 net.cpp:122] Setting up relu2_1
I1211 18:01:50.115943  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.115943  2360 net.cpp:137] Memory required for data: 246990000
I1211 18:01:50.115943  2360 layer_factory.cpp:58] Creating layer conv2_2
I1211 18:01:50.115943  2360 net.cpp:84] Creating Layer conv2_2
I1211 18:01:50.115943  2360 net.cpp:406] conv2_2 <- conv2_1
I1211 18:01:50.115943  2360 net.cpp:380] conv2_2 -> conv2_2
I1211 18:01:50.117944  2360 net.cpp:122] Setting up conv2_2
I1211 18:01:50.117944  2360 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 18:01:50.117944  2360 net.cpp:137] Memory required for data: 267470000
I1211 18:01:50.117944  2360 layer_factory.cpp:58] Creating layer bn2_2
I1211 18:01:50.117944  2360 net.cpp:84] Creating Layer bn2_2
I1211 18:01:50.117944  2360 net.cpp:406] bn2_2 <- conv2_2
I1211 18:01:50.117944  2360 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 18:01:50.118943  2360 net.cpp:122] Setting up bn2_2
I1211 18:01:50.118943  2360 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 18:01:50.118943  2360 net.cpp:137] Memory required for data: 287950000
I1211 18:01:50.118943  2360 layer_factory.cpp:58] Creating layer scale2_2
I1211 18:01:50.118943  2360 net.cpp:84] Creating Layer scale2_2
I1211 18:01:50.118943  2360 net.cpp:406] scale2_2 <- conv2_2
I1211 18:01:50.118943  2360 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 18:01:50.118943  2360 layer_factory.cpp:58] Creating layer scale2_2
I1211 18:01:50.118943  2360 net.cpp:122] Setting up scale2_2
I1211 18:01:50.118943  2360 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 18:01:50.118943  2360 net.cpp:137] Memory required for data: 308430000
I1211 18:01:50.118943  2360 layer_factory.cpp:58] Creating layer relu2_2
I1211 18:01:50.118943  2360 net.cpp:84] Creating Layer relu2_2
I1211 18:01:50.118943  2360 net.cpp:406] relu2_2 <- conv2_2
I1211 18:01:50.118943  2360 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 18:01:50.118943  2360 net.cpp:122] Setting up relu2_2
I1211 18:01:50.118943  2360 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 18:01:50.118943  2360 net.cpp:137] Memory required for data: 328910000
I1211 18:01:50.118943  2360 layer_factory.cpp:58] Creating layer pool2_1
I1211 18:01:50.118943  2360 net.cpp:84] Creating Layer pool2_1
I1211 18:01:50.118943  2360 net.cpp:406] pool2_1 <- conv2_2
I1211 18:01:50.118943  2360 net.cpp:380] pool2_1 -> pool2_1
I1211 18:01:50.120942  2360 net.cpp:122] Setting up pool2_1
I1211 18:01:50.120942  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.120942  2360 net.cpp:137] Memory required for data: 334030000
I1211 18:01:50.120942  2360 layer_factory.cpp:58] Creating layer bn2_pool2_1
I1211 18:01:50.120942  2360 net.cpp:84] Creating Layer bn2_pool2_1
I1211 18:01:50.120942  2360 net.cpp:406] bn2_pool2_1 <- pool2_1
I1211 18:01:50.120942  2360 net.cpp:367] bn2_pool2_1 -> pool2_1 (in-place)
I1211 18:01:50.120942  2360 net.cpp:122] Setting up bn2_pool2_1
I1211 18:01:50.120942  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.120942  2360 net.cpp:137] Memory required for data: 339150000
I1211 18:01:50.120942  2360 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 18:01:50.120942  2360 net.cpp:84] Creating Layer scale2_pool2_1
I1211 18:01:50.120942  2360 net.cpp:406] scale2_pool2_1 <- pool2_1
I1211 18:01:50.120942  2360 net.cpp:367] scale2_pool2_1 -> pool2_1 (in-place)
I1211 18:01:50.120942  2360 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 18:01:50.120942  2360 net.cpp:122] Setting up scale2_pool2_1
I1211 18:01:50.120942  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.120942  2360 net.cpp:137] Memory required for data: 344270000
I1211 18:01:50.120942  2360 layer_factory.cpp:58] Creating layer relu2_pool2_1
I1211 18:01:50.120942  2360 net.cpp:84] Creating Layer relu2_pool2_1
I1211 18:01:50.120942  2360 net.cpp:406] relu2_pool2_1 <- pool2_1
I1211 18:01:50.120942  2360 net.cpp:367] relu2_pool2_1 -> pool2_1 (in-place)
I1211 18:01:50.120942  2360 net.cpp:122] Setting up relu2_pool2_1
I1211 18:01:50.120942  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.120942  2360 net.cpp:137] Memory required for data: 349390000
I1211 18:01:50.120942  2360 layer_factory.cpp:58] Creating layer conv3
I1211 18:01:50.120942  2360 net.cpp:84] Creating Layer conv3
I1211 18:01:50.120942  2360 net.cpp:406] conv3 <- pool2_1
I1211 18:01:50.120942  2360 net.cpp:380] conv3 -> conv3
I1211 18:01:50.122942  2360 net.cpp:122] Setting up conv3
I1211 18:01:50.122942  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.122942  2360 net.cpp:137] Memory required for data: 354510000
I1211 18:01:50.122942  2360 layer_factory.cpp:58] Creating layer bn3
I1211 18:01:50.122942  2360 net.cpp:84] Creating Layer bn3
I1211 18:01:50.122942  2360 net.cpp:406] bn3 <- conv3
I1211 18:01:50.122942  2360 net.cpp:367] bn3 -> conv3 (in-place)
I1211 18:01:50.122942  2360 net.cpp:122] Setting up bn3
I1211 18:01:50.122942  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.122942  2360 net.cpp:137] Memory required for data: 359630000
I1211 18:01:50.122942  2360 layer_factory.cpp:58] Creating layer scale3
I1211 18:01:50.122942  2360 net.cpp:84] Creating Layer scale3
I1211 18:01:50.122942  2360 net.cpp:406] scale3 <- conv3
I1211 18:01:50.122942  2360 net.cpp:367] scale3 -> conv3 (in-place)
I1211 18:01:50.122942  2360 layer_factory.cpp:58] Creating layer scale3
I1211 18:01:50.122942  2360 net.cpp:122] Setting up scale3
I1211 18:01:50.122942  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.122942  2360 net.cpp:137] Memory required for data: 364750000
I1211 18:01:50.122942  2360 layer_factory.cpp:58] Creating layer relu3
I1211 18:01:50.122942  2360 net.cpp:84] Creating Layer relu3
I1211 18:01:50.122942  2360 net.cpp:406] relu3 <- conv3
I1211 18:01:50.122942  2360 net.cpp:367] relu3 -> conv3 (in-place)
I1211 18:01:50.122942  2360 net.cpp:122] Setting up relu3
I1211 18:01:50.122942  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.122942  2360 net.cpp:137] Memory required for data: 369870000
I1211 18:01:50.122942  2360 layer_factory.cpp:58] Creating layer conv3_1
I1211 18:01:50.122942  2360 net.cpp:84] Creating Layer conv3_1
I1211 18:01:50.122942  2360 net.cpp:406] conv3_1 <- conv3
I1211 18:01:50.122942  2360 net.cpp:380] conv3_1 -> conv3_1
I1211 18:01:50.124943  2360 net.cpp:122] Setting up conv3_1
I1211 18:01:50.124943  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.124943  2360 net.cpp:137] Memory required for data: 374990000
I1211 18:01:50.124943  2360 layer_factory.cpp:58] Creating layer bn3_1
I1211 18:01:50.124943  2360 net.cpp:84] Creating Layer bn3_1
I1211 18:01:50.124943  2360 net.cpp:406] bn3_1 <- conv3_1
I1211 18:01:50.124943  2360 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 18:01:50.124943  2360 net.cpp:122] Setting up bn3_1
I1211 18:01:50.124943  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.124943  2360 net.cpp:137] Memory required for data: 380110000
I1211 18:01:50.124943  2360 layer_factory.cpp:58] Creating layer scale3_1
I1211 18:01:50.124943  2360 net.cpp:84] Creating Layer scale3_1
I1211 18:01:50.124943  2360 net.cpp:406] scale3_1 <- conv3_1
I1211 18:01:50.124943  2360 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 18:01:50.124943  2360 layer_factory.cpp:58] Creating layer scale3_1
I1211 18:01:50.124943  2360 net.cpp:122] Setting up scale3_1
I1211 18:01:50.124943  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.124943  2360 net.cpp:137] Memory required for data: 385230000
I1211 18:01:50.124943  2360 layer_factory.cpp:58] Creating layer relu3_1
I1211 18:01:50.124943  2360 net.cpp:84] Creating Layer relu3_1
I1211 18:01:50.124943  2360 net.cpp:406] relu3_1 <- conv3_1
I1211 18:01:50.124943  2360 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 18:01:50.125942  2360 net.cpp:122] Setting up relu3_1
I1211 18:01:50.125942  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.125942  2360 net.cpp:137] Memory required for data: 390350000
I1211 18:01:50.125942  2360 layer_factory.cpp:58] Creating layer conv4
I1211 18:01:50.125942  2360 net.cpp:84] Creating Layer conv4
I1211 18:01:50.125942  2360 net.cpp:406] conv4 <- conv3_1
I1211 18:01:50.125942  2360 net.cpp:380] conv4 -> conv4
I1211 18:01:50.126952  2360 net.cpp:122] Setting up conv4
I1211 18:01:50.126952  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.126952  2360 net.cpp:137] Memory required for data: 395470000
I1211 18:01:50.126952  2360 layer_factory.cpp:58] Creating layer bn4
I1211 18:01:50.126952  2360 net.cpp:84] Creating Layer bn4
I1211 18:01:50.126952  2360 net.cpp:406] bn4 <- conv4
I1211 18:01:50.126952  2360 net.cpp:367] bn4 -> conv4 (in-place)
I1211 18:01:50.126952  2360 net.cpp:122] Setting up bn4
I1211 18:01:50.126952  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.126952  2360 net.cpp:137] Memory required for data: 400590000
I1211 18:01:50.126952  2360 layer_factory.cpp:58] Creating layer scale4
I1211 18:01:50.126952  2360 net.cpp:84] Creating Layer scale4
I1211 18:01:50.126952  2360 net.cpp:406] scale4 <- conv4
I1211 18:01:50.127943  2360 net.cpp:367] scale4 -> conv4 (in-place)
I1211 18:01:50.127943  2360 layer_factory.cpp:58] Creating layer scale4
I1211 18:01:50.127943  2360 net.cpp:122] Setting up scale4
I1211 18:01:50.127943  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.127943  2360 net.cpp:137] Memory required for data: 405710000
I1211 18:01:50.127943  2360 layer_factory.cpp:58] Creating layer relu4
I1211 18:01:50.127943  2360 net.cpp:84] Creating Layer relu4
I1211 18:01:50.127943  2360 net.cpp:406] relu4 <- conv4
I1211 18:01:50.127943  2360 net.cpp:367] relu4 -> conv4 (in-place)
I1211 18:01:50.127943  2360 net.cpp:122] Setting up relu4
I1211 18:01:50.127943  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.127943  2360 net.cpp:137] Memory required for data: 410830000
I1211 18:01:50.127943  2360 layer_factory.cpp:58] Creating layer conv4_1
I1211 18:01:50.127943  2360 net.cpp:84] Creating Layer conv4_1
I1211 18:01:50.127943  2360 net.cpp:406] conv4_1 <- conv4
I1211 18:01:50.127943  2360 net.cpp:380] conv4_1 -> conv4_1
I1211 18:01:50.128943  2360 net.cpp:122] Setting up conv4_1
I1211 18:01:50.128943  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.128943  2360 net.cpp:137] Memory required for data: 415950000
I1211 18:01:50.128943  2360 layer_factory.cpp:58] Creating layer bn4_1
I1211 18:01:50.128943  2360 net.cpp:84] Creating Layer bn4_1
I1211 18:01:50.128943  2360 net.cpp:406] bn4_1 <- conv4_1
I1211 18:01:50.128943  2360 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 18:01:50.128943  2360 net.cpp:122] Setting up bn4_1
I1211 18:01:50.128943  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.128943  2360 net.cpp:137] Memory required for data: 421070000
I1211 18:01:50.128943  2360 layer_factory.cpp:58] Creating layer scale4_1
I1211 18:01:50.128943  2360 net.cpp:84] Creating Layer scale4_1
I1211 18:01:50.128943  2360 net.cpp:406] scale4_1 <- conv4_1
I1211 18:01:50.128943  2360 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 18:01:50.128943  2360 layer_factory.cpp:58] Creating layer scale4_1
I1211 18:01:50.128943  2360 net.cpp:122] Setting up scale4_1
I1211 18:01:50.129945  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.129945  2360 net.cpp:137] Memory required for data: 426190000
I1211 18:01:50.129945  2360 layer_factory.cpp:58] Creating layer relu4_1
I1211 18:01:50.129945  2360 net.cpp:84] Creating Layer relu4_1
I1211 18:01:50.129945  2360 net.cpp:406] relu4_1 <- conv4_1
I1211 18:01:50.129945  2360 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 18:01:50.129945  2360 net.cpp:122] Setting up relu4_1
I1211 18:01:50.129945  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.129945  2360 net.cpp:137] Memory required for data: 431310000
I1211 18:01:50.129945  2360 layer_factory.cpp:58] Creating layer conv4_2
I1211 18:01:50.129945  2360 net.cpp:84] Creating Layer conv4_2
I1211 18:01:50.129945  2360 net.cpp:406] conv4_2 <- conv4_1
I1211 18:01:50.129945  2360 net.cpp:380] conv4_2 -> conv4_2
I1211 18:01:50.131944  2360 net.cpp:122] Setting up conv4_2
I1211 18:01:50.131944  2360 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 18:01:50.131944  2360 net.cpp:137] Memory required for data: 437249200
I1211 18:01:50.131944  2360 layer_factory.cpp:58] Creating layer bn4_2
I1211 18:01:50.131944  2360 net.cpp:84] Creating Layer bn4_2
I1211 18:01:50.131944  2360 net.cpp:406] bn4_2 <- conv4_2
I1211 18:01:50.131944  2360 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 18:01:50.131944  2360 net.cpp:122] Setting up bn4_2
I1211 18:01:50.131944  2360 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 18:01:50.131944  2360 net.cpp:137] Memory required for data: 443188400
I1211 18:01:50.131944  2360 layer_factory.cpp:58] Creating layer scale4_2
I1211 18:01:50.131944  2360 net.cpp:84] Creating Layer scale4_2
I1211 18:01:50.131944  2360 net.cpp:406] scale4_2 <- conv4_2
I1211 18:01:50.131944  2360 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 18:01:50.131944  2360 layer_factory.cpp:58] Creating layer scale4_2
I1211 18:01:50.131944  2360 net.cpp:122] Setting up scale4_2
I1211 18:01:50.131944  2360 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 18:01:50.131944  2360 net.cpp:137] Memory required for data: 449127600
I1211 18:01:50.131944  2360 layer_factory.cpp:58] Creating layer relu4_2
I1211 18:01:50.131944  2360 net.cpp:84] Creating Layer relu4_2
I1211 18:01:50.131944  2360 net.cpp:406] relu4_2 <- conv4_2
I1211 18:01:50.131944  2360 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 18:01:50.131944  2360 net.cpp:122] Setting up relu4_2
I1211 18:01:50.131944  2360 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 18:01:50.131944  2360 net.cpp:137] Memory required for data: 455066800
I1211 18:01:50.131944  2360 layer_factory.cpp:58] Creating layer pool4_2
I1211 18:01:50.132956  2360 net.cpp:84] Creating Layer pool4_2
I1211 18:01:50.132956  2360 net.cpp:406] pool4_2 <- conv4_2
I1211 18:01:50.132956  2360 net.cpp:380] pool4_2 -> pool4_2
I1211 18:01:50.133944  2360 net.cpp:122] Setting up pool4_2
I1211 18:01:50.133944  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.133944  2360 net.cpp:137] Memory required for data: 456551600
I1211 18:01:50.133944  2360 layer_factory.cpp:58] Creating layer bn4_pool4_2
I1211 18:01:50.133944  2360 net.cpp:84] Creating Layer bn4_pool4_2
I1211 18:01:50.133944  2360 net.cpp:406] bn4_pool4_2 <- pool4_2
I1211 18:01:50.133944  2360 net.cpp:367] bn4_pool4_2 -> pool4_2 (in-place)
I1211 18:01:50.133944  2360 net.cpp:122] Setting up bn4_pool4_2
I1211 18:01:50.133944  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.133944  2360 net.cpp:137] Memory required for data: 458036400
I1211 18:01:50.133944  2360 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 18:01:50.133944  2360 net.cpp:84] Creating Layer scale4_pool4_2
I1211 18:01:50.133944  2360 net.cpp:406] scale4_pool4_2 <- pool4_2
I1211 18:01:50.133944  2360 net.cpp:367] scale4_pool4_2 -> pool4_2 (in-place)
I1211 18:01:50.133944  2360 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 18:01:50.133944  2360 net.cpp:122] Setting up scale4_pool4_2
I1211 18:01:50.133944  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.133944  2360 net.cpp:137] Memory required for data: 459521200
I1211 18:01:50.133944  2360 layer_factory.cpp:58] Creating layer relu4_pool4_2
I1211 18:01:50.133944  2360 net.cpp:84] Creating Layer relu4_pool4_2
I1211 18:01:50.133944  2360 net.cpp:406] relu4_pool4_2 <- pool4_2
I1211 18:01:50.133944  2360 net.cpp:367] relu4_pool4_2 -> pool4_2 (in-place)
I1211 18:01:50.134943  2360 net.cpp:122] Setting up relu4_pool4_2
I1211 18:01:50.134943  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.134943  2360 net.cpp:137] Memory required for data: 461006000
I1211 18:01:50.134943  2360 layer_factory.cpp:58] Creating layer conv4_0
I1211 18:01:50.134943  2360 net.cpp:84] Creating Layer conv4_0
I1211 18:01:50.134943  2360 net.cpp:406] conv4_0 <- pool4_2
I1211 18:01:50.134943  2360 net.cpp:380] conv4_0 -> conv4_0
I1211 18:01:50.135957  2360 net.cpp:122] Setting up conv4_0
I1211 18:01:50.135957  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.135957  2360 net.cpp:137] Memory required for data: 462490800
I1211 18:01:50.135957  2360 layer_factory.cpp:58] Creating layer bn4_0
I1211 18:01:50.135957  2360 net.cpp:84] Creating Layer bn4_0
I1211 18:01:50.135957  2360 net.cpp:406] bn4_0 <- conv4_0
I1211 18:01:50.135957  2360 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 18:01:50.135957  2360 net.cpp:122] Setting up bn4_0
I1211 18:01:50.135957  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.135957  2360 net.cpp:137] Memory required for data: 463975600
I1211 18:01:50.135957  2360 layer_factory.cpp:58] Creating layer scale4_0
I1211 18:01:50.135957  2360 net.cpp:84] Creating Layer scale4_0
I1211 18:01:50.135957  2360 net.cpp:406] scale4_0 <- conv4_0
I1211 18:01:50.135957  2360 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 18:01:50.135957  2360 layer_factory.cpp:58] Creating layer scale4_0
I1211 18:01:50.136956  2360 net.cpp:122] Setting up scale4_0
I1211 18:01:50.136956  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.136956  2360 net.cpp:137] Memory required for data: 465460400
I1211 18:01:50.136956  2360 layer_factory.cpp:58] Creating layer relu4_0
I1211 18:01:50.136956  2360 net.cpp:84] Creating Layer relu4_0
I1211 18:01:50.136956  2360 net.cpp:406] relu4_0 <- conv4_0
I1211 18:01:50.136956  2360 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 18:01:50.136956  2360 net.cpp:122] Setting up relu4_0
I1211 18:01:50.136956  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.136956  2360 net.cpp:137] Memory required for data: 466945200
I1211 18:01:50.136956  2360 layer_factory.cpp:58] Creating layer conv11
I1211 18:01:50.136956  2360 net.cpp:84] Creating Layer conv11
I1211 18:01:50.136956  2360 net.cpp:406] conv11 <- conv4_0
I1211 18:01:50.136956  2360 net.cpp:380] conv11 -> conv11
I1211 18:01:50.137956  2360 net.cpp:122] Setting up conv11
I1211 18:01:50.137956  2360 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 18:01:50.137956  2360 net.cpp:137] Memory required for data: 468737200
I1211 18:01:50.137956  2360 layer_factory.cpp:58] Creating layer bn_conv11
I1211 18:01:50.137956  2360 net.cpp:84] Creating Layer bn_conv11
I1211 18:01:50.137956  2360 net.cpp:406] bn_conv11 <- conv11
I1211 18:01:50.138957  2360 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 18:01:50.138957  2360 net.cpp:122] Setting up bn_conv11
I1211 18:01:50.138957  2360 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 18:01:50.138957  2360 net.cpp:137] Memory required for data: 470529200
I1211 18:01:50.138957  2360 layer_factory.cpp:58] Creating layer scale_conv11
I1211 18:01:50.138957  2360 net.cpp:84] Creating Layer scale_conv11
I1211 18:01:50.138957  2360 net.cpp:406] scale_conv11 <- conv11
I1211 18:01:50.138957  2360 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 18:01:50.138957  2360 layer_factory.cpp:58] Creating layer scale_conv11
I1211 18:01:50.138957  2360 net.cpp:122] Setting up scale_conv11
I1211 18:01:50.138957  2360 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 18:01:50.138957  2360 net.cpp:137] Memory required for data: 472321200
I1211 18:01:50.138957  2360 layer_factory.cpp:58] Creating layer relu_conv11
I1211 18:01:50.138957  2360 net.cpp:84] Creating Layer relu_conv11
I1211 18:01:50.138957  2360 net.cpp:406] relu_conv11 <- conv11
I1211 18:01:50.138957  2360 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 18:01:50.138957  2360 net.cpp:122] Setting up relu_conv11
I1211 18:01:50.138957  2360 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 18:01:50.138957  2360 net.cpp:137] Memory required for data: 474113200
I1211 18:01:50.138957  2360 layer_factory.cpp:58] Creating layer conv12
I1211 18:01:50.138957  2360 net.cpp:84] Creating Layer conv12
I1211 18:01:50.138957  2360 net.cpp:406] conv12 <- conv11
I1211 18:01:50.138957  2360 net.cpp:380] conv12 -> conv12
I1211 18:01:50.140956  2360 net.cpp:122] Setting up conv12
I1211 18:01:50.140956  2360 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 18:01:50.140956  2360 net.cpp:137] Memory required for data: 476417200
I1211 18:01:50.140956  2360 layer_factory.cpp:58] Creating layer bn_conv12
I1211 18:01:50.140956  2360 net.cpp:84] Creating Layer bn_conv12
I1211 18:01:50.140956  2360 net.cpp:406] bn_conv12 <- conv12
I1211 18:01:50.140956  2360 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 18:01:50.140956  2360 net.cpp:122] Setting up bn_conv12
I1211 18:01:50.140956  2360 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 18:01:50.140956  2360 net.cpp:137] Memory required for data: 478721200
I1211 18:01:50.140956  2360 layer_factory.cpp:58] Creating layer scale_conv12
I1211 18:01:50.140956  2360 net.cpp:84] Creating Layer scale_conv12
I1211 18:01:50.140956  2360 net.cpp:406] scale_conv12 <- conv12
I1211 18:01:50.140956  2360 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 18:01:50.140956  2360 layer_factory.cpp:58] Creating layer scale_conv12
I1211 18:01:50.140956  2360 net.cpp:122] Setting up scale_conv12
I1211 18:01:50.141957  2360 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 18:01:50.141957  2360 net.cpp:137] Memory required for data: 481025200
I1211 18:01:50.141957  2360 layer_factory.cpp:58] Creating layer relu_conv12
I1211 18:01:50.141957  2360 net.cpp:84] Creating Layer relu_conv12
I1211 18:01:50.141957  2360 net.cpp:406] relu_conv12 <- conv12
I1211 18:01:50.141957  2360 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 18:01:50.141957  2360 net.cpp:122] Setting up relu_conv12
I1211 18:01:50.141957  2360 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 18:01:50.141957  2360 net.cpp:137] Memory required for data: 483329200
I1211 18:01:50.141957  2360 layer_factory.cpp:58] Creating layer poolcp6
I1211 18:01:50.141957  2360 net.cpp:84] Creating Layer poolcp6
I1211 18:01:50.141957  2360 net.cpp:406] poolcp6 <- conv12
I1211 18:01:50.141957  2360 net.cpp:380] poolcp6 -> poolcp6
I1211 18:01:50.141957  2360 net.cpp:122] Setting up poolcp6
I1211 18:01:50.141957  2360 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 18:01:50.141957  2360 net.cpp:137] Memory required for data: 483365200
I1211 18:01:50.141957  2360 layer_factory.cpp:58] Creating layer ip1
I1211 18:01:50.141957  2360 net.cpp:84] Creating Layer ip1
I1211 18:01:50.141957  2360 net.cpp:406] ip1 <- poolcp6
I1211 18:01:50.141957  2360 net.cpp:380] ip1 -> ip1
I1211 18:01:50.141957  2360 net.cpp:122] Setting up ip1
I1211 18:01:50.141957  2360 net.cpp:129] Top shape: 100 100 (10000)
I1211 18:01:50.141957  2360 net.cpp:137] Memory required for data: 483405200
I1211 18:01:50.141957  2360 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 18:01:50.141957  2360 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 18:01:50.141957  2360 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 18:01:50.141957  2360 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 18:01:50.141957  2360 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 18:01:50.141957  2360 net.cpp:122] Setting up ip1_ip1_0_split
I1211 18:01:50.141957  2360 net.cpp:129] Top shape: 100 100 (10000)
I1211 18:01:50.141957  2360 net.cpp:129] Top shape: 100 100 (10000)
I1211 18:01:50.141957  2360 net.cpp:137] Memory required for data: 483485200
I1211 18:01:50.141957  2360 layer_factory.cpp:58] Creating layer accuracy_training
I1211 18:01:50.141957  2360 net.cpp:84] Creating Layer accuracy_training
I1211 18:01:50.141957  2360 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1211 18:01:50.141957  2360 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1211 18:01:50.141957  2360 net.cpp:380] accuracy_training -> accuracy_training
I1211 18:01:50.141957  2360 net.cpp:122] Setting up accuracy_training
I1211 18:01:50.141957  2360 net.cpp:129] Top shape: (1)
I1211 18:01:50.141957  2360 net.cpp:137] Memory required for data: 483485204
I1211 18:01:50.141957  2360 layer_factory.cpp:58] Creating layer loss
I1211 18:01:50.141957  2360 net.cpp:84] Creating Layer loss
I1211 18:01:50.141957  2360 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 18:01:50.141957  2360 net.cpp:406] loss <- label_cifar_1_split_1
I1211 18:01:50.141957  2360 net.cpp:380] loss -> loss
I1211 18:01:50.141957  2360 layer_factory.cpp:58] Creating layer loss
I1211 18:01:50.142956  2360 net.cpp:122] Setting up loss
I1211 18:01:50.142956  2360 net.cpp:129] Top shape: (1)
I1211 18:01:50.142956  2360 net.cpp:132]     with loss weight 1
I1211 18:01:50.142956  2360 net.cpp:137] Memory required for data: 483485208
I1211 18:01:50.142956  2360 net.cpp:198] loss needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:200] accuracy_training does not need backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] ip1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] poolcp6 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu_conv12 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale_conv12 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn_conv12 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] conv12 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu_conv11 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale_conv11 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn_conv11 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] conv11 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu4_0 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale4_0 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn4_0 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] conv4_0 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu4_pool4_2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale4_pool4_2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn4_pool4_2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] pool4_2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu4_2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale4_2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn4_2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] conv4_2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu4_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale4_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn4_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] conv4_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu4 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale4 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn4 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] conv4 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu3_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale3_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn3_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] conv3_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu3 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale3 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn3 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] conv3 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu2_pool2_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale2_pool2_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn2_pool2_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] pool2_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu2_2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale2_2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn2_2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] conv2_2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu2_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale2_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn2_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] conv2_1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] conv2 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu1_0 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale1_0 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn1_0 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] conv1_0 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] relu1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] scale1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] bn1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:198] conv1 needs backward computation.
I1211 18:01:50.142956  2360 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 18:01:50.142956  2360 net.cpp:200] cifar does not need backward computation.
I1211 18:01:50.142956  2360 net.cpp:242] This network produces output accuracy_training
I1211 18:01:50.142956  2360 net.cpp:242] This network produces output loss
I1211 18:01:50.142956  2360 net.cpp:255] Network initialization done.
I1211 18:01:50.143956  2360 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 18:01:50.143956  2360 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 18:01:50.143956  2360 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 18:01:50.143956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1211 18:01:50.143956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1211 18:01:50.143956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1211 18:01:50.143956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1211 18:01:50.143956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1211 18:01:50.143956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1211 18:01:50.143956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_pool2_1
I1211 18:01:50.143956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1211 18:01:50.143956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1211 18:01:50.143956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1211 18:01:50.143956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1211 18:01:50.143956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1211 18:01:50.143956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_pool4_2
I1211 18:01:50.144956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1211 18:01:50.144956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1211 18:01:50.144956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1211 18:01:50.144956  2360 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1211 18:01:50.144956  2360 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV2_WnonLin_360k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "pool2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_pool2_1"
  type: "BatchNorm"
  bottom: "pool2_1"
  top: "pool2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_pool2_1"
  type: "Scale"
  bottom: "pool2_1"
  top: "pool2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_pool2_1"
  type: "ReLU"
  bottom: "pool2_1"
  top: "pool2_1"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "pool4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_pool4_2"
  type: "BatchNorm"
  bottom: "pool4_2"
  top: "pool4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_pool4_2"
  type: "Scale"
  bottom: "pool4_2"
  top: "pool4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_pool4_2"
  type: "ReLU"
  bottom: "pool4_2"
  top: "pool4_2"
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 18:01:50.144956  2360 layer_factory.cpp:58] Creating layer cifar
I1211 18:01:50.147946  2360 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1211 18:01:50.147946  2360 net.cpp:84] Creating Layer cifar
I1211 18:01:50.147946  2360 net.cpp:380] cifar -> data
I1211 18:01:50.147946  2360 net.cpp:380] cifar -> label
I1211 18:01:50.147946  2360 data_layer.cpp:45] output data size: 100,3,32,32
I1211 18:01:50.156945  2360 net.cpp:122] Setting up cifar
I1211 18:01:50.156945  2360 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 18:01:50.156945  2360 net.cpp:129] Top shape: 100 (100)
I1211 18:01:50.156945  2360 net.cpp:137] Memory required for data: 1229200
I1211 18:01:50.156945  2360 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 18:01:50.156945  2360 net.cpp:84] Creating Layer label_cifar_1_split
I1211 18:01:50.156945  2360 net.cpp:406] label_cifar_1_split <- label
I1211 18:01:50.156945  2360 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 18:01:50.156945  2360 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 18:01:50.156945  2360 net.cpp:122] Setting up label_cifar_1_split
I1211 18:01:50.156945  2360 net.cpp:129] Top shape: 100 (100)
I1211 18:01:50.156945  2360 net.cpp:129] Top shape: 100 (100)
I1211 18:01:50.156945  2360 net.cpp:137] Memory required for data: 1230000
I1211 18:01:50.156945  2360 layer_factory.cpp:58] Creating layer conv1
I1211 18:01:50.156945  2360 net.cpp:84] Creating Layer conv1
I1211 18:01:50.156945  2360 net.cpp:406] conv1 <- data
I1211 18:01:50.156945  2360 net.cpp:380] conv1 -> conv1
I1211 18:01:50.158946 14584 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 18:01:50.158946  2360 net.cpp:122] Setting up conv1
I1211 18:01:50.158946  2360 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 18:01:50.158946  2360 net.cpp:137] Memory required for data: 13518000
I1211 18:01:50.158946  2360 layer_factory.cpp:58] Creating layer bn1
I1211 18:01:50.158946  2360 net.cpp:84] Creating Layer bn1
I1211 18:01:50.158946  2360 net.cpp:406] bn1 <- conv1
I1211 18:01:50.158946  2360 net.cpp:367] bn1 -> conv1 (in-place)
I1211 18:01:50.158946  2360 net.cpp:122] Setting up bn1
I1211 18:01:50.158946  2360 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 18:01:50.158946  2360 net.cpp:137] Memory required for data: 25806000
I1211 18:01:50.158946  2360 layer_factory.cpp:58] Creating layer scale1
I1211 18:01:50.158946  2360 net.cpp:84] Creating Layer scale1
I1211 18:01:50.158946  2360 net.cpp:406] scale1 <- conv1
I1211 18:01:50.158946  2360 net.cpp:367] scale1 -> conv1 (in-place)
I1211 18:01:50.158946  2360 layer_factory.cpp:58] Creating layer scale1
I1211 18:01:50.159957  2360 net.cpp:122] Setting up scale1
I1211 18:01:50.159957  2360 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 18:01:50.159957  2360 net.cpp:137] Memory required for data: 38094000
I1211 18:01:50.159957  2360 layer_factory.cpp:58] Creating layer relu1
I1211 18:01:50.159957  2360 net.cpp:84] Creating Layer relu1
I1211 18:01:50.159957  2360 net.cpp:406] relu1 <- conv1
I1211 18:01:50.159957  2360 net.cpp:367] relu1 -> conv1 (in-place)
I1211 18:01:50.160948  2360 net.cpp:122] Setting up relu1
I1211 18:01:50.160948  2360 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 18:01:50.160948  2360 net.cpp:137] Memory required for data: 50382000
I1211 18:01:50.160948  2360 layer_factory.cpp:58] Creating layer conv1_0
I1211 18:01:50.160948  2360 net.cpp:84] Creating Layer conv1_0
I1211 18:01:50.160948  2360 net.cpp:406] conv1_0 <- conv1
I1211 18:01:50.160948  2360 net.cpp:380] conv1_0 -> conv1_0
I1211 18:01:50.161945  2360 net.cpp:122] Setting up conv1_0
I1211 18:01:50.161945  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.161945  2360 net.cpp:137] Memory required for data: 66766000
I1211 18:01:50.161945  2360 layer_factory.cpp:58] Creating layer bn1_0
I1211 18:01:50.161945  2360 net.cpp:84] Creating Layer bn1_0
I1211 18:01:50.161945  2360 net.cpp:406] bn1_0 <- conv1_0
I1211 18:01:50.161945  2360 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 18:01:50.161945  2360 net.cpp:122] Setting up bn1_0
I1211 18:01:50.161945  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.161945  2360 net.cpp:137] Memory required for data: 83150000
I1211 18:01:50.161945  2360 layer_factory.cpp:58] Creating layer scale1_0
I1211 18:01:50.161945  2360 net.cpp:84] Creating Layer scale1_0
I1211 18:01:50.161945  2360 net.cpp:406] scale1_0 <- conv1_0
I1211 18:01:50.161945  2360 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 18:01:50.162947  2360 layer_factory.cpp:58] Creating layer scale1_0
I1211 18:01:50.162947  2360 net.cpp:122] Setting up scale1_0
I1211 18:01:50.162947  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.162947  2360 net.cpp:137] Memory required for data: 99534000
I1211 18:01:50.162947  2360 layer_factory.cpp:58] Creating layer relu1_0
I1211 18:01:50.162947  2360 net.cpp:84] Creating Layer relu1_0
I1211 18:01:50.162947  2360 net.cpp:406] relu1_0 <- conv1_0
I1211 18:01:50.162947  2360 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 18:01:50.162947  2360 net.cpp:122] Setting up relu1_0
I1211 18:01:50.162947  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.162947  2360 net.cpp:137] Memory required for data: 115918000
I1211 18:01:50.162947  2360 layer_factory.cpp:58] Creating layer conv2
I1211 18:01:50.162947  2360 net.cpp:84] Creating Layer conv2
I1211 18:01:50.162947  2360 net.cpp:406] conv2 <- conv1_0
I1211 18:01:50.162947  2360 net.cpp:380] conv2 -> conv2
I1211 18:01:50.164949  2360 net.cpp:122] Setting up conv2
I1211 18:01:50.164949  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.164949  2360 net.cpp:137] Memory required for data: 132302000
I1211 18:01:50.164949  2360 layer_factory.cpp:58] Creating layer bn2
I1211 18:01:50.164949  2360 net.cpp:84] Creating Layer bn2
I1211 18:01:50.164949  2360 net.cpp:406] bn2 <- conv2
I1211 18:01:50.164949  2360 net.cpp:367] bn2 -> conv2 (in-place)
I1211 18:01:50.164949  2360 net.cpp:122] Setting up bn2
I1211 18:01:50.164949  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.164949  2360 net.cpp:137] Memory required for data: 148686000
I1211 18:01:50.164949  2360 layer_factory.cpp:58] Creating layer scale2
I1211 18:01:50.164949  2360 net.cpp:84] Creating Layer scale2
I1211 18:01:50.164949  2360 net.cpp:406] scale2 <- conv2
I1211 18:01:50.164949  2360 net.cpp:367] scale2 -> conv2 (in-place)
I1211 18:01:50.164949  2360 layer_factory.cpp:58] Creating layer scale2
I1211 18:01:50.164949  2360 net.cpp:122] Setting up scale2
I1211 18:01:50.165947  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.165947  2360 net.cpp:137] Memory required for data: 165070000
I1211 18:01:50.165947  2360 layer_factory.cpp:58] Creating layer relu2
I1211 18:01:50.165947  2360 net.cpp:84] Creating Layer relu2
I1211 18:01:50.165947  2360 net.cpp:406] relu2 <- conv2
I1211 18:01:50.165947  2360 net.cpp:367] relu2 -> conv2 (in-place)
I1211 18:01:50.165947  2360 net.cpp:122] Setting up relu2
I1211 18:01:50.165947  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.165947  2360 net.cpp:137] Memory required for data: 181454000
I1211 18:01:50.165947  2360 layer_factory.cpp:58] Creating layer conv2_1
I1211 18:01:50.165947  2360 net.cpp:84] Creating Layer conv2_1
I1211 18:01:50.165947  2360 net.cpp:406] conv2_1 <- conv2
I1211 18:01:50.165947  2360 net.cpp:380] conv2_1 -> conv2_1
I1211 18:01:50.168457  2360 net.cpp:122] Setting up conv2_1
I1211 18:01:50.168457  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.168457  2360 net.cpp:137] Memory required for data: 197838000
I1211 18:01:50.168457  2360 layer_factory.cpp:58] Creating layer bn2_1
I1211 18:01:50.168457  2360 net.cpp:84] Creating Layer bn2_1
I1211 18:01:50.168457  2360 net.cpp:406] bn2_1 <- conv2_1
I1211 18:01:50.168457  2360 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 18:01:50.168457  2360 net.cpp:122] Setting up bn2_1
I1211 18:01:50.168457  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.168957  2360 net.cpp:137] Memory required for data: 214222000
I1211 18:01:50.168957  2360 layer_factory.cpp:58] Creating layer scale2_1
I1211 18:01:50.168957  2360 net.cpp:84] Creating Layer scale2_1
I1211 18:01:50.168957  2360 net.cpp:406] scale2_1 <- conv2_1
I1211 18:01:50.168957  2360 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 18:01:50.168957  2360 layer_factory.cpp:58] Creating layer scale2_1
I1211 18:01:50.168957  2360 net.cpp:122] Setting up scale2_1
I1211 18:01:50.168957  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.168957  2360 net.cpp:137] Memory required for data: 230606000
I1211 18:01:50.168957  2360 layer_factory.cpp:58] Creating layer relu2_1
I1211 18:01:50.168957  2360 net.cpp:84] Creating Layer relu2_1
I1211 18:01:50.168957  2360 net.cpp:406] relu2_1 <- conv2_1
I1211 18:01:50.168957  2360 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 18:01:50.168957  2360 net.cpp:122] Setting up relu2_1
I1211 18:01:50.168957  2360 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 18:01:50.168957  2360 net.cpp:137] Memory required for data: 246990000
I1211 18:01:50.168957  2360 layer_factory.cpp:58] Creating layer conv2_2
I1211 18:01:50.169458  2360 net.cpp:84] Creating Layer conv2_2
I1211 18:01:50.169458  2360 net.cpp:406] conv2_2 <- conv2_1
I1211 18:01:50.169458  2360 net.cpp:380] conv2_2 -> conv2_2
I1211 18:01:50.170958  2360 net.cpp:122] Setting up conv2_2
I1211 18:01:50.170958  2360 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 18:01:50.170958  2360 net.cpp:137] Memory required for data: 267470000
I1211 18:01:50.170958  2360 layer_factory.cpp:58] Creating layer bn2_2
I1211 18:01:50.170958  2360 net.cpp:84] Creating Layer bn2_2
I1211 18:01:50.170958  2360 net.cpp:406] bn2_2 <- conv2_2
I1211 18:01:50.170958  2360 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 18:01:50.170958  2360 net.cpp:122] Setting up bn2_2
I1211 18:01:50.170958  2360 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 18:01:50.170958  2360 net.cpp:137] Memory required for data: 287950000
I1211 18:01:50.170958  2360 layer_factory.cpp:58] Creating layer scale2_2
I1211 18:01:50.171473  2360 net.cpp:84] Creating Layer scale2_2
I1211 18:01:50.171473  2360 net.cpp:406] scale2_2 <- conv2_2
I1211 18:01:50.171473  2360 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 18:01:50.171473  2360 layer_factory.cpp:58] Creating layer scale2_2
I1211 18:01:50.171473  2360 net.cpp:122] Setting up scale2_2
I1211 18:01:50.171473  2360 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 18:01:50.171473  2360 net.cpp:137] Memory required for data: 308430000
I1211 18:01:50.171473  2360 layer_factory.cpp:58] Creating layer relu2_2
I1211 18:01:50.171473  2360 net.cpp:84] Creating Layer relu2_2
I1211 18:01:50.171473  2360 net.cpp:406] relu2_2 <- conv2_2
I1211 18:01:50.171473  2360 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 18:01:50.171969  2360 net.cpp:122] Setting up relu2_2
I1211 18:01:50.171969  2360 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 18:01:50.171969  2360 net.cpp:137] Memory required for data: 328910000
I1211 18:01:50.171969  2360 layer_factory.cpp:58] Creating layer pool2_1
I1211 18:01:50.171969  2360 net.cpp:84] Creating Layer pool2_1
I1211 18:01:50.171969  2360 net.cpp:406] pool2_1 <- conv2_2
I1211 18:01:50.171969  2360 net.cpp:380] pool2_1 -> pool2_1
I1211 18:01:50.173470  2360 net.cpp:122] Setting up pool2_1
I1211 18:01:50.173470  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.173470  2360 net.cpp:137] Memory required for data: 334030000
I1211 18:01:50.173470  2360 layer_factory.cpp:58] Creating layer bn2_pool2_1
I1211 18:01:50.173470  2360 net.cpp:84] Creating Layer bn2_pool2_1
I1211 18:01:50.173957  2360 net.cpp:406] bn2_pool2_1 <- pool2_1
I1211 18:01:50.173957  2360 net.cpp:367] bn2_pool2_1 -> pool2_1 (in-place)
I1211 18:01:50.173957  2360 net.cpp:122] Setting up bn2_pool2_1
I1211 18:01:50.173957  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.173957  2360 net.cpp:137] Memory required for data: 339150000
I1211 18:01:50.173957  2360 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 18:01:50.173957  2360 net.cpp:84] Creating Layer scale2_pool2_1
I1211 18:01:50.173957  2360 net.cpp:406] scale2_pool2_1 <- pool2_1
I1211 18:01:50.173957  2360 net.cpp:367] scale2_pool2_1 -> pool2_1 (in-place)
I1211 18:01:50.173957  2360 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 18:01:50.174464  2360 net.cpp:122] Setting up scale2_pool2_1
I1211 18:01:50.174464  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.174464  2360 net.cpp:137] Memory required for data: 344270000
I1211 18:01:50.174464  2360 layer_factory.cpp:58] Creating layer relu2_pool2_1
I1211 18:01:50.174464  2360 net.cpp:84] Creating Layer relu2_pool2_1
I1211 18:01:50.174464  2360 net.cpp:406] relu2_pool2_1 <- pool2_1
I1211 18:01:50.174464  2360 net.cpp:367] relu2_pool2_1 -> pool2_1 (in-place)
I1211 18:01:50.175457  2360 net.cpp:122] Setting up relu2_pool2_1
I1211 18:01:50.175457  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.175457  2360 net.cpp:137] Memory required for data: 349390000
I1211 18:01:50.175457  2360 layer_factory.cpp:58] Creating layer conv3
I1211 18:01:50.175457  2360 net.cpp:84] Creating Layer conv3
I1211 18:01:50.175457  2360 net.cpp:406] conv3 <- pool2_1
I1211 18:01:50.175457  2360 net.cpp:380] conv3 -> conv3
I1211 18:01:50.176455  2360 net.cpp:122] Setting up conv3
I1211 18:01:50.176455  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.176455  2360 net.cpp:137] Memory required for data: 354510000
I1211 18:01:50.176455  2360 layer_factory.cpp:58] Creating layer bn3
I1211 18:01:50.176455  2360 net.cpp:84] Creating Layer bn3
I1211 18:01:50.176455  2360 net.cpp:406] bn3 <- conv3
I1211 18:01:50.176455  2360 net.cpp:367] bn3 -> conv3 (in-place)
I1211 18:01:50.176957  2360 net.cpp:122] Setting up bn3
I1211 18:01:50.176957  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.176957  2360 net.cpp:137] Memory required for data: 359630000
I1211 18:01:50.176957  2360 layer_factory.cpp:58] Creating layer scale3
I1211 18:01:50.176957  2360 net.cpp:84] Creating Layer scale3
I1211 18:01:50.176957  2360 net.cpp:406] scale3 <- conv3
I1211 18:01:50.176957  2360 net.cpp:367] scale3 -> conv3 (in-place)
I1211 18:01:50.176957  2360 layer_factory.cpp:58] Creating layer scale3
I1211 18:01:50.176957  2360 net.cpp:122] Setting up scale3
I1211 18:01:50.176957  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.176957  2360 net.cpp:137] Memory required for data: 364750000
I1211 18:01:50.176957  2360 layer_factory.cpp:58] Creating layer relu3
I1211 18:01:50.176957  2360 net.cpp:84] Creating Layer relu3
I1211 18:01:50.176957  2360 net.cpp:406] relu3 <- conv3
I1211 18:01:50.176957  2360 net.cpp:367] relu3 -> conv3 (in-place)
I1211 18:01:50.177458  2360 net.cpp:122] Setting up relu3
I1211 18:01:50.177458  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.177458  2360 net.cpp:137] Memory required for data: 369870000
I1211 18:01:50.177458  2360 layer_factory.cpp:58] Creating layer conv3_1
I1211 18:01:50.177458  2360 net.cpp:84] Creating Layer conv3_1
I1211 18:01:50.177458  2360 net.cpp:406] conv3_1 <- conv3
I1211 18:01:50.177458  2360 net.cpp:380] conv3_1 -> conv3_1
I1211 18:01:50.178956  2360 net.cpp:122] Setting up conv3_1
I1211 18:01:50.178956  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.178956  2360 net.cpp:137] Memory required for data: 374990000
I1211 18:01:50.178956  2360 layer_factory.cpp:58] Creating layer bn3_1
I1211 18:01:50.178956  2360 net.cpp:84] Creating Layer bn3_1
I1211 18:01:50.178956  2360 net.cpp:406] bn3_1 <- conv3_1
I1211 18:01:50.178956  2360 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 18:01:50.178956  2360 net.cpp:122] Setting up bn3_1
I1211 18:01:50.178956  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.178956  2360 net.cpp:137] Memory required for data: 380110000
I1211 18:01:50.178956  2360 layer_factory.cpp:58] Creating layer scale3_1
I1211 18:01:50.178956  2360 net.cpp:84] Creating Layer scale3_1
I1211 18:01:50.178956  2360 net.cpp:406] scale3_1 <- conv3_1
I1211 18:01:50.178956  2360 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 18:01:50.179455  2360 layer_factory.cpp:58] Creating layer scale3_1
I1211 18:01:50.179455  2360 net.cpp:122] Setting up scale3_1
I1211 18:01:50.179455  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.179455  2360 net.cpp:137] Memory required for data: 385230000
I1211 18:01:50.179455  2360 layer_factory.cpp:58] Creating layer relu3_1
I1211 18:01:50.179455  2360 net.cpp:84] Creating Layer relu3_1
I1211 18:01:50.179455  2360 net.cpp:406] relu3_1 <- conv3_1
I1211 18:01:50.179455  2360 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 18:01:50.179455  2360 net.cpp:122] Setting up relu3_1
I1211 18:01:50.179455  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.179455  2360 net.cpp:137] Memory required for data: 390350000
I1211 18:01:50.179455  2360 layer_factory.cpp:58] Creating layer conv4
I1211 18:01:50.179455  2360 net.cpp:84] Creating Layer conv4
I1211 18:01:50.179455  2360 net.cpp:406] conv4 <- conv3_1
I1211 18:01:50.179455  2360 net.cpp:380] conv4 -> conv4
I1211 18:01:50.181457  2360 net.cpp:122] Setting up conv4
I1211 18:01:50.181457  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.181457  2360 net.cpp:137] Memory required for data: 395470000
I1211 18:01:50.181457  2360 layer_factory.cpp:58] Creating layer bn4
I1211 18:01:50.181457  2360 net.cpp:84] Creating Layer bn4
I1211 18:01:50.181457  2360 net.cpp:406] bn4 <- conv4
I1211 18:01:50.181457  2360 net.cpp:367] bn4 -> conv4 (in-place)
I1211 18:01:50.181457  2360 net.cpp:122] Setting up bn4
I1211 18:01:50.181457  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.181457  2360 net.cpp:137] Memory required for data: 400590000
I1211 18:01:50.181457  2360 layer_factory.cpp:58] Creating layer scale4
I1211 18:01:50.181457  2360 net.cpp:84] Creating Layer scale4
I1211 18:01:50.181457  2360 net.cpp:406] scale4 <- conv4
I1211 18:01:50.181457  2360 net.cpp:367] scale4 -> conv4 (in-place)
I1211 18:01:50.181457  2360 layer_factory.cpp:58] Creating layer scale4
I1211 18:01:50.181457  2360 net.cpp:122] Setting up scale4
I1211 18:01:50.181457  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.181457  2360 net.cpp:137] Memory required for data: 405710000
I1211 18:01:50.181457  2360 layer_factory.cpp:58] Creating layer relu4
I1211 18:01:50.181457  2360 net.cpp:84] Creating Layer relu4
I1211 18:01:50.181457  2360 net.cpp:406] relu4 <- conv4
I1211 18:01:50.181457  2360 net.cpp:367] relu4 -> conv4 (in-place)
I1211 18:01:50.182485  2360 net.cpp:122] Setting up relu4
I1211 18:01:50.182485  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.182485  2360 net.cpp:137] Memory required for data: 410830000
I1211 18:01:50.182485  2360 layer_factory.cpp:58] Creating layer conv4_1
I1211 18:01:50.182485  2360 net.cpp:84] Creating Layer conv4_1
I1211 18:01:50.182485  2360 net.cpp:406] conv4_1 <- conv4
I1211 18:01:50.182485  2360 net.cpp:380] conv4_1 -> conv4_1
I1211 18:01:50.184485  2360 net.cpp:122] Setting up conv4_1
I1211 18:01:50.184485  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.184485  2360 net.cpp:137] Memory required for data: 415950000
I1211 18:01:50.184485  2360 layer_factory.cpp:58] Creating layer bn4_1
I1211 18:01:50.184485  2360 net.cpp:84] Creating Layer bn4_1
I1211 18:01:50.184485  2360 net.cpp:406] bn4_1 <- conv4_1
I1211 18:01:50.184485  2360 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 18:01:50.184485  2360 net.cpp:122] Setting up bn4_1
I1211 18:01:50.184485  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.184485  2360 net.cpp:137] Memory required for data: 421070000
I1211 18:01:50.184485  2360 layer_factory.cpp:58] Creating layer scale4_1
I1211 18:01:50.184485  2360 net.cpp:84] Creating Layer scale4_1
I1211 18:01:50.185485  2360 net.cpp:406] scale4_1 <- conv4_1
I1211 18:01:50.185485  2360 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 18:01:50.185485  2360 layer_factory.cpp:58] Creating layer scale4_1
I1211 18:01:50.185485  2360 net.cpp:122] Setting up scale4_1
I1211 18:01:50.185485  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.185485  2360 net.cpp:137] Memory required for data: 426190000
I1211 18:01:50.185485  2360 layer_factory.cpp:58] Creating layer relu4_1
I1211 18:01:50.185485  2360 net.cpp:84] Creating Layer relu4_1
I1211 18:01:50.185485  2360 net.cpp:406] relu4_1 <- conv4_1
I1211 18:01:50.185485  2360 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 18:01:50.185485  2360 net.cpp:122] Setting up relu4_1
I1211 18:01:50.185485  2360 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 18:01:50.185485  2360 net.cpp:137] Memory required for data: 431310000
I1211 18:01:50.185485  2360 layer_factory.cpp:58] Creating layer conv4_2
I1211 18:01:50.185485  2360 net.cpp:84] Creating Layer conv4_2
I1211 18:01:50.185485  2360 net.cpp:406] conv4_2 <- conv4_1
I1211 18:01:50.185485  2360 net.cpp:380] conv4_2 -> conv4_2
I1211 18:01:50.187484  2360 net.cpp:122] Setting up conv4_2
I1211 18:01:50.187484  2360 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 18:01:50.187484  2360 net.cpp:137] Memory required for data: 437249200
I1211 18:01:50.187484  2360 layer_factory.cpp:58] Creating layer bn4_2
I1211 18:01:50.187484  2360 net.cpp:84] Creating Layer bn4_2
I1211 18:01:50.188484  2360 net.cpp:406] bn4_2 <- conv4_2
I1211 18:01:50.188484  2360 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 18:01:50.188484  2360 net.cpp:122] Setting up bn4_2
I1211 18:01:50.188484  2360 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 18:01:50.188484  2360 net.cpp:137] Memory required for data: 443188400
I1211 18:01:50.188484  2360 layer_factory.cpp:58] Creating layer scale4_2
I1211 18:01:50.188484  2360 net.cpp:84] Creating Layer scale4_2
I1211 18:01:50.188484  2360 net.cpp:406] scale4_2 <- conv4_2
I1211 18:01:50.188484  2360 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 18:01:50.188484  2360 layer_factory.cpp:58] Creating layer scale4_2
I1211 18:01:50.188484  2360 net.cpp:122] Setting up scale4_2
I1211 18:01:50.188484  2360 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 18:01:50.188484  2360 net.cpp:137] Memory required for data: 449127600
I1211 18:01:50.188484  2360 layer_factory.cpp:58] Creating layer relu4_2
I1211 18:01:50.188484  2360 net.cpp:84] Creating Layer relu4_2
I1211 18:01:50.188484  2360 net.cpp:406] relu4_2 <- conv4_2
I1211 18:01:50.188484  2360 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 18:01:50.188484  2360 net.cpp:122] Setting up relu4_2
I1211 18:01:50.188484  2360 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 18:01:50.188484  2360 net.cpp:137] Memory required for data: 455066800
I1211 18:01:50.188484  2360 layer_factory.cpp:58] Creating layer pool4_2
I1211 18:01:50.188484  2360 net.cpp:84] Creating Layer pool4_2
I1211 18:01:50.188484  2360 net.cpp:406] pool4_2 <- conv4_2
I1211 18:01:50.188484  2360 net.cpp:380] pool4_2 -> pool4_2
I1211 18:01:50.190488  2360 net.cpp:122] Setting up pool4_2
I1211 18:01:50.190488  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.190488  2360 net.cpp:137] Memory required for data: 456551600
I1211 18:01:50.190488  2360 layer_factory.cpp:58] Creating layer bn4_pool4_2
I1211 18:01:50.190488  2360 net.cpp:84] Creating Layer bn4_pool4_2
I1211 18:01:50.190488  2360 net.cpp:406] bn4_pool4_2 <- pool4_2
I1211 18:01:50.190488  2360 net.cpp:367] bn4_pool4_2 -> pool4_2 (in-place)
I1211 18:01:50.190488  2360 net.cpp:122] Setting up bn4_pool4_2
I1211 18:01:50.190488  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.190488  2360 net.cpp:137] Memory required for data: 458036400
I1211 18:01:50.190488  2360 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 18:01:50.190488  2360 net.cpp:84] Creating Layer scale4_pool4_2
I1211 18:01:50.190488  2360 net.cpp:406] scale4_pool4_2 <- pool4_2
I1211 18:01:50.190488  2360 net.cpp:367] scale4_pool4_2 -> pool4_2 (in-place)
I1211 18:01:50.190488  2360 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 18:01:50.191504  2360 net.cpp:122] Setting up scale4_pool4_2
I1211 18:01:50.191504  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.191504  2360 net.cpp:137] Memory required for data: 459521200
I1211 18:01:50.191504  2360 layer_factory.cpp:58] Creating layer relu4_pool4_2
I1211 18:01:50.191504  2360 net.cpp:84] Creating Layer relu4_pool4_2
I1211 18:01:50.191504  2360 net.cpp:406] relu4_pool4_2 <- pool4_2
I1211 18:01:50.191504  2360 net.cpp:367] relu4_pool4_2 -> pool4_2 (in-place)
I1211 18:01:50.191504  2360 net.cpp:122] Setting up relu4_pool4_2
I1211 18:01:50.191504  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.191504  2360 net.cpp:137] Memory required for data: 461006000
I1211 18:01:50.191504  2360 layer_factory.cpp:58] Creating layer conv4_0
I1211 18:01:50.191504  2360 net.cpp:84] Creating Layer conv4_0
I1211 18:01:50.191504  2360 net.cpp:406] conv4_0 <- pool4_2
I1211 18:01:50.191504  2360 net.cpp:380] conv4_0 -> conv4_0
I1211 18:01:50.193490  2360 net.cpp:122] Setting up conv4_0
I1211 18:01:50.193490  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.193490  2360 net.cpp:137] Memory required for data: 462490800
I1211 18:01:50.193490  2360 layer_factory.cpp:58] Creating layer bn4_0
I1211 18:01:50.193490  2360 net.cpp:84] Creating Layer bn4_0
I1211 18:01:50.193490  2360 net.cpp:406] bn4_0 <- conv4_0
I1211 18:01:50.193490  2360 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 18:01:50.194489  2360 net.cpp:122] Setting up bn4_0
I1211 18:01:50.194489  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.194489  2360 net.cpp:137] Memory required for data: 463975600
I1211 18:01:50.194489  2360 layer_factory.cpp:58] Creating layer scale4_0
I1211 18:01:50.194489  2360 net.cpp:84] Creating Layer scale4_0
I1211 18:01:50.194489  2360 net.cpp:406] scale4_0 <- conv4_0
I1211 18:01:50.194489  2360 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 18:01:50.194489  2360 layer_factory.cpp:58] Creating layer scale4_0
I1211 18:01:50.194489  2360 net.cpp:122] Setting up scale4_0
I1211 18:01:50.194489  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.194489  2360 net.cpp:137] Memory required for data: 465460400
I1211 18:01:50.194489  2360 layer_factory.cpp:58] Creating layer relu4_0
I1211 18:01:50.194489  2360 net.cpp:84] Creating Layer relu4_0
I1211 18:01:50.194489  2360 net.cpp:406] relu4_0 <- conv4_0
I1211 18:01:50.194489  2360 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 18:01:50.195487  2360 net.cpp:122] Setting up relu4_0
I1211 18:01:50.195487  2360 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 18:01:50.195487  2360 net.cpp:137] Memory required for data: 466945200
I1211 18:01:50.195487  2360 layer_factory.cpp:58] Creating layer conv11
I1211 18:01:50.195487  2360 net.cpp:84] Creating Layer conv11
I1211 18:01:50.195487  2360 net.cpp:406] conv11 <- conv4_0
I1211 18:01:50.195487  2360 net.cpp:380] conv11 -> conv11
I1211 18:01:50.196499  2360 net.cpp:122] Setting up conv11
I1211 18:01:50.196499  2360 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 18:01:50.196499  2360 net.cpp:137] Memory required for data: 468737200
I1211 18:01:50.196499  2360 layer_factory.cpp:58] Creating layer bn_conv11
I1211 18:01:50.196499  2360 net.cpp:84] Creating Layer bn_conv11
I1211 18:01:50.196499  2360 net.cpp:406] bn_conv11 <- conv11
I1211 18:01:50.196499  2360 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 18:01:50.196499  2360 net.cpp:122] Setting up bn_conv11
I1211 18:01:50.196499  2360 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 18:01:50.196499  2360 net.cpp:137] Memory required for data: 470529200
I1211 18:01:50.196499  2360 layer_factory.cpp:58] Creating layer scale_conv11
I1211 18:01:50.196499  2360 net.cpp:84] Creating Layer scale_conv11
I1211 18:01:50.197499  2360 net.cpp:406] scale_conv11 <- conv11
I1211 18:01:50.197499  2360 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 18:01:50.197499  2360 layer_factory.cpp:58] Creating layer scale_conv11
I1211 18:01:50.197499  2360 net.cpp:122] Setting up scale_conv11
I1211 18:01:50.197499  2360 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 18:01:50.197499  2360 net.cpp:137] Memory required for data: 472321200
I1211 18:01:50.197499  2360 layer_factory.cpp:58] Creating layer relu_conv11
I1211 18:01:50.197499  2360 net.cpp:84] Creating Layer relu_conv11
I1211 18:01:50.197499  2360 net.cpp:406] relu_conv11 <- conv11
I1211 18:01:50.197499  2360 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 18:01:50.197499  2360 net.cpp:122] Setting up relu_conv11
I1211 18:01:50.197499  2360 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 18:01:50.197499  2360 net.cpp:137] Memory required for data: 474113200
I1211 18:01:50.197499  2360 layer_factory.cpp:58] Creating layer conv12
I1211 18:01:50.197499  2360 net.cpp:84] Creating Layer conv12
I1211 18:01:50.197499  2360 net.cpp:406] conv12 <- conv11
I1211 18:01:50.197499  2360 net.cpp:380] conv12 -> conv12
I1211 18:01:50.199487  2360 net.cpp:122] Setting up conv12
I1211 18:01:50.199487  2360 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 18:01:50.199487  2360 net.cpp:137] Memory required for data: 476417200
I1211 18:01:50.199487  2360 layer_factory.cpp:58] Creating layer bn_conv12
I1211 18:01:50.199487  2360 net.cpp:84] Creating Layer bn_conv12
I1211 18:01:50.199487  2360 net.cpp:406] bn_conv12 <- conv12
I1211 18:01:50.199487  2360 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 18:01:50.199487  2360 net.cpp:122] Setting up bn_conv12
I1211 18:01:50.199487  2360 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 18:01:50.199487  2360 net.cpp:137] Memory required for data: 478721200
I1211 18:01:50.199487  2360 layer_factory.cpp:58] Creating layer scale_conv12
I1211 18:01:50.199487  2360 net.cpp:84] Creating Layer scale_conv12
I1211 18:01:50.199487  2360 net.cpp:406] scale_conv12 <- conv12
I1211 18:01:50.199487  2360 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 18:01:50.199487  2360 layer_factory.cpp:58] Creating layer scale_conv12
I1211 18:01:50.200496  2360 net.cpp:122] Setting up scale_conv12
I1211 18:01:50.200496  2360 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 18:01:50.200496  2360 net.cpp:137] Memory required for data: 481025200
I1211 18:01:50.200496  2360 layer_factory.cpp:58] Creating layer relu_conv12
I1211 18:01:50.200496  2360 net.cpp:84] Creating Layer relu_conv12
I1211 18:01:50.200496  2360 net.cpp:406] relu_conv12 <- conv12
I1211 18:01:50.200496  2360 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 18:01:50.200496  2360 net.cpp:122] Setting up relu_conv12
I1211 18:01:50.200496  2360 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 18:01:50.200496  2360 net.cpp:137] Memory required for data: 483329200
I1211 18:01:50.200496  2360 layer_factory.cpp:58] Creating layer poolcp6
I1211 18:01:50.200496  2360 net.cpp:84] Creating Layer poolcp6
I1211 18:01:50.200496  2360 net.cpp:406] poolcp6 <- conv12
I1211 18:01:50.200496  2360 net.cpp:380] poolcp6 -> poolcp6
I1211 18:01:50.200496  2360 net.cpp:122] Setting up poolcp6
I1211 18:01:50.200496  2360 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 18:01:50.200496  2360 net.cpp:137] Memory required for data: 483365200
I1211 18:01:50.200496  2360 layer_factory.cpp:58] Creating layer ip1
I1211 18:01:50.200496  2360 net.cpp:84] Creating Layer ip1
I1211 18:01:50.200496  2360 net.cpp:406] ip1 <- poolcp6
I1211 18:01:50.200496  2360 net.cpp:380] ip1 -> ip1
I1211 18:01:50.200496  2360 net.cpp:122] Setting up ip1
I1211 18:01:50.200496  2360 net.cpp:129] Top shape: 100 100 (10000)
I1211 18:01:50.200496  2360 net.cpp:137] Memory required for data: 483405200
I1211 18:01:50.201488  2360 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 18:01:50.201488  2360 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 18:01:50.201488  2360 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 18:01:50.201488  2360 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 18:01:50.201488  2360 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 18:01:50.201488  2360 net.cpp:122] Setting up ip1_ip1_0_split
I1211 18:01:50.201488  2360 net.cpp:129] Top shape: 100 100 (10000)
I1211 18:01:50.201488  2360 net.cpp:129] Top shape: 100 100 (10000)
I1211 18:01:50.201488  2360 net.cpp:137] Memory required for data: 483485200
I1211 18:01:50.201488  2360 layer_factory.cpp:58] Creating layer accuracy
I1211 18:01:50.201488  2360 net.cpp:84] Creating Layer accuracy
I1211 18:01:50.201488  2360 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1211 18:01:50.201488  2360 net.cpp:406] accuracy <- label_cifar_1_split_0
I1211 18:01:50.201488  2360 net.cpp:380] accuracy -> accuracy
I1211 18:01:50.201488  2360 net.cpp:122] Setting up accuracy
I1211 18:01:50.201488  2360 net.cpp:129] Top shape: (1)
I1211 18:01:50.201488  2360 net.cpp:137] Memory required for data: 483485204
I1211 18:01:50.201488  2360 layer_factory.cpp:58] Creating layer loss
I1211 18:01:50.201488  2360 net.cpp:84] Creating Layer loss
I1211 18:01:50.201488  2360 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 18:01:50.201488  2360 net.cpp:406] loss <- label_cifar_1_split_1
I1211 18:01:50.201488  2360 net.cpp:380] loss -> loss
I1211 18:01:50.201488  2360 layer_factory.cpp:58] Creating layer loss
I1211 18:01:50.201488  2360 net.cpp:122] Setting up loss
I1211 18:01:50.201488  2360 net.cpp:129] Top shape: (1)
I1211 18:01:50.201488  2360 net.cpp:132]     with loss weight 1
I1211 18:01:50.201488  2360 net.cpp:137] Memory required for data: 483485208
I1211 18:01:50.201488  2360 net.cpp:198] loss needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:200] accuracy does not need backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] ip1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] poolcp6 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu_conv12 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale_conv12 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn_conv12 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] conv12 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu_conv11 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale_conv11 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn_conv11 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] conv11 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu4_0 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale4_0 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn4_0 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] conv4_0 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu4_pool4_2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale4_pool4_2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn4_pool4_2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] pool4_2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu4_2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale4_2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn4_2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] conv4_2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu4_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale4_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn4_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] conv4_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu4 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale4 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn4 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] conv4 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu3_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale3_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn3_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] conv3_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu3 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale3 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn3 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] conv3 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu2_pool2_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale2_pool2_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn2_pool2_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] pool2_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu2_2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale2_2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn2_2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] conv2_2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu2_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale2_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn2_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] conv2_1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] conv2 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu1_0 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale1_0 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn1_0 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] conv1_0 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] relu1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] scale1 needs backward computation.
I1211 18:01:50.201488  2360 net.cpp:198] bn1 needs backward computation.
I1211 18:01:50.202499  2360 net.cpp:198] conv1 needs backward computation.
I1211 18:01:50.202499  2360 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 18:01:50.202499  2360 net.cpp:200] cifar does not need backward computation.
I1211 18:01:50.202499  2360 net.cpp:242] This network produces output accuracy
I1211 18:01:50.202499  2360 net.cpp:242] This network produces output loss
I1211 18:01:50.202499  2360 net.cpp:255] Network initialization done.
I1211 18:01:50.202499  2360 solver.cpp:56] Solver scaffolding done.
I1211 18:01:50.207486  2360 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90000.solverstate
I1211 18:01:50.211485  2360 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90000.caffemodel
I1211 18:01:50.211485  2360 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 18:01:50.211485  2360 sgd_solver.cpp:318] SGDSolver: restoring history
I1211 18:01:50.215488  2360 caffe.cpp:249] Starting Optimization
I1211 18:01:50.215488  2360 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV2_WnonLin_360k
I1211 18:01:50.215488  2360 solver.cpp:273] Learning Rate Policy: multistep
I1211 18:01:50.219483  2360 solver.cpp:330] Iteration 90000, Testing net (#0)
I1211 18:01:50.221498  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:01:51.621237 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:01:51.674744  2360 solver.cpp:397]     Test net output #0: accuracy = 0.5639
I1211 18:01:51.674744  2360 solver.cpp:397]     Test net output #1: loss = 1.73094 (* 1 = 1.73094 loss)
I1211 18:01:51.792369  2360 solver.cpp:218] Iteration 90000 (57145.7 iter/s, 1.57492s/100 iters), loss = 0.578607
I1211 18:01:51.792369  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 18:01:51.792369  2360 solver.cpp:237]     Train net output #1: loss = 0.578607 (* 1 = 0.578607 loss)
I1211 18:01:51.792369  2360 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1211 18:01:58.021556  2360 solver.cpp:218] Iteration 90100 (16.0549 iter/s, 6.22861s/100 iters), loss = 0.719435
I1211 18:01:58.021556  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 18:01:58.021556  2360 solver.cpp:237]     Train net output #1: loss = 0.719435 (* 1 = 0.719435 loss)
I1211 18:01:58.021556  2360 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1211 18:02:04.271831  2360 solver.cpp:218] Iteration 90200 (16.0003 iter/s, 6.24989s/100 iters), loss = 0.663413
I1211 18:02:04.272332  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 18:02:04.272332  2360 solver.cpp:237]     Train net output #1: loss = 0.663413 (* 1 = 0.663413 loss)
I1211 18:02:04.272332  2360 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1211 18:02:10.523732  2360 solver.cpp:218] Iteration 90300 (15.9957 iter/s, 6.2517s/100 iters), loss = 0.733808
I1211 18:02:10.523732  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 18:02:10.523732  2360 solver.cpp:237]     Train net output #1: loss = 0.733808 (* 1 = 0.733808 loss)
I1211 18:02:10.523732  2360 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1211 18:02:16.761363  2360 solver.cpp:218] Iteration 90400 (16.0347 iter/s, 6.23648s/100 iters), loss = 0.646504
I1211 18:02:16.761363  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 18:02:16.761363  2360 solver.cpp:237]     Train net output #1: loss = 0.646504 (* 1 = 0.646504 loss)
I1211 18:02:16.761363  2360 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1211 18:02:22.702188  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:02:22.948200  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90500.caffemodel
I1211 18:02:22.966202  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90500.solverstate
I1211 18:02:22.972708  2360 solver.cpp:330] Iteration 90500, Testing net (#0)
I1211 18:02:22.972708  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:02:24.338340 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:02:24.392346  2360 solver.cpp:397]     Test net output #0: accuracy = 0.5782
I1211 18:02:24.392346  2360 solver.cpp:397]     Test net output #1: loss = 1.63827 (* 1 = 1.63827 loss)
I1211 18:02:24.451345  2360 solver.cpp:218] Iteration 90500 (13.004 iter/s, 7.68992s/100 iters), loss = 0.523905
I1211 18:02:24.451345  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:02:24.451345  2360 solver.cpp:237]     Train net output #1: loss = 0.523905 (* 1 = 0.523905 loss)
I1211 18:02:24.451345  2360 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1211 18:02:30.679401  2360 solver.cpp:218] Iteration 90600 (16.0583 iter/s, 6.22733s/100 iters), loss = 0.61682
I1211 18:02:30.679401  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 18:02:30.679401  2360 solver.cpp:237]     Train net output #1: loss = 0.61682 (* 1 = 0.61682 loss)
I1211 18:02:30.679401  2360 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1211 18:02:36.835430  2360 solver.cpp:218] Iteration 90700 (16.2449 iter/s, 6.15579s/100 iters), loss = 0.650078
I1211 18:02:36.835430  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 18:02:36.835430  2360 solver.cpp:237]     Train net output #1: loss = 0.650078 (* 1 = 0.650078 loss)
I1211 18:02:36.835430  2360 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1211 18:02:42.972570  2360 solver.cpp:218] Iteration 90800 (16.2951 iter/s, 6.13682s/100 iters), loss = 0.706568
I1211 18:02:42.972570  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 18:02:42.972570  2360 solver.cpp:237]     Train net output #1: loss = 0.706568 (* 1 = 0.706568 loss)
I1211 18:02:42.972570  2360 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1211 18:02:49.140125  2360 solver.cpp:218] Iteration 90900 (16.2158 iter/s, 6.16683s/100 iters), loss = 0.740249
I1211 18:02:49.140125  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 18:02:49.140125  2360 solver.cpp:237]     Train net output #1: loss = 0.740249 (* 1 = 0.740249 loss)
I1211 18:02:49.140125  2360 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1211 18:02:55.016537  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:02:55.260777  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91000.caffemodel
I1211 18:02:55.275779  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91000.solverstate
I1211 18:02:55.280779  2360 solver.cpp:330] Iteration 91000, Testing net (#0)
I1211 18:02:55.280779  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:02:56.619730 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:02:56.672726  2360 solver.cpp:397]     Test net output #0: accuracy = 0.5751
I1211 18:02:56.672726  2360 solver.cpp:397]     Test net output #1: loss = 1.7051 (* 1 = 1.7051 loss)
I1211 18:02:56.731240  2360 solver.cpp:218] Iteration 91000 (13.174 iter/s, 7.59068s/100 iters), loss = 0.635815
I1211 18:02:56.731240  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 18:02:56.731240  2360 solver.cpp:237]     Train net output #1: loss = 0.635815 (* 1 = 0.635815 loss)
I1211 18:02:56.731240  2360 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1211 18:03:02.907850  2360 solver.cpp:218] Iteration 91100 (16.1901 iter/s, 6.1766s/100 iters), loss = 0.631698
I1211 18:03:02.907850  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 18:03:02.907850  2360 solver.cpp:237]     Train net output #1: loss = 0.631698 (* 1 = 0.631698 loss)
I1211 18:03:02.907850  2360 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1211 18:03:09.084910  2360 solver.cpp:218] Iteration 91200 (16.1919 iter/s, 6.17593s/100 iters), loss = 0.636565
I1211 18:03:09.084910  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 18:03:09.084910  2360 solver.cpp:237]     Train net output #1: loss = 0.636565 (* 1 = 0.636565 loss)
I1211 18:03:09.084910  2360 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1211 18:03:15.231758  2360 solver.cpp:218] Iteration 91300 (16.2684 iter/s, 6.14688s/100 iters), loss = 0.620126
I1211 18:03:15.231758  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 18:03:15.231758  2360 solver.cpp:237]     Train net output #1: loss = 0.620126 (* 1 = 0.620126 loss)
I1211 18:03:15.231758  2360 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1211 18:03:21.368909  2360 solver.cpp:218] Iteration 91400 (16.2958 iter/s, 6.13655s/100 iters), loss = 0.757412
I1211 18:03:21.368909  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 18:03:21.368909  2360 solver.cpp:237]     Train net output #1: loss = 0.757412 (* 1 = 0.757412 loss)
I1211 18:03:21.368909  2360 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1211 18:03:27.213685  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:03:27.455323  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91500.caffemodel
I1211 18:03:27.469324  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91500.solverstate
I1211 18:03:27.474324  2360 solver.cpp:330] Iteration 91500, Testing net (#0)
I1211 18:03:27.474324  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:03:28.815085 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:03:28.867240  2360 solver.cpp:397]     Test net output #0: accuracy = 0.5776
I1211 18:03:28.867240  2360 solver.cpp:397]     Test net output #1: loss = 1.66473 (* 1 = 1.66473 loss)
I1211 18:03:28.925797  2360 solver.cpp:218] Iteration 91500 (13.233 iter/s, 7.55687s/100 iters), loss = 0.626474
I1211 18:03:28.926796  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 18:03:28.926796  2360 solver.cpp:237]     Train net output #1: loss = 0.626474 (* 1 = 0.626474 loss)
I1211 18:03:28.926796  2360 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1211 18:03:35.088428  2360 solver.cpp:218] Iteration 91600 (16.2303 iter/s, 6.16133s/100 iters), loss = 0.714997
I1211 18:03:35.088428  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 18:03:35.088428  2360 solver.cpp:237]     Train net output #1: loss = 0.714997 (* 1 = 0.714997 loss)
I1211 18:03:35.088428  2360 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1211 18:03:41.273373  2360 solver.cpp:218] Iteration 91700 (16.1696 iter/s, 6.18445s/100 iters), loss = 0.58407
I1211 18:03:41.273373  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 18:03:41.273373  2360 solver.cpp:237]     Train net output #1: loss = 0.58407 (* 1 = 0.58407 loss)
I1211 18:03:41.273373  2360 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1211 18:03:47.499110  2360 solver.cpp:218] Iteration 91800 (16.0635 iter/s, 6.22529s/100 iters), loss = 0.637863
I1211 18:03:47.499110  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 18:03:47.499110  2360 solver.cpp:237]     Train net output #1: loss = 0.637863 (* 1 = 0.637863 loss)
I1211 18:03:47.499110  2360 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1211 18:03:53.714579  2360 solver.cpp:218] Iteration 91900 (16.0903 iter/s, 6.21492s/100 iters), loss = 0.710617
I1211 18:03:53.714579  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 18:03:53.714579  2360 solver.cpp:237]     Train net output #1: loss = 0.710617 (* 1 = 0.710617 loss)
I1211 18:03:53.714579  2360 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1211 18:03:59.622608  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:03:59.869009  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92000.caffemodel
I1211 18:03:59.887069  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92000.solverstate
I1211 18:03:59.892079  2360 solver.cpp:330] Iteration 92000, Testing net (#0)
I1211 18:03:59.892079  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:04:01.242064 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:04:01.295075  2360 solver.cpp:397]     Test net output #0: accuracy = 0.5451
I1211 18:04:01.295075  2360 solver.cpp:397]     Test net output #1: loss = 1.81261 (* 1 = 1.81261 loss)
I1211 18:04:01.356576  2360 solver.cpp:218] Iteration 92000 (13.086 iter/s, 7.64174s/100 iters), loss = 0.521749
I1211 18:04:01.356576  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:04:01.356576  2360 solver.cpp:237]     Train net output #1: loss = 0.521749 (* 1 = 0.521749 loss)
I1211 18:04:01.356576  2360 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1211 18:04:07.568794  2360 solver.cpp:218] Iteration 92100 (16.0991 iter/s, 6.21154s/100 iters), loss = 0.739511
I1211 18:04:07.568794  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 18:04:07.568794  2360 solver.cpp:237]     Train net output #1: loss = 0.739511 (* 1 = 0.739511 loss)
I1211 18:04:07.568794  2360 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1211 18:04:13.778702  2360 solver.cpp:218] Iteration 92200 (16.1042 iter/s, 6.20956s/100 iters), loss = 0.651056
I1211 18:04:13.778702  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 18:04:13.778702  2360 solver.cpp:237]     Train net output #1: loss = 0.651056 (* 1 = 0.651056 loss)
I1211 18:04:13.778702  2360 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1211 18:04:19.999405  2360 solver.cpp:218] Iteration 92300 (16.0756 iter/s, 6.2206s/100 iters), loss = 0.699834
I1211 18:04:19.999405  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 18:04:19.999405  2360 solver.cpp:237]     Train net output #1: loss = 0.699834 (* 1 = 0.699834 loss)
I1211 18:04:19.999405  2360 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1211 18:04:26.151886  2360 solver.cpp:218] Iteration 92400 (16.2563 iter/s, 6.15145s/100 iters), loss = 0.690604
I1211 18:04:26.151886  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 18:04:26.151886  2360 solver.cpp:237]     Train net output #1: loss = 0.690604 (* 1 = 0.690604 loss)
I1211 18:04:26.151886  2360 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1211 18:04:31.993322  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:04:32.237336  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92500.caffemodel
I1211 18:04:32.254335  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92500.solverstate
I1211 18:04:32.259337  2360 solver.cpp:330] Iteration 92500, Testing net (#0)
I1211 18:04:32.259337  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:04:33.601467 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:04:33.654474  2360 solver.cpp:397]     Test net output #0: accuracy = 0.5739
I1211 18:04:33.654474  2360 solver.cpp:397]     Test net output #1: loss = 1.6479 (* 1 = 1.6479 loss)
I1211 18:04:33.714473  2360 solver.cpp:218] Iteration 92500 (13.2236 iter/s, 7.56223s/100 iters), loss = 0.607976
I1211 18:04:33.714473  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:04:33.714473  2360 solver.cpp:237]     Train net output #1: loss = 0.607976 (* 1 = 0.607976 loss)
I1211 18:04:33.714473  2360 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1211 18:04:39.855897  2360 solver.cpp:218] Iteration 92600 (16.2836 iter/s, 6.14113s/100 iters), loss = 0.619557
I1211 18:04:39.855897  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 18:04:39.855897  2360 solver.cpp:237]     Train net output #1: loss = 0.619557 (* 1 = 0.619557 loss)
I1211 18:04:39.855897  2360 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1211 18:04:46.000330  2360 solver.cpp:218] Iteration 92700 (16.2752 iter/s, 6.14434s/100 iters), loss = 0.737319
I1211 18:04:46.000330  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 18:04:46.000330  2360 solver.cpp:237]     Train net output #1: loss = 0.737319 (* 1 = 0.737319 loss)
I1211 18:04:46.000330  2360 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1211 18:04:52.154767  2360 solver.cpp:218] Iteration 92800 (16.2504 iter/s, 6.15371s/100 iters), loss = 0.652373
I1211 18:04:52.154767  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 18:04:52.154767  2360 solver.cpp:237]     Train net output #1: loss = 0.652373 (* 1 = 0.652373 loss)
I1211 18:04:52.154767  2360 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1211 18:04:58.311348  2360 solver.cpp:218] Iteration 92900 (16.2444 iter/s, 6.15598s/100 iters), loss = 0.886123
I1211 18:04:58.311348  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1211 18:04:58.311348  2360 solver.cpp:237]     Train net output #1: loss = 0.886123 (* 1 = 0.886123 loss)
I1211 18:04:58.311348  2360 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1211 18:05:04.170773  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:05:04.413784  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93000.caffemodel
I1211 18:05:04.429785  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93000.solverstate
I1211 18:05:04.434784  2360 solver.cpp:330] Iteration 93000, Testing net (#0)
I1211 18:05:04.434784  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:05:05.777392 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:05:05.829895  2360 solver.cpp:397]     Test net output #0: accuracy = 0.5862
I1211 18:05:05.829895  2360 solver.cpp:397]     Test net output #1: loss = 1.58579 (* 1 = 1.58579 loss)
I1211 18:05:05.888898  2360 solver.cpp:218] Iteration 93000 (13.1964 iter/s, 7.57781s/100 iters), loss = 0.605898
I1211 18:05:05.888898  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:05:05.888898  2360 solver.cpp:237]     Train net output #1: loss = 0.605898 (* 1 = 0.605898 loss)
I1211 18:05:05.888898  2360 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1211 18:05:12.045369  2360 solver.cpp:218] Iteration 93100 (16.2447 iter/s, 6.15584s/100 iters), loss = 0.733198
I1211 18:05:12.045369  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 18:05:12.045369  2360 solver.cpp:237]     Train net output #1: loss = 0.733198 (* 1 = 0.733198 loss)
I1211 18:05:12.045369  2360 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1211 18:05:18.202235  2360 solver.cpp:218] Iteration 93200 (16.2439 iter/s, 6.15617s/100 iters), loss = 0.741987
I1211 18:05:18.202235  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 18:05:18.202235  2360 solver.cpp:237]     Train net output #1: loss = 0.741987 (* 1 = 0.741987 loss)
I1211 18:05:18.202235  2360 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1211 18:05:24.353855  2360 solver.cpp:218] Iteration 93300 (16.2567 iter/s, 6.1513s/100 iters), loss = 0.784638
I1211 18:05:24.353855  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 18:05:24.353855  2360 solver.cpp:237]     Train net output #1: loss = 0.784638 (* 1 = 0.784638 loss)
I1211 18:05:24.353855  2360 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1211 18:05:30.505792  2360 solver.cpp:218] Iteration 93400 (16.2571 iter/s, 6.15116s/100 iters), loss = 0.73142
I1211 18:05:30.505792  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 18:05:30.505792  2360 solver.cpp:237]     Train net output #1: loss = 0.73142 (* 1 = 0.73142 loss)
I1211 18:05:30.505792  2360 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1211 18:05:36.355518  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:05:36.597508  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93500.caffemodel
I1211 18:05:36.615509  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93500.solverstate
I1211 18:05:36.620504  2360 solver.cpp:330] Iteration 93500, Testing net (#0)
I1211 18:05:36.620504  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:05:37.962723 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:05:38.015645  2360 solver.cpp:397]     Test net output #0: accuracy = 0.555
I1211 18:05:38.015645  2360 solver.cpp:397]     Test net output #1: loss = 1.76543 (* 1 = 1.76543 loss)
I1211 18:05:38.074672  2360 solver.cpp:218] Iteration 93500 (13.2134 iter/s, 7.56805s/100 iters), loss = 0.574665
I1211 18:05:38.074672  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:05:38.074672  2360 solver.cpp:237]     Train net output #1: loss = 0.574665 (* 1 = 0.574665 loss)
I1211 18:05:38.074672  2360 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1211 18:05:44.222643  2360 solver.cpp:218] Iteration 93600 (16.2666 iter/s, 6.14759s/100 iters), loss = 0.690341
I1211 18:05:44.222643  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 18:05:44.222643  2360 solver.cpp:237]     Train net output #1: loss = 0.690341 (* 1 = 0.690341 loss)
I1211 18:05:44.222643  2360 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1211 18:05:50.364743  2360 solver.cpp:218] Iteration 93700 (16.281 iter/s, 6.14213s/100 iters), loss = 0.515423
I1211 18:05:50.364743  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 18:05:50.364743  2360 solver.cpp:237]     Train net output #1: loss = 0.515423 (* 1 = 0.515423 loss)
I1211 18:05:50.364743  2360 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1211 18:05:56.510356  2360 solver.cpp:218] Iteration 93800 (16.2748 iter/s, 6.14445s/100 iters), loss = 0.769701
I1211 18:05:56.510356  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 18:05:56.510356  2360 solver.cpp:237]     Train net output #1: loss = 0.769701 (* 1 = 0.769701 loss)
I1211 18:05:56.510356  2360 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1211 18:06:02.658900  2360 solver.cpp:218] Iteration 93900 (16.2638 iter/s, 6.14864s/100 iters), loss = 0.71959
I1211 18:06:02.658900  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 18:06:02.658900  2360 solver.cpp:237]     Train net output #1: loss = 0.71959 (* 1 = 0.71959 loss)
I1211 18:06:02.658900  2360 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1211 18:06:08.506333  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:06:08.748345  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94000.caffemodel
I1211 18:06:08.766345  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94000.solverstate
I1211 18:06:08.771345  2360 solver.cpp:330] Iteration 94000, Testing net (#0)
I1211 18:06:08.771345  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:06:10.114442 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:06:10.167440  2360 solver.cpp:397]     Test net output #0: accuracy = 0.5418
I1211 18:06:10.168444  2360 solver.cpp:397]     Test net output #1: loss = 1.90833 (* 1 = 1.90833 loss)
I1211 18:06:10.226462  2360 solver.cpp:218] Iteration 94000 (13.2149 iter/s, 7.5672s/100 iters), loss = 0.558307
I1211 18:06:10.226462  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 18:06:10.226462  2360 solver.cpp:237]     Train net output #1: loss = 0.558307 (* 1 = 0.558307 loss)
I1211 18:06:10.226462  2360 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1211 18:06:16.375968  2360 solver.cpp:218] Iteration 94100 (16.2638 iter/s, 6.14863s/100 iters), loss = 0.615399
I1211 18:06:16.375968  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 18:06:16.375968  2360 solver.cpp:237]     Train net output #1: loss = 0.615399 (* 1 = 0.615399 loss)
I1211 18:06:16.375968  2360 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1211 18:06:22.521406  2360 solver.cpp:218] Iteration 94200 (16.2718 iter/s, 6.14559s/100 iters), loss = 0.641089
I1211 18:06:22.521406  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 18:06:22.521406  2360 solver.cpp:237]     Train net output #1: loss = 0.641089 (* 1 = 0.641089 loss)
I1211 18:06:22.521406  2360 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1211 18:06:28.670832  2360 solver.cpp:218] Iteration 94300 (16.2637 iter/s, 6.14868s/100 iters), loss = 0.764944
I1211 18:06:28.670832  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 18:06:28.670832  2360 solver.cpp:237]     Train net output #1: loss = 0.764944 (* 1 = 0.764944 loss)
I1211 18:06:28.670832  2360 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1211 18:06:34.825299  2360 solver.cpp:218] Iteration 94400 (16.2497 iter/s, 6.15395s/100 iters), loss = 0.740075
I1211 18:06:34.825299  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 18:06:34.825299  2360 solver.cpp:237]     Train net output #1: loss = 0.740075 (* 1 = 0.740075 loss)
I1211 18:06:34.825299  2360 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1211 18:06:40.667711  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:06:40.908727  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94500.caffemodel
I1211 18:06:40.927728  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94500.solverstate
I1211 18:06:40.932727  2360 solver.cpp:330] Iteration 94500, Testing net (#0)
I1211 18:06:40.932727  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:06:42.274809 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:06:42.327813  2360 solver.cpp:397]     Test net output #0: accuracy = 0.5795
I1211 18:06:42.327813  2360 solver.cpp:397]     Test net output #1: loss = 1.64902 (* 1 = 1.64902 loss)
I1211 18:06:42.386315  2360 solver.cpp:218] Iteration 94500 (13.227 iter/s, 7.5603s/100 iters), loss = 0.596891
I1211 18:06:42.386315  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:06:42.386315  2360 solver.cpp:237]     Train net output #1: loss = 0.596891 (* 1 = 0.596891 loss)
I1211 18:06:42.386315  2360 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1211 18:06:48.543251  2360 solver.cpp:218] Iteration 94600 (16.2431 iter/s, 6.15644s/100 iters), loss = 0.57322
I1211 18:06:48.543251  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 18:06:48.543251  2360 solver.cpp:237]     Train net output #1: loss = 0.57322 (* 1 = 0.57322 loss)
I1211 18:06:48.543251  2360 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1211 18:06:54.690187  2360 solver.cpp:218] Iteration 94700 (16.2687 iter/s, 6.14677s/100 iters), loss = 0.593873
I1211 18:06:54.690187  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 18:06:54.690187  2360 solver.cpp:237]     Train net output #1: loss = 0.593873 (* 1 = 0.593873 loss)
I1211 18:06:54.690187  2360 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1211 18:07:00.836174  2360 solver.cpp:218] Iteration 94800 (16.2715 iter/s, 6.14572s/100 iters), loss = 0.773879
I1211 18:07:00.836174  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 18:07:00.836174  2360 solver.cpp:237]     Train net output #1: loss = 0.773879 (* 1 = 0.773879 loss)
I1211 18:07:00.836174  2360 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1211 18:07:06.993147  2360 solver.cpp:218] Iteration 94900 (16.2432 iter/s, 6.15642s/100 iters), loss = 0.82768
I1211 18:07:06.993147  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 18:07:06.993649  2360 solver.cpp:237]     Train net output #1: loss = 0.82768 (* 1 = 0.82768 loss)
I1211 18:07:06.993649  2360 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1211 18:07:12.842123  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:07:13.083134  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95000.caffemodel
I1211 18:07:13.100144  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95000.solverstate
I1211 18:07:13.105144  2360 solver.cpp:330] Iteration 95000, Testing net (#0)
I1211 18:07:13.106144  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:07:14.446283 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:07:14.500288  2360 solver.cpp:397]     Test net output #0: accuracy = 0.5813
I1211 18:07:14.500288  2360 solver.cpp:397]     Test net output #1: loss = 1.59267 (* 1 = 1.59267 loss)
I1211 18:07:14.559288  2360 solver.cpp:218] Iteration 95000 (13.2169 iter/s, 7.56608s/100 iters), loss = 0.559925
I1211 18:07:14.559288  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:07:14.559288  2360 solver.cpp:237]     Train net output #1: loss = 0.559925 (* 1 = 0.559925 loss)
I1211 18:07:14.559288  2360 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1211 18:07:14.559288  2360 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1211 18:07:20.705700  2360 solver.cpp:218] Iteration 95100 (16.2721 iter/s, 6.14549s/100 iters), loss = 0.632382
I1211 18:07:20.705700  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 18:07:20.705700  2360 solver.cpp:237]     Train net output #1: loss = 0.632382 (* 1 = 0.632382 loss)
I1211 18:07:20.705700  2360 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1211 18:07:26.859225  2360 solver.cpp:218] Iteration 95200 (16.2515 iter/s, 6.15326s/100 iters), loss = 0.558899
I1211 18:07:26.859225  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 18:07:26.859225  2360 solver.cpp:237]     Train net output #1: loss = 0.558899 (* 1 = 0.558899 loss)
I1211 18:07:26.859225  2360 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1211 18:07:33.016701  2360 solver.cpp:218] Iteration 95300 (16.2419 iter/s, 6.15692s/100 iters), loss = 0.532367
I1211 18:07:33.016701  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:07:33.016701  2360 solver.cpp:237]     Train net output #1: loss = 0.532367 (* 1 = 0.532367 loss)
I1211 18:07:33.016701  2360 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1211 18:07:39.171212  2360 solver.cpp:218] Iteration 95400 (16.2501 iter/s, 6.1538s/100 iters), loss = 0.472244
I1211 18:07:39.171212  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 18:07:39.171212  2360 solver.cpp:237]     Train net output #1: loss = 0.472244 (* 1 = 0.472244 loss)
I1211 18:07:39.171212  2360 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1211 18:07:45.018674  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:07:45.261689  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95500.caffemodel
I1211 18:07:45.282690  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95500.solverstate
I1211 18:07:45.287694  2360 solver.cpp:330] Iteration 95500, Testing net (#0)
I1211 18:07:45.287694  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:07:46.628849 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:07:46.681848  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6744
I1211 18:07:46.681848  2360 solver.cpp:397]     Test net output #1: loss = 1.19638 (* 1 = 1.19638 loss)
I1211 18:07:46.740855  2360 solver.cpp:218] Iteration 95500 (13.2106 iter/s, 7.5697s/100 iters), loss = 0.467357
I1211 18:07:46.741856  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:07:46.741856  2360 solver.cpp:237]     Train net output #1: loss = 0.467357 (* 1 = 0.467357 loss)
I1211 18:07:46.741856  2360 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1211 18:07:52.891809  2360 solver.cpp:218] Iteration 95600 (16.2614 iter/s, 6.14953s/100 iters), loss = 0.486098
I1211 18:07:52.891809  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:07:52.891809  2360 solver.cpp:237]     Train net output #1: loss = 0.486098 (* 1 = 0.486098 loss)
I1211 18:07:52.891809  2360 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1211 18:07:59.034780  2360 solver.cpp:218] Iteration 95700 (16.2787 iter/s, 6.14298s/100 iters), loss = 0.413017
I1211 18:07:59.034780  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:07:59.034780  2360 solver.cpp:237]     Train net output #1: loss = 0.413017 (* 1 = 0.413017 loss)
I1211 18:07:59.034780  2360 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1211 18:08:05.180245  2360 solver.cpp:218] Iteration 95800 (16.273 iter/s, 6.14513s/100 iters), loss = 0.498538
I1211 18:08:05.180245  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 18:08:05.180245  2360 solver.cpp:237]     Train net output #1: loss = 0.498538 (* 1 = 0.498538 loss)
I1211 18:08:05.180245  2360 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1211 18:08:11.333660  2360 solver.cpp:218] Iteration 95900 (16.2521 iter/s, 6.15305s/100 iters), loss = 0.545406
I1211 18:08:11.333660  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 18:08:11.333660  2360 solver.cpp:237]     Train net output #1: loss = 0.545406 (* 1 = 0.545406 loss)
I1211 18:08:11.333660  2360 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1211 18:08:17.174103  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:08:17.415150  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96000.caffemodel
I1211 18:08:17.432150  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96000.solverstate
I1211 18:08:17.437150  2360 solver.cpp:330] Iteration 96000, Testing net (#0)
I1211 18:08:17.437150  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:08:18.779258 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:08:18.832265  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6757
I1211 18:08:18.832265  2360 solver.cpp:397]     Test net output #1: loss = 1.19333 (* 1 = 1.19333 loss)
I1211 18:08:18.890767  2360 solver.cpp:218] Iteration 96000 (13.2343 iter/s, 7.55611s/100 iters), loss = 0.410716
I1211 18:08:18.890767  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:08:18.890767  2360 solver.cpp:237]     Train net output #1: loss = 0.410716 (* 1 = 0.410716 loss)
I1211 18:08:18.890767  2360 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1211 18:08:25.040678  2360 solver.cpp:218] Iteration 96100 (16.2618 iter/s, 6.14939s/100 iters), loss = 0.533797
I1211 18:08:25.040678  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 18:08:25.040678  2360 solver.cpp:237]     Train net output #1: loss = 0.533797 (* 1 = 0.533797 loss)
I1211 18:08:25.040678  2360 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1211 18:08:31.193665  2360 solver.cpp:218] Iteration 96200 (16.2531 iter/s, 6.15268s/100 iters), loss = 0.4781
I1211 18:08:31.193665  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:08:31.193665  2360 solver.cpp:237]     Train net output #1: loss = 0.4781 (* 1 = 0.4781 loss)
I1211 18:08:31.193665  2360 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1211 18:08:37.342661  2360 solver.cpp:218] Iteration 96300 (16.2629 iter/s, 6.14895s/100 iters), loss = 0.466508
I1211 18:08:37.342661  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:08:37.342661  2360 solver.cpp:237]     Train net output #1: loss = 0.466508 (* 1 = 0.466508 loss)
I1211 18:08:37.342661  2360 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1211 18:08:43.499395  2360 solver.cpp:218] Iteration 96400 (16.2444 iter/s, 6.15595s/100 iters), loss = 0.464349
I1211 18:08:43.499395  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:08:43.499908  2360 solver.cpp:237]     Train net output #1: loss = 0.464349 (* 1 = 0.464349 loss)
I1211 18:08:43.499908  2360 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1211 18:08:49.354425  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:08:49.596441  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96500.caffemodel
I1211 18:08:49.614457  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96500.solverstate
I1211 18:08:49.619958  2360 solver.cpp:330] Iteration 96500, Testing net (#0)
I1211 18:08:49.619958  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:08:50.962613 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:08:51.016130  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6747
I1211 18:08:51.016130  2360 solver.cpp:397]     Test net output #1: loss = 1.19086 (* 1 = 1.19086 loss)
I1211 18:08:51.074627  2360 solver.cpp:218] Iteration 96500 (13.2025 iter/s, 7.57434s/100 iters), loss = 0.427552
I1211 18:08:51.074627  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:08:51.074627  2360 solver.cpp:237]     Train net output #1: loss = 0.427552 (* 1 = 0.427552 loss)
I1211 18:08:51.074627  2360 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1211 18:08:57.228319  2360 solver.cpp:218] Iteration 96600 (16.2499 iter/s, 6.15387s/100 iters), loss = 0.492753
I1211 18:08:57.228319  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:08:57.228319  2360 solver.cpp:237]     Train net output #1: loss = 0.492753 (* 1 = 0.492753 loss)
I1211 18:08:57.228319  2360 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1211 18:09:03.383875  2360 solver.cpp:218] Iteration 96700 (16.2465 iter/s, 6.15518s/100 iters), loss = 0.432892
I1211 18:09:03.383875  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:09:03.383875  2360 solver.cpp:237]     Train net output #1: loss = 0.432892 (* 1 = 0.432892 loss)
I1211 18:09:03.383875  2360 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1211 18:09:09.534445  2360 solver.cpp:218] Iteration 96800 (16.2615 iter/s, 6.14948s/100 iters), loss = 0.489085
I1211 18:09:09.534445  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:09:09.534445  2360 solver.cpp:237]     Train net output #1: loss = 0.489085 (* 1 = 0.489085 loss)
I1211 18:09:09.534445  2360 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1211 18:09:15.691965  2360 solver.cpp:218] Iteration 96900 (16.2418 iter/s, 6.15694s/100 iters), loss = 0.470102
I1211 18:09:15.691965  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 18:09:15.691965  2360 solver.cpp:237]     Train net output #1: loss = 0.470102 (* 1 = 0.470102 loss)
I1211 18:09:15.691965  2360 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1211 18:09:21.545414  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:09:21.787438  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97000.caffemodel
I1211 18:09:21.805444  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97000.solverstate
I1211 18:09:21.812943  2360 solver.cpp:330] Iteration 97000, Testing net (#0)
I1211 18:09:21.812943  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:09:23.155612 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:09:23.208616  2360 solver.cpp:397]     Test net output #0: accuracy = 0.677
I1211 18:09:23.208616  2360 solver.cpp:397]     Test net output #1: loss = 1.20023 (* 1 = 1.20023 loss)
I1211 18:09:23.266618  2360 solver.cpp:218] Iteration 97000 (13.2025 iter/s, 7.57433s/100 iters), loss = 0.39808
I1211 18:09:23.266618  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:09:23.266618  2360 solver.cpp:237]     Train net output #1: loss = 0.39808 (* 1 = 0.39808 loss)
I1211 18:09:23.266618  2360 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1211 18:09:29.430037  2360 solver.cpp:218] Iteration 97100 (16.2259 iter/s, 6.16299s/100 iters), loss = 0.487324
I1211 18:09:29.430037  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 18:09:29.430537  2360 solver.cpp:237]     Train net output #1: loss = 0.487324 (* 1 = 0.487324 loss)
I1211 18:09:29.430537  2360 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1211 18:09:35.580451  2360 solver.cpp:218] Iteration 97200 (16.2596 iter/s, 6.15022s/100 iters), loss = 0.386991
I1211 18:09:35.580451  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:09:35.580451  2360 solver.cpp:237]     Train net output #1: loss = 0.386991 (* 1 = 0.386991 loss)
I1211 18:09:35.580451  2360 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1211 18:09:41.774173  2360 solver.cpp:218] Iteration 97300 (16.147 iter/s, 6.19312s/100 iters), loss = 0.457412
I1211 18:09:41.774173  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:09:41.774173  2360 solver.cpp:237]     Train net output #1: loss = 0.457412 (* 1 = 0.457412 loss)
I1211 18:09:41.774173  2360 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1211 18:09:47.918848  2360 solver.cpp:218] Iteration 97400 (16.2752 iter/s, 6.1443s/100 iters), loss = 0.452292
I1211 18:09:47.918848  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:09:47.918848  2360 solver.cpp:237]     Train net output #1: loss = 0.452292 (* 1 = 0.452292 loss)
I1211 18:09:47.918848  2360 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1211 18:09:53.762342  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:09:54.005386  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97500.caffemodel
I1211 18:09:54.022387  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97500.solverstate
I1211 18:09:54.027386  2360 solver.cpp:330] Iteration 97500, Testing net (#0)
I1211 18:09:54.027386  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:09:55.370529 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:09:55.423535  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6786
I1211 18:09:55.423535  2360 solver.cpp:397]     Test net output #1: loss = 1.2005 (* 1 = 1.2005 loss)
I1211 18:09:55.482533  2360 solver.cpp:218] Iteration 97500 (13.2222 iter/s, 7.56303s/100 iters), loss = 0.387183
I1211 18:09:55.482533  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:09:55.482533  2360 solver.cpp:237]     Train net output #1: loss = 0.387183 (* 1 = 0.387183 loss)
I1211 18:09:55.482533  2360 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1211 18:10:01.654031  2360 solver.cpp:218] Iteration 97600 (16.2049 iter/s, 6.17096s/100 iters), loss = 0.491763
I1211 18:10:01.654031  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 18:10:01.654031  2360 solver.cpp:237]     Train net output #1: loss = 0.491763 (* 1 = 0.491763 loss)
I1211 18:10:01.654031  2360 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1211 18:10:07.828619  2360 solver.cpp:218] Iteration 97700 (16.1959 iter/s, 6.17441s/100 iters), loss = 0.352158
I1211 18:10:07.828619  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:10:07.828619  2360 solver.cpp:237]     Train net output #1: loss = 0.352158 (* 1 = 0.352158 loss)
I1211 18:10:07.828619  2360 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1211 18:10:13.985098  2360 solver.cpp:218] Iteration 97800 (16.2438 iter/s, 6.15619s/100 iters), loss = 0.487512
I1211 18:10:13.985098  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:10:13.985098  2360 solver.cpp:237]     Train net output #1: loss = 0.487512 (* 1 = 0.487512 loss)
I1211 18:10:13.985098  2360 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1211 18:10:20.136798  2360 solver.cpp:218] Iteration 97900 (16.2576 iter/s, 6.15096s/100 iters), loss = 0.48518
I1211 18:10:20.136798  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:10:20.136798  2360 solver.cpp:237]     Train net output #1: loss = 0.48518 (* 1 = 0.48518 loss)
I1211 18:10:20.136798  2360 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1211 18:10:25.987594  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:10:26.228698  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98000.caffemodel
I1211 18:10:26.243696  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98000.solverstate
I1211 18:10:26.248697  2360 solver.cpp:330] Iteration 98000, Testing net (#0)
I1211 18:10:26.248697  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:10:27.590293 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:10:27.642928  2360 solver.cpp:397]     Test net output #0: accuracy = 0.682
I1211 18:10:27.642928  2360 solver.cpp:397]     Test net output #1: loss = 1.19432 (* 1 = 1.19432 loss)
I1211 18:10:27.701932  2360 solver.cpp:218] Iteration 98000 (13.2198 iter/s, 7.56439s/100 iters), loss = 0.425574
I1211 18:10:27.701932  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:10:27.701932  2360 solver.cpp:237]     Train net output #1: loss = 0.425574 (* 1 = 0.425574 loss)
I1211 18:10:27.701932  2360 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1211 18:10:33.854235  2360 solver.cpp:218] Iteration 98100 (16.2556 iter/s, 6.15172s/100 iters), loss = 0.540583
I1211 18:10:33.854235  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 18:10:33.854235  2360 solver.cpp:237]     Train net output #1: loss = 0.540583 (* 1 = 0.540583 loss)
I1211 18:10:33.854235  2360 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1211 18:10:40.003501  2360 solver.cpp:218] Iteration 98200 (16.2626 iter/s, 6.14909s/100 iters), loss = 0.362115
I1211 18:10:40.003501  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:10:40.003501  2360 solver.cpp:237]     Train net output #1: loss = 0.362115 (* 1 = 0.362115 loss)
I1211 18:10:40.003501  2360 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1211 18:10:46.152734  2360 solver.cpp:218] Iteration 98300 (16.262 iter/s, 6.14932s/100 iters), loss = 0.400641
I1211 18:10:46.153734  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:10:46.153734  2360 solver.cpp:237]     Train net output #1: loss = 0.400641 (* 1 = 0.400641 loss)
I1211 18:10:46.153734  2360 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1211 18:10:52.309429  2360 solver.cpp:218] Iteration 98400 (16.2449 iter/s, 6.15579s/100 iters), loss = 0.493665
I1211 18:10:52.309929  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:10:52.309929  2360 solver.cpp:237]     Train net output #1: loss = 0.493665 (* 1 = 0.493665 loss)
I1211 18:10:52.309929  2360 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1211 18:10:58.158426  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:10:58.399435  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98500.caffemodel
I1211 18:10:58.417443  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98500.solverstate
I1211 18:10:58.422446  2360 solver.cpp:330] Iteration 98500, Testing net (#0)
I1211 18:10:58.422446  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:10:59.766602 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:10:59.819605  2360 solver.cpp:397]     Test net output #0: accuracy = 0.677
I1211 18:10:59.819605  2360 solver.cpp:397]     Test net output #1: loss = 1.20404 (* 1 = 1.20404 loss)
I1211 18:10:59.877610  2360 solver.cpp:218] Iteration 98500 (13.2135 iter/s, 7.56805s/100 iters), loss = 0.37814
I1211 18:10:59.877610  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:10:59.877610  2360 solver.cpp:237]     Train net output #1: loss = 0.37814 (* 1 = 0.37814 loss)
I1211 18:10:59.877610  2360 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1211 18:11:06.045044  2360 solver.cpp:218] Iteration 98600 (16.2174 iter/s, 6.16622s/100 iters), loss = 0.388762
I1211 18:11:06.045044  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:11:06.045044  2360 solver.cpp:237]     Train net output #1: loss = 0.388762 (* 1 = 0.388762 loss)
I1211 18:11:06.045044  2360 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1211 18:11:12.202477  2360 solver.cpp:218] Iteration 98700 (16.2399 iter/s, 6.15766s/100 iters), loss = 0.415624
I1211 18:11:12.202477  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:11:12.202477  2360 solver.cpp:237]     Train net output #1: loss = 0.415624 (* 1 = 0.415624 loss)
I1211 18:11:12.202477  2360 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1211 18:11:18.352177  2360 solver.cpp:218] Iteration 98800 (16.2637 iter/s, 6.14866s/100 iters), loss = 0.41445
I1211 18:11:18.352177  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:11:18.352177  2360 solver.cpp:237]     Train net output #1: loss = 0.41445 (* 1 = 0.41445 loss)
I1211 18:11:18.352177  2360 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1211 18:11:24.505597  2360 solver.cpp:218] Iteration 98900 (16.2519 iter/s, 6.15313s/100 iters), loss = 0.46926
I1211 18:11:24.505597  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 18:11:24.505597  2360 solver.cpp:237]     Train net output #1: loss = 0.46926 (* 1 = 0.46926 loss)
I1211 18:11:24.505597  2360 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1211 18:11:30.355027  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:11:30.596038  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99000.caffemodel
I1211 18:11:30.613543  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99000.solverstate
I1211 18:11:30.619042  2360 solver.cpp:330] Iteration 99000, Testing net (#0)
I1211 18:11:30.619042  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:11:31.963136 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:11:32.016140  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6798
I1211 18:11:32.016140  2360 solver.cpp:397]     Test net output #1: loss = 1.20272 (* 1 = 1.20272 loss)
I1211 18:11:32.074141  2360 solver.cpp:218] Iteration 99000 (13.2129 iter/s, 7.56836s/100 iters), loss = 0.384618
I1211 18:11:32.074141  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:11:32.074141  2360 solver.cpp:237]     Train net output #1: loss = 0.384618 (* 1 = 0.384618 loss)
I1211 18:11:32.074141  2360 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1211 18:11:38.223776  2360 solver.cpp:218] Iteration 99100 (16.2629 iter/s, 6.14897s/100 iters), loss = 0.480338
I1211 18:11:38.223776  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 18:11:38.223776  2360 solver.cpp:237]     Train net output #1: loss = 0.480338 (* 1 = 0.480338 loss)
I1211 18:11:38.223776  2360 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1211 18:11:44.367228  2360 solver.cpp:218] Iteration 99200 (16.2793 iter/s, 6.14278s/100 iters), loss = 0.31882
I1211 18:11:44.367228  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:11:44.367228  2360 solver.cpp:237]     Train net output #1: loss = 0.31882 (* 1 = 0.31882 loss)
I1211 18:11:44.367228  2360 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1211 18:11:50.510701  2360 solver.cpp:218] Iteration 99300 (16.277 iter/s, 6.14363s/100 iters), loss = 0.414786
I1211 18:11:50.510701  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:11:50.510701  2360 solver.cpp:237]     Train net output #1: loss = 0.414786 (* 1 = 0.414786 loss)
I1211 18:11:50.510701  2360 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1211 18:11:56.670197  2360 solver.cpp:218] Iteration 99400 (16.2354 iter/s, 6.15938s/100 iters), loss = 0.417472
I1211 18:11:56.671197  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:11:56.671197  2360 solver.cpp:237]     Train net output #1: loss = 0.417472 (* 1 = 0.417472 loss)
I1211 18:11:56.671197  2360 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1211 18:12:02.529539  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:12:02.771813  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99500.caffemodel
I1211 18:12:02.788816  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99500.solverstate
I1211 18:12:02.793817  2360 solver.cpp:330] Iteration 99500, Testing net (#0)
I1211 18:12:02.793817  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:12:04.134351 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:12:04.188359  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6794
I1211 18:12:04.188359  2360 solver.cpp:397]     Test net output #1: loss = 1.20931 (* 1 = 1.20931 loss)
I1211 18:12:04.245860  2360 solver.cpp:218] Iteration 99500 (13.2015 iter/s, 7.57491s/100 iters), loss = 0.346618
I1211 18:12:04.245860  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:12:04.245860  2360 solver.cpp:237]     Train net output #1: loss = 0.346618 (* 1 = 0.346618 loss)
I1211 18:12:04.245860  2360 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1211 18:12:10.401182  2360 solver.cpp:218] Iteration 99600 (16.2486 iter/s, 6.15437s/100 iters), loss = 0.498096
I1211 18:12:10.401182  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 18:12:10.401182  2360 solver.cpp:237]     Train net output #1: loss = 0.498096 (* 1 = 0.498096 loss)
I1211 18:12:10.401182  2360 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1211 18:12:16.549367  2360 solver.cpp:218] Iteration 99700 (16.266 iter/s, 6.14781s/100 iters), loss = 0.443342
I1211 18:12:16.549367  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:12:16.549367  2360 solver.cpp:237]     Train net output #1: loss = 0.443342 (* 1 = 0.443342 loss)
I1211 18:12:16.549367  2360 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1211 18:12:22.707871  2360 solver.cpp:218] Iteration 99800 (16.2378 iter/s, 6.15846s/100 iters), loss = 0.448546
I1211 18:12:22.707871  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:12:22.707871  2360 solver.cpp:237]     Train net output #1: loss = 0.448546 (* 1 = 0.448546 loss)
I1211 18:12:22.707871  2360 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1211 18:12:28.855082  2360 solver.cpp:218] Iteration 99900 (16.2694 iter/s, 6.14652s/100 iters), loss = 0.437337
I1211 18:12:28.855082  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:12:28.855082  2360 solver.cpp:237]     Train net output #1: loss = 0.437337 (* 1 = 0.437337 loss)
I1211 18:12:28.855082  2360 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1211 18:12:34.705926  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:12:34.947085  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100000.caffemodel
I1211 18:12:34.962087  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100000.solverstate
I1211 18:12:34.967087  2360 solver.cpp:330] Iteration 100000, Testing net (#0)
I1211 18:12:34.967087  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:12:36.309623 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:12:36.362349  2360 solver.cpp:397]     Test net output #0: accuracy = 0.677
I1211 18:12:36.362349  2360 solver.cpp:397]     Test net output #1: loss = 1.20523 (* 1 = 1.20523 loss)
I1211 18:12:36.421346  2360 solver.cpp:218] Iteration 100000 (13.218 iter/s, 7.56542s/100 iters), loss = 0.332791
I1211 18:12:36.421346  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:12:36.421346  2360 solver.cpp:237]     Train net output #1: loss = 0.332791 (* 1 = 0.332791 loss)
I1211 18:12:36.421346  2360 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1211 18:12:42.570264  2360 solver.cpp:218] Iteration 100100 (16.2647 iter/s, 6.14828s/100 iters), loss = 0.41853
I1211 18:12:42.570264  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:12:42.570264  2360 solver.cpp:237]     Train net output #1: loss = 0.41853 (* 1 = 0.41853 loss)
I1211 18:12:42.570264  2360 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1211 18:12:48.722430  2360 solver.cpp:218] Iteration 100200 (16.2552 iter/s, 6.15187s/100 iters), loss = 0.377983
I1211 18:12:48.722430  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:12:48.722430  2360 solver.cpp:237]     Train net output #1: loss = 0.377983 (* 1 = 0.377983 loss)
I1211 18:12:48.722430  2360 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1211 18:12:54.875581  2360 solver.cpp:218] Iteration 100300 (16.2508 iter/s, 6.15353s/100 iters), loss = 0.381717
I1211 18:12:54.876582  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:12:54.876582  2360 solver.cpp:237]     Train net output #1: loss = 0.381717 (* 1 = 0.381717 loss)
I1211 18:12:54.876582  2360 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1211 18:13:01.031059  2360 solver.cpp:218] Iteration 100400 (16.2481 iter/s, 6.15456s/100 iters), loss = 0.446158
I1211 18:13:01.031059  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:13:01.031059  2360 solver.cpp:237]     Train net output #1: loss = 0.446158 (* 1 = 0.446158 loss)
I1211 18:13:01.031059  2360 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1211 18:13:06.878522  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:13:07.120537  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100500.caffemodel
I1211 18:13:07.138536  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100500.solverstate
I1211 18:13:07.144045  2360 solver.cpp:330] Iteration 100500, Testing net (#0)
I1211 18:13:07.144045  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:13:08.485658 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:13:08.538657  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6821
I1211 18:13:08.538657  2360 solver.cpp:397]     Test net output #1: loss = 1.19945 (* 1 = 1.19945 loss)
I1211 18:13:08.597668  2360 solver.cpp:218] Iteration 100500 (13.2172 iter/s, 7.56588s/100 iters), loss = 0.397571
I1211 18:13:08.597668  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:13:08.597668  2360 solver.cpp:237]     Train net output #1: loss = 0.397571 (* 1 = 0.397571 loss)
I1211 18:13:08.597668  2360 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1211 18:13:14.749611  2360 solver.cpp:218] Iteration 100600 (16.2563 iter/s, 6.15147s/100 iters), loss = 0.433606
I1211 18:13:14.749611  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:13:14.749611  2360 solver.cpp:237]     Train net output #1: loss = 0.433606 (* 1 = 0.433606 loss)
I1211 18:13:14.749611  2360 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1211 18:13:20.912540  2360 solver.cpp:218] Iteration 100700 (16.2257 iter/s, 6.16306s/100 iters), loss = 0.45317
I1211 18:13:20.912540  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:13:20.912540  2360 solver.cpp:237]     Train net output #1: loss = 0.45317 (* 1 = 0.45317 loss)
I1211 18:13:20.912540  2360 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1211 18:13:27.061460  2360 solver.cpp:218] Iteration 100800 (16.2647 iter/s, 6.14829s/100 iters), loss = 0.448452
I1211 18:13:27.061460  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:13:27.061460  2360 solver.cpp:237]     Train net output #1: loss = 0.448452 (* 1 = 0.448452 loss)
I1211 18:13:27.061460  2360 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1211 18:13:33.217761  2360 solver.cpp:218] Iteration 100900 (16.2448 iter/s, 6.15582s/100 iters), loss = 0.381894
I1211 18:13:33.217761  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:13:33.217761  2360 solver.cpp:237]     Train net output #1: loss = 0.381894 (* 1 = 0.381894 loss)
I1211 18:13:33.217761  2360 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1211 18:13:39.066210  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:13:39.308225  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101000.caffemodel
I1211 18:13:39.325225  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101000.solverstate
I1211 18:13:39.330225  2360 solver.cpp:330] Iteration 101000, Testing net (#0)
I1211 18:13:39.330225  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:13:40.674340 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:13:40.727340  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6827
I1211 18:13:40.727340  2360 solver.cpp:397]     Test net output #1: loss = 1.20725 (* 1 = 1.20725 loss)
I1211 18:13:40.785348  2360 solver.cpp:218] Iteration 101000 (13.2145 iter/s, 7.56744s/100 iters), loss = 0.348798
I1211 18:13:40.785348  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:13:40.785348  2360 solver.cpp:237]     Train net output #1: loss = 0.348798 (* 1 = 0.348798 loss)
I1211 18:13:40.785348  2360 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1211 18:13:46.945858  2360 solver.cpp:218] Iteration 101100 (16.234 iter/s, 6.1599s/100 iters), loss = 0.417845
I1211 18:13:46.945858  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:13:46.945858  2360 solver.cpp:237]     Train net output #1: loss = 0.417845 (* 1 = 0.417845 loss)
I1211 18:13:46.945858  2360 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1211 18:13:53.126358  2360 solver.cpp:218] Iteration 101200 (16.1816 iter/s, 6.17987s/100 iters), loss = 0.353938
I1211 18:13:53.126358  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:13:53.126358  2360 solver.cpp:237]     Train net output #1: loss = 0.353938 (* 1 = 0.353938 loss)
I1211 18:13:53.126358  2360 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1211 18:13:59.359048  2360 solver.cpp:218] Iteration 101300 (16.0469 iter/s, 6.23175s/100 iters), loss = 0.396005
I1211 18:13:59.359048  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:13:59.359048  2360 solver.cpp:237]     Train net output #1: loss = 0.396005 (* 1 = 0.396005 loss)
I1211 18:13:59.359048  2360 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1211 18:14:05.559578  2360 solver.cpp:218] Iteration 101400 (16.1297 iter/s, 6.19974s/100 iters), loss = 0.433444
I1211 18:14:05.559578  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 18:14:05.559578  2360 solver.cpp:237]     Train net output #1: loss = 0.433444 (* 1 = 0.433444 loss)
I1211 18:14:05.559578  2360 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1211 18:14:11.433101  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:14:11.675118  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101500.caffemodel
I1211 18:14:11.692119  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101500.solverstate
I1211 18:14:11.697132  2360 solver.cpp:330] Iteration 101500, Testing net (#0)
I1211 18:14:11.697132  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:14:13.041225 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:14:13.094234  2360 solver.cpp:397]     Test net output #0: accuracy = 0.68
I1211 18:14:13.094234  2360 solver.cpp:397]     Test net output #1: loss = 1.20983 (* 1 = 1.20983 loss)
I1211 18:14:13.153237  2360 solver.cpp:218] Iteration 101500 (13.169 iter/s, 7.59357s/100 iters), loss = 0.321794
I1211 18:14:13.153237  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:14:13.153736  2360 solver.cpp:237]     Train net output #1: loss = 0.321794 (* 1 = 0.321794 loss)
I1211 18:14:13.153736  2360 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1211 18:14:19.365896  2360 solver.cpp:218] Iteration 101600 (16.0976 iter/s, 6.21212s/100 iters), loss = 0.411593
I1211 18:14:19.365896  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:14:19.365896  2360 solver.cpp:237]     Train net output #1: loss = 0.411593 (* 1 = 0.411593 loss)
I1211 18:14:19.365896  2360 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1211 18:14:25.573633  2360 solver.cpp:218] Iteration 101700 (16.1094 iter/s, 6.20757s/100 iters), loss = 0.285796
I1211 18:14:25.573633  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:14:25.573633  2360 solver.cpp:237]     Train net output #1: loss = 0.285796 (* 1 = 0.285796 loss)
I1211 18:14:25.573633  2360 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1211 18:14:31.890890  2360 solver.cpp:218] Iteration 101800 (15.8306 iter/s, 6.31687s/100 iters), loss = 0.368978
I1211 18:14:31.890890  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:14:31.890890  2360 solver.cpp:237]     Train net output #1: loss = 0.368978 (* 1 = 0.368978 loss)
I1211 18:14:31.890890  2360 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1211 18:14:38.155985  2360 solver.cpp:218] Iteration 101900 (15.9636 iter/s, 6.26426s/100 iters), loss = 0.400635
I1211 18:14:38.155985  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 18:14:38.155985  2360 solver.cpp:237]     Train net output #1: loss = 0.400635 (* 1 = 0.400635 loss)
I1211 18:14:38.155985  2360 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1211 18:14:44.035993  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:14:44.281234  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102000.caffemodel
I1211 18:14:44.297235  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102000.solverstate
I1211 18:14:44.302235  2360 solver.cpp:330] Iteration 102000, Testing net (#0)
I1211 18:14:44.302235  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:14:45.664335 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:14:45.718336  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6808
I1211 18:14:45.718336  2360 solver.cpp:397]     Test net output #1: loss = 1.21304 (* 1 = 1.21304 loss)
I1211 18:14:45.779454  2360 solver.cpp:218] Iteration 102000 (13.1189 iter/s, 7.62256s/100 iters), loss = 0.347078
I1211 18:14:45.779454  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:14:45.779454  2360 solver.cpp:237]     Train net output #1: loss = 0.347078 (* 1 = 0.347078 loss)
I1211 18:14:45.779454  2360 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1211 18:14:51.986014  2360 solver.cpp:218] Iteration 102100 (16.1118 iter/s, 6.20663s/100 iters), loss = 0.372771
I1211 18:14:51.986014  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:14:51.987016  2360 solver.cpp:237]     Train net output #1: loss = 0.372771 (* 1 = 0.372771 loss)
I1211 18:14:51.987016  2360 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1211 18:14:58.205668  2360 solver.cpp:218] Iteration 102200 (16.0796 iter/s, 6.21907s/100 iters), loss = 0.318043
I1211 18:14:58.205668  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:14:58.205668  2360 solver.cpp:237]     Train net output #1: loss = 0.318043 (* 1 = 0.318043 loss)
I1211 18:14:58.205668  2360 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1211 18:15:04.498193  2360 solver.cpp:218] Iteration 102300 (15.8936 iter/s, 6.29185s/100 iters), loss = 0.323982
I1211 18:15:04.498193  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:15:04.498193  2360 solver.cpp:237]     Train net output #1: loss = 0.323982 (* 1 = 0.323982 loss)
I1211 18:15:04.498193  2360 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1211 18:15:10.740217  2360 solver.cpp:218] Iteration 102400 (16.023 iter/s, 6.24103s/100 iters), loss = 0.43606
I1211 18:15:10.740217  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:15:10.740217  2360 solver.cpp:237]     Train net output #1: loss = 0.43606 (* 1 = 0.43606 loss)
I1211 18:15:10.740217  2360 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1211 18:15:16.730386  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:15:16.982465  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102500.caffemodel
I1211 18:15:17.001466  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102500.solverstate
I1211 18:15:17.006466  2360 solver.cpp:330] Iteration 102500, Testing net (#0)
I1211 18:15:17.006466  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:15:18.390686 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:15:18.445207  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6788
I1211 18:15:18.445708  2360 solver.cpp:397]     Test net output #1: loss = 1.22045 (* 1 = 1.22045 loss)
I1211 18:15:18.505718  2360 solver.cpp:218] Iteration 102500 (12.8775 iter/s, 7.76547s/100 iters), loss = 0.314105
I1211 18:15:18.505718  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:15:18.505718  2360 solver.cpp:237]     Train net output #1: loss = 0.314105 (* 1 = 0.314105 loss)
I1211 18:15:18.505718  2360 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1211 18:15:24.814528  2360 solver.cpp:218] Iteration 102600 (15.8536 iter/s, 6.30772s/100 iters), loss = 0.410746
I1211 18:15:24.814528  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:15:24.814528  2360 solver.cpp:237]     Train net output #1: loss = 0.410746 (* 1 = 0.410746 loss)
I1211 18:15:24.814528  2360 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1211 18:15:31.044253  2360 solver.cpp:218] Iteration 102700 (16.0537 iter/s, 6.22909s/100 iters), loss = 0.349965
I1211 18:15:31.044253  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:15:31.044253  2360 solver.cpp:237]     Train net output #1: loss = 0.349965 (* 1 = 0.349965 loss)
I1211 18:15:31.044253  2360 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1211 18:15:37.228835  2360 solver.cpp:218] Iteration 102800 (16.1704 iter/s, 6.18412s/100 iters), loss = 0.399303
I1211 18:15:37.228835  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:15:37.228835  2360 solver.cpp:237]     Train net output #1: loss = 0.399303 (* 1 = 0.399303 loss)
I1211 18:15:37.228835  2360 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1211 18:15:43.426913  2360 solver.cpp:218] Iteration 102900 (16.1351 iter/s, 6.19767s/100 iters), loss = 0.442105
I1211 18:15:43.427412  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 18:15:43.427412  2360 solver.cpp:237]     Train net output #1: loss = 0.442105 (* 1 = 0.442105 loss)
I1211 18:15:43.427412  2360 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1211 18:15:49.436725  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:15:49.687738  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103000.caffemodel
I1211 18:15:49.703737  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103000.solverstate
I1211 18:15:49.709738  2360 solver.cpp:330] Iteration 103000, Testing net (#0)
I1211 18:15:49.709738  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:15:51.083209 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:15:51.136260  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6793
I1211 18:15:51.136260  2360 solver.cpp:397]     Test net output #1: loss = 1.21575 (* 1 = 1.21575 loss)
I1211 18:15:51.198261  2360 solver.cpp:218] Iteration 103000 (12.8686 iter/s, 7.77086s/100 iters), loss = 0.30864
I1211 18:15:51.198261  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:15:51.198261  2360 solver.cpp:237]     Train net output #1: loss = 0.30864 (* 1 = 0.30864 loss)
I1211 18:15:51.198261  2360 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1211 18:15:57.509244  2360 solver.cpp:218] Iteration 103100 (15.8477 iter/s, 6.31005s/100 iters), loss = 0.391849
I1211 18:15:57.509244  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:15:57.509244  2360 solver.cpp:237]     Train net output #1: loss = 0.391849 (* 1 = 0.391849 loss)
I1211 18:15:57.509244  2360 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1211 18:16:03.713786  2360 solver.cpp:218] Iteration 103200 (16.1186 iter/s, 6.20401s/100 iters), loss = 0.318003
I1211 18:16:03.713786  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:16:03.713786  2360 solver.cpp:237]     Train net output #1: loss = 0.318003 (* 1 = 0.318003 loss)
I1211 18:16:03.713786  2360 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1211 18:16:09.885303  2360 solver.cpp:218] Iteration 103300 (16.204 iter/s, 6.17133s/100 iters), loss = 0.415509
I1211 18:16:09.885303  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:16:09.885303  2360 solver.cpp:237]     Train net output #1: loss = 0.415509 (* 1 = 0.415509 loss)
I1211 18:16:09.885303  2360 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1211 18:16:16.084110  2360 solver.cpp:218] Iteration 103400 (16.1336 iter/s, 6.19824s/100 iters), loss = 0.381868
I1211 18:16:16.084110  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:16:16.084110  2360 solver.cpp:237]     Train net output #1: loss = 0.381868 (* 1 = 0.381868 loss)
I1211 18:16:16.084110  2360 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1211 18:16:22.028422  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:16:22.272935  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103500.caffemodel
I1211 18:16:22.290935  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103500.solverstate
I1211 18:16:22.295936  2360 solver.cpp:330] Iteration 103500, Testing net (#0)
I1211 18:16:22.295936  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:16:23.661201 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:16:23.715199  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6779
I1211 18:16:23.715199  2360 solver.cpp:397]     Test net output #1: loss = 1.23055 (* 1 = 1.23055 loss)
I1211 18:16:23.777217  2360 solver.cpp:218] Iteration 103500 (12.9998 iter/s, 7.69241s/100 iters), loss = 0.303136
I1211 18:16:23.777217  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 18:16:23.777217  2360 solver.cpp:237]     Train net output #1: loss = 0.303136 (* 1 = 0.303136 loss)
I1211 18:16:23.777217  2360 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1211 18:16:29.979872  2360 solver.cpp:218] Iteration 103600 (16.1217 iter/s, 6.20282s/100 iters), loss = 0.403387
I1211 18:16:29.979872  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:16:29.979872  2360 solver.cpp:237]     Train net output #1: loss = 0.403387 (* 1 = 0.403387 loss)
I1211 18:16:29.979872  2360 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1211 18:16:36.183492  2360 solver.cpp:218] Iteration 103700 (16.1218 iter/s, 6.20277s/100 iters), loss = 0.358155
I1211 18:16:36.183492  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:16:36.183492  2360 solver.cpp:237]     Train net output #1: loss = 0.358155 (* 1 = 0.358155 loss)
I1211 18:16:36.183492  2360 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1211 18:16:42.373991  2360 solver.cpp:218] Iteration 103800 (16.1559 iter/s, 6.18969s/100 iters), loss = 0.350722
I1211 18:16:42.373991  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:16:42.373991  2360 solver.cpp:237]     Train net output #1: loss = 0.350722 (* 1 = 0.350722 loss)
I1211 18:16:42.373991  2360 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1211 18:16:48.605082  2360 solver.cpp:218] Iteration 103900 (16.0481 iter/s, 6.23126s/100 iters), loss = 0.397926
I1211 18:16:48.605082  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:16:48.605082  2360 solver.cpp:237]     Train net output #1: loss = 0.397926 (* 1 = 0.397926 loss)
I1211 18:16:48.605082  2360 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1211 18:16:54.483520  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:16:54.725553  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104000.caffemodel
I1211 18:16:54.744063  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104000.solverstate
I1211 18:16:54.748564  2360 solver.cpp:330] Iteration 104000, Testing net (#0)
I1211 18:16:54.749065  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:16:56.093317 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:16:56.146829  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6796
I1211 18:16:56.146829  2360 solver.cpp:397]     Test net output #1: loss = 1.22832 (* 1 = 1.22832 loss)
I1211 18:16:56.206326  2360 solver.cpp:218] Iteration 104000 (13.1578 iter/s, 7.60005s/100 iters), loss = 0.289525
I1211 18:16:56.206326  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 18:16:56.206326  2360 solver.cpp:237]     Train net output #1: loss = 0.289525 (* 1 = 0.289525 loss)
I1211 18:16:56.206326  2360 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1211 18:17:02.358340  2360 solver.cpp:218] Iteration 104100 (16.2541 iter/s, 6.15229s/100 iters), loss = 0.378646
I1211 18:17:02.359341  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:17:02.359341  2360 solver.cpp:237]     Train net output #1: loss = 0.378646 (* 1 = 0.378646 loss)
I1211 18:17:02.359341  2360 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1211 18:17:08.508852  2360 solver.cpp:218] Iteration 104200 (16.2604 iter/s, 6.1499s/100 iters), loss = 0.365306
I1211 18:17:08.508852  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:17:08.508852  2360 solver.cpp:237]     Train net output #1: loss = 0.365306 (* 1 = 0.365306 loss)
I1211 18:17:08.508852  2360 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1211 18:17:14.663333  2360 solver.cpp:218] Iteration 104300 (16.2511 iter/s, 6.15345s/100 iters), loss = 0.293907
I1211 18:17:14.663333  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:17:14.663333  2360 solver.cpp:237]     Train net output #1: loss = 0.293907 (* 1 = 0.293907 loss)
I1211 18:17:14.663333  2360 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1211 18:17:20.810730  2360 solver.cpp:218] Iteration 104400 (16.2667 iter/s, 6.14752s/100 iters), loss = 0.368459
I1211 18:17:20.810730  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:17:20.810730  2360 solver.cpp:237]     Train net output #1: loss = 0.368459 (* 1 = 0.368459 loss)
I1211 18:17:20.810730  2360 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1211 18:17:26.657342  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:17:26.899354  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104500.caffemodel
I1211 18:17:26.916358  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104500.solverstate
I1211 18:17:26.921358  2360 solver.cpp:330] Iteration 104500, Testing net (#0)
I1211 18:17:26.921358  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:17:28.265475 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:17:28.317482  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 18:17:28.317482  2360 solver.cpp:397]     Test net output #1: loss = 1.22852 (* 1 = 1.22852 loss)
I1211 18:17:28.377481  2360 solver.cpp:218] Iteration 104500 (13.2179 iter/s, 7.56549s/100 iters), loss = 0.281274
I1211 18:17:28.377481  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 18:17:28.377481  2360 solver.cpp:237]     Train net output #1: loss = 0.281274 (* 1 = 0.281274 loss)
I1211 18:17:28.377481  2360 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1211 18:17:34.525949  2360 solver.cpp:218] Iteration 104600 (16.2649 iter/s, 6.14821s/100 iters), loss = 0.342444
I1211 18:17:34.525949  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:17:34.525949  2360 solver.cpp:237]     Train net output #1: loss = 0.342444 (* 1 = 0.342444 loss)
I1211 18:17:34.525949  2360 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1211 18:17:40.673511  2360 solver.cpp:218] Iteration 104700 (16.2659 iter/s, 6.14781s/100 iters), loss = 0.319611
I1211 18:17:40.673511  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:17:40.674515  2360 solver.cpp:237]     Train net output #1: loss = 0.319611 (* 1 = 0.319611 loss)
I1211 18:17:40.674515  2360 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1211 18:17:46.819563  2360 solver.cpp:218] Iteration 104800 (16.2736 iter/s, 6.14493s/100 iters), loss = 0.388002
I1211 18:17:46.819563  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:17:46.819563  2360 solver.cpp:237]     Train net output #1: loss = 0.388002 (* 1 = 0.388002 loss)
I1211 18:17:46.819563  2360 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1211 18:17:52.969097  2360 solver.cpp:218] Iteration 104900 (16.2616 iter/s, 6.14947s/100 iters), loss = 0.367748
I1211 18:17:52.969097  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:17:52.969097  2360 solver.cpp:237]     Train net output #1: loss = 0.367748 (* 1 = 0.367748 loss)
I1211 18:17:52.969097  2360 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1211 18:17:58.813055  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:17:59.055492  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105000.caffemodel
I1211 18:17:59.072490  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105000.solverstate
I1211 18:17:59.077492  2360 solver.cpp:330] Iteration 105000, Testing net (#0)
I1211 18:17:59.077492  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:18:00.419255 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:18:00.472252  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6796
I1211 18:18:00.472252  2360 solver.cpp:397]     Test net output #1: loss = 1.22916 (* 1 = 1.22916 loss)
I1211 18:18:00.532263  2360 solver.cpp:218] Iteration 105000 (13.224 iter/s, 7.56199s/100 iters), loss = 0.289983
I1211 18:18:00.532263  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:18:00.532263  2360 solver.cpp:237]     Train net output #1: loss = 0.289983 (* 1 = 0.289983 loss)
I1211 18:18:00.532263  2360 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1211 18:18:06.685011  2360 solver.cpp:218] Iteration 105100 (16.2534 iter/s, 6.15256s/100 iters), loss = 0.350763
I1211 18:18:06.685011  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:18:06.685011  2360 solver.cpp:237]     Train net output #1: loss = 0.350763 (* 1 = 0.350763 loss)
I1211 18:18:06.685011  2360 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1211 18:18:12.836498  2360 solver.cpp:218] Iteration 105200 (16.2586 iter/s, 6.1506s/100 iters), loss = 0.3338
I1211 18:18:12.836498  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:18:12.836498  2360 solver.cpp:237]     Train net output #1: loss = 0.3338 (* 1 = 0.3338 loss)
I1211 18:18:12.836498  2360 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1211 18:18:18.984800  2360 solver.cpp:218] Iteration 105300 (16.2645 iter/s, 6.14837s/100 iters), loss = 0.320152
I1211 18:18:18.984800  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:18:18.984800  2360 solver.cpp:237]     Train net output #1: loss = 0.320152 (* 1 = 0.320152 loss)
I1211 18:18:18.984800  2360 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1211 18:18:25.132791  2360 solver.cpp:218] Iteration 105400 (16.2681 iter/s, 6.147s/100 iters), loss = 0.423535
I1211 18:18:25.132791  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:18:25.132791  2360 solver.cpp:237]     Train net output #1: loss = 0.423535 (* 1 = 0.423535 loss)
I1211 18:18:25.132791  2360 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1211 18:18:30.982959  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:18:31.226979  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105500.caffemodel
I1211 18:18:31.244978  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105500.solverstate
I1211 18:18:31.248978  2360 solver.cpp:330] Iteration 105500, Testing net (#0)
I1211 18:18:31.249979  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:18:32.590088 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:18:32.643095  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6782
I1211 18:18:32.643095  2360 solver.cpp:397]     Test net output #1: loss = 1.22566 (* 1 = 1.22566 loss)
I1211 18:18:32.702095  2360 solver.cpp:218] Iteration 105500 (13.2122 iter/s, 7.56876s/100 iters), loss = 0.249905
I1211 18:18:32.702095  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 18:18:32.702095  2360 solver.cpp:237]     Train net output #1: loss = 0.249905 (* 1 = 0.249905 loss)
I1211 18:18:32.702095  2360 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1211 18:18:38.864593  2360 solver.cpp:218] Iteration 105600 (16.2258 iter/s, 6.16301s/100 iters), loss = 0.346217
I1211 18:18:38.865587  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:18:38.865587  2360 solver.cpp:237]     Train net output #1: loss = 0.346217 (* 1 = 0.346217 loss)
I1211 18:18:38.865587  2360 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1211 18:18:45.021019  2360 solver.cpp:218] Iteration 105700 (16.2465 iter/s, 6.15518s/100 iters), loss = 0.325427
I1211 18:18:45.021019  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:18:45.021019  2360 solver.cpp:237]     Train net output #1: loss = 0.325427 (* 1 = 0.325427 loss)
I1211 18:18:45.021019  2360 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1211 18:18:51.172472  2360 solver.cpp:218] Iteration 105800 (16.2576 iter/s, 6.15099s/100 iters), loss = 0.375983
I1211 18:18:51.172472  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:18:51.172472  2360 solver.cpp:237]     Train net output #1: loss = 0.375983 (* 1 = 0.375983 loss)
I1211 18:18:51.172472  2360 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1211 18:18:57.330909  2360 solver.cpp:218] Iteration 105900 (16.2372 iter/s, 6.15871s/100 iters), loss = 0.373481
I1211 18:18:57.330909  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 18:18:57.331910  2360 solver.cpp:237]     Train net output #1: loss = 0.373481 (* 1 = 0.373481 loss)
I1211 18:18:57.331910  2360 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1211 18:19:03.186384  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:19:03.428406  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106000.caffemodel
I1211 18:19:03.443406  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106000.solverstate
I1211 18:19:03.448405  2360 solver.cpp:330] Iteration 106000, Testing net (#0)
I1211 18:19:03.448405  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:19:04.790501 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:19:04.843490  2360 solver.cpp:397]     Test net output #0: accuracy = 0.681
I1211 18:19:04.843490  2360 solver.cpp:397]     Test net output #1: loss = 1.23381 (* 1 = 1.23381 loss)
I1211 18:19:04.902515  2360 solver.cpp:218] Iteration 106000 (13.2095 iter/s, 7.57029s/100 iters), loss = 0.277852
I1211 18:19:04.902515  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 18:19:04.902515  2360 solver.cpp:237]     Train net output #1: loss = 0.277852 (* 1 = 0.277852 loss)
I1211 18:19:04.902515  2360 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1211 18:19:11.058917  2360 solver.cpp:218] Iteration 106100 (16.2431 iter/s, 6.15647s/100 iters), loss = 0.387853
I1211 18:19:11.058917  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:19:11.058917  2360 solver.cpp:237]     Train net output #1: loss = 0.387853 (* 1 = 0.387853 loss)
I1211 18:19:11.058917  2360 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1211 18:19:17.215381  2360 solver.cpp:218] Iteration 106200 (16.2453 iter/s, 6.15561s/100 iters), loss = 0.331719
I1211 18:19:17.215381  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:19:17.215381  2360 solver.cpp:237]     Train net output #1: loss = 0.331719 (* 1 = 0.331719 loss)
I1211 18:19:17.215381  2360 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1211 18:19:23.357885  2360 solver.cpp:218] Iteration 106300 (16.2803 iter/s, 6.14238s/100 iters), loss = 0.330097
I1211 18:19:23.357885  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:19:23.357885  2360 solver.cpp:237]     Train net output #1: loss = 0.330097 (* 1 = 0.330097 loss)
I1211 18:19:23.357885  2360 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1211 18:19:29.518333  2360 solver.cpp:218] Iteration 106400 (16.2342 iter/s, 6.15983s/100 iters), loss = 0.420382
I1211 18:19:29.518333  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:19:29.518333  2360 solver.cpp:237]     Train net output #1: loss = 0.420382 (* 1 = 0.420382 loss)
I1211 18:19:29.518333  2360 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1211 18:19:35.375675  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:19:35.618687  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106500.caffemodel
I1211 18:19:35.635692  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106500.solverstate
I1211 18:19:35.640692  2360 solver.cpp:330] Iteration 106500, Testing net (#0)
I1211 18:19:35.640692  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:19:36.984793 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:19:37.037798  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6784
I1211 18:19:37.037798  2360 solver.cpp:397]     Test net output #1: loss = 1.23216 (* 1 = 1.23216 loss)
I1211 18:19:37.096797  2360 solver.cpp:218] Iteration 106500 (13.1965 iter/s, 7.57778s/100 iters), loss = 0.347639
I1211 18:19:37.096797  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:19:37.096797  2360 solver.cpp:237]     Train net output #1: loss = 0.347639 (* 1 = 0.347639 loss)
I1211 18:19:37.096797  2360 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1211 18:19:43.253265  2360 solver.cpp:218] Iteration 106600 (16.2432 iter/s, 6.15641s/100 iters), loss = 0.363078
I1211 18:19:43.253265  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:19:43.253265  2360 solver.cpp:237]     Train net output #1: loss = 0.363078 (* 1 = 0.363078 loss)
I1211 18:19:43.253265  2360 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1211 18:19:49.401803  2360 solver.cpp:218] Iteration 106700 (16.265 iter/s, 6.14818s/100 iters), loss = 0.318383
I1211 18:19:49.401803  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:19:49.401803  2360 solver.cpp:237]     Train net output #1: loss = 0.318383 (* 1 = 0.318383 loss)
I1211 18:19:49.401803  2360 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1211 18:19:55.556262  2360 solver.cpp:218] Iteration 106800 (16.2503 iter/s, 6.15375s/100 iters), loss = 0.384349
I1211 18:19:55.556262  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:19:55.556262  2360 solver.cpp:237]     Train net output #1: loss = 0.384349 (* 1 = 0.384349 loss)
I1211 18:19:55.556262  2360 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1211 18:20:01.736642  2360 solver.cpp:218] Iteration 106900 (16.1797 iter/s, 6.18058s/100 iters), loss = 0.412816
I1211 18:20:01.736642  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:20:01.736642  2360 solver.cpp:237]     Train net output #1: loss = 0.412816 (* 1 = 0.412816 loss)
I1211 18:20:01.736642  2360 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1211 18:20:07.640571  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:20:07.884585  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107000.caffemodel
I1211 18:20:07.902586  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107000.solverstate
I1211 18:20:07.907586  2360 solver.cpp:330] Iteration 107000, Testing net (#0)
I1211 18:20:07.907586  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:20:09.252686 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:20:09.305685  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6804
I1211 18:20:09.305685  2360 solver.cpp:397]     Test net output #1: loss = 1.23072 (* 1 = 1.23072 loss)
I1211 18:20:09.364692  2360 solver.cpp:218] Iteration 107000 (13.1104 iter/s, 7.62753s/100 iters), loss = 0.316496
I1211 18:20:09.364692  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:20:09.364692  2360 solver.cpp:237]     Train net output #1: loss = 0.316496 (* 1 = 0.316496 loss)
I1211 18:20:09.364692  2360 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1211 18:20:15.515650  2360 solver.cpp:218] Iteration 107100 (16.2609 iter/s, 6.14973s/100 iters), loss = 0.344371
I1211 18:20:15.515650  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:20:15.515650  2360 solver.cpp:237]     Train net output #1: loss = 0.344371 (* 1 = 0.344371 loss)
I1211 18:20:15.515650  2360 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1211 18:20:21.666659  2360 solver.cpp:218] Iteration 107200 (16.2572 iter/s, 6.15111s/100 iters), loss = 0.32614
I1211 18:20:21.666659  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:20:21.666659  2360 solver.cpp:237]     Train net output #1: loss = 0.32614 (* 1 = 0.32614 loss)
I1211 18:20:21.666659  2360 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1211 18:20:27.817227  2360 solver.cpp:218] Iteration 107300 (16.2604 iter/s, 6.14992s/100 iters), loss = 0.372172
I1211 18:20:27.817227  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:20:27.817227  2360 solver.cpp:237]     Train net output #1: loss = 0.372172 (* 1 = 0.372172 loss)
I1211 18:20:27.817227  2360 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1211 18:20:33.969908  2360 solver.cpp:218] Iteration 107400 (16.2542 iter/s, 6.15225s/100 iters), loss = 0.404974
I1211 18:20:33.969908  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:20:33.969908  2360 solver.cpp:237]     Train net output #1: loss = 0.404974 (* 1 = 0.404974 loss)
I1211 18:20:33.969908  2360 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1211 18:20:39.811851  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:20:40.054875  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107500.caffemodel
I1211 18:20:40.071874  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107500.solverstate
I1211 18:20:40.076875  2360 solver.cpp:330] Iteration 107500, Testing net (#0)
I1211 18:20:40.076875  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:20:41.420990 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:20:41.473994  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6751
I1211 18:20:41.473994  2360 solver.cpp:397]     Test net output #1: loss = 1.23431 (* 1 = 1.23431 loss)
I1211 18:20:41.533004  2360 solver.cpp:218] Iteration 107500 (13.2229 iter/s, 7.56262s/100 iters), loss = 0.27952
I1211 18:20:41.533004  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:20:41.533004  2360 solver.cpp:237]     Train net output #1: loss = 0.279519 (* 1 = 0.279519 loss)
I1211 18:20:41.533004  2360 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1211 18:20:47.690469  2360 solver.cpp:218] Iteration 107600 (16.2409 iter/s, 6.15728s/100 iters), loss = 0.326146
I1211 18:20:47.690469  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:20:47.690469  2360 solver.cpp:237]     Train net output #1: loss = 0.326146 (* 1 = 0.326146 loss)
I1211 18:20:47.690469  2360 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1211 18:20:53.848995  2360 solver.cpp:218] Iteration 107700 (16.2402 iter/s, 6.15755s/100 iters), loss = 0.298712
I1211 18:20:53.848995  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 18:20:53.848995  2360 solver.cpp:237]     Train net output #1: loss = 0.298712 (* 1 = 0.298712 loss)
I1211 18:20:53.848995  2360 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1211 18:21:00.001655  2360 solver.cpp:218] Iteration 107800 (16.253 iter/s, 6.15269s/100 iters), loss = 0.34542
I1211 18:21:00.001655  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:21:00.001655  2360 solver.cpp:237]     Train net output #1: loss = 0.34542 (* 1 = 0.34542 loss)
I1211 18:21:00.001655  2360 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1211 18:21:06.156122  2360 solver.cpp:218] Iteration 107900 (16.2487 iter/s, 6.15434s/100 iters), loss = 0.340306
I1211 18:21:06.156122  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:21:06.156122  2360 solver.cpp:237]     Train net output #1: loss = 0.340306 (* 1 = 0.340306 loss)
I1211 18:21:06.156122  2360 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1211 18:21:12.010519  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:21:12.250553  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108000.caffemodel
I1211 18:21:12.266553  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108000.solverstate
I1211 18:21:12.271553  2360 solver.cpp:330] Iteration 108000, Testing net (#0)
I1211 18:21:12.271553  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:21:13.612663 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:21:13.665673  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6784
I1211 18:21:13.665673  2360 solver.cpp:397]     Test net output #1: loss = 1.24341 (* 1 = 1.24341 loss)
I1211 18:21:13.724189  2360 solver.cpp:218] Iteration 108000 (13.2152 iter/s, 7.56706s/100 iters), loss = 0.322862
I1211 18:21:13.724189  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:21:13.724189  2360 solver.cpp:237]     Train net output #1: loss = 0.322861 (* 1 = 0.322861 loss)
I1211 18:21:13.724189  2360 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1211 18:21:19.872196  2360 solver.cpp:218] Iteration 108100 (16.2673 iter/s, 6.14732s/100 iters), loss = 0.307965
I1211 18:21:19.872196  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:21:19.872196  2360 solver.cpp:237]     Train net output #1: loss = 0.307965 (* 1 = 0.307965 loss)
I1211 18:21:19.872196  2360 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1211 18:21:26.017693  2360 solver.cpp:218] Iteration 108200 (16.2731 iter/s, 6.14512s/100 iters), loss = 0.352137
I1211 18:21:26.017693  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:21:26.017693  2360 solver.cpp:237]     Train net output #1: loss = 0.352137 (* 1 = 0.352137 loss)
I1211 18:21:26.017693  2360 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1211 18:21:32.175182  2360 solver.cpp:218] Iteration 108300 (16.2398 iter/s, 6.15771s/100 iters), loss = 0.410711
I1211 18:21:32.175182  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:21:32.175182  2360 solver.cpp:237]     Train net output #1: loss = 0.410711 (* 1 = 0.410711 loss)
I1211 18:21:32.175182  2360 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1211 18:21:38.321658  2360 solver.cpp:218] Iteration 108400 (16.2723 iter/s, 6.14543s/100 iters), loss = 0.348413
I1211 18:21:38.321658  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:21:38.321658  2360 solver.cpp:237]     Train net output #1: loss = 0.348413 (* 1 = 0.348413 loss)
I1211 18:21:38.321658  2360 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1211 18:21:44.182109  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:21:44.424124  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108500.caffemodel
I1211 18:21:44.441128  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108500.solverstate
I1211 18:21:44.446127  2360 solver.cpp:330] Iteration 108500, Testing net (#0)
I1211 18:21:44.446127  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:21:45.789244 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:21:45.843251  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6778
I1211 18:21:45.843251  2360 solver.cpp:397]     Test net output #1: loss = 1.23644 (* 1 = 1.23644 loss)
I1211 18:21:45.901249  2360 solver.cpp:218] Iteration 108500 (13.193 iter/s, 7.57979s/100 iters), loss = 0.236805
I1211 18:21:45.902251  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 18:21:45.902251  2360 solver.cpp:237]     Train net output #1: loss = 0.236805 (* 1 = 0.236805 loss)
I1211 18:21:45.902251  2360 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1211 18:21:52.047703  2360 solver.cpp:218] Iteration 108600 (16.2732 iter/s, 6.14508s/100 iters), loss = 0.368191
I1211 18:21:52.047703  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:21:52.047703  2360 solver.cpp:237]     Train net output #1: loss = 0.368191 (* 1 = 0.368191 loss)
I1211 18:21:52.047703  2360 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1211 18:21:58.197016  2360 solver.cpp:218] Iteration 108700 (16.263 iter/s, 6.14893s/100 iters), loss = 0.351734
I1211 18:21:58.197016  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:21:58.197016  2360 solver.cpp:237]     Train net output #1: loss = 0.351734 (* 1 = 0.351734 loss)
I1211 18:21:58.197016  2360 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1211 18:22:04.364837  2360 solver.cpp:218] Iteration 108800 (16.2139 iter/s, 6.16756s/100 iters), loss = 0.359627
I1211 18:22:04.364837  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:22:04.364837  2360 solver.cpp:237]     Train net output #1: loss = 0.359627 (* 1 = 0.359627 loss)
I1211 18:22:04.364837  2360 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1211 18:22:10.511598  2360 solver.cpp:218] Iteration 108900 (16.2687 iter/s, 6.14676s/100 iters), loss = 0.37692
I1211 18:22:10.511598  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 18:22:10.511598  2360 solver.cpp:237]     Train net output #1: loss = 0.37692 (* 1 = 0.37692 loss)
I1211 18:22:10.511598  2360 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1211 18:22:16.365468  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:22:16.607306  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109000.caffemodel
I1211 18:22:16.624306  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109000.solverstate
I1211 18:22:16.629303  2360 solver.cpp:330] Iteration 109000, Testing net (#0)
I1211 18:22:16.629303  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:22:17.972185 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:22:18.024183  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6799
I1211 18:22:18.024183  2360 solver.cpp:397]     Test net output #1: loss = 1.2457 (* 1 = 1.2457 loss)
I1211 18:22:18.083186  2360 solver.cpp:218] Iteration 109000 (13.2085 iter/s, 7.57089s/100 iters), loss = 0.331527
I1211 18:22:18.083186  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:22:18.083186  2360 solver.cpp:237]     Train net output #1: loss = 0.331527 (* 1 = 0.331527 loss)
I1211 18:22:18.083186  2360 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1211 18:22:24.226673  2360 solver.cpp:218] Iteration 109100 (16.2789 iter/s, 6.14293s/100 iters), loss = 0.334381
I1211 18:22:24.226673  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:22:24.226673  2360 solver.cpp:237]     Train net output #1: loss = 0.334381 (* 1 = 0.334381 loss)
I1211 18:22:24.226673  2360 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1211 18:22:30.363212  2360 solver.cpp:218] Iteration 109200 (16.2971 iter/s, 6.13606s/100 iters), loss = 0.278099
I1211 18:22:30.363212  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:22:30.363212  2360 solver.cpp:237]     Train net output #1: loss = 0.278099 (* 1 = 0.278099 loss)
I1211 18:22:30.363212  2360 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1211 18:22:36.512647  2360 solver.cpp:218] Iteration 109300 (16.2618 iter/s, 6.14939s/100 iters), loss = 0.352889
I1211 18:22:36.512647  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:22:36.512647  2360 solver.cpp:237]     Train net output #1: loss = 0.352889 (* 1 = 0.352889 loss)
I1211 18:22:36.512647  2360 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1211 18:22:42.664237  2360 solver.cpp:218] Iteration 109400 (16.2591 iter/s, 6.15041s/100 iters), loss = 0.387314
I1211 18:22:42.664237  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:22:42.664237  2360 solver.cpp:237]     Train net output #1: loss = 0.387314 (* 1 = 0.387314 loss)
I1211 18:22:42.664237  2360 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1211 18:22:48.517686  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:22:48.760706  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109500.caffemodel
I1211 18:22:48.776706  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109500.solverstate
I1211 18:22:48.781704  2360 solver.cpp:330] Iteration 109500, Testing net (#0)
I1211 18:22:48.781704  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:22:50.123801 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:22:50.176810  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6791
I1211 18:22:50.176810  2360 solver.cpp:397]     Test net output #1: loss = 1.24994 (* 1 = 1.24994 loss)
I1211 18:22:50.235808  2360 solver.cpp:218] Iteration 109500 (13.2079 iter/s, 7.57121s/100 iters), loss = 0.324808
I1211 18:22:50.235808  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:22:50.235808  2360 solver.cpp:237]     Train net output #1: loss = 0.324808 (* 1 = 0.324808 loss)
I1211 18:22:50.235808  2360 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1211 18:22:56.386234  2360 solver.cpp:218] Iteration 109600 (16.2594 iter/s, 6.15027s/100 iters), loss = 0.350134
I1211 18:22:56.386234  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:22:56.386234  2360 solver.cpp:237]     Train net output #1: loss = 0.350134 (* 1 = 0.350134 loss)
I1211 18:22:56.386234  2360 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1211 18:23:02.539703  2360 solver.cpp:218] Iteration 109700 (16.2526 iter/s, 6.15286s/100 iters), loss = 0.266994
I1211 18:23:02.539703  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:23:02.539703  2360 solver.cpp:237]     Train net output #1: loss = 0.266994 (* 1 = 0.266994 loss)
I1211 18:23:02.539703  2360 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1211 18:23:08.688201  2360 solver.cpp:218] Iteration 109800 (16.2639 iter/s, 6.1486s/100 iters), loss = 0.321567
I1211 18:23:08.689201  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:23:08.689201  2360 solver.cpp:237]     Train net output #1: loss = 0.321567 (* 1 = 0.321567 loss)
I1211 18:23:08.689201  2360 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1211 18:23:14.849175  2360 solver.cpp:218] Iteration 109900 (16.2338 iter/s, 6.15998s/100 iters), loss = 0.379745
I1211 18:23:14.849175  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:23:14.849175  2360 solver.cpp:237]     Train net output #1: loss = 0.379745 (* 1 = 0.379745 loss)
I1211 18:23:14.849175  2360 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1211 18:23:20.694578  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:23:20.938704  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110000.caffemodel
I1211 18:23:20.955698  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110000.solverstate
I1211 18:23:20.960697  2360 solver.cpp:330] Iteration 110000, Testing net (#0)
I1211 18:23:20.960697  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:23:22.304512 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:23:22.356511  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6811
I1211 18:23:22.356511  2360 solver.cpp:397]     Test net output #1: loss = 1.24183 (* 1 = 1.24183 loss)
I1211 18:23:22.416023  2360 solver.cpp:218] Iteration 110000 (13.2158 iter/s, 7.56669s/100 iters), loss = 0.269482
I1211 18:23:22.416023  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 18:23:22.416023  2360 solver.cpp:237]     Train net output #1: loss = 0.269482 (* 1 = 0.269482 loss)
I1211 18:23:22.416023  2360 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1211 18:23:28.567443  2360 solver.cpp:218] Iteration 110100 (16.2583 iter/s, 6.1507s/100 iters), loss = 0.449645
I1211 18:23:28.567443  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:23:28.567443  2360 solver.cpp:237]     Train net output #1: loss = 0.449645 (* 1 = 0.449645 loss)
I1211 18:23:28.567443  2360 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1211 18:23:34.724972  2360 solver.cpp:218] Iteration 110200 (16.2406 iter/s, 6.1574s/100 iters), loss = 0.297282
I1211 18:23:34.725972  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:23:34.725972  2360 solver.cpp:237]     Train net output #1: loss = 0.297282 (* 1 = 0.297282 loss)
I1211 18:23:34.725972  2360 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1211 18:23:40.873443  2360 solver.cpp:218] Iteration 110300 (16.2674 iter/s, 6.14728s/100 iters), loss = 0.349268
I1211 18:23:40.873443  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:23:40.873443  2360 solver.cpp:237]     Train net output #1: loss = 0.349268 (* 1 = 0.349268 loss)
I1211 18:23:40.873443  2360 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1211 18:23:47.029196  2360 solver.cpp:218] Iteration 110400 (16.2467 iter/s, 6.15509s/100 iters), loss = 0.329726
I1211 18:23:47.029196  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:23:47.029196  2360 solver.cpp:237]     Train net output #1: loss = 0.329726 (* 1 = 0.329726 loss)
I1211 18:23:47.029196  2360 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1211 18:23:52.873095  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:23:53.117108  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110500.caffemodel
I1211 18:23:53.134114  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110500.solverstate
I1211 18:23:53.139111  2360 solver.cpp:330] Iteration 110500, Testing net (#0)
I1211 18:23:53.139111  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:23:54.481215 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:23:54.534215  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 18:23:54.534215  2360 solver.cpp:397]     Test net output #1: loss = 1.25309 (* 1 = 1.25309 loss)
I1211 18:23:54.593222  2360 solver.cpp:218] Iteration 110500 (13.2209 iter/s, 7.56378s/100 iters), loss = 0.2642
I1211 18:23:54.593222  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 18:23:54.593222  2360 solver.cpp:237]     Train net output #1: loss = 0.2642 (* 1 = 0.2642 loss)
I1211 18:23:54.593222  2360 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1211 18:24:00.748723  2360 solver.cpp:218] Iteration 110600 (16.2469 iter/s, 6.15501s/100 iters), loss = 0.299704
I1211 18:24:00.748723  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:24:00.748723  2360 solver.cpp:237]     Train net output #1: loss = 0.299704 (* 1 = 0.299704 loss)
I1211 18:24:00.748723  2360 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1211 18:24:06.894811  2360 solver.cpp:218] Iteration 110700 (16.2719 iter/s, 6.14557s/100 iters), loss = 0.344517
I1211 18:24:06.894811  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:24:06.894811  2360 solver.cpp:237]     Train net output #1: loss = 0.344517 (* 1 = 0.344517 loss)
I1211 18:24:06.894811  2360 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1211 18:24:13.042291  2360 solver.cpp:218] Iteration 110800 (16.2671 iter/s, 6.14737s/100 iters), loss = 0.362318
I1211 18:24:13.042791  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:24:13.042791  2360 solver.cpp:237]     Train net output #1: loss = 0.362318 (* 1 = 0.362318 loss)
I1211 18:24:13.042791  2360 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1211 18:24:19.197422  2360 solver.cpp:218] Iteration 110900 (16.249 iter/s, 6.15424s/100 iters), loss = 0.340537
I1211 18:24:19.197422  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:24:19.197422  2360 solver.cpp:237]     Train net output #1: loss = 0.340537 (* 1 = 0.340537 loss)
I1211 18:24:19.197422  2360 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1211 18:24:25.045516  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:24:25.288533  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111000.caffemodel
I1211 18:24:25.304534  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111000.solverstate
I1211 18:24:25.310534  2360 solver.cpp:330] Iteration 111000, Testing net (#0)
I1211 18:24:25.310534  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:24:26.653659 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:24:26.705662  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6823
I1211 18:24:26.705662  2360 solver.cpp:397]     Test net output #1: loss = 1.24455 (* 1 = 1.24455 loss)
I1211 18:24:26.763665  2360 solver.cpp:218] Iteration 111000 (13.2162 iter/s, 7.56645s/100 iters), loss = 0.325262
I1211 18:24:26.763665  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:24:26.763665  2360 solver.cpp:237]     Train net output #1: loss = 0.325262 (* 1 = 0.325262 loss)
I1211 18:24:26.763665  2360 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1211 18:24:32.912101  2360 solver.cpp:218] Iteration 111100 (16.2669 iter/s, 6.14747s/100 iters), loss = 0.346067
I1211 18:24:32.912101  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:24:32.912101  2360 solver.cpp:237]     Train net output #1: loss = 0.346067 (* 1 = 0.346067 loss)
I1211 18:24:32.912101  2360 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1211 18:24:39.065032  2360 solver.cpp:218] Iteration 111200 (16.2533 iter/s, 6.15261s/100 iters), loss = 0.281892
I1211 18:24:39.065032  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:24:39.065533  2360 solver.cpp:237]     Train net output #1: loss = 0.281892 (* 1 = 0.281892 loss)
I1211 18:24:39.065533  2360 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1211 18:24:45.208001  2360 solver.cpp:218] Iteration 111300 (16.2793 iter/s, 6.14276s/100 iters), loss = 0.372254
I1211 18:24:45.208001  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:24:45.208001  2360 solver.cpp:237]     Train net output #1: loss = 0.372254 (* 1 = 0.372254 loss)
I1211 18:24:45.208001  2360 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1211 18:24:51.354471  2360 solver.cpp:218] Iteration 111400 (16.272 iter/s, 6.14552s/100 iters), loss = 0.270192
I1211 18:24:51.354471  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1211 18:24:51.354471  2360 solver.cpp:237]     Train net output #1: loss = 0.270192 (* 1 = 0.270192 loss)
I1211 18:24:51.354471  2360 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1211 18:24:57.206902  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:24:57.448912  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111500.caffemodel
I1211 18:24:57.466918  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111500.solverstate
I1211 18:24:57.471918  2360 solver.cpp:330] Iteration 111500, Testing net (#0)
I1211 18:24:57.471918  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:24:58.816021 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:24:58.869526  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6747
I1211 18:24:58.869526  2360 solver.cpp:397]     Test net output #1: loss = 1.26481 (* 1 = 1.26481 loss)
I1211 18:24:58.928030  2360 solver.cpp:218] Iteration 111500 (13.2044 iter/s, 7.57322s/100 iters), loss = 0.225782
I1211 18:24:58.928030  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1211 18:24:58.928030  2360 solver.cpp:237]     Train net output #1: loss = 0.225782 (* 1 = 0.225782 loss)
I1211 18:24:58.928030  2360 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1211 18:25:05.075498  2360 solver.cpp:218] Iteration 111600 (16.2663 iter/s, 6.14767s/100 iters), loss = 0.319184
I1211 18:25:05.076499  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:25:05.076499  2360 solver.cpp:237]     Train net output #1: loss = 0.319184 (* 1 = 0.319184 loss)
I1211 18:25:05.076499  2360 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1211 18:25:11.219938  2360 solver.cpp:218] Iteration 111700 (16.2785 iter/s, 6.14308s/100 iters), loss = 0.329284
I1211 18:25:11.219938  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:25:11.219938  2360 solver.cpp:237]     Train net output #1: loss = 0.329283 (* 1 = 0.329283 loss)
I1211 18:25:11.219938  2360 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1211 18:25:17.377387  2360 solver.cpp:218] Iteration 111800 (16.2416 iter/s, 6.15703s/100 iters), loss = 0.318181
I1211 18:25:17.377387  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:25:17.377387  2360 solver.cpp:237]     Train net output #1: loss = 0.318181 (* 1 = 0.318181 loss)
I1211 18:25:17.377387  2360 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1211 18:25:23.520841  2360 solver.cpp:218] Iteration 111900 (16.2765 iter/s, 6.14384s/100 iters), loss = 0.375143
I1211 18:25:23.520841  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:25:23.520841  2360 solver.cpp:237]     Train net output #1: loss = 0.375143 (* 1 = 0.375143 loss)
I1211 18:25:23.520841  2360 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1211 18:25:29.363812  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:25:29.606331  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_112000.caffemodel
I1211 18:25:29.622331  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_112000.solverstate
I1211 18:25:29.627331  2360 solver.cpp:330] Iteration 112000, Testing net (#0)
I1211 18:25:29.627331  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:25:30.968958 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:25:31.021479  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6768
I1211 18:25:31.021479  2360 solver.cpp:397]     Test net output #1: loss = 1.25286 (* 1 = 1.25286 loss)
I1211 18:25:31.080479  2360 solver.cpp:218] Iteration 112000 (13.2299 iter/s, 7.55864s/100 iters), loss = 0.21251
I1211 18:25:31.080479  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 18:25:31.080479  2360 solver.cpp:237]     Train net output #1: loss = 0.21251 (* 1 = 0.21251 loss)
I1211 18:25:31.080479  2360 sgd_solver.cpp:105] Iteration 112000, lr = 0.001
I1211 18:25:37.234889  2360 solver.cpp:218] Iteration 112100 (16.2483 iter/s, 6.15449s/100 iters), loss = 0.306381
I1211 18:25:37.235890  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:25:37.235890  2360 solver.cpp:237]     Train net output #1: loss = 0.306381 (* 1 = 0.306381 loss)
I1211 18:25:37.235890  2360 sgd_solver.cpp:105] Iteration 112100, lr = 0.001
I1211 18:25:43.383379  2360 solver.cpp:218] Iteration 112200 (16.2661 iter/s, 6.14774s/100 iters), loss = 0.309328
I1211 18:25:43.383379  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:25:43.383379  2360 solver.cpp:237]     Train net output #1: loss = 0.309328 (* 1 = 0.309328 loss)
I1211 18:25:43.383379  2360 sgd_solver.cpp:105] Iteration 112200, lr = 0.001
I1211 18:25:49.535876  2360 solver.cpp:218] Iteration 112300 (16.2559 iter/s, 6.15159s/100 iters), loss = 0.254102
I1211 18:25:49.535876  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:25:49.535876  2360 solver.cpp:237]     Train net output #1: loss = 0.254102 (* 1 = 0.254102 loss)
I1211 18:25:49.535876  2360 sgd_solver.cpp:105] Iteration 112300, lr = 0.001
I1211 18:25:55.689554  2360 solver.cpp:218] Iteration 112400 (16.2503 iter/s, 6.15374s/100 iters), loss = 0.4007
I1211 18:25:55.689554  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:25:55.689554  2360 solver.cpp:237]     Train net output #1: loss = 0.400699 (* 1 = 0.400699 loss)
I1211 18:25:55.689554  2360 sgd_solver.cpp:105] Iteration 112400, lr = 0.001
I1211 18:26:01.532986  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:26:01.776502  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_112500.caffemodel
I1211 18:26:01.793005  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_112500.solverstate
I1211 18:26:01.798004  2360 solver.cpp:330] Iteration 112500, Testing net (#0)
I1211 18:26:01.799005  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:26:03.140147 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:26:03.193153  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6795
I1211 18:26:03.193153  2360 solver.cpp:397]     Test net output #1: loss = 1.26824 (* 1 = 1.26824 loss)
I1211 18:26:03.252152  2360 solver.cpp:218] Iteration 112500 (13.2252 iter/s, 7.56132s/100 iters), loss = 0.286717
I1211 18:26:03.252152  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:26:03.252152  2360 solver.cpp:237]     Train net output #1: loss = 0.286717 (* 1 = 0.286717 loss)
I1211 18:26:03.252152  2360 sgd_solver.cpp:105] Iteration 112500, lr = 0.001
I1211 18:26:09.403512  2360 solver.cpp:218] Iteration 112600 (16.2554 iter/s, 6.1518s/100 iters), loss = 0.348046
I1211 18:26:09.404513  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:26:09.404513  2360 solver.cpp:237]     Train net output #1: loss = 0.348046 (* 1 = 0.348046 loss)
I1211 18:26:09.404513  2360 sgd_solver.cpp:105] Iteration 112600, lr = 0.001
I1211 18:26:15.561000  2360 solver.cpp:218] Iteration 112700 (16.2438 iter/s, 6.15618s/100 iters), loss = 0.31459
I1211 18:26:15.561000  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:26:15.561000  2360 solver.cpp:237]     Train net output #1: loss = 0.31459 (* 1 = 0.31459 loss)
I1211 18:26:15.561000  2360 sgd_solver.cpp:105] Iteration 112700, lr = 0.001
I1211 18:26:21.724522  2360 solver.cpp:218] Iteration 112800 (16.2242 iter/s, 6.16363s/100 iters), loss = 0.368729
I1211 18:26:21.724522  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:26:21.724522  2360 solver.cpp:237]     Train net output #1: loss = 0.368729 (* 1 = 0.368729 loss)
I1211 18:26:21.724522  2360 sgd_solver.cpp:105] Iteration 112800, lr = 0.001
I1211 18:26:27.879984  2360 solver.cpp:218] Iteration 112900 (16.2478 iter/s, 6.1547s/100 iters), loss = 0.399568
I1211 18:26:27.879984  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:26:27.879984  2360 solver.cpp:237]     Train net output #1: loss = 0.399567 (* 1 = 0.399567 loss)
I1211 18:26:27.879984  2360 sgd_solver.cpp:105] Iteration 112900, lr = 0.001
I1211 18:26:33.731508  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:26:33.975019  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_113000.caffemodel
I1211 18:26:33.991531  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_113000.solverstate
I1211 18:26:33.996531  2360 solver.cpp:330] Iteration 113000, Testing net (#0)
I1211 18:26:33.996531  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:26:35.341657 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:26:35.393674  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6745
I1211 18:26:35.393674  2360 solver.cpp:397]     Test net output #1: loss = 1.27258 (* 1 = 1.27258 loss)
I1211 18:26:35.452674  2360 solver.cpp:218] Iteration 113000 (13.2061 iter/s, 7.57226s/100 iters), loss = 0.277251
I1211 18:26:35.452674  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:26:35.452674  2360 solver.cpp:237]     Train net output #1: loss = 0.277251 (* 1 = 0.277251 loss)
I1211 18:26:35.452674  2360 sgd_solver.cpp:105] Iteration 113000, lr = 0.001
I1211 18:26:41.602213  2360 solver.cpp:218] Iteration 113100 (16.2623 iter/s, 6.1492s/100 iters), loss = 0.279214
I1211 18:26:41.602213  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:26:41.602213  2360 solver.cpp:237]     Train net output #1: loss = 0.279214 (* 1 = 0.279214 loss)
I1211 18:26:41.602213  2360 sgd_solver.cpp:105] Iteration 113100, lr = 0.001
I1211 18:26:47.750684  2360 solver.cpp:218] Iteration 113200 (16.2668 iter/s, 6.14748s/100 iters), loss = 0.248749
I1211 18:26:47.750684  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:26:47.750684  2360 solver.cpp:237]     Train net output #1: loss = 0.248749 (* 1 = 0.248749 loss)
I1211 18:26:47.750684  2360 sgd_solver.cpp:105] Iteration 113200, lr = 0.001
I1211 18:26:53.903169  2360 solver.cpp:218] Iteration 113300 (16.2536 iter/s, 6.15249s/100 iters), loss = 0.330279
I1211 18:26:53.903169  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:26:53.903169  2360 solver.cpp:237]     Train net output #1: loss = 0.330278 (* 1 = 0.330278 loss)
I1211 18:26:53.903169  2360 sgd_solver.cpp:105] Iteration 113300, lr = 0.001
I1211 18:27:00.065600  2360 solver.cpp:218] Iteration 113400 (16.2289 iter/s, 6.16185s/100 iters), loss = 0.352253
I1211 18:27:00.065600  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:27:00.065600  2360 solver.cpp:237]     Train net output #1: loss = 0.352253 (* 1 = 0.352253 loss)
I1211 18:27:00.065600  2360 sgd_solver.cpp:105] Iteration 113400, lr = 0.001
I1211 18:27:05.919034  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:27:06.161044  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_113500.caffemodel
I1211 18:27:06.180552  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_113500.solverstate
I1211 18:27:06.185062  2360 solver.cpp:330] Iteration 113500, Testing net (#0)
I1211 18:27:06.185062  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:27:07.527178 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:27:07.580183  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6762
I1211 18:27:07.580183  2360 solver.cpp:397]     Test net output #1: loss = 1.26507 (* 1 = 1.26507 loss)
I1211 18:27:07.638186  2360 solver.cpp:218] Iteration 113500 (13.2057 iter/s, 7.57251s/100 iters), loss = 0.264735
I1211 18:27:07.638186  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:27:07.638186  2360 solver.cpp:237]     Train net output #1: loss = 0.264734 (* 1 = 0.264734 loss)
I1211 18:27:07.638186  2360 sgd_solver.cpp:105] Iteration 113500, lr = 0.001
I1211 18:27:13.778414  2360 solver.cpp:218] Iteration 113600 (16.2896 iter/s, 6.13887s/100 iters), loss = 0.309895
I1211 18:27:13.778414  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:27:13.778414  2360 solver.cpp:237]     Train net output #1: loss = 0.309895 (* 1 = 0.309895 loss)
I1211 18:27:13.778414  2360 sgd_solver.cpp:105] Iteration 113600, lr = 0.001
I1211 18:27:19.915407  2360 solver.cpp:218] Iteration 113700 (16.2934 iter/s, 6.13745s/100 iters), loss = 0.328411
I1211 18:27:19.915407  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:27:19.916409  2360 solver.cpp:237]     Train net output #1: loss = 0.328411 (* 1 = 0.328411 loss)
I1211 18:27:19.916409  2360 sgd_solver.cpp:105] Iteration 113700, lr = 0.001
I1211 18:27:26.060883  2360 solver.cpp:218] Iteration 113800 (16.2747 iter/s, 6.1445s/100 iters), loss = 0.332937
I1211 18:27:26.060883  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:27:26.060883  2360 solver.cpp:237]     Train net output #1: loss = 0.332937 (* 1 = 0.332937 loss)
I1211 18:27:26.060883  2360 sgd_solver.cpp:105] Iteration 113800, lr = 0.001
I1211 18:27:32.215309  2360 solver.cpp:218] Iteration 113900 (16.2502 iter/s, 6.15376s/100 iters), loss = 0.339305
I1211 18:27:32.215309  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:27:32.215309  2360 solver.cpp:237]     Train net output #1: loss = 0.339305 (* 1 = 0.339305 loss)
I1211 18:27:32.215309  2360 sgd_solver.cpp:105] Iteration 113900, lr = 0.001
I1211 18:27:38.067765  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:27:38.309784  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_114000.caffemodel
I1211 18:27:38.326784  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_114000.solverstate
I1211 18:27:38.331785  2360 solver.cpp:330] Iteration 114000, Testing net (#0)
I1211 18:27:38.331785  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:27:39.672035 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:27:39.725040  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6744
I1211 18:27:39.725040  2360 solver.cpp:397]     Test net output #1: loss = 1.27978 (* 1 = 1.27978 loss)
I1211 18:27:39.785542  2360 solver.cpp:218] Iteration 114000 (13.2106 iter/s, 7.56965s/100 iters), loss = 0.312662
I1211 18:27:39.785542  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:27:39.785542  2360 solver.cpp:237]     Train net output #1: loss = 0.312662 (* 1 = 0.312662 loss)
I1211 18:27:39.785542  2360 sgd_solver.cpp:105] Iteration 114000, lr = 0.001
I1211 18:27:45.941445  2360 solver.cpp:218] Iteration 114100 (16.2453 iter/s, 6.15564s/100 iters), loss = 0.293188
I1211 18:27:45.941445  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:27:45.941445  2360 solver.cpp:237]     Train net output #1: loss = 0.293187 (* 1 = 0.293187 loss)
I1211 18:27:45.941445  2360 sgd_solver.cpp:105] Iteration 114100, lr = 0.001
I1211 18:27:52.085397  2360 solver.cpp:218] Iteration 114200 (16.2773 iter/s, 6.14353s/100 iters), loss = 0.317363
I1211 18:27:52.085397  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 18:27:52.085397  2360 solver.cpp:237]     Train net output #1: loss = 0.317363 (* 1 = 0.317363 loss)
I1211 18:27:52.085397  2360 sgd_solver.cpp:105] Iteration 114200, lr = 0.001
I1211 18:27:58.226379  2360 solver.cpp:218] Iteration 114300 (16.2849 iter/s, 6.14066s/100 iters), loss = 0.345917
I1211 18:27:58.226379  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 18:27:58.226379  2360 solver.cpp:237]     Train net output #1: loss = 0.345916 (* 1 = 0.345916 loss)
I1211 18:27:58.226379  2360 sgd_solver.cpp:105] Iteration 114300, lr = 0.001
I1211 18:28:04.371692  2360 solver.cpp:218] Iteration 114400 (16.273 iter/s, 6.14514s/100 iters), loss = 0.280937
I1211 18:28:04.371692  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 18:28:04.371692  2360 solver.cpp:237]     Train net output #1: loss = 0.280937 (* 1 = 0.280937 loss)
I1211 18:28:04.371692  2360 sgd_solver.cpp:105] Iteration 114400, lr = 0.001
I1211 18:28:10.221474  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:28:10.464488  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_114500.caffemodel
I1211 18:28:10.480993  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_114500.solverstate
I1211 18:28:10.486493  2360 solver.cpp:330] Iteration 114500, Testing net (#0)
I1211 18:28:10.486493  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:28:11.826508 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:28:11.879542  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6744
I1211 18:28:11.879542  2360 solver.cpp:397]     Test net output #1: loss = 1.2715 (* 1 = 1.2715 loss)
I1211 18:28:11.938539  2360 solver.cpp:218] Iteration 114500 (13.2169 iter/s, 7.56606s/100 iters), loss = 0.203583
I1211 18:28:11.938539  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1211 18:28:11.938539  2360 solver.cpp:237]     Train net output #1: loss = 0.203583 (* 1 = 0.203583 loss)
I1211 18:28:11.938539  2360 sgd_solver.cpp:105] Iteration 114500, lr = 0.001
I1211 18:28:18.092833  2360 solver.cpp:218] Iteration 114600 (16.2507 iter/s, 6.15358s/100 iters), loss = 0.29373
I1211 18:28:18.092833  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:28:18.092833  2360 solver.cpp:237]     Train net output #1: loss = 0.29373 (* 1 = 0.29373 loss)
I1211 18:28:18.092833  2360 sgd_solver.cpp:105] Iteration 114600, lr = 0.001
I1211 18:28:24.241849  2360 solver.cpp:218] Iteration 114700 (16.2625 iter/s, 6.14911s/100 iters), loss = 0.314545
I1211 18:28:24.241849  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:28:24.241849  2360 solver.cpp:237]     Train net output #1: loss = 0.314544 (* 1 = 0.314544 loss)
I1211 18:28:24.241849  2360 sgd_solver.cpp:105] Iteration 114700, lr = 0.001
I1211 18:28:30.390065  2360 solver.cpp:218] Iteration 114800 (16.2677 iter/s, 6.14716s/100 iters), loss = 0.332393
I1211 18:28:30.390065  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:28:30.390065  2360 solver.cpp:237]     Train net output #1: loss = 0.332392 (* 1 = 0.332392 loss)
I1211 18:28:30.390065  2360 sgd_solver.cpp:105] Iteration 114800, lr = 0.001
I1211 18:28:36.538378  2360 solver.cpp:218] Iteration 114900 (16.2648 iter/s, 6.14826s/100 iters), loss = 0.398988
I1211 18:28:36.538378  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 18:28:36.538378  2360 solver.cpp:237]     Train net output #1: loss = 0.398988 (* 1 = 0.398988 loss)
I1211 18:28:36.538378  2360 sgd_solver.cpp:105] Iteration 114900, lr = 0.001
I1211 18:28:42.392829  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:28:42.634917  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_115000.caffemodel
I1211 18:28:42.651917  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_115000.solverstate
I1211 18:28:42.656908  2360 solver.cpp:330] Iteration 115000, Testing net (#0)
I1211 18:28:42.656908  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:28:43.996336 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:28:44.048338  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6745
I1211 18:28:44.048338  2360 solver.cpp:397]     Test net output #1: loss = 1.2723 (* 1 = 1.2723 loss)
I1211 18:28:44.107359  2360 solver.cpp:218] Iteration 115000 (13.2127 iter/s, 7.56846s/100 iters), loss = 0.251694
I1211 18:28:44.107359  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:28:44.107359  2360 solver.cpp:237]     Train net output #1: loss = 0.251694 (* 1 = 0.251694 loss)
I1211 18:28:44.107359  2360 sgd_solver.cpp:105] Iteration 115000, lr = 0.001
I1211 18:28:50.252774  2360 solver.cpp:218] Iteration 115100 (16.2743 iter/s, 6.14467s/100 iters), loss = 0.320986
I1211 18:28:50.252774  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:28:50.252774  2360 solver.cpp:237]     Train net output #1: loss = 0.320985 (* 1 = 0.320985 loss)
I1211 18:28:50.252774  2360 sgd_solver.cpp:105] Iteration 115100, lr = 0.001
I1211 18:28:56.394726  2360 solver.cpp:218] Iteration 115200 (16.2827 iter/s, 6.14149s/100 iters), loss = 0.281035
I1211 18:28:56.394726  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 18:28:56.394726  2360 solver.cpp:237]     Train net output #1: loss = 0.281035 (* 1 = 0.281035 loss)
I1211 18:28:56.394726  2360 sgd_solver.cpp:105] Iteration 115200, lr = 0.001
I1211 18:29:02.541821  2360 solver.cpp:218] Iteration 115300 (16.2686 iter/s, 6.14682s/100 iters), loss = 0.346838
I1211 18:29:02.541821  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:29:02.541821  2360 solver.cpp:237]     Train net output #1: loss = 0.346838 (* 1 = 0.346838 loss)
I1211 18:29:02.541821  2360 sgd_solver.cpp:105] Iteration 115300, lr = 0.001
I1211 18:29:08.690840  2360 solver.cpp:218] Iteration 115400 (16.2642 iter/s, 6.14846s/100 iters), loss = 0.319001
I1211 18:29:08.690840  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:29:08.690840  2360 solver.cpp:237]     Train net output #1: loss = 0.319001 (* 1 = 0.319001 loss)
I1211 18:29:08.690840  2360 sgd_solver.cpp:105] Iteration 115400, lr = 0.001
I1211 18:29:14.537459  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:29:14.782178  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_115500.caffemodel
I1211 18:29:14.799685  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_115500.solverstate
I1211 18:29:14.804685  2360 solver.cpp:330] Iteration 115500, Testing net (#0)
I1211 18:29:14.804685  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:29:16.149093 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:29:16.203097  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6756
I1211 18:29:16.203097  2360 solver.cpp:397]     Test net output #1: loss = 1.27342 (* 1 = 1.27342 loss)
I1211 18:29:16.261101  2360 solver.cpp:218] Iteration 115500 (13.2089 iter/s, 7.57066s/100 iters), loss = 0.278479
I1211 18:29:16.262101  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:29:16.262101  2360 solver.cpp:237]     Train net output #1: loss = 0.278479 (* 1 = 0.278479 loss)
I1211 18:29:16.262101  2360 sgd_solver.cpp:105] Iteration 115500, lr = 0.001
I1211 18:29:22.409312  2360 solver.cpp:218] Iteration 115600 (16.268 iter/s, 6.14705s/100 iters), loss = 0.366566
I1211 18:29:22.409312  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:29:22.409312  2360 solver.cpp:237]     Train net output #1: loss = 0.366566 (* 1 = 0.366566 loss)
I1211 18:29:22.409312  2360 sgd_solver.cpp:105] Iteration 115600, lr = 0.001
I1211 18:29:28.558280  2360 solver.cpp:218] Iteration 115700 (16.2637 iter/s, 6.14868s/100 iters), loss = 0.302846
I1211 18:29:28.558280  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:29:28.558280  2360 solver.cpp:237]     Train net output #1: loss = 0.302846 (* 1 = 0.302846 loss)
I1211 18:29:28.558280  2360 sgd_solver.cpp:105] Iteration 115700, lr = 0.001
I1211 18:29:34.714646  2360 solver.cpp:218] Iteration 115800 (16.2433 iter/s, 6.1564s/100 iters), loss = 0.27277
I1211 18:29:34.715646  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 18:29:34.715646  2360 solver.cpp:237]     Train net output #1: loss = 0.27277 (* 1 = 0.27277 loss)
I1211 18:29:34.715646  2360 sgd_solver.cpp:105] Iteration 115800, lr = 0.001
I1211 18:29:40.863600  2360 solver.cpp:218] Iteration 115900 (16.2655 iter/s, 6.14798s/100 iters), loss = 0.298371
I1211 18:29:40.863600  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:29:40.864100  2360 solver.cpp:237]     Train net output #1: loss = 0.298371 (* 1 = 0.298371 loss)
I1211 18:29:40.864100  2360 sgd_solver.cpp:105] Iteration 115900, lr = 0.001
I1211 18:29:46.714578  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:29:46.957592  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_116000.caffemodel
I1211 18:29:46.974596  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_116000.solverstate
I1211 18:29:46.979598  2360 solver.cpp:330] Iteration 116000, Testing net (#0)
I1211 18:29:46.979598  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:29:48.321696 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:29:48.373703  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6769
I1211 18:29:48.373703  2360 solver.cpp:397]     Test net output #1: loss = 1.27536 (* 1 = 1.27536 loss)
I1211 18:29:48.432703  2360 solver.cpp:218] Iteration 116000 (13.2129 iter/s, 7.56837s/100 iters), loss = 0.258849
I1211 18:29:48.432703  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:29:48.432703  2360 solver.cpp:237]     Train net output #1: loss = 0.258848 (* 1 = 0.258848 loss)
I1211 18:29:48.432703  2360 sgd_solver.cpp:105] Iteration 116000, lr = 0.001
I1211 18:29:54.579170  2360 solver.cpp:218] Iteration 116100 (16.2708 iter/s, 6.14596s/100 iters), loss = 0.304407
I1211 18:29:54.579170  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:29:54.579170  2360 solver.cpp:237]     Train net output #1: loss = 0.304407 (* 1 = 0.304407 loss)
I1211 18:29:54.579170  2360 sgd_solver.cpp:105] Iteration 116100, lr = 0.001
I1211 18:30:00.740739  2360 solver.cpp:218] Iteration 116200 (16.2308 iter/s, 6.16111s/100 iters), loss = 0.288889
I1211 18:30:00.740739  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:30:00.740739  2360 solver.cpp:237]     Train net output #1: loss = 0.288889 (* 1 = 0.288889 loss)
I1211 18:30:00.740739  2360 sgd_solver.cpp:105] Iteration 116200, lr = 0.001
I1211 18:30:06.924229  2360 solver.cpp:218] Iteration 116300 (16.174 iter/s, 6.18276s/100 iters), loss = 0.313255
I1211 18:30:06.924229  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:30:06.924229  2360 solver.cpp:237]     Train net output #1: loss = 0.313255 (* 1 = 0.313255 loss)
I1211 18:30:06.924229  2360 sgd_solver.cpp:105] Iteration 116300, lr = 0.001
I1211 18:30:13.071553  2360 solver.cpp:218] Iteration 116400 (16.268 iter/s, 6.14703s/100 iters), loss = 0.399356
I1211 18:30:13.071553  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 18:30:13.071553  2360 solver.cpp:237]     Train net output #1: loss = 0.399356 (* 1 = 0.399356 loss)
I1211 18:30:13.071553  2360 sgd_solver.cpp:105] Iteration 116400, lr = 0.001
I1211 18:30:18.931380  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:30:19.174868  2360 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_116500.caffemodel
I1211 18:30:19.192471  2360 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_116500.solverstate
I1211 18:30:19.197470  2360 solver.cpp:330] Iteration 116500, Testing net (#0)
I1211 18:30:19.197470  2360 net.cpp:676] Ignoring source layer accuracy_training
I1211 18:30:20.539770 14584 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:30:20.592422  2360 solver.cpp:397]     Test net output #0: accuracy = 0.6742
I1211 18:30:20.592422  2360 solver.cpp:397]     Test net output #1: loss = 1.28713 (* 1 = 1.28713 loss)
I1211 18:30:20.651437  2360 solver.cpp:218] Iteration 116500 (13.1936 iter/s, 7.57943s/100 iters), loss = 0.183148
I1211 18:30:20.651437  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.98
I1211 18:30:20.651437  2360 solver.cpp:237]     Train net output #1: loss = 0.183147 (* 1 = 0.183147 loss)
I1211 18:30:20.651437  2360 sgd_solver.cpp:105] Iteration 116500, lr = 0.001
I1211 18:30:26.807675  2360 solver.cpp:218] Iteration 116600 (16.2448 iter/s, 6.15582s/100 iters), loss = 0.371682
I1211 18:30:26.807675  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:30:26.807675  2360 solver.cpp:237]     Train net output #1: loss = 0.371682 (* 1 = 0.371682 loss)
I1211 18:30:26.807675  2360 sgd_solver.cpp:105] Iteration 116600, lr = 0.001
I1211 18:30:32.950712  2360 solver.cpp:218] Iteration 116700 (16.2796 iter/s, 6.14267s/100 iters), loss = 0.30204
I1211 18:30:32.950712  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 18:30:32.950712  2360 solver.cpp:237]     Train net output #1: loss = 0.30204 (* 1 = 0.30204 loss)
I1211 18:30:32.950712  2360 sgd_solver.cpp:105] Iteration 116700, lr = 0.001
I1211 18:30:39.094671  2360 solver.cpp:218] Iteration 116800 (16.2761 iter/s, 6.14399s/100 iters), loss = 0.381956
I1211 18:30:39.094671  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 18:30:39.094671  2360 solver.cpp:237]     Train net output #1: loss = 0.381956 (* 1 = 0.381956 loss)
I1211 18:30:39.094671  2360 sgd_solver.cpp:105] Iteration 116800, lr = 0.001
I1211 18:30:45.248881  2360 solver.cpp:218] Iteration 116900 (16.2504 iter/s, 6.15368s/100 iters), loss = 0.298403
I1211 18:30:45.248881  2360 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 18:30:45.248881  2360 solver.cpp:237]     Train net output #1: loss = 0.298403 (* 1 = 0.298403 loss)
I1211 18:30:45.248881  2360 sgd_solver.cpp:105] Iteration 116900, lr = 0.001
I1211 18:30:51.088770  5852 data_layer.cpp:73] Restarting data prefetching from start.
I1211 18:30:51.331533  2360 solver.cpp:447] Snapshotting to binary proto file examples

G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90000.solverstate 
I1211 16:29:40.570749 13896 caffe.cpp:219] Using GPUs 0
I1211 16:29:40.759364 13896 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1211 16:29:41.063372 13896 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 16:29:41.079390 13896 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1211 16:29:41.080389 13896 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 16:29:41.082372 13896 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 16:29:41.082372 13896 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 16:29:41.082372 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1211 16:29:41.082372 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1211 16:29:41.082372 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1211 16:29:41.082372 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1211 16:29:41.083371 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1211 16:29:41.083371 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1211 16:29:41.083371 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_pool2_1
I1211 16:29:41.083371 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1211 16:29:41.083371 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1211 16:29:41.083371 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1211 16:29:41.083371 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1211 16:29:41.083371 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1211 16:29:41.083371 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_pool4_2
I1211 16:29:41.083371 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1211 16:29:41.083371 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1211 16:29:41.083371 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1211 16:29:41.083371 13896 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1211 16:29:41.083371 13896 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV2_WnonLin_360k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "pool2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_pool2_1"
  type: "BatchNorm"
  bottom: "pool2_1"
  top: "pool2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_pool2_1"
  type: "Scale"
  bottom: "pool2_1"
  top: "pool2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_pool2_1"
  type: "ReLU"
  bottom: "pool2_1"
  top: "pool2_1"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "pool4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_pool4_2"
  type: "BatchNorm"
  bottom: "pool4_2"
  top: "pool4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_pool4_2"
  type: "Scale"
  bottom: "pool4_2"
  top: "pool4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_pool4_2"
  type: "ReLU"
  bottom: "pool4_2"
  top: "pool4_2"
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 16:29:41.084386 13896 layer_factory.cpp:58] Creating layer cifar
I1211 16:29:41.089381 13896 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1211 16:29:41.090373 13896 net.cpp:84] Creating Layer cifar
I1211 16:29:41.090373 13896 net.cpp:380] cifar -> data
I1211 16:29:41.090373 13896 net.cpp:380] cifar -> label
I1211 16:29:41.091372 13896 data_layer.cpp:45] output data size: 100,3,32,32
I1211 16:29:41.098390 13896 net.cpp:122] Setting up cifar
I1211 16:29:41.098390 13896 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 16:29:41.098390 13896 net.cpp:129] Top shape: 100 (100)
I1211 16:29:41.099372 13896 net.cpp:137] Memory required for data: 1229200
I1211 16:29:41.099372 13896 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 16:29:41.099372 13896 net.cpp:84] Creating Layer label_cifar_1_split
I1211 16:29:41.099372 13896 net.cpp:406] label_cifar_1_split <- label
I1211 16:29:41.099372 13896 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 16:29:41.099372 13896 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 16:29:41.099372 13896 net.cpp:122] Setting up label_cifar_1_split
I1211 16:29:41.099372 13896 net.cpp:129] Top shape: 100 (100)
I1211 16:29:41.099372 13896 net.cpp:129] Top shape: 100 (100)
I1211 16:29:41.099372 13896 net.cpp:137] Memory required for data: 1230000
I1211 16:29:41.099372 13896 layer_factory.cpp:58] Creating layer conv1
I1211 16:29:41.099372 13896 net.cpp:84] Creating Layer conv1
I1211 16:29:41.099372 13896 net.cpp:406] conv1 <- data
I1211 16:29:41.099372 13896 net.cpp:380] conv1 -> conv1
I1211 16:29:41.102378  1052 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 16:29:41.357468 13896 net.cpp:122] Setting up conv1
I1211 16:29:41.357468 13896 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 16:29:41.357468 13896 net.cpp:137] Memory required for data: 13518000
I1211 16:29:41.357468 13896 layer_factory.cpp:58] Creating layer bn1
I1211 16:29:41.357468 13896 net.cpp:84] Creating Layer bn1
I1211 16:29:41.357468 13896 net.cpp:406] bn1 <- conv1
I1211 16:29:41.357468 13896 net.cpp:367] bn1 -> conv1 (in-place)
I1211 16:29:41.357468 13896 net.cpp:122] Setting up bn1
I1211 16:29:41.357468 13896 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 16:29:41.357468 13896 net.cpp:137] Memory required for data: 25806000
I1211 16:29:41.357468 13896 layer_factory.cpp:58] Creating layer scale1
I1211 16:29:41.357468 13896 net.cpp:84] Creating Layer scale1
I1211 16:29:41.357468 13896 net.cpp:406] scale1 <- conv1
I1211 16:29:41.357468 13896 net.cpp:367] scale1 -> conv1 (in-place)
I1211 16:29:41.357468 13896 layer_factory.cpp:58] Creating layer scale1
I1211 16:29:41.357468 13896 net.cpp:122] Setting up scale1
I1211 16:29:41.357468 13896 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 16:29:41.357468 13896 net.cpp:137] Memory required for data: 38094000
I1211 16:29:41.357468 13896 layer_factory.cpp:58] Creating layer relu1
I1211 16:29:41.357468 13896 net.cpp:84] Creating Layer relu1
I1211 16:29:41.357468 13896 net.cpp:406] relu1 <- conv1
I1211 16:29:41.357468 13896 net.cpp:367] relu1 -> conv1 (in-place)
I1211 16:29:41.357468 13896 net.cpp:122] Setting up relu1
I1211 16:29:41.358469 13896 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 16:29:41.358469 13896 net.cpp:137] Memory required for data: 50382000
I1211 16:29:41.358469 13896 layer_factory.cpp:58] Creating layer conv1_0
I1211 16:29:41.358469 13896 net.cpp:84] Creating Layer conv1_0
I1211 16:29:41.358469 13896 net.cpp:406] conv1_0 <- conv1
I1211 16:29:41.358469 13896 net.cpp:380] conv1_0 -> conv1_0
I1211 16:29:41.359468 13896 net.cpp:122] Setting up conv1_0
I1211 16:29:41.359468 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.359468 13896 net.cpp:137] Memory required for data: 66766000
I1211 16:29:41.359468 13896 layer_factory.cpp:58] Creating layer bn1_0
I1211 16:29:41.359468 13896 net.cpp:84] Creating Layer bn1_0
I1211 16:29:41.359468 13896 net.cpp:406] bn1_0 <- conv1_0
I1211 16:29:41.359468 13896 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 16:29:41.360468 13896 net.cpp:122] Setting up bn1_0
I1211 16:29:41.360468 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.360468 13896 net.cpp:137] Memory required for data: 83150000
I1211 16:29:41.360468 13896 layer_factory.cpp:58] Creating layer scale1_0
I1211 16:29:41.360468 13896 net.cpp:84] Creating Layer scale1_0
I1211 16:29:41.360468 13896 net.cpp:406] scale1_0 <- conv1_0
I1211 16:29:41.360468 13896 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 16:29:41.360468 13896 layer_factory.cpp:58] Creating layer scale1_0
I1211 16:29:41.360468 13896 net.cpp:122] Setting up scale1_0
I1211 16:29:41.360468 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.360468 13896 net.cpp:137] Memory required for data: 99534000
I1211 16:29:41.360468 13896 layer_factory.cpp:58] Creating layer relu1_0
I1211 16:29:41.360468 13896 net.cpp:84] Creating Layer relu1_0
I1211 16:29:41.360468 13896 net.cpp:406] relu1_0 <- conv1_0
I1211 16:29:41.360468 13896 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 16:29:41.360468 13896 net.cpp:122] Setting up relu1_0
I1211 16:29:41.360468 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.360468 13896 net.cpp:137] Memory required for data: 115918000
I1211 16:29:41.360468 13896 layer_factory.cpp:58] Creating layer conv2
I1211 16:29:41.360468 13896 net.cpp:84] Creating Layer conv2
I1211 16:29:41.360468 13896 net.cpp:406] conv2 <- conv1_0
I1211 16:29:41.360468 13896 net.cpp:380] conv2 -> conv2
I1211 16:29:41.361469 13896 net.cpp:122] Setting up conv2
I1211 16:29:41.361469 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.361469 13896 net.cpp:137] Memory required for data: 132302000
I1211 16:29:41.361469 13896 layer_factory.cpp:58] Creating layer bn2
I1211 16:29:41.361469 13896 net.cpp:84] Creating Layer bn2
I1211 16:29:41.361469 13896 net.cpp:406] bn2 <- conv2
I1211 16:29:41.361469 13896 net.cpp:367] bn2 -> conv2 (in-place)
I1211 16:29:41.361469 13896 net.cpp:122] Setting up bn2
I1211 16:29:41.361469 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.361469 13896 net.cpp:137] Memory required for data: 148686000
I1211 16:29:41.361469 13896 layer_factory.cpp:58] Creating layer scale2
I1211 16:29:41.361469 13896 net.cpp:84] Creating Layer scale2
I1211 16:29:41.361469 13896 net.cpp:406] scale2 <- conv2
I1211 16:29:41.361469 13896 net.cpp:367] scale2 -> conv2 (in-place)
I1211 16:29:41.361469 13896 layer_factory.cpp:58] Creating layer scale2
I1211 16:29:41.361469 13896 net.cpp:122] Setting up scale2
I1211 16:29:41.361469 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.361469 13896 net.cpp:137] Memory required for data: 165070000
I1211 16:29:41.361469 13896 layer_factory.cpp:58] Creating layer relu2
I1211 16:29:41.361469 13896 net.cpp:84] Creating Layer relu2
I1211 16:29:41.361469 13896 net.cpp:406] relu2 <- conv2
I1211 16:29:41.361469 13896 net.cpp:367] relu2 -> conv2 (in-place)
I1211 16:29:41.362468 13896 net.cpp:122] Setting up relu2
I1211 16:29:41.362468 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.362468 13896 net.cpp:137] Memory required for data: 181454000
I1211 16:29:41.362468 13896 layer_factory.cpp:58] Creating layer conv2_1
I1211 16:29:41.362468 13896 net.cpp:84] Creating Layer conv2_1
I1211 16:29:41.362468 13896 net.cpp:406] conv2_1 <- conv2
I1211 16:29:41.362468 13896 net.cpp:380] conv2_1 -> conv2_1
I1211 16:29:41.363468 13896 net.cpp:122] Setting up conv2_1
I1211 16:29:41.363468 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.363468 13896 net.cpp:137] Memory required for data: 197838000
I1211 16:29:41.363468 13896 layer_factory.cpp:58] Creating layer bn2_1
I1211 16:29:41.363468 13896 net.cpp:84] Creating Layer bn2_1
I1211 16:29:41.363468 13896 net.cpp:406] bn2_1 <- conv2_1
I1211 16:29:41.363468 13896 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 16:29:41.363468 13896 net.cpp:122] Setting up bn2_1
I1211 16:29:41.363468 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.363468 13896 net.cpp:137] Memory required for data: 214222000
I1211 16:29:41.363468 13896 layer_factory.cpp:58] Creating layer scale2_1
I1211 16:29:41.363468 13896 net.cpp:84] Creating Layer scale2_1
I1211 16:29:41.363468 13896 net.cpp:406] scale2_1 <- conv2_1
I1211 16:29:41.363468 13896 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 16:29:41.363468 13896 layer_factory.cpp:58] Creating layer scale2_1
I1211 16:29:41.363468 13896 net.cpp:122] Setting up scale2_1
I1211 16:29:41.363468 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.363468 13896 net.cpp:137] Memory required for data: 230606000
I1211 16:29:41.363468 13896 layer_factory.cpp:58] Creating layer relu2_1
I1211 16:29:41.363468 13896 net.cpp:84] Creating Layer relu2_1
I1211 16:29:41.363468 13896 net.cpp:406] relu2_1 <- conv2_1
I1211 16:29:41.363468 13896 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 16:29:41.363468 13896 net.cpp:122] Setting up relu2_1
I1211 16:29:41.363468 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.364468 13896 net.cpp:137] Memory required for data: 246990000
I1211 16:29:41.364468 13896 layer_factory.cpp:58] Creating layer conv2_2
I1211 16:29:41.364468 13896 net.cpp:84] Creating Layer conv2_2
I1211 16:29:41.364468 13896 net.cpp:406] conv2_2 <- conv2_1
I1211 16:29:41.364468 13896 net.cpp:380] conv2_2 -> conv2_2
I1211 16:29:41.365468 13896 net.cpp:122] Setting up conv2_2
I1211 16:29:41.365468 13896 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 16:29:41.365468 13896 net.cpp:137] Memory required for data: 267470000
I1211 16:29:41.365468 13896 layer_factory.cpp:58] Creating layer bn2_2
I1211 16:29:41.365468 13896 net.cpp:84] Creating Layer bn2_2
I1211 16:29:41.365468 13896 net.cpp:406] bn2_2 <- conv2_2
I1211 16:29:41.365468 13896 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 16:29:41.365468 13896 net.cpp:122] Setting up bn2_2
I1211 16:29:41.365468 13896 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 16:29:41.365468 13896 net.cpp:137] Memory required for data: 287950000
I1211 16:29:41.365468 13896 layer_factory.cpp:58] Creating layer scale2_2
I1211 16:29:41.365468 13896 net.cpp:84] Creating Layer scale2_2
I1211 16:29:41.365468 13896 net.cpp:406] scale2_2 <- conv2_2
I1211 16:29:41.365468 13896 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 16:29:41.366467 13896 layer_factory.cpp:58] Creating layer scale2_2
I1211 16:29:41.366467 13896 net.cpp:122] Setting up scale2_2
I1211 16:29:41.366467 13896 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 16:29:41.366467 13896 net.cpp:137] Memory required for data: 308430000
I1211 16:29:41.366467 13896 layer_factory.cpp:58] Creating layer relu2_2
I1211 16:29:41.366467 13896 net.cpp:84] Creating Layer relu2_2
I1211 16:29:41.366467 13896 net.cpp:406] relu2_2 <- conv2_2
I1211 16:29:41.366467 13896 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 16:29:41.366467 13896 net.cpp:122] Setting up relu2_2
I1211 16:29:41.366467 13896 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 16:29:41.366467 13896 net.cpp:137] Memory required for data: 328910000
I1211 16:29:41.366467 13896 layer_factory.cpp:58] Creating layer pool2_1
I1211 16:29:41.366467 13896 net.cpp:84] Creating Layer pool2_1
I1211 16:29:41.366467 13896 net.cpp:406] pool2_1 <- conv2_2
I1211 16:29:41.366467 13896 net.cpp:380] pool2_1 -> pool2_1
I1211 16:29:41.367468 13896 net.cpp:122] Setting up pool2_1
I1211 16:29:41.367468 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.367468 13896 net.cpp:137] Memory required for data: 334030000
I1211 16:29:41.367468 13896 layer_factory.cpp:58] Creating layer bn2_pool2_1
I1211 16:29:41.367468 13896 net.cpp:84] Creating Layer bn2_pool2_1
I1211 16:29:41.367468 13896 net.cpp:406] bn2_pool2_1 <- pool2_1
I1211 16:29:41.367468 13896 net.cpp:367] bn2_pool2_1 -> pool2_1 (in-place)
I1211 16:29:41.368468 13896 net.cpp:122] Setting up bn2_pool2_1
I1211 16:29:41.368468 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.368468 13896 net.cpp:137] Memory required for data: 339150000
I1211 16:29:41.368468 13896 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 16:29:41.368468 13896 net.cpp:84] Creating Layer scale2_pool2_1
I1211 16:29:41.368468 13896 net.cpp:406] scale2_pool2_1 <- pool2_1
I1211 16:29:41.368468 13896 net.cpp:367] scale2_pool2_1 -> pool2_1 (in-place)
I1211 16:29:41.368468 13896 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 16:29:41.368468 13896 net.cpp:122] Setting up scale2_pool2_1
I1211 16:29:41.368468 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.368468 13896 net.cpp:137] Memory required for data: 344270000
I1211 16:29:41.368468 13896 layer_factory.cpp:58] Creating layer relu2_pool2_1
I1211 16:29:41.368468 13896 net.cpp:84] Creating Layer relu2_pool2_1
I1211 16:29:41.368468 13896 net.cpp:406] relu2_pool2_1 <- pool2_1
I1211 16:29:41.368468 13896 net.cpp:367] relu2_pool2_1 -> pool2_1 (in-place)
I1211 16:29:41.368468 13896 net.cpp:122] Setting up relu2_pool2_1
I1211 16:29:41.368468 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.368468 13896 net.cpp:137] Memory required for data: 349390000
I1211 16:29:41.368468 13896 layer_factory.cpp:58] Creating layer conv3
I1211 16:29:41.368468 13896 net.cpp:84] Creating Layer conv3
I1211 16:29:41.368468 13896 net.cpp:406] conv3 <- pool2_1
I1211 16:29:41.368468 13896 net.cpp:380] conv3 -> conv3
I1211 16:29:41.369468 13896 net.cpp:122] Setting up conv3
I1211 16:29:41.369468 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.369468 13896 net.cpp:137] Memory required for data: 354510000
I1211 16:29:41.370467 13896 layer_factory.cpp:58] Creating layer bn3
I1211 16:29:41.370467 13896 net.cpp:84] Creating Layer bn3
I1211 16:29:41.370467 13896 net.cpp:406] bn3 <- conv3
I1211 16:29:41.370467 13896 net.cpp:367] bn3 -> conv3 (in-place)
I1211 16:29:41.370467 13896 net.cpp:122] Setting up bn3
I1211 16:29:41.370467 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.370467 13896 net.cpp:137] Memory required for data: 359630000
I1211 16:29:41.370467 13896 layer_factory.cpp:58] Creating layer scale3
I1211 16:29:41.370467 13896 net.cpp:84] Creating Layer scale3
I1211 16:29:41.370467 13896 net.cpp:406] scale3 <- conv3
I1211 16:29:41.370467 13896 net.cpp:367] scale3 -> conv3 (in-place)
I1211 16:29:41.370467 13896 layer_factory.cpp:58] Creating layer scale3
I1211 16:29:41.370467 13896 net.cpp:122] Setting up scale3
I1211 16:29:41.370467 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.370467 13896 net.cpp:137] Memory required for data: 364750000
I1211 16:29:41.370467 13896 layer_factory.cpp:58] Creating layer relu3
I1211 16:29:41.370467 13896 net.cpp:84] Creating Layer relu3
I1211 16:29:41.370467 13896 net.cpp:406] relu3 <- conv3
I1211 16:29:41.370467 13896 net.cpp:367] relu3 -> conv3 (in-place)
I1211 16:29:41.370467 13896 net.cpp:122] Setting up relu3
I1211 16:29:41.370467 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.370467 13896 net.cpp:137] Memory required for data: 369870000
I1211 16:29:41.370467 13896 layer_factory.cpp:58] Creating layer conv3_1
I1211 16:29:41.370467 13896 net.cpp:84] Creating Layer conv3_1
I1211 16:29:41.370467 13896 net.cpp:406] conv3_1 <- conv3
I1211 16:29:41.370467 13896 net.cpp:380] conv3_1 -> conv3_1
I1211 16:29:41.371467 13896 net.cpp:122] Setting up conv3_1
I1211 16:29:41.372479 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.372479 13896 net.cpp:137] Memory required for data: 374990000
I1211 16:29:41.372479 13896 layer_factory.cpp:58] Creating layer bn3_1
I1211 16:29:41.372479 13896 net.cpp:84] Creating Layer bn3_1
I1211 16:29:41.372479 13896 net.cpp:406] bn3_1 <- conv3_1
I1211 16:29:41.372479 13896 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 16:29:41.372479 13896 net.cpp:122] Setting up bn3_1
I1211 16:29:41.372479 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.372479 13896 net.cpp:137] Memory required for data: 380110000
I1211 16:29:41.372479 13896 layer_factory.cpp:58] Creating layer scale3_1
I1211 16:29:41.372479 13896 net.cpp:84] Creating Layer scale3_1
I1211 16:29:41.372479 13896 net.cpp:406] scale3_1 <- conv3_1
I1211 16:29:41.372479 13896 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 16:29:41.372479 13896 layer_factory.cpp:58] Creating layer scale3_1
I1211 16:29:41.372479 13896 net.cpp:122] Setting up scale3_1
I1211 16:29:41.372479 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.372479 13896 net.cpp:137] Memory required for data: 385230000
I1211 16:29:41.372479 13896 layer_factory.cpp:58] Creating layer relu3_1
I1211 16:29:41.372479 13896 net.cpp:84] Creating Layer relu3_1
I1211 16:29:41.372479 13896 net.cpp:406] relu3_1 <- conv3_1
I1211 16:29:41.372479 13896 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 16:29:41.373468 13896 net.cpp:122] Setting up relu3_1
I1211 16:29:41.373468 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.373468 13896 net.cpp:137] Memory required for data: 390350000
I1211 16:29:41.373468 13896 layer_factory.cpp:58] Creating layer conv4
I1211 16:29:41.373468 13896 net.cpp:84] Creating Layer conv4
I1211 16:29:41.373468 13896 net.cpp:406] conv4 <- conv3_1
I1211 16:29:41.373468 13896 net.cpp:380] conv4 -> conv4
I1211 16:29:41.374467 13896 net.cpp:122] Setting up conv4
I1211 16:29:41.374467 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.374467 13896 net.cpp:137] Memory required for data: 395470000
I1211 16:29:41.374467 13896 layer_factory.cpp:58] Creating layer bn4
I1211 16:29:41.374467 13896 net.cpp:84] Creating Layer bn4
I1211 16:29:41.374467 13896 net.cpp:406] bn4 <- conv4
I1211 16:29:41.374467 13896 net.cpp:367] bn4 -> conv4 (in-place)
I1211 16:29:41.374467 13896 net.cpp:122] Setting up bn4
I1211 16:29:41.374467 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.374467 13896 net.cpp:137] Memory required for data: 400590000
I1211 16:29:41.374467 13896 layer_factory.cpp:58] Creating layer scale4
I1211 16:29:41.374467 13896 net.cpp:84] Creating Layer scale4
I1211 16:29:41.374467 13896 net.cpp:406] scale4 <- conv4
I1211 16:29:41.374467 13896 net.cpp:367] scale4 -> conv4 (in-place)
I1211 16:29:41.374467 13896 layer_factory.cpp:58] Creating layer scale4
I1211 16:29:41.374467 13896 net.cpp:122] Setting up scale4
I1211 16:29:41.374467 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.374467 13896 net.cpp:137] Memory required for data: 405710000
I1211 16:29:41.374467 13896 layer_factory.cpp:58] Creating layer relu4
I1211 16:29:41.374467 13896 net.cpp:84] Creating Layer relu4
I1211 16:29:41.374467 13896 net.cpp:406] relu4 <- conv4
I1211 16:29:41.374467 13896 net.cpp:367] relu4 -> conv4 (in-place)
I1211 16:29:41.374467 13896 net.cpp:122] Setting up relu4
I1211 16:29:41.374467 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.374467 13896 net.cpp:137] Memory required for data: 410830000
I1211 16:29:41.374467 13896 layer_factory.cpp:58] Creating layer conv4_1
I1211 16:29:41.374467 13896 net.cpp:84] Creating Layer conv4_1
I1211 16:29:41.374467 13896 net.cpp:406] conv4_1 <- conv4
I1211 16:29:41.374467 13896 net.cpp:380] conv4_1 -> conv4_1
I1211 16:29:41.376467 13896 net.cpp:122] Setting up conv4_1
I1211 16:29:41.376467 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.376467 13896 net.cpp:137] Memory required for data: 415950000
I1211 16:29:41.376467 13896 layer_factory.cpp:58] Creating layer bn4_1
I1211 16:29:41.376467 13896 net.cpp:84] Creating Layer bn4_1
I1211 16:29:41.376467 13896 net.cpp:406] bn4_1 <- conv4_1
I1211 16:29:41.376467 13896 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 16:29:41.376467 13896 net.cpp:122] Setting up bn4_1
I1211 16:29:41.376467 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.376467 13896 net.cpp:137] Memory required for data: 421070000
I1211 16:29:41.376467 13896 layer_factory.cpp:58] Creating layer scale4_1
I1211 16:29:41.376467 13896 net.cpp:84] Creating Layer scale4_1
I1211 16:29:41.376467 13896 net.cpp:406] scale4_1 <- conv4_1
I1211 16:29:41.376467 13896 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 16:29:41.376467 13896 layer_factory.cpp:58] Creating layer scale4_1
I1211 16:29:41.376467 13896 net.cpp:122] Setting up scale4_1
I1211 16:29:41.376467 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.376467 13896 net.cpp:137] Memory required for data: 426190000
I1211 16:29:41.376467 13896 layer_factory.cpp:58] Creating layer relu4_1
I1211 16:29:41.376467 13896 net.cpp:84] Creating Layer relu4_1
I1211 16:29:41.376467 13896 net.cpp:406] relu4_1 <- conv4_1
I1211 16:29:41.376467 13896 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 16:29:41.376467 13896 net.cpp:122] Setting up relu4_1
I1211 16:29:41.376467 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.376467 13896 net.cpp:137] Memory required for data: 431310000
I1211 16:29:41.376467 13896 layer_factory.cpp:58] Creating layer conv4_2
I1211 16:29:41.376467 13896 net.cpp:84] Creating Layer conv4_2
I1211 16:29:41.376467 13896 net.cpp:406] conv4_2 <- conv4_1
I1211 16:29:41.376467 13896 net.cpp:380] conv4_2 -> conv4_2
I1211 16:29:41.378448 13896 net.cpp:122] Setting up conv4_2
I1211 16:29:41.378448 13896 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 16:29:41.378448 13896 net.cpp:137] Memory required for data: 437249200
I1211 16:29:41.378448 13896 layer_factory.cpp:58] Creating layer bn4_2
I1211 16:29:41.378448 13896 net.cpp:84] Creating Layer bn4_2
I1211 16:29:41.378448 13896 net.cpp:406] bn4_2 <- conv4_2
I1211 16:29:41.378448 13896 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 16:29:41.378448 13896 net.cpp:122] Setting up bn4_2
I1211 16:29:41.378448 13896 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 16:29:41.378448 13896 net.cpp:137] Memory required for data: 443188400
I1211 16:29:41.378448 13896 layer_factory.cpp:58] Creating layer scale4_2
I1211 16:29:41.378448 13896 net.cpp:84] Creating Layer scale4_2
I1211 16:29:41.378448 13896 net.cpp:406] scale4_2 <- conv4_2
I1211 16:29:41.378448 13896 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 16:29:41.378448 13896 layer_factory.cpp:58] Creating layer scale4_2
I1211 16:29:41.378448 13896 net.cpp:122] Setting up scale4_2
I1211 16:29:41.378448 13896 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 16:29:41.378448 13896 net.cpp:137] Memory required for data: 449127600
I1211 16:29:41.378448 13896 layer_factory.cpp:58] Creating layer relu4_2
I1211 16:29:41.378448 13896 net.cpp:84] Creating Layer relu4_2
I1211 16:29:41.379451 13896 net.cpp:406] relu4_2 <- conv4_2
I1211 16:29:41.379451 13896 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 16:29:41.379451 13896 net.cpp:122] Setting up relu4_2
I1211 16:29:41.379451 13896 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 16:29:41.379451 13896 net.cpp:137] Memory required for data: 455066800
I1211 16:29:41.379451 13896 layer_factory.cpp:58] Creating layer pool4_2
I1211 16:29:41.379451 13896 net.cpp:84] Creating Layer pool4_2
I1211 16:29:41.379451 13896 net.cpp:406] pool4_2 <- conv4_2
I1211 16:29:41.379451 13896 net.cpp:380] pool4_2 -> pool4_2
I1211 16:29:41.380465 13896 net.cpp:122] Setting up pool4_2
I1211 16:29:41.380465 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.380465 13896 net.cpp:137] Memory required for data: 456551600
I1211 16:29:41.380465 13896 layer_factory.cpp:58] Creating layer bn4_pool4_2
I1211 16:29:41.380465 13896 net.cpp:84] Creating Layer bn4_pool4_2
I1211 16:29:41.380465 13896 net.cpp:406] bn4_pool4_2 <- pool4_2
I1211 16:29:41.380465 13896 net.cpp:367] bn4_pool4_2 -> pool4_2 (in-place)
I1211 16:29:41.380465 13896 net.cpp:122] Setting up bn4_pool4_2
I1211 16:29:41.380465 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.380465 13896 net.cpp:137] Memory required for data: 458036400
I1211 16:29:41.380465 13896 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 16:29:41.380465 13896 net.cpp:84] Creating Layer scale4_pool4_2
I1211 16:29:41.380465 13896 net.cpp:406] scale4_pool4_2 <- pool4_2
I1211 16:29:41.380465 13896 net.cpp:367] scale4_pool4_2 -> pool4_2 (in-place)
I1211 16:29:41.380465 13896 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 16:29:41.380465 13896 net.cpp:122] Setting up scale4_pool4_2
I1211 16:29:41.380465 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.380465 13896 net.cpp:137] Memory required for data: 459521200
I1211 16:29:41.380465 13896 layer_factory.cpp:58] Creating layer relu4_pool4_2
I1211 16:29:41.380465 13896 net.cpp:84] Creating Layer relu4_pool4_2
I1211 16:29:41.380465 13896 net.cpp:406] relu4_pool4_2 <- pool4_2
I1211 16:29:41.380465 13896 net.cpp:367] relu4_pool4_2 -> pool4_2 (in-place)
I1211 16:29:41.381464 13896 net.cpp:122] Setting up relu4_pool4_2
I1211 16:29:41.381464 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.381464 13896 net.cpp:137] Memory required for data: 461006000
I1211 16:29:41.381464 13896 layer_factory.cpp:58] Creating layer conv4_0
I1211 16:29:41.381464 13896 net.cpp:84] Creating Layer conv4_0
I1211 16:29:41.381464 13896 net.cpp:406] conv4_0 <- pool4_2
I1211 16:29:41.381464 13896 net.cpp:380] conv4_0 -> conv4_0
I1211 16:29:41.382464 13896 net.cpp:122] Setting up conv4_0
I1211 16:29:41.382464 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.382464 13896 net.cpp:137] Memory required for data: 462490800
I1211 16:29:41.382464 13896 layer_factory.cpp:58] Creating layer bn4_0
I1211 16:29:41.382464 13896 net.cpp:84] Creating Layer bn4_0
I1211 16:29:41.382464 13896 net.cpp:406] bn4_0 <- conv4_0
I1211 16:29:41.382464 13896 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 16:29:41.383464 13896 net.cpp:122] Setting up bn4_0
I1211 16:29:41.383464 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.383464 13896 net.cpp:137] Memory required for data: 463975600
I1211 16:29:41.383464 13896 layer_factory.cpp:58] Creating layer scale4_0
I1211 16:29:41.383464 13896 net.cpp:84] Creating Layer scale4_0
I1211 16:29:41.383464 13896 net.cpp:406] scale4_0 <- conv4_0
I1211 16:29:41.383464 13896 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 16:29:41.383464 13896 layer_factory.cpp:58] Creating layer scale4_0
I1211 16:29:41.383464 13896 net.cpp:122] Setting up scale4_0
I1211 16:29:41.383464 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.383464 13896 net.cpp:137] Memory required for data: 465460400
I1211 16:29:41.383464 13896 layer_factory.cpp:58] Creating layer relu4_0
I1211 16:29:41.383464 13896 net.cpp:84] Creating Layer relu4_0
I1211 16:29:41.383464 13896 net.cpp:406] relu4_0 <- conv4_0
I1211 16:29:41.383464 13896 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 16:29:41.383464 13896 net.cpp:122] Setting up relu4_0
I1211 16:29:41.383464 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.383464 13896 net.cpp:137] Memory required for data: 466945200
I1211 16:29:41.383464 13896 layer_factory.cpp:58] Creating layer conv11
I1211 16:29:41.383464 13896 net.cpp:84] Creating Layer conv11
I1211 16:29:41.383464 13896 net.cpp:406] conv11 <- conv4_0
I1211 16:29:41.383464 13896 net.cpp:380] conv11 -> conv11
I1211 16:29:41.384464 13896 net.cpp:122] Setting up conv11
I1211 16:29:41.385468 13896 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 16:29:41.385468 13896 net.cpp:137] Memory required for data: 468737200
I1211 16:29:41.385468 13896 layer_factory.cpp:58] Creating layer bn_conv11
I1211 16:29:41.385468 13896 net.cpp:84] Creating Layer bn_conv11
I1211 16:29:41.385468 13896 net.cpp:406] bn_conv11 <- conv11
I1211 16:29:41.385468 13896 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 16:29:41.385468 13896 net.cpp:122] Setting up bn_conv11
I1211 16:29:41.385468 13896 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 16:29:41.385468 13896 net.cpp:137] Memory required for data: 470529200
I1211 16:29:41.385468 13896 layer_factory.cpp:58] Creating layer scale_conv11
I1211 16:29:41.385468 13896 net.cpp:84] Creating Layer scale_conv11
I1211 16:29:41.385468 13896 net.cpp:406] scale_conv11 <- conv11
I1211 16:29:41.385468 13896 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 16:29:41.385468 13896 layer_factory.cpp:58] Creating layer scale_conv11
I1211 16:29:41.385468 13896 net.cpp:122] Setting up scale_conv11
I1211 16:29:41.385468 13896 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 16:29:41.385468 13896 net.cpp:137] Memory required for data: 472321200
I1211 16:29:41.385468 13896 layer_factory.cpp:58] Creating layer relu_conv11
I1211 16:29:41.385468 13896 net.cpp:84] Creating Layer relu_conv11
I1211 16:29:41.385468 13896 net.cpp:406] relu_conv11 <- conv11
I1211 16:29:41.385468 13896 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 16:29:41.385468 13896 net.cpp:122] Setting up relu_conv11
I1211 16:29:41.385468 13896 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 16:29:41.385468 13896 net.cpp:137] Memory required for data: 474113200
I1211 16:29:41.385468 13896 layer_factory.cpp:58] Creating layer conv12
I1211 16:29:41.385468 13896 net.cpp:84] Creating Layer conv12
I1211 16:29:41.385468 13896 net.cpp:406] conv12 <- conv11
I1211 16:29:41.385468 13896 net.cpp:380] conv12 -> conv12
I1211 16:29:41.387468 13896 net.cpp:122] Setting up conv12
I1211 16:29:41.387468 13896 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 16:29:41.387468 13896 net.cpp:137] Memory required for data: 476417200
I1211 16:29:41.387468 13896 layer_factory.cpp:58] Creating layer bn_conv12
I1211 16:29:41.387468 13896 net.cpp:84] Creating Layer bn_conv12
I1211 16:29:41.387468 13896 net.cpp:406] bn_conv12 <- conv12
I1211 16:29:41.387468 13896 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 16:29:41.387468 13896 net.cpp:122] Setting up bn_conv12
I1211 16:29:41.387468 13896 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 16:29:41.387468 13896 net.cpp:137] Memory required for data: 478721200
I1211 16:29:41.387468 13896 layer_factory.cpp:58] Creating layer scale_conv12
I1211 16:29:41.387468 13896 net.cpp:84] Creating Layer scale_conv12
I1211 16:29:41.387468 13896 net.cpp:406] scale_conv12 <- conv12
I1211 16:29:41.387468 13896 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 16:29:41.387468 13896 layer_factory.cpp:58] Creating layer scale_conv12
I1211 16:29:41.387468 13896 net.cpp:122] Setting up scale_conv12
I1211 16:29:41.388468 13896 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 16:29:41.388468 13896 net.cpp:137] Memory required for data: 481025200
I1211 16:29:41.388468 13896 layer_factory.cpp:58] Creating layer relu_conv12
I1211 16:29:41.388468 13896 net.cpp:84] Creating Layer relu_conv12
I1211 16:29:41.388468 13896 net.cpp:406] relu_conv12 <- conv12
I1211 16:29:41.388468 13896 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 16:29:41.388468 13896 net.cpp:122] Setting up relu_conv12
I1211 16:29:41.388468 13896 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 16:29:41.388468 13896 net.cpp:137] Memory required for data: 483329200
I1211 16:29:41.388468 13896 layer_factory.cpp:58] Creating layer poolcp6
I1211 16:29:41.388468 13896 net.cpp:84] Creating Layer poolcp6
I1211 16:29:41.388468 13896 net.cpp:406] poolcp6 <- conv12
I1211 16:29:41.388468 13896 net.cpp:380] poolcp6 -> poolcp6
I1211 16:29:41.388468 13896 net.cpp:122] Setting up poolcp6
I1211 16:29:41.388468 13896 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 16:29:41.388468 13896 net.cpp:137] Memory required for data: 483365200
I1211 16:29:41.388468 13896 layer_factory.cpp:58] Creating layer ip1
I1211 16:29:41.388468 13896 net.cpp:84] Creating Layer ip1
I1211 16:29:41.388468 13896 net.cpp:406] ip1 <- poolcp6
I1211 16:29:41.388468 13896 net.cpp:380] ip1 -> ip1
I1211 16:29:41.388468 13896 net.cpp:122] Setting up ip1
I1211 16:29:41.388468 13896 net.cpp:129] Top shape: 100 100 (10000)
I1211 16:29:41.388468 13896 net.cpp:137] Memory required for data: 483405200
I1211 16:29:41.388468 13896 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 16:29:41.388468 13896 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 16:29:41.388468 13896 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 16:29:41.388468 13896 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 16:29:41.388468 13896 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 16:29:41.388468 13896 net.cpp:122] Setting up ip1_ip1_0_split
I1211 16:29:41.388468 13896 net.cpp:129] Top shape: 100 100 (10000)
I1211 16:29:41.388468 13896 net.cpp:129] Top shape: 100 100 (10000)
I1211 16:29:41.388468 13896 net.cpp:137] Memory required for data: 483485200
I1211 16:29:41.388468 13896 layer_factory.cpp:58] Creating layer accuracy_training
I1211 16:29:41.388468 13896 net.cpp:84] Creating Layer accuracy_training
I1211 16:29:41.388468 13896 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1211 16:29:41.388468 13896 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1211 16:29:41.388468 13896 net.cpp:380] accuracy_training -> accuracy_training
I1211 16:29:41.388468 13896 net.cpp:122] Setting up accuracy_training
I1211 16:29:41.388468 13896 net.cpp:129] Top shape: (1)
I1211 16:29:41.388468 13896 net.cpp:137] Memory required for data: 483485204
I1211 16:29:41.388468 13896 layer_factory.cpp:58] Creating layer loss
I1211 16:29:41.388468 13896 net.cpp:84] Creating Layer loss
I1211 16:29:41.388468 13896 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 16:29:41.388468 13896 net.cpp:406] loss <- label_cifar_1_split_1
I1211 16:29:41.388468 13896 net.cpp:380] loss -> loss
I1211 16:29:41.388468 13896 layer_factory.cpp:58] Creating layer loss
I1211 16:29:41.389468 13896 net.cpp:122] Setting up loss
I1211 16:29:41.389468 13896 net.cpp:129] Top shape: (1)
I1211 16:29:41.389468 13896 net.cpp:132]     with loss weight 1
I1211 16:29:41.389468 13896 net.cpp:137] Memory required for data: 483485208
I1211 16:29:41.389468 13896 net.cpp:198] loss needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:200] accuracy_training does not need backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] ip1 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] poolcp6 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] relu_conv12 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] scale_conv12 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] bn_conv12 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] conv12 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] relu_conv11 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] scale_conv11 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] bn_conv11 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] conv11 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] relu4_0 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] scale4_0 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] bn4_0 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] conv4_0 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] relu4_pool4_2 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] scale4_pool4_2 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] bn4_pool4_2 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] pool4_2 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] relu4_2 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] scale4_2 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] bn4_2 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] conv4_2 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] relu4_1 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] scale4_1 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] bn4_1 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] conv4_1 needs backward computation.
I1211 16:29:41.389468 13896 net.cpp:198] relu4 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] scale4 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] bn4 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] conv4 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] relu3_1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] scale3_1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] bn3_1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] conv3_1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] relu3 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] scale3 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] bn3 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] conv3 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] relu2_pool2_1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] scale2_pool2_1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] bn2_pool2_1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] pool2_1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] relu2_2 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] scale2_2 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] bn2_2 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] conv2_2 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] relu2_1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] scale2_1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] bn2_1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] conv2_1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] relu2 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] scale2 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] bn2 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] conv2 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] relu1_0 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] scale1_0 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] bn1_0 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] conv1_0 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] relu1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] scale1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] bn1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:198] conv1 needs backward computation.
I1211 16:29:41.390470 13896 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 16:29:41.390470 13896 net.cpp:200] cifar does not need backward computation.
I1211 16:29:41.390470 13896 net.cpp:242] This network produces output accuracy_training
I1211 16:29:41.390470 13896 net.cpp:242] This network produces output loss
I1211 16:29:41.390470 13896 net.cpp:255] Network initialization done.
I1211 16:29:41.391469 13896 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 16:29:41.391469 13896 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 16:29:41.391469 13896 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_pool2_1
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_pool4_2
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1211 16:29:41.391469 13896 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1211 16:29:41.391469 13896 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV2_WnonLin_360k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "pool2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_pool2_1"
  type: "BatchNorm"
  bottom: "pool2_1"
  top: "pool2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_pool2_1"
  type: "Scale"
  bottom: "pool2_1"
  top: "pool2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_pool2_1"
  type: "ReLU"
  bottom: "pool2_1"
  top: "pool2_1"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "pool4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_pool4_2"
  type: "BatchNorm"
  bottom: "pool4_2"
  top: "pool4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_pool4_2"
  type: "Scale"
  bottom: "pool4_2"
  top: "pool4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_pool4_2"
  type: "ReLU"
  bottom: "pool4_2"
  top: "pool4_2"
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 16:29:41.391469 13896 layer_factory.cpp:58] Creating layer cifar
I1211 16:29:41.397449 13896 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1211 16:29:41.398460 13896 net.cpp:84] Creating Layer cifar
I1211 16:29:41.398460 13896 net.cpp:380] cifar -> data
I1211 16:29:41.398460 13896 net.cpp:380] cifar -> label
I1211 16:29:41.398460 13896 data_layer.cpp:45] output data size: 100,3,32,32
I1211 16:29:41.407450 13896 net.cpp:122] Setting up cifar
I1211 16:29:41.407450 13896 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 16:29:41.407450 13896 net.cpp:129] Top shape: 100 (100)
I1211 16:29:41.407450 13896 net.cpp:137] Memory required for data: 1229200
I1211 16:29:41.407450 13896 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 16:29:41.407450 13896 net.cpp:84] Creating Layer label_cifar_1_split
I1211 16:29:41.407450 13896 net.cpp:406] label_cifar_1_split <- label
I1211 16:29:41.407450 13896 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 16:29:41.407450 13896 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 16:29:41.407450 13896 net.cpp:122] Setting up label_cifar_1_split
I1211 16:29:41.407450 13896 net.cpp:129] Top shape: 100 (100)
I1211 16:29:41.407450 13896 net.cpp:129] Top shape: 100 (100)
I1211 16:29:41.407450 13896 net.cpp:137] Memory required for data: 1230000
I1211 16:29:41.407450 13896 layer_factory.cpp:58] Creating layer conv1
I1211 16:29:41.407450 13896 net.cpp:84] Creating Layer conv1
I1211 16:29:41.407450 13896 net.cpp:406] conv1 <- data
I1211 16:29:41.407450 13896 net.cpp:380] conv1 -> conv1
I1211 16:29:41.408464  6412 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 16:29:41.409469 13896 net.cpp:122] Setting up conv1
I1211 16:29:41.409469 13896 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 16:29:41.409469 13896 net.cpp:137] Memory required for data: 13518000
I1211 16:29:41.409469 13896 layer_factory.cpp:58] Creating layer bn1
I1211 16:29:41.409469 13896 net.cpp:84] Creating Layer bn1
I1211 16:29:41.409469 13896 net.cpp:406] bn1 <- conv1
I1211 16:29:41.409469 13896 net.cpp:367] bn1 -> conv1 (in-place)
I1211 16:29:41.409469 13896 net.cpp:122] Setting up bn1
I1211 16:29:41.409469 13896 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 16:29:41.409469 13896 net.cpp:137] Memory required for data: 25806000
I1211 16:29:41.409469 13896 layer_factory.cpp:58] Creating layer scale1
I1211 16:29:41.409469 13896 net.cpp:84] Creating Layer scale1
I1211 16:29:41.409469 13896 net.cpp:406] scale1 <- conv1
I1211 16:29:41.409469 13896 net.cpp:367] scale1 -> conv1 (in-place)
I1211 16:29:41.409469 13896 layer_factory.cpp:58] Creating layer scale1
I1211 16:29:41.410465 13896 net.cpp:122] Setting up scale1
I1211 16:29:41.410465 13896 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 16:29:41.410465 13896 net.cpp:137] Memory required for data: 38094000
I1211 16:29:41.410465 13896 layer_factory.cpp:58] Creating layer relu1
I1211 16:29:41.410465 13896 net.cpp:84] Creating Layer relu1
I1211 16:29:41.410465 13896 net.cpp:406] relu1 <- conv1
I1211 16:29:41.410465 13896 net.cpp:367] relu1 -> conv1 (in-place)
I1211 16:29:41.410465 13896 net.cpp:122] Setting up relu1
I1211 16:29:41.410465 13896 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 16:29:41.410465 13896 net.cpp:137] Memory required for data: 50382000
I1211 16:29:41.410465 13896 layer_factory.cpp:58] Creating layer conv1_0
I1211 16:29:41.410465 13896 net.cpp:84] Creating Layer conv1_0
I1211 16:29:41.410465 13896 net.cpp:406] conv1_0 <- conv1
I1211 16:29:41.410465 13896 net.cpp:380] conv1_0 -> conv1_0
I1211 16:29:41.412464 13896 net.cpp:122] Setting up conv1_0
I1211 16:29:41.412464 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.412464 13896 net.cpp:137] Memory required for data: 66766000
I1211 16:29:41.412464 13896 layer_factory.cpp:58] Creating layer bn1_0
I1211 16:29:41.412464 13896 net.cpp:84] Creating Layer bn1_0
I1211 16:29:41.412464 13896 net.cpp:406] bn1_0 <- conv1_0
I1211 16:29:41.412464 13896 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 16:29:41.412464 13896 net.cpp:122] Setting up bn1_0
I1211 16:29:41.412464 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.412464 13896 net.cpp:137] Memory required for data: 83150000
I1211 16:29:41.412464 13896 layer_factory.cpp:58] Creating layer scale1_0
I1211 16:29:41.412464 13896 net.cpp:84] Creating Layer scale1_0
I1211 16:29:41.412464 13896 net.cpp:406] scale1_0 <- conv1_0
I1211 16:29:41.412464 13896 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 16:29:41.412464 13896 layer_factory.cpp:58] Creating layer scale1_0
I1211 16:29:41.412464 13896 net.cpp:122] Setting up scale1_0
I1211 16:29:41.412464 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.412464 13896 net.cpp:137] Memory required for data: 99534000
I1211 16:29:41.412464 13896 layer_factory.cpp:58] Creating layer relu1_0
I1211 16:29:41.412464 13896 net.cpp:84] Creating Layer relu1_0
I1211 16:29:41.412464 13896 net.cpp:406] relu1_0 <- conv1_0
I1211 16:29:41.412464 13896 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 16:29:41.412464 13896 net.cpp:122] Setting up relu1_0
I1211 16:29:41.412464 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.412464 13896 net.cpp:137] Memory required for data: 115918000
I1211 16:29:41.412464 13896 layer_factory.cpp:58] Creating layer conv2
I1211 16:29:41.412464 13896 net.cpp:84] Creating Layer conv2
I1211 16:29:41.412464 13896 net.cpp:406] conv2 <- conv1_0
I1211 16:29:41.412464 13896 net.cpp:380] conv2 -> conv2
I1211 16:29:41.414464 13896 net.cpp:122] Setting up conv2
I1211 16:29:41.414464 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.414464 13896 net.cpp:137] Memory required for data: 132302000
I1211 16:29:41.414464 13896 layer_factory.cpp:58] Creating layer bn2
I1211 16:29:41.414464 13896 net.cpp:84] Creating Layer bn2
I1211 16:29:41.414464 13896 net.cpp:406] bn2 <- conv2
I1211 16:29:41.414464 13896 net.cpp:367] bn2 -> conv2 (in-place)
I1211 16:29:41.414464 13896 net.cpp:122] Setting up bn2
I1211 16:29:41.414464 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.414464 13896 net.cpp:137] Memory required for data: 148686000
I1211 16:29:41.414464 13896 layer_factory.cpp:58] Creating layer scale2
I1211 16:29:41.414464 13896 net.cpp:84] Creating Layer scale2
I1211 16:29:41.414464 13896 net.cpp:406] scale2 <- conv2
I1211 16:29:41.414464 13896 net.cpp:367] scale2 -> conv2 (in-place)
I1211 16:29:41.414464 13896 layer_factory.cpp:58] Creating layer scale2
I1211 16:29:41.414464 13896 net.cpp:122] Setting up scale2
I1211 16:29:41.414464 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.414464 13896 net.cpp:137] Memory required for data: 165070000
I1211 16:29:41.414464 13896 layer_factory.cpp:58] Creating layer relu2
I1211 16:29:41.414464 13896 net.cpp:84] Creating Layer relu2
I1211 16:29:41.414464 13896 net.cpp:406] relu2 <- conv2
I1211 16:29:41.414464 13896 net.cpp:367] relu2 -> conv2 (in-place)
I1211 16:29:41.414464 13896 net.cpp:122] Setting up relu2
I1211 16:29:41.414464 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.414464 13896 net.cpp:137] Memory required for data: 181454000
I1211 16:29:41.414464 13896 layer_factory.cpp:58] Creating layer conv2_1
I1211 16:29:41.414464 13896 net.cpp:84] Creating Layer conv2_1
I1211 16:29:41.414464 13896 net.cpp:406] conv2_1 <- conv2
I1211 16:29:41.414464 13896 net.cpp:380] conv2_1 -> conv2_1
I1211 16:29:41.416465 13896 net.cpp:122] Setting up conv2_1
I1211 16:29:41.416465 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.416465 13896 net.cpp:137] Memory required for data: 197838000
I1211 16:29:41.416465 13896 layer_factory.cpp:58] Creating layer bn2_1
I1211 16:29:41.416465 13896 net.cpp:84] Creating Layer bn2_1
I1211 16:29:41.416465 13896 net.cpp:406] bn2_1 <- conv2_1
I1211 16:29:41.416465 13896 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 16:29:41.416465 13896 net.cpp:122] Setting up bn2_1
I1211 16:29:41.416465 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.416465 13896 net.cpp:137] Memory required for data: 214222000
I1211 16:29:41.416465 13896 layer_factory.cpp:58] Creating layer scale2_1
I1211 16:29:41.416465 13896 net.cpp:84] Creating Layer scale2_1
I1211 16:29:41.416465 13896 net.cpp:406] scale2_1 <- conv2_1
I1211 16:29:41.416465 13896 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 16:29:41.416465 13896 layer_factory.cpp:58] Creating layer scale2_1
I1211 16:29:41.416465 13896 net.cpp:122] Setting up scale2_1
I1211 16:29:41.416465 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.416465 13896 net.cpp:137] Memory required for data: 230606000
I1211 16:29:41.416465 13896 layer_factory.cpp:58] Creating layer relu2_1
I1211 16:29:41.416465 13896 net.cpp:84] Creating Layer relu2_1
I1211 16:29:41.416465 13896 net.cpp:406] relu2_1 <- conv2_1
I1211 16:29:41.416465 13896 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 16:29:41.417464 13896 net.cpp:122] Setting up relu2_1
I1211 16:29:41.417464 13896 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 16:29:41.417464 13896 net.cpp:137] Memory required for data: 246990000
I1211 16:29:41.417464 13896 layer_factory.cpp:58] Creating layer conv2_2
I1211 16:29:41.417464 13896 net.cpp:84] Creating Layer conv2_2
I1211 16:29:41.417464 13896 net.cpp:406] conv2_2 <- conv2_1
I1211 16:29:41.417464 13896 net.cpp:380] conv2_2 -> conv2_2
I1211 16:29:41.418468 13896 net.cpp:122] Setting up conv2_2
I1211 16:29:41.418468 13896 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 16:29:41.418468 13896 net.cpp:137] Memory required for data: 267470000
I1211 16:29:41.418468 13896 layer_factory.cpp:58] Creating layer bn2_2
I1211 16:29:41.418468 13896 net.cpp:84] Creating Layer bn2_2
I1211 16:29:41.418468 13896 net.cpp:406] bn2_2 <- conv2_2
I1211 16:29:41.418468 13896 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 16:29:41.418468 13896 net.cpp:122] Setting up bn2_2
I1211 16:29:41.418468 13896 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 16:29:41.418468 13896 net.cpp:137] Memory required for data: 287950000
I1211 16:29:41.418468 13896 layer_factory.cpp:58] Creating layer scale2_2
I1211 16:29:41.418468 13896 net.cpp:84] Creating Layer scale2_2
I1211 16:29:41.418468 13896 net.cpp:406] scale2_2 <- conv2_2
I1211 16:29:41.418468 13896 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 16:29:41.419468 13896 layer_factory.cpp:58] Creating layer scale2_2
I1211 16:29:41.419468 13896 net.cpp:122] Setting up scale2_2
I1211 16:29:41.419468 13896 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 16:29:41.419468 13896 net.cpp:137] Memory required for data: 308430000
I1211 16:29:41.419468 13896 layer_factory.cpp:58] Creating layer relu2_2
I1211 16:29:41.419468 13896 net.cpp:84] Creating Layer relu2_2
I1211 16:29:41.419468 13896 net.cpp:406] relu2_2 <- conv2_2
I1211 16:29:41.419468 13896 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 16:29:41.419468 13896 net.cpp:122] Setting up relu2_2
I1211 16:29:41.419468 13896 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 16:29:41.419468 13896 net.cpp:137] Memory required for data: 328910000
I1211 16:29:41.419468 13896 layer_factory.cpp:58] Creating layer pool2_1
I1211 16:29:41.419468 13896 net.cpp:84] Creating Layer pool2_1
I1211 16:29:41.419468 13896 net.cpp:406] pool2_1 <- conv2_2
I1211 16:29:41.419468 13896 net.cpp:380] pool2_1 -> pool2_1
I1211 16:29:41.420469 13896 net.cpp:122] Setting up pool2_1
I1211 16:29:41.421468 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.421468 13896 net.cpp:137] Memory required for data: 334030000
I1211 16:29:41.421468 13896 layer_factory.cpp:58] Creating layer bn2_pool2_1
I1211 16:29:41.421468 13896 net.cpp:84] Creating Layer bn2_pool2_1
I1211 16:29:41.421468 13896 net.cpp:406] bn2_pool2_1 <- pool2_1
I1211 16:29:41.421468 13896 net.cpp:367] bn2_pool2_1 -> pool2_1 (in-place)
I1211 16:29:41.421468 13896 net.cpp:122] Setting up bn2_pool2_1
I1211 16:29:41.421468 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.421468 13896 net.cpp:137] Memory required for data: 339150000
I1211 16:29:41.421468 13896 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 16:29:41.421468 13896 net.cpp:84] Creating Layer scale2_pool2_1
I1211 16:29:41.421468 13896 net.cpp:406] scale2_pool2_1 <- pool2_1
I1211 16:29:41.421468 13896 net.cpp:367] scale2_pool2_1 -> pool2_1 (in-place)
I1211 16:29:41.421468 13896 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 16:29:41.421468 13896 net.cpp:122] Setting up scale2_pool2_1
I1211 16:29:41.421468 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.421468 13896 net.cpp:137] Memory required for data: 344270000
I1211 16:29:41.421468 13896 layer_factory.cpp:58] Creating layer relu2_pool2_1
I1211 16:29:41.421468 13896 net.cpp:84] Creating Layer relu2_pool2_1
I1211 16:29:41.421468 13896 net.cpp:406] relu2_pool2_1 <- pool2_1
I1211 16:29:41.421468 13896 net.cpp:367] relu2_pool2_1 -> pool2_1 (in-place)
I1211 16:29:41.422451 13896 net.cpp:122] Setting up relu2_pool2_1
I1211 16:29:41.422451 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.422451 13896 net.cpp:137] Memory required for data: 349390000
I1211 16:29:41.422451 13896 layer_factory.cpp:58] Creating layer conv3
I1211 16:29:41.422451 13896 net.cpp:84] Creating Layer conv3
I1211 16:29:41.422451 13896 net.cpp:406] conv3 <- pool2_1
I1211 16:29:41.422451 13896 net.cpp:380] conv3 -> conv3
I1211 16:29:41.423450 13896 net.cpp:122] Setting up conv3
I1211 16:29:41.423450 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.423450 13896 net.cpp:137] Memory required for data: 354510000
I1211 16:29:41.423450 13896 layer_factory.cpp:58] Creating layer bn3
I1211 16:29:41.423450 13896 net.cpp:84] Creating Layer bn3
I1211 16:29:41.423450 13896 net.cpp:406] bn3 <- conv3
I1211 16:29:41.423450 13896 net.cpp:367] bn3 -> conv3 (in-place)
I1211 16:29:41.423450 13896 net.cpp:122] Setting up bn3
I1211 16:29:41.423450 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.423450 13896 net.cpp:137] Memory required for data: 359630000
I1211 16:29:41.423450 13896 layer_factory.cpp:58] Creating layer scale3
I1211 16:29:41.423450 13896 net.cpp:84] Creating Layer scale3
I1211 16:29:41.423450 13896 net.cpp:406] scale3 <- conv3
I1211 16:29:41.423450 13896 net.cpp:367] scale3 -> conv3 (in-place)
I1211 16:29:41.423450 13896 layer_factory.cpp:58] Creating layer scale3
I1211 16:29:41.423450 13896 net.cpp:122] Setting up scale3
I1211 16:29:41.423450 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.423450 13896 net.cpp:137] Memory required for data: 364750000
I1211 16:29:41.423450 13896 layer_factory.cpp:58] Creating layer relu3
I1211 16:29:41.423450 13896 net.cpp:84] Creating Layer relu3
I1211 16:29:41.423450 13896 net.cpp:406] relu3 <- conv3
I1211 16:29:41.423450 13896 net.cpp:367] relu3 -> conv3 (in-place)
I1211 16:29:41.424450 13896 net.cpp:122] Setting up relu3
I1211 16:29:41.424450 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.424450 13896 net.cpp:137] Memory required for data: 369870000
I1211 16:29:41.424450 13896 layer_factory.cpp:58] Creating layer conv3_1
I1211 16:29:41.424450 13896 net.cpp:84] Creating Layer conv3_1
I1211 16:29:41.424450 13896 net.cpp:406] conv3_1 <- conv3
I1211 16:29:41.424450 13896 net.cpp:380] conv3_1 -> conv3_1
I1211 16:29:41.425468 13896 net.cpp:122] Setting up conv3_1
I1211 16:29:41.425468 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.425468 13896 net.cpp:137] Memory required for data: 374990000
I1211 16:29:41.425468 13896 layer_factory.cpp:58] Creating layer bn3_1
I1211 16:29:41.425468 13896 net.cpp:84] Creating Layer bn3_1
I1211 16:29:41.425468 13896 net.cpp:406] bn3_1 <- conv3_1
I1211 16:29:41.425468 13896 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 16:29:41.426465 13896 net.cpp:122] Setting up bn3_1
I1211 16:29:41.426465 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.426465 13896 net.cpp:137] Memory required for data: 380110000
I1211 16:29:41.426465 13896 layer_factory.cpp:58] Creating layer scale3_1
I1211 16:29:41.426465 13896 net.cpp:84] Creating Layer scale3_1
I1211 16:29:41.426465 13896 net.cpp:406] scale3_1 <- conv3_1
I1211 16:29:41.426465 13896 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 16:29:41.426465 13896 layer_factory.cpp:58] Creating layer scale3_1
I1211 16:29:41.426465 13896 net.cpp:122] Setting up scale3_1
I1211 16:29:41.426465 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.426465 13896 net.cpp:137] Memory required for data: 385230000
I1211 16:29:41.426465 13896 layer_factory.cpp:58] Creating layer relu3_1
I1211 16:29:41.426465 13896 net.cpp:84] Creating Layer relu3_1
I1211 16:29:41.426465 13896 net.cpp:406] relu3_1 <- conv3_1
I1211 16:29:41.426465 13896 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 16:29:41.426465 13896 net.cpp:122] Setting up relu3_1
I1211 16:29:41.426465 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.426465 13896 net.cpp:137] Memory required for data: 390350000
I1211 16:29:41.426465 13896 layer_factory.cpp:58] Creating layer conv4
I1211 16:29:41.426465 13896 net.cpp:84] Creating Layer conv4
I1211 16:29:41.426465 13896 net.cpp:406] conv4 <- conv3_1
I1211 16:29:41.426465 13896 net.cpp:380] conv4 -> conv4
I1211 16:29:41.428470 13896 net.cpp:122] Setting up conv4
I1211 16:29:41.428470 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.428470 13896 net.cpp:137] Memory required for data: 395470000
I1211 16:29:41.428470 13896 layer_factory.cpp:58] Creating layer bn4
I1211 16:29:41.428470 13896 net.cpp:84] Creating Layer bn4
I1211 16:29:41.428470 13896 net.cpp:406] bn4 <- conv4
I1211 16:29:41.428470 13896 net.cpp:367] bn4 -> conv4 (in-place)
I1211 16:29:41.428470 13896 net.cpp:122] Setting up bn4
I1211 16:29:41.428470 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.428470 13896 net.cpp:137] Memory required for data: 400590000
I1211 16:29:41.428470 13896 layer_factory.cpp:58] Creating layer scale4
I1211 16:29:41.428470 13896 net.cpp:84] Creating Layer scale4
I1211 16:29:41.428470 13896 net.cpp:406] scale4 <- conv4
I1211 16:29:41.428470 13896 net.cpp:367] scale4 -> conv4 (in-place)
I1211 16:29:41.428470 13896 layer_factory.cpp:58] Creating layer scale4
I1211 16:29:41.428470 13896 net.cpp:122] Setting up scale4
I1211 16:29:41.428470 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.428470 13896 net.cpp:137] Memory required for data: 405710000
I1211 16:29:41.428470 13896 layer_factory.cpp:58] Creating layer relu4
I1211 16:29:41.428470 13896 net.cpp:84] Creating Layer relu4
I1211 16:29:41.428470 13896 net.cpp:406] relu4 <- conv4
I1211 16:29:41.428470 13896 net.cpp:367] relu4 -> conv4 (in-place)
I1211 16:29:41.429481 13896 net.cpp:122] Setting up relu4
I1211 16:29:41.429481 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.429481 13896 net.cpp:137] Memory required for data: 410830000
I1211 16:29:41.429481 13896 layer_factory.cpp:58] Creating layer conv4_1
I1211 16:29:41.429481 13896 net.cpp:84] Creating Layer conv4_1
I1211 16:29:41.429481 13896 net.cpp:406] conv4_1 <- conv4
I1211 16:29:41.429481 13896 net.cpp:380] conv4_1 -> conv4_1
I1211 16:29:41.430507 13896 net.cpp:122] Setting up conv4_1
I1211 16:29:41.430507 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.430507 13896 net.cpp:137] Memory required for data: 415950000
I1211 16:29:41.430507 13896 layer_factory.cpp:58] Creating layer bn4_1
I1211 16:29:41.430507 13896 net.cpp:84] Creating Layer bn4_1
I1211 16:29:41.430507 13896 net.cpp:406] bn4_1 <- conv4_1
I1211 16:29:41.430507 13896 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 16:29:41.431001 13896 net.cpp:122] Setting up bn4_1
I1211 16:29:41.431001 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.431001 13896 net.cpp:137] Memory required for data: 421070000
I1211 16:29:41.431001 13896 layer_factory.cpp:58] Creating layer scale4_1
I1211 16:29:41.431001 13896 net.cpp:84] Creating Layer scale4_1
I1211 16:29:41.431001 13896 net.cpp:406] scale4_1 <- conv4_1
I1211 16:29:41.431001 13896 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 16:29:41.431001 13896 layer_factory.cpp:58] Creating layer scale4_1
I1211 16:29:41.431001 13896 net.cpp:122] Setting up scale4_1
I1211 16:29:41.431001 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.431001 13896 net.cpp:137] Memory required for data: 426190000
I1211 16:29:41.431001 13896 layer_factory.cpp:58] Creating layer relu4_1
I1211 16:29:41.431001 13896 net.cpp:84] Creating Layer relu4_1
I1211 16:29:41.431001 13896 net.cpp:406] relu4_1 <- conv4_1
I1211 16:29:41.431001 13896 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 16:29:41.431506 13896 net.cpp:122] Setting up relu4_1
I1211 16:29:41.431506 13896 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 16:29:41.431506 13896 net.cpp:137] Memory required for data: 431310000
I1211 16:29:41.431506 13896 layer_factory.cpp:58] Creating layer conv4_2
I1211 16:29:41.431506 13896 net.cpp:84] Creating Layer conv4_2
I1211 16:29:41.431506 13896 net.cpp:406] conv4_2 <- conv4_1
I1211 16:29:41.431506 13896 net.cpp:380] conv4_2 -> conv4_2
I1211 16:29:41.433007 13896 net.cpp:122] Setting up conv4_2
I1211 16:29:41.433007 13896 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 16:29:41.433007 13896 net.cpp:137] Memory required for data: 437249200
I1211 16:29:41.433007 13896 layer_factory.cpp:58] Creating layer bn4_2
I1211 16:29:41.433007 13896 net.cpp:84] Creating Layer bn4_2
I1211 16:29:41.433007 13896 net.cpp:406] bn4_2 <- conv4_2
I1211 16:29:41.433007 13896 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 16:29:41.433007 13896 net.cpp:122] Setting up bn4_2
I1211 16:29:41.433007 13896 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 16:29:41.433007 13896 net.cpp:137] Memory required for data: 443188400
I1211 16:29:41.433007 13896 layer_factory.cpp:58] Creating layer scale4_2
I1211 16:29:41.433007 13896 net.cpp:84] Creating Layer scale4_2
I1211 16:29:41.433007 13896 net.cpp:406] scale4_2 <- conv4_2
I1211 16:29:41.433007 13896 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 16:29:41.433007 13896 layer_factory.cpp:58] Creating layer scale4_2
I1211 16:29:41.433502 13896 net.cpp:122] Setting up scale4_2
I1211 16:29:41.433502 13896 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 16:29:41.433502 13896 net.cpp:137] Memory required for data: 449127600
I1211 16:29:41.433502 13896 layer_factory.cpp:58] Creating layer relu4_2
I1211 16:29:41.433502 13896 net.cpp:84] Creating Layer relu4_2
I1211 16:29:41.433502 13896 net.cpp:406] relu4_2 <- conv4_2
I1211 16:29:41.433502 13896 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 16:29:41.433502 13896 net.cpp:122] Setting up relu4_2
I1211 16:29:41.433502 13896 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 16:29:41.433502 13896 net.cpp:137] Memory required for data: 455066800
I1211 16:29:41.433502 13896 layer_factory.cpp:58] Creating layer pool4_2
I1211 16:29:41.433502 13896 net.cpp:84] Creating Layer pool4_2
I1211 16:29:41.433502 13896 net.cpp:406] pool4_2 <- conv4_2
I1211 16:29:41.433502 13896 net.cpp:380] pool4_2 -> pool4_2
I1211 16:29:41.434986 13896 net.cpp:122] Setting up pool4_2
I1211 16:29:41.434986 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.434986 13896 net.cpp:137] Memory required for data: 456551600
I1211 16:29:41.434986 13896 layer_factory.cpp:58] Creating layer bn4_pool4_2
I1211 16:29:41.434986 13896 net.cpp:84] Creating Layer bn4_pool4_2
I1211 16:29:41.434986 13896 net.cpp:406] bn4_pool4_2 <- pool4_2
I1211 16:29:41.434986 13896 net.cpp:367] bn4_pool4_2 -> pool4_2 (in-place)
I1211 16:29:41.435487 13896 net.cpp:122] Setting up bn4_pool4_2
I1211 16:29:41.435487 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.435487 13896 net.cpp:137] Memory required for data: 458036400
I1211 16:29:41.435487 13896 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 16:29:41.435487 13896 net.cpp:84] Creating Layer scale4_pool4_2
I1211 16:29:41.435487 13896 net.cpp:406] scale4_pool4_2 <- pool4_2
I1211 16:29:41.435487 13896 net.cpp:367] scale4_pool4_2 -> pool4_2 (in-place)
I1211 16:29:41.435487 13896 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 16:29:41.435487 13896 net.cpp:122] Setting up scale4_pool4_2
I1211 16:29:41.435487 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.435487 13896 net.cpp:137] Memory required for data: 459521200
I1211 16:29:41.435487 13896 layer_factory.cpp:58] Creating layer relu4_pool4_2
I1211 16:29:41.435487 13896 net.cpp:84] Creating Layer relu4_pool4_2
I1211 16:29:41.435487 13896 net.cpp:406] relu4_pool4_2 <- pool4_2
I1211 16:29:41.435487 13896 net.cpp:367] relu4_pool4_2 -> pool4_2 (in-place)
I1211 16:29:41.436007 13896 net.cpp:122] Setting up relu4_pool4_2
I1211 16:29:41.436007 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.436007 13896 net.cpp:137] Memory required for data: 461006000
I1211 16:29:41.436007 13896 layer_factory.cpp:58] Creating layer conv4_0
I1211 16:29:41.436007 13896 net.cpp:84] Creating Layer conv4_0
I1211 16:29:41.436007 13896 net.cpp:406] conv4_0 <- pool4_2
I1211 16:29:41.436007 13896 net.cpp:380] conv4_0 -> conv4_0
I1211 16:29:41.437501 13896 net.cpp:122] Setting up conv4_0
I1211 16:29:41.437501 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.437501 13896 net.cpp:137] Memory required for data: 462490800
I1211 16:29:41.437501 13896 layer_factory.cpp:58] Creating layer bn4_0
I1211 16:29:41.437501 13896 net.cpp:84] Creating Layer bn4_0
I1211 16:29:41.438001 13896 net.cpp:406] bn4_0 <- conv4_0
I1211 16:29:41.438001 13896 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 16:29:41.438001 13896 net.cpp:122] Setting up bn4_0
I1211 16:29:41.438001 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.438001 13896 net.cpp:137] Memory required for data: 463975600
I1211 16:29:41.438001 13896 layer_factory.cpp:58] Creating layer scale4_0
I1211 16:29:41.438001 13896 net.cpp:84] Creating Layer scale4_0
I1211 16:29:41.438001 13896 net.cpp:406] scale4_0 <- conv4_0
I1211 16:29:41.438001 13896 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 16:29:41.438001 13896 layer_factory.cpp:58] Creating layer scale4_0
I1211 16:29:41.438001 13896 net.cpp:122] Setting up scale4_0
I1211 16:29:41.438001 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.438001 13896 net.cpp:137] Memory required for data: 465460400
I1211 16:29:41.438001 13896 layer_factory.cpp:58] Creating layer relu4_0
I1211 16:29:41.438001 13896 net.cpp:84] Creating Layer relu4_0
I1211 16:29:41.438001 13896 net.cpp:406] relu4_0 <- conv4_0
I1211 16:29:41.438501 13896 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 16:29:41.439005 13896 net.cpp:122] Setting up relu4_0
I1211 16:29:41.439005 13896 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 16:29:41.439005 13896 net.cpp:137] Memory required for data: 466945200
I1211 16:29:41.439005 13896 layer_factory.cpp:58] Creating layer conv11
I1211 16:29:41.439005 13896 net.cpp:84] Creating Layer conv11
I1211 16:29:41.439005 13896 net.cpp:406] conv11 <- conv4_0
I1211 16:29:41.439005 13896 net.cpp:380] conv11 -> conv11
I1211 16:29:41.441005 13896 net.cpp:122] Setting up conv11
I1211 16:29:41.441005 13896 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 16:29:41.441005 13896 net.cpp:137] Memory required for data: 468737200
I1211 16:29:41.441005 13896 layer_factory.cpp:58] Creating layer bn_conv11
I1211 16:29:41.441005 13896 net.cpp:84] Creating Layer bn_conv11
I1211 16:29:41.441005 13896 net.cpp:406] bn_conv11 <- conv11
I1211 16:29:41.441005 13896 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 16:29:41.441005 13896 net.cpp:122] Setting up bn_conv11
I1211 16:29:41.441005 13896 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 16:29:41.441005 13896 net.cpp:137] Memory required for data: 470529200
I1211 16:29:41.441005 13896 layer_factory.cpp:58] Creating layer scale_conv11
I1211 16:29:41.441005 13896 net.cpp:84] Creating Layer scale_conv11
I1211 16:29:41.441005 13896 net.cpp:406] scale_conv11 <- conv11
I1211 16:29:41.441005 13896 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 16:29:41.441005 13896 layer_factory.cpp:58] Creating layer scale_conv11
I1211 16:29:41.441501 13896 net.cpp:122] Setting up scale_conv11
I1211 16:29:41.441501 13896 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 16:29:41.441501 13896 net.cpp:137] Memory required for data: 472321200
I1211 16:29:41.441501 13896 layer_factory.cpp:58] Creating layer relu_conv11
I1211 16:29:41.441501 13896 net.cpp:84] Creating Layer relu_conv11
I1211 16:29:41.441501 13896 net.cpp:406] relu_conv11 <- conv11
I1211 16:29:41.441501 13896 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 16:29:41.441501 13896 net.cpp:122] Setting up relu_conv11
I1211 16:29:41.441501 13896 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 16:29:41.441501 13896 net.cpp:137] Memory required for data: 474113200
I1211 16:29:41.441501 13896 layer_factory.cpp:58] Creating layer conv12
I1211 16:29:41.441501 13896 net.cpp:84] Creating Layer conv12
I1211 16:29:41.441501 13896 net.cpp:406] conv12 <- conv11
I1211 16:29:41.441501 13896 net.cpp:380] conv12 -> conv12
I1211 16:29:41.443506 13896 net.cpp:122] Setting up conv12
I1211 16:29:41.443506 13896 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 16:29:41.443506 13896 net.cpp:137] Memory required for data: 476417200
I1211 16:29:41.443506 13896 layer_factory.cpp:58] Creating layer bn_conv12
I1211 16:29:41.443506 13896 net.cpp:84] Creating Layer bn_conv12
I1211 16:29:41.443506 13896 net.cpp:406] bn_conv12 <- conv12
I1211 16:29:41.443506 13896 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 16:29:41.443506 13896 net.cpp:122] Setting up bn_conv12
I1211 16:29:41.443506 13896 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 16:29:41.443506 13896 net.cpp:137] Memory required for data: 478721200
I1211 16:29:41.444001 13896 layer_factory.cpp:58] Creating layer scale_conv12
I1211 16:29:41.444001 13896 net.cpp:84] Creating Layer scale_conv12
I1211 16:29:41.444001 13896 net.cpp:406] scale_conv12 <- conv12
I1211 16:29:41.444001 13896 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 16:29:41.444001 13896 layer_factory.cpp:58] Creating layer scale_conv12
I1211 16:29:41.444001 13896 net.cpp:122] Setting up scale_conv12
I1211 16:29:41.444001 13896 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 16:29:41.444001 13896 net.cpp:137] Memory required for data: 481025200
I1211 16:29:41.444001 13896 layer_factory.cpp:58] Creating layer relu_conv12
I1211 16:29:41.444001 13896 net.cpp:84] Creating Layer relu_conv12
I1211 16:29:41.444001 13896 net.cpp:406] relu_conv12 <- conv12
I1211 16:29:41.444001 13896 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 16:29:41.444001 13896 net.cpp:122] Setting up relu_conv12
I1211 16:29:41.444001 13896 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 16:29:41.444001 13896 net.cpp:137] Memory required for data: 483329200
I1211 16:29:41.444001 13896 layer_factory.cpp:58] Creating layer poolcp6
I1211 16:29:41.444001 13896 net.cpp:84] Creating Layer poolcp6
I1211 16:29:41.444001 13896 net.cpp:406] poolcp6 <- conv12
I1211 16:29:41.444001 13896 net.cpp:380] poolcp6 -> poolcp6
I1211 16:29:41.444501 13896 net.cpp:122] Setting up poolcp6
I1211 16:29:41.444501 13896 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 16:29:41.444501 13896 net.cpp:137] Memory required for data: 483365200
I1211 16:29:41.444501 13896 layer_factory.cpp:58] Creating layer ip1
I1211 16:29:41.444501 13896 net.cpp:84] Creating Layer ip1
I1211 16:29:41.444501 13896 net.cpp:406] ip1 <- poolcp6
I1211 16:29:41.444501 13896 net.cpp:380] ip1 -> ip1
I1211 16:29:41.444501 13896 net.cpp:122] Setting up ip1
I1211 16:29:41.444501 13896 net.cpp:129] Top shape: 100 100 (10000)
I1211 16:29:41.444501 13896 net.cpp:137] Memory required for data: 483405200
I1211 16:29:41.444501 13896 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 16:29:41.444501 13896 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 16:29:41.445015 13896 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 16:29:41.445015 13896 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 16:29:41.445015 13896 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 16:29:41.445015 13896 net.cpp:122] Setting up ip1_ip1_0_split
I1211 16:29:41.445015 13896 net.cpp:129] Top shape: 100 100 (10000)
I1211 16:29:41.445015 13896 net.cpp:129] Top shape: 100 100 (10000)
I1211 16:29:41.445015 13896 net.cpp:137] Memory required for data: 483485200
I1211 16:29:41.445015 13896 layer_factory.cpp:58] Creating layer accuracy
I1211 16:29:41.445015 13896 net.cpp:84] Creating Layer accuracy
I1211 16:29:41.445015 13896 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1211 16:29:41.445015 13896 net.cpp:406] accuracy <- label_cifar_1_split_0
I1211 16:29:41.445015 13896 net.cpp:380] accuracy -> accuracy
I1211 16:29:41.445015 13896 net.cpp:122] Setting up accuracy
I1211 16:29:41.445015 13896 net.cpp:129] Top shape: (1)
I1211 16:29:41.445015 13896 net.cpp:137] Memory required for data: 483485204
I1211 16:29:41.445015 13896 layer_factory.cpp:58] Creating layer loss
I1211 16:29:41.445015 13896 net.cpp:84] Creating Layer loss
I1211 16:29:41.445015 13896 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 16:29:41.445015 13896 net.cpp:406] loss <- label_cifar_1_split_1
I1211 16:29:41.445015 13896 net.cpp:380] loss -> loss
I1211 16:29:41.445015 13896 layer_factory.cpp:58] Creating layer loss
I1211 16:29:41.445015 13896 net.cpp:122] Setting up loss
I1211 16:29:41.445015 13896 net.cpp:129] Top shape: (1)
I1211 16:29:41.445015 13896 net.cpp:132]     with loss weight 1
I1211 16:29:41.445015 13896 net.cpp:137] Memory required for data: 483485208
I1211 16:29:41.445015 13896 net.cpp:198] loss needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:200] accuracy does not need backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] ip1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] poolcp6 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu_conv12 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale_conv12 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn_conv12 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] conv12 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu_conv11 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale_conv11 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn_conv11 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] conv11 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu4_0 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale4_0 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn4_0 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] conv4_0 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu4_pool4_2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale4_pool4_2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn4_pool4_2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] pool4_2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu4_2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale4_2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn4_2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] conv4_2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu4_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale4_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn4_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] conv4_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu4 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale4 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn4 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] conv4 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu3_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale3_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn3_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] conv3_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu3 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale3 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn3 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] conv3 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu2_pool2_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale2_pool2_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn2_pool2_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] pool2_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu2_2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale2_2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn2_2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] conv2_2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu2_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale2_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn2_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] conv2_1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] conv2 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu1_0 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale1_0 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn1_0 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] conv1_0 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] relu1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] scale1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] bn1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:198] conv1 needs backward computation.
I1211 16:29:41.445015 13896 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 16:29:41.445015 13896 net.cpp:200] cifar does not need backward computation.
I1211 16:29:41.445015 13896 net.cpp:242] This network produces output accuracy
I1211 16:29:41.445015 13896 net.cpp:242] This network produces output loss
I1211 16:29:41.445015 13896 net.cpp:255] Network initialization done.
I1211 16:29:41.446192 13896 solver.cpp:56] Solver scaffolding done.
I1211 16:29:41.450187 13896 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90000.solverstate
I1211 16:29:41.544690 13896 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90000.caffemodel
I1211 16:29:41.544690 13896 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 16:29:41.545191 13896 sgd_solver.cpp:318] SGDSolver: restoring history
I1211 16:29:41.548693 13896 caffe.cpp:249] Starting Optimization
I1211 16:29:41.548693 13896 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV2_WnonLin_360k
I1211 16:29:41.548693 13896 solver.cpp:273] Learning Rate Policy: multistep
I1211 16:29:41.551712 13896 solver.cpp:330] Iteration 90000, Testing net (#0)
I1211 16:29:41.553711 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:29:42.954156  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:29:43.007158 13896 solver.cpp:397]     Test net output #0: accuracy = 0.5639
I1211 16:29:43.007158 13896 solver.cpp:397]     Test net output #1: loss = 1.73094 (* 1 = 1.73094 loss)
I1211 16:29:43.120218 13896 solver.cpp:218] Iteration 90000 (57331 iter/s, 1.56983s/100 iters), loss = 0.578607
I1211 16:29:43.120218 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 16:29:43.120218 13896 solver.cpp:237]     Train net output #1: loss = 0.578607 (* 1 = 0.578607 loss)
I1211 16:29:43.120218 13896 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1211 16:29:49.293920 13896 solver.cpp:218] Iteration 90100 (16.1996 iter/s, 6.173s/100 iters), loss = 0.743812
I1211 16:29:49.293920 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 16:29:49.293920 13896 solver.cpp:237]     Train net output #1: loss = 0.743812 (* 1 = 0.743812 loss)
I1211 16:29:49.293920 13896 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1211 16:29:55.477597 13896 solver.cpp:218] Iteration 90200 (16.173 iter/s, 6.18313s/100 iters), loss = 0.613147
I1211 16:29:55.477597 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 16:29:55.477597 13896 solver.cpp:237]     Train net output #1: loss = 0.613147 (* 1 = 0.613147 loss)
I1211 16:29:55.477597 13896 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1211 16:30:01.675680 13896 solver.cpp:218] Iteration 90300 (16.1355 iter/s, 6.19751s/100 iters), loss = 0.768081
I1211 16:30:01.675680 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 16:30:01.675680 13896 solver.cpp:237]     Train net output #1: loss = 0.768081 (* 1 = 0.768081 loss)
I1211 16:30:01.675680 13896 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1211 16:30:07.894773 13896 solver.cpp:218] Iteration 90400 (16.0795 iter/s, 6.21912s/100 iters), loss = 0.757778
I1211 16:30:07.894773 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 16:30:07.894773 13896 solver.cpp:237]     Train net output #1: loss = 0.757778 (* 1 = 0.757778 loss)
I1211 16:30:07.894773 13896 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1211 16:30:13.762281  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:30:14.011297 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90500.caffemodel
I1211 16:30:14.029297 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90500.solverstate
I1211 16:30:14.034804 13896 solver.cpp:330] Iteration 90500, Testing net (#0)
I1211 16:30:14.034804 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:30:15.385437  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:30:15.437940 13896 solver.cpp:397]     Test net output #0: accuracy = 0.584
I1211 16:30:15.437940 13896 solver.cpp:397]     Test net output #1: loss = 1.61302 (* 1 = 1.61302 loss)
I1211 16:30:15.496441 13896 solver.cpp:218] Iteration 90500 (13.1568 iter/s, 7.60064s/100 iters), loss = 0.460632
I1211 16:30:15.496441 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:30:15.496441 13896 solver.cpp:237]     Train net output #1: loss = 0.460632 (* 1 = 0.460632 loss)
I1211 16:30:15.496441 13896 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1211 16:30:21.681218 13896 solver.cpp:218] Iteration 90600 (16.1695 iter/s, 6.18447s/100 iters), loss = 0.647445
I1211 16:30:21.681218 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 16:30:21.681218 13896 solver.cpp:237]     Train net output #1: loss = 0.647445 (* 1 = 0.647445 loss)
I1211 16:30:21.681218 13896 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1211 16:30:27.893981 13896 solver.cpp:218] Iteration 90700 (16.0961 iter/s, 6.21268s/100 iters), loss = 0.607706
I1211 16:30:27.893981 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:30:27.893981 13896 solver.cpp:237]     Train net output #1: loss = 0.607706 (* 1 = 0.607706 loss)
I1211 16:30:27.893981 13896 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1211 16:30:34.083638 13896 solver.cpp:218] Iteration 90800 (16.1571 iter/s, 6.18921s/100 iters), loss = 0.636308
I1211 16:30:34.083638 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 16:30:34.083638 13896 solver.cpp:237]     Train net output #1: loss = 0.636308 (* 1 = 0.636308 loss)
I1211 16:30:34.083638 13896 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1211 16:30:40.241111 13896 solver.cpp:218] Iteration 90900 (16.2431 iter/s, 6.15644s/100 iters), loss = 0.686354
I1211 16:30:40.241111 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 16:30:40.241111 13896 solver.cpp:237]     Train net output #1: loss = 0.686354 (* 1 = 0.686354 loss)
I1211 16:30:40.241111 13896 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1211 16:30:46.099562  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:30:46.344588 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91000.caffemodel
I1211 16:30:46.359592 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91000.solverstate
I1211 16:30:46.364593 13896 solver.cpp:330] Iteration 91000, Testing net (#0)
I1211 16:30:46.364593 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:30:47.697726  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:30:47.750731 13896 solver.cpp:397]     Test net output #0: accuracy = 0.5717
I1211 16:30:47.750731 13896 solver.cpp:397]     Test net output #1: loss = 1.68757 (* 1 = 1.68757 loss)
I1211 16:30:47.808735 13896 solver.cpp:218] Iteration 91000 (13.2149 iter/s, 7.56723s/100 iters), loss = 0.586466
I1211 16:30:47.808735 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 16:30:47.808735 13896 solver.cpp:237]     Train net output #1: loss = 0.586466 (* 1 = 0.586466 loss)
I1211 16:30:47.808735 13896 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1211 16:30:53.963215 13896 solver.cpp:218] Iteration 91100 (16.2479 iter/s, 6.15463s/100 iters), loss = 0.602179
I1211 16:30:53.963215 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 16:30:53.963215 13896 solver.cpp:237]     Train net output #1: loss = 0.602179 (* 1 = 0.602179 loss)
I1211 16:30:53.963215 13896 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1211 16:31:00.133630 13896 solver.cpp:218] Iteration 91200 (16.2082 iter/s, 6.16973s/100 iters), loss = 0.748525
I1211 16:31:00.133630 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 16:31:00.133630 13896 solver.cpp:237]     Train net output #1: loss = 0.748525 (* 1 = 0.748525 loss)
I1211 16:31:00.133630 13896 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1211 16:31:06.322113 13896 solver.cpp:218] Iteration 91300 (16.1611 iter/s, 6.18769s/100 iters), loss = 0.631233
I1211 16:31:06.322113 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:31:06.322113 13896 solver.cpp:237]     Train net output #1: loss = 0.631233 (* 1 = 0.631233 loss)
I1211 16:31:06.322113 13896 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1211 16:31:12.467522 13896 solver.cpp:218] Iteration 91400 (16.2723 iter/s, 6.14541s/100 iters), loss = 0.79629
I1211 16:31:12.467522 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 16:31:12.467522 13896 solver.cpp:237]     Train net output #1: loss = 0.79629 (* 1 = 0.79629 loss)
I1211 16:31:12.467522 13896 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1211 16:31:18.329237  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:31:18.572304 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91500.caffemodel
I1211 16:31:18.588292 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91500.solverstate
I1211 16:31:18.594296 13896 solver.cpp:330] Iteration 91500, Testing net (#0)
I1211 16:31:18.594296 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:31:19.929678  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:31:19.981703 13896 solver.cpp:397]     Test net output #0: accuracy = 0.5905
I1211 16:31:19.982720 13896 solver.cpp:397]     Test net output #1: loss = 1.63482 (* 1 = 1.63482 loss)
I1211 16:31:20.041721 13896 solver.cpp:218] Iteration 91500 (13.2045 iter/s, 7.57319s/100 iters), loss = 0.508907
I1211 16:31:20.041721 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:31:20.041721 13896 solver.cpp:237]     Train net output #1: loss = 0.508907 (* 1 = 0.508907 loss)
I1211 16:31:20.041721 13896 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1211 16:31:26.244669 13896 solver.cpp:218] Iteration 91600 (16.1215 iter/s, 6.2029s/100 iters), loss = 0.765838
I1211 16:31:26.244669 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 16:31:26.244669 13896 solver.cpp:237]     Train net output #1: loss = 0.765838 (* 1 = 0.765838 loss)
I1211 16:31:26.244669 13896 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1211 16:31:32.433820 13896 solver.cpp:218] Iteration 91700 (16.1597 iter/s, 6.18824s/100 iters), loss = 0.659986
I1211 16:31:32.433820 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 16:31:32.433820 13896 solver.cpp:237]     Train net output #1: loss = 0.659986 (* 1 = 0.659986 loss)
I1211 16:31:32.433820 13896 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1211 16:31:38.685497 13896 solver.cpp:218] Iteration 91800 (15.9962 iter/s, 6.2515s/100 iters), loss = 0.69265
I1211 16:31:38.685497 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 16:31:38.685497 13896 solver.cpp:237]     Train net output #1: loss = 0.69265 (* 1 = 0.69265 loss)
I1211 16:31:38.685497 13896 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1211 16:31:44.885640 13896 solver.cpp:218] Iteration 91900 (16.1286 iter/s, 6.20015s/100 iters), loss = 0.722882
I1211 16:31:44.885640 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 16:31:44.885640 13896 solver.cpp:237]     Train net output #1: loss = 0.722882 (* 1 = 0.722882 loss)
I1211 16:31:44.885640 13896 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1211 16:31:50.724092  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:31:50.967603 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92000.caffemodel
I1211 16:31:50.987109 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92000.solverstate
I1211 16:31:50.992110 13896 solver.cpp:330] Iteration 92000, Testing net (#0)
I1211 16:31:50.992110 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:31:52.347368  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:31:52.400374 13896 solver.cpp:397]     Test net output #0: accuracy = 0.568
I1211 16:31:52.400374 13896 solver.cpp:397]     Test net output #1: loss = 1.65042 (* 1 = 1.65042 loss)
I1211 16:31:52.458376 13896 solver.cpp:218] Iteration 92000 (13.2071 iter/s, 7.5717s/100 iters), loss = 0.537502
I1211 16:31:52.458376 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:31:52.458376 13896 solver.cpp:237]     Train net output #1: loss = 0.537502 (* 1 = 0.537502 loss)
I1211 16:31:52.458376 13896 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1211 16:31:58.616917 13896 solver.cpp:218] Iteration 92100 (16.2379 iter/s, 6.15843s/100 iters), loss = 0.674341
I1211 16:31:58.616917 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:31:58.616917 13896 solver.cpp:237]     Train net output #1: loss = 0.674341 (* 1 = 0.674341 loss)
I1211 16:31:58.616917 13896 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1211 16:32:04.763864 13896 solver.cpp:218] Iteration 92200 (16.2701 iter/s, 6.14626s/100 iters), loss = 0.722448
I1211 16:32:04.763864 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 16:32:04.763864 13896 solver.cpp:237]     Train net output #1: loss = 0.722448 (* 1 = 0.722448 loss)
I1211 16:32:04.763864 13896 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1211 16:32:10.917840 13896 solver.cpp:218] Iteration 92300 (16.2513 iter/s, 6.15337s/100 iters), loss = 0.753297
I1211 16:32:10.917840 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 16:32:10.917840 13896 solver.cpp:237]     Train net output #1: loss = 0.753297 (* 1 = 0.753297 loss)
I1211 16:32:10.917840 13896 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1211 16:32:17.133215 13896 solver.cpp:218] Iteration 92400 (16.0899 iter/s, 6.21507s/100 iters), loss = 0.693276
I1211 16:32:17.133215 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 16:32:17.133215 13896 solver.cpp:237]     Train net output #1: loss = 0.693276 (* 1 = 0.693276 loss)
I1211 16:32:17.133215 13896 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1211 16:32:23.029721  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:32:23.270737 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92500.caffemodel
I1211 16:32:23.286742 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92500.solverstate
I1211 16:32:23.291743 13896 solver.cpp:330] Iteration 92500, Testing net (#0)
I1211 16:32:23.291743 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:32:24.644896  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:32:24.697906 13896 solver.cpp:397]     Test net output #0: accuracy = 0.5896
I1211 16:32:24.697906 13896 solver.cpp:397]     Test net output #1: loss = 1.56713 (* 1 = 1.56713 loss)
I1211 16:32:24.755905 13896 solver.cpp:218] Iteration 92500 (13.1184 iter/s, 7.62291s/100 iters), loss = 0.477503
I1211 16:32:24.755905 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:32:24.756906 13896 solver.cpp:237]     Train net output #1: loss = 0.477503 (* 1 = 0.477503 loss)
I1211 16:32:24.756906 13896 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1211 16:32:30.980427 13896 solver.cpp:218] Iteration 92600 (16.0683 iter/s, 6.22342s/100 iters), loss = 0.691633
I1211 16:32:30.980427 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 16:32:30.980427 13896 solver.cpp:237]     Train net output #1: loss = 0.691633 (* 1 = 0.691633 loss)
I1211 16:32:30.980427 13896 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1211 16:32:37.265033 13896 solver.cpp:218] Iteration 92700 (15.9135 iter/s, 6.28396s/100 iters), loss = 0.72765
I1211 16:32:37.265033 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 16:32:37.265033 13896 solver.cpp:237]     Train net output #1: loss = 0.72765 (* 1 = 0.72765 loss)
I1211 16:32:37.265033 13896 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1211 16:32:43.576421 13896 solver.cpp:218] Iteration 92800 (15.8458 iter/s, 6.31081s/100 iters), loss = 0.673586
I1211 16:32:43.576421 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 16:32:43.576421 13896 solver.cpp:237]     Train net output #1: loss = 0.673586 (* 1 = 0.673586 loss)
I1211 16:32:43.576421 13896 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1211 16:32:49.776743 13896 solver.cpp:218] Iteration 92900 (16.1289 iter/s, 6.20005s/100 iters), loss = 0.742164
I1211 16:32:49.776743 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 16:32:49.776743 13896 solver.cpp:237]     Train net output #1: loss = 0.742164 (* 1 = 0.742164 loss)
I1211 16:32:49.776743 13896 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1211 16:32:55.640182  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:32:55.882719 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93000.caffemodel
I1211 16:32:55.897220 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93000.solverstate
I1211 16:32:55.902220 13896 solver.cpp:330] Iteration 93000, Testing net (#0)
I1211 16:32:55.902220 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:32:57.237632  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:32:57.289647 13896 solver.cpp:397]     Test net output #0: accuracy = 0.5857
I1211 16:32:57.289647 13896 solver.cpp:397]     Test net output #1: loss = 1.56716 (* 1 = 1.56716 loss)
I1211 16:32:57.348646 13896 solver.cpp:218] Iteration 93000 (13.207 iter/s, 7.57172s/100 iters), loss = 0.626249
I1211 16:32:57.348646 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:32:57.348646 13896 solver.cpp:237]     Train net output #1: loss = 0.626249 (* 1 = 0.626249 loss)
I1211 16:32:57.348646 13896 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1211 16:33:03.500811 13896 solver.cpp:218] Iteration 93100 (16.2546 iter/s, 6.1521s/100 iters), loss = 0.699121
I1211 16:33:03.500811 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 16:33:03.500811 13896 solver.cpp:237]     Train net output #1: loss = 0.699121 (* 1 = 0.699121 loss)
I1211 16:33:03.500811 13896 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1211 16:33:09.643647 13896 solver.cpp:218] Iteration 93200 (16.2824 iter/s, 6.14161s/100 iters), loss = 0.737293
I1211 16:33:09.643647 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 16:33:09.643647 13896 solver.cpp:237]     Train net output #1: loss = 0.737293 (* 1 = 0.737293 loss)
I1211 16:33:09.643647 13896 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1211 16:33:15.789212 13896 solver.cpp:218] Iteration 93300 (16.2732 iter/s, 6.14507s/100 iters), loss = 0.718438
I1211 16:33:15.789212 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 16:33:15.789212 13896 solver.cpp:237]     Train net output #1: loss = 0.718438 (* 1 = 0.718438 loss)
I1211 16:33:15.789212 13896 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1211 16:33:21.945605 13896 solver.cpp:218] Iteration 93400 (16.2434 iter/s, 6.15634s/100 iters), loss = 0.665257
I1211 16:33:21.945605 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 16:33:21.945605 13896 solver.cpp:237]     Train net output #1: loss = 0.665257 (* 1 = 0.665257 loss)
I1211 16:33:21.945605 13896 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1211 16:33:27.799043  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:33:28.042053 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93500.caffemodel
I1211 16:33:28.059052 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93500.solverstate
I1211 16:33:28.064054 13896 solver.cpp:330] Iteration 93500, Testing net (#0)
I1211 16:33:28.064054 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:33:29.398187  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:33:29.451195 13896 solver.cpp:397]     Test net output #0: accuracy = 0.5755
I1211 16:33:29.451195 13896 solver.cpp:397]     Test net output #1: loss = 1.67601 (* 1 = 1.67601 loss)
I1211 16:33:29.510193 13896 solver.cpp:218] Iteration 93500 (13.221 iter/s, 7.56373s/100 iters), loss = 0.573526
I1211 16:33:29.510193 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 16:33:29.510193 13896 solver.cpp:237]     Train net output #1: loss = 0.573526 (* 1 = 0.573526 loss)
I1211 16:33:29.510193 13896 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1211 16:33:35.677141 13896 solver.cpp:218] Iteration 93600 (16.2155 iter/s, 6.16695s/100 iters), loss = 0.699765
I1211 16:33:35.677641 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 16:33:35.677641 13896 solver.cpp:237]     Train net output #1: loss = 0.699765 (* 1 = 0.699765 loss)
I1211 16:33:35.677641 13896 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1211 16:33:41.836107 13896 solver.cpp:218] Iteration 93700 (16.2363 iter/s, 6.15904s/100 iters), loss = 0.494775
I1211 16:33:41.837107 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:33:41.837107 13896 solver.cpp:237]     Train net output #1: loss = 0.494775 (* 1 = 0.494775 loss)
I1211 16:33:41.837107 13896 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1211 16:33:48.024480 13896 solver.cpp:218] Iteration 93800 (16.1619 iter/s, 6.18739s/100 iters), loss = 0.789398
I1211 16:33:48.024480 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 16:33:48.024480 13896 solver.cpp:237]     Train net output #1: loss = 0.789398 (* 1 = 0.789398 loss)
I1211 16:33:48.024480 13896 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1211 16:33:54.190522 13896 solver.cpp:218] Iteration 93900 (16.2176 iter/s, 6.16615s/100 iters), loss = 0.797126
I1211 16:33:54.191522 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 16:33:54.191522 13896 solver.cpp:237]     Train net output #1: loss = 0.797126 (* 1 = 0.797126 loss)
I1211 16:33:54.191522 13896 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1211 16:34:00.105473  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:34:00.350486 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94000.caffemodel
I1211 16:34:00.365485 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94000.solverstate
I1211 16:34:00.370486 13896 solver.cpp:330] Iteration 94000, Testing net (#0)
I1211 16:34:00.370486 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:34:01.714602  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:34:01.767603 13896 solver.cpp:397]     Test net output #0: accuracy = 0.5769
I1211 16:34:01.767603 13896 solver.cpp:397]     Test net output #1: loss = 1.65979 (* 1 = 1.65979 loss)
I1211 16:34:01.826611 13896 solver.cpp:218] Iteration 94000 (13.0979 iter/s, 7.63478s/100 iters), loss = 0.56995
I1211 16:34:01.826611 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:34:01.826611 13896 solver.cpp:237]     Train net output #1: loss = 0.56995 (* 1 = 0.56995 loss)
I1211 16:34:01.826611 13896 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1211 16:34:08.147084 13896 solver.cpp:218] Iteration 94100 (15.8228 iter/s, 6.32s/100 iters), loss = 0.677995
I1211 16:34:08.147084 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 16:34:08.147084 13896 solver.cpp:237]     Train net output #1: loss = 0.677995 (* 1 = 0.677995 loss)
I1211 16:34:08.147084 13896 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1211 16:34:14.388087 13896 solver.cpp:218] Iteration 94200 (16.0244 iter/s, 6.24048s/100 iters), loss = 0.583806
I1211 16:34:14.388087 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:34:14.388087 13896 solver.cpp:237]     Train net output #1: loss = 0.583806 (* 1 = 0.583806 loss)
I1211 16:34:14.388087 13896 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1211 16:34:20.589675 13896 solver.cpp:218] Iteration 94300 (16.1249 iter/s, 6.20158s/100 iters), loss = 0.775998
I1211 16:34:20.589675 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 16:34:20.589675 13896 solver.cpp:237]     Train net output #1: loss = 0.775998 (* 1 = 0.775998 loss)
I1211 16:34:20.589675 13896 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1211 16:34:26.770673 13896 solver.cpp:218] Iteration 94400 (16.1797 iter/s, 6.18058s/100 iters), loss = 0.789129
I1211 16:34:26.770673 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1211 16:34:26.770673 13896 solver.cpp:237]     Train net output #1: loss = 0.789129 (* 1 = 0.789129 loss)
I1211 16:34:26.771677 13896 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1211 16:34:32.661248  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:34:32.908776 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94500.caffemodel
I1211 16:34:32.923775 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94500.solverstate
I1211 16:34:32.928776 13896 solver.cpp:330] Iteration 94500, Testing net (#0)
I1211 16:34:32.928776 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:34:34.270169  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:34:34.323159 13896 solver.cpp:397]     Test net output #0: accuracy = 0.5642
I1211 16:34:34.323159 13896 solver.cpp:397]     Test net output #1: loss = 1.72712 (* 1 = 1.72712 loss)
I1211 16:34:34.386178 13896 solver.cpp:218] Iteration 94500 (13.1327 iter/s, 7.61457s/100 iters), loss = 0.650001
I1211 16:34:34.386178 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:34:34.386178 13896 solver.cpp:237]     Train net output #1: loss = 0.650001 (* 1 = 0.650001 loss)
I1211 16:34:34.386178 13896 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1211 16:34:40.544075 13896 solver.cpp:218] Iteration 94600 (16.2409 iter/s, 6.15728s/100 iters), loss = 0.627092
I1211 16:34:40.544075 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:34:40.544075 13896 solver.cpp:237]     Train net output #1: loss = 0.627092 (* 1 = 0.627092 loss)
I1211 16:34:40.544075 13896 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1211 16:34:46.689067 13896 solver.cpp:218] Iteration 94700 (16.2734 iter/s, 6.145s/100 iters), loss = 0.520729
I1211 16:34:46.690068 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:34:46.690068 13896 solver.cpp:237]     Train net output #1: loss = 0.520729 (* 1 = 0.520729 loss)
I1211 16:34:46.690068 13896 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1211 16:34:52.850932 13896 solver.cpp:218] Iteration 94800 (16.2313 iter/s, 6.16093s/100 iters), loss = 0.697108
I1211 16:34:52.850932 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 16:34:52.850932 13896 solver.cpp:237]     Train net output #1: loss = 0.697108 (* 1 = 0.697108 loss)
I1211 16:34:52.850932 13896 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1211 16:34:59.001224 13896 solver.cpp:218] Iteration 94900 (16.261 iter/s, 6.14967s/100 iters), loss = 0.731017
I1211 16:34:59.001224 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 16:34:59.001224 13896 solver.cpp:237]     Train net output #1: loss = 0.731017 (* 1 = 0.731017 loss)
I1211 16:34:59.001224 13896 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1211 16:35:04.858588  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:35:05.099602 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95000.caffemodel
I1211 16:35:05.114612 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95000.solverstate
I1211 16:35:05.119601 13896 solver.cpp:330] Iteration 95000, Testing net (#0)
I1211 16:35:05.119601 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:35:06.453718  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:35:06.506723 13896 solver.cpp:397]     Test net output #0: accuracy = 0.583
I1211 16:35:06.506723 13896 solver.cpp:397]     Test net output #1: loss = 1.57973 (* 1 = 1.57973 loss)
I1211 16:35:06.565228 13896 solver.cpp:218] Iteration 95000 (13.2211 iter/s, 7.56365s/100 iters), loss = 0.582981
I1211 16:35:06.565228 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 16:35:06.565228 13896 solver.cpp:237]     Train net output #1: loss = 0.582981 (* 1 = 0.582981 loss)
I1211 16:35:06.565726 13896 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1211 16:35:06.565726 13896 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1211 16:35:12.702877 13896 solver.cpp:218] Iteration 95100 (16.2946 iter/s, 6.137s/100 iters), loss = 0.650657
I1211 16:35:12.702877 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 16:35:12.702877 13896 solver.cpp:237]     Train net output #1: loss = 0.650657 (* 1 = 0.650657 loss)
I1211 16:35:12.702877 13896 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1211 16:35:18.844920 13896 solver.cpp:218] Iteration 95200 (16.2822 iter/s, 6.14166s/100 iters), loss = 0.573603
I1211 16:35:18.844920 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 16:35:18.844920 13896 solver.cpp:237]     Train net output #1: loss = 0.573603 (* 1 = 0.573603 loss)
I1211 16:35:18.844920 13896 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1211 16:35:24.997390 13896 solver.cpp:218] Iteration 95300 (16.2532 iter/s, 6.15265s/100 iters), loss = 0.523304
I1211 16:35:24.997390 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:35:24.997390 13896 solver.cpp:237]     Train net output #1: loss = 0.523304 (* 1 = 0.523304 loss)
I1211 16:35:24.997390 13896 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1211 16:35:31.158869 13896 solver.cpp:218] Iteration 95400 (16.2329 iter/s, 6.16032s/100 iters), loss = 0.510943
I1211 16:35:31.158869 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 16:35:31.158869 13896 solver.cpp:237]     Train net output #1: loss = 0.510943 (* 1 = 0.510943 loss)
I1211 16:35:31.158869 13896 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1211 16:35:36.999339  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:35:37.242352 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95500.caffemodel
I1211 16:35:37.259353 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95500.solverstate
I1211 16:35:37.264358 13896 solver.cpp:330] Iteration 95500, Testing net (#0)
I1211 16:35:37.264358 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:35:38.598464  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:35:38.651463 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6762
I1211 16:35:38.651463 13896 solver.cpp:397]     Test net output #1: loss = 1.1945 (* 1 = 1.1945 loss)
I1211 16:35:38.709468 13896 solver.cpp:218] Iteration 95500 (13.2435 iter/s, 7.55085s/100 iters), loss = 0.487526
I1211 16:35:38.710469 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:35:38.710469 13896 solver.cpp:237]     Train net output #1: loss = 0.487526 (* 1 = 0.487526 loss)
I1211 16:35:38.710469 13896 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1211 16:35:44.870450 13896 solver.cpp:218] Iteration 95600 (16.2345 iter/s, 6.15971s/100 iters), loss = 0.502474
I1211 16:35:44.870450 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:35:44.870450 13896 solver.cpp:237]     Train net output #1: loss = 0.502474 (* 1 = 0.502474 loss)
I1211 16:35:44.870450 13896 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1211 16:35:51.024412 13896 solver.cpp:218] Iteration 95700 (16.2507 iter/s, 6.1536s/100 iters), loss = 0.393959
I1211 16:35:51.024412 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:35:51.024412 13896 solver.cpp:237]     Train net output #1: loss = 0.393959 (* 1 = 0.393959 loss)
I1211 16:35:51.024412 13896 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1211 16:35:57.178344 13896 solver.cpp:218] Iteration 95800 (16.2502 iter/s, 6.15375s/100 iters), loss = 0.479442
I1211 16:35:57.178850 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:35:57.178850 13896 solver.cpp:237]     Train net output #1: loss = 0.479442 (* 1 = 0.479442 loss)
I1211 16:35:57.178850 13896 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1211 16:36:03.329319 13896 solver.cpp:218] Iteration 95900 (16.2596 iter/s, 6.15021s/100 iters), loss = 0.53861
I1211 16:36:03.329319 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 16:36:03.329319 13896 solver.cpp:237]     Train net output #1: loss = 0.53861 (* 1 = 0.53861 loss)
I1211 16:36:03.329319 13896 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1211 16:36:09.175757  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:36:09.417788 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96000.caffemodel
I1211 16:36:09.432787 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96000.solverstate
I1211 16:36:09.437788 13896 solver.cpp:330] Iteration 96000, Testing net (#0)
I1211 16:36:09.437788 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:36:10.773946  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:36:10.826952 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6782
I1211 16:36:10.826952 13896 solver.cpp:397]     Test net output #1: loss = 1.1889 (* 1 = 1.1889 loss)
I1211 16:36:10.885968 13896 solver.cpp:218] Iteration 96000 (13.2344 iter/s, 7.55609s/100 iters), loss = 0.426816
I1211 16:36:10.885968 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:36:10.885968 13896 solver.cpp:237]     Train net output #1: loss = 0.426816 (* 1 = 0.426816 loss)
I1211 16:36:10.885968 13896 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1211 16:36:17.033454 13896 solver.cpp:218] Iteration 96100 (16.2659 iter/s, 6.14785s/100 iters), loss = 0.493976
I1211 16:36:17.033454 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 16:36:17.033454 13896 solver.cpp:237]     Train net output #1: loss = 0.493976 (* 1 = 0.493976 loss)
I1211 16:36:17.033454 13896 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1211 16:36:23.180941 13896 solver.cpp:218] Iteration 96200 (16.2695 iter/s, 6.14647s/100 iters), loss = 0.454195
I1211 16:36:23.180941 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:36:23.180941 13896 solver.cpp:237]     Train net output #1: loss = 0.454195 (* 1 = 0.454195 loss)
I1211 16:36:23.180941 13896 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1211 16:36:29.328367 13896 solver.cpp:218] Iteration 96300 (16.2676 iter/s, 6.1472s/100 iters), loss = 0.459645
I1211 16:36:29.328367 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:36:29.328367 13896 solver.cpp:237]     Train net output #1: loss = 0.459645 (* 1 = 0.459645 loss)
I1211 16:36:29.328367 13896 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1211 16:36:35.482908 13896 solver.cpp:218] Iteration 96400 (16.2505 iter/s, 6.15367s/100 iters), loss = 0.434189
I1211 16:36:35.482908 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 16:36:35.482908 13896 solver.cpp:237]     Train net output #1: loss = 0.434189 (* 1 = 0.434189 loss)
I1211 16:36:35.482908 13896 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1211 16:36:41.334379  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:36:41.576884 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96500.caffemodel
I1211 16:36:41.592401 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96500.solverstate
I1211 16:36:41.597399 13896 solver.cpp:330] Iteration 96500, Testing net (#0)
I1211 16:36:41.597399 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:36:42.933512  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:36:42.985517 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6777
I1211 16:36:42.985517 13896 solver.cpp:397]     Test net output #1: loss = 1.18621 (* 1 = 1.18621 loss)
I1211 16:36:43.044515 13896 solver.cpp:218] Iteration 96500 (13.2252 iter/s, 7.56131s/100 iters), loss = 0.462556
I1211 16:36:43.044515 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:36:43.044515 13896 solver.cpp:237]     Train net output #1: loss = 0.462556 (* 1 = 0.462556 loss)
I1211 16:36:43.044515 13896 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1211 16:36:49.198125 13896 solver.cpp:218] Iteration 96600 (16.2513 iter/s, 6.15337s/100 iters), loss = 0.484652
I1211 16:36:49.198125 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:36:49.198125 13896 solver.cpp:237]     Train net output #1: loss = 0.484652 (* 1 = 0.484652 loss)
I1211 16:36:49.198125 13896 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1211 16:36:55.341789 13896 solver.cpp:218] Iteration 96700 (16.2786 iter/s, 6.14302s/100 iters), loss = 0.42014
I1211 16:36:55.341789 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:36:55.341789 13896 solver.cpp:237]     Train net output #1: loss = 0.42014 (* 1 = 0.42014 loss)
I1211 16:36:55.341789 13896 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1211 16:37:01.493225 13896 solver.cpp:218] Iteration 96800 (16.2557 iter/s, 6.1517s/100 iters), loss = 0.468849
I1211 16:37:01.494225 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:37:01.494225 13896 solver.cpp:237]     Train net output #1: loss = 0.468849 (* 1 = 0.468849 loss)
I1211 16:37:01.494225 13896 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1211 16:37:07.649818 13896 solver.cpp:218] Iteration 96900 (16.2457 iter/s, 6.15547s/100 iters), loss = 0.471408
I1211 16:37:07.649818 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:37:07.649818 13896 solver.cpp:237]     Train net output #1: loss = 0.471408 (* 1 = 0.471408 loss)
I1211 16:37:07.649818 13896 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1211 16:37:13.503347  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:37:13.746358 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97000.caffemodel
I1211 16:37:13.761358 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97000.solverstate
I1211 16:37:13.765358 13896 solver.cpp:330] Iteration 97000, Testing net (#0)
I1211 16:37:13.765358 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:37:15.099455  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:37:15.152454 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6796
I1211 16:37:15.152454 13896 solver.cpp:397]     Test net output #1: loss = 1.18605 (* 1 = 1.18605 loss)
I1211 16:37:15.210460 13896 solver.cpp:218] Iteration 97000 (13.2269 iter/s, 7.56032s/100 iters), loss = 0.373172
I1211 16:37:15.210460 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:37:15.210460 13896 solver.cpp:237]     Train net output #1: loss = 0.373172 (* 1 = 0.373172 loss)
I1211 16:37:15.210460 13896 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1211 16:37:21.354902 13896 solver.cpp:218] Iteration 97100 (16.2769 iter/s, 6.14366s/100 iters), loss = 0.515142
I1211 16:37:21.354902 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 16:37:21.354902 13896 solver.cpp:237]     Train net output #1: loss = 0.515142 (* 1 = 0.515142 loss)
I1211 16:37:21.354902 13896 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1211 16:37:27.510351 13896 solver.cpp:218] Iteration 97200 (16.2447 iter/s, 6.15585s/100 iters), loss = 0.374652
I1211 16:37:27.511351 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:37:27.511351 13896 solver.cpp:237]     Train net output #1: loss = 0.374652 (* 1 = 0.374652 loss)
I1211 16:37:27.511351 13896 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1211 16:37:33.655836 13896 solver.cpp:218] Iteration 97300 (16.2756 iter/s, 6.14418s/100 iters), loss = 0.403696
I1211 16:37:33.655836 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:37:33.655836 13896 solver.cpp:237]     Train net output #1: loss = 0.403696 (* 1 = 0.403696 loss)
I1211 16:37:33.655836 13896 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1211 16:37:39.804273 13896 solver.cpp:218] Iteration 97400 (16.2642 iter/s, 6.14847s/100 iters), loss = 0.419532
I1211 16:37:39.804273 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:37:39.804273 13896 solver.cpp:237]     Train net output #1: loss = 0.419532 (* 1 = 0.419532 loss)
I1211 16:37:39.804273 13896 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1211 16:37:45.651820  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:37:45.895858 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97500.caffemodel
I1211 16:37:45.910857 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97500.solverstate
I1211 16:37:45.915858 13896 solver.cpp:330] Iteration 97500, Testing net (#0)
I1211 16:37:45.915858 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:37:47.252941  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:37:47.304968 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6788
I1211 16:37:47.304968 13896 solver.cpp:397]     Test net output #1: loss = 1.19325 (* 1 = 1.19325 loss)
I1211 16:37:47.363953 13896 solver.cpp:218] Iteration 97500 (13.2298 iter/s, 7.55867s/100 iters), loss = 0.448651
I1211 16:37:47.363953 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:37:47.363953 13896 solver.cpp:237]     Train net output #1: loss = 0.448651 (* 1 = 0.448651 loss)
I1211 16:37:47.363953 13896 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1211 16:37:53.508816 13896 solver.cpp:218] Iteration 97600 (16.2737 iter/s, 6.14487s/100 iters), loss = 0.540028
I1211 16:37:53.508816 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 16:37:53.508816 13896 solver.cpp:237]     Train net output #1: loss = 0.540028 (* 1 = 0.540028 loss)
I1211 16:37:53.508816 13896 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1211 16:37:59.657269 13896 solver.cpp:218] Iteration 97700 (16.2662 iter/s, 6.14771s/100 iters), loss = 0.342779
I1211 16:37:59.657269 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:37:59.657269 13896 solver.cpp:237]     Train net output #1: loss = 0.342779 (* 1 = 0.342779 loss)
I1211 16:37:59.657269 13896 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1211 16:38:05.805786 13896 solver.cpp:218] Iteration 97800 (16.2658 iter/s, 6.14785s/100 iters), loss = 0.41507
I1211 16:38:05.805786 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:38:05.805786 13896 solver.cpp:237]     Train net output #1: loss = 0.41507 (* 1 = 0.41507 loss)
I1211 16:38:05.805786 13896 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1211 16:38:11.954113 13896 solver.cpp:218] Iteration 97900 (16.2637 iter/s, 6.14865s/100 iters), loss = 0.522522
I1211 16:38:11.954113 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 16:38:11.954113 13896 solver.cpp:237]     Train net output #1: loss = 0.522522 (* 1 = 0.522522 loss)
I1211 16:38:11.954113 13896 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1211 16:38:17.804725  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:38:18.046738 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98000.caffemodel
I1211 16:38:18.061738 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98000.solverstate
I1211 16:38:18.066737 13896 solver.cpp:330] Iteration 98000, Testing net (#0)
I1211 16:38:18.066737 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:38:19.402825  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:38:19.455826 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6778
I1211 16:38:19.455826 13896 solver.cpp:397]     Test net output #1: loss = 1.19528 (* 1 = 1.19528 loss)
I1211 16:38:19.513828 13896 solver.cpp:218] Iteration 98000 (13.2289 iter/s, 7.55922s/100 iters), loss = 0.473033
I1211 16:38:19.513828 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:38:19.513828 13896 solver.cpp:237]     Train net output #1: loss = 0.473033 (* 1 = 0.473033 loss)
I1211 16:38:19.513828 13896 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1211 16:38:25.667402 13896 solver.cpp:218] Iteration 98100 (16.2525 iter/s, 6.15291s/100 iters), loss = 0.535001
I1211 16:38:25.667402 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 16:38:25.667402 13896 solver.cpp:237]     Train net output #1: loss = 0.535001 (* 1 = 0.535001 loss)
I1211 16:38:25.667402 13896 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1211 16:38:31.810941 13896 solver.cpp:218] Iteration 98200 (16.279 iter/s, 6.14288s/100 iters), loss = 0.378019
I1211 16:38:31.810941 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:38:31.810941 13896 solver.cpp:237]     Train net output #1: loss = 0.378019 (* 1 = 0.378019 loss)
I1211 16:38:31.810941 13896 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1211 16:38:37.947350 13896 solver.cpp:218] Iteration 98300 (16.2961 iter/s, 6.13644s/100 iters), loss = 0.352061
I1211 16:38:37.947350 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:38:37.947350 13896 solver.cpp:237]     Train net output #1: loss = 0.352061 (* 1 = 0.352061 loss)
I1211 16:38:37.947350 13896 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1211 16:38:44.084389 13896 solver.cpp:218] Iteration 98400 (16.2973 iter/s, 6.13597s/100 iters), loss = 0.501109
I1211 16:38:44.084389 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:38:44.084389 13896 solver.cpp:237]     Train net output #1: loss = 0.501109 (* 1 = 0.501109 loss)
I1211 16:38:44.084389 13896 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1211 16:38:49.919644  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:38:50.161185 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98500.caffemodel
I1211 16:38:50.179729 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98500.solverstate
I1211 16:38:50.184729 13896 solver.cpp:330] Iteration 98500, Testing net (#0)
I1211 16:38:50.184729 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:38:51.518381  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:38:51.571385 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6807
I1211 16:38:51.571385 13896 solver.cpp:397]     Test net output #1: loss = 1.19467 (* 1 = 1.19467 loss)
I1211 16:38:51.630405 13896 solver.cpp:218] Iteration 98500 (13.2519 iter/s, 7.54608s/100 iters), loss = 0.408535
I1211 16:38:51.630405 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:38:51.630405 13896 solver.cpp:237]     Train net output #1: loss = 0.408535 (* 1 = 0.408535 loss)
I1211 16:38:51.630405 13896 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1211 16:38:57.778775 13896 solver.cpp:218] Iteration 98600 (16.2675 iter/s, 6.14724s/100 iters), loss = 0.422062
I1211 16:38:57.778775 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:38:57.778775 13896 solver.cpp:237]     Train net output #1: loss = 0.422062 (* 1 = 0.422062 loss)
I1211 16:38:57.778775 13896 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1211 16:39:03.926278 13896 solver.cpp:218] Iteration 98700 (16.2667 iter/s, 6.14752s/100 iters), loss = 0.423224
I1211 16:39:03.926278 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:39:03.926278 13896 solver.cpp:237]     Train net output #1: loss = 0.423224 (* 1 = 0.423224 loss)
I1211 16:39:03.926278 13896 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1211 16:39:10.073853 13896 solver.cpp:218] Iteration 98800 (16.2681 iter/s, 6.14699s/100 iters), loss = 0.40869
I1211 16:39:10.073853 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:39:10.073853 13896 solver.cpp:237]     Train net output #1: loss = 0.40869 (* 1 = 0.40869 loss)
I1211 16:39:10.073853 13896 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1211 16:39:16.229420 13896 solver.cpp:218] Iteration 98900 (16.2467 iter/s, 6.1551s/100 iters), loss = 0.4641
I1211 16:39:16.229420 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:39:16.229420 13896 solver.cpp:237]     Train net output #1: loss = 0.4641 (* 1 = 0.4641 loss)
I1211 16:39:16.229420 13896 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1211 16:39:22.071774  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:39:22.314340 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99000.caffemodel
I1211 16:39:22.330353 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99000.solverstate
I1211 16:39:22.335353 13896 solver.cpp:330] Iteration 99000, Testing net (#0)
I1211 16:39:22.335353 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:39:23.668970  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:39:23.721473 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6768
I1211 16:39:23.721473 13896 solver.cpp:397]     Test net output #1: loss = 1.20671 (* 1 = 1.20671 loss)
I1211 16:39:23.779522 13896 solver.cpp:218] Iteration 99000 (13.2449 iter/s, 7.55007s/100 iters), loss = 0.428005
I1211 16:39:23.779522 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:39:23.779522 13896 solver.cpp:237]     Train net output #1: loss = 0.428005 (* 1 = 0.428005 loss)
I1211 16:39:23.779522 13896 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1211 16:39:29.925897 13896 solver.cpp:218] Iteration 99100 (16.2713 iter/s, 6.14579s/100 iters), loss = 0.438517
I1211 16:39:29.925897 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:39:29.925897 13896 solver.cpp:237]     Train net output #1: loss = 0.438517 (* 1 = 0.438517 loss)
I1211 16:39:29.925897 13896 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1211 16:39:36.076508 13896 solver.cpp:218] Iteration 99200 (16.26 iter/s, 6.15007s/100 iters), loss = 0.340802
I1211 16:39:36.076508 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:39:36.077009 13896 solver.cpp:237]     Train net output #1: loss = 0.340802 (* 1 = 0.340802 loss)
I1211 16:39:36.077009 13896 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1211 16:39:42.225016 13896 solver.cpp:218] Iteration 99300 (16.2651 iter/s, 6.14814s/100 iters), loss = 0.362152
I1211 16:39:42.225016 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:39:42.225016 13896 solver.cpp:237]     Train net output #1: loss = 0.362152 (* 1 = 0.362152 loss)
I1211 16:39:42.225016 13896 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1211 16:39:48.377935 13896 solver.cpp:218] Iteration 99400 (16.2549 iter/s, 6.15199s/100 iters), loss = 0.391068
I1211 16:39:48.377935 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:39:48.377935 13896 solver.cpp:237]     Train net output #1: loss = 0.391068 (* 1 = 0.391068 loss)
I1211 16:39:48.377935 13896 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1211 16:39:54.226897  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:39:54.470921 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99500.caffemodel
I1211 16:39:54.486923 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99500.solverstate
I1211 16:39:54.491925 13896 solver.cpp:330] Iteration 99500, Testing net (#0)
I1211 16:39:54.491925 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:39:55.827034  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:39:55.880040 13896 solver.cpp:397]     Test net output #0: accuracy = 0.678
I1211 16:39:55.880040 13896 solver.cpp:397]     Test net output #1: loss = 1.21212 (* 1 = 1.21212 loss)
I1211 16:39:55.940044 13896 solver.cpp:218] Iteration 99500 (13.2244 iter/s, 7.56176s/100 iters), loss = 0.3904
I1211 16:39:55.940044 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:39:55.940044 13896 solver.cpp:237]     Train net output #1: loss = 0.3904 (* 1 = 0.3904 loss)
I1211 16:39:55.940044 13896 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1211 16:40:02.141644 13896 solver.cpp:218] Iteration 99600 (16.1259 iter/s, 6.20121s/100 iters), loss = 0.467597
I1211 16:40:02.141644 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:40:02.141644 13896 solver.cpp:237]     Train net output #1: loss = 0.467597 (* 1 = 0.467597 loss)
I1211 16:40:02.141644 13896 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1211 16:40:08.307992 13896 solver.cpp:218] Iteration 99700 (16.2162 iter/s, 6.16666s/100 iters), loss = 0.429612
I1211 16:40:08.308993 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:40:08.308993 13896 solver.cpp:237]     Train net output #1: loss = 0.429612 (* 1 = 0.429612 loss)
I1211 16:40:08.308993 13896 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1211 16:40:14.465584 13896 solver.cpp:218] Iteration 99800 (16.2428 iter/s, 6.15656s/100 iters), loss = 0.392115
I1211 16:40:14.465584 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:40:14.465584 13896 solver.cpp:237]     Train net output #1: loss = 0.392115 (* 1 = 0.392115 loss)
I1211 16:40:14.465584 13896 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1211 16:40:20.615087 13896 solver.cpp:218] Iteration 99900 (16.2617 iter/s, 6.14942s/100 iters), loss = 0.471848
I1211 16:40:20.615087 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:40:20.615087 13896 solver.cpp:237]     Train net output #1: loss = 0.471848 (* 1 = 0.471848 loss)
I1211 16:40:20.615087 13896 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1211 16:40:26.469550  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:40:26.710567 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100000.caffemodel
I1211 16:40:26.726567 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100000.solverstate
I1211 16:40:26.730567 13896 solver.cpp:330] Iteration 100000, Testing net (#0)
I1211 16:40:26.730567 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:40:28.064658  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:40:28.117663 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6795
I1211 16:40:28.117663 13896 solver.cpp:397]     Test net output #1: loss = 1.20294 (* 1 = 1.20294 loss)
I1211 16:40:28.176667 13896 solver.cpp:218] Iteration 100000 (13.2266 iter/s, 7.5605s/100 iters), loss = 0.330335
I1211 16:40:28.176667 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:40:28.176667 13896 solver.cpp:237]     Train net output #1: loss = 0.330335 (* 1 = 0.330335 loss)
I1211 16:40:28.176667 13896 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1211 16:40:34.319108 13896 solver.cpp:218] Iteration 100100 (16.2797 iter/s, 6.14262s/100 iters), loss = 0.438923
I1211 16:40:34.319108 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:40:34.319108 13896 solver.cpp:237]     Train net output #1: loss = 0.438923 (* 1 = 0.438923 loss)
I1211 16:40:34.319108 13896 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1211 16:40:40.464530 13896 solver.cpp:218] Iteration 100200 (16.2753 iter/s, 6.14428s/100 iters), loss = 0.397905
I1211 16:40:40.464530 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:40:40.464530 13896 solver.cpp:237]     Train net output #1: loss = 0.397905 (* 1 = 0.397905 loss)
I1211 16:40:40.464530 13896 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1211 16:40:46.615028 13896 solver.cpp:218] Iteration 100300 (16.2596 iter/s, 6.15022s/100 iters), loss = 0.414077
I1211 16:40:46.615028 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:40:46.615028 13896 solver.cpp:237]     Train net output #1: loss = 0.414077 (* 1 = 0.414077 loss)
I1211 16:40:46.615028 13896 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1211 16:40:52.759606 13896 solver.cpp:218] Iteration 100400 (16.2749 iter/s, 6.14444s/100 iters), loss = 0.368912
I1211 16:40:52.759606 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:40:52.759606 13896 solver.cpp:237]     Train net output #1: loss = 0.368912 (* 1 = 0.368912 loss)
I1211 16:40:52.759606 13896 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1211 16:40:58.601227  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:40:58.845366 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100500.caffemodel
I1211 16:40:58.860366 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100500.solverstate
I1211 16:40:58.864367 13896 solver.cpp:330] Iteration 100500, Testing net (#0)
I1211 16:40:58.864367 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:41:00.197474  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:41:00.250491 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6838
I1211 16:41:00.250491 13896 solver.cpp:397]     Test net output #1: loss = 1.20251 (* 1 = 1.20251 loss)
I1211 16:41:00.309080 13896 solver.cpp:218] Iteration 100500 (13.2461 iter/s, 7.54937s/100 iters), loss = 0.415076
I1211 16:41:00.309080 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:41:00.309080 13896 solver.cpp:237]     Train net output #1: loss = 0.415076 (* 1 = 0.415076 loss)
I1211 16:41:00.309080 13896 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1211 16:41:06.455400 13896 solver.cpp:218] Iteration 100600 (16.2711 iter/s, 6.14587s/100 iters), loss = 0.423249
I1211 16:41:06.455400 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:41:06.455400 13896 solver.cpp:237]     Train net output #1: loss = 0.423249 (* 1 = 0.423249 loss)
I1211 16:41:06.455400 13896 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1211 16:41:12.609956 13896 solver.cpp:218] Iteration 100700 (16.2508 iter/s, 6.15356s/100 iters), loss = 0.39571
I1211 16:41:12.609956 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:41:12.609956 13896 solver.cpp:237]     Train net output #1: loss = 0.39571 (* 1 = 0.39571 loss)
I1211 16:41:12.609956 13896 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1211 16:41:18.771410 13896 solver.cpp:218] Iteration 100800 (16.2314 iter/s, 6.1609s/100 iters), loss = 0.37557
I1211 16:41:18.771410 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:41:18.771410 13896 solver.cpp:237]     Train net output #1: loss = 0.37557 (* 1 = 0.37557 loss)
I1211 16:41:18.771410 13896 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1211 16:41:24.933910 13896 solver.cpp:218] Iteration 100900 (16.2271 iter/s, 6.16253s/100 iters), loss = 0.414103
I1211 16:41:24.933910 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:41:24.933910 13896 solver.cpp:237]     Train net output #1: loss = 0.414103 (* 1 = 0.414103 loss)
I1211 16:41:24.933910 13896 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1211 16:41:30.786221  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:41:31.027231 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101000.caffemodel
I1211 16:41:31.049232 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101000.solverstate
I1211 16:41:31.054232 13896 solver.cpp:330] Iteration 101000, Testing net (#0)
I1211 16:41:31.054232 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:41:32.391872  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:41:32.444376 13896 solver.cpp:397]     Test net output #0: accuracy = 0.68
I1211 16:41:32.444376 13896 solver.cpp:397]     Test net output #1: loss = 1.20146 (* 1 = 1.20146 loss)
I1211 16:41:32.503379 13896 solver.cpp:218] Iteration 101000 (13.2111 iter/s, 7.56939s/100 iters), loss = 0.360881
I1211 16:41:32.504380 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:41:32.504380 13896 solver.cpp:237]     Train net output #1: loss = 0.360881 (* 1 = 0.360881 loss)
I1211 16:41:32.504380 13896 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1211 16:41:38.673820 13896 solver.cpp:218] Iteration 101100 (16.2082 iter/s, 6.16973s/100 iters), loss = 0.450438
I1211 16:41:38.673820 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:41:38.673820 13896 solver.cpp:237]     Train net output #1: loss = 0.450438 (* 1 = 0.450438 loss)
I1211 16:41:38.673820 13896 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1211 16:41:44.842324 13896 solver.cpp:218] Iteration 101200 (16.2125 iter/s, 6.16806s/100 iters), loss = 0.368435
I1211 16:41:44.842324 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:41:44.842324 13896 solver.cpp:237]     Train net output #1: loss = 0.368435 (* 1 = 0.368435 loss)
I1211 16:41:44.842324 13896 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1211 16:41:51.013511 13896 solver.cpp:218] Iteration 101300 (16.2063 iter/s, 6.17045s/100 iters), loss = 0.353791
I1211 16:41:51.013511 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:41:51.013511 13896 solver.cpp:237]     Train net output #1: loss = 0.353791 (* 1 = 0.353791 loss)
I1211 16:41:51.013511 13896 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1211 16:41:57.183706 13896 solver.cpp:218] Iteration 101400 (16.2089 iter/s, 6.16944s/100 iters), loss = 0.464596
I1211 16:41:57.183706 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:41:57.183706 13896 solver.cpp:237]     Train net output #1: loss = 0.464596 (* 1 = 0.464596 loss)
I1211 16:41:57.183706 13896 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1211 16:42:03.044566  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:42:03.288094 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101500.caffemodel
I1211 16:42:03.303596 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101500.solverstate
I1211 16:42:03.308600 13896 solver.cpp:330] Iteration 101500, Testing net (#0)
I1211 16:42:03.308600 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:42:04.645929  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:42:04.698937 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6819
I1211 16:42:04.698937 13896 solver.cpp:397]     Test net output #1: loss = 1.19853 (* 1 = 1.19853 loss)
I1211 16:42:04.757938 13896 solver.cpp:218] Iteration 101500 (13.2026 iter/s, 7.57427s/100 iters), loss = 0.323801
I1211 16:42:04.757938 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:42:04.757938 13896 solver.cpp:237]     Train net output #1: loss = 0.323801 (* 1 = 0.323801 loss)
I1211 16:42:04.757938 13896 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1211 16:42:10.931924 13896 solver.cpp:218] Iteration 101600 (16.1993 iter/s, 6.17309s/100 iters), loss = 0.371375
I1211 16:42:10.931924 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:42:10.931924 13896 solver.cpp:237]     Train net output #1: loss = 0.371375 (* 1 = 0.371375 loss)
I1211 16:42:10.931924 13896 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1211 16:42:17.089730 13896 solver.cpp:218] Iteration 101700 (16.2411 iter/s, 6.15721s/100 iters), loss = 0.303638
I1211 16:42:17.089730 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:42:17.089730 13896 solver.cpp:237]     Train net output #1: loss = 0.303638 (* 1 = 0.303638 loss)
I1211 16:42:17.089730 13896 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1211 16:42:23.253940 13896 solver.cpp:218] Iteration 101800 (16.2224 iter/s, 6.16431s/100 iters), loss = 0.369338
I1211 16:42:23.253940 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:42:23.253940 13896 solver.cpp:237]     Train net output #1: loss = 0.369338 (* 1 = 0.369338 loss)
I1211 16:42:23.253940 13896 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1211 16:42:29.405963 13896 solver.cpp:218] Iteration 101900 (16.2572 iter/s, 6.15113s/100 iters), loss = 0.428682
I1211 16:42:29.405963 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:42:29.405963 13896 solver.cpp:237]     Train net output #1: loss = 0.428682 (* 1 = 0.428682 loss)
I1211 16:42:29.405963 13896 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1211 16:42:35.254921  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:42:35.498436 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102000.caffemodel
I1211 16:42:35.512943 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102000.solverstate
I1211 16:42:35.517943 13896 solver.cpp:330] Iteration 102000, Testing net (#0)
I1211 16:42:35.517943 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:42:36.855051  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:42:36.908054 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6792
I1211 16:42:36.908054 13896 solver.cpp:397]     Test net output #1: loss = 1.21141 (* 1 = 1.21141 loss)
I1211 16:42:36.966056 13896 solver.cpp:218] Iteration 102000 (13.2273 iter/s, 7.56012s/100 iters), loss = 0.356337
I1211 16:42:36.966056 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:42:36.966056 13896 solver.cpp:237]     Train net output #1: loss = 0.356337 (* 1 = 0.356337 loss)
I1211 16:42:36.966056 13896 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1211 16:42:43.123584 13896 solver.cpp:218] Iteration 102100 (16.2424 iter/s, 6.15673s/100 iters), loss = 0.425521
I1211 16:42:43.123584 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:42:43.123584 13896 solver.cpp:237]     Train net output #1: loss = 0.425521 (* 1 = 0.425521 loss)
I1211 16:42:43.123584 13896 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1211 16:42:49.320297 13896 solver.cpp:218] Iteration 102200 (16.1386 iter/s, 6.19631s/100 iters), loss = 0.287619
I1211 16:42:49.320297 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:42:49.320297 13896 solver.cpp:237]     Train net output #1: loss = 0.287619 (* 1 = 0.287619 loss)
I1211 16:42:49.320297 13896 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1211 16:42:55.554410 13896 solver.cpp:218] Iteration 102300 (16.0405 iter/s, 6.23422s/100 iters), loss = 0.325805
I1211 16:42:55.555411 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:42:55.555411 13896 solver.cpp:237]     Train net output #1: loss = 0.325805 (* 1 = 0.325805 loss)
I1211 16:42:55.555411 13896 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1211 16:43:01.739195 13896 solver.cpp:218] Iteration 102400 (16.1705 iter/s, 6.18409s/100 iters), loss = 0.464677
I1211 16:43:01.739195 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 16:43:01.739195 13896 solver.cpp:237]     Train net output #1: loss = 0.464677 (* 1 = 0.464677 loss)
I1211 16:43:01.739195 13896 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1211 16:43:07.633766  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:43:07.877449 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102500.caffemodel
I1211 16:43:07.894444 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102500.solverstate
I1211 16:43:07.898949 13896 solver.cpp:330] Iteration 102500, Testing net (#0)
I1211 16:43:07.899452 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:43:09.246629  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:43:09.300640 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6801
I1211 16:43:09.301141 13896 solver.cpp:397]     Test net output #1: loss = 1.21259 (* 1 = 1.21259 loss)
I1211 16:43:09.359143 13896 solver.cpp:218] Iteration 102500 (13.1246 iter/s, 7.61928s/100 iters), loss = 0.36038
I1211 16:43:09.359143 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:43:09.359143 13896 solver.cpp:237]     Train net output #1: loss = 0.36038 (* 1 = 0.36038 loss)
I1211 16:43:09.359143 13896 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1211 16:43:15.529651 13896 solver.cpp:218] Iteration 102600 (16.2089 iter/s, 6.16946s/100 iters), loss = 0.432876
I1211 16:43:15.529651 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 16:43:15.529651 13896 solver.cpp:237]     Train net output #1: loss = 0.432876 (* 1 = 0.432876 loss)
I1211 16:43:15.529651 13896 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1211 16:43:21.720381 13896 solver.cpp:218] Iteration 102700 (16.1522 iter/s, 6.19109s/100 iters), loss = 0.31177
I1211 16:43:21.720381 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:43:21.720381 13896 solver.cpp:237]     Train net output #1: loss = 0.31177 (* 1 = 0.31177 loss)
I1211 16:43:21.720381 13896 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1211 16:43:27.911379 13896 solver.cpp:218] Iteration 102800 (16.1554 iter/s, 6.18987s/100 iters), loss = 0.362422
I1211 16:43:27.911379 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:43:27.911379 13896 solver.cpp:237]     Train net output #1: loss = 0.362422 (* 1 = 0.362422 loss)
I1211 16:43:27.911379 13896 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1211 16:43:34.082413 13896 solver.cpp:218] Iteration 102900 (16.2056 iter/s, 6.17072s/100 iters), loss = 0.440603
I1211 16:43:34.082413 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:43:34.082413 13896 solver.cpp:237]     Train net output #1: loss = 0.440603 (* 1 = 0.440603 loss)
I1211 16:43:34.082413 13896 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1211 16:43:39.952869  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:43:40.194895 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103000.caffemodel
I1211 16:43:40.210399 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103000.solverstate
I1211 16:43:40.215400 13896 solver.cpp:330] Iteration 103000, Testing net (#0)
I1211 16:43:40.215400 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:43:41.552006  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:43:41.605506 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 16:43:41.605506 13896 solver.cpp:397]     Test net output #1: loss = 1.21909 (* 1 = 1.21909 loss)
I1211 16:43:41.664010 13896 solver.cpp:218] Iteration 103000 (13.1912 iter/s, 7.58082s/100 iters), loss = 0.30687
I1211 16:43:41.664010 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:43:41.664010 13896 solver.cpp:237]     Train net output #1: loss = 0.30687 (* 1 = 0.30687 loss)
I1211 16:43:41.664010 13896 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1211 16:43:47.826436 13896 solver.cpp:218] Iteration 103100 (16.2268 iter/s, 6.16263s/100 iters), loss = 0.393608
I1211 16:43:47.827436 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:43:47.827436 13896 solver.cpp:237]     Train net output #1: loss = 0.393608 (* 1 = 0.393608 loss)
I1211 16:43:47.827436 13896 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1211 16:43:54.004415 13896 solver.cpp:218] Iteration 103200 (16.1891 iter/s, 6.17701s/100 iters), loss = 0.297733
I1211 16:43:54.004415 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:43:54.004415 13896 solver.cpp:237]     Train net output #1: loss = 0.297732 (* 1 = 0.297732 loss)
I1211 16:43:54.004916 13896 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1211 16:44:00.287140 13896 solver.cpp:218] Iteration 103300 (15.9171 iter/s, 6.28254s/100 iters), loss = 0.360581
I1211 16:44:00.287140 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:44:00.287140 13896 solver.cpp:237]     Train net output #1: loss = 0.360581 (* 1 = 0.360581 loss)
I1211 16:44:00.287140 13896 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1211 16:44:06.560331 13896 solver.cpp:218] Iteration 103400 (15.9437 iter/s, 6.27208s/100 iters), loss = 0.409772
I1211 16:44:06.560331 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:44:06.560331 13896 solver.cpp:237]     Train net output #1: loss = 0.409772 (* 1 = 0.409772 loss)
I1211 16:44:06.560331 13896 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1211 16:44:12.627218  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:44:12.879742 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103500.caffemodel
I1211 16:44:12.899744 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103500.solverstate
I1211 16:44:12.904743 13896 solver.cpp:330] Iteration 103500, Testing net (#0)
I1211 16:44:12.905742 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:44:14.287092  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:44:14.341601 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6788
I1211 16:44:14.341601 13896 solver.cpp:397]     Test net output #1: loss = 1.22194 (* 1 = 1.22194 loss)
I1211 16:44:14.405598 13896 solver.cpp:218] Iteration 103500 (12.7464 iter/s, 7.84538s/100 iters), loss = 0.334809
I1211 16:44:14.405598 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:44:14.405598 13896 solver.cpp:237]     Train net output #1: loss = 0.334809 (* 1 = 0.334809 loss)
I1211 16:44:14.405598 13896 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1211 16:44:20.812151 13896 solver.cpp:218] Iteration 103600 (15.6116 iter/s, 6.40549s/100 iters), loss = 0.401776
I1211 16:44:20.812151 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 16:44:20.812151 13896 solver.cpp:237]     Train net output #1: loss = 0.401775 (* 1 = 0.401775 loss)
I1211 16:44:20.812651 13896 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1211 16:44:27.147037 13896 solver.cpp:218] Iteration 103700 (15.7879 iter/s, 6.33395s/100 iters), loss = 0.341873
I1211 16:44:27.147037 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:44:27.147037 13896 solver.cpp:237]     Train net output #1: loss = 0.341873 (* 1 = 0.341873 loss)
I1211 16:44:27.147037 13896 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1211 16:44:33.406733 13896 solver.cpp:218] Iteration 103800 (15.9762 iter/s, 6.25929s/100 iters), loss = 0.30031
I1211 16:44:33.406733 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:44:33.406733 13896 solver.cpp:237]     Train net output #1: loss = 0.30031 (* 1 = 0.30031 loss)
I1211 16:44:33.406733 13896 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1211 16:44:39.655730 13896 solver.cpp:218] Iteration 103900 (16.0039 iter/s, 6.24846s/100 iters), loss = 0.360717
I1211 16:44:39.655730 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:44:39.655730 13896 solver.cpp:237]     Train net output #1: loss = 0.360717 (* 1 = 0.360717 loss)
I1211 16:44:39.655730 13896 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1211 16:44:45.578310  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:44:45.823683 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104000.caffemodel
I1211 16:44:45.839186 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104000.solverstate
I1211 16:44:45.844187 13896 solver.cpp:330] Iteration 104000, Testing net (#0)
I1211 16:44:45.844187 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:44:47.199183  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:44:47.252676 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6792
I1211 16:44:47.252676 13896 solver.cpp:397]     Test net output #1: loss = 1.21845 (* 1 = 1.21845 loss)
I1211 16:44:47.312186 13896 solver.cpp:218] Iteration 104000 (13.0618 iter/s, 7.65589s/100 iters), loss = 0.300095
I1211 16:44:47.312186 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 16:44:47.312186 13896 solver.cpp:237]     Train net output #1: loss = 0.300095 (* 1 = 0.300095 loss)
I1211 16:44:47.312186 13896 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1211 16:44:53.540769 13896 solver.cpp:218] Iteration 104100 (16.0557 iter/s, 6.22832s/100 iters), loss = 0.425123
I1211 16:44:53.540769 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:44:53.540769 13896 solver.cpp:237]     Train net output #1: loss = 0.425123 (* 1 = 0.425123 loss)
I1211 16:44:53.540769 13896 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1211 16:44:59.786052 13896 solver.cpp:218] Iteration 104200 (16.0139 iter/s, 6.24459s/100 iters), loss = 0.344276
I1211 16:44:59.786052 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:44:59.786052 13896 solver.cpp:237]     Train net output #1: loss = 0.344276 (* 1 = 0.344276 loss)
I1211 16:44:59.786052 13896 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1211 16:45:06.015275 13896 solver.cpp:218] Iteration 104300 (16.0539 iter/s, 6.229s/100 iters), loss = 0.287106
I1211 16:45:06.015275 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:45:06.015275 13896 solver.cpp:237]     Train net output #1: loss = 0.287106 (* 1 = 0.287106 loss)
I1211 16:45:06.015275 13896 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1211 16:45:12.262097 13896 solver.cpp:218] Iteration 104400 (16.0094 iter/s, 6.24635s/100 iters), loss = 0.395441
I1211 16:45:12.262598 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:45:12.262598 13896 solver.cpp:237]     Train net output #1: loss = 0.395441 (* 1 = 0.395441 loss)
I1211 16:45:12.262598 13896 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1211 16:45:18.210599  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:45:18.457542 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104500.caffemodel
I1211 16:45:18.478042 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104500.solverstate
I1211 16:45:18.483041 13896 solver.cpp:330] Iteration 104500, Testing net (#0)
I1211 16:45:18.483041 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:45:19.844544  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:45:19.898044 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6796
I1211 16:45:19.898550 13896 solver.cpp:397]     Test net output #1: loss = 1.22017 (* 1 = 1.22017 loss)
I1211 16:45:19.958052 13896 solver.cpp:218] Iteration 104500 (12.9951 iter/s, 7.69519s/100 iters), loss = 0.352493
I1211 16:45:19.958052 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:45:19.958052 13896 solver.cpp:237]     Train net output #1: loss = 0.352493 (* 1 = 0.352493 loss)
I1211 16:45:19.958052 13896 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1211 16:45:26.206104 13896 solver.cpp:218] Iteration 104600 (16.0068 iter/s, 6.24734s/100 iters), loss = 0.355017
I1211 16:45:26.206104 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:45:26.206104 13896 solver.cpp:237]     Train net output #1: loss = 0.355017 (* 1 = 0.355017 loss)
I1211 16:45:26.206104 13896 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1211 16:45:32.451108 13896 solver.cpp:218] Iteration 104700 (16.0136 iter/s, 6.24468s/100 iters), loss = 0.342819
I1211 16:45:32.451108 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:45:32.451108 13896 solver.cpp:237]     Train net output #1: loss = 0.342819 (* 1 = 0.342819 loss)
I1211 16:45:32.451108 13896 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1211 16:45:38.702246 13896 solver.cpp:218] Iteration 104800 (15.9978 iter/s, 6.25085s/100 iters), loss = 0.367793
I1211 16:45:38.702246 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:45:38.702747 13896 solver.cpp:237]     Train net output #1: loss = 0.367793 (* 1 = 0.367793 loss)
I1211 16:45:38.702747 13896 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1211 16:45:44.950770 13896 solver.cpp:218] Iteration 104900 (16.0054 iter/s, 6.2479s/100 iters), loss = 0.338495
I1211 16:45:44.950770 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:45:44.950770 13896 solver.cpp:237]     Train net output #1: loss = 0.338495 (* 1 = 0.338495 loss)
I1211 16:45:44.950770 13896 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1211 16:45:50.895779  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:45:51.141779 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105000.caffemodel
I1211 16:45:51.157779 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105000.solverstate
I1211 16:45:51.162780 13896 solver.cpp:330] Iteration 105000, Testing net (#0)
I1211 16:45:51.162780 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:45:52.520822  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:45:52.574818 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6804
I1211 16:45:52.574818 13896 solver.cpp:397]     Test net output #1: loss = 1.22397 (* 1 = 1.22397 loss)
I1211 16:45:52.633818 13896 solver.cpp:218] Iteration 105000 (13.0164 iter/s, 7.68262s/100 iters), loss = 0.275988
I1211 16:45:52.633818 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:45:52.633818 13896 solver.cpp:237]     Train net output #1: loss = 0.275988 (* 1 = 0.275988 loss)
I1211 16:45:52.633818 13896 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1211 16:45:58.886037 13896 solver.cpp:218] Iteration 105100 (15.9961 iter/s, 6.25153s/100 iters), loss = 0.354492
I1211 16:45:58.886037 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:45:58.886037 13896 solver.cpp:237]     Train net output #1: loss = 0.354492 (* 1 = 0.354492 loss)
I1211 16:45:58.886037 13896 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1211 16:46:05.144299 13896 solver.cpp:218] Iteration 105200 (15.9797 iter/s, 6.25795s/100 iters), loss = 0.315052
I1211 16:46:05.144799 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:46:05.144799 13896 solver.cpp:237]     Train net output #1: loss = 0.315052 (* 1 = 0.315052 loss)
I1211 16:46:05.144799 13896 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1211 16:46:11.395735 13896 solver.cpp:218] Iteration 105300 (15.9981 iter/s, 6.25076s/100 iters), loss = 0.281605
I1211 16:46:11.395735 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:46:11.395735 13896 solver.cpp:237]     Train net output #1: loss = 0.281605 (* 1 = 0.281605 loss)
I1211 16:46:11.395735 13896 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1211 16:46:17.639626 13896 solver.cpp:218] Iteration 105400 (16.0162 iter/s, 6.24368s/100 iters), loss = 0.425238
I1211 16:46:17.640126 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:46:17.640126 13896 solver.cpp:237]     Train net output #1: loss = 0.425238 (* 1 = 0.425238 loss)
I1211 16:46:17.640126 13896 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1211 16:46:23.584656  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:46:23.832656 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105500.caffemodel
I1211 16:46:23.848655 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105500.solverstate
I1211 16:46:23.854156 13896 solver.cpp:330] Iteration 105500, Testing net (#0)
I1211 16:46:23.854156 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:46:25.212659  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:46:25.265655 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6815
I1211 16:46:25.266155 13896 solver.cpp:397]     Test net output #1: loss = 1.22631 (* 1 = 1.22631 loss)
I1211 16:46:25.326154 13896 solver.cpp:218] Iteration 105500 (13.011 iter/s, 7.68579s/100 iters), loss = 0.252509
I1211 16:46:25.326154 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 16:46:25.326154 13896 solver.cpp:237]     Train net output #1: loss = 0.252509 (* 1 = 0.252509 loss)
I1211 16:46:25.326154 13896 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1211 16:46:31.576031 13896 solver.cpp:218] Iteration 105600 (16.0009 iter/s, 6.24965s/100 iters), loss = 0.390606
I1211 16:46:31.576531 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:46:31.576531 13896 solver.cpp:237]     Train net output #1: loss = 0.390606 (* 1 = 0.390606 loss)
I1211 16:46:31.576531 13896 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1211 16:46:37.827584 13896 solver.cpp:218] Iteration 105700 (15.9983 iter/s, 6.25068s/100 iters), loss = 0.385822
I1211 16:46:37.827584 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:46:37.827584 13896 solver.cpp:237]     Train net output #1: loss = 0.385822 (* 1 = 0.385822 loss)
I1211 16:46:37.827584 13896 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1211 16:46:44.077448 13896 solver.cpp:218] Iteration 105800 (16.0015 iter/s, 6.24941s/100 iters), loss = 0.316824
I1211 16:46:44.077448 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:46:44.077448 13896 solver.cpp:237]     Train net output #1: loss = 0.316824 (* 1 = 0.316824 loss)
I1211 16:46:44.077448 13896 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1211 16:46:50.360476 13896 solver.cpp:218] Iteration 105900 (15.9157 iter/s, 6.28311s/100 iters), loss = 0.370615
I1211 16:46:50.360476 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:46:50.360476 13896 solver.cpp:237]     Train net output #1: loss = 0.370615 (* 1 = 0.370615 loss)
I1211 16:46:50.360476 13896 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1211 16:46:56.237272  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:46:56.479851 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106000.caffemodel
I1211 16:46:56.494851 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106000.solverstate
I1211 16:46:56.498852 13896 solver.cpp:330] Iteration 106000, Testing net (#0)
I1211 16:46:56.499851 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:46:57.835315  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:46:57.888312 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6765
I1211 16:46:57.888312 13896 solver.cpp:397]     Test net output #1: loss = 1.23391 (* 1 = 1.23391 loss)
I1211 16:46:57.947629 13896 solver.cpp:218] Iteration 106000 (13.1815 iter/s, 7.5864s/100 iters), loss = 0.291107
I1211 16:46:57.947629 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:46:57.947629 13896 solver.cpp:237]     Train net output #1: loss = 0.291107 (* 1 = 0.291107 loss)
I1211 16:46:57.947629 13896 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1211 16:47:04.111232 13896 solver.cpp:218] Iteration 106100 (16.2262 iter/s, 6.16288s/100 iters), loss = 0.383173
I1211 16:47:04.111232 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:47:04.111232 13896 solver.cpp:237]     Train net output #1: loss = 0.383173 (* 1 = 0.383173 loss)
I1211 16:47:04.111232 13896 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1211 16:47:10.273790 13896 solver.cpp:218] Iteration 106200 (16.2263 iter/s, 6.16283s/100 iters), loss = 0.313395
I1211 16:47:10.273790 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:47:10.273790 13896 solver.cpp:237]     Train net output #1: loss = 0.313395 (* 1 = 0.313395 loss)
I1211 16:47:10.273790 13896 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1211 16:47:16.443225 13896 solver.cpp:218] Iteration 106300 (16.2097 iter/s, 6.16913s/100 iters), loss = 0.313149
I1211 16:47:16.444241 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:47:16.444241 13896 solver.cpp:237]     Train net output #1: loss = 0.313149 (* 1 = 0.313149 loss)
I1211 16:47:16.444241 13896 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1211 16:47:22.619958 13896 solver.cpp:218] Iteration 106400 (16.1925 iter/s, 6.1757s/100 iters), loss = 0.366477
I1211 16:47:22.619958 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:47:22.619958 13896 solver.cpp:237]     Train net output #1: loss = 0.366477 (* 1 = 0.366477 loss)
I1211 16:47:22.619958 13896 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1211 16:47:28.502427  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:47:28.747444 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106500.caffemodel
I1211 16:47:28.762444 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106500.solverstate
I1211 16:47:28.767444 13896 solver.cpp:330] Iteration 106500, Testing net (#0)
I1211 16:47:28.767444 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:47:30.105545  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:47:30.158551 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6794
I1211 16:47:30.158551 13896 solver.cpp:397]     Test net output #1: loss = 1.23348 (* 1 = 1.23348 loss)
I1211 16:47:30.218053 13896 solver.cpp:218] Iteration 106500 (13.162 iter/s, 7.59766s/100 iters), loss = 0.322976
I1211 16:47:30.218561 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:47:30.218561 13896 solver.cpp:237]     Train net output #1: loss = 0.322976 (* 1 = 0.322976 loss)
I1211 16:47:30.218561 13896 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1211 16:47:36.388999 13896 solver.cpp:218] Iteration 106600 (16.2053 iter/s, 6.1708s/100 iters), loss = 0.397133
I1211 16:47:36.388999 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:47:36.389999 13896 solver.cpp:237]     Train net output #1: loss = 0.397133 (* 1 = 0.397133 loss)
I1211 16:47:36.389999 13896 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1211 16:47:42.559263 13896 solver.cpp:218] Iteration 106700 (16.2102 iter/s, 6.16895s/100 iters), loss = 0.315154
I1211 16:47:42.559263 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:47:42.559263 13896 solver.cpp:237]     Train net output #1: loss = 0.315154 (* 1 = 0.315154 loss)
I1211 16:47:42.559263 13896 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1211 16:47:48.899577 13896 solver.cpp:218] Iteration 106800 (15.7732 iter/s, 6.33986s/100 iters), loss = 0.360989
I1211 16:47:48.899577 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:47:48.899577 13896 solver.cpp:237]     Train net output #1: loss = 0.360989 (* 1 = 0.360989 loss)
I1211 16:47:48.899577 13896 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1211 16:47:55.258898 13896 solver.cpp:218] Iteration 106900 (15.7264 iter/s, 6.35874s/100 iters), loss = 0.386547
I1211 16:47:55.258898 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:47:55.258898 13896 solver.cpp:237]     Train net output #1: loss = 0.386547 (* 1 = 0.386547 loss)
I1211 16:47:55.258898 13896 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1211 16:48:01.143332  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:48:01.389367 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107000.caffemodel
I1211 16:48:01.404367 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107000.solverstate
I1211 16:48:01.409368 13896 solver.cpp:330] Iteration 107000, Testing net (#0)
I1211 16:48:01.409368 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:48:02.748486  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:48:02.800510 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6778
I1211 16:48:02.800510 13896 solver.cpp:397]     Test net output #1: loss = 1.23068 (* 1 = 1.23068 loss)
I1211 16:48:02.859510 13896 solver.cpp:218] Iteration 107000 (13.1572 iter/s, 7.60037s/100 iters), loss = 0.314064
I1211 16:48:02.859510 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 16:48:02.860011 13896 solver.cpp:237]     Train net output #1: loss = 0.314064 (* 1 = 0.314064 loss)
I1211 16:48:02.860011 13896 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1211 16:48:09.080371 13896 solver.cpp:218] Iteration 107100 (16.0757 iter/s, 6.22057s/100 iters), loss = 0.418135
I1211 16:48:09.080371 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:48:09.080371 13896 solver.cpp:237]     Train net output #1: loss = 0.418135 (* 1 = 0.418135 loss)
I1211 16:48:09.080371 13896 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1211 16:48:15.330801 13896 solver.cpp:218] Iteration 107200 (16.0012 iter/s, 6.24953s/100 iters), loss = 0.347846
I1211 16:48:15.330801 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:48:15.330801 13896 solver.cpp:237]     Train net output #1: loss = 0.347846 (* 1 = 0.347846 loss)
I1211 16:48:15.330801 13896 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1211 16:48:21.712216 13896 solver.cpp:218] Iteration 107300 (15.6713 iter/s, 6.38111s/100 iters), loss = 0.358189
I1211 16:48:21.712216 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:48:21.712216 13896 solver.cpp:237]     Train net output #1: loss = 0.358189 (* 1 = 0.358189 loss)
I1211 16:48:21.712216 13896 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1211 16:48:28.023957 13896 solver.cpp:218] Iteration 107400 (15.8445 iter/s, 6.31135s/100 iters), loss = 0.417929
I1211 16:48:28.024456 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:48:28.024456 13896 solver.cpp:237]     Train net output #1: loss = 0.417929 (* 1 = 0.417929 loss)
I1211 16:48:28.024456 13896 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1211 16:48:33.989187  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:48:34.236227 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107500.caffemodel
I1211 16:48:34.251724 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107500.solverstate
I1211 16:48:34.256724 13896 solver.cpp:330] Iteration 107500, Testing net (#0)
I1211 16:48:34.257226 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:48:35.620726  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:48:35.674226 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6808
I1211 16:48:35.674226 13896 solver.cpp:397]     Test net output #1: loss = 1.23296 (* 1 = 1.23296 loss)
I1211 16:48:35.738225 13896 solver.cpp:218] Iteration 107500 (12.9638 iter/s, 7.71377s/100 iters), loss = 0.298119
I1211 16:48:35.738726 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:48:35.738726 13896 solver.cpp:237]     Train net output #1: loss = 0.298119 (* 1 = 0.298119 loss)
I1211 16:48:35.738726 13896 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1211 16:48:41.950145 13896 solver.cpp:218] Iteration 107600 (16.1002 iter/s, 6.21112s/100 iters), loss = 0.350313
I1211 16:48:41.950145 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:48:41.950145 13896 solver.cpp:237]     Train net output #1: loss = 0.350313 (* 1 = 0.350313 loss)
I1211 16:48:41.950145 13896 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1211 16:48:48.220396 13896 solver.cpp:218] Iteration 107700 (15.9499 iter/s, 6.26962s/100 iters), loss = 0.303651
I1211 16:48:48.220396 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:48:48.220396 13896 solver.cpp:237]     Train net output #1: loss = 0.303651 (* 1 = 0.303651 loss)
I1211 16:48:48.220396 13896 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1211 16:48:54.508054 13896 solver.cpp:218] Iteration 107800 (15.9062 iter/s, 6.28684s/100 iters), loss = 0.360336
I1211 16:48:54.508054 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:48:54.508054 13896 solver.cpp:237]     Train net output #1: loss = 0.360336 (* 1 = 0.360336 loss)
I1211 16:48:54.508054 13896 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1211 16:49:00.782567 13896 solver.cpp:218] Iteration 107900 (15.9387 iter/s, 6.27406s/100 iters), loss = 0.381029
I1211 16:49:00.782567 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:49:00.782567 13896 solver.cpp:237]     Train net output #1: loss = 0.381028 (* 1 = 0.381028 loss)
I1211 16:49:00.782567 13896 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1211 16:49:06.749212  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:49:06.997211 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108000.caffemodel
I1211 16:49:07.013212 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108000.solverstate
I1211 16:49:07.018713 13896 solver.cpp:330] Iteration 108000, Testing net (#0)
I1211 16:49:07.019215 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:49:08.384712  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:49:08.438211 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6803
I1211 16:49:08.438211 13896 solver.cpp:397]     Test net output #1: loss = 1.24113 (* 1 = 1.24113 loss)
I1211 16:49:08.497711 13896 solver.cpp:218] Iteration 108000 (12.962 iter/s, 7.71484s/100 iters), loss = 0.361555
I1211 16:49:08.497711 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:49:08.497711 13896 solver.cpp:237]     Train net output #1: loss = 0.361555 (* 1 = 0.361555 loss)
I1211 16:49:08.497711 13896 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1211 16:49:14.786595 13896 solver.cpp:218] Iteration 108100 (15.9024 iter/s, 6.28837s/100 iters), loss = 0.332616
I1211 16:49:14.786595 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 16:49:14.786595 13896 solver.cpp:237]     Train net output #1: loss = 0.332616 (* 1 = 0.332616 loss)
I1211 16:49:14.786595 13896 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1211 16:49:21.070895 13896 solver.cpp:218] Iteration 108200 (15.9139 iter/s, 6.28381s/100 iters), loss = 0.379463
I1211 16:49:21.070895 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:49:21.070895 13896 solver.cpp:237]     Train net output #1: loss = 0.379463 (* 1 = 0.379463 loss)
I1211 16:49:21.070895 13896 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1211 16:49:27.350417 13896 solver.cpp:218] Iteration 108300 (15.9257 iter/s, 6.27915s/100 iters), loss = 0.375024
I1211 16:49:27.350417 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:49:27.350417 13896 solver.cpp:237]     Train net output #1: loss = 0.375024 (* 1 = 0.375024 loss)
I1211 16:49:27.350417 13896 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1211 16:49:33.611441 13896 solver.cpp:218] Iteration 108400 (15.9732 iter/s, 6.26049s/100 iters), loss = 0.382139
I1211 16:49:33.611441 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:49:33.611441 13896 solver.cpp:237]     Train net output #1: loss = 0.382139 (* 1 = 0.382139 loss)
I1211 16:49:33.611441 13896 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1211 16:49:39.589583  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:49:39.836583 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108500.caffemodel
I1211 16:49:39.853582 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108500.solverstate
I1211 16:49:39.858583 13896 solver.cpp:330] Iteration 108500, Testing net (#0)
I1211 16:49:39.859084 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:49:41.222586  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:49:41.276583 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6795
I1211 16:49:41.276583 13896 solver.cpp:397]     Test net output #1: loss = 1.23378 (* 1 = 1.23378 loss)
I1211 16:49:41.337083 13896 solver.cpp:218] Iteration 108500 (12.945 iter/s, 7.72497s/100 iters), loss = 0.239726
I1211 16:49:41.337083 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 16:49:41.337083 13896 solver.cpp:237]     Train net output #1: loss = 0.239726 (* 1 = 0.239726 loss)
I1211 16:49:41.337083 13896 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1211 16:49:47.611007 13896 solver.cpp:218] Iteration 108600 (15.9396 iter/s, 6.27368s/100 iters), loss = 0.350855
I1211 16:49:47.611007 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:49:47.611007 13896 solver.cpp:237]     Train net output #1: loss = 0.350855 (* 1 = 0.350855 loss)
I1211 16:49:47.611007 13896 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1211 16:49:53.898141 13896 solver.cpp:218] Iteration 108700 (15.9073 iter/s, 6.28644s/100 iters), loss = 0.34033
I1211 16:49:53.898141 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:49:53.898141 13896 solver.cpp:237]     Train net output #1: loss = 0.34033 (* 1 = 0.34033 loss)
I1211 16:49:53.898141 13896 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1211 16:50:00.163899 13896 solver.cpp:218] Iteration 108800 (15.9608 iter/s, 6.26537s/100 iters), loss = 0.349741
I1211 16:50:00.163899 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:50:00.163899 13896 solver.cpp:237]     Train net output #1: loss = 0.349741 (* 1 = 0.349741 loss)
I1211 16:50:00.163899 13896 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1211 16:50:06.455946 13896 solver.cpp:218] Iteration 108900 (15.8944 iter/s, 6.29154s/100 iters), loss = 0.409171
I1211 16:50:06.455946 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:50:06.455946 13896 solver.cpp:237]     Train net output #1: loss = 0.409171 (* 1 = 0.409171 loss)
I1211 16:50:06.455946 13896 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1211 16:50:12.420822  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:50:12.666821 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109000.caffemodel
I1211 16:50:12.683322 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109000.solverstate
I1211 16:50:12.688323 13896 solver.cpp:330] Iteration 109000, Testing net (#0)
I1211 16:50:12.688323 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:50:14.052824  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:50:14.106322 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6781
I1211 16:50:14.106322 13896 solver.cpp:397]     Test net output #1: loss = 1.24901 (* 1 = 1.24901 loss)
I1211 16:50:14.166821 13896 solver.cpp:218] Iteration 109000 (12.9701 iter/s, 7.71002s/100 iters), loss = 0.330241
I1211 16:50:14.166821 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:50:14.166821 13896 solver.cpp:237]     Train net output #1: loss = 0.330241 (* 1 = 0.330241 loss)
I1211 16:50:14.166821 13896 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1211 16:50:20.440399 13896 solver.cpp:218] Iteration 109100 (15.9407 iter/s, 6.27326s/100 iters), loss = 0.324542
I1211 16:50:20.440399 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:50:20.440399 13896 solver.cpp:237]     Train net output #1: loss = 0.324542 (* 1 = 0.324542 loss)
I1211 16:50:20.440399 13896 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1211 16:50:26.706332 13896 solver.cpp:218] Iteration 109200 (15.9606 iter/s, 6.26543s/100 iters), loss = 0.269552
I1211 16:50:26.706332 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:50:26.706332 13896 solver.cpp:237]     Train net output #1: loss = 0.269552 (* 1 = 0.269552 loss)
I1211 16:50:26.706332 13896 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1211 16:50:32.964376 13896 solver.cpp:218] Iteration 109300 (15.9814 iter/s, 6.25726s/100 iters), loss = 0.309362
I1211 16:50:32.964376 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:50:32.964376 13896 solver.cpp:237]     Train net output #1: loss = 0.309362 (* 1 = 0.309362 loss)
I1211 16:50:32.964376 13896 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1211 16:50:39.240895 13896 solver.cpp:218] Iteration 109400 (15.9326 iter/s, 6.27643s/100 iters), loss = 0.399327
I1211 16:50:39.240895 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:50:39.240895 13896 solver.cpp:237]     Train net output #1: loss = 0.399327 (* 1 = 0.399327 loss)
I1211 16:50:39.240895 13896 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1211 16:50:45.213897  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:50:45.463893 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109500.caffemodel
I1211 16:50:45.485395 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109500.solverstate
I1211 16:50:45.491894 13896 solver.cpp:330] Iteration 109500, Testing net (#0)
I1211 16:50:45.491894 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:50:46.858935  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:50:46.912436 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6777
I1211 16:50:46.912935 13896 solver.cpp:397]     Test net output #1: loss = 1.24672 (* 1 = 1.24672 loss)
I1211 16:50:46.972434 13896 solver.cpp:218] Iteration 109500 (12.9352 iter/s, 7.73084s/100 iters), loss = 0.272125
I1211 16:50:46.972434 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 16:50:46.972434 13896 solver.cpp:237]     Train net output #1: loss = 0.272125 (* 1 = 0.272125 loss)
I1211 16:50:46.972434 13896 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1211 16:50:53.254750 13896 solver.cpp:218] Iteration 109600 (15.9194 iter/s, 6.28166s/100 iters), loss = 0.348673
I1211 16:50:53.254750 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:50:53.254750 13896 solver.cpp:237]     Train net output #1: loss = 0.348673 (* 1 = 0.348673 loss)
I1211 16:50:53.254750 13896 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1211 16:50:59.532439 13896 solver.cpp:218] Iteration 109700 (15.9311 iter/s, 6.27701s/100 iters), loss = 0.245125
I1211 16:50:59.532439 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:50:59.532439 13896 solver.cpp:237]     Train net output #1: loss = 0.245125 (* 1 = 0.245125 loss)
I1211 16:50:59.532439 13896 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1211 16:51:05.810461 13896 solver.cpp:218] Iteration 109800 (15.9299 iter/s, 6.2775s/100 iters), loss = 0.308633
I1211 16:51:05.810461 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:51:05.810461 13896 solver.cpp:237]     Train net output #1: loss = 0.308632 (* 1 = 0.308632 loss)
I1211 16:51:05.810461 13896 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1211 16:51:12.101550 13896 solver.cpp:218] Iteration 109900 (15.8967 iter/s, 6.29062s/100 iters), loss = 0.362135
I1211 16:51:12.101550 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:51:12.101550 13896 solver.cpp:237]     Train net output #1: loss = 0.362134 (* 1 = 0.362134 loss)
I1211 16:51:12.101550 13896 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1211 16:51:18.075589  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:51:18.324093 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110000.caffemodel
I1211 16:51:18.339089 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110000.solverstate
I1211 16:51:18.345589 13896 solver.cpp:330] Iteration 110000, Testing net (#0)
I1211 16:51:18.346091 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:51:19.730589  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:51:19.784593 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6795
I1211 16:51:19.784593 13896 solver.cpp:397]     Test net output #1: loss = 1.23732 (* 1 = 1.23732 loss)
I1211 16:51:19.844087 13896 solver.cpp:218] Iteration 110000 (12.9159 iter/s, 7.74237s/100 iters), loss = 0.270332
I1211 16:51:19.844588 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 16:51:19.844588 13896 solver.cpp:237]     Train net output #1: loss = 0.270332 (* 1 = 0.270332 loss)
I1211 16:51:19.844588 13896 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1211 16:51:26.120559 13896 solver.cpp:218] Iteration 110100 (15.9347 iter/s, 6.27561s/100 iters), loss = 0.433347
I1211 16:51:26.120559 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 16:51:26.120559 13896 solver.cpp:237]     Train net output #1: loss = 0.433347 (* 1 = 0.433347 loss)
I1211 16:51:26.120559 13896 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1211 16:51:32.397985 13896 solver.cpp:218] Iteration 110200 (15.931 iter/s, 6.27706s/100 iters), loss = 0.24207
I1211 16:51:32.397985 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 16:51:32.397985 13896 solver.cpp:237]     Train net output #1: loss = 0.24207 (* 1 = 0.24207 loss)
I1211 16:51:32.397985 13896 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1211 16:51:38.677134 13896 solver.cpp:218] Iteration 110300 (15.9272 iter/s, 6.27858s/100 iters), loss = 0.323319
I1211 16:51:38.677134 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:51:38.677134 13896 solver.cpp:237]     Train net output #1: loss = 0.323319 (* 1 = 0.323319 loss)
I1211 16:51:38.677134 13896 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1211 16:51:44.946004 13896 solver.cpp:218] Iteration 110400 (15.9533 iter/s, 6.2683s/100 iters), loss = 0.325947
I1211 16:51:44.946004 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:51:44.946004 13896 solver.cpp:237]     Train net output #1: loss = 0.325947 (* 1 = 0.325947 loss)
I1211 16:51:44.946004 13896 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1211 16:51:50.913972  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:51:51.161973 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110500.caffemodel
I1211 16:51:51.178973 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110500.solverstate
I1211 16:51:51.183473 13896 solver.cpp:330] Iteration 110500, Testing net (#0)
I1211 16:51:51.183972 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:51:52.547472  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:51:52.601472 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6779
I1211 16:51:52.601472 13896 solver.cpp:397]     Test net output #1: loss = 1.25358 (* 1 = 1.25358 loss)
I1211 16:51:52.660971 13896 solver.cpp:218] Iteration 110500 (12.9624 iter/s, 7.71462s/100 iters), loss = 0.298452
I1211 16:51:52.660971 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:51:52.660971 13896 solver.cpp:237]     Train net output #1: loss = 0.298452 (* 1 = 0.298452 loss)
I1211 16:51:52.660971 13896 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1211 16:51:58.929894 13896 solver.cpp:218] Iteration 110600 (15.9535 iter/s, 6.26824s/100 iters), loss = 0.333048
I1211 16:51:58.929894 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:51:58.929894 13896 solver.cpp:237]     Train net output #1: loss = 0.333048 (* 1 = 0.333048 loss)
I1211 16:51:58.929894 13896 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1211 16:52:05.204676 13896 solver.cpp:218] Iteration 110700 (15.9376 iter/s, 6.27449s/100 iters), loss = 0.289131
I1211 16:52:05.204676 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 16:52:05.204676 13896 solver.cpp:237]     Train net output #1: loss = 0.289131 (* 1 = 0.289131 loss)
I1211 16:52:05.204676 13896 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1211 16:52:11.476835 13896 solver.cpp:218] Iteration 110800 (15.9453 iter/s, 6.27145s/100 iters), loss = 0.352628
I1211 16:52:11.476835 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:52:11.476835 13896 solver.cpp:237]     Train net output #1: loss = 0.352628 (* 1 = 0.352628 loss)
I1211 16:52:11.476835 13896 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1211 16:52:17.746057 13896 solver.cpp:218] Iteration 110900 (15.9513 iter/s, 6.26908s/100 iters), loss = 0.327509
I1211 16:52:17.746558 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:52:17.746558 13896 solver.cpp:237]     Train net output #1: loss = 0.327509 (* 1 = 0.327509 loss)
I1211 16:52:17.746558 13896 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1211 16:52:23.713992  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:52:23.961493 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111000.caffemodel
I1211 16:52:23.982992 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111000.solverstate
I1211 16:52:23.987993 13896 solver.cpp:330] Iteration 111000, Testing net (#0)
I1211 16:52:23.987993 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:52:25.353992  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:52:25.407493 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6783
I1211 16:52:25.407994 13896 solver.cpp:397]     Test net output #1: loss = 1.24369 (* 1 = 1.24369 loss)
I1211 16:52:25.467993 13896 solver.cpp:218] Iteration 111000 (12.9517 iter/s, 7.72101s/100 iters), loss = 0.350513
I1211 16:52:25.467993 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:52:25.467993 13896 solver.cpp:237]     Train net output #1: loss = 0.350513 (* 1 = 0.350513 loss)
I1211 16:52:25.467993 13896 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1211 16:52:31.747756 13896 solver.cpp:218] Iteration 111100 (15.9257 iter/s, 6.27917s/100 iters), loss = 0.348433
I1211 16:52:31.747756 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:52:31.747756 13896 solver.cpp:237]     Train net output #1: loss = 0.348433 (* 1 = 0.348433 loss)
I1211 16:52:31.747756 13896 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1211 16:52:38.025372 13896 solver.cpp:218] Iteration 111200 (15.9308 iter/s, 6.27716s/100 iters), loss = 0.298534
I1211 16:52:38.025372 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:52:38.025372 13896 solver.cpp:237]     Train net output #1: loss = 0.298534 (* 1 = 0.298534 loss)
I1211 16:52:38.025372 13896 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1211 16:52:44.299022 13896 solver.cpp:218] Iteration 111300 (15.9401 iter/s, 6.2735s/100 iters), loss = 0.362772
I1211 16:52:44.299022 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:52:44.299522 13896 solver.cpp:237]     Train net output #1: loss = 0.362772 (* 1 = 0.362772 loss)
I1211 16:52:44.299522 13896 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1211 16:52:50.572578 13896 solver.cpp:218] Iteration 111400 (15.9418 iter/s, 6.27282s/100 iters), loss = 0.281304
I1211 16:52:50.572578 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:52:50.572578 13896 solver.cpp:237]     Train net output #1: loss = 0.281304 (* 1 = 0.281304 loss)
I1211 16:52:50.572578 13896 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1211 16:52:56.540575  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:52:56.787073 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111500.caffemodel
I1211 16:52:56.803072 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111500.solverstate
I1211 16:52:56.808073 13896 solver.cpp:330] Iteration 111500, Testing net (#0)
I1211 16:52:56.808073 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:52:58.176573  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:52:58.230073 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6761
I1211 16:52:58.230073 13896 solver.cpp:397]     Test net output #1: loss = 1.25829 (* 1 = 1.25829 loss)
I1211 16:52:58.289072 13896 solver.cpp:218] Iteration 111500 (12.96 iter/s, 7.71606s/100 iters), loss = 0.220488
I1211 16:52:58.289072 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 16:52:58.289072 13896 solver.cpp:237]     Train net output #1: loss = 0.220488 (* 1 = 0.220488 loss)
I1211 16:52:58.289072 13896 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1211 16:53:04.556649 13896 solver.cpp:218] Iteration 111600 (15.9565 iter/s, 6.26704s/100 iters), loss = 0.351455
I1211 16:53:04.556649 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:53:04.556649 13896 solver.cpp:237]     Train net output #1: loss = 0.351454 (* 1 = 0.351454 loss)
I1211 16:53:04.556649 13896 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1211 16:53:10.832500 13896 solver.cpp:218] Iteration 111700 (15.936 iter/s, 6.27511s/100 iters), loss = 0.277647
I1211 16:53:10.832500 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:53:10.832500 13896 solver.cpp:237]     Train net output #1: loss = 0.277647 (* 1 = 0.277647 loss)
I1211 16:53:10.832500 13896 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1211 16:53:17.101405 13896 solver.cpp:218] Iteration 111800 (15.9518 iter/s, 6.2689s/100 iters), loss = 0.307105
I1211 16:53:17.101905 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:53:17.101905 13896 solver.cpp:237]     Train net output #1: loss = 0.307105 (* 1 = 0.307105 loss)
I1211 16:53:17.101905 13896 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1211 16:53:23.377115 13896 solver.cpp:218] Iteration 111900 (15.9364 iter/s, 6.27495s/100 iters), loss = 0.377937
I1211 16:53:23.377115 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:53:23.377115 13896 solver.cpp:237]     Train net output #1: loss = 0.377937 (* 1 = 0.377937 loss)
I1211 16:53:23.377115 13896 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1211 16:53:29.345342  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:53:29.591840 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_112000.caffemodel
I1211 16:53:29.607841 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_112000.solverstate
I1211 16:53:29.613342 13896 solver.cpp:330] Iteration 112000, Testing net (#0)
I1211 16:53:29.613839 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:53:30.973343  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:53:31.026342 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6769
I1211 16:53:31.026342 13896 solver.cpp:397]     Test net output #1: loss = 1.25035 (* 1 = 1.25035 loss)
I1211 16:53:31.086850 13896 solver.cpp:218] Iteration 112000 (12.9713 iter/s, 7.70933s/100 iters), loss = 0.217799
I1211 16:53:31.086850 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:53:31.086850 13896 solver.cpp:237]     Train net output #1: loss = 0.217798 (* 1 = 0.217798 loss)
I1211 16:53:31.086850 13896 sgd_solver.cpp:105] Iteration 112000, lr = 0.001
I1211 16:53:37.373481 13896 solver.cpp:218] Iteration 112100 (15.908 iter/s, 6.28613s/100 iters), loss = 0.346668
I1211 16:53:37.373481 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:53:37.373481 13896 solver.cpp:237]     Train net output #1: loss = 0.346668 (* 1 = 0.346668 loss)
I1211 16:53:37.373481 13896 sgd_solver.cpp:105] Iteration 112100, lr = 0.001
I1211 16:53:43.651190 13896 solver.cpp:218] Iteration 112200 (15.9302 iter/s, 6.27737s/100 iters), loss = 0.264451
I1211 16:53:43.651688 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:53:43.651688 13896 solver.cpp:237]     Train net output #1: loss = 0.26445 (* 1 = 0.26445 loss)
I1211 16:53:43.651688 13896 sgd_solver.cpp:105] Iteration 112200, lr = 0.001
I1211 16:53:49.914187 13896 solver.cpp:218] Iteration 112300 (15.968 iter/s, 6.26253s/100 iters), loss = 0.241209
I1211 16:53:49.914687 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:53:49.914687 13896 solver.cpp:237]     Train net output #1: loss = 0.241209 (* 1 = 0.241209 loss)
I1211 16:53:49.914687 13896 sgd_solver.cpp:105] Iteration 112300, lr = 0.001
I1211 16:53:56.176308 13896 solver.cpp:218] Iteration 112400 (15.9706 iter/s, 6.26149s/100 iters), loss = 0.377029
I1211 16:53:56.176308 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:53:56.176308 13896 solver.cpp:237]     Train net output #1: loss = 0.377029 (* 1 = 0.377029 loss)
I1211 16:53:56.176308 13896 sgd_solver.cpp:105] Iteration 112400, lr = 0.001
I1211 16:54:02.135519  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:54:02.384531 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_112500.caffemodel
I1211 16:54:02.402029 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_112500.solverstate
I1211 16:54:02.407029 13896 solver.cpp:330] Iteration 112500, Testing net (#0)
I1211 16:54:02.407029 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:54:03.765529  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:54:03.818527 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6803
I1211 16:54:03.819028 13896 solver.cpp:397]     Test net output #1: loss = 1.25302 (* 1 = 1.25302 loss)
I1211 16:54:03.878530 13896 solver.cpp:218] Iteration 112500 (12.9839 iter/s, 7.70184s/100 iters), loss = 0.278638
I1211 16:54:03.879029 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:54:03.879029 13896 solver.cpp:237]     Train net output #1: loss = 0.278638 (* 1 = 0.278638 loss)
I1211 16:54:03.879029 13896 sgd_solver.cpp:105] Iteration 112500, lr = 0.001
I1211 16:54:10.149088 13896 solver.cpp:218] Iteration 112600 (15.9491 iter/s, 6.26994s/100 iters), loss = 0.327718
I1211 16:54:10.149088 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:54:10.149088 13896 solver.cpp:237]     Train net output #1: loss = 0.327718 (* 1 = 0.327718 loss)
I1211 16:54:10.149088 13896 sgd_solver.cpp:105] Iteration 112600, lr = 0.001
I1211 16:54:16.416888 13896 solver.cpp:218] Iteration 112700 (15.9556 iter/s, 6.26739s/100 iters), loss = 0.282913
I1211 16:54:16.417388 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:54:16.417388 13896 solver.cpp:237]     Train net output #1: loss = 0.282913 (* 1 = 0.282913 loss)
I1211 16:54:16.417388 13896 sgd_solver.cpp:105] Iteration 112700, lr = 0.001
I1211 16:54:22.694211 13896 solver.cpp:218] Iteration 112800 (15.9327 iter/s, 6.27642s/100 iters), loss = 0.290057
I1211 16:54:22.694211 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:54:22.694211 13896 solver.cpp:237]     Train net output #1: loss = 0.290056 (* 1 = 0.290056 loss)
I1211 16:54:22.694211 13896 sgd_solver.cpp:105] Iteration 112800, lr = 0.001
I1211 16:54:28.970605 13896 solver.cpp:218] Iteration 112900 (15.9342 iter/s, 6.27582s/100 iters), loss = 0.355031
I1211 16:54:28.970605 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:54:28.970605 13896 solver.cpp:237]     Train net output #1: loss = 0.355031 (* 1 = 0.355031 loss)
I1211 16:54:28.970605 13896 sgd_solver.cpp:105] Iteration 112900, lr = 0.001
I1211 16:54:34.918680  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:54:35.167264 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_113000.caffemodel
I1211 16:54:35.182762 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_113000.solverstate
I1211 16:54:35.187764 13896 solver.cpp:330] Iteration 113000, Testing net (#0)
I1211 16:54:35.187764 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:54:36.548759  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:54:36.602273 13896 solver.cpp:397]     Test net output #0: accuracy = 0.677
I1211 16:54:36.602273 13896 solver.cpp:397]     Test net output #1: loss = 1.26532 (* 1 = 1.26532 loss)
I1211 16:54:36.662760 13896 solver.cpp:218] Iteration 113000 (13.001 iter/s, 7.6917s/100 iters), loss = 0.261266
I1211 16:54:36.662760 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 16:54:36.662760 13896 solver.cpp:237]     Train net output #1: loss = 0.261266 (* 1 = 0.261266 loss)
I1211 16:54:36.662760 13896 sgd_solver.cpp:105] Iteration 113000, lr = 0.001
I1211 16:54:42.943416 13896 solver.cpp:218] Iteration 113100 (15.923 iter/s, 6.28021s/100 iters), loss = 0.310485
I1211 16:54:42.943416 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:54:42.943416 13896 solver.cpp:237]     Train net output #1: loss = 0.310484 (* 1 = 0.310484 loss)
I1211 16:54:42.943416 13896 sgd_solver.cpp:105] Iteration 113100, lr = 0.001
I1211 16:54:49.219789 13896 solver.cpp:218] Iteration 113200 (15.9344 iter/s, 6.27574s/100 iters), loss = 0.256809
I1211 16:54:49.219789 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:54:49.219789 13896 solver.cpp:237]     Train net output #1: loss = 0.256809 (* 1 = 0.256809 loss)
I1211 16:54:49.219789 13896 sgd_solver.cpp:105] Iteration 113200, lr = 0.001
I1211 16:54:55.491878 13896 solver.cpp:218] Iteration 113300 (15.9444 iter/s, 6.2718s/100 iters), loss = 0.327364
I1211 16:54:55.492378 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:54:55.492378 13896 solver.cpp:237]     Train net output #1: loss = 0.327364 (* 1 = 0.327364 loss)
I1211 16:54:55.492378 13896 sgd_solver.cpp:105] Iteration 113300, lr = 0.001
I1211 16:55:01.770838 13896 solver.cpp:218] Iteration 113400 (15.9281 iter/s, 6.27821s/100 iters), loss = 0.3632
I1211 16:55:01.770838 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:55:01.770838 13896 solver.cpp:237]     Train net output #1: loss = 0.363199 (* 1 = 0.363199 loss)
I1211 16:55:01.770838 13896 sgd_solver.cpp:105] Iteration 113400, lr = 0.001
I1211 16:55:07.731153  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:55:07.978152 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_113500.caffemodel
I1211 16:55:07.995653 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_113500.solverstate
I1211 16:55:08.000154 13896 solver.cpp:330] Iteration 113500, Testing net (#0)
I1211 16:55:08.000659 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:55:09.364153  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:55:09.417654 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6793
I1211 16:55:09.417654 13896 solver.cpp:397]     Test net output #1: loss = 1.25211 (* 1 = 1.25211 loss)
I1211 16:55:09.477151 13896 solver.cpp:218] Iteration 113500 (12.9772 iter/s, 7.70584s/100 iters), loss = 0.295694
I1211 16:55:09.477151 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:55:09.477151 13896 solver.cpp:237]     Train net output #1: loss = 0.295694 (* 1 = 0.295694 loss)
I1211 16:55:09.477151 13896 sgd_solver.cpp:105] Iteration 113500, lr = 0.001
I1211 16:55:15.753042 13896 solver.cpp:218] Iteration 113600 (15.9357 iter/s, 6.27521s/100 iters), loss = 0.304717
I1211 16:55:15.753042 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:55:15.753042 13896 solver.cpp:237]     Train net output #1: loss = 0.304717 (* 1 = 0.304717 loss)
I1211 16:55:15.753042 13896 sgd_solver.cpp:105] Iteration 113600, lr = 0.001
I1211 16:55:22.033344 13896 solver.cpp:218] Iteration 113700 (15.9229 iter/s, 6.28026s/100 iters), loss = 0.316059
I1211 16:55:22.033845 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:55:22.033845 13896 solver.cpp:237]     Train net output #1: loss = 0.316059 (* 1 = 0.316059 loss)
I1211 16:55:22.033845 13896 sgd_solver.cpp:105] Iteration 113700, lr = 0.001
I1211 16:55:28.293849 13896 solver.cpp:218] Iteration 113800 (15.9744 iter/s, 6.26002s/100 iters), loss = 0.322837
I1211 16:55:28.294349 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:55:28.294349 13896 solver.cpp:237]     Train net output #1: loss = 0.322837 (* 1 = 0.322837 loss)
I1211 16:55:28.294349 13896 sgd_solver.cpp:105] Iteration 113800, lr = 0.001
I1211 16:55:34.566521 13896 solver.cpp:218] Iteration 113900 (15.9445 iter/s, 6.27177s/100 iters), loss = 0.373486
I1211 16:55:34.566521 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 16:55:34.566521 13896 solver.cpp:237]     Train net output #1: loss = 0.373486 (* 1 = 0.373486 loss)
I1211 16:55:34.566521 13896 sgd_solver.cpp:105] Iteration 113900, lr = 0.001
I1211 16:55:40.530902  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:55:40.775298 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_114000.caffemodel
I1211 16:55:40.794797 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_114000.solverstate
I1211 16:55:40.800299 13896 solver.cpp:330] Iteration 114000, Testing net (#0)
I1211 16:55:40.800299 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:55:42.163797  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:55:42.217296 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6774
I1211 16:55:42.217296 13896 solver.cpp:397]     Test net output #1: loss = 1.25861 (* 1 = 1.25861 loss)
I1211 16:55:42.277312 13896 solver.cpp:218] Iteration 114000 (12.9696 iter/s, 7.71036s/100 iters), loss = 0.29795
I1211 16:55:42.277312 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:55:42.277312 13896 solver.cpp:237]     Train net output #1: loss = 0.29795 (* 1 = 0.29795 loss)
I1211 16:55:42.277312 13896 sgd_solver.cpp:105] Iteration 114000, lr = 0.001
I1211 16:55:48.552008 13896 solver.cpp:218] Iteration 114100 (15.9376 iter/s, 6.27447s/100 iters), loss = 0.348869
I1211 16:55:48.552507 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:55:48.552507 13896 solver.cpp:237]     Train net output #1: loss = 0.348869 (* 1 = 0.348869 loss)
I1211 16:55:48.552507 13896 sgd_solver.cpp:105] Iteration 114100, lr = 0.001
I1211 16:55:54.842417 13896 solver.cpp:218] Iteration 114200 (15.899 iter/s, 6.2897s/100 iters), loss = 0.368077
I1211 16:55:54.842417 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:55:54.842417 13896 solver.cpp:237]     Train net output #1: loss = 0.368077 (* 1 = 0.368077 loss)
I1211 16:55:54.842417 13896 sgd_solver.cpp:105] Iteration 114200, lr = 0.001
I1211 16:56:01.182381 13896 solver.cpp:218] Iteration 114300 (15.7743 iter/s, 6.33943s/100 iters), loss = 0.323352
I1211 16:56:01.182381 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:56:01.182381 13896 solver.cpp:237]     Train net output #1: loss = 0.323352 (* 1 = 0.323352 loss)
I1211 16:56:01.182381 13896 sgd_solver.cpp:105] Iteration 114300, lr = 0.001
I1211 16:56:07.534023 13896 solver.cpp:218] Iteration 114400 (15.7455 iter/s, 6.35101s/100 iters), loss = 0.28192
I1211 16:56:07.534023 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:56:07.534023 13896 solver.cpp:237]     Train net output #1: loss = 0.28192 (* 1 = 0.28192 loss)
I1211 16:56:07.534023 13896 sgd_solver.cpp:105] Iteration 114400, lr = 0.001
I1211 16:56:13.548702  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:56:13.799706 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_114500.caffemodel
I1211 16:56:13.818202 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_114500.solverstate
I1211 16:56:13.823202 13896 solver.cpp:330] Iteration 114500, Testing net (#0)
I1211 16:56:13.823202 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:56:15.197703  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:56:15.252202 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6787
I1211 16:56:15.252202 13896 solver.cpp:397]     Test net output #1: loss = 1.26532 (* 1 = 1.26532 loss)
I1211 16:56:15.313202 13896 solver.cpp:218] Iteration 114500 (12.8555 iter/s, 7.77877s/100 iters), loss = 0.214914
I1211 16:56:15.313202 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 16:56:15.313202 13896 solver.cpp:237]     Train net output #1: loss = 0.214914 (* 1 = 0.214914 loss)
I1211 16:56:15.313202 13896 sgd_solver.cpp:105] Iteration 114500, lr = 0.001
I1211 16:56:21.635242 13896 solver.cpp:218] Iteration 114600 (15.8191 iter/s, 6.32147s/100 iters), loss = 0.333899
I1211 16:56:21.635242 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:56:21.635242 13896 solver.cpp:237]     Train net output #1: loss = 0.333899 (* 1 = 0.333899 loss)
I1211 16:56:21.635242 13896 sgd_solver.cpp:105] Iteration 114600, lr = 0.001
I1211 16:56:27.907300 13896 solver.cpp:218] Iteration 114700 (15.9443 iter/s, 6.27183s/100 iters), loss = 0.298413
I1211 16:56:27.907300 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:56:27.907300 13896 solver.cpp:237]     Train net output #1: loss = 0.298413 (* 1 = 0.298413 loss)
I1211 16:56:27.907300 13896 sgd_solver.cpp:105] Iteration 114700, lr = 0.001
I1211 16:56:34.066234 13896 solver.cpp:218] Iteration 114800 (16.2378 iter/s, 6.15846s/100 iters), loss = 0.31507
I1211 16:56:34.066234 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:56:34.066234 13896 solver.cpp:237]     Train net output #1: loss = 0.315069 (* 1 = 0.315069 loss)
I1211 16:56:34.066234 13896 sgd_solver.cpp:105] Iteration 114800, lr = 0.001
I1211 16:56:40.291211 13896 solver.cpp:218] Iteration 114900 (16.0641 iter/s, 6.22504s/100 iters), loss = 0.433574
I1211 16:56:40.291211 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 16:56:40.291211 13896 solver.cpp:237]     Train net output #1: loss = 0.433574 (* 1 = 0.433574 loss)
I1211 16:56:40.291211 13896 sgd_solver.cpp:105] Iteration 114900, lr = 0.001
I1211 16:56:46.200589  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:56:46.444314 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_115000.caffemodel
I1211 16:56:46.462316 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_115000.solverstate
I1211 16:56:46.467334 13896 solver.cpp:330] Iteration 115000, Testing net (#0)
I1211 16:56:46.467334 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:56:47.814090  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:56:47.867599 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6752
I1211 16:56:47.867599 13896 solver.cpp:397]     Test net output #1: loss = 1.27593 (* 1 = 1.27593 loss)
I1211 16:56:47.928601 13896 solver.cpp:218] Iteration 115000 (13.0956 iter/s, 7.63617s/100 iters), loss = 0.246697
I1211 16:56:47.928601 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 16:56:47.928601 13896 solver.cpp:237]     Train net output #1: loss = 0.246697 (* 1 = 0.246697 loss)
I1211 16:56:47.928601 13896 sgd_solver.cpp:105] Iteration 115000, lr = 0.001
I1211 16:56:54.112596 13896 solver.cpp:218] Iteration 115100 (16.172 iter/s, 6.18353s/100 iters), loss = 0.336895
I1211 16:56:54.112596 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:56:54.112596 13896 solver.cpp:237]     Train net output #1: loss = 0.336895 (* 1 = 0.336895 loss)
I1211 16:56:54.112596 13896 sgd_solver.cpp:105] Iteration 115100, lr = 0.001
I1211 16:57:00.317837 13896 solver.cpp:218] Iteration 115200 (16.116 iter/s, 6.20502s/100 iters), loss = 0.286061
I1211 16:57:00.317837 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:57:00.317837 13896 solver.cpp:237]     Train net output #1: loss = 0.286061 (* 1 = 0.286061 loss)
I1211 16:57:00.317837 13896 sgd_solver.cpp:105] Iteration 115200, lr = 0.001
I1211 16:57:06.494427 13896 solver.cpp:218] Iteration 115300 (16.1913 iter/s, 6.17615s/100 iters), loss = 0.331614
I1211 16:57:06.494427 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:57:06.494427 13896 solver.cpp:237]     Train net output #1: loss = 0.331614 (* 1 = 0.331614 loss)
I1211 16:57:06.494427 13896 sgd_solver.cpp:105] Iteration 115300, lr = 0.001
I1211 16:57:12.788913 13896 solver.cpp:218] Iteration 115400 (15.8869 iter/s, 6.29451s/100 iters), loss = 0.292651
I1211 16:57:12.788913 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:57:12.788913 13896 solver.cpp:237]     Train net output #1: loss = 0.29265 (* 1 = 0.29265 loss)
I1211 16:57:12.788913 13896 sgd_solver.cpp:105] Iteration 115400, lr = 0.001
I1211 16:57:18.911090  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:57:19.169126 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_115500.caffemodel
I1211 16:57:19.191125 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_115500.solverstate
I1211 16:57:19.196642 13896 solver.cpp:330] Iteration 115500, Testing net (#0)
I1211 16:57:19.196642 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:57:20.588572  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:57:20.641567 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6807
I1211 16:57:20.641567 13896 solver.cpp:397]     Test net output #1: loss = 1.26469 (* 1 = 1.26469 loss)
I1211 16:57:20.703078 13896 solver.cpp:218] Iteration 115500 (12.6373 iter/s, 7.91308s/100 iters), loss = 0.286807
I1211 16:57:20.703078 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 16:57:20.703078 13896 solver.cpp:237]     Train net output #1: loss = 0.286807 (* 1 = 0.286807 loss)
I1211 16:57:20.703078 13896 sgd_solver.cpp:105] Iteration 115500, lr = 0.001
I1211 16:57:27.116212 13896 solver.cpp:218] Iteration 115600 (15.5946 iter/s, 6.41249s/100 iters), loss = 0.432729
I1211 16:57:27.116212 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 16:57:27.116212 13896 solver.cpp:237]     Train net output #1: loss = 0.432729 (* 1 = 0.432729 loss)
I1211 16:57:27.116212 13896 sgd_solver.cpp:105] Iteration 115600, lr = 0.001
I1211 16:57:33.514277 13896 solver.cpp:218] Iteration 115700 (15.6307 iter/s, 6.39767s/100 iters), loss = 0.312494
I1211 16:57:33.514277 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:57:33.514277 13896 solver.cpp:237]     Train net output #1: loss = 0.312494 (* 1 = 0.312494 loss)
I1211 16:57:33.514277 13896 sgd_solver.cpp:105] Iteration 115700, lr = 0.001
I1211 16:57:39.911695 13896 solver.cpp:218] Iteration 115800 (15.6328 iter/s, 6.3968s/100 iters), loss = 0.230379
I1211 16:57:39.911695 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 16:57:39.911695 13896 solver.cpp:237]     Train net output #1: loss = 0.230378 (* 1 = 0.230378 loss)
I1211 16:57:39.911695 13896 sgd_solver.cpp:105] Iteration 115800, lr = 0.001
I1211 16:57:46.304165 13896 solver.cpp:218] Iteration 115900 (15.6447 iter/s, 6.39194s/100 iters), loss = 0.295503
I1211 16:57:46.304165 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:57:46.304165 13896 solver.cpp:237]     Train net output #1: loss = 0.295503 (* 1 = 0.295503 loss)
I1211 16:57:46.304165 13896 sgd_solver.cpp:105] Iteration 115900, lr = 0.001
I1211 16:57:52.188462  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:57:52.430521 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_116000.caffemodel
I1211 16:57:52.446020 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_116000.solverstate
I1211 16:57:52.451521 13896 solver.cpp:330] Iteration 116000, Testing net (#0)
I1211 16:57:52.451521 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:57:53.791611  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:57:53.843617 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6764
I1211 16:57:53.843617 13896 solver.cpp:397]     Test net output #1: loss = 1.26812 (* 1 = 1.26812 loss)
I1211 16:57:53.902626 13896 solver.cpp:218] Iteration 116000 (13.1603 iter/s, 7.59863s/100 iters), loss = 0.249814
I1211 16:57:53.902626 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:57:53.902626 13896 solver.cpp:237]     Train net output #1: loss = 0.249814 (* 1 = 0.249814 loss)
I1211 16:57:53.902626 13896 sgd_solver.cpp:105] Iteration 116000, lr = 0.001
I1211 16:58:00.050570 13896 solver.cpp:218] Iteration 116100 (16.2686 iter/s, 6.14681s/100 iters), loss = 0.332172
I1211 16:58:00.050570 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:58:00.050570 13896 solver.cpp:237]     Train net output #1: loss = 0.332172 (* 1 = 0.332172 loss)
I1211 16:58:00.050570 13896 sgd_solver.cpp:105] Iteration 116100, lr = 0.001
I1211 16:58:06.212191 13896 solver.cpp:218] Iteration 116200 (16.2302 iter/s, 6.16136s/100 iters), loss = 0.265887
I1211 16:58:06.212191 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 16:58:06.212191 13896 solver.cpp:237]     Train net output #1: loss = 0.265887 (* 1 = 0.265887 loss)
I1211 16:58:06.212191 13896 sgd_solver.cpp:105] Iteration 116200, lr = 0.001
I1211 16:58:12.501473 13896 solver.cpp:218] Iteration 116300 (15.9024 iter/s, 6.28835s/100 iters), loss = 0.291625
I1211 16:58:12.501473 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:58:12.501473 13896 solver.cpp:237]     Train net output #1: loss = 0.291625 (* 1 = 0.291625 loss)
I1211 16:58:12.501473 13896 sgd_solver.cpp:105] Iteration 116300, lr = 0.001
I1211 16:58:18.862337 13896 solver.cpp:218] Iteration 116400 (15.7223 iter/s, 6.3604s/100 iters), loss = 0.353698
I1211 16:58:18.862337 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:58:18.862337 13896 solver.cpp:237]     Train net output #1: loss = 0.353697 (* 1 = 0.353697 loss)
I1211 16:58:18.862337 13896 sgd_solver.cpp:105] Iteration 116400, lr = 0.001
I1211 16:58:24.711755  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:58:24.953836 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_116500.caffemodel
I1211 16:58:24.969334 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_116500.solverstate
I1211 16:58:24.974337 13896 solver.cpp:330] Iteration 116500, Testing net (#0)
I1211 16:58:24.974337 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:58:26.315992  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:58:26.368495 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6744
I1211 16:58:26.368495 13896 solver.cpp:397]     Test net output #1: loss = 1.27613 (* 1 = 1.27613 loss)
I1211 16:58:26.427505 13896 solver.cpp:218] Iteration 116500 (13.2186 iter/s, 7.5651s/100 iters), loss = 0.190946
I1211 16:58:26.427505 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.99
I1211 16:58:26.427505 13896 solver.cpp:237]     Train net output #1: loss = 0.190946 (* 1 = 0.190946 loss)
I1211 16:58:26.427505 13896 sgd_solver.cpp:105] Iteration 116500, lr = 0.001
I1211 16:58:32.655889 13896 solver.cpp:218] Iteration 116600 (16.0571 iter/s, 6.22779s/100 iters), loss = 0.339357
I1211 16:58:32.655889 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:58:32.655889 13896 solver.cpp:237]     Train net output #1: loss = 0.339357 (* 1 = 0.339357 loss)
I1211 16:58:32.655889 13896 sgd_solver.cpp:105] Iteration 116600, lr = 0.001
I1211 16:58:39.064447 13896 solver.cpp:218] Iteration 116700 (15.6061 iter/s, 6.40774s/100 iters), loss = 0.27897
I1211 16:58:39.064447 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:58:39.064447 13896 solver.cpp:237]     Train net output #1: loss = 0.27897 (* 1 = 0.27897 loss)
I1211 16:58:39.064447 13896 sgd_solver.cpp:105] Iteration 116700, lr = 0.001
I1211 16:58:45.466053 13896 solver.cpp:218] Iteration 116800 (15.6219 iter/s, 6.40127s/100 iters), loss = 0.355448
I1211 16:58:45.466053 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:58:45.466053 13896 solver.cpp:237]     Train net output #1: loss = 0.355448 (* 1 = 0.355448 loss)
I1211 16:58:45.466053 13896 sgd_solver.cpp:105] Iteration 116800, lr = 0.001
I1211 16:58:51.874054 13896 solver.cpp:218] Iteration 116900 (15.6067 iter/s, 6.40752s/100 iters), loss = 0.281989
I1211 16:58:51.874054 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 16:58:51.874054 13896 solver.cpp:237]     Train net output #1: loss = 0.281988 (* 1 = 0.281988 loss)
I1211 16:58:51.874054 13896 sgd_solver.cpp:105] Iteration 116900, lr = 0.001
I1211 16:58:57.982867  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:58:58.237866 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_117000.caffemodel
I1211 16:58:58.253865 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_117000.solverstate
I1211 16:58:58.259867 13896 solver.cpp:330] Iteration 117000, Testing net (#0)
I1211 16:58:58.260375 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:58:59.643867  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:58:59.698865 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6758
I1211 16:58:59.698865 13896 solver.cpp:397]     Test net output #1: loss = 1.28101 (* 1 = 1.28101 loss)
I1211 16:58:59.760864 13896 solver.cpp:218] Iteration 117000 (12.6802 iter/s, 7.88631s/100 iters), loss = 0.244705
I1211 16:58:59.760864 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:58:59.760864 13896 solver.cpp:237]     Train net output #1: loss = 0.244705 (* 1 = 0.244705 loss)
I1211 16:58:59.760864 13896 sgd_solver.cpp:105] Iteration 117000, lr = 0.001
I1211 16:59:06.177364 13896 solver.cpp:218] Iteration 117100 (15.5868 iter/s, 6.41568s/100 iters), loss = 0.314487
I1211 16:59:06.177364 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:59:06.177364 13896 solver.cpp:237]     Train net output #1: loss = 0.314487 (* 1 = 0.314487 loss)
I1211 16:59:06.177364 13896 sgd_solver.cpp:105] Iteration 117100, lr = 0.001
I1211 16:59:12.598866 13896 solver.cpp:218] Iteration 117200 (15.5736 iter/s, 6.42112s/100 iters), loss = 0.35882
I1211 16:59:12.598866 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:59:12.598866 13896 solver.cpp:237]     Train net output #1: loss = 0.35882 (* 1 = 0.35882 loss)
I1211 16:59:12.598866 13896 sgd_solver.cpp:105] Iteration 117200, lr = 0.001
I1211 16:59:19.007366 13896 solver.cpp:218] Iteration 117300 (15.6051 iter/s, 6.40818s/100 iters), loss = 0.319013
I1211 16:59:19.007366 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:59:19.007366 13896 solver.cpp:237]     Train net output #1: loss = 0.319013 (* 1 = 0.319013 loss)
I1211 16:59:19.007366 13896 sgd_solver.cpp:105] Iteration 117300, lr = 0.001
I1211 16:59:25.408365 13896 solver.cpp:218] Iteration 117400 (15.6233 iter/s, 6.4007s/100 iters), loss = 0.370108
I1211 16:59:25.408866 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 16:59:25.408866 13896 solver.cpp:237]     Train net output #1: loss = 0.370107 (* 1 = 0.370107 loss)
I1211 16:59:25.408866 13896 sgd_solver.cpp:105] Iteration 117400, lr = 0.001
I1211 16:59:31.521831  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:59:31.776849 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_117500.caffemodel
I1211 16:59:31.792850 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_117500.solverstate
I1211 16:59:31.797847 13896 solver.cpp:330] Iteration 117500, Testing net (#0)
I1211 16:59:31.797847 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 16:59:33.180847  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 16:59:33.236351 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6762
I1211 16:59:33.236351 13896 solver.cpp:397]     Test net output #1: loss = 1.279 (* 1 = 1.279 loss)
I1211 16:59:33.298347 13896 solver.cpp:218] Iteration 117500 (12.6757 iter/s, 7.88913s/100 iters), loss = 0.22455
I1211 16:59:33.298347 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 16:59:33.298347 13896 solver.cpp:237]     Train net output #1: loss = 0.22455 (* 1 = 0.22455 loss)
I1211 16:59:33.298347 13896 sgd_solver.cpp:105] Iteration 117500, lr = 0.001
I1211 16:59:39.687182 13896 solver.cpp:218] Iteration 117600 (15.6527 iter/s, 6.38866s/100 iters), loss = 0.326783
I1211 16:59:39.687693 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 16:59:39.687693 13896 solver.cpp:237]     Train net output #1: loss = 0.326783 (* 1 = 0.326783 loss)
I1211 16:59:39.687693 13896 sgd_solver.cpp:105] Iteration 117600, lr = 0.001
I1211 16:59:46.073140 13896 solver.cpp:218] Iteration 117700 (15.6621 iter/s, 6.38486s/100 iters), loss = 0.239597
I1211 16:59:46.073140 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 16:59:46.073140 13896 solver.cpp:237]     Train net output #1: loss = 0.239597 (* 1 = 0.239597 loss)
I1211 16:59:46.073140 13896 sgd_solver.cpp:105] Iteration 117700, lr = 0.001
I1211 16:59:52.392441 13896 solver.cpp:218] Iteration 117800 (15.8262 iter/s, 6.31862s/100 iters), loss = 0.304074
I1211 16:59:52.392441 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:59:52.392441 13896 solver.cpp:237]     Train net output #1: loss = 0.304074 (* 1 = 0.304074 loss)
I1211 16:59:52.392441 13896 sgd_solver.cpp:105] Iteration 117800, lr = 0.001
I1211 16:59:58.562700 13896 solver.cpp:218] Iteration 117900 (16.2069 iter/s, 6.17022s/100 iters), loss = 0.360219
I1211 16:59:58.562700 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 16:59:58.562700 13896 solver.cpp:237]     Train net output #1: loss = 0.360219 (* 1 = 0.360219 loss)
I1211 16:59:58.562700 13896 sgd_solver.cpp:105] Iteration 117900, lr = 0.001
I1211 17:00:04.458277  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:00:04.701803 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_118000.caffemodel
I1211 17:00:04.718313 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_118000.solverstate
I1211 17:00:04.724313 13896 solver.cpp:330] Iteration 118000, Testing net (#0)
I1211 17:00:04.724313 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 17:00:06.060413  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:00:06.113417 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6753
I1211 17:00:06.113417 13896 solver.cpp:397]     Test net output #1: loss = 1.27503 (* 1 = 1.27503 loss)
I1211 17:00:06.171434 13896 solver.cpp:218] Iteration 118000 (13.1432 iter/s, 7.6085s/100 iters), loss = 0.286453
I1211 17:00:06.171434 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 17:00:06.171434 13896 solver.cpp:237]     Train net output #1: loss = 0.286453 (* 1 = 0.286453 loss)
I1211 17:00:06.171434 13896 sgd_solver.cpp:105] Iteration 118000, lr = 0.001
I1211 17:00:12.332937 13896 solver.cpp:218] Iteration 118100 (16.232 iter/s, 6.16067s/100 iters), loss = 0.245465
I1211 17:00:12.332937 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 17:00:12.332937 13896 solver.cpp:237]     Train net output #1: loss = 0.245465 (* 1 = 0.245465 loss)
I1211 17:00:12.332937 13896 sgd_solver.cpp:105] Iteration 118100, lr = 0.001
I1211 17:00:18.491451 13896 solver.cpp:218] Iteration 118200 (16.2394 iter/s, 6.15786s/100 iters), loss = 0.266717
I1211 17:00:18.491451 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 17:00:18.491451 13896 solver.cpp:237]     Train net output #1: loss = 0.266717 (* 1 = 0.266717 loss)
I1211 17:00:18.491451 13896 sgd_solver.cpp:105] Iteration 118200, lr = 0.001
I1211 17:00:24.647461 13896 solver.cpp:218] Iteration 118300 (16.2438 iter/s, 6.15621s/100 iters), loss = 0.264085
I1211 17:00:24.647461 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 17:00:24.647461 13896 solver.cpp:237]     Train net output #1: loss = 0.264085 (* 1 = 0.264085 loss)
I1211 17:00:24.647461 13896 sgd_solver.cpp:105] Iteration 118300, lr = 0.001
I1211 17:00:30.817412 13896 solver.cpp:218] Iteration 118400 (16.2104 iter/s, 6.16889s/100 iters), loss = 0.403126
I1211 17:00:30.817412 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 17:00:30.817412 13896 solver.cpp:237]     Train net output #1: loss = 0.403126 (* 1 = 0.403126 loss)
I1211 17:00:30.817412 13896 sgd_solver.cpp:105] Iteration 118400, lr = 0.001
I1211 17:00:36.684342  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:00:36.927358 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_118500.caffemodel
I1211 17:00:36.943359 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_118500.solverstate
I1211 17:00:36.949360 13896 solver.cpp:330] Iteration 118500, Testing net (#0)
I1211 17:00:36.949360 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 17:00:38.290480  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:00:38.342489 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6764
I1211 17:00:38.342489 13896 solver.cpp:397]     Test net output #1: loss = 1.28652 (* 1 = 1.28652 loss)
I1211 17:00:38.402992 13896 solver.cpp:218] Iteration 118500 (13.1836 iter/s, 7.58518s/100 iters), loss = 0.287634
I1211 17:00:38.403493 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 17:00:38.403493 13896 solver.cpp:237]     Train net output #1: loss = 0.287634 (* 1 = 0.287634 loss)
I1211 17:00:38.403493 13896 sgd_solver.cpp:105] Iteration 118500, lr = 0.001
I1211 17:00:44.560000 13896 solver.cpp:218] Iteration 118600 (16.2436 iter/s, 6.15626s/100 iters), loss = 0.290536
I1211 17:00:44.560000 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 17:00:44.560000 13896 solver.cpp:237]     Train net output #1: loss = 0.290536 (* 1 = 0.290536 loss)
I1211 17:00:44.560000 13896 sgd_solver.cpp:105] Iteration 118600, lr = 0.001
I1211 17:00:50.714239 13896 solver.cpp:218] Iteration 118700 (16.2499 iter/s, 6.1539s/100 iters), loss = 0.230233
I1211 17:00:50.714239 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 17:00:50.714239 13896 solver.cpp:237]     Train net output #1: loss = 0.230233 (* 1 = 0.230233 loss)
I1211 17:00:50.714239 13896 sgd_solver.cpp:105] Iteration 118700, lr = 0.001
I1211 17:00:56.882088 13896 solver.cpp:218] Iteration 118800 (16.2135 iter/s, 6.1677s/100 iters), loss = 0.312172
I1211 17:00:56.882088 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 17:00:56.882088 13896 solver.cpp:237]     Train net output #1: loss = 0.312172 (* 1 = 0.312172 loss)
I1211 17:00:56.882088 13896 sgd_solver.cpp:105] Iteration 118800, lr = 0.001
I1211 17:01:03.052942 13896 solver.cpp:218] Iteration 118900 (16.2076 iter/s, 6.16994s/100 iters), loss = 0.312106
I1211 17:01:03.052942 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 17:01:03.052942 13896 solver.cpp:237]     Train net output #1: loss = 0.312106 (* 1 = 0.312106 loss)
I1211 17:01:03.052942 13896 sgd_solver.cpp:105] Iteration 118900, lr = 0.001
I1211 17:01:08.908857  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:01:09.150889 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_119000.caffemodel
I1211 17:01:09.166890 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_119000.solverstate
I1211 17:01:09.171891 13896 solver.cpp:330] Iteration 119000, Testing net (#0)
I1211 17:01:09.171891 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 17:01:10.512650  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:01:10.564149 13896 solver.cpp:397]     Test net output #0: accuracy = 0.677
I1211 17:01:10.564149 13896 solver.cpp:397]     Test net output #1: loss = 1.28478 (* 1 = 1.28478 loss)
I1211 17:01:10.623656 13896 solver.cpp:218] Iteration 119000 (13.2093 iter/s, 7.57045s/100 iters), loss = 0.261427
I1211 17:01:10.623656 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1211 17:01:10.623656 13896 solver.cpp:237]     Train net output #1: loss = 0.261427 (* 1 = 0.261427 loss)
I1211 17:01:10.623656 13896 sgd_solver.cpp:105] Iteration 119000, lr = 0.001
I1211 17:01:16.794225 13896 solver.cpp:218] Iteration 119100 (16.2069 iter/s, 6.1702s/100 iters), loss = 0.26499
I1211 17:01:16.794225 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 17:01:16.794225 13896 solver.cpp:237]     Train net output #1: loss = 0.26499 (* 1 = 0.26499 loss)
I1211 17:01:16.794225 13896 sgd_solver.cpp:105] Iteration 119100, lr = 0.001
I1211 17:01:23.044950 13896 solver.cpp:218] Iteration 119200 (16 iter/s, 6.25002s/100 iters), loss = 0.212785
I1211 17:01:23.044950 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 17:01:23.044950 13896 solver.cpp:237]     Train net output #1: loss = 0.212785 (* 1 = 0.212785 loss)
I1211 17:01:23.044950 13896 sgd_solver.cpp:105] Iteration 119200, lr = 0.001
I1211 17:01:29.469569 13896 solver.cpp:218] Iteration 119300 (15.5666 iter/s, 6.42401s/100 iters), loss = 0.270891
I1211 17:01:29.469569 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 17:01:29.469569 13896 solver.cpp:237]     Train net output #1: loss = 0.270891 (* 1 = 0.270891 loss)
I1211 17:01:29.469569 13896 sgd_solver.cpp:105] Iteration 119300, lr = 0.001
I1211 17:01:35.801641 13896 solver.cpp:218] Iteration 119400 (15.7929 iter/s, 6.33195s/100 iters), loss = 0.315895
I1211 17:01:35.801641 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 17:01:35.801641 13896 solver.cpp:237]     Train net output #1: loss = 0.315895 (* 1 = 0.315895 loss)
I1211 17:01:35.801641 13896 sgd_solver.cpp:105] Iteration 119400, lr = 0.001
I1211 17:01:41.667740  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:01:41.909242 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_119500.caffemodel
I1211 17:01:41.925242 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_119500.solverstate
I1211 17:01:41.930243 13896 solver.cpp:330] Iteration 119500, Testing net (#0)
I1211 17:01:41.930243 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 17:01:43.269340  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:01:43.321342 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6779
I1211 17:01:43.321342 13896 solver.cpp:397]     Test net output #1: loss = 1.27714 (* 1 = 1.27714 loss)
I1211 17:01:43.379343 13896 solver.cpp:218] Iteration 119500 (13.1967 iter/s, 7.57766s/100 iters), loss = 0.271835
I1211 17:01:43.379343 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 17:01:43.379343 13896 solver.cpp:237]     Train net output #1: loss = 0.271835 (* 1 = 0.271835 loss)
I1211 17:01:43.379343 13896 sgd_solver.cpp:105] Iteration 119500, lr = 0.001
I1211 17:01:49.632479 13896 solver.cpp:218] Iteration 119600 (15.9957 iter/s, 6.2517s/100 iters), loss = 0.325233
I1211 17:01:49.632479 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 17:01:49.632479 13896 solver.cpp:237]     Train net output #1: loss = 0.325233 (* 1 = 0.325233 loss)
I1211 17:01:49.632479 13896 sgd_solver.cpp:105] Iteration 119600, lr = 0.001
I1211 17:01:56.091647 13896 solver.cpp:218] Iteration 119700 (15.4825 iter/s, 6.45889s/100 iters), loss = 0.280394
I1211 17:01:56.091647 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 17:01:56.091647 13896 solver.cpp:237]     Train net output #1: loss = 0.280394 (* 1 = 0.280394 loss)
I1211 17:01:56.091647 13896 sgd_solver.cpp:105] Iteration 119700, lr = 0.001
I1211 17:02:02.576946 13896 solver.cpp:218] Iteration 119800 (15.4211 iter/s, 6.4846s/100 iters), loss = 0.307519
I1211 17:02:02.576946 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 17:02:02.576946 13896 solver.cpp:237]     Train net output #1: loss = 0.307519 (* 1 = 0.307519 loss)
I1211 17:02:02.576946 13896 sgd_solver.cpp:105] Iteration 119800, lr = 0.001
I1211 17:02:09.042650 13896 solver.cpp:218] Iteration 119900 (15.4674 iter/s, 6.46522s/100 iters), loss = 0.321246
I1211 17:02:09.042650 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 17:02:09.042650 13896 solver.cpp:237]     Train net output #1: loss = 0.321246 (* 1 = 0.321246 loss)
I1211 17:02:09.042650 13896 sgd_solver.cpp:105] Iteration 119900, lr = 0.001
I1211 17:02:15.148303  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:02:15.401829 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_120000.caffemodel
I1211 17:02:15.417829 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_120000.solverstate
I1211 17:02:15.422327 13896 solver.cpp:330] Iteration 120000, Testing net (#0)
I1211 17:02:15.422828 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 17:02:16.813832  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:02:16.868832 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6716
I1211 17:02:16.868832 13896 solver.cpp:397]     Test net output #1: loss = 1.29243 (* 1 = 1.29243 loss)
I1211 17:02:16.930330 13896 solver.cpp:218] Iteration 120000 (12.6788 iter/s, 7.88719s/100 iters), loss = 0.251264
I1211 17:02:16.930330 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 17:02:16.930330 13896 solver.cpp:237]     Train net output #1: loss = 0.251264 (* 1 = 0.251264 loss)
I1211 17:02:16.930330 13896 sgd_solver.cpp:105] Iteration 120000, lr = 0.001
I1211 17:02:23.353953 13896 solver.cpp:218] Iteration 120100 (15.5689 iter/s, 6.42305s/100 iters), loss = 0.317612
I1211 17:02:23.353953 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 17:02:23.353953 13896 solver.cpp:237]     Train net output #1: loss = 0.317612 (* 1 = 0.317612 loss)
I1211 17:02:23.353953 13896 sgd_solver.cpp:105] Iteration 120100, lr = 0.001
I1211 17:02:29.752943 13896 solver.cpp:218] Iteration 120200 (15.628 iter/s, 6.39877s/100 iters), loss = 0.253382
I1211 17:02:29.752943 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 17:02:29.753443 13896 solver.cpp:237]     Train net output #1: loss = 0.253382 (* 1 = 0.253382 loss)
I1211 17:02:29.753443 13896 sgd_solver.cpp:105] Iteration 120200, lr = 0.001
I1211 17:02:36.144477 13896 solver.cpp:218] Iteration 120300 (15.6477 iter/s, 6.39071s/100 iters), loss = 0.261132
I1211 17:02:36.144477 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 17:02:36.144477 13896 solver.cpp:237]     Train net output #1: loss = 0.261132 (* 1 = 0.261132 loss)
I1211 17:02:36.144477 13896 sgd_solver.cpp:105] Iteration 120300, lr = 0.001
I1211 17:02:42.564766 13896 solver.cpp:218] Iteration 120400 (15.5763 iter/s, 6.42002s/100 iters), loss = 0.327829
I1211 17:02:42.564766 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 17:02:42.564766 13896 solver.cpp:237]     Train net output #1: loss = 0.327829 (* 1 = 0.327829 loss)
I1211 17:02:42.564766 13896 sgd_solver.cpp:105] Iteration 120400, lr = 0.001
I1211 17:02:48.661710  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:02:48.913209 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_120500.caffemodel
I1211 17:02:48.929708 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_120500.solverstate
I1211 17:02:48.934708 13896 solver.cpp:330] Iteration 120500, Testing net (#0)
I1211 17:02:48.934708 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 17:02:50.320711  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:02:50.374209 13896 solver.cpp:397]     Test net output #0: accuracy = 0.674
I1211 17:02:50.374209 13896 solver.cpp:397]     Test net output #1: loss = 1.29113 (* 1 = 1.29113 loss)
I1211 17:02:50.435209 13896 solver.cpp:218] Iteration 120500 (12.7068 iter/s, 7.86983s/100 iters), loss = 0.205646
I1211 17:02:50.435209 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 17:02:50.435209 13896 solver.cpp:237]     Train net output #1: loss = 0.205646 (* 1 = 0.205646 loss)
I1211 17:02:50.435209 13896 sgd_solver.cpp:105] Iteration 120500, lr = 0.001
I1211 17:02:56.909421 13896 solver.cpp:218] Iteration 120600 (15.4477 iter/s, 6.47345s/100 iters), loss = 0.329859
I1211 17:02:56.909421 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 17:02:56.909421 13896 solver.cpp:237]     Train net output #1: loss = 0.329859 (* 1 = 0.329859 loss)
I1211 17:02:56.909421 13896 sgd_solver.cpp:105] Iteration 120600, lr = 0.001
I1211 17:03:03.166002 13896 solver.cpp:218] Iteration 120700 (15.9844 iter/s, 6.2561s/100 iters), loss = 0.220835
I1211 17:03:03.166002 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 17:03:03.166002 13896 solver.cpp:237]     Train net output #1: loss = 0.220835 (* 1 = 0.220835 loss)
I1211 17:03:03.166002 13896 sgd_solver.cpp:105] Iteration 120700, lr = 0.001
I1211 17:03:09.320232 13896 solver.cpp:218] Iteration 120800 (16.2492 iter/s, 6.15415s/100 iters), loss = 0.327097
I1211 17:03:09.320232 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 17:03:09.320232 13896 solver.cpp:237]     Train net output #1: loss = 0.327097 (* 1 = 0.327097 loss)
I1211 17:03:09.320232 13896 sgd_solver.cpp:105] Iteration 120800, lr = 0.001
I1211 17:03:15.480108 13896 solver.cpp:218] Iteration 120900 (16.2353 iter/s, 6.15943s/100 iters), loss = 0.29634
I1211 17:03:15.480607 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 17:03:15.480607 13896 solver.cpp:237]     Train net output #1: loss = 0.296339 (* 1 = 0.296339 loss)
I1211 17:03:15.480607 13896 sgd_solver.cpp:105] Iteration 120900, lr = 0.001
I1211 17:03:21.335095  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:03:21.578897 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_121000.caffemodel
I1211 17:03:21.594602 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_121000.solverstate
I1211 17:03:21.599603 13896 solver.cpp:330] Iteration 121000, Testing net (#0)
I1211 17:03:21.599603 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 17:03:22.939472  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:03:22.991500 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6735
I1211 17:03:22.991500 13896 solver.cpp:397]     Test net output #1: loss = 1.30015 (* 1 = 1.30015 loss)
I1211 17:03:23.050451 13896 solver.cpp:218] Iteration 121000 (13.2109 iter/s, 7.56951s/100 iters), loss = 0.207928
I1211 17:03:23.050451 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1211 17:03:23.050451 13896 solver.cpp:237]     Train net output #1: loss = 0.207928 (* 1 = 0.207928 loss)
I1211 17:03:23.050451 13896 sgd_solver.cpp:105] Iteration 121000, lr = 0.001
I1211 17:03:29.204432 13896 solver.cpp:218] Iteration 121100 (16.2488 iter/s, 6.15431s/100 iters), loss = 0.344785
I1211 17:03:29.204432 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 17:03:29.205405 13896 solver.cpp:237]     Train net output #1: loss = 0.344785 (* 1 = 0.344785 loss)
I1211 17:03:29.205405 13896 sgd_solver.cpp:105] Iteration 121100, lr = 0.001
I1211 17:03:35.356134 13896 solver.cpp:218] Iteration 121200 (16.258 iter/s, 6.15082s/100 iters), loss = 0.250936
I1211 17:03:35.356134 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 17:03:35.356134 13896 solver.cpp:237]     Train net output #1: loss = 0.250936 (* 1 = 0.250936 loss)
I1211 17:03:35.356134 13896 sgd_solver.cpp:105] Iteration 121200, lr = 0.001
I1211 17:03:41.526670 13896 solver.cpp:218] Iteration 121300 (16.2079 iter/s, 6.16983s/100 iters), loss = 0.270285
I1211 17:03:41.526670 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 17:03:41.526670 13896 solver.cpp:237]     Train net output #1: loss = 0.270285 (* 1 = 0.270285 loss)
I1211 17:03:41.526670 13896 sgd_solver.cpp:105] Iteration 121300, lr = 0.001
I1211 17:03:47.696122 13896 solver.cpp:218] Iteration 121400 (16.2103 iter/s, 6.16892s/100 iters), loss = 0.317445
I1211 17:03:47.696122 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 17:03:47.696122 13896 solver.cpp:237]     Train net output #1: loss = 0.317445 (* 1 = 0.317445 loss)
I1211 17:03:47.696122 13896 sgd_solver.cpp:105] Iteration 121400, lr = 0.001
I1211 17:03:53.554280  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:03:53.797391 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_121500.caffemodel
I1211 17:03:53.812906 13896 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_121500.solverstate
I1211 17:03:53.817906 13896 solver.cpp:330] Iteration 121500, Testing net (#0)
I1211 17:03:53.817906 13896 net.cpp:676] Ignoring source layer accuracy_training
I1211 17:03:55.193343  6412 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:03:55.247843 13896 solver.cpp:397]     Test net output #0: accuracy = 0.6717
I1211 17:03:55.247843 13896 solver.cpp:397]     Test net output #1: loss = 1.29468 (* 1 = 1.29468 loss)
I1211 17:03:55.308843 13896 solver.cpp:218] Iteration 121500 (13.1366 iter/s, 7.61231s/100 iters), loss = 0.307731
I1211 17:03:55.308843 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 17:03:55.308843 13896 solver.cpp:237]     Train net output #1: loss = 0.307731 (* 1 = 0.307731 loss)
I1211 17:03:55.308843 13896 sgd_solver.cpp:105] Iteration 121500, lr = 0.001
I1211 17:04:01.755874 13896 solver.cpp:218] Iteration 121600 (15.5123 iter/s, 6.44649s/100 iters), loss = 0.308814
I1211 17:04:01.755874 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 17:04:01.755874 13896 solver.cpp:237]     Train net output #1: loss = 0.308814 (* 1 = 0.308814 loss)
I1211 17:04:01.755874 13896 sgd_solver.cpp:105] Iteration 121600, lr = 0.001
I1211 17:04:08.209873 13896 solver.cpp:218] Iteration 121700 (15.4957 iter/s, 6.45341s/100 iters), loss = 0.273939
I1211 17:04:08.209873 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 17:04:08.209873 13896 solver.cpp:237]     Train net output #1: loss = 0.273939 (* 1 = 0.273939 loss)
I1211 17:04:08.209873 13896 sgd_solver.cpp:105] Iteration 121700, lr = 0.001
I1211 17:04:14.589720 13896 solver.cpp:218] Iteration 121800 (15.6759 iter/s, 6.37923s/100 iters), loss = 0.275999
I1211 17:04:14.589720 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 17:04:14.589720 13896 solver.cpp:237]     Train net output #1: loss = 0.275999 (* 1 = 0.275999 loss)
I1211 17:04:14.589720 13896 sgd_solver.cpp:105] Iteration 121800, lr = 0.001
I1211 17:04:21.035001 13896 solver.cpp:218] Iteration 121900 (15.5168 iter/s, 6.44464s/100 iters), loss = 0.348384
I1211 17:04:21.035001 13896 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 17:04:21.035001 13896 solver.cpp:237]     Train net output #1: loss = 0.348384 (* 1 = 0.348384 loss)
I1211 17:04:21.035001 13896 sgd_solver.cpp:105] Iteration 121900, lr = 0.001
I1211 17:04:27.193706  1052 data_layer.cpp:73] Restarting data prefetching from start.
I1211 17:04:27.446209 13896 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_122000.caffemodel
I1211 17:04:27.468705 13896 sgd_solver.cpp:273] Snapshottin

G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90000.solverstate 
I1211 06:02:42.068737 13088 caffe.cpp:219] Using GPUs 0
I1211 06:02:42.247251 13088 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1211 06:02:42.553298 13088 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 06:02:42.569298 13088 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1211 06:02:42.570298 13088 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 06:02:42.571300 13088 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 06:02:42.571300 13088 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_pool2_1
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_pool4_2
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1211 06:02:42.571300 13088 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1211 06:02:42.571300 13088 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV2_WnonLin_360k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "pool2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_pool2_1"
  type: "BatchNorm"
  bottom: "pool2_1"
  top: "pool2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_pool2_1"
  type: "Scale"
  bottom: "pool2_1"
  top: "pool2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_pool2_1"
  type: "ReLU"
  bottom: "pool2_1"
  top: "pool2_1"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "pool4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_pool4_2"
  type: "BatchNorm"
  bottom: "pool4_2"
  top: "pool4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_pool4_2"
  type: "Scale"
  bottom: "pool4_2"
  top: "pool4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_pool4_2"
  type: "ReLU"
  bottom: "pool4_2"
  top: "pool4_2"
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 06:02:42.583302 13088 layer_factory.cpp:58] Creating layer cifar
I1211 06:02:42.586297 13088 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1211 06:02:42.587296 13088 net.cpp:84] Creating Layer cifar
I1211 06:02:42.587296 13088 net.cpp:380] cifar -> data
I1211 06:02:42.587296 13088 net.cpp:380] cifar -> label
I1211 06:02:42.588299 13088 data_layer.cpp:45] output data size: 100,3,32,32
I1211 06:02:42.594295 13088 net.cpp:122] Setting up cifar
I1211 06:02:42.595296 13088 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 06:02:42.595296 13088 net.cpp:129] Top shape: 100 (100)
I1211 06:02:42.595296 13088 net.cpp:137] Memory required for data: 1229200
I1211 06:02:42.595296 13088 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 06:02:42.595296 13088 net.cpp:84] Creating Layer label_cifar_1_split
I1211 06:02:42.595296 13088 net.cpp:406] label_cifar_1_split <- label
I1211 06:02:42.595296 13088 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 06:02:42.595296 13088 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 06:02:42.595296 13088 net.cpp:122] Setting up label_cifar_1_split
I1211 06:02:42.595296 13088 net.cpp:129] Top shape: 100 (100)
I1211 06:02:42.595296 13088 net.cpp:129] Top shape: 100 (100)
I1211 06:02:42.595296 13088 net.cpp:137] Memory required for data: 1230000
I1211 06:02:42.595296 13088 layer_factory.cpp:58] Creating layer conv1
I1211 06:02:42.595296 13088 net.cpp:84] Creating Layer conv1
I1211 06:02:42.595296 13088 net.cpp:406] conv1 <- data
I1211 06:02:42.595296 13088 net.cpp:380] conv1 -> conv1
I1211 06:02:42.596294 12900 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 06:02:42.848340 13088 net.cpp:122] Setting up conv1
I1211 06:02:42.849339 13088 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:02:42.849339 13088 net.cpp:137] Memory required for data: 13518000
I1211 06:02:42.849339 13088 layer_factory.cpp:58] Creating layer bn1
I1211 06:02:42.849339 13088 net.cpp:84] Creating Layer bn1
I1211 06:02:42.849339 13088 net.cpp:406] bn1 <- conv1
I1211 06:02:42.849339 13088 net.cpp:367] bn1 -> conv1 (in-place)
I1211 06:02:42.849339 13088 net.cpp:122] Setting up bn1
I1211 06:02:42.849339 13088 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:02:42.849339 13088 net.cpp:137] Memory required for data: 25806000
I1211 06:02:42.849339 13088 layer_factory.cpp:58] Creating layer scale1
I1211 06:02:42.849339 13088 net.cpp:84] Creating Layer scale1
I1211 06:02:42.849339 13088 net.cpp:406] scale1 <- conv1
I1211 06:02:42.849339 13088 net.cpp:367] scale1 -> conv1 (in-place)
I1211 06:02:42.849339 13088 layer_factory.cpp:58] Creating layer scale1
I1211 06:02:42.849339 13088 net.cpp:122] Setting up scale1
I1211 06:02:42.849339 13088 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:02:42.849339 13088 net.cpp:137] Memory required for data: 38094000
I1211 06:02:42.849339 13088 layer_factory.cpp:58] Creating layer relu1
I1211 06:02:42.849339 13088 net.cpp:84] Creating Layer relu1
I1211 06:02:42.849339 13088 net.cpp:406] relu1 <- conv1
I1211 06:02:42.849339 13088 net.cpp:367] relu1 -> conv1 (in-place)
I1211 06:02:42.849339 13088 net.cpp:122] Setting up relu1
I1211 06:02:42.849339 13088 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:02:42.849339 13088 net.cpp:137] Memory required for data: 50382000
I1211 06:02:42.849339 13088 layer_factory.cpp:58] Creating layer conv1_0
I1211 06:02:42.849339 13088 net.cpp:84] Creating Layer conv1_0
I1211 06:02:42.849339 13088 net.cpp:406] conv1_0 <- conv1
I1211 06:02:42.849339 13088 net.cpp:380] conv1_0 -> conv1_0
I1211 06:02:42.851338 13088 net.cpp:122] Setting up conv1_0
I1211 06:02:42.851338 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.851338 13088 net.cpp:137] Memory required for data: 66766000
I1211 06:02:42.851338 13088 layer_factory.cpp:58] Creating layer bn1_0
I1211 06:02:42.851338 13088 net.cpp:84] Creating Layer bn1_0
I1211 06:02:42.851338 13088 net.cpp:406] bn1_0 <- conv1_0
I1211 06:02:42.851338 13088 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 06:02:42.851338 13088 net.cpp:122] Setting up bn1_0
I1211 06:02:42.851338 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.851338 13088 net.cpp:137] Memory required for data: 83150000
I1211 06:02:42.851338 13088 layer_factory.cpp:58] Creating layer scale1_0
I1211 06:02:42.851338 13088 net.cpp:84] Creating Layer scale1_0
I1211 06:02:42.851338 13088 net.cpp:406] scale1_0 <- conv1_0
I1211 06:02:42.851338 13088 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 06:02:42.851338 13088 layer_factory.cpp:58] Creating layer scale1_0
I1211 06:02:42.851338 13088 net.cpp:122] Setting up scale1_0
I1211 06:02:42.851338 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.851338 13088 net.cpp:137] Memory required for data: 99534000
I1211 06:02:42.851338 13088 layer_factory.cpp:58] Creating layer relu1_0
I1211 06:02:42.851338 13088 net.cpp:84] Creating Layer relu1_0
I1211 06:02:42.851338 13088 net.cpp:406] relu1_0 <- conv1_0
I1211 06:02:42.851338 13088 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 06:02:42.851338 13088 net.cpp:122] Setting up relu1_0
I1211 06:02:42.852339 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.852339 13088 net.cpp:137] Memory required for data: 115918000
I1211 06:02:42.852339 13088 layer_factory.cpp:58] Creating layer conv2
I1211 06:02:42.852339 13088 net.cpp:84] Creating Layer conv2
I1211 06:02:42.852339 13088 net.cpp:406] conv2 <- conv1_0
I1211 06:02:42.852339 13088 net.cpp:380] conv2 -> conv2
I1211 06:02:42.853338 13088 net.cpp:122] Setting up conv2
I1211 06:02:42.853338 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.853338 13088 net.cpp:137] Memory required for data: 132302000
I1211 06:02:42.853338 13088 layer_factory.cpp:58] Creating layer bn2
I1211 06:02:42.853338 13088 net.cpp:84] Creating Layer bn2
I1211 06:02:42.853338 13088 net.cpp:406] bn2 <- conv2
I1211 06:02:42.853338 13088 net.cpp:367] bn2 -> conv2 (in-place)
I1211 06:02:42.853338 13088 net.cpp:122] Setting up bn2
I1211 06:02:42.853338 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.853338 13088 net.cpp:137] Memory required for data: 148686000
I1211 06:02:42.853338 13088 layer_factory.cpp:58] Creating layer scale2
I1211 06:02:42.853338 13088 net.cpp:84] Creating Layer scale2
I1211 06:02:42.853338 13088 net.cpp:406] scale2 <- conv2
I1211 06:02:42.853338 13088 net.cpp:367] scale2 -> conv2 (in-place)
I1211 06:02:42.853338 13088 layer_factory.cpp:58] Creating layer scale2
I1211 06:02:42.853338 13088 net.cpp:122] Setting up scale2
I1211 06:02:42.853338 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.853338 13088 net.cpp:137] Memory required for data: 165070000
I1211 06:02:42.853338 13088 layer_factory.cpp:58] Creating layer relu2
I1211 06:02:42.853338 13088 net.cpp:84] Creating Layer relu2
I1211 06:02:42.853338 13088 net.cpp:406] relu2 <- conv2
I1211 06:02:42.853338 13088 net.cpp:367] relu2 -> conv2 (in-place)
I1211 06:02:42.853338 13088 net.cpp:122] Setting up relu2
I1211 06:02:42.853338 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.853338 13088 net.cpp:137] Memory required for data: 181454000
I1211 06:02:42.853338 13088 layer_factory.cpp:58] Creating layer conv2_1
I1211 06:02:42.853338 13088 net.cpp:84] Creating Layer conv2_1
I1211 06:02:42.853338 13088 net.cpp:406] conv2_1 <- conv2
I1211 06:02:42.853338 13088 net.cpp:380] conv2_1 -> conv2_1
I1211 06:02:42.854339 13088 net.cpp:122] Setting up conv2_1
I1211 06:02:42.854339 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.854339 13088 net.cpp:137] Memory required for data: 197838000
I1211 06:02:42.854339 13088 layer_factory.cpp:58] Creating layer bn2_1
I1211 06:02:42.854339 13088 net.cpp:84] Creating Layer bn2_1
I1211 06:02:42.854339 13088 net.cpp:406] bn2_1 <- conv2_1
I1211 06:02:42.854339 13088 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 06:02:42.855340 13088 net.cpp:122] Setting up bn2_1
I1211 06:02:42.855340 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.855340 13088 net.cpp:137] Memory required for data: 214222000
I1211 06:02:42.855340 13088 layer_factory.cpp:58] Creating layer scale2_1
I1211 06:02:42.855340 13088 net.cpp:84] Creating Layer scale2_1
I1211 06:02:42.855340 13088 net.cpp:406] scale2_1 <- conv2_1
I1211 06:02:42.855340 13088 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 06:02:42.855340 13088 layer_factory.cpp:58] Creating layer scale2_1
I1211 06:02:42.855340 13088 net.cpp:122] Setting up scale2_1
I1211 06:02:42.855340 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.855340 13088 net.cpp:137] Memory required for data: 230606000
I1211 06:02:42.855340 13088 layer_factory.cpp:58] Creating layer relu2_1
I1211 06:02:42.855340 13088 net.cpp:84] Creating Layer relu2_1
I1211 06:02:42.855340 13088 net.cpp:406] relu2_1 <- conv2_1
I1211 06:02:42.855340 13088 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 06:02:42.855340 13088 net.cpp:122] Setting up relu2_1
I1211 06:02:42.855340 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.855340 13088 net.cpp:137] Memory required for data: 246990000
I1211 06:02:42.855340 13088 layer_factory.cpp:58] Creating layer conv2_2
I1211 06:02:42.855340 13088 net.cpp:84] Creating Layer conv2_2
I1211 06:02:42.855340 13088 net.cpp:406] conv2_2 <- conv2_1
I1211 06:02:42.855340 13088 net.cpp:380] conv2_2 -> conv2_2
I1211 06:02:42.857338 13088 net.cpp:122] Setting up conv2_2
I1211 06:02:42.857338 13088 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:02:42.857338 13088 net.cpp:137] Memory required for data: 267470000
I1211 06:02:42.857338 13088 layer_factory.cpp:58] Creating layer bn2_2
I1211 06:02:42.857338 13088 net.cpp:84] Creating Layer bn2_2
I1211 06:02:42.857338 13088 net.cpp:406] bn2_2 <- conv2_2
I1211 06:02:42.857338 13088 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 06:02:42.857338 13088 net.cpp:122] Setting up bn2_2
I1211 06:02:42.857338 13088 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:02:42.857338 13088 net.cpp:137] Memory required for data: 287950000
I1211 06:02:42.857338 13088 layer_factory.cpp:58] Creating layer scale2_2
I1211 06:02:42.857338 13088 net.cpp:84] Creating Layer scale2_2
I1211 06:02:42.857338 13088 net.cpp:406] scale2_2 <- conv2_2
I1211 06:02:42.857338 13088 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 06:02:42.857338 13088 layer_factory.cpp:58] Creating layer scale2_2
I1211 06:02:42.857338 13088 net.cpp:122] Setting up scale2_2
I1211 06:02:42.857338 13088 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:02:42.857338 13088 net.cpp:137] Memory required for data: 308430000
I1211 06:02:42.857338 13088 layer_factory.cpp:58] Creating layer relu2_2
I1211 06:02:42.857338 13088 net.cpp:84] Creating Layer relu2_2
I1211 06:02:42.857338 13088 net.cpp:406] relu2_2 <- conv2_2
I1211 06:02:42.857338 13088 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 06:02:42.858348 13088 net.cpp:122] Setting up relu2_2
I1211 06:02:42.858348 13088 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:02:42.858348 13088 net.cpp:137] Memory required for data: 328910000
I1211 06:02:42.858348 13088 layer_factory.cpp:58] Creating layer pool2_1
I1211 06:02:42.858348 13088 net.cpp:84] Creating Layer pool2_1
I1211 06:02:42.858348 13088 net.cpp:406] pool2_1 <- conv2_2
I1211 06:02:42.858348 13088 net.cpp:380] pool2_1 -> pool2_1
I1211 06:02:42.859338 13088 net.cpp:122] Setting up pool2_1
I1211 06:02:42.859338 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.859338 13088 net.cpp:137] Memory required for data: 334030000
I1211 06:02:42.859338 13088 layer_factory.cpp:58] Creating layer bn2_pool2_1
I1211 06:02:42.859338 13088 net.cpp:84] Creating Layer bn2_pool2_1
I1211 06:02:42.859338 13088 net.cpp:406] bn2_pool2_1 <- pool2_1
I1211 06:02:42.859338 13088 net.cpp:367] bn2_pool2_1 -> pool2_1 (in-place)
I1211 06:02:42.860338 13088 net.cpp:122] Setting up bn2_pool2_1
I1211 06:02:42.860338 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.860338 13088 net.cpp:137] Memory required for data: 339150000
I1211 06:02:42.860338 13088 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 06:02:42.860338 13088 net.cpp:84] Creating Layer scale2_pool2_1
I1211 06:02:42.860338 13088 net.cpp:406] scale2_pool2_1 <- pool2_1
I1211 06:02:42.860338 13088 net.cpp:367] scale2_pool2_1 -> pool2_1 (in-place)
I1211 06:02:42.860338 13088 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 06:02:42.860338 13088 net.cpp:122] Setting up scale2_pool2_1
I1211 06:02:42.860338 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.860338 13088 net.cpp:137] Memory required for data: 344270000
I1211 06:02:42.860338 13088 layer_factory.cpp:58] Creating layer relu2_pool2_1
I1211 06:02:42.860338 13088 net.cpp:84] Creating Layer relu2_pool2_1
I1211 06:02:42.860338 13088 net.cpp:406] relu2_pool2_1 <- pool2_1
I1211 06:02:42.860338 13088 net.cpp:367] relu2_pool2_1 -> pool2_1 (in-place)
I1211 06:02:42.860338 13088 net.cpp:122] Setting up relu2_pool2_1
I1211 06:02:42.860338 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.860338 13088 net.cpp:137] Memory required for data: 349390000
I1211 06:02:42.860338 13088 layer_factory.cpp:58] Creating layer conv3
I1211 06:02:42.860338 13088 net.cpp:84] Creating Layer conv3
I1211 06:02:42.860338 13088 net.cpp:406] conv3 <- pool2_1
I1211 06:02:42.860338 13088 net.cpp:380] conv3 -> conv3
I1211 06:02:42.861340 13088 net.cpp:122] Setting up conv3
I1211 06:02:42.862340 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.862340 13088 net.cpp:137] Memory required for data: 354510000
I1211 06:02:42.862340 13088 layer_factory.cpp:58] Creating layer bn3
I1211 06:02:42.862340 13088 net.cpp:84] Creating Layer bn3
I1211 06:02:42.862340 13088 net.cpp:406] bn3 <- conv3
I1211 06:02:42.862340 13088 net.cpp:367] bn3 -> conv3 (in-place)
I1211 06:02:42.862340 13088 net.cpp:122] Setting up bn3
I1211 06:02:42.862340 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.862340 13088 net.cpp:137] Memory required for data: 359630000
I1211 06:02:42.862340 13088 layer_factory.cpp:58] Creating layer scale3
I1211 06:02:42.862340 13088 net.cpp:84] Creating Layer scale3
I1211 06:02:42.862340 13088 net.cpp:406] scale3 <- conv3
I1211 06:02:42.862340 13088 net.cpp:367] scale3 -> conv3 (in-place)
I1211 06:02:42.862340 13088 layer_factory.cpp:58] Creating layer scale3
I1211 06:02:42.862340 13088 net.cpp:122] Setting up scale3
I1211 06:02:42.862340 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.862340 13088 net.cpp:137] Memory required for data: 364750000
I1211 06:02:42.862340 13088 layer_factory.cpp:58] Creating layer relu3
I1211 06:02:42.862340 13088 net.cpp:84] Creating Layer relu3
I1211 06:02:42.862340 13088 net.cpp:406] relu3 <- conv3
I1211 06:02:42.862340 13088 net.cpp:367] relu3 -> conv3 (in-place)
I1211 06:02:42.862340 13088 net.cpp:122] Setting up relu3
I1211 06:02:42.862340 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.862340 13088 net.cpp:137] Memory required for data: 369870000
I1211 06:02:42.862340 13088 layer_factory.cpp:58] Creating layer conv3_1
I1211 06:02:42.862340 13088 net.cpp:84] Creating Layer conv3_1
I1211 06:02:42.862340 13088 net.cpp:406] conv3_1 <- conv3
I1211 06:02:42.862340 13088 net.cpp:380] conv3_1 -> conv3_1
I1211 06:02:42.864341 13088 net.cpp:122] Setting up conv3_1
I1211 06:02:42.864341 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.864341 13088 net.cpp:137] Memory required for data: 374990000
I1211 06:02:42.864341 13088 layer_factory.cpp:58] Creating layer bn3_1
I1211 06:02:42.864341 13088 net.cpp:84] Creating Layer bn3_1
I1211 06:02:42.864341 13088 net.cpp:406] bn3_1 <- conv3_1
I1211 06:02:42.864341 13088 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 06:02:42.864341 13088 net.cpp:122] Setting up bn3_1
I1211 06:02:42.864341 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.864341 13088 net.cpp:137] Memory required for data: 380110000
I1211 06:02:42.864341 13088 layer_factory.cpp:58] Creating layer scale3_1
I1211 06:02:42.864341 13088 net.cpp:84] Creating Layer scale3_1
I1211 06:02:42.864341 13088 net.cpp:406] scale3_1 <- conv3_1
I1211 06:02:42.864341 13088 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 06:02:42.864341 13088 layer_factory.cpp:58] Creating layer scale3_1
I1211 06:02:42.864341 13088 net.cpp:122] Setting up scale3_1
I1211 06:02:42.864341 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.864341 13088 net.cpp:137] Memory required for data: 385230000
I1211 06:02:42.864341 13088 layer_factory.cpp:58] Creating layer relu3_1
I1211 06:02:42.864341 13088 net.cpp:84] Creating Layer relu3_1
I1211 06:02:42.864341 13088 net.cpp:406] relu3_1 <- conv3_1
I1211 06:02:42.864341 13088 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 06:02:42.865339 13088 net.cpp:122] Setting up relu3_1
I1211 06:02:42.865339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.865339 13088 net.cpp:137] Memory required for data: 390350000
I1211 06:02:42.865339 13088 layer_factory.cpp:58] Creating layer conv4
I1211 06:02:42.865339 13088 net.cpp:84] Creating Layer conv4
I1211 06:02:42.865339 13088 net.cpp:406] conv4 <- conv3_1
I1211 06:02:42.865339 13088 net.cpp:380] conv4 -> conv4
I1211 06:02:42.866339 13088 net.cpp:122] Setting up conv4
I1211 06:02:42.866339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.866339 13088 net.cpp:137] Memory required for data: 395470000
I1211 06:02:42.866339 13088 layer_factory.cpp:58] Creating layer bn4
I1211 06:02:42.866339 13088 net.cpp:84] Creating Layer bn4
I1211 06:02:42.866339 13088 net.cpp:406] bn4 <- conv4
I1211 06:02:42.866339 13088 net.cpp:367] bn4 -> conv4 (in-place)
I1211 06:02:42.866339 13088 net.cpp:122] Setting up bn4
I1211 06:02:42.866339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.866339 13088 net.cpp:137] Memory required for data: 400590000
I1211 06:02:42.866339 13088 layer_factory.cpp:58] Creating layer scale4
I1211 06:02:42.866339 13088 net.cpp:84] Creating Layer scale4
I1211 06:02:42.866339 13088 net.cpp:406] scale4 <- conv4
I1211 06:02:42.866339 13088 net.cpp:367] scale4 -> conv4 (in-place)
I1211 06:02:42.866339 13088 layer_factory.cpp:58] Creating layer scale4
I1211 06:02:42.866339 13088 net.cpp:122] Setting up scale4
I1211 06:02:42.866339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.866339 13088 net.cpp:137] Memory required for data: 405710000
I1211 06:02:42.866339 13088 layer_factory.cpp:58] Creating layer relu4
I1211 06:02:42.866339 13088 net.cpp:84] Creating Layer relu4
I1211 06:02:42.866339 13088 net.cpp:406] relu4 <- conv4
I1211 06:02:42.866339 13088 net.cpp:367] relu4 -> conv4 (in-place)
I1211 06:02:42.866339 13088 net.cpp:122] Setting up relu4
I1211 06:02:42.866339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.866339 13088 net.cpp:137] Memory required for data: 410830000
I1211 06:02:42.866339 13088 layer_factory.cpp:58] Creating layer conv4_1
I1211 06:02:42.866339 13088 net.cpp:84] Creating Layer conv4_1
I1211 06:02:42.866339 13088 net.cpp:406] conv4_1 <- conv4
I1211 06:02:42.866339 13088 net.cpp:380] conv4_1 -> conv4_1
I1211 06:02:42.867339 13088 net.cpp:122] Setting up conv4_1
I1211 06:02:42.868340 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.868340 13088 net.cpp:137] Memory required for data: 415950000
I1211 06:02:42.868340 13088 layer_factory.cpp:58] Creating layer bn4_1
I1211 06:02:42.868340 13088 net.cpp:84] Creating Layer bn4_1
I1211 06:02:42.868340 13088 net.cpp:406] bn4_1 <- conv4_1
I1211 06:02:42.868340 13088 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 06:02:42.868340 13088 net.cpp:122] Setting up bn4_1
I1211 06:02:42.868340 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.868340 13088 net.cpp:137] Memory required for data: 421070000
I1211 06:02:42.868340 13088 layer_factory.cpp:58] Creating layer scale4_1
I1211 06:02:42.868340 13088 net.cpp:84] Creating Layer scale4_1
I1211 06:02:42.868340 13088 net.cpp:406] scale4_1 <- conv4_1
I1211 06:02:42.868340 13088 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 06:02:42.868340 13088 layer_factory.cpp:58] Creating layer scale4_1
I1211 06:02:42.868340 13088 net.cpp:122] Setting up scale4_1
I1211 06:02:42.868340 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.868340 13088 net.cpp:137] Memory required for data: 426190000
I1211 06:02:42.868340 13088 layer_factory.cpp:58] Creating layer relu4_1
I1211 06:02:42.868340 13088 net.cpp:84] Creating Layer relu4_1
I1211 06:02:42.868340 13088 net.cpp:406] relu4_1 <- conv4_1
I1211 06:02:42.868340 13088 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 06:02:42.868340 13088 net.cpp:122] Setting up relu4_1
I1211 06:02:42.868340 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.868340 13088 net.cpp:137] Memory required for data: 431310000
I1211 06:02:42.868340 13088 layer_factory.cpp:58] Creating layer conv4_2
I1211 06:02:42.868340 13088 net.cpp:84] Creating Layer conv4_2
I1211 06:02:42.868340 13088 net.cpp:406] conv4_2 <- conv4_1
I1211 06:02:42.868340 13088 net.cpp:380] conv4_2 -> conv4_2
I1211 06:02:42.870338 13088 net.cpp:122] Setting up conv4_2
I1211 06:02:42.870338 13088 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:02:42.870338 13088 net.cpp:137] Memory required for data: 437249200
I1211 06:02:42.870338 13088 layer_factory.cpp:58] Creating layer bn4_2
I1211 06:02:42.870338 13088 net.cpp:84] Creating Layer bn4_2
I1211 06:02:42.870338 13088 net.cpp:406] bn4_2 <- conv4_2
I1211 06:02:42.870338 13088 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 06:02:42.870338 13088 net.cpp:122] Setting up bn4_2
I1211 06:02:42.870338 13088 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:02:42.870338 13088 net.cpp:137] Memory required for data: 443188400
I1211 06:02:42.870338 13088 layer_factory.cpp:58] Creating layer scale4_2
I1211 06:02:42.870338 13088 net.cpp:84] Creating Layer scale4_2
I1211 06:02:42.870338 13088 net.cpp:406] scale4_2 <- conv4_2
I1211 06:02:42.870338 13088 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 06:02:42.870338 13088 layer_factory.cpp:58] Creating layer scale4_2
I1211 06:02:42.870338 13088 net.cpp:122] Setting up scale4_2
I1211 06:02:42.870338 13088 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:02:42.870338 13088 net.cpp:137] Memory required for data: 449127600
I1211 06:02:42.870338 13088 layer_factory.cpp:58] Creating layer relu4_2
I1211 06:02:42.870338 13088 net.cpp:84] Creating Layer relu4_2
I1211 06:02:42.870338 13088 net.cpp:406] relu4_2 <- conv4_2
I1211 06:02:42.870338 13088 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 06:02:42.870338 13088 net.cpp:122] Setting up relu4_2
I1211 06:02:42.870338 13088 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:02:42.870338 13088 net.cpp:137] Memory required for data: 455066800
I1211 06:02:42.870338 13088 layer_factory.cpp:58] Creating layer pool4_2
I1211 06:02:42.870338 13088 net.cpp:84] Creating Layer pool4_2
I1211 06:02:42.870338 13088 net.cpp:406] pool4_2 <- conv4_2
I1211 06:02:42.870338 13088 net.cpp:380] pool4_2 -> pool4_2
I1211 06:02:42.871340 13088 net.cpp:122] Setting up pool4_2
I1211 06:02:42.872339 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.872339 13088 net.cpp:137] Memory required for data: 456551600
I1211 06:02:42.872339 13088 layer_factory.cpp:58] Creating layer bn4_pool4_2
I1211 06:02:42.872339 13088 net.cpp:84] Creating Layer bn4_pool4_2
I1211 06:02:42.872339 13088 net.cpp:406] bn4_pool4_2 <- pool4_2
I1211 06:02:42.872339 13088 net.cpp:367] bn4_pool4_2 -> pool4_2 (in-place)
I1211 06:02:42.872339 13088 net.cpp:122] Setting up bn4_pool4_2
I1211 06:02:42.872339 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.872339 13088 net.cpp:137] Memory required for data: 458036400
I1211 06:02:42.872339 13088 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 06:02:42.872339 13088 net.cpp:84] Creating Layer scale4_pool4_2
I1211 06:02:42.872339 13088 net.cpp:406] scale4_pool4_2 <- pool4_2
I1211 06:02:42.872339 13088 net.cpp:367] scale4_pool4_2 -> pool4_2 (in-place)
I1211 06:02:42.872339 13088 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 06:02:42.872339 13088 net.cpp:122] Setting up scale4_pool4_2
I1211 06:02:42.872339 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.872339 13088 net.cpp:137] Memory required for data: 459521200
I1211 06:02:42.872339 13088 layer_factory.cpp:58] Creating layer relu4_pool4_2
I1211 06:02:42.872339 13088 net.cpp:84] Creating Layer relu4_pool4_2
I1211 06:02:42.872339 13088 net.cpp:406] relu4_pool4_2 <- pool4_2
I1211 06:02:42.872339 13088 net.cpp:367] relu4_pool4_2 -> pool4_2 (in-place)
I1211 06:02:42.873340 13088 net.cpp:122] Setting up relu4_pool4_2
I1211 06:02:42.873340 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.873340 13088 net.cpp:137] Memory required for data: 461006000
I1211 06:02:42.873340 13088 layer_factory.cpp:58] Creating layer conv4_0
I1211 06:02:42.873340 13088 net.cpp:84] Creating Layer conv4_0
I1211 06:02:42.873340 13088 net.cpp:406] conv4_0 <- pool4_2
I1211 06:02:42.873340 13088 net.cpp:380] conv4_0 -> conv4_0
I1211 06:02:42.874349 13088 net.cpp:122] Setting up conv4_0
I1211 06:02:42.874349 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.874349 13088 net.cpp:137] Memory required for data: 462490800
I1211 06:02:42.874349 13088 layer_factory.cpp:58] Creating layer bn4_0
I1211 06:02:42.874349 13088 net.cpp:84] Creating Layer bn4_0
I1211 06:02:42.874349 13088 net.cpp:406] bn4_0 <- conv4_0
I1211 06:02:42.874349 13088 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 06:02:42.874349 13088 net.cpp:122] Setting up bn4_0
I1211 06:02:42.874349 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.874349 13088 net.cpp:137] Memory required for data: 463975600
I1211 06:02:42.874349 13088 layer_factory.cpp:58] Creating layer scale4_0
I1211 06:02:42.874349 13088 net.cpp:84] Creating Layer scale4_0
I1211 06:02:42.874349 13088 net.cpp:406] scale4_0 <- conv4_0
I1211 06:02:42.874349 13088 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 06:02:42.874349 13088 layer_factory.cpp:58] Creating layer scale4_0
I1211 06:02:42.875337 13088 net.cpp:122] Setting up scale4_0
I1211 06:02:42.875337 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.875337 13088 net.cpp:137] Memory required for data: 465460400
I1211 06:02:42.875337 13088 layer_factory.cpp:58] Creating layer relu4_0
I1211 06:02:42.875337 13088 net.cpp:84] Creating Layer relu4_0
I1211 06:02:42.875337 13088 net.cpp:406] relu4_0 <- conv4_0
I1211 06:02:42.875337 13088 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 06:02:42.875337 13088 net.cpp:122] Setting up relu4_0
I1211 06:02:42.875337 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.875337 13088 net.cpp:137] Memory required for data: 466945200
I1211 06:02:42.875337 13088 layer_factory.cpp:58] Creating layer conv11
I1211 06:02:42.875337 13088 net.cpp:84] Creating Layer conv11
I1211 06:02:42.875337 13088 net.cpp:406] conv11 <- conv4_0
I1211 06:02:42.875337 13088 net.cpp:380] conv11 -> conv11
I1211 06:02:42.876336 13088 net.cpp:122] Setting up conv11
I1211 06:02:42.876336 13088 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:02:42.876336 13088 net.cpp:137] Memory required for data: 468737200
I1211 06:02:42.876336 13088 layer_factory.cpp:58] Creating layer bn_conv11
I1211 06:02:42.876336 13088 net.cpp:84] Creating Layer bn_conv11
I1211 06:02:42.876336 13088 net.cpp:406] bn_conv11 <- conv11
I1211 06:02:42.876336 13088 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 06:02:42.877339 13088 net.cpp:122] Setting up bn_conv11
I1211 06:02:42.877339 13088 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:02:42.877339 13088 net.cpp:137] Memory required for data: 470529200
I1211 06:02:42.877339 13088 layer_factory.cpp:58] Creating layer scale_conv11
I1211 06:02:42.877339 13088 net.cpp:84] Creating Layer scale_conv11
I1211 06:02:42.877339 13088 net.cpp:406] scale_conv11 <- conv11
I1211 06:02:42.877339 13088 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 06:02:42.877339 13088 layer_factory.cpp:58] Creating layer scale_conv11
I1211 06:02:42.877339 13088 net.cpp:122] Setting up scale_conv11
I1211 06:02:42.877339 13088 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:02:42.877339 13088 net.cpp:137] Memory required for data: 472321200
I1211 06:02:42.877339 13088 layer_factory.cpp:58] Creating layer relu_conv11
I1211 06:02:42.877339 13088 net.cpp:84] Creating Layer relu_conv11
I1211 06:02:42.877339 13088 net.cpp:406] relu_conv11 <- conv11
I1211 06:02:42.877339 13088 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 06:02:42.877339 13088 net.cpp:122] Setting up relu_conv11
I1211 06:02:42.877339 13088 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:02:42.877339 13088 net.cpp:137] Memory required for data: 474113200
I1211 06:02:42.877339 13088 layer_factory.cpp:58] Creating layer conv12
I1211 06:02:42.877339 13088 net.cpp:84] Creating Layer conv12
I1211 06:02:42.877339 13088 net.cpp:406] conv12 <- conv11
I1211 06:02:42.877339 13088 net.cpp:380] conv12 -> conv12
I1211 06:02:42.879339 13088 net.cpp:122] Setting up conv12
I1211 06:02:42.879339 13088 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:02:42.879339 13088 net.cpp:137] Memory required for data: 476417200
I1211 06:02:42.879339 13088 layer_factory.cpp:58] Creating layer bn_conv12
I1211 06:02:42.879339 13088 net.cpp:84] Creating Layer bn_conv12
I1211 06:02:42.879339 13088 net.cpp:406] bn_conv12 <- conv12
I1211 06:02:42.879339 13088 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 06:02:42.879339 13088 net.cpp:122] Setting up bn_conv12
I1211 06:02:42.879339 13088 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:02:42.879339 13088 net.cpp:137] Memory required for data: 478721200
I1211 06:02:42.879339 13088 layer_factory.cpp:58] Creating layer scale_conv12
I1211 06:02:42.879339 13088 net.cpp:84] Creating Layer scale_conv12
I1211 06:02:42.879339 13088 net.cpp:406] scale_conv12 <- conv12
I1211 06:02:42.879339 13088 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 06:02:42.879339 13088 layer_factory.cpp:58] Creating layer scale_conv12
I1211 06:02:42.879339 13088 net.cpp:122] Setting up scale_conv12
I1211 06:02:42.879339 13088 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:02:42.879339 13088 net.cpp:137] Memory required for data: 481025200
I1211 06:02:42.879339 13088 layer_factory.cpp:58] Creating layer relu_conv12
I1211 06:02:42.879339 13088 net.cpp:84] Creating Layer relu_conv12
I1211 06:02:42.879339 13088 net.cpp:406] relu_conv12 <- conv12
I1211 06:02:42.879339 13088 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 06:02:42.879339 13088 net.cpp:122] Setting up relu_conv12
I1211 06:02:42.879339 13088 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:02:42.879339 13088 net.cpp:137] Memory required for data: 483329200
I1211 06:02:42.879339 13088 layer_factory.cpp:58] Creating layer poolcp6
I1211 06:02:42.879339 13088 net.cpp:84] Creating Layer poolcp6
I1211 06:02:42.879339 13088 net.cpp:406] poolcp6 <- conv12
I1211 06:02:42.880339 13088 net.cpp:380] poolcp6 -> poolcp6
I1211 06:02:42.880339 13088 net.cpp:122] Setting up poolcp6
I1211 06:02:42.880339 13088 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 06:02:42.880339 13088 net.cpp:137] Memory required for data: 483365200
I1211 06:02:42.880339 13088 layer_factory.cpp:58] Creating layer ip1
I1211 06:02:42.880339 13088 net.cpp:84] Creating Layer ip1
I1211 06:02:42.880339 13088 net.cpp:406] ip1 <- poolcp6
I1211 06:02:42.880339 13088 net.cpp:380] ip1 -> ip1
I1211 06:02:42.880339 13088 net.cpp:122] Setting up ip1
I1211 06:02:42.880339 13088 net.cpp:129] Top shape: 100 100 (10000)
I1211 06:02:42.880339 13088 net.cpp:137] Memory required for data: 483405200
I1211 06:02:42.880339 13088 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 06:02:42.880339 13088 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 06:02:42.880339 13088 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 06:02:42.880339 13088 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 06:02:42.880339 13088 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 06:02:42.880339 13088 net.cpp:122] Setting up ip1_ip1_0_split
I1211 06:02:42.880339 13088 net.cpp:129] Top shape: 100 100 (10000)
I1211 06:02:42.880339 13088 net.cpp:129] Top shape: 100 100 (10000)
I1211 06:02:42.880339 13088 net.cpp:137] Memory required for data: 483485200
I1211 06:02:42.880339 13088 layer_factory.cpp:58] Creating layer accuracy_training
I1211 06:02:42.880339 13088 net.cpp:84] Creating Layer accuracy_training
I1211 06:02:42.880339 13088 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1211 06:02:42.880339 13088 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1211 06:02:42.880339 13088 net.cpp:380] accuracy_training -> accuracy_training
I1211 06:02:42.880339 13088 net.cpp:122] Setting up accuracy_training
I1211 06:02:42.880339 13088 net.cpp:129] Top shape: (1)
I1211 06:02:42.880339 13088 net.cpp:137] Memory required for data: 483485204
I1211 06:02:42.880339 13088 layer_factory.cpp:58] Creating layer loss
I1211 06:02:42.880339 13088 net.cpp:84] Creating Layer loss
I1211 06:02:42.880339 13088 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 06:02:42.880339 13088 net.cpp:406] loss <- label_cifar_1_split_1
I1211 06:02:42.880339 13088 net.cpp:380] loss -> loss
I1211 06:02:42.880339 13088 layer_factory.cpp:58] Creating layer loss
I1211 06:02:42.881340 13088 net.cpp:122] Setting up loss
I1211 06:02:42.881340 13088 net.cpp:129] Top shape: (1)
I1211 06:02:42.881340 13088 net.cpp:132]     with loss weight 1
I1211 06:02:42.881340 13088 net.cpp:137] Memory required for data: 483485208
I1211 06:02:42.881340 13088 net.cpp:198] loss needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:200] accuracy_training does not need backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] ip1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] poolcp6 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu_conv12 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale_conv12 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn_conv12 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] conv12 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu_conv11 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale_conv11 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn_conv11 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] conv11 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu4_0 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale4_0 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn4_0 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] conv4_0 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu4_pool4_2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale4_pool4_2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn4_pool4_2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] pool4_2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu4_2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale4_2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn4_2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] conv4_2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu4_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale4_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn4_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] conv4_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu4 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale4 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn4 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] conv4 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu3_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale3_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn3_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] conv3_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu3 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale3 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn3 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] conv3 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu2_pool2_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale2_pool2_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn2_pool2_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] pool2_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu2_2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale2_2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn2_2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] conv2_2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu2_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale2_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn2_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] conv2_1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] conv2 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu1_0 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale1_0 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn1_0 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] conv1_0 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] relu1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] scale1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] bn1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:198] conv1 needs backward computation.
I1211 06:02:42.881340 13088 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 06:02:42.881340 13088 net.cpp:200] cifar does not need backward computation.
I1211 06:02:42.881340 13088 net.cpp:242] This network produces output accuracy_training
I1211 06:02:42.881340 13088 net.cpp:242] This network produces output loss
I1211 06:02:42.881340 13088 net.cpp:255] Network initialization done.
I1211 06:02:42.882339 13088 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 06:02:42.882339 13088 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 06:02:42.882339 13088 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_pool2_1
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_pool4_2
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1211 06:02:42.882339 13088 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1211 06:02:42.883339 13088 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV2_WnonLin_360k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2_1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "pool2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_pool2_1"
  type: "BatchNorm"
  bottom: "pool2_1"
  top: "pool2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_pool2_1"
  type: "Scale"
  bottom: "pool2_1"
  top: "pool2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_pool2_1"
  type: "ReLU"
  bottom: "pool2_1"
  top: "pool2_1"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "pool4_2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "pool4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_pool4_2"
  type: "BatchNorm"
  bottom: "pool4_2"
  top: "pool4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_pool4_2"
  type: "Scale"
  bottom: "pool4_2"
  top: "pool4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_pool4_2"
  type: "ReLU"
  bottom: "pool4_2"
  top: "pool4_2"
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 06:02:42.883339 13088 layer_factory.cpp:58] Creating layer cifar
I1211 06:02:42.888342 13088 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1211 06:02:42.889350 13088 net.cpp:84] Creating Layer cifar
I1211 06:02:42.889350 13088 net.cpp:380] cifar -> data
I1211 06:02:42.889350 13088 net.cpp:380] cifar -> label
I1211 06:02:42.889350 13088 data_layer.cpp:45] output data size: 100,3,32,32
I1211 06:02:42.896337 13088 net.cpp:122] Setting up cifar
I1211 06:02:42.896337 13088 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 06:02:42.896337 13088 net.cpp:129] Top shape: 100 (100)
I1211 06:02:42.896337 13088 net.cpp:137] Memory required for data: 1229200
I1211 06:02:42.896337 13088 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 06:02:42.896337 13088 net.cpp:84] Creating Layer label_cifar_1_split
I1211 06:02:42.896337 13088 net.cpp:406] label_cifar_1_split <- label
I1211 06:02:42.896337 13088 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 06:02:42.896337 13088 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 06:02:42.896337 13088 net.cpp:122] Setting up label_cifar_1_split
I1211 06:02:42.896337 13088 net.cpp:129] Top shape: 100 (100)
I1211 06:02:42.896337 13088 net.cpp:129] Top shape: 100 (100)
I1211 06:02:42.896337 13088 net.cpp:137] Memory required for data: 1230000
I1211 06:02:42.896337 13088 layer_factory.cpp:58] Creating layer conv1
I1211 06:02:42.896337 13088 net.cpp:84] Creating Layer conv1
I1211 06:02:42.896337 13088 net.cpp:406] conv1 <- data
I1211 06:02:42.896337 13088 net.cpp:380] conv1 -> conv1
I1211 06:02:42.897347 12612 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 06:02:42.898344 13088 net.cpp:122] Setting up conv1
I1211 06:02:42.898344 13088 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:02:42.898344 13088 net.cpp:137] Memory required for data: 13518000
I1211 06:02:42.898344 13088 layer_factory.cpp:58] Creating layer bn1
I1211 06:02:42.898344 13088 net.cpp:84] Creating Layer bn1
I1211 06:02:42.898344 13088 net.cpp:406] bn1 <- conv1
I1211 06:02:42.898344 13088 net.cpp:367] bn1 -> conv1 (in-place)
I1211 06:02:42.898344 13088 net.cpp:122] Setting up bn1
I1211 06:02:42.898344 13088 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:02:42.898344 13088 net.cpp:137] Memory required for data: 25806000
I1211 06:02:42.898344 13088 layer_factory.cpp:58] Creating layer scale1
I1211 06:02:42.898344 13088 net.cpp:84] Creating Layer scale1
I1211 06:02:42.898344 13088 net.cpp:406] scale1 <- conv1
I1211 06:02:42.898344 13088 net.cpp:367] scale1 -> conv1 (in-place)
I1211 06:02:42.898344 13088 layer_factory.cpp:58] Creating layer scale1
I1211 06:02:42.898344 13088 net.cpp:122] Setting up scale1
I1211 06:02:42.898344 13088 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:02:42.898344 13088 net.cpp:137] Memory required for data: 38094000
I1211 06:02:42.898344 13088 layer_factory.cpp:58] Creating layer relu1
I1211 06:02:42.898344 13088 net.cpp:84] Creating Layer relu1
I1211 06:02:42.898344 13088 net.cpp:406] relu1 <- conv1
I1211 06:02:42.898344 13088 net.cpp:367] relu1 -> conv1 (in-place)
I1211 06:02:42.899338 13088 net.cpp:122] Setting up relu1
I1211 06:02:42.899338 13088 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 06:02:42.899338 13088 net.cpp:137] Memory required for data: 50382000
I1211 06:02:42.899338 13088 layer_factory.cpp:58] Creating layer conv1_0
I1211 06:02:42.899338 13088 net.cpp:84] Creating Layer conv1_0
I1211 06:02:42.899338 13088 net.cpp:406] conv1_0 <- conv1
I1211 06:02:42.899338 13088 net.cpp:380] conv1_0 -> conv1_0
I1211 06:02:42.900339 13088 net.cpp:122] Setting up conv1_0
I1211 06:02:42.900339 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.900339 13088 net.cpp:137] Memory required for data: 66766000
I1211 06:02:42.900339 13088 layer_factory.cpp:58] Creating layer bn1_0
I1211 06:02:42.900339 13088 net.cpp:84] Creating Layer bn1_0
I1211 06:02:42.900339 13088 net.cpp:406] bn1_0 <- conv1_0
I1211 06:02:42.900339 13088 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 06:02:42.900339 13088 net.cpp:122] Setting up bn1_0
I1211 06:02:42.900339 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.900339 13088 net.cpp:137] Memory required for data: 83150000
I1211 06:02:42.900339 13088 layer_factory.cpp:58] Creating layer scale1_0
I1211 06:02:42.900339 13088 net.cpp:84] Creating Layer scale1_0
I1211 06:02:42.900339 13088 net.cpp:406] scale1_0 <- conv1_0
I1211 06:02:42.900339 13088 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 06:02:42.900339 13088 layer_factory.cpp:58] Creating layer scale1_0
I1211 06:02:42.901340 13088 net.cpp:122] Setting up scale1_0
I1211 06:02:42.901340 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.901340 13088 net.cpp:137] Memory required for data: 99534000
I1211 06:02:42.901340 13088 layer_factory.cpp:58] Creating layer relu1_0
I1211 06:02:42.901340 13088 net.cpp:84] Creating Layer relu1_0
I1211 06:02:42.901340 13088 net.cpp:406] relu1_0 <- conv1_0
I1211 06:02:42.901340 13088 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 06:02:42.901340 13088 net.cpp:122] Setting up relu1_0
I1211 06:02:42.901340 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.901340 13088 net.cpp:137] Memory required for data: 115918000
I1211 06:02:42.901340 13088 layer_factory.cpp:58] Creating layer conv2
I1211 06:02:42.901340 13088 net.cpp:84] Creating Layer conv2
I1211 06:02:42.901340 13088 net.cpp:406] conv2 <- conv1_0
I1211 06:02:42.901340 13088 net.cpp:380] conv2 -> conv2
I1211 06:02:42.902339 13088 net.cpp:122] Setting up conv2
I1211 06:02:42.902339 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.902339 13088 net.cpp:137] Memory required for data: 132302000
I1211 06:02:42.902339 13088 layer_factory.cpp:58] Creating layer bn2
I1211 06:02:42.902339 13088 net.cpp:84] Creating Layer bn2
I1211 06:02:42.902339 13088 net.cpp:406] bn2 <- conv2
I1211 06:02:42.902339 13088 net.cpp:367] bn2 -> conv2 (in-place)
I1211 06:02:42.903339 13088 net.cpp:122] Setting up bn2
I1211 06:02:42.903339 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.903339 13088 net.cpp:137] Memory required for data: 148686000
I1211 06:02:42.903339 13088 layer_factory.cpp:58] Creating layer scale2
I1211 06:02:42.903339 13088 net.cpp:84] Creating Layer scale2
I1211 06:02:42.903339 13088 net.cpp:406] scale2 <- conv2
I1211 06:02:42.903339 13088 net.cpp:367] scale2 -> conv2 (in-place)
I1211 06:02:42.903339 13088 layer_factory.cpp:58] Creating layer scale2
I1211 06:02:42.903339 13088 net.cpp:122] Setting up scale2
I1211 06:02:42.903339 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.903339 13088 net.cpp:137] Memory required for data: 165070000
I1211 06:02:42.903339 13088 layer_factory.cpp:58] Creating layer relu2
I1211 06:02:42.903339 13088 net.cpp:84] Creating Layer relu2
I1211 06:02:42.903339 13088 net.cpp:406] relu2 <- conv2
I1211 06:02:42.903339 13088 net.cpp:367] relu2 -> conv2 (in-place)
I1211 06:02:42.903339 13088 net.cpp:122] Setting up relu2
I1211 06:02:42.903339 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.903339 13088 net.cpp:137] Memory required for data: 181454000
I1211 06:02:42.903339 13088 layer_factory.cpp:58] Creating layer conv2_1
I1211 06:02:42.903339 13088 net.cpp:84] Creating Layer conv2_1
I1211 06:02:42.903339 13088 net.cpp:406] conv2_1 <- conv2
I1211 06:02:42.903339 13088 net.cpp:380] conv2_1 -> conv2_1
I1211 06:02:42.905339 13088 net.cpp:122] Setting up conv2_1
I1211 06:02:42.905339 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.905339 13088 net.cpp:137] Memory required for data: 197838000
I1211 06:02:42.905339 13088 layer_factory.cpp:58] Creating layer bn2_1
I1211 06:02:42.905339 13088 net.cpp:84] Creating Layer bn2_1
I1211 06:02:42.905339 13088 net.cpp:406] bn2_1 <- conv2_1
I1211 06:02:42.905339 13088 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 06:02:42.905339 13088 net.cpp:122] Setting up bn2_1
I1211 06:02:42.905339 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.905339 13088 net.cpp:137] Memory required for data: 214222000
I1211 06:02:42.905339 13088 layer_factory.cpp:58] Creating layer scale2_1
I1211 06:02:42.905339 13088 net.cpp:84] Creating Layer scale2_1
I1211 06:02:42.905339 13088 net.cpp:406] scale2_1 <- conv2_1
I1211 06:02:42.905339 13088 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 06:02:42.905339 13088 layer_factory.cpp:58] Creating layer scale2_1
I1211 06:02:42.905339 13088 net.cpp:122] Setting up scale2_1
I1211 06:02:42.905339 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.905339 13088 net.cpp:137] Memory required for data: 230606000
I1211 06:02:42.905339 13088 layer_factory.cpp:58] Creating layer relu2_1
I1211 06:02:42.905339 13088 net.cpp:84] Creating Layer relu2_1
I1211 06:02:42.905339 13088 net.cpp:406] relu2_1 <- conv2_1
I1211 06:02:42.905339 13088 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 06:02:42.905339 13088 net.cpp:122] Setting up relu2_1
I1211 06:02:42.905339 13088 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 06:02:42.905339 13088 net.cpp:137] Memory required for data: 246990000
I1211 06:02:42.905339 13088 layer_factory.cpp:58] Creating layer conv2_2
I1211 06:02:42.905339 13088 net.cpp:84] Creating Layer conv2_2
I1211 06:02:42.905339 13088 net.cpp:406] conv2_2 <- conv2_1
I1211 06:02:42.905339 13088 net.cpp:380] conv2_2 -> conv2_2
I1211 06:02:42.907340 13088 net.cpp:122] Setting up conv2_2
I1211 06:02:42.907340 13088 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:02:42.907340 13088 net.cpp:137] Memory required for data: 267470000
I1211 06:02:42.907340 13088 layer_factory.cpp:58] Creating layer bn2_2
I1211 06:02:42.907340 13088 net.cpp:84] Creating Layer bn2_2
I1211 06:02:42.907340 13088 net.cpp:406] bn2_2 <- conv2_2
I1211 06:02:42.907340 13088 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 06:02:42.907340 13088 net.cpp:122] Setting up bn2_2
I1211 06:02:42.907340 13088 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:02:42.907340 13088 net.cpp:137] Memory required for data: 287950000
I1211 06:02:42.907340 13088 layer_factory.cpp:58] Creating layer scale2_2
I1211 06:02:42.907340 13088 net.cpp:84] Creating Layer scale2_2
I1211 06:02:42.907340 13088 net.cpp:406] scale2_2 <- conv2_2
I1211 06:02:42.907340 13088 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 06:02:42.907340 13088 layer_factory.cpp:58] Creating layer scale2_2
I1211 06:02:42.907340 13088 net.cpp:122] Setting up scale2_2
I1211 06:02:42.907340 13088 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:02:42.907340 13088 net.cpp:137] Memory required for data: 308430000
I1211 06:02:42.907340 13088 layer_factory.cpp:58] Creating layer relu2_2
I1211 06:02:42.907340 13088 net.cpp:84] Creating Layer relu2_2
I1211 06:02:42.907340 13088 net.cpp:406] relu2_2 <- conv2_2
I1211 06:02:42.907340 13088 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 06:02:42.907340 13088 net.cpp:122] Setting up relu2_2
I1211 06:02:42.907340 13088 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 06:02:42.907340 13088 net.cpp:137] Memory required for data: 328910000
I1211 06:02:42.907340 13088 layer_factory.cpp:58] Creating layer pool2_1
I1211 06:02:42.907340 13088 net.cpp:84] Creating Layer pool2_1
I1211 06:02:42.907340 13088 net.cpp:406] pool2_1 <- conv2_2
I1211 06:02:42.907340 13088 net.cpp:380] pool2_1 -> pool2_1
I1211 06:02:42.909339 13088 net.cpp:122] Setting up pool2_1
I1211 06:02:42.909339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.909339 13088 net.cpp:137] Memory required for data: 334030000
I1211 06:02:42.909339 13088 layer_factory.cpp:58] Creating layer bn2_pool2_1
I1211 06:02:42.909339 13088 net.cpp:84] Creating Layer bn2_pool2_1
I1211 06:02:42.909339 13088 net.cpp:406] bn2_pool2_1 <- pool2_1
I1211 06:02:42.909339 13088 net.cpp:367] bn2_pool2_1 -> pool2_1 (in-place)
I1211 06:02:42.909339 13088 net.cpp:122] Setting up bn2_pool2_1
I1211 06:02:42.909339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.909339 13088 net.cpp:137] Memory required for data: 339150000
I1211 06:02:42.909339 13088 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 06:02:42.909339 13088 net.cpp:84] Creating Layer scale2_pool2_1
I1211 06:02:42.909339 13088 net.cpp:406] scale2_pool2_1 <- pool2_1
I1211 06:02:42.909339 13088 net.cpp:367] scale2_pool2_1 -> pool2_1 (in-place)
I1211 06:02:42.909339 13088 layer_factory.cpp:58] Creating layer scale2_pool2_1
I1211 06:02:42.909339 13088 net.cpp:122] Setting up scale2_pool2_1
I1211 06:02:42.909339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.909339 13088 net.cpp:137] Memory required for data: 344270000
I1211 06:02:42.909339 13088 layer_factory.cpp:58] Creating layer relu2_pool2_1
I1211 06:02:42.909339 13088 net.cpp:84] Creating Layer relu2_pool2_1
I1211 06:02:42.909339 13088 net.cpp:406] relu2_pool2_1 <- pool2_1
I1211 06:02:42.909339 13088 net.cpp:367] relu2_pool2_1 -> pool2_1 (in-place)
I1211 06:02:42.910341 13088 net.cpp:122] Setting up relu2_pool2_1
I1211 06:02:42.910341 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.910341 13088 net.cpp:137] Memory required for data: 349390000
I1211 06:02:42.910341 13088 layer_factory.cpp:58] Creating layer conv3
I1211 06:02:42.910341 13088 net.cpp:84] Creating Layer conv3
I1211 06:02:42.910341 13088 net.cpp:406] conv3 <- pool2_1
I1211 06:02:42.910341 13088 net.cpp:380] conv3 -> conv3
I1211 06:02:42.911339 13088 net.cpp:122] Setting up conv3
I1211 06:02:42.911339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.911339 13088 net.cpp:137] Memory required for data: 354510000
I1211 06:02:42.911339 13088 layer_factory.cpp:58] Creating layer bn3
I1211 06:02:42.911339 13088 net.cpp:84] Creating Layer bn3
I1211 06:02:42.911339 13088 net.cpp:406] bn3 <- conv3
I1211 06:02:42.911339 13088 net.cpp:367] bn3 -> conv3 (in-place)
I1211 06:02:42.911339 13088 net.cpp:122] Setting up bn3
I1211 06:02:42.911339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.911339 13088 net.cpp:137] Memory required for data: 359630000
I1211 06:02:42.911339 13088 layer_factory.cpp:58] Creating layer scale3
I1211 06:02:42.911339 13088 net.cpp:84] Creating Layer scale3
I1211 06:02:42.911339 13088 net.cpp:406] scale3 <- conv3
I1211 06:02:42.911339 13088 net.cpp:367] scale3 -> conv3 (in-place)
I1211 06:02:42.911339 13088 layer_factory.cpp:58] Creating layer scale3
I1211 06:02:42.911339 13088 net.cpp:122] Setting up scale3
I1211 06:02:42.911339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.911339 13088 net.cpp:137] Memory required for data: 364750000
I1211 06:02:42.911339 13088 layer_factory.cpp:58] Creating layer relu3
I1211 06:02:42.911339 13088 net.cpp:84] Creating Layer relu3
I1211 06:02:42.911339 13088 net.cpp:406] relu3 <- conv3
I1211 06:02:42.911339 13088 net.cpp:367] relu3 -> conv3 (in-place)
I1211 06:02:42.912351 13088 net.cpp:122] Setting up relu3
I1211 06:02:42.912351 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.912351 13088 net.cpp:137] Memory required for data: 369870000
I1211 06:02:42.912351 13088 layer_factory.cpp:58] Creating layer conv3_1
I1211 06:02:42.912351 13088 net.cpp:84] Creating Layer conv3_1
I1211 06:02:42.912351 13088 net.cpp:406] conv3_1 <- conv3
I1211 06:02:42.912351 13088 net.cpp:380] conv3_1 -> conv3_1
I1211 06:02:42.913339 13088 net.cpp:122] Setting up conv3_1
I1211 06:02:42.913339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.913339 13088 net.cpp:137] Memory required for data: 374990000
I1211 06:02:42.913339 13088 layer_factory.cpp:58] Creating layer bn3_1
I1211 06:02:42.913339 13088 net.cpp:84] Creating Layer bn3_1
I1211 06:02:42.913339 13088 net.cpp:406] bn3_1 <- conv3_1
I1211 06:02:42.913339 13088 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 06:02:42.913339 13088 net.cpp:122] Setting up bn3_1
I1211 06:02:42.913339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.913339 13088 net.cpp:137] Memory required for data: 380110000
I1211 06:02:42.913339 13088 layer_factory.cpp:58] Creating layer scale3_1
I1211 06:02:42.913339 13088 net.cpp:84] Creating Layer scale3_1
I1211 06:02:42.913339 13088 net.cpp:406] scale3_1 <- conv3_1
I1211 06:02:42.913339 13088 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 06:02:42.914336 13088 layer_factory.cpp:58] Creating layer scale3_1
I1211 06:02:42.914336 13088 net.cpp:122] Setting up scale3_1
I1211 06:02:42.914336 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.914336 13088 net.cpp:137] Memory required for data: 385230000
I1211 06:02:42.914336 13088 layer_factory.cpp:58] Creating layer relu3_1
I1211 06:02:42.914336 13088 net.cpp:84] Creating Layer relu3_1
I1211 06:02:42.914336 13088 net.cpp:406] relu3_1 <- conv3_1
I1211 06:02:42.914336 13088 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 06:02:42.914336 13088 net.cpp:122] Setting up relu3_1
I1211 06:02:42.914336 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.914336 13088 net.cpp:137] Memory required for data: 390350000
I1211 06:02:42.914336 13088 layer_factory.cpp:58] Creating layer conv4
I1211 06:02:42.914336 13088 net.cpp:84] Creating Layer conv4
I1211 06:02:42.914336 13088 net.cpp:406] conv4 <- conv3_1
I1211 06:02:42.914336 13088 net.cpp:380] conv4 -> conv4
I1211 06:02:42.916337 13088 net.cpp:122] Setting up conv4
I1211 06:02:42.916337 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.916337 13088 net.cpp:137] Memory required for data: 395470000
I1211 06:02:42.916337 13088 layer_factory.cpp:58] Creating layer bn4
I1211 06:02:42.916337 13088 net.cpp:84] Creating Layer bn4
I1211 06:02:42.916337 13088 net.cpp:406] bn4 <- conv4
I1211 06:02:42.916337 13088 net.cpp:367] bn4 -> conv4 (in-place)
I1211 06:02:42.916337 13088 net.cpp:122] Setting up bn4
I1211 06:02:42.916337 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.916337 13088 net.cpp:137] Memory required for data: 400590000
I1211 06:02:42.916337 13088 layer_factory.cpp:58] Creating layer scale4
I1211 06:02:42.916337 13088 net.cpp:84] Creating Layer scale4
I1211 06:02:42.916337 13088 net.cpp:406] scale4 <- conv4
I1211 06:02:42.916337 13088 net.cpp:367] scale4 -> conv4 (in-place)
I1211 06:02:42.916337 13088 layer_factory.cpp:58] Creating layer scale4
I1211 06:02:42.916337 13088 net.cpp:122] Setting up scale4
I1211 06:02:42.916337 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.916337 13088 net.cpp:137] Memory required for data: 405710000
I1211 06:02:42.916337 13088 layer_factory.cpp:58] Creating layer relu4
I1211 06:02:42.916337 13088 net.cpp:84] Creating Layer relu4
I1211 06:02:42.916337 13088 net.cpp:406] relu4 <- conv4
I1211 06:02:42.916337 13088 net.cpp:367] relu4 -> conv4 (in-place)
I1211 06:02:42.916337 13088 net.cpp:122] Setting up relu4
I1211 06:02:42.917340 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.917340 13088 net.cpp:137] Memory required for data: 410830000
I1211 06:02:42.917340 13088 layer_factory.cpp:58] Creating layer conv4_1
I1211 06:02:42.917340 13088 net.cpp:84] Creating Layer conv4_1
I1211 06:02:42.917340 13088 net.cpp:406] conv4_1 <- conv4
I1211 06:02:42.917340 13088 net.cpp:380] conv4_1 -> conv4_1
I1211 06:02:42.918339 13088 net.cpp:122] Setting up conv4_1
I1211 06:02:42.918339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.918339 13088 net.cpp:137] Memory required for data: 415950000
I1211 06:02:42.918339 13088 layer_factory.cpp:58] Creating layer bn4_1
I1211 06:02:42.918339 13088 net.cpp:84] Creating Layer bn4_1
I1211 06:02:42.918339 13088 net.cpp:406] bn4_1 <- conv4_1
I1211 06:02:42.918339 13088 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 06:02:42.918339 13088 net.cpp:122] Setting up bn4_1
I1211 06:02:42.918339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.918339 13088 net.cpp:137] Memory required for data: 421070000
I1211 06:02:42.918339 13088 layer_factory.cpp:58] Creating layer scale4_1
I1211 06:02:42.918339 13088 net.cpp:84] Creating Layer scale4_1
I1211 06:02:42.918339 13088 net.cpp:406] scale4_1 <- conv4_1
I1211 06:02:42.918339 13088 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 06:02:42.918339 13088 layer_factory.cpp:58] Creating layer scale4_1
I1211 06:02:42.918339 13088 net.cpp:122] Setting up scale4_1
I1211 06:02:42.918339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.918339 13088 net.cpp:137] Memory required for data: 426190000
I1211 06:02:42.918339 13088 layer_factory.cpp:58] Creating layer relu4_1
I1211 06:02:42.918339 13088 net.cpp:84] Creating Layer relu4_1
I1211 06:02:42.918339 13088 net.cpp:406] relu4_1 <- conv4_1
I1211 06:02:42.918339 13088 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 06:02:42.918339 13088 net.cpp:122] Setting up relu4_1
I1211 06:02:42.918339 13088 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 06:02:42.918339 13088 net.cpp:137] Memory required for data: 431310000
I1211 06:02:42.918339 13088 layer_factory.cpp:58] Creating layer conv4_2
I1211 06:02:42.918339 13088 net.cpp:84] Creating Layer conv4_2
I1211 06:02:42.918339 13088 net.cpp:406] conv4_2 <- conv4_1
I1211 06:02:42.918339 13088 net.cpp:380] conv4_2 -> conv4_2
I1211 06:02:42.920339 13088 net.cpp:122] Setting up conv4_2
I1211 06:02:42.920339 13088 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:02:42.920339 13088 net.cpp:137] Memory required for data: 437249200
I1211 06:02:42.920339 13088 layer_factory.cpp:58] Creating layer bn4_2
I1211 06:02:42.920339 13088 net.cpp:84] Creating Layer bn4_2
I1211 06:02:42.920339 13088 net.cpp:406] bn4_2 <- conv4_2
I1211 06:02:42.920339 13088 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 06:02:42.920339 13088 net.cpp:122] Setting up bn4_2
I1211 06:02:42.920339 13088 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:02:42.920339 13088 net.cpp:137] Memory required for data: 443188400
I1211 06:02:42.920339 13088 layer_factory.cpp:58] Creating layer scale4_2
I1211 06:02:42.920339 13088 net.cpp:84] Creating Layer scale4_2
I1211 06:02:42.920339 13088 net.cpp:406] scale4_2 <- conv4_2
I1211 06:02:42.920339 13088 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 06:02:42.920339 13088 layer_factory.cpp:58] Creating layer scale4_2
I1211 06:02:42.920339 13088 net.cpp:122] Setting up scale4_2
I1211 06:02:42.920339 13088 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:02:42.920339 13088 net.cpp:137] Memory required for data: 449127600
I1211 06:02:42.920339 13088 layer_factory.cpp:58] Creating layer relu4_2
I1211 06:02:42.920339 13088 net.cpp:84] Creating Layer relu4_2
I1211 06:02:42.921339 13088 net.cpp:406] relu4_2 <- conv4_2
I1211 06:02:42.921339 13088 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 06:02:42.921339 13088 net.cpp:122] Setting up relu4_2
I1211 06:02:42.921339 13088 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 06:02:42.921339 13088 net.cpp:137] Memory required for data: 455066800
I1211 06:02:42.921339 13088 layer_factory.cpp:58] Creating layer pool4_2
I1211 06:02:42.921339 13088 net.cpp:84] Creating Layer pool4_2
I1211 06:02:42.921339 13088 net.cpp:406] pool4_2 <- conv4_2
I1211 06:02:42.921339 13088 net.cpp:380] pool4_2 -> pool4_2
I1211 06:02:42.922348 13088 net.cpp:122] Setting up pool4_2
I1211 06:02:42.922348 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.922847 13088 net.cpp:137] Memory required for data: 456551600
I1211 06:02:42.922847 13088 layer_factory.cpp:58] Creating layer bn4_pool4_2
I1211 06:02:42.922847 13088 net.cpp:84] Creating Layer bn4_pool4_2
I1211 06:02:42.922847 13088 net.cpp:406] bn4_pool4_2 <- pool4_2
I1211 06:02:42.922847 13088 net.cpp:367] bn4_pool4_2 -> pool4_2 (in-place)
I1211 06:02:42.922847 13088 net.cpp:122] Setting up bn4_pool4_2
I1211 06:02:42.922847 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.922847 13088 net.cpp:137] Memory required for data: 458036400
I1211 06:02:42.922847 13088 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 06:02:42.922847 13088 net.cpp:84] Creating Layer scale4_pool4_2
I1211 06:02:42.922847 13088 net.cpp:406] scale4_pool4_2 <- pool4_2
I1211 06:02:42.922847 13088 net.cpp:367] scale4_pool4_2 -> pool4_2 (in-place)
I1211 06:02:42.922847 13088 layer_factory.cpp:58] Creating layer scale4_pool4_2
I1211 06:02:42.922847 13088 net.cpp:122] Setting up scale4_pool4_2
I1211 06:02:42.922847 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.922847 13088 net.cpp:137] Memory required for data: 459521200
I1211 06:02:42.922847 13088 layer_factory.cpp:58] Creating layer relu4_pool4_2
I1211 06:02:42.922847 13088 net.cpp:84] Creating Layer relu4_pool4_2
I1211 06:02:42.922847 13088 net.cpp:406] relu4_pool4_2 <- pool4_2
I1211 06:02:42.922847 13088 net.cpp:367] relu4_pool4_2 -> pool4_2 (in-place)
I1211 06:02:42.923347 13088 net.cpp:122] Setting up relu4_pool4_2
I1211 06:02:42.923347 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.923347 13088 net.cpp:137] Memory required for data: 461006000
I1211 06:02:42.923347 13088 layer_factory.cpp:58] Creating layer conv4_0
I1211 06:02:42.923347 13088 net.cpp:84] Creating Layer conv4_0
I1211 06:02:42.923347 13088 net.cpp:406] conv4_0 <- pool4_2
I1211 06:02:42.923347 13088 net.cpp:380] conv4_0 -> conv4_0
I1211 06:02:42.925348 13088 net.cpp:122] Setting up conv4_0
I1211 06:02:42.925348 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.925348 13088 net.cpp:137] Memory required for data: 462490800
I1211 06:02:42.925348 13088 layer_factory.cpp:58] Creating layer bn4_0
I1211 06:02:42.925348 13088 net.cpp:84] Creating Layer bn4_0
I1211 06:02:42.925348 13088 net.cpp:406] bn4_0 <- conv4_0
I1211 06:02:42.925348 13088 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 06:02:42.925348 13088 net.cpp:122] Setting up bn4_0
I1211 06:02:42.925348 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.925348 13088 net.cpp:137] Memory required for data: 463975600
I1211 06:02:42.925348 13088 layer_factory.cpp:58] Creating layer scale4_0
I1211 06:02:42.925348 13088 net.cpp:84] Creating Layer scale4_0
I1211 06:02:42.925348 13088 net.cpp:406] scale4_0 <- conv4_0
I1211 06:02:42.925348 13088 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 06:02:42.925348 13088 layer_factory.cpp:58] Creating layer scale4_0
I1211 06:02:42.925848 13088 net.cpp:122] Setting up scale4_0
I1211 06:02:42.925848 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.925848 13088 net.cpp:137] Memory required for data: 465460400
I1211 06:02:42.925848 13088 layer_factory.cpp:58] Creating layer relu4_0
I1211 06:02:42.925848 13088 net.cpp:84] Creating Layer relu4_0
I1211 06:02:42.925848 13088 net.cpp:406] relu4_0 <- conv4_0
I1211 06:02:42.925848 13088 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 06:02:42.926348 13088 net.cpp:122] Setting up relu4_0
I1211 06:02:42.926348 13088 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 06:02:42.926348 13088 net.cpp:137] Memory required for data: 466945200
I1211 06:02:42.926348 13088 layer_factory.cpp:58] Creating layer conv11
I1211 06:02:42.926348 13088 net.cpp:84] Creating Layer conv11
I1211 06:02:42.926348 13088 net.cpp:406] conv11 <- conv4_0
I1211 06:02:42.926348 13088 net.cpp:380] conv11 -> conv11
I1211 06:02:42.927847 13088 net.cpp:122] Setting up conv11
I1211 06:02:42.927847 13088 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:02:42.927847 13088 net.cpp:137] Memory required for data: 468737200
I1211 06:02:42.927847 13088 layer_factory.cpp:58] Creating layer bn_conv11
I1211 06:02:42.927847 13088 net.cpp:84] Creating Layer bn_conv11
I1211 06:02:42.927847 13088 net.cpp:406] bn_conv11 <- conv11
I1211 06:02:42.927847 13088 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 06:02:42.927847 13088 net.cpp:122] Setting up bn_conv11
I1211 06:02:42.927847 13088 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:02:42.927847 13088 net.cpp:137] Memory required for data: 470529200
I1211 06:02:42.927847 13088 layer_factory.cpp:58] Creating layer scale_conv11
I1211 06:02:42.927847 13088 net.cpp:84] Creating Layer scale_conv11
I1211 06:02:42.927847 13088 net.cpp:406] scale_conv11 <- conv11
I1211 06:02:42.927847 13088 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 06:02:42.927847 13088 layer_factory.cpp:58] Creating layer scale_conv11
I1211 06:02:42.928352 13088 net.cpp:122] Setting up scale_conv11
I1211 06:02:42.928352 13088 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:02:42.928352 13088 net.cpp:137] Memory required for data: 472321200
I1211 06:02:42.928352 13088 layer_factory.cpp:58] Creating layer relu_conv11
I1211 06:02:42.928352 13088 net.cpp:84] Creating Layer relu_conv11
I1211 06:02:42.928352 13088 net.cpp:406] relu_conv11 <- conv11
I1211 06:02:42.928352 13088 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 06:02:42.928352 13088 net.cpp:122] Setting up relu_conv11
I1211 06:02:42.928352 13088 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 06:02:42.928352 13088 net.cpp:137] Memory required for data: 474113200
I1211 06:02:42.928352 13088 layer_factory.cpp:58] Creating layer conv12
I1211 06:02:42.928352 13088 net.cpp:84] Creating Layer conv12
I1211 06:02:42.928352 13088 net.cpp:406] conv12 <- conv11
I1211 06:02:42.928352 13088 net.cpp:380] conv12 -> conv12
I1211 06:02:42.929847 13088 net.cpp:122] Setting up conv12
I1211 06:02:42.929847 13088 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:02:42.929847 13088 net.cpp:137] Memory required for data: 476417200
I1211 06:02:42.929847 13088 layer_factory.cpp:58] Creating layer bn_conv12
I1211 06:02:42.930348 13088 net.cpp:84] Creating Layer bn_conv12
I1211 06:02:42.930348 13088 net.cpp:406] bn_conv12 <- conv12
I1211 06:02:42.930348 13088 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 06:02:42.930348 13088 net.cpp:122] Setting up bn_conv12
I1211 06:02:42.930348 13088 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:02:42.930348 13088 net.cpp:137] Memory required for data: 478721200
I1211 06:02:42.930348 13088 layer_factory.cpp:58] Creating layer scale_conv12
I1211 06:02:42.930348 13088 net.cpp:84] Creating Layer scale_conv12
I1211 06:02:42.930348 13088 net.cpp:406] scale_conv12 <- conv12
I1211 06:02:42.930348 13088 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 06:02:42.930348 13088 layer_factory.cpp:58] Creating layer scale_conv12
I1211 06:02:42.930348 13088 net.cpp:122] Setting up scale_conv12
I1211 06:02:42.930348 13088 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:02:42.930348 13088 net.cpp:137] Memory required for data: 481025200
I1211 06:02:42.930348 13088 layer_factory.cpp:58] Creating layer relu_conv12
I1211 06:02:42.930348 13088 net.cpp:84] Creating Layer relu_conv12
I1211 06:02:42.930848 13088 net.cpp:406] relu_conv12 <- conv12
I1211 06:02:42.930848 13088 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 06:02:42.930848 13088 net.cpp:122] Setting up relu_conv12
I1211 06:02:42.930848 13088 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 06:02:42.930848 13088 net.cpp:137] Memory required for data: 483329200
I1211 06:02:42.930848 13088 layer_factory.cpp:58] Creating layer poolcp6
I1211 06:02:42.930848 13088 net.cpp:84] Creating Layer poolcp6
I1211 06:02:42.930848 13088 net.cpp:406] poolcp6 <- conv12
I1211 06:02:42.930848 13088 net.cpp:380] poolcp6 -> poolcp6
I1211 06:02:42.930848 13088 net.cpp:122] Setting up poolcp6
I1211 06:02:42.930848 13088 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 06:02:42.930848 13088 net.cpp:137] Memory required for data: 483365200
I1211 06:02:42.930848 13088 layer_factory.cpp:58] Creating layer ip1
I1211 06:02:42.930848 13088 net.cpp:84] Creating Layer ip1
I1211 06:02:42.930848 13088 net.cpp:406] ip1 <- poolcp6
I1211 06:02:42.930848 13088 net.cpp:380] ip1 -> ip1
I1211 06:02:42.930848 13088 net.cpp:122] Setting up ip1
I1211 06:02:42.931349 13088 net.cpp:129] Top shape: 100 100 (10000)
I1211 06:02:42.931349 13088 net.cpp:137] Memory required for data: 483405200
I1211 06:02:42.931349 13088 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 06:02:42.931349 13088 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 06:02:42.931349 13088 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 06:02:42.931349 13088 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 06:02:42.931349 13088 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 06:02:42.931349 13088 net.cpp:122] Setting up ip1_ip1_0_split
I1211 06:02:42.931349 13088 net.cpp:129] Top shape: 100 100 (10000)
I1211 06:02:42.931349 13088 net.cpp:129] Top shape: 100 100 (10000)
I1211 06:02:42.931349 13088 net.cpp:137] Memory required for data: 483485200
I1211 06:02:42.931349 13088 layer_factory.cpp:58] Creating layer accuracy
I1211 06:02:42.931349 13088 net.cpp:84] Creating Layer accuracy
I1211 06:02:42.931349 13088 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1211 06:02:42.931349 13088 net.cpp:406] accuracy <- label_cifar_1_split_0
I1211 06:02:42.931349 13088 net.cpp:380] accuracy -> accuracy
I1211 06:02:42.931349 13088 net.cpp:122] Setting up accuracy
I1211 06:02:42.931349 13088 net.cpp:129] Top shape: (1)
I1211 06:02:42.931349 13088 net.cpp:137] Memory required for data: 483485204
I1211 06:02:42.931349 13088 layer_factory.cpp:58] Creating layer loss
I1211 06:02:42.931349 13088 net.cpp:84] Creating Layer loss
I1211 06:02:42.931349 13088 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 06:02:42.931349 13088 net.cpp:406] loss <- label_cifar_1_split_1
I1211 06:02:42.931349 13088 net.cpp:380] loss -> loss
I1211 06:02:42.931349 13088 layer_factory.cpp:58] Creating layer loss
I1211 06:02:42.931849 13088 net.cpp:122] Setting up loss
I1211 06:02:42.931849 13088 net.cpp:129] Top shape: (1)
I1211 06:02:42.931849 13088 net.cpp:132]     with loss weight 1
I1211 06:02:42.931849 13088 net.cpp:137] Memory required for data: 483485208
I1211 06:02:42.931849 13088 net.cpp:198] loss needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:200] accuracy does not need backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] ip1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] poolcp6 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] relu_conv12 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] scale_conv12 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] bn_conv12 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] conv12 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] relu_conv11 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] scale_conv11 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] bn_conv11 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] conv11 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] relu4_0 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] scale4_0 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] bn4_0 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] conv4_0 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] relu4_pool4_2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] scale4_pool4_2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] bn4_pool4_2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] pool4_2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] relu4_2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] scale4_2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] bn4_2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] conv4_2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] relu4_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] scale4_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] bn4_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] conv4_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] relu4 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] scale4 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] bn4 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] conv4 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] relu3_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] scale3_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] bn3_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] conv3_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] relu3 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] scale3 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] bn3 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] conv3 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] relu2_pool2_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] scale2_pool2_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] bn2_pool2_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] pool2_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] relu2_2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] scale2_2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] bn2_2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] conv2_2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] relu2_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] scale2_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] bn2_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] conv2_1 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] relu2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] scale2 needs backward computation.
I1211 06:02:42.931849 13088 net.cpp:198] bn2 needs backward computation.
I1211 06:02:42.932348 13088 net.cpp:198] conv2 needs backward computation.
I1211 06:02:42.932348 13088 net.cpp:198] relu1_0 needs backward computation.
I1211 06:02:42.932348 13088 net.cpp:198] scale1_0 needs backward computation.
I1211 06:02:42.932348 13088 net.cpp:198] bn1_0 needs backward computation.
I1211 06:02:42.932348 13088 net.cpp:198] conv1_0 needs backward computation.
I1211 06:02:42.932348 13088 net.cpp:198] relu1 needs backward computation.
I1211 06:02:42.932348 13088 net.cpp:198] scale1 needs backward computation.
I1211 06:02:42.932348 13088 net.cpp:198] bn1 needs backward computation.
I1211 06:02:42.932348 13088 net.cpp:198] conv1 needs backward computation.
I1211 06:02:42.932348 13088 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 06:02:42.932348 13088 net.cpp:200] cifar does not need backward computation.
I1211 06:02:42.932348 13088 net.cpp:242] This network produces output accuracy
I1211 06:02:42.932348 13088 net.cpp:242] This network produces output loss
I1211 06:02:42.932348 13088 net.cpp:255] Network initialization done.
I1211 06:02:42.932348 13088 solver.cpp:56] Solver scaffolding done.
I1211 06:02:42.937350 13088 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90000.solverstate
I1211 06:02:42.940352 13088 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90000.caffemodel
I1211 06:02:42.940352 13088 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 06:02:42.940352 13088 sgd_solver.cpp:318] SGDSolver: restoring history
I1211 06:02:42.945358 13088 caffe.cpp:249] Starting Optimization
I1211 06:02:42.945358 13088 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_stridedConvV2_WnonLin_360k
I1211 06:02:42.945358 13088 solver.cpp:273] Learning Rate Policy: multistep
I1211 06:02:42.947352 13088 solver.cpp:330] Iteration 90000, Testing net (#0)
I1211 06:02:42.949352 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:02:44.341594 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:02:44.393594 13088 solver.cpp:397]     Test net output #0: accuracy = 0.5975
I1211 06:02:44.393594 13088 solver.cpp:397]     Test net output #1: loss = 1.57719 (* 1 = 1.57719 loss)
I1211 06:02:44.504276 13088 solver.cpp:218] Iteration 90000 (57761.2 iter/s, 1.55814s/100 iters), loss = 0.713892
I1211 06:02:44.504276 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:02:44.504276 13088 solver.cpp:237]     Train net output #1: loss = 0.713892 (* 1 = 0.713892 loss)
I1211 06:02:44.504276 13088 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1211 06:02:50.635906 13088 solver.cpp:218] Iteration 90100 (16.3104 iter/s, 6.13105s/100 iters), loss = 0.547555
I1211 06:02:50.635906 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 06:02:50.635906 13088 solver.cpp:237]     Train net output #1: loss = 0.547555 (* 1 = 0.547555 loss)
I1211 06:02:50.635906 13088 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1211 06:02:56.755343 13088 solver.cpp:218] Iteration 90200 (16.3416 iter/s, 6.11937s/100 iters), loss = 0.564149
I1211 06:02:56.755343 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:02:56.755343 13088 solver.cpp:237]     Train net output #1: loss = 0.564149 (* 1 = 0.564149 loss)
I1211 06:02:56.755343 13088 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1211 06:03:02.921252 13088 solver.cpp:218] Iteration 90300 (16.2195 iter/s, 6.16543s/100 iters), loss = 0.702624
I1211 06:03:02.921252 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 06:03:02.921252 13088 solver.cpp:237]     Train net output #1: loss = 0.702624 (* 1 = 0.702624 loss)
I1211 06:03:02.921252 13088 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1211 06:03:09.032987 13088 solver.cpp:218] Iteration 90400 (16.3645 iter/s, 6.1108s/100 iters), loss = 0.682333
I1211 06:03:09.032987 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:03:09.032987 13088 solver.cpp:237]     Train net output #1: loss = 0.682333 (* 1 = 0.682333 loss)
I1211 06:03:09.032987 13088 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1211 06:03:14.859630 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:03:15.102659 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90500.caffemodel
I1211 06:03:15.117660 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_90500.solverstate
I1211 06:03:15.122660 13088 solver.cpp:330] Iteration 90500, Testing net (#0)
I1211 06:03:15.122660 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:03:16.458832 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:03:16.511842 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6081
I1211 06:03:16.511842 13088 solver.cpp:397]     Test net output #1: loss = 1.48806 (* 1 = 1.48806 loss)
I1211 06:03:16.569838 13088 solver.cpp:218] Iteration 90500 (13.2679 iter/s, 7.537s/100 iters), loss = 0.617438
I1211 06:03:16.569838 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:03:16.569838 13088 solver.cpp:237]     Train net output #1: loss = 0.617438 (* 1 = 0.617438 loss)
I1211 06:03:16.569838 13088 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1211 06:03:22.711539 13088 solver.cpp:218] Iteration 90600 (16.2831 iter/s, 6.14134s/100 iters), loss = 0.709241
I1211 06:03:22.711539 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 06:03:22.711539 13088 solver.cpp:237]     Train net output #1: loss = 0.709241 (* 1 = 0.709241 loss)
I1211 06:03:22.712539 13088 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1211 06:03:28.858273 13088 solver.cpp:218] Iteration 90700 (16.2721 iter/s, 6.14548s/100 iters), loss = 0.599614
I1211 06:03:28.858273 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 06:03:28.858273 13088 solver.cpp:237]     Train net output #1: loss = 0.599614 (* 1 = 0.599614 loss)
I1211 06:03:28.858273 13088 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1211 06:03:35.005971 13088 solver.cpp:218] Iteration 90800 (16.2655 iter/s, 6.14798s/100 iters), loss = 0.72909
I1211 06:03:35.005971 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 06:03:35.005971 13088 solver.cpp:237]     Train net output #1: loss = 0.72909 (* 1 = 0.72909 loss)
I1211 06:03:35.005971 13088 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1211 06:03:41.159587 13088 solver.cpp:218] Iteration 90900 (16.2516 iter/s, 6.15322s/100 iters), loss = 0.793316
I1211 06:03:41.159587 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 06:03:41.159587 13088 solver.cpp:237]     Train net output #1: loss = 0.793316 (* 1 = 0.793316 loss)
I1211 06:03:41.159587 13088 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1211 06:03:47.011757 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:03:47.253772 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91000.caffemodel
I1211 06:03:47.268772 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91000.solverstate
I1211 06:03:47.273772 13088 solver.cpp:330] Iteration 91000, Testing net (#0)
I1211 06:03:47.273772 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:03:48.609907 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:03:48.662915 13088 solver.cpp:397]     Test net output #0: accuracy = 0.5569
I1211 06:03:48.662915 13088 solver.cpp:397]     Test net output #1: loss = 1.75319 (* 1 = 1.75319 loss)
I1211 06:03:48.720912 13088 solver.cpp:218] Iteration 91000 (13.2266 iter/s, 7.56051s/100 iters), loss = 0.737421
I1211 06:03:48.720912 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 06:03:48.720912 13088 solver.cpp:237]     Train net output #1: loss = 0.737421 (* 1 = 0.737421 loss)
I1211 06:03:48.720912 13088 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1211 06:03:54.868362 13088 solver.cpp:218] Iteration 91100 (16.2679 iter/s, 6.14709s/100 iters), loss = 0.641397
I1211 06:03:54.868362 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:03:54.868362 13088 solver.cpp:237]     Train net output #1: loss = 0.641397 (* 1 = 0.641397 loss)
I1211 06:03:54.868362 13088 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1211 06:04:01.018899 13088 solver.cpp:218] Iteration 91200 (16.2588 iter/s, 6.15052s/100 iters), loss = 0.648533
I1211 06:04:01.019901 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 06:04:01.019901 13088 solver.cpp:237]     Train net output #1: loss = 0.648533 (* 1 = 0.648533 loss)
I1211 06:04:01.019901 13088 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1211 06:04:07.224422 13088 solver.cpp:218] Iteration 91300 (16.1166 iter/s, 6.20477s/100 iters), loss = 0.65275
I1211 06:04:07.224422 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 06:04:07.224422 13088 solver.cpp:237]     Train net output #1: loss = 0.65275 (* 1 = 0.65275 loss)
I1211 06:04:07.224422 13088 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1211 06:04:13.395915 13088 solver.cpp:218] Iteration 91400 (16.2063 iter/s, 6.17043s/100 iters), loss = 0.732788
I1211 06:04:13.395915 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 06:04:13.395915 13088 solver.cpp:237]     Train net output #1: loss = 0.732788 (* 1 = 0.732788 loss)
I1211 06:04:13.395915 13088 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1211 06:04:19.268430 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:04:19.510457 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91500.caffemodel
I1211 06:04:19.527456 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_91500.solverstate
I1211 06:04:19.533457 13088 solver.cpp:330] Iteration 91500, Testing net (#0)
I1211 06:04:19.533457 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:04:20.884562 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:04:20.936559 13088 solver.cpp:397]     Test net output #0: accuracy = 0.5769
I1211 06:04:20.936559 13088 solver.cpp:397]     Test net output #1: loss = 1.67953 (* 1 = 1.67953 loss)
I1211 06:04:20.995566 13088 solver.cpp:218] Iteration 91500 (13.158 iter/s, 7.59991s/100 iters), loss = 0.638427
I1211 06:04:20.995566 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:04:20.995566 13088 solver.cpp:237]     Train net output #1: loss = 0.638427 (* 1 = 0.638427 loss)
I1211 06:04:20.995566 13088 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1211 06:04:27.154073 13088 solver.cpp:218] Iteration 91600 (16.2407 iter/s, 6.15736s/100 iters), loss = 0.681055
I1211 06:04:27.154073 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:04:27.154073 13088 solver.cpp:237]     Train net output #1: loss = 0.681055 (* 1 = 0.681055 loss)
I1211 06:04:27.154073 13088 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1211 06:04:33.499096 13088 solver.cpp:218] Iteration 91700 (15.7619 iter/s, 6.34443s/100 iters), loss = 0.518312
I1211 06:04:33.499096 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 06:04:33.499096 13088 solver.cpp:237]     Train net output #1: loss = 0.518312 (* 1 = 0.518312 loss)
I1211 06:04:33.499096 13088 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1211 06:04:39.772830 13088 solver.cpp:218] Iteration 91800 (15.9402 iter/s, 6.27346s/100 iters), loss = 0.640074
I1211 06:04:39.772830 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:04:39.772830 13088 solver.cpp:237]     Train net output #1: loss = 0.640074 (* 1 = 0.640074 loss)
I1211 06:04:39.772830 13088 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1211 06:04:45.921294 13088 solver.cpp:218] Iteration 91900 (16.2654 iter/s, 6.14803s/100 iters), loss = 0.653222
I1211 06:04:45.921294 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 06:04:45.921294 13088 solver.cpp:237]     Train net output #1: loss = 0.653222 (* 1 = 0.653222 loss)
I1211 06:04:45.921294 13088 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1211 06:04:51.837823 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:04:52.079852 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92000.caffemodel
I1211 06:04:52.095852 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92000.solverstate
I1211 06:04:52.099853 13088 solver.cpp:330] Iteration 92000, Testing net (#0)
I1211 06:04:52.100852 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:04:53.437947 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:04:53.489954 13088 solver.cpp:397]     Test net output #0: accuracy = 0.5879
I1211 06:04:53.489954 13088 solver.cpp:397]     Test net output #1: loss = 1.56493 (* 1 = 1.56493 loss)
I1211 06:04:53.548956 13088 solver.cpp:218] Iteration 92000 (13.1106 iter/s, 7.6274s/100 iters), loss = 0.5613
I1211 06:04:53.549456 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:04:53.549456 13088 solver.cpp:237]     Train net output #1: loss = 0.5613 (* 1 = 0.5613 loss)
I1211 06:04:53.549456 13088 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1211 06:04:59.820248 13088 solver.cpp:218] Iteration 92100 (15.947 iter/s, 6.27079s/100 iters), loss = 0.668019
I1211 06:04:59.820248 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 06:04:59.820248 13088 solver.cpp:237]     Train net output #1: loss = 0.668019 (* 1 = 0.668019 loss)
I1211 06:04:59.820248 13088 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1211 06:05:05.999429 13088 solver.cpp:218] Iteration 92200 (16.1835 iter/s, 6.17913s/100 iters), loss = 0.567841
I1211 06:05:06.000429 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:05:06.000429 13088 solver.cpp:237]     Train net output #1: loss = 0.567841 (* 1 = 0.567841 loss)
I1211 06:05:06.000429 13088 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1211 06:05:12.158784 13088 solver.cpp:218] Iteration 92300 (16.239 iter/s, 6.15803s/100 iters), loss = 0.687097
I1211 06:05:12.158784 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:05:12.158784 13088 solver.cpp:237]     Train net output #1: loss = 0.687097 (* 1 = 0.687097 loss)
I1211 06:05:12.158784 13088 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1211 06:05:18.371204 13088 solver.cpp:218] Iteration 92400 (16.0971 iter/s, 6.21231s/100 iters), loss = 0.672792
I1211 06:05:18.371204 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 06:05:18.371204 13088 solver.cpp:237]     Train net output #1: loss = 0.672792 (* 1 = 0.672792 loss)
I1211 06:05:18.371204 13088 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1211 06:05:24.248613 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:05:24.489647 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92500.caffemodel
I1211 06:05:24.505648 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_92500.solverstate
I1211 06:05:24.510648 13088 solver.cpp:330] Iteration 92500, Testing net (#0)
I1211 06:05:24.510648 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:05:25.861785 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:05:25.914793 13088 solver.cpp:397]     Test net output #0: accuracy = 0.5904
I1211 06:05:25.914793 13088 solver.cpp:397]     Test net output #1: loss = 1.59852 (* 1 = 1.59852 loss)
I1211 06:05:25.973794 13088 solver.cpp:218] Iteration 92500 (13.1543 iter/s, 7.60206s/100 iters), loss = 0.577679
I1211 06:05:25.973794 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 06:05:25.973794 13088 solver.cpp:237]     Train net output #1: loss = 0.577679 (* 1 = 0.577679 loss)
I1211 06:05:25.973794 13088 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1211 06:05:32.178808 13088 solver.cpp:218] Iteration 92600 (16.1167 iter/s, 6.20475s/100 iters), loss = 0.5857
I1211 06:05:32.179308 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:05:32.179308 13088 solver.cpp:237]     Train net output #1: loss = 0.5857 (* 1 = 0.5857 loss)
I1211 06:05:32.179308 13088 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1211 06:05:38.347781 13088 solver.cpp:218] Iteration 92700 (16.2124 iter/s, 6.16813s/100 iters), loss = 0.653716
I1211 06:05:38.347781 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 06:05:38.347781 13088 solver.cpp:237]     Train net output #1: loss = 0.653716 (* 1 = 0.653716 loss)
I1211 06:05:38.347781 13088 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1211 06:05:44.563133 13088 solver.cpp:218] Iteration 92800 (16.0888 iter/s, 6.2155s/100 iters), loss = 0.666445
I1211 06:05:44.563133 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 06:05:44.563133 13088 solver.cpp:237]     Train net output #1: loss = 0.666445 (* 1 = 0.666445 loss)
I1211 06:05:44.563133 13088 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1211 06:05:50.782141 13088 solver.cpp:218] Iteration 92900 (16.0819 iter/s, 6.21817s/100 iters), loss = 0.654401
I1211 06:05:50.782141 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 06:05:50.782141 13088 solver.cpp:237]     Train net output #1: loss = 0.654401 (* 1 = 0.654401 loss)
I1211 06:05:50.782141 13088 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1211 06:05:56.633172 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:05:56.875736 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93000.caffemodel
I1211 06:05:56.890266 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93000.solverstate
I1211 06:05:56.895265 13088 solver.cpp:330] Iteration 93000, Testing net (#0)
I1211 06:05:56.896265 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:05:58.232175 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:05:58.285185 13088 solver.cpp:397]     Test net output #0: accuracy = 0.5917
I1211 06:05:58.285185 13088 solver.cpp:397]     Test net output #1: loss = 1.64472 (* 1 = 1.64472 loss)
I1211 06:05:58.343248 13088 solver.cpp:218] Iteration 93000 (13.2262 iter/s, 7.56073s/100 iters), loss = 0.664144
I1211 06:05:58.343248 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:05:58.343248 13088 solver.cpp:237]     Train net output #1: loss = 0.664144 (* 1 = 0.664144 loss)
I1211 06:05:58.343248 13088 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1211 06:06:04.519220 13088 solver.cpp:218] Iteration 93100 (16.1935 iter/s, 6.17533s/100 iters), loss = 0.696401
I1211 06:06:04.519220 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 06:06:04.519220 13088 solver.cpp:237]     Train net output #1: loss = 0.696401 (* 1 = 0.696401 loss)
I1211 06:06:04.519220 13088 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1211 06:06:10.849972 13088 solver.cpp:218] Iteration 93200 (15.7966 iter/s, 6.33046s/100 iters), loss = 0.680404
I1211 06:06:10.849972 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 06:06:10.849972 13088 solver.cpp:237]     Train net output #1: loss = 0.680404 (* 1 = 0.680404 loss)
I1211 06:06:10.849972 13088 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1211 06:06:17.094919 13088 solver.cpp:218] Iteration 93300 (16.0128 iter/s, 6.245s/100 iters), loss = 0.76476
I1211 06:06:17.094919 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 06:06:17.094919 13088 solver.cpp:237]     Train net output #1: loss = 0.76476 (* 1 = 0.76476 loss)
I1211 06:06:17.094919 13088 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1211 06:06:23.264266 13088 solver.cpp:218] Iteration 93400 (16.2119 iter/s, 6.16831s/100 iters), loss = 0.81678
I1211 06:06:23.264266 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 06:06:23.264266 13088 solver.cpp:237]     Train net output #1: loss = 0.81678 (* 1 = 0.81678 loss)
I1211 06:06:23.264266 13088 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1211 06:06:29.141422 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:06:29.383451 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93500.caffemodel
I1211 06:06:29.398457 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_93500.solverstate
I1211 06:06:29.403482 13088 solver.cpp:330] Iteration 93500, Testing net (#0)
I1211 06:06:29.403482 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:06:30.747594 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:06:30.800618 13088 solver.cpp:397]     Test net output #0: accuracy = 0.586
I1211 06:06:30.800618 13088 solver.cpp:397]     Test net output #1: loss = 1.65743 (* 1 = 1.65743 loss)
I1211 06:06:30.859604 13088 solver.cpp:218] Iteration 93500 (13.1674 iter/s, 7.59453s/100 iters), loss = 0.6115
I1211 06:06:30.859604 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 06:06:30.859604 13088 solver.cpp:237]     Train net output #1: loss = 0.6115 (* 1 = 0.6115 loss)
I1211 06:06:30.859604 13088 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1211 06:06:37.154803 13088 solver.cpp:218] Iteration 93600 (15.8847 iter/s, 6.29535s/100 iters), loss = 0.566366
I1211 06:06:37.154803 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:06:37.154803 13088 solver.cpp:237]     Train net output #1: loss = 0.566366 (* 1 = 0.566366 loss)
I1211 06:06:37.154803 13088 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1211 06:06:43.345911 13088 solver.cpp:218] Iteration 93700 (16.1549 iter/s, 6.19007s/100 iters), loss = 0.49712
I1211 06:06:43.345911 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:06:43.345911 13088 solver.cpp:237]     Train net output #1: loss = 0.49712 (* 1 = 0.49712 loss)
I1211 06:06:43.345911 13088 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1211 06:06:49.710714 13088 solver.cpp:218] Iteration 93800 (15.7114 iter/s, 6.3648s/100 iters), loss = 0.651001
I1211 06:06:49.710714 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:06:49.710714 13088 solver.cpp:237]     Train net output #1: loss = 0.651001 (* 1 = 0.651001 loss)
I1211 06:06:49.710714 13088 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1211 06:06:56.045318 13088 solver.cpp:218] Iteration 93900 (15.7884 iter/s, 6.33377s/100 iters), loss = 0.785993
I1211 06:06:56.045318 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 06:06:56.045318 13088 solver.cpp:237]     Train net output #1: loss = 0.785993 (* 1 = 0.785993 loss)
I1211 06:06:56.045318 13088 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1211 06:07:01.976791 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:07:02.228821 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94000.caffemodel
I1211 06:07:02.244824 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94000.solverstate
I1211 06:07:02.249821 13088 solver.cpp:330] Iteration 94000, Testing net (#0)
I1211 06:07:02.249821 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:07:03.603284 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:07:03.656291 13088 solver.cpp:397]     Test net output #0: accuracy = 0.5894
I1211 06:07:03.656291 13088 solver.cpp:397]     Test net output #1: loss = 1.57873 (* 1 = 1.57873 loss)
I1211 06:07:03.715286 13088 solver.cpp:218] Iteration 94000 (13.0385 iter/s, 7.66962s/100 iters), loss = 0.625902
I1211 06:07:03.715286 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 06:07:03.715286 13088 solver.cpp:237]     Train net output #1: loss = 0.625902 (* 1 = 0.625902 loss)
I1211 06:07:03.715286 13088 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1211 06:07:09.912765 13088 solver.cpp:218] Iteration 94100 (16.1364 iter/s, 6.19716s/100 iters), loss = 0.673391
I1211 06:07:09.912765 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 06:07:09.912765 13088 solver.cpp:237]     Train net output #1: loss = 0.673391 (* 1 = 0.673391 loss)
I1211 06:07:09.912765 13088 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1211 06:07:16.197616 13088 solver.cpp:218] Iteration 94200 (15.9124 iter/s, 6.28443s/100 iters), loss = 0.618305
I1211 06:07:16.197616 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 06:07:16.197616 13088 solver.cpp:237]     Train net output #1: loss = 0.618305 (* 1 = 0.618305 loss)
I1211 06:07:16.197616 13088 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1211 06:07:22.432431 13088 solver.cpp:218] Iteration 94300 (16.0406 iter/s, 6.23417s/100 iters), loss = 0.867799
I1211 06:07:22.432431 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1211 06:07:22.432431 13088 solver.cpp:237]     Train net output #1: loss = 0.867799 (* 1 = 0.867799 loss)
I1211 06:07:22.432431 13088 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1211 06:07:28.625844 13088 solver.cpp:218] Iteration 94400 (16.1479 iter/s, 6.19275s/100 iters), loss = 0.700798
I1211 06:07:28.625844 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 06:07:28.625844 13088 solver.cpp:237]     Train net output #1: loss = 0.700798 (* 1 = 0.700798 loss)
I1211 06:07:28.625844 13088 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1211 06:07:34.521317 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:07:34.775337 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94500.caffemodel
I1211 06:07:34.792341 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_94500.solverstate
I1211 06:07:34.797343 13088 solver.cpp:330] Iteration 94500, Testing net (#0)
I1211 06:07:34.797343 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:07:36.168483 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:07:36.221526 13088 solver.cpp:397]     Test net output #0: accuracy = 0.5792
I1211 06:07:36.221526 13088 solver.cpp:397]     Test net output #1: loss = 1.71315 (* 1 = 1.71315 loss)
I1211 06:07:36.282038 13088 solver.cpp:218] Iteration 94500 (13.0633 iter/s, 7.65506s/100 iters), loss = 0.704145
I1211 06:07:36.282038 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 06:07:36.282038 13088 solver.cpp:237]     Train net output #1: loss = 0.704145 (* 1 = 0.704145 loss)
I1211 06:07:36.282038 13088 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1211 06:07:42.677692 13088 solver.cpp:218] Iteration 94600 (15.6363 iter/s, 6.39539s/100 iters), loss = 0.643635
I1211 06:07:42.677692 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 06:07:42.677692 13088 solver.cpp:237]     Train net output #1: loss = 0.643635 (* 1 = 0.643635 loss)
I1211 06:07:42.677692 13088 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1211 06:07:49.021379 13088 solver.cpp:218] Iteration 94700 (15.7657 iter/s, 6.34289s/100 iters), loss = 0.638868
I1211 06:07:49.021379 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 06:07:49.021379 13088 solver.cpp:237]     Train net output #1: loss = 0.638868 (* 1 = 0.638868 loss)
I1211 06:07:49.021379 13088 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1211 06:07:55.363775 13088 solver.cpp:218] Iteration 94800 (15.7675 iter/s, 6.34214s/100 iters), loss = 0.748029
I1211 06:07:55.363775 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 06:07:55.363775 13088 solver.cpp:237]     Train net output #1: loss = 0.748029 (* 1 = 0.748029 loss)
I1211 06:07:55.363775 13088 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1211 06:08:01.710896 13088 solver.cpp:218] Iteration 94900 (15.757 iter/s, 6.34638s/100 iters), loss = 0.741984
I1211 06:08:01.710896 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 06:08:01.710896 13088 solver.cpp:237]     Train net output #1: loss = 0.741984 (* 1 = 0.741984 loss)
I1211 06:08:01.710896 13088 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1211 06:08:07.678092 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:08:07.927093 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95000.caffemodel
I1211 06:08:07.943094 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95000.solverstate
I1211 06:08:07.947592 13088 solver.cpp:330] Iteration 95000, Testing net (#0)
I1211 06:08:07.948091 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:08:09.326606 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:08:09.380614 13088 solver.cpp:397]     Test net output #0: accuracy = 0.5745
I1211 06:08:09.380614 13088 solver.cpp:397]     Test net output #1: loss = 1.71101 (* 1 = 1.71101 loss)
I1211 06:08:09.440613 13088 solver.cpp:218] Iteration 95000 (12.9369 iter/s, 7.72983s/100 iters), loss = 0.637834
I1211 06:08:09.440613 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 06:08:09.440613 13088 solver.cpp:237]     Train net output #1: loss = 0.637834 (* 1 = 0.637834 loss)
I1211 06:08:09.440613 13088 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1211 06:08:09.440613 13088 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1211 06:08:15.720927 13088 solver.cpp:218] Iteration 95100 (15.9245 iter/s, 6.27963s/100 iters), loss = 0.731361
I1211 06:08:15.720927 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 06:08:15.720927 13088 solver.cpp:237]     Train net output #1: loss = 0.731361 (* 1 = 0.731361 loss)
I1211 06:08:15.720927 13088 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1211 06:08:22.079180 13088 solver.cpp:218] Iteration 95200 (15.7302 iter/s, 6.3572s/100 iters), loss = 0.343887
I1211 06:08:22.079180 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:08:22.079180 13088 solver.cpp:237]     Train net output #1: loss = 0.343887 (* 1 = 0.343887 loss)
I1211 06:08:22.079180 13088 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1211 06:08:28.443964 13088 solver.cpp:218] Iteration 95300 (15.7127 iter/s, 6.3643s/100 iters), loss = 0.585884
I1211 06:08:28.443964 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 06:08:28.443964 13088 solver.cpp:237]     Train net output #1: loss = 0.585884 (* 1 = 0.585884 loss)
I1211 06:08:28.443964 13088 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1211 06:08:34.820664 13088 solver.cpp:218] Iteration 95400 (15.6837 iter/s, 6.37603s/100 iters), loss = 0.519256
I1211 06:08:34.820664 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 06:08:34.820664 13088 solver.cpp:237]     Train net output #1: loss = 0.519256 (* 1 = 0.519256 loss)
I1211 06:08:34.820664 13088 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1211 06:08:40.890723 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:08:41.138219 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95500.caffemodel
I1211 06:08:41.153220 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_95500.solverstate
I1211 06:08:41.158221 13088 solver.cpp:330] Iteration 95500, Testing net (#0)
I1211 06:08:41.158720 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:08:42.540221 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:08:42.595221 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6752
I1211 06:08:42.595721 13088 solver.cpp:397]     Test net output #1: loss = 1.18974 (* 1 = 1.18974 loss)
I1211 06:08:42.655719 13088 solver.cpp:218] Iteration 95500 (12.7634 iter/s, 7.8349s/100 iters), loss = 0.44552
I1211 06:08:42.655719 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:08:42.656219 13088 solver.cpp:237]     Train net output #1: loss = 0.44552 (* 1 = 0.44552 loss)
I1211 06:08:42.656219 13088 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1211 06:08:48.954660 13088 solver.cpp:218] Iteration 95600 (15.8772 iter/s, 6.29834s/100 iters), loss = 0.496082
I1211 06:08:48.954660 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:08:48.954660 13088 solver.cpp:237]     Train net output #1: loss = 0.496082 (* 1 = 0.496082 loss)
I1211 06:08:48.954660 13088 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1211 06:08:55.256839 13088 solver.cpp:218] Iteration 95700 (15.8686 iter/s, 6.30174s/100 iters), loss = 0.425394
I1211 06:08:55.256839 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:08:55.256839 13088 solver.cpp:237]     Train net output #1: loss = 0.425394 (* 1 = 0.425394 loss)
I1211 06:08:55.256839 13088 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1211 06:09:01.541445 13088 solver.cpp:218] Iteration 95800 (15.9137 iter/s, 6.28388s/100 iters), loss = 0.515717
I1211 06:09:01.541445 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 06:09:01.541445 13088 solver.cpp:237]     Train net output #1: loss = 0.515717 (* 1 = 0.515717 loss)
I1211 06:09:01.541445 13088 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1211 06:09:07.860244 13088 solver.cpp:218] Iteration 95900 (15.8262 iter/s, 6.31863s/100 iters), loss = 0.53988
I1211 06:09:07.860244 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:09:07.860244 13088 solver.cpp:237]     Train net output #1: loss = 0.53988 (* 1 = 0.53988 loss)
I1211 06:09:07.860244 13088 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1211 06:09:13.847113 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:09:14.096431 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96000.caffemodel
I1211 06:09:14.115429 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96000.solverstate
I1211 06:09:14.120934 13088 solver.cpp:330] Iteration 96000, Testing net (#0)
I1211 06:09:14.120934 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:09:15.493429 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:09:15.547948 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6767
I1211 06:09:15.547948 13088 solver.cpp:397]     Test net output #1: loss = 1.18155 (* 1 = 1.18155 loss)
I1211 06:09:15.607444 13088 solver.cpp:218] Iteration 96000 (12.9091 iter/s, 7.74648s/100 iters), loss = 0.460158
I1211 06:09:15.607444 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:09:15.607444 13088 solver.cpp:237]     Train net output #1: loss = 0.460158 (* 1 = 0.460158 loss)
I1211 06:09:15.607444 13088 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1211 06:09:21.917346 13088 solver.cpp:218] Iteration 96100 (15.8499 iter/s, 6.30919s/100 iters), loss = 0.510396
I1211 06:09:21.917346 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:09:21.917346 13088 solver.cpp:237]     Train net output #1: loss = 0.510396 (* 1 = 0.510396 loss)
I1211 06:09:21.917346 13088 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1211 06:09:28.237880 13088 solver.cpp:218] Iteration 96200 (15.8221 iter/s, 6.32028s/100 iters), loss = 0.387048
I1211 06:09:28.237880 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:09:28.237880 13088 solver.cpp:237]     Train net output #1: loss = 0.387048 (* 1 = 0.387048 loss)
I1211 06:09:28.237880 13088 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1211 06:09:34.553047 13088 solver.cpp:218] Iteration 96300 (15.8367 iter/s, 6.31444s/100 iters), loss = 0.504028
I1211 06:09:34.553047 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:09:34.553047 13088 solver.cpp:237]     Train net output #1: loss = 0.504028 (* 1 = 0.504028 loss)
I1211 06:09:34.553047 13088 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1211 06:09:40.844671 13088 solver.cpp:218] Iteration 96400 (15.8953 iter/s, 6.29116s/100 iters), loss = 0.484735
I1211 06:09:40.844671 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:09:40.844671 13088 solver.cpp:237]     Train net output #1: loss = 0.484735 (* 1 = 0.484735 loss)
I1211 06:09:40.844671 13088 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1211 06:09:46.844648 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:09:47.095693 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96500.caffemodel
I1211 06:09:47.111196 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_96500.solverstate
I1211 06:09:47.116194 13088 solver.cpp:330] Iteration 96500, Testing net (#0)
I1211 06:09:47.116194 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:09:48.501194 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:09:48.554694 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6753
I1211 06:09:48.554694 13088 solver.cpp:397]     Test net output #1: loss = 1.18397 (* 1 = 1.18397 loss)
I1211 06:09:48.615696 13088 solver.cpp:218] Iteration 96500 (12.8691 iter/s, 7.77057s/100 iters), loss = 0.447912
I1211 06:09:48.615696 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:09:48.615696 13088 solver.cpp:237]     Train net output #1: loss = 0.447912 (* 1 = 0.447912 loss)
I1211 06:09:48.615696 13088 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1211 06:09:54.805903 13088 solver.cpp:218] Iteration 96600 (16.1552 iter/s, 6.18994s/100 iters), loss = 0.441884
I1211 06:09:54.806403 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:09:54.806403 13088 solver.cpp:237]     Train net output #1: loss = 0.441884 (* 1 = 0.441884 loss)
I1211 06:09:54.806403 13088 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1211 06:10:01.090235 13088 solver.cpp:218] Iteration 96700 (15.9147 iter/s, 6.28349s/100 iters), loss = 0.363093
I1211 06:10:01.090235 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:10:01.090235 13088 solver.cpp:237]     Train net output #1: loss = 0.363093 (* 1 = 0.363093 loss)
I1211 06:10:01.090235 13088 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1211 06:10:07.262931 13088 solver.cpp:218] Iteration 96800 (16.2011 iter/s, 6.17242s/100 iters), loss = 0.489885
I1211 06:10:07.262931 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:10:07.262931 13088 solver.cpp:237]     Train net output #1: loss = 0.489885 (* 1 = 0.489885 loss)
I1211 06:10:07.262931 13088 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1211 06:10:13.424964 13088 solver.cpp:218] Iteration 96900 (16.2303 iter/s, 6.16132s/100 iters), loss = 0.458845
I1211 06:10:13.424964 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:10:13.424964 13088 solver.cpp:237]     Train net output #1: loss = 0.458845 (* 1 = 0.458845 loss)
I1211 06:10:13.424964 13088 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1211 06:10:19.505672 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:10:19.756671 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97000.caffemodel
I1211 06:10:19.773674 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97000.solverstate
I1211 06:10:19.779175 13088 solver.cpp:330] Iteration 97000, Testing net (#0)
I1211 06:10:19.779175 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:10:21.153672 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:10:21.207672 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 06:10:21.207672 13088 solver.cpp:397]     Test net output #1: loss = 1.1802 (* 1 = 1.1802 loss)
I1211 06:10:21.269171 13088 solver.cpp:218] Iteration 97000 (12.7493 iter/s, 7.84355s/100 iters), loss = 0.383919
I1211 06:10:21.269171 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:10:21.269171 13088 solver.cpp:237]     Train net output #1: loss = 0.383919 (* 1 = 0.383919 loss)
I1211 06:10:21.269171 13088 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1211 06:10:27.509071 13088 solver.cpp:218] Iteration 97100 (16.0273 iter/s, 6.23936s/100 iters), loss = 0.48083
I1211 06:10:27.509071 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:10:27.509071 13088 solver.cpp:237]     Train net output #1: loss = 0.48083 (* 1 = 0.48083 loss)
I1211 06:10:27.509071 13088 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1211 06:10:33.673549 13088 solver.cpp:218] Iteration 97200 (16.2228 iter/s, 6.16415s/100 iters), loss = 0.395257
I1211 06:10:33.673549 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:10:33.673549 13088 solver.cpp:237]     Train net output #1: loss = 0.395257 (* 1 = 0.395257 loss)
I1211 06:10:33.673549 13088 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1211 06:10:39.834619 13088 solver.cpp:218] Iteration 97300 (16.2322 iter/s, 6.1606s/100 iters), loss = 0.443001
I1211 06:10:39.834619 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:10:39.834619 13088 solver.cpp:237]     Train net output #1: loss = 0.443001 (* 1 = 0.443001 loss)
I1211 06:10:39.834619 13088 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1211 06:10:45.992272 13088 solver.cpp:218] Iteration 97400 (16.2413 iter/s, 6.15715s/100 iters), loss = 0.492816
I1211 06:10:45.992272 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 06:10:45.992272 13088 solver.cpp:237]     Train net output #1: loss = 0.492816 (* 1 = 0.492816 loss)
I1211 06:10:45.992272 13088 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1211 06:10:51.846127 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:10:52.087628 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97500.caffemodel
I1211 06:10:52.102629 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_97500.solverstate
I1211 06:10:52.107640 13088 solver.cpp:330] Iteration 97500, Testing net (#0)
I1211 06:10:52.107640 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:10:53.438628 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:10:53.491627 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6766
I1211 06:10:53.491627 13088 solver.cpp:397]     Test net output #1: loss = 1.18314 (* 1 = 1.18314 loss)
I1211 06:10:53.551137 13088 solver.cpp:218] Iteration 97500 (13.2298 iter/s, 7.5587s/100 iters), loss = 0.336516
I1211 06:10:53.551137 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:10:53.551137 13088 solver.cpp:237]     Train net output #1: loss = 0.336516 (* 1 = 0.336516 loss)
I1211 06:10:53.551137 13088 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1211 06:10:59.709141 13088 solver.cpp:218] Iteration 97600 (16.24 iter/s, 6.15764s/100 iters), loss = 0.508899
I1211 06:10:59.709641 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 06:10:59.709641 13088 solver.cpp:237]     Train net output #1: loss = 0.508899 (* 1 = 0.508899 loss)
I1211 06:10:59.709641 13088 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1211 06:11:05.861047 13088 solver.cpp:218] Iteration 97700 (16.2576 iter/s, 6.15096s/100 iters), loss = 0.339164
I1211 06:11:05.861047 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:11:05.861047 13088 solver.cpp:237]     Train net output #1: loss = 0.339164 (* 1 = 0.339164 loss)
I1211 06:11:05.861047 13088 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1211 06:11:12.011152 13088 solver.cpp:218] Iteration 97800 (16.2611 iter/s, 6.14964s/100 iters), loss = 0.399908
I1211 06:11:12.011152 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:11:12.011152 13088 solver.cpp:237]     Train net output #1: loss = 0.399908 (* 1 = 0.399908 loss)
I1211 06:11:12.011152 13088 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1211 06:11:18.175253 13088 solver.cpp:218] Iteration 97900 (16.2239 iter/s, 6.16376s/100 iters), loss = 0.428791
I1211 06:11:18.175253 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:11:18.175253 13088 solver.cpp:237]     Train net output #1: loss = 0.428791 (* 1 = 0.428791 loss)
I1211 06:11:18.175253 13088 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1211 06:11:24.028280 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:11:24.269773 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98000.caffemodel
I1211 06:11:24.285773 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98000.solverstate
I1211 06:11:24.290773 13088 solver.cpp:330] Iteration 98000, Testing net (#0)
I1211 06:11:24.290773 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:11:25.627804 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:11:25.680305 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6789
I1211 06:11:25.680305 13088 solver.cpp:397]     Test net output #1: loss = 1.18973 (* 1 = 1.18973 loss)
I1211 06:11:25.738803 13088 solver.cpp:218] Iteration 98000 (13.2218 iter/s, 7.56326s/100 iters), loss = 0.425892
I1211 06:11:25.738803 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:11:25.738803 13088 solver.cpp:237]     Train net output #1: loss = 0.425892 (* 1 = 0.425892 loss)
I1211 06:11:25.738803 13088 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1211 06:11:31.884277 13088 solver.cpp:218] Iteration 98100 (16.2742 iter/s, 6.14469s/100 iters), loss = 0.435197
I1211 06:11:31.884277 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:11:31.884277 13088 solver.cpp:237]     Train net output #1: loss = 0.435197 (* 1 = 0.435197 loss)
I1211 06:11:31.884277 13088 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1211 06:11:38.083319 13088 solver.cpp:218] Iteration 98200 (16.1327 iter/s, 6.19857s/100 iters), loss = 0.329251
I1211 06:11:38.083319 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:11:38.083319 13088 solver.cpp:237]     Train net output #1: loss = 0.329251 (* 1 = 0.329251 loss)
I1211 06:11:38.083319 13088 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1211 06:11:44.232818 13088 solver.cpp:218] Iteration 98300 (16.2618 iter/s, 6.14939s/100 iters), loss = 0.447021
I1211 06:11:44.232818 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 06:11:44.232818 13088 solver.cpp:237]     Train net output #1: loss = 0.447021 (* 1 = 0.447021 loss)
I1211 06:11:44.232818 13088 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1211 06:11:50.395411 13088 solver.cpp:218] Iteration 98400 (16.2283 iter/s, 6.16208s/100 iters), loss = 0.514337
I1211 06:11:50.395411 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 06:11:50.395411 13088 solver.cpp:237]     Train net output #1: loss = 0.514337 (* 1 = 0.514337 loss)
I1211 06:11:50.395411 13088 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1211 06:11:56.253404 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:11:56.497903 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98500.caffemodel
I1211 06:11:56.512904 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_98500.solverstate
I1211 06:11:56.517905 13088 solver.cpp:330] Iteration 98500, Testing net (#0)
I1211 06:11:56.517905 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:11:57.856904 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:11:57.908903 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6777
I1211 06:11:57.909404 13088 solver.cpp:397]     Test net output #1: loss = 1.19222 (* 1 = 1.19222 loss)
I1211 06:11:57.967902 13088 solver.cpp:218] Iteration 98500 (13.2064 iter/s, 7.57208s/100 iters), loss = 0.320625
I1211 06:11:57.968403 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:11:57.968403 13088 solver.cpp:237]     Train net output #1: loss = 0.320625 (* 1 = 0.320625 loss)
I1211 06:11:57.968403 13088 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1211 06:12:04.193222 13088 solver.cpp:218] Iteration 98600 (16.0652 iter/s, 6.22463s/100 iters), loss = 0.4269
I1211 06:12:04.193222 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:12:04.193222 13088 solver.cpp:237]     Train net output #1: loss = 0.4269 (* 1 = 0.4269 loss)
I1211 06:12:04.193222 13088 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1211 06:12:10.360811 13088 solver.cpp:218] Iteration 98700 (16.2154 iter/s, 6.16698s/100 iters), loss = 0.35052
I1211 06:12:10.360811 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:12:10.360811 13088 solver.cpp:237]     Train net output #1: loss = 0.35052 (* 1 = 0.35052 loss)
I1211 06:12:10.360811 13088 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1211 06:12:16.527834 13088 solver.cpp:218] Iteration 98800 (16.2162 iter/s, 6.16669s/100 iters), loss = 0.421563
I1211 06:12:16.527834 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:12:16.527834 13088 solver.cpp:237]     Train net output #1: loss = 0.421563 (* 1 = 0.421563 loss)
I1211 06:12:16.527834 13088 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1211 06:12:22.676844 13088 solver.cpp:218] Iteration 98900 (16.2639 iter/s, 6.14859s/100 iters), loss = 0.459407
I1211 06:12:22.676844 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:12:22.676844 13088 solver.cpp:237]     Train net output #1: loss = 0.459407 (* 1 = 0.459407 loss)
I1211 06:12:22.676844 13088 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1211 06:12:28.601418 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:12:28.844434 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99000.caffemodel
I1211 06:12:28.860430 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99000.solverstate
I1211 06:12:28.865432 13088 solver.cpp:330] Iteration 99000, Testing net (#0)
I1211 06:12:28.865432 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:12:30.200440 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:12:30.253418 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6775
I1211 06:12:30.253418 13088 solver.cpp:397]     Test net output #1: loss = 1.19125 (* 1 = 1.19125 loss)
I1211 06:12:30.311929 13088 solver.cpp:218] Iteration 99000 (13.0988 iter/s, 7.63428s/100 iters), loss = 0.371089
I1211 06:12:30.311929 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 06:12:30.311929 13088 solver.cpp:237]     Train net output #1: loss = 0.371089 (* 1 = 0.371089 loss)
I1211 06:12:30.311929 13088 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1211 06:12:36.608268 13088 solver.cpp:218] Iteration 99100 (15.8835 iter/s, 6.29586s/100 iters), loss = 0.404119
I1211 06:12:36.608268 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:12:36.608268 13088 solver.cpp:237]     Train net output #1: loss = 0.404119 (* 1 = 0.404119 loss)
I1211 06:12:36.608268 13088 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1211 06:12:42.801962 13088 solver.cpp:218] Iteration 99200 (16.1464 iter/s, 6.19331s/100 iters), loss = 0.277394
I1211 06:12:42.801962 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 06:12:42.801962 13088 solver.cpp:237]     Train net output #1: loss = 0.277394 (* 1 = 0.277394 loss)
I1211 06:12:42.801962 13088 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1211 06:12:49.078871 13088 solver.cpp:218] Iteration 99300 (15.932 iter/s, 6.27666s/100 iters), loss = 0.418139
I1211 06:12:49.078871 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:12:49.079370 13088 solver.cpp:237]     Train net output #1: loss = 0.418139 (* 1 = 0.418139 loss)
I1211 06:12:49.079370 13088 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1211 06:12:55.509701 13088 solver.cpp:218] Iteration 99400 (15.5517 iter/s, 6.43016s/100 iters), loss = 0.463467
I1211 06:12:55.509701 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:12:55.509701 13088 solver.cpp:237]     Train net output #1: loss = 0.463467 (* 1 = 0.463467 loss)
I1211 06:12:55.509701 13088 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1211 06:13:01.418257 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:13:01.661255 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99500.caffemodel
I1211 06:13:01.677270 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_99500.solverstate
I1211 06:13:01.682258 13088 solver.cpp:330] Iteration 99500, Testing net (#0)
I1211 06:13:01.682258 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:13:03.019757 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:13:03.072270 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6763
I1211 06:13:03.072770 13088 solver.cpp:397]     Test net output #1: loss = 1.19871 (* 1 = 1.19871 loss)
I1211 06:13:03.130766 13088 solver.cpp:218] Iteration 99500 (13.1221 iter/s, 7.62071s/100 iters), loss = 0.360847
I1211 06:13:03.130766 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:13:03.130766 13088 solver.cpp:237]     Train net output #1: loss = 0.360847 (* 1 = 0.360847 loss)
I1211 06:13:03.130766 13088 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1211 06:13:09.402393 13088 solver.cpp:218] Iteration 99600 (15.9466 iter/s, 6.27093s/100 iters), loss = 0.440351
I1211 06:13:09.402393 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:13:09.402393 13088 solver.cpp:237]     Train net output #1: loss = 0.440351 (* 1 = 0.440351 loss)
I1211 06:13:09.402393 13088 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1211 06:13:15.684798 13088 solver.cpp:218] Iteration 99700 (15.9182 iter/s, 6.28213s/100 iters), loss = 0.33029
I1211 06:13:15.684798 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:13:15.684798 13088 solver.cpp:237]     Train net output #1: loss = 0.33029 (* 1 = 0.33029 loss)
I1211 06:13:15.684798 13088 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1211 06:13:21.908762 13088 solver.cpp:218] Iteration 99800 (16.0691 iter/s, 6.22311s/100 iters), loss = 0.376686
I1211 06:13:21.908762 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:13:21.908762 13088 solver.cpp:237]     Train net output #1: loss = 0.376686 (* 1 = 0.376686 loss)
I1211 06:13:21.908762 13088 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1211 06:13:28.044596 13088 solver.cpp:218] Iteration 99900 (16.2977 iter/s, 6.13582s/100 iters), loss = 0.454423
I1211 06:13:28.045106 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:13:28.045106 13088 solver.cpp:237]     Train net output #1: loss = 0.454423 (* 1 = 0.454423 loss)
I1211 06:13:28.045106 13088 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1211 06:13:34.016961 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:13:34.258446 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100000.caffemodel
I1211 06:13:34.273962 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100000.solverstate
I1211 06:13:34.278956 13088 solver.cpp:330] Iteration 100000, Testing net (#0)
I1211 06:13:34.278956 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:13:35.615960 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:13:35.668447 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6769
I1211 06:13:35.668447 13088 solver.cpp:397]     Test net output #1: loss = 1.19155 (* 1 = 1.19155 loss)
I1211 06:13:35.727458 13088 solver.cpp:218] Iteration 100000 (13.0171 iter/s, 7.68222s/100 iters), loss = 0.323779
I1211 06:13:35.727962 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:13:35.727962 13088 solver.cpp:237]     Train net output #1: loss = 0.323779 (* 1 = 0.323779 loss)
I1211 06:13:35.727962 13088 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1211 06:13:41.934092 13088 solver.cpp:218] Iteration 100100 (16.1143 iter/s, 6.20569s/100 iters), loss = 0.4107
I1211 06:13:41.934092 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:13:41.934092 13088 solver.cpp:237]     Train net output #1: loss = 0.4107 (* 1 = 0.4107 loss)
I1211 06:13:41.934092 13088 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1211 06:13:48.233594 13088 solver.cpp:218] Iteration 100200 (15.8748 iter/s, 6.29929s/100 iters), loss = 0.302442
I1211 06:13:48.233594 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:13:48.233594 13088 solver.cpp:237]     Train net output #1: loss = 0.302442 (* 1 = 0.302442 loss)
I1211 06:13:48.233594 13088 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1211 06:13:54.607554 13088 solver.cpp:218] Iteration 100300 (15.6909 iter/s, 6.37311s/100 iters), loss = 0.457474
I1211 06:13:54.607554 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 06:13:54.607554 13088 solver.cpp:237]     Train net output #1: loss = 0.457474 (* 1 = 0.457474 loss)
I1211 06:13:54.607554 13088 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1211 06:14:00.951807 13088 solver.cpp:218] Iteration 100400 (15.7638 iter/s, 6.34365s/100 iters), loss = 0.434566
I1211 06:14:00.951807 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:14:00.951807 13088 solver.cpp:237]     Train net output #1: loss = 0.434566 (* 1 = 0.434566 loss)
I1211 06:14:00.951807 13088 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1211 06:14:06.966631 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:14:07.215077 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100500.caffemodel
I1211 06:14:07.230578 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_100500.solverstate
I1211 06:14:07.235577 13088 solver.cpp:330] Iteration 100500, Testing net (#0)
I1211 06:14:07.236078 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:14:08.589094 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:14:08.642580 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6791
I1211 06:14:08.642580 13088 solver.cpp:397]     Test net output #1: loss = 1.19658 (* 1 = 1.19658 loss)
I1211 06:14:08.701588 13088 solver.cpp:218] Iteration 100500 (12.9041 iter/s, 7.74945s/100 iters), loss = 0.47284
I1211 06:14:08.701588 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:14:08.701588 13088 solver.cpp:237]     Train net output #1: loss = 0.47284 (* 1 = 0.47284 loss)
I1211 06:14:08.701588 13088 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1211 06:14:14.959609 13088 solver.cpp:218] Iteration 100600 (15.9806 iter/s, 6.25759s/100 iters), loss = 0.442015
I1211 06:14:14.959609 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 06:14:14.960110 13088 solver.cpp:237]     Train net output #1: loss = 0.442015 (* 1 = 0.442015 loss)
I1211 06:14:14.960110 13088 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1211 06:14:21.213281 13088 solver.cpp:218] Iteration 100700 (15.9919 iter/s, 6.25315s/100 iters), loss = 0.342961
I1211 06:14:21.213783 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:14:21.213783 13088 solver.cpp:237]     Train net output #1: loss = 0.342961 (* 1 = 0.342961 loss)
I1211 06:14:21.213783 13088 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1211 06:14:27.462188 13088 solver.cpp:218] Iteration 100800 (16.0047 iter/s, 6.24815s/100 iters), loss = 0.388009
I1211 06:14:27.462188 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:14:27.462188 13088 solver.cpp:237]     Train net output #1: loss = 0.388009 (* 1 = 0.388009 loss)
I1211 06:14:27.462188 13088 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1211 06:14:33.715719 13088 solver.cpp:218] Iteration 100900 (15.9917 iter/s, 6.25324s/100 iters), loss = 0.536164
I1211 06:14:33.715719 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 06:14:33.715719 13088 solver.cpp:237]     Train net output #1: loss = 0.536164 (* 1 = 0.536164 loss)
I1211 06:14:33.715719 13088 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1211 06:14:39.671121 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:14:39.919643 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101000.caffemodel
I1211 06:14:39.940143 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101000.solverstate
I1211 06:14:39.945147 13088 solver.cpp:330] Iteration 101000, Testing net (#0)
I1211 06:14:39.945662 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:14:41.307158 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:14:41.360654 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6785
I1211 06:14:41.360654 13088 solver.cpp:397]     Test net output #1: loss = 1.20758 (* 1 = 1.20758 loss)
I1211 06:14:41.420145 13088 solver.cpp:218] Iteration 101000 (12.9806 iter/s, 7.70379s/100 iters), loss = 0.277465
I1211 06:14:41.420145 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:14:41.420145 13088 solver.cpp:237]     Train net output #1: loss = 0.277465 (* 1 = 0.277465 loss)
I1211 06:14:41.420145 13088 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1211 06:14:47.682996 13088 solver.cpp:218] Iteration 101100 (15.968 iter/s, 6.26253s/100 iters), loss = 0.37694
I1211 06:14:47.682996 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:14:47.682996 13088 solver.cpp:237]     Train net output #1: loss = 0.37694 (* 1 = 0.37694 loss)
I1211 06:14:47.682996 13088 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1211 06:14:53.940767 13088 solver.cpp:218] Iteration 101200 (15.9817 iter/s, 6.25714s/100 iters), loss = 0.403582
I1211 06:14:53.940767 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:14:53.940767 13088 solver.cpp:237]     Train net output #1: loss = 0.403582 (* 1 = 0.403582 loss)
I1211 06:14:53.940767 13088 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1211 06:15:00.193768 13088 solver.cpp:218] Iteration 101300 (15.993 iter/s, 6.25275s/100 iters), loss = 0.417611
I1211 06:15:00.193768 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:15:00.194268 13088 solver.cpp:237]     Train net output #1: loss = 0.417611 (* 1 = 0.417611 loss)
I1211 06:15:00.194268 13088 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1211 06:15:06.443274 13088 solver.cpp:218] Iteration 101400 (16.0036 iter/s, 6.24858s/100 iters), loss = 0.351271
I1211 06:15:06.443274 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:15:06.443274 13088 solver.cpp:237]     Train net output #1: loss = 0.351271 (* 1 = 0.351271 loss)
I1211 06:15:06.443274 13088 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1211 06:15:12.391726 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:15:12.639734 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101500.caffemodel
I1211 06:15:12.655226 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_101500.solverstate
I1211 06:15:12.660228 13088 solver.cpp:330] Iteration 101500, Testing net (#0)
I1211 06:15:12.660228 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:15:14.021225 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:15:14.073726 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6769
I1211 06:15:14.073726 13088 solver.cpp:397]     Test net output #1: loss = 1.20371 (* 1 = 1.20371 loss)
I1211 06:15:14.133225 13088 solver.cpp:218] Iteration 101500 (13.0044 iter/s, 7.68973s/100 iters), loss = 0.302602
I1211 06:15:14.133225 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:15:14.133225 13088 solver.cpp:237]     Train net output #1: loss = 0.302602 (* 1 = 0.302602 loss)
I1211 06:15:14.133225 13088 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1211 06:15:20.409783 13088 solver.cpp:218] Iteration 101600 (15.9337 iter/s, 6.27601s/100 iters), loss = 0.405722
I1211 06:15:20.409783 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:15:20.409783 13088 solver.cpp:237]     Train net output #1: loss = 0.405722 (* 1 = 0.405722 loss)
I1211 06:15:20.409783 13088 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1211 06:15:26.583631 13088 solver.cpp:218] Iteration 101700 (16.1988 iter/s, 6.1733s/100 iters), loss = 0.342476
I1211 06:15:26.583631 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:15:26.583631 13088 solver.cpp:237]     Train net output #1: loss = 0.342476 (* 1 = 0.342476 loss)
I1211 06:15:26.583631 13088 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1211 06:15:32.827129 13088 solver.cpp:218] Iteration 101800 (16.0177 iter/s, 6.24308s/100 iters), loss = 0.358694
I1211 06:15:32.827129 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:15:32.827129 13088 solver.cpp:237]     Train net output #1: loss = 0.358694 (* 1 = 0.358694 loss)
I1211 06:15:32.827129 13088 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1211 06:15:39.163694 13088 solver.cpp:218] Iteration 101900 (15.7822 iter/s, 6.33627s/100 iters), loss = 0.432573
I1211 06:15:39.163694 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:15:39.163694 13088 solver.cpp:237]     Train net output #1: loss = 0.432573 (* 1 = 0.432573 loss)
I1211 06:15:39.163694 13088 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1211 06:15:45.200387 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:15:45.446887 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102000.caffemodel
I1211 06:15:45.463387 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102000.solverstate
I1211 06:15:45.468387 13088 solver.cpp:330] Iteration 102000, Testing net (#0)
I1211 06:15:45.468387 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:15:46.836887 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:15:46.889888 13088 solver.cpp:397]     Test net output #0: accuracy = 0.678
I1211 06:15:46.890388 13088 solver.cpp:397]     Test net output #1: loss = 1.20546 (* 1 = 1.20546 loss)
I1211 06:15:46.949887 13088 solver.cpp:218] Iteration 102000 (12.8446 iter/s, 7.78538s/100 iters), loss = 0.255822
I1211 06:15:46.949887 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:15:46.949887 13088 solver.cpp:237]     Train net output #1: loss = 0.255822 (* 1 = 0.255822 loss)
I1211 06:15:46.949887 13088 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1211 06:15:53.257542 13088 solver.cpp:218] Iteration 102100 (15.8547 iter/s, 6.30726s/100 iters), loss = 0.366651
I1211 06:15:53.257542 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:15:53.257542 13088 solver.cpp:237]     Train net output #1: loss = 0.366651 (* 1 = 0.366651 loss)
I1211 06:15:53.257542 13088 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1211 06:15:59.544852 13088 solver.cpp:218] Iteration 102200 (15.9062 iter/s, 6.28687s/100 iters), loss = 0.314573
I1211 06:15:59.544852 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:15:59.544852 13088 solver.cpp:237]     Train net output #1: loss = 0.314573 (* 1 = 0.314573 loss)
I1211 06:15:59.544852 13088 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1211 06:16:05.816347 13088 solver.cpp:218] Iteration 102300 (15.9467 iter/s, 6.27089s/100 iters), loss = 0.411185
I1211 06:16:05.816347 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:16:05.816347 13088 solver.cpp:237]     Train net output #1: loss = 0.411185 (* 1 = 0.411185 loss)
I1211 06:16:05.816347 13088 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1211 06:16:12.068173 13088 solver.cpp:218] Iteration 102400 (15.9962 iter/s, 6.2515s/100 iters), loss = 0.426218
I1211 06:16:12.068173 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:16:12.068173 13088 solver.cpp:237]     Train net output #1: loss = 0.426218 (* 1 = 0.426218 loss)
I1211 06:16:12.068173 13088 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1211 06:16:17.998615 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:16:18.247601 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102500.caffemodel
I1211 06:16:18.268615 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_102500.solverstate
I1211 06:16:18.274603 13088 solver.cpp:330] Iteration 102500, Testing net (#0)
I1211 06:16:18.274603 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:16:19.635110 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:16:19.688621 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6732
I1211 06:16:19.688621 13088 solver.cpp:397]     Test net output #1: loss = 1.21329 (* 1 = 1.21329 loss)
I1211 06:16:19.748600 13088 solver.cpp:218] Iteration 102500 (13.0211 iter/s, 7.67986s/100 iters), loss = 0.31795
I1211 06:16:19.748600 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:16:19.748600 13088 solver.cpp:237]     Train net output #1: loss = 0.31795 (* 1 = 0.31795 loss)
I1211 06:16:19.748600 13088 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1211 06:16:25.994704 13088 solver.cpp:218] Iteration 102600 (16.0113 iter/s, 6.2456s/100 iters), loss = 0.279361
I1211 06:16:25.994704 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:16:25.994704 13088 solver.cpp:237]     Train net output #1: loss = 0.279361 (* 1 = 0.279361 loss)
I1211 06:16:25.994704 13088 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1211 06:16:32.243094 13088 solver.cpp:218] Iteration 102700 (16.0062 iter/s, 6.24757s/100 iters), loss = 0.346875
I1211 06:16:32.243094 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:16:32.243094 13088 solver.cpp:237]     Train net output #1: loss = 0.346875 (* 1 = 0.346875 loss)
I1211 06:16:32.243094 13088 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1211 06:16:38.483988 13088 solver.cpp:218] Iteration 102800 (16.0235 iter/s, 6.24082s/100 iters), loss = 0.324786
I1211 06:16:38.484488 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:16:38.484488 13088 solver.cpp:237]     Train net output #1: loss = 0.324786 (* 1 = 0.324786 loss)
I1211 06:16:38.484488 13088 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1211 06:16:44.741422 13088 solver.cpp:218] Iteration 102900 (15.9827 iter/s, 6.25675s/100 iters), loss = 0.431897
I1211 06:16:44.741422 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:16:44.741422 13088 solver.cpp:237]     Train net output #1: loss = 0.431897 (* 1 = 0.431897 loss)
I1211 06:16:44.741422 13088 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1211 06:16:50.679855 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:16:50.927855 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103000.caffemodel
I1211 06:16:50.947355 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103000.solverstate
I1211 06:16:50.952855 13088 solver.cpp:330] Iteration 103000, Testing net (#0)
I1211 06:16:50.952855 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:16:52.312364 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:16:52.365865 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6759
I1211 06:16:52.365865 13088 solver.cpp:397]     Test net output #1: loss = 1.2091 (* 1 = 1.2091 loss)
I1211 06:16:52.425863 13088 solver.cpp:218] Iteration 103000 (13.014 iter/s, 7.68404s/100 iters), loss = 0.364596
I1211 06:16:52.425863 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:16:52.425863 13088 solver.cpp:237]     Train net output #1: loss = 0.364596 (* 1 = 0.364596 loss)
I1211 06:16:52.425863 13088 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1211 06:16:58.683974 13088 solver.cpp:218] Iteration 103100 (15.9805 iter/s, 6.25761s/100 iters), loss = 0.42413
I1211 06:16:58.683974 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:16:58.683974 13088 solver.cpp:237]     Train net output #1: loss = 0.42413 (* 1 = 0.42413 loss)
I1211 06:16:58.683974 13088 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1211 06:17:04.938680 13088 solver.cpp:218] Iteration 103200 (15.9895 iter/s, 6.25412s/100 iters), loss = 0.301522
I1211 06:17:04.938680 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:17:04.938680 13088 solver.cpp:237]     Train net output #1: loss = 0.301522 (* 1 = 0.301522 loss)
I1211 06:17:04.938680 13088 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1211 06:17:11.187309 13088 solver.cpp:218] Iteration 103300 (16.0056 iter/s, 6.24783s/100 iters), loss = 0.360628
I1211 06:17:11.187309 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:17:11.187309 13088 solver.cpp:237]     Train net output #1: loss = 0.360628 (* 1 = 0.360628 loss)
I1211 06:17:11.187309 13088 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1211 06:17:17.438745 13088 solver.cpp:218] Iteration 103400 (15.9972 iter/s, 6.25107s/100 iters), loss = 0.456802
I1211 06:17:17.438745 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:17:17.438745 13088 solver.cpp:237]     Train net output #1: loss = 0.456802 (* 1 = 0.456802 loss)
I1211 06:17:17.438745 13088 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1211 06:17:23.370591 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:17:23.617593 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103500.caffemodel
I1211 06:17:23.632591 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_103500.solverstate
I1211 06:17:23.637092 13088 solver.cpp:330] Iteration 103500, Testing net (#0)
I1211 06:17:23.637590 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:17:24.987110 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:17:25.040592 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6781
I1211 06:17:25.040592 13088 solver.cpp:397]     Test net output #1: loss = 1.21334 (* 1 = 1.21334 loss)
I1211 06:17:25.099591 13088 solver.cpp:218] Iteration 103500 (13.0544 iter/s, 7.66024s/100 iters), loss = 0.323312
I1211 06:17:25.099591 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:17:25.099591 13088 solver.cpp:237]     Train net output #1: loss = 0.323312 (* 1 = 0.323312 loss)
I1211 06:17:25.099591 13088 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1211 06:17:31.297309 13088 solver.cpp:218] Iteration 103600 (16.1362 iter/s, 6.19726s/100 iters), loss = 0.347809
I1211 06:17:31.297309 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:17:31.297309 13088 solver.cpp:237]     Train net output #1: loss = 0.347809 (* 1 = 0.347809 loss)
I1211 06:17:31.297309 13088 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1211 06:17:37.543439 13088 solver.cpp:218] Iteration 103700 (16.0116 iter/s, 6.24548s/100 iters), loss = 0.28345
I1211 06:17:37.543439 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:17:37.543439 13088 solver.cpp:237]     Train net output #1: loss = 0.283451 (* 1 = 0.283451 loss)
I1211 06:17:37.543439 13088 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1211 06:17:43.801654 13088 solver.cpp:218] Iteration 103800 (15.9803 iter/s, 6.25769s/100 iters), loss = 0.426462
I1211 06:17:43.801654 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:17:43.801654 13088 solver.cpp:237]     Train net output #1: loss = 0.426462 (* 1 = 0.426462 loss)
I1211 06:17:43.801654 13088 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1211 06:17:50.043619 13088 solver.cpp:218] Iteration 103900 (16.0215 iter/s, 6.24162s/100 iters), loss = 0.509034
I1211 06:17:50.043619 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 06:17:50.043619 13088 solver.cpp:237]     Train net output #1: loss = 0.509034 (* 1 = 0.509034 loss)
I1211 06:17:50.043619 13088 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1211 06:17:55.982475 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:17:56.229974 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104000.caffemodel
I1211 06:17:56.247474 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104000.solverstate
I1211 06:17:56.252979 13088 solver.cpp:330] Iteration 104000, Testing net (#0)
I1211 06:17:56.253480 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:17:57.608474 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:17:57.661974 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6754
I1211 06:17:57.661974 13088 solver.cpp:397]     Test net output #1: loss = 1.21431 (* 1 = 1.21431 loss)
I1211 06:17:57.720973 13088 solver.cpp:218] Iteration 104000 (13.0258 iter/s, 7.67708s/100 iters), loss = 0.379464
I1211 06:17:57.721473 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:17:57.721473 13088 solver.cpp:237]     Train net output #1: loss = 0.379464 (* 1 = 0.379464 loss)
I1211 06:17:57.721473 13088 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1211 06:18:03.976300 13088 solver.cpp:218] Iteration 104100 (15.9877 iter/s, 6.25481s/100 iters), loss = 0.370098
I1211 06:18:03.976300 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:18:03.976300 13088 solver.cpp:237]     Train net output #1: loss = 0.370098 (* 1 = 0.370098 loss)
I1211 06:18:03.976300 13088 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1211 06:18:10.209910 13088 solver.cpp:218] Iteration 104200 (16.0438 iter/s, 6.23294s/100 iters), loss = 0.270519
I1211 06:18:10.209910 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:18:10.209910 13088 solver.cpp:237]     Train net output #1: loss = 0.270519 (* 1 = 0.270519 loss)
I1211 06:18:10.209910 13088 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1211 06:18:16.460223 13088 solver.cpp:218] Iteration 104300 (16.0012 iter/s, 6.24955s/100 iters), loss = 0.309041
I1211 06:18:16.460223 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:18:16.460223 13088 solver.cpp:237]     Train net output #1: loss = 0.309041 (* 1 = 0.309041 loss)
I1211 06:18:16.460223 13088 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1211 06:18:22.703589 13088 solver.cpp:218] Iteration 104400 (16.0184 iter/s, 6.24281s/100 iters), loss = 0.429765
I1211 06:18:22.703589 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:18:22.703589 13088 solver.cpp:237]     Train net output #1: loss = 0.429765 (* 1 = 0.429765 loss)
I1211 06:18:22.703589 13088 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1211 06:18:28.631511 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:18:28.877995 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104500.caffemodel
I1211 06:18:28.893993 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_104500.solverstate
I1211 06:18:28.898995 13088 solver.cpp:330] Iteration 104500, Testing net (#0)
I1211 06:18:28.898995 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:18:30.250993 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:18:30.304497 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6767
I1211 06:18:30.304497 13088 solver.cpp:397]     Test net output #1: loss = 1.22214 (* 1 = 1.22214 loss)
I1211 06:18:30.365003 13088 solver.cpp:218] Iteration 104500 (13.0531 iter/s, 7.661s/100 iters), loss = 0.323888
I1211 06:18:30.365003 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:18:30.365003 13088 solver.cpp:237]     Train net output #1: loss = 0.323888 (* 1 = 0.323888 loss)
I1211 06:18:30.365003 13088 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1211 06:18:36.599004 13088 solver.cpp:218] Iteration 104600 (16.0413 iter/s, 6.23391s/100 iters), loss = 0.362778
I1211 06:18:36.599503 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:18:36.599503 13088 solver.cpp:237]     Train net output #1: loss = 0.362778 (* 1 = 0.362778 loss)
I1211 06:18:36.599503 13088 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1211 06:18:42.838511 13088 solver.cpp:218] Iteration 104700 (16.0283 iter/s, 6.23898s/100 iters), loss = 0.327454
I1211 06:18:42.838511 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:18:42.838511 13088 solver.cpp:237]     Train net output #1: loss = 0.327454 (* 1 = 0.327454 loss)
I1211 06:18:42.838511 13088 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1211 06:18:49.083223 13088 solver.cpp:218] Iteration 104800 (16.0146 iter/s, 6.24432s/100 iters), loss = 0.334476
I1211 06:18:49.083724 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:18:49.083724 13088 solver.cpp:237]     Train net output #1: loss = 0.334476 (* 1 = 0.334476 loss)
I1211 06:18:49.083724 13088 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1211 06:18:55.335824 13088 solver.cpp:218] Iteration 104900 (15.9949 iter/s, 6.252s/100 iters), loss = 0.450168
I1211 06:18:55.335824 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 06:18:55.335824 13088 solver.cpp:237]     Train net output #1: loss = 0.450168 (* 1 = 0.450168 loss)
I1211 06:18:55.335824 13088 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1211 06:19:01.253674 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:19:01.499706 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105000.caffemodel
I1211 06:19:01.515205 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105000.solverstate
I1211 06:19:01.520206 13088 solver.cpp:330] Iteration 105000, Testing net (#0)
I1211 06:19:01.520206 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:19:02.870705 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:19:02.923708 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6742
I1211 06:19:02.923708 13088 solver.cpp:397]     Test net output #1: loss = 1.23189 (* 1 = 1.23189 loss)
I1211 06:19:02.983206 13088 solver.cpp:218] Iteration 105000 (13.0775 iter/s, 7.64675s/100 iters), loss = 0.254804
I1211 06:19:02.983206 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:19:02.983206 13088 solver.cpp:237]     Train net output #1: loss = 0.254804 (* 1 = 0.254804 loss)
I1211 06:19:02.983206 13088 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1211 06:19:09.255065 13088 solver.cpp:218] Iteration 105100 (15.9451 iter/s, 6.2715s/100 iters), loss = 0.288705
I1211 06:19:09.255065 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:19:09.255065 13088 solver.cpp:237]     Train net output #1: loss = 0.288705 (* 1 = 0.288705 loss)
I1211 06:19:09.255065 13088 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1211 06:19:15.623522 13088 solver.cpp:218] Iteration 105200 (15.7036 iter/s, 6.36798s/100 iters), loss = 0.262666
I1211 06:19:15.623522 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:19:15.623522 13088 solver.cpp:237]     Train net output #1: loss = 0.262666 (* 1 = 0.262666 loss)
I1211 06:19:15.623522 13088 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1211 06:19:21.940124 13088 solver.cpp:218] Iteration 105300 (15.8334 iter/s, 6.31576s/100 iters), loss = 0.29578
I1211 06:19:21.940124 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:19:21.940124 13088 solver.cpp:237]     Train net output #1: loss = 0.29578 (* 1 = 0.29578 loss)
I1211 06:19:21.940124 13088 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1211 06:19:28.305043 13088 solver.cpp:218] Iteration 105400 (15.7121 iter/s, 6.3645s/100 iters), loss = 0.430177
I1211 06:19:28.305043 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:19:28.305043 13088 solver.cpp:237]     Train net output #1: loss = 0.430177 (* 1 = 0.430177 loss)
I1211 06:19:28.305043 13088 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1211 06:19:34.301641 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:19:34.546140 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105500.caffemodel
I1211 06:19:34.562139 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_105500.solverstate
I1211 06:19:34.567656 13088 solver.cpp:330] Iteration 105500, Testing net (#0)
I1211 06:19:34.567656 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:19:35.944645 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:19:35.998761 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6768
I1211 06:19:35.998761 13088 solver.cpp:397]     Test net output #1: loss = 1.22734 (* 1 = 1.22734 loss)
I1211 06:19:36.059761 13088 solver.cpp:218] Iteration 105500 (12.8964 iter/s, 7.75409s/100 iters), loss = 0.2702
I1211 06:19:36.059761 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:19:36.059761 13088 solver.cpp:237]     Train net output #1: loss = 0.2702 (* 1 = 0.2702 loss)
I1211 06:19:36.059761 13088 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1211 06:19:42.403307 13088 solver.cpp:218] Iteration 105600 (15.7645 iter/s, 6.34336s/100 iters), loss = 0.362932
I1211 06:19:42.403307 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:19:42.403307 13088 solver.cpp:237]     Train net output #1: loss = 0.362932 (* 1 = 0.362932 loss)
I1211 06:19:42.403307 13088 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1211 06:19:48.708636 13088 solver.cpp:218] Iteration 105700 (15.8607 iter/s, 6.3049s/100 iters), loss = 0.281114
I1211 06:19:48.708636 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 06:19:48.708636 13088 solver.cpp:237]     Train net output #1: loss = 0.281114 (* 1 = 0.281114 loss)
I1211 06:19:48.708636 13088 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1211 06:19:55.024915 13088 solver.cpp:218] Iteration 105800 (15.8338 iter/s, 6.3156s/100 iters), loss = 0.345386
I1211 06:19:55.024915 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:19:55.024915 13088 solver.cpp:237]     Train net output #1: loss = 0.345386 (* 1 = 0.345386 loss)
I1211 06:19:55.024915 13088 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1211 06:20:01.308115 13088 solver.cpp:218] Iteration 105900 (15.9169 iter/s, 6.28265s/100 iters), loss = 0.334408
I1211 06:20:01.308115 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:20:01.308115 13088 solver.cpp:237]     Train net output #1: loss = 0.334409 (* 1 = 0.334409 loss)
I1211 06:20:01.308115 13088 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1211 06:20:07.286180 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:20:07.533680 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106000.caffemodel
I1211 06:20:07.549682 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106000.solverstate
I1211 06:20:07.555181 13088 solver.cpp:330] Iteration 106000, Testing net (#0)
I1211 06:20:07.555181 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:20:08.918680 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:20:08.972681 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6724
I1211 06:20:08.972681 13088 solver.cpp:397]     Test net output #1: loss = 1.23213 (* 1 = 1.23213 loss)
I1211 06:20:09.032680 13088 solver.cpp:218] Iteration 106000 (12.9464 iter/s, 7.72418s/100 iters), loss = 0.261487
I1211 06:20:09.032680 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:20:09.032680 13088 solver.cpp:237]     Train net output #1: loss = 0.261487 (* 1 = 0.261487 loss)
I1211 06:20:09.032680 13088 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1211 06:20:15.294461 13088 solver.cpp:218] Iteration 106100 (15.9711 iter/s, 6.26132s/100 iters), loss = 0.404584
I1211 06:20:15.294461 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:20:15.294461 13088 solver.cpp:237]     Train net output #1: loss = 0.404584 (* 1 = 0.404584 loss)
I1211 06:20:15.294461 13088 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1211 06:20:21.599958 13088 solver.cpp:218] Iteration 106200 (15.8601 iter/s, 6.30512s/100 iters), loss = 0.264651
I1211 06:20:21.600461 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:20:21.600461 13088 solver.cpp:237]     Train net output #1: loss = 0.264651 (* 1 = 0.264651 loss)
I1211 06:20:21.600461 13088 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1211 06:20:27.865418 13088 solver.cpp:218] Iteration 106300 (15.9618 iter/s, 6.26495s/100 iters), loss = 0.379142
I1211 06:20:27.865918 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:20:27.865918 13088 solver.cpp:237]     Train net output #1: loss = 0.379142 (* 1 = 0.379142 loss)
I1211 06:20:27.865918 13088 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1211 06:20:34.119626 13088 solver.cpp:218] Iteration 106400 (15.9911 iter/s, 6.25346s/100 iters), loss = 0.376742
I1211 06:20:34.119626 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:20:34.119626 13088 solver.cpp:237]     Train net output #1: loss = 0.376743 (* 1 = 0.376743 loss)
I1211 06:20:34.119626 13088 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1211 06:20:40.065924 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:20:40.311424 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106500.caffemodel
I1211 06:20:40.328927 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_106500.solverstate
I1211 06:20:40.333425 13088 solver.cpp:330] Iteration 106500, Testing net (#0)
I1211 06:20:40.333927 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:20:41.688927 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:20:41.742439 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6779
I1211 06:20:41.742439 13088 solver.cpp:397]     Test net output #1: loss = 1.23339 (* 1 = 1.23339 loss)
I1211 06:20:41.802436 13088 solver.cpp:218] Iteration 106500 (13.0171 iter/s, 7.68219s/100 iters), loss = 0.33888
I1211 06:20:41.802436 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:20:41.802436 13088 solver.cpp:237]     Train net output #1: loss = 0.33888 (* 1 = 0.33888 loss)
I1211 06:20:41.802436 13088 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1211 06:20:48.066295 13088 solver.cpp:218] Iteration 106600 (15.9658 iter/s, 6.26339s/100 iters), loss = 0.420344
I1211 06:20:48.066295 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:20:48.066295 13088 solver.cpp:237]     Train net output #1: loss = 0.420344 (* 1 = 0.420344 loss)
I1211 06:20:48.066295 13088 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1211 06:20:54.313601 13088 solver.cpp:218] Iteration 106700 (16.0077 iter/s, 6.24701s/100 iters), loss = 0.284541
I1211 06:20:54.313601 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:20:54.313601 13088 solver.cpp:237]     Train net output #1: loss = 0.284541 (* 1 = 0.284541 loss)
I1211 06:20:54.313601 13088 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1211 06:21:00.559356 13088 solver.cpp:218] Iteration 106800 (16.012 iter/s, 6.24533s/100 iters), loss = 0.357162
I1211 06:21:00.559356 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:21:00.559356 13088 solver.cpp:237]     Train net output #1: loss = 0.357162 (* 1 = 0.357162 loss)
I1211 06:21:00.559356 13088 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1211 06:21:06.804581 13088 solver.cpp:218] Iteration 106900 (16.0143 iter/s, 6.24443s/100 iters), loss = 0.413183
I1211 06:21:06.804581 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:21:06.804581 13088 solver.cpp:237]     Train net output #1: loss = 0.413183 (* 1 = 0.413183 loss)
I1211 06:21:06.804581 13088 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1211 06:21:12.752055 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:21:12.999054 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107000.caffemodel
I1211 06:21:13.015053 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107000.solverstate
I1211 06:21:13.021553 13088 solver.cpp:330] Iteration 107000, Testing net (#0)
I1211 06:21:13.021553 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:21:14.380080 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:21:14.433584 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6759
I1211 06:21:14.433584 13088 solver.cpp:397]     Test net output #1: loss = 1.22424 (* 1 = 1.22424 loss)
I1211 06:21:14.493082 13088 solver.cpp:218] Iteration 107000 (13.0072 iter/s, 7.68802s/100 iters), loss = 0.329725
I1211 06:21:14.493082 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:21:14.493082 13088 solver.cpp:237]     Train net output #1: loss = 0.329725 (* 1 = 0.329725 loss)
I1211 06:21:14.493082 13088 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1211 06:21:20.762197 13088 solver.cpp:218] Iteration 107100 (15.9526 iter/s, 6.26857s/100 iters), loss = 0.408047
I1211 06:21:20.762197 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:21:20.762197 13088 solver.cpp:237]     Train net output #1: loss = 0.408047 (* 1 = 0.408047 loss)
I1211 06:21:20.762197 13088 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1211 06:21:27.096931 13088 solver.cpp:218] Iteration 107200 (15.7871 iter/s, 6.33427s/100 iters), loss = 0.352252
I1211 06:21:27.096931 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:21:27.096931 13088 solver.cpp:237]     Train net output #1: loss = 0.352252 (* 1 = 0.352252 loss)
I1211 06:21:27.096931 13088 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1211 06:21:33.340457 13088 solver.cpp:218] Iteration 107300 (16.0179 iter/s, 6.24303s/100 iters), loss = 0.41356
I1211 06:21:33.340457 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:21:33.340457 13088 solver.cpp:237]     Train net output #1: loss = 0.41356 (* 1 = 0.41356 loss)
I1211 06:21:33.340457 13088 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1211 06:21:39.587460 13088 solver.cpp:218] Iteration 107400 (16.0095 iter/s, 6.24628s/100 iters), loss = 0.438214
I1211 06:21:39.587460 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:21:39.587460 13088 solver.cpp:237]     Train net output #1: loss = 0.438214 (* 1 = 0.438214 loss)
I1211 06:21:39.587460 13088 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1211 06:21:45.576119 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:21:45.839119 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107500.caffemodel
I1211 06:21:45.859622 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_107500.solverstate
I1211 06:21:45.865121 13088 solver.cpp:330] Iteration 107500, Testing net (#0)
I1211 06:21:45.865620 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:21:47.230638 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:21:47.283635 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6762
I1211 06:21:47.283635 13088 solver.cpp:397]     Test net output #1: loss = 1.2316 (* 1 = 1.2316 loss)
I1211 06:21:47.343119 13088 solver.cpp:218] Iteration 107500 (12.894 iter/s, 7.75553s/100 iters), loss = 0.255131
I1211 06:21:47.343619 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:21:47.343619 13088 solver.cpp:237]     Train net output #1: loss = 0.255131 (* 1 = 0.255131 loss)
I1211 06:21:47.343619 13088 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1211 06:21:53.590286 13088 solver.cpp:218] Iteration 107600 (16.0092 iter/s, 6.24643s/100 iters), loss = 0.308809
I1211 06:21:53.590286 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:21:53.590286 13088 solver.cpp:237]     Train net output #1: loss = 0.308809 (* 1 = 0.308809 loss)
I1211 06:21:53.590286 13088 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1211 06:21:59.871117 13088 solver.cpp:218] Iteration 107700 (15.9229 iter/s, 6.28026s/100 iters), loss = 0.291648
I1211 06:21:59.871117 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:21:59.871117 13088 solver.cpp:237]     Train net output #1: loss = 0.291648 (* 1 = 0.291648 loss)
I1211 06:21:59.871117 13088 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1211 06:22:06.130147 13088 solver.cpp:218] Iteration 107800 (15.9775 iter/s, 6.25882s/100 iters), loss = 0.321222
I1211 06:22:06.130147 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:22:06.130147 13088 solver.cpp:237]     Train net output #1: loss = 0.321222 (* 1 = 0.321222 loss)
I1211 06:22:06.130147 13088 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1211 06:22:12.395397 13088 solver.cpp:218] Iteration 107900 (15.9629 iter/s, 6.26453s/100 iters), loss = 0.433821
I1211 06:22:12.395397 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 06:22:12.395397 13088 solver.cpp:237]     Train net output #1: loss = 0.433821 (* 1 = 0.433821 loss)
I1211 06:22:12.395397 13088 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1211 06:22:18.381224 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:22:18.628767 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108000.caffemodel
I1211 06:22:18.644768 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108000.solverstate
I1211 06:22:18.649781 13088 solver.cpp:330] Iteration 108000, Testing net (#0)
I1211 06:22:18.650265 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:22:20.015838 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:22:20.069335 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6751
I1211 06:22:20.069335 13088 solver.cpp:397]     Test net output #1: loss = 1.23719 (* 1 = 1.23719 loss)
I1211 06:22:20.129341 13088 solver.cpp:218] Iteration 108000 (12.9308 iter/s, 7.73348s/100 iters), loss = 0.255647
I1211 06:22:20.129341 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 06:22:20.129341 13088 solver.cpp:237]     Train net output #1: loss = 0.255647 (* 1 = 0.255647 loss)
I1211 06:22:20.129341 13088 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1211 06:22:26.374994 13088 solver.cpp:218] Iteration 108100 (16.0121 iter/s, 6.24529s/100 iters), loss = 0.341182
I1211 06:22:26.374994 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:22:26.374994 13088 solver.cpp:237]     Train net output #1: loss = 0.341182 (* 1 = 0.341182 loss)
I1211 06:22:26.374994 13088 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1211 06:22:32.617465 13088 solver.cpp:218] Iteration 108200 (16.0204 iter/s, 6.24204s/100 iters), loss = 0.324951
I1211 06:22:32.617465 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:22:32.617465 13088 solver.cpp:237]     Train net output #1: loss = 0.324951 (* 1 = 0.324951 loss)
I1211 06:22:32.617465 13088 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1211 06:22:38.870939 13088 solver.cpp:218] Iteration 108300 (15.9928 iter/s, 6.25281s/100 iters), loss = 0.28961
I1211 06:22:38.870939 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:22:38.870939 13088 solver.cpp:237]     Train net output #1: loss = 0.28961 (* 1 = 0.28961 loss)
I1211 06:22:38.870939 13088 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1211 06:22:45.125843 13088 solver.cpp:218] Iteration 108400 (15.989 iter/s, 6.25432s/100 iters), loss = 0.342384
I1211 06:22:45.125843 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:22:45.125843 13088 solver.cpp:237]     Train net output #1: loss = 0.342384 (* 1 = 0.342384 loss)
I1211 06:22:45.125843 13088 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1211 06:22:51.060531 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:22:51.309032 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108500.caffemodel
I1211 06:22:51.324546 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_108500.solverstate
I1211 06:22:51.329546 13088 solver.cpp:330] Iteration 108500, Testing net (#0)
I1211 06:22:51.329546 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:22:52.686034 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:22:52.739033 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6787
I1211 06:22:52.739033 13088 solver.cpp:397]     Test net output #1: loss = 1.23628 (* 1 = 1.23628 loss)
I1211 06:22:52.798033 13088 solver.cpp:218] Iteration 108500 (13.0347 iter/s, 7.67182s/100 iters), loss = 0.255771
I1211 06:22:52.798033 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:22:52.798033 13088 solver.cpp:237]     Train net output #1: loss = 0.255771 (* 1 = 0.255771 loss)
I1211 06:22:52.798033 13088 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1211 06:22:59.053642 13088 solver.cpp:218] Iteration 108600 (15.9871 iter/s, 6.25505s/100 iters), loss = 0.347051
I1211 06:22:59.053642 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:22:59.053642 13088 solver.cpp:237]     Train net output #1: loss = 0.347051 (* 1 = 0.347051 loss)
I1211 06:22:59.053642 13088 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1211 06:23:05.299033 13088 solver.cpp:218] Iteration 108700 (16.0127 iter/s, 6.24503s/100 iters), loss = 0.303985
I1211 06:23:05.299033 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:23:05.299033 13088 solver.cpp:237]     Train net output #1: loss = 0.303985 (* 1 = 0.303985 loss)
I1211 06:23:05.299033 13088 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1211 06:23:11.543822 13088 solver.cpp:218] Iteration 108800 (16.0147 iter/s, 6.24428s/100 iters), loss = 0.389374
I1211 06:23:11.543822 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 06:23:11.543822 13088 solver.cpp:237]     Train net output #1: loss = 0.389375 (* 1 = 0.389375 loss)
I1211 06:23:11.543822 13088 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1211 06:23:17.822926 13088 solver.cpp:218] Iteration 108900 (15.9268 iter/s, 6.27873s/100 iters), loss = 0.371187
I1211 06:23:17.822926 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:23:17.822926 13088 solver.cpp:237]     Train net output #1: loss = 0.371187 (* 1 = 0.371187 loss)
I1211 06:23:17.822926 13088 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1211 06:23:23.816293 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:23:24.062325 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109000.caffemodel
I1211 06:23:24.079327 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109000.solverstate
I1211 06:23:24.085327 13088 solver.cpp:330] Iteration 109000, Testing net (#0)
I1211 06:23:24.085327 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:23:25.443825 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:23:25.497326 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6779
I1211 06:23:25.497326 13088 solver.cpp:397]     Test net output #1: loss = 1.23632 (* 1 = 1.23632 loss)
I1211 06:23:25.557826 13088 solver.cpp:218] Iteration 109000 (12.9295 iter/s, 7.73424s/100 iters), loss = 0.28673
I1211 06:23:25.557826 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:23:25.557826 13088 solver.cpp:237]     Train net output #1: loss = 0.28673 (* 1 = 0.28673 loss)
I1211 06:23:25.557826 13088 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1211 06:23:31.853193 13088 solver.cpp:218] Iteration 109100 (15.8856 iter/s, 6.29502s/100 iters), loss = 0.366858
I1211 06:23:31.853193 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:23:31.853193 13088 solver.cpp:237]     Train net output #1: loss = 0.366858 (* 1 = 0.366858 loss)
I1211 06:23:31.853193 13088 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1211 06:23:38.109803 13088 solver.cpp:218] Iteration 109200 (15.985 iter/s, 6.25585s/100 iters), loss = 0.290458
I1211 06:23:38.109803 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:23:38.109803 13088 solver.cpp:237]     Train net output #1: loss = 0.290458 (* 1 = 0.290458 loss)
I1211 06:23:38.109803 13088 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1211 06:23:44.423018 13088 solver.cpp:218] Iteration 109300 (15.841 iter/s, 6.31272s/100 iters), loss = 0.319208
I1211 06:23:44.423018 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:23:44.423018 13088 solver.cpp:237]     Train net output #1: loss = 0.319208 (* 1 = 0.319208 loss)
I1211 06:23:44.423018 13088 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1211 06:23:50.695514 13088 solver.cpp:218] Iteration 109400 (15.9438 iter/s, 6.27201s/100 iters), loss = 0.409334
I1211 06:23:50.695514 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:23:50.695514 13088 solver.cpp:237]     Train net output #1: loss = 0.409335 (* 1 = 0.409335 loss)
I1211 06:23:50.695514 13088 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1211 06:23:56.683691 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:23:56.935688 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109500.caffemodel
I1211 06:23:56.952189 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_109500.solverstate
I1211 06:23:56.957188 13088 solver.cpp:330] Iteration 109500, Testing net (#0)
I1211 06:23:56.957188 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:23:58.327215 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:23:58.380713 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6758
I1211 06:23:58.380713 13088 solver.cpp:397]     Test net output #1: loss = 1.24532 (* 1 = 1.24532 loss)
I1211 06:23:58.439712 13088 solver.cpp:218] Iteration 109500 (12.9133 iter/s, 7.74395s/100 iters), loss = 0.228782
I1211 06:23:58.439712 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 06:23:58.439712 13088 solver.cpp:237]     Train net output #1: loss = 0.228782 (* 1 = 0.228782 loss)
I1211 06:23:58.439712 13088 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1211 06:24:04.680548 13088 solver.cpp:218] Iteration 109600 (16.0256 iter/s, 6.24002s/100 iters), loss = 0.354666
I1211 06:24:04.680548 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:24:04.680548 13088 solver.cpp:237]     Train net output #1: loss = 0.354666 (* 1 = 0.354666 loss)
I1211 06:24:04.680548 13088 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1211 06:24:10.920956 13088 solver.cpp:218] Iteration 109700 (16.0256 iter/s, 6.24001s/100 iters), loss = 0.233155
I1211 06:24:10.920956 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1211 06:24:10.920956 13088 solver.cpp:237]     Train net output #1: loss = 0.233155 (* 1 = 0.233155 loss)
I1211 06:24:10.920956 13088 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1211 06:24:17.191555 13088 solver.cpp:218] Iteration 109800 (15.9485 iter/s, 6.27017s/100 iters), loss = 0.319498
I1211 06:24:17.191555 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:24:17.191555 13088 solver.cpp:237]     Train net output #1: loss = 0.319498 (* 1 = 0.319498 loss)
I1211 06:24:17.191555 13088 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1211 06:24:23.522217 13088 solver.cpp:218] Iteration 109900 (15.7972 iter/s, 6.33025s/100 iters), loss = 0.407611
I1211 06:24:23.522217 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 06:24:23.522217 13088 solver.cpp:237]     Train net output #1: loss = 0.407611 (* 1 = 0.407611 loss)
I1211 06:24:23.522217 13088 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1211 06:24:29.475425 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:24:29.721714 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110000.caffemodel
I1211 06:24:29.737711 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110000.solverstate
I1211 06:24:29.742210 13088 solver.cpp:330] Iteration 110000, Testing net (#0)
I1211 06:24:29.742210 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:24:31.103516 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:24:31.157516 13088 solver.cpp:397]     Test net output #0: accuracy = 0.675
I1211 06:24:31.157516 13088 solver.cpp:397]     Test net output #1: loss = 1.24412 (* 1 = 1.24412 loss)
I1211 06:24:31.217034 13088 solver.cpp:218] Iteration 110000 (12.9967 iter/s, 7.69425s/100 iters), loss = 0.30802
I1211 06:24:31.217034 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:24:31.217034 13088 solver.cpp:237]     Train net output #1: loss = 0.30802 (* 1 = 0.30802 loss)
I1211 06:24:31.217034 13088 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1211 06:24:37.515038 13088 solver.cpp:218] Iteration 110100 (15.8799 iter/s, 6.29729s/100 iters), loss = 0.352398
I1211 06:24:37.515038 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:24:37.515038 13088 solver.cpp:237]     Train net output #1: loss = 0.352398 (* 1 = 0.352398 loss)
I1211 06:24:37.515038 13088 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1211 06:24:43.818593 13088 solver.cpp:218] Iteration 110200 (15.8654 iter/s, 6.30302s/100 iters), loss = 0.259798
I1211 06:24:43.818593 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 06:24:43.818593 13088 solver.cpp:237]     Train net output #1: loss = 0.259798 (* 1 = 0.259798 loss)
I1211 06:24:43.818593 13088 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1211 06:24:50.086980 13088 solver.cpp:218] Iteration 110300 (15.9543 iter/s, 6.26791s/100 iters), loss = 0.351616
I1211 06:24:50.086980 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:24:50.086980 13088 solver.cpp:237]     Train net output #1: loss = 0.351616 (* 1 = 0.351616 loss)
I1211 06:24:50.086980 13088 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1211 06:24:56.334987 13088 solver.cpp:218] Iteration 110400 (16.0061 iter/s, 6.24761s/100 iters), loss = 0.388216
I1211 06:24:56.335479 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:24:56.335479 13088 solver.cpp:237]     Train net output #1: loss = 0.388217 (* 1 = 0.388217 loss)
I1211 06:24:56.335479 13088 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1211 06:25:02.302271 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:25:02.548774 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110500.caffemodel
I1211 06:25:02.566292 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_110500.solverstate
I1211 06:25:02.571286 13088 solver.cpp:330] Iteration 110500, Testing net (#0)
I1211 06:25:02.571286 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:25:03.944773 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:25:03.998291 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6728
I1211 06:25:03.998790 13088 solver.cpp:397]     Test net output #1: loss = 1.25844 (* 1 = 1.25844 loss)
I1211 06:25:04.061990 13088 solver.cpp:218] Iteration 110500 (12.9431 iter/s, 7.7261s/100 iters), loss = 0.262657
I1211 06:25:04.061990 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:25:04.061990 13088 solver.cpp:237]     Train net output #1: loss = 0.262657 (* 1 = 0.262657 loss)
I1211 06:25:04.061990 13088 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1211 06:25:10.365551 13088 solver.cpp:218] Iteration 110600 (15.8652 iter/s, 6.30311s/100 iters), loss = 0.272146
I1211 06:25:10.365551 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:25:10.365551 13088 solver.cpp:237]     Train net output #1: loss = 0.272146 (* 1 = 0.272146 loss)
I1211 06:25:10.365551 13088 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1211 06:25:16.684044 13088 solver.cpp:218] Iteration 110700 (15.828 iter/s, 6.31792s/100 iters), loss = 0.224215
I1211 06:25:16.684044 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 06:25:16.684044 13088 solver.cpp:237]     Train net output #1: loss = 0.224215 (* 1 = 0.224215 loss)
I1211 06:25:16.684044 13088 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1211 06:25:22.935353 13088 solver.cpp:218] Iteration 110800 (15.9971 iter/s, 6.25114s/100 iters), loss = 0.378273
I1211 06:25:22.935353 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:25:22.935353 13088 solver.cpp:237]     Train net output #1: loss = 0.378273 (* 1 = 0.378273 loss)
I1211 06:25:22.935353 13088 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1211 06:25:29.180943 13088 solver.cpp:218] Iteration 110900 (16.0134 iter/s, 6.24478s/100 iters), loss = 0.36304
I1211 06:25:29.180943 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:25:29.180943 13088 solver.cpp:237]     Train net output #1: loss = 0.36304 (* 1 = 0.36304 loss)
I1211 06:25:29.180943 13088 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1211 06:25:35.165881 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:25:35.411381 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111000.caffemodel
I1211 06:25:35.427386 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111000.solverstate
I1211 06:25:35.432381 13088 solver.cpp:330] Iteration 111000, Testing net (#0)
I1211 06:25:35.432381 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:25:36.780912 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:25:36.833444 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6735
I1211 06:25:36.833945 13088 solver.cpp:397]     Test net output #1: loss = 1.26029 (* 1 = 1.26029 loss)
I1211 06:25:36.893447 13088 solver.cpp:218] Iteration 111000 (12.9671 iter/s, 7.71183s/100 iters), loss = 0.27292
I1211 06:25:36.893447 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:25:36.893447 13088 solver.cpp:237]     Train net output #1: loss = 0.27292 (* 1 = 0.27292 loss)
I1211 06:25:36.893447 13088 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1211 06:25:43.133008 13088 solver.cpp:218] Iteration 111100 (16.0273 iter/s, 6.23936s/100 iters), loss = 0.285581
I1211 06:25:43.133008 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:25:43.133507 13088 solver.cpp:237]     Train net output #1: loss = 0.285581 (* 1 = 0.285581 loss)
I1211 06:25:43.133507 13088 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1211 06:25:49.377930 13088 solver.cpp:218] Iteration 111200 (16.0153 iter/s, 6.24403s/100 iters), loss = 0.294675
I1211 06:25:49.377930 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 06:25:49.377930 13088 solver.cpp:237]     Train net output #1: loss = 0.294675 (* 1 = 0.294675 loss)
I1211 06:25:49.377930 13088 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1211 06:25:55.616075 13088 solver.cpp:218] Iteration 111300 (16.0307 iter/s, 6.23805s/100 iters), loss = 0.334325
I1211 06:25:55.616575 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 06:25:55.616575 13088 solver.cpp:237]     Train net output #1: loss = 0.334325 (* 1 = 0.334325 loss)
I1211 06:25:55.616575 13088 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1211 06:26:01.865164 13088 solver.cpp:218] Iteration 111400 (16.004 iter/s, 6.24844s/100 iters), loss = 0.300784
I1211 06:26:01.865164 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 06:26:01.865164 13088 solver.cpp:237]     Train net output #1: loss = 0.300784 (* 1 = 0.300784 loss)
I1211 06:26:01.865164 13088 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1211 06:26:07.806422 12900 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:26:08.052419 13088 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111500.caffemodel
I1211 06:26:08.069420 13088 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_stridedconv_L15_v2_wnonlin_iter_111500.solverstate
I1211 06:26:08.073920 13088 solver.cpp:330] Iteration 111500, Testing net (#0)
I1211 06:26:08.074421 13088 net.cpp:676] Ignoring source layer accuracy_training
I1211 06:26:09.431422 12612 data_layer.cpp:73] Restarting data prefetching from start.
I1211 06:26:09.483922 13088 solver.cpp:397]     Test net output #0: accuracy = 0.6716
I1211 06:26:09.483922 13088 solver.cpp:397]     Test net output #1: loss = 1.26007 (* 1 = 1.26007 loss)
I1211 06:26:09.542932 13088 solver.cpp:218] Iteration 111500 (13.0252 iter/s, 7.67743s/100 iters), loss = 0.218306
I1211 06:26:09.543431 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 06:26:09.543431 13088 solver.cpp:237]     Train net output #1: loss = 0.218306 (* 1 = 0.218306 loss)
I1211 06:26:09.543431 13088 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1211 06:26:15.791734 13088 solver.cpp:218] Iteration 111600 (16.0048 iter/s, 6.24814s/100 iters), loss = 0.383834
I1211 06:26:15.791734 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:26:15.791734 13088 solver.cpp:237]     Train net output #1: loss = 0.383834 (* 1 = 0.383834 loss)
I1211 06:26:15.791734 13088 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1211 06:26:22.022727 13088 solver.cpp:218] Iteration 111700 (16.0502 iter/s, 6.23047s/100 iters), loss = 0.285559
I1211 06:26:22.022727 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 06:26:22.022727 13088 solver.cpp:237]     Train net output #1: loss = 0.285559 (* 1 = 0.285559 loss)
I1211 06:26:22.022727 13088 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1211 06:26:28.273247 13088 solver.cpp:218] Iteration 111800 (15.9992 iter/s, 6.25033s/100 iters), loss = 0.319849
I1211 06:26:28.273747 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 06:26:28.273747 13088 solver.cpp:237]     Train net output #1: loss = 0.319849 (* 1 = 0.319849 loss)
I1211 06:26:28.273747 13088 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1211 06:26:34.544034 13088 solver.cpp:218] Iteration 111900 (15.9483 iter/s, 6.27028s/100 iters), loss = 0.396261
I1211 06:26:34.544034 13088 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 06:26:34.544034 13088 solver.cpp:237]     Train net output #1: loss = 0.396261 (* 1 = 0.396261 loss)
I1211 06:26:34.544535 13088 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1211 06:26:40.522505 12900 data_layer.
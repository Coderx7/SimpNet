
G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.solverstate 
I1211 13:21:44.193120 12512 caffe.cpp:219] Using GPUs 0
I1211 13:21:44.380167 12512 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1211 13:21:44.686028 12512 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 13:21:44.704030 12512 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1211 13:21:44.704030 12512 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 13:21:44.705027 12512 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 13:21:44.705027 12512 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_added1
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_added2
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1211 13:21:44.705027 12512 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1211 13:21:44.706027 12512 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added1"
  type: "BatchNorm"
  bottom: "newconv_added1"
  top: "newconv_added1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added1"
  type: "Scale"
  bottom: "newconv_added1"
  top: "newconv_added1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added1"
  type: "ReLU"
  bottom: "newconv_added1"
  top: "newconv_added1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added2"
  type: "BatchNorm"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added2"
  type: "Scale"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added2"
  type: "ReLU"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 13:21:44.739544 12512 layer_factory.cpp:58] Creating layer cifar
I1211 13:21:44.742545 12512 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1211 13:21:44.743044 12512 net.cpp:84] Creating Layer cifar
I1211 13:21:44.743044 12512 net.cpp:380] cifar -> data
I1211 13:21:44.743044 12512 net.cpp:380] cifar -> label
I1211 13:21:44.744051 12512 data_layer.cpp:45] output data size: 100,3,32,32
I1211 13:21:44.755048 12512 net.cpp:122] Setting up cifar
I1211 13:21:44.755048 12512 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 13:21:44.755048 12512 net.cpp:129] Top shape: 100 (100)
I1211 13:21:44.755048 12512 net.cpp:137] Memory required for data: 1229200
I1211 13:21:44.755048 12512 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 13:21:44.755048 12512 net.cpp:84] Creating Layer label_cifar_1_split
I1211 13:21:44.755048 12512 net.cpp:406] label_cifar_1_split <- label
I1211 13:21:44.755048 12512 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 13:21:44.755048 12512 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 13:21:44.755048 12512 net.cpp:122] Setting up label_cifar_1_split
I1211 13:21:44.755048 12512 net.cpp:129] Top shape: 100 (100)
I1211 13:21:44.755048 12512 net.cpp:129] Top shape: 100 (100)
I1211 13:21:44.755048 12512 net.cpp:137] Memory required for data: 1230000
I1211 13:21:44.755048 12512 layer_factory.cpp:58] Creating layer conv1
I1211 13:21:44.755048 12512 net.cpp:84] Creating Layer conv1
I1211 13:21:44.755048 12512 net.cpp:406] conv1 <- data
I1211 13:21:44.755048 12512 net.cpp:380] conv1 -> conv1
I1211 13:21:44.756049 15104 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 13:21:45.025071 12512 net.cpp:122] Setting up conv1
I1211 13:21:45.025071 12512 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:21:45.025071 12512 net.cpp:137] Memory required for data: 13518000
I1211 13:21:45.025071 12512 layer_factory.cpp:58] Creating layer bn1
I1211 13:21:45.025071 12512 net.cpp:84] Creating Layer bn1
I1211 13:21:45.025071 12512 net.cpp:406] bn1 <- conv1
I1211 13:21:45.026070 12512 net.cpp:367] bn1 -> conv1 (in-place)
I1211 13:21:45.026070 12512 net.cpp:122] Setting up bn1
I1211 13:21:45.026070 12512 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:21:45.026070 12512 net.cpp:137] Memory required for data: 25806000
I1211 13:21:45.026070 12512 layer_factory.cpp:58] Creating layer scale1
I1211 13:21:45.026070 12512 net.cpp:84] Creating Layer scale1
I1211 13:21:45.026070 12512 net.cpp:406] scale1 <- conv1
I1211 13:21:45.026070 12512 net.cpp:367] scale1 -> conv1 (in-place)
I1211 13:21:45.026070 12512 layer_factory.cpp:58] Creating layer scale1
I1211 13:21:45.026070 12512 net.cpp:122] Setting up scale1
I1211 13:21:45.026070 12512 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:21:45.026070 12512 net.cpp:137] Memory required for data: 38094000
I1211 13:21:45.026070 12512 layer_factory.cpp:58] Creating layer relu1
I1211 13:21:45.026070 12512 net.cpp:84] Creating Layer relu1
I1211 13:21:45.026070 12512 net.cpp:406] relu1 <- conv1
I1211 13:21:45.026070 12512 net.cpp:367] relu1 -> conv1 (in-place)
I1211 13:21:45.026070 12512 net.cpp:122] Setting up relu1
I1211 13:21:45.026070 12512 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:21:45.026070 12512 net.cpp:137] Memory required for data: 50382000
I1211 13:21:45.026070 12512 layer_factory.cpp:58] Creating layer conv1_0
I1211 13:21:45.026070 12512 net.cpp:84] Creating Layer conv1_0
I1211 13:21:45.026070 12512 net.cpp:406] conv1_0 <- conv1
I1211 13:21:45.026070 12512 net.cpp:380] conv1_0 -> conv1_0
I1211 13:21:45.028070 12512 net.cpp:122] Setting up conv1_0
I1211 13:21:45.028070 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.028070 12512 net.cpp:137] Memory required for data: 66766000
I1211 13:21:45.028070 12512 layer_factory.cpp:58] Creating layer bn1_0
I1211 13:21:45.028070 12512 net.cpp:84] Creating Layer bn1_0
I1211 13:21:45.028070 12512 net.cpp:406] bn1_0 <- conv1_0
I1211 13:21:45.028070 12512 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 13:21:45.028070 12512 net.cpp:122] Setting up bn1_0
I1211 13:21:45.028070 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.028070 12512 net.cpp:137] Memory required for data: 83150000
I1211 13:21:45.028070 12512 layer_factory.cpp:58] Creating layer scale1_0
I1211 13:21:45.028070 12512 net.cpp:84] Creating Layer scale1_0
I1211 13:21:45.028070 12512 net.cpp:406] scale1_0 <- conv1_0
I1211 13:21:45.028070 12512 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 13:21:45.028070 12512 layer_factory.cpp:58] Creating layer scale1_0
I1211 13:21:45.029078 12512 net.cpp:122] Setting up scale1_0
I1211 13:21:45.029078 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.029078 12512 net.cpp:137] Memory required for data: 99534000
I1211 13:21:45.029078 12512 layer_factory.cpp:58] Creating layer relu1_0
I1211 13:21:45.029078 12512 net.cpp:84] Creating Layer relu1_0
I1211 13:21:45.029078 12512 net.cpp:406] relu1_0 <- conv1_0
I1211 13:21:45.029078 12512 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 13:21:45.029078 12512 net.cpp:122] Setting up relu1_0
I1211 13:21:45.029078 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.029078 12512 net.cpp:137] Memory required for data: 115918000
I1211 13:21:45.029078 12512 layer_factory.cpp:58] Creating layer conv2
I1211 13:21:45.029078 12512 net.cpp:84] Creating Layer conv2
I1211 13:21:45.029078 12512 net.cpp:406] conv2 <- conv1_0
I1211 13:21:45.029078 12512 net.cpp:380] conv2 -> conv2
I1211 13:21:45.030071 12512 net.cpp:122] Setting up conv2
I1211 13:21:45.030071 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.030071 12512 net.cpp:137] Memory required for data: 132302000
I1211 13:21:45.030071 12512 layer_factory.cpp:58] Creating layer bn2
I1211 13:21:45.030071 12512 net.cpp:84] Creating Layer bn2
I1211 13:21:45.030071 12512 net.cpp:406] bn2 <- conv2
I1211 13:21:45.030071 12512 net.cpp:367] bn2 -> conv2 (in-place)
I1211 13:21:45.030071 12512 net.cpp:122] Setting up bn2
I1211 13:21:45.030071 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.030071 12512 net.cpp:137] Memory required for data: 148686000
I1211 13:21:45.030071 12512 layer_factory.cpp:58] Creating layer scale2
I1211 13:21:45.030071 12512 net.cpp:84] Creating Layer scale2
I1211 13:21:45.030071 12512 net.cpp:406] scale2 <- conv2
I1211 13:21:45.030071 12512 net.cpp:367] scale2 -> conv2 (in-place)
I1211 13:21:45.030071 12512 layer_factory.cpp:58] Creating layer scale2
I1211 13:21:45.031070 12512 net.cpp:122] Setting up scale2
I1211 13:21:45.031070 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.031070 12512 net.cpp:137] Memory required for data: 165070000
I1211 13:21:45.031070 12512 layer_factory.cpp:58] Creating layer relu2
I1211 13:21:45.031070 12512 net.cpp:84] Creating Layer relu2
I1211 13:21:45.031070 12512 net.cpp:406] relu2 <- conv2
I1211 13:21:45.031070 12512 net.cpp:367] relu2 -> conv2 (in-place)
I1211 13:21:45.031070 12512 net.cpp:122] Setting up relu2
I1211 13:21:45.031070 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.031070 12512 net.cpp:137] Memory required for data: 181454000
I1211 13:21:45.031070 12512 layer_factory.cpp:58] Creating layer conv2_1
I1211 13:21:45.031070 12512 net.cpp:84] Creating Layer conv2_1
I1211 13:21:45.031070 12512 net.cpp:406] conv2_1 <- conv2
I1211 13:21:45.031070 12512 net.cpp:380] conv2_1 -> conv2_1
I1211 13:21:45.032070 12512 net.cpp:122] Setting up conv2_1
I1211 13:21:45.032070 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.032070 12512 net.cpp:137] Memory required for data: 197838000
I1211 13:21:45.032070 12512 layer_factory.cpp:58] Creating layer bn2_1
I1211 13:21:45.032070 12512 net.cpp:84] Creating Layer bn2_1
I1211 13:21:45.032070 12512 net.cpp:406] bn2_1 <- conv2_1
I1211 13:21:45.032070 12512 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 13:21:45.032070 12512 net.cpp:122] Setting up bn2_1
I1211 13:21:45.032070 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.032070 12512 net.cpp:137] Memory required for data: 214222000
I1211 13:21:45.032070 12512 layer_factory.cpp:58] Creating layer scale2_1
I1211 13:21:45.032070 12512 net.cpp:84] Creating Layer scale2_1
I1211 13:21:45.032070 12512 net.cpp:406] scale2_1 <- conv2_1
I1211 13:21:45.032070 12512 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 13:21:45.032070 12512 layer_factory.cpp:58] Creating layer scale2_1
I1211 13:21:45.032070 12512 net.cpp:122] Setting up scale2_1
I1211 13:21:45.032070 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.032070 12512 net.cpp:137] Memory required for data: 230606000
I1211 13:21:45.032070 12512 layer_factory.cpp:58] Creating layer relu2_1
I1211 13:21:45.032070 12512 net.cpp:84] Creating Layer relu2_1
I1211 13:21:45.032070 12512 net.cpp:406] relu2_1 <- conv2_1
I1211 13:21:45.032070 12512 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 13:21:45.033071 12512 net.cpp:122] Setting up relu2_1
I1211 13:21:45.033071 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.033071 12512 net.cpp:137] Memory required for data: 246990000
I1211 13:21:45.033071 12512 layer_factory.cpp:58] Creating layer conv2_2
I1211 13:21:45.033071 12512 net.cpp:84] Creating Layer conv2_2
I1211 13:21:45.033071 12512 net.cpp:406] conv2_2 <- conv2_1
I1211 13:21:45.033071 12512 net.cpp:380] conv2_2 -> conv2_2
I1211 13:21:45.035070 12512 net.cpp:122] Setting up conv2_2
I1211 13:21:45.035070 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.035070 12512 net.cpp:137] Memory required for data: 267470000
I1211 13:21:45.035070 12512 layer_factory.cpp:58] Creating layer bn2_2
I1211 13:21:45.035070 12512 net.cpp:84] Creating Layer bn2_2
I1211 13:21:45.035070 12512 net.cpp:406] bn2_2 <- conv2_2
I1211 13:21:45.035070 12512 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 13:21:45.035070 12512 net.cpp:122] Setting up bn2_2
I1211 13:21:45.035070 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.035070 12512 net.cpp:137] Memory required for data: 287950000
I1211 13:21:45.035070 12512 layer_factory.cpp:58] Creating layer scale2_2
I1211 13:21:45.035070 12512 net.cpp:84] Creating Layer scale2_2
I1211 13:21:45.035070 12512 net.cpp:406] scale2_2 <- conv2_2
I1211 13:21:45.035070 12512 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 13:21:45.035070 12512 layer_factory.cpp:58] Creating layer scale2_2
I1211 13:21:45.035070 12512 net.cpp:122] Setting up scale2_2
I1211 13:21:45.035070 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.035070 12512 net.cpp:137] Memory required for data: 308430000
I1211 13:21:45.035070 12512 layer_factory.cpp:58] Creating layer relu2_2
I1211 13:21:45.035070 12512 net.cpp:84] Creating Layer relu2_2
I1211 13:21:45.035070 12512 net.cpp:406] relu2_2 <- conv2_2
I1211 13:21:45.035070 12512 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 13:21:45.036072 12512 net.cpp:122] Setting up relu2_2
I1211 13:21:45.036072 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.036072 12512 net.cpp:137] Memory required for data: 328910000
I1211 13:21:45.036072 12512 layer_factory.cpp:58] Creating layer newconv_added1
I1211 13:21:45.036072 12512 net.cpp:84] Creating Layer newconv_added1
I1211 13:21:45.036072 12512 net.cpp:406] newconv_added1 <- conv2_2
I1211 13:21:45.036072 12512 net.cpp:380] newconv_added1 -> newconv_added1
I1211 13:21:45.037070 12512 net.cpp:122] Setting up newconv_added1
I1211 13:21:45.037070 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.037070 12512 net.cpp:137] Memory required for data: 349390000
I1211 13:21:45.037070 12512 layer_factory.cpp:58] Creating layer bn_added1
I1211 13:21:45.037070 12512 net.cpp:84] Creating Layer bn_added1
I1211 13:21:45.037070 12512 net.cpp:406] bn_added1 <- newconv_added1
I1211 13:21:45.037070 12512 net.cpp:367] bn_added1 -> newconv_added1 (in-place)
I1211 13:21:45.037070 12512 net.cpp:122] Setting up bn_added1
I1211 13:21:45.037070 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.037070 12512 net.cpp:137] Memory required for data: 369870000
I1211 13:21:45.037070 12512 layer_factory.cpp:58] Creating layer scale_added1
I1211 13:21:45.037070 12512 net.cpp:84] Creating Layer scale_added1
I1211 13:21:45.037070 12512 net.cpp:406] scale_added1 <- newconv_added1
I1211 13:21:45.037070 12512 net.cpp:367] scale_added1 -> newconv_added1 (in-place)
I1211 13:21:45.037070 12512 layer_factory.cpp:58] Creating layer scale_added1
I1211 13:21:45.037070 12512 net.cpp:122] Setting up scale_added1
I1211 13:21:45.037070 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.037070 12512 net.cpp:137] Memory required for data: 390350000
I1211 13:21:45.037070 12512 layer_factory.cpp:58] Creating layer relu_added1
I1211 13:21:45.037070 12512 net.cpp:84] Creating Layer relu_added1
I1211 13:21:45.037070 12512 net.cpp:406] relu_added1 <- newconv_added1
I1211 13:21:45.037070 12512 net.cpp:367] relu_added1 -> newconv_added1 (in-place)
I1211 13:21:45.038071 12512 net.cpp:122] Setting up relu_added1
I1211 13:21:45.038071 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.038071 12512 net.cpp:137] Memory required for data: 410830000
I1211 13:21:45.038071 12512 layer_factory.cpp:58] Creating layer pool2_1
I1211 13:21:45.038071 12512 net.cpp:84] Creating Layer pool2_1
I1211 13:21:45.038071 12512 net.cpp:406] pool2_1 <- newconv_added1
I1211 13:21:45.038071 12512 net.cpp:380] pool2_1 -> pool2_1
I1211 13:21:45.038071 12512 net.cpp:122] Setting up pool2_1
I1211 13:21:45.038071 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.038071 12512 net.cpp:137] Memory required for data: 415950000
I1211 13:21:45.038071 12512 layer_factory.cpp:58] Creating layer conv3
I1211 13:21:45.038071 12512 net.cpp:84] Creating Layer conv3
I1211 13:21:45.038071 12512 net.cpp:406] conv3 <- pool2_1
I1211 13:21:45.038071 12512 net.cpp:380] conv3 -> conv3
I1211 13:21:45.039577 12512 net.cpp:122] Setting up conv3
I1211 13:21:45.039577 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.039577 12512 net.cpp:137] Memory required for data: 421070000
I1211 13:21:45.039577 12512 layer_factory.cpp:58] Creating layer bn3
I1211 13:21:45.039577 12512 net.cpp:84] Creating Layer bn3
I1211 13:21:45.039577 12512 net.cpp:406] bn3 <- conv3
I1211 13:21:45.039577 12512 net.cpp:367] bn3 -> conv3 (in-place)
I1211 13:21:45.039577 12512 net.cpp:122] Setting up bn3
I1211 13:21:45.039577 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.039577 12512 net.cpp:137] Memory required for data: 426190000
I1211 13:21:45.039577 12512 layer_factory.cpp:58] Creating layer scale3
I1211 13:21:45.039577 12512 net.cpp:84] Creating Layer scale3
I1211 13:21:45.040076 12512 net.cpp:406] scale3 <- conv3
I1211 13:21:45.040076 12512 net.cpp:367] scale3 -> conv3 (in-place)
I1211 13:21:45.040076 12512 layer_factory.cpp:58] Creating layer scale3
I1211 13:21:45.040076 12512 net.cpp:122] Setting up scale3
I1211 13:21:45.040076 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.040076 12512 net.cpp:137] Memory required for data: 431310000
I1211 13:21:45.040076 12512 layer_factory.cpp:58] Creating layer relu3
I1211 13:21:45.040076 12512 net.cpp:84] Creating Layer relu3
I1211 13:21:45.040076 12512 net.cpp:406] relu3 <- conv3
I1211 13:21:45.040076 12512 net.cpp:367] relu3 -> conv3 (in-place)
I1211 13:21:45.040076 12512 net.cpp:122] Setting up relu3
I1211 13:21:45.040076 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.040076 12512 net.cpp:137] Memory required for data: 436430000
I1211 13:21:45.040576 12512 layer_factory.cpp:58] Creating layer conv3_1
I1211 13:21:45.040576 12512 net.cpp:84] Creating Layer conv3_1
I1211 13:21:45.040576 12512 net.cpp:406] conv3_1 <- conv3
I1211 13:21:45.040576 12512 net.cpp:380] conv3_1 -> conv3_1
I1211 13:21:45.041576 12512 net.cpp:122] Setting up conv3_1
I1211 13:21:45.041576 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.041576 12512 net.cpp:137] Memory required for data: 441550000
I1211 13:21:45.041576 12512 layer_factory.cpp:58] Creating layer bn3_1
I1211 13:21:45.041576 12512 net.cpp:84] Creating Layer bn3_1
I1211 13:21:45.041576 12512 net.cpp:406] bn3_1 <- conv3_1
I1211 13:21:45.041576 12512 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 13:21:45.042076 12512 net.cpp:122] Setting up bn3_1
I1211 13:21:45.042076 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.042076 12512 net.cpp:137] Memory required for data: 446670000
I1211 13:21:45.042076 12512 layer_factory.cpp:58] Creating layer scale3_1
I1211 13:21:45.042076 12512 net.cpp:84] Creating Layer scale3_1
I1211 13:21:45.042076 12512 net.cpp:406] scale3_1 <- conv3_1
I1211 13:21:45.042076 12512 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 13:21:45.042076 12512 layer_factory.cpp:58] Creating layer scale3_1
I1211 13:21:45.042076 12512 net.cpp:122] Setting up scale3_1
I1211 13:21:45.042076 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.042076 12512 net.cpp:137] Memory required for data: 451790000
I1211 13:21:45.042076 12512 layer_factory.cpp:58] Creating layer relu3_1
I1211 13:21:45.042076 12512 net.cpp:84] Creating Layer relu3_1
I1211 13:21:45.042076 12512 net.cpp:406] relu3_1 <- conv3_1
I1211 13:21:45.042076 12512 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 13:21:45.042577 12512 net.cpp:122] Setting up relu3_1
I1211 13:21:45.042577 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.042577 12512 net.cpp:137] Memory required for data: 456910000
I1211 13:21:45.042577 12512 layer_factory.cpp:58] Creating layer conv4
I1211 13:21:45.042577 12512 net.cpp:84] Creating Layer conv4
I1211 13:21:45.042577 12512 net.cpp:406] conv4 <- conv3_1
I1211 13:21:45.042577 12512 net.cpp:380] conv4 -> conv4
I1211 13:21:45.043577 12512 net.cpp:122] Setting up conv4
I1211 13:21:45.043577 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.043577 12512 net.cpp:137] Memory required for data: 462030000
I1211 13:21:45.043577 12512 layer_factory.cpp:58] Creating layer bn4
I1211 13:21:45.043577 12512 net.cpp:84] Creating Layer bn4
I1211 13:21:45.043577 12512 net.cpp:406] bn4 <- conv4
I1211 13:21:45.043577 12512 net.cpp:367] bn4 -> conv4 (in-place)
I1211 13:21:45.044076 12512 net.cpp:122] Setting up bn4
I1211 13:21:45.044076 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.044076 12512 net.cpp:137] Memory required for data: 467150000
I1211 13:21:45.044076 12512 layer_factory.cpp:58] Creating layer scale4
I1211 13:21:45.044076 12512 net.cpp:84] Creating Layer scale4
I1211 13:21:45.044076 12512 net.cpp:406] scale4 <- conv4
I1211 13:21:45.044076 12512 net.cpp:367] scale4 -> conv4 (in-place)
I1211 13:21:45.044076 12512 layer_factory.cpp:58] Creating layer scale4
I1211 13:21:45.044076 12512 net.cpp:122] Setting up scale4
I1211 13:21:45.044076 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.044076 12512 net.cpp:137] Memory required for data: 472270000
I1211 13:21:45.044076 12512 layer_factory.cpp:58] Creating layer relu4
I1211 13:21:45.044076 12512 net.cpp:84] Creating Layer relu4
I1211 13:21:45.044076 12512 net.cpp:406] relu4 <- conv4
I1211 13:21:45.044076 12512 net.cpp:367] relu4 -> conv4 (in-place)
I1211 13:21:45.044584 12512 net.cpp:122] Setting up relu4
I1211 13:21:45.044584 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.044584 12512 net.cpp:137] Memory required for data: 477390000
I1211 13:21:45.044584 12512 layer_factory.cpp:58] Creating layer conv4_1
I1211 13:21:45.044584 12512 net.cpp:84] Creating Layer conv4_1
I1211 13:21:45.044584 12512 net.cpp:406] conv4_1 <- conv4
I1211 13:21:45.044584 12512 net.cpp:380] conv4_1 -> conv4_1
I1211 13:21:45.046077 12512 net.cpp:122] Setting up conv4_1
I1211 13:21:45.046077 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.046077 12512 net.cpp:137] Memory required for data: 482510000
I1211 13:21:45.046077 12512 layer_factory.cpp:58] Creating layer bn4_1
I1211 13:21:45.046077 12512 net.cpp:84] Creating Layer bn4_1
I1211 13:21:45.046077 12512 net.cpp:406] bn4_1 <- conv4_1
I1211 13:21:45.046077 12512 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 13:21:45.046576 12512 net.cpp:122] Setting up bn4_1
I1211 13:21:45.046576 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.046576 12512 net.cpp:137] Memory required for data: 487630000
I1211 13:21:45.046576 12512 layer_factory.cpp:58] Creating layer scale4_1
I1211 13:21:45.046576 12512 net.cpp:84] Creating Layer scale4_1
I1211 13:21:45.046576 12512 net.cpp:406] scale4_1 <- conv4_1
I1211 13:21:45.046576 12512 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 13:21:45.046576 12512 layer_factory.cpp:58] Creating layer scale4_1
I1211 13:21:45.046576 12512 net.cpp:122] Setting up scale4_1
I1211 13:21:45.046576 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.046576 12512 net.cpp:137] Memory required for data: 492750000
I1211 13:21:45.046576 12512 layer_factory.cpp:58] Creating layer relu4_1
I1211 13:21:45.046576 12512 net.cpp:84] Creating Layer relu4_1
I1211 13:21:45.046576 12512 net.cpp:406] relu4_1 <- conv4_1
I1211 13:21:45.046576 12512 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 13:21:45.047076 12512 net.cpp:122] Setting up relu4_1
I1211 13:21:45.047076 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.047076 12512 net.cpp:137] Memory required for data: 497870000
I1211 13:21:45.047076 12512 layer_factory.cpp:58] Creating layer conv4_2
I1211 13:21:45.047076 12512 net.cpp:84] Creating Layer conv4_2
I1211 13:21:45.047076 12512 net.cpp:406] conv4_2 <- conv4_1
I1211 13:21:45.047076 12512 net.cpp:380] conv4_2 -> conv4_2
I1211 13:21:45.048076 12512 net.cpp:122] Setting up conv4_2
I1211 13:21:45.048076 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.048076 12512 net.cpp:137] Memory required for data: 503809200
I1211 13:21:45.048076 12512 layer_factory.cpp:58] Creating layer bn4_2
I1211 13:21:45.048076 12512 net.cpp:84] Creating Layer bn4_2
I1211 13:21:45.048576 12512 net.cpp:406] bn4_2 <- conv4_2
I1211 13:21:45.048576 12512 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 13:21:45.048576 12512 net.cpp:122] Setting up bn4_2
I1211 13:21:45.048576 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.048576 12512 net.cpp:137] Memory required for data: 509748400
I1211 13:21:45.048576 12512 layer_factory.cpp:58] Creating layer scale4_2
I1211 13:21:45.048576 12512 net.cpp:84] Creating Layer scale4_2
I1211 13:21:45.048576 12512 net.cpp:406] scale4_2 <- conv4_2
I1211 13:21:45.048576 12512 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 13:21:45.048576 12512 layer_factory.cpp:58] Creating layer scale4_2
I1211 13:21:45.048576 12512 net.cpp:122] Setting up scale4_2
I1211 13:21:45.048576 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.048576 12512 net.cpp:137] Memory required for data: 515687600
I1211 13:21:45.048576 12512 layer_factory.cpp:58] Creating layer relu4_2
I1211 13:21:45.048576 12512 net.cpp:84] Creating Layer relu4_2
I1211 13:21:45.048576 12512 net.cpp:406] relu4_2 <- conv4_2
I1211 13:21:45.048576 12512 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 13:21:45.049077 12512 net.cpp:122] Setting up relu4_2
I1211 13:21:45.049077 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.049077 12512 net.cpp:137] Memory required for data: 521626800
I1211 13:21:45.049077 12512 layer_factory.cpp:58] Creating layer added_new_conv2
I1211 13:21:45.049077 12512 net.cpp:84] Creating Layer added_new_conv2
I1211 13:21:45.049077 12512 net.cpp:406] added_new_conv2 <- conv4_2
I1211 13:21:45.049077 12512 net.cpp:380] added_new_conv2 -> added_new_conv2
I1211 13:21:45.050576 12512 net.cpp:122] Setting up added_new_conv2
I1211 13:21:45.050576 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.050576 12512 net.cpp:137] Memory required for data: 527566000
I1211 13:21:45.050576 12512 layer_factory.cpp:58] Creating layer bn_added2
I1211 13:21:45.050576 12512 net.cpp:84] Creating Layer bn_added2
I1211 13:21:45.050576 12512 net.cpp:406] bn_added2 <- added_new_conv2
I1211 13:21:45.050576 12512 net.cpp:367] bn_added2 -> added_new_conv2 (in-place)
I1211 13:21:45.050576 12512 net.cpp:122] Setting up bn_added2
I1211 13:21:45.050576 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.050576 12512 net.cpp:137] Memory required for data: 533505200
I1211 13:21:45.050576 12512 layer_factory.cpp:58] Creating layer scale_added2
I1211 13:21:45.050576 12512 net.cpp:84] Creating Layer scale_added2
I1211 13:21:45.050576 12512 net.cpp:406] scale_added2 <- added_new_conv2
I1211 13:21:45.050576 12512 net.cpp:367] scale_added2 -> added_new_conv2 (in-place)
I1211 13:21:45.050576 12512 layer_factory.cpp:58] Creating layer scale_added2
I1211 13:21:45.051076 12512 net.cpp:122] Setting up scale_added2
I1211 13:21:45.051076 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.051076 12512 net.cpp:137] Memory required for data: 539444400
I1211 13:21:45.051076 12512 layer_factory.cpp:58] Creating layer relu_added2
I1211 13:21:45.051076 12512 net.cpp:84] Creating Layer relu_added2
I1211 13:21:45.051076 12512 net.cpp:406] relu_added2 <- added_new_conv2
I1211 13:21:45.051076 12512 net.cpp:367] relu_added2 -> added_new_conv2 (in-place)
I1211 13:21:45.051576 12512 net.cpp:122] Setting up relu_added2
I1211 13:21:45.051576 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.051576 12512 net.cpp:137] Memory required for data: 545383600
I1211 13:21:45.051576 12512 layer_factory.cpp:58] Creating layer pool4_2
I1211 13:21:45.051576 12512 net.cpp:84] Creating Layer pool4_2
I1211 13:21:45.051576 12512 net.cpp:406] pool4_2 <- added_new_conv2
I1211 13:21:45.051576 12512 net.cpp:380] pool4_2 -> pool4_2
I1211 13:21:45.051576 12512 net.cpp:122] Setting up pool4_2
I1211 13:21:45.051576 12512 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:21:45.051576 12512 net.cpp:137] Memory required for data: 546868400
I1211 13:21:45.051576 12512 layer_factory.cpp:58] Creating layer conv4_0
I1211 13:21:45.051576 12512 net.cpp:84] Creating Layer conv4_0
I1211 13:21:45.051576 12512 net.cpp:406] conv4_0 <- pool4_2
I1211 13:21:45.051576 12512 net.cpp:380] conv4_0 -> conv4_0
I1211 13:21:45.053076 12512 net.cpp:122] Setting up conv4_0
I1211 13:21:45.053076 12512 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:21:45.053076 12512 net.cpp:137] Memory required for data: 548353200
I1211 13:21:45.053076 12512 layer_factory.cpp:58] Creating layer bn4_0
I1211 13:21:45.053076 12512 net.cpp:84] Creating Layer bn4_0
I1211 13:21:45.053076 12512 net.cpp:406] bn4_0 <- conv4_0
I1211 13:21:45.053076 12512 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 13:21:45.053076 12512 net.cpp:122] Setting up bn4_0
I1211 13:21:45.053076 12512 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:21:45.053076 12512 net.cpp:137] Memory required for data: 549838000
I1211 13:21:45.053076 12512 layer_factory.cpp:58] Creating layer scale4_0
I1211 13:21:45.053076 12512 net.cpp:84] Creating Layer scale4_0
I1211 13:21:45.053076 12512 net.cpp:406] scale4_0 <- conv4_0
I1211 13:21:45.053076 12512 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 13:21:45.053076 12512 layer_factory.cpp:58] Creating layer scale4_0
I1211 13:21:45.053076 12512 net.cpp:122] Setting up scale4_0
I1211 13:21:45.053076 12512 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:21:45.053576 12512 net.cpp:137] Memory required for data: 551322800
I1211 13:21:45.053576 12512 layer_factory.cpp:58] Creating layer relu4_0
I1211 13:21:45.053576 12512 net.cpp:84] Creating Layer relu4_0
I1211 13:21:45.053576 12512 net.cpp:406] relu4_0 <- conv4_0
I1211 13:21:45.053576 12512 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 13:21:45.053576 12512 net.cpp:122] Setting up relu4_0
I1211 13:21:45.053576 12512 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:21:45.053576 12512 net.cpp:137] Memory required for data: 552807600
I1211 13:21:45.053576 12512 layer_factory.cpp:58] Creating layer conv11
I1211 13:21:45.053576 12512 net.cpp:84] Creating Layer conv11
I1211 13:21:45.053576 12512 net.cpp:406] conv11 <- conv4_0
I1211 13:21:45.053576 12512 net.cpp:380] conv11 -> conv11
I1211 13:21:45.055080 12512 net.cpp:122] Setting up conv11
I1211 13:21:45.055080 12512 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:21:45.055080 12512 net.cpp:137] Memory required for data: 554599600
I1211 13:21:45.055080 12512 layer_factory.cpp:58] Creating layer bn_conv11
I1211 13:21:45.055080 12512 net.cpp:84] Creating Layer bn_conv11
I1211 13:21:45.055080 12512 net.cpp:406] bn_conv11 <- conv11
I1211 13:21:45.055080 12512 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 13:21:45.055080 12512 net.cpp:122] Setting up bn_conv11
I1211 13:21:45.055080 12512 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:21:45.055080 12512 net.cpp:137] Memory required for data: 556391600
I1211 13:21:45.055080 12512 layer_factory.cpp:58] Creating layer scale_conv11
I1211 13:21:45.055080 12512 net.cpp:84] Creating Layer scale_conv11
I1211 13:21:45.055080 12512 net.cpp:406] scale_conv11 <- conv11
I1211 13:21:45.055080 12512 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 13:21:45.055080 12512 layer_factory.cpp:58] Creating layer scale_conv11
I1211 13:21:45.055080 12512 net.cpp:122] Setting up scale_conv11
I1211 13:21:45.055080 12512 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:21:45.055080 12512 net.cpp:137] Memory required for data: 558183600
I1211 13:21:45.055080 12512 layer_factory.cpp:58] Creating layer relu_conv11
I1211 13:21:45.055080 12512 net.cpp:84] Creating Layer relu_conv11
I1211 13:21:45.055080 12512 net.cpp:406] relu_conv11 <- conv11
I1211 13:21:45.055080 12512 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 13:21:45.056080 12512 net.cpp:122] Setting up relu_conv11
I1211 13:21:45.056080 12512 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:21:45.056080 12512 net.cpp:137] Memory required for data: 559975600
I1211 13:21:45.056080 12512 layer_factory.cpp:58] Creating layer conv12
I1211 13:21:45.056080 12512 net.cpp:84] Creating Layer conv12
I1211 13:21:45.056080 12512 net.cpp:406] conv12 <- conv11
I1211 13:21:45.056080 12512 net.cpp:380] conv12 -> conv12
I1211 13:21:45.057081 12512 net.cpp:122] Setting up conv12
I1211 13:21:45.057081 12512 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:21:45.057081 12512 net.cpp:137] Memory required for data: 562279600
I1211 13:21:45.057081 12512 layer_factory.cpp:58] Creating layer bn_conv12
I1211 13:21:45.057081 12512 net.cpp:84] Creating Layer bn_conv12
I1211 13:21:45.057081 12512 net.cpp:406] bn_conv12 <- conv12
I1211 13:21:45.057081 12512 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 13:21:45.058081 12512 net.cpp:122] Setting up bn_conv12
I1211 13:21:45.058081 12512 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:21:45.058081 12512 net.cpp:137] Memory required for data: 564583600
I1211 13:21:45.058081 12512 layer_factory.cpp:58] Creating layer scale_conv12
I1211 13:21:45.058081 12512 net.cpp:84] Creating Layer scale_conv12
I1211 13:21:45.058081 12512 net.cpp:406] scale_conv12 <- conv12
I1211 13:21:45.058081 12512 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 13:21:45.058081 12512 layer_factory.cpp:58] Creating layer scale_conv12
I1211 13:21:45.058081 12512 net.cpp:122] Setting up scale_conv12
I1211 13:21:45.058081 12512 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:21:45.058081 12512 net.cpp:137] Memory required for data: 566887600
I1211 13:21:45.058081 12512 layer_factory.cpp:58] Creating layer relu_conv12
I1211 13:21:45.058081 12512 net.cpp:84] Creating Layer relu_conv12
I1211 13:21:45.058081 12512 net.cpp:406] relu_conv12 <- conv12
I1211 13:21:45.058081 12512 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 13:21:45.058081 12512 net.cpp:122] Setting up relu_conv12
I1211 13:21:45.058081 12512 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:21:45.058081 12512 net.cpp:137] Memory required for data: 569191600
I1211 13:21:45.058081 12512 layer_factory.cpp:58] Creating layer poolcp6
I1211 13:21:45.058081 12512 net.cpp:84] Creating Layer poolcp6
I1211 13:21:45.058081 12512 net.cpp:406] poolcp6 <- conv12
I1211 13:21:45.058081 12512 net.cpp:380] poolcp6 -> poolcp6
I1211 13:21:45.058081 12512 net.cpp:122] Setting up poolcp6
I1211 13:21:45.058081 12512 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 13:21:45.058081 12512 net.cpp:137] Memory required for data: 569227600
I1211 13:21:45.058081 12512 layer_factory.cpp:58] Creating layer ip1
I1211 13:21:45.058081 12512 net.cpp:84] Creating Layer ip1
I1211 13:21:45.058081 12512 net.cpp:406] ip1 <- poolcp6
I1211 13:21:45.058081 12512 net.cpp:380] ip1 -> ip1
I1211 13:21:45.058081 12512 net.cpp:122] Setting up ip1
I1211 13:21:45.058081 12512 net.cpp:129] Top shape: 100 100 (10000)
I1211 13:21:45.058081 12512 net.cpp:137] Memory required for data: 569267600
I1211 13:21:45.058081 12512 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 13:21:45.058081 12512 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 13:21:45.058081 12512 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 13:21:45.058081 12512 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 13:21:45.058081 12512 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 13:21:45.058081 12512 net.cpp:122] Setting up ip1_ip1_0_split
I1211 13:21:45.059082 12512 net.cpp:129] Top shape: 100 100 (10000)
I1211 13:21:45.059082 12512 net.cpp:129] Top shape: 100 100 (10000)
I1211 13:21:45.059082 12512 net.cpp:137] Memory required for data: 569347600
I1211 13:21:45.059082 12512 layer_factory.cpp:58] Creating layer accuracy_training
I1211 13:21:45.059082 12512 net.cpp:84] Creating Layer accuracy_training
I1211 13:21:45.059082 12512 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1211 13:21:45.059082 12512 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1211 13:21:45.059082 12512 net.cpp:380] accuracy_training -> accuracy_training
I1211 13:21:45.059082 12512 net.cpp:122] Setting up accuracy_training
I1211 13:21:45.059082 12512 net.cpp:129] Top shape: (1)
I1211 13:21:45.059082 12512 net.cpp:137] Memory required for data: 569347604
I1211 13:21:45.059082 12512 layer_factory.cpp:58] Creating layer loss
I1211 13:21:45.059082 12512 net.cpp:84] Creating Layer loss
I1211 13:21:45.059082 12512 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 13:21:45.059082 12512 net.cpp:406] loss <- label_cifar_1_split_1
I1211 13:21:45.059082 12512 net.cpp:380] loss -> loss
I1211 13:21:45.059082 12512 layer_factory.cpp:58] Creating layer loss
I1211 13:21:45.059082 12512 net.cpp:122] Setting up loss
I1211 13:21:45.059082 12512 net.cpp:129] Top shape: (1)
I1211 13:21:45.059082 12512 net.cpp:132]     with loss weight 1
I1211 13:21:45.059082 12512 net.cpp:137] Memory required for data: 569347608
I1211 13:21:45.059082 12512 net.cpp:198] loss needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:200] accuracy_training does not need backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] ip1 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] poolcp6 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] relu_conv12 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] scale_conv12 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] bn_conv12 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] conv12 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] relu_conv11 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] scale_conv11 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] bn_conv11 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] conv11 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] relu4_0 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] scale4_0 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] bn4_0 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] conv4_0 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] pool4_2 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] relu_added2 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] scale_added2 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] bn_added2 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] added_new_conv2 needs backward computation.
I1211 13:21:45.059082 12512 net.cpp:198] relu4_2 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] scale4_2 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] bn4_2 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] conv4_2 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] relu4_1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] scale4_1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] bn4_1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] conv4_1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] relu4 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] scale4 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] bn4 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] conv4 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] relu3_1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] scale3_1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] bn3_1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] conv3_1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] relu3 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] scale3 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] bn3 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] conv3 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] pool2_1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] relu_added1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] scale_added1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] bn_added1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] newconv_added1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] relu2_2 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] scale2_2 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] bn2_2 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] conv2_2 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] relu2_1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] scale2_1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] bn2_1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] conv2_1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] relu2 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] scale2 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] bn2 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] conv2 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] relu1_0 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] scale1_0 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] bn1_0 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] conv1_0 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] relu1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] scale1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] bn1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:198] conv1 needs backward computation.
I1211 13:21:45.060082 12512 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 13:21:45.060082 12512 net.cpp:200] cifar does not need backward computation.
I1211 13:21:45.060082 12512 net.cpp:242] This network produces output accuracy_training
I1211 13:21:45.060082 12512 net.cpp:242] This network produces output loss
I1211 13:21:45.060082 12512 net.cpp:255] Network initialization done.
I1211 13:21:45.061080 12512 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 13:21:45.061080 12512 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 13:21:45.061080 12512 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_added1
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_added2
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1211 13:21:45.061080 12512 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1211 13:21:45.061080 12512 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added1"
  type: "BatchNorm"
  bottom: "newconv_added1"
  top: "newconv_added1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added1"
  type: "Scale"
  bottom: "newconv_added1"
  top: "newconv_added1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added1"
  type: "ReLU"
  bottom: "newconv_added1"
  top: "newconv_added1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added2"
  type: "BatchNorm"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added2"
  type: "Scale"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added2"
  type: "ReLU"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 13:21:45.062080 12512 layer_factory.cpp:58] Creating layer cifar
I1211 13:21:45.067081 12512 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1211 13:21:45.068081 12512 net.cpp:84] Creating Layer cifar
I1211 13:21:45.068081 12512 net.cpp:380] cifar -> data
I1211 13:21:45.068081 12512 net.cpp:380] cifar -> label
I1211 13:21:45.068081 12512 data_layer.cpp:45] output data size: 100,3,32,32
I1211 13:21:45.075080 12512 net.cpp:122] Setting up cifar
I1211 13:21:45.075080 12512 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 13:21:45.075080 12512 net.cpp:129] Top shape: 100 (100)
I1211 13:21:45.075080 12512 net.cpp:137] Memory required for data: 1229200
I1211 13:21:45.075080 12512 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 13:21:45.075080 12512 net.cpp:84] Creating Layer label_cifar_1_split
I1211 13:21:45.075080 12512 net.cpp:406] label_cifar_1_split <- label
I1211 13:21:45.075080 12512 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 13:21:45.075080 12512 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 13:21:45.075080 12512 net.cpp:122] Setting up label_cifar_1_split
I1211 13:21:45.075080 12512 net.cpp:129] Top shape: 100 (100)
I1211 13:21:45.075080 12512 net.cpp:129] Top shape: 100 (100)
I1211 13:21:45.075080 12512 net.cpp:137] Memory required for data: 1230000
I1211 13:21:45.075080 12512 layer_factory.cpp:58] Creating layer conv1
I1211 13:21:45.075080 12512 net.cpp:84] Creating Layer conv1
I1211 13:21:45.075080 12512 net.cpp:406] conv1 <- data
I1211 13:21:45.075080 12512 net.cpp:380] conv1 -> conv1
I1211 13:21:45.077080 12512 net.cpp:122] Setting up conv1
I1211 13:21:45.077080 12512 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:21:45.077080 12512 net.cpp:137] Memory required for data: 13518000
I1211 13:21:45.077080 12512 layer_factory.cpp:58] Creating layer bn1
I1211 13:21:45.077080 12512 net.cpp:84] Creating Layer bn1
I1211 13:21:45.077080 12512 net.cpp:406] bn1 <- conv1
I1211 13:21:45.077080 12512 net.cpp:367] bn1 -> conv1 (in-place)
I1211 13:21:45.077080  5628 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 13:21:45.077080 12512 net.cpp:122] Setting up bn1
I1211 13:21:45.077080 12512 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:21:45.077080 12512 net.cpp:137] Memory required for data: 25806000
I1211 13:21:45.077080 12512 layer_factory.cpp:58] Creating layer scale1
I1211 13:21:45.077080 12512 net.cpp:84] Creating Layer scale1
I1211 13:21:45.078081 12512 net.cpp:406] scale1 <- conv1
I1211 13:21:45.078081 12512 net.cpp:367] scale1 -> conv1 (in-place)
I1211 13:21:45.078081 12512 layer_factory.cpp:58] Creating layer scale1
I1211 13:21:45.078081 12512 net.cpp:122] Setting up scale1
I1211 13:21:45.078081 12512 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:21:45.078081 12512 net.cpp:137] Memory required for data: 38094000
I1211 13:21:45.078081 12512 layer_factory.cpp:58] Creating layer relu1
I1211 13:21:45.078081 12512 net.cpp:84] Creating Layer relu1
I1211 13:21:45.078081 12512 net.cpp:406] relu1 <- conv1
I1211 13:21:45.078081 12512 net.cpp:367] relu1 -> conv1 (in-place)
I1211 13:21:45.078081 12512 net.cpp:122] Setting up relu1
I1211 13:21:45.079082 12512 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:21:45.079082 12512 net.cpp:137] Memory required for data: 50382000
I1211 13:21:45.079082 12512 layer_factory.cpp:58] Creating layer conv1_0
I1211 13:21:45.079082 12512 net.cpp:84] Creating Layer conv1_0
I1211 13:21:45.079082 12512 net.cpp:406] conv1_0 <- conv1
I1211 13:21:45.079082 12512 net.cpp:380] conv1_0 -> conv1_0
I1211 13:21:45.080081 12512 net.cpp:122] Setting up conv1_0
I1211 13:21:45.080081 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.080081 12512 net.cpp:137] Memory required for data: 66766000
I1211 13:21:45.080081 12512 layer_factory.cpp:58] Creating layer bn1_0
I1211 13:21:45.080081 12512 net.cpp:84] Creating Layer bn1_0
I1211 13:21:45.080081 12512 net.cpp:406] bn1_0 <- conv1_0
I1211 13:21:45.080081 12512 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 13:21:45.081082 12512 net.cpp:122] Setting up bn1_0
I1211 13:21:45.081082 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.081082 12512 net.cpp:137] Memory required for data: 83150000
I1211 13:21:45.081082 12512 layer_factory.cpp:58] Creating layer scale1_0
I1211 13:21:45.081082 12512 net.cpp:84] Creating Layer scale1_0
I1211 13:21:45.081082 12512 net.cpp:406] scale1_0 <- conv1_0
I1211 13:21:45.081082 12512 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 13:21:45.081082 12512 layer_factory.cpp:58] Creating layer scale1_0
I1211 13:21:45.081082 12512 net.cpp:122] Setting up scale1_0
I1211 13:21:45.081082 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.081082 12512 net.cpp:137] Memory required for data: 99534000
I1211 13:21:45.081082 12512 layer_factory.cpp:58] Creating layer relu1_0
I1211 13:21:45.081082 12512 net.cpp:84] Creating Layer relu1_0
I1211 13:21:45.081082 12512 net.cpp:406] relu1_0 <- conv1_0
I1211 13:21:45.081082 12512 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 13:21:45.081082 12512 net.cpp:122] Setting up relu1_0
I1211 13:21:45.081082 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.081082 12512 net.cpp:137] Memory required for data: 115918000
I1211 13:21:45.081082 12512 layer_factory.cpp:58] Creating layer conv2
I1211 13:21:45.081082 12512 net.cpp:84] Creating Layer conv2
I1211 13:21:45.081082 12512 net.cpp:406] conv2 <- conv1_0
I1211 13:21:45.081082 12512 net.cpp:380] conv2 -> conv2
I1211 13:21:45.083081 12512 net.cpp:122] Setting up conv2
I1211 13:21:45.083081 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.083081 12512 net.cpp:137] Memory required for data: 132302000
I1211 13:21:45.083081 12512 layer_factory.cpp:58] Creating layer bn2
I1211 13:21:45.083081 12512 net.cpp:84] Creating Layer bn2
I1211 13:21:45.083081 12512 net.cpp:406] bn2 <- conv2
I1211 13:21:45.083081 12512 net.cpp:367] bn2 -> conv2 (in-place)
I1211 13:21:45.083081 12512 net.cpp:122] Setting up bn2
I1211 13:21:45.083081 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.083081 12512 net.cpp:137] Memory required for data: 148686000
I1211 13:21:45.083081 12512 layer_factory.cpp:58] Creating layer scale2
I1211 13:21:45.083081 12512 net.cpp:84] Creating Layer scale2
I1211 13:21:45.083081 12512 net.cpp:406] scale2 <- conv2
I1211 13:21:45.083081 12512 net.cpp:367] scale2 -> conv2 (in-place)
I1211 13:21:45.083081 12512 layer_factory.cpp:58] Creating layer scale2
I1211 13:21:45.083081 12512 net.cpp:122] Setting up scale2
I1211 13:21:45.083081 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.083081 12512 net.cpp:137] Memory required for data: 165070000
I1211 13:21:45.083081 12512 layer_factory.cpp:58] Creating layer relu2
I1211 13:21:45.083081 12512 net.cpp:84] Creating Layer relu2
I1211 13:21:45.084081 12512 net.cpp:406] relu2 <- conv2
I1211 13:21:45.084081 12512 net.cpp:367] relu2 -> conv2 (in-place)
I1211 13:21:45.084081 12512 net.cpp:122] Setting up relu2
I1211 13:21:45.084081 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.084081 12512 net.cpp:137] Memory required for data: 181454000
I1211 13:21:45.084081 12512 layer_factory.cpp:58] Creating layer conv2_1
I1211 13:21:45.084081 12512 net.cpp:84] Creating Layer conv2_1
I1211 13:21:45.084081 12512 net.cpp:406] conv2_1 <- conv2
I1211 13:21:45.084081 12512 net.cpp:380] conv2_1 -> conv2_1
I1211 13:21:45.085081 12512 net.cpp:122] Setting up conv2_1
I1211 13:21:45.085081 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.085081 12512 net.cpp:137] Memory required for data: 197838000
I1211 13:21:45.085081 12512 layer_factory.cpp:58] Creating layer bn2_1
I1211 13:21:45.085081 12512 net.cpp:84] Creating Layer bn2_1
I1211 13:21:45.085081 12512 net.cpp:406] bn2_1 <- conv2_1
I1211 13:21:45.085081 12512 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 13:21:45.086081 12512 net.cpp:122] Setting up bn2_1
I1211 13:21:45.086081 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.086081 12512 net.cpp:137] Memory required for data: 214222000
I1211 13:21:45.086081 12512 layer_factory.cpp:58] Creating layer scale2_1
I1211 13:21:45.086081 12512 net.cpp:84] Creating Layer scale2_1
I1211 13:21:45.086081 12512 net.cpp:406] scale2_1 <- conv2_1
I1211 13:21:45.086081 12512 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 13:21:45.086081 12512 layer_factory.cpp:58] Creating layer scale2_1
I1211 13:21:45.086081 12512 net.cpp:122] Setting up scale2_1
I1211 13:21:45.086081 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.086081 12512 net.cpp:137] Memory required for data: 230606000
I1211 13:21:45.086081 12512 layer_factory.cpp:58] Creating layer relu2_1
I1211 13:21:45.086081 12512 net.cpp:84] Creating Layer relu2_1
I1211 13:21:45.086081 12512 net.cpp:406] relu2_1 <- conv2_1
I1211 13:21:45.086081 12512 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 13:21:45.086081 12512 net.cpp:122] Setting up relu2_1
I1211 13:21:45.086081 12512 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:21:45.086081 12512 net.cpp:137] Memory required for data: 246990000
I1211 13:21:45.086081 12512 layer_factory.cpp:58] Creating layer conv2_2
I1211 13:21:45.086081 12512 net.cpp:84] Creating Layer conv2_2
I1211 13:21:45.086081 12512 net.cpp:406] conv2_2 <- conv2_1
I1211 13:21:45.086081 12512 net.cpp:380] conv2_2 -> conv2_2
I1211 13:21:45.088080 12512 net.cpp:122] Setting up conv2_2
I1211 13:21:45.088080 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.088080 12512 net.cpp:137] Memory required for data: 267470000
I1211 13:21:45.088080 12512 layer_factory.cpp:58] Creating layer bn2_2
I1211 13:21:45.088080 12512 net.cpp:84] Creating Layer bn2_2
I1211 13:21:45.088080 12512 net.cpp:406] bn2_2 <- conv2_2
I1211 13:21:45.088080 12512 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 13:21:45.088080 12512 net.cpp:122] Setting up bn2_2
I1211 13:21:45.088080 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.088080 12512 net.cpp:137] Memory required for data: 287950000
I1211 13:21:45.088080 12512 layer_factory.cpp:58] Creating layer scale2_2
I1211 13:21:45.088080 12512 net.cpp:84] Creating Layer scale2_2
I1211 13:21:45.088080 12512 net.cpp:406] scale2_2 <- conv2_2
I1211 13:21:45.088080 12512 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 13:21:45.088080 12512 layer_factory.cpp:58] Creating layer scale2_2
I1211 13:21:45.088080 12512 net.cpp:122] Setting up scale2_2
I1211 13:21:45.088080 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.088080 12512 net.cpp:137] Memory required for data: 308430000
I1211 13:21:45.088080 12512 layer_factory.cpp:58] Creating layer relu2_2
I1211 13:21:45.088080 12512 net.cpp:84] Creating Layer relu2_2
I1211 13:21:45.088080 12512 net.cpp:406] relu2_2 <- conv2_2
I1211 13:21:45.088080 12512 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 13:21:45.089082 12512 net.cpp:122] Setting up relu2_2
I1211 13:21:45.089082 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.089082 12512 net.cpp:137] Memory required for data: 328910000
I1211 13:21:45.089082 12512 layer_factory.cpp:58] Creating layer newconv_added1
I1211 13:21:45.089082 12512 net.cpp:84] Creating Layer newconv_added1
I1211 13:21:45.089082 12512 net.cpp:406] newconv_added1 <- conv2_2
I1211 13:21:45.089082 12512 net.cpp:380] newconv_added1 -> newconv_added1
I1211 13:21:45.090080 12512 net.cpp:122] Setting up newconv_added1
I1211 13:21:45.091081 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.091081 12512 net.cpp:137] Memory required for data: 349390000
I1211 13:21:45.091081 12512 layer_factory.cpp:58] Creating layer bn_added1
I1211 13:21:45.091081 12512 net.cpp:84] Creating Layer bn_added1
I1211 13:21:45.091081 12512 net.cpp:406] bn_added1 <- newconv_added1
I1211 13:21:45.091081 12512 net.cpp:367] bn_added1 -> newconv_added1 (in-place)
I1211 13:21:45.091081 12512 net.cpp:122] Setting up bn_added1
I1211 13:21:45.091081 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.091081 12512 net.cpp:137] Memory required for data: 369870000
I1211 13:21:45.091081 12512 layer_factory.cpp:58] Creating layer scale_added1
I1211 13:21:45.091081 12512 net.cpp:84] Creating Layer scale_added1
I1211 13:21:45.091081 12512 net.cpp:406] scale_added1 <- newconv_added1
I1211 13:21:45.091081 12512 net.cpp:367] scale_added1 -> newconv_added1 (in-place)
I1211 13:21:45.091081 12512 layer_factory.cpp:58] Creating layer scale_added1
I1211 13:21:45.091081 12512 net.cpp:122] Setting up scale_added1
I1211 13:21:45.091081 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.091081 12512 net.cpp:137] Memory required for data: 390350000
I1211 13:21:45.091081 12512 layer_factory.cpp:58] Creating layer relu_added1
I1211 13:21:45.091081 12512 net.cpp:84] Creating Layer relu_added1
I1211 13:21:45.091081 12512 net.cpp:406] relu_added1 <- newconv_added1
I1211 13:21:45.091081 12512 net.cpp:367] relu_added1 -> newconv_added1 (in-place)
I1211 13:21:45.092082 12512 net.cpp:122] Setting up relu_added1
I1211 13:21:45.092082 12512 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:21:45.092082 12512 net.cpp:137] Memory required for data: 410830000
I1211 13:21:45.092082 12512 layer_factory.cpp:58] Creating layer pool2_1
I1211 13:21:45.092082 12512 net.cpp:84] Creating Layer pool2_1
I1211 13:21:45.092082 12512 net.cpp:406] pool2_1 <- newconv_added1
I1211 13:21:45.092082 12512 net.cpp:380] pool2_1 -> pool2_1
I1211 13:21:45.092082 12512 net.cpp:122] Setting up pool2_1
I1211 13:21:45.092082 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.092082 12512 net.cpp:137] Memory required for data: 415950000
I1211 13:21:45.092082 12512 layer_factory.cpp:58] Creating layer conv3
I1211 13:21:45.092082 12512 net.cpp:84] Creating Layer conv3
I1211 13:21:45.092082 12512 net.cpp:406] conv3 <- pool2_1
I1211 13:21:45.092082 12512 net.cpp:380] conv3 -> conv3
I1211 13:21:45.093080 12512 net.cpp:122] Setting up conv3
I1211 13:21:45.093080 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.093080 12512 net.cpp:137] Memory required for data: 421070000
I1211 13:21:45.093080 12512 layer_factory.cpp:58] Creating layer bn3
I1211 13:21:45.093080 12512 net.cpp:84] Creating Layer bn3
I1211 13:21:45.093080 12512 net.cpp:406] bn3 <- conv3
I1211 13:21:45.093080 12512 net.cpp:367] bn3 -> conv3 (in-place)
I1211 13:21:45.093080 12512 net.cpp:122] Setting up bn3
I1211 13:21:45.093080 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.093080 12512 net.cpp:137] Memory required for data: 426190000
I1211 13:21:45.093080 12512 layer_factory.cpp:58] Creating layer scale3
I1211 13:21:45.093080 12512 net.cpp:84] Creating Layer scale3
I1211 13:21:45.093080 12512 net.cpp:406] scale3 <- conv3
I1211 13:21:45.093080 12512 net.cpp:367] scale3 -> conv3 (in-place)
I1211 13:21:45.093080 12512 layer_factory.cpp:58] Creating layer scale3
I1211 13:21:45.093080 12512 net.cpp:122] Setting up scale3
I1211 13:21:45.093080 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.093080 12512 net.cpp:137] Memory required for data: 431310000
I1211 13:21:45.093080 12512 layer_factory.cpp:58] Creating layer relu3
I1211 13:21:45.093080 12512 net.cpp:84] Creating Layer relu3
I1211 13:21:45.093080 12512 net.cpp:406] relu3 <- conv3
I1211 13:21:45.093080 12512 net.cpp:367] relu3 -> conv3 (in-place)
I1211 13:21:45.094081 12512 net.cpp:122] Setting up relu3
I1211 13:21:45.094081 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.094081 12512 net.cpp:137] Memory required for data: 436430000
I1211 13:21:45.094081 12512 layer_factory.cpp:58] Creating layer conv3_1
I1211 13:21:45.094081 12512 net.cpp:84] Creating Layer conv3_1
I1211 13:21:45.094081 12512 net.cpp:406] conv3_1 <- conv3
I1211 13:21:45.094081 12512 net.cpp:380] conv3_1 -> conv3_1
I1211 13:21:45.096081 12512 net.cpp:122] Setting up conv3_1
I1211 13:21:45.096081 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.096081 12512 net.cpp:137] Memory required for data: 441550000
I1211 13:21:45.096081 12512 layer_factory.cpp:58] Creating layer bn3_1
I1211 13:21:45.096081 12512 net.cpp:84] Creating Layer bn3_1
I1211 13:21:45.096081 12512 net.cpp:406] bn3_1 <- conv3_1
I1211 13:21:45.096081 12512 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 13:21:45.096081 12512 net.cpp:122] Setting up bn3_1
I1211 13:21:45.096081 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.096081 12512 net.cpp:137] Memory required for data: 446670000
I1211 13:21:45.096081 12512 layer_factory.cpp:58] Creating layer scale3_1
I1211 13:21:45.096081 12512 net.cpp:84] Creating Layer scale3_1
I1211 13:21:45.096081 12512 net.cpp:406] scale3_1 <- conv3_1
I1211 13:21:45.096081 12512 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 13:21:45.096081 12512 layer_factory.cpp:58] Creating layer scale3_1
I1211 13:21:45.096081 12512 net.cpp:122] Setting up scale3_1
I1211 13:21:45.096081 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.096081 12512 net.cpp:137] Memory required for data: 451790000
I1211 13:21:45.096081 12512 layer_factory.cpp:58] Creating layer relu3_1
I1211 13:21:45.096081 12512 net.cpp:84] Creating Layer relu3_1
I1211 13:21:45.096081 12512 net.cpp:406] relu3_1 <- conv3_1
I1211 13:21:45.096081 12512 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 13:21:45.097081 12512 net.cpp:122] Setting up relu3_1
I1211 13:21:45.097081 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.097081 12512 net.cpp:137] Memory required for data: 456910000
I1211 13:21:45.097081 12512 layer_factory.cpp:58] Creating layer conv4
I1211 13:21:45.097081 12512 net.cpp:84] Creating Layer conv4
I1211 13:21:45.097081 12512 net.cpp:406] conv4 <- conv3_1
I1211 13:21:45.097081 12512 net.cpp:380] conv4 -> conv4
I1211 13:21:45.098081 12512 net.cpp:122] Setting up conv4
I1211 13:21:45.098081 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.098081 12512 net.cpp:137] Memory required for data: 462030000
I1211 13:21:45.098081 12512 layer_factory.cpp:58] Creating layer bn4
I1211 13:21:45.098081 12512 net.cpp:84] Creating Layer bn4
I1211 13:21:45.098081 12512 net.cpp:406] bn4 <- conv4
I1211 13:21:45.098081 12512 net.cpp:367] bn4 -> conv4 (in-place)
I1211 13:21:45.098081 12512 net.cpp:122] Setting up bn4
I1211 13:21:45.098081 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.098081 12512 net.cpp:137] Memory required for data: 467150000
I1211 13:21:45.098081 12512 layer_factory.cpp:58] Creating layer scale4
I1211 13:21:45.098081 12512 net.cpp:84] Creating Layer scale4
I1211 13:21:45.098081 12512 net.cpp:406] scale4 <- conv4
I1211 13:21:45.098081 12512 net.cpp:367] scale4 -> conv4 (in-place)
I1211 13:21:45.098081 12512 layer_factory.cpp:58] Creating layer scale4
I1211 13:21:45.098081 12512 net.cpp:122] Setting up scale4
I1211 13:21:45.098081 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.098081 12512 net.cpp:137] Memory required for data: 472270000
I1211 13:21:45.098081 12512 layer_factory.cpp:58] Creating layer relu4
I1211 13:21:45.099081 12512 net.cpp:84] Creating Layer relu4
I1211 13:21:45.099081 12512 net.cpp:406] relu4 <- conv4
I1211 13:21:45.099081 12512 net.cpp:367] relu4 -> conv4 (in-place)
I1211 13:21:45.099081 12512 net.cpp:122] Setting up relu4
I1211 13:21:45.099081 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.099081 12512 net.cpp:137] Memory required for data: 477390000
I1211 13:21:45.099081 12512 layer_factory.cpp:58] Creating layer conv4_1
I1211 13:21:45.099081 12512 net.cpp:84] Creating Layer conv4_1
I1211 13:21:45.099081 12512 net.cpp:406] conv4_1 <- conv4
I1211 13:21:45.099081 12512 net.cpp:380] conv4_1 -> conv4_1
I1211 13:21:45.100081 12512 net.cpp:122] Setting up conv4_1
I1211 13:21:45.100081 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.100081 12512 net.cpp:137] Memory required for data: 482510000
I1211 13:21:45.100081 12512 layer_factory.cpp:58] Creating layer bn4_1
I1211 13:21:45.100081 12512 net.cpp:84] Creating Layer bn4_1
I1211 13:21:45.100081 12512 net.cpp:406] bn4_1 <- conv4_1
I1211 13:21:45.100081 12512 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 13:21:45.100081 12512 net.cpp:122] Setting up bn4_1
I1211 13:21:45.100081 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.100081 12512 net.cpp:137] Memory required for data: 487630000
I1211 13:21:45.100081 12512 layer_factory.cpp:58] Creating layer scale4_1
I1211 13:21:45.100081 12512 net.cpp:84] Creating Layer scale4_1
I1211 13:21:45.100081 12512 net.cpp:406] scale4_1 <- conv4_1
I1211 13:21:45.100081 12512 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 13:21:45.100081 12512 layer_factory.cpp:58] Creating layer scale4_1
I1211 13:21:45.101081 12512 net.cpp:122] Setting up scale4_1
I1211 13:21:45.101081 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.101081 12512 net.cpp:137] Memory required for data: 492750000
I1211 13:21:45.101081 12512 layer_factory.cpp:58] Creating layer relu4_1
I1211 13:21:45.101081 12512 net.cpp:84] Creating Layer relu4_1
I1211 13:21:45.101081 12512 net.cpp:406] relu4_1 <- conv4_1
I1211 13:21:45.101081 12512 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 13:21:45.101081 12512 net.cpp:122] Setting up relu4_1
I1211 13:21:45.101081 12512 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:21:45.101081 12512 net.cpp:137] Memory required for data: 497870000
I1211 13:21:45.101081 12512 layer_factory.cpp:58] Creating layer conv4_2
I1211 13:21:45.101081 12512 net.cpp:84] Creating Layer conv4_2
I1211 13:21:45.101081 12512 net.cpp:406] conv4_2 <- conv4_1
I1211 13:21:45.101081 12512 net.cpp:380] conv4_2 -> conv4_2
I1211 13:21:45.103081 12512 net.cpp:122] Setting up conv4_2
I1211 13:21:45.103081 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.103081 12512 net.cpp:137] Memory required for data: 503809200
I1211 13:21:45.103081 12512 layer_factory.cpp:58] Creating layer bn4_2
I1211 13:21:45.103081 12512 net.cpp:84] Creating Layer bn4_2
I1211 13:21:45.103081 12512 net.cpp:406] bn4_2 <- conv4_2
I1211 13:21:45.103081 12512 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 13:21:45.103081 12512 net.cpp:122] Setting up bn4_2
I1211 13:21:45.103081 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.103081 12512 net.cpp:137] Memory required for data: 509748400
I1211 13:21:45.103081 12512 layer_factory.cpp:58] Creating layer scale4_2
I1211 13:21:45.103081 12512 net.cpp:84] Creating Layer scale4_2
I1211 13:21:45.103081 12512 net.cpp:406] scale4_2 <- conv4_2
I1211 13:21:45.103081 12512 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 13:21:45.103081 12512 layer_factory.cpp:58] Creating layer scale4_2
I1211 13:21:45.103081 12512 net.cpp:122] Setting up scale4_2
I1211 13:21:45.103081 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.103081 12512 net.cpp:137] Memory required for data: 515687600
I1211 13:21:45.103081 12512 layer_factory.cpp:58] Creating layer relu4_2
I1211 13:21:45.103081 12512 net.cpp:84] Creating Layer relu4_2
I1211 13:21:45.103081 12512 net.cpp:406] relu4_2 <- conv4_2
I1211 13:21:45.103081 12512 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 13:21:45.103081 12512 net.cpp:122] Setting up relu4_2
I1211 13:21:45.103081 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.103081 12512 net.cpp:137] Memory required for data: 521626800
I1211 13:21:45.103081 12512 layer_factory.cpp:58] Creating layer added_new_conv2
I1211 13:21:45.103081 12512 net.cpp:84] Creating Layer added_new_conv2
I1211 13:21:45.103081 12512 net.cpp:406] added_new_conv2 <- conv4_2
I1211 13:21:45.103081 12512 net.cpp:380] added_new_conv2 -> added_new_conv2
I1211 13:21:45.105080 12512 net.cpp:122] Setting up added_new_conv2
I1211 13:21:45.105080 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.105080 12512 net.cpp:137] Memory required for data: 527566000
I1211 13:21:45.105080 12512 layer_factory.cpp:58] Creating layer bn_added2
I1211 13:21:45.105080 12512 net.cpp:84] Creating Layer bn_added2
I1211 13:21:45.105080 12512 net.cpp:406] bn_added2 <- added_new_conv2
I1211 13:21:45.105080 12512 net.cpp:367] bn_added2 -> added_new_conv2 (in-place)
I1211 13:21:45.105080 12512 net.cpp:122] Setting up bn_added2
I1211 13:21:45.105080 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.105080 12512 net.cpp:137] Memory required for data: 533505200
I1211 13:21:45.105080 12512 layer_factory.cpp:58] Creating layer scale_added2
I1211 13:21:45.105080 12512 net.cpp:84] Creating Layer scale_added2
I1211 13:21:45.105080 12512 net.cpp:406] scale_added2 <- added_new_conv2
I1211 13:21:45.105080 12512 net.cpp:367] scale_added2 -> added_new_conv2 (in-place)
I1211 13:21:45.105080 12512 layer_factory.cpp:58] Creating layer scale_added2
I1211 13:21:45.105080 12512 net.cpp:122] Setting up scale_added2
I1211 13:21:45.105080 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.105080 12512 net.cpp:137] Memory required for data: 539444400
I1211 13:21:45.105080 12512 layer_factory.cpp:58] Creating layer relu_added2
I1211 13:21:45.105080 12512 net.cpp:84] Creating Layer relu_added2
I1211 13:21:45.105080 12512 net.cpp:406] relu_added2 <- added_new_conv2
I1211 13:21:45.105080 12512 net.cpp:367] relu_added2 -> added_new_conv2 (in-place)
I1211 13:21:45.106082 12512 net.cpp:122] Setting up relu_added2
I1211 13:21:45.106082 12512 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:21:45.106082 12512 net.cpp:137] Memory required for data: 545383600
I1211 13:21:45.106082 12512 layer_factory.cpp:58] Creating layer pool4_2
I1211 13:21:45.106082 12512 net.cpp:84] Creating Layer pool4_2
I1211 13:21:45.106082 12512 net.cpp:406] pool4_2 <- added_new_conv2
I1211 13:21:45.106082 12512 net.cpp:380] pool4_2 -> pool4_2
I1211 13:21:45.106082 12512 net.cpp:122] Setting up pool4_2
I1211 13:21:45.106082 12512 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:21:45.106082 12512 net.cpp:137] Memory required for data: 546868400
I1211 13:21:45.106082 12512 layer_factory.cpp:58] Creating layer conv4_0
I1211 13:21:45.106082 12512 net.cpp:84] Creating Layer conv4_0
I1211 13:21:45.106082 12512 net.cpp:406] conv4_0 <- pool4_2
I1211 13:21:45.106082 12512 net.cpp:380] conv4_0 -> conv4_0
I1211 13:21:45.108080 12512 net.cpp:122] Setting up conv4_0
I1211 13:21:45.108080 12512 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:21:45.108080 12512 net.cpp:137] Memory required for data: 548353200
I1211 13:21:45.108080 12512 layer_factory.cpp:58] Creating layer bn4_0
I1211 13:21:45.108080 12512 net.cpp:84] Creating Layer bn4_0
I1211 13:21:45.108080 12512 net.cpp:406] bn4_0 <- conv4_0
I1211 13:21:45.108080 12512 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 13:21:45.108080 12512 net.cpp:122] Setting up bn4_0
I1211 13:21:45.108080 12512 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:21:45.108080 12512 net.cpp:137] Memory required for data: 549838000
I1211 13:21:45.108080 12512 layer_factory.cpp:58] Creating layer scale4_0
I1211 13:21:45.108080 12512 net.cpp:84] Creating Layer scale4_0
I1211 13:21:45.108080 12512 net.cpp:406] scale4_0 <- conv4_0
I1211 13:21:45.108080 12512 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 13:21:45.108080 12512 layer_factory.cpp:58] Creating layer scale4_0
I1211 13:21:45.108080 12512 net.cpp:122] Setting up scale4_0
I1211 13:21:45.108080 12512 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:21:45.108080 12512 net.cpp:137] Memory required for data: 551322800
I1211 13:21:45.108080 12512 layer_factory.cpp:58] Creating layer relu4_0
I1211 13:21:45.108080 12512 net.cpp:84] Creating Layer relu4_0
I1211 13:21:45.108080 12512 net.cpp:406] relu4_0 <- conv4_0
I1211 13:21:45.108080 12512 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 13:21:45.109081 12512 net.cpp:122] Setting up relu4_0
I1211 13:21:45.109081 12512 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:21:45.109081 12512 net.cpp:137] Memory required for data: 552807600
I1211 13:21:45.109081 12512 layer_factory.cpp:58] Creating layer conv11
I1211 13:21:45.109081 12512 net.cpp:84] Creating Layer conv11
I1211 13:21:45.109081 12512 net.cpp:406] conv11 <- conv4_0
I1211 13:21:45.109081 12512 net.cpp:380] conv11 -> conv11
I1211 13:21:45.110080 12512 net.cpp:122] Setting up conv11
I1211 13:21:45.110080 12512 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:21:45.110080 12512 net.cpp:137] Memory required for data: 554599600
I1211 13:21:45.110080 12512 layer_factory.cpp:58] Creating layer bn_conv11
I1211 13:21:45.110080 12512 net.cpp:84] Creating Layer bn_conv11
I1211 13:21:45.110080 12512 net.cpp:406] bn_conv11 <- conv11
I1211 13:21:45.110080 12512 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 13:21:45.111081 12512 net.cpp:122] Setting up bn_conv11
I1211 13:21:45.111081 12512 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:21:45.111081 12512 net.cpp:137] Memory required for data: 556391600
I1211 13:21:45.111081 12512 layer_factory.cpp:58] Creating layer scale_conv11
I1211 13:21:45.111081 12512 net.cpp:84] Creating Layer scale_conv11
I1211 13:21:45.111081 12512 net.cpp:406] scale_conv11 <- conv11
I1211 13:21:45.111081 12512 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 13:21:45.111081 12512 layer_factory.cpp:58] Creating layer scale_conv11
I1211 13:21:45.111081 12512 net.cpp:122] Setting up scale_conv11
I1211 13:21:45.111081 12512 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:21:45.111081 12512 net.cpp:137] Memory required for data: 558183600
I1211 13:21:45.111081 12512 layer_factory.cpp:58] Creating layer relu_conv11
I1211 13:21:45.111081 12512 net.cpp:84] Creating Layer relu_conv11
I1211 13:21:45.111081 12512 net.cpp:406] relu_conv11 <- conv11
I1211 13:21:45.111081 12512 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 13:21:45.112082 12512 net.cpp:122] Setting up relu_conv11
I1211 13:21:45.112082 12512 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:21:45.112082 12512 net.cpp:137] Memory required for data: 559975600
I1211 13:21:45.112082 12512 layer_factory.cpp:58] Creating layer conv12
I1211 13:21:45.112082 12512 net.cpp:84] Creating Layer conv12
I1211 13:21:45.112082 12512 net.cpp:406] conv12 <- conv11
I1211 13:21:45.112082 12512 net.cpp:380] conv12 -> conv12
I1211 13:21:45.113080 12512 net.cpp:122] Setting up conv12
I1211 13:21:45.113080 12512 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:21:45.113080 12512 net.cpp:137] Memory required for data: 562279600
I1211 13:21:45.113080 12512 layer_factory.cpp:58] Creating layer bn_conv12
I1211 13:21:45.113080 12512 net.cpp:84] Creating Layer bn_conv12
I1211 13:21:45.113080 12512 net.cpp:406] bn_conv12 <- conv12
I1211 13:21:45.113080 12512 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 13:21:45.114081 12512 net.cpp:122] Setting up bn_conv12
I1211 13:21:45.114081 12512 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:21:45.114081 12512 net.cpp:137] Memory required for data: 564583600
I1211 13:21:45.114081 12512 layer_factory.cpp:58] Creating layer scale_conv12
I1211 13:21:45.114081 12512 net.cpp:84] Creating Layer scale_conv12
I1211 13:21:45.114081 12512 net.cpp:406] scale_conv12 <- conv12
I1211 13:21:45.114081 12512 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 13:21:45.114081 12512 layer_factory.cpp:58] Creating layer scale_conv12
I1211 13:21:45.114081 12512 net.cpp:122] Setting up scale_conv12
I1211 13:21:45.114081 12512 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:21:45.114081 12512 net.cpp:137] Memory required for data: 566887600
I1211 13:21:45.114081 12512 layer_factory.cpp:58] Creating layer relu_conv12
I1211 13:21:45.114081 12512 net.cpp:84] Creating Layer relu_conv12
I1211 13:21:45.114081 12512 net.cpp:406] relu_conv12 <- conv12
I1211 13:21:45.114081 12512 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 13:21:45.114081 12512 net.cpp:122] Setting up relu_conv12
I1211 13:21:45.114081 12512 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:21:45.114081 12512 net.cpp:137] Memory required for data: 569191600
I1211 13:21:45.114081 12512 layer_factory.cpp:58] Creating layer poolcp6
I1211 13:21:45.114081 12512 net.cpp:84] Creating Layer poolcp6
I1211 13:21:45.114081 12512 net.cpp:406] poolcp6 <- conv12
I1211 13:21:45.114081 12512 net.cpp:380] poolcp6 -> poolcp6
I1211 13:21:45.114081 12512 net.cpp:122] Setting up poolcp6
I1211 13:21:45.114081 12512 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 13:21:45.114081 12512 net.cpp:137] Memory required for data: 569227600
I1211 13:21:45.114081 12512 layer_factory.cpp:58] Creating layer ip1
I1211 13:21:45.114081 12512 net.cpp:84] Creating Layer ip1
I1211 13:21:45.114081 12512 net.cpp:406] ip1 <- poolcp6
I1211 13:21:45.114081 12512 net.cpp:380] ip1 -> ip1
I1211 13:21:45.115082 12512 net.cpp:122] Setting up ip1
I1211 13:21:45.115082 12512 net.cpp:129] Top shape: 100 100 (10000)
I1211 13:21:45.115082 12512 net.cpp:137] Memory required for data: 569267600
I1211 13:21:45.115082 12512 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 13:21:45.115082 12512 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 13:21:45.115082 12512 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 13:21:45.115082 12512 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 13:21:45.115082 12512 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 13:21:45.115082 12512 net.cpp:122] Setting up ip1_ip1_0_split
I1211 13:21:45.115082 12512 net.cpp:129] Top shape: 100 100 (10000)
I1211 13:21:45.115082 12512 net.cpp:129] Top shape: 100 100 (10000)
I1211 13:21:45.115082 12512 net.cpp:137] Memory required for data: 569347600
I1211 13:21:45.115082 12512 layer_factory.cpp:58] Creating layer accuracy
I1211 13:21:45.115082 12512 net.cpp:84] Creating Layer accuracy
I1211 13:21:45.115082 12512 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1211 13:21:45.115082 12512 net.cpp:406] accuracy <- label_cifar_1_split_0
I1211 13:21:45.115082 12512 net.cpp:380] accuracy -> accuracy
I1211 13:21:45.115082 12512 net.cpp:122] Setting up accuracy
I1211 13:21:45.115082 12512 net.cpp:129] Top shape: (1)
I1211 13:21:45.115082 12512 net.cpp:137] Memory required for data: 569347604
I1211 13:21:45.115082 12512 layer_factory.cpp:58] Creating layer loss
I1211 13:21:45.115082 12512 net.cpp:84] Creating Layer loss
I1211 13:21:45.115082 12512 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 13:21:45.115082 12512 net.cpp:406] loss <- label_cifar_1_split_1
I1211 13:21:45.115082 12512 net.cpp:380] loss -> loss
I1211 13:21:45.115082 12512 layer_factory.cpp:58] Creating layer loss
I1211 13:21:45.115082 12512 net.cpp:122] Setting up loss
I1211 13:21:45.115082 12512 net.cpp:129] Top shape: (1)
I1211 13:21:45.115082 12512 net.cpp:132]     with loss weight 1
I1211 13:21:45.115082 12512 net.cpp:137] Memory required for data: 569347608
I1211 13:21:45.115082 12512 net.cpp:198] loss needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:200] accuracy does not need backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] ip1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] poolcp6 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu_conv12 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale_conv12 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn_conv12 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] conv12 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu_conv11 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale_conv11 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn_conv11 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] conv11 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu4_0 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale4_0 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn4_0 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] conv4_0 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] pool4_2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu_added2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale_added2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn_added2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] added_new_conv2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu4_2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale4_2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn4_2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] conv4_2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu4_1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale4_1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn4_1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] conv4_1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu4 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale4 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn4 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] conv4 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu3_1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale3_1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn3_1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] conv3_1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu3 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale3 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn3 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] conv3 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] pool2_1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu_added1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale_added1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn_added1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] newconv_added1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu2_2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale2_2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn2_2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] conv2_2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu2_1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale2_1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn2_1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] conv2_1 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] conv2 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] relu1_0 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] scale1_0 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] bn1_0 needs backward computation.
I1211 13:21:45.115082 12512 net.cpp:198] conv1_0 needs backward computation.
I1211 13:21:45.116081 12512 net.cpp:198] relu1 needs backward computation.
I1211 13:21:45.116081 12512 net.cpp:198] scale1 needs backward computation.
I1211 13:21:45.116081 12512 net.cpp:198] bn1 needs backward computation.
I1211 13:21:45.116081 12512 net.cpp:198] conv1 needs backward computation.
I1211 13:21:45.116081 12512 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 13:21:45.116081 12512 net.cpp:200] cifar does not need backward computation.
I1211 13:21:45.116081 12512 net.cpp:242] This network produces output accuracy
I1211 13:21:45.116081 12512 net.cpp:242] This network produces output loss
I1211 13:21:45.116081 12512 net.cpp:255] Network initialization done.
I1211 13:21:45.116081 12512 solver.cpp:56] Solver scaffolding done.
I1211 13:21:45.121080 12512 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.solverstate
I1211 13:21:45.124081 12512 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.caffemodel
I1211 13:21:45.124081 12512 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 13:21:45.124081 12512 sgd_solver.cpp:318] SGDSolver: restoring history
I1211 13:21:45.129081 12512 caffe.cpp:249] Starting Optimization
I1211 13:21:45.129081 12512 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k
I1211 13:21:45.129081 12512 solver.cpp:273] Learning Rate Policy: multistep
I1211 13:21:45.132081 12512 solver.cpp:330] Iteration 90000, Testing net (#0)
I1211 13:21:45.134080 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:21:46.720227  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:21:46.781242 12512 solver.cpp:397]     Test net output #0: accuracy = 0.5711
I1211 13:21:46.781242 12512 solver.cpp:397]     Test net output #1: loss = 1.67838 (* 1 = 1.67838 loss)
I1211 13:21:46.900240 12512 solver.cpp:218] Iteration 90000 (50831.6 iter/s, 1.77055s/100 iters), loss = 0.601179
I1211 13:21:46.900240 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:21:46.900240 12512 solver.cpp:237]     Train net output #1: loss = 0.601179 (* 1 = 0.601179 loss)
I1211 13:21:46.900240 12512 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1211 13:21:53.287582 12512 solver.cpp:218] Iteration 90100 (15.6579 iter/s, 6.38655s/100 iters), loss = 0.796454
I1211 13:21:53.287582 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 13:21:53.287582 12512 solver.cpp:237]     Train net output #1: loss = 0.796454 (* 1 = 0.796454 loss)
I1211 13:21:53.287582 12512 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1211 13:21:59.617116 12512 solver.cpp:218] Iteration 90200 (15.7992 iter/s, 6.32943s/100 iters), loss = 0.570449
I1211 13:21:59.617116 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:21:59.617116 12512 solver.cpp:237]     Train net output #1: loss = 0.570449 (* 1 = 0.570449 loss)
I1211 13:21:59.617116 12512 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1211 13:22:05.934624 12512 solver.cpp:218] Iteration 90300 (15.8312 iter/s, 6.31666s/100 iters), loss = 0.726408
I1211 13:22:05.934624 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 13:22:05.934624 12512 solver.cpp:237]     Train net output #1: loss = 0.726408 (* 1 = 0.726408 loss)
I1211 13:22:05.934624 12512 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1211 13:22:12.393502 12512 solver.cpp:218] Iteration 90400 (15.4827 iter/s, 6.45882s/100 iters), loss = 0.767528
I1211 13:22:12.393502 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.66
I1211 13:22:12.393502 12512 solver.cpp:237]     Train net output #1: loss = 0.767528 (* 1 = 0.767528 loss)
I1211 13:22:12.393502 12512 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1211 13:22:18.570996 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:22:18.829010 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90500.caffemodel
I1211 13:22:18.845016 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90500.solverstate
I1211 13:22:18.850514 12512 solver.cpp:330] Iteration 90500, Testing net (#0)
I1211 13:22:18.850514 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:22:20.395148  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:22:20.456652 12512 solver.cpp:397]     Test net output #0: accuracy = 0.5951
I1211 13:22:20.456652 12512 solver.cpp:397]     Test net output #1: loss = 1.62468 (* 1 = 1.62468 loss)
I1211 13:22:20.518162 12512 solver.cpp:218] Iteration 90500 (12.3096 iter/s, 8.12371s/100 iters), loss = 0.529996
I1211 13:22:20.518162 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:22:20.518162 12512 solver.cpp:237]     Train net output #1: loss = 0.529996 (* 1 = 0.529996 loss)
I1211 13:22:20.518162 12512 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1211 13:22:27.042359 12512 solver.cpp:218] Iteration 90600 (15.3276 iter/s, 6.52416s/100 iters), loss = 0.633386
I1211 13:22:27.042860 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 13:22:27.042860 12512 solver.cpp:237]     Train net output #1: loss = 0.633386 (* 1 = 0.633386 loss)
I1211 13:22:27.042860 12512 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1211 13:22:33.472515 12512 solver.cpp:218] Iteration 90700 (15.5539 iter/s, 6.42924s/100 iters), loss = 0.664458
I1211 13:22:33.472515 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 13:22:33.472515 12512 solver.cpp:237]     Train net output #1: loss = 0.664458 (* 1 = 0.664458 loss)
I1211 13:22:33.472515 12512 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1211 13:22:39.925529 12512 solver.cpp:218] Iteration 90800 (15.4975 iter/s, 6.45265s/100 iters), loss = 0.71863
I1211 13:22:39.925529 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 13:22:39.925529 12512 solver.cpp:237]     Train net output #1: loss = 0.71863 (* 1 = 0.71863 loss)
I1211 13:22:39.925529 12512 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1211 13:22:46.359606 12512 solver.cpp:218] Iteration 90900 (15.5438 iter/s, 6.43345s/100 iters), loss = 0.687273
I1211 13:22:46.359606 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 13:22:46.359606 12512 solver.cpp:237]     Train net output #1: loss = 0.687273 (* 1 = 0.687273 loss)
I1211 13:22:46.359606 12512 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1211 13:22:52.409519 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:22:52.663542 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91000.caffemodel
I1211 13:22:52.678536 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91000.solverstate
I1211 13:22:52.683535 12512 solver.cpp:330] Iteration 91000, Testing net (#0)
I1211 13:22:52.683535 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:22:54.232655  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:22:54.292660 12512 solver.cpp:397]     Test net output #0: accuracy = 0.5765
I1211 13:22:54.292660 12512 solver.cpp:397]     Test net output #1: loss = 1.6387 (* 1 = 1.6387 loss)
I1211 13:22:54.354662 12512 solver.cpp:218] Iteration 91000 (12.5082 iter/s, 7.99474s/100 iters), loss = 0.659059
I1211 13:22:54.354662 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 13:22:54.354662 12512 solver.cpp:237]     Train net output #1: loss = 0.659059 (* 1 = 0.659059 loss)
I1211 13:22:54.354662 12512 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1211 13:23:00.698122 12512 solver.cpp:218] Iteration 91100 (15.7641 iter/s, 6.34352s/100 iters), loss = 0.772799
I1211 13:23:00.698122 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 13:23:00.698122 12512 solver.cpp:237]     Train net output #1: loss = 0.772799 (* 1 = 0.772799 loss)
I1211 13:23:00.698122 12512 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1211 13:23:07.065580 12512 solver.cpp:218] Iteration 91200 (15.7068 iter/s, 6.36666s/100 iters), loss = 0.609353
I1211 13:23:07.065580 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:23:07.065580 12512 solver.cpp:237]     Train net output #1: loss = 0.609353 (* 1 = 0.609353 loss)
I1211 13:23:07.065580 12512 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1211 13:23:13.487079 12512 solver.cpp:218] Iteration 91300 (15.5744 iter/s, 6.42078s/100 iters), loss = 0.71073
I1211 13:23:13.487079 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:23:13.487079 12512 solver.cpp:237]     Train net output #1: loss = 0.71073 (* 1 = 0.71073 loss)
I1211 13:23:13.487079 12512 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1211 13:23:19.850561 12512 solver.cpp:218] Iteration 91400 (15.7154 iter/s, 6.36319s/100 iters), loss = 0.776026
I1211 13:23:19.850561 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 13:23:19.850561 12512 solver.cpp:237]     Train net output #1: loss = 0.776026 (* 1 = 0.776026 loss)
I1211 13:23:19.850561 12512 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1211 13:23:25.940955 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:23:26.193974 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91500.caffemodel
I1211 13:23:26.211975 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91500.solverstate
I1211 13:23:26.216974 12512 solver.cpp:330] Iteration 91500, Testing net (#0)
I1211 13:23:26.216974 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:23:27.763142  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:23:27.825147 12512 solver.cpp:397]     Test net output #0: accuracy = 0.5819
I1211 13:23:27.825147 12512 solver.cpp:397]     Test net output #1: loss = 1.61589 (* 1 = 1.61589 loss)
I1211 13:23:27.887156 12512 solver.cpp:218] Iteration 91500 (12.444 iter/s, 8.036s/100 iters), loss = 0.641897
I1211 13:23:27.887156 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 13:23:27.887156 12512 solver.cpp:237]     Train net output #1: loss = 0.641897 (* 1 = 0.641897 loss)
I1211 13:23:27.887156 12512 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1211 13:23:34.276819 12512 solver.cpp:218] Iteration 91600 (15.6501 iter/s, 6.38975s/100 iters), loss = 0.877702
I1211 13:23:34.276819 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 13:23:34.276819 12512 solver.cpp:237]     Train net output #1: loss = 0.877702 (* 1 = 0.877702 loss)
I1211 13:23:34.276819 12512 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1211 13:23:40.663410 12512 solver.cpp:218] Iteration 91700 (15.66 iter/s, 6.3857s/100 iters), loss = 0.603465
I1211 13:23:40.663410 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:23:40.663410 12512 solver.cpp:237]     Train net output #1: loss = 0.603465 (* 1 = 0.603465 loss)
I1211 13:23:40.663410 12512 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1211 13:23:47.067112 12512 solver.cpp:218] Iteration 91800 (15.6156 iter/s, 6.40386s/100 iters), loss = 0.78706
I1211 13:23:47.067112 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 13:23:47.068114 12512 solver.cpp:237]     Train net output #1: loss = 0.78706 (* 1 = 0.78706 loss)
I1211 13:23:47.068114 12512 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1211 13:23:53.570415 12512 solver.cpp:218] Iteration 91900 (15.38 iter/s, 6.50196s/100 iters), loss = 0.753093
I1211 13:23:53.570415 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 13:23:53.570415 12512 solver.cpp:237]     Train net output #1: loss = 0.753093 (* 1 = 0.753093 loss)
I1211 13:23:53.570415 12512 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1211 13:23:59.639945 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:23:59.891964 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92000.caffemodel
I1211 13:23:59.906962 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92000.solverstate
I1211 13:23:59.911963 12512 solver.cpp:330] Iteration 92000, Testing net (#0)
I1211 13:23:59.911963 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:24:01.434082  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:24:01.494102 12512 solver.cpp:397]     Test net output #0: accuracy = 0.5949
I1211 13:24:01.494102 12512 solver.cpp:397]     Test net output #1: loss = 1.59537 (* 1 = 1.59537 loss)
I1211 13:24:01.555086 12512 solver.cpp:218] Iteration 92000 (12.5237 iter/s, 7.98488s/100 iters), loss = 0.763156
I1211 13:24:01.555086 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 13:24:01.555086 12512 solver.cpp:237]     Train net output #1: loss = 0.763156 (* 1 = 0.763156 loss)
I1211 13:24:01.555086 12512 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1211 13:24:07.932690 12512 solver.cpp:218] Iteration 92100 (15.6811 iter/s, 6.37709s/100 iters), loss = 0.771497
I1211 13:24:07.932690 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 13:24:07.932690 12512 solver.cpp:237]     Train net output #1: loss = 0.771497 (* 1 = 0.771497 loss)
I1211 13:24:07.932690 12512 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1211 13:24:14.311067 12512 solver.cpp:218] Iteration 92200 (15.6802 iter/s, 6.37747s/100 iters), loss = 0.712785
I1211 13:24:14.311067 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 13:24:14.311067 12512 solver.cpp:237]     Train net output #1: loss = 0.712785 (* 1 = 0.712785 loss)
I1211 13:24:14.311067 12512 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1211 13:24:20.678833 12512 solver.cpp:218] Iteration 92300 (15.7048 iter/s, 6.36748s/100 iters), loss = 0.716471
I1211 13:24:20.678833 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 13:24:20.678833 12512 solver.cpp:237]     Train net output #1: loss = 0.716471 (* 1 = 0.716471 loss)
I1211 13:24:20.678833 12512 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1211 13:24:27.063282 12512 solver.cpp:218] Iteration 92400 (15.6647 iter/s, 6.38379s/100 iters), loss = 0.774909
I1211 13:24:27.063282 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 13:24:27.063282 12512 solver.cpp:237]     Train net output #1: loss = 0.774909 (* 1 = 0.774909 loss)
I1211 13:24:27.063282 12512 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1211 13:24:33.237555 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:24:33.493574 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92500.caffemodel
I1211 13:24:33.510574 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92500.solverstate
I1211 13:24:33.516575 12512 solver.cpp:330] Iteration 92500, Testing net (#0)
I1211 13:24:33.516575 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:24:35.048801  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:24:35.108825 12512 solver.cpp:397]     Test net output #0: accuracy = 0.5773
I1211 13:24:35.108825 12512 solver.cpp:397]     Test net output #1: loss = 1.62631 (* 1 = 1.62631 loss)
I1211 13:24:35.170325 12512 solver.cpp:218] Iteration 92500 (12.3359 iter/s, 8.10645s/100 iters), loss = 0.751072
I1211 13:24:35.170325 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 13:24:35.170325 12512 solver.cpp:237]     Train net output #1: loss = 0.751072 (* 1 = 0.751072 loss)
I1211 13:24:35.170325 12512 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1211 13:24:41.522321 12512 solver.cpp:218] Iteration 92600 (15.7443 iter/s, 6.35152s/100 iters), loss = 0.79177
I1211 13:24:41.522321 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 13:24:41.522321 12512 solver.cpp:237]     Train net output #1: loss = 0.79177 (* 1 = 0.79177 loss)
I1211 13:24:41.522321 12512 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1211 13:24:47.864013 12512 solver.cpp:218] Iteration 92700 (15.7694 iter/s, 6.34138s/100 iters), loss = 0.71097
I1211 13:24:47.864013 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 13:24:47.864013 12512 solver.cpp:237]     Train net output #1: loss = 0.71097 (* 1 = 0.71097 loss)
I1211 13:24:47.864013 12512 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1211 13:24:54.222570 12512 solver.cpp:218] Iteration 92800 (15.7272 iter/s, 6.35841s/100 iters), loss = 0.845726
I1211 13:24:54.222570 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 13:24:54.222570 12512 solver.cpp:237]     Train net output #1: loss = 0.845726 (* 1 = 0.845726 loss)
I1211 13:24:54.222570 12512 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1211 13:25:00.603688 12512 solver.cpp:218] Iteration 92900 (15.6732 iter/s, 6.38032s/100 iters), loss = 0.666629
I1211 13:25:00.603688 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 13:25:00.603688 12512 solver.cpp:237]     Train net output #1: loss = 0.666629 (* 1 = 0.666629 loss)
I1211 13:25:00.603688 12512 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1211 13:25:06.640565 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:25:06.891100 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93000.caffemodel
I1211 13:25:06.907629 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93000.solverstate
I1211 13:25:06.912631 12512 solver.cpp:330] Iteration 93000, Testing net (#0)
I1211 13:25:06.912631 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:25:08.436142  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:25:08.496647 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6013
I1211 13:25:08.496647 12512 solver.cpp:397]     Test net output #1: loss = 1.54526 (* 1 = 1.54526 loss)
I1211 13:25:08.557150 12512 solver.cpp:218] Iteration 93000 (12.5732 iter/s, 7.95341s/100 iters), loss = 0.683329
I1211 13:25:08.557150 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:25:08.557150 12512 solver.cpp:237]     Train net output #1: loss = 0.683329 (* 1 = 0.683329 loss)
I1211 13:25:08.557150 12512 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1211 13:25:14.972121 12512 solver.cpp:218] Iteration 93100 (15.5912 iter/s, 6.41386s/100 iters), loss = 0.745122
I1211 13:25:14.972121 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 13:25:14.972121 12512 solver.cpp:237]     Train net output #1: loss = 0.745122 (* 1 = 0.745122 loss)
I1211 13:25:14.972121 12512 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1211 13:25:21.469748 12512 solver.cpp:218] Iteration 93200 (15.3908 iter/s, 6.4974s/100 iters), loss = 0.69572
I1211 13:25:21.469748 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 13:25:21.469748 12512 solver.cpp:237]     Train net output #1: loss = 0.69572 (* 1 = 0.69572 loss)
I1211 13:25:21.469748 12512 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1211 13:25:27.908809 12512 solver.cpp:218] Iteration 93300 (15.5313 iter/s, 6.43859s/100 iters), loss = 0.789704
I1211 13:25:27.908809 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 13:25:27.908809 12512 solver.cpp:237]     Train net output #1: loss = 0.789704 (* 1 = 0.789704 loss)
I1211 13:25:27.908809 12512 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1211 13:25:34.255280 12512 solver.cpp:218] Iteration 93400 (15.7581 iter/s, 6.34595s/100 iters), loss = 0.653144
I1211 13:25:34.255280 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:25:34.255280 12512 solver.cpp:237]     Train net output #1: loss = 0.653144 (* 1 = 0.653144 loss)
I1211 13:25:34.255280 12512 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1211 13:25:40.300462 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:25:40.550123 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93500.caffemodel
I1211 13:25:40.567126 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93500.solverstate
I1211 13:25:40.572125 12512 solver.cpp:330] Iteration 93500, Testing net (#0)
I1211 13:25:40.572125 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:25:42.095341  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:25:42.155936 12512 solver.cpp:397]     Test net output #0: accuracy = 0.5893
I1211 13:25:42.155936 12512 solver.cpp:397]     Test net output #1: loss = 1.60723 (* 1 = 1.60723 loss)
I1211 13:25:42.217443 12512 solver.cpp:218] Iteration 93500 (12.5602 iter/s, 7.96168s/100 iters), loss = 0.658276
I1211 13:25:42.217443 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 13:25:42.217443 12512 solver.cpp:237]     Train net output #1: loss = 0.658276 (* 1 = 0.658276 loss)
I1211 13:25:42.217443 12512 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1211 13:25:48.579083 12512 solver.cpp:218] Iteration 93600 (15.7185 iter/s, 6.36193s/100 iters), loss = 0.598403
I1211 13:25:48.579083 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 13:25:48.579083 12512 solver.cpp:237]     Train net output #1: loss = 0.598403 (* 1 = 0.598403 loss)
I1211 13:25:48.579083 12512 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1211 13:25:54.931872 12512 solver.cpp:218] Iteration 93700 (15.7438 iter/s, 6.35173s/100 iters), loss = 0.523984
I1211 13:25:54.931872 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:25:54.931872 12512 solver.cpp:237]     Train net output #1: loss = 0.523984 (* 1 = 0.523984 loss)
I1211 13:25:54.931872 12512 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1211 13:26:01.288750 12512 solver.cpp:218] Iteration 93800 (15.732 iter/s, 6.35649s/100 iters), loss = 0.834551
I1211 13:26:01.288750 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 13:26:01.288750 12512 solver.cpp:237]     Train net output #1: loss = 0.834551 (* 1 = 0.834551 loss)
I1211 13:26:01.288750 12512 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1211 13:26:07.637887 12512 solver.cpp:218] Iteration 93900 (15.7515 iter/s, 6.34859s/100 iters), loss = 0.806318
I1211 13:26:07.637887 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 13:26:07.637887 12512 solver.cpp:237]     Train net output #1: loss = 0.806318 (* 1 = 0.806318 loss)
I1211 13:26:07.637887 12512 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1211 13:26:13.663152 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:26:13.912175 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94000.caffemodel
I1211 13:26:13.928174 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94000.solverstate
I1211 13:26:13.933681 12512 solver.cpp:330] Iteration 94000, Testing net (#0)
I1211 13:26:13.933681 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:26:15.454838  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:26:15.514837 12512 solver.cpp:397]     Test net output #0: accuracy = 0.5966
I1211 13:26:15.514837 12512 solver.cpp:397]     Test net output #1: loss = 1.5257 (* 1 = 1.5257 loss)
I1211 13:26:15.576427 12512 solver.cpp:218] Iteration 94000 (12.5974 iter/s, 7.93815s/100 iters), loss = 0.635742
I1211 13:26:15.576427 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:26:15.576427 12512 solver.cpp:237]     Train net output #1: loss = 0.635742 (* 1 = 0.635742 loss)
I1211 13:26:15.576427 12512 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1211 13:26:21.911034 12512 solver.cpp:218] Iteration 94100 (15.7861 iter/s, 6.33469s/100 iters), loss = 0.721295
I1211 13:26:21.911034 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 13:26:21.911034 12512 solver.cpp:237]     Train net output #1: loss = 0.721295 (* 1 = 0.721295 loss)
I1211 13:26:21.911034 12512 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1211 13:26:28.248615 12512 solver.cpp:218] Iteration 94200 (15.7811 iter/s, 6.33671s/100 iters), loss = 0.608887
I1211 13:26:28.248615 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 13:26:28.248615 12512 solver.cpp:237]     Train net output #1: loss = 0.608887 (* 1 = 0.608887 loss)
I1211 13:26:28.248615 12512 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1211 13:26:34.574877 12512 solver.cpp:218] Iteration 94300 (15.8085 iter/s, 6.32573s/100 iters), loss = 0.921116
I1211 13:26:34.574877 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1211 13:26:34.574877 12512 solver.cpp:237]     Train net output #1: loss = 0.921116 (* 1 = 0.921116 loss)
I1211 13:26:34.574877 12512 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1211 13:26:40.924549 12512 solver.cpp:218] Iteration 94400 (15.749 iter/s, 6.34962s/100 iters), loss = 0.805535
I1211 13:26:40.924549 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 13:26:40.924549 12512 solver.cpp:237]     Train net output #1: loss = 0.805535 (* 1 = 0.805535 loss)
I1211 13:26:40.924549 12512 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1211 13:26:46.966449 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:26:47.217701 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94500.caffemodel
I1211 13:26:47.234701 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94500.solverstate
I1211 13:26:47.239699 12512 solver.cpp:330] Iteration 94500, Testing net (#0)
I1211 13:26:47.239699 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:26:48.761531  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:26:48.822032 12512 solver.cpp:397]     Test net output #0: accuracy = 0.5657
I1211 13:26:48.822032 12512 solver.cpp:397]     Test net output #1: loss = 1.74839 (* 1 = 1.74839 loss)
I1211 13:26:48.882751 12512 solver.cpp:218] Iteration 94500 (12.5661 iter/s, 7.9579s/100 iters), loss = 0.625654
I1211 13:26:48.882751 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:26:48.882751 12512 solver.cpp:237]     Train net output #1: loss = 0.625654 (* 1 = 0.625654 loss)
I1211 13:26:48.882751 12512 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1211 13:26:55.237614 12512 solver.cpp:218] Iteration 94600 (15.7387 iter/s, 6.35375s/100 iters), loss = 0.757849
I1211 13:26:55.237614 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 13:26:55.237614 12512 solver.cpp:237]     Train net output #1: loss = 0.757849 (* 1 = 0.757849 loss)
I1211 13:26:55.237614 12512 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1211 13:27:01.582612 12512 solver.cpp:218] Iteration 94700 (15.7599 iter/s, 6.34522s/100 iters), loss = 0.638814
I1211 13:27:01.582612 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 13:27:01.582612 12512 solver.cpp:237]     Train net output #1: loss = 0.638814 (* 1 = 0.638814 loss)
I1211 13:27:01.582612 12512 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1211 13:27:07.934563 12512 solver.cpp:218] Iteration 94800 (15.7436 iter/s, 6.35179s/100 iters), loss = 0.731823
I1211 13:27:07.935564 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 13:27:07.935564 12512 solver.cpp:237]     Train net output #1: loss = 0.731823 (* 1 = 0.731823 loss)
I1211 13:27:07.935564 12512 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1211 13:27:14.284078 12512 solver.cpp:218] Iteration 94900 (15.7523 iter/s, 6.34826s/100 iters), loss = 0.833842
I1211 13:27:14.284078 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 13:27:14.284078 12512 solver.cpp:237]     Train net output #1: loss = 0.833842 (* 1 = 0.833842 loss)
I1211 13:27:14.284078 12512 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1211 13:27:20.323150 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:27:20.573665 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95000.caffemodel
I1211 13:27:20.591171 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95000.solverstate
I1211 13:27:20.596171 12512 solver.cpp:330] Iteration 95000, Testing net (#0)
I1211 13:27:20.596171 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:27:22.118278  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:27:22.179285 12512 solver.cpp:397]     Test net output #0: accuracy = 0.5916
I1211 13:27:22.179285 12512 solver.cpp:397]     Test net output #1: loss = 1.59371 (* 1 = 1.59371 loss)
I1211 13:27:22.239287 12512 solver.cpp:218] Iteration 95000 (12.5712 iter/s, 7.95471s/100 iters), loss = 0.644982
I1211 13:27:22.239287 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 13:27:22.239287 12512 solver.cpp:237]     Train net output #1: loss = 0.644982 (* 1 = 0.644982 loss)
I1211 13:27:22.239287 12512 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1211 13:27:22.239287 12512 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1211 13:27:28.597802 12512 solver.cpp:218] Iteration 95100 (15.7268 iter/s, 6.35857s/100 iters), loss = 0.642054
I1211 13:27:28.597802 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 13:27:28.597802 12512 solver.cpp:237]     Train net output #1: loss = 0.642054 (* 1 = 0.642054 loss)
I1211 13:27:28.597802 12512 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1211 13:27:34.954250 12512 solver.cpp:218] Iteration 95200 (15.7352 iter/s, 6.35517s/100 iters), loss = 0.463778
I1211 13:27:34.954250 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:27:34.954250 12512 solver.cpp:237]     Train net output #1: loss = 0.463778 (* 1 = 0.463778 loss)
I1211 13:27:34.954250 12512 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1211 13:27:41.305734 12512 solver.cpp:218] Iteration 95300 (15.7448 iter/s, 6.35132s/100 iters), loss = 0.623354
I1211 13:27:41.305734 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 13:27:41.305734 12512 solver.cpp:237]     Train net output #1: loss = 0.623354 (* 1 = 0.623354 loss)
I1211 13:27:41.305734 12512 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1211 13:27:47.660223 12512 solver.cpp:218] Iteration 95400 (15.7371 iter/s, 6.35442s/100 iters), loss = 0.501038
I1211 13:27:47.661223 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 13:27:47.661223 12512 solver.cpp:237]     Train net output #1: loss = 0.501038 (* 1 = 0.501038 loss)
I1211 13:27:47.661223 12512 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1211 13:27:53.706674 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:27:53.955698 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95500.caffemodel
I1211 13:27:53.973698 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95500.solverstate
I1211 13:27:53.978202 12512 solver.cpp:330] Iteration 95500, Testing net (#0)
I1211 13:27:53.978202 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:27:55.501812  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:27:55.562811 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6757
I1211 13:27:55.562811 12512 solver.cpp:397]     Test net output #1: loss = 1.20271 (* 1 = 1.20271 loss)
I1211 13:27:55.622818 12512 solver.cpp:218] Iteration 95500 (12.5599 iter/s, 7.96182s/100 iters), loss = 0.516812
I1211 13:27:55.622818 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:27:55.622818 12512 solver.cpp:237]     Train net output #1: loss = 0.516812 (* 1 = 0.516812 loss)
I1211 13:27:55.622818 12512 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1211 13:28:01.968345 12512 solver.cpp:218] Iteration 95600 (15.7604 iter/s, 6.34504s/100 iters), loss = 0.66828
I1211 13:28:01.968345 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 13:28:01.968345 12512 solver.cpp:237]     Train net output #1: loss = 0.66828 (* 1 = 0.66828 loss)
I1211 13:28:01.968345 12512 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1211 13:28:08.321806 12512 solver.cpp:218] Iteration 95700 (15.7404 iter/s, 6.35307s/100 iters), loss = 0.455755
I1211 13:28:08.321806 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:28:08.321806 12512 solver.cpp:237]     Train net output #1: loss = 0.455755 (* 1 = 0.455755 loss)
I1211 13:28:08.321806 12512 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1211 13:28:14.665218 12512 solver.cpp:218] Iteration 95800 (15.7665 iter/s, 6.34258s/100 iters), loss = 0.493131
I1211 13:28:14.665218 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:28:14.665218 12512 solver.cpp:237]     Train net output #1: loss = 0.493131 (* 1 = 0.493131 loss)
I1211 13:28:14.665218 12512 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1211 13:28:21.009730 12512 solver.cpp:218] Iteration 95900 (15.7626 iter/s, 6.34412s/100 iters), loss = 0.565735
I1211 13:28:21.009730 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:28:21.009730 12512 solver.cpp:237]     Train net output #1: loss = 0.565735 (* 1 = 0.565735 loss)
I1211 13:28:21.009730 12512 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1211 13:28:27.045244 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:28:27.296268 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96000.caffemodel
I1211 13:28:27.314270 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96000.solverstate
I1211 13:28:27.319269 12512 solver.cpp:330] Iteration 96000, Testing net (#0)
I1211 13:28:27.319269 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:28:28.842381  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:28:28.902384 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6753
I1211 13:28:28.902384 12512 solver.cpp:397]     Test net output #1: loss = 1.19617 (* 1 = 1.19617 loss)
I1211 13:28:28.963382 12512 solver.cpp:218] Iteration 96000 (12.5731 iter/s, 7.95349s/100 iters), loss = 0.510042
I1211 13:28:28.963382 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:28:28.963382 12512 solver.cpp:237]     Train net output #1: loss = 0.510042 (* 1 = 0.510042 loss)
I1211 13:28:28.963382 12512 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1211 13:28:35.303674 12512 solver.cpp:218] Iteration 96100 (15.7723 iter/s, 6.34024s/100 iters), loss = 0.539349
I1211 13:28:35.304675 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 13:28:35.304675 12512 solver.cpp:237]     Train net output #1: loss = 0.539349 (* 1 = 0.539349 loss)
I1211 13:28:35.304675 12512 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1211 13:28:41.639122 12512 solver.cpp:218] Iteration 96200 (15.7871 iter/s, 6.33428s/100 iters), loss = 0.437251
I1211 13:28:41.639122 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:28:41.639122 12512 solver.cpp:237]     Train net output #1: loss = 0.437251 (* 1 = 0.437251 loss)
I1211 13:28:41.639122 12512 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1211 13:28:47.977596 12512 solver.cpp:218] Iteration 96300 (15.7765 iter/s, 6.33853s/100 iters), loss = 0.548309
I1211 13:28:47.977596 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 13:28:47.977596 12512 solver.cpp:237]     Train net output #1: loss = 0.548309 (* 1 = 0.548309 loss)
I1211 13:28:47.977596 12512 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1211 13:28:54.315129 12512 solver.cpp:218] Iteration 96400 (15.7821 iter/s, 6.33631s/100 iters), loss = 0.459484
I1211 13:28:54.315129 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:28:54.315129 12512 solver.cpp:237]     Train net output #1: loss = 0.459484 (* 1 = 0.459484 loss)
I1211 13:28:54.315129 12512 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1211 13:29:00.337599 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:29:00.585626 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96500.caffemodel
I1211 13:29:00.602632 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96500.solverstate
I1211 13:29:00.607633 12512 solver.cpp:330] Iteration 96500, Testing net (#0)
I1211 13:29:00.607633 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:29:02.128824  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:29:02.190325 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6782
I1211 13:29:02.190325 12512 solver.cpp:397]     Test net output #1: loss = 1.19527 (* 1 = 1.19527 loss)
I1211 13:29:02.249827 12512 solver.cpp:218] Iteration 96500 (12.6024 iter/s, 7.93498s/100 iters), loss = 0.52326
I1211 13:29:02.249827 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:29:02.249827 12512 solver.cpp:237]     Train net output #1: loss = 0.52326 (* 1 = 0.52326 loss)
I1211 13:29:02.249827 12512 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1211 13:29:08.571293 12512 solver.cpp:218] Iteration 96600 (15.8219 iter/s, 6.32035s/100 iters), loss = 0.493612
I1211 13:29:08.571293 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:29:08.571293 12512 solver.cpp:237]     Train net output #1: loss = 0.493612 (* 1 = 0.493612 loss)
I1211 13:29:08.571293 12512 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1211 13:29:14.904724 12512 solver.cpp:218] Iteration 96700 (15.7889 iter/s, 6.33357s/100 iters), loss = 0.388192
I1211 13:29:14.904724 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:29:14.904724 12512 solver.cpp:237]     Train net output #1: loss = 0.388192 (* 1 = 0.388192 loss)
I1211 13:29:14.904724 12512 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1211 13:29:21.239177 12512 solver.cpp:218] Iteration 96800 (15.7879 iter/s, 6.33396s/100 iters), loss = 0.556416
I1211 13:29:21.239177 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:29:21.239177 12512 solver.cpp:237]     Train net output #1: loss = 0.556416 (* 1 = 0.556416 loss)
I1211 13:29:21.239177 12512 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1211 13:29:27.567668 12512 solver.cpp:218] Iteration 96900 (15.8025 iter/s, 6.32812s/100 iters), loss = 0.589239
I1211 13:29:27.567668 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:29:27.567668 12512 solver.cpp:237]     Train net output #1: loss = 0.589239 (* 1 = 0.589239 loss)
I1211 13:29:27.567668 12512 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1211 13:29:33.602116 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:29:33.854126 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97000.caffemodel
I1211 13:29:33.871127 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97000.solverstate
I1211 13:29:33.875128 12512 solver.cpp:330] Iteration 97000, Testing net (#0)
I1211 13:29:33.875128 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:29:35.396773  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:29:35.456275 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6805
I1211 13:29:35.456275 12512 solver.cpp:397]     Test net output #1: loss = 1.19639 (* 1 = 1.19639 loss)
I1211 13:29:35.516295 12512 solver.cpp:218] Iteration 97000 (12.5812 iter/s, 7.94835s/100 iters), loss = 0.518461
I1211 13:29:35.516295 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:29:35.516295 12512 solver.cpp:237]     Train net output #1: loss = 0.518461 (* 1 = 0.518461 loss)
I1211 13:29:35.516295 12512 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1211 13:29:41.855716 12512 solver.cpp:218] Iteration 97100 (15.7766 iter/s, 6.33851s/100 iters), loss = 0.572667
I1211 13:29:41.855716 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:29:41.855716 12512 solver.cpp:237]     Train net output #1: loss = 0.572667 (* 1 = 0.572667 loss)
I1211 13:29:41.855716 12512 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1211 13:29:48.186657 12512 solver.cpp:218] Iteration 97200 (15.7964 iter/s, 6.33055s/100 iters), loss = 0.407815
I1211 13:29:48.186657 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:29:48.186657 12512 solver.cpp:237]     Train net output #1: loss = 0.407815 (* 1 = 0.407815 loss)
I1211 13:29:48.186657 12512 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1211 13:29:54.531399 12512 solver.cpp:218] Iteration 97300 (15.763 iter/s, 6.34396s/100 iters), loss = 0.427075
I1211 13:29:54.531399 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:29:54.531399 12512 solver.cpp:237]     Train net output #1: loss = 0.427075 (* 1 = 0.427075 loss)
I1211 13:29:54.531399 12512 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1211 13:30:00.906625 12512 solver.cpp:218] Iteration 97400 (15.6848 iter/s, 6.37561s/100 iters), loss = 0.490819
I1211 13:30:00.906625 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:30:00.906625 12512 solver.cpp:237]     Train net output #1: loss = 0.490818 (* 1 = 0.490818 loss)
I1211 13:30:00.906625 12512 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1211 13:30:06.973395 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:30:07.224417 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97500.caffemodel
I1211 13:30:07.241420 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97500.solverstate
I1211 13:30:07.246419 12512 solver.cpp:330] Iteration 97500, Testing net (#0)
I1211 13:30:07.246419 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:30:08.766849  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:30:08.827656 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6795
I1211 13:30:08.827656 12512 solver.cpp:397]     Test net output #1: loss = 1.20152 (* 1 = 1.20152 loss)
I1211 13:30:08.887653 12512 solver.cpp:218] Iteration 97500 (12.5314 iter/s, 7.97997s/100 iters), loss = 0.435451
I1211 13:30:08.887653 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:30:08.887653 12512 solver.cpp:237]     Train net output #1: loss = 0.435451 (* 1 = 0.435451 loss)
I1211 13:30:08.887653 12512 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1211 13:30:15.228135 12512 solver.cpp:218] Iteration 97600 (15.7718 iter/s, 6.34041s/100 iters), loss = 0.46788
I1211 13:30:15.228135 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:30:15.228135 12512 solver.cpp:237]     Train net output #1: loss = 0.46788 (* 1 = 0.46788 loss)
I1211 13:30:15.228135 12512 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1211 13:30:21.577520 12512 solver.cpp:218] Iteration 97700 (15.7516 iter/s, 6.34858s/100 iters), loss = 0.409862
I1211 13:30:21.577520 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:30:21.577520 12512 solver.cpp:237]     Train net output #1: loss = 0.409862 (* 1 = 0.409862 loss)
I1211 13:30:21.577520 12512 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1211 13:30:27.913704 12512 solver.cpp:218] Iteration 97800 (15.7837 iter/s, 6.33564s/100 iters), loss = 0.51523
I1211 13:30:27.913704 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:30:27.913704 12512 solver.cpp:237]     Train net output #1: loss = 0.51523 (* 1 = 0.51523 loss)
I1211 13:30:27.913704 12512 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1211 13:30:34.255444 12512 solver.cpp:218] Iteration 97900 (15.7689 iter/s, 6.34161s/100 iters), loss = 0.484785
I1211 13:30:34.255444 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:30:34.255444 12512 solver.cpp:237]     Train net output #1: loss = 0.484785 (* 1 = 0.484785 loss)
I1211 13:30:34.255444 12512 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1211 13:30:40.283504 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:30:40.533558 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98000.caffemodel
I1211 13:30:40.550559 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98000.solverstate
I1211 13:30:40.555560 12512 solver.cpp:330] Iteration 98000, Testing net (#0)
I1211 13:30:40.555560 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:30:42.077294  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:30:42.137132 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6779
I1211 13:30:42.137132 12512 solver.cpp:397]     Test net output #1: loss = 1.19716 (* 1 = 1.19716 loss)
I1211 13:30:42.198132 12512 solver.cpp:218] Iteration 98000 (12.5904 iter/s, 7.94257s/100 iters), loss = 0.454759
I1211 13:30:42.198132 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:30:42.198132 12512 solver.cpp:237]     Train net output #1: loss = 0.454759 (* 1 = 0.454759 loss)
I1211 13:30:42.198132 12512 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1211 13:30:48.551203 12512 solver.cpp:218] Iteration 98100 (15.7433 iter/s, 6.3519s/100 iters), loss = 0.507511
I1211 13:30:48.551203 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:30:48.551203 12512 solver.cpp:237]     Train net output #1: loss = 0.507511 (* 1 = 0.507511 loss)
I1211 13:30:48.551203 12512 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1211 13:30:54.894268 12512 solver.cpp:218] Iteration 98200 (15.7659 iter/s, 6.34278s/100 iters), loss = 0.374326
I1211 13:30:54.894268 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:30:54.894268 12512 solver.cpp:237]     Train net output #1: loss = 0.374326 (* 1 = 0.374326 loss)
I1211 13:30:54.894268 12512 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1211 13:31:01.240739 12512 solver.cpp:218] Iteration 98300 (15.7582 iter/s, 6.34589s/100 iters), loss = 0.460512
I1211 13:31:01.240739 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:31:01.240739 12512 solver.cpp:237]     Train net output #1: loss = 0.460512 (* 1 = 0.460512 loss)
I1211 13:31:01.240739 12512 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1211 13:31:07.585680 12512 solver.cpp:218] Iteration 98400 (15.7606 iter/s, 6.34494s/100 iters), loss = 0.527496
I1211 13:31:07.585680 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:31:07.585680 12512 solver.cpp:237]     Train net output #1: loss = 0.527496 (* 1 = 0.527496 loss)
I1211 13:31:07.585680 12512 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1211 13:31:13.616194 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:31:13.863941 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98500.caffemodel
I1211 13:31:13.881942 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98500.solverstate
I1211 13:31:13.886942 12512 solver.cpp:330] Iteration 98500, Testing net (#0)
I1211 13:31:13.886942 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:31:15.408767  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:31:15.468269 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6804
I1211 13:31:15.469270 12512 solver.cpp:397]     Test net output #1: loss = 1.20124 (* 1 = 1.20124 loss)
I1211 13:31:15.529269 12512 solver.cpp:218] Iteration 98500 (12.59 iter/s, 7.94279s/100 iters), loss = 0.403798
I1211 13:31:15.529269 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:31:15.529269 12512 solver.cpp:237]     Train net output #1: loss = 0.403798 (* 1 = 0.403798 loss)
I1211 13:31:15.529269 12512 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1211 13:31:21.886693 12512 solver.cpp:218] Iteration 98600 (15.7313 iter/s, 6.35673s/100 iters), loss = 0.457163
I1211 13:31:21.886693 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:31:21.886693 12512 solver.cpp:237]     Train net output #1: loss = 0.457163 (* 1 = 0.457163 loss)
I1211 13:31:21.886693 12512 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1211 13:31:28.239001 12512 solver.cpp:218] Iteration 98700 (15.7435 iter/s, 6.35182s/100 iters), loss = 0.41492
I1211 13:31:28.239001 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:31:28.239001 12512 solver.cpp:237]     Train net output #1: loss = 0.41492 (* 1 = 0.41492 loss)
I1211 13:31:28.239001 12512 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1211 13:31:34.588618 12512 solver.cpp:218] Iteration 98800 (15.7492 iter/s, 6.34954s/100 iters), loss = 0.496414
I1211 13:31:34.588618 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:31:34.588618 12512 solver.cpp:237]     Train net output #1: loss = 0.496414 (* 1 = 0.496414 loss)
I1211 13:31:34.588618 12512 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1211 13:31:40.938185 12512 solver.cpp:218] Iteration 98900 (15.7495 iter/s, 6.34939s/100 iters), loss = 0.404263
I1211 13:31:40.938185 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:31:40.938185 12512 solver.cpp:237]     Train net output #1: loss = 0.404263 (* 1 = 0.404263 loss)
I1211 13:31:40.938185 12512 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1211 13:31:46.967649 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:31:47.218670 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99000.caffemodel
I1211 13:31:47.236666 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99000.solverstate
I1211 13:31:47.241667 12512 solver.cpp:330] Iteration 99000, Testing net (#0)
I1211 13:31:47.241667 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:31:48.761801  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:31:48.822804 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 13:31:48.822804 12512 solver.cpp:397]     Test net output #1: loss = 1.20478 (* 1 = 1.20478 loss)
I1211 13:31:48.883811 12512 solver.cpp:218] Iteration 99000 (12.5871 iter/s, 7.94464s/100 iters), loss = 0.455141
I1211 13:31:48.883811 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:31:48.883811 12512 solver.cpp:237]     Train net output #1: loss = 0.455141 (* 1 = 0.455141 loss)
I1211 13:31:48.883811 12512 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1211 13:31:55.237249 12512 solver.cpp:218] Iteration 99100 (15.7411 iter/s, 6.35281s/100 iters), loss = 0.464681
I1211 13:31:55.237249 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:31:55.237249 12512 solver.cpp:237]     Train net output #1: loss = 0.464681 (* 1 = 0.464681 loss)
I1211 13:31:55.237249 12512 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1211 13:32:01.584713 12512 solver.cpp:218] Iteration 99200 (15.7537 iter/s, 6.3477s/100 iters), loss = 0.350588
I1211 13:32:01.584713 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:32:01.584713 12512 solver.cpp:237]     Train net output #1: loss = 0.350588 (* 1 = 0.350588 loss)
I1211 13:32:01.584713 12512 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1211 13:32:07.933166 12512 solver.cpp:218] Iteration 99300 (15.7536 iter/s, 6.34777s/100 iters), loss = 0.482699
I1211 13:32:07.933166 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:32:07.933166 12512 solver.cpp:237]     Train net output #1: loss = 0.482699 (* 1 = 0.482699 loss)
I1211 13:32:07.933166 12512 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1211 13:32:14.280688 12512 solver.cpp:218] Iteration 99400 (15.7558 iter/s, 6.34685s/100 iters), loss = 0.473245
I1211 13:32:14.280688 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 13:32:14.280688 12512 solver.cpp:237]     Train net output #1: loss = 0.473245 (* 1 = 0.473245 loss)
I1211 13:32:14.280688 12512 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1211 13:32:20.316110 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:32:20.565850 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99500.caffemodel
I1211 13:32:20.582048 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99500.solverstate
I1211 13:32:20.587050 12512 solver.cpp:330] Iteration 99500, Testing net (#0)
I1211 13:32:20.588048 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:32:22.108323  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:32:22.168653 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6835
I1211 13:32:22.168653 12512 solver.cpp:397]     Test net output #1: loss = 1.20044 (* 1 = 1.20044 loss)
I1211 13:32:22.228654 12512 solver.cpp:218] Iteration 99500 (12.5824 iter/s, 7.94763s/100 iters), loss = 0.409428
I1211 13:32:22.228654 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:32:22.228654 12512 solver.cpp:237]     Train net output #1: loss = 0.409428 (* 1 = 0.409428 loss)
I1211 13:32:22.228654 12512 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1211 13:32:28.571540 12512 solver.cpp:218] Iteration 99600 (15.7654 iter/s, 6.34301s/100 iters), loss = 0.495768
I1211 13:32:28.571540 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:32:28.572541 12512 solver.cpp:237]     Train net output #1: loss = 0.495768 (* 1 = 0.495768 loss)
I1211 13:32:28.572541 12512 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1211 13:32:34.912314 12512 solver.cpp:218] Iteration 99700 (15.7722 iter/s, 6.34028s/100 iters), loss = 0.501663
I1211 13:32:34.912314 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:32:34.912314 12512 solver.cpp:237]     Train net output #1: loss = 0.501663 (* 1 = 0.501663 loss)
I1211 13:32:34.912314 12512 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1211 13:32:41.256906 12512 solver.cpp:218] Iteration 99800 (15.7637 iter/s, 6.34369s/100 iters), loss = 0.409837
I1211 13:32:41.256906 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:32:41.256906 12512 solver.cpp:237]     Train net output #1: loss = 0.409837 (* 1 = 0.409837 loss)
I1211 13:32:41.256906 12512 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1211 13:32:47.602354 12512 solver.cpp:218] Iteration 99900 (15.7608 iter/s, 6.34485s/100 iters), loss = 0.470026
I1211 13:32:47.602354 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:32:47.602354 12512 solver.cpp:237]     Train net output #1: loss = 0.470025 (* 1 = 0.470025 loss)
I1211 13:32:47.602354 12512 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1211 13:32:53.634758 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:32:53.884786 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100000.caffemodel
I1211 13:32:53.901787 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100000.solverstate
I1211 13:32:53.906787 12512 solver.cpp:330] Iteration 100000, Testing net (#0)
I1211 13:32:53.906787 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:32:55.429906  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:32:55.489917 12512 solver.cpp:397]     Test net output #0: accuracy = 0.681
I1211 13:32:55.489917 12512 solver.cpp:397]     Test net output #1: loss = 1.20636 (* 1 = 1.20636 loss)
I1211 13:32:55.550915 12512 solver.cpp:218] Iteration 100000 (12.5817 iter/s, 7.94805s/100 iters), loss = 0.3882
I1211 13:32:55.550915 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 13:32:55.550915 12512 solver.cpp:237]     Train net output #1: loss = 0.3882 (* 1 = 0.3882 loss)
I1211 13:32:55.550915 12512 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1211 13:33:01.902415 12512 solver.cpp:218] Iteration 100100 (15.7452 iter/s, 6.35114s/100 iters), loss = 0.494595
I1211 13:33:01.902415 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:33:01.902415 12512 solver.cpp:237]     Train net output #1: loss = 0.494595 (* 1 = 0.494595 loss)
I1211 13:33:01.902415 12512 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1211 13:33:08.245817 12512 solver.cpp:218] Iteration 100200 (15.7645 iter/s, 6.34338s/100 iters), loss = 0.343515
I1211 13:33:08.245817 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:33:08.245817 12512 solver.cpp:237]     Train net output #1: loss = 0.343515 (* 1 = 0.343515 loss)
I1211 13:33:08.245817 12512 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1211 13:33:14.583254 12512 solver.cpp:218] Iteration 100300 (15.782 iter/s, 6.33635s/100 iters), loss = 0.393984
I1211 13:33:14.583254 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 13:33:14.583254 12512 solver.cpp:237]     Train net output #1: loss = 0.393984 (* 1 = 0.393984 loss)
I1211 13:33:14.583254 12512 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1211 13:33:20.924751 12512 solver.cpp:218] Iteration 100400 (15.7688 iter/s, 6.34165s/100 iters), loss = 0.371217
I1211 13:33:20.924751 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:33:20.924751 12512 solver.cpp:237]     Train net output #1: loss = 0.371217 (* 1 = 0.371217 loss)
I1211 13:33:20.924751 12512 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1211 13:33:26.956310 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:33:27.206336 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100500.caffemodel
I1211 13:33:27.223335 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100500.solverstate
I1211 13:33:27.228335 12512 solver.cpp:330] Iteration 100500, Testing net (#0)
I1211 13:33:27.228335 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:33:28.750453  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:33:28.810459 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6788
I1211 13:33:28.810459 12512 solver.cpp:397]     Test net output #1: loss = 1.21834 (* 1 = 1.21834 loss)
I1211 13:33:28.870962 12512 solver.cpp:218] Iteration 100500 (12.586 iter/s, 7.94535s/100 iters), loss = 0.4379
I1211 13:33:28.870962 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:33:28.870962 12512 solver.cpp:237]     Train net output #1: loss = 0.4379 (* 1 = 0.4379 loss)
I1211 13:33:28.870962 12512 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1211 13:33:35.200955 12512 solver.cpp:218] Iteration 100600 (15.7985 iter/s, 6.32972s/100 iters), loss = 0.487042
I1211 13:33:35.200955 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:33:35.200955 12512 solver.cpp:237]     Train net output #1: loss = 0.487042 (* 1 = 0.487042 loss)
I1211 13:33:35.200955 12512 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1211 13:33:41.524384 12512 solver.cpp:218] Iteration 100700 (15.8154 iter/s, 6.32295s/100 iters), loss = 0.369438
I1211 13:33:41.524384 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:33:41.524384 12512 solver.cpp:237]     Train net output #1: loss = 0.369438 (* 1 = 0.369438 loss)
I1211 13:33:41.524384 12512 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1211 13:33:47.851848 12512 solver.cpp:218] Iteration 100800 (15.8058 iter/s, 6.32678s/100 iters), loss = 0.395317
I1211 13:33:47.851848 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:33:47.851848 12512 solver.cpp:237]     Train net output #1: loss = 0.395317 (* 1 = 0.395317 loss)
I1211 13:33:47.851848 12512 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1211 13:33:54.174324 12512 solver.cpp:218] Iteration 100900 (15.8173 iter/s, 6.3222s/100 iters), loss = 0.417993
I1211 13:33:54.174324 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:33:54.174324 12512 solver.cpp:237]     Train net output #1: loss = 0.417993 (* 1 = 0.417993 loss)
I1211 13:33:54.174324 12512 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1211 13:34:00.192736 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:34:00.440762 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101000.caffemodel
I1211 13:34:00.457762 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101000.solverstate
I1211 13:34:00.462762 12512 solver.cpp:330] Iteration 101000, Testing net (#0)
I1211 13:34:00.462762 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:34:01.984923  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:34:02.044924 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6819
I1211 13:34:02.044924 12512 solver.cpp:397]     Test net output #1: loss = 1.21171 (* 1 = 1.21171 loss)
I1211 13:34:02.106928 12512 solver.cpp:218] Iteration 101000 (12.6073 iter/s, 7.93192s/100 iters), loss = 0.376626
I1211 13:34:02.106928 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:34:02.106928 12512 solver.cpp:237]     Train net output #1: loss = 0.376626 (* 1 = 0.376626 loss)
I1211 13:34:02.106928 12512 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1211 13:34:08.440407 12512 solver.cpp:218] Iteration 101100 (15.7901 iter/s, 6.33306s/100 iters), loss = 0.445698
I1211 13:34:08.440407 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:34:08.440407 12512 solver.cpp:237]     Train net output #1: loss = 0.445698 (* 1 = 0.445698 loss)
I1211 13:34:08.440407 12512 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1211 13:34:14.775352 12512 solver.cpp:218] Iteration 101200 (15.7862 iter/s, 6.33464s/100 iters), loss = 0.419324
I1211 13:34:14.775352 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:34:14.775352 12512 solver.cpp:237]     Train net output #1: loss = 0.419324 (* 1 = 0.419324 loss)
I1211 13:34:14.775352 12512 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1211 13:34:21.104331 12512 solver.cpp:218] Iteration 101300 (15.7997 iter/s, 6.32925s/100 iters), loss = 0.520033
I1211 13:34:21.104331 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:34:21.104331 12512 solver.cpp:237]     Train net output #1: loss = 0.520033 (* 1 = 0.520033 loss)
I1211 13:34:21.104331 12512 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1211 13:34:27.443827 12512 solver.cpp:218] Iteration 101400 (15.7764 iter/s, 6.33859s/100 iters), loss = 0.4502
I1211 13:34:27.443827 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:34:27.443827 12512 solver.cpp:237]     Train net output #1: loss = 0.4502 (* 1 = 0.4502 loss)
I1211 13:34:27.443827 12512 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1211 13:34:33.466296 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:34:33.715318 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101500.caffemodel
I1211 13:34:33.732317 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101500.solverstate
I1211 13:34:33.737318 12512 solver.cpp:330] Iteration 101500, Testing net (#0)
I1211 13:34:33.737318 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:34:35.259446  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:34:35.320452 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 13:34:35.320452 12512 solver.cpp:397]     Test net output #1: loss = 1.22223 (* 1 = 1.22223 loss)
I1211 13:34:35.380954 12512 solver.cpp:218] Iteration 101500 (12.5999 iter/s, 7.93658s/100 iters), loss = 0.43891
I1211 13:34:35.380954 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:34:35.380954 12512 solver.cpp:237]     Train net output #1: loss = 0.43891 (* 1 = 0.43891 loss)
I1211 13:34:35.380954 12512 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1211 13:34:41.712999 12512 solver.cpp:218] Iteration 101600 (15.7929 iter/s, 6.33197s/100 iters), loss = 0.464358
I1211 13:34:41.712999 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:34:41.712999 12512 solver.cpp:237]     Train net output #1: loss = 0.464358 (* 1 = 0.464358 loss)
I1211 13:34:41.712999 12512 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1211 13:34:48.053488 12512 solver.cpp:218] Iteration 101700 (15.7738 iter/s, 6.33961s/100 iters), loss = 0.378039
I1211 13:34:48.053488 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:34:48.053488 12512 solver.cpp:237]     Train net output #1: loss = 0.378038 (* 1 = 0.378038 loss)
I1211 13:34:48.053488 12512 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1211 13:34:54.399988 12512 solver.cpp:218] Iteration 101800 (15.757 iter/s, 6.3464s/100 iters), loss = 0.556849
I1211 13:34:54.399988 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:34:54.399988 12512 solver.cpp:237]     Train net output #1: loss = 0.556849 (* 1 = 0.556849 loss)
I1211 13:34:54.399988 12512 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1211 13:35:00.728458 12512 solver.cpp:218] Iteration 101900 (15.8029 iter/s, 6.32795s/100 iters), loss = 0.471778
I1211 13:35:00.728458 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:35:00.728458 12512 solver.cpp:237]     Train net output #1: loss = 0.471778 (* 1 = 0.471778 loss)
I1211 13:35:00.728458 12512 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1211 13:35:06.765784 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:35:07.016804 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102000.caffemodel
I1211 13:35:07.033805 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102000.solverstate
I1211 13:35:07.038805 12512 solver.cpp:330] Iteration 102000, Testing net (#0)
I1211 13:35:07.038805 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:35:08.561022  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:35:08.621031 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6808
I1211 13:35:08.621031 12512 solver.cpp:397]     Test net output #1: loss = 1.21401 (* 1 = 1.21401 loss)
I1211 13:35:08.682029 12512 solver.cpp:218] Iteration 102000 (12.5733 iter/s, 7.95339s/100 iters), loss = 0.390527
I1211 13:35:08.682029 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 13:35:08.682029 12512 solver.cpp:237]     Train net output #1: loss = 0.390527 (* 1 = 0.390527 loss)
I1211 13:35:08.682029 12512 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1211 13:35:15.017488 12512 solver.cpp:218] Iteration 102100 (15.7864 iter/s, 6.33457s/100 iters), loss = 0.407925
I1211 13:35:15.017488 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:35:15.017488 12512 solver.cpp:237]     Train net output #1: loss = 0.407925 (* 1 = 0.407925 loss)
I1211 13:35:15.017488 12512 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1211 13:35:21.350965 12512 solver.cpp:218] Iteration 102200 (15.7896 iter/s, 6.33327s/100 iters), loss = 0.331235
I1211 13:35:21.350965 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:35:21.350965 12512 solver.cpp:237]     Train net output #1: loss = 0.331235 (* 1 = 0.331235 loss)
I1211 13:35:21.350965 12512 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1211 13:35:27.690502 12512 solver.cpp:218] Iteration 102300 (15.7759 iter/s, 6.33878s/100 iters), loss = 0.423139
I1211 13:35:27.690502 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:35:27.690502 12512 solver.cpp:237]     Train net output #1: loss = 0.423139 (* 1 = 0.423139 loss)
I1211 13:35:27.690502 12512 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1211 13:35:34.033000 12512 solver.cpp:218] Iteration 102400 (15.7664 iter/s, 6.34259s/100 iters), loss = 0.424578
I1211 13:35:34.033000 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:35:34.033000 12512 solver.cpp:237]     Train net output #1: loss = 0.424578 (* 1 = 0.424578 loss)
I1211 13:35:34.033000 12512 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1211 13:35:40.056448 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:35:40.307477 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102500.caffemodel
I1211 13:35:40.324482 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102500.solverstate
I1211 13:35:40.329483 12512 solver.cpp:330] Iteration 102500, Testing net (#0)
I1211 13:35:40.329483 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:35:41.852586  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:35:41.913592 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6766
I1211 13:35:41.913592 12512 solver.cpp:397]     Test net output #1: loss = 1.21813 (* 1 = 1.21813 loss)
I1211 13:35:41.973592 12512 solver.cpp:218] Iteration 102500 (12.5952 iter/s, 7.93952s/100 iters), loss = 0.396483
I1211 13:35:41.973592 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:35:41.973592 12512 solver.cpp:237]     Train net output #1: loss = 0.396483 (* 1 = 0.396483 loss)
I1211 13:35:41.973592 12512 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1211 13:35:48.335065 12512 solver.cpp:218] Iteration 102600 (15.7207 iter/s, 6.36104s/100 iters), loss = 0.473815
I1211 13:35:48.335065 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:35:48.335065 12512 solver.cpp:237]     Train net output #1: loss = 0.473815 (* 1 = 0.473815 loss)
I1211 13:35:48.335065 12512 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1211 13:35:54.716482 12512 solver.cpp:218] Iteration 102700 (15.6703 iter/s, 6.38149s/100 iters), loss = 0.323557
I1211 13:35:54.716482 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:35:54.716482 12512 solver.cpp:237]     Train net output #1: loss = 0.323557 (* 1 = 0.323557 loss)
I1211 13:35:54.716482 12512 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1211 13:36:01.083022 12512 solver.cpp:218] Iteration 102800 (15.7079 iter/s, 6.36621s/100 iters), loss = 0.495071
I1211 13:36:01.083022 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:36:01.083022 12512 solver.cpp:237]     Train net output #1: loss = 0.49507 (* 1 = 0.49507 loss)
I1211 13:36:01.083022 12512 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1211 13:36:07.438935 12512 solver.cpp:218] Iteration 102900 (15.7358 iter/s, 6.35493s/100 iters), loss = 0.446713
I1211 13:36:07.438935 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:36:07.438935 12512 solver.cpp:237]     Train net output #1: loss = 0.446713 (* 1 = 0.446713 loss)
I1211 13:36:07.438935 12512 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1211 13:36:13.470842 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:36:13.722853 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103000.caffemodel
I1211 13:36:13.736853 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103000.solverstate
I1211 13:36:13.742857 12512 solver.cpp:330] Iteration 103000, Testing net (#0)
I1211 13:36:13.742857 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:36:15.264981  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:36:15.324988 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6816
I1211 13:36:15.324988 12512 solver.cpp:397]     Test net output #1: loss = 1.2176 (* 1 = 1.2176 loss)
I1211 13:36:15.385984 12512 solver.cpp:218] Iteration 103000 (12.5832 iter/s, 7.94709s/100 iters), loss = 0.372461
I1211 13:36:15.385984 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:36:15.385984 12512 solver.cpp:237]     Train net output #1: loss = 0.37246 (* 1 = 0.37246 loss)
I1211 13:36:15.385984 12512 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1211 13:36:21.757496 12512 solver.cpp:218] Iteration 103100 (15.6955 iter/s, 6.37127s/100 iters), loss = 0.440436
I1211 13:36:21.757496 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:36:21.757496 12512 solver.cpp:237]     Train net output #1: loss = 0.440436 (* 1 = 0.440436 loss)
I1211 13:36:21.757496 12512 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1211 13:36:28.166438 12512 solver.cpp:218] Iteration 103200 (15.6044 iter/s, 6.40844s/100 iters), loss = 0.432831
I1211 13:36:28.166438 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:36:28.166438 12512 solver.cpp:237]     Train net output #1: loss = 0.432831 (* 1 = 0.432831 loss)
I1211 13:36:28.166438 12512 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1211 13:36:34.634914 12512 solver.cpp:218] Iteration 103300 (15.4612 iter/s, 6.46782s/100 iters), loss = 0.369991
I1211 13:36:34.634914 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:36:34.634914 12512 solver.cpp:237]     Train net output #1: loss = 0.369991 (* 1 = 0.369991 loss)
I1211 13:36:34.634914 12512 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1211 13:36:41.076453 12512 solver.cpp:218] Iteration 103400 (15.5257 iter/s, 6.44092s/100 iters), loss = 0.378361
I1211 13:36:41.076453 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:36:41.076453 12512 solver.cpp:237]     Train net output #1: loss = 0.378361 (* 1 = 0.378361 loss)
I1211 13:36:41.076453 12512 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1211 13:36:47.186843 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:36:47.444859 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103500.caffemodel
I1211 13:36:47.461865 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103500.solverstate
I1211 13:36:47.466866 12512 solver.cpp:330] Iteration 103500, Testing net (#0)
I1211 13:36:47.466866 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:36:48.999975  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:36:49.060982 12512 solver.cpp:397]     Test net output #0: accuracy = 0.682
I1211 13:36:49.060982 12512 solver.cpp:397]     Test net output #1: loss = 1.21785 (* 1 = 1.21785 loss)
I1211 13:36:49.120980 12512 solver.cpp:218] Iteration 103500 (12.4311 iter/s, 8.04435s/100 iters), loss = 0.480813
I1211 13:36:49.120980 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:36:49.120980 12512 solver.cpp:237]     Train net output #1: loss = 0.480813 (* 1 = 0.480813 loss)
I1211 13:36:49.120980 12512 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1211 13:36:55.564369 12512 solver.cpp:218] Iteration 103600 (15.5219 iter/s, 6.4425s/100 iters), loss = 0.461012
I1211 13:36:55.564369 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:36:55.564369 12512 solver.cpp:237]     Train net output #1: loss = 0.461012 (* 1 = 0.461012 loss)
I1211 13:36:55.564369 12512 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1211 13:37:01.969039 12512 solver.cpp:218] Iteration 103700 (15.6143 iter/s, 6.40437s/100 iters), loss = 0.324439
I1211 13:37:01.969039 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:37:01.969039 12512 solver.cpp:237]     Train net output #1: loss = 0.324439 (* 1 = 0.324439 loss)
I1211 13:37:01.969039 12512 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1211 13:37:08.364550 12512 solver.cpp:218] Iteration 103800 (15.6368 iter/s, 6.39519s/100 iters), loss = 0.496744
I1211 13:37:08.364550 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:37:08.364550 12512 solver.cpp:237]     Train net output #1: loss = 0.496744 (* 1 = 0.496744 loss)
I1211 13:37:08.364550 12512 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1211 13:37:14.761113 12512 solver.cpp:218] Iteration 103900 (15.6348 iter/s, 6.396s/100 iters), loss = 0.41762
I1211 13:37:14.761113 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:37:14.761113 12512 solver.cpp:237]     Train net output #1: loss = 0.41762 (* 1 = 0.41762 loss)
I1211 13:37:14.761113 12512 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1211 13:37:20.837532 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:37:21.089051 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104000.caffemodel
I1211 13:37:21.106552 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104000.solverstate
I1211 13:37:21.111553 12512 solver.cpp:330] Iteration 104000, Testing net (#0)
I1211 13:37:21.112561 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:37:22.645678  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:37:22.706681 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6788
I1211 13:37:22.706681 12512 solver.cpp:397]     Test net output #1: loss = 1.23126 (* 1 = 1.23126 loss)
I1211 13:37:22.768683 12512 solver.cpp:218] Iteration 104000 (12.49 iter/s, 8.00639s/100 iters), loss = 0.472623
I1211 13:37:22.768683 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:37:22.768683 12512 solver.cpp:237]     Train net output #1: loss = 0.472623 (* 1 = 0.472623 loss)
I1211 13:37:22.768683 12512 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1211 13:37:29.146203 12512 solver.cpp:218] Iteration 104100 (15.6797 iter/s, 6.37766s/100 iters), loss = 0.410862
I1211 13:37:29.146203 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:37:29.146203 12512 solver.cpp:237]     Train net output #1: loss = 0.410862 (* 1 = 0.410862 loss)
I1211 13:37:29.146203 12512 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1211 13:37:35.522747 12512 solver.cpp:218] Iteration 104200 (15.6843 iter/s, 6.37582s/100 iters), loss = 0.282627
I1211 13:37:35.522747 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 13:37:35.522747 12512 solver.cpp:237]     Train net output #1: loss = 0.282627 (* 1 = 0.282627 loss)
I1211 13:37:35.522747 12512 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1211 13:37:41.877300 12512 solver.cpp:218] Iteration 104300 (15.7371 iter/s, 6.3544s/100 iters), loss = 0.370624
I1211 13:37:41.877300 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:37:41.877300 12512 solver.cpp:237]     Train net output #1: loss = 0.370624 (* 1 = 0.370624 loss)
I1211 13:37:41.877300 12512 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1211 13:37:48.219756 12512 solver.cpp:218] Iteration 104400 (15.7687 iter/s, 6.34166s/100 iters), loss = 0.366481
I1211 13:37:48.219756 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:37:48.219756 12512 solver.cpp:237]     Train net output #1: loss = 0.366481 (* 1 = 0.366481 loss)
I1211 13:37:48.219756 12512 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1211 13:37:54.250236 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:37:54.498749 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104500.caffemodel
I1211 13:37:54.515259 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104500.solverstate
I1211 13:37:54.520259 12512 solver.cpp:330] Iteration 104500, Testing net (#0)
I1211 13:37:54.520259 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:37:56.044381  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:37:56.105388 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 13:37:56.105388 12512 solver.cpp:397]     Test net output #1: loss = 1.22691 (* 1 = 1.22691 loss)
I1211 13:37:56.165388 12512 solver.cpp:218] Iteration 104500 (12.5874 iter/s, 7.94447s/100 iters), loss = 0.395248
I1211 13:37:56.165388 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:37:56.165388 12512 solver.cpp:237]     Train net output #1: loss = 0.395248 (* 1 = 0.395248 loss)
I1211 13:37:56.165388 12512 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1211 13:38:02.499912 12512 solver.cpp:218] Iteration 104600 (15.788 iter/s, 6.33391s/100 iters), loss = 0.402604
I1211 13:38:02.499912 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:38:02.499912 12512 solver.cpp:237]     Train net output #1: loss = 0.402604 (* 1 = 0.402604 loss)
I1211 13:38:02.499912 12512 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1211 13:38:08.833364 12512 solver.cpp:218] Iteration 104700 (15.7896 iter/s, 6.33328s/100 iters), loss = 0.34351
I1211 13:38:08.833364 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:38:08.833364 12512 solver.cpp:237]     Train net output #1: loss = 0.34351 (* 1 = 0.34351 loss)
I1211 13:38:08.833364 12512 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1211 13:38:15.166908 12512 solver.cpp:218] Iteration 104800 (15.7895 iter/s, 6.33332s/100 iters), loss = 0.360556
I1211 13:38:15.166908 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:38:15.166908 12512 solver.cpp:237]     Train net output #1: loss = 0.360556 (* 1 = 0.360556 loss)
I1211 13:38:15.166908 12512 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1211 13:38:21.498440 12512 solver.cpp:218] Iteration 104900 (15.7957 iter/s, 6.33086s/100 iters), loss = 0.40211
I1211 13:38:21.498440 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:38:21.498440 12512 solver.cpp:237]     Train net output #1: loss = 0.40211 (* 1 = 0.40211 loss)
I1211 13:38:21.498440 12512 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1211 13:38:27.521929 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:38:27.770951 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105000.caffemodel
I1211 13:38:27.787953 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105000.solverstate
I1211 13:38:27.793457 12512 solver.cpp:330] Iteration 105000, Testing net (#0)
I1211 13:38:27.793457 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:38:29.315147  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:38:29.375149 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 13:38:29.375149 12512 solver.cpp:397]     Test net output #1: loss = 1.23276 (* 1 = 1.23276 loss)
I1211 13:38:29.436154 12512 solver.cpp:218] Iteration 105000 (12.5984 iter/s, 7.9375s/100 iters), loss = 0.321903
I1211 13:38:29.436154 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:38:29.436154 12512 solver.cpp:237]     Train net output #1: loss = 0.321903 (* 1 = 0.321903 loss)
I1211 13:38:29.436154 12512 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1211 13:38:35.791466 12512 solver.cpp:218] Iteration 105100 (15.737 iter/s, 6.35443s/100 iters), loss = 0.378462
I1211 13:38:35.791466 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:38:35.791466 12512 solver.cpp:237]     Train net output #1: loss = 0.378462 (* 1 = 0.378462 loss)
I1211 13:38:35.791466 12512 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1211 13:38:42.139006 12512 solver.cpp:218] Iteration 105200 (15.7548 iter/s, 6.34729s/100 iters), loss = 0.268348
I1211 13:38:42.139006 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 13:38:42.139006 12512 solver.cpp:237]     Train net output #1: loss = 0.268347 (* 1 = 0.268347 loss)
I1211 13:38:42.139006 12512 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1211 13:38:48.493471 12512 solver.cpp:218] Iteration 105300 (15.7386 iter/s, 6.35381s/100 iters), loss = 0.351403
I1211 13:38:48.493471 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:38:48.493471 12512 solver.cpp:237]     Train net output #1: loss = 0.351403 (* 1 = 0.351403 loss)
I1211 13:38:48.493471 12512 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1211 13:38:54.838975 12512 solver.cpp:218] Iteration 105400 (15.7589 iter/s, 6.3456s/100 iters), loss = 0.385381
I1211 13:38:54.838975 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:38:54.838975 12512 solver.cpp:237]     Train net output #1: loss = 0.385381 (* 1 = 0.385381 loss)
I1211 13:38:54.838975 12512 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1211 13:39:00.867509 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:39:01.116529 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105500.caffemodel
I1211 13:39:01.133529 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105500.solverstate
I1211 13:39:01.138530 12512 solver.cpp:330] Iteration 105500, Testing net (#0)
I1211 13:39:01.138530 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:39:02.662650  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:39:02.722661 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6802
I1211 13:39:02.723661 12512 solver.cpp:397]     Test net output #1: loss = 1.23383 (* 1 = 1.23383 loss)
I1211 13:39:02.783663 12512 solver.cpp:218] Iteration 105500 (12.5885 iter/s, 7.94379s/100 iters), loss = 0.282455
I1211 13:39:02.783663 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 13:39:02.783663 12512 solver.cpp:237]     Train net output #1: loss = 0.282455 (* 1 = 0.282455 loss)
I1211 13:39:02.783663 12512 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1211 13:39:09.123152 12512 solver.cpp:218] Iteration 105600 (15.7737 iter/s, 6.33968s/100 iters), loss = 0.369796
I1211 13:39:09.123152 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:39:09.123152 12512 solver.cpp:237]     Train net output #1: loss = 0.369796 (* 1 = 0.369796 loss)
I1211 13:39:09.123152 12512 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1211 13:39:15.461599 12512 solver.cpp:218] Iteration 105700 (15.7789 iter/s, 6.33757s/100 iters), loss = 0.304798
I1211 13:39:15.461599 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 13:39:15.461599 12512 solver.cpp:237]     Train net output #1: loss = 0.304798 (* 1 = 0.304798 loss)
I1211 13:39:15.461599 12512 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1211 13:39:21.808989 12512 solver.cpp:218] Iteration 105800 (15.7561 iter/s, 6.34674s/100 iters), loss = 0.442664
I1211 13:39:21.808989 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:39:21.808989 12512 solver.cpp:237]     Train net output #1: loss = 0.442664 (* 1 = 0.442664 loss)
I1211 13:39:21.808989 12512 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1211 13:39:28.157447 12512 solver.cpp:218] Iteration 105900 (15.7523 iter/s, 6.34827s/100 iters), loss = 0.3837
I1211 13:39:28.157447 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:39:28.157447 12512 solver.cpp:237]     Train net output #1: loss = 0.3837 (* 1 = 0.3837 loss)
I1211 13:39:28.157447 12512 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1211 13:39:34.198426 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:39:34.448935 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106000.caffemodel
I1211 13:39:34.462934 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106000.solverstate
I1211 13:39:34.467933 12512 solver.cpp:330] Iteration 106000, Testing net (#0)
I1211 13:39:34.467933 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:39:35.993755  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:39:36.053261 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6779
I1211 13:39:36.053261 12512 solver.cpp:397]     Test net output #1: loss = 1.23988 (* 1 = 1.23988 loss)
I1211 13:39:36.114266 12512 solver.cpp:218] Iteration 106000 (12.569 iter/s, 7.95608s/100 iters), loss = 0.310674
I1211 13:39:36.114266 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 13:39:36.114266 12512 solver.cpp:237]     Train net output #1: loss = 0.310673 (* 1 = 0.310673 loss)
I1211 13:39:36.114266 12512 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1211 13:39:42.442685 12512 solver.cpp:218] Iteration 106100 (15.8018 iter/s, 6.32838s/100 iters), loss = 0.405544
I1211 13:39:42.442685 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:39:42.442685 12512 solver.cpp:237]     Train net output #1: loss = 0.405544 (* 1 = 0.405544 loss)
I1211 13:39:42.442685 12512 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1211 13:39:48.779197 12512 solver.cpp:218] Iteration 106200 (15.7827 iter/s, 6.33605s/100 iters), loss = 0.244368
I1211 13:39:48.779197 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 13:39:48.779197 12512 solver.cpp:237]     Train net output #1: loss = 0.244368 (* 1 = 0.244368 loss)
I1211 13:39:48.779197 12512 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1211 13:39:55.115682 12512 solver.cpp:218] Iteration 106300 (15.7817 iter/s, 6.33645s/100 iters), loss = 0.460004
I1211 13:39:55.116683 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:39:55.116683 12512 solver.cpp:237]     Train net output #1: loss = 0.460004 (* 1 = 0.460004 loss)
I1211 13:39:55.116683 12512 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1211 13:40:01.506105 12512 solver.cpp:218] Iteration 106400 (15.6506 iter/s, 6.38954s/100 iters), loss = 0.468387
I1211 13:40:01.506105 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:40:01.506105 12512 solver.cpp:237]     Train net output #1: loss = 0.468387 (* 1 = 0.468387 loss)
I1211 13:40:01.506105 12512 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1211 13:40:07.590950 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:40:07.840966 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106500.caffemodel
I1211 13:40:07.857972 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106500.solverstate
I1211 13:40:07.862972 12512 solver.cpp:330] Iteration 106500, Testing net (#0)
I1211 13:40:07.862972 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:40:09.385083  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:40:09.446085 12512 solver.cpp:397]     Test net output #0: accuracy = 0.678
I1211 13:40:09.446085 12512 solver.cpp:397]     Test net output #1: loss = 1.23981 (* 1 = 1.23981 loss)
I1211 13:40:09.507087 12512 solver.cpp:218] Iteration 106500 (12.5003 iter/s, 7.99982s/100 iters), loss = 0.31342
I1211 13:40:09.507087 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 13:40:09.507087 12512 solver.cpp:237]     Train net output #1: loss = 0.313419 (* 1 = 0.313419 loss)
I1211 13:40:09.507087 12512 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1211 13:40:15.853574 12512 solver.cpp:218] Iteration 106600 (15.7571 iter/s, 6.34636s/100 iters), loss = 0.382286
I1211 13:40:15.853574 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:40:15.853574 12512 solver.cpp:237]     Train net output #1: loss = 0.382286 (* 1 = 0.382286 loss)
I1211 13:40:15.853574 12512 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1211 13:40:22.207080 12512 solver.cpp:218] Iteration 106700 (15.7394 iter/s, 6.35348s/100 iters), loss = 0.271103
I1211 13:40:22.207080 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 13:40:22.207080 12512 solver.cpp:237]     Train net output #1: loss = 0.271103 (* 1 = 0.271103 loss)
I1211 13:40:22.207080 12512 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1211 13:40:28.549558 12512 solver.cpp:218] Iteration 106800 (15.7695 iter/s, 6.34134s/100 iters), loss = 0.390206
I1211 13:40:28.549558 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:40:28.549558 12512 solver.cpp:237]     Train net output #1: loss = 0.390206 (* 1 = 0.390206 loss)
I1211 13:40:28.549558 12512 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1211 13:40:34.901010 12512 solver.cpp:218] Iteration 106900 (15.7452 iter/s, 6.35113s/100 iters), loss = 0.428714
I1211 13:40:34.901010 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:40:34.901010 12512 solver.cpp:237]     Train net output #1: loss = 0.428714 (* 1 = 0.428714 loss)
I1211 13:40:34.901010 12512 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1211 13:40:40.933498 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:40:41.182520 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107000.caffemodel
I1211 13:40:41.200520 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107000.solverstate
I1211 13:40:41.204519 12512 solver.cpp:330] Iteration 107000, Testing net (#0)
I1211 13:40:41.205520 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:40:42.726632  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:40:42.787642 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6772
I1211 13:40:42.787642 12512 solver.cpp:397]     Test net output #1: loss = 1.24178 (* 1 = 1.24178 loss)
I1211 13:40:42.848644 12512 solver.cpp:218] Iteration 107000 (12.5831 iter/s, 7.94718s/100 iters), loss = 0.32672
I1211 13:40:42.848644 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 13:40:42.848644 12512 solver.cpp:237]     Train net output #1: loss = 0.32672 (* 1 = 0.32672 loss)
I1211 13:40:42.848644 12512 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1211 13:40:49.205121 12512 solver.cpp:218] Iteration 107100 (15.7318 iter/s, 6.35654s/100 iters), loss = 0.476475
I1211 13:40:49.205121 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:40:49.205121 12512 solver.cpp:237]     Train net output #1: loss = 0.476475 (* 1 = 0.476475 loss)
I1211 13:40:49.205121 12512 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1211 13:40:55.557116 12512 solver.cpp:218] Iteration 107200 (15.7447 iter/s, 6.35134s/100 iters), loss = 0.349462
I1211 13:40:55.557617 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:40:55.557617 12512 solver.cpp:237]     Train net output #1: loss = 0.349462 (* 1 = 0.349462 loss)
I1211 13:40:55.557617 12512 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1211 13:41:01.913100 12512 solver.cpp:218] Iteration 107300 (15.7354 iter/s, 6.3551s/100 iters), loss = 0.406661
I1211 13:41:01.913100 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:41:01.913100 12512 solver.cpp:237]     Train net output #1: loss = 0.406661 (* 1 = 0.406661 loss)
I1211 13:41:01.913100 12512 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1211 13:41:08.268565 12512 solver.cpp:218] Iteration 107400 (15.7342 iter/s, 6.3556s/100 iters), loss = 0.383875
I1211 13:41:08.268565 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:41:08.268565 12512 solver.cpp:237]     Train net output #1: loss = 0.383875 (* 1 = 0.383875 loss)
I1211 13:41:08.268565 12512 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1211 13:41:14.315063 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:41:14.567083 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107500.caffemodel
I1211 13:41:14.584084 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107500.solverstate
I1211 13:41:14.589083 12512 solver.cpp:330] Iteration 107500, Testing net (#0)
I1211 13:41:14.589083 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:41:16.110316  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:41:16.170327 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6787
I1211 13:41:16.170327 12512 solver.cpp:397]     Test net output #1: loss = 1.24679 (* 1 = 1.24679 loss)
I1211 13:41:16.231326 12512 solver.cpp:218] Iteration 107500 (12.5593 iter/s, 7.96223s/100 iters), loss = 0.353461
I1211 13:41:16.231326 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:41:16.231326 12512 solver.cpp:237]     Train net output #1: loss = 0.353461 (* 1 = 0.353461 loss)
I1211 13:41:16.231326 12512 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1211 13:41:22.578850 12512 solver.cpp:218] Iteration 107600 (15.7546 iter/s, 6.34737s/100 iters), loss = 0.374842
I1211 13:41:22.578850 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:41:22.578850 12512 solver.cpp:237]     Train net output #1: loss = 0.374842 (* 1 = 0.374842 loss)
I1211 13:41:22.578850 12512 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1211 13:41:28.933446 12512 solver.cpp:218] Iteration 107700 (15.7386 iter/s, 6.35382s/100 iters), loss = 0.333496
I1211 13:41:28.933446 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:41:28.933446 12512 solver.cpp:237]     Train net output #1: loss = 0.333496 (* 1 = 0.333496 loss)
I1211 13:41:28.933446 12512 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1211 13:41:35.285725 12512 solver.cpp:218] Iteration 107800 (15.7433 iter/s, 6.35192s/100 iters), loss = 0.294065
I1211 13:41:35.285725 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:41:35.285725 12512 solver.cpp:237]     Train net output #1: loss = 0.294065 (* 1 = 0.294065 loss)
I1211 13:41:35.285725 12512 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1211 13:41:41.631187 12512 solver.cpp:218] Iteration 107900 (15.7608 iter/s, 6.34487s/100 iters), loss = 0.437935
I1211 13:41:41.631187 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:41:41.631187 12512 solver.cpp:237]     Train net output #1: loss = 0.437935 (* 1 = 0.437935 loss)
I1211 13:41:41.631187 12512 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1211 13:41:47.668772 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:41:47.919783 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108000.caffemodel
I1211 13:41:47.936784 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108000.solverstate
I1211 13:41:47.940783 12512 solver.cpp:330] Iteration 108000, Testing net (#0)
I1211 13:41:47.941784 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:41:49.462393  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:41:49.522895 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6783
I1211 13:41:49.522895 12512 solver.cpp:397]     Test net output #1: loss = 1.24064 (* 1 = 1.24064 loss)
I1211 13:41:49.581913 12512 solver.cpp:218] Iteration 108000 (12.5783 iter/s, 7.95021s/100 iters), loss = 0.375682
I1211 13:41:49.581913 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:41:49.581913 12512 solver.cpp:237]     Train net output #1: loss = 0.375682 (* 1 = 0.375682 loss)
I1211 13:41:49.581913 12512 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1211 13:41:55.930330 12512 solver.cpp:218] Iteration 108100 (15.7526 iter/s, 6.34816s/100 iters), loss = 0.421378
I1211 13:41:55.930330 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:41:55.930330 12512 solver.cpp:237]     Train net output #1: loss = 0.421378 (* 1 = 0.421378 loss)
I1211 13:41:55.930330 12512 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1211 13:42:02.283766 12512 solver.cpp:218] Iteration 108200 (15.7412 iter/s, 6.35277s/100 iters), loss = 0.357272
I1211 13:42:02.283766 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:42:02.283766 12512 solver.cpp:237]     Train net output #1: loss = 0.357272 (* 1 = 0.357272 loss)
I1211 13:42:02.283766 12512 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1211 13:42:08.632221 12512 solver.cpp:218] Iteration 108300 (15.7529 iter/s, 6.34802s/100 iters), loss = 0.332994
I1211 13:42:08.632221 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 13:42:08.632221 12512 solver.cpp:237]     Train net output #1: loss = 0.332994 (* 1 = 0.332994 loss)
I1211 13:42:08.632221 12512 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1211 13:42:14.977700 12512 solver.cpp:218] Iteration 108400 (15.76 iter/s, 6.34517s/100 iters), loss = 0.353303
I1211 13:42:14.977700 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:42:14.977700 12512 solver.cpp:237]     Train net output #1: loss = 0.353303 (* 1 = 0.353303 loss)
I1211 13:42:14.977700 12512 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1211 13:42:21.006148 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:42:21.256666 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108500.caffemodel
I1211 13:42:21.274170 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108500.solverstate
I1211 13:42:21.280174 12512 solver.cpp:330] Iteration 108500, Testing net (#0)
I1211 13:42:21.280174 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:42:22.799295  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:42:22.860298 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6793
I1211 13:42:22.860298 12512 solver.cpp:397]     Test net output #1: loss = 1.24555 (* 1 = 1.24555 loss)
I1211 13:42:22.920300 12512 solver.cpp:218] Iteration 108500 (12.5905 iter/s, 7.94247s/100 iters), loss = 0.321941
I1211 13:42:22.920300 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 13:42:22.920300 12512 solver.cpp:237]     Train net output #1: loss = 0.321941 (* 1 = 0.321941 loss)
I1211 13:42:22.920300 12512 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1211 13:42:29.250795 12512 solver.cpp:218] Iteration 108600 (15.7979 iter/s, 6.32996s/100 iters), loss = 0.461202
I1211 13:42:29.250795 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:42:29.250795 12512 solver.cpp:237]     Train net output #1: loss = 0.461202 (* 1 = 0.461202 loss)
I1211 13:42:29.250795 12512 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1211 13:42:35.580214 12512 solver.cpp:218] Iteration 108700 (15.8013 iter/s, 6.3286s/100 iters), loss = 0.339444
I1211 13:42:35.580214 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:42:35.580214 12512 solver.cpp:237]     Train net output #1: loss = 0.339444 (* 1 = 0.339444 loss)
I1211 13:42:35.580214 12512 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1211 13:42:41.912669 12512 solver.cpp:218] Iteration 108800 (15.7928 iter/s, 6.33199s/100 iters), loss = 0.368232
I1211 13:42:41.912669 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:42:41.912669 12512 solver.cpp:237]     Train net output #1: loss = 0.368232 (* 1 = 0.368232 loss)
I1211 13:42:41.912669 12512 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1211 13:42:48.257174 12512 solver.cpp:218] Iteration 108900 (15.7627 iter/s, 6.3441s/100 iters), loss = 0.382574
I1211 13:42:48.257174 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:42:48.257174 12512 solver.cpp:237]     Train net output #1: loss = 0.382574 (* 1 = 0.382574 loss)
I1211 13:42:48.257174 12512 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1211 13:42:54.284617 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:42:54.536628 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109000.caffemodel
I1211 13:42:54.551628 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109000.solverstate
I1211 13:42:54.556632 12512 solver.cpp:330] Iteration 109000, Testing net (#0)
I1211 13:42:54.556632 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:42:56.078744  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:42:56.138746 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6782
I1211 13:42:56.138746 12512 solver.cpp:397]     Test net output #1: loss = 1.24008 (* 1 = 1.24008 loss)
I1211 13:42:56.199748 12512 solver.cpp:218] Iteration 109000 (12.5915 iter/s, 7.94183s/100 iters), loss = 0.269629
I1211 13:42:56.199748 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 13:42:56.199748 12512 solver.cpp:237]     Train net output #1: loss = 0.269629 (* 1 = 0.269629 loss)
I1211 13:42:56.199748 12512 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1211 13:43:02.526214 12512 solver.cpp:218] Iteration 109100 (15.8056 iter/s, 6.32688s/100 iters), loss = 0.403467
I1211 13:43:02.527215 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:43:02.527215 12512 solver.cpp:237]     Train net output #1: loss = 0.403467 (* 1 = 0.403467 loss)
I1211 13:43:02.527215 12512 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1211 13:43:08.864187 12512 solver.cpp:218] Iteration 109200 (15.7809 iter/s, 6.33679s/100 iters), loss = 0.304669
I1211 13:43:08.864187 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:43:08.864187 12512 solver.cpp:237]     Train net output #1: loss = 0.304669 (* 1 = 0.304669 loss)
I1211 13:43:08.864187 12512 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1211 13:43:15.205193 12512 solver.cpp:218] Iteration 109300 (15.7698 iter/s, 6.34123s/100 iters), loss = 0.364106
I1211 13:43:15.205193 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:43:15.205193 12512 solver.cpp:237]     Train net output #1: loss = 0.364106 (* 1 = 0.364106 loss)
I1211 13:43:15.205193 12512 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1211 13:43:21.543552 12512 solver.cpp:218] Iteration 109400 (15.7789 iter/s, 6.33759s/100 iters), loss = 0.373234
I1211 13:43:21.543552 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:43:21.543552 12512 solver.cpp:237]     Train net output #1: loss = 0.373234 (* 1 = 0.373234 loss)
I1211 13:43:21.543552 12512 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1211 13:43:27.570101 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:43:27.818114 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109500.caffemodel
I1211 13:43:27.834115 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109500.solverstate
I1211 13:43:27.838114 12512 solver.cpp:330] Iteration 109500, Testing net (#0)
I1211 13:43:27.838114 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:43:29.360214  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:43:29.420222 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6798
I1211 13:43:29.420222 12512 solver.cpp:397]     Test net output #1: loss = 1.25245 (* 1 = 1.25245 loss)
I1211 13:43:29.480223 12512 solver.cpp:218] Iteration 109500 (12.5999 iter/s, 7.93659s/100 iters), loss = 0.333121
I1211 13:43:29.481223 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:43:29.481223 12512 solver.cpp:237]     Train net output #1: loss = 0.333121 (* 1 = 0.333121 loss)
I1211 13:43:29.481223 12512 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1211 13:43:35.822701 12512 solver.cpp:218] Iteration 109600 (15.768 iter/s, 6.34197s/100 iters), loss = 0.415573
I1211 13:43:35.822701 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:43:35.822701 12512 solver.cpp:237]     Train net output #1: loss = 0.415573 (* 1 = 0.415573 loss)
I1211 13:43:35.822701 12512 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1211 13:43:42.167721 12512 solver.cpp:218] Iteration 109700 (15.7633 iter/s, 6.34384s/100 iters), loss = 0.281649
I1211 13:43:42.167721 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 13:43:42.167721 12512 solver.cpp:237]     Train net output #1: loss = 0.281649 (* 1 = 0.281649 loss)
I1211 13:43:42.167721 12512 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1211 13:43:48.518682 12512 solver.cpp:218] Iteration 109800 (15.7457 iter/s, 6.35094s/100 iters), loss = 0.380873
I1211 13:43:48.518682 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:43:48.518682 12512 solver.cpp:237]     Train net output #1: loss = 0.380873 (* 1 = 0.380873 loss)
I1211 13:43:48.518682 12512 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1211 13:43:54.866364 12512 solver.cpp:218] Iteration 109900 (15.7552 iter/s, 6.3471s/100 iters), loss = 0.428793
I1211 13:43:54.866364 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:43:54.866865 12512 solver.cpp:237]     Train net output #1: loss = 0.428793 (* 1 = 0.428793 loss)
I1211 13:43:54.866865 12512 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1211 13:44:00.898846 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:44:01.148851 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_110000.caffemodel
I1211 13:44:01.167851 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_110000.solverstate
I1211 13:44:01.172850 12512 solver.cpp:330] Iteration 110000, Testing net (#0)
I1211 13:44:01.172850 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:44:02.695953  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:44:02.755960 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6788
I1211 13:44:02.755960 12512 solver.cpp:397]     Test net output #1: loss = 1.25187 (* 1 = 1.25187 loss)
I1211 13:44:02.817462 12512 solver.cpp:218] Iteration 110000 (12.5783 iter/s, 7.95022s/100 iters), loss = 0.336805
I1211 13:44:02.817462 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:44:02.817462 12512 solver.cpp:237]     Train net output #1: loss = 0.336805 (* 1 = 0.336805 loss)
I1211 13:44:02.817462 12512 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1211 13:44:09.168429 12512 solver.cpp:218] Iteration 110100 (15.7452 iter/s, 6.35114s/100 iters), loss = 0.395358
I1211 13:44:09.168429 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:44:09.168429 12512 solver.cpp:237]     Train net output #1: loss = 0.395358 (* 1 = 0.395358 loss)
I1211 13:44:09.168429 12512 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1211 13:44:15.521873 12512 solver.cpp:218] Iteration 110200 (15.7421 iter/s, 6.35239s/100 iters), loss = 0.312139
I1211 13:44:15.521873 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:44:15.521873 12512 solver.cpp:237]     Train net output #1: loss = 0.312139 (* 1 = 0.312139 loss)
I1211 13:44:15.521873 12512 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1211 13:44:21.868347 12512 solver.cpp:218] Iteration 110300 (15.7563 iter/s, 6.34665s/100 iters), loss = 0.344569
I1211 13:44:21.868347 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:44:21.868347 12512 solver.cpp:237]     Train net output #1: loss = 0.344569 (* 1 = 0.344569 loss)
I1211 13:44:21.868347 12512 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1211 13:44:28.211320 12512 solver.cpp:218] Iteration 110400 (15.7677 iter/s, 6.34207s/100 iters), loss = 0.376733
I1211 13:44:28.211320 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:44:28.211320 12512 solver.cpp:237]     Train net output #1: loss = 0.376733 (* 1 = 0.376733 loss)
I1211 13:44:28.211320 12512 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1211 13:44:34.241283 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:44:34.492296 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_110500.caffemodel
I1211 13:44:34.509304 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_110500.solverstate
I1211 13:44:34.514303 12512 solver.cpp:330] Iteration 110500, Testing net (#0)
I1211 13:44:34.514303 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:44:36.037405  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:44:36.097404 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6767
I1211 13:44:36.097404 12512 solver.cpp:397]     Test net output #1: loss = 1.25536 (* 1 = 1.25536 loss)
I1211 13:44:36.157409 12512 solver.cpp:218] Iteration 110500 (12.5855 iter/s, 7.94565s/100 iters), loss = 0.30808
I1211 13:44:36.157409 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:44:36.157409 12512 solver.cpp:237]     Train net output #1: loss = 0.30808 (* 1 = 0.30808 loss)
I1211 13:44:36.157409 12512 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1211 13:44:42.494912 12512 solver.cpp:218] Iteration 110600 (15.7791 iter/s, 6.33749s/100 iters), loss = 0.442004
I1211 13:44:42.494912 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:44:42.495913 12512 solver.cpp:237]     Train net output #1: loss = 0.442004 (* 1 = 0.442004 loss)
I1211 13:44:42.495913 12512 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1211 13:44:48.832371 12512 solver.cpp:218] Iteration 110700 (15.782 iter/s, 6.33632s/100 iters), loss = 0.330616
I1211 13:44:48.832371 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:44:48.832371 12512 solver.cpp:237]     Train net output #1: loss = 0.330616 (* 1 = 0.330616 loss)
I1211 13:44:48.832371 12512 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1211 13:44:55.160830 12512 solver.cpp:218] Iteration 110800 (15.8025 iter/s, 6.32811s/100 iters), loss = 0.427531
I1211 13:44:55.160830 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:44:55.160830 12512 solver.cpp:237]     Train net output #1: loss = 0.427531 (* 1 = 0.427531 loss)
I1211 13:44:55.160830 12512 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1211 13:45:01.497274 12512 solver.cpp:218] Iteration 110900 (15.7838 iter/s, 6.33559s/100 iters), loss = 0.336811
I1211 13:45:01.497274 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:45:01.497274 12512 solver.cpp:237]     Train net output #1: loss = 0.336811 (* 1 = 0.336811 loss)
I1211 13:45:01.497274 12512 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1211 13:45:07.522183 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:45:07.769711 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_111000.caffemodel
I1211 13:45:07.787714 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_111000.solverstate
I1211 13:45:07.792712 12512 solver.cpp:330] Iteration 111000, Testing net (#0)
I1211 13:45:07.792712 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:45:09.315318  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:45:09.375829 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6765
I1211 13:45:09.375829 12512 solver.cpp:397]     Test net output #1: loss = 1.25966 (* 1 = 1.25966 loss)
I1211 13:45:09.436836 12512 solver.cpp:218] Iteration 111000 (12.5956 iter/s, 7.93927s/100 iters), loss = 0.33951
I1211 13:45:09.436836 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 13:45:09.436836 12512 solver.cpp:237]     Train net output #1: loss = 0.33951 (* 1 = 0.33951 loss)
I1211 13:45:09.436836 12512 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1211 13:45:15.767299 12512 solver.cpp:218] Iteration 111100 (15.7986 iter/s, 6.32969s/100 iters), loss = 0.362647
I1211 13:45:15.767299 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 13:45:15.767299 12512 solver.cpp:237]     Train net output #1: loss = 0.362647 (* 1 = 0.362647 loss)
I1211 13:45:15.767299 12512 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1211 13:45:22.111291 12512 solver.cpp:218] Iteration 111200 (15.7632 iter/s, 6.3439s/100 iters), loss = 0.292603
I1211 13:45:22.111793 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 13:45:22.111793 12512 solver.cpp:237]     Train net output #1: loss = 0.292603 (* 1 = 0.292603 loss)
I1211 13:45:22.111793 12512 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1211 13:45:28.446255 12512 solver.cpp:218] Iteration 111300 (15.7858 iter/s, 6.33483s/100 iters), loss = 0.383887
I1211 13:45:28.446255 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:45:28.446255 12512 solver.cpp:237]     Train net output #1: loss = 0.383887 (* 1 = 0.383887 loss)
I1211 13:45:28.446255 12512 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1211 13:45:34.779696 12512 solver.cpp:218] Iteration 111400 (15.7913 iter/s, 6.3326s/100 iters), loss = 0.370766
I1211 13:45:34.779696 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:45:34.779696 12512 solver.cpp:237]     Train net output #1: loss = 0.370766 (* 1 = 0.370766 loss)
I1211 13:45:34.779696 12512 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1211 13:45:40.812682 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:45:41.062196 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_111500.caffemodel
I1211 13:45:41.079196 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_111500.solverstate
I1211 13:45:41.084197 12512 solver.cpp:330] Iteration 111500, Testing net (#0)
I1211 13:45:41.084197 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:45:42.606318  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:45:42.666366 12512 solver.cpp:397]     Test net output #0: accuracy = 0.6779
I1211 13:45:42.666366 12512 solver.cpp:397]     Test net output #1: loss = 1.26714 (* 1 = 1.26714 loss)
I1211 13:45:42.727356 12512 solver.cpp:218] Iteration 111500 (12.5828 iter/s, 7.94736s/100 iters), loss = 0.285861
I1211 13:45:42.727356 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 13:45:42.727356 12512 solver.cpp:237]     Train net output #1: loss = 0.285861 (* 1 = 0.285861 loss)
I1211 13:45:42.727356 12512 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1211 13:45:49.063798 12512 solver.cpp:218] Iteration 111600 (15.783 iter/s, 6.33591s/100 iters), loss = 0.35528
I1211 13:45:49.063798 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:45:49.063798 12512 solver.cpp:237]     Train net output #1: loss = 0.35528 (* 1 = 0.35528 loss)
I1211 13:45:49.063798 12512 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1211 13:45:55.396287 12512 solver.cpp:218] Iteration 111700 (15.7931 iter/s, 6.33187s/100 iters), loss = 0.323149
I1211 13:45:55.396287 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 13:45:55.396287 12512 solver.cpp:237]     Train net output #1: loss = 0.323149 (* 1 = 0.323149 loss)
I1211 13:45:55.396287 12512 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1211 13:46:01.727237 12512 solver.cpp:218] Iteration 111800 (15.7967 iter/s, 6.33043s/100 iters), loss = 0.296402
I1211 13:46:01.727237 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 13:46:01.727237 12512 solver.cpp:237]     Train net output #1: loss = 0.296402 (* 1 = 0.296402 loss)
I1211 13:46:01.727237 12512 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1211 13:46:08.063179 12512 solver.cpp:218] Iteration 111900 (15.7833 iter/s, 6.33581s/100 iters), loss = 0.395466
I1211 13:46:08.063179 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:46:08.063179 12512 solver.cpp:237]     Train net output #1: loss = 0.395466 (* 1 = 0.395466 loss)
I1211 13:46:08.063179 12512 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1211 13:46:14.081657 15104 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:46:14.330673 12512 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_112000.caffemodel
I1211 13:46:14.347673 12512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_112000.solverstate
I1211 13:46:14.352674 12512 solver.cpp:330] Iteration 112000, Testing net (#0)
I1211 13:46:14.352674 12512 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:46:15.875787  5628 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:46:15.936791 12512 solver.cpp:397]     Test net output #0: accuracy = 0.678
I1211 13:46:15.936791 12512 solver.cpp:397]     Test net output #1: loss = 1.27138 (* 1 = 1.27138 loss)
I1211 13:46:15.997790 12512 solver.cpp:218] Iteration 112000 (12.6035 iter/s, 7.93431s/100 iters), loss = 0.238844
I1211 13:46:15.997790 12512 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 13:46:15.997790 12512 solver.cpp:237]     Train net output #1: loss = 0.238844 (* 1 = 0.238844 loss)
I1211 13:46:15.997790 12512 sgd_solver.cpp:105] Iteration 112000, lr = 0.001
I1211 13:46:22.349313 12512 solver.cpp:2

G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.solverstate 
I1211 10:21:12.877177 11272 caffe.cpp:219] Using GPUs 0
I1211 10:21:13.058215 11272 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1211 10:21:13.402429 11272 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 10:21:13.419447 11272 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1211 10:21:13.420447 11272 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 10:21:13.420447 11272 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 10:21:13.420447 11272 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_added1
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_added2
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1211 10:21:13.421447 11272 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1211 10:21:13.421447 11272 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added1"
  type: "BatchNorm"
  bottom: "newconv_added1"
  top: "newconv_added1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added1"
  type: "Scale"
  bottom: "newconv_added1"
  top: "newconv_added1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added1"
  type: "ReLU"
  bottom: "newconv_added1"
  top: "newconv_added1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added2"
  type: "BatchNorm"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added2"
  type: "Scale"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added2"
  type: "ReLU"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 10:21:13.436448 11272 layer_factory.cpp:58] Creating layer cifar
I1211 10:21:13.443433 11272 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1211 10:21:13.443433 11272 net.cpp:84] Creating Layer cifar
I1211 10:21:13.443433 11272 net.cpp:380] cifar -> data
I1211 10:21:13.443433 11272 net.cpp:380] cifar -> label
I1211 10:21:13.444429 11272 data_layer.cpp:45] output data size: 100,3,32,32
I1211 10:21:13.449957 11272 net.cpp:122] Setting up cifar
I1211 10:21:13.449957 11272 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 10:21:13.449957 11272 net.cpp:129] Top shape: 100 (100)
I1211 10:21:13.449957 11272 net.cpp:137] Memory required for data: 1229200
I1211 10:21:13.449957 11272 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 10:21:13.449957 11272 net.cpp:84] Creating Layer label_cifar_1_split
I1211 10:21:13.449957 11272 net.cpp:406] label_cifar_1_split <- label
I1211 10:21:13.449957 11272 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 10:21:13.449957 11272 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 10:21:13.450441 11272 net.cpp:122] Setting up label_cifar_1_split
I1211 10:21:13.450441 11272 net.cpp:129] Top shape: 100 (100)
I1211 10:21:13.450441 11272 net.cpp:129] Top shape: 100 (100)
I1211 10:21:13.450441 11272 net.cpp:137] Memory required for data: 1230000
I1211 10:21:13.450441 11272 layer_factory.cpp:58] Creating layer conv1
I1211 10:21:13.450441 11272 net.cpp:84] Creating Layer conv1
I1211 10:21:13.450441 11272 net.cpp:406] conv1 <- data
I1211 10:21:13.450441 11272 net.cpp:380] conv1 -> conv1
I1211 10:21:13.451442 13688 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 10:21:13.695565 11272 net.cpp:122] Setting up conv1
I1211 10:21:13.695565 11272 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 10:21:13.695565 11272 net.cpp:137] Memory required for data: 13518000
I1211 10:21:13.695565 11272 layer_factory.cpp:58] Creating layer bn1
I1211 10:21:13.696566 11272 net.cpp:84] Creating Layer bn1
I1211 10:21:13.696566 11272 net.cpp:406] bn1 <- conv1
I1211 10:21:13.696566 11272 net.cpp:367] bn1 -> conv1 (in-place)
I1211 10:21:13.696566 11272 net.cpp:122] Setting up bn1
I1211 10:21:13.696566 11272 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 10:21:13.696566 11272 net.cpp:137] Memory required for data: 25806000
I1211 10:21:13.696566 11272 layer_factory.cpp:58] Creating layer scale1
I1211 10:21:13.696566 11272 net.cpp:84] Creating Layer scale1
I1211 10:21:13.696566 11272 net.cpp:406] scale1 <- conv1
I1211 10:21:13.696566 11272 net.cpp:367] scale1 -> conv1 (in-place)
I1211 10:21:13.696566 11272 layer_factory.cpp:58] Creating layer scale1
I1211 10:21:13.696566 11272 net.cpp:122] Setting up scale1
I1211 10:21:13.696566 11272 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 10:21:13.696566 11272 net.cpp:137] Memory required for data: 38094000
I1211 10:21:13.696566 11272 layer_factory.cpp:58] Creating layer relu1
I1211 10:21:13.696566 11272 net.cpp:84] Creating Layer relu1
I1211 10:21:13.696566 11272 net.cpp:406] relu1 <- conv1
I1211 10:21:13.696566 11272 net.cpp:367] relu1 -> conv1 (in-place)
I1211 10:21:13.696566 11272 net.cpp:122] Setting up relu1
I1211 10:21:13.696566 11272 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 10:21:13.696566 11272 net.cpp:137] Memory required for data: 50382000
I1211 10:21:13.696566 11272 layer_factory.cpp:58] Creating layer conv1_0
I1211 10:21:13.696566 11272 net.cpp:84] Creating Layer conv1_0
I1211 10:21:13.696566 11272 net.cpp:406] conv1_0 <- conv1
I1211 10:21:13.696566 11272 net.cpp:380] conv1_0 -> conv1_0
I1211 10:21:13.698570 11272 net.cpp:122] Setting up conv1_0
I1211 10:21:13.698570 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.698570 11272 net.cpp:137] Memory required for data: 66766000
I1211 10:21:13.698570 11272 layer_factory.cpp:58] Creating layer bn1_0
I1211 10:21:13.698570 11272 net.cpp:84] Creating Layer bn1_0
I1211 10:21:13.698570 11272 net.cpp:406] bn1_0 <- conv1_0
I1211 10:21:13.698570 11272 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 10:21:13.698570 11272 net.cpp:122] Setting up bn1_0
I1211 10:21:13.698570 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.698570 11272 net.cpp:137] Memory required for data: 83150000
I1211 10:21:13.698570 11272 layer_factory.cpp:58] Creating layer scale1_0
I1211 10:21:13.698570 11272 net.cpp:84] Creating Layer scale1_0
I1211 10:21:13.698570 11272 net.cpp:406] scale1_0 <- conv1_0
I1211 10:21:13.698570 11272 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 10:21:13.698570 11272 layer_factory.cpp:58] Creating layer scale1_0
I1211 10:21:13.698570 11272 net.cpp:122] Setting up scale1_0
I1211 10:21:13.698570 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.698570 11272 net.cpp:137] Memory required for data: 99534000
I1211 10:21:13.698570 11272 layer_factory.cpp:58] Creating layer relu1_0
I1211 10:21:13.698570 11272 net.cpp:84] Creating Layer relu1_0
I1211 10:21:13.698570 11272 net.cpp:406] relu1_0 <- conv1_0
I1211 10:21:13.698570 11272 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 10:21:13.699566 11272 net.cpp:122] Setting up relu1_0
I1211 10:21:13.699566 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.699566 11272 net.cpp:137] Memory required for data: 115918000
I1211 10:21:13.699566 11272 layer_factory.cpp:58] Creating layer conv2
I1211 10:21:13.699566 11272 net.cpp:84] Creating Layer conv2
I1211 10:21:13.699566 11272 net.cpp:406] conv2 <- conv1_0
I1211 10:21:13.699566 11272 net.cpp:380] conv2 -> conv2
I1211 10:21:13.700567 11272 net.cpp:122] Setting up conv2
I1211 10:21:13.700567 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.700567 11272 net.cpp:137] Memory required for data: 132302000
I1211 10:21:13.700567 11272 layer_factory.cpp:58] Creating layer bn2
I1211 10:21:13.700567 11272 net.cpp:84] Creating Layer bn2
I1211 10:21:13.700567 11272 net.cpp:406] bn2 <- conv2
I1211 10:21:13.700567 11272 net.cpp:367] bn2 -> conv2 (in-place)
I1211 10:21:13.700567 11272 net.cpp:122] Setting up bn2
I1211 10:21:13.700567 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.700567 11272 net.cpp:137] Memory required for data: 148686000
I1211 10:21:13.700567 11272 layer_factory.cpp:58] Creating layer scale2
I1211 10:21:13.700567 11272 net.cpp:84] Creating Layer scale2
I1211 10:21:13.700567 11272 net.cpp:406] scale2 <- conv2
I1211 10:21:13.700567 11272 net.cpp:367] scale2 -> conv2 (in-place)
I1211 10:21:13.700567 11272 layer_factory.cpp:58] Creating layer scale2
I1211 10:21:13.700567 11272 net.cpp:122] Setting up scale2
I1211 10:21:13.700567 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.700567 11272 net.cpp:137] Memory required for data: 165070000
I1211 10:21:13.700567 11272 layer_factory.cpp:58] Creating layer relu2
I1211 10:21:13.700567 11272 net.cpp:84] Creating Layer relu2
I1211 10:21:13.700567 11272 net.cpp:406] relu2 <- conv2
I1211 10:21:13.700567 11272 net.cpp:367] relu2 -> conv2 (in-place)
I1211 10:21:13.700567 11272 net.cpp:122] Setting up relu2
I1211 10:21:13.700567 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.700567 11272 net.cpp:137] Memory required for data: 181454000
I1211 10:21:13.700567 11272 layer_factory.cpp:58] Creating layer conv2_1
I1211 10:21:13.700567 11272 net.cpp:84] Creating Layer conv2_1
I1211 10:21:13.700567 11272 net.cpp:406] conv2_1 <- conv2
I1211 10:21:13.700567 11272 net.cpp:380] conv2_1 -> conv2_1
I1211 10:21:13.701565 11272 net.cpp:122] Setting up conv2_1
I1211 10:21:13.701565 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.701565 11272 net.cpp:137] Memory required for data: 197838000
I1211 10:21:13.701565 11272 layer_factory.cpp:58] Creating layer bn2_1
I1211 10:21:13.701565 11272 net.cpp:84] Creating Layer bn2_1
I1211 10:21:13.701565 11272 net.cpp:406] bn2_1 <- conv2_1
I1211 10:21:13.701565 11272 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 10:21:13.701565 11272 net.cpp:122] Setting up bn2_1
I1211 10:21:13.701565 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.701565 11272 net.cpp:137] Memory required for data: 214222000
I1211 10:21:13.701565 11272 layer_factory.cpp:58] Creating layer scale2_1
I1211 10:21:13.701565 11272 net.cpp:84] Creating Layer scale2_1
I1211 10:21:13.702565 11272 net.cpp:406] scale2_1 <- conv2_1
I1211 10:21:13.702565 11272 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 10:21:13.702565 11272 layer_factory.cpp:58] Creating layer scale2_1
I1211 10:21:13.702565 11272 net.cpp:122] Setting up scale2_1
I1211 10:21:13.702565 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.702565 11272 net.cpp:137] Memory required for data: 230606000
I1211 10:21:13.702565 11272 layer_factory.cpp:58] Creating layer relu2_1
I1211 10:21:13.702565 11272 net.cpp:84] Creating Layer relu2_1
I1211 10:21:13.702565 11272 net.cpp:406] relu2_1 <- conv2_1
I1211 10:21:13.702565 11272 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 10:21:13.702565 11272 net.cpp:122] Setting up relu2_1
I1211 10:21:13.702565 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.702565 11272 net.cpp:137] Memory required for data: 246990000
I1211 10:21:13.702565 11272 layer_factory.cpp:58] Creating layer conv2_2
I1211 10:21:13.702565 11272 net.cpp:84] Creating Layer conv2_2
I1211 10:21:13.702565 11272 net.cpp:406] conv2_2 <- conv2_1
I1211 10:21:13.702565 11272 net.cpp:380] conv2_2 -> conv2_2
I1211 10:21:13.704586 11272 net.cpp:122] Setting up conv2_2
I1211 10:21:13.704586 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.704586 11272 net.cpp:137] Memory required for data: 267470000
I1211 10:21:13.704586 11272 layer_factory.cpp:58] Creating layer bn2_2
I1211 10:21:13.704586 11272 net.cpp:84] Creating Layer bn2_2
I1211 10:21:13.704586 11272 net.cpp:406] bn2_2 <- conv2_2
I1211 10:21:13.704586 11272 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 10:21:13.704586 11272 net.cpp:122] Setting up bn2_2
I1211 10:21:13.704586 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.704586 11272 net.cpp:137] Memory required for data: 287950000
I1211 10:21:13.704586 11272 layer_factory.cpp:58] Creating layer scale2_2
I1211 10:21:13.704586 11272 net.cpp:84] Creating Layer scale2_2
I1211 10:21:13.704586 11272 net.cpp:406] scale2_2 <- conv2_2
I1211 10:21:13.704586 11272 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 10:21:13.704586 11272 layer_factory.cpp:58] Creating layer scale2_2
I1211 10:21:13.704586 11272 net.cpp:122] Setting up scale2_2
I1211 10:21:13.704586 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.704586 11272 net.cpp:137] Memory required for data: 308430000
I1211 10:21:13.704586 11272 layer_factory.cpp:58] Creating layer relu2_2
I1211 10:21:13.704586 11272 net.cpp:84] Creating Layer relu2_2
I1211 10:21:13.704586 11272 net.cpp:406] relu2_2 <- conv2_2
I1211 10:21:13.704586 11272 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 10:21:13.705569 11272 net.cpp:122] Setting up relu2_2
I1211 10:21:13.705569 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.705569 11272 net.cpp:137] Memory required for data: 328910000
I1211 10:21:13.705569 11272 layer_factory.cpp:58] Creating layer newconv_added1
I1211 10:21:13.705569 11272 net.cpp:84] Creating Layer newconv_added1
I1211 10:21:13.705569 11272 net.cpp:406] newconv_added1 <- conv2_2
I1211 10:21:13.705569 11272 net.cpp:380] newconv_added1 -> newconv_added1
I1211 10:21:13.706568 11272 net.cpp:122] Setting up newconv_added1
I1211 10:21:13.706568 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.706568 11272 net.cpp:137] Memory required for data: 349390000
I1211 10:21:13.706568 11272 layer_factory.cpp:58] Creating layer bn_added1
I1211 10:21:13.706568 11272 net.cpp:84] Creating Layer bn_added1
I1211 10:21:13.706568 11272 net.cpp:406] bn_added1 <- newconv_added1
I1211 10:21:13.706568 11272 net.cpp:367] bn_added1 -> newconv_added1 (in-place)
I1211 10:21:13.706568 11272 net.cpp:122] Setting up bn_added1
I1211 10:21:13.706568 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.706568 11272 net.cpp:137] Memory required for data: 369870000
I1211 10:21:13.706568 11272 layer_factory.cpp:58] Creating layer scale_added1
I1211 10:21:13.706568 11272 net.cpp:84] Creating Layer scale_added1
I1211 10:21:13.706568 11272 net.cpp:406] scale_added1 <- newconv_added1
I1211 10:21:13.706568 11272 net.cpp:367] scale_added1 -> newconv_added1 (in-place)
I1211 10:21:13.706568 11272 layer_factory.cpp:58] Creating layer scale_added1
I1211 10:21:13.706568 11272 net.cpp:122] Setting up scale_added1
I1211 10:21:13.706568 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.706568 11272 net.cpp:137] Memory required for data: 390350000
I1211 10:21:13.706568 11272 layer_factory.cpp:58] Creating layer relu_added1
I1211 10:21:13.706568 11272 net.cpp:84] Creating Layer relu_added1
I1211 10:21:13.706568 11272 net.cpp:406] relu_added1 <- newconv_added1
I1211 10:21:13.706568 11272 net.cpp:367] relu_added1 -> newconv_added1 (in-place)
I1211 10:21:13.707586 11272 net.cpp:122] Setting up relu_added1
I1211 10:21:13.707586 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.707586 11272 net.cpp:137] Memory required for data: 410830000
I1211 10:21:13.707586 11272 layer_factory.cpp:58] Creating layer pool2_1
I1211 10:21:13.707586 11272 net.cpp:84] Creating Layer pool2_1
I1211 10:21:13.707586 11272 net.cpp:406] pool2_1 <- newconv_added1
I1211 10:21:13.707586 11272 net.cpp:380] pool2_1 -> pool2_1
I1211 10:21:13.707586 11272 net.cpp:122] Setting up pool2_1
I1211 10:21:13.707586 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.707586 11272 net.cpp:137] Memory required for data: 415950000
I1211 10:21:13.707586 11272 layer_factory.cpp:58] Creating layer conv3
I1211 10:21:13.707586 11272 net.cpp:84] Creating Layer conv3
I1211 10:21:13.707586 11272 net.cpp:406] conv3 <- pool2_1
I1211 10:21:13.707586 11272 net.cpp:380] conv3 -> conv3
I1211 10:21:13.708585 11272 net.cpp:122] Setting up conv3
I1211 10:21:13.708585 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.708585 11272 net.cpp:137] Memory required for data: 421070000
I1211 10:21:13.708585 11272 layer_factory.cpp:58] Creating layer bn3
I1211 10:21:13.708585 11272 net.cpp:84] Creating Layer bn3
I1211 10:21:13.708585 11272 net.cpp:406] bn3 <- conv3
I1211 10:21:13.708585 11272 net.cpp:367] bn3 -> conv3 (in-place)
I1211 10:21:13.709581 11272 net.cpp:122] Setting up bn3
I1211 10:21:13.709581 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.709581 11272 net.cpp:137] Memory required for data: 426190000
I1211 10:21:13.709581 11272 layer_factory.cpp:58] Creating layer scale3
I1211 10:21:13.709581 11272 net.cpp:84] Creating Layer scale3
I1211 10:21:13.709581 11272 net.cpp:406] scale3 <- conv3
I1211 10:21:13.709581 11272 net.cpp:367] scale3 -> conv3 (in-place)
I1211 10:21:13.709581 11272 layer_factory.cpp:58] Creating layer scale3
I1211 10:21:13.709581 11272 net.cpp:122] Setting up scale3
I1211 10:21:13.709581 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.709581 11272 net.cpp:137] Memory required for data: 431310000
I1211 10:21:13.709581 11272 layer_factory.cpp:58] Creating layer relu3
I1211 10:21:13.709581 11272 net.cpp:84] Creating Layer relu3
I1211 10:21:13.709581 11272 net.cpp:406] relu3 <- conv3
I1211 10:21:13.709581 11272 net.cpp:367] relu3 -> conv3 (in-place)
I1211 10:21:13.709581 11272 net.cpp:122] Setting up relu3
I1211 10:21:13.709581 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.709581 11272 net.cpp:137] Memory required for data: 436430000
I1211 10:21:13.709581 11272 layer_factory.cpp:58] Creating layer conv3_1
I1211 10:21:13.709581 11272 net.cpp:84] Creating Layer conv3_1
I1211 10:21:13.709581 11272 net.cpp:406] conv3_1 <- conv3
I1211 10:21:13.709581 11272 net.cpp:380] conv3_1 -> conv3_1
I1211 10:21:13.710580 11272 net.cpp:122] Setting up conv3_1
I1211 10:21:13.710580 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.710580 11272 net.cpp:137] Memory required for data: 441550000
I1211 10:21:13.710580 11272 layer_factory.cpp:58] Creating layer bn3_1
I1211 10:21:13.710580 11272 net.cpp:84] Creating Layer bn3_1
I1211 10:21:13.710580 11272 net.cpp:406] bn3_1 <- conv3_1
I1211 10:21:13.710580 11272 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 10:21:13.710580 11272 net.cpp:122] Setting up bn3_1
I1211 10:21:13.710580 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.711580 11272 net.cpp:137] Memory required for data: 446670000
I1211 10:21:13.711580 11272 layer_factory.cpp:58] Creating layer scale3_1
I1211 10:21:13.711580 11272 net.cpp:84] Creating Layer scale3_1
I1211 10:21:13.711580 11272 net.cpp:406] scale3_1 <- conv3_1
I1211 10:21:13.711580 11272 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 10:21:13.711580 11272 layer_factory.cpp:58] Creating layer scale3_1
I1211 10:21:13.711580 11272 net.cpp:122] Setting up scale3_1
I1211 10:21:13.711580 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.711580 11272 net.cpp:137] Memory required for data: 451790000
I1211 10:21:13.711580 11272 layer_factory.cpp:58] Creating layer relu3_1
I1211 10:21:13.711580 11272 net.cpp:84] Creating Layer relu3_1
I1211 10:21:13.711580 11272 net.cpp:406] relu3_1 <- conv3_1
I1211 10:21:13.711580 11272 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 10:21:13.711580 11272 net.cpp:122] Setting up relu3_1
I1211 10:21:13.711580 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.711580 11272 net.cpp:137] Memory required for data: 456910000
I1211 10:21:13.711580 11272 layer_factory.cpp:58] Creating layer conv4
I1211 10:21:13.711580 11272 net.cpp:84] Creating Layer conv4
I1211 10:21:13.711580 11272 net.cpp:406] conv4 <- conv3_1
I1211 10:21:13.711580 11272 net.cpp:380] conv4 -> conv4
I1211 10:21:13.712580 11272 net.cpp:122] Setting up conv4
I1211 10:21:13.712580 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.712580 11272 net.cpp:137] Memory required for data: 462030000
I1211 10:21:13.712580 11272 layer_factory.cpp:58] Creating layer bn4
I1211 10:21:13.712580 11272 net.cpp:84] Creating Layer bn4
I1211 10:21:13.712580 11272 net.cpp:406] bn4 <- conv4
I1211 10:21:13.712580 11272 net.cpp:367] bn4 -> conv4 (in-place)
I1211 10:21:13.712580 11272 net.cpp:122] Setting up bn4
I1211 10:21:13.712580 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.712580 11272 net.cpp:137] Memory required for data: 467150000
I1211 10:21:13.712580 11272 layer_factory.cpp:58] Creating layer scale4
I1211 10:21:13.712580 11272 net.cpp:84] Creating Layer scale4
I1211 10:21:13.712580 11272 net.cpp:406] scale4 <- conv4
I1211 10:21:13.712580 11272 net.cpp:367] scale4 -> conv4 (in-place)
I1211 10:21:13.713582 11272 layer_factory.cpp:58] Creating layer scale4
I1211 10:21:13.713582 11272 net.cpp:122] Setting up scale4
I1211 10:21:13.713582 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.713582 11272 net.cpp:137] Memory required for data: 472270000
I1211 10:21:13.713582 11272 layer_factory.cpp:58] Creating layer relu4
I1211 10:21:13.713582 11272 net.cpp:84] Creating Layer relu4
I1211 10:21:13.713582 11272 net.cpp:406] relu4 <- conv4
I1211 10:21:13.713582 11272 net.cpp:367] relu4 -> conv4 (in-place)
I1211 10:21:13.713582 11272 net.cpp:122] Setting up relu4
I1211 10:21:13.713582 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.713582 11272 net.cpp:137] Memory required for data: 477390000
I1211 10:21:13.713582 11272 layer_factory.cpp:58] Creating layer conv4_1
I1211 10:21:13.713582 11272 net.cpp:84] Creating Layer conv4_1
I1211 10:21:13.713582 11272 net.cpp:406] conv4_1 <- conv4
I1211 10:21:13.713582 11272 net.cpp:380] conv4_1 -> conv4_1
I1211 10:21:13.715567 11272 net.cpp:122] Setting up conv4_1
I1211 10:21:13.715567 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.715567 11272 net.cpp:137] Memory required for data: 482510000
I1211 10:21:13.715567 11272 layer_factory.cpp:58] Creating layer bn4_1
I1211 10:21:13.715567 11272 net.cpp:84] Creating Layer bn4_1
I1211 10:21:13.715567 11272 net.cpp:406] bn4_1 <- conv4_1
I1211 10:21:13.715567 11272 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 10:21:13.715567 11272 net.cpp:122] Setting up bn4_1
I1211 10:21:13.715567 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.715567 11272 net.cpp:137] Memory required for data: 487630000
I1211 10:21:13.715567 11272 layer_factory.cpp:58] Creating layer scale4_1
I1211 10:21:13.715567 11272 net.cpp:84] Creating Layer scale4_1
I1211 10:21:13.715567 11272 net.cpp:406] scale4_1 <- conv4_1
I1211 10:21:13.715567 11272 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 10:21:13.715567 11272 layer_factory.cpp:58] Creating layer scale4_1
I1211 10:21:13.715567 11272 net.cpp:122] Setting up scale4_1
I1211 10:21:13.715567 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.715567 11272 net.cpp:137] Memory required for data: 492750000
I1211 10:21:13.715567 11272 layer_factory.cpp:58] Creating layer relu4_1
I1211 10:21:13.715567 11272 net.cpp:84] Creating Layer relu4_1
I1211 10:21:13.715567 11272 net.cpp:406] relu4_1 <- conv4_1
I1211 10:21:13.715567 11272 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 10:21:13.715567 11272 net.cpp:122] Setting up relu4_1
I1211 10:21:13.715567 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.715567 11272 net.cpp:137] Memory required for data: 497870000
I1211 10:21:13.715567 11272 layer_factory.cpp:58] Creating layer conv4_2
I1211 10:21:13.715567 11272 net.cpp:84] Creating Layer conv4_2
I1211 10:21:13.715567 11272 net.cpp:406] conv4_2 <- conv4_1
I1211 10:21:13.715567 11272 net.cpp:380] conv4_2 -> conv4_2
I1211 10:21:13.717581 11272 net.cpp:122] Setting up conv4_2
I1211 10:21:13.717581 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.717581 11272 net.cpp:137] Memory required for data: 503809200
I1211 10:21:13.717581 11272 layer_factory.cpp:58] Creating layer bn4_2
I1211 10:21:13.717581 11272 net.cpp:84] Creating Layer bn4_2
I1211 10:21:13.717581 11272 net.cpp:406] bn4_2 <- conv4_2
I1211 10:21:13.717581 11272 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 10:21:13.717581 11272 net.cpp:122] Setting up bn4_2
I1211 10:21:13.717581 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.717581 11272 net.cpp:137] Memory required for data: 509748400
I1211 10:21:13.717581 11272 layer_factory.cpp:58] Creating layer scale4_2
I1211 10:21:13.717581 11272 net.cpp:84] Creating Layer scale4_2
I1211 10:21:13.717581 11272 net.cpp:406] scale4_2 <- conv4_2
I1211 10:21:13.717581 11272 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 10:21:13.717581 11272 layer_factory.cpp:58] Creating layer scale4_2
I1211 10:21:13.717581 11272 net.cpp:122] Setting up scale4_2
I1211 10:21:13.717581 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.717581 11272 net.cpp:137] Memory required for data: 515687600
I1211 10:21:13.717581 11272 layer_factory.cpp:58] Creating layer relu4_2
I1211 10:21:13.717581 11272 net.cpp:84] Creating Layer relu4_2
I1211 10:21:13.717581 11272 net.cpp:406] relu4_2 <- conv4_2
I1211 10:21:13.717581 11272 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 10:21:13.718581 11272 net.cpp:122] Setting up relu4_2
I1211 10:21:13.718581 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.718581 11272 net.cpp:137] Memory required for data: 521626800
I1211 10:21:13.718581 11272 layer_factory.cpp:58] Creating layer added_new_conv2
I1211 10:21:13.718581 11272 net.cpp:84] Creating Layer added_new_conv2
I1211 10:21:13.718581 11272 net.cpp:406] added_new_conv2 <- conv4_2
I1211 10:21:13.718581 11272 net.cpp:380] added_new_conv2 -> added_new_conv2
I1211 10:21:13.719575 11272 net.cpp:122] Setting up added_new_conv2
I1211 10:21:13.719575 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.719575 11272 net.cpp:137] Memory required for data: 527566000
I1211 10:21:13.719575 11272 layer_factory.cpp:58] Creating layer bn_added2
I1211 10:21:13.719575 11272 net.cpp:84] Creating Layer bn_added2
I1211 10:21:13.719575 11272 net.cpp:406] bn_added2 <- added_new_conv2
I1211 10:21:13.719575 11272 net.cpp:367] bn_added2 -> added_new_conv2 (in-place)
I1211 10:21:13.720580 11272 net.cpp:122] Setting up bn_added2
I1211 10:21:13.720580 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.720580 11272 net.cpp:137] Memory required for data: 533505200
I1211 10:21:13.720580 11272 layer_factory.cpp:58] Creating layer scale_added2
I1211 10:21:13.720580 11272 net.cpp:84] Creating Layer scale_added2
I1211 10:21:13.720580 11272 net.cpp:406] scale_added2 <- added_new_conv2
I1211 10:21:13.720580 11272 net.cpp:367] scale_added2 -> added_new_conv2 (in-place)
I1211 10:21:13.720580 11272 layer_factory.cpp:58] Creating layer scale_added2
I1211 10:21:13.720580 11272 net.cpp:122] Setting up scale_added2
I1211 10:21:13.720580 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.720580 11272 net.cpp:137] Memory required for data: 539444400
I1211 10:21:13.720580 11272 layer_factory.cpp:58] Creating layer relu_added2
I1211 10:21:13.720580 11272 net.cpp:84] Creating Layer relu_added2
I1211 10:21:13.720580 11272 net.cpp:406] relu_added2 <- added_new_conv2
I1211 10:21:13.720580 11272 net.cpp:367] relu_added2 -> added_new_conv2 (in-place)
I1211 10:21:13.720580 11272 net.cpp:122] Setting up relu_added2
I1211 10:21:13.720580 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.720580 11272 net.cpp:137] Memory required for data: 545383600
I1211 10:21:13.720580 11272 layer_factory.cpp:58] Creating layer pool4_2
I1211 10:21:13.720580 11272 net.cpp:84] Creating Layer pool4_2
I1211 10:21:13.720580 11272 net.cpp:406] pool4_2 <- added_new_conv2
I1211 10:21:13.720580 11272 net.cpp:380] pool4_2 -> pool4_2
I1211 10:21:13.720580 11272 net.cpp:122] Setting up pool4_2
I1211 10:21:13.720580 11272 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 10:21:13.720580 11272 net.cpp:137] Memory required for data: 546868400
I1211 10:21:13.720580 11272 layer_factory.cpp:58] Creating layer conv4_0
I1211 10:21:13.720580 11272 net.cpp:84] Creating Layer conv4_0
I1211 10:21:13.720580 11272 net.cpp:406] conv4_0 <- pool4_2
I1211 10:21:13.720580 11272 net.cpp:380] conv4_0 -> conv4_0
I1211 10:21:13.722589 11272 net.cpp:122] Setting up conv4_0
I1211 10:21:13.722589 11272 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 10:21:13.722589 11272 net.cpp:137] Memory required for data: 548353200
I1211 10:21:13.722589 11272 layer_factory.cpp:58] Creating layer bn4_0
I1211 10:21:13.722589 11272 net.cpp:84] Creating Layer bn4_0
I1211 10:21:13.722589 11272 net.cpp:406] bn4_0 <- conv4_0
I1211 10:21:13.722589 11272 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 10:21:13.722589 11272 net.cpp:122] Setting up bn4_0
I1211 10:21:13.722589 11272 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 10:21:13.722589 11272 net.cpp:137] Memory required for data: 549838000
I1211 10:21:13.722589 11272 layer_factory.cpp:58] Creating layer scale4_0
I1211 10:21:13.722589 11272 net.cpp:84] Creating Layer scale4_0
I1211 10:21:13.722589 11272 net.cpp:406] scale4_0 <- conv4_0
I1211 10:21:13.722589 11272 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 10:21:13.722589 11272 layer_factory.cpp:58] Creating layer scale4_0
I1211 10:21:13.722589 11272 net.cpp:122] Setting up scale4_0
I1211 10:21:13.722589 11272 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 10:21:13.722589 11272 net.cpp:137] Memory required for data: 551322800
I1211 10:21:13.722589 11272 layer_factory.cpp:58] Creating layer relu4_0
I1211 10:21:13.722589 11272 net.cpp:84] Creating Layer relu4_0
I1211 10:21:13.722589 11272 net.cpp:406] relu4_0 <- conv4_0
I1211 10:21:13.722589 11272 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 10:21:13.722589 11272 net.cpp:122] Setting up relu4_0
I1211 10:21:13.722589 11272 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 10:21:13.722589 11272 net.cpp:137] Memory required for data: 552807600
I1211 10:21:13.722589 11272 layer_factory.cpp:58] Creating layer conv11
I1211 10:21:13.722589 11272 net.cpp:84] Creating Layer conv11
I1211 10:21:13.723582 11272 net.cpp:406] conv11 <- conv4_0
I1211 10:21:13.723582 11272 net.cpp:380] conv11 -> conv11
I1211 10:21:13.724568 11272 net.cpp:122] Setting up conv11
I1211 10:21:13.724568 11272 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 10:21:13.724568 11272 net.cpp:137] Memory required for data: 554599600
I1211 10:21:13.724568 11272 layer_factory.cpp:58] Creating layer bn_conv11
I1211 10:21:13.724568 11272 net.cpp:84] Creating Layer bn_conv11
I1211 10:21:13.724568 11272 net.cpp:406] bn_conv11 <- conv11
I1211 10:21:13.724568 11272 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 10:21:13.724568 11272 net.cpp:122] Setting up bn_conv11
I1211 10:21:13.724568 11272 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 10:21:13.724568 11272 net.cpp:137] Memory required for data: 556391600
I1211 10:21:13.724568 11272 layer_factory.cpp:58] Creating layer scale_conv11
I1211 10:21:13.724568 11272 net.cpp:84] Creating Layer scale_conv11
I1211 10:21:13.724568 11272 net.cpp:406] scale_conv11 <- conv11
I1211 10:21:13.724568 11272 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 10:21:13.724568 11272 layer_factory.cpp:58] Creating layer scale_conv11
I1211 10:21:13.725580 11272 net.cpp:122] Setting up scale_conv11
I1211 10:21:13.725580 11272 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 10:21:13.725580 11272 net.cpp:137] Memory required for data: 558183600
I1211 10:21:13.725580 11272 layer_factory.cpp:58] Creating layer relu_conv11
I1211 10:21:13.725580 11272 net.cpp:84] Creating Layer relu_conv11
I1211 10:21:13.725580 11272 net.cpp:406] relu_conv11 <- conv11
I1211 10:21:13.725580 11272 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 10:21:13.725580 11272 net.cpp:122] Setting up relu_conv11
I1211 10:21:13.725580 11272 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 10:21:13.725580 11272 net.cpp:137] Memory required for data: 559975600
I1211 10:21:13.725580 11272 layer_factory.cpp:58] Creating layer conv12
I1211 10:21:13.725580 11272 net.cpp:84] Creating Layer conv12
I1211 10:21:13.725580 11272 net.cpp:406] conv12 <- conv11
I1211 10:21:13.725580 11272 net.cpp:380] conv12 -> conv12
I1211 10:21:13.727567 11272 net.cpp:122] Setting up conv12
I1211 10:21:13.727567 11272 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 10:21:13.727567 11272 net.cpp:137] Memory required for data: 562279600
I1211 10:21:13.727567 11272 layer_factory.cpp:58] Creating layer bn_conv12
I1211 10:21:13.727567 11272 net.cpp:84] Creating Layer bn_conv12
I1211 10:21:13.727567 11272 net.cpp:406] bn_conv12 <- conv12
I1211 10:21:13.727567 11272 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 10:21:13.727567 11272 net.cpp:122] Setting up bn_conv12
I1211 10:21:13.727567 11272 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 10:21:13.727567 11272 net.cpp:137] Memory required for data: 564583600
I1211 10:21:13.727567 11272 layer_factory.cpp:58] Creating layer scale_conv12
I1211 10:21:13.727567 11272 net.cpp:84] Creating Layer scale_conv12
I1211 10:21:13.727567 11272 net.cpp:406] scale_conv12 <- conv12
I1211 10:21:13.727567 11272 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 10:21:13.727567 11272 layer_factory.cpp:58] Creating layer scale_conv12
I1211 10:21:13.727567 11272 net.cpp:122] Setting up scale_conv12
I1211 10:21:13.727567 11272 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 10:21:13.727567 11272 net.cpp:137] Memory required for data: 566887600
I1211 10:21:13.727567 11272 layer_factory.cpp:58] Creating layer relu_conv12
I1211 10:21:13.727567 11272 net.cpp:84] Creating Layer relu_conv12
I1211 10:21:13.727567 11272 net.cpp:406] relu_conv12 <- conv12
I1211 10:21:13.727567 11272 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 10:21:13.727567 11272 net.cpp:122] Setting up relu_conv12
I1211 10:21:13.727567 11272 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 10:21:13.727567 11272 net.cpp:137] Memory required for data: 569191600
I1211 10:21:13.727567 11272 layer_factory.cpp:58] Creating layer poolcp6
I1211 10:21:13.727567 11272 net.cpp:84] Creating Layer poolcp6
I1211 10:21:13.727567 11272 net.cpp:406] poolcp6 <- conv12
I1211 10:21:13.727567 11272 net.cpp:380] poolcp6 -> poolcp6
I1211 10:21:13.727567 11272 net.cpp:122] Setting up poolcp6
I1211 10:21:13.727567 11272 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 10:21:13.727567 11272 net.cpp:137] Memory required for data: 569227600
I1211 10:21:13.727567 11272 layer_factory.cpp:58] Creating layer ip1
I1211 10:21:13.727567 11272 net.cpp:84] Creating Layer ip1
I1211 10:21:13.727567 11272 net.cpp:406] ip1 <- poolcp6
I1211 10:21:13.727567 11272 net.cpp:380] ip1 -> ip1
I1211 10:21:13.728580 11272 net.cpp:122] Setting up ip1
I1211 10:21:13.728580 11272 net.cpp:129] Top shape: 100 100 (10000)
I1211 10:21:13.728580 11272 net.cpp:137] Memory required for data: 569267600
I1211 10:21:13.728580 11272 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 10:21:13.728580 11272 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 10:21:13.728580 11272 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 10:21:13.728580 11272 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 10:21:13.728580 11272 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 10:21:13.728580 11272 net.cpp:122] Setting up ip1_ip1_0_split
I1211 10:21:13.728580 11272 net.cpp:129] Top shape: 100 100 (10000)
I1211 10:21:13.728580 11272 net.cpp:129] Top shape: 100 100 (10000)
I1211 10:21:13.728580 11272 net.cpp:137] Memory required for data: 569347600
I1211 10:21:13.728580 11272 layer_factory.cpp:58] Creating layer accuracy_training
I1211 10:21:13.728580 11272 net.cpp:84] Creating Layer accuracy_training
I1211 10:21:13.728580 11272 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1211 10:21:13.728580 11272 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1211 10:21:13.728580 11272 net.cpp:380] accuracy_training -> accuracy_training
I1211 10:21:13.728580 11272 net.cpp:122] Setting up accuracy_training
I1211 10:21:13.728580 11272 net.cpp:129] Top shape: (1)
I1211 10:21:13.728580 11272 net.cpp:137] Memory required for data: 569347604
I1211 10:21:13.728580 11272 layer_factory.cpp:58] Creating layer loss
I1211 10:21:13.728580 11272 net.cpp:84] Creating Layer loss
I1211 10:21:13.728580 11272 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 10:21:13.728580 11272 net.cpp:406] loss <- label_cifar_1_split_1
I1211 10:21:13.728580 11272 net.cpp:380] loss -> loss
I1211 10:21:13.728580 11272 layer_factory.cpp:58] Creating layer loss
I1211 10:21:13.728580 11272 net.cpp:122] Setting up loss
I1211 10:21:13.728580 11272 net.cpp:129] Top shape: (1)
I1211 10:21:13.728580 11272 net.cpp:132]     with loss weight 1
I1211 10:21:13.728580 11272 net.cpp:137] Memory required for data: 569347608
I1211 10:21:13.728580 11272 net.cpp:198] loss needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:200] accuracy_training does not need backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] ip1 needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] poolcp6 needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] relu_conv12 needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] scale_conv12 needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] bn_conv12 needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] conv12 needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] relu_conv11 needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] scale_conv11 needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] bn_conv11 needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] conv11 needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] relu4_0 needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] scale4_0 needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] bn4_0 needs backward computation.
I1211 10:21:13.728580 11272 net.cpp:198] conv4_0 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] pool4_2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] relu_added2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] scale_added2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] bn_added2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] added_new_conv2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] relu4_2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] scale4_2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] bn4_2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] conv4_2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] relu4_1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] scale4_1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] bn4_1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] conv4_1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] relu4 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] scale4 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] bn4 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] conv4 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] relu3_1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] scale3_1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] bn3_1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] conv3_1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] relu3 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] scale3 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] bn3 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] conv3 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] pool2_1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] relu_added1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] scale_added1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] bn_added1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] newconv_added1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] relu2_2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] scale2_2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] bn2_2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] conv2_2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] relu2_1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] scale2_1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] bn2_1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] conv2_1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] relu2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] scale2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] bn2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] conv2 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] relu1_0 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] scale1_0 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] bn1_0 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] conv1_0 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] relu1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] scale1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] bn1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:198] conv1 needs backward computation.
I1211 10:21:13.729581 11272 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 10:21:13.729581 11272 net.cpp:200] cifar does not need backward computation.
I1211 10:21:13.729581 11272 net.cpp:242] This network produces output accuracy_training
I1211 10:21:13.729581 11272 net.cpp:242] This network produces output loss
I1211 10:21:13.729581 11272 net.cpp:255] Network initialization done.
I1211 10:21:13.730581 11272 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 10:21:13.730581 11272 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 10:21:13.730581 11272 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_added1
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_added2
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1211 10:21:13.730581 11272 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1211 10:21:13.730581 11272 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added1"
  type: "BatchNorm"
  bottom: "newconv_added1"
  top: "newconv_added1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added1"
  type: "Scale"
  bottom: "newconv_added1"
  top: "newconv_added1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added1"
  type: "ReLU"
  bottom: "newconv_added1"
  top: "newconv_added1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added2"
  type: "BatchNorm"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added2"
  type: "Scale"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added2"
  type: "ReLU"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 10:21:13.730581 11272 layer_factory.cpp:58] Creating layer cifar
I1211 10:21:13.733566 11272 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1211 10:21:13.733566 11272 net.cpp:84] Creating Layer cifar
I1211 10:21:13.733566 11272 net.cpp:380] cifar -> data
I1211 10:21:13.733566 11272 net.cpp:380] cifar -> label
I1211 10:21:13.734566 11272 data_layer.cpp:45] output data size: 100,3,32,32
I1211 10:21:13.740566 11272 net.cpp:122] Setting up cifar
I1211 10:21:13.741585 11272 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 10:21:13.741585 11272 net.cpp:129] Top shape: 100 (100)
I1211 10:21:13.741585 11272 net.cpp:137] Memory required for data: 1229200
I1211 10:21:13.741585 11272 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 10:21:13.741585 11272 net.cpp:84] Creating Layer label_cifar_1_split
I1211 10:21:13.741585 11272 net.cpp:406] label_cifar_1_split <- label
I1211 10:21:13.741585 11272 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 10:21:13.741585 11272 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 10:21:13.741585 11272 net.cpp:122] Setting up label_cifar_1_split
I1211 10:21:13.741585 11272 net.cpp:129] Top shape: 100 (100)
I1211 10:21:13.741585 11272 net.cpp:129] Top shape: 100 (100)
I1211 10:21:13.741585 11272 net.cpp:137] Memory required for data: 1230000
I1211 10:21:13.741585 11272 layer_factory.cpp:58] Creating layer conv1
I1211 10:21:13.741585 11272 net.cpp:84] Creating Layer conv1
I1211 10:21:13.741585 11272 net.cpp:406] conv1 <- data
I1211 10:21:13.741585 11272 net.cpp:380] conv1 -> conv1
I1211 10:21:13.742569 17304 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 10:21:13.742569 11272 net.cpp:122] Setting up conv1
I1211 10:21:13.742569 11272 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 10:21:13.742569 11272 net.cpp:137] Memory required for data: 13518000
I1211 10:21:13.742569 11272 layer_factory.cpp:58] Creating layer bn1
I1211 10:21:13.743584 11272 net.cpp:84] Creating Layer bn1
I1211 10:21:13.743584 11272 net.cpp:406] bn1 <- conv1
I1211 10:21:13.743584 11272 net.cpp:367] bn1 -> conv1 (in-place)
I1211 10:21:13.743584 11272 net.cpp:122] Setting up bn1
I1211 10:21:13.743584 11272 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 10:21:13.743584 11272 net.cpp:137] Memory required for data: 25806000
I1211 10:21:13.743584 11272 layer_factory.cpp:58] Creating layer scale1
I1211 10:21:13.743584 11272 net.cpp:84] Creating Layer scale1
I1211 10:21:13.743584 11272 net.cpp:406] scale1 <- conv1
I1211 10:21:13.743584 11272 net.cpp:367] scale1 -> conv1 (in-place)
I1211 10:21:13.743584 11272 layer_factory.cpp:58] Creating layer scale1
I1211 10:21:13.743584 11272 net.cpp:122] Setting up scale1
I1211 10:21:13.743584 11272 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 10:21:13.743584 11272 net.cpp:137] Memory required for data: 38094000
I1211 10:21:13.743584 11272 layer_factory.cpp:58] Creating layer relu1
I1211 10:21:13.743584 11272 net.cpp:84] Creating Layer relu1
I1211 10:21:13.743584 11272 net.cpp:406] relu1 <- conv1
I1211 10:21:13.743584 11272 net.cpp:367] relu1 -> conv1 (in-place)
I1211 10:21:13.743584 11272 net.cpp:122] Setting up relu1
I1211 10:21:13.744585 11272 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 10:21:13.744585 11272 net.cpp:137] Memory required for data: 50382000
I1211 10:21:13.744585 11272 layer_factory.cpp:58] Creating layer conv1_0
I1211 10:21:13.744585 11272 net.cpp:84] Creating Layer conv1_0
I1211 10:21:13.744585 11272 net.cpp:406] conv1_0 <- conv1
I1211 10:21:13.744585 11272 net.cpp:380] conv1_0 -> conv1_0
I1211 10:21:13.745568 11272 net.cpp:122] Setting up conv1_0
I1211 10:21:13.745568 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.745568 11272 net.cpp:137] Memory required for data: 66766000
I1211 10:21:13.745568 11272 layer_factory.cpp:58] Creating layer bn1_0
I1211 10:21:13.745568 11272 net.cpp:84] Creating Layer bn1_0
I1211 10:21:13.745568 11272 net.cpp:406] bn1_0 <- conv1_0
I1211 10:21:13.745568 11272 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 10:21:13.745568 11272 net.cpp:122] Setting up bn1_0
I1211 10:21:13.745568 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.745568 11272 net.cpp:137] Memory required for data: 83150000
I1211 10:21:13.745568 11272 layer_factory.cpp:58] Creating layer scale1_0
I1211 10:21:13.745568 11272 net.cpp:84] Creating Layer scale1_0
I1211 10:21:13.745568 11272 net.cpp:406] scale1_0 <- conv1_0
I1211 10:21:13.745568 11272 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 10:21:13.745568 11272 layer_factory.cpp:58] Creating layer scale1_0
I1211 10:21:13.745568 11272 net.cpp:122] Setting up scale1_0
I1211 10:21:13.745568 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.745568 11272 net.cpp:137] Memory required for data: 99534000
I1211 10:21:13.745568 11272 layer_factory.cpp:58] Creating layer relu1_0
I1211 10:21:13.745568 11272 net.cpp:84] Creating Layer relu1_0
I1211 10:21:13.745568 11272 net.cpp:406] relu1_0 <- conv1_0
I1211 10:21:13.745568 11272 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 10:21:13.746570 11272 net.cpp:122] Setting up relu1_0
I1211 10:21:13.746570 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.746570 11272 net.cpp:137] Memory required for data: 115918000
I1211 10:21:13.746570 11272 layer_factory.cpp:58] Creating layer conv2
I1211 10:21:13.746570 11272 net.cpp:84] Creating Layer conv2
I1211 10:21:13.746570 11272 net.cpp:406] conv2 <- conv1_0
I1211 10:21:13.746570 11272 net.cpp:380] conv2 -> conv2
I1211 10:21:13.747586 11272 net.cpp:122] Setting up conv2
I1211 10:21:13.747586 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.747586 11272 net.cpp:137] Memory required for data: 132302000
I1211 10:21:13.747586 11272 layer_factory.cpp:58] Creating layer bn2
I1211 10:21:13.747586 11272 net.cpp:84] Creating Layer bn2
I1211 10:21:13.747586 11272 net.cpp:406] bn2 <- conv2
I1211 10:21:13.747586 11272 net.cpp:367] bn2 -> conv2 (in-place)
I1211 10:21:13.747586 11272 net.cpp:122] Setting up bn2
I1211 10:21:13.747586 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.747586 11272 net.cpp:137] Memory required for data: 148686000
I1211 10:21:13.747586 11272 layer_factory.cpp:58] Creating layer scale2
I1211 10:21:13.747586 11272 net.cpp:84] Creating Layer scale2
I1211 10:21:13.747586 11272 net.cpp:406] scale2 <- conv2
I1211 10:21:13.747586 11272 net.cpp:367] scale2 -> conv2 (in-place)
I1211 10:21:13.747586 11272 layer_factory.cpp:58] Creating layer scale2
I1211 10:21:13.748585 11272 net.cpp:122] Setting up scale2
I1211 10:21:13.748585 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.748585 11272 net.cpp:137] Memory required for data: 165070000
I1211 10:21:13.748585 11272 layer_factory.cpp:58] Creating layer relu2
I1211 10:21:13.748585 11272 net.cpp:84] Creating Layer relu2
I1211 10:21:13.748585 11272 net.cpp:406] relu2 <- conv2
I1211 10:21:13.748585 11272 net.cpp:367] relu2 -> conv2 (in-place)
I1211 10:21:13.748585 11272 net.cpp:122] Setting up relu2
I1211 10:21:13.748585 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.748585 11272 net.cpp:137] Memory required for data: 181454000
I1211 10:21:13.748585 11272 layer_factory.cpp:58] Creating layer conv2_1
I1211 10:21:13.748585 11272 net.cpp:84] Creating Layer conv2_1
I1211 10:21:13.748585 11272 net.cpp:406] conv2_1 <- conv2
I1211 10:21:13.748585 11272 net.cpp:380] conv2_1 -> conv2_1
I1211 10:21:13.750097 11272 net.cpp:122] Setting up conv2_1
I1211 10:21:13.750097 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.750097 11272 net.cpp:137] Memory required for data: 197838000
I1211 10:21:13.750097 11272 layer_factory.cpp:58] Creating layer bn2_1
I1211 10:21:13.750097 11272 net.cpp:84] Creating Layer bn2_1
I1211 10:21:13.750097 11272 net.cpp:406] bn2_1 <- conv2_1
I1211 10:21:13.750097 11272 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 10:21:13.750612 11272 net.cpp:122] Setting up bn2_1
I1211 10:21:13.750612 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.750612 11272 net.cpp:137] Memory required for data: 214222000
I1211 10:21:13.750612 11272 layer_factory.cpp:58] Creating layer scale2_1
I1211 10:21:13.750612 11272 net.cpp:84] Creating Layer scale2_1
I1211 10:21:13.750612 11272 net.cpp:406] scale2_1 <- conv2_1
I1211 10:21:13.750612 11272 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 10:21:13.750612 11272 layer_factory.cpp:58] Creating layer scale2_1
I1211 10:21:13.750612 11272 net.cpp:122] Setting up scale2_1
I1211 10:21:13.750612 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.750612 11272 net.cpp:137] Memory required for data: 230606000
I1211 10:21:13.750612 11272 layer_factory.cpp:58] Creating layer relu2_1
I1211 10:21:13.750612 11272 net.cpp:84] Creating Layer relu2_1
I1211 10:21:13.750612 11272 net.cpp:406] relu2_1 <- conv2_1
I1211 10:21:13.750612 11272 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 10:21:13.751111 11272 net.cpp:122] Setting up relu2_1
I1211 10:21:13.751111 11272 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 10:21:13.751111 11272 net.cpp:137] Memory required for data: 246990000
I1211 10:21:13.751111 11272 layer_factory.cpp:58] Creating layer conv2_2
I1211 10:21:13.751111 11272 net.cpp:84] Creating Layer conv2_2
I1211 10:21:13.751111 11272 net.cpp:406] conv2_2 <- conv2_1
I1211 10:21:13.751111 11272 net.cpp:380] conv2_2 -> conv2_2
I1211 10:21:13.752110 11272 net.cpp:122] Setting up conv2_2
I1211 10:21:13.752110 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.752110 11272 net.cpp:137] Memory required for data: 267470000
I1211 10:21:13.752110 11272 layer_factory.cpp:58] Creating layer bn2_2
I1211 10:21:13.752110 11272 net.cpp:84] Creating Layer bn2_2
I1211 10:21:13.752612 11272 net.cpp:406] bn2_2 <- conv2_2
I1211 10:21:13.752612 11272 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 10:21:13.752612 11272 net.cpp:122] Setting up bn2_2
I1211 10:21:13.752612 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.752612 11272 net.cpp:137] Memory required for data: 287950000
I1211 10:21:13.752612 11272 layer_factory.cpp:58] Creating layer scale2_2
I1211 10:21:13.752612 11272 net.cpp:84] Creating Layer scale2_2
I1211 10:21:13.752612 11272 net.cpp:406] scale2_2 <- conv2_2
I1211 10:21:13.752612 11272 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 10:21:13.752612 11272 layer_factory.cpp:58] Creating layer scale2_2
I1211 10:21:13.752612 11272 net.cpp:122] Setting up scale2_2
I1211 10:21:13.752612 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.752612 11272 net.cpp:137] Memory required for data: 308430000
I1211 10:21:13.752612 11272 layer_factory.cpp:58] Creating layer relu2_2
I1211 10:21:13.752612 11272 net.cpp:84] Creating Layer relu2_2
I1211 10:21:13.752612 11272 net.cpp:406] relu2_2 <- conv2_2
I1211 10:21:13.752612 11272 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 10:21:13.753111 11272 net.cpp:122] Setting up relu2_2
I1211 10:21:13.753111 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.753111 11272 net.cpp:137] Memory required for data: 328910000
I1211 10:21:13.753111 11272 layer_factory.cpp:58] Creating layer newconv_added1
I1211 10:21:13.753111 11272 net.cpp:84] Creating Layer newconv_added1
I1211 10:21:13.753111 11272 net.cpp:406] newconv_added1 <- conv2_2
I1211 10:21:13.753111 11272 net.cpp:380] newconv_added1 -> newconv_added1
I1211 10:21:13.754591 11272 net.cpp:122] Setting up newconv_added1
I1211 10:21:13.754591 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.754591 11272 net.cpp:137] Memory required for data: 349390000
I1211 10:21:13.754591 11272 layer_factory.cpp:58] Creating layer bn_added1
I1211 10:21:13.754591 11272 net.cpp:84] Creating Layer bn_added1
I1211 10:21:13.754591 11272 net.cpp:406] bn_added1 <- newconv_added1
I1211 10:21:13.754591 11272 net.cpp:367] bn_added1 -> newconv_added1 (in-place)
I1211 10:21:13.755111 11272 net.cpp:122] Setting up bn_added1
I1211 10:21:13.755111 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.755111 11272 net.cpp:137] Memory required for data: 369870000
I1211 10:21:13.755111 11272 layer_factory.cpp:58] Creating layer scale_added1
I1211 10:21:13.755111 11272 net.cpp:84] Creating Layer scale_added1
I1211 10:21:13.755111 11272 net.cpp:406] scale_added1 <- newconv_added1
I1211 10:21:13.755111 11272 net.cpp:367] scale_added1 -> newconv_added1 (in-place)
I1211 10:21:13.755111 11272 layer_factory.cpp:58] Creating layer scale_added1
I1211 10:21:13.755111 11272 net.cpp:122] Setting up scale_added1
I1211 10:21:13.755111 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.755111 11272 net.cpp:137] Memory required for data: 390350000
I1211 10:21:13.755111 11272 layer_factory.cpp:58] Creating layer relu_added1
I1211 10:21:13.755111 11272 net.cpp:84] Creating Layer relu_added1
I1211 10:21:13.755111 11272 net.cpp:406] relu_added1 <- newconv_added1
I1211 10:21:13.755111 11272 net.cpp:367] relu_added1 -> newconv_added1 (in-place)
I1211 10:21:13.755610 11272 net.cpp:122] Setting up relu_added1
I1211 10:21:13.756111 11272 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 10:21:13.756111 11272 net.cpp:137] Memory required for data: 410830000
I1211 10:21:13.756111 11272 layer_factory.cpp:58] Creating layer pool2_1
I1211 10:21:13.756111 11272 net.cpp:84] Creating Layer pool2_1
I1211 10:21:13.756111 11272 net.cpp:406] pool2_1 <- newconv_added1
I1211 10:21:13.756111 11272 net.cpp:380] pool2_1 -> pool2_1
I1211 10:21:13.756111 11272 net.cpp:122] Setting up pool2_1
I1211 10:21:13.756111 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.756111 11272 net.cpp:137] Memory required for data: 415950000
I1211 10:21:13.756111 11272 layer_factory.cpp:58] Creating layer conv3
I1211 10:21:13.756111 11272 net.cpp:84] Creating Layer conv3
I1211 10:21:13.756111 11272 net.cpp:406] conv3 <- pool2_1
I1211 10:21:13.756111 11272 net.cpp:380] conv3 -> conv3
I1211 10:21:13.757092 11272 net.cpp:122] Setting up conv3
I1211 10:21:13.757092 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.757092 11272 net.cpp:137] Memory required for data: 421070000
I1211 10:21:13.757092 11272 layer_factory.cpp:58] Creating layer bn3
I1211 10:21:13.757092 11272 net.cpp:84] Creating Layer bn3
I1211 10:21:13.757092 11272 net.cpp:406] bn3 <- conv3
I1211 10:21:13.757092 11272 net.cpp:367] bn3 -> conv3 (in-place)
I1211 10:21:13.757092 11272 net.cpp:122] Setting up bn3
I1211 10:21:13.757092 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.757092 11272 net.cpp:137] Memory required for data: 426190000
I1211 10:21:13.757092 11272 layer_factory.cpp:58] Creating layer scale3
I1211 10:21:13.757092 11272 net.cpp:84] Creating Layer scale3
I1211 10:21:13.757092 11272 net.cpp:406] scale3 <- conv3
I1211 10:21:13.757092 11272 net.cpp:367] scale3 -> conv3 (in-place)
I1211 10:21:13.757092 11272 layer_factory.cpp:58] Creating layer scale3
I1211 10:21:13.757592 11272 net.cpp:122] Setting up scale3
I1211 10:21:13.757592 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.757592 11272 net.cpp:137] Memory required for data: 431310000
I1211 10:21:13.757592 11272 layer_factory.cpp:58] Creating layer relu3
I1211 10:21:13.757592 11272 net.cpp:84] Creating Layer relu3
I1211 10:21:13.757592 11272 net.cpp:406] relu3 <- conv3
I1211 10:21:13.757592 11272 net.cpp:367] relu3 -> conv3 (in-place)
I1211 10:21:13.758097 11272 net.cpp:122] Setting up relu3
I1211 10:21:13.758097 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.758097 11272 net.cpp:137] Memory required for data: 436430000
I1211 10:21:13.758097 11272 layer_factory.cpp:58] Creating layer conv3_1
I1211 10:21:13.758097 11272 net.cpp:84] Creating Layer conv3_1
I1211 10:21:13.758097 11272 net.cpp:406] conv3_1 <- conv3
I1211 10:21:13.758097 11272 net.cpp:380] conv3_1 -> conv3_1
I1211 10:21:13.759606 11272 net.cpp:122] Setting up conv3_1
I1211 10:21:13.759606 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.759606 11272 net.cpp:137] Memory required for data: 441550000
I1211 10:21:13.759606 11272 layer_factory.cpp:58] Creating layer bn3_1
I1211 10:21:13.759606 11272 net.cpp:84] Creating Layer bn3_1
I1211 10:21:13.760105 11272 net.cpp:406] bn3_1 <- conv3_1
I1211 10:21:13.760105 11272 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 10:21:13.760105 11272 net.cpp:122] Setting up bn3_1
I1211 10:21:13.760105 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.760105 11272 net.cpp:137] Memory required for data: 446670000
I1211 10:21:13.760105 11272 layer_factory.cpp:58] Creating layer scale3_1
I1211 10:21:13.760105 11272 net.cpp:84] Creating Layer scale3_1
I1211 10:21:13.760105 11272 net.cpp:406] scale3_1 <- conv3_1
I1211 10:21:13.760105 11272 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 10:21:13.760105 11272 layer_factory.cpp:58] Creating layer scale3_1
I1211 10:21:13.760105 11272 net.cpp:122] Setting up scale3_1
I1211 10:21:13.760105 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.760105 11272 net.cpp:137] Memory required for data: 451790000
I1211 10:21:13.760105 11272 layer_factory.cpp:58] Creating layer relu3_1
I1211 10:21:13.760105 11272 net.cpp:84] Creating Layer relu3_1
I1211 10:21:13.760105 11272 net.cpp:406] relu3_1 <- conv3_1
I1211 10:21:13.760105 11272 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 10:21:13.760608 11272 net.cpp:122] Setting up relu3_1
I1211 10:21:13.760608 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.760608 11272 net.cpp:137] Memory required for data: 456910000
I1211 10:21:13.760608 11272 layer_factory.cpp:58] Creating layer conv4
I1211 10:21:13.760608 11272 net.cpp:84] Creating Layer conv4
I1211 10:21:13.760608 11272 net.cpp:406] conv4 <- conv3_1
I1211 10:21:13.760608 11272 net.cpp:380] conv4 -> conv4
I1211 10:21:13.762104 11272 net.cpp:122] Setting up conv4
I1211 10:21:13.762104 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.762104 11272 net.cpp:137] Memory required for data: 462030000
I1211 10:21:13.762104 11272 layer_factory.cpp:58] Creating layer bn4
I1211 10:21:13.762104 11272 net.cpp:84] Creating Layer bn4
I1211 10:21:13.762104 11272 net.cpp:406] bn4 <- conv4
I1211 10:21:13.762104 11272 net.cpp:367] bn4 -> conv4 (in-place)
I1211 10:21:13.762104 11272 net.cpp:122] Setting up bn4
I1211 10:21:13.762104 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.762104 11272 net.cpp:137] Memory required for data: 467150000
I1211 10:21:13.762104 11272 layer_factory.cpp:58] Creating layer scale4
I1211 10:21:13.762104 11272 net.cpp:84] Creating Layer scale4
I1211 10:21:13.762104 11272 net.cpp:406] scale4 <- conv4
I1211 10:21:13.762104 11272 net.cpp:367] scale4 -> conv4 (in-place)
I1211 10:21:13.762104 11272 layer_factory.cpp:58] Creating layer scale4
I1211 10:21:13.762104 11272 net.cpp:122] Setting up scale4
I1211 10:21:13.762104 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.762104 11272 net.cpp:137] Memory required for data: 472270000
I1211 10:21:13.762591 11272 layer_factory.cpp:58] Creating layer relu4
I1211 10:21:13.762591 11272 net.cpp:84] Creating Layer relu4
I1211 10:21:13.762591 11272 net.cpp:406] relu4 <- conv4
I1211 10:21:13.762591 11272 net.cpp:367] relu4 -> conv4 (in-place)
I1211 10:21:13.762591 11272 net.cpp:122] Setting up relu4
I1211 10:21:13.762591 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.762591 11272 net.cpp:137] Memory required for data: 477390000
I1211 10:21:13.762591 11272 layer_factory.cpp:58] Creating layer conv4_1
I1211 10:21:13.762591 11272 net.cpp:84] Creating Layer conv4_1
I1211 10:21:13.762591 11272 net.cpp:406] conv4_1 <- conv4
I1211 10:21:13.762591 11272 net.cpp:380] conv4_1 -> conv4_1
I1211 10:21:13.764102 11272 net.cpp:122] Setting up conv4_1
I1211 10:21:13.764102 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.764102 11272 net.cpp:137] Memory required for data: 482510000
I1211 10:21:13.764102 11272 layer_factory.cpp:58] Creating layer bn4_1
I1211 10:21:13.764102 11272 net.cpp:84] Creating Layer bn4_1
I1211 10:21:13.764102 11272 net.cpp:406] bn4_1 <- conv4_1
I1211 10:21:13.764102 11272 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 10:21:13.764102 11272 net.cpp:122] Setting up bn4_1
I1211 10:21:13.764102 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.764102 11272 net.cpp:137] Memory required for data: 487630000
I1211 10:21:13.764102 11272 layer_factory.cpp:58] Creating layer scale4_1
I1211 10:21:13.764102 11272 net.cpp:84] Creating Layer scale4_1
I1211 10:21:13.764102 11272 net.cpp:406] scale4_1 <- conv4_1
I1211 10:21:13.764102 11272 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 10:21:13.764102 11272 layer_factory.cpp:58] Creating layer scale4_1
I1211 10:21:13.764603 11272 net.cpp:122] Setting up scale4_1
I1211 10:21:13.764603 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.764603 11272 net.cpp:137] Memory required for data: 492750000
I1211 10:21:13.764603 11272 layer_factory.cpp:58] Creating layer relu4_1
I1211 10:21:13.764603 11272 net.cpp:84] Creating Layer relu4_1
I1211 10:21:13.764603 11272 net.cpp:406] relu4_1 <- conv4_1
I1211 10:21:13.764603 11272 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 10:21:13.764603 11272 net.cpp:122] Setting up relu4_1
I1211 10:21:13.764603 11272 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 10:21:13.764603 11272 net.cpp:137] Memory required for data: 497870000
I1211 10:21:13.764603 11272 layer_factory.cpp:58] Creating layer conv4_2
I1211 10:21:13.764603 11272 net.cpp:84] Creating Layer conv4_2
I1211 10:21:13.764603 11272 net.cpp:406] conv4_2 <- conv4_1
I1211 10:21:13.764603 11272 net.cpp:380] conv4_2 -> conv4_2
I1211 10:21:13.765605 11272 net.cpp:122] Setting up conv4_2
I1211 10:21:13.765605 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.765605 11272 net.cpp:137] Memory required for data: 503809200
I1211 10:21:13.765605 11272 layer_factory.cpp:58] Creating layer bn4_2
I1211 10:21:13.765605 11272 net.cpp:84] Creating Layer bn4_2
I1211 10:21:13.765605 11272 net.cpp:406] bn4_2 <- conv4_2
I1211 10:21:13.765605 11272 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 10:21:13.766618 11272 net.cpp:122] Setting up bn4_2
I1211 10:21:13.766618 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.766618 11272 net.cpp:137] Memory required for data: 509748400
I1211 10:21:13.766618 11272 layer_factory.cpp:58] Creating layer scale4_2
I1211 10:21:13.766618 11272 net.cpp:84] Creating Layer scale4_2
I1211 10:21:13.766618 11272 net.cpp:406] scale4_2 <- conv4_2
I1211 10:21:13.766618 11272 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 10:21:13.766618 11272 layer_factory.cpp:58] Creating layer scale4_2
I1211 10:21:13.766618 11272 net.cpp:122] Setting up scale4_2
I1211 10:21:13.766618 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.766618 11272 net.cpp:137] Memory required for data: 515687600
I1211 10:21:13.766618 11272 layer_factory.cpp:58] Creating layer relu4_2
I1211 10:21:13.766618 11272 net.cpp:84] Creating Layer relu4_2
I1211 10:21:13.766618 11272 net.cpp:406] relu4_2 <- conv4_2
I1211 10:21:13.766618 11272 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 10:21:13.766618 11272 net.cpp:122] Setting up relu4_2
I1211 10:21:13.766618 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.766618 11272 net.cpp:137] Memory required for data: 521626800
I1211 10:21:13.766618 11272 layer_factory.cpp:58] Creating layer added_new_conv2
I1211 10:21:13.766618 11272 net.cpp:84] Creating Layer added_new_conv2
I1211 10:21:13.766618 11272 net.cpp:406] added_new_conv2 <- conv4_2
I1211 10:21:13.766618 11272 net.cpp:380] added_new_conv2 -> added_new_conv2
I1211 10:21:13.767618 11272 net.cpp:122] Setting up added_new_conv2
I1211 10:21:13.767618 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.767618 11272 net.cpp:137] Memory required for data: 527566000
I1211 10:21:13.767618 11272 layer_factory.cpp:58] Creating layer bn_added2
I1211 10:21:13.767618 11272 net.cpp:84] Creating Layer bn_added2
I1211 10:21:13.767618 11272 net.cpp:406] bn_added2 <- added_new_conv2
I1211 10:21:13.767618 11272 net.cpp:367] bn_added2 -> added_new_conv2 (in-place)
I1211 10:21:13.768618 11272 net.cpp:122] Setting up bn_added2
I1211 10:21:13.768618 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.768618 11272 net.cpp:137] Memory required for data: 533505200
I1211 10:21:13.768618 11272 layer_factory.cpp:58] Creating layer scale_added2
I1211 10:21:13.768618 11272 net.cpp:84] Creating Layer scale_added2
I1211 10:21:13.768618 11272 net.cpp:406] scale_added2 <- added_new_conv2
I1211 10:21:13.768618 11272 net.cpp:367] scale_added2 -> added_new_conv2 (in-place)
I1211 10:21:13.768618 11272 layer_factory.cpp:58] Creating layer scale_added2
I1211 10:21:13.768618 11272 net.cpp:122] Setting up scale_added2
I1211 10:21:13.768618 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.768618 11272 net.cpp:137] Memory required for data: 539444400
I1211 10:21:13.768618 11272 layer_factory.cpp:58] Creating layer relu_added2
I1211 10:21:13.768618 11272 net.cpp:84] Creating Layer relu_added2
I1211 10:21:13.768618 11272 net.cpp:406] relu_added2 <- added_new_conv2
I1211 10:21:13.768618 11272 net.cpp:367] relu_added2 -> added_new_conv2 (in-place)
I1211 10:21:13.768618 11272 net.cpp:122] Setting up relu_added2
I1211 10:21:13.768618 11272 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 10:21:13.768618 11272 net.cpp:137] Memory required for data: 545383600
I1211 10:21:13.768618 11272 layer_factory.cpp:58] Creating layer pool4_2
I1211 10:21:13.768618 11272 net.cpp:84] Creating Layer pool4_2
I1211 10:21:13.768618 11272 net.cpp:406] pool4_2 <- added_new_conv2
I1211 10:21:13.768618 11272 net.cpp:380] pool4_2 -> pool4_2
I1211 10:21:13.768618 11272 net.cpp:122] Setting up pool4_2
I1211 10:21:13.768618 11272 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 10:21:13.768618 11272 net.cpp:137] Memory required for data: 546868400
I1211 10:21:13.768618 11272 layer_factory.cpp:58] Creating layer conv4_0
I1211 10:21:13.768618 11272 net.cpp:84] Creating Layer conv4_0
I1211 10:21:13.768618 11272 net.cpp:406] conv4_0 <- pool4_2
I1211 10:21:13.768618 11272 net.cpp:380] conv4_0 -> conv4_0
I1211 10:21:13.770606 11272 net.cpp:122] Setting up conv4_0
I1211 10:21:13.770606 11272 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 10:21:13.770606 11272 net.cpp:137] Memory required for data: 548353200
I1211 10:21:13.770606 11272 layer_factory.cpp:58] Creating layer bn4_0
I1211 10:21:13.770606 11272 net.cpp:84] Creating Layer bn4_0
I1211 10:21:13.770606 11272 net.cpp:406] bn4_0 <- conv4_0
I1211 10:21:13.770606 11272 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 10:21:13.770606 11272 net.cpp:122] Setting up bn4_0
I1211 10:21:13.770606 11272 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 10:21:13.770606 11272 net.cpp:137] Memory required for data: 549838000
I1211 10:21:13.770606 11272 layer_factory.cpp:58] Creating layer scale4_0
I1211 10:21:13.770606 11272 net.cpp:84] Creating Layer scale4_0
I1211 10:21:13.770606 11272 net.cpp:406] scale4_0 <- conv4_0
I1211 10:21:13.771623 11272 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 10:21:13.771623 11272 layer_factory.cpp:58] Creating layer scale4_0
I1211 10:21:13.771623 11272 net.cpp:122] Setting up scale4_0
I1211 10:21:13.771623 11272 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 10:21:13.771623 11272 net.cpp:137] Memory required for data: 551322800
I1211 10:21:13.771623 11272 layer_factory.cpp:58] Creating layer relu4_0
I1211 10:21:13.771623 11272 net.cpp:84] Creating Layer relu4_0
I1211 10:21:13.771623 11272 net.cpp:406] relu4_0 <- conv4_0
I1211 10:21:13.771623 11272 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 10:21:13.771623 11272 net.cpp:122] Setting up relu4_0
I1211 10:21:13.771623 11272 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 10:21:13.771623 11272 net.cpp:137] Memory required for data: 552807600
I1211 10:21:13.771623 11272 layer_factory.cpp:58] Creating layer conv11
I1211 10:21:13.771623 11272 net.cpp:84] Creating Layer conv11
I1211 10:21:13.771623 11272 net.cpp:406] conv11 <- conv4_0
I1211 10:21:13.771623 11272 net.cpp:380] conv11 -> conv11
I1211 10:21:13.773627 11272 net.cpp:122] Setting up conv11
I1211 10:21:13.773627 11272 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 10:21:13.773627 11272 net.cpp:137] Memory required for data: 554599600
I1211 10:21:13.773627 11272 layer_factory.cpp:58] Creating layer bn_conv11
I1211 10:21:13.773627 11272 net.cpp:84] Creating Layer bn_conv11
I1211 10:21:13.773627 11272 net.cpp:406] bn_conv11 <- conv11
I1211 10:21:13.773627 11272 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 10:21:13.773627 11272 net.cpp:122] Setting up bn_conv11
I1211 10:21:13.773627 11272 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 10:21:13.773627 11272 net.cpp:137] Memory required for data: 556391600
I1211 10:21:13.773627 11272 layer_factory.cpp:58] Creating layer scale_conv11
I1211 10:21:13.773627 11272 net.cpp:84] Creating Layer scale_conv11
I1211 10:21:13.773627 11272 net.cpp:406] scale_conv11 <- conv11
I1211 10:21:13.773627 11272 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 10:21:13.773627 11272 layer_factory.cpp:58] Creating layer scale_conv11
I1211 10:21:13.773627 11272 net.cpp:122] Setting up scale_conv11
I1211 10:21:13.773627 11272 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 10:21:13.773627 11272 net.cpp:137] Memory required for data: 558183600
I1211 10:21:13.773627 11272 layer_factory.cpp:58] Creating layer relu_conv11
I1211 10:21:13.773627 11272 net.cpp:84] Creating Layer relu_conv11
I1211 10:21:13.773627 11272 net.cpp:406] relu_conv11 <- conv11
I1211 10:21:13.773627 11272 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 10:21:13.774624 11272 net.cpp:122] Setting up relu_conv11
I1211 10:21:13.774624 11272 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 10:21:13.774624 11272 net.cpp:137] Memory required for data: 559975600
I1211 10:21:13.774624 11272 layer_factory.cpp:58] Creating layer conv12
I1211 10:21:13.774624 11272 net.cpp:84] Creating Layer conv12
I1211 10:21:13.774624 11272 net.cpp:406] conv12 <- conv11
I1211 10:21:13.774624 11272 net.cpp:380] conv12 -> conv12
I1211 10:21:13.775624 11272 net.cpp:122] Setting up conv12
I1211 10:21:13.775624 11272 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 10:21:13.775624 11272 net.cpp:137] Memory required for data: 562279600
I1211 10:21:13.775624 11272 layer_factory.cpp:58] Creating layer bn_conv12
I1211 10:21:13.776623 11272 net.cpp:84] Creating Layer bn_conv12
I1211 10:21:13.776623 11272 net.cpp:406] bn_conv12 <- conv12
I1211 10:21:13.776623 11272 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 10:21:13.776623 11272 net.cpp:122] Setting up bn_conv12
I1211 10:21:13.776623 11272 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 10:21:13.776623 11272 net.cpp:137] Memory required for data: 564583600
I1211 10:21:13.776623 11272 layer_factory.cpp:58] Creating layer scale_conv12
I1211 10:21:13.776623 11272 net.cpp:84] Creating Layer scale_conv12
I1211 10:21:13.776623 11272 net.cpp:406] scale_conv12 <- conv12
I1211 10:21:13.776623 11272 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 10:21:13.776623 11272 layer_factory.cpp:58] Creating layer scale_conv12
I1211 10:21:13.776623 11272 net.cpp:122] Setting up scale_conv12
I1211 10:21:13.776623 11272 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 10:21:13.776623 11272 net.cpp:137] Memory required for data: 566887600
I1211 10:21:13.776623 11272 layer_factory.cpp:58] Creating layer relu_conv12
I1211 10:21:13.776623 11272 net.cpp:84] Creating Layer relu_conv12
I1211 10:21:13.776623 11272 net.cpp:406] relu_conv12 <- conv12
I1211 10:21:13.776623 11272 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 10:21:13.776623 11272 net.cpp:122] Setting up relu_conv12
I1211 10:21:13.776623 11272 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 10:21:13.776623 11272 net.cpp:137] Memory required for data: 569191600
I1211 10:21:13.776623 11272 layer_factory.cpp:58] Creating layer poolcp6
I1211 10:21:13.776623 11272 net.cpp:84] Creating Layer poolcp6
I1211 10:21:13.776623 11272 net.cpp:406] poolcp6 <- conv12
I1211 10:21:13.776623 11272 net.cpp:380] poolcp6 -> poolcp6
I1211 10:21:13.776623 11272 net.cpp:122] Setting up poolcp6
I1211 10:21:13.776623 11272 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 10:21:13.776623 11272 net.cpp:137] Memory required for data: 569227600
I1211 10:21:13.776623 11272 layer_factory.cpp:58] Creating layer ip1
I1211 10:21:13.776623 11272 net.cpp:84] Creating Layer ip1
I1211 10:21:13.776623 11272 net.cpp:406] ip1 <- poolcp6
I1211 10:21:13.776623 11272 net.cpp:380] ip1 -> ip1
I1211 10:21:13.777622 11272 net.cpp:122] Setting up ip1
I1211 10:21:13.777622 11272 net.cpp:129] Top shape: 100 100 (10000)
I1211 10:21:13.777622 11272 net.cpp:137] Memory required for data: 569267600
I1211 10:21:13.777622 11272 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 10:21:13.777622 11272 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 10:21:13.777622 11272 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 10:21:13.777622 11272 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 10:21:13.777622 11272 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 10:21:13.777622 11272 net.cpp:122] Setting up ip1_ip1_0_split
I1211 10:21:13.777622 11272 net.cpp:129] Top shape: 100 100 (10000)
I1211 10:21:13.777622 11272 net.cpp:129] Top shape: 100 100 (10000)
I1211 10:21:13.777622 11272 net.cpp:137] Memory required for data: 569347600
I1211 10:21:13.777622 11272 layer_factory.cpp:58] Creating layer accuracy
I1211 10:21:13.777622 11272 net.cpp:84] Creating Layer accuracy
I1211 10:21:13.777622 11272 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1211 10:21:13.777622 11272 net.cpp:406] accuracy <- label_cifar_1_split_0
I1211 10:21:13.777622 11272 net.cpp:380] accuracy -> accuracy
I1211 10:21:13.777622 11272 net.cpp:122] Setting up accuracy
I1211 10:21:13.777622 11272 net.cpp:129] Top shape: (1)
I1211 10:21:13.777622 11272 net.cpp:137] Memory required for data: 569347604
I1211 10:21:13.777622 11272 layer_factory.cpp:58] Creating layer loss
I1211 10:21:13.777622 11272 net.cpp:84] Creating Layer loss
I1211 10:21:13.777622 11272 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 10:21:13.777622 11272 net.cpp:406] loss <- label_cifar_1_split_1
I1211 10:21:13.777622 11272 net.cpp:380] loss -> loss
I1211 10:21:13.777622 11272 layer_factory.cpp:58] Creating layer loss
I1211 10:21:13.777622 11272 net.cpp:122] Setting up loss
I1211 10:21:13.777622 11272 net.cpp:129] Top shape: (1)
I1211 10:21:13.777622 11272 net.cpp:132]     with loss weight 1
I1211 10:21:13.777622 11272 net.cpp:137] Memory required for data: 569347608
I1211 10:21:13.777622 11272 net.cpp:198] loss needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:200] accuracy does not need backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] ip1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] poolcp6 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] relu_conv12 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] scale_conv12 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] bn_conv12 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] conv12 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] relu_conv11 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] scale_conv11 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] bn_conv11 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] conv11 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] relu4_0 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] scale4_0 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] bn4_0 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] conv4_0 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] pool4_2 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] relu_added2 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] scale_added2 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] bn_added2 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] added_new_conv2 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] relu4_2 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] scale4_2 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] bn4_2 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] conv4_2 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] relu4_1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] scale4_1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] bn4_1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] conv4_1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] relu4 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] scale4 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] bn4 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] conv4 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] relu3_1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] scale3_1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] bn3_1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] conv3_1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] relu3 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] scale3 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] bn3 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] conv3 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] pool2_1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] relu_added1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] scale_added1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] bn_added1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] newconv_added1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] relu2_2 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] scale2_2 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] bn2_2 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] conv2_2 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] relu2_1 needs backward computation.
I1211 10:21:13.777622 11272 net.cpp:198] scale2_1 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] bn2_1 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] conv2_1 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] relu2 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] scale2 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] bn2 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] conv2 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] relu1_0 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] scale1_0 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] bn1_0 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] conv1_0 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] relu1 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] scale1 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] bn1 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:198] conv1 needs backward computation.
I1211 10:21:13.778612 11272 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 10:21:13.778612 11272 net.cpp:200] cifar does not need backward computation.
I1211 10:21:13.778612 11272 net.cpp:242] This network produces output accuracy
I1211 10:21:13.778612 11272 net.cpp:242] This network produces output loss
I1211 10:21:13.778612 11272 net.cpp:255] Network initialization done.
I1211 10:21:13.778612 11272 solver.cpp:56] Solver scaffolding done.
I1211 10:21:13.783622 11272 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.solverstate
I1211 10:21:13.786635 11272 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.caffemodel
I1211 10:21:13.787619 11272 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 10:21:13.787619 11272 sgd_solver.cpp:318] SGDSolver: restoring history
I1211 10:21:13.791622 11272 caffe.cpp:249] Starting Optimization
I1211 10:21:13.791622 11272 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k
I1211 10:21:13.791622 11272 solver.cpp:273] Learning Rate Policy: multistep
I1211 10:21:13.794610 11272 solver.cpp:330] Iteration 90000, Testing net (#0)
I1211 10:21:13.796627 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:21:15.386777 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:21:15.447777 11272 solver.cpp:397]     Test net output #0: accuracy = 0.5908
I1211 10:21:15.447777 11272 solver.cpp:397]     Test net output #1: loss = 1.61171 (* 1 = 1.61171 loss)
I1211 10:21:15.562836 11272 solver.cpp:218] Iteration 90000 (50851.1 iter/s, 1.76987s/100 iters), loss = 0.74886
I1211 10:21:15.562836 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:21:15.563346 11272 solver.cpp:237]     Train net output #1: loss = 0.74886 (* 1 = 0.74886 loss)
I1211 10:21:15.563346 11272 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1211 10:21:21.936278 11272 solver.cpp:218] Iteration 90100 (15.6897 iter/s, 6.37359s/100 iters), loss = 0.647712
I1211 10:21:21.937278 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:21:21.937278 11272 solver.cpp:237]     Train net output #1: loss = 0.647712 (* 1 = 0.647712 loss)
I1211 10:21:21.937278 11272 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1211 10:21:28.293841 11272 solver.cpp:218] Iteration 90200 (15.7321 iter/s, 6.35643s/100 iters), loss = 0.641011
I1211 10:21:28.293841 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 10:21:28.293841 11272 solver.cpp:237]     Train net output #1: loss = 0.641011 (* 1 = 0.641011 loss)
I1211 10:21:28.293841 11272 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1211 10:21:34.641297 11272 solver.cpp:218] Iteration 90300 (15.7559 iter/s, 6.34682s/100 iters), loss = 0.787109
I1211 10:21:34.641297 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 10:21:34.641297 11272 solver.cpp:237]     Train net output #1: loss = 0.787109 (* 1 = 0.787109 loss)
I1211 10:21:34.641297 11272 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1211 10:21:40.991595 11272 solver.cpp:218] Iteration 90400 (15.7477 iter/s, 6.35013s/100 iters), loss = 0.80172
I1211 10:21:40.991595 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1211 10:21:40.991595 11272 solver.cpp:237]     Train net output #1: loss = 0.80172 (* 1 = 0.80172 loss)
I1211 10:21:40.992094 11272 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1211 10:21:47.073283 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:21:47.327316 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90500.caffemodel
I1211 10:21:47.343329 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90500.solverstate
I1211 10:21:47.348330 11272 solver.cpp:330] Iteration 90500, Testing net (#0)
I1211 10:21:47.348330 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:21:48.875427 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:21:48.935431 11272 solver.cpp:397]     Test net output #0: accuracy = 0.5745
I1211 10:21:48.935431 11272 solver.cpp:397]     Test net output #1: loss = 1.73032 (* 1 = 1.73032 loss)
I1211 10:21:48.997434 11272 solver.cpp:218] Iteration 90500 (12.4918 iter/s, 8.00523s/100 iters), loss = 0.706666
I1211 10:21:48.997434 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 10:21:48.997434 11272 solver.cpp:237]     Train net output #1: loss = 0.706666 (* 1 = 0.706666 loss)
I1211 10:21:48.997434 11272 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1211 10:21:55.370033 11272 solver.cpp:218] Iteration 90600 (15.693 iter/s, 6.37225s/100 iters), loss = 0.746224
I1211 10:21:55.370033 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 10:21:55.370033 11272 solver.cpp:237]     Train net output #1: loss = 0.746224 (* 1 = 0.746224 loss)
I1211 10:21:55.370033 11272 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1211 10:22:01.720819 11272 solver.cpp:218] Iteration 90700 (15.7471 iter/s, 6.35037s/100 iters), loss = 0.730412
I1211 10:22:01.720819 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 10:22:01.720819 11272 solver.cpp:237]     Train net output #1: loss = 0.730412 (* 1 = 0.730412 loss)
I1211 10:22:01.720819 11272 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1211 10:22:08.072245 11272 solver.cpp:218] Iteration 90800 (15.7465 iter/s, 6.35061s/100 iters), loss = 0.665295
I1211 10:22:08.072245 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 10:22:08.072245 11272 solver.cpp:237]     Train net output #1: loss = 0.665295 (* 1 = 0.665295 loss)
I1211 10:22:08.072245 11272 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1211 10:22:14.416684 11272 solver.cpp:218] Iteration 90900 (15.7624 iter/s, 6.34421s/100 iters), loss = 0.777698
I1211 10:22:14.416684 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 10:22:14.416684 11272 solver.cpp:237]     Train net output #1: loss = 0.777698 (* 1 = 0.777698 loss)
I1211 10:22:14.416684 11272 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1211 10:22:20.460168 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:22:20.709688 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91000.caffemodel
I1211 10:22:20.724191 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91000.solverstate
I1211 10:22:20.729192 11272 solver.cpp:330] Iteration 91000, Testing net (#0)
I1211 10:22:20.729192 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:22:22.251315 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:22:22.312322 11272 solver.cpp:397]     Test net output #0: accuracy = 0.5881
I1211 10:22:22.312322 11272 solver.cpp:397]     Test net output #1: loss = 1.60377 (* 1 = 1.60377 loss)
I1211 10:22:22.373322 11272 solver.cpp:218] Iteration 91000 (12.5678 iter/s, 7.95687s/100 iters), loss = 0.63653
I1211 10:22:22.373322 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 10:22:22.373322 11272 solver.cpp:237]     Train net output #1: loss = 0.63653 (* 1 = 0.63653 loss)
I1211 10:22:22.374321 11272 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1211 10:22:28.710264 11272 solver.cpp:218] Iteration 91100 (15.7834 iter/s, 6.33576s/100 iters), loss = 0.697725
I1211 10:22:28.710264 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 10:22:28.710264 11272 solver.cpp:237]     Train net output #1: loss = 0.697725 (* 1 = 0.697725 loss)
I1211 10:22:28.710264 11272 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1211 10:22:35.052286 11272 solver.cpp:218] Iteration 91200 (15.7671 iter/s, 6.34232s/100 iters), loss = 0.644127
I1211 10:22:35.053287 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 10:22:35.053287 11272 solver.cpp:237]     Train net output #1: loss = 0.644127 (* 1 = 0.644127 loss)
I1211 10:22:35.053287 11272 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1211 10:22:41.386837 11272 solver.cpp:218] Iteration 91300 (15.788 iter/s, 6.33391s/100 iters), loss = 0.65458
I1211 10:22:41.386837 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 10:22:41.386837 11272 solver.cpp:237]     Train net output #1: loss = 0.65458 (* 1 = 0.65458 loss)
I1211 10:22:41.386837 11272 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1211 10:22:47.754369 11272 solver.cpp:218] Iteration 91400 (15.7053 iter/s, 6.36728s/100 iters), loss = 0.772025
I1211 10:22:47.755370 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 10:22:47.755370 11272 solver.cpp:237]     Train net output #1: loss = 0.772025 (* 1 = 0.772025 loss)
I1211 10:22:47.755370 11272 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1211 10:22:53.814801 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:22:54.073850 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91500.caffemodel
I1211 10:22:54.093847 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91500.solverstate
I1211 10:22:54.102358 11272 solver.cpp:330] Iteration 91500, Testing net (#0)
I1211 10:22:54.102358 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:22:55.654237 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:22:55.714237 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6019
I1211 10:22:55.714237 11272 solver.cpp:397]     Test net output #1: loss = 1.57602 (* 1 = 1.57602 loss)
I1211 10:22:55.775240 11272 solver.cpp:218] Iteration 91500 (12.4695 iter/s, 8.01955s/100 iters), loss = 0.754456
I1211 10:22:55.775240 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 10:22:55.775240 11272 solver.cpp:237]     Train net output #1: loss = 0.754456 (* 1 = 0.754456 loss)
I1211 10:22:55.775240 11272 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1211 10:23:02.112720 11272 solver.cpp:218] Iteration 91600 (15.7802 iter/s, 6.33704s/100 iters), loss = 0.718639
I1211 10:23:02.112720 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 10:23:02.112720 11272 solver.cpp:237]     Train net output #1: loss = 0.718639 (* 1 = 0.718639 loss)
I1211 10:23:02.112720 11272 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1211 10:23:08.453215 11272 solver.cpp:218] Iteration 91700 (15.7721 iter/s, 6.34031s/100 iters), loss = 0.538403
I1211 10:23:08.453215 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 10:23:08.453215 11272 solver.cpp:237]     Train net output #1: loss = 0.538403 (* 1 = 0.538403 loss)
I1211 10:23:08.453215 11272 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1211 10:23:14.857542 11272 solver.cpp:218] Iteration 91800 (15.6155 iter/s, 6.40391s/100 iters), loss = 0.741684
I1211 10:23:14.857542 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 10:23:14.857542 11272 solver.cpp:237]     Train net output #1: loss = 0.741684 (* 1 = 0.741684 loss)
I1211 10:23:14.857542 11272 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1211 10:23:21.182963 11272 solver.cpp:218] Iteration 91900 (15.8097 iter/s, 6.32524s/100 iters), loss = 0.680686
I1211 10:23:21.182963 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 10:23:21.182963 11272 solver.cpp:237]     Train net output #1: loss = 0.680686 (* 1 = 0.680686 loss)
I1211 10:23:21.182963 11272 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1211 10:23:27.218484 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:23:27.467499 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92000.caffemodel
I1211 10:23:27.483500 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92000.solverstate
I1211 10:23:27.488499 11272 solver.cpp:330] Iteration 92000, Testing net (#0)
I1211 10:23:27.488499 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:23:29.009634 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:23:29.070647 11272 solver.cpp:397]     Test net output #0: accuracy = 0.5536
I1211 10:23:29.070647 11272 solver.cpp:397]     Test net output #1: loss = 1.86832 (* 1 = 1.86832 loss)
I1211 10:23:29.132645 11272 solver.cpp:218] Iteration 92000 (12.581 iter/s, 7.94846s/100 iters), loss = 0.579853
I1211 10:23:29.132645 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 10:23:29.132645 11272 solver.cpp:237]     Train net output #1: loss = 0.579853 (* 1 = 0.579853 loss)
I1211 10:23:29.132645 11272 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1211 10:23:35.475150 11272 solver.cpp:218] Iteration 92100 (15.7683 iter/s, 6.34184s/100 iters), loss = 0.73783
I1211 10:23:35.475150 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 10:23:35.475150 11272 solver.cpp:237]     Train net output #1: loss = 0.73783 (* 1 = 0.73783 loss)
I1211 10:23:35.475150 11272 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1211 10:23:41.886643 11272 solver.cpp:218] Iteration 92200 (15.5969 iter/s, 6.41154s/100 iters), loss = 0.518727
I1211 10:23:41.886643 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:23:41.886643 11272 solver.cpp:237]     Train net output #1: loss = 0.518727 (* 1 = 0.518727 loss)
I1211 10:23:41.886643 11272 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1211 10:23:48.238167 11272 solver.cpp:218] Iteration 92300 (15.7453 iter/s, 6.35109s/100 iters), loss = 0.794467
I1211 10:23:48.238167 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 10:23:48.238167 11272 solver.cpp:237]     Train net output #1: loss = 0.794467 (* 1 = 0.794467 loss)
I1211 10:23:48.238167 11272 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1211 10:23:54.579648 11272 solver.cpp:218] Iteration 92400 (15.7698 iter/s, 6.34123s/100 iters), loss = 0.740552
I1211 10:23:54.579648 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 10:23:54.579648 11272 solver.cpp:237]     Train net output #1: loss = 0.740552 (* 1 = 0.740552 loss)
I1211 10:23:54.579648 11272 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1211 10:24:00.603075 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:24:00.852103 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92500.caffemodel
I1211 10:24:00.867105 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92500.solverstate
I1211 10:24:00.872104 11272 solver.cpp:330] Iteration 92500, Testing net (#0)
I1211 10:24:00.872104 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:24:02.392271 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:24:02.452275 11272 solver.cpp:397]     Test net output #0: accuracy = 0.5805
I1211 10:24:02.452275 11272 solver.cpp:397]     Test net output #1: loss = 1.64163 (* 1 = 1.64163 loss)
I1211 10:24:02.512274 11272 solver.cpp:218] Iteration 92500 (12.6075 iter/s, 7.93181s/100 iters), loss = 0.592779
I1211 10:24:02.512274 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 10:24:02.512274 11272 solver.cpp:237]     Train net output #1: loss = 0.592779 (* 1 = 0.592779 loss)
I1211 10:24:02.512274 11272 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1211 10:24:08.828213 11272 solver.cpp:218] Iteration 92600 (15.8344 iter/s, 6.31538s/100 iters), loss = 0.644392
I1211 10:24:08.828213 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 10:24:08.828213 11272 solver.cpp:237]     Train net output #1: loss = 0.644392 (* 1 = 0.644392 loss)
I1211 10:24:08.828213 11272 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1211 10:24:15.215219 11272 solver.cpp:218] Iteration 92700 (15.6581 iter/s, 6.38647s/100 iters), loss = 0.651564
I1211 10:24:15.215219 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 10:24:15.215219 11272 solver.cpp:237]     Train net output #1: loss = 0.651564 (* 1 = 0.651564 loss)
I1211 10:24:15.215219 11272 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1211 10:24:21.562705 11272 solver.cpp:218] Iteration 92800 (15.7559 iter/s, 6.34682s/100 iters), loss = 0.898511
I1211 10:24:21.562705 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 10:24:21.562705 11272 solver.cpp:237]     Train net output #1: loss = 0.898511 (* 1 = 0.898511 loss)
I1211 10:24:21.562705 11272 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1211 10:24:27.894243 11272 solver.cpp:218] Iteration 92900 (15.7934 iter/s, 6.33175s/100 iters), loss = 0.751305
I1211 10:24:27.894243 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 10:24:27.894243 11272 solver.cpp:237]     Train net output #1: loss = 0.751305 (* 1 = 0.751305 loss)
I1211 10:24:27.894243 11272 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1211 10:24:33.911749 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:24:34.162772 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93000.caffemodel
I1211 10:24:34.178772 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93000.solverstate
I1211 10:24:34.182772 11272 solver.cpp:330] Iteration 93000, Testing net (#0)
I1211 10:24:34.182772 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:24:35.701869 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:24:35.761883 11272 solver.cpp:397]     Test net output #0: accuracy = 0.5811
I1211 10:24:35.761883 11272 solver.cpp:397]     Test net output #1: loss = 1.67087 (* 1 = 1.67087 loss)
I1211 10:24:35.822383 11272 solver.cpp:218] Iteration 93000 (12.6149 iter/s, 7.92713s/100 iters), loss = 0.526476
I1211 10:24:35.822383 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:24:35.822383 11272 solver.cpp:237]     Train net output #1: loss = 0.526476 (* 1 = 0.526476 loss)
I1211 10:24:35.822383 11272 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1211 10:24:42.155340 11272 solver.cpp:218] Iteration 93100 (15.7908 iter/s, 6.33279s/100 iters), loss = 0.73893
I1211 10:24:42.155340 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 10:24:42.155340 11272 solver.cpp:237]     Train net output #1: loss = 0.73893 (* 1 = 0.73893 loss)
I1211 10:24:42.155340 11272 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1211 10:24:48.580016 11272 solver.cpp:218] Iteration 93200 (15.5655 iter/s, 6.42445s/100 iters), loss = 0.562275
I1211 10:24:48.580016 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 10:24:48.580016 11272 solver.cpp:237]     Train net output #1: loss = 0.562275 (* 1 = 0.562275 loss)
I1211 10:24:48.580016 11272 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1211 10:24:54.959517 11272 solver.cpp:218] Iteration 93300 (15.6773 iter/s, 6.37866s/100 iters), loss = 0.707513
I1211 10:24:54.959517 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 10:24:54.959517 11272 solver.cpp:237]     Train net output #1: loss = 0.707513 (* 1 = 0.707513 loss)
I1211 10:24:54.959517 11272 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1211 10:25:01.374019 11272 solver.cpp:218] Iteration 93400 (15.59 iter/s, 6.41438s/100 iters), loss = 0.829285
I1211 10:25:01.374019 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 10:25:01.374019 11272 solver.cpp:237]     Train net output #1: loss = 0.829285 (* 1 = 0.829285 loss)
I1211 10:25:01.374019 11272 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1211 10:25:07.536180 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:25:07.797170 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93500.caffemodel
I1211 10:25:07.813170 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93500.solverstate
I1211 10:25:07.818672 11272 solver.cpp:330] Iteration 93500, Testing net (#0)
I1211 10:25:07.818672 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:25:09.394672 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:25:09.457173 11272 solver.cpp:397]     Test net output #0: accuracy = 0.574
I1211 10:25:09.457173 11272 solver.cpp:397]     Test net output #1: loss = 1.71654 (* 1 = 1.71654 loss)
I1211 10:25:09.520169 11272 solver.cpp:218] Iteration 93500 (12.2771 iter/s, 8.14523s/100 iters), loss = 0.537674
I1211 10:25:09.520169 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:25:09.520169 11272 solver.cpp:237]     Train net output #1: loss = 0.537674 (* 1 = 0.537674 loss)
I1211 10:25:09.520169 11272 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1211 10:25:16.064469 11272 solver.cpp:218] Iteration 93600 (15.2809 iter/s, 6.54411s/100 iters), loss = 0.705804
I1211 10:25:16.064957 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 10:25:16.064957 11272 solver.cpp:237]     Train net output #1: loss = 0.705804 (* 1 = 0.705804 loss)
I1211 10:25:16.064957 11272 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1211 10:25:22.554747 11272 solver.cpp:218] Iteration 93700 (15.4085 iter/s, 6.48993s/100 iters), loss = 0.651329
I1211 10:25:22.554747 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:25:22.554747 11272 solver.cpp:237]     Train net output #1: loss = 0.651329 (* 1 = 0.651329 loss)
I1211 10:25:22.554747 11272 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1211 10:25:28.879254 11272 solver.cpp:218] Iteration 93800 (15.8134 iter/s, 6.32377s/100 iters), loss = 0.66843
I1211 10:25:28.879254 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 10:25:28.879254 11272 solver.cpp:237]     Train net output #1: loss = 0.66843 (* 1 = 0.66843 loss)
I1211 10:25:28.879254 11272 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1211 10:25:35.204761 11272 solver.cpp:218] Iteration 93900 (15.8101 iter/s, 6.32508s/100 iters), loss = 0.760895
I1211 10:25:35.204761 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 10:25:35.204761 11272 solver.cpp:237]     Train net output #1: loss = 0.760895 (* 1 = 0.760895 loss)
I1211 10:25:35.204761 11272 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1211 10:25:41.236227 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:25:41.486250 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94000.caffemodel
I1211 10:25:41.502250 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94000.solverstate
I1211 10:25:41.506253 11272 solver.cpp:330] Iteration 94000, Testing net (#0)
I1211 10:25:41.506253 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:25:43.027375 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:25:43.087391 11272 solver.cpp:397]     Test net output #0: accuracy = 0.5604
I1211 10:25:43.087391 11272 solver.cpp:397]     Test net output #1: loss = 1.79345 (* 1 = 1.79345 loss)
I1211 10:25:43.148382 11272 solver.cpp:218] Iteration 94000 (12.5893 iter/s, 7.94323s/100 iters), loss = 0.78537
I1211 10:25:43.148382 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 10:25:43.148382 11272 solver.cpp:237]     Train net output #1: loss = 0.78537 (* 1 = 0.78537 loss)
I1211 10:25:43.148382 11272 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1211 10:25:49.492833 11272 solver.cpp:218] Iteration 94100 (15.7616 iter/s, 6.34454s/100 iters), loss = 0.702926
I1211 10:25:49.493834 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 10:25:49.493834 11272 solver.cpp:237]     Train net output #1: loss = 0.702926 (* 1 = 0.702926 loss)
I1211 10:25:49.493834 11272 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1211 10:25:55.841300 11272 solver.cpp:218] Iteration 94200 (15.7548 iter/s, 6.34726s/100 iters), loss = 0.587843
I1211 10:25:55.841300 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:25:55.841300 11272 solver.cpp:237]     Train net output #1: loss = 0.587843 (* 1 = 0.587843 loss)
I1211 10:25:55.841300 11272 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1211 10:26:02.173255 11272 solver.cpp:218] Iteration 94300 (15.7942 iter/s, 6.33145s/100 iters), loss = 0.60844
I1211 10:26:02.173255 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 10:26:02.173255 11272 solver.cpp:237]     Train net output #1: loss = 0.60844 (* 1 = 0.60844 loss)
I1211 10:26:02.173255 11272 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1211 10:26:08.550362 11272 solver.cpp:218] Iteration 94400 (15.6824 iter/s, 6.37659s/100 iters), loss = 0.875358
I1211 10:26:08.550362 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.69
I1211 10:26:08.550362 11272 solver.cpp:237]     Train net output #1: loss = 0.875358 (* 1 = 0.875358 loss)
I1211 10:26:08.550362 11272 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1211 10:26:14.646011 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:26:14.895573 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94500.caffemodel
I1211 10:26:14.911574 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94500.solverstate
I1211 10:26:14.915575 11272 solver.cpp:330] Iteration 94500, Testing net (#0)
I1211 10:26:14.916576 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:26:16.436741 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:26:16.496601 11272 solver.cpp:397]     Test net output #0: accuracy = 0.5912
I1211 10:26:16.496601 11272 solver.cpp:397]     Test net output #1: loss = 1.63859 (* 1 = 1.63859 loss)
I1211 10:26:16.557601 11272 solver.cpp:218] Iteration 94500 (12.4885 iter/s, 8.00735s/100 iters), loss = 0.675186
I1211 10:26:16.557601 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 10:26:16.557601 11272 solver.cpp:237]     Train net output #1: loss = 0.675186 (* 1 = 0.675186 loss)
I1211 10:26:16.557601 11272 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1211 10:26:22.964032 11272 solver.cpp:218] Iteration 94600 (15.6118 iter/s, 6.40543s/100 iters), loss = 0.66509
I1211 10:26:22.964032 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 10:26:22.964032 11272 solver.cpp:237]     Train net output #1: loss = 0.66509 (* 1 = 0.66509 loss)
I1211 10:26:22.964032 11272 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1211 10:26:29.514549 11272 solver.cpp:218] Iteration 94700 (15.2673 iter/s, 6.54997s/100 iters), loss = 0.507038
I1211 10:26:29.514549 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:26:29.514549 11272 solver.cpp:237]     Train net output #1: loss = 0.507038 (* 1 = 0.507038 loss)
I1211 10:26:29.514549 11272 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1211 10:26:35.977725 11272 solver.cpp:218] Iteration 94800 (15.4737 iter/s, 6.46259s/100 iters), loss = 0.721144
I1211 10:26:35.977725 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 10:26:35.977725 11272 solver.cpp:237]     Train net output #1: loss = 0.721144 (* 1 = 0.721144 loss)
I1211 10:26:35.977725 11272 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1211 10:26:42.435536 11272 solver.cpp:218] Iteration 94900 (15.4861 iter/s, 6.45739s/100 iters), loss = 0.763482
I1211 10:26:42.435536 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 10:26:42.435536 11272 solver.cpp:237]     Train net output #1: loss = 0.763482 (* 1 = 0.763482 loss)
I1211 10:26:42.435536 11272 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1211 10:26:48.571643 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:26:48.826642 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95000.caffemodel
I1211 10:26:48.842644 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95000.solverstate
I1211 10:26:48.847142 11272 solver.cpp:330] Iteration 95000, Testing net (#0)
I1211 10:26:48.847142 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:26:50.396643 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:26:50.458143 11272 solver.cpp:397]     Test net output #0: accuracy = 0.536
I1211 10:26:50.458143 11272 solver.cpp:397]     Test net output #1: loss = 1.98223 (* 1 = 1.98223 loss)
I1211 10:26:50.519143 11272 solver.cpp:218] Iteration 95000 (12.3712 iter/s, 8.08328s/100 iters), loss = 0.621588
I1211 10:26:50.519143 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:26:50.519143 11272 solver.cpp:237]     Train net output #1: loss = 0.621588 (* 1 = 0.621588 loss)
I1211 10:26:50.519143 11272 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1211 10:26:50.519143 11272 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1211 10:26:56.961143 11272 solver.cpp:218] Iteration 95100 (15.524 iter/s, 6.44165s/100 iters), loss = 0.561219
I1211 10:26:56.961143 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:26:56.961143 11272 solver.cpp:237]     Train net output #1: loss = 0.561219 (* 1 = 0.561219 loss)
I1211 10:26:56.961143 11272 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1211 10:27:03.424899 11272 solver.cpp:218] Iteration 95200 (15.4727 iter/s, 6.46301s/100 iters), loss = 0.478487
I1211 10:27:03.424899 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 10:27:03.424899 11272 solver.cpp:237]     Train net output #1: loss = 0.478487 (* 1 = 0.478487 loss)
I1211 10:27:03.424899 11272 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1211 10:27:09.871289 11272 solver.cpp:218] Iteration 95300 (15.5139 iter/s, 6.44584s/100 iters), loss = 0.529287
I1211 10:27:09.871289 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:27:09.871289 11272 solver.cpp:237]     Train net output #1: loss = 0.529287 (* 1 = 0.529287 loss)
I1211 10:27:09.871289 11272 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1211 10:27:16.380338 11272 solver.cpp:218] Iteration 95400 (15.3638 iter/s, 6.5088s/100 iters), loss = 0.536033
I1211 10:27:16.380338 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 10:27:16.380338 11272 solver.cpp:237]     Train net output #1: loss = 0.536033 (* 1 = 0.536033 loss)
I1211 10:27:16.380338 11272 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1211 10:27:22.526788 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:27:22.781304 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95500.caffemodel
I1211 10:27:22.798807 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95500.solverstate
I1211 10:27:22.803807 11272 solver.cpp:330] Iteration 95500, Testing net (#0)
I1211 10:27:22.803807 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:27:24.364789 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:27:24.425788 11272 solver.cpp:397]     Test net output #0: accuracy = 0.673
I1211 10:27:24.426287 11272 solver.cpp:397]     Test net output #1: loss = 1.19186 (* 1 = 1.19186 loss)
I1211 10:27:24.488287 11272 solver.cpp:218] Iteration 95500 (12.3345 iter/s, 8.10733s/100 iters), loss = 0.469855
I1211 10:27:24.488287 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:27:24.488287 11272 solver.cpp:237]     Train net output #1: loss = 0.469855 (* 1 = 0.469855 loss)
I1211 10:27:24.488287 11272 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1211 10:27:30.937090 11272 solver.cpp:218] Iteration 95600 (15.5084 iter/s, 6.44814s/100 iters), loss = 0.468849
I1211 10:27:30.937090 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:27:30.937090 11272 solver.cpp:237]     Train net output #1: loss = 0.468849 (* 1 = 0.468849 loss)
I1211 10:27:30.937090 11272 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1211 10:27:37.387508 11272 solver.cpp:218] Iteration 95700 (15.5044 iter/s, 6.44979s/100 iters), loss = 0.400622
I1211 10:27:37.387508 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:27:37.387508 11272 solver.cpp:237]     Train net output #1: loss = 0.400622 (* 1 = 0.400622 loss)
I1211 10:27:37.387508 11272 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1211 10:27:43.841598 11272 solver.cpp:218] Iteration 95800 (15.4943 iter/s, 6.454s/100 iters), loss = 0.472071
I1211 10:27:43.842098 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:27:43.842098 11272 solver.cpp:237]     Train net output #1: loss = 0.472071 (* 1 = 0.472071 loss)
I1211 10:27:43.842098 11272 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1211 10:27:50.305217 11272 solver.cpp:218] Iteration 95900 (15.4729 iter/s, 6.46291s/100 iters), loss = 0.512081
I1211 10:27:50.305217 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:27:50.305217 11272 solver.cpp:237]     Train net output #1: loss = 0.512081 (* 1 = 0.512081 loss)
I1211 10:27:50.305217 11272 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1211 10:27:56.440732 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:27:56.694721 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96000.caffemodel
I1211 10:27:56.710722 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96000.solverstate
I1211 10:27:56.715723 11272 solver.cpp:330] Iteration 96000, Testing net (#0)
I1211 10:27:56.715723 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:27:58.266748 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:27:58.328248 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6813
I1211 10:27:58.328248 11272 solver.cpp:397]     Test net output #1: loss = 1.19052 (* 1 = 1.19052 loss)
I1211 10:27:58.390249 11272 solver.cpp:218] Iteration 96000 (12.3692 iter/s, 8.08458s/100 iters), loss = 0.485907
I1211 10:27:58.390249 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:27:58.390249 11272 solver.cpp:237]     Train net output #1: loss = 0.485907 (* 1 = 0.485907 loss)
I1211 10:27:58.390249 11272 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1211 10:28:04.851747 11272 solver.cpp:218] Iteration 96100 (15.4771 iter/s, 6.46115s/100 iters), loss = 0.529323
I1211 10:28:04.852246 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 10:28:04.852246 11272 solver.cpp:237]     Train net output #1: loss = 0.529323 (* 1 = 0.529323 loss)
I1211 10:28:04.852246 11272 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1211 10:28:11.326095 11272 solver.cpp:218] Iteration 96200 (15.4476 iter/s, 6.47351s/100 iters), loss = 0.358404
I1211 10:28:11.326095 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:28:11.326095 11272 solver.cpp:237]     Train net output #1: loss = 0.358404 (* 1 = 0.358404 loss)
I1211 10:28:11.326095 11272 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1211 10:28:17.794775 11272 solver.cpp:218] Iteration 96300 (15.4599 iter/s, 6.46836s/100 iters), loss = 0.491174
I1211 10:28:17.794775 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:28:17.794775 11272 solver.cpp:237]     Train net output #1: loss = 0.491174 (* 1 = 0.491174 loss)
I1211 10:28:17.794775 11272 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1211 10:28:24.387538 11272 solver.cpp:218] Iteration 96400 (15.169 iter/s, 6.59237s/100 iters), loss = 0.450693
I1211 10:28:24.388037 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:28:24.388037 11272 solver.cpp:237]     Train net output #1: loss = 0.450693 (* 1 = 0.450693 loss)
I1211 10:28:24.388037 11272 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1211 10:28:30.470485 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:28:30.720486 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96500.caffemodel
I1211 10:28:30.736486 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96500.solverstate
I1211 10:28:30.741487 11272 solver.cpp:330] Iteration 96500, Testing net (#0)
I1211 10:28:30.741487 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:28:32.270485 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:28:32.332984 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6805
I1211 10:28:32.332984 11272 solver.cpp:397]     Test net output #1: loss = 1.19103 (* 1 = 1.19103 loss)
I1211 10:28:32.396484 11272 solver.cpp:218] Iteration 96500 (12.487 iter/s, 8.00832s/100 iters), loss = 0.525384
I1211 10:28:32.396484 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 10:28:32.396484 11272 solver.cpp:237]     Train net output #1: loss = 0.525384 (* 1 = 0.525384 loss)
I1211 10:28:32.396484 11272 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1211 10:28:38.751950 11272 solver.cpp:218] Iteration 96600 (15.7357 iter/s, 6.35497s/100 iters), loss = 0.485443
I1211 10:28:38.751950 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:28:38.751950 11272 solver.cpp:237]     Train net output #1: loss = 0.485443 (* 1 = 0.485443 loss)
I1211 10:28:38.751950 11272 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1211 10:28:45.086951 11272 solver.cpp:218] Iteration 96700 (15.787 iter/s, 6.33431s/100 iters), loss = 0.318473
I1211 10:28:45.086951 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:28:45.086951 11272 solver.cpp:237]     Train net output #1: loss = 0.318473 (* 1 = 0.318473 loss)
I1211 10:28:45.086951 11272 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1211 10:28:51.445000 11272 solver.cpp:218] Iteration 96800 (15.7281 iter/s, 6.35803s/100 iters), loss = 0.512961
I1211 10:28:51.445508 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:28:51.445508 11272 solver.cpp:237]     Train net output #1: loss = 0.512961 (* 1 = 0.512961 loss)
I1211 10:28:51.445508 11272 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1211 10:28:57.796005 11272 solver.cpp:218] Iteration 96900 (15.7469 iter/s, 6.35047s/100 iters), loss = 0.481664
I1211 10:28:57.796005 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:28:57.796005 11272 solver.cpp:237]     Train net output #1: loss = 0.481664 (* 1 = 0.481664 loss)
I1211 10:28:57.796005 11272 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1211 10:29:04.003640 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:29:04.264464 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97000.caffemodel
I1211 10:29:04.279464 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97000.solverstate
I1211 10:29:04.285465 11272 solver.cpp:330] Iteration 97000, Testing net (#0)
I1211 10:29:04.285465 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:29:05.852383 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:29:05.914891 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6803
I1211 10:29:05.914891 11272 solver.cpp:397]     Test net output #1: loss = 1.18497 (* 1 = 1.18497 loss)
I1211 10:29:05.982902 11272 solver.cpp:218] Iteration 97000 (12.2157 iter/s, 8.18619s/100 iters), loss = 0.444408
I1211 10:29:05.982902 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:29:05.982902 11272 solver.cpp:237]     Train net output #1: loss = 0.444408 (* 1 = 0.444408 loss)
I1211 10:29:05.982902 11272 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1211 10:29:12.480478 11272 solver.cpp:218] Iteration 97100 (15.391 iter/s, 6.4973s/100 iters), loss = 0.469346
I1211 10:29:12.480978 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:29:12.480978 11272 solver.cpp:237]     Train net output #1: loss = 0.469346 (* 1 = 0.469346 loss)
I1211 10:29:12.480978 11272 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1211 10:29:18.954794 11272 solver.cpp:218] Iteration 97200 (15.4471 iter/s, 6.47369s/100 iters), loss = 0.353618
I1211 10:29:18.954794 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:29:18.954794 11272 solver.cpp:237]     Train net output #1: loss = 0.353618 (* 1 = 0.353618 loss)
I1211 10:29:18.954794 11272 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1211 10:29:25.417958 11272 solver.cpp:218] Iteration 97300 (15.4743 iter/s, 6.46235s/100 iters), loss = 0.331643
I1211 10:29:25.417958 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:29:25.417958 11272 solver.cpp:237]     Train net output #1: loss = 0.331643 (* 1 = 0.331643 loss)
I1211 10:29:25.417958 11272 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1211 10:29:31.878033 11272 solver.cpp:218] Iteration 97400 (15.4801 iter/s, 6.45989s/100 iters), loss = 0.409432
I1211 10:29:31.878033 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:29:31.878033 11272 solver.cpp:237]     Train net output #1: loss = 0.409432 (* 1 = 0.409432 loss)
I1211 10:29:31.878033 11272 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1211 10:29:38.021792 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:29:38.276806 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97500.caffemodel
I1211 10:29:38.292805 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97500.solverstate
I1211 10:29:38.297806 11272 solver.cpp:330] Iteration 97500, Testing net (#0)
I1211 10:29:38.298292 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:29:39.845793 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:29:39.907292 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6821
I1211 10:29:39.907292 11272 solver.cpp:397]     Test net output #1: loss = 1.18748 (* 1 = 1.18748 loss)
I1211 10:29:39.968292 11272 solver.cpp:218] Iteration 97500 (12.3611 iter/s, 8.08987s/100 iters), loss = 0.483032
I1211 10:29:39.968796 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:29:39.968796 11272 solver.cpp:237]     Train net output #1: loss = 0.483032 (* 1 = 0.483032 loss)
I1211 10:29:39.968796 11272 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1211 10:29:46.428900 11272 solver.cpp:218] Iteration 97600 (15.4804 iter/s, 6.45978s/100 iters), loss = 0.4653
I1211 10:29:46.428900 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:29:46.428900 11272 solver.cpp:237]     Train net output #1: loss = 0.4653 (* 1 = 0.4653 loss)
I1211 10:29:46.428900 11272 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1211 10:29:52.879442 11272 solver.cpp:218] Iteration 97700 (15.5033 iter/s, 6.45025s/100 iters), loss = 0.383258
I1211 10:29:52.879931 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:29:52.879931 11272 solver.cpp:237]     Train net output #1: loss = 0.383258 (* 1 = 0.383258 loss)
I1211 10:29:52.879931 11272 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1211 10:29:59.334805 11272 solver.cpp:218] Iteration 97800 (15.4926 iter/s, 6.45469s/100 iters), loss = 0.386916
I1211 10:29:59.334805 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:29:59.334805 11272 solver.cpp:237]     Train net output #1: loss = 0.386916 (* 1 = 0.386916 loss)
I1211 10:29:59.334805 11272 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1211 10:30:05.827996 11272 solver.cpp:218] Iteration 97900 (15.4021 iter/s, 6.49262s/100 iters), loss = 0.475538
I1211 10:30:05.827996 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:30:05.827996 11272 solver.cpp:237]     Train net output #1: loss = 0.475538 (* 1 = 0.475538 loss)
I1211 10:30:05.827996 11272 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1211 10:30:11.883355 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:30:12.134851 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98000.caffemodel
I1211 10:30:12.151350 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98000.solverstate
I1211 10:30:12.156352 11272 solver.cpp:330] Iteration 98000, Testing net (#0)
I1211 10:30:12.156352 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:30:13.684854 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:30:13.744854 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6816
I1211 10:30:13.744854 11272 solver.cpp:397]     Test net output #1: loss = 1.19376 (* 1 = 1.19376 loss)
I1211 10:30:13.806352 11272 solver.cpp:218] Iteration 98000 (12.5344 iter/s, 7.97804s/100 iters), loss = 0.450155
I1211 10:30:13.806854 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:30:13.806854 11272 solver.cpp:237]     Train net output #1: loss = 0.450155 (* 1 = 0.450155 loss)
I1211 10:30:13.806854 11272 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1211 10:30:20.167289 11272 solver.cpp:218] Iteration 98100 (15.722 iter/s, 6.3605s/100 iters), loss = 0.501881
I1211 10:30:20.167790 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 10:30:20.167790 11272 solver.cpp:237]     Train net output #1: loss = 0.501881 (* 1 = 0.501881 loss)
I1211 10:30:20.167790 11272 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1211 10:30:26.560951 11272 solver.cpp:218] Iteration 98200 (15.6424 iter/s, 6.39288s/100 iters), loss = 0.326334
I1211 10:30:26.560951 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:30:26.560951 11272 solver.cpp:237]     Train net output #1: loss = 0.326334 (* 1 = 0.326334 loss)
I1211 10:30:26.560951 11272 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1211 10:30:33.047317 11272 solver.cpp:218] Iteration 98300 (15.4179 iter/s, 6.48596s/100 iters), loss = 0.422888
I1211 10:30:33.047317 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:30:33.047317 11272 solver.cpp:237]     Train net output #1: loss = 0.422888 (* 1 = 0.422888 loss)
I1211 10:30:33.047317 11272 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1211 10:30:39.506209 11272 solver.cpp:218] Iteration 98400 (15.4835 iter/s, 6.4585s/100 iters), loss = 0.494114
I1211 10:30:39.506209 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:30:39.506209 11272 solver.cpp:237]     Train net output #1: loss = 0.494114 (* 1 = 0.494114 loss)
I1211 10:30:39.506209 11272 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1211 10:30:45.652179 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:30:45.904682 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98500.caffemodel
I1211 10:30:45.919682 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98500.solverstate
I1211 10:30:45.924682 11272 solver.cpp:330] Iteration 98500, Testing net (#0)
I1211 10:30:45.925182 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:30:47.474213 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:30:47.534791 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6815
I1211 10:30:47.534791 11272 solver.cpp:397]     Test net output #1: loss = 1.19858 (* 1 = 1.19858 loss)
I1211 10:30:47.596791 11272 solver.cpp:218] Iteration 98500 (12.3611 iter/s, 8.08992s/100 iters), loss = 0.447692
I1211 10:30:47.596791 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:30:47.596791 11272 solver.cpp:237]     Train net output #1: loss = 0.447692 (* 1 = 0.447692 loss)
I1211 10:30:47.596791 11272 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1211 10:30:54.051507 11272 solver.cpp:218] Iteration 98600 (15.493 iter/s, 6.45452s/100 iters), loss = 0.357297
I1211 10:30:54.051507 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:30:54.051507 11272 solver.cpp:237]     Train net output #1: loss = 0.357297 (* 1 = 0.357297 loss)
I1211 10:30:54.051507 11272 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1211 10:31:00.508004 11272 solver.cpp:218] Iteration 98700 (15.4895 iter/s, 6.45599s/100 iters), loss = 0.376786
I1211 10:31:00.508004 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:31:00.508004 11272 solver.cpp:237]     Train net output #1: loss = 0.376786 (* 1 = 0.376786 loss)
I1211 10:31:00.508004 11272 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1211 10:31:06.949255 11272 solver.cpp:218] Iteration 98800 (15.5265 iter/s, 6.4406s/100 iters), loss = 0.579497
I1211 10:31:06.949255 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:31:06.949255 11272 solver.cpp:237]     Train net output #1: loss = 0.579497 (* 1 = 0.579497 loss)
I1211 10:31:06.949255 11272 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1211 10:31:13.404343 11272 solver.cpp:218] Iteration 98900 (15.4931 iter/s, 6.45447s/100 iters), loss = 0.374812
I1211 10:31:13.404343 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:31:13.404343 11272 solver.cpp:237]     Train net output #1: loss = 0.374812 (* 1 = 0.374812 loss)
I1211 10:31:13.404343 11272 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1211 10:31:19.540388 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:31:19.794888 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99000.caffemodel
I1211 10:31:19.810887 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99000.solverstate
I1211 10:31:19.816387 11272 solver.cpp:330] Iteration 99000, Testing net (#0)
I1211 10:31:19.816387 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:31:21.356386 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:31:21.417387 11272 solver.cpp:397]     Test net output #0: accuracy = 0.683
I1211 10:31:21.417387 11272 solver.cpp:397]     Test net output #1: loss = 1.19954 (* 1 = 1.19954 loss)
I1211 10:31:21.479897 11272 solver.cpp:218] Iteration 99000 (12.3837 iter/s, 8.07511s/100 iters), loss = 0.417738
I1211 10:31:21.479897 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:31:21.479897 11272 solver.cpp:237]     Train net output #1: loss = 0.417738 (* 1 = 0.417738 loss)
I1211 10:31:21.479897 11272 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1211 10:31:28.019327 11272 solver.cpp:218] Iteration 99100 (15.292 iter/s, 6.53935s/100 iters), loss = 0.411738
I1211 10:31:28.019327 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:31:28.019327 11272 solver.cpp:237]     Train net output #1: loss = 0.411738 (* 1 = 0.411738 loss)
I1211 10:31:28.019327 11272 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1211 10:31:34.564559 11272 solver.cpp:218] Iteration 99200 (15.2801 iter/s, 6.54447s/100 iters), loss = 0.295707
I1211 10:31:34.565069 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:31:34.565069 11272 solver.cpp:237]     Train net output #1: loss = 0.295707 (* 1 = 0.295707 loss)
I1211 10:31:34.565069 11272 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1211 10:31:41.067746 11272 solver.cpp:218] Iteration 99300 (15.3795 iter/s, 6.50215s/100 iters), loss = 0.332111
I1211 10:31:41.067746 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 10:31:41.067746 11272 solver.cpp:237]     Train net output #1: loss = 0.332111 (* 1 = 0.332111 loss)
I1211 10:31:41.067746 11272 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1211 10:31:47.542065 11272 solver.cpp:218] Iteration 99400 (15.4463 iter/s, 6.47405s/100 iters), loss = 0.447681
I1211 10:31:47.542065 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:31:47.542065 11272 solver.cpp:237]     Train net output #1: loss = 0.447681 (* 1 = 0.447681 loss)
I1211 10:31:47.542065 11272 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1211 10:31:53.738207 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:31:53.992216 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99500.caffemodel
I1211 10:31:54.007706 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99500.solverstate
I1211 10:31:54.012706 11272 solver.cpp:330] Iteration 99500, Testing net (#0)
I1211 10:31:54.012706 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:31:55.555707 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:31:55.616706 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6802
I1211 10:31:55.616706 11272 solver.cpp:397]     Test net output #1: loss = 1.20716 (* 1 = 1.20716 loss)
I1211 10:31:55.678706 11272 solver.cpp:218] Iteration 99500 (12.2914 iter/s, 8.13574s/100 iters), loss = 0.431048
I1211 10:31:55.678706 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:31:55.678706 11272 solver.cpp:237]     Train net output #1: loss = 0.431048 (* 1 = 0.431048 loss)
I1211 10:31:55.678706 11272 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1211 10:32:02.126392 11272 solver.cpp:218] Iteration 99600 (15.5106 iter/s, 6.44718s/100 iters), loss = 0.413318
I1211 10:32:02.126392 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:32:02.126392 11272 solver.cpp:237]     Train net output #1: loss = 0.413318 (* 1 = 0.413318 loss)
I1211 10:32:02.126392 11272 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1211 10:32:08.586941 11272 solver.cpp:218] Iteration 99700 (15.4796 iter/s, 6.46013s/100 iters), loss = 0.400746
I1211 10:32:08.586941 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:32:08.586941 11272 solver.cpp:237]     Train net output #1: loss = 0.400746 (* 1 = 0.400746 loss)
I1211 10:32:08.586941 11272 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1211 10:32:15.101884 11272 solver.cpp:218] Iteration 99800 (15.3507 iter/s, 6.51436s/100 iters), loss = 0.377054
I1211 10:32:15.101884 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:32:15.101884 11272 solver.cpp:237]     Train net output #1: loss = 0.377054 (* 1 = 0.377054 loss)
I1211 10:32:15.101884 11272 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1211 10:32:21.581081 11272 solver.cpp:218] Iteration 99900 (15.4346 iter/s, 6.47894s/100 iters), loss = 0.336213
I1211 10:32:21.581081 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:32:21.581081 11272 solver.cpp:237]     Train net output #1: loss = 0.336213 (* 1 = 0.336213 loss)
I1211 10:32:21.581081 11272 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1211 10:32:27.713610 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:32:27.969110 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100000.caffemodel
I1211 10:32:27.984609 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100000.solverstate
I1211 10:32:27.990110 11272 solver.cpp:330] Iteration 100000, Testing net (#0)
I1211 10:32:27.990110 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:32:29.551110 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:32:29.612110 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6822
I1211 10:32:29.612110 11272 solver.cpp:397]     Test net output #1: loss = 1.19889 (* 1 = 1.19889 loss)
I1211 10:32:29.673609 11272 solver.cpp:218] Iteration 100000 (12.3583 iter/s, 8.09173s/100 iters), loss = 0.384202
I1211 10:32:29.673609 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:32:29.673609 11272 solver.cpp:237]     Train net output #1: loss = 0.384202 (* 1 = 0.384202 loss)
I1211 10:32:29.673609 11272 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1211 10:32:36.155625 11272 solver.cpp:218] Iteration 100100 (15.4282 iter/s, 6.48163s/100 iters), loss = 0.324515
I1211 10:32:36.155625 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:32:36.155625 11272 solver.cpp:237]     Train net output #1: loss = 0.324515 (* 1 = 0.324515 loss)
I1211 10:32:36.155625 11272 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1211 10:32:42.609876 11272 solver.cpp:218] Iteration 100200 (15.4952 iter/s, 6.45361s/100 iters), loss = 0.400203
I1211 10:32:42.609876 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:32:42.609876 11272 solver.cpp:237]     Train net output #1: loss = 0.400203 (* 1 = 0.400203 loss)
I1211 10:32:42.609876 11272 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1211 10:32:49.061700 11272 solver.cpp:218] Iteration 100300 (15.5007 iter/s, 6.45131s/100 iters), loss = 0.323075
I1211 10:32:49.061700 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:32:49.061700 11272 solver.cpp:237]     Train net output #1: loss = 0.323075 (* 1 = 0.323075 loss)
I1211 10:32:49.061700 11272 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1211 10:32:55.501890 11272 solver.cpp:218] Iteration 100400 (15.5288 iter/s, 6.43963s/100 iters), loss = 0.448389
I1211 10:32:55.501890 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:32:55.501890 11272 solver.cpp:237]     Train net output #1: loss = 0.448389 (* 1 = 0.448389 loss)
I1211 10:32:55.501890 11272 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1211 10:33:01.621793 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:33:01.877792 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100500.caffemodel
I1211 10:33:01.893795 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100500.solverstate
I1211 10:33:01.898793 11272 solver.cpp:330] Iteration 100500, Testing net (#0)
I1211 10:33:01.899292 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:33:03.443338 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:33:03.504341 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6821
I1211 10:33:03.504341 11272 solver.cpp:397]     Test net output #1: loss = 1.21042 (* 1 = 1.21042 loss)
I1211 10:33:03.567338 11272 solver.cpp:218] Iteration 100500 (12.3996 iter/s, 8.06476s/100 iters), loss = 0.438922
I1211 10:33:03.567338 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:33:03.567338 11272 solver.cpp:237]     Train net output #1: loss = 0.438922 (* 1 = 0.438922 loss)
I1211 10:33:03.567338 11272 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1211 10:33:10.030221 11272 solver.cpp:218] Iteration 100600 (15.4733 iter/s, 6.46275s/100 iters), loss = 0.481857
I1211 10:33:10.030221 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 10:33:10.030221 11272 solver.cpp:237]     Train net output #1: loss = 0.481857 (* 1 = 0.481857 loss)
I1211 10:33:10.030221 11272 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1211 10:33:16.489670 11272 solver.cpp:218] Iteration 100700 (15.4824 iter/s, 6.45896s/100 iters), loss = 0.364798
I1211 10:33:16.490157 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:33:16.490157 11272 solver.cpp:237]     Train net output #1: loss = 0.364798 (* 1 = 0.364798 loss)
I1211 10:33:16.490157 11272 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1211 10:33:22.952491 11272 solver.cpp:218] Iteration 100800 (15.4748 iter/s, 6.46212s/100 iters), loss = 0.356133
I1211 10:33:22.952491 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:33:22.952491 11272 solver.cpp:237]     Train net output #1: loss = 0.356133 (* 1 = 0.356133 loss)
I1211 10:33:22.952491 11272 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1211 10:33:29.409257 11272 solver.cpp:218] Iteration 100900 (15.4891 iter/s, 6.45614s/100 iters), loss = 0.383727
I1211 10:33:29.409257 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:33:29.409257 11272 solver.cpp:237]     Train net output #1: loss = 0.383727 (* 1 = 0.383727 loss)
I1211 10:33:29.409257 11272 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1211 10:33:35.543534 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:33:35.800034 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101000.caffemodel
I1211 10:33:35.817034 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101000.solverstate
I1211 10:33:35.822548 11272 solver.cpp:330] Iteration 101000, Testing net (#0)
I1211 10:33:35.822548 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:33:37.370534 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:33:37.432034 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6784
I1211 10:33:37.432034 11272 solver.cpp:397]     Test net output #1: loss = 1.21699 (* 1 = 1.21699 loss)
I1211 10:33:37.492533 11272 solver.cpp:218] Iteration 101000 (12.3717 iter/s, 8.083s/100 iters), loss = 0.371391
I1211 10:33:37.492533 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:33:37.492533 11272 solver.cpp:237]     Train net output #1: loss = 0.371391 (* 1 = 0.371391 loss)
I1211 10:33:37.492533 11272 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1211 10:33:43.952533 11272 solver.cpp:218] Iteration 101100 (15.4818 iter/s, 6.45921s/100 iters), loss = 0.346056
I1211 10:33:43.952533 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:33:43.952533 11272 solver.cpp:237]     Train net output #1: loss = 0.346056 (* 1 = 0.346056 loss)
I1211 10:33:43.952533 11272 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1211 10:33:50.403126 11272 solver.cpp:218] Iteration 101200 (15.5035 iter/s, 6.45016s/100 iters), loss = 0.354798
I1211 10:33:50.403126 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:33:50.403126 11272 solver.cpp:237]     Train net output #1: loss = 0.354798 (* 1 = 0.354798 loss)
I1211 10:33:50.403126 11272 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1211 10:33:56.854177 11272 solver.cpp:218] Iteration 101300 (15.5023 iter/s, 6.45064s/100 iters), loss = 0.444174
I1211 10:33:56.854177 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:33:56.854177 11272 solver.cpp:237]     Train net output #1: loss = 0.444174 (* 1 = 0.444174 loss)
I1211 10:33:56.854177 11272 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1211 10:34:03.311437 11272 solver.cpp:218] Iteration 101400 (15.488 iter/s, 6.45661s/100 iters), loss = 0.37634
I1211 10:34:03.311437 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:34:03.311437 11272 solver.cpp:237]     Train net output #1: loss = 0.37634 (* 1 = 0.37634 loss)
I1211 10:34:03.311437 11272 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1211 10:34:09.444869 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:34:09.700383 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101500.caffemodel
I1211 10:34:09.719388 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101500.solverstate
I1211 10:34:09.724889 11272 solver.cpp:330] Iteration 101500, Testing net (#0)
I1211 10:34:09.724889 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:34:11.271368 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:34:11.332386 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6794
I1211 10:34:11.332386 11272 solver.cpp:397]     Test net output #1: loss = 1.20977 (* 1 = 1.20977 loss)
I1211 10:34:11.393878 11272 solver.cpp:218] Iteration 101500 (12.3733 iter/s, 8.08189s/100 iters), loss = 0.393992
I1211 10:34:11.393878 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:34:11.393878 11272 solver.cpp:237]     Train net output #1: loss = 0.393992 (* 1 = 0.393992 loss)
I1211 10:34:11.393878 11272 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1211 10:34:17.857328 11272 solver.cpp:218] Iteration 101600 (15.4723 iter/s, 6.46315s/100 iters), loss = 0.44688
I1211 10:34:17.857328 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:34:17.857328 11272 solver.cpp:237]     Train net output #1: loss = 0.44688 (* 1 = 0.44688 loss)
I1211 10:34:17.857328 11272 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1211 10:34:24.306782 11272 solver.cpp:218] Iteration 101700 (15.5058 iter/s, 6.44918s/100 iters), loss = 0.355273
I1211 10:34:24.307294 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:34:24.307294 11272 solver.cpp:237]     Train net output #1: loss = 0.355273 (* 1 = 0.355273 loss)
I1211 10:34:24.307294 11272 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1211 10:34:30.755492 11272 solver.cpp:218] Iteration 101800 (15.509 iter/s, 6.44786s/100 iters), loss = 0.312428
I1211 10:34:30.755492 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:34:30.755492 11272 solver.cpp:237]     Train net output #1: loss = 0.312428 (* 1 = 0.312428 loss)
I1211 10:34:30.755492 11272 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1211 10:34:37.214651 11272 solver.cpp:218] Iteration 101900 (15.4831 iter/s, 6.45865s/100 iters), loss = 0.445618
I1211 10:34:37.214651 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:34:37.214651 11272 solver.cpp:237]     Train net output #1: loss = 0.445618 (* 1 = 0.445618 loss)
I1211 10:34:37.214651 11272 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1211 10:34:43.361152 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:34:43.617651 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102000.caffemodel
I1211 10:34:43.633152 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102000.solverstate
I1211 10:34:43.638152 11272 solver.cpp:330] Iteration 102000, Testing net (#0)
I1211 10:34:43.638653 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:34:45.186152 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:34:45.247651 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6772
I1211 10:34:45.247651 11272 solver.cpp:397]     Test net output #1: loss = 1.22155 (* 1 = 1.22155 loss)
I1211 10:34:45.308651 11272 solver.cpp:218] Iteration 102000 (12.3552 iter/s, 8.09375s/100 iters), loss = 0.40567
I1211 10:34:45.309151 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:34:45.309151 11272 solver.cpp:237]     Train net output #1: loss = 0.40567 (* 1 = 0.40567 loss)
I1211 10:34:45.309151 11272 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1211 10:34:51.773890 11272 solver.cpp:218] Iteration 102100 (15.4693 iter/s, 6.46441s/100 iters), loss = 0.377758
I1211 10:34:51.773890 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:34:51.773890 11272 solver.cpp:237]     Train net output #1: loss = 0.377758 (* 1 = 0.377758 loss)
I1211 10:34:51.773890 11272 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1211 10:34:58.236277 11272 solver.cpp:218] Iteration 102200 (15.4755 iter/s, 6.46183s/100 iters), loss = 0.289761
I1211 10:34:58.236277 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:34:58.236277 11272 solver.cpp:237]     Train net output #1: loss = 0.289761 (* 1 = 0.289761 loss)
I1211 10:34:58.236277 11272 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1211 10:35:04.684072 11272 solver.cpp:218] Iteration 102300 (15.5098 iter/s, 6.44754s/100 iters), loss = 0.392281
I1211 10:35:04.684072 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:35:04.684072 11272 solver.cpp:237]     Train net output #1: loss = 0.392281 (* 1 = 0.392281 loss)
I1211 10:35:04.684072 11272 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1211 10:35:11.143821 11272 solver.cpp:218] Iteration 102400 (15.4815 iter/s, 6.45933s/100 iters), loss = 0.436539
I1211 10:35:11.143821 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:35:11.143821 11272 solver.cpp:237]     Train net output #1: loss = 0.436539 (* 1 = 0.436539 loss)
I1211 10:35:11.143821 11272 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1211 10:35:17.295948 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:35:17.551445 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102500.caffemodel
I1211 10:35:17.567447 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102500.solverstate
I1211 10:35:17.572450 11272 solver.cpp:330] Iteration 102500, Testing net (#0)
I1211 10:35:17.572450 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:35:19.121948 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:35:19.182947 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6813
I1211 10:35:19.183447 11272 solver.cpp:397]     Test net output #1: loss = 1.22087 (* 1 = 1.22087 loss)
I1211 10:35:19.244946 11272 solver.cpp:218] Iteration 102500 (12.3446 iter/s, 8.10072s/100 iters), loss = 0.379239
I1211 10:35:19.244946 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 10:35:19.244946 11272 solver.cpp:237]     Train net output #1: loss = 0.379239 (* 1 = 0.379239 loss)
I1211 10:35:19.244946 11272 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1211 10:35:25.693910 11272 solver.cpp:218] Iteration 102600 (15.5078 iter/s, 6.44839s/100 iters), loss = 0.449152
I1211 10:35:25.693910 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:35:25.693910 11272 solver.cpp:237]     Train net output #1: loss = 0.449152 (* 1 = 0.449152 loss)
I1211 10:35:25.693910 11272 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1211 10:35:32.151079 11272 solver.cpp:218] Iteration 102700 (15.4881 iter/s, 6.45658s/100 iters), loss = 0.31641
I1211 10:35:32.151079 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:35:32.151079 11272 solver.cpp:237]     Train net output #1: loss = 0.31641 (* 1 = 0.31641 loss)
I1211 10:35:32.151079 11272 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1211 10:35:38.597101 11272 solver.cpp:218] Iteration 102800 (15.5149 iter/s, 6.4454s/100 iters), loss = 0.380882
I1211 10:35:38.597101 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:35:38.597101 11272 solver.cpp:237]     Train net output #1: loss = 0.380882 (* 1 = 0.380882 loss)
I1211 10:35:38.597101 11272 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1211 10:35:45.040002 11272 solver.cpp:218] Iteration 102900 (15.5226 iter/s, 6.44224s/100 iters), loss = 0.453582
I1211 10:35:45.040002 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:35:45.040002 11272 solver.cpp:237]     Train net output #1: loss = 0.453582 (* 1 = 0.453582 loss)
I1211 10:35:45.040002 11272 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1211 10:35:51.177630 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:35:51.431663 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103000.caffemodel
I1211 10:35:51.446661 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103000.solverstate
I1211 10:35:51.451663 11272 solver.cpp:330] Iteration 103000, Testing net (#0)
I1211 10:35:51.451663 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:35:52.993726 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:35:53.055724 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6796
I1211 10:35:53.055724 11272 solver.cpp:397]     Test net output #1: loss = 1.22359 (* 1 = 1.22359 loss)
I1211 10:35:53.117725 11272 solver.cpp:218] Iteration 103000 (12.3805 iter/s, 8.07722s/100 iters), loss = 0.430661
I1211 10:35:53.117725 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:35:53.117725 11272 solver.cpp:237]     Train net output #1: loss = 0.430661 (* 1 = 0.430661 loss)
I1211 10:35:53.117725 11272 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1211 10:35:59.569325 11272 solver.cpp:218] Iteration 103100 (15.5012 iter/s, 6.4511s/100 iters), loss = 0.432466
I1211 10:35:59.569325 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:35:59.569325 11272 solver.cpp:237]     Train net output #1: loss = 0.432466 (* 1 = 0.432466 loss)
I1211 10:35:59.569325 11272 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1211 10:36:06.030817 11272 solver.cpp:218] Iteration 103200 (15.4766 iter/s, 6.46139s/100 iters), loss = 0.339215
I1211 10:36:06.031317 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:36:06.031317 11272 solver.cpp:237]     Train net output #1: loss = 0.339215 (* 1 = 0.339215 loss)
I1211 10:36:06.031317 11272 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1211 10:36:12.492902 11272 solver.cpp:218] Iteration 103300 (15.4773 iter/s, 6.46109s/100 iters), loss = 0.330003
I1211 10:36:12.492902 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:36:12.492902 11272 solver.cpp:237]     Train net output #1: loss = 0.330003 (* 1 = 0.330003 loss)
I1211 10:36:12.492902 11272 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1211 10:36:18.960098 11272 solver.cpp:218] Iteration 103400 (15.4637 iter/s, 6.46674s/100 iters), loss = 0.356814
I1211 10:36:18.960098 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:36:18.960098 11272 solver.cpp:237]     Train net output #1: loss = 0.356814 (* 1 = 0.356814 loss)
I1211 10:36:18.960098 11272 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1211 10:36:25.109666 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:36:25.367663 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103500.caffemodel
I1211 10:36:25.387163 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103500.solverstate
I1211 10:36:25.392163 11272 solver.cpp:330] Iteration 103500, Testing net (#0)
I1211 10:36:25.392163 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:36:26.954663 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:36:27.015661 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6798
I1211 10:36:27.015661 11272 solver.cpp:397]     Test net output #1: loss = 1.23411 (* 1 = 1.23411 loss)
I1211 10:36:27.077663 11272 solver.cpp:218] Iteration 103500 (12.3197 iter/s, 8.11705s/100 iters), loss = 0.467017
I1211 10:36:27.077663 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:36:27.077663 11272 solver.cpp:237]     Train net output #1: loss = 0.467017 (* 1 = 0.467017 loss)
I1211 10:36:27.077663 11272 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1211 10:36:33.566583 11272 solver.cpp:218] Iteration 103600 (15.412 iter/s, 6.48844s/100 iters), loss = 0.429395
I1211 10:36:33.566583 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:36:33.566583 11272 solver.cpp:237]     Train net output #1: loss = 0.429395 (* 1 = 0.429395 loss)
I1211 10:36:33.566583 11272 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1211 10:36:40.015592 11272 solver.cpp:218] Iteration 103700 (15.5071 iter/s, 6.44866s/100 iters), loss = 0.341879
I1211 10:36:40.015592 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:36:40.015592 11272 solver.cpp:237]     Train net output #1: loss = 0.341879 (* 1 = 0.341879 loss)
I1211 10:36:40.015592 11272 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1211 10:36:46.548804 11272 solver.cpp:218] Iteration 103800 (15.3076 iter/s, 6.53271s/100 iters), loss = 0.339216
I1211 10:36:46.548804 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:36:46.548804 11272 solver.cpp:237]     Train net output #1: loss = 0.339216 (* 1 = 0.339216 loss)
I1211 10:36:46.548804 11272 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1211 10:36:53.031764 11272 solver.cpp:218] Iteration 103900 (15.4264 iter/s, 6.4824s/100 iters), loss = 0.426236
I1211 10:36:53.031764 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:36:53.031764 11272 solver.cpp:237]     Train net output #1: loss = 0.426236 (* 1 = 0.426236 loss)
I1211 10:36:53.031764 11272 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1211 10:36:59.212630 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:36:59.469110 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104000.caffemodel
I1211 10:36:59.485610 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104000.solverstate
I1211 10:36:59.490111 11272 solver.cpp:330] Iteration 104000, Testing net (#0)
I1211 10:36:59.490111 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:37:01.038619 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:37:01.100111 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6795
I1211 10:37:01.100111 11272 solver.cpp:397]     Test net output #1: loss = 1.23705 (* 1 = 1.23705 loss)
I1211 10:37:01.161609 11272 solver.cpp:218] Iteration 104000 (12.3014 iter/s, 8.12919s/100 iters), loss = 0.466249
I1211 10:37:01.161609 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:37:01.161609 11272 solver.cpp:237]     Train net output #1: loss = 0.466249 (* 1 = 0.466249 loss)
I1211 10:37:01.161609 11272 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1211 10:37:07.635990 11272 solver.cpp:218] Iteration 104100 (15.4464 iter/s, 6.47401s/100 iters), loss = 0.368053
I1211 10:37:07.635990 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:37:07.635990 11272 solver.cpp:237]     Train net output #1: loss = 0.368053 (* 1 = 0.368053 loss)
I1211 10:37:07.635990 11272 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1211 10:37:14.149730 11272 solver.cpp:218] Iteration 104200 (15.3537 iter/s, 6.51307s/100 iters), loss = 0.268873
I1211 10:37:14.149730 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:37:14.149730 11272 solver.cpp:237]     Train net output #1: loss = 0.268873 (* 1 = 0.268873 loss)
I1211 10:37:14.149730 11272 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1211 10:37:20.633795 11272 solver.cpp:218] Iteration 104300 (15.4234 iter/s, 6.48367s/100 iters), loss = 0.284769
I1211 10:37:20.633795 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:37:20.633795 11272 solver.cpp:237]     Train net output #1: loss = 0.284769 (* 1 = 0.284769 loss)
I1211 10:37:20.633795 11272 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1211 10:37:27.086061 11272 solver.cpp:218] Iteration 104400 (15.4998 iter/s, 6.45168s/100 iters), loss = 0.377947
I1211 10:37:27.086061 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 10:37:27.086061 11272 solver.cpp:237]     Train net output #1: loss = 0.377947 (* 1 = 0.377947 loss)
I1211 10:37:27.086061 11272 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1211 10:37:33.247548 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:37:33.502548 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104500.caffemodel
I1211 10:37:33.518548 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104500.solverstate
I1211 10:37:33.523047 11272 solver.cpp:330] Iteration 104500, Testing net (#0)
I1211 10:37:33.523548 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:37:35.082548 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:37:35.146049 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6751
I1211 10:37:35.146548 11272 solver.cpp:397]     Test net output #1: loss = 1.24434 (* 1 = 1.24434 loss)
I1211 10:37:35.208547 11272 solver.cpp:218] Iteration 104500 (12.3128 iter/s, 8.12164s/100 iters), loss = 0.383675
I1211 10:37:35.208547 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:37:35.208547 11272 solver.cpp:237]     Train net output #1: loss = 0.383675 (* 1 = 0.383675 loss)
I1211 10:37:35.208547 11272 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1211 10:37:41.660357 11272 solver.cpp:218] Iteration 104600 (15.5007 iter/s, 6.4513s/100 iters), loss = 0.400275
I1211 10:37:41.660357 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:37:41.660357 11272 solver.cpp:237]     Train net output #1: loss = 0.400275 (* 1 = 0.400275 loss)
I1211 10:37:41.660357 11272 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1211 10:37:48.106858 11272 solver.cpp:218] Iteration 104700 (15.5132 iter/s, 6.44614s/100 iters), loss = 0.304037
I1211 10:37:48.106858 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:37:48.106858 11272 solver.cpp:237]     Train net output #1: loss = 0.304037 (* 1 = 0.304037 loss)
I1211 10:37:48.106858 11272 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1211 10:37:54.585629 11272 solver.cpp:218] Iteration 104800 (15.4364 iter/s, 6.47818s/100 iters), loss = 0.384089
I1211 10:37:54.585629 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:37:54.585629 11272 solver.cpp:237]     Train net output #1: loss = 0.384089 (* 1 = 0.384089 loss)
I1211 10:37:54.585629 11272 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1211 10:38:01.067845 11272 solver.cpp:218] Iteration 104900 (15.4278 iter/s, 6.48182s/100 iters), loss = 0.316283
I1211 10:38:01.068346 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:38:01.068346 11272 solver.cpp:237]     Train net output #1: loss = 0.316283 (* 1 = 0.316283 loss)
I1211 10:38:01.068346 11272 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1211 10:38:07.199640 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:38:07.453639 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105000.caffemodel
I1211 10:38:07.469638 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105000.solverstate
I1211 10:38:07.474640 11272 solver.cpp:330] Iteration 105000, Testing net (#0)
I1211 10:38:07.474640 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:38:09.018638 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:38:09.080140 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6769
I1211 10:38:09.080140 11272 solver.cpp:397]     Test net output #1: loss = 1.23193 (* 1 = 1.23193 loss)
I1211 10:38:09.142138 11272 solver.cpp:218] Iteration 105000 (12.386 iter/s, 8.07361s/100 iters), loss = 0.327055
I1211 10:38:09.142138 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:38:09.142138 11272 solver.cpp:237]     Train net output #1: loss = 0.327055 (* 1 = 0.327055 loss)
I1211 10:38:09.142138 11272 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1211 10:38:15.625551 11272 solver.cpp:218] Iteration 105100 (15.4257 iter/s, 6.48267s/100 iters), loss = 0.277065
I1211 10:38:15.625551 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:38:15.625551 11272 solver.cpp:237]     Train net output #1: loss = 0.277065 (* 1 = 0.277065 loss)
I1211 10:38:15.625551 11272 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1211 10:38:22.097564 11272 solver.cpp:218] Iteration 105200 (15.4517 iter/s, 6.47176s/100 iters), loss = 0.340002
I1211 10:38:22.097564 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:38:22.097564 11272 solver.cpp:237]     Train net output #1: loss = 0.340002 (* 1 = 0.340002 loss)
I1211 10:38:22.097564 11272 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1211 10:38:28.608800 11272 solver.cpp:218] Iteration 105300 (15.3602 iter/s, 6.51035s/100 iters), loss = 0.329342
I1211 10:38:28.608800 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:38:28.608800 11272 solver.cpp:237]     Train net output #1: loss = 0.329342 (* 1 = 0.329342 loss)
I1211 10:38:28.608800 11272 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1211 10:38:35.180585 11272 solver.cpp:218] Iteration 105400 (15.2168 iter/s, 6.5717s/100 iters), loss = 0.340864
I1211 10:38:35.180585 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:38:35.181087 11272 solver.cpp:237]     Train net output #1: loss = 0.340864 (* 1 = 0.340864 loss)
I1211 10:38:35.181087 11272 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1211 10:38:41.352210 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:38:41.609719 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105500.caffemodel
I1211 10:38:41.627709 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105500.solverstate
I1211 10:38:41.632709 11272 solver.cpp:330] Iteration 105500, Testing net (#0)
I1211 10:38:41.632709 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:38:43.184710 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:38:43.246210 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6797
I1211 10:38:43.246210 11272 solver.cpp:397]     Test net output #1: loss = 1.23283 (* 1 = 1.23283 loss)
I1211 10:38:43.307709 11272 solver.cpp:218] Iteration 105500 (12.3058 iter/s, 8.12623s/100 iters), loss = 0.435491
I1211 10:38:43.307709 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:38:43.307709 11272 solver.cpp:237]     Train net output #1: loss = 0.435491 (* 1 = 0.435491 loss)
I1211 10:38:43.307709 11272 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1211 10:38:49.789023 11272 solver.cpp:218] Iteration 105600 (15.4302 iter/s, 6.48079s/100 iters), loss = 0.367818
I1211 10:38:49.789023 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:38:49.789023 11272 solver.cpp:237]     Train net output #1: loss = 0.367818 (* 1 = 0.367818 loss)
I1211 10:38:49.789023 11272 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1211 10:38:56.263048 11272 solver.cpp:218] Iteration 105700 (15.4471 iter/s, 6.47371s/100 iters), loss = 0.308179
I1211 10:38:56.263048 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:38:56.263048 11272 solver.cpp:237]     Train net output #1: loss = 0.308179 (* 1 = 0.308179 loss)
I1211 10:38:56.263048 11272 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1211 10:39:02.712657 11272 solver.cpp:218] Iteration 105800 (15.5055 iter/s, 6.44934s/100 iters), loss = 0.374957
I1211 10:39:02.712657 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:39:02.712657 11272 solver.cpp:237]     Train net output #1: loss = 0.374957 (* 1 = 0.374957 loss)
I1211 10:39:02.712657 11272 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1211 10:39:09.202872 11272 solver.cpp:218] Iteration 105900 (15.409 iter/s, 6.4897s/100 iters), loss = 0.353147
I1211 10:39:09.202872 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:39:09.202872 11272 solver.cpp:237]     Train net output #1: loss = 0.353147 (* 1 = 0.353147 loss)
I1211 10:39:09.202872 11272 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1211 10:39:15.441280 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:39:15.694299 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106000.caffemodel
I1211 10:39:15.710799 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106000.solverstate
I1211 10:39:15.715798 11272 solver.cpp:330] Iteration 106000, Testing net (#0)
I1211 10:39:15.715798 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:39:17.261298 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:39:17.322299 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6779
I1211 10:39:17.322299 11272 solver.cpp:397]     Test net output #1: loss = 1.24525 (* 1 = 1.24525 loss)
I1211 10:39:17.383798 11272 solver.cpp:218] Iteration 106000 (12.2248 iter/s, 8.18007s/100 iters), loss = 0.384177
I1211 10:39:17.383798 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:39:17.383798 11272 solver.cpp:237]     Train net output #1: loss = 0.384177 (* 1 = 0.384177 loss)
I1211 10:39:17.383798 11272 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1211 10:39:23.869329 11272 solver.cpp:218] Iteration 106100 (15.4198 iter/s, 6.48518s/100 iters), loss = 0.316928
I1211 10:39:23.869329 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:39:23.869329 11272 solver.cpp:237]     Train net output #1: loss = 0.316928 (* 1 = 0.316928 loss)
I1211 10:39:23.869329 11272 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1211 10:39:30.358873 11272 solver.cpp:218] Iteration 106200 (15.411 iter/s, 6.48889s/100 iters), loss = 0.229416
I1211 10:39:30.358873 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1211 10:39:30.358873 11272 solver.cpp:237]     Train net output #1: loss = 0.229416 (* 1 = 0.229416 loss)
I1211 10:39:30.358873 11272 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1211 10:39:36.859623 11272 solver.cpp:218] Iteration 106300 (15.3834 iter/s, 6.50053s/100 iters), loss = 0.410433
I1211 10:39:36.859623 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:39:36.859623 11272 solver.cpp:237]     Train net output #1: loss = 0.410433 (* 1 = 0.410433 loss)
I1211 10:39:36.859623 11272 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1211 10:39:43.342909 11272 solver.cpp:218] Iteration 106400 (15.4254 iter/s, 6.4828s/100 iters), loss = 0.371403
I1211 10:39:43.342909 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:39:43.342909 11272 solver.cpp:237]     Train net output #1: loss = 0.371403 (* 1 = 0.371403 loss)
I1211 10:39:43.342909 11272 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1211 10:39:49.501022 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:39:49.757768 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106500.caffemodel
I1211 10:39:49.774269 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106500.solverstate
I1211 10:39:49.779270 11272 solver.cpp:330] Iteration 106500, Testing net (#0)
I1211 10:39:49.779270 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:39:51.331267 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:39:51.392765 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6756
I1211 10:39:51.392765 11272 solver.cpp:397]     Test net output #1: loss = 1.24167 (* 1 = 1.24167 loss)
I1211 10:39:51.453766 11272 solver.cpp:218] Iteration 106500 (12.33 iter/s, 8.11031s/100 iters), loss = 0.332125
I1211 10:39:51.453766 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:39:51.453766 11272 solver.cpp:237]     Train net output #1: loss = 0.332125 (* 1 = 0.332125 loss)
I1211 10:39:51.453766 11272 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1211 10:39:57.930822 11272 solver.cpp:218] Iteration 106600 (15.4407 iter/s, 6.47638s/100 iters), loss = 0.418292
I1211 10:39:57.930822 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:39:57.930822 11272 solver.cpp:237]     Train net output #1: loss = 0.418292 (* 1 = 0.418292 loss)
I1211 10:39:57.930822 11272 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1211 10:40:04.458526 11272 solver.cpp:218] Iteration 106700 (15.3203 iter/s, 6.52729s/100 iters), loss = 0.238605
I1211 10:40:04.458526 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 10:40:04.458526 11272 solver.cpp:237]     Train net output #1: loss = 0.238605 (* 1 = 0.238605 loss)
I1211 10:40:04.458526 11272 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1211 10:40:10.971612 11272 solver.cpp:218] Iteration 106800 (15.3551 iter/s, 6.51249s/100 iters), loss = 0.37175
I1211 10:40:10.971612 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:40:10.971612 11272 solver.cpp:237]     Train net output #1: loss = 0.37175 (* 1 = 0.37175 loss)
I1211 10:40:10.971612 11272 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1211 10:40:17.320449 11272 solver.cpp:218] Iteration 106900 (15.751 iter/s, 6.34882s/100 iters), loss = 0.378006
I1211 10:40:17.320449 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:40:17.320449 11272 solver.cpp:237]     Train net output #1: loss = 0.378006 (* 1 = 0.378006 loss)
I1211 10:40:17.320449 11272 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1211 10:40:23.381696 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:40:23.631772 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107000.caffemodel
I1211 10:40:23.646772 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107000.solverstate
I1211 10:40:23.651796 11272 solver.cpp:330] Iteration 107000, Testing net (#0)
I1211 10:40:23.651796 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:40:25.177258 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:40:25.239251 11272 solver.cpp:397]     Test net output #0: accuracy = 0.679
I1211 10:40:25.239251 11272 solver.cpp:397]     Test net output #1: loss = 1.24166 (* 1 = 1.24166 loss)
I1211 10:40:25.303280 11272 solver.cpp:218] Iteration 107000 (12.5285 iter/s, 7.9818s/100 iters), loss = 0.440214
I1211 10:40:25.303280 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:40:25.303280 11272 solver.cpp:237]     Train net output #1: loss = 0.440214 (* 1 = 0.440214 loss)
I1211 10:40:25.303280 11272 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1211 10:40:31.802913 11272 solver.cpp:218] Iteration 107100 (15.3854 iter/s, 6.49967s/100 iters), loss = 0.337357
I1211 10:40:31.802913 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:40:31.802913 11272 solver.cpp:237]     Train net output #1: loss = 0.337357 (* 1 = 0.337357 loss)
I1211 10:40:31.802913 11272 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1211 10:40:38.325199 11272 solver.cpp:218] Iteration 107200 (15.3343 iter/s, 6.52134s/100 iters), loss = 0.305533
I1211 10:40:38.325199 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:40:38.325199 11272 solver.cpp:237]     Train net output #1: loss = 0.305533 (* 1 = 0.305533 loss)
I1211 10:40:38.325199 11272 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1211 10:40:44.863189 11272 solver.cpp:218] Iteration 107300 (15.2963 iter/s, 6.53754s/100 iters), loss = 0.36386
I1211 10:40:44.863189 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:40:44.863189 11272 solver.cpp:237]     Train net output #1: loss = 0.36386 (* 1 = 0.36386 loss)
I1211 10:40:44.863189 11272 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1211 10:40:51.354904 11272 solver.cpp:218] Iteration 107400 (15.4052 iter/s, 6.49133s/100 iters), loss = 0.394147
I1211 10:40:51.354904 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:40:51.354904 11272 solver.cpp:237]     Train net output #1: loss = 0.394147 (* 1 = 0.394147 loss)
I1211 10:40:51.354904 11272 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1211 10:40:57.516031 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:40:57.778029 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107500.caffemodel
I1211 10:40:57.794529 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107500.solverstate
I1211 10:40:57.799530 11272 solver.cpp:330] Iteration 107500, Testing net (#0)
I1211 10:40:57.799530 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:40:59.382530 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:40:59.444530 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6769
I1211 10:40:59.444530 11272 solver.cpp:397]     Test net output #1: loss = 1.24891 (* 1 = 1.24891 loss)
I1211 10:40:59.507030 11272 solver.cpp:218] Iteration 107500 (12.2678 iter/s, 8.15141s/100 iters), loss = 0.288535
I1211 10:40:59.507030 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:40:59.507030 11272 solver.cpp:237]     Train net output #1: loss = 0.288535 (* 1 = 0.288535 loss)
I1211 10:40:59.507030 11272 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1211 10:41:06.098995 11272 solver.cpp:218] Iteration 107600 (15.1706 iter/s, 6.59168s/100 iters), loss = 0.301773
I1211 10:41:06.098995 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 10:41:06.099508 11272 solver.cpp:237]     Train net output #1: loss = 0.301773 (* 1 = 0.301773 loss)
I1211 10:41:06.099508 11272 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1211 10:41:12.702149 11272 solver.cpp:218] Iteration 107700 (15.1462 iter/s, 6.60231s/100 iters), loss = 0.255459
I1211 10:41:12.702149 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 10:41:12.702149 11272 solver.cpp:237]     Train net output #1: loss = 0.255459 (* 1 = 0.255459 loss)
I1211 10:41:12.702149 11272 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1211 10:41:19.291821 11272 solver.cpp:218] Iteration 107800 (15.1759 iter/s, 6.58937s/100 iters), loss = 0.315589
I1211 10:41:19.291821 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:41:19.291821 11272 solver.cpp:237]     Train net output #1: loss = 0.315589 (* 1 = 0.315589 loss)
I1211 10:41:19.291821 11272 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1211 10:41:25.885272 11272 solver.cpp:218] Iteration 107900 (15.1678 iter/s, 6.59291s/100 iters), loss = 0.373239
I1211 10:41:25.885272 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:41:25.885272 11272 solver.cpp:237]     Train net output #1: loss = 0.373239 (* 1 = 0.373239 loss)
I1211 10:41:25.885272 11272 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1211 10:41:32.148017 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:41:32.407513 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108000.caffemodel
I1211 10:41:32.424013 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108000.solverstate
I1211 10:41:32.429013 11272 solver.cpp:330] Iteration 108000, Testing net (#0)
I1211 10:41:32.429013 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:41:34.008014 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:41:34.070513 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6777
I1211 10:41:34.070513 11272 solver.cpp:397]     Test net output #1: loss = 1.24249 (* 1 = 1.24249 loss)
I1211 10:41:34.134012 11272 solver.cpp:218] Iteration 108000 (12.1241 iter/s, 8.24804s/100 iters), loss = 0.396996
I1211 10:41:34.134012 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:41:34.134012 11272 solver.cpp:237]     Train net output #1: loss = 0.396996 (* 1 = 0.396996 loss)
I1211 10:41:34.134012 11272 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1211 10:41:40.712729 11272 solver.cpp:218] Iteration 108100 (15.2021 iter/s, 6.57804s/100 iters), loss = 0.347318
I1211 10:41:40.712729 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:41:40.712729 11272 solver.cpp:237]     Train net output #1: loss = 0.347318 (* 1 = 0.347318 loss)
I1211 10:41:40.712729 11272 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1211 10:41:47.282140 11272 solver.cpp:218] Iteration 108200 (15.2227 iter/s, 6.56913s/100 iters), loss = 0.321299
I1211 10:41:47.282140 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:41:47.282140 11272 solver.cpp:237]     Train net output #1: loss = 0.321299 (* 1 = 0.321299 loss)
I1211 10:41:47.282140 11272 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1211 10:41:53.864377 11272 solver.cpp:218] Iteration 108300 (15.1944 iter/s, 6.58137s/100 iters), loss = 0.316485
I1211 10:41:53.864377 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:41:53.864377 11272 solver.cpp:237]     Train net output #1: loss = 0.316485 (* 1 = 0.316485 loss)
I1211 10:41:53.864377 11272 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1211 10:42:00.447723 11272 solver.cpp:218] Iteration 108400 (15.1902 iter/s, 6.58318s/100 iters), loss = 0.3602
I1211 10:42:00.447723 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:42:00.447723 11272 solver.cpp:237]     Train net output #1: loss = 0.360199 (* 1 = 0.360199 loss)
I1211 10:42:00.447723 11272 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1211 10:42:06.701174 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:42:06.959175 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108500.caffemodel
I1211 10:42:06.974675 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108500.solverstate
I1211 10:42:06.979673 11272 solver.cpp:330] Iteration 108500, Testing net (#0)
I1211 10:42:06.979673 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:42:08.554674 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:42:08.616174 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6758
I1211 10:42:08.616174 11272 solver.cpp:397]     Test net output #1: loss = 1.25502 (* 1 = 1.25502 loss)
I1211 10:42:08.679674 11272 solver.cpp:218] Iteration 108500 (12.1488 iter/s, 8.23125s/100 iters), loss = 0.332017
I1211 10:42:08.679674 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:42:08.679674 11272 solver.cpp:237]     Train net output #1: loss = 0.332017 (* 1 = 0.332017 loss)
I1211 10:42:08.679674 11272 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1211 10:42:15.256073 11272 solver.cpp:218] Iteration 108600 (15.2063 iter/s, 6.57621s/100 iters), loss = 0.294671
I1211 10:42:15.256573 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:42:15.256573 11272 solver.cpp:237]     Train net output #1: loss = 0.294671 (* 1 = 0.294671 loss)
I1211 10:42:15.256573 11272 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1211 10:42:21.825258 11272 solver.cpp:218] Iteration 108700 (15.2248 iter/s, 6.56821s/100 iters), loss = 0.274681
I1211 10:42:21.825258 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:42:21.825258 11272 solver.cpp:237]     Train net output #1: loss = 0.274681 (* 1 = 0.274681 loss)
I1211 10:42:21.825258 11272 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1211 10:42:28.403260 11272 solver.cpp:218] Iteration 108800 (15.2037 iter/s, 6.57735s/100 iters), loss = 0.314238
I1211 10:42:28.403260 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:42:28.403260 11272 solver.cpp:237]     Train net output #1: loss = 0.314238 (* 1 = 0.314238 loss)
I1211 10:42:28.403260 11272 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1211 10:42:34.973261 11272 solver.cpp:218] Iteration 108900 (15.2211 iter/s, 6.56981s/100 iters), loss = 0.396052
I1211 10:42:34.973261 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:42:34.973261 11272 solver.cpp:237]     Train net output #1: loss = 0.396052 (* 1 = 0.396052 loss)
I1211 10:42:34.973261 11272 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1211 10:42:41.233537 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:42:41.493536 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109000.caffemodel
I1211 10:42:41.509536 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109000.solverstate
I1211 10:42:41.514036 11272 solver.cpp:330] Iteration 109000, Testing net (#0)
I1211 10:42:41.514539 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:42:43.089037 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:42:43.151551 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6767
I1211 10:42:43.151551 11272 solver.cpp:397]     Test net output #1: loss = 1.26184 (* 1 = 1.26184 loss)
I1211 10:42:43.215536 11272 solver.cpp:218] Iteration 109000 (12.1338 iter/s, 8.24141s/100 iters), loss = 0.311491
I1211 10:42:43.215536 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:42:43.215536 11272 solver.cpp:237]     Train net output #1: loss = 0.311491 (* 1 = 0.311491 loss)
I1211 10:42:43.215536 11272 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1211 10:42:49.781108 11272 solver.cpp:218] Iteration 109100 (15.2315 iter/s, 6.56534s/100 iters), loss = 0.360891
I1211 10:42:49.781108 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:42:49.781108 11272 solver.cpp:237]     Train net output #1: loss = 0.360891 (* 1 = 0.360891 loss)
I1211 10:42:49.781108 11272 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1211 10:42:56.366694 11272 solver.cpp:218] Iteration 109200 (15.186 iter/s, 6.58499s/100 iters), loss = 0.326983
I1211 10:42:56.366694 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:42:56.366694 11272 solver.cpp:237]     Train net output #1: loss = 0.326983 (* 1 = 0.326983 loss)
I1211 10:42:56.366694 11272 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1211 10:43:02.947937 11272 solver.cpp:218] Iteration 109300 (15.1961 iter/s, 6.58064s/100 iters), loss = 0.33613
I1211 10:43:02.947937 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:43:02.947937 11272 solver.cpp:237]     Train net output #1: loss = 0.33613 (* 1 = 0.33613 loss)
I1211 10:43:02.947937 11272 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1211 10:43:09.532176 11272 solver.cpp:218] Iteration 109400 (15.1881 iter/s, 6.58412s/100 iters), loss = 0.339371
I1211 10:43:09.532675 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:43:09.532675 11272 solver.cpp:237]     Train net output #1: loss = 0.339371 (* 1 = 0.339371 loss)
I1211 10:43:09.532675 11272 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1211 10:43:15.786967 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:43:16.045969 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109500.caffemodel
I1211 10:43:16.062467 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109500.solverstate
I1211 10:43:16.067468 11272 solver.cpp:330] Iteration 109500, Testing net (#0)
I1211 10:43:16.067468 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:43:17.640467 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:43:17.703467 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6786
I1211 10:43:17.703969 11272 solver.cpp:397]     Test net output #1: loss = 1.25835 (* 1 = 1.25835 loss)
I1211 10:43:17.766468 11272 solver.cpp:218] Iteration 109500 (12.1456 iter/s, 8.23345s/100 iters), loss = 0.36125
I1211 10:43:17.766468 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:43:17.766468 11272 solver.cpp:237]     Train net output #1: loss = 0.361249 (* 1 = 0.361249 loss)
I1211 10:43:17.766468 11272 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1211 10:43:24.335530 11272 solver.cpp:218] Iteration 109600 (15.2235 iter/s, 6.56878s/100 iters), loss = 0.409492
I1211 10:43:24.335530 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:43:24.335530 11272 solver.cpp:237]     Train net output #1: loss = 0.409492 (* 1 = 0.409492 loss)
I1211 10:43:24.335530 11272 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1211 10:43:30.911561 11272 solver.cpp:218] Iteration 109700 (15.2081 iter/s, 6.57544s/100 iters), loss = 0.258688
I1211 10:43:30.911561 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:43:30.911561 11272 solver.cpp:237]     Train net output #1: loss = 0.258688 (* 1 = 0.258688 loss)
I1211 10:43:30.911561 11272 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1211 10:43:37.474823 11272 solver.cpp:218] Iteration 109800 (15.2381 iter/s, 6.56249s/100 iters), loss = 0.35613
I1211 10:43:37.474823 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:43:37.474823 11272 solver.cpp:237]     Train net output #1: loss = 0.35613 (* 1 = 0.35613 loss)
I1211 10:43:37.474823 11272 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1211 10:43:44.043421 11272 solver.cpp:218] Iteration 109900 (15.2247 iter/s, 6.5683s/100 iters), loss = 0.391647
I1211 10:43:44.043421 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 10:43:44.043421 11272 solver.cpp:237]     Train net output #1: loss = 0.391647 (* 1 = 0.391647 loss)
I1211 10:43:44.043421 11272 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1211 10:43:50.292467 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:43:50.552469 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_110000.caffemodel
I1211 10:43:50.568969 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_110000.solverstate
I1211 10:43:50.574470 11272 solver.cpp:330] Iteration 110000, Testing net (#0)
I1211 10:43:50.574470 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:43:52.141587 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:43:52.203574 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6768
I1211 10:43:52.203574 11272 solver.cpp:397]     Test net output #1: loss = 1.26488 (* 1 = 1.26488 loss)
I1211 10:43:52.267565 11272 solver.cpp:218] Iteration 110000 (12.1601 iter/s, 8.22359s/100 iters), loss = 0.352098
I1211 10:43:52.267565 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:43:52.267565 11272 solver.cpp:237]     Train net output #1: loss = 0.352098 (* 1 = 0.352098 loss)
I1211 10:43:52.267565 11272 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1211 10:43:58.841513 11272 solver.cpp:218] Iteration 110100 (15.2131 iter/s, 6.57328s/100 iters), loss = 0.355868
I1211 10:43:58.841513 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:43:58.841513 11272 solver.cpp:237]     Train net output #1: loss = 0.355868 (* 1 = 0.355868 loss)
I1211 10:43:58.841513 11272 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1211 10:44:05.421942 11272 solver.cpp:218] Iteration 110200 (15.1974 iter/s, 6.58008s/100 iters), loss = 0.234146
I1211 10:44:05.421942 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:44:05.421942 11272 solver.cpp:237]     Train net output #1: loss = 0.234146 (* 1 = 0.234146 loss)
I1211 10:44:05.421942 11272 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1211 10:44:11.985635 11272 solver.cpp:218] Iteration 110300 (15.236 iter/s, 6.56341s/100 iters), loss = 0.270317
I1211 10:44:11.985635 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:44:11.985635 11272 solver.cpp:237]     Train net output #1: loss = 0.270317 (* 1 = 0.270317 loss)
I1211 10:44:11.985635 11272 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1211 10:44:18.560078 11272 solver.cpp:218] Iteration 110400 (15.2115 iter/s, 6.57396s/100 iters), loss = 0.348515
I1211 10:44:18.560078 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:44:18.560078 11272 solver.cpp:237]     Train net output #1: loss = 0.348514 (* 1 = 0.348514 loss)
I1211 10:44:18.560569 11272 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1211 10:44:24.810638 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:44:25.070138 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_110500.caffemodel
I1211 10:44:25.085649 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_110500.solverstate
I1211 10:44:25.090641 11272 solver.cpp:330] Iteration 110500, Testing net (#0)
I1211 10:44:25.090641 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:44:26.658639 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:44:26.721638 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6761
I1211 10:44:26.721638 11272 solver.cpp:397]     Test net output #1: loss = 1.25537 (* 1 = 1.25537 loss)
I1211 10:44:26.784639 11272 solver.cpp:218] Iteration 110500 (12.1596 iter/s, 8.22396s/100 iters), loss = 0.249031
I1211 10:44:26.784639 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:44:26.784639 11272 solver.cpp:237]     Train net output #1: loss = 0.249031 (* 1 = 0.249031 loss)
I1211 10:44:26.784639 11272 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1211 10:44:33.343603 11272 solver.cpp:218] Iteration 110600 (15.2475 iter/s, 6.55843s/100 iters), loss = 0.371679
I1211 10:44:33.343603 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:44:33.343603 11272 solver.cpp:237]     Train net output #1: loss = 0.371679 (* 1 = 0.371679 loss)
I1211 10:44:33.343603 11272 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1211 10:44:39.904436 11272 solver.cpp:218] Iteration 110700 (15.2435 iter/s, 6.56016s/100 iters), loss = 0.258551
I1211 10:44:39.904436 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:44:39.904436 11272 solver.cpp:237]     Train net output #1: loss = 0.258551 (* 1 = 0.258551 loss)
I1211 10:44:39.904436 11272 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1211 10:44:46.479478 11272 solver.cpp:218] Iteration 110800 (15.2094 iter/s, 6.5749s/100 iters), loss = 0.318119
I1211 10:44:46.479979 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:44:46.479979 11272 solver.cpp:237]     Train net output #1: loss = 0.318119 (* 1 = 0.318119 loss)
I1211 10:44:46.479979 11272 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1211 10:44:53.052058 11272 solver.cpp:218] Iteration 110900 (15.2167 iter/s, 6.57174s/100 iters), loss = 0.399623
I1211 10:44:53.052058 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:44:53.052058 11272 solver.cpp:237]     Train net output #1: loss = 0.399623 (* 1 = 0.399623 loss)
I1211 10:44:53.052058 11272 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1211 10:44:59.297674 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:44:59.556951 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_111000.caffemodel
I1211 10:44:59.572471 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_111000.solverstate
I1211 10:44:59.577471 11272 solver.cpp:330] Iteration 111000, Testing net (#0)
I1211 10:44:59.577471 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:45:01.152004 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:45:01.211993 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6753
I1211 10:45:01.211993 11272 solver.cpp:397]     Test net output #1: loss = 1.26507 (* 1 = 1.26507 loss)
I1211 10:45:01.274996 11272 solver.cpp:218] Iteration 111000 (12.1619 iter/s, 8.22241s/100 iters), loss = 0.264626
I1211 10:45:01.274996 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:45:01.274996 11272 solver.cpp:237]     Train net output #1: loss = 0.264626 (* 1 = 0.264626 loss)
I1211 10:45:01.274996 11272 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1211 10:45:07.851531 11272 solver.cpp:218] Iteration 111100 (15.2067 iter/s, 6.57606s/100 iters), loss = 0.282746
I1211 10:45:07.851531 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:45:07.851531 11272 solver.cpp:237]     Train net output #1: loss = 0.282746 (* 1 = 0.282746 loss)
I1211 10:45:07.851531 11272 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1211 10:45:14.419119 11272 solver.cpp:218] Iteration 111200 (15.2271 iter/s, 6.56725s/100 iters), loss = 0.265382
I1211 10:45:14.419119 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:45:14.419119 11272 solver.cpp:237]     Train net output #1: loss = 0.265382 (* 1 = 0.265382 loss)
I1211 10:45:14.419119 11272 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1211 10:45:20.993697 11272 solver.cpp:218] Iteration 111300 (15.2108 iter/s, 6.57429s/100 iters), loss = 0.237561
I1211 10:45:20.993697 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 10:45:20.993697 11272 solver.cpp:237]     Train net output #1: loss = 0.237561 (* 1 = 0.237561 loss)
I1211 10:45:20.993697 11272 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1211 10:45:27.566510 11272 solver.cpp:218] Iteration 111400 (15.2156 iter/s, 6.57218s/100 iters), loss = 0.335134
I1211 10:45:27.566510 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:45:27.566510 11272 solver.cpp:237]     Train net output #1: loss = 0.335134 (* 1 = 0.335134 loss)
I1211 10:45:27.566510 11272 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1211 10:45:33.816927 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:45:34.075928 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_111500.caffemodel
I1211 10:45:34.091429 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_111500.solverstate
I1211 10:45:34.097430 11272 solver.cpp:330] Iteration 111500, Testing net (#0)
I1211 10:45:34.097929 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:45:35.662428 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:45:35.724930 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6763
I1211 10:45:35.724930 11272 solver.cpp:397]     Test net output #1: loss = 1.26614 (* 1 = 1.26614 loss)
I1211 10:45:35.788429 11272 solver.cpp:218] Iteration 111500 (12.1634 iter/s, 8.22139s/100 iters), loss = 0.363584
I1211 10:45:35.788429 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:45:35.788429 11272 solver.cpp:237]     Train net output #1: loss = 0.363584 (* 1 = 0.363584 loss)
I1211 10:45:35.788429 11272 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1211 10:45:42.360625 11272 solver.cpp:218] Iteration 111600 (15.2161 iter/s, 6.57198s/100 iters), loss = 0.290847
I1211 10:45:42.361124 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:45:42.361124 11272 solver.cpp:237]     Train net output #1: loss = 0.290847 (* 1 = 0.290847 loss)
I1211 10:45:42.361124 11272 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1211 10:45:48.932585 11272 solver.cpp:218] Iteration 111700 (15.2182 iter/s, 6.57107s/100 iters), loss = 0.231451
I1211 10:45:48.932585 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1211 10:45:48.932585 11272 solver.cpp:237]     Train net output #1: loss = 0.231451 (* 1 = 0.231451 loss)
I1211 10:45:48.932585 11272 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1211 10:45:55.502748 11272 solver.cpp:218] Iteration 111800 (15.2206 iter/s, 6.57006s/100 iters), loss = 0.296537
I1211 10:45:55.503248 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:45:55.503248 11272 solver.cpp:237]     Train net output #1: loss = 0.296537 (* 1 = 0.296537 loss)
I1211 10:45:55.503248 11272 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1211 10:46:02.079288 11272 solver.cpp:218] Iteration 111900 (15.2073 iter/s, 6.57577s/100 iters), loss = 0.287986
I1211 10:46:02.079288 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:46:02.079288 11272 solver.cpp:237]     Train net output #1: loss = 0.287986 (* 1 = 0.287986 loss)
I1211 10:46:02.079288 11272 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1211 10:46:08.328917 13688 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:46:08.588415 11272 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_112000.caffemodel
I1211 10:46:08.604415 11272 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_112000.solverstate
I1211 10:46:08.609414 11272 solver.cpp:330] Iteration 112000, Testing net (#0)
I1211 10:46:08.609414 11272 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:46:10.180600 17304 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:46:10.243100 11272 solver.cpp:397]     Test net output #0: accuracy = 0.6804
I1211 10:46:10.243100 11272 solver.cpp:397]     Test net output #1: loss = 1.26165 (* 1 = 1.26165 loss)
I1211 10:46:10.307098 11272 solver.cpp:218] Iteration 112000 (12.1544 iter/s, 8.22748s/100 iters), loss = 0.341712
I1211 10:46:10.307098 11272 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:46:10.307098 11272 solver.cpp:237]     Train net output #1: loss = 0.341712 (* 1 = 0.341712 loss)
I1211 10:46:10.307098 11272 sgd_solver.cpp:105] Iteration 112000, lr = 0.001
I1211 10:4